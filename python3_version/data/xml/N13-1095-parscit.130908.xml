<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.916201">
Distant Supervision for Relation Extraction
with an Incomplete Knowledge Base
</title>
<author confidence="0.9237">
Bonan Min, Ralph Grishman, Li Wan
</author>
<affiliation confidence="0.817317">
New York University
</affiliation>
<address confidence="0.98234">
New York, NY 10003
</address>
<email confidence="0.820526">
{min,grishman,wanli}
@cs.nyu.edu
</email>
<note confidence="0.8856545">
Chang Wang, David Gondek
IBM T. J. Watson Research Center
</note>
<author confidence="0.707574">
Yorktown Heights, NY 10598
</author>
<affiliation confidence="0.540572">
{wangchan,dgondek}
</affiliation>
<email confidence="0.994158">
@us.ibm.com
</email>
<sectionHeader confidence="0.998552" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9987348">
Distant supervision, heuristically labeling a
corpus using a knowledge base, has emerged
as a popular choice for training relation ex-
tractors. In this paper, we show that a sig-
nificant number of “negative“ examples gen-
erated by the labeling process are false neg-
atives because the knowledge base is incom-
plete. Therefore the heuristic for generating
negative examples has a serious flaw. Building
on a state-of-the-art distantly-supervised ex-
traction algorithm, we proposed an algorithm
that learns from only positive and unlabeled
labels at the pair-of-entity level. Experimental
results demonstrate its advantage over existing
algorithms.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999635785714286">
Relation Extraction is a well-studied problem
(Miller et al., 2000; Zhou et al., 2005; Kambhatla,
2004; Min et al., 2012a). Recently, Distant Super-
vision (DS) (Craven and Kumlien, 1999; Mintz et
al., 2009) has emerged to be a popular choice for
training relation extractors without using manually
labeled data. It automatically generates training ex-
amples by labeling relation mentions1 in the source
corpus according to whether the argument pair is
listed in the target relational tables in a knowledge
base (KB). This method significantly reduces human
efforts for relation extraction.
The labeling heuristic has a serious flaw. Knowl-
edge bases are usually highly incomplete. For exam-
</bodyText>
<subsectionHeader confidence="0.309439">
1An occurrence of a pair of entities with the source sentence.
</subsectionHeader>
<bodyText confidence="0.999956083333333">
ple, 93.8% of persons from Freebase2 have no place
of birth, and 78.5% have no nationality (section 3).
Previous work typically assumes that if the argument
entity pair is not listed in the KB as having a re-
lation, all the corresponding relation mentions are
considered negative examples.3 This crude assump-
tion labeled many entity pairs as negative when in
fact some of their mentions express a relation. The
number of such false negative matches even exceeds
the number of positive pairs, by 3 to 10 times, lead-
ing to a significant problem for training. Previous
approaches (Riedel et al., 2010; Hoffmann et al.,
2011; Surdeanu et al., 2012) bypassed this problem
by heavily under-sampling the “negative“ class.
We instead deal with a learning scenario where we
only have entity-pair level labels that are either posi-
tive or unlabeled. We proposed an extension to Sur-
deanu et al. (2012) that can train on this dataset. Our
contribution also includes an analysis on the incom-
pleteness of Freebase and the false negative match
rate in two datasets of labeled examples generated
by DS. Experimental results on a realistic and chal-
lenging dataset demonstrate the advantage of the al-
gorithm over existing solutions.
</bodyText>
<sectionHeader confidence="0.999887" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9955685">
Distant supervision was first proposed by Craven
and Kumlien (1999) in the biomedical domain.
</bodyText>
<footnote confidence="0.997885571428571">
2Freebase is a large collaboratively-edited KB. It is available
at http://www.freebase.com.
3There are variants of labeling heuristics. For example, Sur-
deanu et al. (2011) and Sun et al. (2011) use a pair &lt; e, v &gt;
as a negative example, when it is not listed in Freebase, but e is
listed with a different v′. These assumptions are also problem-
atic in cases where the relation is not functional.
</footnote>
<page confidence="0.951917">
777
</page>
<subsectionHeader confidence="0.28784">
Proceedings of NAACL-HLT 2013, pages 777–782,
</subsectionHeader>
<bodyText confidence="0.984165">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
Since then, it has gain popularity (Mintz et al., 2009;
Bunescu and Mooney, 2007; Wu and Weld, 2007;
Riedel et al., 2010; Hoffmann et al., 2011; Sur-
deanu et al., 2012; Nguyen and Moschitti, 2011).
To tolerate noisy labels in positive examples, Riedel
et al. (2010) use Multiple Instance Learning (MIL),
which assumes only at-least-one of the relation men-
tions in each “bag“ of mentions sharing a pair of ar-
gument entities which bears a relation, indeed ex-
presses the target relation. MultiR (Hoffmann et
al., 2011) and Multi-Instance Multi-Label (MIML)
learning (Surdeanu et al., 2012) further improve it
to support multiple relations expressed by different
sentences in a bag. Takamatsu et al. (2012) mod-
els the probabilities of a pattern showing relations,
estimated from the heuristically labeled dataset.
Their algorithm removes mentions that match low-
probability patterns. Sun et al. (2011) and Min et
al. (2012b) also estimate the probablities of patterns
showing relations, but instead use them to relabel ex-
amples to their most likely classes. Their approach
can correct highly-confident false negative matches.
</bodyText>
<sectionHeader confidence="0.991128" genericHeader="method">
3 Problem Definition
</sectionHeader>
<bodyText confidence="0.99748715">
Distant Supervision: Given a KB D (a collection
of relational tables r(e1, e2), in which rǫR (R is the
set of relation labels), and &lt; e1, e2 &gt; is a pair of
entities that is known to have relation r) and a cor-
pus C, the key idea of distant supervision is that we
align D to C, label each bag4 of relation mentions
that share argument pair &lt; e1, e2 &gt; with r, other-
wise OTHER. This generates a dataset that has labels
on entity-pair (bag) level. Then a relation extractor
is trained with single-instance learning (by assum-
ing all mentions have the same label as the bag), or
Multiple-Instance Learning (by assuming at-least-
one of the mentions expresses the bag-level label),
or Multi-Instance Multi-Label learning (further as-
suming a bag can have multiple labels) algorithms.
All of these works treat the OTHER class as exam-
ples that are labeled as negative.
The incomplete KB problem: KBs are usually
incomplete because they are manually constructed,
and it is not possible to cover all human knowledge
</bodyText>
<footnote confidence="0.963366666666667">
4A bag is defined as a set of relation mentions sharing the
same entity pair as relation arguments. We will use the terms
bag and entity pair interchangeably in this paper.
</footnote>
<bodyText confidence="0.99785825">
nor stay current. We took frequent relations, which
involve an entity of type PERSON, from Freebase
for analysis. We define the incompleteness a(r) of a
relation r as follows:
</bodyText>
<equation confidence="0.991888">
a(r) =|{e}|−|{e|∃e′,s.t.r(e,e′)ǫD}|
|{e}|
</equation>
<bodyText confidence="0.999808">
a(r) is the percentage of all persons {e} that do
not have an attribute e′ (with which r(e, e′) holds).
Table 1 shows that 93.8% of persons have no place
of birth, and 78.5% of them have no nationality.
These are must-have attributes for a person. This
shows that Freebase is highly incomplete.
</bodyText>
<figure confidence="0.977280857142857">
Freebase relation types Incompleteness
/people/person/education 0.792
/people/person/employment history 0.923
/people/person/nationality* 0.785
/people/person/parents* 0.988
/people/person/place of birth* 0.938
/people/person/places lived* 0.966
</figure>
<tableCaption confidence="0.9762945">
Table 1: The incompleteness of Freebase (* are must-
have attributes for a person).
</tableCaption>
<bodyText confidence="0.999995285714286">
We further investigate the rate of false negative
matches, as the percentage of entity-pairs that are
not listed in Freebase but one of its mentions gen-
erated by DS does express a relation in the tar-
get set of types. We randomly picked 200 unla-
beled bags5 from each of the two datasets (Riedel
et al., 2010; Surdeanu et al., 2012) generated by DS,
and we manually annotate all relation mentions in
these bags. The result is shown in Table 2, along
with a few examples that indicate a relation holds in
the set of false negative matches (bag-level). Both
datasets have around 10% false negative matches in
the unlabeled set of bags. Taking into considera-
tion that the number of positive bags and unlabeled
bags are highly imbalanced (1:134 and 1:37 in the
Riedel and KBP dataset respectively, before under-
sampling the unlabeled class), the number of false
negative matches are 11 and 4 times the number
of positive bags in Reidel and KBP dataset, respec-
tively. Such a large ratio shows false negatives do
have a significant impact on the learning process.
</bodyText>
<sectionHeader confidence="0.988369" genericHeader="method">
4 A semi-supervised MIML algorithm
</sectionHeader>
<bodyText confidence="0.99529575">
Our goal is to model the bag-level label noise,
caused by the incomplete KB problem, in addition
585% and 95.7% of the bags in the Riedel and KBP datasets
have only one relation mention.
</bodyText>
<page confidence="0.992442">
778
</page>
<table confidence="0.9951467">
Dataset # pos- # positive : % are # positive has human Examples of false negative mentions
(train- itive # unlabeled false : # false assessment
ing) bags negatives negative
no (/location/location/contains)... in Brooklyn ’s Williamsburg.
Riedel 4,700 1:134(BD*) 8.5% 1:11.4
(/people/person/place lived) Cheryl Rogowski , a farmer from
Orange County ...
KBP 183,062 1:37(BD*) 11.5% 1:4 yes (per:city of birth) Juan Martn Maldacena (born September
10, 1968) is a theoretical physicist born in Buenos Aires
(per:employee of)Dave Matthews, from the ABC News,...
</table>
<tableCaption confidence="0.974684333333333">
Table 2: False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset (Surdeanu et al., 2012). All
numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and
5% in Riedel and KBP dataset, respectively.
</tableCaption>
<bodyText confidence="0.999948375">
to modeling the instance-level noise using a 3-layer
MIL or MIML model (e.g., Surdeanu et al. (2012)).
We propose a 4-layer model as shown in Figure 1.
The input to the model is a list of n bags with a
vector of binary labels, either Positive (P), or Un-
labled (U) for each relation r. Our model can be
viewed as a semi-supervised6 framework that ex-
tends a state-of-the-art Multi-Instance Multi-Label
(MIML) model (Surdeanu et al., 2012). Since the
input to previous MIML models are bags with per-
relation binary labels of either Positive (P) or Neg-
ative (N), we add a set of latent variables ℓ which
models the true bag-level labels, to bridge the ob-
served bag labels y and the MIML layers. We con-
sider this as our main contribution to the model. Our
hierarchical model is shown in Figure 1.
</bodyText>
<figureCaption confidence="0.998127">
Figure 1: Plate diagram of our model.
</figureCaption>
<bodyText confidence="0.9970846">
Let i, j be the index in the bag and mention level,
respectively. Following Surdeanu et al. (2012), we
model mention-level extraction p(zrij|xij; wz) and
multi-instance multi-label aggregation p(ℓri |zi; wrℓ)
in the bottom 3 layers. We define:
</bodyText>
<listItem confidence="0.9989755">
• r is a relation label. rcR ∪ {OTHER}, in
which OTHER denotes no relation expressed.
• yri c{P, U}: r holds for ith bag or the bag is
unlabeled.
</listItem>
<footnote confidence="0.698245333333333">
6We use the term semi-supervised because the algorithm
uses unlabeled bags but existing solutions requires bags to be
labeled either positive or negative.
</footnote>
<listItem confidence="0.82351">
• ℓri c{P, N}: a hidden variable that denotes
whether r holds for the ith bag.
• 0 is an observed constant controlling the total
number of bags whose latent label is positive.
</listItem>
<bodyText confidence="0.989028">
We define the following conditional probabilities:
</bodyText>
<equation confidence="0.96357475">
{ 1/2 if yri = P ∧ ℓri = P;
1/2 if yri = U ∧ ℓri = P;
1 if yri = U ∧ ℓri = N;
0 otherwise ;
</equation>
<bodyText confidence="0.9999705">
It encodes the constraints between true bag-
level labels and the entity pair labels in the KB.
</bodyText>
<listItem confidence="0.796749">
• p(0|ℓ) ∼ N(E 1 E n WR i ,P), k) where
S(x, y) = 1 if x = y, 0 otherwise. k is a large
number. 0 is the fraction of the bags that are
positive. It is an observed parameter that de-
pends on both the source corpus and the KB
used.
</listItem>
<bodyText confidence="0.648858333333333">
Similar to Surdeanu et al. (2012), we also define
the following parameters and conditional probabili-
ties (details are in Surdeanu et al. (2012)):
</bodyText>
<listItem confidence="0.953575210526316">
• zijcR ∪ {OTHER}: a latent variable that de-
notes the relation type of the jth mention in the
ith bag.
• xij is the feature representation of the jth rela-
tion mention in the ith bag. We use the set of
features in Surdeanu et al. (2012).
• wz is the weight vector for the multi-class rela-
tion mention-level classifier.
• wr ℓ is the weight vector for the rth binary top-
level aggregation classifier (from mention la-
bels to bag-level prediction). We use wℓ to rep-
resent wP, wt, ...w|R|
ℓ .
• p(ℓri |zi; wrℓ) ∼ Bern(fℓ(wrℓ, zi)) where fℓ is
probability produced by the rth top-level clas-
sifier, from the mention-label level to the bag-
label level.
• p(zrij|xij; wz) ∼ Multi(fz(wz, xij)) where fz
• p(yri |ℓri) =
</listItem>
<page confidence="0.98867">
779
</page>
<bodyText confidence="0.999801">
is probability produced by the mention-level
classifier, from the mentions to the mention-
label level.7
</bodyText>
<subsectionHeader confidence="0.987473">
4.1 Training
</subsectionHeader>
<bodyText confidence="0.999504666666667">
We use hard Expectation-Maximization (EM) algo-
rithm for training the model. Our objective function
is to maximize log-likelihood:
</bodyText>
<equation confidence="0.900201333333333">
L(wz, wℓ) = logp(y, θ|x; wz, wℓ)
X= log p(y, θ, ℓ|x; wz, wℓ)
ℓ
</equation>
<bodyText confidence="0.977149333333333">
Since solving it exactly involves exploring an expo-
nential assignment space for ℓ, we approximate and
iteratively set ℓ∗ = argℓ maxp(ℓ|y, θ, x; wz, wℓ)
</bodyText>
<equation confidence="0.759022">
p(ℓ|y, θ, x; wz, wℓ) ∝ p(y, θ, ℓ|x; wz, wℓ)
= p(y, θ|ℓ, x)p(ℓ|x; wz, wℓ)
= p(y|ℓ)p(θ|ℓ)p(ℓ|x; wz, wℓ)
Rewriting in log form:
logp(ℓ|y, θ, x; wz, wℓ)
= logp(y|ℓ) + logp(θ|ℓ) + logp(ℓ|x; wz, wℓ)
Algorithm 1 Training (E-step:2-11; M-step:12-15)
</equation>
<listItem confidence="0.968565642857143">
1: for i = 1,2 to T do
2: ℓri ← N for all yri = U and rǫR
3: ℓri ← P for all yri = P and rǫR
4: I = {&lt; i,r &gt; |ℓri = N}; I′ = {&lt; i,r &gt; |ℓri = P}
5: fork = 0,1 to θn − |I′ |do
6: &lt; i′, r′ &gt;= arg max&lt;i,r&gt;ǫI p(ℓr i |xi; wz, wℓ)
ℓr′
7: i′ ← P;I = I\{&lt; i′,r′ &gt;}
8: end for
9: for i = 1, 2 to n do
10: z∗i = arg maxzi p(zi|ℓi, xi; wz, wℓ)
11: end for
12: w∗z = arg max,z PZ 1 P�xzl logp(zij|xij, wz)
13: for all rǫR do
</listItem>
<figure confidence="0.836623571428571">
Pn
14: wr(∗)
ℓ = arg maxwr i=1 p(ℓr i |zi, wr ℓ)
ℓ
15: end for
16: end for
17: return wz, wℓ
</figure>
<footnote confidence="0.903172">
7All classifiers are implemented with L2-regularized logistic
regression with Stanford CoreNLP package.
</footnote>
<bodyText confidence="0.960829">
In the E-step, we do a greedy search (steps 5-8
in algorithm 1) in all p(ℓri |xi; wz, wℓ) and update ℓri
until the second term is maximized. wz, wℓ are the
model weights learned from the previous iteration.
After fixed ℓ, we seek to maximize:
</bodyText>
<equation confidence="0.9833356">
logp(ℓ|xi;wz,wℓ) = Xn logp(ℓi|xi; wz, wℓ)
i=1
Xn X p(ℓi, zi|xi; wz, wℓ)
i=1 log
zi
</equation>
<bodyText confidence="0.9972015">
which can be solved with an approxi-
mate solution in Surdeanu et al. (2012)
(step 9-11): update zi independently with:
z∗i = arg maxzi p(zi|ℓi, xi; wz, wℓ). More details
can be found in Surdeanu et al. (2012).
In the M-step, we retrain both of the mention-
level and the aggregation level classifiers.
The full EM algorithm is shown in algorithm 1.
</bodyText>
<sectionHeader confidence="0.597032" genericHeader="method">
4.2 Inference
</sectionHeader>
<bodyText confidence="0.952397333333333">
Inference on a bag xi is trivial. For each mention:
z∗ij = argzijǫR∪{OTHER} maxp(zij|xij, wz)
Followed by the aggregation (directly with wℓ):
</bodyText>
<equation confidence="0.998488">
yi = argyr
r(∗) i ǫ{P,N} max p(yri |zi; wrℓ)
</equation>
<subsectionHeader confidence="0.994678">
4.3 Implementation details
</subsectionHeader>
<bodyText confidence="0.999986777777778">
We implement our model on top of the
MIML(Surdeanu et al., 2012) code base.8 We
use the same mention-level and aggregate-level
feature sets as Surdeanu et al. (2012). We adopt
the same idea of using cross validation for the E
and M steps to avoid overfitting. We initialize our
algorithm by sampling 5% unlabeled examples as
negative, in essence using 1 epoch of MIML to
initialize. Empirically it performs well.
</bodyText>
<sectionHeader confidence="0.998908" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.996555875">
Data set: We use the KBP (Ji et al., 2011)
dataset9 prepared and publicly released by Surdeanu
et al. (2012) for our experiment since it is 1) large
and realistic, 2) publicly available, 3) most im-
portantly, it is the only dataset that has associated
human-labeled ground truth. Any KB held-out eval-
uation without manual assessment will be signif-
icantly affected by KB incompleteness. In KBP
</bodyText>
<footnote confidence="0.995270333333333">
8Available at http://nlp.stanford.edu/software/mimlre.shtml
9Available from Linguistic Data Consortium (LDC).
http://projects.ldc.upenn.edu/kbp/data/
</footnote>
<equation confidence="0.9976741875">
logp(yri |ℓri
) − k(
n X logp(ℓri |xi; wz, wℓ) + const
+X rǫR
i=1
X
rǫR
Xn
i=1
Pn
i=1
δ(ℓri , P)
P
rǫR
θ)2
n
</equation>
<page confidence="0.993071">
780
</page>
<figureCaption confidence="0.975052666666667">
Figure 2: Performance on the KBP dataset. The figures on the left, middle and right show MIML, Hoffmann, and
Mintz++ compared to the same MIML-Semi curve, respectively. MIML-Semi is shown in red curves (lighter curves in
black and white) while other algorithms are shown in black curves (darker curves in black and white).
</figureCaption>
<bodyText confidence="0.999994">
dataset, the training bags are generated by mapping
Wikipedia (http://en.wikipedia.org) infoboxes (after
merging similar types following the KBP 2011 task
definition) into a large unlabeled corpus (consisting
of 1.5M documents from the KBP source corpus and
a complete snapshot of Wikipedia). The KBP shared
task provided 200 query named entities with their as-
sociated slot values (in total several thousand pairs).
We use 40 queries as development dataset (dev), and
the rest (160 queries) as evaluation dataset. We set
θ = 0.25 by tuning on the dev set and use it in the
experiments. For a fair comparison, we follow Sur-
deanu et al. (2012) and begin by downsampling the
“negative“ class to 5%. We also set T=8 and use
the following noisy-or (for ith bag) of mention-level
probability to rank predicted types (r) of pairs and
plot the precision-recall curves for all experiments.
</bodyText>
<equation confidence="0.770692">
�Probi(r) = 1 − (1 − p(zij = r|Xij;Wz))
j
</equation>
<bodyText confidence="0.999987541666667">
Evaluation: We compare our algorithm (MIML-
semi) to three algorithms: 1) MIML (Surdeanu et
al., 2012), the Multiple-Instance Multiple Label al-
gorithm which labels the bags directly with the KB
(y = E). 2) MultiR (denoted as Hoffmann) (Hoff-
mann et al., 2011), a Multiple-Instance algorithm
that supports overlapping relations. It also imposes
y = E. 3) Mintz++ (Surdeanu et al., 2012), a vari-
ant of the single-instance learning algorithm (section
3). The first two are stat-of-the-art Multi-Instance
Multi-Label algorithms. Mintz++ is a strong base-
line (Surdeanu et al., 2012) and an improved ver-
sion of Mintz et al. (2009). Figure 2 shows that
our algorithm consistently outperforms all three al-
gorithms at almost all recall levels (with the excep-
tion of a very small region in the PR-curve). This
demonstrates that by treating unla-beled data set dif-
ferently and leveraging the missing positive bags,
MIML-semi is able to learn a more accurate model
for extraction. Although the proposed solution is a
specific algorithm, we believe the idea of treating
unlabeled data differently can be incorporated into
any of these algorithms that only use unlabeled data
as negative examples.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999932333333333">
We show that the distant-supervision labeling pro-
cess generates a significant number of false nega-
tives because the knowledge base is incomplete. We
proposed an algorithm that learns from only positive
and unlabeled bags. Experimental results demon-
strate its advantage over existing algorithms.
</bodyText>
<sectionHeader confidence="0.998139" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999547272727273">
Supported in part by the Intelligence Advanced Re-
search Projects Activity (IARPA) via Department of
Interior National Business Center contract number
D11PC20154. The U.S. Government is authorized
to reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright annota-
tion thereon. The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Government.
</bodyText>
<page confidence="0.996438">
781
</page>
<sectionHeader confidence="0.995965" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999250780487805">
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics.
Mark Craven and Johan Kumlien. 1999. Constructing bi-
ological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics.
Heng Ji, Ralph Grishman, and Hoa T. Dang. 2011.
Overview of the TAC 2011 knowledge base popula-
tion track. In Proceedings of the Text Analytics Con-
ference.
Jing Jiang and ChengXiang Zhai. 2007. A systematic ex-
ploration of the feature space for relation extraction. In
Proceedings of HLT-NAACL-2007.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for information extraction. In Proceedings of ACL-
2004.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
NAACL-2000.
Bonan Min, Shuming Shi, Ralph Grishman and Chin-
Yew Lin. 2012a. Ensemble Semantics for Large-scale
Unsupervised Relation Extraction. In Proceedings of
EMNLP-CoNLL 2012.
Bonan Min, Xiang Li, Ralph Grishman and Ang Sun.
2012b. New York University 2012 System for KBP
Slot Filling. In Proceedings of the Text Analysis Con-
ference (TAC) 2012.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the 47th An-
nual Meeting of the Association for Computational
Linguistics.
Truc Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant supervi-
sion from external semantic repositories. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD 10).
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New York University 2011 system for KBP slot
filling. In Proceedings of the Text Analytics Confer-
ence.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2011. Stanfords distantly-
supervised slot-filling system. In Proceedings of the
Text Analytics Conference.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
Christopher D. Manning. 2012. Multi-instance Multi-
label Learning for Relation Extraction. In Proceed-
ings of the 2012 Conference on Empirical Methods in
Natural Language Processing and Natural Language
Learning.
TAC KBP 2011 task definition. 2011. http://nlp
.cs.qc.cuny.edu/kbp/2011/KBP2011 TaskDefinition.pdf
Shingo Takamatsu, Issei Sato, Hiroshi Nakagawa. 2012.
Reducing Wrong Labels in Distant Supervision for Re-
lation Extraction. In Proceedings of 50th Annual Meet-
ing of the Association for Computational Linguistics.
Fei Wu and Daniel S. Weld. 2007. Autonomously seman-
tifying wikipedia. In Proceedings of the International
Conference on Information and Knowledge Manage-
ment (CIKM-2007).
Guodong Zhou, Jian Su, Jie Zhang and Min Zhang. 2005.
Exploring various knowledge in relation extraction. In
Proceedings of ACL-2005.
</reference>
<page confidence="0.99733">
782
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.283796">
<title confidence="0.99478">Distant Supervision for Relation with an Incomplete Knowledge Base</title>
<author confidence="0.654408333333333">Bonan Min</author>
<author confidence="0.654408333333333">Ralph Grishman</author>
<author confidence="0.654408333333333">Li New York New York</author>
<author confidence="0.654408333333333">NY</author>
<email confidence="0.996692">@cs.nyu.edu</email>
<author confidence="0.982175">Chang Wang</author>
<author confidence="0.982175">David</author>
<affiliation confidence="0.8944565">IBM T. J. Watson Research Yorktown Heights, NY</affiliation>
<email confidence="0.999198">@us.ibm.com</email>
<abstract confidence="0.9975126875">Distant supervision, heuristically labeling a corpus using a knowledge base, has emerged as a popular choice for training relation extractors. In this paper, we show that a signumber of examples genby the labeling process are negthe knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm learns from only labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3644" citStr="Bunescu and Mooney, 2007" startWordPosition="569" endWordPosition="572">medical domain. 2Freebase is a large collaboratively-edited KB. It is available at http://www.freebase.com. 3There are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair &lt; e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v′. These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples, Riedel et al. (2010) use Multiple Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Ta</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology.</booktitle>
<contexts>
<context position="1149" citStr="Craven and Kumlien, 1999" startWordPosition="163" endWordPosition="166">enerated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms. 1 Introduction Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a). Recently, Distant Supervision (DS) (Craven and Kumlien, 1999; Mintz et al., 2009) has emerged to be a popular choice for training relation extractors without using manually labeled data. It automatically generates training examples by labeling relation mentions1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base (KB). This method significantly reduces human efforts for relation extraction. The labeling heuristic has a serious flaw. Knowledge bases are usually highly incomplete. For exam1An occurrence of a pair of entities with the source sentence. ple, 93.8% of persons from Freebase</context>
<context position="3009" citStr="Craven and Kumlien (1999)" startWordPosition="465" endWordPosition="468">eavily under-sampling the “negative“ class. We instead deal with a learning scenario where we only have entity-pair level labels that are either positive or unlabeled. We proposed an extension to Surdeanu et al. (2012) that can train on this dataset. Our contribution also includes an analysis on the incompleteness of Freebase and the false negative match rate in two datasets of labeled examples generated by DS. Experimental results on a realistic and challenging dataset demonstrate the advantage of the algorithm over existing solutions. 2 Related Work Distant supervision was first proposed by Craven and Kumlien (1999) in the biomedical domain. 2Freebase is a large collaboratively-edited KB. It is available at http://www.freebase.com. 3There are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair &lt; e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v′. These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et </context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2333" citStr="Hoffmann et al., 2011" startWordPosition="356" endWordPosition="359">. ple, 93.8% of persons from Freebase2 have no place of birth, and 78.5% have no nationality (section 3). Previous work typically assumes that if the argument entity pair is not listed in the KB as having a relation, all the corresponding relation mentions are considered negative examples.3 This crude assumption labeled many entity pairs as negative when in fact some of their mentions express a relation. The number of such false negative matches even exceeds the number of positive pairs, by 3 to 10 times, leading to a significant problem for training. Previous approaches (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) bypassed this problem by heavily under-sampling the “negative“ class. We instead deal with a learning scenario where we only have entity-pair level labels that are either positive or unlabeled. We proposed an extension to Surdeanu et al. (2012) that can train on this dataset. Our contribution also includes an analysis on the incompleteness of Freebase and the false negative match rate in two datasets of labeled examples generated by DS. Experimental results on a realistic and challenging dataset demonstrate the advantage of the algorithm over existing solutions. 2 Rela</context>
<context position="3707" citStr="Hoffmann et al., 2011" startWordPosition="581" endWordPosition="584"> is available at http://www.freebase.com. 3There are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair &lt; e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v′. These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples, Riedel et al. (2010) use Multiple Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Takamatsu et al. (2012) models the probabilities of a pattern sho</context>
<context position="16499" citStr="Hoffmann et al., 2011" startWordPosition="2813" endWordPosition="2817">d use it in the experiments. For a fair comparison, we follow Surdeanu et al. (2012) and begin by downsampling the “negative“ class to 5%. We also set T=8 and use the following noisy-or (for ith bag) of mention-level probability to rank predicted types (r) of pairs and plot the precision-recall curves for all experiments. �Probi(r) = 1 − (1 − p(zij = r|Xij;Wz)) j Evaluation: We compare our algorithm (MIMLsemi) to three algorithms: 1) MIML (Surdeanu et al., 2012), the Multiple-Instance Multiple Label algorithm which labels the bags directly with the KB (y = E). 2) MultiR (denoted as Hoffmann) (Hoffmann et al., 2011), a Multiple-Instance algorithm that supports overlapping relations. It also imposes y = E. 3) Mintz++ (Surdeanu et al., 2012), a variant of the single-instance learning algorithm (section 3). The first two are stat-of-the-art Multi-Instance Multi-Label algorithms. Mintz++ is a strong baseline (Surdeanu et al., 2012) and an improved version of Mintz et al. (2009). Figure 2 shows that our algorithm consistently outperforms all three algorithms at almost all recall levels (with the exception of a very small region in the PR-curve). This demonstrates that by treating unla-beled data set different</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Hoa T Dang</author>
</authors>
<title>knowledge base population track.</title>
<date>2011</date>
<journal>Overview of the TAC</journal>
<booktitle>In Proceedings of the Text Analytics Conference.</booktitle>
<contexts>
<context position="14384" citStr="Ji et al., 2011" startWordPosition="2468" endWordPosition="2471">∪{OTHER} maxp(zij|xij, wz) Followed by the aggregation (directly with wℓ): yi = argyr r(∗) i ǫ{P,N} max p(yri |zi; wrℓ) 4.3 Implementation details We implement our model on top of the MIML(Surdeanu et al., 2012) code base.8 We use the same mention-level and aggregate-level feature sets as Surdeanu et al. (2012). We adopt the same idea of using cross validation for the E and M steps to avoid overfitting. We initialize our algorithm by sampling 5% unlabeled examples as negative, in essence using 1 epoch of MIML to initialize. Empirically it performs well. 5 Experiments Data set: We use the KBP (Ji et al., 2011) dataset9 prepared and publicly released by Surdeanu et al. (2012) for our experiment since it is 1) large and realistic, 2) publicly available, 3) most importantly, it is the only dataset that has associated human-labeled ground truth. Any KB held-out evaluation without manual assessment will be significantly affected by KB incompleteness. In KBP 8Available at http://nlp.stanford.edu/software/mimlre.shtml 9Available from Linguistic Data Consortium (LDC). http://projects.ldc.upenn.edu/kbp/data/ logp(yri |ℓri ) − k( n X logp(ℓri |xi; wz, wℓ) + const +X rǫR i=1 X rǫR Xn i=1 Pn i=1 δ(ℓri , P) P r</context>
</contexts>
<marker>Ji, Grishman, Dang, 2011</marker>
<rawString>Heng Ji, Ralph Grishman, and Hoa T. Dang. 2011. Overview of the TAC 2011 knowledge base population track. In Proceedings of the Text Analytics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>A systematic exploration of the feature space for relation extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL-2007.</booktitle>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. A systematic exploration of the feature space for relation extraction. In Proceedings of HLT-NAACL-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL2004.</booktitle>
<contexts>
<context position="1067" citStr="Kambhatla, 2004" startWordPosition="152" endWordPosition="153">In this paper, we show that a significant number of “negative“ examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms. 1 Introduction Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a). Recently, Distant Supervision (DS) (Craven and Kumlien, 1999; Mintz et al., 2009) has emerged to be a popular choice for training relation extractors without using manually labeled data. It automatically generates training examples by labeling relation mentions1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base (KB). This method significantly reduces human efforts for relation extraction. The labeling heuristic has a serious flaw. Knowledge bases are usually highly incomplete. For exam1An occurrence o</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction. In Proceedings of ACL2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL-2000.</booktitle>
<contexts>
<context position="1031" citStr="Miller et al., 2000" startWordPosition="144" endWordPosition="147">hoice for training relation extractors. In this paper, we show that a significant number of “negative“ examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms. 1 Introduction Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a). Recently, Distant Supervision (DS) (Craven and Kumlien, 1999; Mintz et al., 2009) has emerged to be a popular choice for training relation extractors without using manually labeled data. It automatically generates training examples by labeling relation mentions1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base (KB). This method significantly reduces human efforts for relation extraction. The labeling heuristic has a serious flaw. Knowledge bases are usually highly </context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 2000</marker>
<rawString>Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph Weischedel. 2000. A novel use of statistical parsing to extract information from text. In Proceedings of NAACL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Shuming Shi</author>
<author>Ralph Grishman</author>
<author>ChinYew Lin</author>
</authors>
<title>Ensemble Semantics for Large-scale Unsupervised Relation Extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL</booktitle>
<contexts>
<context position="1085" citStr="Min et al., 2012" startWordPosition="154" endWordPosition="157"> show that a significant number of “negative“ examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms. 1 Introduction Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a). Recently, Distant Supervision (DS) (Craven and Kumlien, 1999; Mintz et al., 2009) has emerged to be a popular choice for training relation extractors without using manually labeled data. It automatically generates training examples by labeling relation mentions1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base (KB). This method significantly reduces human efforts for relation extraction. The labeling heuristic has a serious flaw. Knowledge bases are usually highly incomplete. For exam1An occurrence of a pair of entiti</context>
<context position="4480" citStr="Min et al. (2012" startWordPosition="702" endWordPosition="705">), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Takamatsu et al. (2012) models the probabilities of a pattern showing relations, estimated from the heuristically labeled dataset. Their algorithm removes mentions that match lowprobability patterns. Sun et al. (2011) and Min et al. (2012b) also estimate the probablities of patterns showing relations, but instead use them to relabel examples to their most likely classes. Their approach can correct highly-confident false negative matches. 3 Problem Definition Distant Supervision: Given a KB D (a collection of relational tables r(e1, e2), in which rǫR (R is the set of relation labels), and &lt; e1, e2 &gt; is a pair of entities that is known to have relation r) and a corpus C, the key idea of distant supervision is that we align D to C, label each bag4 of relation mentions that share argument pair &lt; e1, e2 &gt; with r, otherwise OTHER. T</context>
</contexts>
<marker>Min, Shi, Grishman, Lin, 2012</marker>
<rawString>Bonan Min, Shuming Shi, Ralph Grishman and ChinYew Lin. 2012a. Ensemble Semantics for Large-scale Unsupervised Relation Extraction. In Proceedings of EMNLP-CoNLL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Xiang Li</author>
<author>Ralph Grishman</author>
<author>Ang Sun</author>
</authors>
<title>System for KBP Slot Filling.</title>
<date>2012</date>
<booktitle>In Proceedings of the Text Analysis Conference (TAC)</booktitle>
<location>New York University</location>
<contexts>
<context position="1085" citStr="Min et al., 2012" startWordPosition="154" endWordPosition="157"> show that a significant number of “negative“ examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms. 1 Introduction Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a). Recently, Distant Supervision (DS) (Craven and Kumlien, 1999; Mintz et al., 2009) has emerged to be a popular choice for training relation extractors without using manually labeled data. It automatically generates training examples by labeling relation mentions1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base (KB). This method significantly reduces human efforts for relation extraction. The labeling heuristic has a serious flaw. Knowledge bases are usually highly incomplete. For exam1An occurrence of a pair of entiti</context>
<context position="4480" citStr="Min et al. (2012" startWordPosition="702" endWordPosition="705">), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Takamatsu et al. (2012) models the probabilities of a pattern showing relations, estimated from the heuristically labeled dataset. Their algorithm removes mentions that match lowprobability patterns. Sun et al. (2011) and Min et al. (2012b) also estimate the probablities of patterns showing relations, but instead use them to relabel examples to their most likely classes. Their approach can correct highly-confident false negative matches. 3 Problem Definition Distant Supervision: Given a KB D (a collection of relational tables r(e1, e2), in which rǫR (R is the set of relation labels), and &lt; e1, e2 &gt; is a pair of entities that is known to have relation r) and a corpus C, the key idea of distant supervision is that we align D to C, label each bag4 of relation mentions that share argument pair &lt; e1, e2 &gt; with r, otherwise OTHER. T</context>
</contexts>
<marker>Min, Li, Grishman, Sun, 2012</marker>
<rawString>Bonan Min, Xiang Li, Ralph Grishman and Ang Sun. 2012b. New York University 2012 System for KBP Slot Filling. In Proceedings of the Text Analysis Conference (TAC) 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1170" citStr="Mintz et al., 2009" startWordPosition="167" endWordPosition="170">rocess are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms. 1 Introduction Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a). Recently, Distant Supervision (DS) (Craven and Kumlien, 1999; Mintz et al., 2009) has emerged to be a popular choice for training relation extractors without using manually labeled data. It automatically generates training examples by labeling relation mentions1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base (KB). This method significantly reduces human efforts for relation extraction. The labeling heuristic has a serious flaw. Knowledge bases are usually highly incomplete. For exam1An occurrence of a pair of entities with the source sentence. ple, 93.8% of persons from Freebase2 have no place of bi</context>
<context position="3618" citStr="Mintz et al., 2009" startWordPosition="565" endWordPosition="568">en (1999) in the biomedical domain. 2Freebase is a large collaboratively-edited KB. It is available at http://www.freebase.com. 3There are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair &lt; e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v′. These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples, Riedel et al. (2010) use Multiple Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by differ</context>
<context position="16864" citStr="Mintz et al. (2009)" startWordPosition="2871" endWordPosition="2874">aluation: We compare our algorithm (MIMLsemi) to three algorithms: 1) MIML (Surdeanu et al., 2012), the Multiple-Instance Multiple Label algorithm which labels the bags directly with the KB (y = E). 2) MultiR (denoted as Hoffmann) (Hoffmann et al., 2011), a Multiple-Instance algorithm that supports overlapping relations. It also imposes y = E. 3) Mintz++ (Surdeanu et al., 2012), a variant of the single-instance learning algorithm (section 3). The first two are stat-of-the-art Multi-Instance Multi-Label algorithms. Mintz++ is a strong baseline (Surdeanu et al., 2012) and an improved version of Mintz et al. (2009). Figure 2 shows that our algorithm consistently outperforms all three algorithms at almost all recall levels (with the exception of a very small region in the PR-curve). This demonstrates that by treating unla-beled data set differently and leveraging the missing positive bags, MIML-semi is able to learn a more accurate model for extraction. Although the proposed solution is a specific algorithm, we believe the idea of treating unlabeled data differently can be incorporated into any of these algorithms that only use unlabeled data as negative examples. 6 Conclusion We show that the distant-su</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
</authors>
<title>End-to-end relation extraction using distant supervision from external semantic repositories.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="3759" citStr="Nguyen and Moschitti, 2011" startWordPosition="590" endWordPosition="593">re are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair &lt; e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v′. These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples, Riedel et al. (2010) use Multiple Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Takamatsu et al. (2012) models the probabilities of a pattern showing relations, estimated from the heuristically lab</context>
</contexts>
<marker>Nguyen, Moschitti, 2011</marker>
<rawString>Truc Vien T. Nguyen and Alessandro Moschitti. 2011. End-to-end relation extraction using distant supervision from external semantic repositories. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD 10).</booktitle>
<contexts>
<context position="2310" citStr="Riedel et al., 2010" startWordPosition="352" endWordPosition="355">h the source sentence. ple, 93.8% of persons from Freebase2 have no place of birth, and 78.5% have no nationality (section 3). Previous work typically assumes that if the argument entity pair is not listed in the KB as having a relation, all the corresponding relation mentions are considered negative examples.3 This crude assumption labeled many entity pairs as negative when in fact some of their mentions express a relation. The number of such false negative matches even exceeds the number of positive pairs, by 3 to 10 times, leading to a significant problem for training. Previous approaches (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) bypassed this problem by heavily under-sampling the “negative“ class. We instead deal with a learning scenario where we only have entity-pair level labels that are either positive or unlabeled. We proposed an extension to Surdeanu et al. (2012) that can train on this dataset. Our contribution also includes an analysis on the incompleteness of Freebase and the false negative match rate in two datasets of labeled examples generated by DS. Experimental results on a realistic and challenging dataset demonstrate the advantage of the algorithm over exi</context>
<context position="3684" citStr="Riedel et al., 2010" startWordPosition="577" endWordPosition="580">atively-edited KB. It is available at http://www.freebase.com. 3There are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair &lt; e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v′. These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples, Riedel et al. (2010) use Multiple Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Takamatsu et al. (2012) models the probabi</context>
<context position="7026" citStr="Riedel et al., 2010" startWordPosition="1117" endWordPosition="1120">on types Incompleteness /people/person/education 0.792 /people/person/employment history 0.923 /people/person/nationality* 0.785 /people/person/parents* 0.988 /people/person/place of birth* 0.938 /people/person/places lived* 0.966 Table 1: The incompleteness of Freebase (* are musthave attributes for a person). We further investigate the rate of false negative matches, as the percentage of entity-pairs that are not listed in Freebase but one of its mentions generated by DS does express a relation in the target set of types. We randomly picked 200 unlabeled bags5 from each of the two datasets (Riedel et al., 2010; Surdeanu et al., 2012) generated by DS, and we manually annotate all relation mentions in these bags. The result is shown in Table 2, along with a few examples that indicate a relation holds in the set of false negative matches (bag-level). Both datasets have around 10% false negative matches in the unlabeled set of bags. Taking into consideration that the number of positive bags and unlabeled bags are highly imbalanced (1:134 and 1:37 in the Riedel and KBP dataset respectively, before undersampling the unlabeled class), the number of false negative matches are 11 and 4 times the number of p</context>
<context position="8626" citStr="Riedel et al., 2010" startWordPosition="1379" endWordPosition="1382">et # pos- # positive : % are # positive has human Examples of false negative mentions (train- itive # unlabeled false : # false assessment ing) bags negatives negative no (/location/location/contains)... in Brooklyn ’s Williamsburg. Riedel 4,700 1:134(BD*) 8.5% 1:11.4 (/people/person/place lived) Cheryl Rogowski , a farmer from Orange County ... KBP 183,062 1:37(BD*) 11.5% 1:4 yes (per:city of birth) Juan Martn Maldacena (born September 10, 1968) is a theoretical physicist born in Buenos Aires (per:employee of)Dave Matthews, from the ABC News,... Table 2: False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset (Surdeanu et al., 2012). All numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and 5% in Riedel and KBP dataset, respectively. to modeling the instance-level noise using a 3-layer MIL or MIML model (e.g., Surdeanu et al. (2012)). We propose a 4-layer model as shown in Figure 1. The input to the model is a list of n bags with a vector of binary labels, either Positive (P), or Unlabled (U) for each relation r. Our model can be viewed as a semi-supervised6 framework that extends a state-of-the-art Multi-Instance Multi-La</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD 10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Sun</author>
<author>Ralph Grishman</author>
<author>Wei Xu</author>
<author>Bonan Min</author>
</authors>
<title>system for KBP slot filling.</title>
<date>2011</date>
<booktitle>In Proceedings of the Text Analytics Conference.</booktitle>
<location>New York University</location>
<contexts>
<context position="3229" citStr="Sun et al. (2011)" startWordPosition="498" endWordPosition="501">train on this dataset. Our contribution also includes an analysis on the incompleteness of Freebase and the false negative match rate in two datasets of labeled examples generated by DS. Experimental results on a realistic and challenging dataset demonstrate the advantage of the algorithm over existing solutions. 2 Related Work Distant supervision was first proposed by Craven and Kumlien (1999) in the biomedical domain. 2Freebase is a large collaboratively-edited KB. It is available at http://www.freebase.com. 3There are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair &lt; e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v′. These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples, Riedel et al. (2010) </context>
<context position="4459" citStr="Sun et al. (2011)" startWordPosition="697" endWordPosition="700">Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Takamatsu et al. (2012) models the probabilities of a pattern showing relations, estimated from the heuristically labeled dataset. Their algorithm removes mentions that match lowprobability patterns. Sun et al. (2011) and Min et al. (2012b) also estimate the probablities of patterns showing relations, but instead use them to relabel examples to their most likely classes. Their approach can correct highly-confident false negative matches. 3 Problem Definition Distant Supervision: Given a KB D (a collection of relational tables r(e1, e2), in which rǫR (R is the set of relation labels), and &lt; e1, e2 &gt; is a pair of entities that is known to have relation r) and a corpus C, the key idea of distant supervision is that we align D to C, label each bag4 of relation mentions that share argument pair &lt; e1, e2 &gt; with </context>
</contexts>
<marker>Sun, Grishman, Xu, Min, 2011</marker>
<rawString>Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min. 2011. New York University 2011 system for KBP slot filling. In Proceedings of the Text Analytics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sonal Gupta</author>
<author>John Bauer</author>
<author>David McClosky</author>
<author>Angel X Chang</author>
<author>Valentin I Spitkovsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanfords distantlysupervised slot-filling system.</title>
<date>2011</date>
<booktitle>In Proceedings of the Text Analytics Conference.</booktitle>
<contexts>
<context position="3207" citStr="Surdeanu et al. (2011)" startWordPosition="492" endWordPosition="496">anu et al. (2012) that can train on this dataset. Our contribution also includes an analysis on the incompleteness of Freebase and the false negative match rate in two datasets of labeled examples generated by DS. Experimental results on a realistic and challenging dataset demonstrate the advantage of the algorithm over existing solutions. 2 Related Work Distant supervision was first proposed by Craven and Kumlien (1999) in the biomedical domain. 2Freebase is a large collaboratively-edited KB. It is available at http://www.freebase.com. 3There are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair &lt; e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v′. These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples,</context>
</contexts>
<marker>Surdeanu, Gupta, Bauer, McClosky, Chang, Spitkovsky, Manning, 2011</marker>
<rawString>Mihai Surdeanu, Sonal Gupta, John Bauer, David McClosky, Angel X. Chang, Valentin I. Spitkovsky, and Christopher D. Manning. 2011. Stanfords distantlysupervised slot-filling system. In Proceedings of the Text Analytics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance Multilabel Learning for Relation Extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Natural Language Learning.</booktitle>
<contexts>
<context position="2357" citStr="Surdeanu et al., 2012" startWordPosition="360" endWordPosition="363"> from Freebase2 have no place of birth, and 78.5% have no nationality (section 3). Previous work typically assumes that if the argument entity pair is not listed in the KB as having a relation, all the corresponding relation mentions are considered negative examples.3 This crude assumption labeled many entity pairs as negative when in fact some of their mentions express a relation. The number of such false negative matches even exceeds the number of positive pairs, by 3 to 10 times, leading to a significant problem for training. Previous approaches (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) bypassed this problem by heavily under-sampling the “negative“ class. We instead deal with a learning scenario where we only have entity-pair level labels that are either positive or unlabeled. We proposed an extension to Surdeanu et al. (2012) that can train on this dataset. Our contribution also includes an analysis on the incompleteness of Freebase and the false negative match rate in two datasets of labeled examples generated by DS. Experimental results on a realistic and challenging dataset demonstrate the advantage of the algorithm over existing solutions. 2 Related Work Distant supervi</context>
<context position="3730" citStr="Surdeanu et al., 2012" startWordPosition="585" endWordPosition="589">/www.freebase.com. 3There are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair &lt; e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v′. These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples, Riedel et al. (2010) use Multiple Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Takamatsu et al. (2012) models the probabilities of a pattern showing relations, estimat</context>
<context position="7050" citStr="Surdeanu et al., 2012" startWordPosition="1121" endWordPosition="1124">ss /people/person/education 0.792 /people/person/employment history 0.923 /people/person/nationality* 0.785 /people/person/parents* 0.988 /people/person/place of birth* 0.938 /people/person/places lived* 0.966 Table 1: The incompleteness of Freebase (* are musthave attributes for a person). We further investigate the rate of false negative matches, as the percentage of entity-pairs that are not listed in Freebase but one of its mentions generated by DS does express a relation in the target set of types. We randomly picked 200 unlabeled bags5 from each of the two datasets (Riedel et al., 2010; Surdeanu et al., 2012) generated by DS, and we manually annotate all relation mentions in these bags. The result is shown in Table 2, along with a few examples that indicate a relation holds in the set of false negative matches (bag-level). Both datasets have around 10% false negative matches in the unlabeled set of bags. Taking into consideration that the number of positive bags and unlabeled bags are highly imbalanced (1:134 and 1:37 in the Riedel and KBP dataset respectively, before undersampling the unlabeled class), the number of false negative matches are 11 and 4 times the number of positive bags in Reidel a</context>
<context position="8666" citStr="Surdeanu et al., 2012" startWordPosition="1386" endWordPosition="1389">e has human Examples of false negative mentions (train- itive # unlabeled false : # false assessment ing) bags negatives negative no (/location/location/contains)... in Brooklyn ’s Williamsburg. Riedel 4,700 1:134(BD*) 8.5% 1:11.4 (/people/person/place lived) Cheryl Rogowski , a farmer from Orange County ... KBP 183,062 1:37(BD*) 11.5% 1:4 yes (per:city of birth) Juan Martn Maldacena (born September 10, 1968) is a theoretical physicist born in Buenos Aires (per:employee of)Dave Matthews, from the ABC News,... Table 2: False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset (Surdeanu et al., 2012). All numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and 5% in Riedel and KBP dataset, respectively. to modeling the instance-level noise using a 3-layer MIL or MIML model (e.g., Surdeanu et al. (2012)). We propose a 4-layer model as shown in Figure 1. The input to the model is a list of n bags with a vector of binary labels, either Positive (P), or Unlabled (U) for each relation r. Our model can be viewed as a semi-supervised6 framework that extends a state-of-the-art Multi-Instance Multi-Label (MIML) model (Surdeanu et al., 2012)</context>
<context position="10879" citStr="Surdeanu et al. (2012)" startWordPosition="1803" endWordPosition="1806"> r holds for the ith bag. • 0 is an observed constant controlling the total number of bags whose latent label is positive. We define the following conditional probabilities: { 1/2 if yri = P ∧ ℓri = P; 1/2 if yri = U ∧ ℓri = P; 1 if yri = U ∧ ℓri = N; 0 otherwise ; It encodes the constraints between true baglevel labels and the entity pair labels in the KB. • p(0|ℓ) ∼ N(E 1 E n WR i ,P), k) where S(x, y) = 1 if x = y, 0 otherwise. k is a large number. 0 is the fraction of the bags that are positive. It is an observed parameter that depends on both the source corpus and the KB used. Similar to Surdeanu et al. (2012), we also define the following parameters and conditional probabilities (details are in Surdeanu et al. (2012)): • zijcR ∪ {OTHER}: a latent variable that denotes the relation type of the jth mention in the ith bag. • xij is the feature representation of the jth relation mention in the ith bag. We use the set of features in Surdeanu et al. (2012). • wz is the weight vector for the multi-class relation mention-level classifier. • wr ℓ is the weight vector for the rth binary toplevel aggregation classifier (from mention labels to bag-level prediction). We use wℓ to represent wP, wt, ...w|R| ℓ . </context>
<context position="13415" citStr="Surdeanu et al. (2012)" startWordPosition="2301" endWordPosition="2304">, wz) 13: for all rǫR do Pn 14: wr(∗) ℓ = arg maxwr i=1 p(ℓr i |zi, wr ℓ) ℓ 15: end for 16: end for 17: return wz, wℓ 7All classifiers are implemented with L2-regularized logistic regression with Stanford CoreNLP package. In the E-step, we do a greedy search (steps 5-8 in algorithm 1) in all p(ℓri |xi; wz, wℓ) and update ℓri until the second term is maximized. wz, wℓ are the model weights learned from the previous iteration. After fixed ℓ, we seek to maximize: logp(ℓ|xi;wz,wℓ) = Xn logp(ℓi|xi; wz, wℓ) i=1 Xn X p(ℓi, zi|xi; wz, wℓ) i=1 log zi which can be solved with an approximate solution in Surdeanu et al. (2012) (step 9-11): update zi independently with: z∗i = arg maxzi p(zi|ℓi, xi; wz, wℓ). More details can be found in Surdeanu et al. (2012). In the M-step, we retrain both of the mentionlevel and the aggregation level classifiers. The full EM algorithm is shown in algorithm 1. 4.2 Inference Inference on a bag xi is trivial. For each mention: z∗ij = argzijǫR∪{OTHER} maxp(zij|xij, wz) Followed by the aggregation (directly with wℓ): yi = argyr r(∗) i ǫ{P,N} max p(yri |zi; wrℓ) 4.3 Implementation details We implement our model on top of the MIML(Surdeanu et al., 2012) code base.8 We use the same mention</context>
<context position="15961" citStr="Surdeanu et al. (2012)" startWordPosition="2721" endWordPosition="2725">ng bags are generated by mapping Wikipedia (http://en.wikipedia.org) infoboxes (after merging similar types following the KBP 2011 task definition) into a large unlabeled corpus (consisting of 1.5M documents from the KBP source corpus and a complete snapshot of Wikipedia). The KBP shared task provided 200 query named entities with their associated slot values (in total several thousand pairs). We use 40 queries as development dataset (dev), and the rest (160 queries) as evaluation dataset. We set θ = 0.25 by tuning on the dev set and use it in the experiments. For a fair comparison, we follow Surdeanu et al. (2012) and begin by downsampling the “negative“ class to 5%. We also set T=8 and use the following noisy-or (for ith bag) of mention-level probability to rank predicted types (r) of pairs and plot the precision-recall curves for all experiments. �Probi(r) = 1 − (1 − p(zij = r|Xij;Wz)) j Evaluation: We compare our algorithm (MIMLsemi) to three algorithms: 1) MIML (Surdeanu et al., 2012), the Multiple-Instance Multiple Label algorithm which labels the bags directly with the KB (y = E). 2) MultiR (denoted as Hoffmann) (Hoffmann et al., 2011), a Multiple-Instance algorithm that supports overlapping rela</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, Christopher D. Manning. 2012. Multi-instance Multilabel Learning for Relation Extraction. In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TAC KBP</author>
</authors>
<title>task definition.</title>
<date>2011</date>
<note>http://nlp .cs.qc.cuny.edu/kbp/2011/KBP2011 TaskDefinition.pdf</note>
<contexts>
<context position="15469" citStr="KBP 2011" startWordPosition="2638" endWordPosition="2639">dc.upenn.edu/kbp/data/ logp(yri |ℓri ) − k( n X logp(ℓri |xi; wz, wℓ) + const +X rǫR i=1 X rǫR Xn i=1 Pn i=1 δ(ℓri , P) P rǫR θ)2 n 780 Figure 2: Performance on the KBP dataset. The figures on the left, middle and right show MIML, Hoffmann, and Mintz++ compared to the same MIML-Semi curve, respectively. MIML-Semi is shown in red curves (lighter curves in black and white) while other algorithms are shown in black curves (darker curves in black and white). dataset, the training bags are generated by mapping Wikipedia (http://en.wikipedia.org) infoboxes (after merging similar types following the KBP 2011 task definition) into a large unlabeled corpus (consisting of 1.5M documents from the KBP source corpus and a complete snapshot of Wikipedia). The KBP shared task provided 200 query named entities with their associated slot values (in total several thousand pairs). We use 40 queries as development dataset (dev), and the rest (160 queries) as evaluation dataset. We set θ = 0.25 by tuning on the dev set and use it in the experiments. For a fair comparison, we follow Surdeanu et al. (2012) and begin by downsampling the “negative“ class to 5%. We also set T=8 and use the following noisy-or (for i</context>
</contexts>
<marker>KBP, 2011</marker>
<rawString>TAC KBP 2011 task definition. 2011. http://nlp .cs.qc.cuny.edu/kbp/2011/KBP2011 TaskDefinition.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shingo Takamatsu</author>
<author>Issei Sato</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Reducing Wrong Labels in Distant Supervision for Relation Extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of 50th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4265" citStr="Takamatsu et al. (2012)" startWordPosition="669" endWordPosition="672">07; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples, Riedel et al. (2010) use Multiple Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Takamatsu et al. (2012) models the probabilities of a pattern showing relations, estimated from the heuristically labeled dataset. Their algorithm removes mentions that match lowprobability patterns. Sun et al. (2011) and Min et al. (2012b) also estimate the probablities of patterns showing relations, but instead use them to relabel examples to their most likely classes. Their approach can correct highly-confident false negative matches. 3 Problem Definition Distant Supervision: Given a KB D (a collection of relational tables r(e1, e2), in which rǫR (R is the set of relation labels), and &lt; e1, e2 &gt; is a pair of enti</context>
</contexts>
<marker>Takamatsu, Sato, Nakagawa, 2012</marker>
<rawString>Shingo Takamatsu, Issei Sato, Hiroshi Nakagawa. 2012. Reducing Wrong Labels in Distant Supervision for Relation Extraction. In Proceedings of 50th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Autonomously semantifying wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Information and Knowledge Management (CIKM-2007).</booktitle>
<contexts>
<context position="3663" citStr="Wu and Weld, 2007" startWordPosition="573" endWordPosition="576">is a large collaboratively-edited KB. It is available at http://www.freebase.com. 3There are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair &lt; e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v′. These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples, Riedel et al. (2010) use Multiple Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Takamatsu et al. (201</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel S. Weld. 2007. Autonomously semantifying wikipedia. In Proceedings of the International Conference on Information and Knowledge Management (CIKM-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Jie Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005.</booktitle>
<contexts>
<context position="1050" citStr="Zhou et al., 2005" startWordPosition="148" endWordPosition="151">lation extractors. In this paper, we show that a significant number of “negative“ examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms. 1 Introduction Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a). Recently, Distant Supervision (DS) (Craven and Kumlien, 1999; Mintz et al., 2009) has emerged to be a popular choice for training relation extractors without using manually labeled data. It automatically generates training examples by labeling relation mentions1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base (KB). This method significantly reduces human efforts for relation extraction. The labeling heuristic has a serious flaw. Knowledge bases are usually highly incomplete. For exa</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>Guodong Zhou, Jian Su, Jie Zhang and Min Zhang. 2005. Exploring various knowledge in relation extraction. In Proceedings of ACL-2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>