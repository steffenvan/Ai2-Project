<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.061415">
<title confidence="0.992989">
Arabic Morphological Tagging, Diacritization, and Lemmatization
Using Lexeme Models and Feature Ranking
</title>
<author confidence="0.999363">
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab, and Cynthia Rudin
</author>
<affiliation confidence="0.9963685">
Center for Computational Learning Systems
Columbia University
</affiliation>
<address confidence="0.993133">
New York, NY 10115 USA
</address>
<email confidence="0.999732">
{ryanr,rambow,habash,mdiab,rudin}@ccls.columbia.edu,
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999409571428571">
We investigate the tasks of general morpho-
logical tagging, diacritization, and lemmatiza-
tion for Arabic. We show that for all tasks we
consider, both modeling the lexeme explicitly,
and retuning the weights of individual classi-
fiers for the specific task, improve the perfor-
mance.
</bodyText>
<sectionHeader confidence="0.996548" genericHeader="related work">
1 Previous Work
</sectionHeader>
<bodyText confidence="0.99976745">
Arabic is a morphologically rich language: in our
training corpus of about 288,000 words we find 3279
distinct morphological tags, with up to 100,000 pos-
sible tags.1 Because of the large number of tags, it
is clear that morphological tagging cannot be con-
strued as a simple classification task. Hajiˇc (2000)
is the first to use a dictionary as a source of possible
morphological analyses (and hence tags) for an in-
flected word form. He redefines the tagging task as
a choice among the tags proposed by the dictionary,
using a log-linear model trained on specific ambi-
guity classes for individual morphological features.
Hajiˇc et al. (2005) implement the approach of Hajiˇc
(2000) for Arabic. In previous work, we follow the
same approach (Habash and Rambow, 2005), using
SVM-classifiers for individual morphological fea-
tures and a simple combining scheme for choosing
among competing analyses proposed by the dictio-
nary. Since the dictionary we use, BAMA (Buck-
walter, 2004), also includes diacritics (orthographic
</bodyText>
<footnote confidence="0.94049">
1This work was funded under the DARPA GALE, program,
contract HR0011-06-C-0023. We thank several anonymous re-
viewers for helpful comments. A longer version of this paper is
available as a technical report.
</footnote>
<bodyText confidence="0.9997524">
marks not usually written), we extend this approach
to the diacritization task in (Habash and Rambow,
2007). The work presented in this paper differs from
this previous work in that (a) we introduce a new
task for Arabic, namely lemmatization; (b) we use
an explicit modeling of lexemes as a component in
all tasks discussed in this paper (morphological tag-
ging, diacritization, and lemmatization); and (c) we
tune the weights of the feature classifiers on a tuning
corpus (different tuning for different tasks).
</bodyText>
<sectionHeader confidence="0.9871235" genericHeader="method">
2 Morphological Disambiguation Tasks for
Arabic
</sectionHeader>
<bodyText confidence="0.999768952380952">
We define the task of morphological tagging
as choosing an inflectional morphological tag (in
this paper, the term “morphological tagging” never
refers to derivational morphology). The morphol-
ogy of an Arabic word can be described by the 14
(nearly) orthogonal features shown in Figure 1. For
different tasks, different subsets may be useful: for
example, when translating into a language without
case, we may want to omit the case feature. For the
experiments we discuss in this paper, we investigate
three variants of the morphological tagging tasks:
MorphPOS (determining the feature POS, which is
the core part-of-speech – verb, noun, adjective, etc.);
MorphPart (determining the set of the first ten basic
morphological features listed in Figure 1); and Mor-
phAll (determining the full inflectional morpholog-
ical tag, i.e., all 14 features).
The task of diacritization involves adding diacrit-
ics (short vowels, gemination marker shadda, and
indefiniteness marker nunation) to the standard writ-
ten form. We have two variants of the diacritization
</bodyText>
<page confidence="0.982768">
117
</page>
<reference confidence="0.2340935">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 117–120,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<table confidence="0.984803722222222">
Feature name Explanation
POS Simple part-of-speech
CNJ Presence of a conjunction clitic
PRT Presence of a particle clitic
PRO Presence of a pronominal clitic
DET Presence of the definite deter-
miner
GEN Gender
NUM Number
PER Person
VOX Voice
ASP Aspect
MOD Mood
NUN Presence of nunation (indefinite-
ness marker)
CON Construct state (head of a geni-
tive construction)
CAS Case
</table>
<figureCaption confidence="0.9762822">
Figure 1: List of (inflectional) morphological features
used in our system; the first ten are features which
(roughly) can be determined with higher accuracy since
they rely less on syntactic context and more on visible
inflectional morphology
</figureCaption>
<bodyText confidence="0.999892454545455">
tasks: DiacFull (predicting all diacritics of a given
word), which relates to lexeme choice and morphol-
ogy tagging, and DiacPart (predicting all diacritics
of a given word except those associated with the fi-
nal letter), which relates largely to lexeme choice.
Lemmatization (LexChoice) for Arabic has not
been discussed in the literature to our knowledge. A
lexeme is an abstraction over a set of inflected word
forms, and it is usually represented by its citation
form, also called lemma.
Finally, AllChoice is the combined task of choos-
ing all inflectional and lexemic aspects of a word in
context.
This gives us a total of seven tasks. AllChoice is
the hardest of our tasks, since it subsumes all other
tasks. MorphAll is the hardest of the three mor-
phological tagging tasks, subsuming MorphPart
and MorphPOS, and DiacFull is the hardest lexical
task, subsuming DiacPart, which in turn subsumes
LexChoice. However, MorphAll and DiacFull are
(in general) orthogonal, since MorphAll has no lex-
emic component, while DiacFull does.
</bodyText>
<sectionHeader confidence="0.968151" genericHeader="method">
3 Our System
</sectionHeader>
<bodyText confidence="0.999955021739131">
Our system, MADA, makes use of 19 orthogonal
features to select, for each word, a proper anal-
ysis from a list of potential analyses provided by
the BAMA dictionary. The BAMA analysis which
matches the most of the predicted features wins; the
weighting of the features is one of the topics of this
paper. These 19 features consist of the 14 morpho-
logical features shown in Figure 1, which MADA
predicts using 14 distinct Support Vector Machines
trained on ATB3-Train (as defined by Zitouni et al.
(2006)). In addition, MADA uses five additional
features. Spellmatch determines whether the dia-
critized form of the suggested analysis and the input
word match if both are stripped of all of their di-
acritics. This is useful because sometimes BAMA
suggests analyses which imply a different spelling
of the undiacritized word, but these analyses are of-
ten incorrect. Isdefault identifies those analyses that
are the default output of BAMA (typically, these
are guesses that the word in question is a proper
noun); these analyses are less likely to be correct
than others suggested by BAMA. MADA can de-
rive the values of Spellmatch and Isdefault by di-
rect examination of the analysis in question, and
no predictive model is needed. The fourteen mor-
phological features plus Spellmatch and Isdefault
form a feature collection that is entirely based on
morphological (rather than lexemic) features; we re-
fer to this collection as BASE-16. UnigramDiac
and UnigramLex are unigram models of the sur-
face diacritized form and the lexeme respectively,
and contain lexical information. We also build 4-
gram lexeme models using an open-vocabulary lan-
guage model with Kneser-Ney smoothing, by means
of the SRILM toolkit (Stolcke, 2002). The model is
trained on the same corpus used to train the other
classifiers, ATB3-Train. (We also tested other n-
gram models, and found that a 4-gram lexeme model
outperforms the other orders with n &lt; 5, although
the improvement over the trigram and 5-gram mod-
els was less than 0.01%.) The 4-gram model, on
its own, correctly selects the lexeme of words in
ATB3-DevTest 94.1% of the time. The 4-gram lex-
eme model was incorporated into our system as a
full feature (NGRAM). We refer to the feature set
consisting of BASE-16 plus the two unigram mod-
</bodyText>
<page confidence="0.993582">
118
</page>
<bodyText confidence="0.999665573333333">
els and NGRAM as FULL-19.
Optimizing the feature weights is a machine
learning task. To provide learning data for this task,
we take the ATB3-DevTest data set and divide it into
two sections; the first half (-26K words) is used
for tuning the weights and the second half (-25K
words) for testing. In a pre-processing step, each
analysis in appended with a set of labels which in-
dicate whether the analysis is correct according to
seven different evaluation metrics. These metrics
correspond in a one-to-one manner to the seven dif-
ferent disambiguation tasks discussed in Section 2,
and we use the task name for the evaluation la-
bel. Specifically, the MorphPOS label is positive
if the analysis has the same POS value as the cor-
rect analysis in the gold standard; the LexChoice
label provides the same information about the lex-
eme choice. The MorphPart label is positive if the
analysis agrees with the gold for each of the 10 ba-
sic features used by Habash and Rambow (2005).
A positive MorphAll label requires that the analy-
sis match the gold in all morphological features, i.e.,
in every feature except the lexeme choice and dia-
critics. The DiacFull label is only positive if the
surface diacritics of the analysis match the gold di-
acritics exactly; DiacPart is less strict in that the
trailing sequence diacritic markers in each surface
diacritic are stripped before the analysis and the gold
are compared. Finally, AllChoice is only positive if
the analysis was one chosen as correct in the gold;
this is the strictest form of evaluation, and there can
be only one positive AllChoice label per word.
In addition to labeling as described in the preced-
ing paragraph, we run MADA on the tuning and test
sets. This gives us a set of model predictions for ev-
ery feature of every word in the tuning and test sets.
We use an implementation of a Downhill Simplex
Method in many dimensions based on the method
developed by Nelder and Mead (1965) to tune the
weights applied to each feature. In a given itera-
tion, the Simplex algorithm proposes a set of feature
weights. These weights are given to a weight eval-
uation function; this function determines how effec-
tive a particular set of weights is at a given disam-
biguation task by calculating an overall score for
the weight set: the number of words in the tuning
set that were correctly disambiguated. In order to
compute this score, the weight evaluation function
examines each proposed analysis for each word in
the tuning set. If the analysis and the model predic-
tion for a feature of a given word agree, the analysis
score for that analysis is incremented by the weight
corresponding to that feature. The analysis with the
highest analysis score is selected as the proper anal-
ysis for that word. If the selected analysis has a pos-
itive task label (i.e., it is a good answer for the dis-
ambiguation task in question), the overall score for
the proposed weight set is incremented. The Sim-
plex algorithm seeks to maximize this overall score
(and thus choose the weight set that performs best
for a given task).
Once the Simplex algorithm has converged, the
optimal feature weights for a given task are known.
Our system makes use of these weights to select a
correct analysis in the test set. Each analysis of each
word is given a score that is the sum of optimal fea-
ture weights for features where the model predic-
tion and the analysis agree. The analysis with the
highest score is then chosen as the correct analysis
for that word. The system can be evaluated simply
by comparing the chosen analysis to the gold stan-
dard. Since the Simplex weight evaluation function
and the system use identical means of scoring anal-
yses, the Simplex algorithm has the potential to find
very optimized weights.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="conclusions">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999950277777778">
We have three main research hypotheses: (1) Using
lexemic features helps in all tasks, but especially in
the diacritization and lexeme choice tasks. (2) Tun-
ing the weights helps over using identical weights.
(3) Tuning to the task that is evaluated improves over
tuning to other tasks. For each of the two feature
sets, BASE-16 and FULL-19, we tune the weights
using seven tuning metrics, producing seven sets of
weights. We then evaluate the seven automatically
weighted systems using seven evaluation metrics.
The tuning metrics are identical to the evaluation
metrics and they correspond to the seven tasks de-
scribed in Section 2. Instead of showing 98 results,
we show in Figure 2 four results for each of the
seven tasks: for both the BASE-16 and FULL-19
feature sets, we give the untuned performance, and
then the best-performing tuned performance. We in-
dicate which tuning metric provided the best tun-
</bodyText>
<page confidence="0.995677">
119
</page>
<table confidence="0.999774333333333">
Task Baseline BASE-16 (Morph Feats Only) FULL-19 (All Feats)
Not Tuned Tuned Tuning metric Not Tuned Tuned Tuning metric
MorphPOS 95.5 95.6 96.0 MorphAll 96.0 96.4 MorphPOS
MorphPart 93.8 94.1 94.8 AllChoice 94.7 95.1 DiacPart
MorphAll 83.8 84.0 84.8 AllChoice 82.2 85.1 MorphAll
LexChoice 85.5 86.6 87.5 MorphAll 95.4 96.3 LexChoice
DiacPart 85.1 86.4 87.3 AllChoice 94.8 95.4 DiacPart
DiacFull 76.0 77.1 78.2 MorphAll 82.6 86.1 MorphAll
AllChoice 73.3 74.5 75.6 AllChoice 80.3 83.8 MorphAll
</table>
<figureCaption confidence="0.8262135">
Figure 2: Results for morphological tagging tasks (percent correct); the baseline uses only 14 morphological features
with identical weights; “Tuning Metric” refers to the tuning metric that produced the best tuned results, as shown in
the “Tuned” column
ing performance. The Baseline indicated in Fig-
ure 2 uses the 14 morphological features (listed in
Figure 1) only, with no tuning (i.e., all 14 features
</figureCaption>
<bodyText confidence="0.987761861111111">
have a weight of 1). The untuned results were deter-
mined by also setting almost all feature weights to 1;
the only exception is the Isdefault feature, which is
given a weight of -(8/14) when included in untuned
sets. Since this feature is meant to penalize analy-
ses, its value must be negative; we use this particu-
lar value so that our results can be readily compared
to previous work. All results are the best published
results to date on these test sets; for a deeper discus-
sion, see the longer version of this paper which is
available as a technical report.
We thus find our three hypotheses confirmed: (1)
Using lexemic features reduces error for the mor-
phological tagging tasks (measured on tuned data)
by 3% to 11%, but by 36% to 71% for the diacritic
and lexeme choice tasks. The highest error reduc-
tion is indeed for the lexical choice task. (2) Tuning
the weights helps over using identical weights. With
only morphological features, we obtain an error re-
duction of between 4% and 12%; with all features,
the error reduction from tuning ranges between 8%
and 20%. (3) As for the correlation between tuning
task and evaluation task, it turned out that when we
use only morphological features, two tuning tasks
worked best for all evaluation tasks, namely Mor-
phAll and AllChoice, thus not confirming our hy-
pothesis. We speculate that in the absence of the lex-
ical features, more features is better (these two tasks
are the two hardest tasks for morphological features
only). If we add the lexemic features, we do find
our hypothesis confirmed, with almost all evaluation
tasks performing best when the weights are tuned for
that task. In the case of the three exceptions, the dif-
ferences between the best performance and perfor-
mance when tuned to the same task are very slight
(&lt; 0.06%).
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999684206896551">
Tim Buckwalter. 2004. Buckwalter Arabic morphologi-
cal analyzer version 2.0.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In ACL’05, Ann
Arbor, MI, USA.
Nizar Habash and Owen Rambow. 2007. Arabic di-
acritization through full morphological tagging. In
NAACL HLT 2007 Companion Volume, Short Papers,
Rochester, NY, USA.
Jan Hajiˇc, Otakar Smrˇz, Tim Buckwalter, and Hubert
Jin. 2005. Feature-based tagger of approximations
of functional Arabic morphology. In Proceedings of
the Workshop on Treebanks and Linguistic Theories
(TLT), Barcelona, Spain.
Jan Hajiˇc. 2000. Morphological tagging: Data vs. dic-
tionaries. In 1st Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL’00), Seattle, WA.
J.A Nelder and R Mead. 1965. A simplex method for
function minimization. In Computer Journal, pages
303–333.
Andreas Stolcke. 2002. Srilm - an extensible language
toolkit. In Proceedings of the International Confer-
ence on Spoken Language Processing (ICSLP).
Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya.
2006. Maximum entropy based restoration of arabic
diacritics. In Coling-ACL’06, pages 577–584, Sydney,
Australia.
</reference>
<page confidence="0.996274">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.9995565">Arabic Morphological Tagging, Diacritization, and Lemmatization Using Lexeme Models and Feature Ranking</title>
<author confidence="0.999815">Ryan Roth</author>
<author confidence="0.999815">Owen Rambow</author>
<author confidence="0.999815">Nizar Habash</author>
<author confidence="0.999815">Mona Diab</author>
<author confidence="0.999815">Cynthia Rudin</author>
<affiliation confidence="0.999753">Center for Computational Learning Systems Columbia University</affiliation>
<address confidence="0.99955">New York, NY 10115 USA</address>
<abstract confidence="0.995504833333333">We investigate the tasks of general morphological tagging, diacritization, and lemmatization for Arabic. We show that for all tasks we consider, both modeling the lexeme explicitly, and retuning the weights of individual classifiers for the specific task, improve the performance. 1 Previous Work Arabic is a morphologically rich language: in our training corpus of about 288,000 words we find 3279 distinct morphological tags, with up to 100,000 pos- Because of the large number of tags, it is clear that morphological tagging cannot be construed as a simple classification task. Hajiˇc (2000) is the first to use a dictionary as a source of possible morphological analyses (and hence tags) for an inflected word form. He redefines the tagging task as a choice among the tags proposed by the dictionary, using a log-linear model trained on specific ambiguity classes for individual morphological features. Hajiˇc et al. (2005) implement the approach of Hajiˇc (2000) for Arabic. In previous work, we follow the same approach (Habash and Rambow, 2005), using SVM-classifiers for individual morphological features and a simple combining scheme for choosing among competing analyses proposed by the dictionary. Since the dictionary we use, BAMA (Buckwalter, 2004), also includes diacritics (orthographic work was funded under the DARPA GALE, program, contract HR0011-06-C-0023. We thank several anonymous reviewers for helpful comments. A longer version of this paper is available as a technical report. marks not usually written), we extend this approach to the diacritization task in (Habash and Rambow, 2007). The work presented in this paper differs from this previous work in that (a) we introduce a new for Arabic, namely (b) we use an explicit modeling of lexemes as a component in all tasks discussed in this paper (morphological tagging, diacritization, and lemmatization); and (c) we tune the weights of the feature classifiers on a tuning corpus (different tuning for different tasks). 2 Morphological Disambiguation Tasks for Arabic define the task of tagging as choosing an inflectional morphological tag (in this paper, the term “morphological tagging” never refers to derivational morphology). The morphology of an Arabic word can be described by the 14 (nearly) orthogonal features shown in Figure 1. For different tasks, different subsets may be useful: for example, when translating into a language without case, we may want to omit the case feature. For the experiments we discuss in this paper, we investigate three variants of the morphological tagging tasks: the feature which is the core part-of-speech – verb, noun, adjective, etc.); the set of the first ten basic features listed in Figure 1); and Morthe full inflectional morphological tag, i.e., all 14 features). task of adding diacritics (short vowels, gemination marker shadda, and indefiniteness marker nunation) to the standard written form. We have two variants of the diacritization</abstract>
<note confidence="0.887837666666667">117 of ACL-08: HLT, Short Papers (Companion pages 117–120, Ohio, USA, June 2008. Association for Computational Linguistics</note>
<title confidence="0.7812613">Feature name Explanation POS Simple part-of-speech Presence of a conjunction clitic Presence of a particle clitic Presence of a pronominal clitic Presence of the definite deter-miner Gender Number Person Voice</title>
<abstract confidence="0.958605948051949">Aspect Mood Presence of nunation (indefinite-ness marker) Construct state (head of a geni-tive construction) Case Figure 1: List of (inflectional) morphological features used in our system; the first ten are features which (roughly) can be determined with higher accuracy since they rely less on syntactic context and more on visible inflectional morphology all diacritics of a given word), which relates to lexeme choice and morpholtagging, and all diacritics of a given word except those associated with the final letter), which relates largely to lexeme choice. for Arabic has not been discussed in the literature to our knowledge. A an abstraction over a set of inflected word and it is usually represented by its also called the combined task of choosing all inflectional and lexemic aspects of a word in context. gives us a total of seven tasks. the hardest of our tasks, since it subsumes all other the hardest of the three mortagging tasks, subsuming and the hardest lexical subsuming which in turn subsumes However, general) orthogonal, since no lexcomponent, while 3 Our System Our system, MADA, makes use of 19 orthogonal to select, for each word, a proper anala list of potential analyses provided by the BAMA dictionary. The BAMA analysis which matches the most of the predicted features wins; the weighting of the features is one of the topics of this paper. These 19 features consist of the 14 morphological features shown in Figure 1, which MADA predicts using 14 distinct Support Vector Machines trained on ATB3-Train (as defined by Zitouni et al. (2006)). In addition, MADA uses five additional whether the diacritized form of the suggested analysis and the input word match if both are stripped of all of their diacritics. This is useful because sometimes BAMA suggests analyses which imply a different spelling of the undiacritized word, but these analyses are ofincorrect. those analyses that are the default output of BAMA (typically, these are guesses that the word in question is a proper noun); these analyses are less likely to be correct than others suggested by BAMA. MADA can dethe values of direct examination of the analysis in question, and no predictive model is needed. The fourteen morfeatures plus form a feature collection that is entirely based on morphological (rather than lexemic) features; we reto this collection as unigram models of the surface diacritized form and the lexeme respectively, and contain lexical information. We also build 4gram lexeme models using an open-vocabulary language model with Kneser-Ney smoothing, by means of the SRILM toolkit (Stolcke, 2002). The model is trained on the same corpus used to train the other classifiers, ATB3-Train. (We also tested other ngram models, and found that a 4-gram lexeme model the other orders with &lt; although the improvement over the trigram and 5-gram models was less than 0.01%.) The 4-gram model, on its own, correctly selects the lexeme of words in ATB3-DevTest 94.1% of the time. The 4-gram lexeme model was incorporated into our system as a feature We refer to the feature set of the two unigram mod- 118 and Optimizing the feature weights is a machine learning task. To provide learning data for this task, we take the ATB3-DevTest data set and divide it into sections; the first half words) is used tuning the weights and the second half words) for testing. In a pre-processing step, each analysis in appended with a set of labels which indicate whether the analysis is correct according to seven different evaluation metrics. These metrics correspond in a one-to-one manner to the seven different disambiguation tasks discussed in Section 2, and we use the task name for the evaluation la- Specifically, the is positive if the analysis has the same POS value as the coranalysis in the gold standard; the label provides the same information about the lexchoice. The is positive if the analysis agrees with the gold for each of the 10 basic features used by Habash and Rambow (2005). positive requires that the analysis match the gold in all morphological features, i.e., in every feature except the lexeme choice and dia- The is only positive if the surface diacritics of the analysis match the gold diexactly; less strict in that the trailing sequence diacritic markers in each surface diacritic are stripped before the analysis and the gold compared. Finally, only positive if the analysis was one chosen as correct in the gold; this is the strictest form of evaluation, and there can only one positive per word. In addition to labeling as described in the preceding paragraph, we run MADA on the tuning and test sets. This gives us a set of model predictions for every feature of every word in the tuning and test sets. We use an implementation of a Downhill Simplex Method in many dimensions based on the method developed by Nelder and Mead (1965) to tune the weights applied to each feature. In a given iteration, the Simplex algorithm proposes a set of feature weights. These weights are given to a weight evaluation function; this function determines how effective a particular set of weights is at a given disamtask by calculating an score the weight set: the number of words in the tuning set that were correctly disambiguated. In order to compute this score, the weight evaluation function examines each proposed analysis for each word in the tuning set. If the analysis and the model predicfor a feature of a given word agree, the that analysis is incremented by the weight corresponding to that feature. The analysis with the highest analysis score is selected as the proper analysis for that word. If the selected analysis has a positive task label (i.e., it is a good answer for the disambiguation task in question), the overall score for the proposed weight set is incremented. The Simplex algorithm seeks to maximize this overall score (and thus choose the weight set that performs best for a given task). Once the Simplex algorithm has converged, the optimal feature weights for a given task are known. Our system makes use of these weights to select a correct analysis in the test set. Each analysis of each word is given a score that is the sum of optimal feature weights for features where the model prediction and the analysis agree. The analysis with the highest score is then chosen as the correct analysis for that word. The system can be evaluated simply by comparing the chosen analysis to the gold standard. Since the Simplex weight evaluation function and the system use identical means of scoring analyses, the Simplex algorithm has the potential to find very optimized weights. 4 Experiments We have three main research hypotheses: (1) Using lexemic features helps in all tasks, but especially in the diacritization and lexeme choice tasks. (2) Tuning the weights helps over using identical weights. (3) Tuning to the task that is evaluated improves over tuning to other tasks. For each of the two feature we tune the weights seven producing seven sets of weights. We then evaluate the seven automatically systems using seven The tuning metrics are identical to the evaluation metrics and they correspond to the seven tasks described in Section 2. Instead of showing 98 results, we show in Figure 2 four results for each of the tasks: for both the feature sets, we give the untuned performance, and then the best-performing tuned performance. We inwhich tuning metric provided the best tun- 119 Task Baseline Feats Only) Feats) Not Tuned Tuned Tuning metric Not Tuned Tuned Tuning metric MorphPOS 95.5 95.6 96.0 MorphAll 96.0 96.4 MorphPOS MorphPart 93.8 94.1 94.8 AllChoice 94.7 95.1 DiacPart MorphAll 83.8 84.0 84.8 AllChoice 82.2 85.1 MorphAll LexChoice 85.5 86.6 87.5 MorphAll 95.4 96.3 LexChoice DiacPart 85.1 86.4 87.3 AllChoice 94.8 95.4 DiacPart DiacFull 76.0 77.1 78.2 MorphAll 82.6 86.1 MorphAll AllChoice 73.3 74.5 75.6 AllChoice 80.3 83.8 MorphAll Figure 2: Results for morphological tagging tasks (percent correct); the baseline uses only 14 morphological features with identical weights; “Tuning Metric” refers to the tuning metric that produced the best tuned results, as shown in the “Tuned” column ing performance. The Baseline indicated in Figure 2 uses the 14 morphological features (listed in Figure 1) only, with no tuning (i.e., all 14 features have a weight of 1). The untuned results were determined by also setting almost all feature weights to 1; only exception is the which is given a weight of -(8/14) when included in untuned sets. Since this feature is meant to penalize analyses, its value must be negative; we use this particular value so that our results can be readily compared to previous work. All results are the best published results to date on these test sets; for a deeper discussion, see the longer version of this paper which is available as a technical report. We thus find our three hypotheses confirmed: (1) Using lexemic features reduces error for the morphological tagging tasks (measured on tuned data) by 3% to 11%, but by 36% to 71% for the diacritic and lexeme choice tasks. The highest error reduction is indeed for the lexical choice task. (2) Tuning the weights helps over using identical weights. With only morphological features, we obtain an error reduction of between 4% and 12%; with all features, the error reduction from tuning ranges between 8% and 20%. (3) As for the correlation between tuning task and evaluation task, it turned out that when we use only morphological features, two tuning tasks best for all evaluation tasks, namely Morthus not confirming our hypothesis. We speculate that in the absence of the lexical features, more features is better (these two tasks are the two hardest tasks for morphological features only). If we add the lexemic features, we do find our hypothesis confirmed, with almost all evaluation tasks performing best when the weights are tuned for that task. In the case of the three exceptions, the differences between the best performance and performance when tuned to the same task are very slight References Tim Buckwalter. 2004. Buckwalter Arabic morphological analyzer version 2.0. Nizar Habash and Owen Rambow. 2005. Arabic tokenization, part-of-speech tagging and morphological in one fell swoop. In Ann</abstract>
<address confidence="0.905762">Arbor, MI, USA.</address>
<author confidence="0.5399725">Arabic diacritization through full morphological tagging In</author>
<address confidence="0.8447595">HLT 2007 Companion Volume, Short Rochester, NY, USA.</address>
<author confidence="0.731409">Jan Hajiˇc</author>
<author confidence="0.731409">Otakar Smrˇz</author>
<author confidence="0.731409">Tim Buckwalter</author>
<author confidence="0.731409">Hubert</author>
<abstract confidence="0.256051666666667">Jin. 2005. Feature-based tagger of approximations functional Arabic morphology. In of the Workshop on Treebanks and Linguistic Theories</abstract>
<address confidence="0.865841">Barcelona, Spain.</address>
<author confidence="0.8561145">dic- In Meeting of the North American Chap-</author>
<affiliation confidence="0.98112">ter of the Association for Computational Linguistics</affiliation>
<address confidence="0.987315">Seattle, WA.</address>
<note confidence="0.432883333333333">J.A Nelder and R Mead. 1965. A simplex method for minimization. In pages 303–333. Andreas Stolcke. 2002. Srilm an extensible language In of the International Conferon Spoken Language Processing Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya. 2006. Maximum entropy based restoration of arabic In pages 577–584, Sydney,</note>
<address confidence="0.5183145">Australia. 120</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>117--120</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 117–120,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Buckwalter</author>
</authors>
<title>Buckwalter Arabic morphological analyzer version 2.0.</title>
<date>2004</date>
<contexts>
<context position="1590" citStr="Buckwalter, 2004" startWordPosition="239" endWordPosition="241">ossible morphological analyses (and hence tags) for an inflected word form. He redefines the tagging task as a choice among the tags proposed by the dictionary, using a log-linear model trained on specific ambiguity classes for individual morphological features. Hajiˇc et al. (2005) implement the approach of Hajiˇc (2000) for Arabic. In previous work, we follow the same approach (Habash and Rambow, 2005), using SVM-classifiers for individual morphological features and a simple combining scheme for choosing among competing analyses proposed by the dictionary. Since the dictionary we use, BAMA (Buckwalter, 2004), also includes diacritics (orthographic 1This work was funded under the DARPA GALE, program, contract HR0011-06-C-0023. We thank several anonymous reviewers for helpful comments. A longer version of this paper is available as a technical report. marks not usually written), we extend this approach to the diacritization task in (Habash and Rambow, 2007). The work presented in this paper differs from this previous work in that (a) we introduce a new task for Arabic, namely lemmatization; (b) we use an explicit modeling of lexemes as a component in all tasks discussed in this paper (morphological</context>
</contexts>
<marker>Buckwalter, 2004</marker>
<rawString>Tim Buckwalter. 2004. Buckwalter Arabic morphological analyzer version 2.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop.</title>
<date>2005</date>
<booktitle>In ACL’05,</booktitle>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="1380" citStr="Habash and Rambow, 2005" startWordPosition="207" endWordPosition="210"> 100,000 possible tags.1 Because of the large number of tags, it is clear that morphological tagging cannot be construed as a simple classification task. Hajiˇc (2000) is the first to use a dictionary as a source of possible morphological analyses (and hence tags) for an inflected word form. He redefines the tagging task as a choice among the tags proposed by the dictionary, using a log-linear model trained on specific ambiguity classes for individual morphological features. Hajiˇc et al. (2005) implement the approach of Hajiˇc (2000) for Arabic. In previous work, we follow the same approach (Habash and Rambow, 2005), using SVM-classifiers for individual morphological features and a simple combining scheme for choosing among competing analyses proposed by the dictionary. Since the dictionary we use, BAMA (Buckwalter, 2004), also includes diacritics (orthographic 1This work was funded under the DARPA GALE, program, contract HR0011-06-C-0023. We thank several anonymous reviewers for helpful comments. A longer version of this paper is available as a technical report. marks not usually written), we extend this approach to the diacritization task in (Habash and Rambow, 2007). The work presented in this paper d</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow. 2005. Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop. In ACL’05, Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic diacritization through full morphological tagging.</title>
<date>2007</date>
<booktitle>In NAACL HLT 2007 Companion Volume, Short Papers,</booktitle>
<location>Rochester, NY, USA.</location>
<contexts>
<context position="1944" citStr="Habash and Rambow, 2007" startWordPosition="291" endWordPosition="294"> work, we follow the same approach (Habash and Rambow, 2005), using SVM-classifiers for individual morphological features and a simple combining scheme for choosing among competing analyses proposed by the dictionary. Since the dictionary we use, BAMA (Buckwalter, 2004), also includes diacritics (orthographic 1This work was funded under the DARPA GALE, program, contract HR0011-06-C-0023. We thank several anonymous reviewers for helpful comments. A longer version of this paper is available as a technical report. marks not usually written), we extend this approach to the diacritization task in (Habash and Rambow, 2007). The work presented in this paper differs from this previous work in that (a) we introduce a new task for Arabic, namely lemmatization; (b) we use an explicit modeling of lexemes as a component in all tasks discussed in this paper (morphological tagging, diacritization, and lemmatization); and (c) we tune the weights of the feature classifiers on a tuning corpus (different tuning for different tasks). 2 Morphological Disambiguation Tasks for Arabic We define the task of morphological tagging as choosing an inflectional morphological tag (in this paper, the term “morphological tagging” never r</context>
</contexts>
<marker>Habash, Rambow, 2007</marker>
<rawString>Nizar Habash and Owen Rambow. 2007. Arabic diacritization through full morphological tagging. In NAACL HLT 2007 Companion Volume, Short Papers, Rochester, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Otakar Smrˇz</author>
<author>Tim Buckwalter</author>
<author>Hubert Jin</author>
</authors>
<title>Feature-based tagger of approximations of functional Arabic morphology.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories (TLT),</booktitle>
<location>Barcelona,</location>
<marker>Hajiˇc, Smrˇz, Buckwalter, Jin, 2005</marker>
<rawString>Jan Hajiˇc, Otakar Smrˇz, Tim Buckwalter, and Hubert Jin. 2005. Feature-based tagger of approximations of functional Arabic morphology. In Proceedings of the Workshop on Treebanks and Linguistic Theories (TLT), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Morphological tagging: Data vs. dictionaries.</title>
<date>2000</date>
<booktitle>In 1st Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL’00),</booktitle>
<location>Seattle, WA.</location>
<marker>Hajiˇc, 2000</marker>
<rawString>Jan Hajiˇc. 2000. Morphological tagging: Data vs. dictionaries. In 1st Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL’00), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Nelder</author>
<author>R Mead</author>
</authors>
<title>A simplex method for function minimization.</title>
<date>1965</date>
<journal>In Computer Journal,</journal>
<pages>303--333</pages>
<marker>Nelder, Mead, 1965</marker>
<rawString>J.A Nelder and R Mead. 1965. A simplex method for function minimization. In Computer Journal, pages 303–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP).</booktitle>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language toolkit. In Proceedings of the International Conference on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imed Zitouni</author>
<author>Jeffrey S Sorensen</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Maximum entropy based restoration of arabic diacritics.</title>
<date>2006</date>
<booktitle>In Coling-ACL’06,</booktitle>
<pages>577--584</pages>
<location>Sydney, Australia.</location>
<marker>Zitouni, Sorensen, Sarikaya, 2006</marker>
<rawString>Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya. 2006. Maximum entropy based restoration of arabic diacritics. In Coling-ACL’06, pages 577–584, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>