<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000311">
<title confidence="0.9988815">
A Unified Single Scan Algorithm
for Japanese Base Phrase Chunking and Dependency Parsing
</title>
<author confidence="0.965749">
Manabu Sassano
</author>
<affiliation confidence="0.95462">
Yahoo Japan Corporation
</affiliation>
<address confidence="0.904242666666667">
Midtown Tower,
9-7-1 Akasaka, Minato-ku,
Tokyo 107-6211, Japan
</address>
<email confidence="0.933225">
msassano@yahoo-corp.jp
</email>
<sectionHeader confidence="0.996327" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999686727272727">
We describe an algorithm for Japanese
analysis that does both base phrase chunk-
ing and dependency parsing simultane-
ously in linear-time with a single scan of a
sentence. In this paper, we show a pseudo
code of the algorithm and evaluate its per-
formance empirically on the Kyoto Uni-
versity Corpus. Experimental results show
that the proposed algorithm with the voted
perceptron yields reasonably good accu-
racy.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99933035">
Single scan algorithms of parsing are important for
interactive applications of NLP. For instance, such
algorithms would be more suitable for robots ac-
cepting speech inputs or chatbots handling natural
language inputs which should respond quickly in
some situations even when human inputs are not
clearly ended.
Japanese sentence analysis typically consists of
three major steps, namely morphological analysis,
bunsetsu (base phrase) chunking, and dependency
parsing. In this paper, we describe a novel algo-
rithm that combines the last two steps into a sin-
gle scan process. The algorithm, which is an ex-
tension of Sassano’s (2004), allows us to chunk
morphemes into base phrases and decide depen-
dency relations of the phrases in a strict left-to-
right manner. We show a pseudo code of the al-
gorithm and evaluate its performance empirically
with the voted perceptron on the Kyoto University
Corpus (Kurohashi and Nagao, 1998).
</bodyText>
<sectionHeader confidence="0.982195" genericHeader="method">
2 Japanese Sentence Structure
</sectionHeader>
<bodyText confidence="0.982288">
In Japanese NLP, it is often assumed that the struc-
ture of a sentence is given by dependency relations
</bodyText>
<author confidence="0.448735">
Sadao Kurohashi
</author>
<affiliation confidence="0.874085">
Graduate School of Informatics,
Kyoto University
</affiliation>
<address confidence="0.345881">
Yoshida-honmachi, Sakyo-ku,
Kyoto 606-8501, Japan
</address>
<email confidence="0.753503">
kuro@i.kyoto-u.ac.jp
</email>
<bodyText confidence="0.94302075">
Meg-ga kare-ni ano pen-wo age-ta.
Meg-subj to him that pen-acc give-past.
ID 0 1 2 3 4
Head 4 4 3 4 -
</bodyText>
<figureCaption confidence="0.996401">
Figure 1: Sample sentence (bunsetsu-based)
</figureCaption>
<bodyText confidence="0.856037923076923">
among bunsetsus. A bunsetsu is a base phrasal
unit and consists of one or more content words fol-
lowed by zero or more function words.
In addition, most of algorithms of Japanese de-
pendency parsing, e.g., (Sekine et al., 2000; Sas-
sano, 2004), assume the three constraints below.
(1) Each bunsetsu has only one head except the
rightmost one. (2) Dependency links between bun-
setsus go from left to right. (3) Dependency links
do not cross one another. In other words, depen-
dencies are projective.
A sample sentence in Japanese is shown in Fig-
ure 1. We can see all the constraints are satisfied.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="method">
3 Previous Work
</sectionHeader>
<bodyText confidence="0.9950968">
As far as we know, there is no dependency parser
that does simultaneously both bunsetsu chunking
and dependency parsing and, in addition, does
them with a single scan. Most of the modern
dependency parsers for Japanese require bunsetsu
chunking (base phrase chunking) before depen-
dency parsing (Sekine et al., 2000; Kudo and Mat-
sumoto, 2002; Sassano, 2004). Although word-
based parsers are proposed in (Mori et al., 2000;
Mori, 2002), they do not build bunsetsus and are
not compatible with other Japanese dependency
parsers. Multilingual parsers of participants in the
CoNLL 2006 shared task (Buchholz and Marsi,
2006) can handle Japanese sentences. But they are
basically word-based.
</bodyText>
<page confidence="0.995052">
49
</page>
<note confidence="0.925905">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 49–52,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9798824">
Meg ga kare ni ano pen wo age-ta.
Meg subj him to that pen acc give-past.
ID 0 1 2 3 4 5 6 7
Head 1 7 3 7 6 6 7 -
Type B D B D D B D -
</bodyText>
<figureCaption confidence="0.7097945">
Figure 2: Sample sentence (morpheme-based).
“Type” represents the type of dependency relation.
</figureCaption>
<sectionHeader confidence="0.998383" genericHeader="method">
4 Algorithm
</sectionHeader>
<subsectionHeader confidence="0.99945">
4.1 Dependency Representation
</subsectionHeader>
<bodyText confidence="0.920573862745098">
In our proposed algorithm, we use a morpheme-
based dependency structure instead of a bunsetsu-
based one. The morpheme-based representation
is carefully designed to convey the same informa-
tion on dependency structure of a sentence without
the loss from the bunsetsu-based one. The right-
most morpheme of the bunsetsu t should modify
the rightmost morpheme of the bunsetsu u when
the bunsetsu t modifies the bunsetsu u. Every
morpheme except the rightmost one in a bunsetsu
should modify its following one. The sample sen-
tence in Figure 1 is converted to the sentence with
our proposed morpheme-based representation in
Figure 2.
Take for instance, the head of the 0-th bunsetsu
“Meg-ga” is the 4-th bunsetsu “age-ta.” in Fig-
ure 1. This dependency relation is represented by
that the head of the morpheme “ga” is “age-ta.” in
Figure 2.
The morpheme-based representation above can-
not explicitly state the boundaries of bunsetsus.
Thus we add the type to every dependency rela-
tion. A bunsetsu boundary is represented by the
type associated with every dependency relation.
The type “D” represents that this relation is a de-
pendency of two bunsetsus, while the type “B”
represents a sequence of morphemes inside of a
given bunsetsu. In addition, the type “O”, which
represents that two morphemes do not have a de-
pendency relation, is used in implementations of
our algorithm with a trainable classifier. Following
this encoding scheme of the type of dependency
relations bunsetsu boundaries exist just after the
morphemes that have the type “D”. Inserting “|”
after every morpheme with “D” of the sentence in
Figure 2 results in Meg-ga  |kare-ni  |ano  |pen-wo
 |age-ta. This is identical to the sentence with the
bunsetsu-based representation in Figure 1.
Input: wi: morphemes in a given sentence.
N: the number of morphemes.
Output: hj: the head IDs of morphemes wj.
tj: the type of dependency relation. A possible
value is either ”B”, ”D”, or ”O”.
Functions: Push(i, s): pushes i on the stack s.
Pop(s): pops a value off the stack s.
Dep(j, i, w, t): returns true when wj should
modify wi. Otherwise returns false. Sets
always tj.
procedure Analyze(w, N, h, t)
var s: a stack for IDs of modifier morphemes
begin
</bodyText>
<equation confidence="0.898946666666667">
Push(−1, s); { −1 for end-of-sentence }
Push(0, s);
for i ← 1 to N − 1 do begin
j ← Pop(s);
while (j =6 −1
and (Dep(j, i, w, t) or (i = N − 1)) ) do
begin
hj ← i; j ← Pop(s)
end
Push(j, s); Push(i, s)
end
end
</equation>
<figureCaption confidence="0.9834185">
Figure 3: Pseudo code for base phrase chunking
and dependency parsing.
</figureCaption>
<subsectionHeader confidence="0.964074">
4.2 Pseudo Code for the Proposed Algorithm
</subsectionHeader>
<bodyText confidence="0.99990345">
The algorithm that we propose is based on (Sas-
sano, 2004), which is considered to be a simple
form of shift-reduce parsing. The pseudo code of
our algorithm is presented in Figure 3. Important
variables here are hj and tj where j is an index
of morphemes. The variable hj holds the head ID
and the variable tj has the type of dependency re-
lation. For example, the head and the dependency
relation type of “Meg” in Figure 2 are represented
as ho = 1 and to = “B” respectively. The flow
of the algorithm, which has the same structure as
Sassano’s (2004), is controlled with a stack that
holds IDs for modifier morphemes. Decision of
the relation between two morphemes is made in
Dep(), which uses a machine learning-based clas-
sifier that supports multiclass prediction.
The presented algorithm runs in a left-to-right
manner and its upper bound of the time complex-
ity is O(n). Due to space limitation, we do not
discuss its complexity here. See (Sassano, 2004)
</bodyText>
<page confidence="0.979146">
50
</page>
<bodyText confidence="0.916208">
for further details.
</bodyText>
<sectionHeader confidence="0.995727" genericHeader="evaluation">
5 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.742233">
5.1 Experimental Set-up
</subsectionHeader>
<bodyText confidence="0.996024">
Corpus For evaluation, we used the Kyoto Uni-
versity Corpus Version 2 (Kurohashi and Nagao,
1998). The split for training/test/development is
the same as in other papers, e.g., (Uchimoto et al.,
1999).
Selection of a Classifier and its Setting We im-
plemented a parser with the voted perceptron (VP)
(Freund and Schapire, 1999). We used a poly-
nomial kernel and set its degree to 3 because cu-
bic kernels proved to be effective empirically for
Japanese parsing (Kudo and Matsumoto, 2002).
The number of epoch T of VP was selected using
the development test set. For multiclass predic-
tion, we used the pairwise method (Kreßel, 1999).
Features We have designed rather simple fea-
tures based on the common feature set (Uchimoto
et al., 1999; Kudo and Matsumoto, 2002; Sassano,
2004) for bunsetsu-based parsers. We use the fol-
lowing features for each morpheme:
</bodyText>
<listItem confidence="0.99794375">
1. major POS, minor POS, conjugation type,
conjugation form, surface form (lexicalized
form)
2. Content word or function word
3. Punctuation (periods and commas)
4. Open parentheses and close parentheses
5. Location (at the beginning or end of the sen-
tence)
</listItem>
<bodyText confidence="0.99618725">
Gap features between two morphemes are also
used since they have proven to be very useful and
contribute to the accuracy (Uchimoto et al., 1999;
Kudo and Matsumoto, 2002). They are repre-
sented as a binary feature and include distance (1,
2, 3, 4 – 10, or 11 G), particles, parentheses, and
punctuation.
In our proposed algorithm basically two mor-
phemes are examined to estimate their dependency
relation. Context information about the current
morphemes to be estimated would be very use-
ful and we can incorporate such information into
our model. We assume that we have the j-th mor-
pheme and the i-th one in Figure 3. We also use
the j −n,..., j −1, j +1, ..., j +n morphemes and
the i − n, ..., i − 1, i + 1, ..., i + n ones, where n
</bodyText>
<table confidence="0.914245">
Measure Accuracy (%)
Dependency Acc. 93.96
Dep. Type Acc. 99.49
Both 93.92
</table>
<tableCaption confidence="0.937235333333333">
Table 1: Performance on the test set. This result is
achieved by the following parameters: The size of
context window is 2 and epoch T is 4.
</tableCaption>
<table confidence="0.855046666666667">
Bunsetsu-based Morpheme-based
Previous 88.48 95.09
Ours NA 93.96
</table>
<tableCaption confidence="0.971218">
Table 2: Dependency accuracy. The system with
</tableCaption>
<bodyText confidence="0.90286175">
the previous method employs the algorithm (Sas-
sano, 2004) with the voted perceptron.
is the size of the context window. We examined 0,
1, 2 and 3 for n.
</bodyText>
<subsectionHeader confidence="0.955764">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9999383">
Accuracy Performances of our parser on the test
set is shown in Table 1. The dependency accuracy
is the percentage of the morphemes that have a
correct head. The dependency type accuracy is the
percentage of the morphemes that have a correct
dependency type, i.e., “B” or “D”. The bottom line
of Table 1 shows the percentage of the morphemes
that have both a correct head and a correct depen-
dency type. In all these measures we excluded the
last morpheme in a sentence, which does not have
a head and its associated dependency type.
The accuracy of dependency type in Table 1
is interpreted to be accuracy of base phrase
(bunsetsu) chunking. Very accurate chunking is
achieved.
Next we examine the dependency accuracy. In
order to recognize how accurate it is, we com-
pared the performance of our parser with that of
the parser that uses one of previous methods. We
implemented a parser that employs the algorithm
of (Sassano, 2004) with the commonly used fea-
tures and runs with VP instead of SVM, which
Sassano (2004) originally used. His parser, which
cannot do bunsetsu chunking, accepts only a chun-
ked sentence and then produces a bunsetsu-based
dependency structure. Thus we cannot directly
compare results with ours. To enable us to com-
pare them we gave bunsetsu chunked sentences by
our parser to the parser of (Sassano, 2004) instead
of giving directly the correct chunked sentences
</bodyText>
<page confidence="0.997606">
51
</page>
<table confidence="0.9882038">
Window Size Dep. Acc. Dep. Type Acc.
0 (T = 1) 82.71 99.29
1 (T = 2) 93.57 99.49
2 (T = 4) 93.96 99.49
3 (T = 3) 93.79 99.42
</table>
<tableCaption confidence="0.9701275">
Table 3: Performance change depending on the
context window size
</tableCaption>
<figure confidence="0.98983">
3
2.5
2
Seconds 1.5
1
0.5
0
0 10 20 30 40 50 60 70 80 90 100
Sentence Length (Number of Morphemes)
</figure>
<figureCaption confidence="0.99586">
Figure 4: Running time on the test set. We used
a PC (Intel Xeon 2.33 GHz with 8GB memory on
FreeBSD 6.3).
</figureCaption>
<bodyText confidence="0.995201833333333">
in the Kyoto University Corpus. And then we re-
ceived results from the parser of (Sassano, 2004),
which are bunsetsu-based dependency structures,
and converted them to morpheme-based structures
that follow the scheme we propose in this paper.
Finally we have got results that have the compat-
ible format and show a comparison with them in
Table 2.
Although the bunsetsu-based parser outper-
formed slightly our morpheme-based parser in this
experiment, it is still notable that our method
yields comparable performance with even a sin-
gle scan of a sentence for dependency parsing in
addition to bunsetsu chunking. According to the
results in Table 2, we suppose that performance of
our parser roughly corresponds to about 86–87%
in terms of bunsetsu-based accuracy.
Context Window Size Performance change de-
pending on the size of context window is shown
in Table 3. Among them the best size is 2. In
this case, we use ten morphemes to determine
whether or not given two morphemes have a de-
pendency relation. That is, to decide the relation
of morphemes j and i (j &lt; i), we use morphemes
j−2, j−1, j, j+1, j+2 and i−2, i−1, i, i+1, i+2.
Running Time and Asymptotic Time Complex-
ity We have observed that the running time is
proportional to the sentence length (Figure 4). The
theoretical time complexity of the proposed algo-
rithm is confirmed with this observation.
</bodyText>
<sectionHeader confidence="0.995638" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999978">
We have described a novel algorithm that com-
bines Japanese base phrase chunking and depen-
dency parsing into a single scan process. The pro-
posed algorithm runs in linear-time with a single
scan of a sentence.
In future work we plan to combine morpholog-
ical analysis or word segmentation into our pro-
posed algorithm. We also expect that structure
analysis of compound nouns can be incorporated
by extending the dependency relation types. Fur-
thermore, we believe it would be interesting to
discuss linguistically and psycholinguistically the
differences between Japanese and other European
languages such as English. We would like to know
what differences lead to easiness of analyzing a
Japanese sentence.
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999955413793103">
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proc. of CoNLL
2006, pages 149–164.
Y. Freund and R. E. Schapire. 1999. Large margin classifi-
cation using the perceptron algorithm. Machine Learning,
37(3):277–296.
U. Kreliel. 1999. Pairwise classification and support vec-
tor machines. In B. Sch¨olkopf, C. J. Burges, and A. J.
Smola, editors, Advances in Kernel Methods: Support
Vector Learning, pages 255–268. MIT Press.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In Proc. of CoNLL-
2002, pages 63–69.
S. Kurohashi and M. Nagao. 1998. Building a Japanese
parsed corpus while improving the parsing system. In
Proc. ofLREC-1998, pages 719–724.
S. Mori, M. Nishimura, N. Itoh, S. Ogino, and H. Watanabe.
2000. A stochastic parser based on a structural word pre-
diction model. In Proc. of COLING 2000, pages 558–564.
S. Mori. 2002. A stochastic parser based on an SLM with
arboreal context trees. In Proc. of COLING 2002.
M. Sassano. 2004. Linear-time dependency analysis for
Japanese. In Proc. of COLING 2004, pages 8–14.
S. Sekine, K. Uchimoto, and H. Isahara. 2000. Back-
ward beam search algorithm for dependency analysis of
Japanese. In Proc. of COLING-00, pages 754–760.
K. Uchimoto, S. Sekine, and H. Isahara. 1999. Japanese
dependency structure analysis based on maximum entropy
models. In Proc. of EACL-99, pages 196–203.
</reference>
<page confidence="0.998858">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.476153">
<title confidence="0.9991235">A Unified Single Scan Algorithm for Japanese Base Phrase Chunking and Dependency Parsing</title>
<author confidence="0.943493">Manabu Sassano</author>
<affiliation confidence="0.805455">Yahoo Japan Corporation Midtown Tower,</affiliation>
<address confidence="0.954629">9-7-1 Akasaka, Minato-ku, Tokyo 107-6211, Japan</address>
<email confidence="0.979088">msassano@yahoo-corp.jp</email>
<abstract confidence="0.981682083333333">We describe an algorithm for Japanese analysis that does both base phrase chunking and dependency parsing simultaneously in linear-time with a single scan of a sentence. In this paper, we show a pseudo code of the algorithm and evaluate its performance empirically on the Kyoto University Corpus. Experimental results show that the proposed algorithm with the voted perceptron yields reasonably good accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL</booktitle>
<pages>149--164</pages>
<contexts>
<context position="3213" citStr="Buchholz and Marsi, 2006" startWordPosition="509" endWordPosition="512">s Work As far as we know, there is no dependency parser that does simultaneously both bunsetsu chunking and dependency parsing and, in addition, does them with a single scan. Most of the modern dependency parsers for Japanese require bunsetsu chunking (base phrase chunking) before dependency parsing (Sekine et al., 2000; Kudo and Matsumoto, 2002; Sassano, 2004). Although wordbased parsers are proposed in (Mori et al., 2000; Mori, 2002), they do not build bunsetsus and are not compatible with other Japanese dependency parsers. Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. But they are basically word-based. 49 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 49–52, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP Meg ga kare ni ano pen wo age-ta. Meg subj him to that pen acc give-past. ID 0 1 2 3 4 5 6 7 Head 1 7 3 7 6 6 7 - Type B D B D D B D - Figure 2: Sample sentence (morpheme-based). “Type” represents the type of dependency relation. 4 Algorithm 4.1 Dependency Representation In our proposed algorithm, we use a morphemebased dependency structure instead of a bunsetsubased one. The morpheme-based represen</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of CoNLL 2006, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="7568" citStr="Freund and Schapire, 1999" startWordPosition="1270" endWordPosition="1273"> supports multiclass prediction. The presented algorithm runs in a left-to-right manner and its upper bound of the time complexity is O(n). Due to space limitation, we do not discuss its complexity here. See (Sassano, 2004) 50 for further details. 5 Experiments and Discussion 5.1 Experimental Set-up Corpus For evaluation, we used the Kyoto University Corpus Version 2 (Kurohashi and Nagao, 1998). The split for training/test/development is the same as in other papers, e.g., (Uchimoto et al., 1999). Selection of a Classifier and its Setting We implemented a parser with the voted perceptron (VP) (Freund and Schapire, 1999). We used a polynomial kernel and set its degree to 3 because cubic kernels proved to be effective empirically for Japanese parsing (Kudo and Matsumoto, 2002). The number of epoch T of VP was selected using the development test set. For multiclass prediction, we used the pairwise method (Kreßel, 1999). Features We have designed rather simple features based on the common feature set (Uchimoto et al., 1999; Kudo and Matsumoto, 2002; Sassano, 2004) for bunsetsu-based parsers. We use the following features for each morpheme: 1. major POS, minor POS, conjugation type, conjugation form, surface form</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Kreliel</author>
</authors>
<title>Pairwise classification and support vector machines.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods: Support Vector Learning,</booktitle>
<pages>255--268</pages>
<editor>In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<marker>Kreliel, 1999</marker>
<rawString>U. Kreliel. 1999. Pairwise classification and support vector machines. In B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors, Advances in Kernel Methods: Support Vector Learning, pages 255–268. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proc. of CoNLL2002,</booktitle>
<pages>63--69</pages>
<contexts>
<context position="2935" citStr="Kudo and Matsumoto, 2002" startWordPosition="465" endWordPosition="469"> rightmost one. (2) Dependency links between bunsetsus go from left to right. (3) Dependency links do not cross one another. In other words, dependencies are projective. A sample sentence in Japanese is shown in Figure 1. We can see all the constraints are satisfied. 3 Previous Work As far as we know, there is no dependency parser that does simultaneously both bunsetsu chunking and dependency parsing and, in addition, does them with a single scan. Most of the modern dependency parsers for Japanese require bunsetsu chunking (base phrase chunking) before dependency parsing (Sekine et al., 2000; Kudo and Matsumoto, 2002; Sassano, 2004). Although wordbased parsers are proposed in (Mori et al., 2000; Mori, 2002), they do not build bunsetsus and are not compatible with other Japanese dependency parsers. Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. But they are basically word-based. 49 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 49–52, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP Meg ga kare ni ano pen wo age-ta. Meg subj him to that pen acc give-past. ID 0 1 2 3 4 5 6 7 Head 1 7 3 7 6 6 7 - Type B D B</context>
<context position="7726" citStr="Kudo and Matsumoto, 2002" startWordPosition="1298" endWordPosition="1301">tion, we do not discuss its complexity here. See (Sassano, 2004) 50 for further details. 5 Experiments and Discussion 5.1 Experimental Set-up Corpus For evaluation, we used the Kyoto University Corpus Version 2 (Kurohashi and Nagao, 1998). The split for training/test/development is the same as in other papers, e.g., (Uchimoto et al., 1999). Selection of a Classifier and its Setting We implemented a parser with the voted perceptron (VP) (Freund and Schapire, 1999). We used a polynomial kernel and set its degree to 3 because cubic kernels proved to be effective empirically for Japanese parsing (Kudo and Matsumoto, 2002). The number of epoch T of VP was selected using the development test set. For multiclass prediction, we used the pairwise method (Kreßel, 1999). Features We have designed rather simple features based on the common feature set (Uchimoto et al., 1999; Kudo and Matsumoto, 2002; Sassano, 2004) for bunsetsu-based parsers. We use the following features for each morpheme: 1. major POS, minor POS, conjugation type, conjugation form, surface form (lexicalized form) 2. Content word or function word 3. Punctuation (periods and commas) 4. Open parentheses and close parentheses 5. Location (at the beginni</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>T. Kudo and Y. Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Proc. of CoNLL2002, pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kurohashi</author>
<author>M Nagao</author>
</authors>
<title>Building a Japanese parsed corpus while improving the parsing system.</title>
<date>1998</date>
<booktitle>In Proc. ofLREC-1998,</booktitle>
<pages>719--724</pages>
<contexts>
<context position="1571" citStr="Kurohashi and Nagao, 1998" startWordPosition="239" endWordPosition="242">clearly ended. Japanese sentence analysis typically consists of three major steps, namely morphological analysis, bunsetsu (base phrase) chunking, and dependency parsing. In this paper, we describe a novel algorithm that combines the last two steps into a single scan process. The algorithm, which is an extension of Sassano’s (2004), allows us to chunk morphemes into base phrases and decide dependency relations of the phrases in a strict left-toright manner. We show a pseudo code of the algorithm and evaluate its performance empirically with the voted perceptron on the Kyoto University Corpus (Kurohashi and Nagao, 1998). 2 Japanese Sentence Structure In Japanese NLP, it is often assumed that the structure of a sentence is given by dependency relations Sadao Kurohashi Graduate School of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan kuro@i.kyoto-u.ac.jp Meg-ga kare-ni ano pen-wo age-ta. Meg-subj to him that pen-acc give-past. ID 0 1 2 3 4 Head 4 4 3 4 - Figure 1: Sample sentence (bunsetsu-based) among bunsetsus. A bunsetsu is a base phrasal unit and consists of one or more content words followed by zero or more function words. In addition, most of algorithms of Japanese depend</context>
<context position="7339" citStr="Kurohashi and Nagao, 1998" startWordPosition="1233" endWordPosition="1236">ch has the same structure as Sassano’s (2004), is controlled with a stack that holds IDs for modifier morphemes. Decision of the relation between two morphemes is made in Dep(), which uses a machine learning-based classifier that supports multiclass prediction. The presented algorithm runs in a left-to-right manner and its upper bound of the time complexity is O(n). Due to space limitation, we do not discuss its complexity here. See (Sassano, 2004) 50 for further details. 5 Experiments and Discussion 5.1 Experimental Set-up Corpus For evaluation, we used the Kyoto University Corpus Version 2 (Kurohashi and Nagao, 1998). The split for training/test/development is the same as in other papers, e.g., (Uchimoto et al., 1999). Selection of a Classifier and its Setting We implemented a parser with the voted perceptron (VP) (Freund and Schapire, 1999). We used a polynomial kernel and set its degree to 3 because cubic kernels proved to be effective empirically for Japanese parsing (Kudo and Matsumoto, 2002). The number of epoch T of VP was selected using the development test set. For multiclass prediction, we used the pairwise method (Kreßel, 1999). Features We have designed rather simple features based on the commo</context>
</contexts>
<marker>Kurohashi, Nagao, 1998</marker>
<rawString>S. Kurohashi and M. Nagao. 1998. Building a Japanese parsed corpus while improving the parsing system. In Proc. ofLREC-1998, pages 719–724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mori</author>
<author>M Nishimura</author>
<author>N Itoh</author>
<author>S Ogino</author>
<author>H Watanabe</author>
</authors>
<title>A stochastic parser based on a structural word prediction model.</title>
<date>2000</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>558--564</pages>
<contexts>
<context position="3014" citStr="Mori et al., 2000" startWordPosition="479" endWordPosition="482">ndency links do not cross one another. In other words, dependencies are projective. A sample sentence in Japanese is shown in Figure 1. We can see all the constraints are satisfied. 3 Previous Work As far as we know, there is no dependency parser that does simultaneously both bunsetsu chunking and dependency parsing and, in addition, does them with a single scan. Most of the modern dependency parsers for Japanese require bunsetsu chunking (base phrase chunking) before dependency parsing (Sekine et al., 2000; Kudo and Matsumoto, 2002; Sassano, 2004). Although wordbased parsers are proposed in (Mori et al., 2000; Mori, 2002), they do not build bunsetsus and are not compatible with other Japanese dependency parsers. Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. But they are basically word-based. 49 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 49–52, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP Meg ga kare ni ano pen wo age-ta. Meg subj him to that pen acc give-past. ID 0 1 2 3 4 5 6 7 Head 1 7 3 7 6 6 7 - Type B D B D D B D - Figure 2: Sample sentence (morpheme-based). “Type” represents the ty</context>
</contexts>
<marker>Mori, Nishimura, Itoh, Ogino, Watanabe, 2000</marker>
<rawString>S. Mori, M. Nishimura, N. Itoh, S. Ogino, and H. Watanabe. 2000. A stochastic parser based on a structural word prediction model. In Proc. of COLING 2000, pages 558–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mori</author>
</authors>
<title>A stochastic parser based on an SLM with arboreal context trees.</title>
<date>2002</date>
<booktitle>In Proc. of COLING</booktitle>
<contexts>
<context position="3027" citStr="Mori, 2002" startWordPosition="483" endWordPosition="484"> cross one another. In other words, dependencies are projective. A sample sentence in Japanese is shown in Figure 1. We can see all the constraints are satisfied. 3 Previous Work As far as we know, there is no dependency parser that does simultaneously both bunsetsu chunking and dependency parsing and, in addition, does them with a single scan. Most of the modern dependency parsers for Japanese require bunsetsu chunking (base phrase chunking) before dependency parsing (Sekine et al., 2000; Kudo and Matsumoto, 2002; Sassano, 2004). Although wordbased parsers are proposed in (Mori et al., 2000; Mori, 2002), they do not build bunsetsus and are not compatible with other Japanese dependency parsers. Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. But they are basically word-based. 49 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 49–52, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP Meg ga kare ni ano pen wo age-ta. Meg subj him to that pen acc give-past. ID 0 1 2 3 4 5 6 7 Head 1 7 3 7 6 6 7 - Type B D B D D B D - Figure 2: Sample sentence (morpheme-based). “Type” represents the type of depende</context>
</contexts>
<marker>Mori, 2002</marker>
<rawString>S. Mori. 2002. A stochastic parser based on an SLM with arboreal context trees. In Proc. of COLING 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sassano</author>
</authors>
<title>Linear-time dependency analysis for Japanese.</title>
<date>2004</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>8--14</pages>
<contexts>
<context position="2227" citStr="Sassano, 2004" startWordPosition="348" endWordPosition="350">e NLP, it is often assumed that the structure of a sentence is given by dependency relations Sadao Kurohashi Graduate School of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan kuro@i.kyoto-u.ac.jp Meg-ga kare-ni ano pen-wo age-ta. Meg-subj to him that pen-acc give-past. ID 0 1 2 3 4 Head 4 4 3 4 - Figure 1: Sample sentence (bunsetsu-based) among bunsetsus. A bunsetsu is a base phrasal unit and consists of one or more content words followed by zero or more function words. In addition, most of algorithms of Japanese dependency parsing, e.g., (Sekine et al., 2000; Sassano, 2004), assume the three constraints below. (1) Each bunsetsu has only one head except the rightmost one. (2) Dependency links between bunsetsus go from left to right. (3) Dependency links do not cross one another. In other words, dependencies are projective. A sample sentence in Japanese is shown in Figure 1. We can see all the constraints are satisfied. 3 Previous Work As far as we know, there is no dependency parser that does simultaneously both bunsetsu chunking and dependency parsing and, in addition, does them with a single scan. Most of the modern dependency parsers for Japanese require bunse</context>
<context position="6264" citStr="Sassano, 2004" startWordPosition="1049" endWordPosition="1051">i on the stack s. Pop(s): pops a value off the stack s. Dep(j, i, w, t): returns true when wj should modify wi. Otherwise returns false. Sets always tj. procedure Analyze(w, N, h, t) var s: a stack for IDs of modifier morphemes begin Push(−1, s); { −1 for end-of-sentence } Push(0, s); for i ← 1 to N − 1 do begin j ← Pop(s); while (j =6 −1 and (Dep(j, i, w, t) or (i = N − 1)) ) do begin hj ← i; j ← Pop(s) end Push(j, s); Push(i, s) end end Figure 3: Pseudo code for base phrase chunking and dependency parsing. 4.2 Pseudo Code for the Proposed Algorithm The algorithm that we propose is based on (Sassano, 2004), which is considered to be a simple form of shift-reduce parsing. The pseudo code of our algorithm is presented in Figure 3. Important variables here are hj and tj where j is an index of morphemes. The variable hj holds the head ID and the variable tj has the type of dependency relation. For example, the head and the dependency relation type of “Meg” in Figure 2 are represented as ho = 1 and to = “B” respectively. The flow of the algorithm, which has the same structure as Sassano’s (2004), is controlled with a stack that holds IDs for modifier morphemes. Decision of the relation between two m</context>
<context position="8017" citStr="Sassano, 2004" startWordPosition="1349" endWordPosition="1350">r papers, e.g., (Uchimoto et al., 1999). Selection of a Classifier and its Setting We implemented a parser with the voted perceptron (VP) (Freund and Schapire, 1999). We used a polynomial kernel and set its degree to 3 because cubic kernels proved to be effective empirically for Japanese parsing (Kudo and Matsumoto, 2002). The number of epoch T of VP was selected using the development test set. For multiclass prediction, we used the pairwise method (Kreßel, 1999). Features We have designed rather simple features based on the common feature set (Uchimoto et al., 1999; Kudo and Matsumoto, 2002; Sassano, 2004) for bunsetsu-based parsers. We use the following features for each morpheme: 1. major POS, minor POS, conjugation type, conjugation form, surface form (lexicalized form) 2. Content word or function word 3. Punctuation (periods and commas) 4. Open parentheses and close parentheses 5. Location (at the beginning or end of the sentence) Gap features between two morphemes are also used since they have proven to be very useful and contribute to the accuracy (Uchimoto et al., 1999; Kudo and Matsumoto, 2002). They are represented as a binary feature and include distance (1, 2, 3, 4 – 10, or 11 G), pa</context>
<context position="9471" citStr="Sassano, 2004" startWordPosition="1604" endWordPosition="1606">orporate such information into our model. We assume that we have the j-th morpheme and the i-th one in Figure 3. We also use the j −n,..., j −1, j +1, ..., j +n morphemes and the i − n, ..., i − 1, i + 1, ..., i + n ones, where n Measure Accuracy (%) Dependency Acc. 93.96 Dep. Type Acc. 99.49 Both 93.92 Table 1: Performance on the test set. This result is achieved by the following parameters: The size of context window is 2 and epoch T is 4. Bunsetsu-based Morpheme-based Previous 88.48 95.09 Ours NA 93.96 Table 2: Dependency accuracy. The system with the previous method employs the algorithm (Sassano, 2004) with the voted perceptron. is the size of the context window. We examined 0, 1, 2 and 3 for n. 5.2 Results and Discussion Accuracy Performances of our parser on the test set is shown in Table 1. The dependency accuracy is the percentage of the morphemes that have a correct head. The dependency type accuracy is the percentage of the morphemes that have a correct dependency type, i.e., “B” or “D”. The bottom line of Table 1 shows the percentage of the morphemes that have both a correct head and a correct dependency type. In all these measures we excluded the last morpheme in a sentence, which d</context>
<context position="10926" citStr="Sassano, 2004" startWordPosition="1858" endWordPosition="1859"> In order to recognize how accurate it is, we compared the performance of our parser with that of the parser that uses one of previous methods. We implemented a parser that employs the algorithm of (Sassano, 2004) with the commonly used features and runs with VP instead of SVM, which Sassano (2004) originally used. His parser, which cannot do bunsetsu chunking, accepts only a chunked sentence and then produces a bunsetsu-based dependency structure. Thus we cannot directly compare results with ours. To enable us to compare them we gave bunsetsu chunked sentences by our parser to the parser of (Sassano, 2004) instead of giving directly the correct chunked sentences 51 Window Size Dep. Acc. Dep. Type Acc. 0 (T = 1) 82.71 99.29 1 (T = 2) 93.57 99.49 2 (T = 4) 93.96 99.49 3 (T = 3) 93.79 99.42 Table 3: Performance change depending on the context window size 3 2.5 2 Seconds 1.5 1 0.5 0 0 10 20 30 40 50 60 70 80 90 100 Sentence Length (Number of Morphemes) Figure 4: Running time on the test set. We used a PC (Intel Xeon 2.33 GHz with 8GB memory on FreeBSD 6.3). in the Kyoto University Corpus. And then we received results from the parser of (Sassano, 2004), which are bunsetsu-based dependency structures</context>
</contexts>
<marker>Sassano, 2004</marker>
<rawString>M. Sassano. 2004. Linear-time dependency analysis for Japanese. In Proc. of COLING 2004, pages 8–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
<author>K Uchimoto</author>
<author>H Isahara</author>
</authors>
<title>Backward beam search algorithm for dependency analysis of Japanese.</title>
<date>2000</date>
<booktitle>In Proc. of COLING-00,</booktitle>
<pages>754--760</pages>
<contexts>
<context position="2211" citStr="Sekine et al., 2000" startWordPosition="344" endWordPosition="347"> Structure In Japanese NLP, it is often assumed that the structure of a sentence is given by dependency relations Sadao Kurohashi Graduate School of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan kuro@i.kyoto-u.ac.jp Meg-ga kare-ni ano pen-wo age-ta. Meg-subj to him that pen-acc give-past. ID 0 1 2 3 4 Head 4 4 3 4 - Figure 1: Sample sentence (bunsetsu-based) among bunsetsus. A bunsetsu is a base phrasal unit and consists of one or more content words followed by zero or more function words. In addition, most of algorithms of Japanese dependency parsing, e.g., (Sekine et al., 2000; Sassano, 2004), assume the three constraints below. (1) Each bunsetsu has only one head except the rightmost one. (2) Dependency links between bunsetsus go from left to right. (3) Dependency links do not cross one another. In other words, dependencies are projective. A sample sentence in Japanese is shown in Figure 1. We can see all the constraints are satisfied. 3 Previous Work As far as we know, there is no dependency parser that does simultaneously both bunsetsu chunking and dependency parsing and, in addition, does them with a single scan. Most of the modern dependency parsers for Japane</context>
</contexts>
<marker>Sekine, Uchimoto, Isahara, 2000</marker>
<rawString>S. Sekine, K. Uchimoto, and H. Isahara. 2000. Backward beam search algorithm for dependency analysis of Japanese. In Proc. of COLING-00, pages 754–760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Uchimoto</author>
<author>S Sekine</author>
<author>H Isahara</author>
</authors>
<title>Japanese dependency structure analysis based on maximum entropy models.</title>
<date>1999</date>
<booktitle>In Proc. of EACL-99,</booktitle>
<pages>196--203</pages>
<contexts>
<context position="7442" citStr="Uchimoto et al., 1999" startWordPosition="1249" endWordPosition="1252">emes. Decision of the relation between two morphemes is made in Dep(), which uses a machine learning-based classifier that supports multiclass prediction. The presented algorithm runs in a left-to-right manner and its upper bound of the time complexity is O(n). Due to space limitation, we do not discuss its complexity here. See (Sassano, 2004) 50 for further details. 5 Experiments and Discussion 5.1 Experimental Set-up Corpus For evaluation, we used the Kyoto University Corpus Version 2 (Kurohashi and Nagao, 1998). The split for training/test/development is the same as in other papers, e.g., (Uchimoto et al., 1999). Selection of a Classifier and its Setting We implemented a parser with the voted perceptron (VP) (Freund and Schapire, 1999). We used a polynomial kernel and set its degree to 3 because cubic kernels proved to be effective empirically for Japanese parsing (Kudo and Matsumoto, 2002). The number of epoch T of VP was selected using the development test set. For multiclass prediction, we used the pairwise method (Kreßel, 1999). Features We have designed rather simple features based on the common feature set (Uchimoto et al., 1999; Kudo and Matsumoto, 2002; Sassano, 2004) for bunsetsu-based parse</context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 1999</marker>
<rawString>K. Uchimoto, S. Sekine, and H. Isahara. 1999. Japanese dependency structure analysis based on maximum entropy models. In Proc. of EACL-99, pages 196–203.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>