<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.9959915">
Automatic Creation of Arabic Named Entity Annotated Corpus Using
Wikipedia
</title>
<author confidence="0.99831">
Maha Althobaiti, Udo Kruschwitz, and Massimo Poesio
</author>
<affiliation confidence="0.919257333333333">
School of Computer Science and Electronic Engineering
University of Essex
Colchester, UK
</affiliation>
<email confidence="0.99368">
{mjaltha, udo, poesio}@essex.ac.uk
</email>
<sectionHeader confidence="0.994612" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.98924885">
In this paper we propose a new methodology to ex-
ploit Wikipedia features and structure to automati-
cally develop an Arabic NE annotated corpus. Each
Wikipedia link is transformed into an NE type of
the target article in order to produce the NE an-
notation. Other Wikipedia features - namely redi-
rects, anchor texts, and inter-language links - are
used to tag additional NEs, which appear without
links in Wikipedia texts. Furthermore, we have de-
veloped a filtering algorithm to eliminate ambiguity
when tagging candidate NEs. Herein we also in-
troduce a mechanism based on the high coverage of
Wikipedia in order to address two challenges partic-
ular to tagging NEs in Arabic text: rich morphology
and the absence of capitalisation. The corpus cre-
ated with our new method (WDC) has been used to
train an NE tagger which has been tested on differ-
ent domains. Judging by the results, an NE tagger
trained on WDC can compete with those trained on
manually annotated corpora.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944333333333">
Supervised learning techniques are well known
for their effectiveness to develop Named Entity
Recognition (NER) taggers (Bikel et al., 1997;
Sekine and others, 1998; McCallum and Li, 2003;
Benajiba et al., 2008). The main disadvantage of
supervised learning is that it requires a large an-
notated corpus. Although a substantial amount
of annotated data is available for some languages,
for other languages, including Arabic, more work
is needed to enrich their linguistic resources. In
fact, changing the domain or just expanding the
set of classes always requires domain-specific ex-
perts and new annotated data, both of which cost
time and effort. Therefore, current research fo-
cuses on approaches that require minimal human
intervention to facilitate the process of moving the
NE classifiers to new domains and to expand NE
classes.
Semi-supervised and unsupervised learning ap-
proaches, along with the automatic creation of
tagged corpora, are alternatives that avoid manu-
ally annotated data (Richman and Schone, 2008;
Althobaiti et al., 2013). The high coverage and
rich informational structure of online encyclope-
dias can be exploited for the automatic creation of
datasets. For example, many researchers have in-
vestigated the use of Wikipedia’s structure to clas-
sify Wikipedia articles and to transform links into
NE annotations according to the link target type
(Nothman et al., 2008; Ringland et al., 2009).
In this paper we present our approach to au-
tomatically derive a large NE annotated corpora
from Arabic Wikipedia. The key to our method
lies in the exploitation of Wikipedia’s concepts,
specifically anchor texts1 and redirects, to handle
the rich morphology in Arabic, and thereby elim-
inate the need to perform any deep morphologi-
cal analysis. In addition, a capitalisation probabil-
ity measure has been introduced and incorporated
into the approach in order to replace the capitalisa-
tion feature that does not exist in the Arabic script.
This capitalisation measure has been utilised in or-
der to filter ambiguous Arabic NE phrases during
annotation process.
The remainder of this paper is structured as fol-
lows: Section 2 illustrates structural information
about Wikipedia. Section 3 includes background
information on NER, including recent work. Sec-
tion 4 summarises the proposed methodology.
Sections 5, 6, and 7 describe the proposed algo-
rithm in detail. The experimental setup and the
evaluation results are reported and discussed in
Section 8. Finally, the conclusion features com-
ments regarding our future work.
</bodyText>
<footnote confidence="0.9957965">
1The terms ‘anchor texts’ and ‘link labels’ are used inter-
changeably in this paper.
</footnote>
<page confidence="0.912895">
106
</page>
<note confidence="0.9973665">
Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 106–115,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.818917" genericHeader="introduction">
2 The Structure of Wikipedia
</sectionHeader>
<bodyText confidence="0.999969666666667">
Wikipedia is a free online encyclopedia project
written collaboratively by thousands of volunteers,
using MediaWiki2. Each article in Wikipedia is
uniquely identified by its title. The title is usually
the most common name for the entity explained
in the article.
</bodyText>
<subsectionHeader confidence="0.988849">
2.1 Types of Wikipedia Pages
2.1.1 Content Pages
</subsectionHeader>
<bodyText confidence="0.999710333333333">
Content pages (aka Wikipedia articles) contain the
majority of Wikipedia’s informative content. Each
content page describes a single topic and has a
unique title. In addition to the text describing the
topic of the article, content pages may contain ta-
bles, images, links and templates.
</bodyText>
<subsubsectionHeader confidence="0.48144">
2.1.2 Redirect Pages
</subsubsectionHeader>
<bodyText confidence="0.999560666666667">
A redirect page is used if there are two or more
alternative names that can refer to one entity
in Wikipedia. Thus, each alternative name is
changed into a title whose article contains a redi-
rect link to the actual article for that entity. For ex-
ample, ‘UK’ is an alternative name for the ‘United
Kingdom’, and consequently, the article with the
title ‘UK’ is just a pointer to the article with the
title ‘United Kingdom’.
</bodyText>
<subsectionHeader confidence="0.992426">
2.1.3 List of Pages
</subsectionHeader>
<bodyText confidence="0.999979428571429">
Wikipedia offers several ways to group articles.
One method is to group articles by lists. The items
on these lists include links to articles in a particu-
lar subject area, and may include additional infor-
mation about the listed items. For example, ‘list
of scientists’ contains links to articles of scientists
and also links to more specific lists of scientists.
</bodyText>
<subsectionHeader confidence="0.9987865">
2.2 The Structure of Wikipedia Articles
2.2.1 Categories
</subsectionHeader>
<bodyText confidence="0.998963">
Every article in the Wikipedia collection should
have at least one category. Categories should be
on vital topics that are useful to the reader. For
example, the Wikipedia article about the United
Kingdom in Wikipedia is associated with a set of
categories that includes ‘Countries bordering the
Atlantic Ocean’, and ‘Countries in Europe’.
</bodyText>
<footnote confidence="0.747504">
2An open source wiki package written in PHP
</footnote>
<subsectionHeader confidence="0.478667">
2.2.2 Infobox
</subsectionHeader>
<bodyText confidence="0.999942166666667">
An infobox is a fixed-format table added to the
top right-hand or left-hand corner of articles to
provide a summary of some unifying parameters
shared by the articles. For instance, every scientist
has a name, date of birth, birthplace, nationality,
and field of study.
</bodyText>
<subsectionHeader confidence="0.998925">
2.3 Links
</subsectionHeader>
<bodyText confidence="0.999279533333333">
A link is a method used by Wikipedia to link pages
within wiki environments. Links are enclosed in
doubled square brackets. A vertical bar, the ‘pipe’
symbol, is used to create a link while labelling it
with a different name on the current page. Look at
the following two examples,
1 - [[a]] is labelled ‘a’ on the current page and
links to taget page ‘a’.
2 - [[alb]] is labelled ‘b’ on the current page, but
links to target page ‘a’.
In the second example, the anchor text (aka link
label) is ‘a’, while ‘b’, a link target, refers to the
title of the target article. In the first example, the
anchor text shown on the page and the title of the
target article are the same.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.99577724">
Current NE research seeks out adequate alter-
natives to traditional techniques such that they
require minimal human intervention and solve
deficiencies of traditional methods. Specific
deficiencies include the limited number of NE
classes resulting from the high cost of setting up
corpora, and the difficulty of adapting the system
to new domains.
One of these trends is distant learning, which
depends on the recruitment of external knowledge
to increase the performance of the classifier, or
to automatically create new resources used in the
learning stage.
Kazama and Torisawa (2007) exploited
Wikipedia-based features to improve their NE
machine learning recogniser’s F-score by three
percent. Their method retrieved the corresponding
Wikipedia entry for each candidate word sequence
in the CoNLL 2003 dataset and extracted a cate-
gory label from the first sentence of the entry.
The automatic creation of training data has
also been investigated using external knowledge.
An et al. (2003) extracted sentences containing
listed entities from the web, and produced a
1.8 million Korean word dataset. Their corpus
</bodyText>
<page confidence="0.99818">
107
</page>
<bodyText confidence="0.999943428571429">
performed as well as manually annotated training
data. Nothman et al. (2008) exploited Wikipedia
to create a massive corpus of named entity
annotated text. They transformed Wikipedia’s
links into named entity annotations by classifying
the target articles into standard entity types3.
Compared to MUC, CoNLL, and BBN corpora,
their Wikipedia-derived corpora tend to perform
better than other cross-corpus train/test pairs.
Nothman et al. (2013) automatically created
massive, multilingual training annotations for
named entity recognition by exploiting the text
and internal structure of Wikipedia. They first
categorised each Wikipedia article into named
entity types, training and evaluating on 7,200
manually-labelled Wikipedia articles across nine
languages: English, German, French, Italian,
Polish, Spanish, Dutch, Portuguese, and Russian.
Their cross-lingual approach achieved up to 95%
accuracy. They transformed Wikipedia’s links
into named entity annotations by classifying the
target articles into standard entity types. This
technique produced reasonable annotations, but
was not immediately able to compete with exist-
ing gold-standard data. They better aligned their
automatic annotations to the gold standard corpus
by deducing additional links and heuristically
tweaking the Wikipedia corpora. Following this
approach, millions of words in nine languages
were annotated. Wikipedia-trained models were
evaluated against CONLL shared task data and
other gold-standard corpora. Their method out-
performed Richman and Schone (2008) and Mika
et al. (2008), and achieved scores 10% higher
than models trained on newswire when tested on
manually annotated Wikipedia text.
Alotaibi and Lee (2013) automatically de-
veloped two NE-annotated sets from Arabic
Wikipedia. The corpora were built using the
mechanism that transforms links into NE an-
notations, by classifying the target articles into
named entity types. They used POS-tagging,
morphological analysis, and linked NE phrases to
detect other mentions of NEs that appear without
links in text. By contrast, our method does not
require POS-tagging or morphological analysis
and just identifies unlinked NEs by matching
phrases from an automatically constructed and
filtered alternative names with identical terms in
</bodyText>
<footnote confidence="0.9424245">
3The terms ‘type’, ‘class’ and ‘category’ are used inter-
changeably in this paper.
</footnote>
<bodyText confidence="0.999353428571429">
the articles texts, see Section 6. The first dataset
created by Alotaibi and Lee (2013) is called
WikiFANE(whole) and contains all sentences
retrieved from the articles. The second set, which
is called WikiFANE(selective), is constructed by
selecting only the sentences that have at least one
named entity phrase.
</bodyText>
<sectionHeader confidence="0.784309" genericHeader="method">
4 Summary of the Approach
</sectionHeader>
<bodyText confidence="0.998869125">
All of our experiments were conducted on the
26 March 2013 Arabic version of the Wikipedia
dump4. A parser was created to handle the medi-
awiki markup and to extract structural information
from the Wikipedia dump such as a list of redirect
pages along with their target articles, a list of pairs
containing link labels and their target articles in
the form ’anchor text, target article’, and essential
information for each article (e.g., title, body text,
categories, and templates).
Many of Wikipedia’s concepts such as links, an-
chor texts, redirects, and inter-language links have
been exploited to transform Wikipedia into a NE
annotated corpus. More details can be found in
the next sections. Generally, the following steps
are necessary to develop the dataset:
</bodyText>
<listItem confidence="0.995433230769231">
1. Classify Wikipedia articles into a specific set
of NE types.
2. Identify matching text in the title and the first
sentence of each article and label the match-
ing phrases according to the article type.
3. Label linked phrases in the text according to
the NE type of the target article.
4. Compile a list of alternative titles for articles
and filter out ambiguous ones.
5. Identify matching phrases in the list and the
Wikipedia text.
6. Filter sentences to prevent noisy sentences
being included in the corpus.
</listItem>
<bodyText confidence="0.997515">
We explain each step in turn in the following sec-
tions.
</bodyText>
<sectionHeader confidence="0.9746965" genericHeader="method">
5 Classifying Wikipedia Articles into NE
Categories
</sectionHeader>
<bodyText confidence="0.999742">
Categorising Wikipedia articles is the initial step
in producing NE training data. Therefore, all
Wikipedia articles need to be classified into a
specific set of named entity types.
</bodyText>
<footnote confidence="0.970786">
4http://dumps.wikimedia.org/arwiki/
</footnote>
<page confidence="0.993224">
108
</page>
<subsectionHeader confidence="0.997104">
5.1 The Dataset and Annotation
</subsectionHeader>
<bodyText confidence="0.999626571428571">
In order to develop a Wikipedia document clas-
sifier, we used a set of 4,000 manually classi-
fied Wikipedia articles that are available free on-
line5. The set was manually classified using the
ACE (2008) taxonomy and a new class (Product).
Therefore, there were eight coarse-grained cate-
gories in total: Facility, Geo-Political, Location,
Organisation, Person, Vehicle, Weapon, and Prod-
uct. As our work adheres to the CoNLL definition,
we mapped these classified Wikipedia articles into
CoNLL NE types – namely person, location, or-
ganisation, miscellaneous, or other – based on the
CoNLL 2003 annotation guidelines (Chinchor et
al., 1999).
</bodyText>
<subsectionHeader confidence="0.994564">
5.2 The Classification of Wikipedia Articles
</subsectionHeader>
<bodyText confidence="0.988278454545455">
Many researchers have already addressed the task
of classifying Wikipedia articles into named entity
types (Dakka and Cucerzan, 2008; Tardif et al.,
2009). Alotaibi and Lee (2012) is the only study
that has experimented with classifying the Arabic
version of Wikipedia into NE classes. They have
explored the use of Naive Bayes, Multinomial
Naive Bayes, and SVM for classifying Wikipedia
articles, and achieved a F-score ranging from 78%
and 90% using different language-dependent and
independent features.
We conducted three experiments that used a
simple bag-of-words features extracted from dif-
ferent portions of the Wikipedia document and
metadata. We summarise the portions of the doc-
ument included in each experiment below:
Exp1: Experiment 1 involved tokens from the
article title and the entire article body.
Exp2: Rich metadata in Wikipedia proved ef-
fective for the classification of articles (Tardif et
al., 2009; Alotaibi and Lee, 2012). Therefore, in
Experiment 2 we included tokens from categories,
templates – specifically ‘Infobox’ – as well as to-
kens from the article title and first sentence of the
document.
Exp3: Experiment 3 involved the same set of
tokens as experiment 2 except that categories and
infobox features were marked with suffixes to dif-
ferentiate them from tokens extracted from the ar-
ticle body text. This step of distinguishing tokens
based on their location in the document improved
the accuracy of document’s classification (Tardif
et al., 2009; Alotaibi and Lee, 2012).
</bodyText>
<footnote confidence="0.783755">
5www.cs.bham.ac.uk/∼fsa081/
</footnote>
<bodyText confidence="0.999986285714286">
In order to optimise features, we implemented a
filtered version of the bag-of-words article repre-
sentation (e.g., removing punctuation marks and
symbols) to classify the Arabic Wikipedia doc-
uments instead of using a raw dataset (Alotaibi
and Lee, 2012). In addition, the same study
shows the high impact of applying tokenisation6
as opposed to the neutral effect of using stem-
ming. We used the filtered features proposed in
the study of Alotaibi and Lee (2012), which in-
cluded removing punctuation marks, symbols, fil-
tering stop words, and normalising digits. We ex-
tended the features, however, by utilising the to-
kenisation scheme that involves separating con-
junctions, prepositions, and pronouns from each
word.
The feature set has been represented using Term
Frequency-Inverse Document Frequency (TF −
IDF). This representation method is a numeri-
cal statistic that reflects how important a token is
to a document.
</bodyText>
<subsectionHeader confidence="0.91862">
5.3 The Results of Classifying the Wikipedia
Articles
</subsectionHeader>
<bodyText confidence="0.9955918">
As for the learning process, our Wikipedia doc-
uments classifier was trained using Liblinear7.
80% of the 4,000 hand-classified Wikipedia
articles were dedicated to the training stage, while
20% were specified to test the classifier. Table
1 is a comparison of the precision, recall, and
F-measure of the classifiers that resulted from the
three experiments. The Exp3 classifier performed
better than the other classifiers. Therefore, it was
selected to classify all of the Wikipedia articles.
At the end of this stage, we obtained a list of
pairs containing each Wikipedia article and its
NE Type. We stored this list in a database in
preparation for the next stage: developing the
NE-tagged training corpus.
</bodyText>
<tableCaption confidence="0.967596">
Table 1: The results of the three Wikipedia docu-
ment classifiers.
</tableCaption>
<footnote confidence="0.9999775">
6It is also called decliticization or segmentation.
7www.csie.ntu.edu.tw/∼cjlin/liblinear/
</footnote>
<page confidence="0.997709">
109
</page>
<sectionHeader confidence="0.994439" genericHeader="method">
6 The Annotation Process
</sectionHeader>
<subsectionHeader confidence="0.9753945">
6.1 Utilising the Titles of Articles and Link
Targets
</subsectionHeader>
<bodyText confidence="0.9991633">
Identifying corresponding words in the article ti-
tle and the entire body of text and then tagging the
matching phrases with the NE-type can be a risky
process, especially for terms with more than one
meaning. For example, the title of the article de-
scribing the city ( ,)A¿, ‘Cannes’)8 can also, in Ara-
bic, refer to the past verb ( ,)A¿, ‘was’). The portion
of the Wikipedia article unlikely to produce errors
during the matching process is the first sentence,
which usually contains the definition of the term
the Wikipedia article is written about (Zesch et al.,
2007).
When identifying matching terms in the arti-
cle title and the first sentence, we found that ar-
ticle titles often contain abbreviations, while the
first sentence spells out entire words. This pat-
tern makes it difficult to identify matching terms
in the title and first sentence, and frequently ap-
pears in biographical Wikipedia articles. For ex-
ample, one article is entitled (ø� �@QË@ QºK. ñK.@, ‘Abu
Bakr Al-Razi’), but the first sentence states the full
name of the person: (ø� j@QË@ AK�Q» P �áK.
úæ�m� � áK. YÒm× QºK. ñK.@,
‘Abu Bakr Mohammad Bin Yahia Bin Zakaria Al-
Razi’). Therefore, we decided to address the prob-
lem with partial matching. In this case, the sys-
tem should first identify all corresponding words
in the title and the first sentence. Second, the sys-
tem should annotate them and all words that fall
between, provided that:
</bodyText>
<listItem confidence="0.9905241">
• the sequence of the words in the article title
and the text are the same in order to avoid
errors in tagging. For example, if the title of
the article is (;ÖßdJ@Qî�E, ‘The River Thames’),
but the first sentence reads (.... ú�¯� ©�®K� Qî ñë fA�JË@
, ‘The Thames is a river flowing through
southern England....’), then the text will not
be properly tagged.
• the number of tokens located between
matched tokens is less than or equal to five9.
</listItem>
<figureCaption confidence="0.888478">
Figure 1 shows one example of partial matching.
</figureCaption>
<footnote confidence="0.977699">
8Throughout the entire paper, Arabic words are repre-
sented as follows: ( Arabic word,‘English translation’).
9An informal experiment showed that the longest proper
Arabic names are 5 to 7 tokens in length.
</footnote>
<figureCaption confidence="0.999949">
Figure 1: Example of Partial Matching
</figureCaption>
<bodyText confidence="0.9988364">
The next step is to transform the links be-
tween Wikipedia articles into NE annotations ac-
cording to the link target type. Therefore, the
link ([[AÓAK.ð@ 1/4@PAK.|AÓAK.ð@]]/[[Barack Obama|Obama]])
would be changed to (AÓAK.ð@ PER) (Obama PER),
since the link target (Barack Obama) is the title of
an article about person. By the end of this stage,
all NE anchor texts (anchor texts referring to NE
articles) on Wikipedia should be annotated based
on the NE-type of the target article.
</bodyText>
<subsectionHeader confidence="0.999627">
6.2 Dictionaries of Alternative Names
</subsectionHeader>
<bodyText confidence="0.999975391304348">
Depending only on NE anchor texts in order to
derive and annotate data from Wikipedia results
in a low-quality dataset, as Wikipedia contains
a fair amount of NEs mentioned without links.
This can be attributed to the fact that each term
on Wikipedia is more likely to be linked only
on its first appearance in the article (Nothman et
al., 2008). These unlinked NE phrases can be
found simply by identifying the matching terms
in the list of linked NE phrases10 and the text.
The process is not as straightforward as it seems,
however, because identifying corresponding terms
may prove ineffective, especially in the case of
morphologically rich language in which unlinked
NE phrases are sometimes found agglutinated to
prefixes and conjunctions. In order to detect un-
linked and inflected forms of NEs in Wikipedia
text, we extended the list of articles titles that were
used in the previous step to find and match the pos-
sible NEs in the text by including NE anchor texts.
Adding NE anchor texts to the list assists in find-
ing possible morphologically inflected NEs in the
text while eliminating the need for any morpho-
</bodyText>
<footnote confidence="0.708807">
10The list of anchor texts that refer to NE articles
</footnote>
<page confidence="0.995764">
110
</page>
<bodyText confidence="0.6506555">
logical analysis. Table 2 shows examples from the
dictionary of NE anchor texts.
</bodyText>
<tableCaption confidence="0.7013865">
Table 2: Examples from the dictionary of NE An-
chor Texts.
</tableCaption>
<bodyText confidence="0.996799857142857">
Spelling variations resulting from varied
transliteration of foreign named entities in some
cases prevent the accurate matching and identifi-
cation of some unlinked NEs, if only the list of
NE anchor texts is used. For example, (@Qam.�� @, ‘Eng-
land’) has been written five different ways: (èQ��Êm.�� @,
@Q;Ê�ª�K@, èQ;Ê�ª�K@, @Q��Ê3/4�K@, èQ��Ê3/4�K@). Therefore, we compiled
a list of the titles of redirected pages that send
the reader to articles describing NEs. We refer
to these titles in this paper as NE redirects. We
consider to the lists of NE redirects and anchor
texts a list of alternative names, since they can be
used as alternative names for article titles.
The list of alternative names is used to find
unlinked NEs in the text by matching phrases
from the list with identical terms in the articles
texts. This list is essential for managing spelling
and morphological variations of unlinked NEs, as
well as misspelling. Consequently, the process
increases the coverage of NE tags augmented
within the plain texts of Wikipedia articles.
</bodyText>
<subsectionHeader confidence="0.8970805">
6.2.1 Filtering the Dictionaries of Alternative
Names
</subsectionHeader>
<bodyText confidence="0.986087842105263">
One-word alternative names: Identifying
matching phrases in the list of alternative names
and the text inevitably results in a lower quality
corpus due to noisy names. The noisy alternative
names usually occur with meaningful named
entities. For example, the article on the person
( &amp;~
ÓB@ éÊË@YJ.« ñK.@, ‘Abu Abdullah Alamyn’) has an
alternative name consisting only of his last
name ( c,�ÓB@, ‘Alameen’), which means ‘custo-
dian’. Therefore, annotating every occurrence of
‘Alamyn’ as PER would lead to incorrect tagging
and ambiguity. The same applies to the city with
the name (èYK�Ym.Ì @, ‘Aljadydah’), which literally
means ‘new’. Thus, the list of alternative names
should be filtered to omit one-word NE phrases
that usually have a meaning and are ambiguous
when taken out of context.
In order to solve this problem, we introduced
a capitalisation probability measure for Arabic
words, which are never capitalised. This involved
finding the English gloss for each one-word alter-
native name and then computing its probability
of being capitalised using the English Wikipedia.
To find the English gloss for Arabic words, we
exploited Wikipedia Arabic-to-English cross-
lingual links that provided us with a reasonable
number of Arabic and corresponding English
terms. If the English gloss for the Arabic word
could not be found using inter-language links, we
resorted to an online translator. Before translating
the Arabic word, a light stemmer was used to
remove prefixes and conjunctions in order to
get the translation of the word itself without its
associated affixes. Otherwise, the Arabic word
(XCJ.ÊË) would be translated as (in the country).
The capitalisation probability was computed as
follows:
</bodyText>
<equation confidence="0.955438666666667">
f(EN)isCapitalised
Pr[EN] =
f(EN)isCapitalised+f(EN)notCapitalised
</equation>
<bodyText confidence="0.997390066666667">
where: EN is the English gloss of the alter-
native name; f(EN)isCapitalised is the number
of times the English gloss EN is capitalised in
English Wikipedia; and f(EN)notCapitalised is
the number of times the English gloss EN is not
capitalised in English Wikipedia.
This way, we managed to build a list of Arabic
words and their probabilities of being capitalised.
It is evident that the meaningful one-word NEs
usually achieve a low probability. By specifying
a capitalisation threshold constraint, we prevented
such words from being included in the list of
alternative names. After a set of experiments, we
decided to use the capitalisation threshold equal
to 0.75.
</bodyText>
<subsectionHeader confidence="0.509313">
Multi-word alternative names: Multi-word
</subsectionHeader>
<bodyText confidence="0.9984251">
alternative names (e.g., XñÒm× ùj’Ó /‘MusTafae
Mahmud’), ÈXA« YÔg@ /‘Ahmad Adel’) rarely cause
errors in the automatic annotation process.
Wikipedians, however, at times append personal
and job titles to the person’s name contained in
the anchor text, which refers to the article about
that person. Examples of such anchor texts are
(Y �ƒ@P áK. YÒm× ú�G.X Õ»Ag,‘Ruler of Dubai Muhammad
bin Rashid’) and (Y °ƒ@P �áK. YÒm× Z@P JñË@ •Êm.×•���KP, ‘Presi-
dent of the Council of Ministers Muhammad bin
</bodyText>
<page confidence="0.997609">
111
</page>
<bodyText confidence="0.998488">
Rashid’). As a result, the system will mistakenly
annotate words like Dubai, Council, Ministers
as PER. Our solution to this problem is to omit
the multi-word alternative name, if any of its
words belong to the list of apposition words,
which usually appear adjacent to NEs such as
(������, ‘President’), (� ��, ‘Minister’), and (e,
‘Ruler’). The filtering algorithm managed to
exclude 22.95% of the alternative names from the
original list. Algorithm 1 shows pseudo code of
the filtering algorithm.
</bodyText>
<figure confidence="0.935085421052631">
Algorithm 1: Filtering Alternative Names
Input: A set L = {l1, l2, ... , lnI of all alternative
names of Wikipedia articles
Output: A set RL = {rl1, rl2, ... , rlnI of reliable
alternative names
1 for i +— 1 to n do
T +— split li into tokens
if (T.size() &gt;= 2) then
/* All tokens of T do not
belong to apposition list
*/
if (! containAppositiveWord(T)) then
add li to the set RL
else
lightstem +— findLightStem(li)
englishgloss +— translate(lightstem)
/* Compute Capitalisation
Probability for English
gloss */
</figure>
<bodyText confidence="0.959432535714286">
the Spanish corpus consider them NOT named en-
tities (Nothman et al., 2013). As far as we know,
almost all Arabic NER datasets that followed the
CoNLL style and guidelines in the annotation pro-
cess consider nationalities NOT named entities.
On Wikipedia all nationalities are linked to ar-
ticles about the corresponding countries, which
makes the annotation tool tag them as LOC. We
decided to consider them NOT named entities in
accordance with the CoNLL-style Arabic datasets.
Therefore, in order to resolve this issue, we com-
piled a list of nationalities, and other adjectival
forms of religion and ethnic groups, so that any
anchor text matching an entry in the list was re-
tagged as a NOT named entity.
The list of nationalities and apposition words
used in section 6.2.1 were compiled by exploiting
the ‘List of’ articles in Wikipedia such as list of
people by nationality, list of ethnic groups, list of
adjectival forms of place names, and list of titles.
Some English versions of these ‘List of’ pages
have been translated into Arabic, either because
they are more comprehensive than the Arabic ver-
sion, or because there is no corresponding page in
Arabic.
capprob +— compCapProb(englishgloss)
if (capprob &gt; 0.75) then
add li to the set RL
</bodyText>
<sectionHeader confidence="0.778577" genericHeader="method">
7 Building the Corpus
</sectionHeader>
<bodyText confidence="0.999833333333333">
The dictionaries derived from Wikipedia by
exploiting Wikipedia’s structure and adopting the
filtering algorithm is shown in Table 3.
</bodyText>
<tableCaption confidence="0.994051">
Table 3: Dictionaries derived from Wikipedia.
</tableCaption>
<subsectionHeader confidence="0.999831">
6.3 Post-processing
</subsectionHeader>
<bodyText confidence="0.999072">
The goal of Post-processing was to address some
issues that arose during the annotation process as
a result of different domains, genres, and con-
ventions of entity types. For example, national-
ities and other adjectival forms of nations, reli-
gions, and ethnic groups are considered MISC in
the CoNLL NER task in the English corpus, while
After the annotation process, the last step was
to incorporate sentences into the corpus. This
resulted in obtaining an annotated dataset with
around ten million tokens. However, in order to
obtain a corpus with a large number of tags with-
out affecting its quality, we created a dataset called
Wikipedia-derived corpus (WDC), which included
only sentences with at least three annotated named
entity tokens. The WDC dataset contains 165,119
sentences consisting of around 6 million tokens.
The annotation style of the WDC dataset followed
the CoNLL format, where each token and its tag
are placed together in the same file in the form
&lt; token &gt; \s &lt; tag &gt;. The NE boundary
is specified using the BIO representation scheme,
where B- indicates the beginning of the NE, I-
refers to the continuation (Inside) of the NE, and
O indicates that the word is not a NE. The WDC
dataset is available online to the community of re-
searchers11
</bodyText>
<footnote confidence="0.504286">
11https://www.dropbox.com/sh/27afkiqvlpwyfq0/1hwWGqAcTL
</footnote>
<page confidence="0.504644">
2
</page>
<figure confidence="0.961187">
3
4
5
6
7
8
9
10
11
</figure>
<page confidence="0.99312">
112
</page>
<sectionHeader confidence="0.987489" genericHeader="method">
8 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9999765">
To evaluate the quality of the methodology, we
used WDC as training data to build an NER model.
Then we tested the resulting classifier on datasets
from different domains.
</bodyText>
<subsectionHeader confidence="0.982349">
8.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999937833333333">
For the evaluation purposes, we used three
datasets: ANERcorp, NEWS, and TWEETS.
ANERcorp is a news-wire domain dataset built
and tagged especially for the NER task by Bena-
jiba et al. (2007). It contains around 150k tokens
and is available for free. We tested our method-
ology on the ANERcorp test corpus because it is
widely used in the literature for comparing with
existing systems. The NEWS dataset is also a
news-wire domain dataset collected by Darwish
(2013) from the RSS feed of the Arabic version
of news.google.com from October 2012. The
RSS consists of the headline and the first 50 to
100 words in the news articles. This set contains
approximately 15k tokens. The third test set was
extracted randomly from Twitter and contains a
set of 1,423 tweets authored in November 2011.
It has approximately 26k tokens (Darwish, 2013).
</bodyText>
<subsectionHeader confidence="0.998421">
8.2 Our Supervised Classifier
</subsectionHeader>
<bodyText confidence="0.999957857142857">
All experiments to train and build a probabilistic
classifier were conducted using Conditional Ran-
dom Fields (CRF)12. Regarding the features used
in all our experiments, we selected the most suc-
cessful features from Arabic NER work (Benajiba
et al., 2008; Abdul-Hamid and Darwish, 2010;
Darwish, 2013). These features include:
</bodyText>
<listItem confidence="0.9990035">
• The words immediately before and after the
current word in their raw and stemmed forms.
• The first 1, 2, 3, 4 characters in a word.
• The last 1, 2, 3, 4 characters in a word.
• The appearance of the word in the gazetteer.
• The stemmed form of the word.
</listItem>
<bodyText confidence="0.997423">
The gazetteer used contains around 5,000 entries
and was developed by Benajiba et al. (2008). A
light stemmer was used to determine the stem
form of the word by using simple rules to re-
move conjunctions, prepositions, and definite ar-
ticles (Larkey et al., 2002).
</bodyText>
<footnote confidence="0.764966">
12http://www.chokkan.org/software/crfsuite/
</footnote>
<subsectionHeader confidence="0.7523225">
8.3 Training the Supervised Classifier on
Manually-annotated Data
</subsectionHeader>
<bodyText confidence="0.999685222222222">
The supervised classifier in Section 8.2 was
trained on the ANERcorp training set. We refer to
the resulting model as the ANERcorp-Model. Ta-
ble 4 shows the results of the ANERcorp-Model
on the ANERcorp test set. The table also shows
the results of the state-of-the-art supervised clas-
sifier ‘ANERcorp-Model(SoA)’ developed by Dar-
wish (2013) when trained and tested on the same
datasets used for ANERcorp-Model.
</bodyText>
<tableCaption confidence="0.96252">
Table 4: The results of Supervised Classifiers.
</tableCaption>
<sectionHeader confidence="0.884085" genericHeader="evaluation">
8.4 Results
</sectionHeader>
<bodyText confidence="0.997661192307692">
We compared a system trained on WDC with
the systems trained by Alotaibi and Lee (2013)
on two datasets, WikiFANE(whole) and Wiki-
FANE(selective), which are also automatically col-
lected from Arabic Wikipedia. The evaluation pro-
cess was conducted by testing them on the AN-
ERcorp set. The results shown in Table 5 prove
that the methodology we proposed in this paper
produces a dataset that outperforms the two other
datasets in terms of recall and F-measure.
Table 5: Comparison of the system trained on
WDC dataset with the systems trained on Wiki-
FANE datasets.
Table 6 compares the results of the ANERcorp-
Model and the WDC-Model when testing them on
datasets from different domains. Firstly, We de-
cided to test the ANERcorp-Model and the WDC-
Model on Wikipedia. Thus, a subset, contain-
ing around 14k tokens, of WDC set was allocated
for testing purpose. The results in Table 6 shows
that WDC classifier outperforms the F-score of the
news-based classifier by around 48%.The obvi-
ous difference in the performance of the two clas-
sifiers can be attributed to the difference in an-
notation convention for different domains. For
example, many key words in Arabic Wikipedia,
</bodyText>
<page confidence="0.99699">
113
</page>
<bodyText confidence="0.98922406557377">
which appear in the text along with NEs (e.g.,
éªÓAg. /university, �é�JK�YÓ/ city, �é»Qå�.../company), are usu-
�
ally considered part of NE names. So, the phrase
‘Shizuoka Prefecture’ that is mentioned in some
Arabic Wikipedia articles is considered an entity
and linked to an article that talks about Shizuoka,
making the system annotate all words in the phrase
as NEs as follows: (; 6�¯Am× B-LOC A¿ð _;K� ƒ- I-LOC/
Shizuoka B-LOC Prefecture I-LOC). On the other
hand, in ANERcorp corpus, only the the word af-
ter the keyword (v�Bð, ‘Prefecture’) is considered
NE. In addition, although sport facilities (e.g., sta-
diums) are categorized in Wikipedia as location,
some of them are not even considered entities in
ANERcorp test corpus.
Secondly, the ANERcorp-Model and the WDC-
Model were tested on the ANERcorp test data.
The point of this comparison is to show how well
the WDC dataset works on a news-wire domain,
which is more specific than Wikipedia’s open do-
main. The table shows that the ANERcorp-model
outperforms the F-score of the WDC-Model by
around 13 points. However, in addition to the fact
that training and test datasets for the ANERcorp-
Model are drawn from the same domain, 69% of
NEs in the test data were seen in the training set
(Darwish, 2013).
Thirdly, the ANERcorp-Model and the WDC-
Model were tested on NEWS corpus, which is also
a news-wire based dataset. The results from Ta-
ble 6 reveal the quality of the WDC dataset on the
NEWS corpus. The WDC-Model achieves rela-
tively similar results to the ANERcorp-Model, al-
though the latter has the advantage of being trained
on a manually annotated corpus extracted from the
similar domain of the NEWS test set.
Finally, testing the ANERcorp-Model and the
WDC-Model on data extracted from a social net-
works like Twitter proves that models trained on
open-domain datasets like Wikipedia perform bet-
ter on social network text than classifiers trained
on domain-specific datasets, as shown in Table 6.
In order to show the effect of combining our
corpus (WDC) with a manually annotated dataset
from a different domain, we merged WDC with the
ANERcorp dataset. Table 7 shows the results of a
system trained on the combined corpus when test-
ing it on three test sets. The system trained on the
combined corpus achieves results that fall between
the results of the systems trained on each corpus
separately when testing them on the ANERcorp
Table 6: The F-scores of ANERcorp-Model and
WDC-Model on ANERcorp, NEWS, &amp; TWEETS
datasets.
test set and NEWS test set. On the other hand,
the results of the system trained on the combined
corpus when tested on the third test set (TWEETS)
show no significant improvement.
Table 7: The results of combining WDC with AN-
ERcorp dataset.
</bodyText>
<sectionHeader confidence="0.976448" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999919375">
We have presented a methodology that requires
minimal time and human intervention to gener-
ate an NE-annotated corpus from Wikipedia. The
evaluation results showed the high quality of the
developed corpus WDC, which contains around
6 million tokens representing different genres, as
Wikipedia is considered an open domain. Further-
more, WDC outperforms other NE corpora gen-
erated automatically from Arabic Wikipedia by 8
to 12 points in terms of F-measure. Our methodol-
ogy can easily be adapted to extend to new classes.
Therefore, in future we intend to experiment with
finer-grained NE hierarchies. In addition, we plan
to carry out some domain adaptation experiments
to handle the difference in annotation convention
for different domains.
</bodyText>
<sectionHeader confidence="0.989928" genericHeader="references">
References
</sectionHeader>
<footnote confidence="0.4866575">
Ahmed Abdul-Hamid and Kareem Darwish. 2010.
Simplified feature set for Arabic named entity recog-
</footnote>
<page confidence="0.994316">
114
</page>
<reference confidence="0.99992141">
nition. In Proceedings of the 2010 Named Entities
Workshop, pages 110–115. Association for Compu-
tational Linguistics.
Fahd Alotaibi and Mark Lee. 2012. Mapping Arabic
Wikipedia into the Named Entities Taxonomy. In
COLING (Posters), pages 43–52.
Fahd Alotaibi and Mark Lee. 2013. Automatically De-
veloping a Fine-grained Arabic Named Entity Cor-
pus and Gazetteer by utilizing Wikipedia. In IJC-
NLP.
Maha Althobaiti, Udo Kruschwitz, and Massimo Poe-
sio. 2013. A Semi-supervised Learning Approach
to Arabic Named Entity Recognition. In RANLP,
pages 32–40.
Joohui An, Seungwoo Lee, and Gary Geunbae Lee.
2003. Automatic acquisition of named entity tagged
corpus from world wide web. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 2, pages 165–168. Asso-
ciation for Computational Linguistics.
Yassine Benajiba, Paolo Rosso, and Jos´e Miguel
Benediruiz. 2007. Anersys: An Arabic Named
Entity Recognition System based on Maximum En-
tropy. In Computational Linguistics and Intelligent
Text Processing, pages 143–153. Springer.
Yassine Benajiba, Mona Diab, and Paolo Rosso. 2008.
Arabic Named Entity Recognition using optimized
feature sets. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 284–293. Association for Computational
Linguistics.
Daniel M Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings
of the fifth conference on Applied natural language
processing, pages 194–201. Association for Compu-
tational Linguistics.
Nancy Chinchor, Erica Brown, Lisa Ferro, and Patty
Robinson. 1999. 1999 Named Entity Recognition
Task Definition. MITRE and SAIC.
Wisam Dakka and Silviu Cucerzan. 2008. Augment-
ing Wikipedia with Named Entity Tags. In IJCNLP,
pages 545–552.
Kareem Darwish. 2013. Named Entity Recognition
using Cross-lingual Resources: Arabic as an Exam-
ple. In ACL.
Junichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as external knowledge for named en-
tity recognition. In Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 698–
707.
L.S. Larkey, L. Ballesteros, and M.E. Connell. 2002.
Improving stemming for Arabic information re-
trieval: light stemming and co-occurrence analysis.
In Annual ACM Conference on Research and De-
velopment in Information Retrieval: Proceedings of
the 25 th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, volume 11, pages 275–282.
Andrew McCallum and Wei Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 188–191. Association for Computational Lin-
guistics.
Peter Mika, Massimiliano Ciaramita, Hugo Zaragoza,
and Jordi Atserias. 2008. Learning to Tag and Tag-
ging to Learn: A Case Study on Wikipedia. vol-
ume 23, pages 26–33.
Joel Nothman, James R Curran, and Tara Murphy.
2008. Transforming Wikipedia into Named Entity
training data. In Proceedings of the Australian Lan-
guage Technology Workshop, pages 124–132.
Joel Nothman, Nicky Ringland, Will Radford, Tara
Murphy, and James R Curran. 2013. Learn-
ing multilingual Named Entity Recognition from
Wikipedia. Artificial Intelligence, 194:151–175.
Alexander E Richman and Patrick Schone. 2008. Min-
ing Wiki Resources for Multilingual Named Entity
Recognition. In ACL, pages 1–9.
Nicky Ringland, Joel Nothman, Tara Murphy, and
James R Curran. 2009. Classifying articles in
English and German Wikipedia. In Australasian
Language Technology Association Workshop 2009,
page 20.
Satoshi Sekine et al. 1998. NYU: Description of the
Japanese NE system used for MET-2. In Proc. of the
Seventh Message Understanding Conference (MUC-
7), volume 17.
Sam Tardif, James R. Curran, and Tara Murphy.
2009. Improved Text Categorisation for Wikipedia
Named Entities. In Proceedings of the Australasian
Language Technology Association Workshop, pages
104–108.
Torsten Zesch, Iryna Gurevych, and Max M¨uhlh¨auser.
2007. Analyzing and accessing Wikipedia as a lex-
ical semantic resource. pages 197–205. Tuebingen,
Germany: Gunter Narr, T¨ubingen.
</reference>
<page confidence="0.999027">
115
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.574352">
<title confidence="0.9448055">Automatic Creation of Arabic Named Entity Annotated Corpus Using Wikipedia</title>
<author confidence="0.995483">Maha Althobaiti</author>
<author confidence="0.995483">Udo Kruschwitz</author>
<author confidence="0.995483">Massimo</author>
<affiliation confidence="0.920091666666667">School of Computer Science and Electronic University of Colchester,</affiliation>
<email confidence="0.895542">udo,</email>
<abstract confidence="0.998700523809524">In this paper we propose a new methodology to exploit Wikipedia features and structure to automatically develop an Arabic NE annotated corpus. Each Wikipedia link is transformed into an NE type of the target article in order to produce the NE annotation. Other Wikipedia features namely redirects, anchor texts, and inter-language links are used to tag additional NEs, which appear without links in Wikipedia texts. Furthermore, we have developed a filtering algorithm to eliminate ambiguity when tagging candidate NEs. Herein we also introduce a mechanism based on the high coverage of Wikipedia in order to address two challenges particular to tagging NEs in Arabic text: rich morphology and the absence of capitalisation. The corpus crewith our new method has been used to train an NE tagger which has been tested on different domains. Judging by the results, an NE tagger on compete with those trained on manually annotated corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>nition</author>
</authors>
<booktitle>In Proceedings of the 2010 Named Entities Workshop,</booktitle>
<pages>110--115</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>nition, </marker>
<rawString>nition. In Proceedings of the 2010 Named Entities Workshop, pages 110–115. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fahd Alotaibi</author>
<author>Mark Lee</author>
</authors>
<title>Mapping Arabic Wikipedia into the Named Entities Taxonomy.</title>
<date>2012</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>43--52</pages>
<contexts>
<context position="13305" citStr="Alotaibi and Lee (2012)" startWordPosition="2076" endWordPosition="2079">herefore, there were eight coarse-grained categories in total: Facility, Geo-Political, Location, Organisation, Person, Vehicle, Weapon, and Product. As our work adheres to the CoNLL definition, we mapped these classified Wikipedia articles into CoNLL NE types – namely person, location, organisation, miscellaneous, or other – based on the CoNLL 2003 annotation guidelines (Chinchor et al., 1999). 5.2 The Classification of Wikipedia Articles Many researchers have already addressed the task of classifying Wikipedia articles into named entity types (Dakka and Cucerzan, 2008; Tardif et al., 2009). Alotaibi and Lee (2012) is the only study that has experimented with classifying the Arabic version of Wikipedia into NE classes. They have explored the use of Naive Bayes, Multinomial Naive Bayes, and SVM for classifying Wikipedia articles, and achieved a F-score ranging from 78% and 90% using different language-dependent and independent features. We conducted three experiments that used a simple bag-of-words features extracted from different portions of the Wikipedia document and metadata. We summarise the portions of the document included in each experiment below: Exp1: Experiment 1 involved tokens from the artic</context>
<context position="14632" citStr="Alotaibi and Lee, 2012" startWordPosition="2284" endWordPosition="2287">cation of articles (Tardif et al., 2009; Alotaibi and Lee, 2012). Therefore, in Experiment 2 we included tokens from categories, templates – specifically ‘Infobox’ – as well as tokens from the article title and first sentence of the document. Exp3: Experiment 3 involved the same set of tokens as experiment 2 except that categories and infobox features were marked with suffixes to differentiate them from tokens extracted from the article body text. This step of distinguishing tokens based on their location in the document improved the accuracy of document’s classification (Tardif et al., 2009; Alotaibi and Lee, 2012). 5www.cs.bham.ac.uk/∼fsa081/ In order to optimise features, we implemented a filtered version of the bag-of-words article representation (e.g., removing punctuation marks and symbols) to classify the Arabic Wikipedia documents instead of using a raw dataset (Alotaibi and Lee, 2012). In addition, the same study shows the high impact of applying tokenisation6 as opposed to the neutral effect of using stemming. We used the filtered features proposed in the study of Alotaibi and Lee (2012), which included removing punctuation marks, symbols, filtering stop words, and normalising digits. We extend</context>
</contexts>
<marker>Alotaibi, Lee, 2012</marker>
<rawString>Fahd Alotaibi and Mark Lee. 2012. Mapping Arabic Wikipedia into the Named Entities Taxonomy. In COLING (Posters), pages 43–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fahd Alotaibi</author>
<author>Mark Lee</author>
</authors>
<title>Automatically Developing a Fine-grained Arabic Named Entity Corpus and Gazetteer by utilizing Wikipedia.</title>
<date>2013</date>
<booktitle>In IJCNLP.</booktitle>
<contexts>
<context position="9809" citStr="Alotaibi and Lee (2013)" startWordPosition="1529" endWordPosition="1532"> but was not immediately able to compete with existing gold-standard data. They better aligned their automatic annotations to the gold standard corpus by deducing additional links and heuristically tweaking the Wikipedia corpora. Following this approach, millions of words in nine languages were annotated. Wikipedia-trained models were evaluated against CONLL shared task data and other gold-standard corpora. Their method outperformed Richman and Schone (2008) and Mika et al. (2008), and achieved scores 10% higher than models trained on newswire when tested on manually annotated Wikipedia text. Alotaibi and Lee (2013) automatically developed two NE-annotated sets from Arabic Wikipedia. The corpora were built using the mechanism that transforms links into NE annotations, by classifying the target articles into named entity types. They used POS-tagging, morphological analysis, and linked NE phrases to detect other mentions of NEs that appear without links in text. By contrast, our method does not require POS-tagging or morphological analysis and just identifies unlinked NEs by matching phrases from an automatically constructed and filtered alternative names with identical terms in 3The terms ‘type’, ‘class’ </context>
<context position="31076" citStr="Alotaibi and Lee (2013)" startWordPosition="4966" endWordPosition="4969">.3 Training the Supervised Classifier on Manually-annotated Data The supervised classifier in Section 8.2 was trained on the ANERcorp training set. We refer to the resulting model as the ANERcorp-Model. Table 4 shows the results of the ANERcorp-Model on the ANERcorp test set. The table also shows the results of the state-of-the-art supervised classifier ‘ANERcorp-Model(SoA)’ developed by Darwish (2013) when trained and tested on the same datasets used for ANERcorp-Model. Table 4: The results of Supervised Classifiers. 8.4 Results We compared a system trained on WDC with the systems trained by Alotaibi and Lee (2013) on two datasets, WikiFANE(whole) and WikiFANE(selective), which are also automatically collected from Arabic Wikipedia. The evaluation process was conducted by testing them on the ANERcorp set. The results shown in Table 5 prove that the methodology we proposed in this paper produces a dataset that outperforms the two other datasets in terms of recall and F-measure. Table 5: Comparison of the system trained on WDC dataset with the systems trained on WikiFANE datasets. Table 6 compares the results of the ANERcorpModel and the WDC-Model when testing them on datasets from different domains. Firs</context>
</contexts>
<marker>Alotaibi, Lee, 2013</marker>
<rawString>Fahd Alotaibi and Mark Lee. 2013. Automatically Developing a Fine-grained Arabic Named Entity Corpus and Gazetteer by utilizing Wikipedia. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maha Althobaiti</author>
<author>Udo Kruschwitz</author>
<author>Massimo Poesio</author>
</authors>
<title>A Semi-supervised Learning Approach to Arabic Named Entity Recognition. In</title>
<date>2013</date>
<booktitle>RANLP,</booktitle>
<pages>32--40</pages>
<contexts>
<context position="2287" citStr="Althobaiti et al., 2013" startWordPosition="356" endWordPosition="359">ic, more work is needed to enrich their linguistic resources. In fact, changing the domain or just expanding the set of classes always requires domain-specific experts and new annotated data, both of which cost time and effort. Therefore, current research focuses on approaches that require minimal human intervention to facilitate the process of moving the NE classifiers to new domains and to expand NE classes. Semi-supervised and unsupervised learning approaches, along with the automatic creation of tagged corpora, are alternatives that avoid manually annotated data (Richman and Schone, 2008; Althobaiti et al., 2013). The high coverage and rich informational structure of online encyclopedias can be exploited for the automatic creation of datasets. For example, many researchers have investigated the use of Wikipedia’s structure to classify Wikipedia articles and to transform links into NE annotations according to the link target type (Nothman et al., 2008; Ringland et al., 2009). In this paper we present our approach to automatically derive a large NE annotated corpora from Arabic Wikipedia. The key to our method lies in the exploitation of Wikipedia’s concepts, specifically anchor texts1 and redirects, to</context>
</contexts>
<marker>Althobaiti, Kruschwitz, Poesio, 2013</marker>
<rawString>Maha Althobaiti, Udo Kruschwitz, and Massimo Poesio. 2013. A Semi-supervised Learning Approach to Arabic Named Entity Recognition. In RANLP, pages 32–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joohui An</author>
<author>Seungwoo Lee</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Automatic acquisition of named entity tagged corpus from world wide web.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 2,</booktitle>
<pages>165--168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7980" citStr="An et al. (2003)" startWordPosition="1275" endWordPosition="1278">nt learning, which depends on the recruitment of external knowledge to increase the performance of the classifier, or to automatically create new resources used in the learning stage. Kazama and Torisawa (2007) exploited Wikipedia-based features to improve their NE machine learning recogniser’s F-score by three percent. Their method retrieved the corresponding Wikipedia entry for each candidate word sequence in the CoNLL 2003 dataset and extracted a category label from the first sentence of the entry. The automatic creation of training data has also been investigated using external knowledge. An et al. (2003) extracted sentences containing listed entities from the web, and produced a 1.8 million Korean word dataset. Their corpus 107 performed as well as manually annotated training data. Nothman et al. (2008) exploited Wikipedia to create a massive corpus of named entity annotated text. They transformed Wikipedia’s links into named entity annotations by classifying the target articles into standard entity types3. Compared to MUC, CoNLL, and BBN corpora, their Wikipedia-derived corpora tend to perform better than other cross-corpus train/test pairs. Nothman et al. (2013) automatically created massiv</context>
</contexts>
<marker>An, Lee, Lee, 2003</marker>
<rawString>Joohui An, Seungwoo Lee, and Gary Geunbae Lee. 2003. Automatic acquisition of named entity tagged corpus from world wide web. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 2, pages 165–168. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yassine Benajiba</author>
<author>Paolo Rosso</author>
<author>Jos´e Miguel Benediruiz</author>
</authors>
<title>Anersys: An Arabic Named Entity Recognition System based on Maximum Entropy.</title>
<date>2007</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>143--153</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="28882" citStr="Benajiba et al. (2007)" startWordPosition="4605" endWordPosition="4609">ation (Inside) of the NE, and O indicates that the word is not a NE. The WDC dataset is available online to the community of researchers11 11https://www.dropbox.com/sh/27afkiqvlpwyfq0/1hwWGqAcTL 2 3 4 5 6 7 8 9 10 11 112 8 Experimental Evaluation To evaluate the quality of the methodology, we used WDC as training data to build an NER model. Then we tested the resulting classifier on datasets from different domains. 8.1 Datasets For the evaluation purposes, we used three datasets: ANERcorp, NEWS, and TWEETS. ANERcorp is a news-wire domain dataset built and tagged especially for the NER task by Benajiba et al. (2007). It contains around 150k tokens and is available for free. We tested our methodology on the ANERcorp test corpus because it is widely used in the literature for comparing with existing systems. The NEWS dataset is also a news-wire domain dataset collected by Darwish (2013) from the RSS feed of the Arabic version of news.google.com from October 2012. The RSS consists of the headline and the first 50 to 100 words in the news articles. This set contains approximately 15k tokens. The third test set was extracted randomly from Twitter and contains a set of 1,423 tweets authored in November 2011. I</context>
</contexts>
<marker>Benajiba, Rosso, Benediruiz, 2007</marker>
<rawString>Yassine Benajiba, Paolo Rosso, and Jos´e Miguel Benediruiz. 2007. Anersys: An Arabic Named Entity Recognition System based on Maximum Entropy. In Computational Linguistics and Intelligent Text Processing, pages 143–153. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yassine Benajiba</author>
<author>Mona Diab</author>
<author>Paolo Rosso</author>
</authors>
<title>Arabic Named Entity Recognition using optimized feature sets.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>284--293</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1454" citStr="Benajiba et al., 2008" startWordPosition="228" endWordPosition="231">e high coverage of Wikipedia in order to address two challenges particular to tagging NEs in Arabic text: rich morphology and the absence of capitalisation. The corpus created with our new method (WDC) has been used to train an NE tagger which has been tested on different domains. Judging by the results, an NE tagger trained on WDC can compete with those trained on manually annotated corpora. 1 Introduction Supervised learning techniques are well known for their effectiveness to develop Named Entity Recognition (NER) taggers (Bikel et al., 1997; Sekine and others, 1998; McCallum and Li, 2003; Benajiba et al., 2008). The main disadvantage of supervised learning is that it requires a large annotated corpus. Although a substantial amount of annotated data is available for some languages, for other languages, including Arabic, more work is needed to enrich their linguistic resources. In fact, changing the domain or just expanding the set of classes always requires domain-specific experts and new annotated data, both of which cost time and effort. Therefore, current research focuses on approaches that require minimal human intervention to facilitate the process of moving the NE classifiers to new domains and</context>
<context position="29814" citStr="Benajiba et al., 2008" startWordPosition="4759" endWordPosition="4762">ion of news.google.com from October 2012. The RSS consists of the headline and the first 50 to 100 words in the news articles. This set contains approximately 15k tokens. The third test set was extracted randomly from Twitter and contains a set of 1,423 tweets authored in November 2011. It has approximately 26k tokens (Darwish, 2013). 8.2 Our Supervised Classifier All experiments to train and build a probabilistic classifier were conducted using Conditional Random Fields (CRF)12. Regarding the features used in all our experiments, we selected the most successful features from Arabic NER work (Benajiba et al., 2008; Abdul-Hamid and Darwish, 2010; Darwish, 2013). These features include: • The words immediately before and after the current word in their raw and stemmed forms. • The first 1, 2, 3, 4 characters in a word. • The last 1, 2, 3, 4 characters in a word. • The appearance of the word in the gazetteer. • The stemmed form of the word. The gazetteer used contains around 5,000 entries and was developed by Benajiba et al. (2008). A light stemmer was used to determine the stem form of the word by using simple rules to remove conjunctions, prepositions, and definite articles (Larkey et al., 2002). 12http</context>
</contexts>
<marker>Benajiba, Diab, Rosso, 2008</marker>
<rawString>Yassine Benajiba, Mona Diab, and Paolo Rosso. 2008. Arabic Named Entity Recognition using optimized feature sets. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 284–293. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Scott Miller</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>Nymble: a highperformance learning name-finder.</title>
<date>1997</date>
<booktitle>In Proceedings of the fifth conference on Applied natural language processing,</booktitle>
<pages>194--201</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1382" citStr="Bikel et al., 1997" startWordPosition="216" endWordPosition="219">ging candidate NEs. Herein we also introduce a mechanism based on the high coverage of Wikipedia in order to address two challenges particular to tagging NEs in Arabic text: rich morphology and the absence of capitalisation. The corpus created with our new method (WDC) has been used to train an NE tagger which has been tested on different domains. Judging by the results, an NE tagger trained on WDC can compete with those trained on manually annotated corpora. 1 Introduction Supervised learning techniques are well known for their effectiveness to develop Named Entity Recognition (NER) taggers (Bikel et al., 1997; Sekine and others, 1998; McCallum and Li, 2003; Benajiba et al., 2008). The main disadvantage of supervised learning is that it requires a large annotated corpus. Although a substantial amount of annotated data is available for some languages, for other languages, including Arabic, more work is needed to enrich their linguistic resources. In fact, changing the domain or just expanding the set of classes always requires domain-specific experts and new annotated data, both of which cost time and effort. Therefore, current research focuses on approaches that require minimal human intervention t</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>Daniel M Bikel, Scott Miller, Richard Schwartz, and Ralph Weischedel. 1997. Nymble: a highperformance learning name-finder. In Proceedings of the fifth conference on Applied natural language processing, pages 194–201. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
<author>Erica Brown</author>
<author>Lisa Ferro</author>
<author>Patty Robinson</author>
</authors>
<title>Named Entity Recognition Task Definition. MITRE and SAIC.</title>
<date>1999</date>
<contexts>
<context position="13079" citStr="Chinchor et al., 1999" startWordPosition="2043" endWordPosition="2046">evelop a Wikipedia document classifier, we used a set of 4,000 manually classified Wikipedia articles that are available free online5. The set was manually classified using the ACE (2008) taxonomy and a new class (Product). Therefore, there were eight coarse-grained categories in total: Facility, Geo-Political, Location, Organisation, Person, Vehicle, Weapon, and Product. As our work adheres to the CoNLL definition, we mapped these classified Wikipedia articles into CoNLL NE types – namely person, location, organisation, miscellaneous, or other – based on the CoNLL 2003 annotation guidelines (Chinchor et al., 1999). 5.2 The Classification of Wikipedia Articles Many researchers have already addressed the task of classifying Wikipedia articles into named entity types (Dakka and Cucerzan, 2008; Tardif et al., 2009). Alotaibi and Lee (2012) is the only study that has experimented with classifying the Arabic version of Wikipedia into NE classes. They have explored the use of Naive Bayes, Multinomial Naive Bayes, and SVM for classifying Wikipedia articles, and achieved a F-score ranging from 78% and 90% using different language-dependent and independent features. We conducted three experiments that used a sim</context>
</contexts>
<marker>Chinchor, Brown, Ferro, Robinson, 1999</marker>
<rawString>Nancy Chinchor, Erica Brown, Lisa Ferro, and Patty Robinson. 1999. 1999 Named Entity Recognition Task Definition. MITRE and SAIC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wisam Dakka</author>
<author>Silviu Cucerzan</author>
</authors>
<title>Augmenting Wikipedia with Named Entity Tags. In</title>
<date>2008</date>
<booktitle>IJCNLP,</booktitle>
<pages>545--552</pages>
<contexts>
<context position="13258" citStr="Dakka and Cucerzan, 2008" startWordPosition="2068" endWordPosition="2071">ACE (2008) taxonomy and a new class (Product). Therefore, there were eight coarse-grained categories in total: Facility, Geo-Political, Location, Organisation, Person, Vehicle, Weapon, and Product. As our work adheres to the CoNLL definition, we mapped these classified Wikipedia articles into CoNLL NE types – namely person, location, organisation, miscellaneous, or other – based on the CoNLL 2003 annotation guidelines (Chinchor et al., 1999). 5.2 The Classification of Wikipedia Articles Many researchers have already addressed the task of classifying Wikipedia articles into named entity types (Dakka and Cucerzan, 2008; Tardif et al., 2009). Alotaibi and Lee (2012) is the only study that has experimented with classifying the Arabic version of Wikipedia into NE classes. They have explored the use of Naive Bayes, Multinomial Naive Bayes, and SVM for classifying Wikipedia articles, and achieved a F-score ranging from 78% and 90% using different language-dependent and independent features. We conducted three experiments that used a simple bag-of-words features extracted from different portions of the Wikipedia document and metadata. We summarise the portions of the document included in each experiment below: Ex</context>
</contexts>
<marker>Dakka, Cucerzan, 2008</marker>
<rawString>Wisam Dakka and Silviu Cucerzan. 2008. Augmenting Wikipedia with Named Entity Tags. In IJCNLP, pages 545–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kareem Darwish</author>
</authors>
<title>Named Entity Recognition using Cross-lingual Resources: Arabic as an Example.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="29156" citStr="Darwish (2013)" startWordPosition="4654" endWordPosition="4655">odology, we used WDC as training data to build an NER model. Then we tested the resulting classifier on datasets from different domains. 8.1 Datasets For the evaluation purposes, we used three datasets: ANERcorp, NEWS, and TWEETS. ANERcorp is a news-wire domain dataset built and tagged especially for the NER task by Benajiba et al. (2007). It contains around 150k tokens and is available for free. We tested our methodology on the ANERcorp test corpus because it is widely used in the literature for comparing with existing systems. The NEWS dataset is also a news-wire domain dataset collected by Darwish (2013) from the RSS feed of the Arabic version of news.google.com from October 2012. The RSS consists of the headline and the first 50 to 100 words in the news articles. This set contains approximately 15k tokens. The third test set was extracted randomly from Twitter and contains a set of 1,423 tweets authored in November 2011. It has approximately 26k tokens (Darwish, 2013). 8.2 Our Supervised Classifier All experiments to train and build a probabilistic classifier were conducted using Conditional Random Fields (CRF)12. Regarding the features used in all our experiments, we selected the most succe</context>
<context position="30858" citStr="Darwish (2013)" startWordPosition="4931" endWordPosition="4933">ht stemmer was used to determine the stem form of the word by using simple rules to remove conjunctions, prepositions, and definite articles (Larkey et al., 2002). 12http://www.chokkan.org/software/crfsuite/ 8.3 Training the Supervised Classifier on Manually-annotated Data The supervised classifier in Section 8.2 was trained on the ANERcorp training set. We refer to the resulting model as the ANERcorp-Model. Table 4 shows the results of the ANERcorp-Model on the ANERcorp test set. The table also shows the results of the state-of-the-art supervised classifier ‘ANERcorp-Model(SoA)’ developed by Darwish (2013) when trained and tested on the same datasets used for ANERcorp-Model. Table 4: The results of Supervised Classifiers. 8.4 Results We compared a system trained on WDC with the systems trained by Alotaibi and Lee (2013) on two datasets, WikiFANE(whole) and WikiFANE(selective), which are also automatically collected from Arabic Wikipedia. The evaluation process was conducted by testing them on the ANERcorp set. The results shown in Table 5 prove that the methodology we proposed in this paper produces a dataset that outperforms the two other datasets in terms of recall and F-measure. Table 5: Com</context>
<context position="33419" citStr="Darwish, 2013" startWordPosition="5360" endWordPosition="5361">dia as location, some of them are not even considered entities in ANERcorp test corpus. Secondly, the ANERcorp-Model and the WDCModel were tested on the ANERcorp test data. The point of this comparison is to show how well the WDC dataset works on a news-wire domain, which is more specific than Wikipedia’s open domain. The table shows that the ANERcorp-model outperforms the F-score of the WDC-Model by around 13 points. However, in addition to the fact that training and test datasets for the ANERcorpModel are drawn from the same domain, 69% of NEs in the test data were seen in the training set (Darwish, 2013). Thirdly, the ANERcorp-Model and the WDCModel were tested on NEWS corpus, which is also a news-wire based dataset. The results from Table 6 reveal the quality of the WDC dataset on the NEWS corpus. The WDC-Model achieves relatively similar results to the ANERcorp-Model, although the latter has the advantage of being trained on a manually annotated corpus extracted from the similar domain of the NEWS test set. Finally, testing the ANERcorp-Model and the WDC-Model on data extracted from a social networks like Twitter proves that models trained on open-domain datasets like Wikipedia perform bett</context>
</contexts>
<marker>Darwish, 2013</marker>
<rawString>Kareem Darwish. 2013. Named Entity Recognition using Cross-lingual Resources: Arabic as an Example. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Exploiting Wikipedia as external knowledge for named entity recognition.</title>
<date>2007</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>698--707</pages>
<contexts>
<context position="7574" citStr="Kazama and Torisawa (2007)" startWordPosition="1214" endWordPosition="1217">et article are the same. 3 Related Work Current NE research seeks out adequate alternatives to traditional techniques such that they require minimal human intervention and solve deficiencies of traditional methods. Specific deficiencies include the limited number of NE classes resulting from the high cost of setting up corpora, and the difficulty of adapting the system to new domains. One of these trends is distant learning, which depends on the recruitment of external knowledge to increase the performance of the classifier, or to automatically create new resources used in the learning stage. Kazama and Torisawa (2007) exploited Wikipedia-based features to improve their NE machine learning recogniser’s F-score by three percent. Their method retrieved the corresponding Wikipedia entry for each candidate word sequence in the CoNLL 2003 dataset and extracted a category label from the first sentence of the entry. The automatic creation of training data has also been investigated using external knowledge. An et al. (2003) extracted sentences containing listed entities from the web, and produced a 1.8 million Korean word dataset. Their corpus 107 performed as well as manually annotated training data. Nothman et a</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Junichi Kazama and Kentaro Torisawa. 2007. Exploiting Wikipedia as external knowledge for named entity recognition. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 698– 707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Larkey</author>
<author>L Ballesteros</author>
<author>M E Connell</author>
</authors>
<title>Improving stemming for Arabic information retrieval: light stemming and co-occurrence analysis.</title>
<date>2002</date>
<booktitle>In Annual ACM Conference on Research and Development in Information Retrieval: Proceedings of the 25 th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<volume>11</volume>
<pages>275--282</pages>
<contexts>
<context position="30406" citStr="Larkey et al., 2002" startWordPosition="4867" endWordPosition="4870"> work (Benajiba et al., 2008; Abdul-Hamid and Darwish, 2010; Darwish, 2013). These features include: • The words immediately before and after the current word in their raw and stemmed forms. • The first 1, 2, 3, 4 characters in a word. • The last 1, 2, 3, 4 characters in a word. • The appearance of the word in the gazetteer. • The stemmed form of the word. The gazetteer used contains around 5,000 entries and was developed by Benajiba et al. (2008). A light stemmer was used to determine the stem form of the word by using simple rules to remove conjunctions, prepositions, and definite articles (Larkey et al., 2002). 12http://www.chokkan.org/software/crfsuite/ 8.3 Training the Supervised Classifier on Manually-annotated Data The supervised classifier in Section 8.2 was trained on the ANERcorp training set. We refer to the resulting model as the ANERcorp-Model. Table 4 shows the results of the ANERcorp-Model on the ANERcorp test set. The table also shows the results of the state-of-the-art supervised classifier ‘ANERcorp-Model(SoA)’ developed by Darwish (2013) when trained and tested on the same datasets used for ANERcorp-Model. Table 4: The results of Supervised Classifiers. 8.4 Results We compared a sys</context>
</contexts>
<marker>Larkey, Ballesteros, Connell, 2002</marker>
<rawString>L.S. Larkey, L. Ballesteros, and M.E. Connell. 2002. Improving stemming for Arabic information retrieval: light stemming and co-occurrence analysis. In Annual ACM Conference on Research and Development in Information Retrieval: Proceedings of the 25 th annual international ACM SIGIR conference on Research and development in information retrieval, volume 11, pages 275–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4,</booktitle>
<pages>188--191</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1430" citStr="McCallum and Li, 2003" startWordPosition="224" endWordPosition="227">a mechanism based on the high coverage of Wikipedia in order to address two challenges particular to tagging NEs in Arabic text: rich morphology and the absence of capitalisation. The corpus created with our new method (WDC) has been used to train an NE tagger which has been tested on different domains. Judging by the results, an NE tagger trained on WDC can compete with those trained on manually annotated corpora. 1 Introduction Supervised learning techniques are well known for their effectiveness to develop Named Entity Recognition (NER) taggers (Bikel et al., 1997; Sekine and others, 1998; McCallum and Li, 2003; Benajiba et al., 2008). The main disadvantage of supervised learning is that it requires a large annotated corpus. Although a substantial amount of annotated data is available for some languages, for other languages, including Arabic, more work is needed to enrich their linguistic resources. In fact, changing the domain or just expanding the set of classes always requires domain-specific experts and new annotated data, both of which cost time and effort. Therefore, current research focuses on approaches that require minimal human intervention to facilitate the process of moving the NE classi</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 188–191. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Mika</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
<author>Jordi Atserias</author>
</authors>
<title>Learning to Tag and Tagging to Learn: A Case Study on Wikipedia.</title>
<date>2008</date>
<volume>23</volume>
<pages>26--33</pages>
<contexts>
<context position="9671" citStr="Mika et al. (2008)" startWordPosition="1508" endWordPosition="1511">med entity annotations by classifying the target articles into standard entity types. This technique produced reasonable annotations, but was not immediately able to compete with existing gold-standard data. They better aligned their automatic annotations to the gold standard corpus by deducing additional links and heuristically tweaking the Wikipedia corpora. Following this approach, millions of words in nine languages were annotated. Wikipedia-trained models were evaluated against CONLL shared task data and other gold-standard corpora. Their method outperformed Richman and Schone (2008) and Mika et al. (2008), and achieved scores 10% higher than models trained on newswire when tested on manually annotated Wikipedia text. Alotaibi and Lee (2013) automatically developed two NE-annotated sets from Arabic Wikipedia. The corpora were built using the mechanism that transforms links into NE annotations, by classifying the target articles into named entity types. They used POS-tagging, morphological analysis, and linked NE phrases to detect other mentions of NEs that appear without links in text. By contrast, our method does not require POS-tagging or morphological analysis and just identifies unlinked NE</context>
</contexts>
<marker>Mika, Ciaramita, Zaragoza, Atserias, 2008</marker>
<rawString>Peter Mika, Massimiliano Ciaramita, Hugo Zaragoza, and Jordi Atserias. 2008. Learning to Tag and Tagging to Learn: A Case Study on Wikipedia. volume 23, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Nothman</author>
<author>James R Curran</author>
<author>Tara Murphy</author>
</authors>
<title>Transforming Wikipedia into Named Entity training data.</title>
<date>2008</date>
<booktitle>In Proceedings of the Australian Language Technology Workshop,</booktitle>
<pages>124--132</pages>
<contexts>
<context position="2631" citStr="Nothman et al., 2008" startWordPosition="410" endWordPosition="413">oving the NE classifiers to new domains and to expand NE classes. Semi-supervised and unsupervised learning approaches, along with the automatic creation of tagged corpora, are alternatives that avoid manually annotated data (Richman and Schone, 2008; Althobaiti et al., 2013). The high coverage and rich informational structure of online encyclopedias can be exploited for the automatic creation of datasets. For example, many researchers have investigated the use of Wikipedia’s structure to classify Wikipedia articles and to transform links into NE annotations according to the link target type (Nothman et al., 2008; Ringland et al., 2009). In this paper we present our approach to automatically derive a large NE annotated corpora from Arabic Wikipedia. The key to our method lies in the exploitation of Wikipedia’s concepts, specifically anchor texts1 and redirects, to handle the rich morphology in Arabic, and thereby eliminate the need to perform any deep morphological analysis. In addition, a capitalisation probability measure has been introduced and incorporated into the approach in order to replace the capitalisation feature that does not exist in the Arabic script. This capitalisation measure has been</context>
<context position="8183" citStr="Nothman et al. (2008)" startWordPosition="1306" endWordPosition="1309">isawa (2007) exploited Wikipedia-based features to improve their NE machine learning recogniser’s F-score by three percent. Their method retrieved the corresponding Wikipedia entry for each candidate word sequence in the CoNLL 2003 dataset and extracted a category label from the first sentence of the entry. The automatic creation of training data has also been investigated using external knowledge. An et al. (2003) extracted sentences containing listed entities from the web, and produced a 1.8 million Korean word dataset. Their corpus 107 performed as well as manually annotated training data. Nothman et al. (2008) exploited Wikipedia to create a massive corpus of named entity annotated text. They transformed Wikipedia’s links into named entity annotations by classifying the target articles into standard entity types3. Compared to MUC, CoNLL, and BBN corpora, their Wikipedia-derived corpora tend to perform better than other cross-corpus train/test pairs. Nothman et al. (2013) automatically created massive, multilingual training annotations for named entity recognition by exploiting the text and internal structure of Wikipedia. They first categorised each Wikipedia article into named entity types, traini</context>
<context position="19597" citStr="Nothman et al., 2008" startWordPosition="3098" endWordPosition="3101">ce the link target (Barack Obama) is the title of an article about person. By the end of this stage, all NE anchor texts (anchor texts referring to NE articles) on Wikipedia should be annotated based on the NE-type of the target article. 6.2 Dictionaries of Alternative Names Depending only on NE anchor texts in order to derive and annotate data from Wikipedia results in a low-quality dataset, as Wikipedia contains a fair amount of NEs mentioned without links. This can be attributed to the fact that each term on Wikipedia is more likely to be linked only on its first appearance in the article (Nothman et al., 2008). These unlinked NE phrases can be found simply by identifying the matching terms in the list of linked NE phrases10 and the text. The process is not as straightforward as it seems, however, because identifying corresponding terms may prove ineffective, especially in the case of morphologically rich language in which unlinked NE phrases are sometimes found agglutinated to prefixes and conjunctions. In order to detect unlinked and inflected forms of NEs in Wikipedia text, we extended the list of articles titles that were used in the previous step to find and match the possible NEs in the text b</context>
</contexts>
<marker>Nothman, Curran, Murphy, 2008</marker>
<rawString>Joel Nothman, James R Curran, and Tara Murphy. 2008. Transforming Wikipedia into Named Entity training data. In Proceedings of the Australian Language Technology Workshop, pages 124–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Nothman</author>
<author>Nicky Ringland</author>
<author>Will Radford</author>
<author>Tara Murphy</author>
<author>James R Curran</author>
</authors>
<date>2013</date>
<booktitle>Learning multilingual Named Entity Recognition from Wikipedia. Artificial Intelligence,</booktitle>
<pages>194--151</pages>
<contexts>
<context position="8551" citStr="Nothman et al. (2013)" startWordPosition="1358" endWordPosition="1361">tigated using external knowledge. An et al. (2003) extracted sentences containing listed entities from the web, and produced a 1.8 million Korean word dataset. Their corpus 107 performed as well as manually annotated training data. Nothman et al. (2008) exploited Wikipedia to create a massive corpus of named entity annotated text. They transformed Wikipedia’s links into named entity annotations by classifying the target articles into standard entity types3. Compared to MUC, CoNLL, and BBN corpora, their Wikipedia-derived corpora tend to perform better than other cross-corpus train/test pairs. Nothman et al. (2013) automatically created massive, multilingual training annotations for named entity recognition by exploiting the text and internal structure of Wikipedia. They first categorised each Wikipedia article into named entity types, training and evaluating on 7,200 manually-labelled Wikipedia articles across nine languages: English, German, French, Italian, Polish, Spanish, Dutch, Portuguese, and Russian. Their cross-lingual approach achieved up to 95% accuracy. They transformed Wikipedia’s links into named entity annotations by classifying the target articles into standard entity types. This techniq</context>
<context position="25741" citStr="Nothman et al., 2013" startWordPosition="4086" endWordPosition="4089">do code of the filtering algorithm. Algorithm 1: Filtering Alternative Names Input: A set L = {l1, l2, ... , lnI of all alternative names of Wikipedia articles Output: A set RL = {rl1, rl2, ... , rlnI of reliable alternative names 1 for i +— 1 to n do T +— split li into tokens if (T.size() &gt;= 2) then /* All tokens of T do not belong to apposition list */ if (! containAppositiveWord(T)) then add li to the set RL else lightstem +— findLightStem(li) englishgloss +— translate(lightstem) /* Compute Capitalisation Probability for English gloss */ the Spanish corpus consider them NOT named entities (Nothman et al., 2013). As far as we know, almost all Arabic NER datasets that followed the CoNLL style and guidelines in the annotation process consider nationalities NOT named entities. On Wikipedia all nationalities are linked to articles about the corresponding countries, which makes the annotation tool tag them as LOC. We decided to consider them NOT named entities in accordance with the CoNLL-style Arabic datasets. Therefore, in order to resolve this issue, we compiled a list of nationalities, and other adjectival forms of religion and ethnic groups, so that any anchor text matching an entry in the list was r</context>
</contexts>
<marker>Nothman, Ringland, Radford, Murphy, Curran, 2013</marker>
<rawString>Joel Nothman, Nicky Ringland, Will Radford, Tara Murphy, and James R Curran. 2013. Learning multilingual Named Entity Recognition from Wikipedia. Artificial Intelligence, 194:151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander E Richman</author>
<author>Patrick Schone</author>
</authors>
<title>Mining Wiki Resources for Multilingual Named Entity Recognition. In</title>
<date>2008</date>
<booktitle>ACL,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="2261" citStr="Richman and Schone, 2008" startWordPosition="352" endWordPosition="355"> languages, including Arabic, more work is needed to enrich their linguistic resources. In fact, changing the domain or just expanding the set of classes always requires domain-specific experts and new annotated data, both of which cost time and effort. Therefore, current research focuses on approaches that require minimal human intervention to facilitate the process of moving the NE classifiers to new domains and to expand NE classes. Semi-supervised and unsupervised learning approaches, along with the automatic creation of tagged corpora, are alternatives that avoid manually annotated data (Richman and Schone, 2008; Althobaiti et al., 2013). The high coverage and rich informational structure of online encyclopedias can be exploited for the automatic creation of datasets. For example, many researchers have investigated the use of Wikipedia’s structure to classify Wikipedia articles and to transform links into NE annotations according to the link target type (Nothman et al., 2008; Ringland et al., 2009). In this paper we present our approach to automatically derive a large NE annotated corpora from Arabic Wikipedia. The key to our method lies in the exploitation of Wikipedia’s concepts, specifically ancho</context>
<context position="9648" citStr="Richman and Schone (2008)" startWordPosition="1503" endWordPosition="1506">rmed Wikipedia’s links into named entity annotations by classifying the target articles into standard entity types. This technique produced reasonable annotations, but was not immediately able to compete with existing gold-standard data. They better aligned their automatic annotations to the gold standard corpus by deducing additional links and heuristically tweaking the Wikipedia corpora. Following this approach, millions of words in nine languages were annotated. Wikipedia-trained models were evaluated against CONLL shared task data and other gold-standard corpora. Their method outperformed Richman and Schone (2008) and Mika et al. (2008), and achieved scores 10% higher than models trained on newswire when tested on manually annotated Wikipedia text. Alotaibi and Lee (2013) automatically developed two NE-annotated sets from Arabic Wikipedia. The corpora were built using the mechanism that transforms links into NE annotations, by classifying the target articles into named entity types. They used POS-tagging, morphological analysis, and linked NE phrases to detect other mentions of NEs that appear without links in text. By contrast, our method does not require POS-tagging or morphological analysis and just</context>
</contexts>
<marker>Richman, Schone, 2008</marker>
<rawString>Alexander E Richman and Patrick Schone. 2008. Mining Wiki Resources for Multilingual Named Entity Recognition. In ACL, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicky Ringland</author>
<author>Joel Nothman</author>
<author>Tara Murphy</author>
<author>James R Curran</author>
</authors>
<title>Classifying articles in English and German Wikipedia.</title>
<date>2009</date>
<booktitle>In Australasian Language Technology Association Workshop 2009,</booktitle>
<pages>20</pages>
<contexts>
<context position="2655" citStr="Ringland et al., 2009" startWordPosition="414" endWordPosition="417">rs to new domains and to expand NE classes. Semi-supervised and unsupervised learning approaches, along with the automatic creation of tagged corpora, are alternatives that avoid manually annotated data (Richman and Schone, 2008; Althobaiti et al., 2013). The high coverage and rich informational structure of online encyclopedias can be exploited for the automatic creation of datasets. For example, many researchers have investigated the use of Wikipedia’s structure to classify Wikipedia articles and to transform links into NE annotations according to the link target type (Nothman et al., 2008; Ringland et al., 2009). In this paper we present our approach to automatically derive a large NE annotated corpora from Arabic Wikipedia. The key to our method lies in the exploitation of Wikipedia’s concepts, specifically anchor texts1 and redirects, to handle the rich morphology in Arabic, and thereby eliminate the need to perform any deep morphological analysis. In addition, a capitalisation probability measure has been introduced and incorporated into the approach in order to replace the capitalisation feature that does not exist in the Arabic script. This capitalisation measure has been utilised in order to fi</context>
</contexts>
<marker>Ringland, Nothman, Murphy, Curran, 2009</marker>
<rawString>Nicky Ringland, Joel Nothman, Tara Murphy, and James R Curran. 2009. Classifying articles in English and German Wikipedia. In Australasian Language Technology Association Workshop 2009, page 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>NYU: Description of the Japanese NE system used for MET-2.</title>
<date>1998</date>
<booktitle>In Proc. of the Seventh Message Understanding Conference (MUC7),</booktitle>
<volume>17</volume>
<marker>Sekine, 1998</marker>
<rawString>Satoshi Sekine et al. 1998. NYU: Description of the Japanese NE system used for MET-2. In Proc. of the Seventh Message Understanding Conference (MUC7), volume 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Tardif</author>
<author>James R Curran</author>
<author>Tara Murphy</author>
</authors>
<title>Improved Text Categorisation for Wikipedia Named Entities.</title>
<date>2009</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop,</booktitle>
<pages>104--108</pages>
<contexts>
<context position="13280" citStr="Tardif et al., 2009" startWordPosition="2072" endWordPosition="2075">new class (Product). Therefore, there were eight coarse-grained categories in total: Facility, Geo-Political, Location, Organisation, Person, Vehicle, Weapon, and Product. As our work adheres to the CoNLL definition, we mapped these classified Wikipedia articles into CoNLL NE types – namely person, location, organisation, miscellaneous, or other – based on the CoNLL 2003 annotation guidelines (Chinchor et al., 1999). 5.2 The Classification of Wikipedia Articles Many researchers have already addressed the task of classifying Wikipedia articles into named entity types (Dakka and Cucerzan, 2008; Tardif et al., 2009). Alotaibi and Lee (2012) is the only study that has experimented with classifying the Arabic version of Wikipedia into NE classes. They have explored the use of Naive Bayes, Multinomial Naive Bayes, and SVM for classifying Wikipedia articles, and achieved a F-score ranging from 78% and 90% using different language-dependent and independent features. We conducted three experiments that used a simple bag-of-words features extracted from different portions of the Wikipedia document and metadata. We summarise the portions of the document included in each experiment below: Exp1: Experiment 1 invol</context>
<context position="14607" citStr="Tardif et al., 2009" startWordPosition="2280" endWordPosition="2283">tive for the classification of articles (Tardif et al., 2009; Alotaibi and Lee, 2012). Therefore, in Experiment 2 we included tokens from categories, templates – specifically ‘Infobox’ – as well as tokens from the article title and first sentence of the document. Exp3: Experiment 3 involved the same set of tokens as experiment 2 except that categories and infobox features were marked with suffixes to differentiate them from tokens extracted from the article body text. This step of distinguishing tokens based on their location in the document improved the accuracy of document’s classification (Tardif et al., 2009; Alotaibi and Lee, 2012). 5www.cs.bham.ac.uk/∼fsa081/ In order to optimise features, we implemented a filtered version of the bag-of-words article representation (e.g., removing punctuation marks and symbols) to classify the Arabic Wikipedia documents instead of using a raw dataset (Alotaibi and Lee, 2012). In addition, the same study shows the high impact of applying tokenisation6 as opposed to the neutral effect of using stemming. We used the filtered features proposed in the study of Alotaibi and Lee (2012), which included removing punctuation marks, symbols, filtering stop words, and norm</context>
</contexts>
<marker>Tardif, Curran, Murphy, 2009</marker>
<rawString>Sam Tardif, James R. Curran, and Tara Murphy. 2009. Improved Text Categorisation for Wikipedia Named Entities. In Proceedings of the Australasian Language Technology Association Workshop, pages 104–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
<author>Max M¨uhlh¨auser</author>
</authors>
<title>Analyzing and accessing Wikipedia as a lexical semantic resource.</title>
<date>2007</date>
<pages>197--205</pages>
<location>Tuebingen, Germany: Gunter Narr, T¨ubingen.</location>
<marker>Zesch, Gurevych, M¨uhlh¨auser, 2007</marker>
<rawString>Torsten Zesch, Iryna Gurevych, and Max M¨uhlh¨auser. 2007. Analyzing and accessing Wikipedia as a lexical semantic resource. pages 197–205. Tuebingen, Germany: Gunter Narr, T¨ubingen.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>