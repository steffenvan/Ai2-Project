<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000349">
<title confidence="0.9848325">
Hello, Who is Calling?: Can Words Reveal the Social Nature of
Conversations?
</title>
<author confidence="0.990692">
Anthony Stark, Izhak Shafran and Jeffrey Kaye
</author>
<affiliation confidence="0.95176">
Center for Spoken Language Understanding, OHSU, Portland USA.
</affiliation>
<email confidence="0.999305">
{starkan,shafrani,kaye}@ohsu.edu
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999856064516129">
This study aims to infer the social nature
of conversations from their content automat-
ically. To place this work in context, our moti-
vation stems from the need to understand how
social disengagement affects cognitive decline
or depression among older adults. For this
purpose, we collected a comprehensive and
naturalistic corpus comprising of all the in-
coming and outgoing telephone calls from 10
subjects over the duration of a year. As a
first step, we learned a binary classifier to fil-
ter out business related conversation, achiev-
ing an accuracy of about 85%. This clas-
sification task provides a convenient tool to
probe the nature of telephone conversations.
We evaluated the utility of openings and clos-
ing in differentiating personal calls, and find
that empirical results on a large corpus do
not support the hypotheses by Schegloff and
Sacks that personal conversations are marked
by unique closing structures. For classifying
different types of social relationships such as
family vs other, we investigated features re-
lated to language use (entropy), hand-crafted
dictionary (LIWC) and topics learned using
unsupervised latent Dirichlet models (LDA).
Our results show that the posteriors over top-
ics from LDA provide consistently higher ac-
curacy (60-81%) compared to LIWC or lan-
guage use features in distinguishing different
types of conversations.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999605">
In recent years, there has been a growing interest
in analyzing text in informal interactions such as
in Internet chat, newsgroups and twitter. The em-
phasis of most such research has been in estimating
network structure (Kwak et al., 2010) and detecting
trending topics (Ritter et al., 2010), sentiments (Pak
and Paroubek, 2010) and first stories (Petrovi´c et al.,
2010). The focus has been on aggregating informa-
tion from large number of users to analyze popula-
tion level statistics.
The study reported in this paper, in contrast, fo-
cuses on understanding the social interactions of an
individual over long periods of time. Our motiva-
tion stems from the need to understand the factors of
social engagement that ameliorate the rate of cogni-
tive decline and depression in older adults. Since the
early work of Glass (1997) and colleagues, several
studies on large cohorts over extended duration have
confirmed that older adults with few social relation-
ships are at an increased risk of suffering depression
and dementia. The limited information available in
currently used coarse measures, often based on self-
reports, have hindered epidemiologists from probing
the nature of this association further.
While social engagement is typically multi-
faceted, older adults, who are often less mobile, rely
on telephone conversations to maintain their social
relationships. This is reflected in a recent survey
by Pew Research Center which reported that among
adults 65 years and older, nine in ten talk with family
or friends every day and more than 95% use land-
line telephones for all or most of their calls (Tay-
lor et al., June 29 2009). Conveniently for us, tele-
phone conversations present several advantages for
analysis. Unlike many other forms of communica-
tion, the interaction is restricted solely to an audio
</bodyText>
<page confidence="0.654045">
112
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 112–119,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999987830508475">
channel, without recourse to gestures or facial ex-
pressions. While we do not discount the importance
of multi-modal communication, having a communi-
cation channel restricted to a unimodal format does
significantly simplify both collection and analysis.
Furthermore, the use of a handset affords the oppor-
tunity to capture naturalistic speech samples at rela-
tively high signal-to-noise ratio. Lastly, automatic
speech recognition (ASR) systems can now tran-
scribe telephone conversations with sufficient accu-
racy for useful automated analysis.
Given the above premise, we focus our attention
on studying social interactions of older adults over
land-line telephones. To facilitate such a study, we
collected telephone conversations from several older
adults for approximately one year. Note that our
corpus is unlike the publicly available Switchboard
and Fisher corpora, which contain conversations be-
tween unfamiliar speakers discussing a topic from a
pre-determined list such as music, crime, air pollu-
tion (Godfrey et al., 1992). In contrast, the conversa-
tions in our corpus are completely natural, covering
a wide range of topics, conversational partners and
types of interactions. Our corpus is also compre-
hensive in that it includes all the outgoing/incoming
calls from subjects’ homes during the observation
period.
As a step toward understanding social networks
and associated relationships, our first task was to
classify social and non-social (business) conversa-
tions. While reverse listing was useful to a certain
extent, we were unable to find listing on up to 50%
of the calls in our corpus due to lack of caller ID in-
formation on many calls as well as unlisted numbers.
Moreover, we cannot preclude the possibility that a
social conversation may occur on a business num-
ber (e.g., a friend or a relative working in a business
establishment) and vice versa. Using the subset of
calls for which we have reliable listing, we learned
a supervised classifier and then employed the classi-
fier to label the remaining calls for further analysis.
The focus of this study was not so much on learn-
ing a binary classifier, but using the resulting classi-
fier as a tool to probe the nature of telephone con-
versations as well as to test whether the scores ob-
tained from it can serve as a proxy for degree of
social familiarity. The classifier also affords us an
opportunity to re-examine hypotheses proposed by
Schegloff and Sacks (1974; 1968; 1973) about the
structure of openings and closing in business and
personal conversations. Within social conversation,
we investigated the accuracy of identifying conver-
sations with close friends and relatives from others.
The rest of this paper is arranged as follows. After
describing the corpus and ASR system in Sections 2
and 3, we probe the nature of telephone conversa-
tions in Section 4. We present direct binary classifi-
cation experiments in Section 5 and lastly, we close
with a few remarks in Section 6.
</bodyText>
<sectionHeader confidence="0.830068" genericHeader="method">
2 Corpus: Everyday Telephone
</sectionHeader>
<subsectionHeader confidence="0.905531">
Conversations Spanning a Year
</subsectionHeader>
<bodyText confidence="0.99997921875">
Our corpus consists of 12,067 digitized land-line
telephone conversations. Recordings were taken
from 10 volunteers, 79 years or older, over a pe-
riod of approximately 12 months. Subjects were all
native English speakers recruited from the USA. In
addition to the conversations, our corpus includes
a rich set of meta-data, such as call direction (in-
coming vs outgoing), time of call, duration and
DTMF/caller ID when available. At the end of the
data collection, for each subject, twenty telephone
numbers were identified corresponding to top ten
most frequent calls and top ten longest calls. Sub-
jects were asked to identify their relationship with
the speakers at these numbers as immediate family,
near relatives, close friends, casual friends, strangers
and business.
For this initial study, we discard conversations
with less than 30 automatically transcribed words.
This was done primarily to get rid of spurious and/or
noisy recordings related to device failure as well
as incorrectly dialed telephone numbers. Moreover,
short conversations are less likely to provide enough
social context to be useful.
Of the 8,558 available conversations, 2,728 were
identified as residential conversations and 1,095
were identified as business conversations using re-
verse listings from multiple sources; e.g. phone
directory lookup, exit interviews, internet lookup.
This left 4,395 unlabeled records, for which the re-
verse listing was either inconclusive or for which the
phone number information was missing and/or im-
properly recorded.
</bodyText>
<page confidence="0.999497">
113
</page>
<sectionHeader confidence="0.984925" genericHeader="method">
3 Automatic Speech Recognition
</sectionHeader>
<bodyText confidence="0.999989714285714">
Conversations in our corpus were automatically
transcribed using an ASR system. Our ASR sys-
tem is structured after IBM’s conversation telephony
system which gave the top performance in the most
recent evaluation of speech recognition technology
for telephony by National Institute of Standards and
Technology (Soltau et al., 2005). The acoustic mod-
els were trained on about 2000 hours of telephone
speech from Switchboard and Fisher corpora (God-
frey et al., 1992). The system has a vocabulary of
47K and uses a trigram language model with about
10M n-grams, estimated from a mix of transcripts
and web-harvested data. Decoding is performed
in three stages using speaker-independent models,
vocal-tract normalized models and speaker-adapted
models. The three sets of models are similar in
complexity with 4000 clustered pentaphone states
and 150K Gaussians with diagonal covariances. Our
system does not include discriminative training and
performs at a word error rate of about 24% on NIST
RT Dev04 which is comparable to state of the art
performance for such systems. The privacy require-
ments in place for our corpus prohibit human lis-
tening – precluding the transcriptions needed report-
ing recognition accuracy. However, while our cor-
pus differs from Switchboard, we expect the perfor-
mance of the 2000 hour recognizer to be relatively
close to results on NIST benchmark.
</bodyText>
<sectionHeader confidence="0.848519" genericHeader="method">
4 Nature of Telephone Conversations
</sectionHeader>
<subsectionHeader confidence="0.995453">
4.1 Classification Experiments
</subsectionHeader>
<bodyText confidence="0.9999942">
As mentioned earlier, we first learned a baseline bi-
nary classifier to filter out business calls from res-
idential calls. Apart from using this as a tool to
probe the characteristics of social calls, it also helps
us to classify unlabeled calls and thus avoid discard
half the corpus from subsequent analysis of social
network and relationships. Recall, the labels for
the calls were obtained using reverse lookup from
multiple sources. We assume that the majority of
our training set reflect the true nature of the con-
versations and expect to employ the classifier sub-
sequently for correcting the errors arising when per-
sonal conversations occur on business lines and vice
versa.
We learned a baseline SVM classifier using a bal-
anced training set. From the labeled records we cre-
ated a balanced verification set containing 164,115
words over 328 conversations. The remainder was
used to create a balanced training set consisting of
866,696 words over 1,862 conversations. The SVM
was trained on 20-fold cross validation and evalu-
ated on the verification set. After experimenting
with different kernels, we found an RBF kernel to
be most effective, achieving an accuracy of 87.50%
on the verification data.
</bodyText>
<subsectionHeader confidence="0.998255">
4.2 Can the Scores of the Binary Classifier
Differentiate Types of Social Relationship?
</subsectionHeader>
<bodyText confidence="0.999849217391304">
Since the SVM score has utility in measuring a con-
versation on the social-business axis, we now exam-
ine its usefulness in differentiating social ties. To
test this, we computed SVM score statistics for all
conversations with family and friends. For compar-
ison, we also computed the statistics for all conver-
sations automatically tagged as residential as well as
all conversations in the data. Table 1 shows the av-
erage family score is unambiguously higher than the
average residential conversation (independent sam-
ple t-test, p &lt; 0.001). This is an interesting re-
sult since distinction of family conversations (from
general social calls) never factored into the SVM.
Rather, it appears to arise naturally as an extrap-
olation from the more general residential/business
discriminator. The friend sub-population exhibited
statistics much closer to the general residential pop-
ulation and its differences were not significant to any
degree. The overlap between scores for conversa-
tions with family and friends overlap significantly.
Notably, the conversations with family have a sig-
nificantly higher mean and a tighter variance than
with other social ties.
</bodyText>
<tableCaption confidence="0.999487">
Table 1: SVM scores for phone number sub-categories.
</tableCaption>
<table confidence="0.999288666666667">
Category # Calls Mean score STD
Family 1162 1.12 0.50
Friends 532 0.95 0.51
Residential 2728 0.93 0.63
Business 1095 -1.16 0.70
Global 8558 0.46 0.96
</table>
<page confidence="0.846407">
114
</page>
<figure confidence="0.39798175">
Res/biz classification errror (%)
4.3 How Informative are Openings and
Closings in Differentiating Telephone
Conversations?
</figure>
<bodyText confidence="0.971156909090909">
Schegloff and Sacks assert openings (beginnings)
and closings (ends) of telephone conversations have
certain identifiable structures (Sacks et al., 1974).
For example, the structure of openings facilitate es-
tablishing identity of the conversants and the pur-
pose of their call (Schegloff, 1968). Closings in
personal conversations are likely to include a pre-
closing signal that allows either party to mention
any unmentioned mentionables before conversation
ends (Schegloff and Sacks, 1973).
Given the above assertions, we expect openings
and closings to be informative about the type of con-
versations. Using our classifier, we compare the ac-
curacy of predicting the type from openings, clos-
ings and random segments of the conversations. For
different lengths of the three types of segments, the
observed performance of the classifier is plotted in
Figure 1. The results for the random segment were
computed by averaging over 100 trials. Several im-
portant results are immediately apparent. Openings
possess much higher utility than closings. This is
consistent with general intuition that the opening ex-
change is expected to clarify the nature and topic
of the call. Closings were found to be only as in-
formative as random segments from the conversa-
tions. This is contrary to what one might expect
from Schegloff and Sack’s assertion that pre-closing
differ significantly in personal telephone calls (Sche-
gloff and Sacks, 1973). Less intuitive is the fact that
increasing the length of the opening segment does
not improve performance. Surprisingly, a 30-word
segment from the opening appears to be sufficient to
achieve high classification accuracy (87.20%).
</bodyText>
<listItem confidence="0.501375333333333">
4.4 Data Sparsity or Inherent Ambiguity: Why
are Short Conversations difficult to
Classify?
</listItem>
<bodyText confidence="0.993793142857143">
Sparsity often has a deleterious effect on classifica-
tion performance. In our experiments, we noticed
that shorter conversations suffer from poor classifi-
cation. However, the results from the above section
appear to contradict this assertion, as a 30-word win-
dow can give very good performance. This seems to
suggest short conversations suffer poor recognition
</bodyText>
<figure confidence="0.749829">
Number of words sampled
</figure>
<figureCaption confidence="0.99755375">
Figure 1: Comparison of classification accuracy in pre-
dicting the type of conversation from openings, closings
and random segments. Error bars are one standard devia-
tion.
</figureCaption>
<bodyText confidence="0.999765428571428">
due to properties beyond the obvious sparsity effect.
To test this, we investigated the differences in short
and long conversations in greater detail. We sepa-
rate calls into quintile groups based on word counts.
However, we now calculate all features from a 30-
word opening – eliminating effects directly related
to size. The results in Table 2 show that the abil-
</bodyText>
<tableCaption confidence="0.95885425">
Table 2: Accuracy in predicting the type of conversation
when they are truncated to 30-words of openings based
on conversation length quintiles. The column, Res / Biz,
split gives the label distributions for the quintiles.
</tableCaption>
<table confidence="0.997375428571428">
Orig. Word Counts Split Accuracy
Quintile #Words Res. / Biz.
0-20 30-87 62.12 / 37.88 78.6
20-40 88-167 48.48 / 51.52 82.8
40-60 168-295 39.39 / 60.61 91.4
60-80 296-740 40.91 / 59.09 87.8
80-100 741+ 59.38 / 40.62 93.4
</table>
<bodyText confidence="0.999917461538462">
ity to predict the type of conversation does not de-
grade when long conversations are truncated. Mean-
while, the accuracy of classification drops for (orig-
inally) short conversations. There is a surprisingly
small performance loss due to the artificial trunca-
tion. These observations suggest that the long and
short conversations are inherently different in na-
ture, at least in their openings.
We should point out that spurious recordings
in our corpus are concentrated in the low word
count group – undoubtedly dropping their accura-
cies. However, the trend of improving accuracy per-
sists well into the high word count ranges where spu-
</bodyText>
<figure confidence="0.963505545454545">
30 50 100 250 500 1000
30
25
20
15
10
5
0
Word sample from start
Word sample from end
Word sample randomly taken
</figure>
<page confidence="0.992306">
115
</page>
<bodyText confidence="0.9998895">
rious records are rare. Given this fact, it appears that
individuals in our corpus are more careful in enun-
ciating the reasons for calling if an extended phone
conversation is anticipated.
</bodyText>
<subsectionHeader confidence="0.9936965">
4.5 Can Openings Help Predict Relative
Lengths of Conversations?
</subsectionHeader>
<bodyText confidence="0.999972911764706">
From the results presented so far, we know that
openings are good predictors of the type of conver-
sations yet to unfold. We also know that there are in-
herent language differences between short and long
conversations. So, it is natural to ask whether open-
ings can predict relative lengths of conversations.
To test this hypothesis, we bin conversations into
5 groups or ranks based on their percentile lengths
(word counts) – very short, short, moderate, long
and very long durations, as in Table 2. Using in-
dependent features from the 30-word opening, we
attempt to predict the relative rank of two conver-
sations by learning a rank SVM (Joachims, 2006).
We found the ranker to give 27% error rate, signifi-
cantly lower (independent sample t-test, d.f. Pz� 1M,
p&lt;0.01) than the random chance of 40%. Chance
baseline was determined using Monte Carlo simula-
tion (1M random rankings) in conjunction with the
rank SVM evaluation (Joachims, 2006).
Features from very short conversations may con-
tain both openings and closings, i.e., both a hello and
a goodbye, making them easier to rank. To avoid this
confounding factor, we also compute performance
after discarding the shortest grouping of conversa-
tions (&lt; 88 words) to ensure closings are avoided in
the 30-word window. The resulting classifier over
short, medium, long, very long conversations ranked
30% of the pairs erroneously, somewhat better than
chance at 37%. Though the performance gain over
the random ranker has shrunk considerably, there is
still some utility in using the opening of a conversa-
tion to determine its ultimate duration. However, it
is clear predicting duration via conversation opening
is a much more difficult task overall.
</bodyText>
<sectionHeader confidence="0.9256785" genericHeader="method">
5 Supervised Classification of Types of
Social Relationships
</sectionHeader>
<bodyText confidence="0.898437">
While the scores of the binary classifier provided
statistically significant differences between calls to
different types of social relationships, they are not
particularly useful in classifying the calls with high
accuracy. In this section, we investigate the perfor-
mance of classifiers to differentiate the following bi-
nary classes.
</bodyText>
<listItem confidence="0.999965">
• Residential vs business
• Family vs all other
• Family vs other residential
• Familiar vs non-familiar
</listItem>
<bodyText confidence="0.999053">
Familiar denotes calls to those numbers with whom
subject has conversed more than 5 times. Recall
that the numbers corresponding to family members
were identified by the subjects in a post-collection
interview. We learned binary classifier for the four
cases, a few of which were reported in our early
work (Stark et al., 2011). We investigated a vari-
ety of features in these tasks. A breakdown of the
corpus is give in Table 3. Not all categories are mu-
tually exclusive. For example the majority of family
conversations also fall into the familiar and residen-
tial categories.
</bodyText>
<tableCaption confidence="0.993528">
Table 3: Number of conversations per category.
</tableCaption>
<figure confidence="0.984762142857143">
Category Instances
Biz. 1095
Residential 2728
Family 1111
Res. non-family 1462
Familiar 3010
All 8558
</figure>
<subsectionHeader confidence="0.996824">
5.1 Lexical Statistics
</subsectionHeader>
<bodyText confidence="0.999890357142857">
Speakers who share close social ties are likely to
engage in conversations on a wide variety of top-
ics and this is likely to reflect in the entropy of their
language use. We capture this aspect of language
use by computing language entropy over the uni-
gram word distribution for each conversation, i.e;
H(d) _ − E,,, p(w|d) log p(w|d), where p(w|d) is
the probability of word w given conversation d. We
also included two other lexical statistics namely the
speaking rate and the word count (in log domain).
Table 4 lists the utility of these language proper-
ties for differentiating the four binary classes men-
tioned earlier, where the p-value is computed using
two tailed independent sample t-test.
</bodyText>
<page confidence="0.999575">
116
</page>
<tableCaption confidence="0.89410275">
Table 4: T-statistics for different context groups. Labels:
a) Log-word count, b) speaking rate, c) language entropy.
Asterisk denotes significance at p&lt;0.0001. Sample sizes
(n) may be found in Table 3.
</tableCaption>
<figureCaption confidence="0.9754208">
Task d.f. a) b) c)
Res. v. biz. 7646 1.9 10.1* -1.9
Family v. other 8556 16.3* 9.0* 13.4*
Family v. other res. 2571 12.9* 5.1* 11.3*
Familiar v. other 8556 10.4* 6.4* 9.3*
</figureCaption>
<bodyText confidence="0.9999661">
For the most part, the significance tests conform
with preconceived ideas of language use over the
telephone. It is shown that people talk longer,
more rapidly and have wider range of language use
when conversing with a familiar contact and/or fam-
ily member. Surprisingly, only the speaking rate
showed significant differences among the residen-
tial/business categories, with business conversations
being conducted at a slower pace at least for the el-
derly demographic in our corpus.
</bodyText>
<subsectionHeader confidence="0.999756">
5.2 Linguistic inquiry and Word Count
</subsectionHeader>
<bodyText confidence="0.99996175">
We investigated a hand-crafted dictionary of salient
words, called Linguistic Inquiry and Word Count
(LIWC), employed in social psychology stud-
ies (Pennebaker et al., 2003). This dictionary group
words into 64 categories such as pronouns, activ-
ity words, positive emotion and health. The cate-
gories have significant overlap and a given word can
map to zero or more categories. The clear benefit
of LIWC is that the word categories have very clear
and pre-labeled meanings. They suffer from the ob-
vious drawback that the words are labeled in isola-
tion without taking their context into account. The
tags are not chosen under any mathematical criteria
and so there are no guarantees the resultant feature
will be useful or optimal for classifying utterances.
Table 5 lists the LIWC categories significant (p&lt;
0.001) to the different classes. The listed terms are
sorted according to their t-statistic, with early and
later terms more indicative of first and second class
labels respectively.
</bodyText>
<subsectionHeader confidence="0.998614">
5.3 Latent Dirichlet allocation
</subsectionHeader>
<bodyText confidence="0.999964152173913">
Unsupervised clustering and feature selection can
make use of data for which we have no labels. For
example, in the case of business and residential la-
bels, unlabeled data amounts to about 50% of our
corpus. Motivated by this consideration, we exam-
ined unsupervised clustering using Latent Dirichlet
Allocation (LDA) (Blei et al., 2003).
LDA models a conversation as a bag of words.
The model generates a conversation by: (a) sam-
pling a topic distribution 0 for the conversation using
a per-conversation Dirichlet topic distribution with a
hyper-parameter α, (b) sampling a topic z for each
word in the conversation using a multinomial distri-
bution using the topic mixture 0, and (c) sampling
the word from a per-topic multinomial word distri-
bution with a hyper-parameter Q (Blei et al., 2003).
The number of topics are assumed to be given. The
per-conversation topic distribution and the per-topic
word distribution can be automatically estimated to
maximize the likelihood of training data. The spar-
sity of these two distributions can be controlled by
tweaking α and Q; lower values increase sparsity.
For our experiments, we estimated a maximum
likelihood 30-topic LDA model from the corpus.
Experimentally, we found best cross-validation re-
sults were obtained when α and Q were set to 0.01
and 0.1 respectively.
When peering into the topics learned by the LDA
method, it did appear that topics were approximately
separated into contextual categories. Most interest-
ing, when the number of clusters are reduced to two,
the LDA model managed to segment residential and
business conversations with relatively high accuracy
(80%). This suggests the LDA model was able to
approximately learn these classes in an unsupervised
manner.
Table 6 lists words strongly associated with the
two topics and clearly the unsupervised cluster-
ing appears to have automatically differentiated the
business-oriented calls from the rest. On closer ex-
amination, we found that most of the probability
was distributed in a limited number of words in the
business-oriented topic. On the contrary, the proba-
bility was more widely distributed among words in
the other cluster, reflecting the diversity of content
in personal calls.
</bodyText>
<subsectionHeader confidence="0.998747">
5.4 Classifying Types of Social Relationships
</subsectionHeader>
<bodyText confidence="0.999620666666667">
Though t-tests are useful for ruling out insignificant
relationships, they are insufficient for quantifying
the degree of separability – and thus, ultimately their
</bodyText>
<page confidence="0.998657">
117
</page>
<tableCaption confidence="0.998521">
Table 5: LIWC categories found to be significant in classifying relationships, ranked according to their t-statistic.
</tableCaption>
<reference confidence="0.744445222222222">
Relationship Categories
Res. v. biz. I, Past, Self, Motion, Other, Insight, Eating, Pronoun, Down, Physcal, Excl, Space, Cogmech, Home,
Sleep, Tentat, Assent, / Article, Optim, Fillers, Senses, Hear, We, Feel, Inhib, Incl, You, School, Money,
Occup, Job, Number
Family v. all Other, Past, Assent, Sleep, Insight, I, Pronoun, Cogmech, Tentat, Motion, Self / Affect, Optim, Certain,
Future, School, Comm, Job, We, Preps, Incl, Occup, You, Number
Family v. res. Other, Past, Sleep, Pronoun, Tentat, Cogmech, Insight, Humans / Comm, We, Incl, You, Preps, Number
Familiar v. other Other, Assent, Past, I, Leisure, Self, Insight / Fillers, Certain, Social, Posemo, We, Future, Affect, Incl,
Comm, Achieve, School, You, Optim, Job, Occup
</reference>
<tableCaption confidence="0.98364225">
Table 6: Per-topic word distribution learned using unsu-
pervised clustering with LDA. Words are sorted accord-
ing to their posterior topic distribution. Words with iden-
tical distributions are sorted alphabetically.
</tableCaption>
<bodyText confidence="0.9446655">
Topic 1 Topic 2
Invalid, helpline, eligibility, Adorable, aeroplanes,
transactions, promo- Arlene, Astoria, baked,
tional, representative, biscuits, bitches, blisters,
mastercard, touchtone, bluegrass, bracelet, brains,
activation, nominating, bushes, calorie, casinos,
receiver, voicemail, digit, Charlene, cheeses, chit,
representatives, Chrysler, Chris, clam, clientele,
ballots, staggering, refills, cock, cookie, copying,
resented, classics, metro, crab, Davenport, debating,
represented, administer, dementia, dictionary, dime,
transfers, reselling, recom- Disneyland, eek, Eileen,
mendations, explanation, fascinated, follies, fry,
floral, exclusive, submit. gained.
utility in discrimination. To directly test discrimi-
nation performance, we use support vector machine
classifiers. Before performing classification, we pro-
duce balanced datasets that have equal numbers of
conversations for each category. Our primary moti-
vation for artificially balancing the label distribution
in each experiment is to provide a consistent base-
line over which each classifier may be compared.
We learn SVM classifiers with an RBF kernel us-
ing 85% of data for development. SVM parameters
are tuned with 20-fold cross-validation on the dev-
set. The accuracies of the classifiers, measured on a
held out set, are reported in Table 7.
We tested four feature vectors: 1) unigram fre-
quencies, 2) surface language features (log word
count, speaking rate, entropy), 3) the 64 dimension
LIWC frequency vector and 4) a 30-dimension vec-
tor of LDA topic posterior log-probabilities.
</bodyText>
<tableCaption confidence="0.908940666666667">
Table 7: SVM performance for the language features. La-
bels: a) unigram vector, b) lexical statistics, c) LIWC and
d) LDA topic posterior log-probabilities
</tableCaption>
<table confidence="0.8404544">
Task 1-grams L.Stats LIWC LDA
Res. v. biz. 84.95 67.61 78.70 81.03
Family v. all 78.03 61.16 72.77 74.75
Family v. res. 76.13 62.92 71.06 72.37
Familiarity 69.17 60.92 64.20 69.56
</table>
<bodyText confidence="0.999657230769231">
Overall, the plain unigram frequency vector pro-
vided the best discrimination performance. How-
ever, this comes at significant training costs as
the unigram feature vector has a dimensionality
of approximately 20,000. While the surface fea-
tures did possess a degree of classification utility,
there are clearly outclassed by the content-based
features. Furthermore, their integration into the
content-features yielded only insignificant improve-
ments to accuracy. Finally, it is of interest to note
that the 30-topic LDA feature trained with ML cri-
terion outperformed the 64-topic LIWC vector in all
cases.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999935636363636">
This paper studies a unique corpus of conversational
telephone speech, a comprehensive and naturalis-
tic sample of all the incoming and outgoing tele-
phone calls from 10 older adults over the duration
of one year. Through empirical experiments we
show that the business calls can be separated from
social calls with accuracies as high as 85% using
standard techniques. Subgroups such as family can
also be differentiated automatically with accuracies
above 74%. When compared to language use (en-
tropy) and hand-crafted dictionaries (LIWC), poste-
</bodyText>
<page confidence="0.995448">
118
</page>
<bodyText confidence="0.999778571428572">
riors over topics computed using a latent Dirichlet
model provide superior performance.
For the elderly demographic, openings of conver-
sations were found to be more informative in clas-
sifying conversation than closings or random seg-
ments, when using automated transcripts. The high
accuracy in classifying business from personal con-
versations suggests potential applications in design-
ing context user interface for smartphones to offer
icons related to work email, work calendar or Face-
book apps. In future work, we plan to examine
subject specific language use, turn taking and af-
fect to further improve the classification of social
calls (Shafran et al., 2003).
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999409">
This research was supported in part by NIH Grants
5K25AG033723-02 and P30 AG024978-05 and
NSF Awards 1027834, 0958585 and 0905095. Any
opinions, findings, conclusions or recommendations
expressed in this publication are those of the authors
and do not reflect the views of the NIH or NSF. We
thank Brian Kingsbury and IBM for making their
ASR software tools available to us. We are also
grateful to Nicole Larimer, Maider Lehr and Kather-
ine Wild for their contributions to data collection.
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999671108108108">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.
T A Glass, C F Mendes de Leon, T E Seeman, and L F
Berkman. 1997. Beyond single indicators of social
networks: a lisrel analysis of social ties among the el-
derly. Soc Sci Med, 44(10):1503–1517.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In IEEE International Con-
ference on Acoustics, Speech, and Signal Processing,
pages 517–520.
T. Joachims. 2006. Training linear svms in linear
time. In ACM Conference on Knowledge Discovery
and Data Mining.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is twitter, a social network or
a news media? In Proceedings of the 19th inter-
national conference on World wide web, WWW ’10,
pages 591–600, New York, NY, USA. ACM.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC’10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
J. W. Pennebaker, M. R. Mehl, and K. G. Niederhoffer.
2003. Psychological aspects of natural language use:
Our words, our selves. Annual Review of Psychology,
54(1):547–577.
Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with applica-
tion to twitter. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ’10, pages 181–189, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT ’10, pages 172–180,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Harvey Sacks, Emanuel A. Schegloff, and Gail Jeffer-
son. 1974. A simplest systematics for the organization
of turn-taking for conversation language. Language,
50(4(1)):696–735.
Emanuel A. Schegloff and Harvey Sacks. 1973. Opening
up closings. Semiotica, 8:289–327.
Emanuel A. Schegloff. 1968. Sequencing in con-
versational openings. American Anthropologist,
70(6):1075–1095.
Izhak Shafran, Michael Riley, and Mehryar Mohri. 2003.
Voice signatures. In Proc. IEEE Automatic Speech
Recognition and Understanding Workshop.
H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
and G. Zweig. 2005. The IBM 2004 conversational
telephony system for rich transcription. In IEEE Inter-
national Conference on Acoustics, Speech, and Signal
Processing, volume 1, pages 205–208.
Anthony Stark, Izhak Shafran, and Jeffrey Kaye. 2011.
Supervised and unsupervised feature selection for in-
ferring social nature of telephone conversations from
their content. In Proc. IEEE Automatic Speech Recog-
nition and Understanding Workshop.
Paul Taylor, Rich Morin, Kim Parker, D’Vera Cohn,
and Wendy Wang. June 29, 2009. Grow-
ing old in America: Expectations vs. reality.
http://pewsocialtrends.org/files/2010/10/Getting-Old-
in-America.pdf.
</reference>
<page confidence="0.999074">
119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.904352">
<title confidence="0.991453">Hello, Who is Calling?: Can Words Reveal the Social Nature Conversations?</title>
<author confidence="0.996848">Anthony Stark</author>
<author confidence="0.996848">Izhak Shafran</author>
<author confidence="0.996848">Jeffrey Kaye</author>
<affiliation confidence="0.927835">Center for Spoken Language Understanding, OHSU, Portland</affiliation>
<abstract confidence="0.99964784375">This study aims to infer the social nature of conversations from their content automatically. To place this work in context, our motivation stems from the need to understand how social disengagement affects cognitive decline or depression among older adults. For this purpose, we collected a comprehensive and naturalistic corpus comprising of all the incoming and outgoing telephone calls from 10 subjects over the duration of a year. As a first step, we learned a binary classifier to filter out business related conversation, achieving an accuracy of about 85%. This classification task provides a convenient tool to probe the nature of telephone conversations. We evaluated the utility of openings and closing in differentiating personal calls, and find that empirical results on a large corpus do not support the hypotheses by Schegloff and Sacks that personal conversations are marked by unique closing structures. For classifying different types of social relationships such as family vs other, we investigated features related to language use (entropy), hand-crafted dictionary (LIWC) and topics learned using unsupervised latent Dirichlet models (LDA). Our results show that the posteriors over topics from LDA provide consistently higher accuracy (60-81%) compared to LIWC or language use features in distinguishing different types of conversations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>v biz I</author>
<author>Past</author>
</authors>
<location>Self, Motion, Other, Insight, Eating, Pronoun, Down, Physcal, Excl, Space, Cogmech, Home, Sleep, Tentat, Assent, / Article, Optim, Fillers, Senses, Hear, We, Feel, Inhib, Incl, You, School, Money, Occup, Job, Number</location>
<marker>I, Past, </marker>
<rawString>Relationship Categories Res. v. biz. I, Past, Self, Motion, Other, Insight, Eating, Pronoun, Down, Physcal, Excl, Space, Cogmech, Home, Sleep, Tentat, Assent, / Article, Optim, Fillers, Senses, Hear, We, Feel, Inhib, Incl, You, School, Money, Occup, Job, Number</rawString>
</citation>
<citation valid="false">
<authors>
<author>Family v all Other</author>
<author>Assent Past</author>
<author>Insight Sleep</author>
<author>I Pronoun</author>
</authors>
<location>Cogmech, Tentat, Motion, Self / Affect, Optim, Certain, Future, School, Comm, Job, We, Preps, Incl, Occup, You, Number</location>
<marker>Other, Past, Sleep, Pronoun, </marker>
<rawString>Family v. all Other, Past, Assent, Sleep, Insight, I, Pronoun, Cogmech, Tentat, Motion, Self / Affect, Optim, Certain, Future, School, Comm, Job, We, Preps, Incl, Occup, You, Number</rawString>
</citation>
<citation valid="false">
<authors>
<author>Past Other</author>
<author>Pronoun Sleep</author>
<author>Tentat</author>
</authors>
<title>Cogmech, Insight, Humans / Comm, We, Incl, You, Preps, Number Familiar v. other Other,</title>
<location>Assent, Past, I, Leisure, Self, Insight / Fillers, Certain, Social, Posemo, We, Future, Affect, Incl, Comm, Achieve, School, You, Optim, Job, Occup</location>
<marker>Other, Sleep, Tentat, </marker>
<rawString>Family v. res. Other, Past, Sleep, Pronoun, Tentat, Cogmech, Insight, Humans / Comm, We, Incl, You, Preps, Number Familiar v. other Other, Assent, Past, I, Leisure, Self, Insight / Fillers, Certain, Social, Posemo, We, Future, Affect, Incl, Comm, Achieve, School, You, Optim, Job, Occup</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="22459" citStr="Blei et al., 2003" startWordPosition="3548" endWordPosition="3551">ying utterances. Table 5 lists the LIWC categories significant (p&lt; 0.001) to the different classes. The listed terms are sorted according to their t-statistic, with early and later terms more indicative of first and second class labels respectively. 5.3 Latent Dirichlet allocation Unsupervised clustering and feature selection can make use of data for which we have no labels. For example, in the case of business and residential labels, unlabeled data amounts to about 50% of our corpus. Motivated by this consideration, we examined unsupervised clustering using Latent Dirichlet Allocation (LDA) (Blei et al., 2003). LDA models a conversation as a bag of words. The model generates a conversation by: (a) sampling a topic distribution 0 for the conversation using a per-conversation Dirichlet topic distribution with a hyper-parameter α, (b) sampling a topic z for each word in the conversation using a multinomial distribution using the topic mixture 0, and (c) sampling the word from a per-topic multinomial word distribution with a hyper-parameter Q (Blei et al., 2003). The number of topics are assumed to be given. The per-conversation topic distribution and the per-topic word distribution can be automaticall</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A Glass</author>
<author>C F Mendes de Leon</author>
<author>T E Seeman</author>
<author>L F Berkman</author>
</authors>
<title>Beyond single indicators of social networks: a lisrel analysis of social ties among the elderly.</title>
<date>1997</date>
<journal>Soc Sci Med,</journal>
<volume>44</volume>
<issue>10</issue>
<marker>Glass, de Leon, Seeman, Berkman, 1997</marker>
<rawString>T A Glass, C F Mendes de Leon, T E Seeman, and L F Berkman. 1997. Beyond single indicators of social networks: a lisrel analysis of social ties among the elderly. Soc Sci Med, 44(10):1503–1517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Godfrey</author>
<author>E Holliman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>517--520</pages>
<contexts>
<context position="4668" citStr="Godfrey et al., 1992" startWordPosition="710" endWordPosition="713"> speech recognition (ASR) systems can now transcribe telephone conversations with sufficient accuracy for useful automated analysis. Given the above premise, we focus our attention on studying social interactions of older adults over land-line telephones. To facilitate such a study, we collected telephone conversations from several older adults for approximately one year. Note that our corpus is unlike the publicly available Switchboard and Fisher corpora, which contain conversations between unfamiliar speakers discussing a topic from a pre-determined list such as music, crime, air pollution (Godfrey et al., 1992). In contrast, the conversations in our corpus are completely natural, covering a wide range of topics, conversational partners and types of interactions. Our corpus is also comprehensive in that it includes all the outgoing/incoming calls from subjects’ homes during the observation period. As a step toward understanding social networks and associated relationships, our first task was to classify social and non-social (business) conversations. While reverse listing was useful to a certain extent, we were unable to find listing on up to 50% of the calls in our corpus due to lack of caller ID in</context>
<context position="8681" citStr="Godfrey et al., 1992" startWordPosition="1348" endWordPosition="1352"> reverse listing was either inconclusive or for which the phone number information was missing and/or improperly recorded. 113 3 Automatic Speech Recognition Conversations in our corpus were automatically transcribed using an ASR system. Our ASR system is structured after IBM’s conversation telephony system which gave the top performance in the most recent evaluation of speech recognition technology for telephony by National Institute of Standards and Technology (Soltau et al., 2005). The acoustic models were trained on about 2000 hours of telephone speech from Switchboard and Fisher corpora (Godfrey et al., 1992). The system has a vocabulary of 47K and uses a trigram language model with about 10M n-grams, estimated from a mix of transcripts and web-harvested data. Decoding is performed in three stages using speaker-independent models, vocal-tract normalized models and speaker-adapted models. The three sets of models are similar in complexity with 4000 clustered pentaphone states and 150K Gaussians with diagonal covariances. Our system does not include discriminative training and performs at a word error rate of about 24% on NIST RT Dev04 which is comparable to state of the art performance for such sys</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J. Godfrey, E. Holliman, and J. McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. In IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 517–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Training linear svms in linear time.</title>
<date>2006</date>
<booktitle>In ACM Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="17205" citStr="Joachims, 2006" startWordPosition="2706" endWordPosition="2707"> know that openings are good predictors of the type of conversations yet to unfold. We also know that there are inherent language differences between short and long conversations. So, it is natural to ask whether openings can predict relative lengths of conversations. To test this hypothesis, we bin conversations into 5 groups or ranks based on their percentile lengths (word counts) – very short, short, moderate, long and very long durations, as in Table 2. Using independent features from the 30-word opening, we attempt to predict the relative rank of two conversations by learning a rank SVM (Joachims, 2006). We found the ranker to give 27% error rate, significantly lower (independent sample t-test, d.f. Pz� 1M, p&lt;0.01) than the random chance of 40%. Chance baseline was determined using Monte Carlo simulation (1M random rankings) in conjunction with the rank SVM evaluation (Joachims, 2006). Features from very short conversations may contain both openings and closings, i.e., both a hello and a goodbye, making them easier to rank. To avoid this confounding factor, we also compute performance after discarding the shortest grouping of conversations (&lt; 88 words) to ensure closings are avoided in the 3</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>T. Joachims. 2006. Training linear svms in linear time. In ACM Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haewoon Kwak</author>
<author>Changhyun Lee</author>
<author>Hosung Park</author>
<author>Sue Moon</author>
</authors>
<title>What is twitter, a social network or a news media?</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web, WWW ’10,</booktitle>
<pages>591--600</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1841" citStr="Kwak et al., 2010" startWordPosition="279" endWordPosition="282">ther, we investigated features related to language use (entropy), hand-crafted dictionary (LIWC) and topics learned using unsupervised latent Dirichlet models (LDA). Our results show that the posteriors over topics from LDA provide consistently higher accuracy (60-81%) compared to LIWC or language use features in distinguishing different types of conversations. 1 Introduction In recent years, there has been a growing interest in analyzing text in informal interactions such as in Internet chat, newsgroups and twitter. The emphasis of most such research has been in estimating network structure (Kwak et al., 2010) and detecting trending topics (Ritter et al., 2010), sentiments (Pak and Paroubek, 2010) and first stories (Petrovi´c et al., 2010). The focus has been on aggregating information from large number of users to analyze population level statistics. The study reported in this paper, in contrast, focuses on understanding the social interactions of an individual over long periods of time. Our motivation stems from the need to understand the factors of social engagement that ameliorate the rate of cognitive decline and depression in older adults. Since the early work of Glass (1997) and colleagues, </context>
</contexts>
<marker>Kwak, Lee, Park, Moon, 2010</marker>
<rawString>Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue Moon. 2010. What is twitter, a social network or a news media? In Proceedings of the 19th international conference on World wide web, WWW ’10, pages 591–600, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="1930" citStr="Pak and Paroubek, 2010" startWordPosition="292" endWordPosition="295">onary (LIWC) and topics learned using unsupervised latent Dirichlet models (LDA). Our results show that the posteriors over topics from LDA provide consistently higher accuracy (60-81%) compared to LIWC or language use features in distinguishing different types of conversations. 1 Introduction In recent years, there has been a growing interest in analyzing text in informal interactions such as in Internet chat, newsgroups and twitter. The emphasis of most such research has been in estimating network structure (Kwak et al., 2010) and detecting trending topics (Ritter et al., 2010), sentiments (Pak and Paroubek, 2010) and first stories (Petrovi´c et al., 2010). The focus has been on aggregating information from large number of users to analyze population level statistics. The study reported in this paper, in contrast, focuses on understanding the social interactions of an individual over long periods of time. Our motivation stems from the need to understand the factors of social engagement that ameliorate the rate of cognitive decline and depression in older adults. Since the early work of Glass (1997) and colleagues, several studies on large cohorts over extended duration have confirmed that older adults </context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>M R Mehl</author>
<author>K G Niederhoffer</author>
</authors>
<title>Psychological aspects of natural language use: Our words, our selves.</title>
<date>2003</date>
<journal>Annual Review of Psychology,</journal>
<volume>54</volume>
<issue>1</issue>
<contexts>
<context position="21273" citStr="Pennebaker et al., 2003" startWordPosition="3357" endWordPosition="3360">deas of language use over the telephone. It is shown that people talk longer, more rapidly and have wider range of language use when conversing with a familiar contact and/or family member. Surprisingly, only the speaking rate showed significant differences among the residential/business categories, with business conversations being conducted at a slower pace at least for the elderly demographic in our corpus. 5.2 Linguistic inquiry and Word Count We investigated a hand-crafted dictionary of salient words, called Linguistic Inquiry and Word Count (LIWC), employed in social psychology studies (Pennebaker et al., 2003). This dictionary group words into 64 categories such as pronouns, activity words, positive emotion and health. The categories have significant overlap and a given word can map to zero or more categories. The clear benefit of LIWC is that the word categories have very clear and pre-labeled meanings. They suffer from the obvious drawback that the words are labeled in isolation without taking their context into account. The tags are not chosen under any mathematical criteria and so there are no guarantees the resultant feature will be useful or optimal for classifying utterances. Table 5 lists t</context>
</contexts>
<marker>Pennebaker, Mehl, Niederhoffer, 2003</marker>
<rawString>J. W. Pennebaker, M. R. Mehl, and K. G. Niederhoffer. 2003. Psychological aspects of natural language use: Our words, our selves. Annual Review of Psychology, 54(1):547–577.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇsa Petrovi´c</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Streaming first story detection with application to twitter.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>181--189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Petrovi´c, Osborne, Lavrenko, 2010</marker>
<rawString>Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to twitter. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 181–189, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>172--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1893" citStr="Ritter et al., 2010" startWordPosition="287" endWordPosition="290"> use (entropy), hand-crafted dictionary (LIWC) and topics learned using unsupervised latent Dirichlet models (LDA). Our results show that the posteriors over topics from LDA provide consistently higher accuracy (60-81%) compared to LIWC or language use features in distinguishing different types of conversations. 1 Introduction In recent years, there has been a growing interest in analyzing text in informal interactions such as in Internet chat, newsgroups and twitter. The emphasis of most such research has been in estimating network structure (Kwak et al., 2010) and detecting trending topics (Ritter et al., 2010), sentiments (Pak and Paroubek, 2010) and first stories (Petrovi´c et al., 2010). The focus has been on aggregating information from large number of users to analyze population level statistics. The study reported in this paper, in contrast, focuses on understanding the social interactions of an individual over long periods of time. Our motivation stems from the need to understand the factors of social engagement that ameliorate the rate of cognitive decline and depression in older adults. Since the early work of Glass (1997) and colleagues, several studies on large cohorts over extended durat</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 172–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harvey Sacks</author>
<author>Emanuel A Schegloff</author>
<author>Gail Jefferson</author>
</authors>
<title>A simplest systematics for the organization of turn-taking for conversation language.</title>
<date>1974</date>
<journal>Language,</journal>
<volume>50</volume>
<issue>4</issue>
<contexts>
<context position="12588" citStr="Sacks et al., 1974" startWordPosition="1957" endWordPosition="1960">ificantly. Notably, the conversations with family have a significantly higher mean and a tighter variance than with other social ties. Table 1: SVM scores for phone number sub-categories. Category # Calls Mean score STD Family 1162 1.12 0.50 Friends 532 0.95 0.51 Residential 2728 0.93 0.63 Business 1095 -1.16 0.70 Global 8558 0.46 0.96 114 Res/biz classification errror (%) 4.3 How Informative are Openings and Closings in Differentiating Telephone Conversations? Schegloff and Sacks assert openings (beginnings) and closings (ends) of telephone conversations have certain identifiable structures (Sacks et al., 1974). For example, the structure of openings facilitate establishing identity of the conversants and the purpose of their call (Schegloff, 1968). Closings in personal conversations are likely to include a preclosing signal that allows either party to mention any unmentioned mentionables before conversation ends (Schegloff and Sacks, 1973). Given the above assertions, we expect openings and closings to be informative about the type of conversations. Using our classifier, we compare the accuracy of predicting the type from openings, closings and random segments of the conversations. For different le</context>
</contexts>
<marker>Sacks, Schegloff, Jefferson, 1974</marker>
<rawString>Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson. 1974. A simplest systematics for the organization of turn-taking for conversation language. Language, 50(4(1)):696–735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
<author>Harvey Sacks</author>
</authors>
<title>Opening up closings.</title>
<date>1973</date>
<journal>Semiotica,</journal>
<pages>8--289</pages>
<contexts>
<context position="12924" citStr="Schegloff and Sacks, 1973" startWordPosition="2007" endWordPosition="2010">.46 0.96 114 Res/biz classification errror (%) 4.3 How Informative are Openings and Closings in Differentiating Telephone Conversations? Schegloff and Sacks assert openings (beginnings) and closings (ends) of telephone conversations have certain identifiable structures (Sacks et al., 1974). For example, the structure of openings facilitate establishing identity of the conversants and the purpose of their call (Schegloff, 1968). Closings in personal conversations are likely to include a preclosing signal that allows either party to mention any unmentioned mentionables before conversation ends (Schegloff and Sacks, 1973). Given the above assertions, we expect openings and closings to be informative about the type of conversations. Using our classifier, we compare the accuracy of predicting the type from openings, closings and random segments of the conversations. For different lengths of the three types of segments, the observed performance of the classifier is plotted in Figure 1. The results for the random segment were computed by averaging over 100 trials. Several important results are immediately apparent. Openings possess much higher utility than closings. This is consistent with general intuition that t</context>
</contexts>
<marker>Schegloff, Sacks, 1973</marker>
<rawString>Emanuel A. Schegloff and Harvey Sacks. 1973. Opening up closings. Semiotica, 8:289–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
</authors>
<title>Sequencing in conversational openings.</title>
<date>1968</date>
<journal>American Anthropologist,</journal>
<volume>70</volume>
<issue>6</issue>
<contexts>
<context position="12728" citStr="Schegloff, 1968" startWordPosition="1981" endWordPosition="1982"> SVM scores for phone number sub-categories. Category # Calls Mean score STD Family 1162 1.12 0.50 Friends 532 0.95 0.51 Residential 2728 0.93 0.63 Business 1095 -1.16 0.70 Global 8558 0.46 0.96 114 Res/biz classification errror (%) 4.3 How Informative are Openings and Closings in Differentiating Telephone Conversations? Schegloff and Sacks assert openings (beginnings) and closings (ends) of telephone conversations have certain identifiable structures (Sacks et al., 1974). For example, the structure of openings facilitate establishing identity of the conversants and the purpose of their call (Schegloff, 1968). Closings in personal conversations are likely to include a preclosing signal that allows either party to mention any unmentioned mentionables before conversation ends (Schegloff and Sacks, 1973). Given the above assertions, we expect openings and closings to be informative about the type of conversations. Using our classifier, we compare the accuracy of predicting the type from openings, closings and random segments of the conversations. For different lengths of the three types of segments, the observed performance of the classifier is plotted in Figure 1. The results for the random segment </context>
</contexts>
<marker>Schegloff, 1968</marker>
<rawString>Emanuel A. Schegloff. 1968. Sequencing in conversational openings. American Anthropologist, 70(6):1075–1095.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Izhak Shafran</author>
<author>Michael Riley</author>
<author>Mehryar Mohri</author>
</authors>
<title>Voice signatures.</title>
<date>2003</date>
<booktitle>In Proc. IEEE Automatic Speech Recognition and Understanding Workshop.</booktitle>
<marker>Shafran, Riley, Mohri, 2003</marker>
<rawString>Izhak Shafran, Michael Riley, and Mehryar Mohri. 2003. Voice signatures. In Proc. IEEE Automatic Speech Recognition and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Soltau</author>
<author>B Kingsbury</author>
<author>L Mangu</author>
<author>D Povey</author>
<author>G Saon</author>
<author>G Zweig</author>
</authors>
<title>conversational telephony system for rich transcription.</title>
<date>2005</date>
<booktitle>The IBM</booktitle>
<volume>1</volume>
<pages>205--208</pages>
<contexts>
<context position="8548" citStr="Soltau et al., 2005" startWordPosition="1326" endWordPosition="1329">om multiple sources; e.g. phone directory lookup, exit interviews, internet lookup. This left 4,395 unlabeled records, for which the reverse listing was either inconclusive or for which the phone number information was missing and/or improperly recorded. 113 3 Automatic Speech Recognition Conversations in our corpus were automatically transcribed using an ASR system. Our ASR system is structured after IBM’s conversation telephony system which gave the top performance in the most recent evaluation of speech recognition technology for telephony by National Institute of Standards and Technology (Soltau et al., 2005). The acoustic models were trained on about 2000 hours of telephone speech from Switchboard and Fisher corpora (Godfrey et al., 1992). The system has a vocabulary of 47K and uses a trigram language model with about 10M n-grams, estimated from a mix of transcripts and web-harvested data. Decoding is performed in three stages using speaker-independent models, vocal-tract normalized models and speaker-adapted models. The three sets of models are similar in complexity with 4000 clustered pentaphone states and 150K Gaussians with diagonal covariances. Our system does not include discriminative trai</context>
</contexts>
<marker>Soltau, Kingsbury, Mangu, Povey, Saon, Zweig, 2005</marker>
<rawString>H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon, and G. Zweig. 2005. The IBM 2004 conversational telephony system for rich transcription. In IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 205–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Stark</author>
<author>Izhak Shafran</author>
<author>Jeffrey Kaye</author>
</authors>
<title>Supervised and unsupervised feature selection for inferring social nature of telephone conversations from their content.</title>
<date>2011</date>
<booktitle>In Proc. IEEE Automatic Speech Recognition and Understanding Workshop.</booktitle>
<contexts>
<context position="19080" citStr="Stark et al., 2011" startWordPosition="2998" endWordPosition="3001">hey are not particularly useful in classifying the calls with high accuracy. In this section, we investigate the performance of classifiers to differentiate the following binary classes. • Residential vs business • Family vs all other • Family vs other residential • Familiar vs non-familiar Familiar denotes calls to those numbers with whom subject has conversed more than 5 times. Recall that the numbers corresponding to family members were identified by the subjects in a post-collection interview. We learned binary classifier for the four cases, a few of which were reported in our early work (Stark et al., 2011). We investigated a variety of features in these tasks. A breakdown of the corpus is give in Table 3. Not all categories are mutually exclusive. For example the majority of family conversations also fall into the familiar and residential categories. Table 3: Number of conversations per category. Category Instances Biz. 1095 Residential 2728 Family 1111 Res. non-family 1462 Familiar 3010 All 8558 5.1 Lexical Statistics Speakers who share close social ties are likely to engage in conversations on a wide variety of topics and this is likely to reflect in the entropy of their language use. We capt</context>
</contexts>
<marker>Stark, Shafran, Kaye, 2011</marker>
<rawString>Anthony Stark, Izhak Shafran, and Jeffrey Kaye. 2011. Supervised and unsupervised feature selection for inferring social nature of telephone conversations from their content. In Proc. IEEE Automatic Speech Recognition and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Taylor</author>
<author>Rich Morin</author>
<author>Kim Parker</author>
<author>D’Vera Cohn</author>
<author>Wendy Wang</author>
</authors>
<title>Growing old in America: Expectations vs.</title>
<date>2009</date>
<note>reality. http://pewsocialtrends.org/files/2010/10/Getting-Oldin-America.pdf.</note>
<marker>Taylor, Morin, Parker, Cohn, Wang, 2009</marker>
<rawString>Paul Taylor, Rich Morin, Kim Parker, D’Vera Cohn, and Wendy Wang. June 29, 2009. Growing old in America: Expectations vs. reality. http://pewsocialtrends.org/files/2010/10/Getting-Oldin-America.pdf.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>