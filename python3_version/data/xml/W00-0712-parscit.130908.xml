<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003222">
<note confidence="0.831749">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 67-72, Lisbon, Portugal, 2000.
</note>
<title confidence="0.8977885">
Knowledge-Free Induction of Morphology
Using Latent Semantic Analysis
</title>
<author confidence="0.997268">
Patrick Schone and Daniel Jurafsky
</author>
<affiliation confidence="0.999472">
University of Colorado
</affiliation>
<address confidence="0.624776">
Boulder, Colorado 80309
</address>
<email confidence="0.995216">
Ischone, jurafskyl@cs.colorado.edu
</email>
<sectionHeader confidence="0.989388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999025">
Morphology induction is a subproblem of
important tasks like automatic learning of
machine-readable dictionaries and grammar in-
duction. Previous morphology induction ap-
proaches have relied solely on statistics of hy-
pothesized stems and affixes to choose which
affixes to consider legitimate. Relying on stem-
and-affix statistics rather than semantic knowl-
edge leads to a number of problems, such as the
inappropriate use of valid affixes (&amp;quot;ally&amp;quot; stem-
ming to &amp;quot;all&amp;quot;). We introduce a semantic-based
algorithm for learning morphology which only
proposes affixes when the stem and stem-plus-
affix are sufficiently similar semantically. We
implement our approach using Latent Seman-
tic Analysis and show that our semantics-only
approach provides morphology induction results
that rival a current state-of-the-art system.
</bodyText>
<sectionHeader confidence="0.996384" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995265267857143">
Computational morphological analyzers have
existed in various languages for years and it has
been said that &amp;quot;the quest for an efficient method
for the analysis and generation of word-forms is
no longer an academic research topic&amp;quot; (Karlsson
and Karttunen, 1997). However, development
of these analyzers typically begins with human
intervention requiring time spans from days to
weeks. If it were possible to build such ana-
lyzers automatically without human knowledge,
significant development time could be saved.
On a larger scale, consider the task
of inducing machine-readable dictionaries
(MRDs) using no human-provided information
(&amp;quot;knowledge-free&amp;quot;). In building an MRD,
&amp;quot;simply expanding the dictionary to encompass
every word one is ever likely to encounter.. .fails
to take advantage of regularities&amp;quot; (Sproat,
1992, p. xiii). Hence, automatic morphological
analysis is also critical for selecting appropriate
and non-redundant MRD headwords.
For the reasons expressed above, we are in-
terested in knowledge-free morphology induc-
tion. Thus, in this paper, we show how to au-
tomatically induce morphological relationships
between words.
Previous morphology induction approaches
(Goldsmith, 1997, 2000; Mean, 1998; Gauss-
ier, 1999) have focused on inflectional languages
and have used statistics of hypothesized stems
and affixes to choose which affixes to consider
legitimate. Several problems can arise using
only stem-and-affix statistics: (1) valid affixes
may be applied inappropriately (&amp;quot;ally&amp;quot; stem-
ming to &amp;quot;all&amp;quot;), (2) morphological ambiguity
may arise (&amp;quot;rating&amp;quot; conflating with &amp;quot;rat&amp;quot; in-
stead of &amp;quot;rate&amp;quot;), and (3) non-productive affixes
may get accidentally pruned (the relationship
between &amp;quot;dirty&amp;quot; and &amp;quot;dirt&amp;quot; may be lost).1
Some of these problems could be resolved
if one could incorporate word semantics. For
instance, &amp;quot;all&amp;quot; is not semantically similar to
&amp;quot;ally,&amp;quot; so with knowledge of semantics, an algo-
rithm could avoid conflating these two words.
To maintain the &amp;quot;knowledge-free&amp;quot; paradigm,
such semantics would need to be automati-
cally induced. Latent Semantic Analysis (LSA)
(Deerwester, et al., 1990); Landauer, et at.,
1998) is a technique which automatically iden-
tifies semantic information from a corpus. We
here show that incorporating LSA-based seman-
tics alone into the morphology-induction pro-
cess can provide results that rival a state-of-
the-art system based on stem-and-affix statis-
tics (Goldsmith&apos;s Linguistica).
lError examples are from Goldsmith&apos;s Linguistica
</bodyText>
<page confidence="0.998795">
67
</page>
<bodyText confidence="0.999800090909091">
Our algorithm automatically extracts poten-
tial affixes from an untagged corpus, identifies
word pairs sharing the same proposed stem but
having different affixes, and uses LSA to judge
semantic relatedness between word pairs. This
process serves to identify valid morphological re-
lations. Though our algorithm could be applied
to any inflectional language, we here restrict
it to English in order to perform evaluations
against the human-labeled CELEX database
(Baayen, et al., 1993).
</bodyText>
<sectionHeader confidence="0.994275" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.999967760869565">
Existing induction algorithms all focus on iden-
tifying prefixes, suffixes, and word stems in in-
flectional languages (avoiding infixes and other
language types like concatenative or aggluti-
native languages (Sproat, 1992)). They also
observe high frequency occurrences of some
word endings or beginnings, perform statistics
thereon, and propose that some of these ap-
pendages are valid morphemes.
However, these algorithms differ in specifics.
DeJean (1998) uses an approach derived from
Harris (1951) where word-splitting occurs if the
number of distinct letters that follows a given
sequence of characters surpasses a threshold.
He uses these hypothesized affixes to resegment
words and thereby identify additional affixes
that were initially overlooked. His overall goal is
different from ours: he primarily seeks an affix
inventory.
Goldsmith (1997) tries cutting each word
in exactly one place based on probability and
lengths of hypothesized stems and affixes. He
applies the EM algorithm to eliminate inappro-
priate parses. He collects the possible suffixes
for each stem calling these a signature which
aid in determining word classes. Goldsmith
(2000) later incorporates minimum description
length to identify stemming characteristics that
most compress the data, but his algorithm oth-
erwise remains similar in nature. Goldsmith&apos;s
algorithm is practically knowledge-free, though
he incorporates capitalization removal and some
word segmentation.
Gaussier (1999) begins with an inflectional
lexicon and seeks to find derivational morphol-
ogy. The words and parts of speech from his
inflectional lexicon serve for building relational
families of words and identifying sets of word
pairs and suffixes therefrom. Gaussier splits
words based on p-similarity — words that agree
in exactly the first p characters. He also builds
a probabilistic model which indicates that the
probability of two words being morphological
variants is based upon the probability of their
respective changes in orthography and morpho-
synt act ics .
</bodyText>
<sectionHeader confidence="0.824095" genericHeader="method">
3 Current approach
</sectionHeader>
<bodyText confidence="0.9998685">
Our algorithm also focuses on inflectional lan-
guages. However, with the exception of word
segmentation, we provide it no human informa-
tion and we consider only the impact of seman-
tics. Our approach (see Figure 1) can be de-
composed into four components: (1) initially
selecting candidate affixes, (2) identifying af-
fixes which are potential morphological vari-
ants of each other, (3) computing semantic vec-
tors for words possessing these candidate affixes,
and (4) selecting as valid morphological variants
those words with similar semantic vectors.
</bodyText>
<figureCaption confidence="0.997744">
Figure 1: Processing Architecture
</figureCaption>
<subsectionHeader confidence="0.999951">
3.1 Hypothesizing affixes
</subsectionHeader>
<bodyText confidence="0.911650863636364">
To select candidate affixes, we, like Gaussier,
identify p-similar words. We insert words into a
trie (Figure 2) and extract potential affixes by
observing those places in the trie where branch-
ing occurs. Figure 2&apos;s hypothesized suffixes are
NULL, &amp;quot;s,&amp;quot; &amp;quot;ed,&amp;quot; &amp;quot;es,&amp;quot; &amp;quot;ing,&amp;quot; &amp;quot;e,&amp;quot; and &amp;quot;eful.&amp;quot;
We retain only the K most-frequent candidate
affixes for subsequent processing. The value for
K needs to be large enough to account for the
number of expected regular affixes in any given
language as well as some of the more frequent
irregular affixes. We arbitrarily chose K to be
200 in our system. (It should also be mentioned
that we can identify potential prefixes by insert-
ing words into the trie in reversed order. This
prefix mode can additionally serve for identify-
ing capitalization.)
Stage 3 Stage 4
ind wo-r\
pairs that
are possible
morpho-
</bodyText>
<figure confidence="0.995310833333333">
semantic
ll
\.y..arianogicat,/ ..N.,rectors.)
Stage 1
Identify
potential
affixes
Stage 2
rDevelo (—Selec
semanticvariants
vectors that have
for similar
</figure>
<page confidence="0.812398">
68
</page>
<figureCaption confidence="0.998336">
Figure 2: Trie structure
</figureCaption>
<subsectionHeader confidence="0.999838">
3.2 Morphological variants
</subsectionHeader>
<bodyText confidence="0.999977352941176">
We next identify pairs of candidate affixes that
descend from a common ancestor node in the
trie. For example, (&amp;quot;s&amp;quot;, NULL) constitutes such
a pair from Figure 2. We call these pairs rules.
Two words sharing the same root and the
same affix rule, such as &amp;quot;cars&amp;quot; and &amp;quot;car,&amp;quot; form
what we call a pair of potential morphological
variants (PPMVs). We define the ruleset of a
given rule to be the set of all PPM Vs that have
that rule in common. For instance, from Figure
2, the ruleset for (&amp;quot;s&amp;quot;, NULL) would be the pairs
&amp;quot;cars/car&amp;quot; and &amp;quot;cares/care.&amp;quot; Our algorithm es-
tablishes a list which identifies the rulesets for
every hypothesized rule extracted from the data
and then it must proceed to determine which
rulesets or PPM Vs describe true morphological
relationships.
</bodyText>
<subsectionHeader confidence="0.999899">
3.3 Computing Semantic Vectors
</subsectionHeader>
<bodyText confidence="0.999975683333333">
Deerwester, et al. (1990) showed that it is
possible to find significant semantic relation-
ships between words and documents in a corpus
with virtually no human intervention (with the
possible exception of a human-built stop word
list). This is typically done by applying singu-
lar value decomposition (SVD) to a matrix, M,
where each entry M(i,j) contains the frequency
of word i as seen in document j of the corpus.
This methodology is referred to as Latent Se-
mantic Analysis (LSA) and is well-described in
the literature (Landauer, et al., 1998; Manning
and Schiitze, 1999).
SVDs seek to decompose a matrix A into the
product of three matrices U, D, and VT where
U and VT are orthogonal matrices and D is
a diagonal matrix containing the singular val-
ues (squared eigenvalues) of A. Since SVD&apos;s
can be performed which identify singular val-
ues by descending order of size (Berry, et al.,
1993), LSA truncates after finding the k largest
singular values. This corresponds to projecting
the vector representation of each word into a
k-dimensional subspace whose axes form k (la-
tent) semantic directions. These projections are
precisely the rows of the matrix product UkDk.
A typical k is 300, which is the value we used.
However, we have altered the algorithm some-
what to fit our needs. First, to stay as close to
the knowledge-free scenario as possible, we nei-
ther apply a stopword list nor remove capitaliza-
tion. Secondly, since SVDs are more designed
to work on normally-distributed data (Manning
and Schiitze, 1999, p. 565), we operate on Z-
scores rather than counts. Lastly, instead of
generating a term-document matrix, we build a
term-term matrix.
Schiitze (1993) achieved excellent perfor-
mance at classifying words into quasi-part-
of-speech classes by building and perform-
ing an SVD on an Nx4N term-term matrix,
M(i,Np+j). The indices i and j represent the
top N highest frequency words. The p values
range from 0 to 3 representing whether the word
indexed by j is positionally offset from the word
indexed by i by -2, -1, +1, or +2, respectively.
For example, if &amp;quot;the&amp;quot; and &amp;quot;people&amp;quot; were re-
spectively the 1st and 100th highest frequency
words, then upon seeing the phrase &amp;quot;the peo-
ple,&amp;quot; Schiitze&apos;s approach would increment the
counts of M(1,2N+100) and M(100,N+1).
We used Schfitze&apos;s general framework but tai-
lored it to identify local semantic information.
We built an Nx2N matrix and our p values cor-
respond to those words whose offsets from word
i are in the intervals [-50,-1] and [1,50], respec-
tively. We also reserve the Nth position as a
catch-all position to account for all words that
are not in the top (N-1). An important issue to
resolve is how large should N be. We would like
</bodyText>
<listItem confidence="0.47906">
•
</listItem>
<page confidence="0.994374">
69
</page>
<bodyText confidence="0.999661904761905">
to be able to incorporate semantics for an arbi-
trarily large number of words and LSA quickly
becomes impractical on large sets. Fortunately,
it is possible to build a matrix with a smaller
value of N (say, 2500), perform an SVD thereon,
and then fold in remaining terms (Manning and
Schaze, 1999, p. 563). Since the U and V ma-
trices of an SVD are orthogonal matrices, then
UUT=VVT=I. This implies that AV=UD.
This means that for a new word, w, one can
build a vector a which identifies how w relates
to the top N words according to the p different
conditions described above. For example, if w
were one of the top N words, then a would
simply represent w&apos;s particular row from the A
matrix. The product aw= avk is the projec-
tion of 6T into the k-dimensional latent seman-
tic space. By storing an index to the words of
the corpus as well as a sorted list of these words,
one can efficiently build a set of semantic vec-
tors which includes each word of interest.
</bodyText>
<subsectionHeader confidence="0.966549">
3.4 Statistical Computations
</subsectionHeader>
<bodyText confidence="0.9999012">
Morphologically-related words frequently share
similar semantics, so we want to see how well se-
mantic vectors of PPMVs correlate. If we know
how PPMVs correlate in comparison to other
word pairs from their same rulesets, we can ac-
tually determine the semantic-based probability
that the variants are legitimate. In this section,
we identify a measure for correlating PPMVs
and illustrate how ruleset-based statistics help
identify legitimate PPMVs.
</bodyText>
<subsubsectionHeader confidence="0.418125">
3.4.1 Semantic Correlation of Words
</subsubsectionHeader>
<bodyText confidence="0.789536">
The cosine of the angle between two vectors v1
and v2 is given by,
</bodyText>
<equation confidence="0.9358675">
COS(vi, v2) =
11V 1111 v2 11.
</equation>
<bodyText confidence="0.9904456">
We want to determine the correlation between
each of the words of every PPMV. We use what
we call a normalized cosine score (NCS) as a cor-
relation. To obtain a NCS, we first calculate the
cosine between each semantic vector, nw, and
the semantic vectors from 200 randomly chosen
words. By this means we obtain w&apos;s correlation
mean (p,w) and standard deviation (cru,). If v
is one of w&apos;s variants, then we define the NCS
between nw and Itv to be
</bodyText>
<equation confidence="0.953164">
o. c s(11w, Siv) — Ay ,
min( )•
ye{w,v} 0-Y
</equation>
<tableCaption confidence="0.6791728">
Table 1 provides normalized cosine scores for
several PPMVs from Figure 2 and from among
words listed originally as errors in other sys-
tems. (NCSs are effectively Z-scores.)
able 1: Normalized
</tableCaption>
<table confidence="0.9351654">
PPMVs NCSs PPMVs NCSs
car/cars 5.6 ally/allies 6.5
car/caring -0.71 ally/all -1.3
car/cares -0.14 dirty/dirt 2.4
car/cared -0.96 rating/rate 0.97
</table>
<subsubsectionHeader confidence="0.736135">
3.4.2 Ruleset-level Statistics
</subsubsectionHeader>
<bodyText confidence="0.99920175">
By considering NCSs for all word pairs cou-
pled under a particular rule, we can deter-
mine semantic-based probabilities that indicate
which PPMVs are legitimate. We expect ran-
dom NCSs to be normally-distributed accord-
ing to .A.r(0,1). Given that a particular ruleset
contains nR PPMVs, we can therefore approx-
imate the number (nT), mean (AT) and stan-
dard deviation (o-T) of true correlations. If we
define (I, z(t, a) to be iy e—( .x)2dx, then we
can compute the probability that the particular
correlation is legitimate:
</bodyText>
<equation confidence="0.921705666666667">
Pr (true) = nT (13&apos; z (1LT aT)
(n R — nT) z (0,1) + nT 4z (/LT, T).
3.4.3 Subrules
</equation>
<bodyText confidence="0.999939631578947">
It is possible that a rule can be hypothesized
at the trie stage that is true under only certain
conditions. A prime example of such a rule is
(&amp;quot;es&amp;quot;, NULL). Observe from Table 1 that the
word &amp;quot;cares&amp;quot; poorly correlates with &amp;quot;car.&amp;quot; Yet,
it is true that &amp;quot;-es&amp;quot; is a valid suffix for the words
&amp;quot;flashes,&amp;quot; &amp;quot;catches,&amp;quot; &amp;quot;kisses,&amp;quot; and many other
words where the &amp;quot;-es&amp;quot; is preceded by a voiceless
sibilant.
Hence, there is merit to considering subrules
that arise while performing analysis on a par-
ticular rule. For instance, while evaluating the
(&amp;quot;es&amp;quot;, NULL) rule, it is desirable to also con-
sider potential subrules such as (&amp;quot;ches&amp;quot;, &amp;quot;ch&amp;quot;)
and (&amp;quot;tes&amp;quot;, &amp;quot;t&amp;quot;). One might expect that the av-
erage NCS for the (&amp;quot;ches&amp;quot;, &amp;quot;ch&amp;quot;) subrule might
be higher than the overall rule (&amp;quot;es&amp;quot;, NULL)
whereas the opposite will likely be true for
(&amp;quot;tes&amp;quot;, &amp;quot;t&amp;quot;). Table 2 confirms this.
</bodyText>
<equation confidence="0.880932">
Vi V2
</equation>
<page confidence="0.996613">
70
</page>
<tableCaption confidence="0.991156">
Table 2: Analysis of subrules
</tableCaption>
<table confidence="0.999412166666667">
Rule/Subrule Average StDev #instances
(&amp;quot;es&amp;quot;, NULL) 1.62 2.43 173
(&amp;quot;ches&amp;quot;,&amp;quot;ch&amp;quot;) 2.20 1.66 32
(&amp;quot;shes&amp;quot;, &amp;quot;sh&amp;quot;) 2.39 1.52 15
(&amp;quot;res&amp;quot;, &amp;quot;r&amp;quot;) -0.69 0.47 6
(&amp;quot;t &amp;quot; ,&amp;quot;t&amp;quot; ) -0.58 0.93 11
</table>
<sectionHeader confidence="0.996535" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9999719">
We compare our algorithm to Goldsmith&apos;s Lin-
guistica (2000) by using CELEX&apos;s (Baayen,
et al., 1993) suffixes as a gold standard.
CELEX is a hand-tagged, morphologically-
analyzed database of English words. CELEX
has limited coverage of the words from our data
set (where our data consists of over eight mil-
lion words from random subcollections of TREC
data (Voorhees, et a1,1997/8)), so we only con-
sidered words with frequencies of 10 or more.
</bodyText>
<figureCaption confidence="0.997477">
Figure 3: Morphological directed graphs
</figureCaption>
<bodyText confidence="0.999934823529412">
Morphological relationships can be represented
graphically as directed graphs (see Figure 3,
where three separate graphs are depicted). De-
veloping a scoring algorithm to compare di-
rected graphs is likely to be prone to disagree-
ments. Therefore, we score only the vertex sets
of directed graphs. We will refer to these ver-
tex sets as conflation sets. For example, con-
cern&apos;s conflation set contains itself as well as
&amp;quot;concerned,&amp;quot; &amp;quot;concerns,&amp;quot; and &amp;quot;concerning&amp;quot; (or,
in shorthand notation, the set is fa,b,c,d1).
To evaluate an algorithm, we sum the num-
ber of correct (C), inserted (I) , and deleted (D)
words it predicts for each hypothesized confla-
tion set. If Xu, represents word w&apos;s conflation
set according to the algorithm, and if Yw repre-
sents its CELEX-based conflation set, then
</bodyText>
<equation confidence="0.996302333333333">
C = Evaxwn-Y.1/117.1),
D = Evw(lYw — (X w nifw)i i TO, and
1= Evw(lxw - (xw CI Yw)I/IYw 1)-
</equation>
<bodyText confidence="0.997975916666667">
However, in making these computations, we dis-
regard any CELEX words that are not in the
algorithm&apos;s data set and vice versa.
For example, suppose two algorithms were be-
ing compared on a data set where all the words
from Figure 3 were available except &amp;quot;concert-
ing&amp;quot; and &amp;quot;concertos.&amp;quot; Suppose further that one
algorithm proposed that { a,b,c,d,e,f,g,i} formed
a single conflation set whereas the other algo-
rithm proposed the three sets { a,b,c,d},{e,g,i},
and {f}. Then Table 3 illustrates how the two
algorithms would be scored.
</bodyText>
<tableCaption confidence="0.9349035">
Table 3: Example of scoring
a b c d e f g i Total
</tableCaption>
<table confidence="0.996984333333333">
Cl 4/4 4/4 4/4 4/4 3/3 3/3 3/3 1/1 8
D1 0/4 0/4 0/4 0/4 0/3 0/3 0/3 0/1 0
/1 4/4 4/4 4/4 4/4 5/3 5/3 5/3 7/1 16
C2 4/4 4/4 4/4 4/4 2/3 2/3 1/3 1/1 20/3
D2 0/4 0/4 0/4 0/4 1/3 1/3 2/3 0/1 4/3
12 0/4 0/4 0/4 0/4 1/3 1/3 0/3 2/1 8/3
</table>
<bodyText confidence="0.993629952380953">
To explain Table 3, consider algorithm one&apos;s
entries for &apos;a.&apos; Algorithm one had pro-
posed that Xa=fa,b,c,d,e,f,g,il when in reality,
=
Ya={ a,b,c,d}. Since IX,, n Ya I = 4 and IYak4,
then CA=4/ 4. The remaining values of the table
can be computed accordingly.
Using the values from Table 3, we can
also compute precision, recall, and F-Score.
Precision is defined to be C/(C+/), recall is
C/(C+D), and F-Score is the product of pre-
cision and recall divided by the average of the
two. For the first algorithm, the precision, re-
call, and F-Score would have respectively been
1/3, 1, and 1/2. In the second algorithm, these
numbers would have been 5/7, 5/6, and 10/13.
Table 4 uses the above scoring mechanism to
compare between Linguistica and our system (at
various probability thresholds). Note that since
Linguistica removes capitalization, it will have
a different total word count than our system.
</bodyText>
<figure confidence="0.99950925">
(b)
/concerned
(a) (c)
concerted
concerts
(0
concert
concer -.-- concerns
(d) (h)
concerning concerting concerting
(i) (j)
concerto -..— concertos
</figure>
<page confidence="0.992627">
71
</page>
<tableCaption confidence="0.99955">
Table 4: Performance on English CELEX
</tableCaption>
<table confidence="0.999494111111111">
Algorithm Linguistica LSA- LSA- LSA-
based based based
pr&gt; 0.5 pr&gt; 0.7 pr&gt; 0.85
#Correct 10515 10529 10203 9863
#Inserts 2157 1852 1138 783
#Deletes 2571 2341 2667 3007
Precision 83.0% 85.0% 90.0% 92.6%
Recall 80.4% 81.8% 79.3% 76.6%
F-Score 81.6% 83.4% 84.3% 83.9%
</table>
<sectionHeader confidence="0.996115" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99934">
These results suggest that semantics and LSA
can play a key part in knowledge-free mor-
phology induction. Semantics alone worked at
least as well as Goldsmith&apos;s frequency-based ap-
proach. Yet we believe that semantics-based
and frequency-based approaches play comple-
mentary roles. In current work, we are examin-
ing how to combine these two approaches.
</bodyText>
<sectionHeader confidence="0.993015" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995229471264368">
Albright, A. and B. P. Hayes. 1999. An au-
tomated learner for phonology and mor-
phology. Dept. of Linguistics, UCLA. At
http://www.humnet.ucla.edu/humnet/linguis-
tics/people/hayes/learning/learner.pdf.
Baayen, R. H., R. Piepenbrock, and H. van Rijn.
1993. The CELEX lexical database (CD-ROM),
Linguistic Data Consortium, University of Penn-
sylvania, Philadelphia, PA.
Berry, M., T. Do, G. O&apos;Brien, V. Krishna, and
S. Varadhan. 1993. SVDPACKC user&apos;s guide. CS-
93-194, University of Tennessee.
Dejean, H. 1998. Morphemes as necessary con-
cepts for structures: Discovery from untagged
corpora. University of Caen-Basse Normandie.
http://www.info.unicaenirr DeJean/travail/ar-
ticles/pg11.htm.
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K.
Landauer, and R. Harshman. 1990. Indexing by
Latent Semantic Analysis. Journal of the Ameri-
can Society for Information Science.
Gaussier, E. 1999. Unsupervised learning of deriva-
tional morphology from inflectional lexicons. ACL
&apos;99 Workshop Proceedings: Unsupervised Learn-
ing in Natural Language Processing, University of
Maryland.
Goldsmith, J. 1997. Unsupervised learning of the
morphology of a natural language. University of
Chicago.
Goldsmith, J. 2000. Unsupervised learning of
the morphology of a natural language. Uni-
versity of Chicago. http://humanities.uchi-
cago.edu/faculty/goldsmith.
Harris, Z. 1951. Structural Linguistics. University of
Chicago Press.
Hull, D. A. and G. Grefenstette. 1996. A de-
tailed analysis of English stemming algorithms.
XEROX Technical Report, http://www.xrce.xe-
rox.com/publis/m1tt/m1tt-023.ps.
Krovetz, R. 1993. Viewing morphology as an infer-
ence process. Proceedings of the 16thACM/SIGIR
Conference, pp. 191-202.
Jurafsky, D. S. and J. H. Martin. 2000. Speech and
Language Processing. Prentice Hall, Inc., Engle-
wood, N.J.
Karlsson, F. and L. Karttunen,. 1997. &amp;quot;Sub-
sentencial Processing.&amp;quot; In Survey of the State of
the Art in Human Language Technology, R. Cole,
Ed., Giardini Editori e Stampatori, Italy.
Koskenniemi, K. 1983. Two-level Morphology: a
General Computational Model for Word-Form
Recognition and Production. Ph.D. thesis, Univer-
sity of Helsinki.
Landauer,T. K., P. W. Foltz, and D. Laham. 1998.
Introduction to Latent Semantic Analysis. Dis-
course Processes. Vol. 25, pp. 259-284.
Lovins, J. 1968. Development of a stemming al-
gorithm. Mechanical Translation and Computa-
tional Linguistics, Vol. 11, pp.22-31
Manning, C. D. and H. Schiitze. 1999. Foundations
of Statistical Natural Language Processing, MIT
Press, Cambridge, MA.
Porter, M. 1980. An algorithm for suffix stripping.
Program, Vol. 14(3), pp.130-137.
Ritchie, G. and G. J. Russell. 1992. Computational
morphology: Practical Mechanisms for the En-
glish Lexicon. MIT.
Schiitze, H. 1993. Distributed syntactic representa-
tions with an application to part-of-speech tag-
ging. Proceedings of the IEEE International Con-
ference on Neural Networks, pp. 1504-1509.
Scott, D. 1992. Multivariate Density Estimation:
Theory, Practice, and Visualization. John Wiley
&amp; Sons, New York.
Sproat, R. 1992. Morphology and Computation. MIT
Press, Cambridge, MA.
Van den Bosch, A. and W. Daelemans. 1999.
Memory-based morphological analysis. Proc. of
the 37th Annual Meeting of the ACL, University
of Maryland, pp. 285-292.
Voorhees, E., D. Hoffman, and C. Barnes. 1996-7.
TREC Information Retrieval: Text Research Col-
lection, Vols. 4-5 (CD-ROM), National Institute
of Standards and Technology.
Woods, W. 2000. Aggressive morphology for robust
lexical coverage. Proceedings of the 6th ANLP/lst
NAACL, Seattle, WA.
</reference>
<page confidence="0.998716">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.726077">
<note confidence="0.976315">of CoNLL-2000 and LLL-2000, 67-72, Lisbon, Portugal, 2000.</note>
<title confidence="0.9025785">Knowledge-Free Induction of Using Latent Semantic Analysis</title>
<author confidence="0.954356">Schone</author>
<affiliation confidence="0.999831">University of</affiliation>
<address confidence="0.982634">Boulder, Colorado</address>
<email confidence="0.996766">Ischone,jurafskyl@cs.colorado.edu</email>
<abstract confidence="0.999601157894737">Morphology induction is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction. Previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Relying on stemand-affix statistics rather than semantic knowledge leads to a number of problems, such as the inappropriate use of valid affixes (&amp;quot;ally&amp;quot; stemming to &amp;quot;all&amp;quot;). We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plusaffix are sufficiently similar semantically. We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Albright</author>
<author>B P Hayes</author>
</authors>
<title>An automated learner for phonology and morphology.</title>
<date>1999</date>
<booktitle>Dept. of Linguistics, UCLA. At http://www.humnet.ucla.edu/humnet/linguistics/people/hayes/learning/learner.pdf.</booktitle>
<marker>Albright, Hayes, 1999</marker>
<rawString>Albright, A. and B. P. Hayes. 1999. An automated learner for phonology and morphology. Dept. of Linguistics, UCLA. At http://www.humnet.ucla.edu/humnet/linguistics/people/hayes/learning/learner.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>R Piepenbrock</author>
<author>H van Rijn</author>
</authors>
<date>1993</date>
<booktitle>The CELEX lexical database (CD-ROM), Linguistic Data</booktitle>
<institution>Consortium, University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<marker>Baayen, Piepenbrock, van Rijn, 1993</marker>
<rawString>Baayen, R. H., R. Piepenbrock, and H. van Rijn. 1993. The CELEX lexical database (CD-ROM), Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Berry</author>
<author>T Do</author>
<author>G O&apos;Brien</author>
<author>V Krishna</author>
<author>S Varadhan</author>
</authors>
<title>SVDPACKC user&apos;s guide. CS93-194,</title>
<date>1993</date>
<institution>University of Tennessee.</institution>
<contexts>
<context position="9476" citStr="Berry, et al., 1993" startWordPosition="1432" endWordPosition="1435">ingular value decomposition (SVD) to a matrix, M, where each entry M(i,j) contains the frequency of word i as seen in document j of the corpus. This methodology is referred to as Latent Semantic Analysis (LSA) and is well-described in the literature (Landauer, et al., 1998; Manning and Schiitze, 1999). SVDs seek to decompose a matrix A into the product of three matrices U, D, and VT where U and VT are orthogonal matrices and D is a diagonal matrix containing the singular values (squared eigenvalues) of A. Since SVD&apos;s can be performed which identify singular values by descending order of size (Berry, et al., 1993), LSA truncates after finding the k largest singular values. This corresponds to projecting the vector representation of each word into a k-dimensional subspace whose axes form k (latent) semantic directions. These projections are precisely the rows of the matrix product UkDk. A typical k is 300, which is the value we used. However, we have altered the algorithm somewhat to fit our needs. First, to stay as close to the knowledge-free scenario as possible, we neither apply a stopword list nor remove capitalization. Secondly, since SVDs are more designed to work on normally-distributed data (Man</context>
</contexts>
<marker>Berry, Do, O&apos;Brien, Krishna, Varadhan, 1993</marker>
<rawString>Berry, M., T. Do, G. O&apos;Brien, V. Krishna, and S. Varadhan. 1993. SVDPACKC user&apos;s guide. CS93-194, University of Tennessee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dejean</author>
</authors>
<title>Morphemes as necessary concepts for structures: Discovery from untagged corpora. University of Caen-Basse Normandie.</title>
<date>1998</date>
<note>http://www.info.unicaenirr DeJean/travail/articles/pg11.htm.</note>
<marker>Dejean, 1998</marker>
<rawString>Dejean, H. 1998. Morphemes as necessary concepts for structures: Discovery from untagged corpora. University of Caen-Basse Normandie. http://www.info.unicaenirr DeJean/travail/articles/pg11.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science.</journal>
<contexts>
<context position="3214" citStr="Deerwester, et al., 1990" startWordPosition="453" endWordPosition="456">iately (&amp;quot;ally&amp;quot; stemming to &amp;quot;all&amp;quot;), (2) morphological ambiguity may arise (&amp;quot;rating&amp;quot; conflating with &amp;quot;rat&amp;quot; instead of &amp;quot;rate&amp;quot;), and (3) non-productive affixes may get accidentally pruned (the relationship between &amp;quot;dirty&amp;quot; and &amp;quot;dirt&amp;quot; may be lost).1 Some of these problems could be resolved if one could incorporate word semantics. For instance, &amp;quot;all&amp;quot; is not semantically similar to &amp;quot;ally,&amp;quot; so with knowledge of semantics, an algorithm could avoid conflating these two words. To maintain the &amp;quot;knowledge-free&amp;quot; paradigm, such semantics would need to be automatically induced. Latent Semantic Analysis (LSA) (Deerwester, et al., 1990); Landauer, et at., 1998) is a technique which automatically identifies semantic information from a corpus. We here show that incorporating LSA-based semantics alone into the morphology-induction process can provide results that rival a state-ofthe-art system based on stem-and-affix statistics (Goldsmith&apos;s Linguistica). lError examples are from Goldsmith&apos;s Linguistica 67 Our algorithm automatically extracts potential affixes from an untagged corpus, identifies word pairs sharing the same proposed stem but having different affixes, and uses LSA to judge semantic relatedness between word pairs. </context>
<context position="8609" citStr="Deerwester, et al. (1990)" startWordPosition="1283" endWordPosition="1286">ds sharing the same root and the same affix rule, such as &amp;quot;cars&amp;quot; and &amp;quot;car,&amp;quot; form what we call a pair of potential morphological variants (PPMVs). We define the ruleset of a given rule to be the set of all PPM Vs that have that rule in common. For instance, from Figure 2, the ruleset for (&amp;quot;s&amp;quot;, NULL) would be the pairs &amp;quot;cars/car&amp;quot; and &amp;quot;cares/care.&amp;quot; Our algorithm establishes a list which identifies the rulesets for every hypothesized rule extracted from the data and then it must proceed to determine which rulesets or PPM Vs describe true morphological relationships. 3.3 Computing Semantic Vectors Deerwester, et al. (1990) showed that it is possible to find significant semantic relationships between words and documents in a corpus with virtually no human intervention (with the possible exception of a human-built stop word list). This is typically done by applying singular value decomposition (SVD) to a matrix, M, where each entry M(i,j) contains the frequency of word i as seen in document j of the corpus. This methodology is referred to as Latent Semantic Analysis (LSA) and is well-described in the literature (Landauer, et al., 1998; Manning and Schiitze, 1999). SVDs seek to decompose a matrix A into the produc</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gaussier</author>
</authors>
<title>Unsupervised learning of derivational morphology from inflectional lexicons.</title>
<date>1999</date>
<booktitle>ACL &apos;99 Workshop Proceedings: Unsupervised Learning in Natural Language Processing,</booktitle>
<institution>University of Maryland.</institution>
<contexts>
<context position="2335" citStr="Gaussier, 1999" startWordPosition="325" endWordPosition="327">vided information (&amp;quot;knowledge-free&amp;quot;). In building an MRD, &amp;quot;simply expanding the dictionary to encompass every word one is ever likely to encounter.. .fails to take advantage of regularities&amp;quot; (Sproat, 1992, p. xiii). Hence, automatic morphological analysis is also critical for selecting appropriate and non-redundant MRD headwords. For the reasons expressed above, we are interested in knowledge-free morphology induction. Thus, in this paper, we show how to automatically induce morphological relationships between words. Previous morphology induction approaches (Goldsmith, 1997, 2000; Mean, 1998; Gaussier, 1999) have focused on inflectional languages and have used statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Several problems can arise using only stem-and-affix statistics: (1) valid affixes may be applied inappropriately (&amp;quot;ally&amp;quot; stemming to &amp;quot;all&amp;quot;), (2) morphological ambiguity may arise (&amp;quot;rating&amp;quot; conflating with &amp;quot;rat&amp;quot; instead of &amp;quot;rate&amp;quot;), and (3) non-productive affixes may get accidentally pruned (the relationship between &amp;quot;dirty&amp;quot; and &amp;quot;dirt&amp;quot; may be lost).1 Some of these problems could be resolved if one could incorporate word semantics. For instance, &amp;quot;all&amp;quot; </context>
<context position="5555" citStr="Gaussier (1999)" startWordPosition="793" endWordPosition="794">tries cutting each word in exactly one place based on probability and lengths of hypothesized stems and affixes. He applies the EM algorithm to eliminate inappropriate parses. He collects the possible suffixes for each stem calling these a signature which aid in determining word classes. Goldsmith (2000) later incorporates minimum description length to identify stemming characteristics that most compress the data, but his algorithm otherwise remains similar in nature. Goldsmith&apos;s algorithm is practically knowledge-free, though he incorporates capitalization removal and some word segmentation. Gaussier (1999) begins with an inflectional lexicon and seeks to find derivational morphology. The words and parts of speech from his inflectional lexicon serve for building relational families of words and identifying sets of word pairs and suffixes therefrom. Gaussier splits words based on p-similarity — words that agree in exactly the first p characters. He also builds a probabilistic model which indicates that the probability of two words being morphological variants is based upon the probability of their respective changes in orthography and morphosynt act ics . 3 Current approach Our algorithm also foc</context>
</contexts>
<marker>Gaussier, 1999</marker>
<rawString>Gaussier, E. 1999. Unsupervised learning of derivational morphology from inflectional lexicons. ACL &apos;99 Workshop Proceedings: Unsupervised Learning in Natural Language Processing, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>1997</date>
<institution>University of Chicago.</institution>
<contexts>
<context position="2300" citStr="Goldsmith, 1997" startWordPosition="320" endWordPosition="321">tionaries (MRDs) using no human-provided information (&amp;quot;knowledge-free&amp;quot;). In building an MRD, &amp;quot;simply expanding the dictionary to encompass every word one is ever likely to encounter.. .fails to take advantage of regularities&amp;quot; (Sproat, 1992, p. xiii). Hence, automatic morphological analysis is also critical for selecting appropriate and non-redundant MRD headwords. For the reasons expressed above, we are interested in knowledge-free morphology induction. Thus, in this paper, we show how to automatically induce morphological relationships between words. Previous morphology induction approaches (Goldsmith, 1997, 2000; Mean, 1998; Gaussier, 1999) have focused on inflectional languages and have used statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Several problems can arise using only stem-and-affix statistics: (1) valid affixes may be applied inappropriately (&amp;quot;ally&amp;quot; stemming to &amp;quot;all&amp;quot;), (2) morphological ambiguity may arise (&amp;quot;rating&amp;quot; conflating with &amp;quot;rat&amp;quot; instead of &amp;quot;rate&amp;quot;), and (3) non-productive affixes may get accidentally pruned (the relationship between &amp;quot;dirty&amp;quot; and &amp;quot;dirt&amp;quot; may be lost).1 Some of these problems could be resolved if one could incorporate w</context>
<context position="4939" citStr="Goldsmith (1997)" startWordPosition="706" endWordPosition="707">serve high frequency occurrences of some word endings or beginnings, perform statistics thereon, and propose that some of these appendages are valid morphemes. However, these algorithms differ in specifics. DeJean (1998) uses an approach derived from Harris (1951) where word-splitting occurs if the number of distinct letters that follows a given sequence of characters surpasses a threshold. He uses these hypothesized affixes to resegment words and thereby identify additional affixes that were initially overlooked. His overall goal is different from ours: he primarily seeks an affix inventory. Goldsmith (1997) tries cutting each word in exactly one place based on probability and lengths of hypothesized stems and affixes. He applies the EM algorithm to eliminate inappropriate parses. He collects the possible suffixes for each stem calling these a signature which aid in determining word classes. Goldsmith (2000) later incorporates minimum description length to identify stemming characteristics that most compress the data, but his algorithm otherwise remains similar in nature. Goldsmith&apos;s algorithm is practically knowledge-free, though he incorporates capitalization removal and some word segmentation.</context>
</contexts>
<marker>Goldsmith, 1997</marker>
<rawString>Goldsmith, J. 1997. Unsupervised learning of the morphology of a natural language. University of Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2000</date>
<institution>University of Chicago.</institution>
<note>http://humanities.uchicago.edu/faculty/goldsmith.</note>
<contexts>
<context position="5245" citStr="Goldsmith (2000)" startWordPosition="754" endWordPosition="755"> of distinct letters that follows a given sequence of characters surpasses a threshold. He uses these hypothesized affixes to resegment words and thereby identify additional affixes that were initially overlooked. His overall goal is different from ours: he primarily seeks an affix inventory. Goldsmith (1997) tries cutting each word in exactly one place based on probability and lengths of hypothesized stems and affixes. He applies the EM algorithm to eliminate inappropriate parses. He collects the possible suffixes for each stem calling these a signature which aid in determining word classes. Goldsmith (2000) later incorporates minimum description length to identify stemming characteristics that most compress the data, but his algorithm otherwise remains similar in nature. Goldsmith&apos;s algorithm is practically knowledge-free, though he incorporates capitalization removal and some word segmentation. Gaussier (1999) begins with an inflectional lexicon and seeks to find derivational morphology. The words and parts of speech from his inflectional lexicon serve for building relational families of words and identifying sets of word pairs and suffixes therefrom. Gaussier splits words based on p-similarity</context>
</contexts>
<marker>Goldsmith, 2000</marker>
<rawString>Goldsmith, J. 2000. Unsupervised learning of the morphology of a natural language. University of Chicago. http://humanities.uchicago.edu/faculty/goldsmith.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Structural Linguistics.</title>
<date>1951</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="4587" citStr="Harris (1951)" startWordPosition="655" endWordPosition="656">h in order to perform evaluations against the human-labeled CELEX database (Baayen, et al., 1993). 2 Previous work Existing induction algorithms all focus on identifying prefixes, suffixes, and word stems in inflectional languages (avoiding infixes and other language types like concatenative or agglutinative languages (Sproat, 1992)). They also observe high frequency occurrences of some word endings or beginnings, perform statistics thereon, and propose that some of these appendages are valid morphemes. However, these algorithms differ in specifics. DeJean (1998) uses an approach derived from Harris (1951) where word-splitting occurs if the number of distinct letters that follows a given sequence of characters surpasses a threshold. He uses these hypothesized affixes to resegment words and thereby identify additional affixes that were initially overlooked. His overall goal is different from ours: he primarily seeks an affix inventory. Goldsmith (1997) tries cutting each word in exactly one place based on probability and lengths of hypothesized stems and affixes. He applies the EM algorithm to eliminate inappropriate parses. He collects the possible suffixes for each stem calling these a signatu</context>
</contexts>
<marker>Harris, 1951</marker>
<rawString>Harris, Z. 1951. Structural Linguistics. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Hull</author>
<author>G Grefenstette</author>
</authors>
<title>A detailed analysis of English stemming algorithms.</title>
<date>1996</date>
<tech>XEROX Technical Report, http://www.xrce.xerox.com/publis/m1tt/m1tt-023.ps.</tech>
<marker>Hull, Grefenstette, 1996</marker>
<rawString>Hull, D. A. and G. Grefenstette. 1996. A detailed analysis of English stemming algorithms. XEROX Technical Report, http://www.xrce.xerox.com/publis/m1tt/m1tt-023.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Krovetz</author>
</authors>
<title>Viewing morphology as an inference process.</title>
<date>1993</date>
<booktitle>Proceedings of the 16thACM/SIGIR Conference,</booktitle>
<pages>191--202</pages>
<marker>Krovetz, 1993</marker>
<rawString>Krovetz, R. 1993. Viewing morphology as an inference process. Proceedings of the 16thACM/SIGIR Conference, pp. 191-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech and Language Processing.</title>
<date>2000</date>
<publisher>Prentice Hall, Inc.,</publisher>
<location>Englewood, N.J.</location>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Jurafsky, D. S. and J. H. Martin. 2000. Speech and Language Processing. Prentice Hall, Inc., Englewood, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Karlsson</author>
<author>L Karttunen</author>
</authors>
<title>Subsentencial Processing.&amp;quot;</title>
<date>1997</date>
<booktitle>In Survey of the State of the Art in Human Language Technology, R. Cole, Ed., Giardini Editori e Stampatori,</booktitle>
<location>Italy.</location>
<contexts>
<context position="1364" citStr="Karlsson and Karttunen, 1997" startWordPosition="188" endWordPosition="191">all&amp;quot;). We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plusaffix are sufficiently similar semantically. We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system. 1 Introduction Computational morphological analyzers have existed in various languages for years and it has been said that &amp;quot;the quest for an efficient method for the analysis and generation of word-forms is no longer an academic research topic&amp;quot; (Karlsson and Karttunen, 1997). However, development of these analyzers typically begins with human intervention requiring time spans from days to weeks. If it were possible to build such analyzers automatically without human knowledge, significant development time could be saved. On a larger scale, consider the task of inducing machine-readable dictionaries (MRDs) using no human-provided information (&amp;quot;knowledge-free&amp;quot;). In building an MRD, &amp;quot;simply expanding the dictionary to encompass every word one is ever likely to encounter.. .fails to take advantage of regularities&amp;quot; (Sproat, 1992, p. xiii). Hence, automatic morphologic</context>
</contexts>
<marker>Karlsson, Karttunen, 1997</marker>
<rawString>Karlsson, F. and L. Karttunen,. 1997. &amp;quot;Subsentencial Processing.&amp;quot; In Survey of the State of the Art in Human Language Technology, R. Cole, Ed., Giardini Editori e Stampatori, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Koskenniemi</author>
</authors>
<title>Two-level Morphology: a General Computational Model for Word-Form Recognition and Production.</title>
<date>1983</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Helsinki.</institution>
<marker>Koskenniemi, 1983</marker>
<rawString>Koskenniemi, K. 1983. Two-level Morphology: a General Computational Model for Word-Form Recognition and Production. Ph.D. thesis, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes.</booktitle>
<volume>25</volume>
<pages>259--284</pages>
<contexts>
<context position="9129" citStr="Landauer, et al., 1998" startWordPosition="1370" endWordPosition="1373">Vs describe true morphological relationships. 3.3 Computing Semantic Vectors Deerwester, et al. (1990) showed that it is possible to find significant semantic relationships between words and documents in a corpus with virtually no human intervention (with the possible exception of a human-built stop word list). This is typically done by applying singular value decomposition (SVD) to a matrix, M, where each entry M(i,j) contains the frequency of word i as seen in document j of the corpus. This methodology is referred to as Latent Semantic Analysis (LSA) and is well-described in the literature (Landauer, et al., 1998; Manning and Schiitze, 1999). SVDs seek to decompose a matrix A into the product of three matrices U, D, and VT where U and VT are orthogonal matrices and D is a diagonal matrix containing the singular values (squared eigenvalues) of A. Since SVD&apos;s can be performed which identify singular values by descending order of size (Berry, et al., 1993), LSA truncates after finding the k largest singular values. This corresponds to projecting the vector representation of each word into a k-dimensional subspace whose axes form k (latent) semantic directions. These projections are precisely the rows of </context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Landauer,T. K., P. W. Foltz, and D. Laham. 1998. Introduction to Latent Semantic Analysis. Discourse Processes. Vol. 25, pp. 259-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lovins</author>
</authors>
<title>Development of a stemming algorithm.</title>
<date>1968</date>
<journal>Mechanical Translation and Computational Linguistics,</journal>
<volume>11</volume>
<pages>22--31</pages>
<marker>Lovins, 1968</marker>
<rawString>Lovins, J. 1968. Development of a stemming algorithm. Mechanical Translation and Computational Linguistics, Vol. 11, pp.22-31</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schiitze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing,</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9158" citStr="Manning and Schiitze, 1999" startWordPosition="1374" endWordPosition="1377">ogical relationships. 3.3 Computing Semantic Vectors Deerwester, et al. (1990) showed that it is possible to find significant semantic relationships between words and documents in a corpus with virtually no human intervention (with the possible exception of a human-built stop word list). This is typically done by applying singular value decomposition (SVD) to a matrix, M, where each entry M(i,j) contains the frequency of word i as seen in document j of the corpus. This methodology is referred to as Latent Semantic Analysis (LSA) and is well-described in the literature (Landauer, et al., 1998; Manning and Schiitze, 1999). SVDs seek to decompose a matrix A into the product of three matrices U, D, and VT where U and VT are orthogonal matrices and D is a diagonal matrix containing the singular values (squared eigenvalues) of A. Since SVD&apos;s can be performed which identify singular values by descending order of size (Berry, et al., 1993), LSA truncates after finding the k largest singular values. This corresponds to projecting the vector representation of each word into a k-dimensional subspace whose axes form k (latent) semantic directions. These projections are precisely the rows of the matrix product UkDk. A ty</context>
</contexts>
<marker>Manning, Schiitze, 1999</marker>
<rawString>Manning, C. D. and H. Schiitze. 1999. Foundations of Statistical Natural Language Processing, MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>130--137</pages>
<marker>Porter, 1980</marker>
<rawString>Porter, M. 1980. An algorithm for suffix stripping. Program, Vol. 14(3), pp.130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ritchie</author>
<author>G J Russell</author>
</authors>
<title>Computational morphology: Practical Mechanisms for the English Lexicon.</title>
<date>1992</date>
<publisher>MIT.</publisher>
<marker>Ritchie, Russell, 1992</marker>
<rawString>Ritchie, G. and G. J. Russell. 1992. Computational morphology: Practical Mechanisms for the English Lexicon. MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schiitze</author>
</authors>
<title>Distributed syntactic representations with an application to part-of-speech tagging.</title>
<date>1993</date>
<booktitle>Proceedings of the IEEE International Conference on Neural Networks,</booktitle>
<pages>1504--1509</pages>
<contexts>
<context position="10250" citStr="Schiitze (1993)" startWordPosition="1560" endWordPosition="1561">e whose axes form k (latent) semantic directions. These projections are precisely the rows of the matrix product UkDk. A typical k is 300, which is the value we used. However, we have altered the algorithm somewhat to fit our needs. First, to stay as close to the knowledge-free scenario as possible, we neither apply a stopword list nor remove capitalization. Secondly, since SVDs are more designed to work on normally-distributed data (Manning and Schiitze, 1999, p. 565), we operate on Zscores rather than counts. Lastly, instead of generating a term-document matrix, we build a term-term matrix. Schiitze (1993) achieved excellent performance at classifying words into quasi-partof-speech classes by building and performing an SVD on an Nx4N term-term matrix, M(i,Np+j). The indices i and j represent the top N highest frequency words. The p values range from 0 to 3 representing whether the word indexed by j is positionally offset from the word indexed by i by -2, -1, +1, or +2, respectively. For example, if &amp;quot;the&amp;quot; and &amp;quot;people&amp;quot; were respectively the 1st and 100th highest frequency words, then upon seeing the phrase &amp;quot;the people,&amp;quot; Schiitze&apos;s approach would increment the counts of M(1,2N+100) and M(100,N+1).</context>
</contexts>
<marker>Schiitze, 1993</marker>
<rawString>Schiitze, H. 1993. Distributed syntactic representations with an application to part-of-speech tagging. Proceedings of the IEEE International Conference on Neural Networks, pp. 1504-1509.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Scott</author>
</authors>
<title>Multivariate Density Estimation: Theory, Practice, and Visualization.</title>
<date>1992</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York.</location>
<marker>Scott, 1992</marker>
<rawString>Scott, D. 1992. Multivariate Density Estimation: Theory, Practice, and Visualization. John Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
</authors>
<title>Morphology and Computation.</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1924" citStr="Sproat, 1992" startWordPosition="269" endWordPosition="270">demic research topic&amp;quot; (Karlsson and Karttunen, 1997). However, development of these analyzers typically begins with human intervention requiring time spans from days to weeks. If it were possible to build such analyzers automatically without human knowledge, significant development time could be saved. On a larger scale, consider the task of inducing machine-readable dictionaries (MRDs) using no human-provided information (&amp;quot;knowledge-free&amp;quot;). In building an MRD, &amp;quot;simply expanding the dictionary to encompass every word one is ever likely to encounter.. .fails to take advantage of regularities&amp;quot; (Sproat, 1992, p. xiii). Hence, automatic morphological analysis is also critical for selecting appropriate and non-redundant MRD headwords. For the reasons expressed above, we are interested in knowledge-free morphology induction. Thus, in this paper, we show how to automatically induce morphological relationships between words. Previous morphology induction approaches (Goldsmith, 1997, 2000; Mean, 1998; Gaussier, 1999) have focused on inflectional languages and have used statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Several problems can arise using only stem</context>
<context position="4308" citStr="Sproat, 1992" startWordPosition="614" endWordPosition="615">ng the same proposed stem but having different affixes, and uses LSA to judge semantic relatedness between word pairs. This process serves to identify valid morphological relations. Though our algorithm could be applied to any inflectional language, we here restrict it to English in order to perform evaluations against the human-labeled CELEX database (Baayen, et al., 1993). 2 Previous work Existing induction algorithms all focus on identifying prefixes, suffixes, and word stems in inflectional languages (avoiding infixes and other language types like concatenative or agglutinative languages (Sproat, 1992)). They also observe high frequency occurrences of some word endings or beginnings, perform statistics thereon, and propose that some of these appendages are valid morphemes. However, these algorithms differ in specifics. DeJean (1998) uses an approach derived from Harris (1951) where word-splitting occurs if the number of distinct letters that follows a given sequence of characters surpasses a threshold. He uses these hypothesized affixes to resegment words and thereby identify additional affixes that were initially overlooked. His overall goal is different from ours: he primarily seeks an af</context>
</contexts>
<marker>Sproat, 1992</marker>
<rawString>Sproat, R. 1992. Morphology and Computation. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
<author>W Daelemans</author>
</authors>
<title>Memory-based morphological analysis.</title>
<date>1999</date>
<booktitle>Proc. of the 37th Annual Meeting of the ACL, University of Maryland,</booktitle>
<pages>285--292</pages>
<marker>Van den Bosch, Daelemans, 1999</marker>
<rawString>Van den Bosch, A. and W. Daelemans. 1999. Memory-based morphological analysis. Proc. of the 37th Annual Meeting of the ACL, University of Maryland, pp. 285-292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
<author>D Hoffman</author>
<author>C Barnes</author>
</authors>
<date>1996</date>
<journal>TREC Information Retrieval: Text Research Collection,</journal>
<volume>Vols.</volume>
<pages>4--5</pages>
<institution>(CD-ROM), National Institute of Standards and Technology.</institution>
<marker>Voorhees, Hoffman, Barnes, 1996</marker>
<rawString>Voorhees, E., D. Hoffman, and C. Barnes. 1996-7. TREC Information Retrieval: Text Research Collection, Vols. 4-5 (CD-ROM), National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Woods</author>
</authors>
<title>Aggressive morphology for robust lexical coverage.</title>
<date>2000</date>
<booktitle>Proceedings of the 6th ANLP/lst NAACL,</booktitle>
<location>Seattle, WA.</location>
<marker>Woods, 2000</marker>
<rawString>Woods, W. 2000. Aggressive morphology for robust lexical coverage. Proceedings of the 6th ANLP/lst NAACL, Seattle, WA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>