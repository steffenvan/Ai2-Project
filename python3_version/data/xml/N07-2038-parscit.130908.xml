<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016935">
<title confidence="0.9993745">
Agenda-Based User Simulation for
Bootstrapping a POMDP Dialogue System
</title>
<author confidence="0.998031">
Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye and Steve Young
</author>
<affiliation confidence="0.996228">
Cambridge University Engineering Department
</affiliation>
<address confidence="0.550165">
Trumpington Street, Cambridge, CB21PZ, United Kingdom
</address>
<email confidence="0.980409">
{js532, brmt2, kw278, hy216, sjy}@eng.cam.ac.uk
</email>
<sectionHeader confidence="0.995368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935363636364">
This paper investigates the problem of boot-
strapping a statistical dialogue manager with-
out access to training data and proposes a new
probabilistic agenda-based method for simu-
lating user behaviour. In experiments with a
statistical POMDP dialogue system, the simu-
lator was realistic enough to successfully test
the prototype system and train a dialogue pol-
icy. An extensive study with human subjects
showed that the learned policy was highly com-
petitive, with task completion rates above 90%.
</bodyText>
<sectionHeader confidence="0.952635" genericHeader="categories and subject descriptors">
1 Background and Introduction
</sectionHeader>
<subsectionHeader confidence="0.999845">
1.1 Bootstrapping Statistical Dialogue Managers
</subsectionHeader>
<bodyText confidence="0.999946523809524">
One of the key advantages of a statistical approach to Dia-
logue Manager (DM) design is the ability to formalise de-
sign criteria as objective reward functions and to learn an
optimal dialogue policy from real dialogue data. In cases
where a system is designed from scratch, however, it is
often the case that no suitable in-domain data is available
for training the DM. Collecting dialogue data without a
working prototype is problematic, leaving the developer
with a classic chicken-and-egg problem.
Wizard-of-Oz (WoZ) experiments can be carried out to
record dialogues, but they are often time-consuming and
the recorded data may show characteristics of human-
human conversation rather than typical human-computer
dialogue. Alternatively, human-computer dialogues can
be recorded with a handcrafted DM prototype but neither
of these two methods enables the system designer to test
the implementation of the statistical DM and the learn-
ing algorithm. Moreover, the size of the recorded corpus
(typically « 103 dialogues) usually falls short of the re-
quirements for training a statistical DM (typically » 104
dialogues).
</bodyText>
<subsectionHeader confidence="0.950479">
1.2 User Simulation-Based Training
</subsectionHeader>
<bodyText confidence="0.999929733333333">
In recent years, a number of research groups have inves-
tigated the use of a two-stage simulation-based setup. A
statistical user model is first trained on a limited amount
of dialogue data and the model is then used to simulate
dialogues with the interactively learning DM (see Schatz-
mann et al. (2006) for a literature review).
The simulation-based approach assumes the presence
of a small corpus of suitably annotated in-domain dia-
logues or out-of-domain dialogues with a matching dia-
logue format (Lemon et al., 2006). In cases when no such
data is available, handcrafted values can be assigned to
the model parameters given that the model is sufficiently
simple (Levin et al., 2000; Pietquin and Dutoit, 2005) but
the performance of dialogue policies learned this way has
not been evaluated using real users.
</bodyText>
<subsectionHeader confidence="0.994388">
1.3 Paper Outline
</subsectionHeader>
<bodyText confidence="0.9999609375">
This paper presents a new probabilistic method for simu-
lating user behaviour based on a compact representation
of the user goal and a stack-like user agenda. The model
provides an elegant way of encoding the relevant dialogue
history from a user’s point of view and has a very small
parameter set so that manually chosen priors can be used
to bootstrap the DM training and testing process.
In experiments presented in this paper, the agenda-
based simulator was used to train a statistical POMDP-
based (Young, 2006; Young et al., 2007) DM. Even with-
out any training of its model parameters, the agenda-
based simulator was able to produce dialogue behaviour
realistic enough to train a competitive dialogue policy.
An extensive study1 with 40 human subjects showed that
task completion with the learned policy was above 90%
despite a mix of native and non-native speakers.
</bodyText>
<footnote confidence="0.99840425">
1This research was partly funded by the EU FP6 TALK
Project. The system evaluation was conducted in collabora-
tion with O. Lemon, K. Georgila and J. Henderson at Edinburgh
University and their work is gratefully acknowledged.
</footnote>
<page confidence="0.983198">
149
</page>
<note confidence="0.638955">
Proceedings of NAACL HLT 2007, Companion Volume, pages 149–152,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.985413" genericHeader="method">
2 Agenda-Based Simulation
</sectionHeader>
<subsectionHeader confidence="0.997167">
2.1 User Simulation at a Semantic Level
</subsectionHeader>
<bodyText confidence="0.999991">
Human-machine dialogue can be formalised on a seman-
tic level as a sequence of state transitions and dialogue
acts2. At any time t, the user is in a state S, takes ac-
tion au, transitions into the intermediate state S&apos;, receives
machine action am, and transitions into the next state S&apos;&apos;
where the cycle restarts.
</bodyText>
<equation confidence="0.986206">
S → au → S&apos; → am → S&apos;&apos; → · · · (1)
</equation>
<bodyText confidence="0.99818025">
Assuming a Markovian state representation, user be-
haviour can be decomposed into three models: P(au|S)
for action selection, P(S&apos;|au, S) for the state transition
into S&apos;, and P(S&apos;&apos;|am, S&apos;) for the transition into S&apos;&apos;.
</bodyText>
<subsectionHeader confidence="0.993563">
2.2 Goal- and Agenda-Based State Representation
</subsectionHeader>
<bodyText confidence="0.999875">
Inspired by agenda-based methods to dialogue manage-
ment (Wei and Rudnicky, 1999) the approach described
here factors the user state into an agenda A and a goal G.
</bodyText>
<equation confidence="0.976656">
S = (A, G) and G = (C, R) (2)
</equation>
<bodyText confidence="0.99987508">
During the course of the dialogue, the goal G ensures that
the user behaves in a consistent, goal-directed manner.
G consists of constraints C which specify the required
venue, eg. a centrally located bar serving beer, and re-
quests R which specify the desired pieces of information,
eg. the name, address and phone number (cf. Fig. 1).
The user agenda A is a stack-like structure containing
the pending user dialogue acts that are needed to elicit the
information specified in the goal. At the start of the dia-
logue a new goal is randomly generated using the system
database and the agenda is initially populated by convert-
ing all goal constraints into inform acts and all goal re-
quests into request acts. A bye act is added at the bottom
of the agenda to close the dialogue.
As the dialogue progresses the agenda and goal are dy-
namically updated and acts are selected from the top of
the agenda to form user acts au. In response to incom-
ing machine acts am, new user acts are pushed onto the
agenda and no longer relevant ones are removed. The
agenda thus serves as a convenient way of tracking the
progress of the dialogue as well as encoding the rele-
vant dialogue history. Acts can also be temporarily stored
when actions of higher priority need to be issued first,
hence providing the simulator with a simple model of
user memory3.
</bodyText>
<footnote confidence="0.952384875">
2In this paper, the terms dialogue act and dialogue action
are used interchangeably. The notation act(a=x, b=y,...) is used
to represent a dialogue act of a given type act (such as inform
or request with items a = x, b = y, etc.
3See Fig. 1, turns 1-3, for an example: System turn 1 “What
pricerange?” triggers the user act inform(pricerange=cheap)
to be pushed onto the agenda but it is not executed until turn 3
because negate(drinks=beer) is issued first.
</footnote>
<figureCaption confidence="0.999197">
Figure 1: Sample dialogue and agenda sequence
</figureCaption>
<subsectionHeader confidence="0.995409">
2.3 User Act Selection
</subsectionHeader>
<bodyText confidence="0.999994714285714">
At any time during the dialogue, the updated agenda of
length N contains all dialogue acts the user intends to
convey to the system. Since the agenda is ordered ac-
cording to priority, with A[N] denoting the top and A[1]
denoting the bottom item, selecting the next user act sim-
plifies to popping n items off the top of the stack. Hence,
letting au[i] denote the ith item in the user act au
</bodyText>
<equation confidence="0.984537">
au[i] := A[N − n + i] di E [1..n],1 &lt; n &lt; N. (3)
Using A[N−n+1..N] as a Matlab-like shorthand nota-
� type = bar
C0 = drinks = beer
area = central
� name= �
R0 = addr =
phone =
</equation>
<bodyText confidence="0.638586">
Sys 0 Hello, how may I help you?
</bodyText>
<figure confidence="0.969266371428572">
A1 = ⎡ inform(type = bar) ⎤
⎢ ⎢ ⎢ ⎢ ⎢ ⎣ inform(drinks = beer) ⎦⎥⎥⎥⎥⎥
inform(area = central)
request(name)
request(addr)
request(phone)
bye()
Usr 1 I’m looking for a nice bar serving beer.
Sys 1 Ok, a wine bar. What pricerange?
A2 = ⎡ negate(drinks = beer) ⎤
⎢ ⎢ ⎢ ⎢ ⎢ ⎣ inform(pricerange = cheap) ⎦⎥⎥⎥⎥⎥
inform(area = central)
request(name)
request(addr)
request(phone)
bye()
Usr 2 No, beer please!
Sys 2 A bar serving beer, correct?
A3 = ⎡ affirm() ⎤
⎢ ⎢ ⎢ ⎢ ⎢ ⎣ inform(pricerange = cheap) ⎦⎥⎥⎥⎥⎥
inform(area = central)
request(name)
request(addr)
request(phone)
bye()
Usr 3 Yeah something cheap in the town centre.
Sys 3 Murphy’s on Main Square serves cheap beer.
� �
request(phone)
A4 =
bye()
Usr 4 Ok, and what’s the phone number?
Sys 4 The number is 796 69 94.
A5 = [ bye() �
Usr 5 Thanks, goodbye!
</figure>
<page confidence="0.974941">
150
</page>
<bodyText confidence="0.999513333333333">
can be excluded in the following derivation for simplicity.
Considering only the push-operations, the items 1 to N&apos;
at the bottom of the agenda remain fixed and the update
model can be rewritten as follows:
tion for the top n items on A, the action selection model
becomes a Dirac delta function
</bodyText>
<equation confidence="0.999768">
P(au|S) = P(au|A, G) = δ(au, A[N− n+1..N]) (4)
</equation>
<bodyText confidence="0.999915875">
where the random variable n corresponds to the level
of initiative taken by the simulated user. In a statistical
model the probability distribution over integer values for
n should be conditioned on A and learned from dialogue
data. For the purposes of bootstrapping the system, n can
be assumed independent of A and any distribution P(n)
that places the majority of its probability mass on small
values of n can be used.
</bodyText>
<subsectionHeader confidence="0.989035">
2.4 State Transition Model
</subsectionHeader>
<bodyText confidence="0.9011688">
The factorisation of S into A and G can now be ap-
plied to the state transition models P(S&apos;|au, S) and
P(S&amp;quot;|am, S&apos;). Letting A&apos; denote the agenda after select-
ing au (as explained in the previous subsection) and using
N&apos; = N − n to denote the size of A&apos;, we have
</bodyText>
<equation confidence="0.983288">
A&apos;[i] := A[i] di E [1..N&apos;]. (5)
</equation>
<bodyText confidence="0.999628666666667">
Using this definition of A&apos; and assuming that the goal
remains constant when the user executes au, the first state
transition depending on au simplifies to
</bodyText>
<equation confidence="0.983864">
P(S&apos;|au, S) = P(A&apos;, G&apos;|au, A, G)
= δ(A&apos;, A[1..N&apos;])δ(G&apos;, G). (6)
</equation>
<bodyText confidence="0.99510325">
Using S = (A, G), the chain rule of probability, and rea-
sonable conditional independence assumptions, the sec-
ond state transition based on am can be decomposed into
goal update and agenda update modules:
</bodyText>
<equation confidence="0.996927">
P(S&amp;quot;|am, S&apos;)
= P(A„|am, A&apos;, G„) P(G„|am, G,) . (7)
�� 1.01
</equation>
<bodyText confidence="0.975290444444444">
agenda update goal update
When no restrictions are placed on A” and G&amp;quot;, the space
of possible state transitions is vast. The model parame-
ter set is too large to be handcrafted and even substantial
amounts of training data would be insufficient to obtain
reliable estimates. It can however be assumed that A&amp;quot; is
derived from A&apos; and that G” is derived from G&apos; and that
in each case the transition entails only a limited number
of well-defined atomic operations.
</bodyText>
<subsectionHeader confidence="0.967898">
2.5 Agenda Update Model for System Acts
</subsectionHeader>
<bodyText confidence="0.9999665">
The agenda transition from A&apos; to A” can be viewed as a
sequence of push-operations in which dialogue acts are
added to the top of the agenda. In a second ”clean-up”
step, duplicate dialogue acts, null() acts, and unnecessary
request() acts for already filled goal request slots must
be removed but this is a deterministic procedure so that it
</bodyText>
<equation confidence="0.99222325">
P(A&amp;quot;|am, At, G&amp;quot;)
= P(A„[1..N&amp;quot;]|am,A&apos;[1..N&apos;],G&amp;quot;) (8)
= P(A„[N&apos;+1..N”]|am, G&amp;quot;)
· δ(A11[1..N&apos;],A&apos;[1..N&apos;]). (9)
</equation>
<bodyText confidence="0.983741444444445">
The first term on the RHS of Eq. 9 can now be further
simplified by assuming that every dialogue act item in
am triggers one push-operation. This assumption can be
made without loss of generality, because it is possible to
push a null() act (which is later removed) or to push an
act with more than one item. The advantage of this as-
sumption is that the known number M of items in am
now determines the number of push-operations. Hence
N&amp;quot;=N&apos;+Mand
</bodyText>
<equation confidence="0.9996264">
P(A&amp;quot;[N&apos;+1..N11]|am, G&amp;quot;)
= P(A&amp;quot;[N&apos;+1..N&apos;+M]|am[1..M],G&amp;quot;) (10)
M
= P(A&amp;quot;[N&apos;+i]|am[i], G&amp;quot;) (11)
i=1
</equation>
<bodyText confidence="0.9984095">
The expression in Eq. 11 shows that each item am[i] in
the system act triggers one push operation, and that this
operation is conditioned on the goal. This model is now
simple enough to be handcrafted using heuristics. For ex-
ample, the model parameters can be set so that when the
item x=y in am[i] violates the constraints in G&amp;quot;, one of
the following is pushed onto A&amp;quot;: negate(), inform(x=z),
deny(x=y, x=z), etc.
</bodyText>
<subsectionHeader confidence="0.981838">
2.6 Goal Update Model for System Acts
</subsectionHeader>
<bodyText confidence="0.99982775">
The goal update model P(G&amp;quot;|am, G&apos;) describes how the
user constraints C&apos; and requests R&apos; change with a given
machine action am. Assuming that R&amp;quot; is conditionally
independent of C&apos; given C&amp;quot; it can be shown that
</bodyText>
<equation confidence="0.960947">
P(G&amp;quot;|am, G&apos;)
= P(R&amp;quot;|am, R&apos;, C&amp;quot;)P(C&amp;quot;|am, R&apos;, C&apos;). (12)
</equation>
<bodyText confidence="0.999978555555556">
To restrict the space of transitions from R&apos; to R&amp;quot; it
can be assumed that the request slots are independent and
each slot (eg. addr,phone,etc.) is either filled using infor-
mation in am or left unchanged. Using R[k] to denote the
k’th request slot, we approximate that the value of R&amp;quot;[k]
only depends on its value at the previous time step, the
value provided by am, and M(am, C&amp;quot;) which indicates
a match or mismatch between the information given in
am and the goal constraints.
</bodyText>
<equation confidence="0.7769545">
P(R&amp;quot;|am, R&apos;, C&amp;quot;)
= Hk P(R„[k]|am,R&apos;[k],M(am,C„)). (13)
</equation>
<page confidence="0.986086">
151
</page>
<bodyText confidence="0.999979181818182">
To simplify P(C&apos;&apos;|a,,,,, R&apos;, C&apos;) we assume that C&apos;&apos; is
derived from C&apos; by either adding a new constraint, set-
ting an existing constraint slot to a different value (eg.
drinks=dontcare), or by simply changing nothing. The
choice of transition does not need to be conditioned on
the full space of possible a.,,,,, R&apos; and C&apos;. Instead it can
be conditioned on simple boolean flags such as ”Does a,,,,
ask for a slot in the constraint set?”, ”Does a,,,, signal that
no item in the database matches the given constraints?”,
etc. The model parameter set is then sufficiently small for
handcrafted values to be assigned to the probabilities.
</bodyText>
<sectionHeader confidence="0.999883" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999721">
3.1 Training the HIS Dialogue Manager
</subsectionHeader>
<bodyText confidence="0.99996405">
The Hidden Information State (HIS) model is the first
trainable and scalable implementation of a statistical
spoken dialog system based on the Partially-Observable
Markov Decision Process (POMDP) model of dialogue
(Young, 2006; Young et al., 2007; Williams and Young,
2007). POMDPs extend the standard Markov-Decision-
Process model by maintaining a beliefspace, i.e. a proba-
bility distribution over dialogue states, and hence provide
an explicit model of the uncertainty present in human-
machine communication.
The HIS model uses a grid-based discretisation of the
continuous state space and online c-greedy policy iter-
ation. Fig. 2 shows a typical training run over 60,000
simulated dialogues, starting with a random policy. User
goals are randomly generated and an (arbitrary) reward
function assigning 20 points for successful completion
and -1 for every dialogue turn is used. As can be seen, di-
alogue performance (defined as the average reward over
1000 dialogues) converges after roughly 25,000 iterations
and asymptotes to a value of approx. 14 points.
</bodyText>
<figureCaption confidence="0.881194">
Figure 2: Training a POMDP system
</figureCaption>
<subsectionHeader confidence="0.998067">
3.2 Experimental Evaluation and Results
</subsectionHeader>
<bodyText confidence="0.99997075">
A prototype HIS dialogue system with a learned policy
was built for the Tourist Information Domain and exten-
sively evaluated with 40 human subjects including native
and non-native speakers. A total of 160 dialogues with
21667 words was recorded and the average Word-Error-
Rate was 29.8%. Task scenarios involved finding a spe-
cific bar, hotel or restaurant in a fictitious town (eg. the
address of a cheap, Chinese restaurant in the west).
The performance of the system was measured based
on the recommendation of a correct venue, i.e. a venue
matching all constraints specified in the given task (all
tasks were designed to have a solution). Based on this
definition, 145 out of 160 dialogues (90.6%) were com-
pleted successfully, and the average number of turns to
completion was 5.59 (if no correct venue was offered the
full number of turns was counted).
</bodyText>
<sectionHeader confidence="0.996507" genericHeader="conclusions">
4 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.9999909">
This paper has investigated a new agenda-based user sim-
ulation technique for bootstrapping a statistical dialogue
manager without access to training data. Evaluation re-
sults show that, even with manually set model parame-
ters, the simulator produces dialogue behaviour realistic
enough for training and testing a prototype system. While
the results demonstrate that the learned policy works well
for real users, it is not necessarily optimal. The next step
is hence to use the recorded data to train the simulator,
and to then retrain the DM policy.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999944071428572">
O. Lemon, K. Georgila, and J. Henderson. 2006. Evaluating
Effectiveness and Portability of Reinforcement Learned Di-
alogue Strategies with real users: the TALK TownInfo Eval-
uation. In Proc. ofIEEE/ACL SLT, Palm Beach, Aruba.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A Stochastic
Model of Human-Machine Interaction for Learning Dialog
Strategies. IEEE Trans. on Speech and Audio Processing,
8(1):11–23.
O. Pietquin and T. Dutoit. 2005. A probabilistic framework for
dialog simulation and optimal strategy learning. IEEE Trans.
on Speech and Audio Processing, Special Issue on Data Min-
ing of Speech, Audio and Dialog.
J. Schatzmann, K. Weilhammer, M.N. Stuttle, and S. Young.
2006. A Survey of Statistical User Simulation Tech-
niques for Reinforcement-Learning of Dialogue Manage-
ment Strategies. Knowledge Engineering Review, 21(2):97–
126.
X Wei and AI Rudnicky. 1999. An agenda-based dialog man-
agement architecture for spoken language systems. In Proc.
ofIEEEASRU. Seattle, WA.
J. Williams and S. Young. 2007. Partially Observable Markov
Decision Processes for Spoken Dialog Systems. Computer
Speech and Language, 21(2):231–422.
S. Young, J. Schatzmann, K. Weilhammer, and H. Ye. 2007.
The Hidden Information State Approach to Dialog Manage-
ment. In Proc. ofICASSP, Honolulu, Hawaii.
S. Young. 2006. Using POMDPs for Dialog Management. In
Proc. ofIEEE/ACL SLT, Palm Beach, Aruba.
</reference>
<page confidence="0.998132">
152
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.290228">
<title confidence="0.9990185">Agenda-Based User Simulation Bootstrapping a POMDP Dialogue System</title>
<author confidence="0.983219">Jost Schatzmann</author>
<author confidence="0.983219">Blaise Thomson</author>
<author confidence="0.983219">Karl Weilhammer</author>
<author confidence="0.983219">Hui Ye</author>
<author confidence="0.983219">Steve</author>
<affiliation confidence="0.999292">Cambridge University Engineering</affiliation>
<note confidence="0.446906">Trumpington Street, Cambridge, CB21PZ, United brmt2, kw278, hy216,</note>
<abstract confidence="0.976716333333333">This paper investigates the problem of bootstrapping a statistical dialogue manager without access to training data and proposes a new probabilistic agenda-based method for simulating user behaviour. In experiments with a statistical POMDP dialogue system, the simulator was realistic enough to successfully test the prototype system and train a dialogue policy. An extensive study with human subjects showed that the learned policy was highly competitive, with task completion rates above 90%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>K Georgila</author>
<author>J Henderson</author>
</authors>
<title>Evaluating Effectiveness and Portability of Reinforcement Learned Dialogue Strategies with real users: the TALK TownInfo Evaluation.</title>
<date>2006</date>
<booktitle>In Proc. ofIEEE/ACL SLT,</booktitle>
<location>Palm Beach, Aruba.</location>
<contexts>
<context position="2545" citStr="Lemon et al., 2006" startWordPosition="380" endWordPosition="383">ments for training a statistical DM (typically » 104 dialogues). 1.2 User Simulation-Based Training In recent years, a number of research groups have investigated the use of a two-stage simulation-based setup. A statistical user model is first trained on a limited amount of dialogue data and the model is then used to simulate dialogues with the interactively learning DM (see Schatzmann et al. (2006) for a literature review). The simulation-based approach assumes the presence of a small corpus of suitably annotated in-domain dialogues or out-of-domain dialogues with a matching dialogue format (Lemon et al., 2006). In cases when no such data is available, handcrafted values can be assigned to the model parameters given that the model is sufficiently simple (Levin et al., 2000; Pietquin and Dutoit, 2005) but the performance of dialogue policies learned this way has not been evaluated using real users. 1.3 Paper Outline This paper presents a new probabilistic method for simulating user behaviour based on a compact representation of the user goal and a stack-like user agenda. The model provides an elegant way of encoding the relevant dialogue history from a user’s point of view and has a very small parame</context>
</contexts>
<marker>Lemon, Georgila, Henderson, 2006</marker>
<rawString>O. Lemon, K. Georgila, and J. Henderson. 2006. Evaluating Effectiveness and Portability of Reinforcement Learned Dialogue Strategies with real users: the TALK TownInfo Evaluation. In Proc. ofIEEE/ACL SLT, Palm Beach, Aruba.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
<author>W Eckert</author>
</authors>
<title>A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies.</title>
<date>2000</date>
<booktitle>IEEE Trans. on Speech and Audio Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="2710" citStr="Levin et al., 2000" startWordPosition="408" endWordPosition="411"> use of a two-stage simulation-based setup. A statistical user model is first trained on a limited amount of dialogue data and the model is then used to simulate dialogues with the interactively learning DM (see Schatzmann et al. (2006) for a literature review). The simulation-based approach assumes the presence of a small corpus of suitably annotated in-domain dialogues or out-of-domain dialogues with a matching dialogue format (Lemon et al., 2006). In cases when no such data is available, handcrafted values can be assigned to the model parameters given that the model is sufficiently simple (Levin et al., 2000; Pietquin and Dutoit, 2005) but the performance of dialogue policies learned this way has not been evaluated using real users. 1.3 Paper Outline This paper presents a new probabilistic method for simulating user behaviour based on a compact representation of the user goal and a stack-like user agenda. The model provides an elegant way of encoding the relevant dialogue history from a user’s point of view and has a very small parameter set so that manually chosen priors can be used to bootstrap the DM training and testing process. In experiments presented in this paper, the agendabased simulato</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>E. Levin, R. Pieraccini, and W. Eckert. 2000. A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies. IEEE Trans. on Speech and Audio Processing, 8(1):11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Pietquin</author>
<author>T Dutoit</author>
</authors>
<title>A probabilistic framework for dialog simulation and optimal strategy learning.</title>
<date>2005</date>
<booktitle>IEEE Trans. on Speech and Audio Processing, Special Issue on Data Mining of Speech, Audio and Dialog.</booktitle>
<contexts>
<context position="2738" citStr="Pietquin and Dutoit, 2005" startWordPosition="412" endWordPosition="415">simulation-based setup. A statistical user model is first trained on a limited amount of dialogue data and the model is then used to simulate dialogues with the interactively learning DM (see Schatzmann et al. (2006) for a literature review). The simulation-based approach assumes the presence of a small corpus of suitably annotated in-domain dialogues or out-of-domain dialogues with a matching dialogue format (Lemon et al., 2006). In cases when no such data is available, handcrafted values can be assigned to the model parameters given that the model is sufficiently simple (Levin et al., 2000; Pietquin and Dutoit, 2005) but the performance of dialogue policies learned this way has not been evaluated using real users. 1.3 Paper Outline This paper presents a new probabilistic method for simulating user behaviour based on a compact representation of the user goal and a stack-like user agenda. The model provides an elegant way of encoding the relevant dialogue history from a user’s point of view and has a very small parameter set so that manually chosen priors can be used to bootstrap the DM training and testing process. In experiments presented in this paper, the agendabased simulator was used to train a statis</context>
</contexts>
<marker>Pietquin, Dutoit, 2005</marker>
<rawString>O. Pietquin and T. Dutoit. 2005. A probabilistic framework for dialog simulation and optimal strategy learning. IEEE Trans. on Speech and Audio Processing, Special Issue on Data Mining of Speech, Audio and Dialog.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>M N Stuttle</author>
<author>S Young</author>
</authors>
<title>A Survey of Statistical User Simulation Techniques for Reinforcement-Learning of Dialogue Management Strategies. Knowledge Engineering Review,</title>
<date>2006</date>
<volume>21</volume>
<issue>2</issue>
<pages>126</pages>
<contexts>
<context position="2328" citStr="Schatzmann et al. (2006)" startWordPosition="346" endWordPosition="350"> two methods enables the system designer to test the implementation of the statistical DM and the learning algorithm. Moreover, the size of the recorded corpus (typically « 103 dialogues) usually falls short of the requirements for training a statistical DM (typically » 104 dialogues). 1.2 User Simulation-Based Training In recent years, a number of research groups have investigated the use of a two-stage simulation-based setup. A statistical user model is first trained on a limited amount of dialogue data and the model is then used to simulate dialogues with the interactively learning DM (see Schatzmann et al. (2006) for a literature review). The simulation-based approach assumes the presence of a small corpus of suitably annotated in-domain dialogues or out-of-domain dialogues with a matching dialogue format (Lemon et al., 2006). In cases when no such data is available, handcrafted values can be assigned to the model parameters given that the model is sufficiently simple (Levin et al., 2000; Pietquin and Dutoit, 2005) but the performance of dialogue policies learned this way has not been evaluated using real users. 1.3 Paper Outline This paper presents a new probabilistic method for simulating user behav</context>
</contexts>
<marker>Schatzmann, Weilhammer, Stuttle, Young, 2006</marker>
<rawString>J. Schatzmann, K. Weilhammer, M.N. Stuttle, and S. Young. 2006. A Survey of Statistical User Simulation Techniques for Reinforcement-Learning of Dialogue Management Strategies. Knowledge Engineering Review, 21(2):97– 126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wei</author>
<author>AI Rudnicky</author>
</authors>
<title>An agenda-based dialog management architecture for spoken language systems.</title>
<date>1999</date>
<booktitle>In Proc. ofIEEEASRU.</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="4852" citStr="Wei and Rudnicky, 1999" startWordPosition="762" endWordPosition="765">ence of state transitions and dialogue acts2. At any time t, the user is in a state S, takes action au, transitions into the intermediate state S&apos;, receives machine action am, and transitions into the next state S&apos;&apos; where the cycle restarts. S → au → S&apos; → am → S&apos;&apos; → · · · (1) Assuming a Markovian state representation, user behaviour can be decomposed into three models: P(au|S) for action selection, P(S&apos;|au, S) for the state transition into S&apos;, and P(S&apos;&apos;|am, S&apos;) for the transition into S&apos;&apos;. 2.2 Goal- and Agenda-Based State Representation Inspired by agenda-based methods to dialogue management (Wei and Rudnicky, 1999) the approach described here factors the user state into an agenda A and a goal G. S = (A, G) and G = (C, R) (2) During the course of the dialogue, the goal G ensures that the user behaves in a consistent, goal-directed manner. G consists of constraints C which specify the required venue, eg. a centrally located bar serving beer, and requests R which specify the desired pieces of information, eg. the name, address and phone number (cf. Fig. 1). The user agenda A is a stack-like structure containing the pending user dialogue acts that are needed to elicit the information specified in the goal. </context>
</contexts>
<marker>Wei, Rudnicky, 1999</marker>
<rawString>X Wei and AI Rudnicky. 1999. An agenda-based dialog management architecture for spoken language systems. In Proc. ofIEEEASRU. Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
<author>S Young</author>
</authors>
<title>Partially Observable Markov Decision Processes for Spoken Dialog Systems.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="13509" citStr="Williams and Young, 2007" startWordPosition="2298" endWordPosition="2301"> can be conditioned on simple boolean flags such as ”Does a,,,, ask for a slot in the constraint set?”, ”Does a,,,, signal that no item in the database matches the given constraints?”, etc. The model parameter set is then sufficiently small for handcrafted values to be assigned to the probabilities. 3 Evaluation 3.1 Training the HIS Dialogue Manager The Hidden Information State (HIS) model is the first trainable and scalable implementation of a statistical spoken dialog system based on the Partially-Observable Markov Decision Process (POMDP) model of dialogue (Young, 2006; Young et al., 2007; Williams and Young, 2007). POMDPs extend the standard Markov-DecisionProcess model by maintaining a beliefspace, i.e. a probability distribution over dialogue states, and hence provide an explicit model of the uncertainty present in humanmachine communication. The HIS model uses a grid-based discretisation of the continuous state space and online c-greedy policy iteration. Fig. 2 shows a typical training run over 60,000 simulated dialogues, starting with a random policy. User goals are randomly generated and an (arbitrary) reward function assigning 20 points for successful completion and -1 for every dialogue turn is </context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>J. Williams and S. Young. 2007. Partially Observable Markov Decision Processes for Spoken Dialog Systems. Computer Speech and Language, 21(2):231–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>H Ye</author>
</authors>
<title>The Hidden Information State Approach to Dialog Management.</title>
<date>2007</date>
<booktitle>In Proc. ofICASSP,</booktitle>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="3388" citStr="Young et al., 2007" startWordPosition="522" endWordPosition="525">e policies learned this way has not been evaluated using real users. 1.3 Paper Outline This paper presents a new probabilistic method for simulating user behaviour based on a compact representation of the user goal and a stack-like user agenda. The model provides an elegant way of encoding the relevant dialogue history from a user’s point of view and has a very small parameter set so that manually chosen priors can be used to bootstrap the DM training and testing process. In experiments presented in this paper, the agendabased simulator was used to train a statistical POMDPbased (Young, 2006; Young et al., 2007) DM. Even without any training of its model parameters, the agendabased simulator was able to produce dialogue behaviour realistic enough to train a competitive dialogue policy. An extensive study1 with 40 human subjects showed that task completion with the learned policy was above 90% despite a mix of native and non-native speakers. 1This research was partly funded by the EU FP6 TALK Project. The system evaluation was conducted in collaboration with O. Lemon, K. Georgila and J. Henderson at Edinburgh University and their work is gratefully acknowledged. 149 Proceedings of NAACL HLT 2007, Comp</context>
<context position="13482" citStr="Young et al., 2007" startWordPosition="2294" endWordPosition="2297">&apos; and C&apos;. Instead it can be conditioned on simple boolean flags such as ”Does a,,,, ask for a slot in the constraint set?”, ”Does a,,,, signal that no item in the database matches the given constraints?”, etc. The model parameter set is then sufficiently small for handcrafted values to be assigned to the probabilities. 3 Evaluation 3.1 Training the HIS Dialogue Manager The Hidden Information State (HIS) model is the first trainable and scalable implementation of a statistical spoken dialog system based on the Partially-Observable Markov Decision Process (POMDP) model of dialogue (Young, 2006; Young et al., 2007; Williams and Young, 2007). POMDPs extend the standard Markov-DecisionProcess model by maintaining a beliefspace, i.e. a probability distribution over dialogue states, and hence provide an explicit model of the uncertainty present in humanmachine communication. The HIS model uses a grid-based discretisation of the continuous state space and online c-greedy policy iteration. Fig. 2 shows a typical training run over 60,000 simulated dialogues, starting with a random policy. User goals are randomly generated and an (arbitrary) reward function assigning 20 points for successful completion and -1 </context>
</contexts>
<marker>Young, Schatzmann, Weilhammer, Ye, 2007</marker>
<rawString>S. Young, J. Schatzmann, K. Weilhammer, and H. Ye. 2007. The Hidden Information State Approach to Dialog Management. In Proc. ofICASSP, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
</authors>
<title>Using POMDPs for Dialog Management.</title>
<date>2006</date>
<booktitle>In Proc. ofIEEE/ACL SLT,</booktitle>
<location>Palm Beach, Aruba.</location>
<contexts>
<context position="3367" citStr="Young, 2006" startWordPosition="520" endWordPosition="521">ce of dialogue policies learned this way has not been evaluated using real users. 1.3 Paper Outline This paper presents a new probabilistic method for simulating user behaviour based on a compact representation of the user goal and a stack-like user agenda. The model provides an elegant way of encoding the relevant dialogue history from a user’s point of view and has a very small parameter set so that manually chosen priors can be used to bootstrap the DM training and testing process. In experiments presented in this paper, the agendabased simulator was used to train a statistical POMDPbased (Young, 2006; Young et al., 2007) DM. Even without any training of its model parameters, the agendabased simulator was able to produce dialogue behaviour realistic enough to train a competitive dialogue policy. An extensive study1 with 40 human subjects showed that task completion with the learned policy was above 90% despite a mix of native and non-native speakers. 1This research was partly funded by the EU FP6 TALK Project. The system evaluation was conducted in collaboration with O. Lemon, K. Georgila and J. Henderson at Edinburgh University and their work is gratefully acknowledged. 149 Proceedings of</context>
<context position="13462" citStr="Young, 2006" startWordPosition="2292" endWordPosition="2293">ble a.,,,,, R&apos; and C&apos;. Instead it can be conditioned on simple boolean flags such as ”Does a,,,, ask for a slot in the constraint set?”, ”Does a,,,, signal that no item in the database matches the given constraints?”, etc. The model parameter set is then sufficiently small for handcrafted values to be assigned to the probabilities. 3 Evaluation 3.1 Training the HIS Dialogue Manager The Hidden Information State (HIS) model is the first trainable and scalable implementation of a statistical spoken dialog system based on the Partially-Observable Markov Decision Process (POMDP) model of dialogue (Young, 2006; Young et al., 2007; Williams and Young, 2007). POMDPs extend the standard Markov-DecisionProcess model by maintaining a beliefspace, i.e. a probability distribution over dialogue states, and hence provide an explicit model of the uncertainty present in humanmachine communication. The HIS model uses a grid-based discretisation of the continuous state space and online c-greedy policy iteration. Fig. 2 shows a typical training run over 60,000 simulated dialogues, starting with a random policy. User goals are randomly generated and an (arbitrary) reward function assigning 20 points for successfu</context>
</contexts>
<marker>Young, 2006</marker>
<rawString>S. Young. 2006. Using POMDPs for Dialog Management. In Proc. ofIEEE/ACL SLT, Palm Beach, Aruba.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>