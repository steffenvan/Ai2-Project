<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001092">
<title confidence="0.9975615">
Beauty Before Age?
Applying Subjectivity to Automatic English Adjective Ordering
</title>
<author confidence="0.997534">
Felix Hill
</author>
<affiliation confidence="0.996156666666667">
Dept. of Theoretical &amp; Applied Linguistics
and Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.993645">
Cambridge CB3 9DA, UK
</address>
<email confidence="0.998435">
fh295@cam.ac.uk
</email>
<sectionHeader confidence="0.98699" genericHeader="abstract">
Abstract
</sectionHeader>
<figureCaption confidence="0.648395333333333">
The preferred order of pre-nominal adjectives
in English is determined primarily by seman-
tics. Nevertheless, Adjective Ordering (AO)
systems do not generally exploit semantic fea-
tures. This paper describes a system that or-
ders adjectives with significantly above-
chance accuracy (73.0%) solely on the basis
of semantic features pertaining to the cogni-
tive-semantic dimension of subjectivity. The
results indicate that combining such semantic
approaches with current methods could result
in more accurate and robust AO systems.
</figureCaption>
<sectionHeader confidence="0.942333" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.99991725">
As a significant body of linguistic research has
observed (see e.g. Quirk et al. (1 985)), English pre-
nominal adjective strings exhibit subtle order re-
strictions. Although example (2), below, does not
represent a clear-cut violation of established
grammatical principles, it would sound distinctly
unnatural to native speakers in the majority of con-
texts, in contrast to the entirely unproblematic (1).
</bodyText>
<listItem confidence="0.989428">
(1) He poked it with a long metal fork
(2) ? He poked it with a metal long fork
</listItem>
<bodyText confidence="0.999849806451613">
The problem of determining the principles that go-
vern Adjective Ordering (henceforth, AO) in Eng-
lish has been studied from a range of academic
perspectives, including philosophy, linguistics,
psychology and neuroscience. AO is also of inter-
est in the field of Natural Language Processing
(NLP), since a method that consistently selects
felicitous orders would serve to improve the output
of language modeling and generation systems.
Previous NLP approaches to AO infer the
ordering of adjective combinations from instances
of the same, or superficially similar, combinations
in training corpora (Shaw &amp; Hatzivassiloglou,
1 999) (Malouf, 2000), or from distributional ten-
dencies of the adjectives in multiple-modifier
strings (Mitchell, 200 9) (Dunlop, Mitchell, &amp;
Roark, 2010). Such methods are susceptible to
data sparseness, since the combinations from
which they learn are rare in everyday language.
By contrast, the approach taken here deter-
mines AO based on semantic features of adjec-
tives, guided by the theoretical observation that the
cognitive notion of subjectivity governs ordering in
the general case (Adamson, 2000). The semantic
features developed are each highly significant pre-
dictors of AO, and they combine to classify com-
binations with 73.0% accuracy. These preliminary
results indicate that semantic AO systems can per-
form comparably to existing systems, and that
classifiers exploiting semantic and direct evidence
might surpass the current best-performing systems.
</bodyText>
<sectionHeader confidence="0.920392" genericHeader="method">
2 Previous research
</sectionHeader>
<bodyText confidence="0.999949">
The subtle nature of human ordering preferences
makes AO a particularly challenging NLP task. In
perhaps the first specific attempt to address the
problem, Shaw and Hatzivassiloglou (1 999) apply
a direct evidence method. For a given adjective
combination in the test data, their system searches
a training corpus and selects the most frequent or-
dering of that combination. Because there is no
basis to determine the order of adjective combina-
tions that are not in the training data, Shaw and
Hatzivassiloglou extend the domain of the classifi-
</bodyText>
<page confidence="0.991502">
11
</page>
<note confidence="0.603791">
Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 11–16,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99994142">
er by assuming transitivity in the order relation,
increasing the coverage with only a small reduc-
tion in accuracy. Nevertheless, the system remains
highly dependent on the domain and quantity of
training data. For example, accuracy is 92% when
training and test data are both within the medical
domain but only 54% in cross-domain contexts.
Malouf (2000) combines a direct evidence
approach with an alternative method for extending
the domain of his classifier. His system infers the
order of unseen combinations from `similar&apos; seen
combinations, where similarity is defined purely in
terms of morphological form. The method works
by exploiting a degree of correlation between form
and order (e.g. capital letters indicate nominal
modifiers, which typically occur to the right).
Mitchell (200 9) applies a less `direct&apos; ap-
proach, clustering adjectives based on their posi-
tion in multiple-modifier strings. Although
Mitchell&apos;s classifier requires no direct evidence,
data sparseness is still an issue because the strings
from which the system learns are relatively infre-
quent in everyday language. Dunlop et al. (2010)
apply Multiple Sequence Alignment (MSA), a sta-
tistical technique for automatic sequence ordering,
which, as with Malouf&apos;s system, quantifies word-
similarity based solely on morphological features.
Despite the greater sophistication of these more
recent approaches, Mitchell et al. (2011) showed
that a simple n-gram (direct evidence) classifier
trained on 170 million words of New York Times
and Wall Street Journal text and tested on the
Brown Corpus (82.3% accuracy) outperforms both
the clustering (6 9.0%) and MSA (81.8%) methods.
Wulff (2003) uses Linear Discriminant
Analysis (LDA) to quantify the effects of various
potential AO correlates, and confirms that seman-
tic features are better predictors than morphologi-
cal and syntactic features. The features, extracted
from the 10-million word Spoken British National
Corpus (BNC) and weighted by LDA, combine to
predict unseen adjective orders with 72% accuracy.
Wulff&apos;s study is unique in applying seman-
tics to the problem, although her focus is theoreti-
cal and several features are implemented manually.
The next section describes the theoretical basis for
a fully-automated semantic approach to AO that
could help to resolve the issues of data sparsity and
domain dependence associated with the direct evi-
dence methods described above.
</bodyText>
<sectionHeader confidence="0.702152" genericHeader="method">
2. 1 The subjectivity hypothesis
</sectionHeader>
<bodyText confidence="0.999714214285714">
Although phonetic, morphological and syntactic
factors influence AO in specific contexts, there is
consensus in the theoretical literature that seman-
tics is the determining factor in the general case
(see Quirk et al. (1 985) for further discussion).
Several semantic theories of AO make use of the
cognitive linguistic notion of subjectivity (Quirk et
al. 1 985; Hetzron, 1 978; Adamson 2000). Subjec-
tivity in this context refers to the degree to which
an utterance can or cannot be interpreted indepen-
dently of the speaker&apos;s perspective (Langacker,
1 991). For example, the deictic utterance (3) is
more subjective than (4) since its truth depends on
the speaker&apos;s location at the time of utterance.
</bodyText>
<listItem confidence="0.999501">
(3) James is sitting across the table
(4) James is sitting opposite Sam
</listItem>
<bodyText confidence="0.999048818181818">
In relation to AO, Quirk et al, Hetzron and Adam-
son each support some form of the subjectivity hy-
pothesis: that more subjective modifiers generally
occur to the left of less subjective modifiers in pre-
nominal strings. For example, in (5) the adjective
big tells us about the relation between the car and
the speaker&apos;s idea of typical car size. This ascrip-
tion is less objectively verifiable than that of car
color, so big occurs further from the head noun.
The position of oncoming in (6) reflects the high
inherent subjectivity of deictic modifiers.
</bodyText>
<listItem confidence="0.9980335">
(5) A big red Italian car (BNC)
(6) An oncoming small black car (BNC)
</listItem>
<figureCaption confidence="0.997714">
Figure 1: Diachronic variation of preferred AO
</figureCaption>
<bodyText confidence="0.99955525">
To illustrate a process of changing AO preferences
that can be explained in a compelling way by the
subjectivity hypothesis, the 1 trillion-word Google
n-Gram Viewer was queried (Figure 1). The two
</bodyText>
<figure confidence="0.5493752">
Frequency
(% corpus)
&amp;quot;young gay man&amp;quot;
&amp;quot;gay young man&amp;quot;
Year
</figure>
<page confidence="0.982639">
12
</page>
<bodyText confidence="0.99954625">
lines indicate the frequency of the strings ‘gay
young man’ and ‘young gay man’ in the Corpus
from 1 950 to 2000, as the pre-eminent meaning of
gay evolved from the subjective merry to the cate-
gorical, well-defined homosexual. As the graph
shows, this reduction in subjectivity has been ac-
companied by a marked increase in the tendency of
gay to appear closer to the noun in such strings.
</bodyText>
<sectionHeader confidence="0.961368" genericHeader="method">
3 System design
</sectionHeader>
<bodyText confidence="0.997065375">
The AO system described below applies the theo-
retical findings presented above by extracting from
training data various subjectivity features of adjec-
tives and applying this information to classify in-
put orderings as correct or incorrect.1 System
operation and evaluation consisted of 5 stages.
Extracting feature profiles: The 200 highest-
frequency adjectives in the BNC were extracted.
Following Wulff (2003, p. 6), three items, other,
only and very were removed from this list because
they occur in right-branching structures. For the
remaining adjectives, a `profile&apos; of feature values
(c.f. Table 1, below), was extracted from 24 mil-
lion words (Sections A-C) of the written BNC.
Generating gold-standard orderings: From the
1 97 adjectives, 1 9,306 unordered pairs {Ai , A2}
were generated. The bigram frequencies of the
strings [A1 ,A2] and [A2 ,A1] were then extracted
from the 1 billion-word Google n-gram Corpus.
From this data, the 12,000 pairs [A, , A21 with the
largest proportional difference in frequency be-
tween [A, , A21 and 142 , A,] were selected.
Defining test and training sets: A set of 12,000
ordered triples [Ai , A2 , S[A1 ,A2]] was generated,
where S[A1,A2] is an indicator function taking the
value 1 if [Ai , A21 is the preferred ordering in the
Google corpus and 0 if [A2 ,A1] is preferred.
Some of the triples were re-ordered at random to
leave an equal number of preferred and dispre-
ferred orderings in the data. These triples were
populated with feature profiles, to create vectors
[AA1, ... , f Al, f1 2, ... f A2, 16[A1 ,A2] �
</bodyText>
<footnote confidence="0.717353">
1 The system operates on adjectival and nominal modifiers but
not on articles, determiners, degree modifiers and other non-
adjectival pre-modifiers.
</footnote>
<bodyText confidence="0.994834611111111">
where ���
�� �� is the value of the feature of the ad-
jective Ai, and n is the total number of features.
The set of vectors was then randomly partitioned in
the ratio 80:20 for training and testing respectively.
Training the classifier: A logistic regression was
applied to the set of training vectors, in which the
first 2n elements of the vectors were independent
variables and the final element was the dependent
variable. Logistic regression has been shown to
be preferable to alternatives such as Ordinary Least
Squares and LDA for binary outcome classification
if, as in this case, the independent variables are not
normally distributed (Press &amp; Wilson, 1 978).
Evaluation: Performance was determined by the
number of pairs in the test data correctly ordered
by the classifier. Steps 3-5 were repeated 4 times
(5-fold cross-validation), with the scores averaged.
</bodyText>
<sectionHeader confidence="0.912226" genericHeader="method">
3. 1 The Features
</sectionHeader>
<bodyText confidence="0.9976918">
Of the features included in the model,
COMPARABILITY and POLARITY are shown to
correlate with human subjectivity judgments by
Wiebe and colleagues (see e.g. Hatzivassiloglou &amp;
Wiebe, 2000). The remainder are motivated by
observations in the theoretical literature.
MODIFIABILITY: Gradable adjectives, such as
hot or happy, tend to be more subjective than pro-
totypically categorical adjectives, such as square
or black (Hetzron, 1 978). Unlike categorical ad-
jectives they admit modification by intensifiers
(Paradis, 1 997). Therefore, the feature
MODIFIABILITY is defined as the conditional
probability that an adjective occurs immediately
following an intensifier given that it occurs at all.2
</bodyText>
<equation confidence="0.98842875">
∑ *∈, �&amp;&apos;(&amp;quot;�), ��#
����������� !&amp;quot;�# =
f&amp;&apos;(&amp;quot;A#
M = {d&apos;-&amp;&apos;&apos; )odifi&apos;&amp;.}
</equation>
<bodyText confidence="0.998494">
[/, !] i. ℎ&apos; bi-&amp;a) ′1o&amp;d / follo1&apos;d b! 1o&amp;d !′
COMPARABILITY: Gradable adjectives also
have comparative and superlative forms, whereas
prototypically categorical adjectives do not. Given
the association between gradability and subjectivi-
ty, the feature COMPARABILITY is defined as the
probability of an adjective occurring in compara-
tive or superlative form given it occurs at all.
</bodyText>
<footnote confidence="0.417741">
2 The set of intensifiers is taken from (Paradis, 1 997).
</footnote>
<page confidence="0.988191">
13
</page>
<figure confidence="0.8892255">
4
A = comparative form of A A = superlative form of A
</figure>
<bodyText confidence="0.997416916666667">
PREDICATIVITY: Adjectives can be applied in
both attributive (`the red car’), and predicative
(`the car is red’) constructions. Bolinger (1 967)
suggests that predicative constructions are concep-
tualized more dynamically or temporarily than at-
tributive constructions. Since dynamic properties
are generally ascribed more subjectively than per-
manent properties (Langacker, 1 991), Bolinger&apos;s
intuition implies an association between subjectivi-
ty and predicative constructions. Indeed, many
objective modifiers sit uncomfortably in predica-
tive contexts, as shown by (7) and (8).
</bodyText>
<listItem confidence="0.9512745">
(7) I live in a brick house
(8) ? The house I live in is brick
</listItem>
<bodyText confidence="0.973554277777778">
The feature PREDICATIVITY is therefore defined
as the probability that an adjective occurs in a pre-
dicative construction given that it occurs at all.
The measure is implemented by counting the num-
ber of times the adjective immediately follows
some form of an English copula verb.3
C = set of English copula verbs in all inflected forms
POLARITY: An adjective is said to be polar if it
typically attributes a positive (kind, healthy,
strong) or negative (poor, selfish, rotten) characte-
ristic. Semi-supervised methods for automatically
detecting adjective polarity have been developed
(Hatzivassiloglou &amp; McKeown, 1 997), and applied
to subjectivity analysis by Wiebe (2000).
POLARITY is implemented as a binary feature,
whose value depends on whether or not the adjec-
tive appears in a list of 1,300 polar adjectives ex-
tracted by Hatzivassiloglou &amp; Mackeown.
</bodyText>
<figure confidence="0.9318888">
Polari ty(A) _ 1 if A E PUN
-
10 if A 0 PUN
P = { adjectives labelled as positive)
N = {adjectives labelled as negative)
</figure>
<page confidence="0.7521665">
3 The copula verbs list was compiled manually by the author.
14
</page>
<bodyText confidence="0.9996186">
ADVERBIABILITY: Quirk (1 985, p 133 9) notes
that evaluative adjectives tend to develop derived
adverbial forms, whereas more objective adjectives
do not. For example, nice, beautiful and, careful
correspond to the adverbs nicely, beautifully, and
carefully, whereas no such derived forms exist for
the more objective adjectives male, English and
brown. The ADVERBIABILITY of an adjective is
defined as the ratio of derived adverbial forms to
total base and adverbial forms in the corpus.
</bodyText>
<equation confidence="0.876591333333333">
Adverbiability(A) = freq(A&apos;)
freq(A) + freq(A&apos;)
A&apos; = adverbial form derived from A
</equation>
<bodyText confidence="0.999655772727273">
NOMINALITY: Wullf (2003) reports statistical
evidence that more `noun-like&apos; modifiers appear
closer to the head in modifying strings. Combina-
tions such as `bread knife’ or `police car’, often
analyzed as noun-noun compounds rather than
modifier/noun combinations, represent the clearest
such examples. Amongst more prototypical adjec-
tives, some, such as green, or male have nominal
senses (`village green’, `unidentified male’), whe-
reas others do not. Separately, Hatzivassiloglou
and Wiebe (2000) report a statistical correlation
between the number of adjectives in a text and
human judgments of subjectivity. These observa-
tions suggest that adjectives are inherently more
subjective than nouns, and further that noun-like
`behavior&apos; might indicate relative objectivity with-
in the class of adjectives. Consequently, the fea-
ture NOMINALITY is defined, following Wulff, as
the probability that an adjective is tagged as a noun
given that it is tagged as either an adjective or a
noun. It is the only feature that is expected to exhi-
bit an inverse correlation with subjectivity.
</bodyText>
<equation confidence="0.9472618">
freq( An)
�ominali ty(A) =
freq(Aa) + freq( An)
An = adjective A tagged as noun
Aa = adjective A tagged as adjective
</equation>
<table confidence="0.988130285714286">
new good old different local
MODIF 0.0010 0.0529 0.0208 0.0887 0.0004
COM 0.007 9 0.4881 0.2805 0.0011 0.0045
PRED 0.0100 0.1018 0.028 9 0.0806 0.0069
POL 0.0000 1.0000 0.0000 0.0000 0.0000
ADV 0.0220 0.0008 0.0000 0.0318 0.0478
NOM 0.2 900 0.0 999 0.0113 0.0000 0.0212
</table>
<tableCaption confidence="0.999852">
Table 1: Example feature profiles
</tableCaption>
<equation confidence="0.982347">
freq(A�
Comparabili ty(A) =
) + freq(A)
freq(A) + freq(A) + freq(A)
Predicativiy(A) = freq(A)
Ececfreq([c,A])
</equation>
<sectionHeader confidence="0.99918" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9999408">
The performance of the classifier is promising with
respect to the intuition that semantic features can
be usefully applied to AO systems. A chi-square
test reveals the features collectively to be highly
significant predictors of AO (X2 = 2257.25, 3 &lt;
0.001***). Once trained, the system orders unseen
combinations in the test data with accuracy of
73.0%, as detailed in Table 2. This figure is not
directly comparable with previous work because of
differences in the evaluation framework.
</bodyText>
<tableCaption confidence="0.943421">
Table 2: Overall results of model cross-validation
</tableCaption>
<bodyText confidence="0.983866121212121">
It is notable that the accuracy of the classifi-
er rises to 86.2% when the test data is hand-picked
as the 3000 pairs for which the strength of ordering
preference is highest.4 This suggests that the ap-
proach could be particularly effective at detecting
highly unnatural combinations. Moreover, the per-
formance when tested on the 3000 (unseen) pairs
with the lowest ordering preference is 70.1%, indi-
cating the potential to cope well with marginal cas-
es and rare combinations.
As Table 3 shows, all features apart from
COMPARABILITY are statistically significant pre-
dictors in the model (p &lt; 0.001***). In addition,
the mean value of each feature over adjectives in
first position Ai differs significantly from the mean
over adjectives in second position A2 ( ≥ 28.07
in each case, df = 11,283). Whilst relatively the
weakest predictor, COMPARABILITY in isolation
does predict AO at above-chance
cy (58.7%, 3 &lt; 0.001***)
4 The 3000 pairs for which the proportional preference for one
ordering over another in the Google n-Gram corpus is highest
and for which the total frequency of the pair exceeds 500.
. Its low significance in the overall model re-
flects its high level of interaction with other fea-
tures; in particular, MODIFIABILITY (Pearson
Correlation: .367, 3 &lt; 0.001***). The relative
magnitude of the model coefficients is not infor-
mative, since the measurement scale is not com-
mon to all features. Nevertheless, the negative
regression coefficient of NOMINALITY confirms
that this feature correlates inversely with distance
from the noun.
</bodyText>
<table confidence="0.9987051">
Fea- Regression Predictor Perform- Compari-
ture Coefficient Signifi- ance in son of
cance Isolation A1 / A2
Means
MODIF 5.205 .000 62. 9% 0.000
COM .177 .381 58.7% 0.000
PRED 3.630 .000 68.6% 0.000
POL .33 9 .000 60.4% 0.000
ADV 1.503 .000 62.8% 0.000
NOM -.405 .000 58.4% 0.000
</table>
<tableCaption confidence="0.999822">
Table 3: Influence of individual features
</tableCaption>
<bodyText confidence="0.999713695652174">
To test the influence of the training corpus size on
system performance, features were extracted from
BNC Section A (7 million words) rather than Sec-
tions A-C (24 million words) in a separate experi-
ment. This adjustment resulted in a reduction in
classifier accuracy from 73.0% to 71.4%, indicat-
ing that performance could be significantly im-
proved by training on the full BNC or even larger
corpora. Further improvements could be achieved
through the combination of semantic and `direct&apos;
features. To illustrate this, the feature
LEFTTENDENCY, a measure of the likelihood that
an adjective occurs immediately to the left of
another adjective in the training data, was added.
This adjustment raised the classifier accuracy from
73.0% to 76.3%. It should also be noted that many
of the features in the current system are extracted
via measures that approximate syntactic dependen-
cy with bigram context. It is an empirical question
whether the additional complexity associated with
more precise measures (for example, applying de-
pendency parsing) would be justified by perfor-
mance improvements.
</bodyText>
<figure confidence="0.997618416666667">
Predicted
Training Data
Incorrect Correct % Correct
Incorrect 2773 1637 62.
Correct 1120 4101 78.5
Overall% 71.4
Test Data
Incorrect Correct % Correct
Incorrect 6 96 370 65.3
Correct 270 1031 7 9.2
Overall% 73.0
Observed
</figure>
<page confidence="0.937548">
15
</page>
<sectionHeader confidence="0.992359" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998129695652174">
This paper has tested the efficacy of applying au-
tomatic subjectivity quantification to the problem
of AO. The reported results highlight the utility of
such semantically oriented approaches. Although
direct comparison with existing systems was
beyond the scope of this study, exploratory analys-
es suggested that a refined version of this system
might compare favorably with reported bench-
marks, if trained on a corpus of comparable size.
Nevertheless, the comparatively weak per-
formance of the present system on previously seen
examples (`underfitting&apos;, see Table 2) is strong
evidence that six features alone are insufficient to
capture the complexity of ordering patterns.
Therefore, beyond the adjustments discussed
above, the next stage in this research will evaluate
the effects of combining semantic features with
direct evidence in a single system. Other future
work might apply subjectivity features to cluster
adjectives into classes pertinent to AO, perhaps in
combination with independent distributional meas-
ures of semantic similarity. Finally, the approach
presented here for English AO could have applica-
tions across languages, and may also be applicable
to related tasks, such as ordering binomials5, pars-
ing noun phrases (`wild animal hunt’ vs. `wild
birthday party’) and selecting thematically appro-
priate modifiers for a given head noun.
Some interesting theoretical insights also
emerge as a corollary to the results of this study.
The supposition that gradability, polarity, adver-
biability, predicativity and `nouniness&apos; can be as-
sociated, either positively or negatively, with
subjectivity, was confirmed. Moreover, the per-
formance of the classifier lends support to the sta-
tus of subjectivity as a determining principle of
AO, and an important dimension of adjective se-
mantics in general. As such, the reason we say
beautiful English rose, (c.240,000 direct matches
on Google) and not English beautiful rose
(c.2,730) is because beauty is in the eye of the be-
holder, whereas nationality, evidently, is not.
5 Binomials are noun or adjective combinations separated by
coordinating conjunctions, such as tired and emotional and
salt and pepper. Quirk et al. (1 985, p. 1342) observe connec-
tions between binomial ordering and AO.
</bodyText>
<sectionHeader confidence="0.996486" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9788055">
Thanks to Anna Korhonen, Paula Buttery and Syl-
via Adamson for helpful guidance and comments.
</bodyText>
<sectionHeader confidence="0.993789" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999726">
Adamson, S. 2000. Word Order Options and Category
Shift in the Premodifying String. In O. Fischer,
Pathways of Change: Grammaticalization in English
(pp. 3 9-66). Amsterdam: John Benjamins.
Bolinger, D. 1 967. Adjectives in English: Attribution
and Predication. Lingua 18 , 1-34.
Dunlop, A., Mitchell, M. &amp; Roark, B. 2010.
Prenominal Modifier Ordering via Multiple Sequence
Alignment. 2010 Annual Conference of the North
American Chapter of the ACL (HLT-NAACL 2010).
Hatzivassiloglou, V. &amp; McKeown, K. 1 997. Predicting
the Semantic Orientation of Adjectives. Annual
Meeting Assoc. Comp.Ling. ACL &apos;97, 174-181.
Hatzivassiloglou, V. &amp; Wiebe, J. 2000. Effects of
Adjective Orientation and Gradability on Sentence
Subjectivity. International Conference on
Computational Linguistics, COLING- &apos;00.
Hetzron, R. 1 978. On the Relative Order of Adjectives.
In I. H. (Ed.), Language Universals. Tubingen: Narr.
Langacker, R. 1 991. Foundations of Cognitive
Grammar. Stanford, CA: Stanford University Press.
Malouf, R. 2000. The Order of Prenominal Adjectives
in Natural Language Generation. Proc. 38th Annual
Meeting, Assoc. Comp. Linguistics, ACL ’00, 85-92.
Mitchell, M. 200 9. Class-based Ordering of
Prenominal Modifiers. Proc.12th European
Workshop, Nat.Lang. Generation, ENLG &apos;09, 50-57.
Mitchell, M. Dunlop, A. &amp; Roark, B. 2011. Semi-
Supervised Modeling for Prenominal Modifier
Ordering. Proc. 49th Annual Meeting of the Assoc.
Comp. Ling., ACL &apos;11, 236-241.
Paradis, C. 1997. Degree Modifiers of Adjectives in
Spoken British English. Lund: Lund University Press.
Press, S. J. &amp; Wilson, S. 1 978. Choosing Between
Logistic Regression and Discriminant Analysis.
Journal of American Statistical Association, 699-705.
Quirk, R. Greenbaum, A. Leech, G. &amp; Svartvik, J.
1 985. A Comprehensive Grammar of the English
Language. London: Longmans.
Shaw, J. &amp; Hatzivassiloglou, V. 1 999. Ordering Among
Premodifiers. Proc. 37th Annual Meeting, Association
of Computational Linguistics, ACL ’99 , 135-143.
Wiebe, J. 2000. Learning Subjective Adjectives from
Corpora. Proc. 17th National Conference on Artificial
Intelligence (AAAI-2000).
Wulff, S. 2003. A Multifactorial Analysis of Adjective
Order in English. International Journal of Corpus
Linguistics, 245-282.
</reference>
<page confidence="0.998691">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.679490">
<title confidence="0.997357">Beauty Before Age? Applying Subjectivity to Automatic English Adjective Ordering</title>
<author confidence="0.991384">Felix</author>
<affiliation confidence="0.999467333333333">Dept. of Theoretical &amp; Applied and Computer University of</affiliation>
<address confidence="0.697161">Cambridge CB3 9DA,</address>
<email confidence="0.996997">fh295@cam.ac.uk</email>
<abstract confidence="0.999326307692308">The preferred order of pre-nominal adjectives in English is determined primarily by semantics. Nevertheless, Adjective Ordering (AO) systems do not generally exploit semantic features. This paper describes a system that orders adjectives with significantly abovechance accuracy (73.0%) solely on the basis of semantic features pertaining to the cognidimension of The results indicate that combining such semantic approaches with current methods could result in more accurate and robust AO systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Adamson</author>
</authors>
<title>Word Order Options and Category Shift in the Premodifying String. In O.</title>
<date>2000</date>
<journal>Fischer, Pathways of Change: Grammaticalization in English</journal>
<volume>3</volume>
<pages>9--66</pages>
<location>Amsterdam: John Benjamins.</location>
<contexts>
<context position="2380" citStr="Adamson, 2000" startWordPosition="353" endWordPosition="354">nstances of the same, or superficially similar, combinations in training corpora (Shaw &amp; Hatzivassiloglou, 1 999) (Malouf, 2000), or from distributional tendencies of the adjectives in multiple-modifier strings (Mitchell, 200 9) (Dunlop, Mitchell, &amp; Roark, 2010). Such methods are susceptible to data sparseness, since the combinations from which they learn are rare in everyday language. By contrast, the approach taken here determines AO based on semantic features of adjectives, guided by the theoretical observation that the cognitive notion of subjectivity governs ordering in the general case (Adamson, 2000). The semantic features developed are each highly significant predictors of AO, and they combine to classify combinations with 73.0% accuracy. These preliminary results indicate that semantic AO systems can perform comparably to existing systems, and that classifiers exploiting semantic and direct evidence might surpass the current best-performing systems. 2 Previous research The subtle nature of human ordering preferences makes AO a particularly challenging NLP task. In perhaps the first specific attempt to address the problem, Shaw and Hatzivassiloglou (1 999) apply a direct evidence method.</context>
<context position="6301" citStr="Adamson 2000" startWordPosition="949" endWordPosition="950">is for a fully-automated semantic approach to AO that could help to resolve the issues of data sparsity and domain dependence associated with the direct evidence methods described above. 2. 1 The subjectivity hypothesis Although phonetic, morphological and syntactic factors influence AO in specific contexts, there is consensus in the theoretical literature that semantics is the determining factor in the general case (see Quirk et al. (1 985) for further discussion). Several semantic theories of AO make use of the cognitive linguistic notion of subjectivity (Quirk et al. 1 985; Hetzron, 1 978; Adamson 2000). Subjectivity in this context refers to the degree to which an utterance can or cannot be interpreted independently of the speaker&apos;s perspective (Langacker, 1 991). For example, the deictic utterance (3) is more subjective than (4) since its truth depends on the speaker&apos;s location at the time of utterance. (3) James is sitting across the table (4) James is sitting opposite Sam In relation to AO, Quirk et al, Hetzron and Adamson each support some form of the subjectivity hypothesis: that more subjective modifiers generally occur to the left of less subjective modifiers in prenominal strings. F</context>
</contexts>
<marker>Adamson, 2000</marker>
<rawString>Adamson, S. 2000. Word Order Options and Category Shift in the Premodifying String. In O. Fischer, Pathways of Change: Grammaticalization in English (pp. 3 9-66). Amsterdam: John Benjamins.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Bolinger</author>
</authors>
<title>1 967. Adjectives in English: Attribution and Predication.</title>
<journal>Lingua</journal>
<volume>18</volume>
<pages>1--34</pages>
<marker>Bolinger, </marker>
<rawString>Bolinger, D. 1 967. Adjectives in English: Attribution and Predication. Lingua 18 , 1-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dunlop</author>
<author>M Mitchell</author>
<author>B Roark</author>
</authors>
<title>Prenominal Modifier Ordering via Multiple Sequence Alignment.</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the ACL</booktitle>
<contexts>
<context position="2027" citStr="Dunlop, Mitchell, &amp; Roark, 2010" startWordPosition="296" endWordPosition="300">erspectives, including philosophy, linguistics, psychology and neuroscience. AO is also of interest in the field of Natural Language Processing (NLP), since a method that consistently selects felicitous orders would serve to improve the output of language modeling and generation systems. Previous NLP approaches to AO infer the ordering of adjective combinations from instances of the same, or superficially similar, combinations in training corpora (Shaw &amp; Hatzivassiloglou, 1 999) (Malouf, 2000), or from distributional tendencies of the adjectives in multiple-modifier strings (Mitchell, 200 9) (Dunlop, Mitchell, &amp; Roark, 2010). Such methods are susceptible to data sparseness, since the combinations from which they learn are rare in everyday language. By contrast, the approach taken here determines AO based on semantic features of adjectives, guided by the theoretical observation that the cognitive notion of subjectivity governs ordering in the general case (Adamson, 2000). The semantic features developed are each highly significant predictors of AO, and they combine to classify combinations with 73.0% accuracy. These preliminary results indicate that semantic AO systems can perform comparably to existing systems, </context>
<context position="4581" citStr="Dunlop et al. (2010)" startWordPosition="686" endWordPosition="689">en combinations from `similar&apos; seen combinations, where similarity is defined purely in terms of morphological form. The method works by exploiting a degree of correlation between form and order (e.g. capital letters indicate nominal modifiers, which typically occur to the right). Mitchell (200 9) applies a less `direct&apos; approach, clustering adjectives based on their position in multiple-modifier strings. Although Mitchell&apos;s classifier requires no direct evidence, data sparseness is still an issue because the strings from which the system learns are relatively infrequent in everyday language. Dunlop et al. (2010) apply Multiple Sequence Alignment (MSA), a statistical technique for automatic sequence ordering, which, as with Malouf&apos;s system, quantifies wordsimilarity based solely on morphological features. Despite the greater sophistication of these more recent approaches, Mitchell et al. (2011) showed that a simple n-gram (direct evidence) classifier trained on 170 million words of New York Times and Wall Street Journal text and tested on the Brown Corpus (82.3% accuracy) outperforms both the clustering (6 9.0%) and MSA (81.8%) methods. Wulff (2003) uses Linear Discriminant Analysis (LDA) to quantify </context>
</contexts>
<marker>Dunlop, Mitchell, Roark, 2010</marker>
<rawString>Dunlop, A., Mitchell, M. &amp; Roark, B. 2010. Prenominal Modifier Ordering via Multiple Sequence Alignment. 2010 Annual Conference of the North American Chapter of the ACL (HLT-NAACL 2010).</rawString>
</citation>
<citation valid="false">
<authors>
<author>V Hatzivassiloglou</author>
<author>K McKeown</author>
</authors>
<title>1 997. Predicting the Semantic Orientation of Adjectives.</title>
<journal>Annual Meeting Assoc. Comp.Ling. ACL</journal>
<volume>97</volume>
<pages>174--181</pages>
<marker>Hatzivassiloglou, McKeown, </marker>
<rawString>Hatzivassiloglou, V. &amp; McKeown, K. 1 997. Predicting the Semantic Orientation of Adjectives. Annual Meeting Assoc. Comp.Ling. ACL &apos;97, 174-181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>J Wiebe</author>
</authors>
<date>2000</date>
<booktitle>Effects of Adjective Orientation and Gradability on Sentence Subjectivity. International Conference on Computational Linguistics, COLING- &apos;00.</booktitle>
<contexts>
<context position="14621" citStr="Hatzivassiloglou and Wiebe (2000)" startWordPosition="2290" endWordPosition="2293">rms to total base and adverbial forms in the corpus. Adverbiability(A) = freq(A&apos;) freq(A) + freq(A&apos;) A&apos; = adverbial form derived from A NOMINALITY: Wullf (2003) reports statistical evidence that more `noun-like&apos; modifiers appear closer to the head in modifying strings. Combinations such as `bread knife’ or `police car’, often analyzed as noun-noun compounds rather than modifier/noun combinations, represent the clearest such examples. Amongst more prototypical adjectives, some, such as green, or male have nominal senses (`village green’, `unidentified male’), whereas others do not. Separately, Hatzivassiloglou and Wiebe (2000) report a statistical correlation between the number of adjectives in a text and human judgments of subjectivity. These observations suggest that adjectives are inherently more subjective than nouns, and further that noun-like `behavior&apos; might indicate relative objectivity within the class of adjectives. Consequently, the feature NOMINALITY is defined, following Wulff, as the probability that an adjective is tagged as a noun given that it is tagged as either an adjective or a noun. It is the only feature that is expected to exhibit an inverse correlation with subjectivity. freq( An) �ominali t</context>
<context position="10795" citStr="Hatzivassiloglou &amp; Wiebe, 2000" startWordPosition="1699" endWordPosition="1702">n has been shown to be preferable to alternatives such as Ordinary Least Squares and LDA for binary outcome classification if, as in this case, the independent variables are not normally distributed (Press &amp; Wilson, 1 978). Evaluation: Performance was determined by the number of pairs in the test data correctly ordered by the classifier. Steps 3-5 were repeated 4 times (5-fold cross-validation), with the scores averaged. 3. 1 The Features Of the features included in the model, COMPARABILITY and POLARITY are shown to correlate with human subjectivity judgments by Wiebe and colleagues (see e.g. Hatzivassiloglou &amp; Wiebe, 2000). The remainder are motivated by observations in the theoretical literature. MODIFIABILITY: Gradable adjectives, such as hot or happy, tend to be more subjective than prototypically categorical adjectives, such as square or black (Hetzron, 1 978). Unlike categorical adjectives they admit modification by intensifiers (Paradis, 1 997). Therefore, the feature MODIFIABILITY is defined as the conditional probability that an adjective occurs immediately following an intensifier given that it occurs at all.2 ∑ *∈, �&amp;&apos;(&amp;quot;�), ��# ����������� !&amp;quot;�# = f&amp;&apos;(&amp;quot;A# M = {d&apos;-&amp;&apos;&apos; )odifi&apos;&amp;.} [/, !] i. ℎ&apos; bi-&amp;a) ′1o&amp;</context>
</contexts>
<marker>Hatzivassiloglou, Wiebe, 2000</marker>
<rawString>Hatzivassiloglou, V. &amp; Wiebe, J. 2000. Effects of Adjective Orientation and Gradability on Sentence Subjectivity. International Conference on Computational Linguistics, COLING- &apos;00.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Hetzron</author>
</authors>
<title>1 978. On the Relative Order of Adjectives. In</title>
<location>Tubingen: Narr.</location>
<marker>Hetzron, </marker>
<rawString>Hetzron, R. 1 978. On the Relative Order of Adjectives. In I. H. (Ed.), Language Universals. Tubingen: Narr.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Langacker</author>
</authors>
<title>1 991. Foundations of Cognitive Grammar.</title>
<publisher>Stanford University Press.</publisher>
<location>Stanford, CA:</location>
<marker>Langacker, </marker>
<rawString>Langacker, R. 1 991. Foundations of Cognitive Grammar. Stanford, CA: Stanford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>The Order of Prenominal Adjectives in Natural Language Generation.</title>
<date>2000</date>
<booktitle>Proc. 38th Annual Meeting, Assoc. Comp. Linguistics, ACL</booktitle>
<volume>00</volume>
<pages>85--92</pages>
<contexts>
<context position="1894" citStr="Malouf, 2000" startWordPosition="280" endWordPosition="281">e principles that govern Adjective Ordering (henceforth, AO) in English has been studied from a range of academic perspectives, including philosophy, linguistics, psychology and neuroscience. AO is also of interest in the field of Natural Language Processing (NLP), since a method that consistently selects felicitous orders would serve to improve the output of language modeling and generation systems. Previous NLP approaches to AO infer the ordering of adjective combinations from instances of the same, or superficially similar, combinations in training corpora (Shaw &amp; Hatzivassiloglou, 1 999) (Malouf, 2000), or from distributional tendencies of the adjectives in multiple-modifier strings (Mitchell, 200 9) (Dunlop, Mitchell, &amp; Roark, 2010). Such methods are susceptible to data sparseness, since the combinations from which they learn are rare in everyday language. By contrast, the approach taken here determines AO based on semantic features of adjectives, guided by the theoretical observation that the cognitive notion of subjectivity governs ordering in the general case (Adamson, 2000). The semantic features developed are each highly significant predictors of AO, and they combine to classify combi</context>
<context position="3818" citStr="Malouf (2000)" startWordPosition="574" endWordPosition="575">ns that are not in the training data, Shaw and Hatzivassiloglou extend the domain of the classifi11 Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 11–16, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics er by assuming transitivity in the order relation, increasing the coverage with only a small reduction in accuracy. Nevertheless, the system remains highly dependent on the domain and quantity of training data. For example, accuracy is 92% when training and test data are both within the medical domain but only 54% in cross-domain contexts. Malouf (2000) combines a direct evidence approach with an alternative method for extending the domain of his classifier. His system infers the order of unseen combinations from `similar&apos; seen combinations, where similarity is defined purely in terms of morphological form. The method works by exploiting a degree of correlation between form and order (e.g. capital letters indicate nominal modifiers, which typically occur to the right). Mitchell (200 9) applies a less `direct&apos; approach, clustering adjectives based on their position in multiple-modifier strings. Although Mitchell&apos;s classifier requires no direc</context>
</contexts>
<marker>Malouf, 2000</marker>
<rawString>Malouf, R. 2000. The Order of Prenominal Adjectives in Natural Language Generation. Proc. 38th Annual Meeting, Assoc. Comp. Linguistics, ACL ’00, 85-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mitchell</author>
</authors>
<title>9. Class-based Ordering of Prenominal Modifiers.</title>
<date></date>
<booktitle>Proc.12th European Workshop, Nat.Lang. Generation, ENLG &apos;09,</booktitle>
<pages>50--57</pages>
<marker>Mitchell, </marker>
<rawString>Mitchell, M. 200 9. Class-based Ordering of Prenominal Modifiers. Proc.12th European Workshop, Nat.Lang. Generation, ENLG &apos;09, 50-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dunlop Mitchell</author>
<author>A</author>
<author>B Roark</author>
</authors>
<title>SemiSupervised Modeling for Prenominal Modifier Ordering.</title>
<date>2011</date>
<booktitle>Proc. 49th Annual Meeting of the Assoc. Comp. Ling., ACL &apos;11,</booktitle>
<pages>236--241</pages>
<contexts>
<context position="4868" citStr="Mitchell et al. (2011)" startWordPosition="725" endWordPosition="728">chell (200 9) applies a less `direct&apos; approach, clustering adjectives based on their position in multiple-modifier strings. Although Mitchell&apos;s classifier requires no direct evidence, data sparseness is still an issue because the strings from which the system learns are relatively infrequent in everyday language. Dunlop et al. (2010) apply Multiple Sequence Alignment (MSA), a statistical technique for automatic sequence ordering, which, as with Malouf&apos;s system, quantifies wordsimilarity based solely on morphological features. Despite the greater sophistication of these more recent approaches, Mitchell et al. (2011) showed that a simple n-gram (direct evidence) classifier trained on 170 million words of New York Times and Wall Street Journal text and tested on the Brown Corpus (82.3% accuracy) outperforms both the clustering (6 9.0%) and MSA (81.8%) methods. Wulff (2003) uses Linear Discriminant Analysis (LDA) to quantify the effects of various potential AO correlates, and confirms that semantic features are better predictors than morphological and syntactic features. The features, extracted from the 10-million word Spoken British National Corpus (BNC) and weighted by LDA, combine to predict unseen adjec</context>
</contexts>
<marker>Mitchell, A, Roark, 2011</marker>
<rawString>Mitchell, M. Dunlop, A. &amp; Roark, B. 2011. SemiSupervised Modeling for Prenominal Modifier Ordering. Proc. 49th Annual Meeting of the Assoc. Comp. Ling., ACL &apos;11, 236-241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Paradis</author>
</authors>
<date>1997</date>
<booktitle>Degree Modifiers of Adjectives in Spoken British English.</booktitle>
<publisher>University Press.</publisher>
<location>Lund: Lund</location>
<marker>Paradis, 1997</marker>
<rawString>Paradis, C. 1997. Degree Modifiers of Adjectives in Spoken British English. Lund: Lund University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S J Press</author>
<author>S Wilson</author>
</authors>
<title>1 978. Choosing Between Logistic Regression and Discriminant Analysis.</title>
<journal>Journal of American Statistical Association,</journal>
<pages>699--705</pages>
<marker>Press, Wilson, </marker>
<rawString>Press, S. J. &amp; Wilson, S. 1 978. Choosing Between Logistic Regression and Discriminant Analysis. Journal of American Statistical Association, 699-705.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Greenbaum Quirk</author>
<author>A Leech</author>
<author>G</author>
<author>J Svartvik</author>
</authors>
<booktitle>985. A Comprehensive Grammar of the English Language.</booktitle>
<volume>1</volume>
<location>London: Longmans.</location>
<marker>Quirk, Leech, G, Svartvik, </marker>
<rawString>Quirk, R. Greenbaum, A. Leech, G. &amp; Svartvik, J. 1 985. A Comprehensive Grammar of the English Language. London: Longmans.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Shaw</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>1 999. Ordering Among Premodifiers.</title>
<booktitle>Proc. 37th Annual Meeting, Association of Computational Linguistics, ACL ’99 ,</booktitle>
<pages>135--143</pages>
<marker>Shaw, Hatzivassiloglou, </marker>
<rawString>Shaw, J. &amp; Hatzivassiloglou, V. 1 999. Ordering Among Premodifiers. Proc. 37th Annual Meeting, Association of Computational Linguistics, ACL ’99 , 135-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
</authors>
<title>Learning Subjective Adjectives from Corpora.</title>
<date>2000</date>
<booktitle>Proc. 17th National Conference on Artificial Intelligence (AAAI-2000).</booktitle>
<contexts>
<context position="10795" citStr="Wiebe, 2000" startWordPosition="1701" endWordPosition="1702"> be preferable to alternatives such as Ordinary Least Squares and LDA for binary outcome classification if, as in this case, the independent variables are not normally distributed (Press &amp; Wilson, 1 978). Evaluation: Performance was determined by the number of pairs in the test data correctly ordered by the classifier. Steps 3-5 were repeated 4 times (5-fold cross-validation), with the scores averaged. 3. 1 The Features Of the features included in the model, COMPARABILITY and POLARITY are shown to correlate with human subjectivity judgments by Wiebe and colleagues (see e.g. Hatzivassiloglou &amp; Wiebe, 2000). The remainder are motivated by observations in the theoretical literature. MODIFIABILITY: Gradable adjectives, such as hot or happy, tend to be more subjective than prototypically categorical adjectives, such as square or black (Hetzron, 1 978). Unlike categorical adjectives they admit modification by intensifiers (Paradis, 1 997). Therefore, the feature MODIFIABILITY is defined as the conditional probability that an adjective occurs immediately following an intensifier given that it occurs at all.2 ∑ *∈, �&amp;&apos;(&amp;quot;�), ��# ����������� !&amp;quot;�# = f&amp;&apos;(&amp;quot;A# M = {d&apos;-&amp;&apos;&apos; )odifi&apos;&amp;.} [/, !] i. ℎ&apos; bi-&amp;a) ′1o&amp;</context>
<context position="13182" citStr="Wiebe (2000)" startWordPosition="2065" endWordPosition="2066">lity that an adjective occurs in a predicative construction given that it occurs at all. The measure is implemented by counting the number of times the adjective immediately follows some form of an English copula verb.3 C = set of English copula verbs in all inflected forms POLARITY: An adjective is said to be polar if it typically attributes a positive (kind, healthy, strong) or negative (poor, selfish, rotten) characteristic. Semi-supervised methods for automatically detecting adjective polarity have been developed (Hatzivassiloglou &amp; McKeown, 1 997), and applied to subjectivity analysis by Wiebe (2000). POLARITY is implemented as a binary feature, whose value depends on whether or not the adjective appears in a list of 1,300 polar adjectives extracted by Hatzivassiloglou &amp; Mackeown. Polari ty(A) _ 1 if A E PUN - 10 if A 0 PUN P = { adjectives labelled as positive) N = {adjectives labelled as negative) 3 The copula verbs list was compiled manually by the author. 14 ADVERBIABILITY: Quirk (1 985, p 133 9) notes that evaluative adjectives tend to develop derived adverbial forms, whereas more objective adjectives do not. For example, nice, beautiful and, careful correspond to the adverbs nicely,</context>
<context position="14621" citStr="Wiebe (2000)" startWordPosition="2292" endWordPosition="2293"> adverbial forms in the corpus. Adverbiability(A) = freq(A&apos;) freq(A) + freq(A&apos;) A&apos; = adverbial form derived from A NOMINALITY: Wullf (2003) reports statistical evidence that more `noun-like&apos; modifiers appear closer to the head in modifying strings. Combinations such as `bread knife’ or `police car’, often analyzed as noun-noun compounds rather than modifier/noun combinations, represent the clearest such examples. Amongst more prototypical adjectives, some, such as green, or male have nominal senses (`village green’, `unidentified male’), whereas others do not. Separately, Hatzivassiloglou and Wiebe (2000) report a statistical correlation between the number of adjectives in a text and human judgments of subjectivity. These observations suggest that adjectives are inherently more subjective than nouns, and further that noun-like `behavior&apos; might indicate relative objectivity within the class of adjectives. Consequently, the feature NOMINALITY is defined, following Wulff, as the probability that an adjective is tagged as a noun given that it is tagged as either an adjective or a noun. It is the only feature that is expected to exhibit an inverse correlation with subjectivity. freq( An) �ominali t</context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>Wiebe, J. 2000. Learning Subjective Adjectives from Corpora. Proc. 17th National Conference on Artificial Intelligence (AAAI-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wulff</author>
</authors>
<title>A Multifactorial Analysis of Adjective Order in English.</title>
<date>2003</date>
<journal>International Journal of Corpus Linguistics,</journal>
<pages>245--282</pages>
<contexts>
<context position="5128" citStr="Wulff (2003)" startWordPosition="769" endWordPosition="770"> relatively infrequent in everyday language. Dunlop et al. (2010) apply Multiple Sequence Alignment (MSA), a statistical technique for automatic sequence ordering, which, as with Malouf&apos;s system, quantifies wordsimilarity based solely on morphological features. Despite the greater sophistication of these more recent approaches, Mitchell et al. (2011) showed that a simple n-gram (direct evidence) classifier trained on 170 million words of New York Times and Wall Street Journal text and tested on the Brown Corpus (82.3% accuracy) outperforms both the clustering (6 9.0%) and MSA (81.8%) methods. Wulff (2003) uses Linear Discriminant Analysis (LDA) to quantify the effects of various potential AO correlates, and confirms that semantic features are better predictors than morphological and syntactic features. The features, extracted from the 10-million word Spoken British National Corpus (BNC) and weighted by LDA, combine to predict unseen adjective orders with 72% accuracy. Wulff&apos;s study is unique in applying semantics to the problem, although her focus is theoretical and several features are implemented manually. The next section describes the theoretical basis for a fully-automated semantic approa</context>
<context position="8422" citStr="Wulff (2003" startWordPosition="1303" endWordPosition="1304">well-defined homosexual. As the graph shows, this reduction in subjectivity has been accompanied by a marked increase in the tendency of gay to appear closer to the noun in such strings. 3 System design The AO system described below applies the theoretical findings presented above by extracting from training data various subjectivity features of adjectives and applying this information to classify input orderings as correct or incorrect.1 System operation and evaluation consisted of 5 stages. Extracting feature profiles: The 200 highestfrequency adjectives in the BNC were extracted. Following Wulff (2003, p. 6), three items, other, only and very were removed from this list because they occur in right-branching structures. For the remaining adjectives, a `profile&apos; of feature values (c.f. Table 1, below), was extracted from 24 million words (Sections A-C) of the written BNC. Generating gold-standard orderings: From the 1 97 adjectives, 1 9,306 unordered pairs {Ai , A2} were generated. The bigram frequencies of the strings [A1 ,A2] and [A2 ,A1] were then extracted from the 1 billion-word Google n-gram Corpus. From this data, the 12,000 pairs [A, , A21 with the largest proportional difference in </context>
</contexts>
<marker>Wulff, 2003</marker>
<rawString>Wulff, S. 2003. A Multifactorial Analysis of Adjective Order in English. International Journal of Corpus Linguistics, 245-282.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>