<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.988303">
Machine Translation Detection from Monolingual Web-Text
</title>
<author confidence="0.961806">
Yuki Arase
</author>
<affiliation confidence="0.921213">
Microsoft Research Asia
</affiliation>
<note confidence="0.5705565">
No. 5 Danling St., Haidian Dist.
Beijing, P.R. China
</note>
<email confidence="0.994111">
yukiar@microsoft.com
</email>
<author confidence="0.80719">
Ming Zhou
</author>
<affiliation confidence="0.775039">
Microsoft Research Asia
</affiliation>
<note confidence="0.633721">
No. 5 Danling St., Haidian Dist.
Beijing, P.R. China
</note>
<email confidence="0.99677">
mingzhou@microsoft.com
</email>
<sectionHeader confidence="0.993832" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955294117647">
We propose a method for automatically
detecting low-quality Web-text translated
by statistical machine translation (SMT)
systems. We focus on the phrase salad
phenomenon that is observed in existing
SMT results and propose a set of computa-
tionally inexpensive features to effectively
detect such machine-translated sentences
from a large-scale Web-mined text. Un-
like previous approaches that require bilin-
gual data, our method uses only monolin-
gual text as input; therefore it is applicable
for refining data produced by a variety of
Web-mining activities. Evaluation results
show that the proposed method achieves
an accuracy of 95.8% for sentences and
80.6% for text in noisy Web pages.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946881355933">
The Web provides an extremely large volume
of textual content on diverse topics and areas.
Such data is beneficial for constructing a large
scale monolingual (Microsoft Web N-gram Ser-
vices, 2010; Google N-gram Corpus, 2006) and
bilingual (Nie et al., 1999; Shi et al., 2006;
Ishisaka et al., 2009; Jiang et al., 2009) corpus
that can be used for training statistical models for
NLP tools, as well as for building a large-scale
knowledge-base (Suchanek et al., 2007; Zhu et al.,
2009; Fader et al., 2011; Nakashole et al., 2012).
With recent advances in statistical machine trans-
lation (SMT) systems and their wide adoption in
Web services through APIs (Microsoft Translator,
2009; Google Translate, 2006), a large amount
of text in Web pages is translated by SMT sys-
tems. According to Rarrick et al. (2011), their
Web crawler finds that more than 15% of English-
Japanese parallel documents are machine transla-
tion. Machine-translated sentences are useful if
they are of sufficient quality and indistinguish-
able from human-generated sentences; however,
the quality of these machine-translated sentences
is generally much lower than sentences generated
by native speakers and professional translators.
Therefore, a method to detect and filter such SMT
results is desired to best make use of Web-mined
data.
To solve this problem, we propose a method
for automatically detecting Web-text translated by
SMT systems1. We especially target machine-
translated text produced through the Web APIs
that is rapidly increasing. We focus on the phrase
salad phenomenon (Lopez, 2008), which char-
acterizes translations by existing SMT systems,
i.e., each phrase in a sentence is semantically
and syntactically correct but becomes incorrect
when combined with other phrases in the sentence.
Based on this trait, we propose features for eval-
uating the likelihood of machine-translated sen-
tences and use a classifier to determine whether
the sentence is generated by the SMT systems.
The primary contributions of the proposed
method are threefold. First, unlike previous stud-
ies that use parallel text and bilingual features,
such as (Rarrick et al., 2011), our method only
requires monolingual text as input. Therefore,
our method can be used in monolingual Web data
mining where bilingual information is unavailable.
Second, the proposed features are designed to be
computationally light so that the method is suit-
able for handling a large-scale Web-mined data.
Our method determines if an input sentence con-
tains phrase salads using a simple yet effective fea-
tures, i.e., language models (LMs) and automati-
cally obtained non-contiguous phrases that are fre-
quently used by people but difficult for SMT sys-
tems to generate. Third, our method computes fea-
tures using both human-generated text and SMT
</bodyText>
<footnote confidence="0.9401395">
1In this paper, the term machine-translated is used for in-
dicating translation by SMT systems.
</footnote>
<page confidence="0.871848">
1597
</page>
<note confidence="0.916392">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1597–1607,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99954075">
results to capture a phrase salad by contrasting
these features, which significantly improves detec-
tion accuracy.
We evaluate our method using Japanese and En-
glish datasets, including a human evaluation to as-
sess its performance. The results show that our
method achieves an accuracy of 95.8% for sen-
tences and 80.6% for noisy Web-text.
</bodyText>
<sectionHeader confidence="0.999777" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999829636363637">
Previous methods for detecting machine-
translated text are mostly designed for bilingual
corpus construction. Antonova and Misyurev
(2011) design a phrase-based decoder for
detecting machine-translated documents in
Russian-English Web data. By evaluating the
BLEU score (Papineni et al., 2002) of trans-
lated documents (by their decoder) against the
target-side documents, machine translation (MT)
results are detected. Rarrick et al. (2011) extract a
variety of features, such as the number of tokens
and character types, from sentences in both the
source and target languages to capture words that
are mis-translated by MT systems. With these
features, the likelihood of a bilingual sentence
pair being machine-translated can be determined.
Confidence estimation of MT results is also
a related area. These studies aim to precisely
replicate human judgment in terms of the qual-
ity of machine-translated sentences based on fea-
tures extracted using a syntactic parser (Corston-
Oliver et al., 2001; Gamon et al., 2005; Avramidis
et al., 2011) or essay scoring system (Parton
et al., 2011), assuming that their input is al-
ways machine-translated. In contrast, our method
aims at making a binary judgment to distin-
guish machine-translated sentences from a mix-
ture of machine-translated and human-generated
sentences. In addition, although methods for
confidence estimation can assume sentences of a
known source language and reference translations
as inputs, these are unavailable in our problem set-
ting.
Another related area is automatic grammatical
error detection for English as a second language
(ESL) learners (Leacock et al., 2010). We use
common features that are also used in this area.
They target specific error types commonly made
by ESL learners, such as errors in prepositions and
subject-verb agreement. In contrast, our method
does not specify error types and aims to de-
tect machine-translated sentences focusing on the
phrase salad phenomenon produced by SMT sys-
tems. In addition, errors generated by ESL learn-
ers and SMT systems are different. ESL learners
make spelling and grammar mistakes at the word
level but their sentence are generally structured
while SMT results are unstructured due to phrase
salads. Works on translationese detection (Baroni
and Bernardini, 2005; Kurokawa et al., 2009; Ilisei
et al., 2010) aim to automatically identify human-
translated text by professionals using text gener-
ated by native speakers. These are related, but our
work focuses on machine-translated text.
The closest to our approach is the method pro-
posed by Moore and Lewis (2010). It automat-
ically selects data for creating a domain-specific
LM. Specifically, the method constructs LMs us-
ing corpora of target and non-target domains and
computes a cross-entropy score of an input sen-
tence for estimating the likelihood that the input
sentence belongs to the target or non-target do-
mains. While the context is different, our work
uses a similar idea of data selection for the pur-
pose of detecting low-quality sentences translated
by SMT systems.
</bodyText>
<sectionHeader confidence="0.992167" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.9997416">
When APIs of SMT services are used for machine-
translating an Web page, they typically insert
specific tags into the HTML source. Utilizing
such tags makes MT detection trivial. How-
ever, the actual situation is more complicated in
real Web data. When people manually copy and
paste machine-translated sentences, such tags are
lost. In addition, human-generated and machine-
translated sentences are often mixed together even
in a single paragraph. To observe the distribu-
tion of machine-translated sentences in such diffi-
cult cases, we examine 3K sentences collected by
our in-house Web crawler. Among them, exclud-
ing the pages with the tags of MT APIs, 6.7% of
them are found to be clearly machine translation.
Our goal is to automatically identify these sen-
tences that cannot be simply detected by the tags,
except when the sentences are of sufficient qual-
ity to be indistinguishable from human-generated
sentences.
</bodyText>
<subsectionHeader confidence="0.999195">
3.1 Phrase Salad Phenomenon
</subsectionHeader>
<bodyText confidence="0.936846">
Fig. 1 illustrates the phrase salad phenomenon that
characterizes a sentence translated by an existing
</bodyText>
<page confidence="0.989076">
1598
</page>
<note confidence="0.28403">
 |Of surprise  |was up  |foreigners flocked  |overseas  |as well,  |they publicized not only  |Japan,  |saw an article from the news. |
Natural English: The news was broadcasted not only in Japan but also overseas, and it surprised foreigners who read the article.
</note>
<figureCaption confidence="0.973366">
Figure 1: The phrase salad phenomenon in a sentence translated by an SMT system; each (segmented) phrase is correct and
</figureCaption>
<bodyText confidence="0.976559025641026">
Unnatural phrase sequence
fluent, but dotted arcs show unnatural sequences of phrases and the boxed phrase shows an incomplete non-contiguous phrase.
Mii binatil d
SMT system. Each phrase, a sequence of con-
secutive words, is fluent and grammatically cor-
rect; however, the fluency and grammar correct-
ness are both poor in inter-phrases. In addition, a
phrase salad becomes obvious by observing dis-
tant phrases. For example, the boxed phrase in
Fig. 1 is a part of the non-contiguous phrase “not
only * but also2;” however, it lacks the latter part
of the phrase (“but also”) that is also necessary
for composing a meaning. Such non-contiguous
phrases are difficult for most SMT systems to gen-
erate, since these phrases require insertion of sub-
phrases in distant parts of the sentence.
Based on the observation of these characteris-
tics, we define features to capture a phrase salad
by examining local and distant phrases. These
features evaluate (1) fluency (Sec. 3.2), (2) gram-
maticality (Sec. 3.3), and (3) completeness of
non-contiguous phrases in a sentence (Sec. 3.4).
Furthermore, humans can distinguish machine-
translated text because they have prior knowledge
of how a human-generated sentence would look
like, which has been accumulated by observing a
lot of examples through their life. This knowl-
edge makes phrase-salads, e.g., missing objects
and influent sequence of words, obvious for hu-
mans since they rarely appear on human-generated
sentences. Based on this assumption, we ex-
tract these features using both human-generated
and machine-translated text. Features extracted
from human-generated text represent the similar-
ity to human-generated text. Likewise, features
extracted from machine-translated text depict the
similarity to machine-translated text. By contrast-
ing these feature weights, we can effectively cap-
ture phrase salads in the sentence.
</bodyText>
<subsectionHeader confidence="0.999718">
3.2 Fluency Feature
</subsectionHeader>
<bodyText confidence="0.99825">
In a machine-translated sentence, fluency becomes
poor among phrases where a phrase salad occurs.
We capture this influency using two independent
LM scores; fw,H and fw,MT . The former LM is
</bodyText>
<footnote confidence="0.6093175">
2We use the symbol * to represent a gap in which any
word or phrase can be placed.
</footnote>
<bodyText confidence="0.95686825">
trained with human-generated sentences and the
latter one is trained with machine-translated sen-
tences. We input a sentence into both of the LMs
and use the scores as the fluency features.
</bodyText>
<subsectionHeader confidence="0.999833">
3.3 Grammaticality Feature
</subsectionHeader>
<bodyText confidence="0.999959931034483">
In a sentence with phrase salads, its grammatical-
ity is poor because tense and voice become in-
consistent among phrases. We capture this using
LMs trained with part-of-speech (POS) sequences
of human-generated and machine-translated sen-
tences, and the features of fpos,H and fpos,MT are
respectively computed. In a similar manner with a
word-based LM, such grammatical inconsistency
among phrases is detectable when computing a
POS LM score, since the score becomes worse
when an N-gram covers inter-phrases where a
phrase salad occurs. This approach achieves com-
putational efficiency since it only requires a POS
tagger.
Since a phrase salad may occur among distant
phrases of a sentence, it is also effective to evalu-
ate combinations of phrases that cannot be cov-
ered by the span of N-gram. For this purpose,
we make use of function words that sparsely ap-
pear in a sentence where their combinations are
syntactically constrained. For example, the same
preposition rarely appears many times in a human-
generated sentence, while it does in a machine-
translated sentence due to the phrase salad. Simi-
lar to the POS LM, we first analyze sentences gen-
erated by human or SMT by a POS tagger, extract
sequences of function words, and finally train LMs
with the sequences. We use these LMs to obtain
scores that are used as features ffw,H and ffw,MT.
</bodyText>
<subsectionHeader confidence="0.982079">
3.4 Gappy-Phrase Feature
</subsectionHeader>
<bodyText confidence="0.999980857142857">
There are a lot of common non-contiguous phrases
that consist of sub-phrases (contiguous word
string) and gaps, which we refer to as gappy-
phrases (Bansal et al., 2011). We specifically use
gappy-phrases of 2-tuple, i.e., phrases consisting
of two sub-phrases and one gap in the middle.
Let us take an English example “not only * but
</bodyText>
<page confidence="0.988877">
1599
</page>
<subsectionHeader confidence="0.486661">
Sequences
</subsectionHeader>
<bodyText confidence="0.61627025">
World population not only grows , but grows old.
A press release not only informs but also teases .
Hazelnuts are not only for food, but also fuel.
The coalition must not only listen but also act.
</bodyText>
<tableCaption confidence="0.997856">
Table 1: Example of a sequence database
</tableCaption>
<bodyText confidence="0.999616862068966">
also.” When a sentence contains the phrase “not
only,” the phrase “but also” is likely to appear in
human-generated setences. Such a gappy-phrase
is difficult for SMT systems to correctly generate
and causes a phrase salad. Therefore, we define a
feature to evaluate how likely a sentence contains
gappy-phrases in a complete form without missing
sub-phrases. This feature is effective to comple-
ment LMs that capture characteristics in N-grams.
Sequential Pattern Mining It is costly to man-
ually collect a lot of such gappy-phrases. There-
fore, we regard the task as sequential pattern min-
ing and apply PrefixSpan proposed by Pei et al.
(2001), which is a widely used sequential pattern
mining method3.
Given a set of sequences and a user-specified
min support E N threshold, the sequential pattern
mining finds all frequent subsequences whose oc-
currence frequency is no less than min support.
For example, given a sequence database like Ta-
ble 1, the sequential pattern mining finds all fre-
quent subsequences, e.g., “not only,” “not only �
but also,” “not * but *,” and etc.
To capture a phrase salad by contrasting appear-
ance of gappy-phrases in human-generated and
machine-translated text, we independently extract
gappy-phrases from each of them using PrefixS-
pan. We then compute features fg,H and fg,MT
using the obtained phrases.
</bodyText>
<subsectionHeader confidence="0.690857">
Observation of Extracted Gappy-Phrases
</subsectionHeader>
<bodyText confidence="0.998541363636364">
Based on a preliminary experiment, we set
the parameter min support of PrefixSpan to
100 for computational efficiency. We extract
gappy-phrases (of 2-tuple) from our develop-
ment dataset described in Sec. 4.1 that includes
254K human-generated and 134K machine-
translated sentences in Japanese, and 210K
human-generated and 159K machine-translated
sentences in English.
Regarding the Japanese dataset, we obtain
about 104K and 64K gappy-phrases from human-
</bodyText>
<footnote confidence="0.643363">
3Due to the severe space limitation, readers are referred to
that paper.
</footnote>
<bodyText confidence="0.99985255882353">
generated and machine-translated sentences, re-
spectively. According to our observation of the
extracted phrases, 21K phrases commonly ap-
pear in human-generated and machine-translated
sentences. Many of these common phrases are
incomplete forms of gappy-phrases that lack se-
mantic meaning to humans, such as “not only *
the” and “not only * and.” On the other hand,
complete forms of gappy-phrases that preserve se-
mantic meaning exclusively appear in phrases ex-
tracted from human-generated sentences. We also
obtain about 74K and 42K phrases from human-
generated and machine-translated sentences in the
English dataset (21K of them are common).
Phrase Selection As a result of sequential
pattern mining, we can gather a huge num-
ber of gappy-phrases from human-generated and
machine-translated text, but as we described
above, many of them are common. In addition,
it is computationally expensive to use all of them.
Therefore, our method selects useful phrases for
detecting machine-translated sentences.
Although there are several approaches for fea-
ture selection, e.g., (Sebastiani, 2002), we use a
method that is suitable for handling a large num-
ber of feature candidates. Specifically, we evaluate
gappy-phrases based on the information gain that
measures the amount of information in bits ob-
tained for class prediction when knowing the pres-
ence or absence of a phrase and the corresponding
class distribution. This corresponds to measuring
an expected reduction in entropy, i.e., uncertainty
associated with a random factor. The information
gain G E R for a gappy-phrase g is defined as
</bodyText>
<equation confidence="0.988917">
G(g) .= H(C) − P(X9)H(C�X9)
−P(X9)H(C�X9),
</equation>
<bodyText confidence="0.999483692307692">
where H(C) represents the entropy of the classifi-
cation, C is a stochastic variable taking a class, Xg
is a stochastic variable representing the presence
(Xy) or absence (X�g) of the phrase g, P(Xg) rep-
resents the probability of presence or absence of
the phrase g, and H(CIXg) is the conditional en-
tropy due to the phrase g. We use top-k phrases
based on the information gain G. Specifically, we
use the top 40% of phrases to compute the feature
values. Table 2 shows examples of gappy-phrases
extracted from human-generated and machine-
translated text in our development dataset and re-
main after feature selection.
</bodyText>
<page confidence="0.99418">
1600
</page>
<tableCaption confidence="0.98556525">
Table 2: Example of gappy-phrases extracted from human-
generated and machine-translated text; phrases preserving se-
mantic meaning are extracted only from human-generated
text.
</tableCaption>
<bodyText confidence="0.999965166666667">
The gappy-phrases depend on each other, and
the more phrases extracted from human-generated
(machine-translated) text are found in a sentence,
the more likely the sentence is human-generated
(machine-translated). Therefore, we compute the
feature as
</bodyText>
<equation confidence="0.953878">
�fc(s) = wiδ(i, s),
i∈k
</equation>
<bodyText confidence="0.9999005">
where wi is a weight of the i-th phrase, and δ(i, s)
is a Kronecker’s delta function that takes 1 if the
sentence s includes the i-th phrase and takes 0 oth-
erwise. We may set the weight wi according to the
importance of the phrase, such as the information
gain. In this work, we set wi to 1 for simplicity.
</bodyText>
<subsectionHeader confidence="0.793798">
3.5 Classification
</subsectionHeader>
<bodyText confidence="0.999978214285714">
Table 3 summarizes the features employed in
our method. In addition to the discussed fea-
tures, we use the length of a sentence as a fea-
ture flen to avoid the bias of LM-based fea-
tures that favor shorter sentences. The proposed
method takes a monolingual sentence from Web
data as input and computes a feature vector of
f = (fw,H, . . . , flen) ∈ R9. Each feature is fi-
nally normalized to have a zero-mean and unit
variance distribution. In the feature space, a
support vector machine (SVM) classifier (Vap-
nik, 1995) is used to determine the likelihoods
of machine-translated and human-generated sen-
tences.
</bodyText>
<sectionHeader confidence="0.999806" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.997855">
We evaluate our method using both Japanese and
English datasets from various aspects and investi-
gate its characteristics. In this section, we describe
our experiment settings.
</bodyText>
<subsectionHeader confidence="0.969492">
4.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.974791">
For the purpose of evaluation, we use human-
generated and machine-translated sentences for
</bodyText>
<table confidence="0.9986435">
Feature Notation
Fluency fw,H, fw,MT
Grammaticality fpos,H, fpos,MT
ffw,H, ffw,MT
Gappy-phrase fg,H, fg,MT
Length flen
</table>
<tableCaption confidence="0.999739">
Table 3: List of proposed features and their notations
</tableCaption>
<bodyText confidence="0.99995265625">
constructing LMs, extracting gappy-phrases, and
training a classifier. These sentences should
be ensured to be human-generated or machine-
translated, and the human-generated and machine-
translated sentences express the same content for
fairness of evaluation to avoid effects due to vo-
cabulary difference.
As a dataset that meets these requirements, we
use parallel text in public websites (this is for fair
evaluation and our method can be trained using
nonparallel text on an actual deployment). Eight
popular sites having Japanese and English paral-
lel pages are crawled, whose text is manually veri-
fied to be human-generated. The main textual con-
tent of these 131K parallel pages are extracted,
and the sentences are aligned using (Ma, 2006).
As illustrated in Fig. 2, the text in one language
is fed to the Bing translator, Google Translate,
and an in-house SMT system4 implemented based
on (Chiang, 2005) by ourselves for obtaining sen-
tences translated by SMT systems. Due to a severe
limitation on the number of requests to the APIs,
we randomly subsample sentences before sending
them to these SMT systems. We use text in the
other language as human-generated sentences5.
In this manner, we prepare 508K human-
generated and 268K machine-translated sentences
as a Japanese dataset, and 420K human-generated
and 318K machine-translated sentences as an En-
glish dataset. We split each of them into two even
datasets and use one for development and the other
for evaluation.
</bodyText>
<subsectionHeader confidence="0.992222">
4.2 Experiment Setting
</subsectionHeader>
<bodyText confidence="0.999874">
For the fluency and grammaticality features, we
train 4-gram LMs using the development dataset
with the SRI toolkit (Stolcke, 2002). To obtain
the POS information, we use Mecab (Kudo et al.,
2004) for Japanese and a POS tagger developed by
Toutanova et al. (2003) for English. We evaluate
</bodyText>
<footnote confidence="0.968337">
4A preliminary evaluation of the in-house SMT system
shows that it has comparable quality with Bing translator.
5These are a mixture of sentences generated by native
speakers and professional translators/editors.
</footnote>
<figure confidence="0.889365295454546">
in the early * period
after * after the
and also * and
and * but the
no * not
not * not
MT
Human
not only * but also
with * as well as
known as * to
more * than
1601
Parallel sentences
Japanese
English
Human-
generated
sentences
Human-
generated
sentences
...
...
MT systems
Machine
translated
sentences
Human-
generated
sentences
...
...
Method Accuracy
Cross-Entropy 90.7
Lexical Feature 87.8
Proposed feature Word LMs 94.1
POS LMs 91.3
FW LMs 82.7
GPs 85.7
Japanese
Japanese
Table 4: Accuracy (%) of individual features and compari-
son methods
</figure>
<figureCaption confidence="0.998859666666667">
Figure 2: Experimental data preparation; text in one lan-
guage is fed to SMT systems and the other is used as human-
generated sentences.
</figureCaption>
<bodyText confidence="0.999385615384615">
the effect of the sizes of N-grams and develop-
ment dataset in the experiments.
Using the proposed features, we train an SVM
classifier for detecting machine-translated sen-
tences. We use an implementation of LIB-
SVM (Chang and Lin, 2011) with a radial basis
function kernel due to the relatively small number
of features in the proposed method. We set appro-
priate parameters by grid search in a preliminary
experiment.
We evaluate the performance of MT detection
based on accuracy6 that is a broadly used evalua-
tion metric for classification problems:
</bodyText>
<equation confidence="0.977814">
nTP + nTN
accuracy = ,
n
</equation>
<bodyText confidence="0.9999828">
where nTP and nTN are the numbers of true-
positives and true-negatives, respectively, and n
is the total number of exemplars. The accuracy
scores that we report in Sec. 5 are all based on 10-
fold cross validation.
</bodyText>
<subsectionHeader confidence="0.998587">
4.3 Comparison Method
</subsectionHeader>
<bodyText confidence="0.999926083333333">
We compare our method with the method
of (Moore and Lewis, 2010) (Cross-Entropy). Al-
though the Cross-Entropy method is designed for
the task of domain adaptation of an LM, our prob-
lem is a variant of their original problem and
thus their method is directly relevant. In our
context, the method computes the cross-entropy
scores IMT(s) and IH(s) of an input sentence
s against LMs trained on machine-translated and
human-generated sentences. Cross-entropy and
perplexity are monotonically related, as perplex-
ity of s according to an LM M is simply ob-
</bodyText>
<footnote confidence="0.921723333333333">
6Although we also examine precision and recall of clas-
sification results, they are similar to accuracy reported in this
paper.
</footnote>
<bodyText confidence="0.993992565217391">
tained by bIM(s) where IM(s) is cross-entropy
score and b is a base with regard to which the
cross-entropy is measured. The method scores
the sentence according to the cross-entropy differ-
ence, i.e., IMT(s) − IH(s), and decides that the
sentence is machine-translated when the score is
lower than a predefined threshold. The classifica-
tion is performed by 10-fold cross validation. We
find the best performing threshold on a training set
and evaluate the accuracy with a test set using the
determined threshold.
Additionally, we compare our method to a
method that uses a feature indicating presence or
absence of unigrams, which we call Lexical Fea-
ture. This feature is commonly used for transla-
tionese detection and shows the best performance
as a single feature in (Baroni and Bernardini,
2005). It is also used by Rarrick et al. (2011) and
shows the best performance by itself in detecting
machine-translated sentences in English-Japanese
translation in the setting of bilingual input. We
implement the feature and use it against a mono-
lingual input to fit our problem setting.
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="evaluation">
5 Results and Discussions
</sectionHeader>
<bodyText confidence="0.996399">
In this section, we analyze and discuss the experi-
ment results in detail.
</bodyText>
<subsectionHeader confidence="0.998841">
5.1 Accuracy on Japanese Dataset
</subsectionHeader>
<bodyText confidence="0.99979">
We evaluate the sentence-level and document-
level accuracy of our method using the Japanese
dataset. Specifically, we evaluate effects of indi-
vidual features and their combinations, compare
with human annotations, and assess performance
variations across different sentence lengths and
various settings on LM training.
Effect of Individual Feature Table 4 shows the
accuracy scores of individual features and com-
parison methods. We refer to features for flu-
ency (fw,H, fw,MT) as Word LMs, grammatical-
ity using POS LMs (fpos,H, fpos,MT) as POS LMs
</bodyText>
<page confidence="0.987018">
1602
</page>
<table confidence="0.878671333333333">
Method Accuracy
Word LMs + GPs 94.7
Word LMs + POS LMs 95.1
Word LMs + POS LMs + GPs 95.4
Word LMs + POS LMs + FW LMs 95.5
All 95.8
</table>
<tableCaption confidence="0.6247665">
Table 5: Accuracy (%) of feature combinations; there are
significant differences (p « .01) against the accuracy score
</tableCaption>
<table confidence="0.981342555555555">
of Word LMs.
Error Ratio Accuracy
(%) Word All
LMs
Has wrong content words 37.8 93.1 95.0
Misses content words 12.2 91.8 96.5
Has wrong function words 19.7 92.7 97.1
Misses function words 13.0 93.3 95.6
Has wrong inflections 10.8 97.3 98.7
</table>
<tableCaption confidence="0.99751">
Table 6: Distribution (%) of machine translation errors and
accuracy (%) of proposed method on the different errors
</tableCaption>
<bodyText confidence="0.99990038271605">
and function word LMs (ffw,x, ffw,MT) as FW
LMs, respectively, and for completeness of gappy-
phrases (fg,x, fg,MT) as GPs. The Word LMs
show the best accuracy that outperforms Cross-
Entropy by 3.4% and Lexical Feature by 6.3%.
This high accuracy is achieved by contrasting flu-
ency in human-generated and machine-translated
text to capture the phrase salad phenomenon. The
accuracy of Word LM trained only on human-
generated sentences is limited to 65.5%. On the
other hand, the accuracy of Word LM trained on
machine-translated sentences shows a better per-
formance (84.4%). By combining these into a
single feature vector f = (fw,x, fw,MT, flen), the
accuracy is largely improved.
It is interesting that Lexical Feature achieves
a high accuracy of 87.8% despite its simplicity.
Since Lexical Feature is a bag-of-words model,
it can consider distant words in a sentence. This
is effective for capturing a phrase salad that oc-
curs among distant phrases, which N-gram can-
not cover. As for Cross-Entropy, a simple sub-
traction of cross-entropy scores cannot well con-
trast the fluency in human-generated and machine-
translated text and results in poorer accuracy than
Word LMs.
The accuracy of POS LMs (91.3%) is slightly
lower than that of Word LMs due to the limited
vocabulary, i.e., the number of POSs. The accu-
racy of FW LMs and GPs are even lower. This
is convincing since these features cannot have rea-
sonable values when a sentence does not include a
function word and gappy-phrase. However, these
features are complementary to Word LMs as we
will see in the next paragraph.
Effect of Feature Combination Table 5 shows
the accuracy when combining features. Sign tests
show that the accuracy scores of these feature
combinations are significantly different (p « .01)
against the accuracy of Word LMs. The results
show that the features complement each other. The
combination of all features reaches an accuracy
of 95.8%, which improves the accuracy of Word
LMs by 1.7%. This result supports that FW LMs
and GPs are effective to capture a phrase salad oc-
curring in distant phrases and complement the ev-
idence in N-grams that is captured by LMs. This
effect becomes more obvious in the human evalu-
ation.
We also evaluate the accuracy of the proposed
method at a document level. Due to the high accu-
racy at the sentence-level, we use a voting method
to judge a document, i.e., deciding if the docu-
ment is machine-translated when γ% of its sen-
tences are judged as machine-translated. We use
all features and find that our method achieves 99%
precision and recall with γ = 50.
Human Evaluation To further investigate the
characteristics of our method, we conduct a human
evaluation. We sample Japanese sentences and ask
three native speakers to 1) judge whether a sen-
tence is human-generated or machine-translated
and 2) list errors that the sentence contains. Re-
garding the task 1), we allow the annotators to as-
sign “hard to determine” for difficult cases. We al-
locate about 230 sentences for each annotator (in
total 700 sentences) without overlapping annota-
tion sets.
The accuracy of annotations is found to be
88.2%, which shows that our method is even su-
perior to native speakers. Agreement between the
annotators and our method (with all features) is
85.1%. As we interview the annotators, we find
that human annotations are strongly affected by
the annotators’ domain knowledge. For example,
technical sentences are more often misclassified
by the annotators.
Table 6 shows the distribution of errors on
machine-translated sentences found by the anno-
tators (on sentences that they correctly classified)
with the accuracy of Word LMs and all features on
</bodyText>
<page confidence="0.959267">
1603
</page>
<figure confidence="0.997562617647059">
100
95
Accuracy (%)
90
85
80
75
70
Proposed Method
Cross-Entropy
Lexical Feature
Human
Length distribution
6 10 14 18 22 26 30 34 38 42 46 50 54 58 62 66 70 74 78
Num. of words in a sentence
4
2
8
6
0
10
Ratio(%)
Word LMs
POS LMs
FW LMs
ALL
98
93
Accuracy (%)
88
83
78
2 3 4
N-gram
</figure>
<figureCaption confidence="0.9860168">
Figure 4: Effect of the sizes of N-grams on MT detection
accuracy (%)
Figure 3: Accuracy (%) across different sentence lengths
(the primary axis) and distribution (%) of sentence lengths in
the evaluation dataset (the secondly axis)
</figureCaption>
<bodyText confidence="0.999579233333333">
these sentences (a sentence may contain multiple
errors). It indicates that the accuracy of Word LMs
is improved by feature combination; from 1.4% on
sentences of “Has wrong inflections” to 4.7% on
sentences of “Misses content words”.
Effect of Sentence Length The accuracy of the
proposed method is significantly affected by sen-
tence length (the number of words in a sentence).
Fig. 3 shows the accuracy of the proposed method
(with all features) and comparison methods w.r.t.
sentence lengths (with the primary axis), as well
as the distribution of sentence lengths in the eval-
uation dataset (with the secondly axis). We ag-
gregate the classification results on each cross-
validation (test results). It also shows the accu-
racy of human annotations w.r.t. sentence lengths,
which we obtain for the 700 sentences in the hu-
man evaluation. The accuracy drops on all meth-
ods when sentences are short; the accuracy of our
method is 91.6% when a sentence contains less
than or equal to 10 words. The proposed method
shows the similar trend with the human annota-
tions, and even the accuracy of human annota-
tions significantly drops on such short sentences.
This result indicates that SMT results on short
sentences tend to be of sufficient quality and in-
distinguishable from human-generated sentences.
Since such high-quality machine-translations do
not harm the quality of Web-mined data, we do
not need to detect them.
</bodyText>
<subsectionHeader confidence="0.828912">
Effect of Setting on LM Training We evalu-
</subsectionHeader>
<bodyText confidence="0.999946176470588">
ate the performance variation w.r.t. the sizes of
N-grams and development dataset. Fig. 4 shows
the accuracy of the LM based features and feature
combination when changing sizes of N-grams.
The performance of Word LMs is stabilized after
3-gram while that of POS LMs is still improved
at 4-gram. This is because POS LMs need more
evidence to compensate for their limited vocabu-
lary. FW LMs become stable at 3-gram because
the possible number of function words in a sen-
tence should be small.
When we change the size of the development
dataset with 10% increments, the accuracy curve is
stabilized when the size is 40% of all set. Consid-
ering the fact that the overall development dataset
is small, it shows that our method is deployable
with a small dataset.
</bodyText>
<subsectionHeader confidence="0.999828">
5.2 Accuracy on English Dataset
</subsectionHeader>
<bodyText confidence="0.999986047619048">
To investigate the applicability of our method to
other languages, we apply the same method to
the English dataset. Because English is a config-
urational language, function words are less flex-
ible than case markers in Japanese. Therefore,
SMT systems may better handle English function
words, which potentially decreases the effect of
FW LMs in our method. In addition, because En-
glish is a morphologically poor language, the ef-
fect of POS LMs may be reduced.
Nevertheless, in our experiment, all features
are shown to be effective even with the English
dataset. The combination of all features achieves
the best performance, with an accuracy of 93.1%,
which outperforms Cross-Entropy by 1.9%, and
Lexical Feature by 8.5%. Even though improve-
ments by POS LMs and FW LMs are smaller than
Japanese case, their effects are still positive. We
also find that GPs stably contribute to the accu-
racy. These results show the applicability of our
method to other languages.
</bodyText>
<subsectionHeader confidence="0.998798">
5.3 Accuracy on Raw Web Pages
</subsectionHeader>
<bodyText confidence="0.999932666666667">
To avoid unmodeled factors affecting the evalua-
tion, we have carefully removed noise from our
experiment datasets. However, real Web pages are
</bodyText>
<page confidence="0.987774">
1604
</page>
<bodyText confidence="0.9999858">
more complex; there are often instances of sen-
tence fragments, such as captions and navigational
link text. To evaluate the accuracy of our method
on real Web pages, we conduct experiments using
the dataset generated by Rarrick et al. (2011) that
contains randomly crawled Web pages annotated
by two annotators to judge if a page is human-
generated or machine-translated. We use Japanese
sentences extracted from 69 pages (43 human-
generated and 26 machine-translated pages) where
the annotators’ judgments agree; 3,312 sentences
consisting of 1, 399 machine-translated and 1, 913
human-generated sentences. To replicate the sit-
uation in real Web pages, we conduct a minimal
preprocessing, i.e., simply removing HTML tags,
and then feed all the remaining text to our method.
An SVM classifier is trained with features ob-
tained by the LMs and gappy-phrases computed
from the data described in Sec. 4.1. Our method
shows 80.6% accuracy at a sentence level and
82.4% accuracy at a document level using the vot-
ing method. One factor for this performance dif-
ference is again sentence lengths, as SMT results
of short phrases in Web pages can be of high-
quality. Another factor is the noise in Web pages.
We find that experimental pages contain lots of
non-sentences, such as fragments of scripts and
product codes. The results show that we need a
preprocessing to remove typical noise in Web text
before SMT detection to handle noisy Web pages.
</bodyText>
<subsectionHeader confidence="0.998211">
5.4 Quality of Cleaned Data
</subsectionHeader>
<bodyText confidence="0.999978375">
Finally, we briefly demonstrate the effect of
machine-translation filtering in an end-to-end sce-
nario, taking LM construction as an example.
We construct LMs reusing the Japanese evalua-
tion dataset described in Sec. 4.1 where machine-
translated sentences are removed by the pro-
posed method (LM-Proposed), Lexical Feature
(LM-LF), and Cross-Entropy (LM-CE), as well
as an LM with all sentences, i.e., with machine-
translated sentences (LM-All). As a result of 5-
fold cross-validation, LM-Proposed has 17.8%,
17.1%, and 16.3% lower perplexities on average
compared to LM-All, LM-LF, and LM-CE, re-
spectively. These results show that our method
is useful for improving the quality of Web-mined
data.
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999962">
We propose a method for detecting machine-
translated sentences from monolingual Web-text
focusing on the phrase salad phenomenon pro-
duced by existing SMT systems. The experimen-
tal results show that our method achieves an accu-
racy of 95.8% for sentences and 80.6% for noisy
Web text.
We plan to extend our method to detect
machine-translated sentences produced by differ-
ent MT systems, e.g., a rule-based system, and
develop a unified framework for cleaning various
types of noise in Web-mined data. In addition, we
will investigate the effect of source and target lan-
guages on translation in terms of MT detection. As
Lopez (2008) describes, a phrase-salad is a com-
mon phenomenon that characterizes current SMT
results. Therefore, we expect that our method is
basically effective on different language pairs. We
will conduct experiments to evaluate performance
difference using various language pairs.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999535">
We sincerely appreciate Spencer Rarrick and Will
Lewis for active discussion and sharing the exper-
imental data with us. We thank Junichi Tsujii for
his valuable feedback to improve our work.
</bodyText>
<sectionHeader confidence="0.998313" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.932363454545455">
Alexandra Antonova and Alexey Misyurev. 2011.
Building a web-based parallel corpus and filtering
out machine translated text. In Proceedings of the
Workshop on Building and Using Comparable Cor-
pora, pages 136–144.
Eleftherios Avramidis, Maja Popovic, David Vilar Tor-
res, and Aljoscha Burchardt. 2011. Evaluate with
confidence estimation: Machine ranking of trans-
lation outputs using grammatical features. In Pro-
ceedings of the Workshop on Statistical Machine
Translation (WMT 2011), pages 65–70.
Mohit Bansal, Chris Quirk, and Robert C. Moore.
2011. Gappy phrasal alignment by agreement. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 1308–
1317.
Marco Baroni and Silvia Bernardini. 2005. A new
approach to the study of translationese: Machine-
learning the difference between original and trans-
lated text. Literary and Linguistic Computing,
21(3):259–274.
</reference>
<page confidence="0.947818">
1605
</page>
<reference confidence="0.99409787037037">
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM : a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1–27:27.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL 2005), pages
263–270.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A machine learning approach to the
automatic evaluation of machine translation. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL 2001), pages
148–155.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011), pages 1535–1545.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level MT evaluation without refer-
ence translations: Beyond language modeling. In
Proceedings of European Association for Machine
Translation (EAMT 2005).
Google N-gram Corpus. 2006. http://www.ldc.
upenn.edu/Catalog/CatalogEntry.
jsp?catalogId=LDC2006T13.
Google Translate. 2006. http://code.google.
com/apis/language/.
Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and
Ruslan Mitkov. 2010. Identification of transla-
tionese: A machine learning approach. In Proceed-
ings of the International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLing 2010), pages 503–511.
Tatsuya Ishisaka, Masao Utiyama, Eiichiro Sumita, and
Kazuhide Yamamoto. 2009. Development of a
Japanese-English software manual parallel corpus.
In Proceedings of the Machine Translation Summit
(MT Summit XII).
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu,
and Qingsheng Zhu. 2009. Mining bilingual data
from the web with adaptively learnt patterns. In
Proceedings of the Joint Conference of the Annual
Meeting of the Association for Computational Lin-
guistics and the International Joint Conference on
Natural Language Processing of the Asian Federa-
tion of Natural Language Processing (ACL-IJCNLP
2009), pages 870–878.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP 2004), pages 230–
237.
David Kurokawa, Cyril Goutte, and Pierre Isabelle.
2009. Automatic detection of translated text and its
impact on machine translation. In Proceedings of
the Machine Translation Summit (MT-Summit XII).
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan
and Claypool Publishers.
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):1–49.
Xiaoyi Ma. 2006. Champollion: a robust parallel text
sentence aligner. In Proceedings of the International
Conference on Language Resources and Evaluation
(LREC 2006), pages 489–492.
Microsoft Translator. 2009. http://www.
microsofttranslator.com/dev/.
Microsoft Web N-gram Services. 2010. http://
research.microsoft.com/web-ngram.
Robert Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL 2010), pages
220–224.
Ndapandula Nakashole, Gerhard Weikum, and
Fabian M. Suchanek. 2012. PATTY: A taxonomy
of relational patterns with semantic types. In
Proceedings of the Joint Conference on Empir-
ical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP-CoNLL 2012), pages 1135–1145.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and
Richard Durand. 1999. Cross-language information
retrieval based on parallel texts and automatic min-
ing of parallel texts from the web. In Proceedings
of the Annual International ACM SIGIR Conference
(SIGIR 1999), pages 74–81.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL 2002), pages 311–318.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation.
In Proceedings of the Workshop on Statistical Ma-
chine Translation (WMT 2011), pages 108–115.
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Helen
Pinto, Qiming Chen, Umeshwar Dayal, and Mei-
Chun Hsu. 2001. PrefixSpan: Mining sequen-
tial patterns efficiently by prefix-projected pattern
growth. In Proceedings of the International Con-
ference on Data Engineering (ICDE 2001), pages
215–224.
</reference>
<page confidence="0.807767">
1606
</page>
<reference confidence="0.999110555555556">
Spencer Rarrick, Chris Quirk, and Will Lewis. 2011.
MT detection in web-scraped parallel corpora. In
Proceedings of the Machine Translation Summit
(MT Summit XIII).
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing Sur-
veys, 34(1):1–47.
Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.
2006. A DOM tree alignment model for mining par-
allel data from the web. In Proceedings of the Inter-
national Conference on Computational Linguistics
and the Annual Meeting of the Association for Com-
putational Linguistics (COLING-ACL 2006), pages
489–496.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing (ICSLP 2002), pages 901–904.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of International Conference
on World Wide Web (WWW 2007), pages 697–706.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL 2003), pages 252–259.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and
Ji-Rong Wen. 2009. StatSnowball: a statistical
approach to extracting entity relationships. In Pro-
ceedings of International Conference on World Wide
Web (WWW 2009), pages 101–110.
</reference>
<page confidence="0.993343">
1607
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.420981">
<title confidence="0.997865">Machine Translation Detection from Monolingual Web-Text</title>
<author confidence="0.95623">Yuki</author>
<affiliation confidence="0.997613">Microsoft Research</affiliation>
<address confidence="0.827298">No. 5 Danling St., Haidian Beijing, P.R.</address>
<email confidence="0.997208">yukiar@microsoft.com</email>
<author confidence="0.854532">Ming</author>
<affiliation confidence="0.972502">Microsoft Research</affiliation>
<address confidence="0.739754">No. 5 Danling St., Haidian Beijing, P.R.</address>
<email confidence="0.998483">mingzhou@microsoft.com</email>
<abstract confidence="0.996749111111111">We propose a method for automatically detecting low-quality Web-text translated by statistical machine translation (SMT) We focus on the salad phenomenon that is observed in existing SMT results and propose a set of computationally inexpensive features to effectively detect such machine-translated sentences from a large-scale Web-mined text. Unlike previous approaches that require bilindata, our method uses only monolinas input; therefore it is applicable for refining data produced by a variety of Web-mining activities. Evaluation results show that the proposed method achieves accuracy of for sentences and for text in noisy Web pages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Antonova</author>
<author>Alexey Misyurev</author>
</authors>
<title>Building a web-based parallel corpus and filtering out machine translated text.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Building and Using Comparable Corpora,</booktitle>
<pages>136--144</pages>
<contexts>
<context position="4547" citStr="Antonova and Misyurev (2011)" startWordPosition="690" endWordPosition="693">ciation for Computational Linguistics, pages 1597–1607, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics results to capture a phrase salad by contrasting these features, which significantly improves detection accuracy. We evaluate our method using Japanese and English datasets, including a human evaluation to assess its performance. The results show that our method achieves an accuracy of 95.8% for sentences and 80.6% for noisy Web-text. 2 Related Work Previous methods for detecting machinetranslated text are mostly designed for bilingual corpus construction. Antonova and Misyurev (2011) design a phrase-based decoder for detecting machine-translated documents in Russian-English Web data. By evaluating the BLEU score (Papineni et al., 2002) of translated documents (by their decoder) against the target-side documents, machine translation (MT) results are detected. Rarrick et al. (2011) extract a variety of features, such as the number of tokens and character types, from sentences in both the source and target languages to capture words that are mis-translated by MT systems. With these features, the likelihood of a bilingual sentence pair being machine-translated can be determin</context>
</contexts>
<marker>Antonova, Misyurev, 2011</marker>
<rawString>Alexandra Antonova and Alexey Misyurev. 2011. Building a web-based parallel corpus and filtering out machine translated text. In Proceedings of the Workshop on Building and Using Comparable Corpora, pages 136–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleftherios Avramidis</author>
<author>Maja Popovic</author>
<author>David Vilar Torres</author>
<author>Aljoscha Burchardt</author>
</authors>
<title>Evaluate with confidence estimation: Machine ranking of translation outputs using grammatical features.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation (WMT</booktitle>
<pages>65--70</pages>
<contexts>
<context position="5448" citStr="Avramidis et al., 2011" startWordPosition="828" endWordPosition="831">d. Rarrick et al. (2011) extract a variety of features, such as the number of tokens and character types, from sentences in both the source and target languages to capture words that are mis-translated by MT systems. With these features, the likelihood of a bilingual sentence pair being machine-translated can be determined. Confidence estimation of MT results is also a related area. These studies aim to precisely replicate human judgment in terms of the quality of machine-translated sentences based on features extracted using a syntactic parser (CorstonOliver et al., 2001; Gamon et al., 2005; Avramidis et al., 2011) or essay scoring system (Parton et al., 2011), assuming that their input is always machine-translated. In contrast, our method aims at making a binary judgment to distinguish machine-translated sentences from a mixture of machine-translated and human-generated sentences. In addition, although methods for confidence estimation can assume sentences of a known source language and reference translations as inputs, these are unavailable in our problem setting. Another related area is automatic grammatical error detection for English as a second language (ESL) learners (Leacock et al., 2010). We us</context>
</contexts>
<marker>Avramidis, Popovic, Torres, Burchardt, 2011</marker>
<rawString>Eleftherios Avramidis, Maja Popovic, David Vilar Torres, and Aljoscha Burchardt. 2011. Evaluate with confidence estimation: Machine ranking of translation outputs using grammatical features. In Proceedings of the Workshop on Statistical Machine Translation (WMT 2011), pages 65–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
</authors>
<title>Gappy phrasal alignment by agreement.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011),</booktitle>
<pages>1308--1317</pages>
<contexts>
<context position="12814" citStr="Bansal et al., 2011" startWordPosition="2000" endWordPosition="2003">lly constrained. For example, the same preposition rarely appears many times in a humangenerated sentence, while it does in a machinetranslated sentence due to the phrase salad. Similar to the POS LM, we first analyze sentences generated by human or SMT by a POS tagger, extract sequences of function words, and finally train LMs with the sequences. We use these LMs to obtain scores that are used as features ffw,H and ffw,MT. 3.4 Gappy-Phrase Feature There are a lot of common non-contiguous phrases that consist of sub-phrases (contiguous word string) and gaps, which we refer to as gappyphrases (Bansal et al., 2011). We specifically use gappy-phrases of 2-tuple, i.e., phrases consisting of two sub-phrases and one gap in the middle. Let us take an English example “not only * but 1599 Sequences World population not only grows , but grows old. A press release not only informs but also teases . Hazelnuts are not only for food, but also fuel. The coalition must not only listen but also act. Table 1: Example of a sequence database also.” When a sentence contains the phrase “not only,” the phrase “but also” is likely to appear in human-generated setences. Such a gappy-phrase is difficult for SMT systems to corr</context>
</contexts>
<marker>Bansal, Quirk, Moore, 2011</marker>
<rawString>Mohit Bansal, Chris Quirk, and Robert C. Moore. 2011. Gappy phrasal alignment by agreement. In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011), pages 1308– 1317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>A new approach to the study of translationese: Machinelearning the difference between original and translated text.</title>
<date>2005</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>21--3</pages>
<contexts>
<context position="6694" citStr="Baroni and Bernardini, 2005" startWordPosition="1019" endWordPosition="1022">hat are also used in this area. They target specific error types commonly made by ESL learners, such as errors in prepositions and subject-verb agreement. In contrast, our method does not specify error types and aims to detect machine-translated sentences focusing on the phrase salad phenomenon produced by SMT systems. In addition, errors generated by ESL learners and SMT systems are different. ESL learners make spelling and grammar mistakes at the word level but their sentence are generally structured while SMT results are unstructured due to phrase salads. Works on translationese detection (Baroni and Bernardini, 2005; Kurokawa et al., 2009; Ilisei et al., 2010) aim to automatically identify humantranslated text by professionals using text generated by native speakers. These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). It automatically selects data for creating a domain-specific LM. Specifically, the method constructs LMs using corpora of target and non-target domains and computes a cross-entropy score of an input sentence for estimating the likelihood that the input sentence belongs to the target or non-target d</context>
<context position="24199" citStr="Baroni and Bernardini, 2005" startWordPosition="3841" endWordPosition="3844">ross-entropy difference, i.e., IMT(s) − IH(s), and decides that the sentence is machine-translated when the score is lower than a predefined threshold. The classification is performed by 10-fold cross validation. We find the best performing threshold on a training set and evaluate the accuracy with a test set using the determined threshold. Additionally, we compare our method to a method that uses a feature indicating presence or absence of unigrams, which we call Lexical Feature. This feature is commonly used for translationese detection and shows the best performance as a single feature in (Baroni and Bernardini, 2005). It is also used by Rarrick et al. (2011) and shows the best performance by itself in detecting machine-translated sentences in English-Japanese translation in the setting of bilingual input. We implement the feature and use it against a monolingual input to fit our problem setting. 5 Results and Discussions In this section, we analyze and discuss the experiment results in detail. 5.1 Accuracy on Japanese Dataset We evaluate the sentence-level and documentlevel accuracy of our method using the Japanese dataset. Specifically, we evaluate effects of individual features and their combinations, c</context>
</contexts>
<marker>Baroni, Bernardini, 2005</marker>
<rawString>Marco Baroni and Silvia Bernardini. 2005. A new approach to the study of translationese: Machinelearning the difference between original and translated text. Literary and Linguistic Computing, 21(3):259–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM : a library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="22150" citStr="Chang and Lin, 2011" startWordPosition="3502" endWordPosition="3505">sentences Humangenerated sentences ... ... Method Accuracy Cross-Entropy 90.7 Lexical Feature 87.8 Proposed feature Word LMs 94.1 POS LMs 91.3 FW LMs 82.7 GPs 85.7 Japanese Japanese Table 4: Accuracy (%) of individual features and comparison methods Figure 2: Experimental data preparation; text in one language is fed to SMT systems and the other is used as humangenerated sentences. the effect of the sizes of N-grams and development dataset in the experiments. Using the proposed features, we train an SVM classifier for detecting machine-translated sentences. We use an implementation of LIBSVM (Chang and Lin, 2011) with a radial basis function kernel due to the relatively small number of features in the proposed method. We set appropriate parameters by grid search in a preliminary experiment. We evaluate the performance of MT detection based on accuracy6 that is a broadly used evaluation metric for classification problems: nTP + nTN accuracy = , n where nTP and nTN are the numbers of truepositives and true-negatives, respectively, and n is the total number of exemplars. The accuracy scores that we report in Sec. 5 are all based on 10- fold cross validation. 4.3 Comparison Method We compare our method wi</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM : a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="20148" citStr="Chiang, 2005" startWordPosition="3171" endWordPosition="3172">fference. As a dataset that meets these requirements, we use parallel text in public websites (this is for fair evaluation and our method can be trained using nonparallel text on an actual deployment). Eight popular sites having Japanese and English parallel pages are crawled, whose text is manually verified to be human-generated. The main textual content of these 131K parallel pages are extracted, and the sentences are aligned using (Ma, 2006). As illustrated in Fig. 2, the text in one language is fed to the Bing translator, Google Translate, and an in-house SMT system4 implemented based on (Chiang, 2005) by ourselves for obtaining sentences translated by SMT systems. Due to a severe limitation on the number of requests to the APIs, we randomly subsample sentences before sending them to these SMT systems. We use text in the other language as human-generated sentences5. In this manner, we prepare 508K humangenerated and 268K machine-translated sentences as a Japanese dataset, and 420K human-generated and 318K machine-translated sentences as an English dataset. We split each of them into two even datasets and use one for development and the other for evaluation. 4.2 Experiment Setting For the fl</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Michael Gamon</author>
<author>Chris Brockett</author>
</authors>
<title>A machine learning approach to the automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>148--155</pages>
<marker>Corston-Oliver, Gamon, Brockett, 2001</marker>
<rawString>Simon Corston-Oliver, Michael Gamon, and Chris Brockett. 2001. A machine learning approach to the automatic evaluation of machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2001), pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>1535--1545</pages>
<contexts>
<context position="1490" citStr="Fader et al., 2011" startWordPosition="224" endWordPosition="227">e proposed method achieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages. 1 Introduction The Web provides an extremely large volume of textual content on diverse topics and areas. Such data is beneficial for constructing a large scale monolingual (Microsoft Web N-gram Services, 2010; Google N-gram Corpus, 2006) and bilingual (Nie et al., 1999; Shi et al., 2006; Ishisaka et al., 2009; Jiang et al., 2009) corpus that can be used for training statistical models for NLP tools, as well as for building a large-scale knowledge-base (Suchanek et al., 2007; Zhu et al., 2009; Fader et al., 2011; Nakashole et al., 2012). With recent advances in statistical machine translation (SMT) systems and their wide adoption in Web services through APIs (Microsoft Translator, 2009; Google Translate, 2006), a large amount of text in Web pages is translated by SMT systems. According to Rarrick et al. (2011), their Web crawler finds that more than 15% of EnglishJapanese parallel documents are machine translation. Machine-translated sentences are useful if they are of sufficient quality and indistinguishable from human-generated sentences; however, the quality of these machine-translated sentences i</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2011), pages 1535–1545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Anthony Aue</author>
<author>Martine Smets</author>
</authors>
<title>Sentence-level MT evaluation without reference translations: Beyond language modeling.</title>
<date>2005</date>
<booktitle>In Proceedings of European Association for Machine Translation (EAMT</booktitle>
<contexts>
<context position="5423" citStr="Gamon et al., 2005" startWordPosition="824" endWordPosition="827"> results are detected. Rarrick et al. (2011) extract a variety of features, such as the number of tokens and character types, from sentences in both the source and target languages to capture words that are mis-translated by MT systems. With these features, the likelihood of a bilingual sentence pair being machine-translated can be determined. Confidence estimation of MT results is also a related area. These studies aim to precisely replicate human judgment in terms of the quality of machine-translated sentences based on features extracted using a syntactic parser (CorstonOliver et al., 2001; Gamon et al., 2005; Avramidis et al., 2011) or essay scoring system (Parton et al., 2011), assuming that their input is always machine-translated. In contrast, our method aims at making a binary judgment to distinguish machine-translated sentences from a mixture of machine-translated and human-generated sentences. In addition, although methods for confidence estimation can assume sentences of a known source language and reference translations as inputs, these are unavailable in our problem setting. Another related area is automatic grammatical error detection for English as a second language (ESL) learners (Lea</context>
</contexts>
<marker>Gamon, Aue, Smets, 2005</marker>
<rawString>Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-level MT evaluation without reference translations: Beyond language modeling. In Proceedings of European Association for Machine Translation (EAMT 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google N-gram Corpus</author>
</authors>
<date>2006</date>
<note>http://www.ldc. upenn.edu/Catalog/CatalogEntry. jsp?catalogId=LDC2006T13.</note>
<contexts>
<context position="1211" citStr="Corpus, 2006" startWordPosition="176" endWordPosition="177"> sentences from a large-scale Web-mined text. Unlike previous approaches that require bilingual data, our method uses only monolingual text as input; therefore it is applicable for refining data produced by a variety of Web-mining activities. Evaluation results show that the proposed method achieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages. 1 Introduction The Web provides an extremely large volume of textual content on diverse topics and areas. Such data is beneficial for constructing a large scale monolingual (Microsoft Web N-gram Services, 2010; Google N-gram Corpus, 2006) and bilingual (Nie et al., 1999; Shi et al., 2006; Ishisaka et al., 2009; Jiang et al., 2009) corpus that can be used for training statistical models for NLP tools, as well as for building a large-scale knowledge-base (Suchanek et al., 2007; Zhu et al., 2009; Fader et al., 2011; Nakashole et al., 2012). With recent advances in statistical machine translation (SMT) systems and their wide adoption in Web services through APIs (Microsoft Translator, 2009; Google Translate, 2006), a large amount of text in Web pages is translated by SMT systems. According to Rarrick et al. (2011), their Web crawl</context>
</contexts>
<marker>Corpus, 2006</marker>
<rawString>Google N-gram Corpus. 2006. http://www.ldc. upenn.edu/Catalog/CatalogEntry. jsp?catalogId=LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google Translate</author>
</authors>
<date>2006</date>
<note>http://code.google. com/apis/language/.</note>
<contexts>
<context position="1692" citStr="Translate, 2006" startWordPosition="255" endWordPosition="256">s. Such data is beneficial for constructing a large scale monolingual (Microsoft Web N-gram Services, 2010; Google N-gram Corpus, 2006) and bilingual (Nie et al., 1999; Shi et al., 2006; Ishisaka et al., 2009; Jiang et al., 2009) corpus that can be used for training statistical models for NLP tools, as well as for building a large-scale knowledge-base (Suchanek et al., 2007; Zhu et al., 2009; Fader et al., 2011; Nakashole et al., 2012). With recent advances in statistical machine translation (SMT) systems and their wide adoption in Web services through APIs (Microsoft Translator, 2009; Google Translate, 2006), a large amount of text in Web pages is translated by SMT systems. According to Rarrick et al. (2011), their Web crawler finds that more than 15% of EnglishJapanese parallel documents are machine translation. Machine-translated sentences are useful if they are of sufficient quality and indistinguishable from human-generated sentences; however, the quality of these machine-translated sentences is generally much lower than sentences generated by native speakers and professional translators. Therefore, a method to detect and filter such SMT results is desired to best make use of Web-mined data. </context>
</contexts>
<marker>Translate, 2006</marker>
<rawString>Google Translate. 2006. http://code.google. com/apis/language/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iustina Ilisei</author>
<author>Diana Inkpen</author>
<author>Gloria Corpas Pastor</author>
<author>Ruslan Mitkov</author>
</authors>
<title>Identification of translationese: A machine learning approach.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Intelligent Text Processing and Computational Linguistics (CICLing</booktitle>
<pages>503--511</pages>
<contexts>
<context position="6739" citStr="Ilisei et al., 2010" startWordPosition="1027" endWordPosition="1030"> error types commonly made by ESL learners, such as errors in prepositions and subject-verb agreement. In contrast, our method does not specify error types and aims to detect machine-translated sentences focusing on the phrase salad phenomenon produced by SMT systems. In addition, errors generated by ESL learners and SMT systems are different. ESL learners make spelling and grammar mistakes at the word level but their sentence are generally structured while SMT results are unstructured due to phrase salads. Works on translationese detection (Baroni and Bernardini, 2005; Kurokawa et al., 2009; Ilisei et al., 2010) aim to automatically identify humantranslated text by professionals using text generated by native speakers. These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). It automatically selects data for creating a domain-specific LM. Specifically, the method constructs LMs using corpora of target and non-target domains and computes a cross-entropy score of an input sentence for estimating the likelihood that the input sentence belongs to the target or non-target domains. While the context is different, our w</context>
</contexts>
<marker>Ilisei, Inkpen, Pastor, Mitkov, 2010</marker>
<rawString>Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and Ruslan Mitkov. 2010. Identification of translationese: A machine learning approach. In Proceedings of the International Conference on Intelligent Text Processing and Computational Linguistics (CICLing 2010), pages 503–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tatsuya Ishisaka</author>
<author>Masao Utiyama</author>
<author>Eiichiro Sumita</author>
<author>Kazuhide Yamamoto</author>
</authors>
<title>Development of a Japanese-English software manual parallel corpus.</title>
<date>2009</date>
<booktitle>In Proceedings of the Machine Translation Summit (MT Summit XII).</booktitle>
<contexts>
<context position="1284" citStr="Ishisaka et al., 2009" startWordPosition="188" endWordPosition="191">roaches that require bilingual data, our method uses only monolingual text as input; therefore it is applicable for refining data produced by a variety of Web-mining activities. Evaluation results show that the proposed method achieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages. 1 Introduction The Web provides an extremely large volume of textual content on diverse topics and areas. Such data is beneficial for constructing a large scale monolingual (Microsoft Web N-gram Services, 2010; Google N-gram Corpus, 2006) and bilingual (Nie et al., 1999; Shi et al., 2006; Ishisaka et al., 2009; Jiang et al., 2009) corpus that can be used for training statistical models for NLP tools, as well as for building a large-scale knowledge-base (Suchanek et al., 2007; Zhu et al., 2009; Fader et al., 2011; Nakashole et al., 2012). With recent advances in statistical machine translation (SMT) systems and their wide adoption in Web services through APIs (Microsoft Translator, 2009; Google Translate, 2006), a large amount of text in Web pages is translated by SMT systems. According to Rarrick et al. (2011), their Web crawler finds that more than 15% of EnglishJapanese parallel documents are mac</context>
</contexts>
<marker>Ishisaka, Utiyama, Sumita, Yamamoto, 2009</marker>
<rawString>Tatsuya Ishisaka, Masao Utiyama, Eiichiro Sumita, and Kazuhide Yamamoto. 2009. Development of a Japanese-English software manual parallel corpus. In Proceedings of the Machine Translation Summit (MT Summit XII).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Long Jiang</author>
<author>Shiquan Yang</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Qingsheng Zhu</author>
</authors>
<title>Mining bilingual data from the web with adaptively learnt patterns.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP</booktitle>
<pages>870--878</pages>
<contexts>
<context position="1305" citStr="Jiang et al., 2009" startWordPosition="192" endWordPosition="195">lingual data, our method uses only monolingual text as input; therefore it is applicable for refining data produced by a variety of Web-mining activities. Evaluation results show that the proposed method achieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages. 1 Introduction The Web provides an extremely large volume of textual content on diverse topics and areas. Such data is beneficial for constructing a large scale monolingual (Microsoft Web N-gram Services, 2010; Google N-gram Corpus, 2006) and bilingual (Nie et al., 1999; Shi et al., 2006; Ishisaka et al., 2009; Jiang et al., 2009) corpus that can be used for training statistical models for NLP tools, as well as for building a large-scale knowledge-base (Suchanek et al., 2007; Zhu et al., 2009; Fader et al., 2011; Nakashole et al., 2012). With recent advances in statistical machine translation (SMT) systems and their wide adoption in Web services through APIs (Microsoft Translator, 2009; Google Translate, 2006), a large amount of text in Web pages is translated by SMT systems. According to Rarrick et al. (2011), their Web crawler finds that more than 15% of EnglishJapanese parallel documents are machine translation. Mac</context>
</contexts>
<marker>Jiang, Yang, Zhou, Liu, Zhu, 2009</marker>
<rawString>Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, and Qingsheng Zhu. 2009. Mining bilingual data from the web with adaptively learnt patterns. In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP 2009), pages 870–878.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<pages>230--237</pages>
<contexts>
<context position="20934" citStr="Kudo et al., 2004" startWordPosition="3295" endWordPosition="3298">e sending them to these SMT systems. We use text in the other language as human-generated sentences5. In this manner, we prepare 508K humangenerated and 268K machine-translated sentences as a Japanese dataset, and 420K human-generated and 318K machine-translated sentences as an English dataset. We split each of them into two even datasets and use one for development and the other for evaluation. 4.2 Experiment Setting For the fluency and grammaticality features, we train 4-gram LMs using the development dataset with the SRI toolkit (Stolcke, 2002). To obtain the POS information, we use Mecab (Kudo et al., 2004) for Japanese and a POS tagger developed by Toutanova et al. (2003) for English. We evaluate 4A preliminary evaluation of the in-house SMT system shows that it has comparable quality with Bing translator. 5These are a mixture of sentences generated by native speakers and professional translators/editors. in the early * period after * after the and also * and and * but the no * not not * not MT Human not only * but also with * as well as known as * to more * than 1601 Parallel sentences Japanese English Humangenerated sentences Humangenerated sentences ... ... MT systems Machine translated sent</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pages 230– 237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kurokawa</author>
<author>Cyril Goutte</author>
<author>Pierre Isabelle</author>
</authors>
<title>Automatic detection of translated text and its impact on machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Machine Translation Summit (MT-Summit XII).</booktitle>
<contexts>
<context position="6717" citStr="Kurokawa et al., 2009" startWordPosition="1023" endWordPosition="1026">a. They target specific error types commonly made by ESL learners, such as errors in prepositions and subject-verb agreement. In contrast, our method does not specify error types and aims to detect machine-translated sentences focusing on the phrase salad phenomenon produced by SMT systems. In addition, errors generated by ESL learners and SMT systems are different. ESL learners make spelling and grammar mistakes at the word level but their sentence are generally structured while SMT results are unstructured due to phrase salads. Works on translationese detection (Baroni and Bernardini, 2005; Kurokawa et al., 2009; Ilisei et al., 2010) aim to automatically identify humantranslated text by professionals using text generated by native speakers. These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). It automatically selects data for creating a domain-specific LM. Specifically, the method constructs LMs using corpora of target and non-target domains and computes a cross-entropy score of an input sentence for estimating the likelihood that the input sentence belongs to the target or non-target domains. While the conte</context>
</contexts>
<marker>Kurokawa, Goutte, Isabelle, 2009</marker>
<rawString>David Kurokawa, Cyril Goutte, and Pierre Isabelle. 2009. Automatic detection of translated text and its impact on machine translation. In Proceedings of the Machine Translation Summit (MT-Summit XII).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
<author>Michael Gamon</author>
<author>Joel Tetreault</author>
</authors>
<title>Automated Grammatical Error Detection for Language Learners.</title>
<date>2010</date>
<publisher>Morgan and Claypool Publishers.</publisher>
<contexts>
<context position="6041" citStr="Leacock et al., 2010" startWordPosition="916" endWordPosition="919">005; Avramidis et al., 2011) or essay scoring system (Parton et al., 2011), assuming that their input is always machine-translated. In contrast, our method aims at making a binary judgment to distinguish machine-translated sentences from a mixture of machine-translated and human-generated sentences. In addition, although methods for confidence estimation can assume sentences of a known source language and reference translations as inputs, these are unavailable in our problem setting. Another related area is automatic grammatical error detection for English as a second language (ESL) learners (Leacock et al., 2010). We use common features that are also used in this area. They target specific error types commonly made by ESL learners, such as errors in prepositions and subject-verb agreement. In contrast, our method does not specify error types and aims to detect machine-translated sentences focusing on the phrase salad phenomenon produced by SMT systems. In addition, errors generated by ESL learners and SMT systems are different. ESL learners make spelling and grammar mistakes at the word level but their sentence are generally structured while SMT results are unstructured due to phrase salads. Works on </context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2010. Automated Grammatical Error Detection for Language Learners. Morgan and Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Statistical machine translation.</title>
<date>2008</date>
<journal>ACM Computing Surveys,</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="2555" citStr="Lopez, 2008" startWordPosition="387" endWordPosition="388">f they are of sufficient quality and indistinguishable from human-generated sentences; however, the quality of these machine-translated sentences is generally much lower than sentences generated by native speakers and professional translators. Therefore, a method to detect and filter such SMT results is desired to best make use of Web-mined data. To solve this problem, we propose a method for automatically detecting Web-text translated by SMT systems1. We especially target machinetranslated text produced through the Web APIs that is rapidly increasing. We focus on the phrase salad phenomenon (Lopez, 2008), which characterizes translations by existing SMT systems, i.e., each phrase in a sentence is semantically and syntactically correct but becomes incorrect when combined with other phrases in the sentence. Based on this trait, we propose features for evaluating the likelihood of machine-translated sentences and use a classifier to determine whether the sentence is generated by the SMT systems. The primary contributions of the proposed method are threefold. First, unlike previous studies that use parallel text and bilingual features, such as (Rarrick et al., 2011), our method only requires mono</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam Lopez. 2008. Statistical machine translation. ACM Computing Surveys, 40(3):1–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyi Ma</author>
</authors>
<title>Champollion: a robust parallel text sentence aligner.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>489--492</pages>
<contexts>
<context position="19983" citStr="Ma, 2006" startWordPosition="3143" endWordPosition="3144">hinetranslated, and the human-generated and machinetranslated sentences express the same content for fairness of evaluation to avoid effects due to vocabulary difference. As a dataset that meets these requirements, we use parallel text in public websites (this is for fair evaluation and our method can be trained using nonparallel text on an actual deployment). Eight popular sites having Japanese and English parallel pages are crawled, whose text is manually verified to be human-generated. The main textual content of these 131K parallel pages are extracted, and the sentences are aligned using (Ma, 2006). As illustrated in Fig. 2, the text in one language is fed to the Bing translator, Google Translate, and an in-house SMT system4 implemented based on (Chiang, 2005) by ourselves for obtaining sentences translated by SMT systems. Due to a severe limitation on the number of requests to the APIs, we randomly subsample sentences before sending them to these SMT systems. We use text in the other language as human-generated sentences5. In this manner, we prepare 508K humangenerated and 268K machine-translated sentences as a Japanese dataset, and 420K human-generated and 318K machine-translated sent</context>
</contexts>
<marker>Ma, 2006</marker>
<rawString>Xiaoyi Ma. 2006. Champollion: a robust parallel text sentence aligner. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2006), pages 489–492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Microsoft Translator</author>
</authors>
<date>2009</date>
<note>http://www. microsofttranslator.com/dev/.</note>
<contexts>
<context position="1667" citStr="Translator, 2009" startWordPosition="252" endWordPosition="253">n diverse topics and areas. Such data is beneficial for constructing a large scale monolingual (Microsoft Web N-gram Services, 2010; Google N-gram Corpus, 2006) and bilingual (Nie et al., 1999; Shi et al., 2006; Ishisaka et al., 2009; Jiang et al., 2009) corpus that can be used for training statistical models for NLP tools, as well as for building a large-scale knowledge-base (Suchanek et al., 2007; Zhu et al., 2009; Fader et al., 2011; Nakashole et al., 2012). With recent advances in statistical machine translation (SMT) systems and their wide adoption in Web services through APIs (Microsoft Translator, 2009; Google Translate, 2006), a large amount of text in Web pages is translated by SMT systems. According to Rarrick et al. (2011), their Web crawler finds that more than 15% of EnglishJapanese parallel documents are machine translation. Machine-translated sentences are useful if they are of sufficient quality and indistinguishable from human-generated sentences; however, the quality of these machine-translated sentences is generally much lower than sentences generated by native speakers and professional translators. Therefore, a method to detect and filter such SMT results is desired to best mak</context>
</contexts>
<marker>Translator, 2009</marker>
<rawString>Microsoft Translator. 2009. http://www. microsofttranslator.com/dev/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Microsoft Web</author>
</authors>
<title>N-gram Services.</title>
<date>2010</date>
<note>http:// research.microsoft.com/web-ngram.</note>
<marker>Web, 2010</marker>
<rawString>Microsoft Web N-gram Services. 2010. http:// research.microsoft.com/web-ngram.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>220--224</pages>
<contexts>
<context position="6993" citStr="Moore and Lewis (2010)" startWordPosition="1068" endWordPosition="1071"> by SMT systems. In addition, errors generated by ESL learners and SMT systems are different. ESL learners make spelling and grammar mistakes at the word level but their sentence are generally structured while SMT results are unstructured due to phrase salads. Works on translationese detection (Baroni and Bernardini, 2005; Kurokawa et al., 2009; Ilisei et al., 2010) aim to automatically identify humantranslated text by professionals using text generated by native speakers. These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). It automatically selects data for creating a domain-specific LM. Specifically, the method constructs LMs using corpora of target and non-target domains and computes a cross-entropy score of an input sentence for estimating the likelihood that the input sentence belongs to the target or non-target domains. While the context is different, our work uses a similar idea of data selection for the purpose of detecting low-quality sentences translated by SMT systems. 3 Proposed Method When APIs of SMT services are used for machinetranslating an Web page, they typically insert specific tags into the </context>
<context position="22790" citStr="Moore and Lewis, 2010" startWordPosition="3613" endWordPosition="3616">is function kernel due to the relatively small number of features in the proposed method. We set appropriate parameters by grid search in a preliminary experiment. We evaluate the performance of MT detection based on accuracy6 that is a broadly used evaluation metric for classification problems: nTP + nTN accuracy = , n where nTP and nTN are the numbers of truepositives and true-negatives, respectively, and n is the total number of exemplars. The accuracy scores that we report in Sec. 5 are all based on 10- fold cross validation. 4.3 Comparison Method We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy). Although the Cross-Entropy method is designed for the task of domain adaptation of an LM, our problem is a variant of their original problem and thus their method is directly relevant. In our context, the method computes the cross-entropy scores IMT(s) and IH(s) of an input sentence s against LMs trained on machine-translated and human-generated sentences. Cross-entropy and perplexity are monotonically related, as perplexity of s according to an LM M is simply ob6Although we also examine precision and recall of classification results, they are similar to accuracy reported in </context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages 220–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Gerhard Weikum</author>
<author>Fabian M Suchanek</author>
</authors>
<title>PATTY: A taxonomy of relational patterns with semantic types.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>1135--1145</pages>
<contexts>
<context position="1515" citStr="Nakashole et al., 2012" startWordPosition="228" endWordPosition="231">hieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages. 1 Introduction The Web provides an extremely large volume of textual content on diverse topics and areas. Such data is beneficial for constructing a large scale monolingual (Microsoft Web N-gram Services, 2010; Google N-gram Corpus, 2006) and bilingual (Nie et al., 1999; Shi et al., 2006; Ishisaka et al., 2009; Jiang et al., 2009) corpus that can be used for training statistical models for NLP tools, as well as for building a large-scale knowledge-base (Suchanek et al., 2007; Zhu et al., 2009; Fader et al., 2011; Nakashole et al., 2012). With recent advances in statistical machine translation (SMT) systems and their wide adoption in Web services through APIs (Microsoft Translator, 2009; Google Translate, 2006), a large amount of text in Web pages is translated by SMT systems. According to Rarrick et al. (2011), their Web crawler finds that more than 15% of EnglishJapanese parallel documents are machine translation. Machine-translated sentences are useful if they are of sufficient quality and indistinguishable from human-generated sentences; however, the quality of these machine-translated sentences is generally much lower th</context>
</contexts>
<marker>Nakashole, Weikum, Suchanek, 2012</marker>
<rawString>Ndapandula Nakashole, Gerhard Weikum, and Fabian M. Suchanek. 2012. PATTY: A taxonomy of relational patterns with semantic types. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012), pages 1135–1145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-Yun Nie</author>
<author>Michel Simard</author>
<author>Pierre Isabelle</author>
<author>Richard Durand</author>
</authors>
<title>Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the web.</title>
<date>1999</date>
<booktitle>In Proceedings of the Annual International ACM SIGIR Conference (SIGIR</booktitle>
<pages>74--81</pages>
<contexts>
<context position="1243" citStr="Nie et al., 1999" startWordPosition="180" endWordPosition="183"> Web-mined text. Unlike previous approaches that require bilingual data, our method uses only monolingual text as input; therefore it is applicable for refining data produced by a variety of Web-mining activities. Evaluation results show that the proposed method achieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages. 1 Introduction The Web provides an extremely large volume of textual content on diverse topics and areas. Such data is beneficial for constructing a large scale monolingual (Microsoft Web N-gram Services, 2010; Google N-gram Corpus, 2006) and bilingual (Nie et al., 1999; Shi et al., 2006; Ishisaka et al., 2009; Jiang et al., 2009) corpus that can be used for training statistical models for NLP tools, as well as for building a large-scale knowledge-base (Suchanek et al., 2007; Zhu et al., 2009; Fader et al., 2011; Nakashole et al., 2012). With recent advances in statistical machine translation (SMT) systems and their wide adoption in Web services through APIs (Microsoft Translator, 2009; Google Translate, 2006), a large amount of text in Web pages is translated by SMT systems. According to Rarrick et al. (2011), their Web crawler finds that more than 15% of E</context>
</contexts>
<marker>Nie, Simard, Isabelle, Durand, 1999</marker>
<rawString>Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Richard Durand. 1999. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the web. In Proceedings of the Annual International ACM SIGIR Conference (SIGIR 1999), pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>311--318</pages>
<contexts>
<context position="4702" citStr="Papineni et al., 2002" startWordPosition="711" endWordPosition="714">hrase salad by contrasting these features, which significantly improves detection accuracy. We evaluate our method using Japanese and English datasets, including a human evaluation to assess its performance. The results show that our method achieves an accuracy of 95.8% for sentences and 80.6% for noisy Web-text. 2 Related Work Previous methods for detecting machinetranslated text are mostly designed for bilingual corpus construction. Antonova and Misyurev (2011) design a phrase-based decoder for detecting machine-translated documents in Russian-English Web data. By evaluating the BLEU score (Papineni et al., 2002) of translated documents (by their decoder) against the target-side documents, machine translation (MT) results are detected. Rarrick et al. (2011) extract a variety of features, such as the number of tokens and character types, from sentences in both the source and target languages to capture words that are mis-translated by MT systems. With these features, the likelihood of a bilingual sentence pair being machine-translated can be determined. Confidence estimation of MT results is also a related area. These studies aim to precisely replicate human judgment in terms of the quality of machine-</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristen Parton</author>
<author>Joel Tetreault</author>
<author>Nitin Madnani</author>
<author>Martin Chodorow</author>
</authors>
<title>E-rating machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation (WMT</booktitle>
<pages>108--115</pages>
<contexts>
<context position="5494" citStr="Parton et al., 2011" startWordPosition="836" endWordPosition="839">tures, such as the number of tokens and character types, from sentences in both the source and target languages to capture words that are mis-translated by MT systems. With these features, the likelihood of a bilingual sentence pair being machine-translated can be determined. Confidence estimation of MT results is also a related area. These studies aim to precisely replicate human judgment in terms of the quality of machine-translated sentences based on features extracted using a syntactic parser (CorstonOliver et al., 2001; Gamon et al., 2005; Avramidis et al., 2011) or essay scoring system (Parton et al., 2011), assuming that their input is always machine-translated. In contrast, our method aims at making a binary judgment to distinguish machine-translated sentences from a mixture of machine-translated and human-generated sentences. In addition, although methods for confidence estimation can assume sentences of a known source language and reference translations as inputs, these are unavailable in our problem setting. Another related area is automatic grammatical error detection for English as a second language (ESL) learners (Leacock et al., 2010). We use common features that are also used in this a</context>
</contexts>
<marker>Parton, Tetreault, Madnani, Chodorow, 2011</marker>
<rawString>Kristen Parton, Joel Tetreault, Nitin Madnani, and Martin Chodorow. 2011. E-rating machine translation. In Proceedings of the Workshop on Statistical Machine Translation (WMT 2011), pages 108–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Pei</author>
<author>Jiawei Han</author>
<author>Behzad Mortazavi-Asl</author>
<author>Helen Pinto</author>
<author>Qiming Chen</author>
<author>Umeshwar Dayal</author>
<author>MeiChun Hsu</author>
</authors>
<title>PrefixSpan: Mining sequential patterns efficiently by prefix-projected pattern growth.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Data Engineering (ICDE</booktitle>
<pages>215--224</pages>
<contexts>
<context position="13874" citStr="Pei et al. (2001)" startWordPosition="2178" endWordPosition="2181">ntains the phrase “not only,” the phrase “but also” is likely to appear in human-generated setences. Such a gappy-phrase is difficult for SMT systems to correctly generate and causes a phrase salad. Therefore, we define a feature to evaluate how likely a sentence contains gappy-phrases in a complete form without missing sub-phrases. This feature is effective to complement LMs that capture characteristics in N-grams. Sequential Pattern Mining It is costly to manually collect a lot of such gappy-phrases. Therefore, we regard the task as sequential pattern mining and apply PrefixSpan proposed by Pei et al. (2001), which is a widely used sequential pattern mining method3. Given a set of sequences and a user-specified min support E N threshold, the sequential pattern mining finds all frequent subsequences whose occurrence frequency is no less than min support. For example, given a sequence database like Table 1, the sequential pattern mining finds all frequent subsequences, e.g., “not only,” “not only � but also,” “not * but *,” and etc. To capture a phrase salad by contrasting appearance of gappy-phrases in human-generated and machine-translated text, we independently extract gappy-phrases from each of</context>
</contexts>
<marker>Pei, Han, Mortazavi-Asl, Pinto, Chen, Dayal, Hsu, 2001</marker>
<rawString>Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Helen Pinto, Qiming Chen, Umeshwar Dayal, and MeiChun Hsu. 2001. PrefixSpan: Mining sequential patterns efficiently by prefix-projected pattern growth. In Proceedings of the International Conference on Data Engineering (ICDE 2001), pages 215–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spencer Rarrick</author>
<author>Chris Quirk</author>
<author>Will Lewis</author>
</authors>
<title>MT detection in web-scraped parallel corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of the Machine Translation Summit (MT Summit XIII).</booktitle>
<contexts>
<context position="1794" citStr="Rarrick et al. (2011)" startWordPosition="273" endWordPosition="276">ces, 2010; Google N-gram Corpus, 2006) and bilingual (Nie et al., 1999; Shi et al., 2006; Ishisaka et al., 2009; Jiang et al., 2009) corpus that can be used for training statistical models for NLP tools, as well as for building a large-scale knowledge-base (Suchanek et al., 2007; Zhu et al., 2009; Fader et al., 2011; Nakashole et al., 2012). With recent advances in statistical machine translation (SMT) systems and their wide adoption in Web services through APIs (Microsoft Translator, 2009; Google Translate, 2006), a large amount of text in Web pages is translated by SMT systems. According to Rarrick et al. (2011), their Web crawler finds that more than 15% of EnglishJapanese parallel documents are machine translation. Machine-translated sentences are useful if they are of sufficient quality and indistinguishable from human-generated sentences; however, the quality of these machine-translated sentences is generally much lower than sentences generated by native speakers and professional translators. Therefore, a method to detect and filter such SMT results is desired to best make use of Web-mined data. To solve this problem, we propose a method for automatically detecting Web-text translated by SMT syst</context>
<context position="3124" citStr="Rarrick et al., 2011" startWordPosition="473" endWordPosition="476">e focus on the phrase salad phenomenon (Lopez, 2008), which characterizes translations by existing SMT systems, i.e., each phrase in a sentence is semantically and syntactically correct but becomes incorrect when combined with other phrases in the sentence. Based on this trait, we propose features for evaluating the likelihood of machine-translated sentences and use a classifier to determine whether the sentence is generated by the SMT systems. The primary contributions of the proposed method are threefold. First, unlike previous studies that use parallel text and bilingual features, such as (Rarrick et al., 2011), our method only requires monolingual text as input. Therefore, our method can be used in monolingual Web data mining where bilingual information is unavailable. Second, the proposed features are designed to be computationally light so that the method is suitable for handling a large-scale Web-mined data. Our method determines if an input sentence contains phrase salads using a simple yet effective features, i.e., language models (LMs) and automatically obtained non-contiguous phrases that are frequently used by people but difficult for SMT systems to generate. Third, our method computes feat</context>
<context position="4849" citStr="Rarrick et al. (2011)" startWordPosition="732" endWordPosition="735">ts, including a human evaluation to assess its performance. The results show that our method achieves an accuracy of 95.8% for sentences and 80.6% for noisy Web-text. 2 Related Work Previous methods for detecting machinetranslated text are mostly designed for bilingual corpus construction. Antonova and Misyurev (2011) design a phrase-based decoder for detecting machine-translated documents in Russian-English Web data. By evaluating the BLEU score (Papineni et al., 2002) of translated documents (by their decoder) against the target-side documents, machine translation (MT) results are detected. Rarrick et al. (2011) extract a variety of features, such as the number of tokens and character types, from sentences in both the source and target languages to capture words that are mis-translated by MT systems. With these features, the likelihood of a bilingual sentence pair being machine-translated can be determined. Confidence estimation of MT results is also a related area. These studies aim to precisely replicate human judgment in terms of the quality of machine-translated sentences based on features extracted using a syntactic parser (CorstonOliver et al., 2001; Gamon et al., 2005; Avramidis et al., 2011) </context>
<context position="24241" citStr="Rarrick et al. (2011)" startWordPosition="3850" endWordPosition="3853">d decides that the sentence is machine-translated when the score is lower than a predefined threshold. The classification is performed by 10-fold cross validation. We find the best performing threshold on a training set and evaluate the accuracy with a test set using the determined threshold. Additionally, we compare our method to a method that uses a feature indicating presence or absence of unigrams, which we call Lexical Feature. This feature is commonly used for translationese detection and shows the best performance as a single feature in (Baroni and Bernardini, 2005). It is also used by Rarrick et al. (2011) and shows the best performance by itself in detecting machine-translated sentences in English-Japanese translation in the setting of bilingual input. We implement the feature and use it against a monolingual input to fit our problem setting. 5 Results and Discussions In this section, we analyze and discuss the experiment results in detail. 5.1 Accuracy on Japanese Dataset We evaluate the sentence-level and documentlevel accuracy of our method using the Japanese dataset. Specifically, we evaluate effects of individual features and their combinations, compare with human annotations, and assess </context>
<context position="33573" citStr="Rarrick et al. (2011)" startWordPosition="5416" endWordPosition="5419"> FW LMs are smaller than Japanese case, their effects are still positive. We also find that GPs stably contribute to the accuracy. These results show the applicability of our method to other languages. 5.3 Accuracy on Raw Web Pages To avoid unmodeled factors affecting the evaluation, we have carefully removed noise from our experiment datasets. However, real Web pages are 1604 more complex; there are often instances of sentence fragments, such as captions and navigational link text. To evaluate the accuracy of our method on real Web pages, we conduct experiments using the dataset generated by Rarrick et al. (2011) that contains randomly crawled Web pages annotated by two annotators to judge if a page is humangenerated or machine-translated. We use Japanese sentences extracted from 69 pages (43 humangenerated and 26 machine-translated pages) where the annotators’ judgments agree; 3,312 sentences consisting of 1, 399 machine-translated and 1, 913 human-generated sentences. To replicate the situation in real Web pages, we conduct a minimal preprocessing, i.e., simply removing HTML tags, and then feed all the remaining text to our method. An SVM classifier is trained with features obtained by the LMs and g</context>
</contexts>
<marker>Rarrick, Quirk, Lewis, 2011</marker>
<rawString>Spencer Rarrick, Chris Quirk, and Will Lewis. 2011. MT detection in web-scraped parallel corpora. In Proceedings of the Machine Translation Summit (MT Summit XIII).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="16219" citStr="Sebastiani, 2002" startWordPosition="2535" endWordPosition="2536">m human-generated sentences. We also obtain about 74K and 42K phrases from humangenerated and machine-translated sentences in the English dataset (21K of them are common). Phrase Selection As a result of sequential pattern mining, we can gather a huge number of gappy-phrases from human-generated and machine-translated text, but as we described above, many of them are common. In addition, it is computationally expensive to use all of them. Therefore, our method selects useful phrases for detecting machine-translated sentences. Although there are several approaches for feature selection, e.g., (Sebastiani, 2002), we use a method that is suitable for handling a large number of feature candidates. Specifically, we evaluate gappy-phrases based on the information gain that measures the amount of information in bits obtained for class prediction when knowing the presence or absence of a phrase and the corresponding class distribution. This corresponds to measuring an expected reduction in entropy, i.e., uncertainty associated with a random factor. The information gain G E R for a gappy-phrase g is defined as G(g) .= H(C) − P(X9)H(C�X9) −P(X9)H(C�X9), where H(C) represents the entropy of the classification</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Cheng Niu</author>
<author>Ming Zhou</author>
<author>Jianfeng Gao</author>
</authors>
<title>A DOM tree alignment model for mining parallel data from the web.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics (COLING-ACL</booktitle>
<pages>489--496</pages>
<contexts>
<context position="1261" citStr="Shi et al., 2006" startWordPosition="184" endWordPosition="187">nlike previous approaches that require bilingual data, our method uses only monolingual text as input; therefore it is applicable for refining data produced by a variety of Web-mining activities. Evaluation results show that the proposed method achieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages. 1 Introduction The Web provides an extremely large volume of textual content on diverse topics and areas. Such data is beneficial for constructing a large scale monolingual (Microsoft Web N-gram Services, 2010; Google N-gram Corpus, 2006) and bilingual (Nie et al., 1999; Shi et al., 2006; Ishisaka et al., 2009; Jiang et al., 2009) corpus that can be used for training statistical models for NLP tools, as well as for building a large-scale knowledge-base (Suchanek et al., 2007; Zhu et al., 2009; Fader et al., 2011; Nakashole et al., 2012). With recent advances in statistical machine translation (SMT) systems and their wide adoption in Web services through APIs (Microsoft Translator, 2009; Google Translate, 2006), a large amount of text in Web pages is translated by SMT systems. According to Rarrick et al. (2011), their Web crawler finds that more than 15% of EnglishJapanese par</context>
</contexts>
<marker>Shi, Niu, Zhou, Gao, 2006</marker>
<rawString>Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao. 2006. A DOM tree alignment model for mining parallel data from the web. In Proceedings of the International Conference on Computational Linguistics and the Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006), pages 489–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP</booktitle>
<pages>901--904</pages>
<contexts>
<context position="20869" citStr="Stolcke, 2002" startWordPosition="3285" endWordPosition="3286">f requests to the APIs, we randomly subsample sentences before sending them to these SMT systems. We use text in the other language as human-generated sentences5. In this manner, we prepare 508K humangenerated and 268K machine-translated sentences as a Japanese dataset, and 420K human-generated and 318K machine-translated sentences as an English dataset. We split each of them into two even datasets and use one for development and the other for evaluation. 4.2 Experiment Setting For the fluency and grammaticality features, we train 4-gram LMs using the development dataset with the SRI toolkit (Stolcke, 2002). To obtain the POS information, we use Mecab (Kudo et al., 2004) for Japanese and a POS tagger developed by Toutanova et al. (2003) for English. We evaluate 4A preliminary evaluation of the in-house SMT system shows that it has comparable quality with Bing translator. 5These are a mixture of sentences generated by native speakers and professional translators/editors. in the early * period after * after the and also * and and * but the no * not not * not MT Human not only * but also with * as well as known as * to more * than 1601 Parallel sentences Japanese English Humangenerated sentences Hu</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM-an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing (ICSLP 2002), pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of International Conference on World Wide Web (WWW</booktitle>
<pages>697--706</pages>
<contexts>
<context position="1452" citStr="Suchanek et al., 2007" startWordPosition="216" endWordPosition="219">tivities. Evaluation results show that the proposed method achieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages. 1 Introduction The Web provides an extremely large volume of textual content on diverse topics and areas. Such data is beneficial for constructing a large scale monolingual (Microsoft Web N-gram Services, 2010; Google N-gram Corpus, 2006) and bilingual (Nie et al., 1999; Shi et al., 2006; Ishisaka et al., 2009; Jiang et al., 2009) corpus that can be used for training statistical models for NLP tools, as well as for building a large-scale knowledge-base (Suchanek et al., 2007; Zhu et al., 2009; Fader et al., 2011; Nakashole et al., 2012). With recent advances in statistical machine translation (SMT) systems and their wide adoption in Web services through APIs (Microsoft Translator, 2009; Google Translate, 2006), a large amount of text in Web pages is translated by SMT systems. According to Rarrick et al. (2011), their Web crawler finds that more than 15% of EnglishJapanese parallel documents are machine translation. Machine-translated sentences are useful if they are of sufficient quality and indistinguishable from human-generated sentences; however, the quality o</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of International Conference on World Wide Web (WWW 2007), pages 697–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLTNAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="21001" citStr="Toutanova et al. (2003)" startWordPosition="3307" endWordPosition="3310">language as human-generated sentences5. In this manner, we prepare 508K humangenerated and 268K machine-translated sentences as a Japanese dataset, and 420K human-generated and 318K machine-translated sentences as an English dataset. We split each of them into two even datasets and use one for development and the other for evaluation. 4.2 Experiment Setting For the fluency and grammaticality features, we train 4-gram LMs using the development dataset with the SRI toolkit (Stolcke, 2002). To obtain the POS information, we use Mecab (Kudo et al., 2004) for Japanese and a POS tagger developed by Toutanova et al. (2003) for English. We evaluate 4A preliminary evaluation of the in-house SMT system shows that it has comparable quality with Bing translator. 5These are a mixture of sentences generated by native speakers and professional translators/editors. in the early * period after * after the and also * and and * but the no * not not * not MT Human not only * but also with * as well as known as * to more * than 1601 Parallel sentences Japanese English Humangenerated sentences Humangenerated sentences ... ... MT systems Machine translated sentences Humangenerated sentences ... ... Method Accuracy Cross-Entrop</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLTNAACL 2003), pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="18675" citStr="Vapnik, 1995" startWordPosition="2947" endWordPosition="2949">se, such as the information gain. In this work, we set wi to 1 for simplicity. 3.5 Classification Table 3 summarizes the features employed in our method. In addition to the discussed features, we use the length of a sentence as a feature flen to avoid the bias of LM-based features that favor shorter sentences. The proposed method takes a monolingual sentence from Web data as input and computes a feature vector of f = (fw,H, . . . , flen) ∈ R9. Each feature is finally normalized to have a zero-mean and unit variance distribution. In the feature space, a support vector machine (SVM) classifier (Vapnik, 1995) is used to determine the likelihoods of machine-translated and human-generated sentences. 4 Experiments We evaluate our method using both Japanese and English datasets from various aspects and investigate its characteristics. In this section, we describe our experiment settings. 4.1 Data Preparation For the purpose of evaluation, we use humangenerated and machine-translated sentences for Feature Notation Fluency fw,H, fw,MT Grammaticality fpos,H, fpos,MT ffw,H, ffw,MT Gappy-phrase fg,H, fg,MT Length flen Table 3: List of proposed features and their notations constructing LMs, extracting gappy</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The nature of statistical learning theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Zaiqing Nie</author>
<author>Xiaojiang Liu</author>
<author>Bo Zhang</author>
<author>Ji-Rong Wen</author>
</authors>
<title>StatSnowball: a statistical approach to extracting entity relationships.</title>
<date>2009</date>
<booktitle>In Proceedings of International Conference on World Wide Web (WWW</booktitle>
<pages>101--110</pages>
<contexts>
<context position="1470" citStr="Zhu et al., 2009" startWordPosition="220" endWordPosition="223">sults show that the proposed method achieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages. 1 Introduction The Web provides an extremely large volume of textual content on diverse topics and areas. Such data is beneficial for constructing a large scale monolingual (Microsoft Web N-gram Services, 2010; Google N-gram Corpus, 2006) and bilingual (Nie et al., 1999; Shi et al., 2006; Ishisaka et al., 2009; Jiang et al., 2009) corpus that can be used for training statistical models for NLP tools, as well as for building a large-scale knowledge-base (Suchanek et al., 2007; Zhu et al., 2009; Fader et al., 2011; Nakashole et al., 2012). With recent advances in statistical machine translation (SMT) systems and their wide adoption in Web services through APIs (Microsoft Translator, 2009; Google Translate, 2006), a large amount of text in Web pages is translated by SMT systems. According to Rarrick et al. (2011), their Web crawler finds that more than 15% of EnglishJapanese parallel documents are machine translation. Machine-translated sentences are useful if they are of sufficient quality and indistinguishable from human-generated sentences; however, the quality of these machine-tr</context>
</contexts>
<marker>Zhu, Nie, Liu, Zhang, Wen, 2009</marker>
<rawString>Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. StatSnowball: a statistical approach to extracting entity relationships. In Proceedings of International Conference on World Wide Web (WWW 2009), pages 101–110.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>