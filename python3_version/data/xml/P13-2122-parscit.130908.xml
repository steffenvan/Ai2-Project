<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002862">
<title confidence="0.9981355">
Incremental Topic-Based Translation Model Adaptation for
Conversational Spoken Language Translation
</title>
<author confidence="0.4364875">
Sanjika Hewavitharana, Dennis N. Mehay, Sankaranarayanan Ananthakrishnan
and Prem Natarajan
</author>
<affiliation confidence="0.314578">
Speech, Language and Multimedia Business Unit
Raytheon BBN Technologies
</affiliation>
<address confidence="0.82798">
Cambridge, MA 02138, USA
</address>
<email confidence="0.995278">
{shewavit,dmehay,sanantha,pnataraj}@bbn.com
</email>
<sectionHeader confidence="0.993786" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99999636">
We describe a translation model adapta-
tion approach for conversational spoken
language translation (CSLT), which en-
courages the use of contextually appropri-
ate translation options from relevant train-
ing conversations. Our approach employs
a monolingual LDA topic model to de-
rive a similarity measure between the test
conversation and the set of training con-
versations, which is used to bias trans-
lation choices towards the current con-
text. A significant novelty of our adap-
tation technique is its incremental nature;
we continuously update the topic distribu-
tion on the evolving test conversation as
new utterances become available. Thus,
our approach is well-suited to the causal
constraint of spoken conversations. On
an English-to-Iraqi CSLT task, the pro-
posed approach gives significant improve-
ments over a baseline system as measured
by BLEU, TER, and NIST. Interestingly,
the incremental approach outperforms a
non-incremental oracle that has up-front
knowledge of the whole conversation.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.978766288461539">
Conversational spoken language translation
(CSLT) systems facilitate communication be-
tween subjects who do not speak the same
language. Current systems are typically used to
achieve a specific task (e.g. vehicle checkpoint
search, medical diagnosis, etc.). These task-driven
Disclaimer: This paper is based upon work supported by the
DARPA BOLT program. The views expressed here are those
of the authors and do not reflect the official policy or position
of the Department of Defense or the U.S. Government.
Distribution Statement A (Approved for Public Release,
Distribution Unlimited)
conversations typically revolve around a set of
central topics, which may not be evident at the
beginning of the interaction. As the conversation
progresses, however, the gradual accumulation of
contextual information can be used to infer the
topic(s) of discussion, and to deploy contextually
appropriate translation phrase pairs. For example,
the word ‘drugs’ will predominantly translate
into Spanish as ‘medicamentos’ (medicines) in a
medical scenario, whereas the translation ‘drogas’
(illegal drugs) will predominate in a law enforce-
ment scenario. Most CSLT systems do not take
high-level global context into account, and instead
translate each utterance in isolation. This often
results in contextually inappropriate translations,
and is particularly problematic in conversational
speech, which usually exhibits short, spontaneous,
and often ambiguous utterances.
In this paper, we describe a novel topic-based
adaptation technique for phrase-based statistical
machine translation (SMT) of spoken conversa-
tions. We begin by building a monolingual la-
tent Dirichlet allocation (LDA) topic model on the
training conversations (each conversation corre-
sponds to a “document” in the LDA paradigm).
At run-time, this model is used to infer a topic
distribution over the evolving test conversation up
to and including the current utterance. Transla-
tion phrase pairs that originate in training conver-
sations whose topic distribution is similar to that
of the current conversation are given preference
through a single similarity feature, which aug-
ments the standard phrase-based SMT log-linear
model. The topic distribution for the test conver-
sation is updated incrementally for each new utter-
ance as the available history grows. With this ap-
proach, we demonstrate significant improvements
over a baseline phrase-based SMT system as mea-
sured by BLEU, TER and NIST scores on an
English-to-Iraqi CSLT task.
</bodyText>
<page confidence="0.972182">
697
</page>
<note confidence="0.526492">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 697–701,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.901728" genericHeader="method">
2 Relation to Prior Work
</sectionHeader>
<bodyText confidence="0.999978957446809">
Domain adaptation to improve SMT performance
has attracted considerable attention in recent years
(Foster and Kuhn, 2007; Finch and Sumita, 2008;
Matsoukas et al., 2009). The general theme is to
divide the training data into partitions representing
different domains, and to prefer translation options
for a test sentence from training domains that most
resemble the current document context. Weak-
nesses of this approach include (a) assuming the
existence of discrete, non-overlapping domains;
and (b) the unreliability of models generated by
segments with little training data.
To avoid the need for hard decisions about do-
main membership, some have used topic modeling
to improve SMT performance, e.g., using latent
semantic analysis (Tam et al., 2007) or ‘biTAM’
(Zhao and Xing, 2006). In contrast to our source
language approach, these authors use both source
and target information.
Perhaps most relevant are the approaches of
Gong et al. (2010) and Eidelman et al. (2012),
who both describe adaptation techniques where
monolingual LDA topic models are used to ob-
tain a topic distribution over the training data, fol-
lowed by dynamic adaptation of the phrase table
based on the inferred topic of the test document.
While our proposed approach also employs mono-
lingual LDA topic models, it deviates from the
above methods in the following important ways.
First, the existing approaches are geared towards
batch-mode text translation, and assume that the
full document context of a test sentence is always
available. This assumption is incompatible with
translation of spoken conversations, which are in-
herently causal. Our proposed approach infers
topic distributions incrementally as the conversa-
tion progresses. Thus, it is not only consistent
with the causal requirement, but is also capable
of tracking topical changes during the course of a
conversation.
Second, we do not directly augment the trans-
lation table with the inferred topic distribution.
Rather, we compute a similarity between the cur-
rent conversation history and each of the training
conversations, and use this measure to dynami-
cally score the relevance of candidate translation
phrase pairs during decoding.
</bodyText>
<sectionHeader confidence="0.481778" genericHeader="method">
3 Corpus Data and Baseline SMT
</sectionHeader>
<bodyText confidence="0.999974307692308">
We use the DARPA TransTac English-Iraqi par-
allel two-way spoken dialogue collection to train
both translation and LDA topic models. This data
set contains a variety of scenarios, including med-
ical diagnosis; force protection (e.g. checkpoint,
reconnaissance, patrol); aid, maintenance and in-
frastructure, etc.; each transcribed from spoken
bilingual conversations and manually translated.
The SMT parallel training corpus contains ap-
proximately 773K sentence pairs (7.3M English
words). We used this corpus to extract transla-
tion phrase pairs from bidirectional IBM Model
4 word alignment (Och and Ney, 2003) based on
the heuristic approach of (Koehn et al., 2003). A
4-gram target LM was trained on all Iraqi Ara-
bic transcriptions. Our phrase-based decoder is
similar to Moses (Koehn et al., 2007) and uses
the phrase pairs and target LM to perform beam
search stack decoding based on a standard log-
linear model, the parameters of which were tuned
with MERT (Och, 2003) on a held-out develop-
ment set (3,534 sentence pairs, 45K words) using
BLEU as the tuning metric. Finally, we evaluated
translation performance on a separate, unseen test
set (3,138 sentence pairs, 38K words).
Of the 773K training sentence pairs, about
100K (corresponding to 1,600 conversations) are
marked with conversation boundaries. We use the
English side of these conversations for training
LDA topic models. All other sentence pairs are
assigned to a “background conversation”, which
signals the absence of the topic similarity feature
for phrase pairs derived from these instances. All
of the development and test set data were marked
with conversation boundaries. The training, devel-
opment and test sets were partitioned at the con-
versation level, so that we could model a topic
distribution for entire conversations, both during
training and during tuning and testing.
</bodyText>
<sectionHeader confidence="0.995859" genericHeader="method">
4 Incremental Topic-Based Adaptation
</sectionHeader>
<bodyText confidence="0.999828666666667">
Our approach is based on the premise that biasing
the translation model to favor phrase pairs origi-
nating in training conversations that are contextu-
ally similar to the current conversation will lead
to better translation quality. The topic distribution
is incrementally updated as the conversation his-
tory grows, and we recompute the topic similarity
between the current conversation and the training
conversations for each new source utterance.
</bodyText>
<page confidence="0.99265">
698
</page>
<subsectionHeader confidence="0.890731">
4.1 Topic modeling with LDA
</subsectionHeader>
<bodyText confidence="0.999860904761905">
We use latent Dirichlet allocation, or LDA, (Blei et
al., 2003) to obtain a topic distribution over con-
versations. For each conversation di in the train-
ing collection (1,600 conversations), LDA infers a
topic distribution Bdi = p(zk|di) for all latent top-
ics zk = {1, ..., K}, where K is the number of
topics. In this work, we experiment with values
of K E {20, 30, 40}. The full conversation his-
tory is available for training the topic models and
estimating topic distributions in the training set.
At run-time, however, we construct the con-
versation history for the tuning and test sets in-
crementally, one utterance at a time, mirroring a
real-world scenario where our knowledge is lim-
ited to the utterances that have been spoken up to
that point in time. Thus, each development/test ut-
terance is associated with a different conversation
history d*, for which we infer a topic distribution
Bd* = p(zk|d*) using the trained LDA model. We
use Mallet (McCallum, 2002) for training topic
models and inferring topic distributions.
</bodyText>
<subsectionHeader confidence="0.982838">
4.2 Topic Similarity Computation
</subsectionHeader>
<bodyText confidence="0.999984125">
For each test utterance, we are able to infer the
topic distribution Bd* based on the accumulated
history of the current conversation. We use this
to compute a measure of similarity between the
evolving test conversation and each of the train-
ing conversations, for which we already have topic
distributions Bdi. Because Bdi and Bd* are proba-
bility distributions, we use the Jensen-Shannon di-
vergence (JSD) to evaluate their similarity (Man-
ning and Sch¨utze, 1999). The JSD is a smoothed
and symmetric version of Kullback-Leibler diver-
gence, which is typically used to compare two
probability distributions. We define the similar-
ity score as sim(Bdi, Bd*) = 1 − JSD(Bdi||Bd*).1
Thus, we obtain a vector of similarity scores in-
dexed by the training conversations.
</bodyText>
<subsectionHeader confidence="0.994962">
4.3 Integration with the Decoder
</subsectionHeader>
<bodyText confidence="0.999950142857143">
We provide the SMT decoder with the similar-
ity vector for each test utterance. Additionally,
the SMT phrase table tracks, for each phrase pair,
the set of parent training conversations (including
the “background conversation”) from which that
phrase pair originated. Using this information, the
decoder evaluates, for each candidate phrase pair
</bodyText>
<equation confidence="0.643423">
1JSD(θdi||θd∗) ∈ [0, 1] when defined using log,.
</equation>
<table confidence="0.995370611111111">
REFERENCE TRANSCRIPTIONS
SYSTEM BLEU↑ TER, NIST↑
Baseline 19.32 58.66 6.22
incr20 19.39 58.44 6.26*
incr30 19.36 58.32* 6.26
incr40 19.68* 58.19* 6.28*
conv20 19.60* 58.36* 6.27*
conv30 19.48 58.38* 6.27*
conv40 19.50 58.33* 6.28*
ASR TRANSCRIPTIONS
SYSTEM BLEU↑ TER, NIST↑
Baseline 16.92 62.57 5.75
incr20 16.99 62.28* 5.77
incr30 16.96 62.33* 5.78
incr40 17.31* 61.97* 5.83*
conv20 17.29* 62.28* 5.81*
conv30 17.12 62.19* 5.80*
conv40 17.00 62.14* 5.79*
</table>
<tableCaption confidence="0.998849">
Table 1: Stemmed results on 3,138-utterance test
</tableCaption>
<bodyText confidence="0.724781">
set. Asterisked results are significantly better than
the baseline (p G 0.05) using 1,000 iterations
of paired bootstrap re-sampling (Koehn, 2004).
(Key: incrN = incremental LDA with N topics;
</bodyText>
<equation confidence="0.877738666666667">
convN = non-incremental, whole-conversation
LDA with N topics.)
X → Y added to the search graph, its topic simi-
larity score as follows:
FX-+Y = max sim(Bdi,Bd*) (1)
iEPar(X-+Y)
</equation>
<bodyText confidence="0.99993">
where Par(X → Y ) is the set of training con-
versations from which the candidate phrase pair
originated. Phrase pairs from the “background
conversation” only are assigned a similarity score
FX-+Y = 0.00. In this way we distill the in-
ferred topic distributions down to a single feature
for each candidate phrase pair. We add this fea-
ture to the log-linear translation model with its
own weight, which is tuned with MERT. The in-
tuition behind this feature is that the lower bound
of suitability of a candidate phrase pair should be
directly proportional to the similarity between its
most relevant conversational provenance and the
current context. Phrase pairs which only occur in
the background conversation are not directly pe-
nalized, but contribute nothing to the topic simi-
larity score.
</bodyText>
<page confidence="0.998654">
699
</page>
<figureCaption confidence="0.9988135">
Figure 1: Rank trajectories of 4 LDA inferred topics, with incremental topic inference. The x-axis
indicates the utterance number. The y-axis indicates a topic’s rank at each utterance.
</figureCaption>
<sectionHeader confidence="0.987627" genericHeader="evaluation">
5 Experimental Setup and Results
</sectionHeader>
<bodyText confidence="0.999602">
The baseline English-to-Iraqi phrase-based SMT
system was built as described in Section 3. This
system translated each utterance independently,
ignoring higher-level conversational context.
For the topic-adapted system, we compared
translation performance with a varying number of
LDA topics. In intuitive agreement with the ap-
proximate number of scenario types known to be
covered by our data set, a range of 20-40 topics
yielded the best results. We compared the pro-
posed incremental topic tracking approach to a
non-causal oracle approach that had up-front ac-
cess to the entire source conversations at run-time.
In all cases, we compared translation perfor-
mance on both clean-text and automatic speech
recognition (ASR) transcriptions of the source ut-
terances. ASR transcriptions were generated using
a high-performance two-pass HMM-based sys-
tem, which delivered a word error rate (WER) of
10.6% on the test set utterances.
Table 1 summarizes test set performance in
BLEU (Papineni et al., 2001), NIST (Doddington,
2002) and TER (Snover et al., 2006). Given the
morphological complexity of Iraqi Arabic, com-
puting string-based metrics on raw output can
be misleadingly low and does not always reflect
whether the core message was conveyed. Since
the primary goal of CSLT is information transfer,
we present automatic results that are computed af-
ter stemming with an Iraqi Arabic stemmer.
We note that in all settings (incremental
and non-causal oracle) our adaptation approach
matches or significantly outperforms the baseline
across multiple evaluation metrics. In particular,
the incremental LDA system with 40 topics is the
top-scoring system in both clean-text and ASR set-
tings. In the ASR setting, which simulates a real-
world deployment scenario, this system achieves
improvements of 0.39 (BLEU), -0.6 (TER) and
0.08 (NIST).
</bodyText>
<sectionHeader confidence="0.99331" genericHeader="conclusions">
6 Discussion and Future Directions
</sectionHeader>
<bodyText confidence="0.999914434782609">
We have presented a novel, incremental topic-
based translation model adaptation approach that
obeys the causality constraint imposed by spoken
conversations. This approach yields statistically
significant gains in standard MT metric scores.
We have also demonstrated that incremental
adaptation on an evolving conversation performs
better than oracle adaptation based on the com-
plete conversation history. Although this may
seem counter-intuitive, Figure 1 gives clues as to
why this happens. This figure illustrates the rank
trajectory of four LDA topics as the incremen-
tal conversation grows. The accompanying text
shows excerpts from the conversation. We indi-
cate (in superscript) the topic identity of most rele-
vant words in an utterance that are associated with
that topic. At the first utterance, the top-ranked
topic is “5”, due to the occurrence of “captain”
in the greeting. As the conversation evolves, we
note that this topic become less prominent. The
conversation shifts to a discussion on “windows”,
raising the prominence of topic “4”. Finally, topic
“3” becomes prominent due to the presence of the
</bodyText>
<page confidence="0.979271">
700
</page>
<bodyText confidence="0.999929076923077">
words “project” and “contract”. Thus, the incre-
mental approach is able to track the topic trajecto-
ries in the conversation, and is able to select more
relevant phrase pairs than oracle LDA, which esti-
mates one topic distribution for the entire conver-
sation.
In this work we have used only the source lan-
guage utterance in inferring the topic distribution.
In a two-way CLST system, we also have access
to SMT-generated back-translations in the Iraqi-
English direction. As a next step, we plan to use
SMT-generated English translation of Iraqi utter-
ances to improve topic estimation.
</bodyText>
<sectionHeader confidence="0.998939" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999809372340426">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022, March.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, HLT ’02, pages 138–145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic transla-
tion model adaptation. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Short Papers - Volume 2, ACL
’12, pages 115–119, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Andrew Finch and Eiichiro Sumita. 2008. Dynamic
model interpolation for statistical machine transla-
tion. In Proceedings of the Third Workshop on
Statistical Machine Translation, StatMT ’08, pages
208–215, Stroudsburg, PA, USA. Association for
Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, StatMT ’07, pages 128–135, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zhengxian Gong, Yu Zhang, and Guodong Zhou.
2010. Statistical machine translation based on LDA.
In Universal Communication Symposium (IUCS),
2010 4th International, pages 286–290.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48–54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388–395, Barcelona, Spain, July.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume
2, EMNLP ’09, pages 708–717, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51,
March.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ’03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160–
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. In ACL ’02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311–318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings AMTA, pages 223–231, August.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical ma-
chine translation. Machine Translation, 21(4):187–
207, December.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In In
Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL ‘06).
</reference>
<page confidence="0.997981">
701
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.578441">
<title confidence="0.998676">Incremental Topic-Based Translation Model Adaptation Conversational Spoken Language Translation</title>
<author confidence="0.9713235">Sanjika Hewavitharana</author>
<author confidence="0.9713235">Dennis N Mehay</author>
<author confidence="0.9713235">Sankaranarayanan</author>
<author confidence="0.9713235">Prem Natarajan</author>
<affiliation confidence="0.775907">Speech, Language and Multimedia Business Raytheon BBN</affiliation>
<address confidence="0.99981">Cambridge, MA 02138, USA</address>
<abstract confidence="0.999728192307692">We describe a translation model adaptation approach for conversational spoken language translation (CSLT), which encourages the use of contextually appropriate translation options from relevant training conversations. Our approach employs a monolingual LDA topic model to derive a similarity measure between the test conversation and the set of training conversations, which is used to bias translation choices towards the current context. A significant novelty of our adaptechnique is its we continuously update the topic distribution on the evolving test conversation as new utterances become available. Thus, our approach is well-suited to the causal constraint of spoken conversations. On an English-to-Iraqi CSLT task, the proposed approach gives significant improvements over a baseline system as measured and NIST. Interestingly, the incremental approach outperforms a non-incremental oracle that has up-front knowledge of the whole conversation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="8700" citStr="Blei et al., 2003" startWordPosition="1293" endWordPosition="1296">raining and during tuning and testing. 4 Incremental Topic-Based Adaptation Our approach is based on the premise that biasing the translation model to favor phrase pairs originating in training conversations that are contextually similar to the current conversation will lead to better translation quality. The topic distribution is incrementally updated as the conversation history grows, and we recompute the topic similarity between the current conversation and the training conversations for each new source utterance. 698 4.1 Topic modeling with LDA We use latent Dirichlet allocation, or LDA, (Blei et al., 2003) to obtain a topic distribution over conversations. For each conversation di in the training collection (1,600 conversations), LDA infers a topic distribution Bdi = p(zk|di) for all latent topics zk = {1, ..., K}, where K is the number of topics. In this work, we experiment with values of K E {20, 30, 40}. The full conversation history is available for training the topic models and estimating topic distributions in the training set. At run-time, however, we construct the conversation history for the tuning and test sets incrementally, one utterance at a time, mirroring a real-world scenario wh</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research, HLT ’02,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="13797" citStr="Doddington, 2002" startWordPosition="2101" endWordPosition="2102"> of 20-40 topics yielded the best results. We compared the proposed incremental topic tracking approach to a non-causal oracle approach that had up-front access to the entire source conversations at run-time. In all cases, we compared translation performance on both clean-text and automatic speech recognition (ASR) transcriptions of the source utterances. ASR transcriptions were generated using a high-performance two-pass HMM-based system, which delivered a word error rate (WER) of 10.6% on the test set utterances. Table 1 summarizes test set performance in BLEU (Papineni et al., 2001), NIST (Doddington, 2002) and TER (Snover et al., 2006). Given the morphological complexity of Iraqi Arabic, computing string-based metrics on raw output can be misleadingly low and does not always reflect whether the core message was conveyed. Since the primary goal of CSLT is information transfer, we present automatic results that are computed after stemming with an Iraqi Arabic stemmer. We note that in all settings (incremental and non-causal oracle) our adaptation approach matches or significantly outperforms the baseline across multiple evaluation metrics. In particular, the incremental LDA system with 40 topics </context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, HLT ’02, pages 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Topic models for dynamic translation model adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12,</booktitle>
<pages>115--119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5035" citStr="Eidelman et al. (2012)" startWordPosition="727" endWordPosition="730">esemble the current document context. Weaknesses of this approach include (a) assuming the existence of discrete, non-overlapping domains; and (b) the unreliability of models generated by segments with little training data. To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or ‘biTAM’ (Zhao and Xing, 2006). In contrast to our source language approach, these authors use both source and target information. Perhaps most relevant are the approaches of Gong et al. (2010) and Eidelman et al. (2012), who both describe adaptation techniques where monolingual LDA topic models are used to obtain a topic distribution over the training data, followed by dynamic adaptation of the phrase table based on the inferred topic of the test document. While our proposed approach also employs monolingual LDA topic models, it deviates from the above methods in the following important ways. First, the existing approaches are geared towards batch-mode text translation, and assume that the full document context of a test sentence is always available. This assumption is incompatible with translation of spoken</context>
</contexts>
<marker>Eidelman, Boyd-Graber, Resnik, 2012</marker>
<rawString>Vladimir Eidelman, Jordan Boyd-Graber, and Philip Resnik. 2012. Topic models for dynamic translation model adaptation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12, pages 115–119, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Dynamic model interpolation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation, StatMT ’08,</booktitle>
<pages>208--215</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4202" citStr="Finch and Sumita, 2008" startWordPosition="597" endWordPosition="600">sation is updated incrementally for each new utterance as the available history grows. With this approach, we demonstrate significant improvements over a baseline phrase-based SMT system as measured by BLEU, TER and NIST scores on an English-to-Iraqi CSLT task. 697 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 697–701, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Relation to Prior Work Domain adaptation to improve SMT performance has attracted considerable attention in recent years (Foster and Kuhn, 2007; Finch and Sumita, 2008; Matsoukas et al., 2009). The general theme is to divide the training data into partitions representing different domains, and to prefer translation options for a test sentence from training domains that most resemble the current document context. Weaknesses of this approach include (a) assuming the existence of discrete, non-overlapping domains; and (b) the unreliability of models generated by segments with little training data. To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et </context>
</contexts>
<marker>Finch, Sumita, 2008</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2008. Dynamic model interpolation for statistical machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, StatMT ’08, pages 208–215, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4178" citStr="Foster and Kuhn, 2007" startWordPosition="593" endWordPosition="596">ion for the test conversation is updated incrementally for each new utterance as the available history grows. With this approach, we demonstrate significant improvements over a baseline phrase-based SMT system as measured by BLEU, TER and NIST scores on an English-to-Iraqi CSLT task. 697 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 697–701, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Relation to Prior Work Domain adaptation to improve SMT performance has attracted considerable attention in recent years (Foster and Kuhn, 2007; Finch and Sumita, 2008; Matsoukas et al., 2009). The general theme is to divide the training data into partitions representing different domains, and to prefer translation options for a test sentence from training domains that most resemble the current document context. Weaknesses of this approach include (a) assuming the existence of discrete, non-overlapping domains; and (b) the unreliability of models generated by segments with little training data. To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent se</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for SMT. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 128–135, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengxian Gong</author>
<author>Yu Zhang</author>
<author>Guodong Zhou</author>
</authors>
<title>Statistical machine translation based on LDA.</title>
<date>2010</date>
<booktitle>In Universal Communication Symposium (IUCS), 2010 4th International,</booktitle>
<pages>286--290</pages>
<contexts>
<context position="5008" citStr="Gong et al. (2010)" startWordPosition="722" endWordPosition="725">ing domains that most resemble the current document context. Weaknesses of this approach include (a) assuming the existence of discrete, non-overlapping domains; and (b) the unreliability of models generated by segments with little training data. To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or ‘biTAM’ (Zhao and Xing, 2006). In contrast to our source language approach, these authors use both source and target information. Perhaps most relevant are the approaches of Gong et al. (2010) and Eidelman et al. (2012), who both describe adaptation techniques where monolingual LDA topic models are used to obtain a topic distribution over the training data, followed by dynamic adaptation of the phrase table based on the inferred topic of the test document. While our proposed approach also employs monolingual LDA topic models, it deviates from the above methods in the following important ways. First, the existing approaches are geared towards batch-mode text translation, and assume that the full document context of a test sentence is always available. This assumption is incompatible</context>
</contexts>
<marker>Gong, Zhang, Zhou, 2010</marker>
<rawString>Zhengxian Gong, Yu Zhang, and Guodong Zhou. 2010. Statistical machine translation based on LDA. In Universal Communication Symposium (IUCS), 2010 4th International, pages 286–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6935" citStr="Koehn et al., 2003" startWordPosition="1016" endWordPosition="1019">lel two-way spoken dialogue collection to train both translation and LDA topic models. This data set contains a variety of scenarios, including medical diagnosis; force protection (e.g. checkpoint, reconnaissance, patrol); aid, maintenance and infrastructure, etc.; each transcribed from spoken bilingual conversations and manually translated. The SMT parallel training corpus contains approximately 773K sentence pairs (7.3M English words). We used this corpus to extract translation phrase pairs from bidirectional IBM Model 4 word alignment (Och and Ney, 2003) based on the heuristic approach of (Koehn et al., 2003). A 4-gram target LM was trained on all Iraqi Arabic transcriptions. Our phrase-based decoder is similar to Moses (Koehn et al., 2007) and uses the phrase pairs and target LM to perform beam search stack decoding based on a standard loglinear model, the parameters of which were tuned with MERT (Och, 2003) on a held-out development set (3,534 sentence pairs, 45K words) using BLEU as the tuning metric. Finally, we evaluated translation performance on a separate, unseen test set (3,138 sentence pairs, 38K words). Of the 773K training sentence pairs, about 100K (corresponding to 1,600 conversation</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7069" citStr="Koehn et al., 2007" startWordPosition="1039" endWordPosition="1042">including medical diagnosis; force protection (e.g. checkpoint, reconnaissance, patrol); aid, maintenance and infrastructure, etc.; each transcribed from spoken bilingual conversations and manually translated. The SMT parallel training corpus contains approximately 773K sentence pairs (7.3M English words). We used this corpus to extract translation phrase pairs from bidirectional IBM Model 4 word alignment (Och and Ney, 2003) based on the heuristic approach of (Koehn et al., 2003). A 4-gram target LM was trained on all Iraqi Arabic transcriptions. Our phrase-based decoder is similar to Moses (Koehn et al., 2007) and uses the phrase pairs and target LM to perform beam search stack decoding based on a standard loglinear model, the parameters of which were tuned with MERT (Och, 2003) on a held-out development set (3,534 sentence pairs, 45K words) using BLEU as the tuning metric. Finally, we evaluated translation performance on a separate, unseen test set (3,138 sentence pairs, 38K words). Of the 773K training sentence pairs, about 100K (corresponding to 1,600 conversations) are marked with conversation boundaries. We use the English side of these conversations for training LDA topic models. All other se</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="11538" citStr="Koehn, 2004" startWordPosition="1745" endWordPosition="1746">ER, NIST↑ Baseline 19.32 58.66 6.22 incr20 19.39 58.44 6.26* incr30 19.36 58.32* 6.26 incr40 19.68* 58.19* 6.28* conv20 19.60* 58.36* 6.27* conv30 19.48 58.38* 6.27* conv40 19.50 58.33* 6.28* ASR TRANSCRIPTIONS SYSTEM BLEU↑ TER, NIST↑ Baseline 16.92 62.57 5.75 incr20 16.99 62.28* 5.77 incr30 16.96 62.33* 5.78 incr40 17.31* 61.97* 5.83* conv20 17.29* 62.28* 5.81* conv30 17.12 62.19* 5.80* conv40 17.00 62.14* 5.79* Table 1: Stemmed results on 3,138-utterance test set. Asterisked results are significantly better than the baseline (p G 0.05) using 1,000 iterations of paired bootstrap re-sampling (Koehn, 2004). (Key: incrN = incremental LDA with N topics; convN = non-incremental, whole-conversation LDA with N topics.) X → Y added to the search graph, its topic similarity score as follows: FX-+Y = max sim(Bdi,Bd*) (1) iEPar(X-+Y) where Par(X → Y ) is the set of training conversations from which the candidate phrase pair originated. Phrase pairs from the “background conversation” only are assigned a similarity score FX-+Y = 0.00. In this way we distill the inferred topic distributions down to a single feature for each candidate phrase pair. We add this feature to the log-linear translation model with</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In EMNLP, pages 388–395, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of statistical natural language processing. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09,</booktitle>
<pages>708--717</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4227" citStr="Matsoukas et al., 2009" startWordPosition="601" endWordPosition="604">entally for each new utterance as the available history grows. With this approach, we demonstrate significant improvements over a baseline phrase-based SMT system as measured by BLEU, TER and NIST scores on an English-to-Iraqi CSLT task. 697 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 697–701, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Relation to Prior Work Domain adaptation to improve SMT performance has attracted considerable attention in recent years (Foster and Kuhn, 2007; Finch and Sumita, 2008; Matsoukas et al., 2009). The general theme is to divide the training data into partitions representing different domains, and to prefer translation options for a test sentence from training domains that most resemble the current document context. Weaknesses of this approach include (a) assuming the existence of discrete, non-overlapping domains; and (b) the unreliability of models generated by segments with little training data. To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or ‘biTAM’ (Zh</context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 708–717, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="9603" citStr="McCallum, 2002" startWordPosition="1450" endWordPosition="1451">f K E {20, 30, 40}. The full conversation history is available for training the topic models and estimating topic distributions in the training set. At run-time, however, we construct the conversation history for the tuning and test sets incrementally, one utterance at a time, mirroring a real-world scenario where our knowledge is limited to the utterances that have been spoken up to that point in time. Thus, each development/test utterance is associated with a different conversation history d*, for which we infer a topic distribution Bd* = p(zk|d*) using the trained LDA model. We use Mallet (McCallum, 2002) for training topic models and inferring topic distributions. 4.2 Topic Similarity Computation For each test utterance, we are able to infer the topic distribution Bd* based on the accumulated history of the current conversation. We use this to compute a measure of similarity between the evolving test conversation and each of the training conversations, for which we already have topic distributions Bdi. Because Bdi and Bd* are probability distributions, we use the Jensen-Shannon divergence (JSD) to evaluate their similarity (Manning and Sch¨utze, 1999). The JSD is a smoothed and symmetric vers</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="6879" citStr="Och and Ney, 2003" startWordPosition="1006" endWordPosition="1009">eline SMT We use the DARPA TransTac English-Iraqi parallel two-way spoken dialogue collection to train both translation and LDA topic models. This data set contains a variety of scenarios, including medical diagnosis; force protection (e.g. checkpoint, reconnaissance, patrol); aid, maintenance and infrastructure, etc.; each transcribed from spoken bilingual conversations and manually translated. The SMT parallel training corpus contains approximately 773K sentence pairs (7.3M English words). We used this corpus to extract translation phrase pairs from bidirectional IBM Model 4 word alignment (Och and Ney, 2003) based on the heuristic approach of (Koehn et al., 2003). A 4-gram target LM was trained on all Iraqi Arabic transcriptions. Our phrase-based decoder is similar to Moses (Koehn et al., 2007) and uses the phrase pairs and target LM to perform beam search stack decoding based on a standard loglinear model, the parameters of which were tuned with MERT (Och, 2003) on a held-out development set (3,534 sentence pairs, 45K words) using BLEU as the tuning metric. Finally, we evaluated translation performance on a separate, unseen test set (3,138 sentence pairs, 38K words). Of the 773K training sentenc</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7241" citStr="Och, 2003" startWordPosition="1072" endWordPosition="1073">ons and manually translated. The SMT parallel training corpus contains approximately 773K sentence pairs (7.3M English words). We used this corpus to extract translation phrase pairs from bidirectional IBM Model 4 word alignment (Och and Ney, 2003) based on the heuristic approach of (Koehn et al., 2003). A 4-gram target LM was trained on all Iraqi Arabic transcriptions. Our phrase-based decoder is similar to Moses (Koehn et al., 2007) and uses the phrase pairs and target LM to perform beam search stack decoding based on a standard loglinear model, the parameters of which were tuned with MERT (Och, 2003) on a held-out development set (3,534 sentence pairs, 45K words) using BLEU as the tuning metric. Finally, we evaluated translation performance on a separate, unseen test set (3,138 sentence pairs, 38K words). Of the 773K training sentence pairs, about 100K (corresponding to 1,600 conversations) are marked with conversation boundaries. We use the English side of these conversations for training LDA topic models. All other sentence pairs are assigned to a “background conversation”, which signals the absence of the topic similarity feature for phrase pairs derived from these instances. All of th</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160– 167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="13772" citStr="Papineni et al., 2001" startWordPosition="2096" endWordPosition="2099">vered by our data set, a range of 20-40 topics yielded the best results. We compared the proposed incremental topic tracking approach to a non-causal oracle approach that had up-front access to the entire source conversations at run-time. In all cases, we compared translation performance on both clean-text and automatic speech recognition (ASR) transcriptions of the source utterances. ASR transcriptions were generated using a high-performance two-pass HMM-based system, which delivered a word error rate (WER) of 10.6% on the test set utterances. Table 1 summarizes test set performance in BLEU (Papineni et al., 2001), NIST (Doddington, 2002) and TER (Snover et al., 2006). Given the morphological complexity of Iraqi Arabic, computing string-based metrics on raw output can be misleadingly low and does not always reflect whether the core message was conveyed. Since the primary goal of CSLT is information transfer, we present automatic results that are computed after stemming with an Iraqi Arabic stemmer. We note that in all settings (incremental and non-causal oracle) our adaptation approach matches or significantly outperforms the baseline across multiple evaluation metrics. In particular, the incremental L</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. BLEU: A method for automatic evaluation of machine translation. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings AMTA,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="13827" citStr="Snover et al., 2006" startWordPosition="2105" endWordPosition="2108">e best results. We compared the proposed incremental topic tracking approach to a non-causal oracle approach that had up-front access to the entire source conversations at run-time. In all cases, we compared translation performance on both clean-text and automatic speech recognition (ASR) transcriptions of the source utterances. ASR transcriptions were generated using a high-performance two-pass HMM-based system, which delivered a word error rate (WER) of 10.6% on the test set utterances. Table 1 summarizes test set performance in BLEU (Papineni et al., 2001), NIST (Doddington, 2002) and TER (Snover et al., 2006). Given the morphological complexity of Iraqi Arabic, computing string-based metrics on raw output can be misleadingly low and does not always reflect whether the core message was conveyed. Since the primary goal of CSLT is information transfer, we present automatic results that are computed after stemming with an Iraqi Arabic stemmer. We note that in all settings (incremental and non-causal oracle) our adaptation approach matches or significantly outperforms the baseline across multiple evaluation metrics. In particular, the incremental LDA system with 40 topics is the top-scoring system in b</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings AMTA, pages 223–231, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Ian Lane</author>
<author>Tanja Schultz</author>
</authors>
<title>Bilingual LSA-based adaptation for statistical machine translation.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>4</issue>
<pages>207</pages>
<contexts>
<context position="4812" citStr="Tam et al., 2007" startWordPosition="690" endWordPosition="693">a, 2008; Matsoukas et al., 2009). The general theme is to divide the training data into partitions representing different domains, and to prefer translation options for a test sentence from training domains that most resemble the current document context. Weaknesses of this approach include (a) assuming the existence of discrete, non-overlapping domains; and (b) the unreliability of models generated by segments with little training data. To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or ‘biTAM’ (Zhao and Xing, 2006). In contrast to our source language approach, these authors use both source and target information. Perhaps most relevant are the approaches of Gong et al. (2010) and Eidelman et al. (2012), who both describe adaptation techniques where monolingual LDA topic models are used to obtain a topic distribution over the training data, followed by dynamic adaptation of the phrase table based on the inferred topic of the test document. While our proposed approach also employs monolingual LDA topic models, it deviates from the above methods in the following important wa</context>
</contexts>
<marker>Tam, Lane, Schultz, 2007</marker>
<rawString>Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007. Bilingual LSA-based adaptation for statistical machine translation. Machine Translation, 21(4):187– 207, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>BiTAM: Bilingual topic admixture models for word alignment. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL ‘06).</booktitle>
<contexts>
<context position="4845" citStr="Zhao and Xing, 2006" startWordPosition="696" endWordPosition="699">9). The general theme is to divide the training data into partitions representing different domains, and to prefer translation options for a test sentence from training domains that most resemble the current document context. Weaknesses of this approach include (a) assuming the existence of discrete, non-overlapping domains; and (b) the unreliability of models generated by segments with little training data. To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or ‘biTAM’ (Zhao and Xing, 2006). In contrast to our source language approach, these authors use both source and target information. Perhaps most relevant are the approaches of Gong et al. (2010) and Eidelman et al. (2012), who both describe adaptation techniques where monolingual LDA topic models are used to obtain a topic distribution over the training data, followed by dynamic adaptation of the phrase table based on the inferred topic of the test document. While our proposed approach also employs monolingual LDA topic models, it deviates from the above methods in the following important ways. First, the existing approache</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual topic admixture models for word alignment. In In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL ‘06).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>