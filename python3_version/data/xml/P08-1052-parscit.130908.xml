<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000736">
<title confidence="0.974033">
Solving Relational Similarity Problems Using the Web as a Corpus
</title>
<author confidence="0.895012">
Preslav Nakov*
</author>
<affiliation confidence="0.702102">
EECS, CS division
University of California at Berkeley
</affiliation>
<address confidence="0.647612">
Berkeley, CA 94720, USA
</address>
<email confidence="0.998359">
nakov@cs.berkeley.edu
</email>
<author confidence="0.956624">
Marti A. Hearst
</author>
<affiliation confidence="0.995062">
School of Information
University of California at Berkeley
</affiliation>
<address confidence="0.646652">
Berkeley, CA 94720, USA
</address>
<email confidence="0.998603">
hearst@ischool.berkeley.edu
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999864869565217">
We present a simple linguistically-motivated
method for characterizing the semantic rela-
tions that hold between two nouns. The ap-
proach leverages the vast size of the Web
in order to build lexically-specific features.
The main idea is to look for verbs, preposi-
tions, and coordinating conjunctions that can
help make explicit the hidden relations be-
tween the target nouns. Using these fea-
tures in instance-based classifiers, we demon-
strate state-of-the-art results on various rela-
tional similarity problems, including mapping
noun-modifier pairs to abstract relations like
TIME, LOCATION and CONTAINER, charac-
terizing noun-noun compounds in terms of ab-
stract linguistic predicates like CAUSE, USE,
and FROM, classifying the relations between
nominals in context, and solving SAT verbal
analogy problems. In essence, the approach
puts together some existing ideas, showing
that they apply generally to various semantic
tasks, finding that verbs are especially useful
features.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.958024744186046">
Despite the tremendous amount of work on word
similarity (see (Budanitsky and Hirst, 2006) for an
overview), there is surprisingly little research on the
important related problem of relational similarity –
semantic similarity between pairs of words. Stu-
dents who took the SAT test before 2005 or who
*After January 2008 at the Linguistic Modeling Depart-
ment, Institute for Parallel Processing, Bulgarian Academy of
Sciences, nakov@lml.bas.bg
are taking the GRE test nowadays are familiar with
an instance of this problem – verbal analogy ques-
tions, which ask whether, e.g., the relationship be-
tween ostrich and bird is more similar to that be-
tween lion and cat, or rather between primate and
monkey. These analogies are difficult, and the aver-
age test taker gives a correct answer 57% of the time
(Turney and Littman, 2005).
Many NLP applications could benefit from solv-
ing relational similarity problems, including but
not limited to question answering, information re-
trieval, machine translation, word sense disambigua-
tion, and information extraction. For example, a
relational search engine like TextRunner, which
serves queries like “find all X such that X causes
wrinkles”, asking for all entities that are in a par-
ticular relation with a given entity (Cafarella et al.,
2006), needs to recognize that laugh wrinkles is
an instance of CAUSE-EFFECT. While there are
not many success stories so far, measuring seman-
tic similarity has proven its advantages for textual
entailment (Tatu and Moldovan, 2005).
In this paper, we introduce a novel linguistically-
motivated Web-based approach to relational simi-
larity, which, despite its simplicity, achieves state-
of-the-art performance on a number of problems.
Following Turney (2006b), we test our approach
on SAT verbal analogy questions and on mapping
noun-modifier pairs to abstract relations like TIME,
LOCATION and CONTAINER. We further apply it
to (1) characterizing noun-noun compounds using
abstract linguistic predicates like CAUSE, USE, and
FROM, and (2) classifying the relation between pairs
of nominals in context.
</bodyText>
<page confidence="0.97887">
452
</page>
<note confidence="0.703381">
Proceedings of ACL-08: HLT, pages 452–460,
</note>
<page confidence="0.416846">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<sectionHeader confidence="0.999036" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.998909">
2.1 Characterizing Semantic Relations
</subsectionHeader>
<bodyText confidence="0.9999833">
Turney and Littman (2005) characterize the relation-
ship between two words as a vector with coordinates
corresponding to the Web frequencies of 128 fixed
phrases like “X for Y ” and “Y for X” instantiated
from a fixed set of 64 joining terms like for, such
as, not the, is *, etc. These vectors are used in a
nearest-neighbor classifier to solve SAT verbal anal-
ogy problems, yielding 47% accuracy. The same ap-
proach is applied to classifying noun-modifier pairs:
using the Diverse dataset of Nastase and Szpakow-
icz (2003), Turney&amp;Littman achieve F-measures of
26.5% with 30 fine-grained relations, and 43.2%
with 5 course-grained relations.
Turney (2005) extends the above approach by in-
troducing the latent relational analysis (LRA), which
uses automatically generated synonyms, learns suit-
able patterns, and performs singular value decom-
position in order to smooth the frequencies. The full
algorithm consists of 12 steps described in detail in
(Turney, 2006b). When applied to SAT questions,
it achieves the state-of-the-art accuracy of 56%. On
the Diverse dataset, it yields an F-measure of 39.8%
with 30 classes, and 58% with 5 classes.
Turney (2006a) presents an unsupervised algo-
rithm for mining the Web for patterns expressing
implicit semantic relations. For example, CAUSE
(e.g., cold virus) is best characterized by “Y * causes
X”, and “Y in * early X” is the best pattern for
TEMPORAL (e.g., morning frost). With 5 classes,
he achieves F-measure=50.2%.
</bodyText>
<subsectionHeader confidence="0.999564">
2.2 Noun-Noun Compound Semantics
</subsectionHeader>
<bodyText confidence="0.99996745">
Lauer (1995) reduces the problem of noun com-
pound interpretation to choosing the best paraphras-
ing preposition from the following set: of, for, in,
at, on, from, with or about. He achieved 40% accu-
racy using corpus frequencies. This result was im-
proved to 55.7% by Lapata and Keller (2005) who
used Web-derived n-gram frequencies.
Barker and Szpakowicz (1998) use syntactic clues
and the identity of the nouns in a nearest-neighbor
classifier, achieving 60-70% accuracy.
Rosario and Hearst (2001) used a discriminative
classifier to assign 18 relations for noun compounds
from biomedical text, achieving 60% accuracy.
Rosario et al. (2002) reported 90% accuracy with
a “descent of hierarchy” approach which character-
izes the relationship between the nouns in a bio-
science noun-noun compound based on the MeSH
categories the nouns belong to.
Girju et al. (2005) apply both classic (SVM and
decision trees) and novel supervised models (seman-
tic scattering and iterative semantic specialization),
using WordNet, word sense disambiguation, and a
set of linguistic features. They test their system
against both Lauer’s 8 prepositional paraphrases and
another set of 21 semantic relations, achieving up to
54% accuracy on the latter.
In a previous work (Nakov and Hearst, 2006), we
have shown that the relationship between the nouns
in a noun-noun compound can be characterized us-
ing verbs extracted from the Web, but we provided
no formal evaluation.
Kim and Baldwin (2006) characterized the se-
mantic relationship in a noun-noun compound us-
ing the verbs connecting the two nouns by compar-
ing them to predefined seed verbs. Their approach
is highly resource intensive (uses WordNet, CoreLex
and Moby’s thesaurus), and is quite sensitive to the
seed set of verbs: on a collection of 453 examples
and 19 relations, they achieved 52.6% accuracy with
84 seed verbs, but only 46.7% with 57 seed verbs.
</bodyText>
<subsectionHeader confidence="0.999042">
2.3 Paraphrase Acquisition
</subsectionHeader>
<bodyText confidence="0.998787764705882">
Our method of extraction of paraphrasing verbs and
prepositions is similar to previous paraphrase ac-
quisition approaches. Lin and Pantel (2001) ex-
tract paraphrases from dependency tree paths whose
ends contain semantically similar sets of words by
generalizing over these ends. For example, given
“X solves Y”, they extract paraphrases like “X finds
a solution to Y”, “X tries to solve Y”, “X resolves
Y”, “Y is resolved by X”, etc. The approach is ex-
tended by Shinyama et al. (2002), who use named
entity recognizers and look for anchors belong-
ing to matching semantic classes, e.g., LOCATION,
ORGANIZATION. The idea is further extended by
Nakov et al. (2004), who apply it in the biomedical
domain, imposing the additional restriction that the
sentences from which the paraphrases are extracted
cite the same target paper.
</bodyText>
<page confidence="0.998739">
453
</page>
<subsectionHeader confidence="0.996883">
2.4 Word Similarity
</subsectionHeader>
<bodyText confidence="0.999959285714286">
Another important group of related work is on us-
ing syntactic dependency features in a vector-space
model for measuring word similarity, e.g., (Alshawi
and Carter, 1994), (Grishman and Sterling, 1994),
(Ruge, 1992), and (Lin, 1998). For example, given a
noun, Lin (1998) extracts verbs that have that noun
as a subject or object, and adjectives that modify it.
</bodyText>
<sectionHeader confidence="0.978717" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999808181818182">
Given a pair of nouns, we try to characterize the
semantic relation between them by leveraging the
vast size of the Web to build linguistically-motivated
lexically-specific features. We mine the Web for
sentences containing the target nouns, and we ex-
tract the connecting verbs, prepositions, and coordi-
nating conjunctions, which we use in a vector-space
model to measure relational similarity.
The process of extraction starts with exact phrase
queries issued against a Web search engine (Google)
using the following patterns:
</bodyText>
<equation confidence="0.9979535">
“infl1 THAT * infl2”
“infl2 THAT * infl1”
“infl1 * infl2”
“infl2 * infl1”
</equation>
<bodyText confidence="0.99960365">
where: infl1 and infl2 are inflected variants of
noun1 and noun2 generated using the Java Word-
Net Library1; THAT is a complementizer and can be
that, which, or who; and * stands for 0 or more (up
to 8) instances of Google’s star operator.
The first two patterns are subsumed by the last
two and are used to obtain more sentences from the
search engine since including e.g. that in the query
changes the set of returned results and their ranking.
For each query, we collect the text snippets from
the result set (up to 1,000 per query). We split them
into sentences, and we filter out all incomplete ones
and those that do not contain the target nouns. We
further make sure that the word sequence follow-
ing the second mentioned target noun is nonempty
and contains at least one nonnoun, thus ensuring
the snippet includes the entire noun phrase: snippets
representing incomplete sentences often end with a
period anyway. We then perform POS tagging us-
ing the Stanford POS tagger (Toutanova et al., 2003)
</bodyText>
<footnote confidence="0.955049">
1JWNL: http://jwordnet.sourceforge.net
</footnote>
<table confidence="0.999487666666666">
Freq. Feature POS Direction
2205 of P 2 → 1
1923 be V 1 → 2
771 include V 1 → 2
382 serve on V 2 → 1
189 chair V 2 → 1
189 have V 1 → 2
169 consist of V 1 → 2
148 comprise V 1 → 2
106 sit on V 2 → 1
81 be chaired by V 1 → 2
78 appoint V 1 → 2
77 on P 2 → 1
66 and C 1 → 2
66 be elected V 1 → 2
58 replace V 1 → 2
48 lead V 2 → 1
47 be intended for V 1 → 2
45 join V 2 → 1
� � � � � � � � � � � �
4 be signed up for V 2 → 1
</table>
<tableCaption confidence="0.999816">
Table 1: The most frequent Web-derived features for
</tableCaption>
<bodyText confidence="0.9983222">
committee member. Here V stands for verb (possibly
+preposition and/or +particle), P for preposition and C
for coordinating conjunction; 1 → 2 means committee
precedes the feature and member follows it; 2 → 1 means
member precedes the feature and committee follows it.
and shallow parsing with the OpenNLP tools2, and
we extract the following types of features:
Verb: We extract a verb if the subject NP of that
verb is headed by one of the target nouns (or an in-
flected form), and its direct object NP is headed by
the other target noun (or an inflected form). For ex-
ample, the verb include will be extracted from “The
committee includes many members.” We also ex-
tract verbs from relative clauses, e.g., “This is a com-
mittee which includes many members.” Verb parti-
cles are also recognized, e.g., “The committee must
rotate off 1/3 of its members.” We ignore modals
and auxiliaries, but retain the passive be. Finally, we
lemmatize the main verb using WordNet’s morpho-
logical analyzer Morphy (Fellbaum, 1998).
Verb+Preposition: If the subject NP of a verb is
headed by one of the target nouns (or an inflected
form), and its indirect object is a PP containing an
NP which is headed by the other target noun (or an
inflected form), we extract the verb and the preposi-
</bodyText>
<footnote confidence="0.575884">
2OpenNLP:http://opennlp.sourceforge.net
</footnote>
<page confidence="0.997447">
454
</page>
<bodyText confidence="0.999703545454546">
tion heading that PP, e.g., “The thesis advisory com-
mittee consists of three qualified members.” As in
the verb case, we extract verb+preposition from rel-
ative clauses, we include particles, we ignore modals
and auxiliaries, and we lemmatize the verbs.
Preposition: If one of the target nouns is the head
of an NP containing a PP with an internal NP headed
by the other target noun (or an inflected form), we
extract the preposition heading that PP, e.g., “The
members of the committee held a meeting.”
Coordinating conjunction: If the two target
nouns are the heads of coordinated NPs, we extract
the coordinating conjunction.
In addition to the lexical part, for each extracted
feature, we keep a direction. Therefore the preposi-
tion of represents two different features in the fol-
lowing examples “member of the committee” and
“committee of members”. See Table 1 for examples.
We use the above-described features to calculate
relational similarity, i.e., similarity between pairs of
nouns. In order to downweight very common fea-
tures like of, we use TF.IDF-weighting:
</bodyText>
<equation confidence="0.9987965">
w(x) = TF(x) x log � N (1)
DF(x)
</equation>
<bodyText confidence="0.999492111111111">
In the above formula, TF(x) is the number of
times the feature x has been extracted for the tar-
get noun pair, DF(x) is the total number of training
noun pairs that have that feature, and N is the total
number of training noun pairs.
Given two nouns and their TF.IDF-weighted fre-
quency vectors A and B, we calculate the similarity
between them using the following generalized vari-
ant of the Dice coefficient:
</bodyText>
<equation confidence="0.997638333333333">
2 � En i=1 min(ai� bi)
Dice(A� B) = (2)
EZ 1 ai + Eni= 1 bi
</equation>
<bodyText confidence="0.97003">
Other variants are also possible, e.g., Lin (1998).
</bodyText>
<sectionHeader confidence="0.995125" genericHeader="method">
4 Relational Similarity Experiments
</sectionHeader>
<subsectionHeader confidence="0.96861">
4.1 SAT Verbal Analogy
</subsectionHeader>
<bodyText confidence="0.9998326">
Following Turney (2006b), we use SAT verbal anal-
ogy as a benchmark problem for relational similar-
ity. We experiment with the 374 SAT questions
collected by Turney and Littman (2005). Table 2
shows two sample questions: the top word pairs
</bodyText>
<figure confidence="0.995955166666667">
ostrich:bird palatable:toothsome
(a) lion:cat (a) rancid:fragrant
(b) goose:flock chewy:textured
ewe:sheep (c) coarse:rough
(d) cub:bear (d) solitude:company
(e) primate:monkey (e) no choice
</figure>
<tableCaption confidence="0.981036666666667">
Table 2: SAT verbal analogy: sample questions. The
stem is in bold, the correct answer is in italic, and the
distractors are in plain text.
</tableCaption>
<bodyText confidence="0.999923885714286">
are called stems, the ones in italic are the solu-
tions, and the remaining ones are distractors. Tur-
ney (2006b) achieves 56% accuracy on this dataset,
which matches the average human performance of
57%, and represents a significant improvement over
the 20% random-guessing baseline.
Note that the righthand side example in Table
2 is missing one distractor; so do 21 questions.
The dataset also mixes different parts of speech:
while solitude and company are nouns, all remaining
words are adjectives. Other examples contain verbs
and adverbs, and even relate pairs of different POS.
This is problematic for our approach, which requires
that both words be nouns3. After having filtered all
examples containing nonnouns, we ended up with
184 questions, which we used in the evaluation.
Given a verbal analogy example, we build six fea-
ture vectors – one for each of the six word pairs. We
then calculate the relational similarity between the
stem of the analogy and each of the five candidates,
and we choose the pair with the highest score; we
make no prediction in case of a tie.
The evaluation results for a leave-one-out cross-
validation are shown in Table 3. We also show 95%-
confidence intervals for the accuracy. The last line
in the table shows the performance of Turney’s LRA
when limited to the 184 noun-only examples. Our
best model v + p + c performs a bit better, 71.3%
vs. 67.4%, but the difference is not statistically sig-
nificant. However, this “inferred” accuracy could be
misleading, and the LRA could have performed bet-
ter if it was restricted to solve noun-only analogies,
which seem easier than the general ones, as demon-
strated by the significant increase in accuracy for
LRA when limited to nouns: 67.4% vs. 56%.
</bodyText>
<footnote confidence="0.944946">
3It can be extended to handle adjective-noun pairs as well,
as demonstrated in section 4.2 below.
</footnote>
<page confidence="0.979386">
455
</page>
<table confidence="0.999953666666667">
✓ x 0 Accuracy Cover.
129 52 3 71.3±7.0 98.4
122 56 6 68.5±7.2 96.7
119 61 4 66.1±7.2 97.8
117 62 5 65.4±7.2 97.3
90 90 4 50.0±7.2 97.8
84 94 6 47.2±7.2 96.7
37 147 0 20.0±5.2 100.0
122 59 3 67.4±7.1 98.4
</table>
<tableCaption confidence="0.9699486">
Table 3: SAT verbal analogy: 184 noun-only examples.
v stands for verb, p for preposition, and c for coordinating
conjunction. For each model, the number of correct (✓),
wrong (x), and nonclassified examples (0) is shown, fol-
lowed by accuracy and coverage (in %s).
</tableCaption>
<table confidence="0.999961666666667">
✓ x 0 Accuracy Cover.
240 352 8 40.5±3.9 98.7
238 354 8 40.2±3.9 98.7
234 350 16 40.1±3.9 97.3
230 362 8 38.9±3.8 98.7
114 471 15 19.5±3.0 97.5
110 475 15 19.1±3.0 97.5
49 551 0 8.2±1.9 100.0
239 361 0 39.8±3.8 100.0
</table>
<tableCaption confidence="0.842171">
Table 4: Head-modifier relations, 30 classes: evaluation
on the Diverse dataset, micro-averaged (in %s).
</tableCaption>
<subsectionHeader confidence="0.858456">
4.2 Head-Modifier Relations
</subsectionHeader>
<bodyText confidence="0.999938424242425">
Next, we experiment with the Diverse dataset of
Barker and Szpakowicz (1998), which consists of
600 head-modifier pairs: noun-noun, adjective-noun
and adverb-noun. Each example is annotated with
one of 30 fine-grained relations, which are fur-
ther grouped into the following 5 coarse-grained
classes (the fine-grained relations are shown in
parentheses): CAUSALITY (cause, effect, purpose,
detraction), TEMPORALITY (frequency, time at,
time through), SPATIAL (direction, location, lo-
cation at, location from), PARTICIPANT (agent,
beneficiary, instrument, object, object property,
part, possessor, property, product, source, stative,
whole) and QUALITY (container, content, equa-
tive, material, measure, topic, type). For example,
exam anxiety is classified as effect and therefore as
CAUSALITY, and blue book is property and there-
fore also PARTICIPANT.
Some examples in the dataset are problematic for
our method. First, in three cases, there are two mod-
ifiers, e.g., infectious disease agent, and we had to
ignore the first one. Second, seven examples have
an adverb modifier, e.g., daily exercise, and 262 ex-
amples have an adjective modifier, e.g., tiny cloud.
We treat them as if the modifier was a noun, which
works in many cases, since many adjectives and ad-
verbs can be used predicatively, e.g., ‘This exercise
is performed daily.’ or ‘This cloud looks very tiny.’
For the evaluation, we created a feature vector for
each head-modifier pair, and we performed a leave-
one-out cross-validation: we left one example for
testing and we trained on the remaining 599 ones,
repeating this procedure 600 times so that each ex-
ample be used for testing. Following Turney and
Littman (2005) we used a 1-nearest-neighbor classi-
fier. We calculated the similarity between the feature
vector of the testing example and each of the train-
ing examples’ vectors. If there was a unique most
similar training example, we predicted its class, and
if there were ties, we chose the class predicted by the
majority of tied examples, if there was a majority.
The results for the 30-class Diverse dataset are
shown in Table 4. Our best model achieves 40.5%
accuracy, which is slightly better than LRA’s 39.8%,
but the difference is not statistically significant.
Table 4 shows that the verbs are the most impor-
tant features, yielding about 40% accuracy regard-
less of whether used alone or in combination with
prepositions and/or coordinating conjunctions; not
using them results in 50% drop in accuracy.
The reason coordinating conjunctions do not help
is that head-modifier relations are typically ex-
pressed with verbal or prepositional paraphrases.
Therefore, coordinating conjunctions only help with
some infrequent relations like equative, e.g., finding
player and coach on the Web suggests an equative
relation for player coach (and for coach player).
As Table 3 shows, this is different for SAT ver-
bal analogy, where verbs are still the most important
feature type and the only whose presence/absence
makes a statistical difference. However, this time
coordinating conjunctions (with prepositions) do
help a bit (the difference is not statistically signifi-
cant) since SAT verbal analogy questions ask for a
broader range of relations, e.g., antonymy, for which
coordinating conjunctions like but are helpful.
</bodyText>
<figure confidence="0.998887555555556">
Model
v + p + c
v
v + p
v + c
p + c
p
baseline
LRA
Model
v + p
v + p + c
v
v + c
p + c
p
baseline
LRA
</figure>
<page confidence="0.663566">
456
</page>
<figure confidence="0.968986">
Model Accuracy
v + p + c + sent + query (type C) 68.1±4.0
v 67.9±4.0
v + p + c 67.8±4.0
v + p + c + sent (type A) 67.3±4.0
v + p 66.9±4.0
sent (sentence words only) 59.3±4.2
p 58.4±4.2
Baseline (majority class) 57.0±4.2
v + p + c + sent + query (C), 8 stars 67.0±4.0
v + p + c + sent (A), 8 stars 65.4±4.1
Best type C on SemEval 67.0±4.0
Best type A on SemEval 66.0±4.1
</figure>
<tableCaption confidence="0.967593333333333">
Table 5: Relations between nominals: evaluation on the
SemEval dataset. Accuracy is macro-averaged (in %s),
up to 10 Google stars are used unless otherwise stated.
</tableCaption>
<subsectionHeader confidence="0.997403">
4.3 Relations Between Nominals
</subsectionHeader>
<bodyText confidence="0.97271575">
We further experimented with the SemEval’07 task
4 dataset (Girju et al., 2007), where each example
consists of a sentence, a target semantic relation, two
nominals to be judged on whether they are in that re-
lation, manually annotated WordNet senses, and the
Web query used to obtain the sentence:
&amp;quot;Among the contents of the
&lt;e1&gt;vessel&lt;/e1&gt; were a set of
carpenter’s &lt;e2&gt;tools&lt;/e2&gt;, several
large storage jars, ceramic utensils,
ropes and remnants of food, as well
as a heavy load of ballast stones.&amp;quot;
</bodyText>
<equation confidence="0.99543325">
WordNet(e1) = &amp;quot;vessel%1:06:00::&amp;quot;,
WordNet(e2) = &amp;quot;tool%1:06:00::&amp;quot;,
Content-Container(e2, e1) = &amp;quot;true&amp;quot;,
Query = &amp;quot;contents of the * were a&amp;quot;
</equation>
<bodyText confidence="0.999955354166667">
The following nonexhaustive and possibly over-
lapping relations are possible: Cause-Effect
(e.g., hormone-growth), Instrument-Agency
(e.g., laser-printer), Theme-Tool (e.g., work-
force), Origin-Entity (e.g., grain-alcohol),
Content-Container (e.g., bananas-basket),
Product-Producer (e.g., honey-bee), and
Part-Whole (e.g., leg-table). Each relation is
considered in isolation; there are 140 training and at
least 70 test examples per relation.
Given an example, we reduced the target entities
e1 and e2 to single nouns by retaining their heads
only. We then mined the Web for sentences con-
taining these nouns, and we extracted the above-
described feature types: verbs, prepositions and co-
ordinating conjunctions. We further used the follow-
ing problem-specific contextual feature types:
Sentence words: after stop words removal and
stemming with the Porter (1980) stemmer;
Entity words: lemmata of the words in e1 and e2;
Query words: words part of the query string.
Each feature type has a specific prefix which pre-
vents it from mixing with other feature types; the
last feature type is used for type C only (see below).
The SemEval competition defines four types of
systems, depending on whether the manually anno-
tated WordNet senses and the Google query are used:
A (WordNet=no, Query=no), B (WordNet=yes,
Query=no), C (WordNet=no, Query=yes), and D
(WordNet=yes, Query=yes). We experimented with
types A and C only since we believe that having the
manually annotated WordNet sense keys is an unre-
alistic assumption for a real-world application.
As before, we used a 1-nearest-neighbor classifier
with TF.IDF-weighting, breaking ties by predicting
the majority class on the training data. The evalu-
ation results are shown in Table 5. We studied the
effect of different subsets of features and of more
Google star operators. As the table shows, using
up to ten Google stars instead of up to eight (see
section 3) yields a slight improvement in accuracy
for systems of both type A (65.4% vs. 67.3%) and
type C (67.0% vs. 68.1%). Both results represent
a statistically significant improvement over the ma-
jority class baseline and over using sentence words
only, and a slight improvement over the best type A
and type C systems on SemEval’07, which achieved
66% and 67% accuracy, respectively.4
</bodyText>
<subsectionHeader confidence="0.998907">
4.4 Noun-Noun Compound Relations
</subsectionHeader>
<bodyText confidence="0.789109818181818">
The last dataset we experimented with is a subset
of the 387 examples listed in the appendix of (Levi,
1978). Levi’s theory is one of the most impor-
tant linguistic theories of the syntax and semantics
of complex nominals – a general concept grouping
4The best type B system on SemEval achieved 76.3% ac-
curacy using the manually-annotated WordNet senses in context
for each example, which constitutes an additional data source,
as opposed to an additional resource. The systems that used
WordNet as a resource only, i.e., ignoring the manually anno-
tated senses, were classified as type A or C. (Girju et al., 2007)
</bodyText>
<page confidence="0.992753">
457
</page>
<table confidence="0.998321636363636">
Model USING THAT ASF NOT USING THAT ASF
Accuracy Cover. ANF Accuracy Cover. ANF
Human: all v 78.4±6.0 99.5 34.3 70.9 – – –
Human: first v from each worker 72.3±6.4 99.5 11.6 25.5 – – – –
v + p + c 50.0±6.7 99.1 216.6 1716.0 49.1±6.7 99.1 206.6 1647.6
v + p 50.0±6.7 99.1 208.9 1427.9 47.6±6.6 99.1 198.9 1359.5
v + c 46.7±6.6 99.1 187.8 1107.2 43.9±6.5 99.1 177.8 1038.8
v 45.8±6.6 99.1 180.0 819.1 42.9±6.5 99.1 170.0 750.7
p 33.0±6.0 99.1 28.9 608.8 33.0±6.0 99.1 28.9 608.8
p + c 32.1±5.9 99.1 36.6 896.9 32.1±5.9 99.1 36.6 896.9
Baseline 19.6±4.8 100.0 – – – – – –
</table>
<tableCaption confidence="0.912938">
Table 6: Noun-noun compound relations, 12 classes: evaluation on Levi-214 dataset. Shown are micro-averaged
accuracy and coverage in %s, followed by average number of features (ANF) and average sum of feature frequencies
(ASF) per example. The righthand side reports the results when the query patterns involving THAT were not used. For
comparison purposes, the top rows show the performance with the human-proposed verbs used as features.
</tableCaption>
<bodyText confidence="0.999895272727273">
together the partially overlapping classes of nom-
inal compounds (e.g., peanut butter), nominaliza-
tions (e.g., dream analysis), and nonpredicate noun
phrases (e.g., electric shock).
In Levi’s theory, complex nominals can be derived
from relative clauses by removing one of the fol-
lowing 12 abstract predicates: CAUSE1 (e.g., tear
gas), CAUSE2 (e.g., drug deaths), HAVE1 (e.g., ap-
ple cake), HAVE2 (e.g., lemon peel), MAKE1 (e.g.,
silkworm), MAKE2 (e.g., snowball), USE (e.g., steam
iron), BE (e.g., soldier ant), IN (e.g., field mouse),
FOR (e.g., horse doctor), FROM (e.g., olive oil), and
ABOUT (e.g., price war). In the resulting nominals,
the modifier is typically the object of the predicate;
when it is the subject, the predicate is marked with
the index 2. The second derivational mechanism in
the theory is nominalization; it produces nominals
whose head is a nominalized verb.
Since we are interested in noun compounds only,
we manually cleansed the set of 387 examples. We
first excluded all concatenations (e.g., silkworm) and
examples with adjectival modifiers (e.g., electric
shock), thus obtaining 250 noun-noun compounds
(Levi-250 dataset). We further filtered out all nom-
inalizations for which the dataset provides no ab-
stract predicate (e.g., city planner), thus ending up
with 214 examples (Levi-214 dataset).
As in the previous experiments, for each of the
214 noun-noun compounds, we mined the Web
for sentences containing both target nouns, from
which we extracted paraphrasing verbs, prepositions
and coordinating conjunctions. We then performed
leave-one-out cross-validation experiments with a
1-nearest-neighbor classifier, trying to predict the
correct predicate for the testing example. The re-
sults are shown in Table 6. As we can see, us-
ing prepositions alone yields about 33% accuracy,
which is a statistically significant improvement over
the majority-class baseline. Overall, the most impor-
tant features are the verbs: they yield 45.8% accu-
racy when used alone, and 50% together with prepo-
sitions. Adding coordinating conjunctions helps a
bit with verbs, but not with prepositions. Note how-
ever that none of the differences between the differ-
ent feature combinations involving verbs are statis-
tically significant.
The righthand side of the table reports the results
when the query patterns involving THAT (see section
3) were not used. We can observe a small 1-3% drop
in accuracy for all models involving verbs, but it is
not statistically significant.
We also show the average number of distinct fea-
tures and sum of feature counts per example: as we
can see, there is a strong positive correlation be-
tween number of features and accuracy.
</bodyText>
<sectionHeader confidence="0.984666" genericHeader="method">
5 Comparison to Human Judgments
</sectionHeader>
<bodyText confidence="0.9999028">
Since in all above tasks the most important fea-
tures were the verbs, we decided to compare our
Web-derived verbs to human-proposed ones for all
noun-noun compounds in the Levi-250 dataset. We
asked human subjects to produce verbs, possibly
</bodyText>
<page confidence="0.996293">
458
</page>
<bodyText confidence="0.999909288888889">
followed by prepositions, that could be used in a
paraphrase involving that. For example, olive oil
can be paraphrased as ‘oil that comes from olives’,
‘oil that is obtained from olives’ or ‘oil that is from
olives’. Note that this implicitly allows for prepo-
sitional paraphrases – when the verb is to be and is
followed by a preposition, as in the last paraphrase.
We used the Amazon Mechanical Turk Web ser-
vice5 to recruit human subjects, and we instructed
them to propose at least three paraphrasing verbs
per noun-noun compound, if possible. We randomly
distributed the noun-noun compounds into groups of
5 and we requested 25 different human subjects per
group. Each human subject was allowed to work
on any number of groups, but not on the same one
twice. A total of 174 different human subjects pro-
duced 19,018 verbs. After filtering the bad submis-
sions and normalizing the verbs, we ended up with
17,821 verbs. See (Nakov, 2007) for further de-
tails on the process of extraction and cleansing. The
dataset itself is freely available (Nakov, 2008).
We compared the human-proposed and the Web-
derived verbs for Levi-214, aggregated by relation.
Given a relation, we collected all verbs belong-
ing to noun-noun compounds from that relation to-
gether with their frequencies. From a vector-space
model point of view, we summed their correspond-
ing frequency vectors. We did this separately for
the human- and the program-generated verbs, and
we compared the resulting vectors using Dice co-
efficient with TF.IDF, calculated as before. Figure
1 shows the cosine correlations using all human-
proposed verbs and the first verb from each judge.
We can see a very-high correlation (mid-70% to
mid-90%) for relations like CAUSE1, MAKE1, BE,
but low correlations of 11-30% for reverse relations
like HAVE2 and MAKE2. Interestingly, using the first
verb only improves the results for highly-correlated
relations, but negatively affects low-correlated ones.
Finally, we repeated the cross-validation exper-
iment with the Levi-214 dataset, this time using
the human-proposed verbs6 as features. As Table
6 shows, we achieved 78.4% accuracy using all
verbs (and and 72.3% with the first verb from each
worker), which is a statistically significant improve-
</bodyText>
<footnote confidence="0.994972333333333">
5http://www.mturk.com
6Note that the human subjects proposed their verbs without
any context and independently of our Web-derived sentences.
</footnote>
<figureCaption confidence="0.993591333333333">
Figure 1: Cosine correlation (in %s) between the
human- and the program- generated verbs by rela-
tion: using all human-proposed verbs vs. the first verb.
</figureCaption>
<bodyText confidence="0.999678125">
ment over the 50% of our best Web-based model.
This result is strong for a 12-way classification prob-
lem, and confirms our observation that verbs and
prepositions are among the most important features
for relational similarity problems. It further suggests
that the human-proposed verbs might be an upper
bound on the accuracy that could be achieved with
automatically extracted features.
</bodyText>
<sectionHeader confidence="0.998489" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999995263157895">
We have presented a simple approach for character-
izing the relation between a pair of nouns in terms
of linguistically-motivated features which could be
useful for many NLP tasks. We found that verbs
were especially useful features for this task. An im-
portant advantage of the approach is that it does not
require knowledge about the semantics of the indi-
vidual nouns. A potential drawback is that it might
not work well for low-frequency words.
The evaluation on several relational similarity
problems, including SAT verbal analogy, head-
modifier relations, and relations between complex
nominals has shown state-of-the-art performance.
The presented approach can be further extended to
other combinations of parts of speech: not just noun-
noun and adjective-noun. Using a parser with a
richer set of syntactic dependency features, e.g., as
proposed by Pad´o and Lapata (2007), is another
promising direction for future work.
</bodyText>
<sectionHeader confidence="0.998362" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8354425">
This research was supported in part by NSF DBI-
0317510.
</bodyText>
<page confidence="0.998788">
459
</page>
<sectionHeader confidence="0.990235" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999908446601942">
Hiyan Alshawi and David Carter. 1994. Training
and scaling preference functions for disambiguation.
Computational Linguistics, 20(4):635–648.
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Proc. of
Computational linguistics, pages 96–102.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13–47.
Michael Cafarella, Michele Banko, and Oren Etzioni.
2006. Relational Web search. Technical Report 2006-
04-02, University of Washington, Department of Com-
puter Science and Engineering.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Journal of Computer Speech and Language - Special
Issue on Multiword Expressions, 4(19):479–496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic rela-
tions between nominals. In Proceedings of SemEval,
pages 13–18, Prague, Czech Republic.
Ralph Grishman and John Sterling. 1994. Generalizing
automatically generated selectional patterns. In Pro-
ceedings of the 15th conference on Computational lin-
guistics, pages 742–747.
Su Nam Kim and Timothy Baldwin. 2006. Interpret-
ing semantic relations in noun compounds via verb se-
mantics. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 491–498.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM Trans.
Speech Lang. Process., 2(1):3.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Dept. of Computing, Macquarie University,
Australia.
Judith Levi. 1978. The Syntax and Semantics of Complex
Nominals. Academic Press, New York.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question-answering. Natural Language
Engineering, 7(4):343–360.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML, pages 296–304.
Preslav Nakov and Marti Hearst. 2006. Using verbs to
characterize noun-noun relations. In AIMSA, volume
4183 of LNCS, pages 233–244. Springer.
Preslav Nakov, Ariel Schwartz, and Marti Hearst. 2004.
Citances: Citation sentences for semantic analysis of
bioscience text. In Proceedings of SIGIR’04 Workshop
on Search and Discovery in Bioinformatics, pages 81–
88, Sheffield, UK.
Preslav Nakov. 2007. Using the Web as an Implicit
Training Set: Application to Noun Compound Syntax
and Semantics. Ph.D. thesis, EECS Department, Uni-
versity of California, Berkeley, UCB/EECS-2007-173.
Preslav Nakov. 2008. Paraphrasing verbs for noun com-
pound interpretation. In Proceedings of the LREC’08
Workshop: Towards a Shared Task for Multiword Ex-
pressions (MWE’08), Marrakech, Morocco.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285–301, Tilburg, The Netherlands.
Sebastian Pad´o and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161–199.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Barbara Rosario and Marti Hearst. 2001. Classifying the
semantic relations in noun compounds via a domain-
specific lexical hierarchy. In Proceedings of EMNLP,
pages 82–90.
Barbara Rosario, Marti Hearst, and Charles Fillmore.
2002. The descent of hierarchy, and selection in rela-
tional semantics. In Proceedings of ACL, pages 247–
254.
Gerda Ruge. 1992. Experiment on linguistically-based
term associations. Inf. Process. Manage., 28(3):317–
332.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of HLT, pages 313–318.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Proceed-
ings of HLT, pages 371–378.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL, pages 252–259.
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning Journal, 60(1-3):251–278.
Peter Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proceedings of IJCAI,
pages 1136–1141.
Peter Turney. 2006a. Expressing implicit semantic re-
lations without supervision. In Proceedings of ACL,
pages 313–320.
Peter Turney. 2006b. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
</reference>
<page confidence="0.999213">
460
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.869347">
<title confidence="0.987742">Solving Relational Similarity Problems Using the Web as a Corpus</title>
<affiliation confidence="0.9853105">EECS, CS division University of California at Berkeley</affiliation>
<address confidence="0.99959">Berkeley, CA 94720, USA</address>
<email confidence="0.997349">nakov@cs.berkeley.edu</email>
<author confidence="0.999962">Marti A Hearst</author>
<affiliation confidence="0.9999585">School of Information University of California at Berkeley</affiliation>
<address confidence="0.999589">Berkeley, CA 94720, USA</address>
<email confidence="0.99977">hearst@ischool.berkeley.edu</email>
<abstract confidence="0.996138916666667">We present a simple linguistically-motivated method for characterizing the semantic relations that hold between two nouns. The approach leverages the vast size of the Web in order to build lexically-specific features. The main idea is to look for verbs, prepositions, and coordinating conjunctions that can help make explicit the hidden relations between the target nouns. Using these features in instance-based classifiers, we demonstrate state-of-the-art results on various relational similarity problems, including mapping noun-modifier pairs to abstract relations like characterizing noun-noun compounds in terms of ablinguistic predicates like classifying the relations between nominals in context, and solving SAT verbal analogy problems. In essence, the approach puts together some existing ideas, showing that they apply generally to various semantic tasks, finding that verbs are especially useful features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>David Carter</author>
</authors>
<title>Training and scaling preference functions for disambiguation.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="7998" citStr="Alshawi and Carter, 1994" startWordPosition="1235" endWordPosition="1238">resolves Y”, “Y is resolved by X”, etc. The approach is extended by Shinyama et al. (2002), who use named entity recognizers and look for anchors belonging to matching semantic classes, e.g., LOCATION, ORGANIZATION. The idea is further extended by Nakov et al. (2004), who apply it in the biomedical domain, imposing the additional restriction that the sentences from which the paraphrases are extracted cite the same target paper. 453 2.4 Word Similarity Another important group of related work is on using syntactic dependency features in a vector-space model for measuring word similarity, e.g., (Alshawi and Carter, 1994), (Grishman and Sterling, 1994), (Ruge, 1992), and (Lin, 1998). For example, given a noun, Lin (1998) extracts verbs that have that noun as a subject or object, and adjectives that modify it. 3 Method Given a pair of nouns, we try to characterize the semantic relation between them by leveraging the vast size of the Web to build linguistically-motivated lexically-specific features. We mine the Web for sentences containing the target nouns, and we extract the connecting verbs, prepositions, and coordinating conjunctions, which we use in a vector-space model to measure relational similarity. The </context>
</contexts>
<marker>Alshawi, Carter, 1994</marker>
<rawString>Hiyan Alshawi and David Carter. 1994. Training and scaling preference functions for disambiguation. Computational Linguistics, 20(4):635–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Barker</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semi-automatic recognition of noun modifier relationships.</title>
<date>1998</date>
<booktitle>In Proc. of Computational linguistics,</booktitle>
<pages>96--102</pages>
<contexts>
<context position="5421" citStr="Barker and Szpakowicz (1998)" startWordPosition="825" endWordPosition="828">ssing implicit semantic relations. For example, CAUSE (e.g., cold virus) is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL (e.g., morning frost). With 5 classes, he achieves F-measure=50.2%. 2.2 Noun-Noun Compound Semantics Lauer (1995) reduces the problem of noun compound interpretation to choosing the best paraphrasing preposition from the following set: of, for, in, at, on, from, with or about. He achieved 40% accuracy using corpus frequencies. This result was improved to 55.7% by Lapata and Keller (2005) who used Web-derived n-gram frequencies. Barker and Szpakowicz (1998) use syntactic clues and the identity of the nouns in a nearest-neighbor classifier, achieving 60-70% accuracy. Rosario and Hearst (2001) used a discriminative classifier to assign 18 relations for noun compounds from biomedical text, achieving 60% accuracy. Rosario et al. (2002) reported 90% accuracy with a “descent of hierarchy” approach which characterizes the relationship between the nouns in a bioscience noun-noun compound based on the MeSH categories the nouns belong to. Girju et al. (2005) apply both classic (SVM and decision trees) and novel supervised models (semantic scattering and i</context>
<context position="16611" citStr="Barker and Szpakowicz (1998)" startWordPosition="2760" endWordPosition="2763">or verb, p for preposition, and c for coordinating conjunction. For each model, the number of correct (✓), wrong (x), and nonclassified examples (0) is shown, followed by accuracy and coverage (in %s). ✓ x 0 Accuracy Cover. 240 352 8 40.5±3.9 98.7 238 354 8 40.2±3.9 98.7 234 350 16 40.1±3.9 97.3 230 362 8 38.9±3.8 98.7 114 471 15 19.5±3.0 97.5 110 475 15 19.1±3.0 97.5 49 551 0 8.2±1.9 100.0 239 361 0 39.8±3.8 100.0 Table 4: Head-modifier relations, 30 classes: evaluation on the Diverse dataset, micro-averaged (in %s). 4.2 Head-Modifier Relations Next, we experiment with the Diverse dataset of Barker and Szpakowicz (1998), which consists of 600 head-modifier pairs: noun-noun, adjective-noun and adverb-noun. Each example is annotated with one of 30 fine-grained relations, which are further grouped into the following 5 coarse-grained classes (the fine-grained relations are shown in parentheses): CAUSALITY (cause, effect, purpose, detraction), TEMPORALITY (frequency, time at, time through), SPATIAL (direction, location, location at, location from), PARTICIPANT (agent, beneficiary, instrument, object, object property, part, possessor, property, product, source, stative, whole) and QUALITY (container, content, equa</context>
</contexts>
<marker>Barker, Szpakowicz, 1998</marker>
<rawString>Ken Barker and Stan Szpakowicz. 1998. Semi-automatic recognition of noun modifier relationships. In Proc. of Computational linguistics, pages 96–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating wordnet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="1398" citStr="Budanitsky and Hirst, 2006" startWordPosition="197" endWordPosition="200">rt results on various relational similarity problems, including mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER, characterizing noun-noun compounds in terms of abstract linguistic predicates like CAUSE, USE, and FROM, classifying the relations between nominals in context, and solving SAT verbal analogy problems. In essence, the approach puts together some existing ideas, showing that they apply generally to various semantic tasks, finding that verbs are especially useful features. 1 Introduction Despite the tremendous amount of work on word similarity (see (Budanitsky and Hirst, 2006) for an overview), there is surprisingly little research on the important related problem of relational similarity – semantic similarity between pairs of words. Students who took the SAT test before 2005 or who *After January 2008 at the Linguistic Modeling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, nakov@lml.bas.bg are taking the GRE test nowadays are familiar with an instance of this problem – verbal analogy questions, which ask whether, e.g., the relationship between ostrich and bird is more similar to that between lion and cat, or rather between primate a</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Cafarella</author>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>Relational Web search.</title>
<date>2006</date>
<tech>Technical Report 2006-04-02,</tech>
<institution>University of Washington, Department of Computer Science and Engineering.</institution>
<contexts>
<context position="2590" citStr="Cafarella et al., 2006" startWordPosition="386" endWordPosition="389">at, or rather between primate and monkey. These analogies are difficult, and the average test taker gives a correct answer 57% of the time (Turney and Littman, 2005). Many NLP applications could benefit from solving relational similarity problems, including but not limited to question answering, information retrieval, machine translation, word sense disambiguation, and information extraction. For example, a relational search engine like TextRunner, which serves queries like “find all X such that X causes wrinkles”, asking for all entities that are in a particular relation with a given entity (Cafarella et al., 2006), needs to recognize that laugh wrinkles is an instance of CAUSE-EFFECT. While there are not many success stories so far, measuring semantic similarity has proven its advantages for textual entailment (Tatu and Moldovan, 2005). In this paper, we introduce a novel linguisticallymotivated Web-based approach to relational similarity, which, despite its simplicity, achieves stateof-the-art performance on a number of problems. Following Turney (2006b), we test our approach on SAT verbal analogy questions and on mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER. We </context>
</contexts>
<marker>Cafarella, Banko, Etzioni, 2006</marker>
<rawString>Michael Cafarella, Michele Banko, and Oren Etzioni. 2006. Relational Web search. Technical Report 2006-04-02, University of Washington, Department of Computer Science and Engineering.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5421" citStr="(1998)" startWordPosition="828" endWordPosition="828">c relations. For example, CAUSE (e.g., cold virus) is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL (e.g., morning frost). With 5 classes, he achieves F-measure=50.2%. 2.2 Noun-Noun Compound Semantics Lauer (1995) reduces the problem of noun compound interpretation to choosing the best paraphrasing preposition from the following set: of, for, in, at, on, from, with or about. He achieved 40% accuracy using corpus frequencies. This result was improved to 55.7% by Lapata and Keller (2005) who used Web-derived n-gram frequencies. Barker and Szpakowicz (1998) use syntactic clues and the identity of the nouns in a nearest-neighbor classifier, achieving 60-70% accuracy. Rosario and Hearst (2001) used a discriminative classifier to assign 18 relations for noun compounds from biomedical text, achieving 60% accuracy. Rosario et al. (2002) reported 90% accuracy with a “descent of hierarchy” approach which characterizes the relationship between the nouns in a bioscience noun-noun compound based on the MeSH categories the nouns belong to. Girju et al. (2005) apply both classic (SVM and decision trees) and novel supervised models (semantic scattering and i</context>
<context position="8099" citStr="(1998)" startWordPosition="1254" endWordPosition="1254">zers and look for anchors belonging to matching semantic classes, e.g., LOCATION, ORGANIZATION. The idea is further extended by Nakov et al. (2004), who apply it in the biomedical domain, imposing the additional restriction that the sentences from which the paraphrases are extracted cite the same target paper. 453 2.4 Word Similarity Another important group of related work is on using syntactic dependency features in a vector-space model for measuring word similarity, e.g., (Alshawi and Carter, 1994), (Grishman and Sterling, 1994), (Ruge, 1992), and (Lin, 1998). For example, given a noun, Lin (1998) extracts verbs that have that noun as a subject or object, and adjectives that modify it. 3 Method Given a pair of nouns, we try to characterize the semantic relation between them by leveraging the vast size of the Web to build linguistically-motivated lexically-specific features. We mine the Web for sentences containing the target nouns, and we extract the connecting verbs, prepositions, and coordinating conjunctions, which we use in a vector-space model to measure relational similarity. The process of extraction starts with exact phrase queries issued against a Web search engine (Google) us</context>
<context position="13250" citStr="(1998)" startWordPosition="2196" endWordPosition="2196">very common features like of, we use TF.IDF-weighting: w(x) = TF(x) x log � N (1) DF(x) In the above formula, TF(x) is the number of times the feature x has been extracted for the target noun pair, DF(x) is the total number of training noun pairs that have that feature, and N is the total number of training noun pairs. Given two nouns and their TF.IDF-weighted frequency vectors A and B, we calculate the similarity between them using the following generalized variant of the Dice coefficient: 2 � En i=1 min(ai� bi) Dice(A� B) = (2) EZ 1 ai + Eni= 1 bi Other variants are also possible, e.g., Lin (1998). 4 Relational Similarity Experiments 4.1 SAT Verbal Analogy Following Turney (2006b), we use SAT verbal analogy as a benchmark problem for relational similarity. We experiment with the 374 SAT questions collected by Turney and Littman (2005). Table 2 shows two sample questions: the top word pairs ostrich:bird palatable:toothsome (a) lion:cat (a) rancid:fragrant (b) goose:flock chewy:textured ewe:sheep (c) coarse:rough (d) cub:bear (d) solitude:company (e) primate:monkey (e) no choice Table 2: SAT verbal analogy: sample questions. The stem is in bold, the correct answer is in italic, and the d</context>
<context position="16611" citStr="(1998)" startWordPosition="2763" endWordPosition="2763">tion, and c for coordinating conjunction. For each model, the number of correct (✓), wrong (x), and nonclassified examples (0) is shown, followed by accuracy and coverage (in %s). ✓ x 0 Accuracy Cover. 240 352 8 40.5±3.9 98.7 238 354 8 40.2±3.9 98.7 234 350 16 40.1±3.9 97.3 230 362 8 38.9±3.8 98.7 114 471 15 19.5±3.0 97.5 110 475 15 19.1±3.0 97.5 49 551 0 8.2±1.9 100.0 239 361 0 39.8±3.8 100.0 Table 4: Head-modifier relations, 30 classes: evaluation on the Diverse dataset, micro-averaged (in %s). 4.2 Head-Modifier Relations Next, we experiment with the Diverse dataset of Barker and Szpakowicz (1998), which consists of 600 head-modifier pairs: noun-noun, adjective-noun and adverb-noun. Each example is annotated with one of 30 fine-grained relations, which are further grouped into the following 5 coarse-grained classes (the fine-grained relations are shown in parentheses): CAUSALITY (cause, effect, purpose, detraction), TEMPORALITY (frequency, time at, time through), SPATIAL (direction, location, location at, location from), PARTICIPANT (agent, beneficiary, instrument, object, object property, part, possessor, property, product, source, stative, whole) and QUALITY (container, content, equa</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Dan Moldovan</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
</authors>
<title>On the semantics of noun compounds.</title>
<date>2005</date>
<journal>Journal of Computer Speech and Language - Special Issue on Multiword Expressions,</journal>
<volume>4</volume>
<issue>19</issue>
<contexts>
<context position="5922" citStr="Girju et al. (2005)" startWordPosition="902" endWordPosition="905">as improved to 55.7% by Lapata and Keller (2005) who used Web-derived n-gram frequencies. Barker and Szpakowicz (1998) use syntactic clues and the identity of the nouns in a nearest-neighbor classifier, achieving 60-70% accuracy. Rosario and Hearst (2001) used a discriminative classifier to assign 18 relations for noun compounds from biomedical text, achieving 60% accuracy. Rosario et al. (2002) reported 90% accuracy with a “descent of hierarchy” approach which characterizes the relationship between the nouns in a bioscience noun-noun compound based on the MeSH categories the nouns belong to. Girju et al. (2005) apply both classic (SVM and decision trees) and novel supervised models (semantic scattering and iterative semantic specialization), using WordNet, word sense disambiguation, and a set of linguistic features. They test their system against both Lauer’s 8 prepositional paraphrases and another set of 21 semantic relations, achieving up to 54% accuracy on the latter. In a previous work (Nakov and Hearst, 2006), we have shown that the relationship between the nouns in a noun-noun compound can be characterized using verbs extracted from the Web, but we provided no formal evaluation. Kim and Baldwi</context>
</contexts>
<marker>Girju, Moldovan, Tatu, Antohe, 2005</marker>
<rawString>Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe. 2005. On the semantics of noun compounds. Journal of Computer Speech and Language - Special Issue on Multiword Expressions, 4(19):479–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>Semeval-2007 task 04: Classification of semantic relations between nominals.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval,</booktitle>
<pages>13--18</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="20576" citStr="Girju et al., 2007" startWordPosition="3419" endWordPosition="3422">+ sent + query (type C) 68.1±4.0 v 67.9±4.0 v + p + c 67.8±4.0 v + p + c + sent (type A) 67.3±4.0 v + p 66.9±4.0 sent (sentence words only) 59.3±4.2 p 58.4±4.2 Baseline (majority class) 57.0±4.2 v + p + c + sent + query (C), 8 stars 67.0±4.0 v + p + c + sent (A), 8 stars 65.4±4.1 Best type C on SemEval 67.0±4.0 Best type A on SemEval 66.0±4.1 Table 5: Relations between nominals: evaluation on the SemEval dataset. Accuracy is macro-averaged (in %s), up to 10 Google stars are used unless otherwise stated. 4.3 Relations Between Nominals We further experimented with the SemEval’07 task 4 dataset (Girju et al., 2007), where each example consists of a sentence, a target semantic relation, two nominals to be judged on whether they are in that relation, manually annotated WordNet senses, and the Web query used to obtain the sentence: &amp;quot;Among the contents of the &lt;e1&gt;vessel&lt;/e1&gt; were a set of carpenter’s &lt;e2&gt;tools&lt;/e2&gt;, several large storage jars, ceramic utensils, ropes and remnants of food, as well as a heavy load of ballast stones.&amp;quot; WordNet(e1) = &amp;quot;vessel%1:06:00::&amp;quot;, WordNet(e2) = &amp;quot;tool%1:06:00::&amp;quot;, Content-Container(e2, e1) = &amp;quot;true&amp;quot;, Query = &amp;quot;contents of the * were a&amp;quot; The following nonexhaustive and possibly </context>
<context position="24072" citStr="Girju et al., 2007" startWordPosition="3967" endWordPosition="3970">tions The last dataset we experimented with is a subset of the 387 examples listed in the appendix of (Levi, 1978). Levi’s theory is one of the most important linguistic theories of the syntax and semantics of complex nominals – a general concept grouping 4The best type B system on SemEval achieved 76.3% accuracy using the manually-annotated WordNet senses in context for each example, which constitutes an additional data source, as opposed to an additional resource. The systems that used WordNet as a resource only, i.e., ignoring the manually annotated senses, were classified as type A or C. (Girju et al., 2007) 457 Model USING THAT ASF NOT USING THAT ASF Accuracy Cover. ANF Accuracy Cover. ANF Human: all v 78.4±6.0 99.5 34.3 70.9 – – – Human: first v from each worker 72.3±6.4 99.5 11.6 25.5 – – – – v + p + c 50.0±6.7 99.1 216.6 1716.0 49.1±6.7 99.1 206.6 1647.6 v + p 50.0±6.7 99.1 208.9 1427.9 47.6±6.6 99.1 198.9 1359.5 v + c 46.7±6.6 99.1 187.8 1107.2 43.9±6.5 99.1 177.8 1038.8 v 45.8±6.6 99.1 180.0 819.1 42.9±6.5 99.1 170.0 750.7 p 33.0±6.0 99.1 28.9 608.8 33.0±6.0 99.1 28.9 608.8 p + c 32.1±5.9 99.1 36.6 896.9 32.1±5.9 99.1 36.6 896.9 Baseline 19.6±4.8 100.0 – – – – – – Table 6: Noun-noun compoun</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. Semeval-2007 task 04: Classification of semantic relations between nominals. In Proceedings of SemEval, pages 13–18, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>John Sterling</author>
</authors>
<title>Generalizing automatically generated selectional patterns.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th conference on Computational linguistics,</booktitle>
<pages>742--747</pages>
<contexts>
<context position="8029" citStr="Grishman and Sterling, 1994" startWordPosition="1239" endWordPosition="1242">by X”, etc. The approach is extended by Shinyama et al. (2002), who use named entity recognizers and look for anchors belonging to matching semantic classes, e.g., LOCATION, ORGANIZATION. The idea is further extended by Nakov et al. (2004), who apply it in the biomedical domain, imposing the additional restriction that the sentences from which the paraphrases are extracted cite the same target paper. 453 2.4 Word Similarity Another important group of related work is on using syntactic dependency features in a vector-space model for measuring word similarity, e.g., (Alshawi and Carter, 1994), (Grishman and Sterling, 1994), (Ruge, 1992), and (Lin, 1998). For example, given a noun, Lin (1998) extracts verbs that have that noun as a subject or object, and adjectives that modify it. 3 Method Given a pair of nouns, we try to characterize the semantic relation between them by leveraging the vast size of the Web to build linguistically-motivated lexically-specific features. We mine the Web for sentences containing the target nouns, and we extract the connecting verbs, prepositions, and coordinating conjunctions, which we use in a vector-space model to measure relational similarity. The process of extraction starts wi</context>
</contexts>
<marker>Grishman, Sterling, 1994</marker>
<rawString>Ralph Grishman and John Sterling. 1994. Generalizing automatically generated selectional patterns. In Proceedings of the 15th conference on Computational linguistics, pages 742–747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
</authors>
<title>Interpreting semantic relations in noun compounds via verb semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>491--498</pages>
<contexts>
<context position="6530" citStr="Kim and Baldwin (2006)" startWordPosition="997" endWordPosition="1000"> et al. (2005) apply both classic (SVM and decision trees) and novel supervised models (semantic scattering and iterative semantic specialization), using WordNet, word sense disambiguation, and a set of linguistic features. They test their system against both Lauer’s 8 prepositional paraphrases and another set of 21 semantic relations, achieving up to 54% accuracy on the latter. In a previous work (Nakov and Hearst, 2006), we have shown that the relationship between the nouns in a noun-noun compound can be characterized using verbs extracted from the Web, but we provided no formal evaluation. Kim and Baldwin (2006) characterized the semantic relationship in a noun-noun compound using the verbs connecting the two nouns by comparing them to predefined seed verbs. Their approach is highly resource intensive (uses WordNet, CoreLex and Moby’s thesaurus), and is quite sensitive to the seed set of verbs: on a collection of 453 examples and 19 relations, they achieved 52.6% accuracy with 84 seed verbs, but only 46.7% with 57 seed verbs. 2.3 Paraphrase Acquisition Our method of extraction of paraphrasing verbs and prepositions is similar to previous paraphrase acquisition approaches. Lin and Pantel (2001) extrac</context>
</contexts>
<marker>Kim, Baldwin, 2006</marker>
<rawString>Su Nam Kim and Timothy Baldwin. 2006. Interpreting semantic relations in noun compounds via verb semantics. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 491–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Trans. Speech Lang. Process.,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="5351" citStr="Lapata and Keller (2005)" startWordPosition="816" endWordPosition="819">ts an unsupervised algorithm for mining the Web for patterns expressing implicit semantic relations. For example, CAUSE (e.g., cold virus) is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL (e.g., morning frost). With 5 classes, he achieves F-measure=50.2%. 2.2 Noun-Noun Compound Semantics Lauer (1995) reduces the problem of noun compound interpretation to choosing the best paraphrasing preposition from the following set: of, for, in, at, on, from, with or about. He achieved 40% accuracy using corpus frequencies. This result was improved to 55.7% by Lapata and Keller (2005) who used Web-derived n-gram frequencies. Barker and Szpakowicz (1998) use syntactic clues and the identity of the nouns in a nearest-neighbor classifier, achieving 60-70% accuracy. Rosario and Hearst (2001) used a discriminative classifier to assign 18 relations for noun compounds from biomedical text, achieving 60% accuracy. Rosario et al. (2002) reported 90% accuracy with a “descent of hierarchy” approach which characterizes the relationship between the nouns in a bioscience noun-noun compound based on the MeSH categories the nouns belong to. Girju et al. (2005) apply both classic (SVM and </context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>Mirella Lapata and Frank Keller. 2005. Web-based models for natural language processing. ACM Trans. Speech Lang. Process., 2(1):3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Designing Statistical Language Learners: Experiments on Noun Compounds.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Dept. of Computing, Macquarie University, Australia.</institution>
<contexts>
<context position="5074" citStr="Lauer (1995)" startWordPosition="770" endWordPosition="771">ithm consists of 12 steps described in detail in (Turney, 2006b). When applied to SAT questions, it achieves the state-of-the-art accuracy of 56%. On the Diverse dataset, it yields an F-measure of 39.8% with 30 classes, and 58% with 5 classes. Turney (2006a) presents an unsupervised algorithm for mining the Web for patterns expressing implicit semantic relations. For example, CAUSE (e.g., cold virus) is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL (e.g., morning frost). With 5 classes, he achieves F-measure=50.2%. 2.2 Noun-Noun Compound Semantics Lauer (1995) reduces the problem of noun compound interpretation to choosing the best paraphrasing preposition from the following set: of, for, in, at, on, from, with or about. He achieved 40% accuracy using corpus frequencies. This result was improved to 55.7% by Lapata and Keller (2005) who used Web-derived n-gram frequencies. Barker and Szpakowicz (1998) use syntactic clues and the identity of the nouns in a nearest-neighbor classifier, achieving 60-70% accuracy. Rosario and Hearst (2001) used a discriminative classifier to assign 18 relations for noun compounds from biomedical text, achieving 60% accu</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Designing Statistical Language Learners: Experiments on Noun Compounds. Ph.D. thesis, Dept. of Computing, Macquarie University, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Levi</author>
</authors>
<title>The Syntax and Semantics of Complex Nominals.</title>
<date>1978</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="23567" citStr="Levi, 1978" startWordPosition="3885" endWordPosition="3886">s. As the table shows, using up to ten Google stars instead of up to eight (see section 3) yields a slight improvement in accuracy for systems of both type A (65.4% vs. 67.3%) and type C (67.0% vs. 68.1%). Both results represent a statistically significant improvement over the majority class baseline and over using sentence words only, and a slight improvement over the best type A and type C systems on SemEval’07, which achieved 66% and 67% accuracy, respectively.4 4.4 Noun-Noun Compound Relations The last dataset we experimented with is a subset of the 387 examples listed in the appendix of (Levi, 1978). Levi’s theory is one of the most important linguistic theories of the syntax and semantics of complex nominals – a general concept grouping 4The best type B system on SemEval achieved 76.3% accuracy using the manually-annotated WordNet senses in context for each example, which constitutes an additional data source, as opposed to an additional resource. The systems that used WordNet as a resource only, i.e., ignoring the manually annotated senses, were classified as type A or C. (Girju et al., 2007) 457 Model USING THAT ASF NOT USING THAT ASF Accuracy Cover. ANF Accuracy Cover. ANF Human: all</context>
</contexts>
<marker>Levi, 1978</marker>
<rawString>Judith Levi. 1978. The Syntax and Semantics of Complex Nominals. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question-answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="7123" citStr="Lin and Pantel (2001)" startWordPosition="1092" endWordPosition="1095">tion. Kim and Baldwin (2006) characterized the semantic relationship in a noun-noun compound using the verbs connecting the two nouns by comparing them to predefined seed verbs. Their approach is highly resource intensive (uses WordNet, CoreLex and Moby’s thesaurus), and is quite sensitive to the seed set of verbs: on a collection of 453 examples and 19 relations, they achieved 52.6% accuracy with 84 seed verbs, but only 46.7% with 57 seed verbs. 2.3 Paraphrase Acquisition Our method of extraction of paraphrasing verbs and prepositions is similar to previous paraphrase acquisition approaches. Lin and Pantel (2001) extract paraphrases from dependency tree paths whose ends contain semantically similar sets of words by generalizing over these ends. For example, given “X solves Y”, they extract paraphrases like “X finds a solution to Y”, “X tries to solve Y”, “X resolves Y”, “Y is resolved by X”, etc. The approach is extended by Shinyama et al. (2002), who use named entity recognizers and look for anchors belonging to matching semantic classes, e.g., LOCATION, ORGANIZATION. The idea is further extended by Nakov et al. (2004), who apply it in the biomedical domain, imposing the additional restriction that t</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question-answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>296--304</pages>
<contexts>
<context position="8060" citStr="Lin, 1998" startWordPosition="1246" endWordPosition="1247">et al. (2002), who use named entity recognizers and look for anchors belonging to matching semantic classes, e.g., LOCATION, ORGANIZATION. The idea is further extended by Nakov et al. (2004), who apply it in the biomedical domain, imposing the additional restriction that the sentences from which the paraphrases are extracted cite the same target paper. 453 2.4 Word Similarity Another important group of related work is on using syntactic dependency features in a vector-space model for measuring word similarity, e.g., (Alshawi and Carter, 1994), (Grishman and Sterling, 1994), (Ruge, 1992), and (Lin, 1998). For example, given a noun, Lin (1998) extracts verbs that have that noun as a subject or object, and adjectives that modify it. 3 Method Given a pair of nouns, we try to characterize the semantic relation between them by leveraging the vast size of the Web to build linguistically-motivated lexically-specific features. We mine the Web for sentences containing the target nouns, and we extract the connecting verbs, prepositions, and coordinating conjunctions, which we use in a vector-space model to measure relational similarity. The process of extraction starts with exact phrase queries issued </context>
<context position="13250" citStr="Lin (1998)" startWordPosition="2195" endWordPosition="2196">ght very common features like of, we use TF.IDF-weighting: w(x) = TF(x) x log � N (1) DF(x) In the above formula, TF(x) is the number of times the feature x has been extracted for the target noun pair, DF(x) is the total number of training noun pairs that have that feature, and N is the total number of training noun pairs. Given two nouns and their TF.IDF-weighted frequency vectors A and B, we calculate the similarity between them using the following generalized variant of the Dice coefficient: 2 � En i=1 min(ai� bi) Dice(A� B) = (2) EZ 1 ai + Eni= 1 bi Other variants are also possible, e.g., Lin (1998). 4 Relational Similarity Experiments 4.1 SAT Verbal Analogy Following Turney (2006b), we use SAT verbal analogy as a benchmark problem for relational similarity. We experiment with the 374 SAT questions collected by Turney and Littman (2005). Table 2 shows two sample questions: the top word pairs ostrich:bird palatable:toothsome (a) lion:cat (a) rancid:fragrant (b) goose:flock chewy:textured ewe:sheep (c) coarse:rough (d) cub:bear (d) solitude:company (e) primate:monkey (e) no choice Table 2: SAT verbal analogy: sample questions. The stem is in bold, the correct answer is in italic, and the d</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of ICML, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Using verbs to characterize noun-noun relations.</title>
<date>2006</date>
<booktitle>In AIMSA,</booktitle>
<volume>4183</volume>
<pages>233--244</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6333" citStr="Nakov and Hearst, 2006" startWordPosition="964" endWordPosition="967"> 90% accuracy with a “descent of hierarchy” approach which characterizes the relationship between the nouns in a bioscience noun-noun compound based on the MeSH categories the nouns belong to. Girju et al. (2005) apply both classic (SVM and decision trees) and novel supervised models (semantic scattering and iterative semantic specialization), using WordNet, word sense disambiguation, and a set of linguistic features. They test their system against both Lauer’s 8 prepositional paraphrases and another set of 21 semantic relations, achieving up to 54% accuracy on the latter. In a previous work (Nakov and Hearst, 2006), we have shown that the relationship between the nouns in a noun-noun compound can be characterized using verbs extracted from the Web, but we provided no formal evaluation. Kim and Baldwin (2006) characterized the semantic relationship in a noun-noun compound using the verbs connecting the two nouns by comparing them to predefined seed verbs. Their approach is highly resource intensive (uses WordNet, CoreLex and Moby’s thesaurus), and is quite sensitive to the seed set of verbs: on a collection of 453 examples and 19 relations, they achieved 52.6% accuracy with 84 seed verbs, but only 46.7% </context>
</contexts>
<marker>Nakov, Hearst, 2006</marker>
<rawString>Preslav Nakov and Marti Hearst. 2006. Using verbs to characterize noun-noun relations. In AIMSA, volume 4183 of LNCS, pages 233–244. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Ariel Schwartz</author>
<author>Marti Hearst</author>
</authors>
<title>Citances: Citation sentences for semantic analysis of bioscience text.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGIR’04 Workshop on Search and Discovery in Bioinformatics,</booktitle>
<pages>81--88</pages>
<location>Sheffield, UK.</location>
<contexts>
<context position="7640" citStr="Nakov et al. (2004)" startWordPosition="1180" endWordPosition="1183">verbs and prepositions is similar to previous paraphrase acquisition approaches. Lin and Pantel (2001) extract paraphrases from dependency tree paths whose ends contain semantically similar sets of words by generalizing over these ends. For example, given “X solves Y”, they extract paraphrases like “X finds a solution to Y”, “X tries to solve Y”, “X resolves Y”, “Y is resolved by X”, etc. The approach is extended by Shinyama et al. (2002), who use named entity recognizers and look for anchors belonging to matching semantic classes, e.g., LOCATION, ORGANIZATION. The idea is further extended by Nakov et al. (2004), who apply it in the biomedical domain, imposing the additional restriction that the sentences from which the paraphrases are extracted cite the same target paper. 453 2.4 Word Similarity Another important group of related work is on using syntactic dependency features in a vector-space model for measuring word similarity, e.g., (Alshawi and Carter, 1994), (Grishman and Sterling, 1994), (Ruge, 1992), and (Lin, 1998). For example, given a noun, Lin (1998) extracts verbs that have that noun as a subject or object, and adjectives that modify it. 3 Method Given a pair of nouns, we try to characte</context>
</contexts>
<marker>Nakov, Schwartz, Hearst, 2004</marker>
<rawString>Preslav Nakov, Ariel Schwartz, and Marti Hearst. 2004. Citances: Citation sentences for semantic analysis of bioscience text. In Proceedings of SIGIR’04 Workshop on Search and Discovery in Bioinformatics, pages 81– 88, Sheffield, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Using the Web as an Implicit Training Set: Application to Noun Compound Syntax and Semantics.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<pages>2007--173</pages>
<institution>EECS Department, University of California, Berkeley,</institution>
<contexts>
<context position="28968" citStr="Nakov, 2007" startWordPosition="4767" endWordPosition="4768">a preposition, as in the last paraphrase. We used the Amazon Mechanical Turk Web service5 to recruit human subjects, and we instructed them to propose at least three paraphrasing verbs per noun-noun compound, if possible. We randomly distributed the noun-noun compounds into groups of 5 and we requested 25 different human subjects per group. Each human subject was allowed to work on any number of groups, but not on the same one twice. A total of 174 different human subjects produced 19,018 verbs. After filtering the bad submissions and normalizing the verbs, we ended up with 17,821 verbs. See (Nakov, 2007) for further details on the process of extraction and cleansing. The dataset itself is freely available (Nakov, 2008). We compared the human-proposed and the Webderived verbs for Levi-214, aggregated by relation. Given a relation, we collected all verbs belonging to noun-noun compounds from that relation together with their frequencies. From a vector-space model point of view, we summed their corresponding frequency vectors. We did this separately for the human- and the program-generated verbs, and we compared the resulting vectors using Dice coefficient with TF.IDF, calculated as before. Figu</context>
</contexts>
<marker>Nakov, 2007</marker>
<rawString>Preslav Nakov. 2007. Using the Web as an Implicit Training Set: Application to Noun Compound Syntax and Semantics. Ph.D. thesis, EECS Department, University of California, Berkeley, UCB/EECS-2007-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Paraphrasing verbs for noun compound interpretation.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC’08 Workshop: Towards a Shared Task for Multiword Expressions (MWE’08),</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="29085" citStr="Nakov, 2008" startWordPosition="4786" endWordPosition="4787">and we instructed them to propose at least three paraphrasing verbs per noun-noun compound, if possible. We randomly distributed the noun-noun compounds into groups of 5 and we requested 25 different human subjects per group. Each human subject was allowed to work on any number of groups, but not on the same one twice. A total of 174 different human subjects produced 19,018 verbs. After filtering the bad submissions and normalizing the verbs, we ended up with 17,821 verbs. See (Nakov, 2007) for further details on the process of extraction and cleansing. The dataset itself is freely available (Nakov, 2008). We compared the human-proposed and the Webderived verbs for Levi-214, aggregated by relation. Given a relation, we collected all verbs belonging to noun-noun compounds from that relation together with their frequencies. From a vector-space model point of view, we summed their corresponding frequency vectors. We did this separately for the human- and the program-generated verbs, and we compared the resulting vectors using Dice coefficient with TF.IDF, calculated as before. Figure 1 shows the cosine correlations using all humanproposed verbs and the first verb from each judge. We can see a ver</context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Preslav Nakov. 2008. Paraphrasing verbs for noun compound interpretation. In Proceedings of the LREC’08 Workshop: Towards a Shared Task for Multiword Expressions (MWE’08), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Exploring noun-modifier semantic relations.</title>
<date>2003</date>
<booktitle>In Fifth International Workshop on Computational Semantics (IWCS5),</booktitle>
<pages>285--301</pages>
<location>Tilburg, The Netherlands.</location>
<contexts>
<context position="4085" citStr="Nastase and Szpakowicz (2003)" startWordPosition="616" endWordPosition="620"> 2008. c�2008 Association for Computational Linguistics 2 Related Work 2.1 Characterizing Semantic Relations Turney and Littman (2005) characterize the relationship between two words as a vector with coordinates corresponding to the Web frequencies of 128 fixed phrases like “X for Y ” and “Y for X” instantiated from a fixed set of 64 joining terms like for, such as, not the, is *, etc. These vectors are used in a nearest-neighbor classifier to solve SAT verbal analogy problems, yielding 47% accuracy. The same approach is applied to classifying noun-modifier pairs: using the Diverse dataset of Nastase and Szpakowicz (2003), Turney&amp;Littman achieve F-measures of 26.5% with 30 fine-grained relations, and 43.2% with 5 course-grained relations. Turney (2005) extends the above approach by introducing the latent relational analysis (LRA), which uses automatically generated synonyms, learns suitable patterns, and performs singular value decomposition in order to smooth the frequencies. The full algorithm consists of 12 steps described in detail in (Turney, 2006b). When applied to SAT questions, it achieves the state-of-the-art accuracy of 56%. On the Diverse dataset, it yields an F-measure of 39.8% with 30 classes, and</context>
</contexts>
<marker>Nastase, Szpakowicz, 2003</marker>
<rawString>Vivi Nastase and Stan Szpakowicz. 2003. Exploring noun-modifier semantic relations. In Fifth International Workshop on Computational Semantics (IWCS5), pages 285–301, Tilburg, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependencybased construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependencybased construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="21995" citStr="Porter (1980)" startWordPosition="3625" endWordPosition="3626">bananas-basket), Product-Producer (e.g., honey-bee), and Part-Whole (e.g., leg-table). Each relation is considered in isolation; there are 140 training and at least 70 test examples per relation. Given an example, we reduced the target entities e1 and e2 to single nouns by retaining their heads only. We then mined the Web for sentences containing these nouns, and we extracted the abovedescribed feature types: verbs, prepositions and coordinating conjunctions. We further used the following problem-specific contextual feature types: Sentence words: after stop words removal and stemming with the Porter (1980) stemmer; Entity words: lemmata of the words in e1 and e2; Query words: words part of the query string. Each feature type has a specific prefix which prevents it from mixing with other feature types; the last feature type is used for type C only (see below). The SemEval competition defines four types of systems, depending on whether the manually annotated WordNet senses and the Google query are used: A (WordNet=no, Query=no), B (WordNet=yes, Query=no), C (WordNet=no, Query=yes), and D (WordNet=yes, Query=yes). We experimented with types A and C only since we believe that having the manually an</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Rosario</author>
<author>Marti Hearst</author>
</authors>
<title>Classifying the semantic relations in noun compounds via a domainspecific lexical hierarchy.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>82--90</pages>
<contexts>
<context position="5558" citStr="Rosario and Hearst (2001)" startWordPosition="845" endWordPosition="848">e best pattern for TEMPORAL (e.g., morning frost). With 5 classes, he achieves F-measure=50.2%. 2.2 Noun-Noun Compound Semantics Lauer (1995) reduces the problem of noun compound interpretation to choosing the best paraphrasing preposition from the following set: of, for, in, at, on, from, with or about. He achieved 40% accuracy using corpus frequencies. This result was improved to 55.7% by Lapata and Keller (2005) who used Web-derived n-gram frequencies. Barker and Szpakowicz (1998) use syntactic clues and the identity of the nouns in a nearest-neighbor classifier, achieving 60-70% accuracy. Rosario and Hearst (2001) used a discriminative classifier to assign 18 relations for noun compounds from biomedical text, achieving 60% accuracy. Rosario et al. (2002) reported 90% accuracy with a “descent of hierarchy” approach which characterizes the relationship between the nouns in a bioscience noun-noun compound based on the MeSH categories the nouns belong to. Girju et al. (2005) apply both classic (SVM and decision trees) and novel supervised models (semantic scattering and iterative semantic specialization), using WordNet, word sense disambiguation, and a set of linguistic features. They test their system aga</context>
</contexts>
<marker>Rosario, Hearst, 2001</marker>
<rawString>Barbara Rosario and Marti Hearst. 2001. Classifying the semantic relations in noun compounds via a domainspecific lexical hierarchy. In Proceedings of EMNLP, pages 82–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Rosario</author>
<author>Marti Hearst</author>
<author>Charles Fillmore</author>
</authors>
<title>The descent of hierarchy, and selection in relational semantics.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>247--254</pages>
<contexts>
<context position="5701" citStr="Rosario et al. (2002)" startWordPosition="866" endWordPosition="869">es the problem of noun compound interpretation to choosing the best paraphrasing preposition from the following set: of, for, in, at, on, from, with or about. He achieved 40% accuracy using corpus frequencies. This result was improved to 55.7% by Lapata and Keller (2005) who used Web-derived n-gram frequencies. Barker and Szpakowicz (1998) use syntactic clues and the identity of the nouns in a nearest-neighbor classifier, achieving 60-70% accuracy. Rosario and Hearst (2001) used a discriminative classifier to assign 18 relations for noun compounds from biomedical text, achieving 60% accuracy. Rosario et al. (2002) reported 90% accuracy with a “descent of hierarchy” approach which characterizes the relationship between the nouns in a bioscience noun-noun compound based on the MeSH categories the nouns belong to. Girju et al. (2005) apply both classic (SVM and decision trees) and novel supervised models (semantic scattering and iterative semantic specialization), using WordNet, word sense disambiguation, and a set of linguistic features. They test their system against both Lauer’s 8 prepositional paraphrases and another set of 21 semantic relations, achieving up to 54% accuracy on the latter. In a previo</context>
</contexts>
<marker>Rosario, Hearst, Fillmore, 2002</marker>
<rawString>Barbara Rosario, Marti Hearst, and Charles Fillmore. 2002. The descent of hierarchy, and selection in relational semantics. In Proceedings of ACL, pages 247– 254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerda Ruge</author>
</authors>
<title>Experiment on linguistically-based term associations.</title>
<date>1992</date>
<journal>Inf. Process. Manage.,</journal>
<volume>28</volume>
<issue>3</issue>
<pages>332</pages>
<contexts>
<context position="8043" citStr="Ruge, 1992" startWordPosition="1243" endWordPosition="1244">ended by Shinyama et al. (2002), who use named entity recognizers and look for anchors belonging to matching semantic classes, e.g., LOCATION, ORGANIZATION. The idea is further extended by Nakov et al. (2004), who apply it in the biomedical domain, imposing the additional restriction that the sentences from which the paraphrases are extracted cite the same target paper. 453 2.4 Word Similarity Another important group of related work is on using syntactic dependency features in a vector-space model for measuring word similarity, e.g., (Alshawi and Carter, 1994), (Grishman and Sterling, 1994), (Ruge, 1992), and (Lin, 1998). For example, given a noun, Lin (1998) extracts verbs that have that noun as a subject or object, and adjectives that modify it. 3 Method Given a pair of nouns, we try to characterize the semantic relation between them by leveraging the vast size of the Web to build linguistically-motivated lexically-specific features. We mine the Web for sentences containing the target nouns, and we extract the connecting verbs, prepositions, and coordinating conjunctions, which we use in a vector-space model to measure relational similarity. The process of extraction starts with exact phras</context>
</contexts>
<marker>Ruge, 1992</marker>
<rawString>Gerda Ruge. 1992. Experiment on linguistically-based term associations. Inf. Process. Manage., 28(3):317– 332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
</authors>
<title>Automatic paraphrase acquisition from news articles.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>313--318</pages>
<contexts>
<context position="7463" citStr="Shinyama et al. (2002)" startWordPosition="1152" endWordPosition="1155"> examples and 19 relations, they achieved 52.6% accuracy with 84 seed verbs, but only 46.7% with 57 seed verbs. 2.3 Paraphrase Acquisition Our method of extraction of paraphrasing verbs and prepositions is similar to previous paraphrase acquisition approaches. Lin and Pantel (2001) extract paraphrases from dependency tree paths whose ends contain semantically similar sets of words by generalizing over these ends. For example, given “X solves Y”, they extract paraphrases like “X finds a solution to Y”, “X tries to solve Y”, “X resolves Y”, “Y is resolved by X”, etc. The approach is extended by Shinyama et al. (2002), who use named entity recognizers and look for anchors belonging to matching semantic classes, e.g., LOCATION, ORGANIZATION. The idea is further extended by Nakov et al. (2004), who apply it in the biomedical domain, imposing the additional restriction that the sentences from which the paraphrases are extracted cite the same target paper. 453 2.4 Word Similarity Another important group of related work is on using syntactic dependency features in a vector-space model for measuring word similarity, e.g., (Alshawi and Carter, 1994), (Grishman and Sterling, 1994), (Ruge, 1992), and (Lin, 1998). F</context>
</contexts>
<marker>Shinyama, Sekine, Sudo, 2002</marker>
<rawString>Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo. 2002. Automatic paraphrase acquisition from news articles. In Proceedings of HLT, pages 313–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Tatu</author>
<author>Dan Moldovan</author>
</authors>
<title>A semantic approach to recognizing textual entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>371--378</pages>
<contexts>
<context position="2816" citStr="Tatu and Moldovan, 2005" startWordPosition="421" endWordPosition="424">l similarity problems, including but not limited to question answering, information retrieval, machine translation, word sense disambiguation, and information extraction. For example, a relational search engine like TextRunner, which serves queries like “find all X such that X causes wrinkles”, asking for all entities that are in a particular relation with a given entity (Cafarella et al., 2006), needs to recognize that laugh wrinkles is an instance of CAUSE-EFFECT. While there are not many success stories so far, measuring semantic similarity has proven its advantages for textual entailment (Tatu and Moldovan, 2005). In this paper, we introduce a novel linguisticallymotivated Web-based approach to relational similarity, which, despite its simplicity, achieves stateof-the-art performance on a number of problems. Following Turney (2006b), we test our approach on SAT verbal analogy questions and on mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER. We further apply it to (1) characterizing noun-noun compounds using abstract linguistic predicates like CAUSE, USE, and FROM, and (2) classifying the relation between pairs of nominals in context. 452 Proceedings of ACL-08: HLT, </context>
</contexts>
<marker>Tatu, Moldovan, 2005</marker>
<rawString>Marta Tatu and Dan Moldovan. 2005. A semantic approach to recognizing textual entailment. In Proceedings of HLT, pages 371–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="9803" citStr="Toutanova et al., 2003" startWordPosition="1539" endWordPosition="1542">n the query changes the set of returned results and their ranking. For each query, we collect the text snippets from the result set (up to 1,000 per query). We split them into sentences, and we filter out all incomplete ones and those that do not contain the target nouns. We further make sure that the word sequence following the second mentioned target noun is nonempty and contains at least one nonnoun, thus ensuring the snippet includes the entire noun phrase: snippets representing incomplete sentences often end with a period anyway. We then perform POS tagging using the Stanford POS tagger (Toutanova et al., 2003) 1JWNL: http://jwordnet.sourceforge.net Freq. Feature POS Direction 2205 of P 2 → 1 1923 be V 1 → 2 771 include V 1 → 2 382 serve on V 2 → 1 189 chair V 2 → 1 189 have V 1 → 2 169 consist of V 1 → 2 148 comprise V 1 → 2 106 sit on V 2 → 1 81 be chaired by V 1 → 2 78 appoint V 1 → 2 77 on P 2 → 1 66 and C 1 → 2 66 be elected V 1 → 2 58 replace V 1 → 2 48 lead V 2 → 1 47 be intended for V 1 → 2 45 join V 2 → 1 � � � � � � � � � � � � 4 be signed up for V 2 → 1 Table 1: The most frequent Web-derived features for committee member. Here V stands for verb (possibly +preposition and/or +particle), P </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Corpus-based learning of analogies and semantic relations.</title>
<date>2005</date>
<journal>Machine Learning Journal,</journal>
<pages>60--1</pages>
<contexts>
<context position="2132" citStr="Turney and Littman, 2005" startWordPosition="317" endWordPosition="320">arity – semantic similarity between pairs of words. Students who took the SAT test before 2005 or who *After January 2008 at the Linguistic Modeling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, nakov@lml.bas.bg are taking the GRE test nowadays are familiar with an instance of this problem – verbal analogy questions, which ask whether, e.g., the relationship between ostrich and bird is more similar to that between lion and cat, or rather between primate and monkey. These analogies are difficult, and the average test taker gives a correct answer 57% of the time (Turney and Littman, 2005). Many NLP applications could benefit from solving relational similarity problems, including but not limited to question answering, information retrieval, machine translation, word sense disambiguation, and information extraction. For example, a relational search engine like TextRunner, which serves queries like “find all X such that X causes wrinkles”, asking for all entities that are in a particular relation with a given entity (Cafarella et al., 2006), needs to recognize that laugh wrinkles is an instance of CAUSE-EFFECT. While there are not many success stories so far, measuring semantic s</context>
<context position="3590" citStr="Turney and Littman (2005)" startWordPosition="531" endWordPosition="534">ateof-the-art performance on a number of problems. Following Turney (2006b), we test our approach on SAT verbal analogy questions and on mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER. We further apply it to (1) characterizing noun-noun compounds using abstract linguistic predicates like CAUSE, USE, and FROM, and (2) classifying the relation between pairs of nominals in context. 452 Proceedings of ACL-08: HLT, pages 452–460, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 2 Related Work 2.1 Characterizing Semantic Relations Turney and Littman (2005) characterize the relationship between two words as a vector with coordinates corresponding to the Web frequencies of 128 fixed phrases like “X for Y ” and “Y for X” instantiated from a fixed set of 64 joining terms like for, such as, not the, is *, etc. These vectors are used in a nearest-neighbor classifier to solve SAT verbal analogy problems, yielding 47% accuracy. The same approach is applied to classifying noun-modifier pairs: using the Diverse dataset of Nastase and Szpakowicz (2003), Turney&amp;Littman achieve F-measures of 26.5% with 30 fine-grained relations, and 43.2% with 5 course-grai</context>
<context position="13492" citStr="Turney and Littman (2005)" startWordPosition="2231" endWordPosition="2234"> of training noun pairs that have that feature, and N is the total number of training noun pairs. Given two nouns and their TF.IDF-weighted frequency vectors A and B, we calculate the similarity between them using the following generalized variant of the Dice coefficient: 2 � En i=1 min(ai� bi) Dice(A� B) = (2) EZ 1 ai + Eni= 1 bi Other variants are also possible, e.g., Lin (1998). 4 Relational Similarity Experiments 4.1 SAT Verbal Analogy Following Turney (2006b), we use SAT verbal analogy as a benchmark problem for relational similarity. We experiment with the 374 SAT questions collected by Turney and Littman (2005). Table 2 shows two sample questions: the top word pairs ostrich:bird palatable:toothsome (a) lion:cat (a) rancid:fragrant (b) goose:flock chewy:textured ewe:sheep (c) coarse:rough (d) cub:bear (d) solitude:company (e) primate:monkey (e) no choice Table 2: SAT verbal analogy: sample questions. The stem is in bold, the correct answer is in italic, and the distractors are in plain text. are called stems, the ones in italic are the solutions, and the remaining ones are distractors. Turney (2006b) achieves 56% accuracy on this dataset, which matches the average human performance of 57%, and repres</context>
<context position="18217" citStr="Turney and Littman (2005)" startWordPosition="3004" endWordPosition="3007">n adverb modifier, e.g., daily exercise, and 262 examples have an adjective modifier, e.g., tiny cloud. We treat them as if the modifier was a noun, which works in many cases, since many adjectives and adverbs can be used predicatively, e.g., ‘This exercise is performed daily.’ or ‘This cloud looks very tiny.’ For the evaluation, we created a feature vector for each head-modifier pair, and we performed a leaveone-out cross-validation: we left one example for testing and we trained on the remaining 599 ones, repeating this procedure 600 times so that each example be used for testing. Following Turney and Littman (2005) we used a 1-nearest-neighbor classifier. We calculated the similarity between the feature vector of the testing example and each of the training examples’ vectors. If there was a unique most similar training example, we predicted its class, and if there were ties, we chose the class predicted by the majority of tied examples, if there was a majority. The results for the 30-class Diverse dataset are shown in Table 4. Our best model achieves 40.5% accuracy, which is slightly better than LRA’s 39.8%, but the difference is not statistically significant. Table 4 shows that the verbs are the most i</context>
</contexts>
<marker>Turney, Littman, 2005</marker>
<rawString>Peter Turney and Michael Littman. 2005. Corpus-based learning of analogies and semantic relations. Machine Learning Journal, 60(1-3):251–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Measuring semantic similarity by latent relational analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1136--1141</pages>
<contexts>
<context position="4218" citStr="Turney (2005)" startWordPosition="636" endWordPosition="637">he relationship between two words as a vector with coordinates corresponding to the Web frequencies of 128 fixed phrases like “X for Y ” and “Y for X” instantiated from a fixed set of 64 joining terms like for, such as, not the, is *, etc. These vectors are used in a nearest-neighbor classifier to solve SAT verbal analogy problems, yielding 47% accuracy. The same approach is applied to classifying noun-modifier pairs: using the Diverse dataset of Nastase and Szpakowicz (2003), Turney&amp;Littman achieve F-measures of 26.5% with 30 fine-grained relations, and 43.2% with 5 course-grained relations. Turney (2005) extends the above approach by introducing the latent relational analysis (LRA), which uses automatically generated synonyms, learns suitable patterns, and performs singular value decomposition in order to smooth the frequencies. The full algorithm consists of 12 steps described in detail in (Turney, 2006b). When applied to SAT questions, it achieves the state-of-the-art accuracy of 56%. On the Diverse dataset, it yields an F-measure of 39.8% with 30 classes, and 58% with 5 classes. Turney (2006a) presents an unsupervised algorithm for mining the Web for patterns expressing implicit semantic r</context>
</contexts>
<marker>Turney, 2005</marker>
<rawString>Peter Turney. 2005. Measuring semantic similarity by latent relational analysis. In Proceedings of IJCAI, pages 1136–1141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Expressing implicit semantic relations without supervision.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>313--320</pages>
<contexts>
<context position="3038" citStr="Turney (2006" startWordPosition="454" endWordPosition="455">serves queries like “find all X such that X causes wrinkles”, asking for all entities that are in a particular relation with a given entity (Cafarella et al., 2006), needs to recognize that laugh wrinkles is an instance of CAUSE-EFFECT. While there are not many success stories so far, measuring semantic similarity has proven its advantages for textual entailment (Tatu and Moldovan, 2005). In this paper, we introduce a novel linguisticallymotivated Web-based approach to relational similarity, which, despite its simplicity, achieves stateof-the-art performance on a number of problems. Following Turney (2006b), we test our approach on SAT verbal analogy questions and on mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER. We further apply it to (1) characterizing noun-noun compounds using abstract linguistic predicates like CAUSE, USE, and FROM, and (2) classifying the relation between pairs of nominals in context. 452 Proceedings of ACL-08: HLT, pages 452–460, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 2 Related Work 2.1 Characterizing Semantic Relations Turney and Littman (2005) characterize the relationship between two words</context>
<context position="4524" citStr="Turney, 2006" startWordPosition="682" endWordPosition="683">T verbal analogy problems, yielding 47% accuracy. The same approach is applied to classifying noun-modifier pairs: using the Diverse dataset of Nastase and Szpakowicz (2003), Turney&amp;Littman achieve F-measures of 26.5% with 30 fine-grained relations, and 43.2% with 5 course-grained relations. Turney (2005) extends the above approach by introducing the latent relational analysis (LRA), which uses automatically generated synonyms, learns suitable patterns, and performs singular value decomposition in order to smooth the frequencies. The full algorithm consists of 12 steps described in detail in (Turney, 2006b). When applied to SAT questions, it achieves the state-of-the-art accuracy of 56%. On the Diverse dataset, it yields an F-measure of 39.8% with 30 classes, and 58% with 5 classes. Turney (2006a) presents an unsupervised algorithm for mining the Web for patterns expressing implicit semantic relations. For example, CAUSE (e.g., cold virus) is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL (e.g., morning frost). With 5 classes, he achieves F-measure=50.2%. 2.2 Noun-Noun Compound Semantics Lauer (1995) reduces the problem of noun compound interpretati</context>
<context position="13333" citStr="Turney (2006" startWordPosition="2206" endWordPosition="2207"> (1) DF(x) In the above formula, TF(x) is the number of times the feature x has been extracted for the target noun pair, DF(x) is the total number of training noun pairs that have that feature, and N is the total number of training noun pairs. Given two nouns and their TF.IDF-weighted frequency vectors A and B, we calculate the similarity between them using the following generalized variant of the Dice coefficient: 2 � En i=1 min(ai� bi) Dice(A� B) = (2) EZ 1 ai + Eni= 1 bi Other variants are also possible, e.g., Lin (1998). 4 Relational Similarity Experiments 4.1 SAT Verbal Analogy Following Turney (2006b), we use SAT verbal analogy as a benchmark problem for relational similarity. We experiment with the 374 SAT questions collected by Turney and Littman (2005). Table 2 shows two sample questions: the top word pairs ostrich:bird palatable:toothsome (a) lion:cat (a) rancid:fragrant (b) goose:flock chewy:textured ewe:sheep (c) coarse:rough (d) cub:bear (d) solitude:company (e) primate:monkey (e) no choice Table 2: SAT verbal analogy: sample questions. The stem is in bold, the correct answer is in italic, and the distractors are in plain text. are called stems, the ones in italic are the solution</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter Turney. 2006a. Expressing implicit semantic relations without supervision. In Proceedings of ACL, pages 313–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="3038" citStr="Turney (2006" startWordPosition="454" endWordPosition="455">serves queries like “find all X such that X causes wrinkles”, asking for all entities that are in a particular relation with a given entity (Cafarella et al., 2006), needs to recognize that laugh wrinkles is an instance of CAUSE-EFFECT. While there are not many success stories so far, measuring semantic similarity has proven its advantages for textual entailment (Tatu and Moldovan, 2005). In this paper, we introduce a novel linguisticallymotivated Web-based approach to relational similarity, which, despite its simplicity, achieves stateof-the-art performance on a number of problems. Following Turney (2006b), we test our approach on SAT verbal analogy questions and on mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER. We further apply it to (1) characterizing noun-noun compounds using abstract linguistic predicates like CAUSE, USE, and FROM, and (2) classifying the relation between pairs of nominals in context. 452 Proceedings of ACL-08: HLT, pages 452–460, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 2 Related Work 2.1 Characterizing Semantic Relations Turney and Littman (2005) characterize the relationship between two words</context>
<context position="4524" citStr="Turney, 2006" startWordPosition="682" endWordPosition="683">T verbal analogy problems, yielding 47% accuracy. The same approach is applied to classifying noun-modifier pairs: using the Diverse dataset of Nastase and Szpakowicz (2003), Turney&amp;Littman achieve F-measures of 26.5% with 30 fine-grained relations, and 43.2% with 5 course-grained relations. Turney (2005) extends the above approach by introducing the latent relational analysis (LRA), which uses automatically generated synonyms, learns suitable patterns, and performs singular value decomposition in order to smooth the frequencies. The full algorithm consists of 12 steps described in detail in (Turney, 2006b). When applied to SAT questions, it achieves the state-of-the-art accuracy of 56%. On the Diverse dataset, it yields an F-measure of 39.8% with 30 classes, and 58% with 5 classes. Turney (2006a) presents an unsupervised algorithm for mining the Web for patterns expressing implicit semantic relations. For example, CAUSE (e.g., cold virus) is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL (e.g., morning frost). With 5 classes, he achieves F-measure=50.2%. 2.2 Noun-Noun Compound Semantics Lauer (1995) reduces the problem of noun compound interpretati</context>
<context position="13333" citStr="Turney (2006" startWordPosition="2206" endWordPosition="2207"> (1) DF(x) In the above formula, TF(x) is the number of times the feature x has been extracted for the target noun pair, DF(x) is the total number of training noun pairs that have that feature, and N is the total number of training noun pairs. Given two nouns and their TF.IDF-weighted frequency vectors A and B, we calculate the similarity between them using the following generalized variant of the Dice coefficient: 2 � En i=1 min(ai� bi) Dice(A� B) = (2) EZ 1 ai + Eni= 1 bi Other variants are also possible, e.g., Lin (1998). 4 Relational Similarity Experiments 4.1 SAT Verbal Analogy Following Turney (2006b), we use SAT verbal analogy as a benchmark problem for relational similarity. We experiment with the 374 SAT questions collected by Turney and Littman (2005). Table 2 shows two sample questions: the top word pairs ostrich:bird palatable:toothsome (a) lion:cat (a) rancid:fragrant (b) goose:flock chewy:textured ewe:sheep (c) coarse:rough (d) cub:bear (d) solitude:company (e) primate:monkey (e) no choice Table 2: SAT verbal analogy: sample questions. The stem is in bold, the correct answer is in italic, and the distractors are in plain text. are called stems, the ones in italic are the solution</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter Turney. 2006b. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>