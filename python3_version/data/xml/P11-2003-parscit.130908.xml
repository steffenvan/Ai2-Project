<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000413">
<title confidence="0.991293">
Temporal Restricted Boltzmann Machines for Dependency Parsing
</title>
<author confidence="0.995545">
Nikhil Garg
</author>
<affiliation confidence="0.999002">
Department of Computer Science
University of Geneva
</affiliation>
<address confidence="0.631168">
Switzerland
</address>
<email confidence="0.997046">
nikhil.garg@unige.ch
</email>
<author confidence="0.990876">
James Henderson
</author>
<affiliation confidence="0.9989985">
Department of Computer Science
University of Geneva
</affiliation>
<address confidence="0.630823">
Switzerland
</address>
<email confidence="0.996618">
james.henderson@unige.ch
</email>
<sectionHeader confidence="0.995595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999894692307692">
We propose a generative model based on
Temporal Restricted Boltzmann Machines for
transition based dependency parsing. The
parse tree is built incrementally using a shift-
reduce parse and an RBM is used to model
each decision step. The RBM at the current
time step induces latent features with the help
of temporal connections to the relevant previ-
ous steps which provide context information.
Our parser achieves labeled and unlabeled at-
tachment scores of 88.72% and 91.65% re-
spectively, which compare well with similar
previous models and the state-of-the-art.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972565217392">
There has been significant interest recently in ma-
chine learning methods that induce generative mod-
els with high-dimensional hidden representations,
including neural networks (Bengio et al., 2003; Col-
lobert and Weston, 2008), Bayesian networks (Titov
and Henderson, 2007a), and Deep Belief Networks
(Hinton et al., 2006). In this paper, we investi-
gate how these models can be applied to dependency
parsing. We focus on Shift-Reduce transition-based
parsing proposed by Nivre et al. (2004). In this class
of algorithms, at any given step, the parser has to
choose among a set of possible actions, each repre-
senting an incremental modification to the partially
built tree. To assign probabilities to these actions,
previous work has proposed memory-based classi-
fiers (Nivre et al., 2004), SVMs (Nivre et al., 2006b),
and Incremental Sigmoid Belief Networks (ISBN)
(Titov and Henderson, 2007b). In a related earlier
work, Ratnaparkhi (1999) proposed a maximum en-
tropy model for transition-based constituency pars-
ing. Of these approaches, only ISBNs induce high-
dimensional latent representations to encode parse
history, but suffer from either very approximate or
slow inference procedures.
We propose to address the problem of inference
in a high-dimensional latent space by using an undi-
rected graphical model, Restricted Boltzmann Ma-
chines (RBMs), to model the individual parsing
decisions. Unlike the Sigmoid Belief Networks
(SBNs) used in ISBNs, RBMs have tractable infer-
ence procedures for both forward and backward rea-
soning, which allows us to efficiently infer both the
probability of the decision given the latent variables
and vice versa. The key structural difference be-
tween the two models is that the directed connec-
tions between latent and decision vectors in SBNs
become undirected in RBMs. A complete parsing
model consists of a sequence of RBMs interlinked
via directed edges, which gives us a form of Tempo-
ral Restricted Boltzmann Machines (TRBM) (Tay-
lor et al., 2007), but with the incrementally speci-
fied model structure required by parsing. In this pa-
per, we analyze and contrast ISBNs with TRBMs
and show that the latter provide an accurate and
theoretically sound model for parsing with high-
dimensional latent variables.
</bodyText>
<sectionHeader confidence="0.972589" genericHeader="method">
2 An ISBN Parsing Model
</sectionHeader>
<bodyText confidence="0.9377215">
Our TRBM parser uses the same history-
based probability model as the ISBN
parser of Titov and Henderson (2007b):
P(tree) = HtP(vtjv1, ,vt−1), where each
</bodyText>
<page confidence="0.991897">
11
</page>
<note confidence="0.891296">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 11–17,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figureCaption confidence="0.947508">
Figure 1: An ISBN network. Shaded nodes represent
decision variables and ‘H’ represents a vector of latent
variables. W���
</figureCaption>
<bodyText confidence="0.999904652173913">
�� denotes the weight matrix for directed
connection of type c between two latent vectors.
vt is a parser decision of the type Left-Arc,
Right-Arc, Reduce or Shift. These decisions are fur-
ther decomposed into sub-decisions, as for example
P(Left-Arc|v1, ..., vt−1)P(Label|Left-Arc, v1, ..., vt−1).
The TRBMs and ISBNs model these probabilities.
In the ISBN model shown in Figure 1, the de-
cisions are shown as boxes and the sub-decisions
as shaded circles. At each decision step, the ISBN
model also includes a vector of latent variables, de-
noted by ‘H’, which act as latent features of the
parse history. As explained in (Titov and Hender-
son, 2007b), the temporal connections between la-
tent variables are constructed to take into account the
structural locality in the partial dependency struc-
ture. The model parameters are learned by back-
propagating likelihood gradients.
Because decision probabilities are conditioned on
the history, once a decision is made the correspond-
ing variable becomes observed, or visible. In an
ISBN, the directed edges to these visible variables
and the large numbers of heavily inter-connected la-
tent variables make exact inference of decision prob-
abilities intractable. Titov and Henderson (2007a)
proposed two approximation procedures for infer-
ence. The first was a feed forward approximation
where latent variables were allowed to depend only
on their parent variables, and hence did not take into
account the current or future observations. Due to
this limitation, the authors proposed to make latent
variables conditionally dependent also on a set of
explicit features derived from the parsing history,
specifically, the base features defined in (Nivre et al.,
2006b). As shown in our experiments, this addition
results in a big improvement for the parsing task.
The second approximate inference procedure,
called the incremental mean field approximation, ex-
tended the feed-forward approximation by updating
the current time step’s latent variables after each
sub-decision. Although this approximation is more
accurate than the feed-forward one, there is no ana-
lytical way to maximize likelihood w.r.t. the means
of the latent variables, which requires an iterative
numerical method and thus makes inference very
slow, restricting the model to only shorter sentences.
</bodyText>
<sectionHeader confidence="0.930449" genericHeader="method">
3 Temporal Restricted Boltzmann
Machines
</sectionHeader>
<bodyText confidence="0.999898">
In the proposed TRBM model, RBMs provide an an-
alytical way to do exact inference within each time
step. Although information passing between time
steps is still approximated, TRBM inference is more
accurate than the ISBN approximations.
</bodyText>
<subsectionHeader confidence="0.996494">
3.1 Restricted Boltzmann Machines (RBM)
</subsectionHeader>
<bodyText confidence="0.943910923076923">
An RBM is an undirected graphical model with a
set of binary visible variables v, a set of binary la-
tent variables h, and a weight matrix W for bipar-
tite connections between v and h. The probability
of an RBM configuration is given by: p(v, h) =
(1/Z)e−E(v,h) where Z is the partition function and
E is the energy function defined as:
E(v, h) = −Eiaivi − Ejbjhj − Ei,jvihjwij
where ai and bj are biases for corresponding visi-
ble and latent variables respectively, and wij is the
symmetric weight between vi and hj. Given the vis-
ible variables, the latent variables are conditionally
independent of each other, and vice versa:
</bodyText>
<equation confidence="0.9999695">
p(hj = 1|v) = Q(bj + Eiviwij) (1)
p(vi = 1|h) = Q(ai + Ejhjwij) (2)
</equation>
<bodyText confidence="0.999331909090909">
where Q(x) = 1/(1 + e−x) (the logistic sigmoid).
RBM based models have been successfully used
in image and video processing, such as Deep Belief
Networks (DBNs) for recognition of hand-written
digits (Hinton et al., 2006) and TRBMs for mod-
eling motion capture data (Taylor et al., 2007). De-
spite their success, RBMs have seen limited use in
the NLP community. Previous work includes RBMs
for topic modeling in text documents (Salakhutdinov
and Hinton, 2009), and Temporal Factored RBM for
language modeling (Mnih and Hinton, 2007).
</bodyText>
<subsectionHeader confidence="0.993767">
3.2 Proposed TRBM Model Structure
</subsectionHeader>
<bodyText confidence="0.999775">
TRBMs (Taylor et al., 2007) can be used to model
sequences where the decision at each step requires
some context information from the past. Figure 2
</bodyText>
<page confidence="0.994886">
12
</page>
<figureCaption confidence="0.8952475">
Figure 2: Proposed TRBM Model. Edges with no arrows
represent undirected RBM connections. The directed
temporal connections between time steps contribute a
bias to the latent layer inference in the current step.
</figureCaption>
<subsectionHeader confidence="0.999733">
3.3 TRBM Likelihood and Inference
</subsectionHeader>
<bodyText confidence="0.9999158">
Section 3.1 describes an RBM where visible vari-
ables can take binary values. In our model, similar to
(Salakhutdinov et al., 2007), we have multi-valued
visible variables which we represent as one-hot bi-
nary vectors and model via a softmax distribution:
</bodyText>
<equation confidence="0.8988245">
exp(ak + Ej htjwkj)
= Ei exp(ai + Ej htjwij) (3)
</equation>
<bodyText confidence="0.999375">
Latent variable inference is similar to equation 1
with an additional bias due to the temporal connec-
tions.
</bodyText>
<equation confidence="0.9997636">
µtj = p(htj = 1|vt, historyt)
= (u(bj + Ec,lw(c)
HHljhlc) + Eivtiwij))
N u(b′j + Eivtiwij),
b′j = bj + Ec,lw(c)
</equation>
<bodyText confidence="0.970971357142857">
HHljµlc).
Here, µ denotes the mean of the corresponding la-
tent variable. To keep inference tractable, we do not
do any backward reasoning across directed connec-
tions to update µ(c). Thus, the inference procedure
for latent variables takes into account both the parse
history and the current observation, but no future ob-
servations.
The limited set of possible values for the visi-
ble layer makes it possible to marginalize out latent
variables in linear time to compute the exact likeli-
hood. Let vt(k) denote a vector with vtk = 1 and
vti(i�=k) = 0. The conditional probability of a sub-
decision is:
</bodyText>
<equation confidence="0.999607">
p(vt(k)|historyt) = (1/Z)Ehte−E(vt(k),ht) (5)
= (1/Z)eakHj(1 + eb′j+wkj),
</equation>
<bodyText confidence="0.999197111111111">
where Z = EiEvisibleeaiHjElatent(1 + eb′j+wij).
We actually perform this calculation once for
each sub-decision, ignoring the future sub-decisions
in that time step. This is a slight approximation,
but avoids having to compute the partition function
over all possible combinations of values for all sub-
decisions.1
The complete probability of a derivation is:
p(vT1 ) = p(v1).p(v2|history2)... p(vT|historyT)
</bodyText>
<subsectionHeader confidence="0.901104">
3.4 TRBM Training
</subsectionHeader>
<bodyText confidence="0.988521">
The gradient of an RBM is given by:
</bodyText>
<equation confidence="0.660059">
a log p(v)/awij = (vihj)data − (vihj)model (6)
</equation>
<bodyText confidence="0.999989444444444">
where ()d denotes the expectation under distribu-
tion d. In general, computing the exact gradient
is intractable and previous work proposed a Con-
trastive Divergence (CD) based learning procedure
that approximates the above gradient using only one
step reconstruction (Hinton, 2002). Fortunately, our
model has only a limited set of possible visible val-
ues, which allows us to use a better approximation
by taking the derivative of equation 5:
</bodyText>
<footnote confidence="0.41197025">
1In cases where computing the partition function is still not
feasible (for instance, because of a large vocabulary), sampling
methods could be used. However, we did not find this to be
necessary.
</footnote>
<bodyText confidence="0.9997873">
shows our proposed TRBM model with latent to
latent connections between time steps. Each step
has an RBM with weights WRBM composed of
smaller weight matrices corresponding to different
sub-decisions. For instance, for the action Left-Arc,
WRBM consists of RBM weights between the la-
tent vector and the sub-decisions: “Left-Arc” and
“Label”. Similarly, for the action Shift, the sub-
decisions are “Shift”, “Part-of-Speech” and “Word”.
The probability distribution of a TRBM is:
</bodyText>
<equation confidence="0.9859205">
p(T T T t t 1 C
v1 ,h1 ) = Ht=1p(v , h �h , ..., h )
</equation>
<bodyText confidence="0.997739888888889">
where vT1 denotes the set of visible vectors from time
steps 1 to T i.e. v1 to vT. The notation for latent
vectors h is similar. h(c) denotes the latent vector
in the past time step that is connected to the current
latent vector through a connection of type c. To sim-
plify notation, we will denote the past connections
1h(1), ..., h(C)} by historyt. The conditional distri-
bution of the RBM at each time step is given by:
p(vt, ht|historyt) = (1/Z)exp(Eiaivti + Ei,jvtiht jwij
</bodyText>
<equation confidence="0.8455195">
+ Ej(bj + Ec,lw(c)
HHljhlc))htj
</equation>
<bodyText confidence="0.940594">
where vti and ht j denote the ith visible and jth latent
variable respectively at time step t. h(c)
l denotes a
</bodyText>
<equation confidence="0.8227877">
latent variable in the past time step, and w(c)de-
HHlj
notes the weight of the corresponding connection.
p(vtk = 1|ht)
(4)
13
a log p(vt(k)|historyt)
�
awij (7)
(Ski − p(vt(i)|historyt)) u(b′j + wij)
</equation>
<bodyText confidence="0.999938333333333">
Further, the weights on the temporal connections
are learned by back-propagating the likelihood gra-
dients through the directed links between steps.
The back-proped gradient from future time steps is
also used to train the current RBM weights. This
back-propagation is similar to the Recurrent TRBM
model of Sutskever et al. (2008). However, unlike
their model, we do not use CD at each step to com-
pute gradients.
</bodyText>
<subsectionHeader confidence="0.731611">
3.5 Prediction
</subsectionHeader>
<bodyText confidence="0.999992272727273">
We use the same beam-search decoding strategy as
used in (Titov and Henderson, 2007b). Given a
derivation prefix, its partial parse tree and associ-
ated TRBM, the decoder adds a step to the TRBM
for calculating the probabilities of hypothesized next
decisions using equation 5. If the decoder selects a
decision for addition to the candidate list, then the
current step’s latent variable means are inferred us-
ing equation 4, given that the chosen decision is now
visible. These means are then stored with the new
candidate for use in subsequent TRBM calculations.
</bodyText>
<sectionHeader confidence="0.997715" genericHeader="evaluation">
4 Experiments &amp; Results
</sectionHeader>
<bodyText confidence="0.9999716">
We used syntactic dependencies from the English
section of the CoNLL 2009 shared task dataset
(Hajiˇc et al., 2009). Standard splits of training, de-
velopment and test sets were used. To handle word
sparsity, we replaced all the (POS, word) pairs with
frequency less than 20 in the training set with (POS,
UNKNOWN), giving us only 4530 tag-word pairs.
Since our model can work only with projective trees,
we used MaltParser (Nivre et al., 2006a) to projec-
tivize/deprojectivize the training input/test output.
</bodyText>
<subsectionHeader confidence="0.510713">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.999729875">
Table 1 lists the labeled (LAS) and unlabeled (UAS)
attachment scores. Row a shows that a simple ISBN
model without features, using feed forward infer-
ence procedure, does not work well. As explained
in section 2, this is expected since in the absence of
explicit features, the latent variables in a given layer
do not take into account the observations in the pre-
vious layers. The huge improvement in performance
</bodyText>
<table confidence="0.892107636363636">
Model LAS UAS
ISBN w/o features 38.38 54.52
ISBN w/ features 88.65 91.44
TRBM w/o features 86.01 89.78
TRBM w/ features 88.72 91.65
MST (McDonald et al., 2005) 87.07 89.95
Malt−→AE (Hall et al., 2007) 85.96 88.64
MSTMalt (Nivre and McDonald, 2008) 87.45 90.22
CoNLL 2008 #1 (Johansson and Nugues, 2008) 90.13 92.45
ensemble100% (Surdeanu and Manning, 2010) 88.83 91.47
CoNLL 2009 #1 (Bohnet, 2009) 89.88 unknown
</table>
<tableCaption confidence="0.999578">
Table 1: LAS and UAS for different models.
</tableCaption>
<bodyText confidence="0.999969088235294">
on adding the features (row b) shows that the feed
forward inference procedure for ISBNs relies heav-
ily on these feature connections to compensate for
the lack of backward inference.
The TRBM model avoids this problem as the in-
ference procedure takes into account the current ob-
servation, which makes the latent variables much
more informed. However, as row c shows, the
TRBM model without features falls a bit short of
the ISBN performance, indicating that features are
indeed a powerful substitute for backward inference
in sequential latent variable models. TRBM mod-
els would still be preferred in cases where such fea-
ture engineering is difficult or expensive, or where
the objective is to compute the latent features them-
selves. For a fair comparison, we add the same set
of features to the TRBM model (row d) and the per-
formance improves by about 2% to reach the same
level (non-significantly better) as ISBN with fea-
tures. The improved inference in TRBM does how-
ever come at the cost of increased training and test-
ing time. Keeping the same likelihood convergence
criteria, we could train the ISBN in about 2 days and
TRBM in about 5 days on a 3.3 GHz Xeon proces-
sor. With the same beam search parameters, the test
time was about 1.5 hours for ISBN and about 4.5
hours for TRBM. Although more code optimization
is possible, this trend is likely to remain.
We also tried a Contrastive Divergence based
training procedure for TRBM instead of equation
7, but that resulted in about an absolute 10% lower
LAS. Further, we also tried a very simple model
without latent variables where temporal connections
are between decision variables themselves. This
</bodyText>
<page confidence="0.997762">
14
</page>
<bodyText confidence="0.999943214285714">
model gave an LAS of only 60.46%, which indi-
cates that without latent variables, it is very difficult
to capture the parse history.
For comparison, we also include the performance
numbers for some state-of-the-art dependency pars-
ing systems. Surdeanu and Manning (2010) com-
pare different parsing models using CoNLL 2008
shared task dataset (Surdeanu et al., 2008), which
is the same as our dataset. Rows e − i show the per-
formance numbers of some systems as mentioned in
their paper. Row j shows the best syntactic model
in CoNLL 2009 shared task. The TRBM model has
only 1.4% lower LAS and 0.8% lower UAS com-
pared to the best performing model.
</bodyText>
<subsectionHeader confidence="0.999092">
4.2 Latent Layer Analysis
</subsectionHeader>
<bodyText confidence="0.999995615384615">
We analyzed the latent layers in our models to see if
they captured semantic patterns. A latent layer is a
vector of 100 latent variables. Every Shift operation
gives a latent representation for the corresponding
word. We took all the verbs in the development set2
and partitioned their representations into 50 clus-
ters using the k-means algorithm. Table 2 shows
some partitions for the TRBM model. The partitions
look semantically meaningful but to get a quantita-
tive analysis, we computed pairwise semantic simi-
larity between all word pairs in a given cluster and
aggregated this number over all the clusters. The se-
mantic similarity was calculated using two different
similarity measures on the wordnet corpus (Miller
et al., 1990): path and lin. path similarity is a score
between 0 and 1, equal to the inverse of the shortest
path length between the two word senses. lin simi-
larity (Lin, 1998) is a score between 0 and 1 based
on the Information Content of the two word senses
and of the Least Common Subsumer. Table 3 shows
the similarity scores.3 We observe that TRBM la-
tent representations give a slightly better clustering
than ISBN models. Again, this is because of the fact
that the inference procedure in TRBMs takes into ac-
count the current observation. However, at the same
time, the similarity numbers for ISBN with features
</bodyText>
<footnote confidence="0.977553166666667">
2Verbs are words corresponding to POS tags: VB, VBD,
VBG, VBN, VBP, VBZ. We selected verbs as they have good
coverage in Wordnet.
3To account for randomness in k-means clustering, the clus-
tering was performed 10 times with random initializations, sim-
ilarity scores were computed for each run and a mean was taken.
</footnote>
<table confidence="0.994843">
Cluster 1 Cluster 2 Cluster 3 Cluster 4
says needed pressing renewing
contends expected bridging cause
adds encouraged curing repeat
insists allowed skirting broken
remarked thought tightening extended
</table>
<tableCaption confidence="0.945202">
Table 2: K-means clustering of words according to their
TRBM latent representations. Duplicate words in the
same cluster are not shown.
</tableCaption>
<table confidence="0.9992406">
Model path lin
ISBN w/o features 0.228 0.381
ISBN w/features 0.366 0.466
TRBM w/o features 0.386 0.487
TRBM w/ features 0.390 0.489
</table>
<tableCaption confidence="0.994759">
Table 3: Wordnet similarity scores for clusters given by
different models.
</tableCaption>
<bodyText confidence="0.9989565">
are not very low, which shows that features are a
powerful way to compensate for the lack of back-
ward inference. This is in agreement with their good
performance on the parsing task.
</bodyText>
<sectionHeader confidence="0.998919" genericHeader="conclusions">
5 Conclusions &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999932294117647">
We have presented a Temporal Restricted Boltz-
mann Machines based model for dependency pars-
ing. The model shows how undirected graphical
models can be used to generate latent representa-
tions of local parsing actions, which can then be
used as features for later decisions.
The TRBM model for dependency parsing could
be extended to a Deep Belief Network by adding
one more latent layer on top of the existing one
(Hinton et al., 2006). Furthermore, as done for
unlabeled images (Hinton et al., 2006), one could
learn high-dimensional features from unlabeled text,
which could then be used to aid parsing. Parser la-
tent representations could also help other tasks such
as Semantic Role Labeling (Henderson et al., 2008).
A free distribution of our implementation is avail-
able at http://cui.unige.ch/˜garg.
</bodyText>
<sectionHeader confidence="0.998314" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.970036333333333">
This work was partly funded by Swiss NSF grant
200021 125137 and European Community FP7
grant 216594 (CLASSiC, www.classic-project.org).
</bodyText>
<page confidence="0.996993">
15
</page>
<sectionHeader confidence="0.990008" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999157349056604">
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003.
A neural probabilistic language model. The Journal of
Machine Learning Research, 3:1137–1155.
B. Bohnet. 2009. Efficient parsing of syntactic and
semantic dependency structures. In Proceedings of
the Thirteenth Conference on Computational Natural
Language Learning: Shared Task, CoNLL ’09, pages
67–72. Association for Computational Linguistics.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In Proceedings of the 25th
international conference on Machine learning, pages
160–167. ACM.
J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara, M.A.
Marti, L. M`arquez, A. Meyers, J. Nivre, S. Pad´o,
J. ˇStˇep´anek, et al. 2009. The CoNLL-2009 shared
task: Syntactic and semantic dependencies in multiple
languages. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 1–18. Association for Computa-
tional Linguistics.
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi,
M. Nilsson, and M. Saers. 2007. Single malt or
blended? A study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task Ses-
sion ofEMNLP-CoNLL 2007, pages 933–939. Associ-
ation for Computational Linguistics.
J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008.
A latent variable model of synchronous parsing for
syntactic and semantic dependencies. In Proceedings
of the Twelfth Conference on Computational Natural
Language Learning, pages 178–182. Association for
Computational Linguistics.
G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast
learning algorithm for deep belief nets. Neural com-
putation, 18(7):1527–1554.
G.E. Hinton. 2002. Training products of experts by min-
imizing contrastive divergence. Neural Computation,
14(8):1771–1800.
R. Johansson and P. Nugues. 2008. Dependency-
based syntactic-semantic analysis with PropBank and
NomBank. In Proceedings of the Twelfth Conference
on Computational Natural Language Learning, pages
183–187. Association for Computational Linguistics.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, volume 1, pages
296–304.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, pages 523–530. As-
sociation for Computational Linguistics.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K.J. Miller. 1990. Introduction to wordnet: An on-
line lexical database. International Journal of lexicog-
raphy, 3(4):235.
A. Mnih and G. Hinton. 2007. Three new graphical mod-
els for statistical language modelling. In Proceedings
ofthe 24th international conference on Machine learn-
ing, pages 641–648. ACM.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. Pro-
ceedings ofACL-08: HLT, pages 950–958.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49–56.
J. Nivre, J. Hall, and J. Nilsson. 2006a. MaltParser: A
data-driven parser-generator for dependency parsing.
In Proceedings ofLREC, volume 6.
J. Nivre, J. Hall, J. Nilsson, G. Eryiit, and S. Marinov.
2006b. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning, pages 221–225. Association for
Computational Linguistics.
A. Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1):151–175.
R. Salakhutdinov and G. Hinton. 2009. Replicated soft-
max: an undirected topic model. Advances in Neural
Information Processing Systems, 22.
R. Salakhutdinov, A. Mnih, and G. Hinton. 2007. Re-
stricted Boltzmann machines for collaborative filter-
ing. In Proceedings of the 24th international confer-
ence on Machine learning, page 798. ACM.
M. Surdeanu and C.D. Manning. 2010. Ensemble mod-
els for dependency parsing: cheap and good? In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 649–652.
Association for Computational Linguistics.
M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
In Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, pages 159–177.
Association for Computational Linguistics.
I. Sutskever, G. Hinton, and G. Taylor. 2008. The recur-
rent temporal restricted boltzmann machine. In NIPS,
volume 21, page 2008.
G.W. Taylor, G.E. Hinton, and S.T. Roweis. 2007.
Modeling human motion using binary latent variables.
Advances in neural information processing systems,
19:1345.
</reference>
<page confidence="0.972216">
16
</page>
<reference confidence="0.97348475">
I. Titov and J. Henderson. 2007a. Constituent parsing
with incremental sigmoid belief networks. In Pro-
ceedings of the 45th Annual Meeting on Association
for Computational Linguistics, volume 45, page 632.
I. Titov and J. Henderson. 2007b. Fast and robust mul-
tilingual dependency parsing with a generative latent
variable model. In Proceedings of the CoNLL Shared
Task Session ofEMNLP-CoNLL, pages 947–951.
</reference>
<page confidence="0.999398">
17
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.234517">
<title confidence="0.99662">Temporal Restricted Boltzmann Machines for Dependency Parsing</title>
<author confidence="0.622137">Nikhil</author>
<affiliation confidence="0.9995165">Department of Computer University of</affiliation>
<email confidence="0.473323">nikhil.garg@unige.ch</email>
<author confidence="0.973577">James</author>
<affiliation confidence="0.9995035">Department of Computer University of</affiliation>
<email confidence="0.838442">james.henderson@unige.ch</email>
<abstract confidence="0.998479857142857">We propose a generative model based on Temporal Restricted Boltzmann Machines for transition based dependency parsing. The parse tree is built incrementally using a shiftreduce parse and an RBM is used to model each decision step. The RBM at the current time step induces latent features with the help of temporal connections to the relevant previous steps which provide context information. Our parser achieves labeled and unlabeled attachment scores of 88.72% and 91.65% respectively, which compare well with similar previous models and the state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1044" citStr="Bengio et al., 2003" startWordPosition="143" endWordPosition="146">ally using a shiftreduce parse and an RBM is used to model each decision step. The RBM at the current time step induces latent features with the help of temporal connections to the relevant previous steps which provide context information. Our parser achieves labeled and unlabeled attachment scores of 88.72% and 91.65% respectively, which compare well with similar previous models and the state-of-the-art. 1 Introduction There has been significant interest recently in machine learning methods that induce generative models with high-dimensional hidden representations, including neural networks (Bengio et al., 2003; Collobert and Weston, 2008), Bayesian networks (Titov and Henderson, 2007a), and Deep Belief Networks (Hinton et al., 2006). In this paper, we investigate how these models can be applied to dependency parsing. We focus on Shift-Reduce transition-based parsing proposed by Nivre et al. (2004). In this class of algorithms, at any given step, the parser has to choose among a set of possible actions, each representing an incremental modification to the partially built tree. To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Ni</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
</authors>
<title>Efficient parsing of syntactic and semantic dependency structures.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, CoNLL ’09,</booktitle>
<pages>67--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13907" citStr="Bohnet, 2009" startWordPosition="2227" endWordPosition="2228">d in section 2, this is expected since in the absence of explicit features, the latent variables in a given layer do not take into account the observations in the previous layers. The huge improvement in performance Model LAS UAS ISBN w/o features 38.38 54.52 ISBN w/ features 88.65 91.44 TRBM w/o features 86.01 89.78 TRBM w/ features 88.72 91.65 MST (McDonald et al., 2005) 87.07 89.95 Malt−→AE (Hall et al., 2007) 85.96 88.64 MSTMalt (Nivre and McDonald, 2008) 87.45 90.22 CoNLL 2008 #1 (Johansson and Nugues, 2008) 90.13 92.45 ensemble100% (Surdeanu and Manning, 2010) 88.83 91.47 CoNLL 2009 #1 (Bohnet, 2009) 89.88 unknown Table 1: LAS and UAS for different models. on adding the features (row b) shows that the feed forward inference procedure for ISBNs relies heavily on these feature connections to compensate for the lack of backward inference. The TRBM model avoids this problem as the inference procedure takes into account the current observation, which makes the latent variables much more informed. However, as row c shows, the TRBM model without features falls a bit short of the ISBN performance, indicating that features are indeed a powerful substitute for backward inference in sequential laten</context>
</contexts>
<marker>Bohnet, 2009</marker>
<rawString>B. Bohnet. 2009. Efficient parsing of syntactic and semantic dependency structures. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, CoNLL ’09, pages 67–72. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1073" citStr="Collobert and Weston, 2008" startWordPosition="147" endWordPosition="151">uce parse and an RBM is used to model each decision step. The RBM at the current time step induces latent features with the help of temporal connections to the relevant previous steps which provide context information. Our parser achieves labeled and unlabeled attachment scores of 88.72% and 91.65% respectively, which compare well with similar previous models and the state-of-the-art. 1 Introduction There has been significant interest recently in machine learning methods that induce generative models with high-dimensional hidden representations, including neural networks (Bengio et al., 2003; Collobert and Weston, 2008), Bayesian networks (Titov and Henderson, 2007a), and Deep Belief Networks (Hinton et al., 2006). In this paper, we investigate how these models can be applied to dependency parsing. We focus on Shift-Reduce transition-based parsing proposed by Nivre et al. (2004). In this class of algorithms, at any given step, the parser has to choose among a set of possible actions, each representing an incremental modification to the partially built tree. To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006b), and Incre</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>M Ciaramita</author>
<author>R Johansson</author>
<author>D Kawahara</author>
<author>M A Marti</author>
<author>L M`arquez</author>
<author>A Meyers</author>
<author>J Nivre</author>
<author>S Pad´o</author>
<author>J ˇStˇep´anek</author>
</authors>
<title>The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Marti, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, 2009</marker>
<rawString>J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara, M.A. Marti, L. M`arquez, A. Meyers, J. Nivre, S. Pad´o, J. ˇStˇep´anek, et al. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–18. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hall</author>
<author>J Nilsson</author>
<author>J Nivre</author>
<author>G Eryigit</author>
<author>B Megyesi</author>
<author>M Nilsson</author>
<author>M Saers</author>
</authors>
<title>Single malt or blended? A study in multilingual parser optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL</booktitle>
<pages>933--939</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13710" citStr="Hall et al., 2007" startWordPosition="2195" endWordPosition="2198">ults Table 1 lists the labeled (LAS) and unlabeled (UAS) attachment scores. Row a shows that a simple ISBN model without features, using feed forward inference procedure, does not work well. As explained in section 2, this is expected since in the absence of explicit features, the latent variables in a given layer do not take into account the observations in the previous layers. The huge improvement in performance Model LAS UAS ISBN w/o features 38.38 54.52 ISBN w/ features 88.65 91.44 TRBM w/o features 86.01 89.78 TRBM w/ features 88.72 91.65 MST (McDonald et al., 2005) 87.07 89.95 Malt−→AE (Hall et al., 2007) 85.96 88.64 MSTMalt (Nivre and McDonald, 2008) 87.45 90.22 CoNLL 2008 #1 (Johansson and Nugues, 2008) 90.13 92.45 ensemble100% (Surdeanu and Manning, 2010) 88.83 91.47 CoNLL 2009 #1 (Bohnet, 2009) 89.88 unknown Table 1: LAS and UAS for different models. on adding the features (row b) shows that the feed forward inference procedure for ISBNs relies heavily on these feature connections to compensate for the lack of backward inference. The TRBM model avoids this problem as the inference procedure takes into account the current observation, which makes the latent variables much more informed. How</context>
</contexts>
<marker>Hall, Nilsson, Nivre, Eryigit, Megyesi, Nilsson, Saers, 2007</marker>
<rawString>J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. Nilsson, and M. Saers. 2007. Single malt or blended? A study in multilingual parser optimization. In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL 2007, pages 933–939. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>P Merlo</author>
<author>G Musillo</author>
<author>I Titov</author>
</authors>
<title>A latent variable model of synchronous parsing for syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>178--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Henderson, Merlo, Musillo, Titov, 2008</marker>
<rawString>J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008. A latent variable model of synchronous parsing for syntactic and semantic dependencies. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 178–182. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>S Osindero</author>
<author>Y W Teh</author>
</authors>
<title>A fast learning algorithm for deep belief nets.</title>
<date>2006</date>
<booktitle>Neural computation,</booktitle>
<pages>18--7</pages>
<contexts>
<context position="1169" citStr="Hinton et al., 2006" startWordPosition="162" endWordPosition="165">nt features with the help of temporal connections to the relevant previous steps which provide context information. Our parser achieves labeled and unlabeled attachment scores of 88.72% and 91.65% respectively, which compare well with similar previous models and the state-of-the-art. 1 Introduction There has been significant interest recently in machine learning methods that induce generative models with high-dimensional hidden representations, including neural networks (Bengio et al., 2003; Collobert and Weston, 2008), Bayesian networks (Titov and Henderson, 2007a), and Deep Belief Networks (Hinton et al., 2006). In this paper, we investigate how these models can be applied to dependency parsing. We focus on Shift-Reduce transition-based parsing proposed by Nivre et al. (2004). In this class of algorithms, at any given step, the parser has to choose among a set of possible actions, each representing an incremental modification to the partially built tree. To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). In a related earlier work, R</context>
<context position="7127" citStr="Hinton et al., 2006" startWordPosition="1103" endWordPosition="1106">nd E is the energy function defined as: E(v, h) = −Eiaivi − Ejbjhj − Ei,jvihjwij where ai and bj are biases for corresponding visible and latent variables respectively, and wij is the symmetric weight between vi and hj. Given the visible variables, the latent variables are conditionally independent of each other, and vice versa: p(hj = 1|v) = Q(bj + Eiviwij) (1) p(vi = 1|h) = Q(ai + Ejhjwij) (2) where Q(x) = 1/(1 + e−x) (the logistic sigmoid). RBM based models have been successfully used in image and video processing, such as Deep Belief Networks (DBNs) for recognition of hand-written digits (Hinton et al., 2006) and TRBMs for modeling motion capture data (Taylor et al., 2007). Despite their success, RBMs have seen limited use in the NLP community. Previous work includes RBMs for topic modeling in text documents (Salakhutdinov and Hinton, 2009), and Temporal Factored RBM for language modeling (Mnih and Hinton, 2007). 3.2 Proposed TRBM Model Structure TRBMs (Taylor et al., 2007) can be used to model sequences where the decision at each step requires some context information from the past. Figure 2 12 Figure 2: Proposed TRBM Model. Edges with no arrows represent undirected RBM connections. The directed </context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<issue>8</issue>
<contexts>
<context position="9854" citStr="Hinton, 2002" startWordPosition="1544" endWordPosition="1545">slight approximation, but avoids having to compute the partition function over all possible combinations of values for all subdecisions.1 The complete probability of a derivation is: p(vT1 ) = p(v1).p(v2|history2)... p(vT|historyT) 3.4 TRBM Training The gradient of an RBM is given by: a log p(v)/awij = (vihj)data − (vihj)model (6) where ()d denotes the expectation under distribution d. In general, computing the exact gradient is intractable and previous work proposed a Contrastive Divergence (CD) based learning procedure that approximates the above gradient using only one step reconstruction (Hinton, 2002). Fortunately, our model has only a limited set of possible visible values, which allows us to use a better approximation by taking the derivative of equation 5: 1In cases where computing the partition function is still not feasible (for instance, because of a large vocabulary), sampling methods could be used. However, we did not find this to be necessary. shows our proposed TRBM model with latent to latent connections between time steps. Each step has an RBM with weights WRBM composed of smaller weight matrices corresponding to different sub-decisions. For instance, for the action Left-Arc, W</context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>G.E. Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Dependencybased syntactic-semantic analysis with PropBank and NomBank.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>183--187</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13812" citStr="Johansson and Nugues, 2008" startWordPosition="2211" endWordPosition="2214">a simple ISBN model without features, using feed forward inference procedure, does not work well. As explained in section 2, this is expected since in the absence of explicit features, the latent variables in a given layer do not take into account the observations in the previous layers. The huge improvement in performance Model LAS UAS ISBN w/o features 38.38 54.52 ISBN w/ features 88.65 91.44 TRBM w/o features 86.01 89.78 TRBM w/ features 88.72 91.65 MST (McDonald et al., 2005) 87.07 89.95 Malt−→AE (Hall et al., 2007) 85.96 88.64 MSTMalt (Nivre and McDonald, 2008) 87.45 90.22 CoNLL 2008 #1 (Johansson and Nugues, 2008) 90.13 92.45 ensemble100% (Surdeanu and Manning, 2010) 88.83 91.47 CoNLL 2009 #1 (Bohnet, 2009) 89.88 unknown Table 1: LAS and UAS for different models. on adding the features (row b) shows that the feed forward inference procedure for ISBNs relies heavily on these feature connections to compensate for the lack of backward inference. The TRBM model avoids this problem as the inference procedure takes into account the current observation, which makes the latent variables much more informed. However, as row c shows, the TRBM model without features falls a bit short of the ISBN performance, indic</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>R. Johansson and P. Nugues. 2008. Dependencybased syntactic-semantic analysis with PropBank and NomBank. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 183–187. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<volume>1</volume>
<pages>296--304</pages>
<contexts>
<context position="17193" citStr="Lin, 1998" startWordPosition="2786" endWordPosition="2787">epresentations into 50 clusters using the k-means algorithm. Table 2 shows some partitions for the TRBM model. The partitions look semantically meaningful but to get a quantitative analysis, we computed pairwise semantic similarity between all word pairs in a given cluster and aggregated this number over all the clusters. The semantic similarity was calculated using two different similarity measures on the wordnet corpus (Miller et al., 1990): path and lin. path similarity is a score between 0 and 1, equal to the inverse of the shortest path length between the two word senses. lin similarity (Lin, 1998) is a score between 0 and 1 based on the Information Content of the two word senses and of the Least Common Subsumer. Table 3 shows the similarity scores.3 We observe that TRBM latent representations give a slightly better clustering than ISBN models. Again, this is because of the fact that the inference procedure in TRBMs takes into account the current observation. However, at the same time, the similarity numbers for ISBN with features 2Verbs are words corresponding to POS tags: VB, VBD, VBG, VBN, VBP, VBZ. We selected verbs as they have good coverage in Wordnet. 3To account for randomness i</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, volume 1, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 523–530. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K J Miller</author>
</authors>
<title>Introduction to wordnet: An online lexical database.</title>
<date>1990</date>
<journal>International Journal of lexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="17029" citStr="Miller et al., 1990" startWordPosition="2753" endWordPosition="2756">of 100 latent variables. Every Shift operation gives a latent representation for the corresponding word. We took all the verbs in the development set2 and partitioned their representations into 50 clusters using the k-means algorithm. Table 2 shows some partitions for the TRBM model. The partitions look semantically meaningful but to get a quantitative analysis, we computed pairwise semantic similarity between all word pairs in a given cluster and aggregated this number over all the clusters. The semantic similarity was calculated using two different similarity measures on the wordnet corpus (Miller et al., 1990): path and lin. path similarity is a score between 0 and 1, equal to the inverse of the shortest path length between the two word senses. lin similarity (Lin, 1998) is a score between 0 and 1 based on the Information Content of the two word senses and of the Least Common Subsumer. Table 3 shows the similarity scores.3 We observe that TRBM latent representations give a slightly better clustering than ISBN models. Again, this is because of the fact that the inference procedure in TRBMs takes into account the current observation. However, at the same time, the similarity numbers for ISBN with fea</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K.J. Miller. 1990. Introduction to wordnet: An online lexical database. International Journal of lexicography, 3(4):235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings ofthe 24th international conference on Machine learning,</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7436" citStr="Mnih and Hinton, 2007" startWordPosition="1153" endWordPosition="1156"> other, and vice versa: p(hj = 1|v) = Q(bj + Eiviwij) (1) p(vi = 1|h) = Q(ai + Ejhjwij) (2) where Q(x) = 1/(1 + e−x) (the logistic sigmoid). RBM based models have been successfully used in image and video processing, such as Deep Belief Networks (DBNs) for recognition of hand-written digits (Hinton et al., 2006) and TRBMs for modeling motion capture data (Taylor et al., 2007). Despite their success, RBMs have seen limited use in the NLP community. Previous work includes RBMs for topic modeling in text documents (Salakhutdinov and Hinton, 2009), and Temporal Factored RBM for language modeling (Mnih and Hinton, 2007). 3.2 Proposed TRBM Model Structure TRBMs (Taylor et al., 2007) can be used to model sequences where the decision at each step requires some context information from the past. Figure 2 12 Figure 2: Proposed TRBM Model. Edges with no arrows represent undirected RBM connections. The directed temporal connections between time steps contribute a bias to the latent layer inference in the current step. 3.3 TRBM Likelihood and Inference Section 3.1 describes an RBM where visible variables can take binary values. In our model, similar to (Salakhutdinov et al., 2007), we have multi-valued visible varia</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>A. Mnih and G. Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings ofthe 24th international conference on Machine learning, pages 641–648. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>R McDonald</author>
</authors>
<title>Integrating graphbased and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>Proceedings ofACL-08: HLT,</booktitle>
<pages>950--958</pages>
<contexts>
<context position="13757" citStr="Nivre and McDonald, 2008" startWordPosition="2202" endWordPosition="2205"> unlabeled (UAS) attachment scores. Row a shows that a simple ISBN model without features, using feed forward inference procedure, does not work well. As explained in section 2, this is expected since in the absence of explicit features, the latent variables in a given layer do not take into account the observations in the previous layers. The huge improvement in performance Model LAS UAS ISBN w/o features 38.38 54.52 ISBN w/ features 88.65 91.44 TRBM w/o features 86.01 89.78 TRBM w/ features 88.72 91.65 MST (McDonald et al., 2005) 87.07 89.95 Malt−→AE (Hall et al., 2007) 85.96 88.64 MSTMalt (Nivre and McDonald, 2008) 87.45 90.22 CoNLL 2008 #1 (Johansson and Nugues, 2008) 90.13 92.45 ensemble100% (Surdeanu and Manning, 2010) 88.83 91.47 CoNLL 2009 #1 (Bohnet, 2009) 89.88 unknown Table 1: LAS and UAS for different models. on adding the features (row b) shows that the feed forward inference procedure for ISBNs relies heavily on these feature connections to compensate for the lack of backward inference. The TRBM model avoids this problem as the inference procedure takes into account the current observation, which makes the latent variables much more informed. However, as row c shows, the TRBM model without fe</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>J. Nivre and R. McDonald. 2008. Integrating graphbased and transition-based dependency parsers. Proceedings ofACL-08: HLT, pages 950–958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="1337" citStr="Nivre et al. (2004)" startWordPosition="189" endWordPosition="192">scores of 88.72% and 91.65% respectively, which compare well with similar previous models and the state-of-the-art. 1 Introduction There has been significant interest recently in machine learning methods that induce generative models with high-dimensional hidden representations, including neural networks (Bengio et al., 2003; Collobert and Weston, 2008), Bayesian networks (Titov and Henderson, 2007a), and Deep Belief Networks (Hinton et al., 2006). In this paper, we investigate how these models can be applied to dependency parsing. We focus on Shift-Reduce transition-based parsing proposed by Nivre et al. (2004). In this class of algorithms, at any given step, the parser has to choose among a set of possible actions, each representing an incremental modification to the partially built tree. To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). In a related earlier work, Ratnaparkhi (1999) proposed a maximum entropy model for transition-based constituency parsing. Of these approaches, only ISBNs induce highdimensional latent representati</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based dependency parsing. In Proceedings of CoNLL, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>MaltParser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC,</booktitle>
<volume>6</volume>
<contexts>
<context position="1660" citStr="Nivre et al., 2006" startWordPosition="242" endWordPosition="245">03; Collobert and Weston, 2008), Bayesian networks (Titov and Henderson, 2007a), and Deep Belief Networks (Hinton et al., 2006). In this paper, we investigate how these models can be applied to dependency parsing. We focus on Shift-Reduce transition-based parsing proposed by Nivre et al. (2004). In this class of algorithms, at any given step, the parser has to choose among a set of possible actions, each representing an incremental modification to the partially built tree. To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). In a related earlier work, Ratnaparkhi (1999) proposed a maximum entropy model for transition-based constituency parsing. Of these approaches, only ISBNs induce highdimensional latent representations to encode parse history, but suffer from either very approximate or slow inference procedures. We propose to address the problem of inference in a high-dimensional latent space by using an undirected graphical model, Restricted Boltzmann Machines (RBMs), to model the individual parsing decisions. Unlike the Sigmoid Be</context>
<context position="5291" citStr="Nivre et al., 2006" startWordPosition="804" endWordPosition="807"> large numbers of heavily inter-connected latent variables make exact inference of decision probabilities intractable. Titov and Henderson (2007a) proposed two approximation procedures for inference. The first was a feed forward approximation where latent variables were allowed to depend only on their parent variables, and hence did not take into account the current or future observations. Due to this limitation, the authors proposed to make latent variables conditionally dependent also on a set of explicit features derived from the parsing history, specifically, the base features defined in (Nivre et al., 2006b). As shown in our experiments, this addition results in a big improvement for the parsing task. The second approximate inference procedure, called the incremental mean field approximation, extended the feed-forward approximation by updating the current time step’s latent variables after each sub-decision. Although this approximation is more accurate than the feed-forward one, there is no analytical way to maximize likelihood w.r.t. the means of the latent variables, which requires an iterative numerical method and thus makes inference very slow, restricting the model to only shorter sentence</context>
<context position="13019" citStr="Nivre et al., 2006" startWordPosition="2081" endWordPosition="2084"> equation 4, given that the chosen decision is now visible. These means are then stored with the new candidate for use in subsequent TRBM calculations. 4 Experiments &amp; Results We used syntactic dependencies from the English section of the CoNLL 2009 shared task dataset (Hajiˇc et al., 2009). Standard splits of training, development and test sets were used. To handle word sparsity, we replaced all the (POS, word) pairs with frequency less than 20 in the training set with (POS, UNKNOWN), giving us only 4530 tag-word pairs. Since our model can work only with projective trees, we used MaltParser (Nivre et al., 2006a) to projectivize/deprojectivize the training input/test output. 4.1 Results Table 1 lists the labeled (LAS) and unlabeled (UAS) attachment scores. Row a shows that a simple ISBN model without features, using feed forward inference procedure, does not work well. As explained in section 2, this is expected since in the absence of explicit features, the latent variables in a given layer do not take into account the observations in the previous layers. The huge improvement in performance Model LAS UAS ISBN w/o features 38.38 54.52 ISBN w/ features 88.65 91.44 TRBM w/o features 86.01 89.78 TRBM w</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>J. Nivre, J. Hall, and J. Nilsson. 2006a. MaltParser: A data-driven parser-generator for dependency parsing. In Proceedings ofLREC, volume 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>G Eryiit</author>
<author>S Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<pages>221--225</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1660" citStr="Nivre et al., 2006" startWordPosition="242" endWordPosition="245">03; Collobert and Weston, 2008), Bayesian networks (Titov and Henderson, 2007a), and Deep Belief Networks (Hinton et al., 2006). In this paper, we investigate how these models can be applied to dependency parsing. We focus on Shift-Reduce transition-based parsing proposed by Nivre et al. (2004). In this class of algorithms, at any given step, the parser has to choose among a set of possible actions, each representing an incremental modification to the partially built tree. To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). In a related earlier work, Ratnaparkhi (1999) proposed a maximum entropy model for transition-based constituency parsing. Of these approaches, only ISBNs induce highdimensional latent representations to encode parse history, but suffer from either very approximate or slow inference procedures. We propose to address the problem of inference in a high-dimensional latent space by using an undirected graphical model, Restricted Boltzmann Machines (RBMs), to model the individual parsing decisions. Unlike the Sigmoid Be</context>
<context position="5291" citStr="Nivre et al., 2006" startWordPosition="804" endWordPosition="807"> large numbers of heavily inter-connected latent variables make exact inference of decision probabilities intractable. Titov and Henderson (2007a) proposed two approximation procedures for inference. The first was a feed forward approximation where latent variables were allowed to depend only on their parent variables, and hence did not take into account the current or future observations. Due to this limitation, the authors proposed to make latent variables conditionally dependent also on a set of explicit features derived from the parsing history, specifically, the base features defined in (Nivre et al., 2006b). As shown in our experiments, this addition results in a big improvement for the parsing task. The second approximate inference procedure, called the incremental mean field approximation, extended the feed-forward approximation by updating the current time step’s latent variables after each sub-decision. Although this approximation is more accurate than the feed-forward one, there is no analytical way to maximize likelihood w.r.t. the means of the latent variables, which requires an iterative numerical method and thus makes inference very slow, restricting the model to only shorter sentence</context>
<context position="13019" citStr="Nivre et al., 2006" startWordPosition="2081" endWordPosition="2084"> equation 4, given that the chosen decision is now visible. These means are then stored with the new candidate for use in subsequent TRBM calculations. 4 Experiments &amp; Results We used syntactic dependencies from the English section of the CoNLL 2009 shared task dataset (Hajiˇc et al., 2009). Standard splits of training, development and test sets were used. To handle word sparsity, we replaced all the (POS, word) pairs with frequency less than 20 in the training set with (POS, UNKNOWN), giving us only 4530 tag-word pairs. Since our model can work only with projective trees, we used MaltParser (Nivre et al., 2006a) to projectivize/deprojectivize the training input/test output. 4.1 Results Table 1 lists the labeled (LAS) and unlabeled (UAS) attachment scores. Row a shows that a simple ISBN model without features, using feed forward inference procedure, does not work well. As explained in section 2, this is expected since in the absence of explicit features, the latent variables in a given layer do not take into account the observations in the previous layers. The huge improvement in performance Model LAS UAS ISBN w/o features 38.38 54.52 ISBN w/ features 88.65 91.44 TRBM w/o features 86.01 89.78 TRBM w</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Eryiit, Marinov, 2006</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, G. Eryiit, and S. Marinov. 2006b. Labeled pseudo-projective dependency parsing with support vector machines. In Proceedings of the Tenth Conference on Computational Natural Language Learning, pages 221–225. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1786" citStr="Ratnaparkhi (1999)" startWordPosition="261" endWordPosition="262">). In this paper, we investigate how these models can be applied to dependency parsing. We focus on Shift-Reduce transition-based parsing proposed by Nivre et al. (2004). In this class of algorithms, at any given step, the parser has to choose among a set of possible actions, each representing an incremental modification to the partially built tree. To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). In a related earlier work, Ratnaparkhi (1999) proposed a maximum entropy model for transition-based constituency parsing. Of these approaches, only ISBNs induce highdimensional latent representations to encode parse history, but suffer from either very approximate or slow inference procedures. We propose to address the problem of inference in a high-dimensional latent space by using an undirected graphical model, Restricted Boltzmann Machines (RBMs), to model the individual parsing decisions. Unlike the Sigmoid Belief Networks (SBNs) used in ISBNs, RBMs have tractable inference procedures for both forward and backward reasoning, which al</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>A. Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34(1):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Salakhutdinov</author>
<author>G Hinton</author>
</authors>
<title>Replicated softmax: an undirected topic model.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>22</pages>
<contexts>
<context position="7363" citStr="Salakhutdinov and Hinton, 2009" startWordPosition="1142" endWordPosition="1145"> the visible variables, the latent variables are conditionally independent of each other, and vice versa: p(hj = 1|v) = Q(bj + Eiviwij) (1) p(vi = 1|h) = Q(ai + Ejhjwij) (2) where Q(x) = 1/(1 + e−x) (the logistic sigmoid). RBM based models have been successfully used in image and video processing, such as Deep Belief Networks (DBNs) for recognition of hand-written digits (Hinton et al., 2006) and TRBMs for modeling motion capture data (Taylor et al., 2007). Despite their success, RBMs have seen limited use in the NLP community. Previous work includes RBMs for topic modeling in text documents (Salakhutdinov and Hinton, 2009), and Temporal Factored RBM for language modeling (Mnih and Hinton, 2007). 3.2 Proposed TRBM Model Structure TRBMs (Taylor et al., 2007) can be used to model sequences where the decision at each step requires some context information from the past. Figure 2 12 Figure 2: Proposed TRBM Model. Edges with no arrows represent undirected RBM connections. The directed temporal connections between time steps contribute a bias to the latent layer inference in the current step. 3.3 TRBM Likelihood and Inference Section 3.1 describes an RBM where visible variables can take binary values. In our model, si</context>
</contexts>
<marker>Salakhutdinov, Hinton, 2009</marker>
<rawString>R. Salakhutdinov and G. Hinton. 2009. Replicated softmax: an undirected topic model. Advances in Neural Information Processing Systems, 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Salakhutdinov</author>
<author>A Mnih</author>
<author>G Hinton</author>
</authors>
<title>Restricted Boltzmann machines for collaborative filtering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>798</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8000" citStr="Salakhutdinov et al., 2007" startWordPosition="1244" endWordPosition="1247">oral Factored RBM for language modeling (Mnih and Hinton, 2007). 3.2 Proposed TRBM Model Structure TRBMs (Taylor et al., 2007) can be used to model sequences where the decision at each step requires some context information from the past. Figure 2 12 Figure 2: Proposed TRBM Model. Edges with no arrows represent undirected RBM connections. The directed temporal connections between time steps contribute a bias to the latent layer inference in the current step. 3.3 TRBM Likelihood and Inference Section 3.1 describes an RBM where visible variables can take binary values. In our model, similar to (Salakhutdinov et al., 2007), we have multi-valued visible variables which we represent as one-hot binary vectors and model via a softmax distribution: exp(ak + Ej htjwkj) = Ei exp(ai + Ej htjwij) (3) Latent variable inference is similar to equation 1 with an additional bias due to the temporal connections. µtj = p(htj = 1|vt, historyt) = (u(bj + Ec,lw(c) HHljhlc) + Eivtiwij)) N u(b′j + Eivtiwij), b′j = bj + Ec,lw(c) HHljµlc). Here, µ denotes the mean of the corresponding latent variable. To keep inference tractable, we do not do any backward reasoning across directed connections to update µ(c). Thus, the inference proce</context>
</contexts>
<marker>Salakhutdinov, Mnih, Hinton, 2007</marker>
<rawString>R. Salakhutdinov, A. Mnih, and G. Hinton. 2007. Restricted Boltzmann machines for collaborative filtering. In Proceedings of the 24th international conference on Machine learning, page 798. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>C D Manning</author>
</authors>
<title>Ensemble models for dependency parsing: cheap and good?</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>649--652</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13866" citStr="Surdeanu and Manning, 2010" startWordPosition="2218" endWordPosition="2221">rd inference procedure, does not work well. As explained in section 2, this is expected since in the absence of explicit features, the latent variables in a given layer do not take into account the observations in the previous layers. The huge improvement in performance Model LAS UAS ISBN w/o features 38.38 54.52 ISBN w/ features 88.65 91.44 TRBM w/o features 86.01 89.78 TRBM w/ features 88.72 91.65 MST (McDonald et al., 2005) 87.07 89.95 Malt−→AE (Hall et al., 2007) 85.96 88.64 MSTMalt (Nivre and McDonald, 2008) 87.45 90.22 CoNLL 2008 #1 (Johansson and Nugues, 2008) 90.13 92.45 ensemble100% (Surdeanu and Manning, 2010) 88.83 91.47 CoNLL 2009 #1 (Bohnet, 2009) 89.88 unknown Table 1: LAS and UAS for different models. on adding the features (row b) shows that the feed forward inference procedure for ISBNs relies heavily on these feature connections to compensate for the lack of backward inference. The TRBM model avoids this problem as the inference procedure takes into account the current observation, which makes the latent variables much more informed. However, as row c shows, the TRBM model without features falls a bit short of the ISBN performance, indicating that features are indeed a powerful substitute f</context>
<context position="15893" citStr="Surdeanu and Manning (2010)" startWordPosition="2560" endWordPosition="2563">ode optimization is possible, this trend is likely to remain. We also tried a Contrastive Divergence based training procedure for TRBM instead of equation 7, but that resulted in about an absolute 10% lower LAS. Further, we also tried a very simple model without latent variables where temporal connections are between decision variables themselves. This 14 model gave an LAS of only 60.46%, which indicates that without latent variables, it is very difficult to capture the parse history. For comparison, we also include the performance numbers for some state-of-the-art dependency parsing systems. Surdeanu and Manning (2010) compare different parsing models using CoNLL 2008 shared task dataset (Surdeanu et al., 2008), which is the same as our dataset. Rows e − i show the performance numbers of some systems as mentioned in their paper. Row j shows the best syntactic model in CoNLL 2009 shared task. The TRBM model has only 1.4% lower LAS and 0.8% lower UAS compared to the best performing model. 4.2 Latent Layer Analysis We analyzed the latent layers in our models to see if they captured semantic patterns. A latent layer is a vector of 100 latent variables. Every Shift operation gives a latent representation for the</context>
</contexts>
<marker>Surdeanu, Manning, 2010</marker>
<rawString>M. Surdeanu and C.D. Manning. 2010. Ensemble models for dependency parsing: cheap and good? In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 649–652. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L M`arquez</author>
<author>J Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>159--177</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and J. Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 159–177. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Sutskever</author>
<author>G Hinton</author>
<author>G Taylor</author>
</authors>
<title>The recurrent temporal restricted boltzmann machine.</title>
<date>2008</date>
<booktitle>In NIPS,</booktitle>
<volume>21</volume>
<pages>page</pages>
<contexts>
<context position="11892" citStr="Sutskever et al. (2008)" startWordPosition="1891" endWordPosition="1894"> denote the ith visible and jth latent variable respectively at time step t. h(c) l denotes a latent variable in the past time step, and w(c)deHHlj notes the weight of the corresponding connection. p(vtk = 1|ht) (4) 13 a log p(vt(k)|historyt) � awij (7) (Ski − p(vt(i)|historyt)) u(b′j + wij) Further, the weights on the temporal connections are learned by back-propagating the likelihood gradients through the directed links between steps. The back-proped gradient from future time steps is also used to train the current RBM weights. This back-propagation is similar to the Recurrent TRBM model of Sutskever et al. (2008). However, unlike their model, we do not use CD at each step to compute gradients. 3.5 Prediction We use the same beam-search decoding strategy as used in (Titov and Henderson, 2007b). Given a derivation prefix, its partial parse tree and associated TRBM, the decoder adds a step to the TRBM for calculating the probabilities of hypothesized next decisions using equation 5. If the decoder selects a decision for addition to the candidate list, then the current step’s latent variable means are inferred using equation 4, given that the chosen decision is now visible. These means are then stored wit</context>
</contexts>
<marker>Sutskever, Hinton, Taylor, 2008</marker>
<rawString>I. Sutskever, G. Hinton, and G. Taylor. 2008. The recurrent temporal restricted boltzmann machine. In NIPS, volume 21, page 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W Taylor</author>
<author>G E Hinton</author>
<author>S T Roweis</author>
</authors>
<title>Modeling human motion using binary latent variables. Advances in neural information processing systems,</title>
<date>2007</date>
<pages>19--1345</pages>
<contexts>
<context position="2832" citStr="Taylor et al., 2007" startWordPosition="422" endWordPosition="426">ividual parsing decisions. Unlike the Sigmoid Belief Networks (SBNs) used in ISBNs, RBMs have tractable inference procedures for both forward and backward reasoning, which allows us to efficiently infer both the probability of the decision given the latent variables and vice versa. The key structural difference between the two models is that the directed connections between latent and decision vectors in SBNs become undirected in RBMs. A complete parsing model consists of a sequence of RBMs interlinked via directed edges, which gives us a form of Temporal Restricted Boltzmann Machines (TRBM) (Taylor et al., 2007), but with the incrementally specified model structure required by parsing. In this paper, we analyze and contrast ISBNs with TRBMs and show that the latter provide an accurate and theoretically sound model for parsing with highdimensional latent variables. 2 An ISBN Parsing Model Our TRBM parser uses the same historybased probability model as the ISBN parser of Titov and Henderson (2007b): P(tree) = HtP(vtjv1, ,vt−1), where each 11 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 11–17, Portland, Oregon, June 19-24, 2011. c�2011 Associ</context>
<context position="7192" citStr="Taylor et al., 2007" startWordPosition="1115" endWordPosition="1118">j − Ei,jvihjwij where ai and bj are biases for corresponding visible and latent variables respectively, and wij is the symmetric weight between vi and hj. Given the visible variables, the latent variables are conditionally independent of each other, and vice versa: p(hj = 1|v) = Q(bj + Eiviwij) (1) p(vi = 1|h) = Q(ai + Ejhjwij) (2) where Q(x) = 1/(1 + e−x) (the logistic sigmoid). RBM based models have been successfully used in image and video processing, such as Deep Belief Networks (DBNs) for recognition of hand-written digits (Hinton et al., 2006) and TRBMs for modeling motion capture data (Taylor et al., 2007). Despite their success, RBMs have seen limited use in the NLP community. Previous work includes RBMs for topic modeling in text documents (Salakhutdinov and Hinton, 2009), and Temporal Factored RBM for language modeling (Mnih and Hinton, 2007). 3.2 Proposed TRBM Model Structure TRBMs (Taylor et al., 2007) can be used to model sequences where the decision at each step requires some context information from the past. Figure 2 12 Figure 2: Proposed TRBM Model. Edges with no arrows represent undirected RBM connections. The directed temporal connections between time steps contribute a bias to the </context>
</contexts>
<marker>Taylor, Hinton, Roweis, 2007</marker>
<rawString>G.W. Taylor, G.E. Hinton, and S.T. Roweis. 2007. Modeling human motion using binary latent variables. Advances in neural information processing systems, 19:1345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
</authors>
<title>Constituent parsing with incremental sigmoid belief networks.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting on Association for Computational Linguistics,</booktitle>
<volume>45</volume>
<pages>632</pages>
<contexts>
<context position="1119" citStr="Titov and Henderson, 2007" startWordPosition="154" endWordPosition="157">ion step. The RBM at the current time step induces latent features with the help of temporal connections to the relevant previous steps which provide context information. Our parser achieves labeled and unlabeled attachment scores of 88.72% and 91.65% respectively, which compare well with similar previous models and the state-of-the-art. 1 Introduction There has been significant interest recently in machine learning methods that induce generative models with high-dimensional hidden representations, including neural networks (Bengio et al., 2003; Collobert and Weston, 2008), Bayesian networks (Titov and Henderson, 2007a), and Deep Belief Networks (Hinton et al., 2006). In this paper, we investigate how these models can be applied to dependency parsing. We focus on Shift-Reduce transition-based parsing proposed by Nivre et al. (2004). In this class of algorithms, at any given step, the parser has to choose among a set of possible actions, each representing an incremental modification to the partially built tree. To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov a</context>
<context position="3222" citStr="Titov and Henderson (2007" startWordPosition="488" endWordPosition="491">nd decision vectors in SBNs become undirected in RBMs. A complete parsing model consists of a sequence of RBMs interlinked via directed edges, which gives us a form of Temporal Restricted Boltzmann Machines (TRBM) (Taylor et al., 2007), but with the incrementally specified model structure required by parsing. In this paper, we analyze and contrast ISBNs with TRBMs and show that the latter provide an accurate and theoretically sound model for parsing with highdimensional latent variables. 2 An ISBN Parsing Model Our TRBM parser uses the same historybased probability model as the ISBN parser of Titov and Henderson (2007b): P(tree) = HtP(vtjv1, ,vt−1), where each 11 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 11–17, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Figure 1: An ISBN network. Shaded nodes represent decision variables and ‘H’ represents a vector of latent variables. W��� �� denotes the weight matrix for directed connection of type c between two latent vectors. vt is a parser decision of the type Left-Arc, Right-Arc, Reduce or Shift. These decisions are further decomposed into sub-decisions, as for </context>
<context position="4817" citStr="Titov and Henderson (2007" startWordPosition="731" endWordPosition="734">ry. As explained in (Titov and Henderson, 2007b), the temporal connections between latent variables are constructed to take into account the structural locality in the partial dependency structure. The model parameters are learned by backpropagating likelihood gradients. Because decision probabilities are conditioned on the history, once a decision is made the corresponding variable becomes observed, or visible. In an ISBN, the directed edges to these visible variables and the large numbers of heavily inter-connected latent variables make exact inference of decision probabilities intractable. Titov and Henderson (2007a) proposed two approximation procedures for inference. The first was a feed forward approximation where latent variables were allowed to depend only on their parent variables, and hence did not take into account the current or future observations. Due to this limitation, the authors proposed to make latent variables conditionally dependent also on a set of explicit features derived from the parsing history, specifically, the base features defined in (Nivre et al., 2006b). As shown in our experiments, this addition results in a big improvement for the parsing task. The second approximate infer</context>
<context position="12073" citStr="Titov and Henderson, 2007" startWordPosition="1923" endWordPosition="1926">onding connection. p(vtk = 1|ht) (4) 13 a log p(vt(k)|historyt) � awij (7) (Ski − p(vt(i)|historyt)) u(b′j + wij) Further, the weights on the temporal connections are learned by back-propagating the likelihood gradients through the directed links between steps. The back-proped gradient from future time steps is also used to train the current RBM weights. This back-propagation is similar to the Recurrent TRBM model of Sutskever et al. (2008). However, unlike their model, we do not use CD at each step to compute gradients. 3.5 Prediction We use the same beam-search decoding strategy as used in (Titov and Henderson, 2007b). Given a derivation prefix, its partial parse tree and associated TRBM, the decoder adds a step to the TRBM for calculating the probabilities of hypothesized next decisions using equation 5. If the decoder selects a decision for addition to the candidate list, then the current step’s latent variable means are inferred using equation 4, given that the chosen decision is now visible. These means are then stored with the new candidate for use in subsequent TRBM calculations. 4 Experiments &amp; Results We used syntactic dependencies from the English section of the CoNLL 2009 shared task dataset (H</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>I. Titov and J. Henderson. 2007a. Constituent parsing with incremental sigmoid belief networks. In Proceedings of the 45th Annual Meeting on Association for Computational Linguistics, volume 45, page 632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
</authors>
<title>Fast and robust multilingual dependency parsing with a generative latent variable model.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL,</booktitle>
<pages>947--951</pages>
<contexts>
<context position="1119" citStr="Titov and Henderson, 2007" startWordPosition="154" endWordPosition="157">ion step. The RBM at the current time step induces latent features with the help of temporal connections to the relevant previous steps which provide context information. Our parser achieves labeled and unlabeled attachment scores of 88.72% and 91.65% respectively, which compare well with similar previous models and the state-of-the-art. 1 Introduction There has been significant interest recently in machine learning methods that induce generative models with high-dimensional hidden representations, including neural networks (Bengio et al., 2003; Collobert and Weston, 2008), Bayesian networks (Titov and Henderson, 2007a), and Deep Belief Networks (Hinton et al., 2006). In this paper, we investigate how these models can be applied to dependency parsing. We focus on Shift-Reduce transition-based parsing proposed by Nivre et al. (2004). In this class of algorithms, at any given step, the parser has to choose among a set of possible actions, each representing an incremental modification to the partially built tree. To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov a</context>
<context position="3222" citStr="Titov and Henderson (2007" startWordPosition="488" endWordPosition="491">nd decision vectors in SBNs become undirected in RBMs. A complete parsing model consists of a sequence of RBMs interlinked via directed edges, which gives us a form of Temporal Restricted Boltzmann Machines (TRBM) (Taylor et al., 2007), but with the incrementally specified model structure required by parsing. In this paper, we analyze and contrast ISBNs with TRBMs and show that the latter provide an accurate and theoretically sound model for parsing with highdimensional latent variables. 2 An ISBN Parsing Model Our TRBM parser uses the same historybased probability model as the ISBN parser of Titov and Henderson (2007b): P(tree) = HtP(vtjv1, ,vt−1), where each 11 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 11–17, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Figure 1: An ISBN network. Shaded nodes represent decision variables and ‘H’ represents a vector of latent variables. W��� �� denotes the weight matrix for directed connection of type c between two latent vectors. vt is a parser decision of the type Left-Arc, Right-Arc, Reduce or Shift. These decisions are further decomposed into sub-decisions, as for </context>
<context position="4817" citStr="Titov and Henderson (2007" startWordPosition="731" endWordPosition="734">ry. As explained in (Titov and Henderson, 2007b), the temporal connections between latent variables are constructed to take into account the structural locality in the partial dependency structure. The model parameters are learned by backpropagating likelihood gradients. Because decision probabilities are conditioned on the history, once a decision is made the corresponding variable becomes observed, or visible. In an ISBN, the directed edges to these visible variables and the large numbers of heavily inter-connected latent variables make exact inference of decision probabilities intractable. Titov and Henderson (2007a) proposed two approximation procedures for inference. The first was a feed forward approximation where latent variables were allowed to depend only on their parent variables, and hence did not take into account the current or future observations. Due to this limitation, the authors proposed to make latent variables conditionally dependent also on a set of explicit features derived from the parsing history, specifically, the base features defined in (Nivre et al., 2006b). As shown in our experiments, this addition results in a big improvement for the parsing task. The second approximate infer</context>
<context position="12073" citStr="Titov and Henderson, 2007" startWordPosition="1923" endWordPosition="1926">onding connection. p(vtk = 1|ht) (4) 13 a log p(vt(k)|historyt) � awij (7) (Ski − p(vt(i)|historyt)) u(b′j + wij) Further, the weights on the temporal connections are learned by back-propagating the likelihood gradients through the directed links between steps. The back-proped gradient from future time steps is also used to train the current RBM weights. This back-propagation is similar to the Recurrent TRBM model of Sutskever et al. (2008). However, unlike their model, we do not use CD at each step to compute gradients. 3.5 Prediction We use the same beam-search decoding strategy as used in (Titov and Henderson, 2007b). Given a derivation prefix, its partial parse tree and associated TRBM, the decoder adds a step to the TRBM for calculating the probabilities of hypothesized next decisions using equation 5. If the decoder selects a decision for addition to the candidate list, then the current step’s latent variable means are inferred using equation 4, given that the chosen decision is now visible. These means are then stored with the new candidate for use in subsequent TRBM calculations. 4 Experiments &amp; Results We used syntactic dependencies from the English section of the CoNLL 2009 shared task dataset (H</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>I. Titov and J. Henderson. 2007b. Fast and robust multilingual dependency parsing with a generative latent variable model. In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL, pages 947–951.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>