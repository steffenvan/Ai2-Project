<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000213">
<title confidence="0.450259">
A Finite-State Approach to Machine Translation*
</title>
<note confidence="0.871972666666667">
Srinivas Bangalore and Giuseppe Riccardi
AT&amp;T Labs-Research
180 Park Avenue, Florham Park, NJ 07932.
</note>
<email confidence="0.706974">
fsrini,dsp31@research.att.com
</email>
<sectionHeader confidence="0.978692" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999853736842105">
The problem of machine translation can be viewed as
consisting of two subproblems (a) Lexical Selection
and (b) Lexical Reordering. We propose stochas-
tic finite-state models for these two subproblems in
this paper. Stochastic finite-state models are effi-
ciently learnable from data, effective for decoding
and are associated with a calculus for composing
models which allows for tight integration of con-
straints from various levels of language processing.
We present a method for learning stochastic finite-
state models for lexical choice and lexical reordering
that are trained automatically from pairs of source
and target utterances. We use this method to de-
velop models for English-Japanese translation and
present the performance of these models for trans-
lation on speech and text. We also evaluate the ef-
ficacy of such a translation model in the context of
a call routing task of unconstrained speech utter-
ances.
</bodyText>
<sectionHeader confidence="0.995598" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991173672131148">
The problem of machine translation can be viewed
as consisting of two phases: (a) lexical choice phase
where appropriate target language lexical items are
chosen for each source language lexical item and
(b) lexical reordering phase where the chosen tar-
get language lexical items are rearranged to produce
a meaningful target language string. In this paper,
we develop stochastic finite-state transducer (SFST)
models for these two phases which can then be com-
posed into a single SFST model for Statistical Ma-
chine Translation (SMT). We explore the perfor-
mance limits of such models in the context of trans-
lation in limited domains. We are also interested in
SFST models since they allow for tight integration
with a speech recognizer for speech-to-speech trans-
lation. In particular, we are interested in one-pass
decoding and translation of speech as opposed to the
more prevalent approach of translation of speech lat-
tices.
Finite state models have been extensively applied
to many aspects of language processing including,
speech recognition (Pereira and Riley, 1997; Riccardi
et al., 1996), phonology (Kaplan and Kay, 1994),
morphology (Koskenniemi, 1984), chunking (Ab-
ney, 1991; Bangalore and Joshi, 1999) and pars-
ing (Roche, 1999). Finite-state models are attrac-
tive mechanisms for language processing since they
are (a) efficiently learnable from data (b) generally
effective for decoding (c) associated with a calculus
for composing models which allows for straightfor-
ward integration of constraints from various levels of
language processing.&apos;
A number of approaches to SMT, including the
seminal work at IBM (Brown et al., 1993), are
stochastic string transductions that map source lan-
guage strings directly to target language strings.
There are other approaches to SMT where transla-
tion is achieved through tree transductions that map
source language trees to target language trees (Al-
shawi et al., 1998b; Wu, 1997). There are also in-
ternational multi-site projects such as VERBMO-
BIL (Verbmobil, 2000) and CSTAR (Woszczyna
et al., 1998; Lavie et al., 1999) that are involved
in speech-to-speech translation in limited domains.
The systems developed in these projects employ
various techniques ranging from example-based to
interlingua-based translation methods for transla-
tion between English, French, German, Italian,
Japanese, and Korean.
Finite-state models for SMT have been previ-
ously suggested in the literature (Vilar et al., 1999;
Knight and Al-Onaizan, 1998). In (Vilar et al.,
1999), a deterministic transducer is used to imple-
ment an English-Spanish speech translation system.
In (Knight and Al-Onaizan, 1998), finite-state ma-
chine translation is based on (Brown et al., 1993)
and is used for decoding the target language string.
However, no experimental results are reported using
this approach.
Unlike previous approaches, we subdivide the
translation task into lexical choice and lexical re-
</bodyText>
<footnote confidence="0.6456115">
* Anuvaad is a system embodying our approach and can be &apos;Furthermore, software implementing the finite-state cal-
seen at http://www.research.att.comr srini/Anuvaad.html culus is available for research purposes.
</footnote>
<figure confidence="0.474658304347826">
^
WS
W T
^ *
~
WT
max P(WS
,W)
T
(W )
T
max Pλ
T
Eng: I need to make a collect call
Jap: fLIJ: b&apos;tt6 t&apos;Ob4) 9 11-
Source:-1 1 4 7 6 4 2
Alignment: 1 5 0 3 0 2 4
Target:-1 3 4 5 1
Eng: I&apos;d like to charge this to my home phone
Jap: fl1J: fig) *a) VAIL
Source: -1 1 4 2 4 7 5 7 8
Alignment: 1 7 0 6 2 0 3 4 5
Target: -1 6 2 3 4 7 1
</figure>
<figureCaption confidence="0.994468">
Figure 2: Example bilingual texts with alignment information
</figureCaption>
<bodyText confidence="0.996738384615385">
minimum total cost which maps the source sentence
to its target sentence. This search is carried out in a
hierarchical fashion with recursive decomposition of
the source and target strings around a hypothesized
head word in the source string and its correspond-
ing translation in the target string. The hierarchi-
cal alignment which minimizes the cost function is
computed using a dynamic programming procedure.
Some example bitexts and the result of the align-
ment procedure are shown in Figure 2.3 The align-
ment for the first bitext reads as: first source word
is aligned to the first target word, the second source
word is aligned to the fifth target word, the third
source word not aligned with any target word and
so on. The tree structure resulting from the hier-
archical decomposition of the source string and the
target string is represented along the third and the
fifth line of Figure 2. Each word position is associ-
ated with the word index of its mother in the tree.
The root of the tree is indicated by -1. The tree
structure infomration is used for lexical reordering
as discussed in Section 4.
Note that we use a tree-based alignment unlike
the string-based alignment in IBM statistical mod-
els. We believe that a tree-based alignment is more
natural for modeling lexical reordering operations
than a string-based alignment. We are currently in-
vestigating the quality of the dictionary produced by
a tree-based alignment compared to a string-based
alignment.
From the alignment information in Figure 2, it
is straightforward to compile a bilanguage corpus
consisting of source-target symbol pair sequences
T = . (wi, xi) . . ., where the source word wi E
Ls U E and its aligned word xi E LT U E (E is the
null symbol). Note that the tokens of a bilanguage
could be either ordered according to the word or-
der of the source language or ordered according to
the word order of the target language. From the cor-
</bodyText>
<footnote confidence="0.719670666666667">
3The Japanese string was translated and segmented so
that a token boundary in Japanese corresponds to some token
boundary in English.
</footnote>
<bodyText confidence="0.999763545454545">
pus T, we train a Stochastic Finite State Transducer
(SFST) which is an extension of the Variable Ngram
State Automaton (Riccardi et al., 1996). Stochastic
transducers rsT : Ls x LT —} [0, 1] map the string
Ws E LS into WT E LT and assign a probability to
the transduction Ws Ts4 WT. In our case, the SFST
model will estimate P(Ws T4&apos; WT) = P(Ws,WT)
and the symbol pair (wi, xi) will be associated to
each transducer state q with input label wi and out-
put label xi. The model TsT provides a string-to-
string transduction from Ws into WT.
</bodyText>
<subsectionHeader confidence="0.999668">
3.1 Acquiring Phrasal Translations
</subsectionHeader>
<bodyText confidence="0.999661142857143">
While word-to-word translation is only approximat-
ing the lexical choice process, phrase-to-phrase map-
ping can greatly improve the translation of col-
locations, recurrent strings, etc. Moreover, SF-
STs can take advantage of the phrasal correla-
tion to improve the computation of the probabil-
ity P(Ws,WT) (Bangalore and Riccardi, 2000). In
this section, we describe an alternate method that
uses the result of the alignment module as a seed
to acquire bilingual phrases of more than two words
length.
As mentioned above, we use the alignment in-
formation to construct a bilanguage corpus where
each token is of the form (wi,xi). Bilingual phrases
can be derived from the phrases (substrings) of the
bilinguage corpus that have high mutual informa-
tion score. We acquire bilanguage phrases from the
bilanguage corpus by computing weighted mutual
information metric of n-grams for arbitrarily large
values of n. We use a suffix array to compute the
frequencies of large n-grams similar to the method
presented in (Yamamoto and Church, 1998). Since
the phrases acquired from a source(target) ordered
bilanguage corpus may not have the target(source)
language words in the order of the target(source)
language, we introduce a reordering phase for the
words in a phrase which we call local reordering.
In the local reordering phase, for each phrase we
select an alignment which aligns each source word
with some word(s) in the target phrase. We then
reorder the words of the target phrase such that the
reordering corresponds to a substring (consecutive
words) of the target sentence in the selected align-
ment. A sample set of phrases after reordering is
illustrated in Table 3.
</bodyText>
<table confidence="0.917261">
Japanese Phrases English Phrases
— .7.i 1-&apos;: -7- 4 — A T and T
fl,o) 4:0) WittL to my home phone
1., 1- --)1, I need to make a
t&apos;11.6 t&apos;Ob4) 9 11- collect call
V,,IJ: h */&amp;quot;..&amp;quot;. L. .5 -$&apos;3 --) -C how may I help
f-3f-WilL 1 L .t .5 bl you
yes could you
</table>
<figureCaption confidence="0.9241195">
Figure 3: Examples of acquired phrases after re-
ordering of Japanese phrases
</figureCaption>
<sectionHeader confidence="0.952394" genericHeader="method">
4 Lexical Reordering
</sectionHeader>
<bodyText confidence="0.999989083333334">
The lexical choice model outputs a sequence of tar-
get language words and phrases for a given source
language sentence. Since these target language
words and phrases may not form a well-formed tar-
get language sentence, we need to apply a lexical
reordering (sentence-level) operation.
For the lexical reordering operation, the exact ap-
proach would be to search through all possible per-
mutation sequences of words and phrases and select
the most likely sequence. However, that is computa-
tionally very expensive. To overcome this problem,
we decompose the sequence of words and phrases
into a tree with each arc labeled with position infor-
mation of the daughter with respect to its mother.
This tree structure could be interpreted as a depen-
dency tree.
We use a stochastic finite-state model to parse the
sequence of words and phrases into a tree contain-
ing reordering information. We train this SFST from
a corpus derived from an aligned corpus of source-
ordered target language sentence paired with its tar-
get sentence (Figure 4). The corpus (Figure 5) con-
sists of bracketed representation of dependency trees
which are constructed from the alignment informa-
tion shown in Figure 4.
The composition of the reordering finite-state
transducer on the result of the lexical choice model
results in strings that are annotated with reordering
instructions. To ensure we obtain well-formed brack-
eted strings, we compose the result with a trans-
ducer that checks for all possible well-formed brack-
ets, for a fixed number of brackets. This can be
regarded as a finite-state approximation of a para-
thensis context-free grammar upto a bounded depth.
The resulting string from the composition contains
reordering instructions which are interpreted to form
the reordered target language sentence. Other inter-
esting approaches involve extracting a context-free
grammar from the training corpus and approximat-
ing the resulting grammar by a finite-state grammar
using techniques discussed in (Pereira and Wright,
1997; Nederhof, 2000).
Figure 6 shows the sequence of transductions
starting from a source language string that results
in a target language string. The intermediate steps
involved include lexical choice, parse of the source-
ordered target string, reordered parse tree for the
target string and the final target string.
</bodyText>
<sectionHeader confidence="0.983469" genericHeader="evaluation">
5 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.9999832">
In this section, we discuss issues concerning evalu-
ation of the translation system. The data for the
experiments reported in this section were obtained
from the customer side of operator-customer con-
versations, with the customer-care application de-
scribed in (Riccardi and Gorin, 2000). Each of the
customer&apos;s utterance transcriptions were then man-
ually translated into Japanese. A total of 15,457
English-Japanese sentence pairs was split into 12,204
training sentence pairs and 3,253 test sentence pairs.
</bodyText>
<subsectionHeader confidence="0.8867515">
5.1 Evaluation of Machine Translation
Systems
</subsectionHeader>
<bodyText confidence="0.999959608695652">
Evaluation of a machine translation systems has
been a subject of discussion for many years (Coun-
cil, 1966; Arnold, 1993). A universally acceptable,
objective and reliable metric that can be computed
automatically is yet to be found. However, in the in-
terest of evaluating our translation system automat-
ically and objectively without human intervention,
we report the performance of a machine translation
system in application independent and in the con-
text of an application.
For the application independent evaluation, we
employ two metrics based on string edit distance be-
tween the output of a translation system and the ref-
erence translation string: simple accuracy and trans-
lation accuracy (Alshawi et al., 1998b). Simple ac-
curacy is the number of insertion (/ + /&apos;), deletion
(D + D&apos;) and substitutions (S) errors between the
target language strings in the test corpus and the
strings produced by the translation model. The met-
ric is summarized in Equation 4. R is the number of
tokens in the target string. This metric is similar to
the string distance metric used for measuring speech
recognition accuracy.
</bodyText>
<equation confidence="0.995318">
I + I&apos; + D + D&apos; + S ) (4)
SimpleAccuracy = (1
</equation>
<bodyText confidence="0.86921725">
The simple accuracy metric, however, penalizes
a misplaced token twice, as a deletion from its ex-
pected position and insertion at a different posi-
Eng-Jap: fj,/j: 1t &amp;o4 --; tL flo) v&lt;a) TAIL
Japanese: &amp;ULi1A. fi,o) 0) &apos;WAIL Lt,n,10)-C1-
Source: -1 4 2 1 6 4 6
Alignment: 1 7 6 2 3 4 5
Target:-1 1 4 2 4 7 2
</bodyText>
<figureCaption confidence="0.911262">
Figure 4: Alignment between English-ordered Japanese and Japanese strings
</figureCaption>
<equation confidence="0.889649166666667">
e:+1
E: [
e:[ L L e:-1 e:[ — e:] e:]
e:+2 e:+1
e:[ e:[ fi,o):flo) e:] e:-1
e:]
</equation>
<figureCaption confidence="0.964371">
Figure 5: Bracket representation of a dependency tree with information on reordering words. Each token
consists of the form of a transduction (input:output).
</figureCaption>
<equation confidence="0.417283">
0):*(7) e:+1 E: E:]
</equation>
<bodyText confidence="0.999494142857143">
tion. We use a second metric, Translation Accuracy,
shown in Equation 5, which treats deletion of a to-
ken at one location in the string and the insertion
of the same token at another location in the string
as one single movement error (M=I+D).4 This is
in addition to the remaining insertion, deletion and
substitutions.
</bodyText>
<equation confidence="0.908167">
M + + D&apos; +S
TranslationAccuracy = (1 )
(5)
</equation>
<bodyText confidence="0.999980833333333">
For application dependent evaluation of a transla-
tion system, we employ the translation system in the
context of call type classification. We compare the
classification accuracy using the text produced by
the translation system against that produced using
the reference text.
</bodyText>
<subsectionHeader confidence="0.998808">
5.2 Application Independent Evaluation
</subsectionHeader>
<bodyText confidence="0.9998806">
Using the training sentence pairs and the procedure
described in the earlier sections, we have developed
English to Japanese and Japanese to English trans-
lation systems.
Table 1 presents the performance results of the
English to Japanese translation system using differ-
ent translation models, before and after the reorder-
ing stage.
In both tables, the unigram, bigram and trigram
translation models do not include any phrases while
uniphrase, biphrase and triphrase models include the
automatically acquired phrases. As can be seen,
the performance of models after reordering is signif-
icantly better than the performance before reorder-
ing.
</bodyText>
<footnote confidence="0.987504666666667">
4Note that the movement errors are derived after the
strings are compared using insertion, deletion and substitu-
tion operations.
</footnote>
<table confidence="0.999516">
Trans Accuracy Accuracy
VNSA before after
order Reordering Reordering
Unigram 23.8 32.2
Bigram 56.9 69.4
Trigram 56.4 69.1
UniPhrase 44.0 46.8
BiPhrase 60.4 69.8
TriPhrase 58.9 66.7
</table>
<tableCaption confidence="0.815938">
Table 1: Translation Accuracy of the English to
Japanese Translation System with and without
phrases, before and after reordering on text.
</tableCaption>
<subsectionHeader confidence="0.670841">
5.2.1 Spoken Language Translation
</subsectionHeader>
<bodyText confidence="0.999913">
The English-Japanese translation system was used
to translate spoken language as well. The com-
posed lexical choice transducer and lexical reorder-
ing transducer can be directly plugged into a speech
recognizer in conjunction with the source language
acoustic model to produce a source-speech to target-
text system. We will report the result of such a sys-
tem in the final version of this paper. Currently, we
report performance on one-best output of a speech
recognizer as the input to the translation system.
A VNSA-based trigram language model that was
trained on the 12204 training sentences was used as
the language model for the speech recognizer. An
off-the-shelf context dependent acoustic model for
telephone speech was used as the acoustic model.
The word accuracy of the speech recognizer on the
test data is 74.3%. Table 2 summarizes the trans-
lation accuracies of various models on the one-best
output of the speech recognizer. The simple and
translation accuracy of the triphrase-based transla-
tion system on the one-best output of the recognizer
is 56.9% respectively.
</bodyText>
<figure confidence="0.978888333333333">
English:I&apos;d like to charge this to my home phone
Eng-JapAII Lt,20ay-c-1--
Lt:200D-Ct
1) WAG:
Lexical
Choice
(FST)
Lexical
Reordering
(FS 1k)
fi,o)
Japanese: fLII
</figure>
<figureCaption confidence="0.999826">
Figure 6: Sequence of finite-state transductions from English to Japanese
</figureCaption>
<table confidence="0.999445888888889">
Trans Accuracy Accuracy
VNSA order before after
Reordering Reordering
Unigram 21.4 21.7
Bigram 48.9 55.7
Trigram 49.0 56.8
UniPhrase 39.3 39.6
BiPhrase 51.3 56.5
TriPhrase 50.9 56.9
</table>
<tableCaption confidence="0.992801">
Table 2: Translation Accuracy of the English to
</tableCaption>
<bodyText confidence="0.837606333333333">
Japanese Translation System with and without
phrases, before and after reordering on one-best out-
put of the speech recognizer.
</bodyText>
<subsectionHeader confidence="0.995448">
5.3 Application Dependent Evaluation:
Call Type Classification
</subsectionHeader>
<bodyText confidence="0.999970782608696">
The objective of this experiment is to measure the
performance of a translation system in the context
of an application, in our case, a call type classifi-
cation application task called the How May I Help
You? (Gorin et al., 1997) task. We briefly review
the problem and the spoken language system. The
goal is to sufficiently understand caller&apos;s responses
to the open-ended prompt How May I Help You?
and route such a call based on the meaning of the re-
sponse. Thus we aim at extracting a relatively small
number of semantic actions from the utterances of
a very large set of users who are not trained to the
system&apos;s capabilities and limitations.
The first utterance of each transaction has been
transcribed and marked with a call-type by label-
ers. There are 14 call-types plus a class other for
the complement class. In particular, we focused our
study on the classification of the caller&apos;s first utter-
ance in these dialogs. The spoken sentences vary
widely in duration, with a distribution distinctively
skewed around a mean value of 5.3 seconds corre-
sponding to 19 words per utterance. Some examples
of the first utterances are given below:
</bodyText>
<listItem confidence="0.999486333333333">
• Yes ma&apos;am where is area code two zero
one?
• I&apos;m tryn&apos;a call and I can&apos;t get it to
go through I wondered if you could try
it for me please?
• Hello
</listItem>
<bodyText confidence="0.999884466666667">
In an automated call router there are two impor-
tant performance measures. The first is the prob-
ability of false rejection, where a call is falsely re-
jected or classified as other. Since such calls would
be transferred to a human agent, this corresponds to
a missed opportunity for automation. The second
measure is the probability of correct classification.
Errors in this dimension lead to misinterpretations
that must be resolved by a dialog manager (Abella
and Gorin, 1997).
Using our approach described in the previous
sections, we have trained a unigram, bigram and
trigram VNSA based translation models with and
without phrases. Table 3 shows lexical choice (bag-
of-tokens) accuracy for these different translation
</bodyText>
<figure confidence="0.98099275">
ROC curve for English test set
100
95
Trigram
Phrase−Unigram
Text
Phrase−Trigram
Unigram
75
70
0 10 20 30 40 50 60 70 80 90
False rejection rate (%)
Correct classification rate (%)
90
85
80
</figure>
<reference confidence="0.998557847058823">
E. Arnold. 1993. Special issue on evaluation of MT
Systems. Machine Translation, 8(1-2):1-126.
Srinivas Bangalore and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2).
Srinivas Bangalore and Giuseppe Riccardi. 2000.
Stochastic finite-state models for spoken language
machine translation. In Proceedings of the Work-
shop on Embedded Machine Translation Systems.
P. Brown, S.D. Pietra, V.D. Pietra, and R. Mer-
cer. 1993. The Mathematics of Machine Transla-
tion: Parameter Estimation. Computational Lin-
guistics, 16(2):263-312.
National Research Council. 1966. Alpac report.
Technical report, National Academy of Sciences.
A. L. Gorin, G. Riccardi, and J. H Wright. 1997.
How May I Help You? Speech Communication,
23:113-127.
R.M. Kaplan and M. Kay. 1994. Regular models
of phonological rule systems. Computational Lin-
guistics, 20(3):331-378.
Kevin Knight and Y. Al-Onaizan. 1998. Transla-
tion with finite-state devices. In Machine trans-
lation and the information soup, Langhorne, PA,
October.
K. K. Koskenniemi. 1984. Two-level morphology: a
general computation model for word-form recogni-
tion and production. Ph.D. thesis, University of
Helsinki.
Alon Lavie, Lori Levin, Monika Woszczyna, Donna
Gates, Marsal Gavalda„ and Alex Waibel.
1999. The janus-iii translation system: Speech-
to-speech translation in multiple domains. In
Proceedings of CSTAR Workshop, Schwetzingen,
Germany, September.
Mark-Jan Nederhof. 2000. Practical experiments
with regular approximation of context-free lan-
guages. Computational Linguistics, 26(1).
Fernando C.N. Pereira and Michael D. Riley. 1997.
Speech recognition by composition of weighted
finite automata. In E. Roche and Schabes Y.,
editors, Finite State Devices for Natural Lan-
guage Processing. MIT Press, Cambridge, Mas-
sachusetts.
Fernando C.N Pereira and Rebecca Wright. 1997.
Finite-state approximation of phrase-structure
grammars. In E. Roche and Y. Schabes, editors,
Finite-State Language Processing. MIT Press.
G. Riccardi and A.L. Gorin. 2000. Stochastic Lan-
guage Adaptation over Time and State in Natural
Spoken Dialogue Systems. IEEE Transactions on
Speech and Audio, pages 3-10.
G. Riccardi, E. Bocchieri, and R. Pieraccini. 1995.
Non deterministic stochastic language models for
speech recognition. In Proceedings of ICASSP,
pages 247-250, Detroit.
G. Riccardi, R. Pieraccini, and E. Bocchieri. 1996.
Stochastic Automata for Language Modeling.
Computer Speech and Language, 10(4):265-293.
Emmanuel Roche. 1999. Finite state transducers:
parsing free and frozen sentences. In Andras Ko-
rnai, editor, Extened Finite State Models of Lan-
guage. Cambridge University Press.
Verbmobil. 2000. Verbmobil Web page.
http://verbmobil.dfki.de/.
J. Vilar, V.M. Jimenez, J. Amengual, A. Castel-
lanos, D. Llorens, and E. Vidal. 1999. Text
and speech translation by means of subsequential
transducers. In Andras Kornai, editor, Extened
Finite State Models of Language. Cambridge Uni-
versity Press.
Monika Woszczyna, Matthew Broadhead, Donna
Gates, Marsal Gayalda, Alon Lavie, Lori Levin,
and Alex Waibel. 1998. A modular approach to
spoken language translation for large domains. In
Proceedings of AMTA-98, Langhorne, Pennsylva-
nia, October.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377-404.
M. Yamamoto and K. W. Church. 1998. Using Suf-
fix Arrays to Compute Term Frequency and Doc-
ument Frequency for All Substrings in a Corpus.
In Proceedings of ACL Workshop on Very Large
Corpora, pages 28-37, Montreal.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.831766">
<title confidence="0.999752">A Finite-State Approach to Machine Translation*</title>
<author confidence="0.917321">Srinivas Bangalore</author>
<author confidence="0.917321">Giuseppe</author>
<affiliation confidence="0.992133">AT&amp;T</affiliation>
<address confidence="0.996879">180 Park Avenue, Florham Park, NJ</address>
<email confidence="0.999014">fsrini,dsp31@research.att.com</email>
<abstract confidence="0.99561485">The problem of machine translation can be viewed as consisting of two subproblems (a) Lexical Selection and (b) Lexical Reordering. We propose stochastic finite-state models for these two subproblems in this paper. Stochastic finite-state models are efficiently learnable from data, effective for decoding and are associated with a calculus for composing models which allows for tight integration of constraints from various levels of language processing. We present a method for learning stochastic finitestate models for lexical choice and lexical reordering that are trained automatically from pairs of source and target utterances. We use this method to develop models for English-Japanese translation and present the performance of these models for translation on speech and text. We also evaluate the efficacy of such a translation model in the context of a call routing task of unconstrained speech utterances.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Arnold</author>
</authors>
<title>Special issue on evaluation of MT Systems.</title>
<date>1993</date>
<booktitle>Machine Translation,</booktitle>
<pages>8--1</pages>
<contexts>
<context position="12287" citStr="Arnold, 1993" startWordPosition="2007" endWordPosition="2008">he translation system. The data for the experiments reported in this section were obtained from the customer side of operator-customer conversations, with the customer-care application described in (Riccardi and Gorin, 2000). Each of the customer&apos;s utterance transcriptions were then manually translated into Japanese. A total of 15,457 English-Japanese sentence pairs was split into 12,204 training sentence pairs and 3,253 test sentence pairs. 5.1 Evaluation of Machine Translation Systems Evaluation of a machine translation systems has been a subject of discussion for many years (Council, 1966; Arnold, 1993). A universally acceptable, objective and reliable metric that can be computed automatically is yet to be found. However, in the interest of evaluating our translation system automatically and objectively without human intervention, we report the performance of a machine translation system in application independent and in the context of an application. For the application independent evaluation, we employ two metrics based on string edit distance between the output of a translation system and the reference translation string: simple accuracy and translation accuracy (Alshawi et al., 1998b). S</context>
</contexts>
<marker>Arnold, 1993</marker>
<rawString>E. Arnold. 1993. Special issue on evaluation of MT Systems. Machine Translation, 8(1-2):1-126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="2331" citStr="Bangalore and Joshi, 1999" startWordPosition="352" endWordPosition="355">n the context of translation in limited domains. We are also interested in SFST models since they allow for tight integration with a speech recognizer for speech-to-speech translation. In particular, we are interested in one-pass decoding and translation of speech as opposed to the more prevalent approach of translation of speech lattices. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999) and parsing (Roche, 1999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; A number of approaches to SMT, including the seminal work at IBM (Brown et al., 1993), are stochastic string transductions that map source language strings directly to target language strings. There are other approaches to SMT where translation is achieved </context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Stochastic finite-state models for spoken language machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Workshop on Embedded Machine Translation Systems.</booktitle>
<contexts>
<context position="7587" citStr="Bangalore and Riccardi, 2000" startWordPosition="1245" endWordPosition="1248">Ws Ts4 WT. In our case, the SFST model will estimate P(Ws T4&apos; WT) = P(Ws,WT) and the symbol pair (wi, xi) will be associated to each transducer state q with input label wi and output label xi. The model TsT provides a string-tostring transduction from Ws into WT. 3.1 Acquiring Phrasal Translations While word-to-word translation is only approximating the lexical choice process, phrase-to-phrase mapping can greatly improve the translation of collocations, recurrent strings, etc. Moreover, SFSTs can take advantage of the phrasal correlation to improve the computation of the probability P(Ws,WT) (Bangalore and Riccardi, 2000). In this section, we describe an alternate method that uses the result of the alignment module as a seed to acquire bilingual phrases of more than two words length. As mentioned above, we use the alignment information to construct a bilanguage corpus where each token is of the form (wi,xi). Bilingual phrases can be derived from the phrases (substrings) of the bilinguage corpus that have high mutual information score. We acquire bilanguage phrases from the bilanguage corpus by computing weighted mutual information metric of n-grams for arbitrarily large values of n. We use a suffix array to co</context>
</contexts>
<marker>Bangalore, Riccardi, 2000</marker>
<rawString>Srinivas Bangalore and Giuseppe Riccardi. 2000. Stochastic finite-state models for spoken language machine translation. In Proceedings of the Workshop on Embedded Machine Translation Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S D Pietra</author>
<author>V D Pietra</author>
<author>R Mercer</author>
</authors>
<date>1993</date>
<booktitle>The Mathematics of Machine Translation: Parameter Estimation. Computational Linguistics,</booktitle>
<pages>16--2</pages>
<contexts>
<context position="2759" citStr="Brown et al., 1993" startWordPosition="417" endWordPosition="420">uding, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999) and parsing (Roche, 1999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; A number of approaches to SMT, including the seminal work at IBM (Brown et al., 1993), are stochastic string transductions that map source language strings directly to target language strings. There are other approaches to SMT where translation is achieved through tree transductions that map source language trees to target language trees (Alshawi et al., 1998b; Wu, 1997). There are also international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to inte</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S.D. Pietra, V.D. Pietra, and R. Mercer. 1993. The Mathematics of Machine Translation: Parameter Estimation. Computational Linguistics, 16(2):263-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>National Research Council</author>
</authors>
<title>Alpac report.</title>
<date>1966</date>
<tech>Technical report,</tech>
<institution>National Academy of Sciences.</institution>
<contexts>
<context position="12272" citStr="Council, 1966" startWordPosition="2004" endWordPosition="2006">evaluation of the translation system. The data for the experiments reported in this section were obtained from the customer side of operator-customer conversations, with the customer-care application described in (Riccardi and Gorin, 2000). Each of the customer&apos;s utterance transcriptions were then manually translated into Japanese. A total of 15,457 English-Japanese sentence pairs was split into 12,204 training sentence pairs and 3,253 test sentence pairs. 5.1 Evaluation of Machine Translation Systems Evaluation of a machine translation systems has been a subject of discussion for many years (Council, 1966; Arnold, 1993). A universally acceptable, objective and reliable metric that can be computed automatically is yet to be found. However, in the interest of evaluating our translation system automatically and objectively without human intervention, we report the performance of a machine translation system in application independent and in the context of an application. For the application independent evaluation, we employ two metrics based on string edit distance between the output of a translation system and the reference translation string: simple accuracy and translation accuracy (Alshawi et</context>
</contexts>
<marker>Council, 1966</marker>
<rawString>National Research Council. 1966. Alpac report. Technical report, National Academy of Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Gorin</author>
<author>G Riccardi</author>
<author>J H Wright</author>
</authors>
<title>How May I Help You? Speech Communication,</title>
<date>1997</date>
<pages>23--113</pages>
<contexts>
<context position="17731" citStr="Gorin et al., 1997" startWordPosition="2883" endWordPosition="2886">rder before after Reordering Reordering Unigram 21.4 21.7 Bigram 48.9 55.7 Trigram 49.0 56.8 UniPhrase 39.3 39.6 BiPhrase 51.3 56.5 TriPhrase 50.9 56.9 Table 2: Translation Accuracy of the English to Japanese Translation System with and without phrases, before and after reordering on one-best output of the speech recognizer. 5.3 Application Dependent Evaluation: Call Type Classification The objective of this experiment is to measure the performance of a translation system in the context of an application, in our case, a call type classification application task called the How May I Help You? (Gorin et al., 1997) task. We briefly review the problem and the spoken language system. The goal is to sufficiently understand caller&apos;s responses to the open-ended prompt How May I Help You? and route such a call based on the meaning of the response. Thus we aim at extracting a relatively small number of semantic actions from the utterances of a very large set of users who are not trained to the system&apos;s capabilities and limitations. The first utterance of each transaction has been transcribed and marked with a call-type by labelers. There are 14 call-types plus a class other for the complement class. In particu</context>
</contexts>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>A. L. Gorin, G. Riccardi, and J. H Wright. 1997. How May I Help You? Speech Communication, 23:113-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>M Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--3</pages>
<contexts>
<context position="2248" citStr="Kaplan and Kay, 1994" startWordPosition="341" endWordPosition="344"> Machine Translation (SMT). We explore the performance limits of such models in the context of translation in limited domains. We are also interested in SFST models since they allow for tight integration with a speech recognizer for speech-to-speech translation. In particular, we are interested in one-pass decoding and translation of speech as opposed to the more prevalent approach of translation of speech lattices. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999) and parsing (Roche, 1999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; A number of approaches to SMT, including the seminal work at IBM (Brown et al., 1993), are stochastic string transductions that map source language strings directly to target</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>R.M. Kaplan and M. Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331-378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Y Al-Onaizan</author>
</authors>
<title>Translation with finite-state devices.</title>
<date>1998</date>
<booktitle>In Machine translation and the information soup,</booktitle>
<location>Langhorne, PA,</location>
<contexts>
<context position="3599" citStr="Knight and Al-Onaizan, 1998" startWordPosition="541" endWordPosition="544">ce language trees to target language trees (Alshawi et al., 1998b; Wu, 1997). There are also international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Finite-state models for SMT have been previously suggested in the literature (Vilar et al., 1999; Knight and Al-Onaizan, 1998). In (Vilar et al., 1999), a deterministic transducer is used to implement an English-Spanish speech translation system. In (Knight and Al-Onaizan, 1998), finite-state machine translation is based on (Brown et al., 1993) and is used for decoding the target language string. However, no experimental results are reported using this approach. Unlike previous approaches, we subdivide the translation task into lexical choice and lexical re* Anuvaad is a system embodying our approach and can be &apos;Furthermore, software implementing the finite-state calseen at http://www.research.att.comr srini/Anuvaad.</context>
</contexts>
<marker>Knight, Al-Onaizan, 1998</marker>
<rawString>Kevin Knight and Y. Al-Onaizan. 1998. Translation with finite-state devices. In Machine translation and the information soup, Langhorne, PA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K K Koskenniemi</author>
</authors>
<title>Two-level morphology: a general computation model for word-form recognition and production.</title>
<date>1984</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Helsinki.</institution>
<contexts>
<context position="2280" citStr="Koskenniemi, 1984" startWordPosition="346" endWordPosition="347">ore the performance limits of such models in the context of translation in limited domains. We are also interested in SFST models since they allow for tight integration with a speech recognizer for speech-to-speech translation. In particular, we are interested in one-pass decoding and translation of speech as opposed to the more prevalent approach of translation of speech lattices. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999) and parsing (Roche, 1999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; A number of approaches to SMT, including the seminal work at IBM (Brown et al., 1993), are stochastic string transductions that map source language strings directly to target language strings. There are oth</context>
</contexts>
<marker>Koskenniemi, 1984</marker>
<rawString>K. K. Koskenniemi. 1984. Two-level morphology: a general computation model for word-form recognition and production. Ph.D. thesis, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Lori Levin</author>
<author>Monika Woszczyna</author>
<author>Donna Gates</author>
<author>Marsal Gavalda„</author>
<author>Alex Waibel</author>
</authors>
<title>The janus-iii translation system: Speechto-speech translation in multiple domains.</title>
<date>1999</date>
<booktitle>In Proceedings of CSTAR Workshop,</booktitle>
<location>Schwetzingen, Germany,</location>
<marker>Lavie, Levin, Woszczyna, Gates, Gavalda„, Waibel, 1999</marker>
<rawString>Alon Lavie, Lori Levin, Monika Woszczyna, Donna Gates, Marsal Gavalda„ and Alex Waibel. 1999. The janus-iii translation system: Speechto-speech translation in multiple domains. In Proceedings of CSTAR Workshop, Schwetzingen, Germany, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Practical experiments with regular approximation of context-free languages.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="11286" citStr="Nederhof, 2000" startWordPosition="1856" endWordPosition="1857">we compose the result with a transducer that checks for all possible well-formed brackets, for a fixed number of brackets. This can be regarded as a finite-state approximation of a parathensis context-free grammar upto a bounded depth. The resulting string from the composition contains reordering instructions which are interpreted to form the reordered target language sentence. Other interesting approaches involve extracting a context-free grammar from the training corpus and approximating the resulting grammar by a finite-state grammar using techniques discussed in (Pereira and Wright, 1997; Nederhof, 2000). Figure 6 shows the sequence of transductions starting from a source language string that results in a target language string. The intermediate steps involved include lexical choice, parse of the sourceordered target string, reordered parse tree for the target string and the final target string. 5 Experiments and Evaluation In this section, we discuss issues concerning evaluation of the translation system. The data for the experiments reported in this section were obtained from the customer side of operator-customer conversations, with the customer-care application described in (Riccardi and </context>
</contexts>
<marker>Nederhof, 2000</marker>
<rawString>Mark-Jan Nederhof. 2000. Practical experiments with regular approximation of context-free languages. Computational Linguistics, 26(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Michael D Riley</author>
</authors>
<title>Speech recognition by composition of weighted finite automata.</title>
<date>1997</date>
<booktitle>Finite State Devices for Natural Language Processing.</booktitle>
<editor>In E. Roche and Schabes Y., editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="2190" citStr="Pereira and Riley, 1997" startWordPosition="332" endWordPosition="335">an then be composed into a single SFST model for Statistical Machine Translation (SMT). We explore the performance limits of such models in the context of translation in limited domains. We are also interested in SFST models since they allow for tight integration with a speech recognizer for speech-to-speech translation. In particular, we are interested in one-pass decoding and translation of speech as opposed to the more prevalent approach of translation of speech lattices. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999) and parsing (Roche, 1999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; A number of approaches to SMT, including the seminal work at IBM (Brown et al., 1993), are stochastic string transdu</context>
</contexts>
<marker>Pereira, Riley, 1997</marker>
<rawString>Fernando C.N. Pereira and Michael D. Riley. 1997. Speech recognition by composition of weighted finite automata. In E. Roche and Schabes Y., editors, Finite State Devices for Natural Language Processing. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Rebecca Wright</author>
</authors>
<title>Finite-state approximation of phrase-structure grammars.</title>
<date>1997</date>
<booktitle>Finite-State Language Processing.</booktitle>
<editor>In E. Roche and Y. Schabes, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11269" citStr="Pereira and Wright, 1997" startWordPosition="1852" endWordPosition="1855">formed bracketed strings, we compose the result with a transducer that checks for all possible well-formed brackets, for a fixed number of brackets. This can be regarded as a finite-state approximation of a parathensis context-free grammar upto a bounded depth. The resulting string from the composition contains reordering instructions which are interpreted to form the reordered target language sentence. Other interesting approaches involve extracting a context-free grammar from the training corpus and approximating the resulting grammar by a finite-state grammar using techniques discussed in (Pereira and Wright, 1997; Nederhof, 2000). Figure 6 shows the sequence of transductions starting from a source language string that results in a target language string. The intermediate steps involved include lexical choice, parse of the sourceordered target string, reordered parse tree for the target string and the final target string. 5 Experiments and Evaluation In this section, we discuss issues concerning evaluation of the translation system. The data for the experiments reported in this section were obtained from the customer side of operator-customer conversations, with the customer-care application described </context>
</contexts>
<marker>Pereira, Wright, 1997</marker>
<rawString>Fernando C.N Pereira and Rebecca Wright. 1997. Finite-state approximation of phrase-structure grammars. In E. Roche and Y. Schabes, editors, Finite-State Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Riccardi</author>
<author>A L Gorin</author>
</authors>
<title>Stochastic Language Adaptation over Time and State in Natural Spoken Dialogue Systems.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio,</journal>
<pages>3--10</pages>
<contexts>
<context position="11898" citStr="Riccardi and Gorin, 2000" startWordPosition="1947" endWordPosition="1950">derhof, 2000). Figure 6 shows the sequence of transductions starting from a source language string that results in a target language string. The intermediate steps involved include lexical choice, parse of the sourceordered target string, reordered parse tree for the target string and the final target string. 5 Experiments and Evaluation In this section, we discuss issues concerning evaluation of the translation system. The data for the experiments reported in this section were obtained from the customer side of operator-customer conversations, with the customer-care application described in (Riccardi and Gorin, 2000). Each of the customer&apos;s utterance transcriptions were then manually translated into Japanese. A total of 15,457 English-Japanese sentence pairs was split into 12,204 training sentence pairs and 3,253 test sentence pairs. 5.1 Evaluation of Machine Translation Systems Evaluation of a machine translation systems has been a subject of discussion for many years (Council, 1966; Arnold, 1993). A universally acceptable, objective and reliable metric that can be computed automatically is yet to be found. However, in the interest of evaluating our translation system automatically and objectively withou</context>
</contexts>
<marker>Riccardi, Gorin, 2000</marker>
<rawString>G. Riccardi and A.L. Gorin. 2000. Stochastic Language Adaptation over Time and State in Natural Spoken Dialogue Systems. IEEE Transactions on Speech and Audio, pages 3-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Riccardi</author>
<author>E Bocchieri</author>
<author>R Pieraccini</author>
</authors>
<title>Non deterministic stochastic language models for speech recognition.</title>
<date>1995</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>247--250</pages>
<location>Detroit.</location>
<marker>Riccardi, Bocchieri, Pieraccini, 1995</marker>
<rawString>G. Riccardi, E. Bocchieri, and R. Pieraccini. 1995. Non deterministic stochastic language models for speech recognition. In Proceedings of ICASSP, pages 247-250, Detroit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Riccardi</author>
<author>R Pieraccini</author>
<author>E Bocchieri</author>
</authors>
<title>Stochastic Automata for Language Modeling. Computer Speech and Language,</title>
<date>1996</date>
<pages>10--4</pages>
<contexts>
<context position="2214" citStr="Riccardi et al., 1996" startWordPosition="336" endWordPosition="339">a single SFST model for Statistical Machine Translation (SMT). We explore the performance limits of such models in the context of translation in limited domains. We are also interested in SFST models since they allow for tight integration with a speech recognizer for speech-to-speech translation. In particular, we are interested in one-pass decoding and translation of speech as opposed to the more prevalent approach of translation of speech lattices. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999) and parsing (Roche, 1999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; A number of approaches to SMT, including the seminal work at IBM (Brown et al., 1993), are stochastic string transductions that map source l</context>
<context position="6828" citStr="Riccardi et al., 1996" startWordPosition="1114" endWordPosition="1117">sting of source-target symbol pair sequences T = . (wi, xi) . . ., where the source word wi E Ls U E and its aligned word xi E LT U E (E is the null symbol). Note that the tokens of a bilanguage could be either ordered according to the word order of the source language or ordered according to the word order of the target language. From the cor3The Japanese string was translated and segmented so that a token boundary in Japanese corresponds to some token boundary in English. pus T, we train a Stochastic Finite State Transducer (SFST) which is an extension of the Variable Ngram State Automaton (Riccardi et al., 1996). Stochastic transducers rsT : Ls x LT —} [0, 1] map the string Ws E LS into WT E LT and assign a probability to the transduction Ws Ts4 WT. In our case, the SFST model will estimate P(Ws T4&apos; WT) = P(Ws,WT) and the symbol pair (wi, xi) will be associated to each transducer state q with input label wi and output label xi. The model TsT provides a string-tostring transduction from Ws into WT. 3.1 Acquiring Phrasal Translations While word-to-word translation is only approximating the lexical choice process, phrase-to-phrase mapping can greatly improve the translation of collocations, recurrent st</context>
</contexts>
<marker>Riccardi, Pieraccini, Bocchieri, 1996</marker>
<rawString>G. Riccardi, R. Pieraccini, and E. Bocchieri. 1996. Stochastic Automata for Language Modeling. Computer Speech and Language, 10(4):265-293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Roche</author>
</authors>
<title>Finite state transducers: parsing free and frozen sentences.</title>
<date>1999</date>
<editor>In Andras Kornai, editor, Extened</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2357" citStr="Roche, 1999" startWordPosition="359" endWordPosition="360">domains. We are also interested in SFST models since they allow for tight integration with a speech recognizer for speech-to-speech translation. In particular, we are interested in one-pass decoding and translation of speech as opposed to the more prevalent approach of translation of speech lattices. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999) and parsing (Roche, 1999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; A number of approaches to SMT, including the seminal work at IBM (Brown et al., 1993), are stochastic string transductions that map source language strings directly to target language strings. There are other approaches to SMT where translation is achieved through tree transductions</context>
</contexts>
<marker>Roche, 1999</marker>
<rawString>Emmanuel Roche. 1999. Finite state transducers: parsing free and frozen sentences. In Andras Kornai, editor, Extened Finite State Models of Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verbmobil</author>
</authors>
<title>Verbmobil Web page.</title>
<date>2000</date>
<note>http://verbmobil.dfki.de/.</note>
<contexts>
<context position="3133" citStr="Verbmobil, 2000" startWordPosition="477" endWordPosition="478">(c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; A number of approaches to SMT, including the seminal work at IBM (Brown et al., 1993), are stochastic string transductions that map source language strings directly to target language strings. There are other approaches to SMT where translation is achieved through tree transductions that map source language trees to target language trees (Alshawi et al., 1998b; Wu, 1997). There are also international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Finite-state models for SMT have been previously suggested in the literature (Vilar et al., 1999; Knight and Al-Onaizan, 1998). In (Vilar et al., 1999), a deterministic transducer is used to implement an English-Spanish speech translation system. In (Knight an</context>
</contexts>
<marker>Verbmobil, 2000</marker>
<rawString>Verbmobil. 2000. Verbmobil Web page. http://verbmobil.dfki.de/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Vilar</author>
<author>V M Jimenez</author>
<author>J Amengual</author>
<author>A Castellanos</author>
<author>D Llorens</author>
<author>E Vidal</author>
</authors>
<title>Text and speech translation by means of subsequential transducers.</title>
<date>1999</date>
<editor>In Andras Kornai, editor, Extened</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3569" citStr="Vilar et al., 1999" startWordPosition="537" endWordPosition="540">ctions that map source language trees to target language trees (Alshawi et al., 1998b; Wu, 1997). There are also international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Finite-state models for SMT have been previously suggested in the literature (Vilar et al., 1999; Knight and Al-Onaizan, 1998). In (Vilar et al., 1999), a deterministic transducer is used to implement an English-Spanish speech translation system. In (Knight and Al-Onaizan, 1998), finite-state machine translation is based on (Brown et al., 1993) and is used for decoding the target language string. However, no experimental results are reported using this approach. Unlike previous approaches, we subdivide the translation task into lexical choice and lexical re* Anuvaad is a system embodying our approach and can be &apos;Furthermore, software implementing the finite-state calseen at http://www.re</context>
</contexts>
<marker>Vilar, Jimenez, Amengual, Castellanos, Llorens, Vidal, 1999</marker>
<rawString>J. Vilar, V.M. Jimenez, J. Amengual, A. Castellanos, D. Llorens, and E. Vidal. 1999. Text and speech translation by means of subsequential transducers. In Andras Kornai, editor, Extened Finite State Models of Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monika Woszczyna</author>
<author>Matthew Broadhead</author>
<author>Donna Gates</author>
<author>Marsal Gayalda</author>
<author>Alon Lavie</author>
<author>Lori Levin</author>
<author>Alex Waibel</author>
</authors>
<title>A modular approach to spoken language translation for large domains. In</title>
<date>1998</date>
<booktitle>Proceedings of AMTA-98,</booktitle>
<location>Langhorne, Pennsylvania,</location>
<contexts>
<context position="3167" citStr="Woszczyna et al., 1998" startWordPosition="481" endWordPosition="484">us for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; A number of approaches to SMT, including the seminal work at IBM (Brown et al., 1993), are stochastic string transductions that map source language strings directly to target language strings. There are other approaches to SMT where translation is achieved through tree transductions that map source language trees to target language trees (Alshawi et al., 1998b; Wu, 1997). There are also international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Finite-state models for SMT have been previously suggested in the literature (Vilar et al., 1999; Knight and Al-Onaizan, 1998). In (Vilar et al., 1999), a deterministic transducer is used to implement an English-Spanish speech translation system. In (Knight and Al-Onaizan, 1998), finite-state </context>
</contexts>
<marker>Woszczyna, Broadhead, Gates, Gayalda, Lavie, Levin, Waibel, 1998</marker>
<rawString>Monika Woszczyna, Matthew Broadhead, Donna Gates, Marsal Gayalda, Alon Lavie, Lori Levin, and Alex Waibel. 1998. A modular approach to spoken language translation for large domains. In Proceedings of AMTA-98, Langhorne, Pennsylvania, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="3047" citStr="Wu, 1997" startWordPosition="464" endWordPosition="465">y are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; A number of approaches to SMT, including the seminal work at IBM (Brown et al., 1993), are stochastic string transductions that map source language strings directly to target language strings. There are other approaches to SMT where translation is achieved through tree transductions that map source language trees to target language trees (Alshawi et al., 1998b; Wu, 1997). There are also international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Finite-state models for SMT have been previously suggested in the literature (Vilar et al., 1999; Knight and Al-Onaizan, 1998). In (Vilar et al., 1999), a deterministic trans</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yamamoto</author>
<author>K W Church</author>
</authors>
<title>Using Suffix Arrays to Compute Term Frequency and Document Frequency for All Substrings in a Corpus.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL Workshop on Very Large Corpora,</booktitle>
<pages>28--37</pages>
<location>Montreal.</location>
<contexts>
<context position="8288" citStr="Yamamoto and Church, 1998" startWordPosition="1360" endWordPosition="1363">the alignment module as a seed to acquire bilingual phrases of more than two words length. As mentioned above, we use the alignment information to construct a bilanguage corpus where each token is of the form (wi,xi). Bilingual phrases can be derived from the phrases (substrings) of the bilinguage corpus that have high mutual information score. We acquire bilanguage phrases from the bilanguage corpus by computing weighted mutual information metric of n-grams for arbitrarily large values of n. We use a suffix array to compute the frequencies of large n-grams similar to the method presented in (Yamamoto and Church, 1998). Since the phrases acquired from a source(target) ordered bilanguage corpus may not have the target(source) language words in the order of the target(source) language, we introduce a reordering phase for the words in a phrase which we call local reordering. In the local reordering phase, for each phrase we select an alignment which aligns each source word with some word(s) in the target phrase. We then reorder the words of the target phrase such that the reordering corresponds to a substring (consecutive words) of the target sentence in the selected alignment. A sample set of phrases after re</context>
</contexts>
<marker>Yamamoto, Church, 1998</marker>
<rawString>M. Yamamoto and K. W. Church. 1998. Using Suffix Arrays to Compute Term Frequency and Document Frequency for All Substrings in a Corpus. In Proceedings of ACL Workshop on Very Large Corpora, pages 28-37, Montreal.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>