<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010527">
<title confidence="0.9953395">
Using Supertags and Encoded Annotation Principles for Improved
Dependency to Phrase Structure Conversion
</title>
<author confidence="0.980735">
Seth Kulick and Ann Bies and Justin Mott
</author>
<affiliation confidence="0.9730345">
Linguistic Data Consortium
University of Pennsylvania
</affiliation>
<address confidence="0.896424">
Philadelphia, PA 19104
</address>
<email confidence="0.999782">
{skulick,bies,jmott}@ldc.upenn.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960411764706">
We investigate the problem of automatically
converting from a dependency representa-
tion to a phrase structure representation, a
key aspect of understanding the relationship
between these two representations for NLP
work. We implement a new approach to this
problem, based on a small number of su-
pertags, along with an encoding of some of
the underlying principles of the Penn Tree-
bank guidelines. The resulting system signifi-
cantly outperforms previous work in such au-
tomatic conversion. We also achieve compara-
ble results to a system using a phrase-structure
parser for the conversion. A comparison with
our system using either the part-of-speech tags
or the supertags provides some indication of
what the parser is contributing.
</bodyText>
<sectionHeader confidence="0.995669" genericHeader="keywords">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.997232958333333">
Recent years have seen a significant increase in
interest in dependency treebanks and dependency
parsing. Since the standard training and test set for
English parsing is a phrase structure (PS) treebank,
the Penn Treebank (PTB) (Marcus et al., 1993; Mar-
cus et al., 1994), the usual approach is to convert this
to a dependency structure (DS) treebank, by means
of various heuristics for identifying heads in a PS
tree. The resulting DS representation is then used
for training and parsing, with results reported on the
DS representation.
Our goal in this paper is to go in the reverse di-
rection, from the DS to PS representation, by find-
ing a minimal DS representation from which we can
use an approximate version of the principles of the
PTB guidelines to reconstruct the PS. Work in this
conversion direction is somewhat less studied (Xia
et al., 2009; Xia and Palmer, 2001), but it is still
an important topic for a number of reasons. First,
because both DS and PS treebanks are of current in-
terest, there is an increasing effort made to create
multi-representational treebank resources with both
DS and PS available from the beginning, without a
loss of information in either direction (Xia et al.,
2009). Second, it is sometimes the case that it is
convenient to do annotation in a dependency repre-
sentation (e.g., if the annotators are already famil-
iar with such a representation), though the treebank
will in final form be either phrase-structure or multi-
representational (Xia et al., 2009).
However, our concern is somewhat different. We
are specifically interested in experimenting with de-
pendency parsing of Arabic as a step in the annota-
tion of the Arabic Treebank, which is a phrase struc-
ture treebank (Maamouri et al., 2011). Although we
currently use a phrase structure parser in this annota-
tion pipeline, there are advantages to the flexibility
of being able to experiment with advances in pars-
ing technology for dependency parsing. We would
like to parse with a dependency representation of the
data, and then convert the parser output to a phrase
structure representation so that it can feed into the
annotation pipeline. Therefore, in order to make use
of dependency parsers, we need a conversion from
dependency to phrase structure with very high accu-
racy, which is the goal of this paper.
While one of our underlying concerns is DS to
PS conversion for Arabic, we are first focusing on
</bodyText>
<page confidence="0.655591666666667">
305
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 305–314,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999941022988506">
a conversion routine for the English PTB because it
is so well-established and the results are easier to
interpret. The intent is then to transfer this conver-
sion algorithm work to the Arabic treebank as well.
We expect this to be successful because the ATB has
some fundamental similarities to the PTB in spite of
the language difference (Maamouri and Bies, 2004).
As mentioned above, one goal in our DS to PS
conversion work is to base it on a minimal DS rep-
resentation. By “minimal”, we mean that it does
not include information that is redundant, together
with our conversion code, with the implicit informa-
tion in the dependency structure itself. As discussed
more in Section 2.1, we aim to make our dependency
representation simpler than “hybrid” representations
such as Johansson and Nugues (2007). The rea-
son for our interest in this minimal representation
is parsing. We do not want to require the parser to
recover such a complex dependency representations,
when it is, in fact, unnecessary, as we believe our ap-
proach shows. The benefit of this approach can only
be seen when this line of work is extended to ex-
periments with parsing and Arabic conversion. The
work described here is just the first step in this pro-
cess.
A conversion scheme, such as ours, necessarily
relies on some details of the annotation content in
the DS and PS representations, and so our algorithm
is not an algorithm designed to take as input any ar-
bitrary DS representation. However, the fundamen-
tals of our dependency representation are not radi-
cally different than others - e.g. we make an auxil-
iary verb the child of the main verb, instead of the
other way, but such choices can be adjusted for in
the conversion.
To evaluate the success of this conversion algo-
rithm, we follow the same evaluation procedure as
Xia et al. (2009) and Xia and Palmer (2001). We
convert the PTB to a DS, and then use our algorithm
to convert the DS back to a PS representation. The
original PS and the converted-from-DS PS are then
compared, in exactly the same way as parser output
is compared with the original (gold) tree. We will
show that our results in this area are a significant
improvement above previous efforts.
A key aspect of this work is that our DS-to-PS
conversion encodes many of the properties of the
PTB annotation guidelines (Bies et al., 1995), both
globally and for specific XP projections. The PTB
guidelines are built upon broad decisions about PS
representation that provide an overall framework
and cohesion for the details of the PS trees. To
implement these underlying principles of the guide-
lines, we defined a set of 30 “supertags” that indi-
cate how a lexical item can project in the syntac-
tic structure, allowing us to specify these principles.
We describe these as supertags because of a concep-
tual similarity to the supertagging work in the Tree
Adjoining Grammar (TAG) tradition (Bangalore and
Joshi, 2010), although ours is far smaller than a typ-
ical supertag set, and indeed is actually smaller than
the PTB POS tag set.
Our DS-to-PS code is based on this set of su-
pertags, and can be run using either the supertags
created from the gold POS tags, or using the POS
tags, together with the dependency structure to first
(imperfectly) derive the supertags, and then proceed
with the conversion. This choice of starting point al-
lows us to measure the impact of POS tag complex-
ities on the DS-to-PS conversion, which provides an
interesting insight on what a phrase structure parser
contributes in addition to this sort of automated DS-
to-PS conversion, as discussed in Section 4.
We have chosen this approach of encoding under-
lying principles of the PTB guidelines for two rea-
sons. First, these principles are non-statistical, and
thus we felt it would let us tease apart the contri-
bution of the frequency information relating, e.g.,
heads, on the one hand, and the basic notions of
phrase structure on the other. The second reason is
that it was quite easy to implement these principles.
We did not attempt a complete examination of every
possible rule in Bies et al. (1995), but rather just se-
lected the most obvious ones. As we will see in Sec-
tion 4.2, our results indeed are sometimes hurt by
such lack of thoroughness, although in future work
we will make this more complete.
</bodyText>
<sectionHeader confidence="0.983506" genericHeader="introduction">
2 Overview and Example
</sectionHeader>
<bodyText confidence="0.9991975">
Figures 1-4 provide a running example of the four
steps in the process. Figure 1 is the original tree
from the Penn Treebank. Figures 2 and 3 illustrate
the two-step process of creating the dependency rep-
resentation, and Figure 4 shows the conversion back
to phrase structure.
</bodyText>
<page confidence="0.999247">
306
</page>
<figureCaption confidence="0.9998505">
Figure 2: Tree Insertion Grammar decomposition of Figure
Figure 1: Penn Treebank tree 1
</figureCaption>
<figure confidence="0.999893741176471">
ADVP
NP-SBJ
VP
IN
from
NP
NNP
GM
RB
generally
VBN
mixed
S
ADVP
Aside
PP
from
makers
NP-SBJ
posted
VP
ADJP
results
NP-OBJ
mixed
RB
Aside
PP
JJ
other
NN
car
NNS
makers
VBD
posted
ADJP
NP
NNS
results
NP-OBJ
GM
other car
generally
NP-SBJ
VP
ADVP
NN/P PRENOM
car
JJ/P ADJP
other
RB/P ADVP
Aside
IN/P PP
from
NNS/P NP-SBJ
makers
NNS/P NP-OBJ
results
VBN/P ADJP
mixed
PP
NP-OBJ
NNP
GM
NNS
makers
VBD
posted
NP-OBJ
NNS
results
RB
Aside
JJ
other
NN
car
IN
from
ADJP
VBD/P VP
posted
NNP/P NP-OBJ RB/P ADVP RB VBN
GM generally generally mixed
</figure>
<figureCaption confidence="0.88841525">
Figure 3: Dependency representation derived from TIG de-
composition in Figure 2
Figure 4: Conversion of dependency representation in Fig-
ure 3 back to phrase structure.
</figureCaption>
<subsectionHeader confidence="0.999389">
2.1 Creation of Dependency Representation
</subsectionHeader>
<bodyText confidence="0.999971282051282">
The creation of the dependency representation is
similar in basic aspects to many other approaches, in
that we utilize some basic assumptions about head
relations to decompose the full tree into smaller
units. However, we first decompose the original
trees into a Tree Insertion Grammar representation
(Chiang, 2003), utilizing tree substitution and sister
adjunction. We refer the reader to Chiang (2003) for
details of these operations, and instead focus on the
fact that the TIG derivation tree in Figure 2 parti-
tions the phrase structure representation in Figure 1
into smaller units, called elementary trees. We leave
out the POS tags in Figure 2 to avoid clutter.
The creation of the dependency representation is
structurally a simple rewrite of the TIG derivation,
taking the word associated with each elementary tree
and using it as a node in the dependency tree. In
this way, the dependency representation in Figure 3
follows immediately from Figure 2.
However, in addition, we utilize the TIG deriva-
tion tree and the structures of the elementary trees to
create a supertag (in the sense discussed in Section
1) for each word. For example, aside heads an ele-
mentary tree that projects to ADVP, so it is assigned
the supertag P ADVP in Figure 3, meaning that it
projects to ADVP. We label each node in Figure 3
with both its POS tag and supertag, so in this case
the node for aside has RB/P ADVP.
There are two typical cases that are not so
straightforward. The first concerns elementary trees
with more than one level of projection, such as that
for the verb, posted, which has two levels of pro-
jection, S and VP. In such cases we base the supertag
only on the immediate parent of the word. For ex-
ample, in this case the supertag for posted is P VP,
rather than P S. As will be seen in Section 3.2, our
perspective is that the local context of the depen-
dency tree will provide the necessary disambigua-
tion as to what node is above the VP.
</bodyText>
<page confidence="0.994475">
307
</page>
<table confidence="0.999892">
Projection Type Supertag
NP P NP
ADJP P ADJP
ADVP P ADVP
PP P PP, P WHPP
S,SINV,SQ P VP
QP,NP,QP-NP,QP-ADJP P QP
WHNP P WHNP
default P WHADVP, P INTJ, P PRT, P LST
none P AUX, P PRENOM, P DET, P COMMA, P PERIOD, P CC, P COMP,
P POS, P PRP$, P BACKDQUOTE, P DQUOTE, P COLON, P DOLLAR,
P LRB, P RB, P PDT, P SYM, P FW, P POUND
</table>
<tableCaption confidence="0.992652">
Table 1: 30 supertags handled by 14 projection types. The ambiguity in some, such as P VP projecting as S, SINV,
SQ is handled by an examination of the dependency structure.
</tableCaption>
<bodyText confidence="0.999550030769231">
The second non-straightforward case1 is that of
degenerate elementary trees, in which the “tree”
is just the word itself, as for other, car, and
generally. In such cases we default the supertag
based on the original POS tag, and in some cases, the
tree configuration. For example, a word with the JJ
tag, such as other, would get the supertag P ADJP,
with the RB tag such as generally the supertag
P ADVP. We assign prenominal nouns such as car
here the tag P PRENOM.
Generating supertags in this way is a convenient
way to correct some of the POS tag errors in the PTB
(Manning, 2011). For example, if that has the (in-
correct) tag DT in the complementizer position, it
still receives the new POS tag P COMP.
This procedure results in a set of 30 supertags, and
Table 1 shows how they are partitioned into 14 pro-
jection types. These supertags and projection types
are the basis of our DS-to-PS conversion, as dis-
cussed further in Section 2.2.
We note here a brief comparison with earlier work
on “hybrid” representations, which encode a PS rep-
resentation inside a DS one, in order to convert from
the latter to the former. (Hall and Nivre, 2008; Jo-
han Hall and Nilsson, 2007; Johansson and Nugues,
2007). Our goal is very different. Instead of en-
1There are other details not discussed here. For example, we
do not automatically assign a P NP supertag to the head child
of an NP, since such a head can legitimately be, e.g, a JJ, in
which case we make the supertag P ADJP, on the reasoning that
it would be encoding “too much” to treat it as P NP. Instead, we
rely on the DS and such labels as SBJ or OBJ to determine when
to project it as NP in the converted PS.
coding the phrase structure in the dependency tree
via complex tags such as SBARQ in Johansson and
Nugues (2007), we use a minimal representation and
rely on our encoding of the general principles of
PTB phrase structure to carry much of the weight.
While supertags such as P VP may appear to encode
some of the structure, their primary role is as an in-
termediate link between the POS tags and the phrase
structure conversion. The created supertags are not
in fact necessary for this conversion. As we will see
in the following sections, we convert from DS to PS
using either just the original POS tags, or with our
created supertags.
We also include five labels in the dependency rep-
resentation: SBJ, OBJ, PRN, COORD CONJ, APP.
The example dependency tree in Figure 3 includes
instances of the SBJ and OBJ labels, in italics on
the node instead of the edges, for convenience. The
SBJ label is of course already a function tag in the
PTB. We process the PTB when creating the TIG
decomposition to add an OBJ tag, as well basing the
PRN label on the occurrence of the PRN node. We
also use heuristics to identify cases of coordination
and apposition, resulting in the COORD CONJ and
APP tags. The reasons for including these labels is
that they prove useful in the conversion to phrase
structure, as illustrated in some of the examples be-
low.
Before moving on to the dependency-to-phrase-
stucture conversion, we end this section with a com-
ment on the role of function tags and empty cate-
gories. The PTB makes use of function tags to in-
</bodyText>
<page confidence="0.995335">
308
</page>
<bodyText confidence="0.999984833333333">
dicate certain syntactic and semantic information,
and of empty categories (and co-indexing) for a
more complete and accurate syntactic representa-
tion. There is some overlap between the five la-
bels we use, as just described, and the PTB func-
tion tags, but in general we do not encode the full
range of function tags in our representation, saving
this for future work. More significantly, we also
do not include empty categories and associated co-
indexing, which has the consequence that the depen-
dency trees are projective.
The reason we have not included these aspects in
our representation and conversion yet is that we are
focused here first on the evaluation for comparison
with previous work, and the basis for this previous
work is the usual evalb program (Sekine and Collins,
2008), which ignores function tags and empty cate-
gories. We return to this issue in the conclusion.
</bodyText>
<subsectionHeader confidence="0.996709">
2.2 From Dependency to Phrase Structure
</subsectionHeader>
<bodyText confidence="0.999970818181818">
There are two key aspects to the conversion from de-
pendency to phrase structure. (1) We encode general
conventions about annotation that are used through-
out the annotation guidelines for the PTB. A com-
mon example is that of the “single-word” rule, in
which a constituent consisting of just a single word
is reduced to just that word, without the constituent
bracketing, in many cases. (2) We use the set of su-
pertags as the basis for defining projection-specific
rules for how to attach children on the left or right of
the head, in many cases utilizing the supertag names
that we include to determine the specific attachment.
For example, the leaf GM in Figure 3 has the su-
pertag P NP (with the label OBJ), so heading a NP
projection, (NP GM). Its parent node, from, has
the supertag P PP, indicating that it heads a PP pro-
jection, and so attaches the (NP GM) as a sister of
from. It does not reduce it down as a single word,
because the encoding of the PP projection specifies
that it does not do so for children on its right.
A more substantial case is that of the NP other
car makers. Here the head noun, makers,
has the supertag P NP, and so projects as an NP.
Its first child, other, has the supertag P ADJP,
and so projects as an ADJP, resulting in (ADJP
other). The second child, car, has the supertag
P PRENOM (prenominal), and so does not project
at all. When the NP projection for makers is as-
sembled, it applies the “single-word” constraint to
children on its left (as encoded in the definition
of the NP projection), thus stripping the ADJP off
of other, resulting in the desired flat NP other
car makers. Likewise, the ADVP projection for
generally is stripped off before it is attached as
a left sister of the ADJP projection mixed. The
encoding of a VP projection specifies that it must
project above VP if it is the root of the tree, and so
the VP projection for posted projects to S (by de-
fault).
In this way we can see that encoding some of
the general characteristics of the annotation guide-
lines allows the particular details of the PTB phrase-
structure representation to be created from the less-
specific dependency representation.
</bodyText>
<sectionHeader confidence="0.961378" genericHeader="method">
3 Some Further Examples
</sectionHeader>
<subsectionHeader confidence="0.951027">
3.1 QP Projection or Reduction
</subsectionHeader>
<bodyText confidence="0.99999128">
As mentioned in Section 2.2, the “single word” con-
vention is implemented in the conversion to PS, as
was the case with other in the previous section.
The projection associated with P QP has a slight
twist to this principle, because of the nature of some
of the financialspeak in the PTB. In particular, the
dollar sign is treated as a displaced word and is
therefore not counted, in a QP constituent, as a token
for purposes of the “single token” rule.
For example, (1abc) in Figure 5 illustrates a case
where the QP structure projects to an NP node as
well. (1a) is the original PTB PS tree, and (1b) is
the DS representation. Note that billion heads
the about $ 9 billion subtree, with the su-
pertag P QP and the label OBJ.2 Because it has more
than one child in addition to the $, it is converted to
phrase structure as a QP under an NP, implying the
empty *U*, although we do not actually put it in.
In contrast, (2abc) is a case in which the QP node
is not generated. 100 is the head of the phrase $ 100
*U* in the PTB PS (a), as shown in the dependency
structure (b). However, because it only has one child
in addition to the $, no additional QP node is cre-
ated in the phrase structure representation in (c). We
stress that the presence of the QP in (1a) and its ab-
</bodyText>
<footnote confidence="0.979934666666667">
2A good case can be made that in fact $ should be the daugh-
ter of to in the dependency tree, although we have not imple-
mented this as such.
</footnote>
<page confidence="0.995437">
309
</page>
<figureCaption confidence="0.999758">
Figure 5: Examples of handling of QP in dependency to phrase-structure conversion.
</figureCaption>
<figure confidence="0.999609796875">
(C)
PP
(B)
P PP
for
IN
for
NP
(C)
PP
NP
P PP
to
NP
QP
(1) (A)
PP
TO
to
(B)
P PP
to
P QP-OBJ
billion
$
$
CD
9
IN
about
-NONE-
*U*
P PP
about
P DOLLAR
$
P QP
9
P PP
about
P DOLLAR
$
P QP
9
P QP
billion
QP
CD
billion
P PP
for
NP-OBJ
P QP
100
P DOLLAR
$
P QP-OBJ
100
P DOLLAR
$
(2) (A)
PP
$ CD -NONE-
$ 100 *U*
</figure>
<bodyText confidence="0.5325645">
sence in (2a) is correct annotation, consistent with
the annotation guidelines.
</bodyText>
<subsectionHeader confidence="0.999644">
3.2 Refinement of VP Projections
</subsectionHeader>
<bodyText confidence="0.99971245">
As mentioned above, instead of having separate su-
pertags for S, SINV, SQ, SBAR, SBARQ, we use
only the P VP supertag and let the context determine
the specifics of the projection. Sentences (3ab) in
Figure 6 illustrate how the SBJ label is used to treat
the P VP supertag as indicating projection to SINV
(or SQ) instead of S. The determination is based on
the children of the P VP node. For example, if there
is a child with the P AUX supertag which is before a
child with the SBJ label, which in turn is before the
P VP node itself, then the latter is treated as project-
ing to either SINV or SQ, depending on the some
additional factors, primarily whether there is a WH
word among the children. In this example, there is
no WH word, so it becomes a SINV.3 We note here
that we also include a simple listing of verbs that
take complements of certain types - such as verbs of
saying, etc., that take SBAR complements, so that a
VP will project not just to S, but SBAR, even if the
complement is missing.
</bodyText>
<subsectionHeader confidence="0.99777">
3.3 Coordination
</subsectionHeader>
<bodyText confidence="0.999809">
We represent coordination in the dependency in
one of the standard ways, by making the follow-
ing conjuncts be children of the head word of
</bodyText>
<footnote confidence="0.975979333333333">
3This is not a fully precise implementation of the condi-
tions distinguishing SQ and SINV projections, in that it does
not properly check for whether the clause is a question.
</footnote>
<figureCaption confidence="0.947599666666667">
Figure 6: (3ab) shows that the local context of the P VP
supertag in the dependency tree results in a SINV struc-
ture in the converted phrase structure tree (3b).
</figureCaption>
<bodyText confidence="0.9315476">
the first conjunct. For example, a dependency
representation of ...turn down the volume
and close the curtains is shown in (4a) in
Figure 7. The conjunct close the curtains
is converted as a VP projection projecting to S. How-
ever, when the projection for turn is assembled, the
code checks if the conjuncts are missing subjects,
and if so, reduces the configuration to standard VP
coordination, as in (4b). The COORD label is used
to identify such structures for examination.
</bodyText>
<sectionHeader confidence="0.997399" genericHeader="evaluation">
4 Results of Dependency to Phrase
</sectionHeader>
<subsectionHeader confidence="0.65807">
Structure Conversion
</subsectionHeader>
<bodyText confidence="0.999803428571428">
To evaluate the correctness of conversion from de-
pendency to phrase structure, we follow the same
strategy as Xia and Palmer (2001) and Xia et al.
(2009). We convert the phrase structure trees in the
PTB to dependency structure and convert the depen-
dency back to phrase structure. We then compare
the original PTB trees with the newly-created phrase
</bodyText>
<figure confidence="0.995433407407407">
(3) (A)
P VP
absorbed
(B)
SINV
P DET
the
DT
the
NN
cost
VBN
been
VP
VBN
absorbed
P AUX
had
P NP-SBJ
cost
P VP
been
VBD
had
NP-SBJ
VP
310
(B)
VP
P VP-COORD
close
P CC
and
P NP
volume
P PRT
down
VP
P DET
the
P NP-OBJ
curtains
(4) (A)
P VP
turn
and VP
close NP-OBJ
the curtains
P DET
the
turn PRT
down
NP-OBJ
the volume
</figure>
<figureCaption confidence="0.997401">
Figure 7: (4a) is the dependency representation of a coordination structure, and the resulting phrase structure (4b)
shows that the conversion treated it as VP coordination, due to the absence of a subject.
</figureCaption>
<table confidence="0.999883857142857">
Sec System rec prec f
00 Xia &amp; Palmer ’01 86.2 88.7 87.5
Xia et al. ’09 91.8 89.2 90.5
USE-POS-UNLABEL 96.6 97.4 97.0
USE-POS 94.6 95.4 95.0
USE-SUPER 95.9 97.0 96.4
22 Xia et al. ’09 90.7 88.1 89.4
USE-POS 95.0 95.5 95.3
USE-SUPER 96.4 97.1 96.7
23 Wang &amp; Zong ’10 95.9 96.3 96.1
USE-POS 94.8 95.7 95.3
USE-SUPER 96.2 97.3 96.7
24 USE-POS 94.0 94.7 94.4
USE-SUPER 95.9 97.1 96.5
</table>
<tableCaption confidence="0.99876">
Table 2: Results of dependency to phrase structure con-
</tableCaption>
<bodyText confidence="0.991463653061225">
version. For our system, the results are presented in two
ways, using either the gold part-of-speech tags (USE-
POS) or our gold supertags (USE-SUPER). For purposes
of comparison with Xia and Palmer (2001) and Xia et
al. (2009), we also present the results for Section 00 us-
ing part-of-speech tags, but with an unlabeled evaluation
(USE-POS-UNLABEL).
structure trees, using the standard evalb scoring code
(Sekine and Collins, 2008). Xia and Palmer (2001)
defined three different algorithms for the conversion,
utilizing different heuristics for how to build projec-
tion chains, and where to attach dependent subtrees.
They reported results for their system for Section 00
of the PTB, and we include in Table 2 only their
highest scoring algorithm. The system of Xia et al.
(2009) uses conversion rules learned from Section
19, and then tested on Sections 00 and Section 22.
We developed the algorithm using Section 24, and
we also report results for Sections 00, 22, and 23, for
comparison with previous work. We ran our system
in two ways. In one we use the “gold” supertags
that were created as described in Section 2.1 (USE-
SUPER), based on the TIG decomposition of the
original tree. In the other (USE-POS) we use the
gold POS tags, and not the supertags. Because our
DS-to-PS algorithm is based on using the supertags
to guide the conversion, the USE-POS runs work
by using a few straightforward heuristics to guess
the correct supertag from the POS tag and the de-
pendency structure. For example, if a word x has
the POS tag “TO” and the word y to its immediate
right is its parent in the dependency tree and y has
one of the verbal POS tags, then x receives the su-
pertag P AUX, and otherwise P PP. Any word with
the POS tag JJ, JJR, or JJS, receives the supertag
P ADJP, and so on. The results for Xia and Palmer
(2001) and Xia et al. (2009) were reported using an
unlabeled version of evalb, so to compare properly
we also report our results for Section 00 using an
unlabeled evaluation of the run using the POS tags
(USE-POS-UNLABEL), while all the other results
use a labeled evaluation.
We also compare our system with that of Wang
and Zong (2010). Unlike the three other systems
(including ours), this was not based on an automatic
conversion from a gold dependency tree to phrase
structure, but rather used the gold dependency tree
as additional input for a phrase structure parser (the
Berkeley parser).
</bodyText>
<subsectionHeader confidence="0.998997">
4.1 Analysis
</subsectionHeader>
<bodyText confidence="0.9957285">
While our system was developed using Section 24,
the f-measure results for USE-SUPER are virtually
identical across all four sections (96.4, 96.7, 96.7,
96.5). Interestingly, there is more variation in the
</bodyText>
<page confidence="0.997486">
311
</page>
<bodyText confidence="0.999929565217392">
USE-POS results (95.0, 95.3, 95.3, 94.4). We take
this to be an indication of a difference in the sec-
tions as to the utility of the POS tags to “bootstrap”
the syntactic structure. As just mentioned above, the
USE-POS runs work by using heuristics to approxi-
mate the gold supertags from the POS tags.
The supertags, because they are partially derived
from the phrase structure, can obscure a discon-
nect between a POS tag and the syntactic structure
it projects. For example, the word according
in the structure (PP (VBG according) (PP
(TO to) ...)) receives the gold supertag P PP,
a more explicit representation of the word’s role in
the structure than the ambiguous VBG. This is why
the USE-POS score is lower than the USE-SUPER
score, since the POS tag and dependency structure
do not always, at least with our simple heuristics,
lead to the gold supertag. For example, in the USE-
POS run, according receives the incorrect su-
pertag P VP, leading to an incorrect structure, while
in the USE-SUPER run, it is able to use P PP, lead-
ing to the correct structure.
However, even with the lower performance of
USE-POS, it is well above the results reported in Xia
et al. (2009) for Section 22, and even more so with
the unlabeled evaluation of Section 00 compared to
Xia and Palmer (2001) and Xia et al. (2009). The
comparison with Wang and Zong (2010) for Section
23 (they did not report results for any other section)
shows something very different, however. Their re-
sult, using a gold dependency tree together with the
Berkeley parser, is above our USE-POS version and
below our USE-SUPER version.
Our interpretation of this is that it provides an
indication of what the parser is providing on top
of the gold dependency structure, which is roughly
the same information that we have encoded in our
DS to PS code. However, because the Wang and
Zong (2010) system performs better than our USE-
POS version, it is likely learning some of the non-
straightforward cases of how USE-POS tags can
bootstrap the syntactic structure that our USE-POS
version is missing. However, any conclusions must
be tentative since our dependency structures are not
necessarily the same as theirs and so it is not an ex-
act comparison.
</bodyText>
<table confidence="0.830044857142857">
Error type count
problem with PTB annotation 8
ambiguous ADVP placement 3
incorrect use of “single token rule” 3
FRAG/X 2
multiple levels of recursion 2
other 5
</table>
<tableCaption confidence="0.949381">
Table 3: Analysis of errors in first 50 sentences of USE-
SUPER run for Section 24
</tableCaption>
<subsectionHeader confidence="0.9593225">
4.2 Errors from Dependency Structure with
Supertags to Phrase Structure
</subsectionHeader>
<bodyText confidence="0.999971941176471">
We stressed in the introduction that we are interested
in understanding better the relationship between the
DS and PS representations. Identifying areas where
the conversion from DS did not result in a perfect
(evalb score) PS is therefore of particular interest.
For this analysis, we used our dev section, 24,
with the run USE-SUPER. We use this run because
we are interested in cases where, even with the gold
supertags, there was still a problem with the conver-
sion to the PS. We examined the first 50 sentences in
the section, with a total of 23 errors. We recognize
that this is a very small sample. An eyeball exam-
ination of other sentences does not reveal anything
significantly different than what we present here as
far as the sorts of errors, although we have only per-
formed a rigorous analysis of these 23 errors, which
is why we limit our discussion here to these cases.
Table 3 shows a breakdown of these 23 errors.
Note that by “error” here we mean a difference be-
tween the reconstructed PS structure, and the PTB
gold PS structure, causing the score for Section 24,
USE-SUPER (last row) in Table 2 to be less than
perfect.
The most common “error” is that in which the
PTB annotation is itself in error, while our algo-
rithm actually creates a correct phrase structure, in
the sense that it is consistent with the PTB guide-
lines. Three of these eight annotation problems are
of the same type, in which a NP is headed by a word
with the RB tag. An example is shown in (5) in
which (5a) shows (a fragment of) the original tree
in the PTB, and (5b) is the resulting DS, with (5c)
the reconstructed PS tree. The word here receives
the supertag P ADVP, thus resulting in a different re-
</bodyText>
<page confidence="0.995422">
312
</page>
<bodyText confidence="0.999833583333333">
constructed PS, with an ADVP. There is a mismatch
between the POS tag and the node label in the origi-
nal tree (5a), and in fact in this case the node label in
the PTB tree should have been ADVP-LOC, instead
of NP-LOC.
An example of the “ambiguous ADVP place-
ment” error is shown in (6), in which the PTB tree
has the adverb frantically inside the VP, infor-
mation which is not available in the DS (6b). Our
conversion code has to choose as to where to put
such ADVPs, and it puts them outside the VP, as in
(6c), which is sometimes correct, but not in this case.
</bodyText>
<sectionHeader confidence="0.997382" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999960297872341">
In this work we have described an approach to auto-
matically converting DS to PS with significantly im-
proved accuracy over previous efforts, and compara-
ble results to that of using a phrase structure parser
guided by the dependency structure.
Following the motivation discussed in Section 1,
the next step is straightforward - to adapt the al-
gorithm to work on conversion from a dependency
representation of the Arabic Treebank to the phrase
structure representation necessary for the annotation
pipeline. Following this, we will then experiment
with parsing the Arabic dependency representation,
converting to phrase structure, and evaluating the re-
sulting phrase structure representation as usual for
parsing evaluation. We will also experiment with
dependency parsing for the PTB dependency repre-
sentation discussed in this paper. Habash and Roth
(2009) discuss an already-existing dependency rep-
resentation of parts of the ATB and it will be inter-
esting to compare the conversion accuracy using the
different dependency representations, although we
expect that there will not be any major differences
in the representations.
One other aspect of future work is to implement
the algorithm in Wang and Zong (2010), using our
own dependency representation, since this would al-
low a precise investigation of what the phrase struc-
ture parser is contributing as compared to our auto-
matic conversion. We note that this work also ex-
perimented with dependency parsing, and then auto-
matically converting the results to PS, a further basis
of comparison.
Finally, we would like to stress that while we have
used evalb for scoring the converting PS because it
is the standard evaluation for PS work, it is a very
insufficient standard for this work. As discussed at
the end of Section 2, we have not included all the
function tags or empty categories in our representa-
tion, a significant omission. We would like to ex-
pand our dependency representation to allow all the
function tags and empty categories to be included
in the converted PS. Our plan is to take our anal-
ogy to TAG more seriously (e.g., (Joshi and Ram-
bow, 2003)) and use a label akin to adjunction to en-
code leftward (non-projective) movement in the tree,
also using an appropriate dependency parser as well
(Shen and Joshi, 2008).
</bodyText>
<sectionHeader confidence="0.997363" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998250181818182">
This work was supported in part by the Defense Ad-
vanced Research Projects Agency, GALE Program
Grant No. HR0011-06-1-0003. The views, opinions
and/or findings contained in this article/presentation
are those of the author/presenter and should not be
interpreted as representing the official views or poli-
cies, either expressed or implied, of the Defense Ad-
vanced Research Projects Agency or the Department
of Defense. We would also like to thank Mohamed
Maamouri, Colin Warner, Aravind Joshi, and Mitch
Marcus for valuable conversations and feedback.
</bodyText>
<figure confidence="0.99989954">
(B)
P VP
premiered
P ADVP
here
(C)
VP
VBD
premiered
ADVP
VBD
premiered
NP-LOC
RB
here
RB
here
(5) (A)
VP
(6) (A)
S
(C)
S
(B)
P VP
selling
NP
VBG
selling
ADVP-MNR
RB
frantically
VBG
selling
NP
NNS
bonds
NNS
bonds
NP-SBJ
-NONE-
P ADVP
frantically
P NP
bonds
ADVP
P ADVP
frantically
VP
VP
</figure>
<page confidence="0.99829">
313
</page>
<sectionHeader confidence="0.998238" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999719855263158">
Srinivas Bangalore and Aravind K. Joshi, editors. 2010.
Supertagging: Using Complex Lexical Descriptions in
Natural Language Processing. MIT Press.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II-
style Penn Treebank project. Technical Report MS-
CIS-95-06, University of Pennsylvania.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted Tree Adjoining Grammar. In Data
Oriented Parsing. CSLI.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
221–224, Suntec, Singapore, August. Association for
Computational Linguistics.
Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for German dependency and con-
stituency representations. In Proceedings of the Work-
shop on Parsing German, pages 47–54, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Joakim Nivre Johan Hall and Jens Nilsson. 2007. Hy-
brid constituency-dependency parser for Swedish. In
Proceedings of NODALIDA, Tartu, Estonia.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proceedings of NODALIDA, Tartu, Estonia.
Aravind Joshi and Owen Rambow. 2003. A formal-
ism for dependency grammar based on Tree Adjoin-
ing Grammar. In Proceedings of the Conference on
Meaning-Text Theory, Paris, France.
Mohamed Maamouri and Ann Bies. 2004. Developing
an arabic treebank: Methods, guidelines, procedures,
and tools. In Ali Farghaly and Karine Megerdoomian,
editors, COLING 2004 Computational Approaches to
Arabic Script-based Languages, pages 2–9, Geneva,
Switzerland, August 28th. COLING.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2011.
Upgrading and enhancing the Penn Arabic Treebank.
In Joseph Olive, Caitlin Christianson, and John Mc-
Cary, editors, Handbook of Natural Language Pro-
cessing and Machine Translation: DARPA Global Au-
tonomous Language Exploitation. Springer.
Christopher Manning. 2011. Part-of-speech tagging
from 97% to 100%: Is it time for some linguistics?
In Alexander Gelbukh, editor, Computational Linguis-
tics and Intelligent Text Processing, 12th International
Conference, CICLing 2011, Proceedings, Part I. Lec-
ture Notes in Computer Science 6608.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, 19:313–330.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of HLT.
Satoshi Sekine and Michael Collins. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
Libin Shen and Aravind Joshi. 2008. LTAG dependency
parsing with bidirectional incremental construction.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 495–
504, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Zhiguo Wang and Chengqing Zong. 2010. Phrase struc-
ture parsing with dependency structure. In COLING
2010: Posters, pages 1292–1300, Beijing, China, Au-
gust.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In HLT-2001.
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,
and Dipti Misra Sharma. 2009. Towards a multi-
representational treebank. In Proceedings of the Work-
shop on Treebanks and Linguistic Theories.
</reference>
<page confidence="0.999125">
314
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.443069">
<title confidence="0.996893">Using Supertags and Encoded Annotation Principles for Improved Dependency to Phrase Structure Conversion</title>
<author confidence="0.7301735">Kulick Bies Linguistic Data</author>
<affiliation confidence="0.999869">University of</affiliation>
<address confidence="0.951304">Philadelphia, PA</address>
<abstract confidence="0.999742388888889">We investigate the problem of automatically converting from a dependency representation to a phrase structure representation, a key aspect of understanding the relationship between these two representations for NLP work. We implement a new approach to this problem, based on a small number of supertags, along with an encoding of some of the underlying principles of the Penn Treebank guidelines. The resulting system significantly outperforms previous work in such automatic conversion. We also achieve comparable results to a system using a phrase-structure parser for the conversion. A comparison with our system using either the part-of-speech tags or the supertags provides some indication of what the parser is contributing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
<author>editors</author>
</authors>
<date>2010</date>
<booktitle>Supertagging: Using Complex Lexical Descriptions in Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<marker>Bangalore, Joshi, editors, 2010</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi, editors. 2010. Supertagging: Using Complex Lexical Descriptions in Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Robert MacIntyre</author>
</authors>
<title>Bracketing guidelines for Treebank IIstyle Penn Treebank project.</title>
<date>1995</date>
<tech>Technical Report MSCIS-95-06,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5995" citStr="Bies et al., 1995" startWordPosition="987" endWordPosition="990">the success of this conversion algorithm, we follow the same evaluation procedure as Xia et al. (2009) and Xia and Palmer (2001). We convert the PTB to a DS, and then use our algorithm to convert the DS back to a PS representation. The original PS and the converted-from-DS PS are then compared, in exactly the same way as parser output is compared with the original (gold) tree. We will show that our results in this area are a significant improvement above previous efforts. A key aspect of this work is that our DS-to-PS conversion encodes many of the properties of the PTB annotation guidelines (Bies et al., 1995), both globally and for specific XP projections. The PTB guidelines are built upon broad decisions about PS representation that provide an overall framework and cohesion for the details of the PS trees. To implement these underlying principles of the guidelines, we defined a set of 30 “supertags” that indicate how a lexical item can project in the syntactic structure, allowing us to specify these principles. We describe these as supertags because of a conceptual similarity to the supertagging work in the Tree Adjoining Grammar (TAG) tradition (Bangalore and Joshi, 2010), although ours is far s</context>
<context position="7738" citStr="Bies et al. (1995)" startWordPosition="1284" endWordPosition="1287">a phrase structure parser contributes in addition to this sort of automated DSto-PS conversion, as discussed in Section 4. We have chosen this approach of encoding underlying principles of the PTB guidelines for two reasons. First, these principles are non-statistical, and thus we felt it would let us tease apart the contribution of the frequency information relating, e.g., heads, on the one hand, and the basic notions of phrase structure on the other. The second reason is that it was quite easy to implement these principles. We did not attempt a complete examination of every possible rule in Bies et al. (1995), but rather just selected the most obvious ones. As we will see in Section 4.2, our results indeed are sometimes hurt by such lack of thoroughness, although in future work we will make this more complete. 2 Overview and Example Figures 1-4 provide a running example of the four steps in the process. Figure 1 is the original tree from the Penn Treebank. Figures 2 and 3 illustrate the two-step process of creating the dependency representation, and Figure 4 shows the conversion back to phrase structure. 306 Figure 2: Tree Insertion Grammar decomposition of Figure Figure 1: Penn Treebank tree 1 AD</context>
</contexts>
<marker>Bies, Ferguson, Katz, MacIntyre, 1995</marker>
<rawString>Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre. 1995. Bracketing guidelines for Treebank IIstyle Penn Treebank project. Technical Report MSCIS-95-06, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically extracted Tree Adjoining Grammar. In Data Oriented Parsing.</title>
<date>2003</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="9393" citStr="Chiang, 2003" startWordPosition="1571" endWordPosition="1572">IN from ADJP VBD/P VP posted NNP/P NP-OBJ RB/P ADVP RB VBN GM generally generally mixed Figure 3: Dependency representation derived from TIG decomposition in Figure 2 Figure 4: Conversion of dependency representation in Figure 3 back to phrase structure. 2.1 Creation of Dependency Representation The creation of the dependency representation is similar in basic aspects to many other approaches, in that we utilize some basic assumptions about head relations to decompose the full tree into smaller units. However, we first decompose the original trees into a Tree Insertion Grammar representation (Chiang, 2003), utilizing tree substitution and sister adjunction. We refer the reader to Chiang (2003) for details of these operations, and instead focus on the fact that the TIG derivation tree in Figure 2 partitions the phrase structure representation in Figure 1 into smaller units, called elementary trees. We leave out the POS tags in Figure 2 to avoid clutter. The creation of the dependency representation is structurally a simple rewrite of the TIG derivation, taking the word associated with each elementary tree and using it as a node in the dependency tree. In this way, the dependency representation i</context>
</contexts>
<marker>Chiang, 2003</marker>
<rawString>David Chiang. 2003. Statistical parsing with an automatically extracted Tree Adjoining Grammar. In Data Oriented Parsing. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Ryan Roth</author>
</authors>
<title>CATiB: The Columbia Arabic Treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>221--224</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="31407" citStr="Habash and Roth (2009)" startWordPosition="5560" endWordPosition="5563">ructure. Following the motivation discussed in Section 1, the next step is straightforward - to adapt the algorithm to work on conversion from a dependency representation of the Arabic Treebank to the phrase structure representation necessary for the annotation pipeline. Following this, we will then experiment with parsing the Arabic dependency representation, converting to phrase structure, and evaluating the resulting phrase structure representation as usual for parsing evaluation. We will also experiment with dependency parsing for the PTB dependency representation discussed in this paper. Habash and Roth (2009) discuss an already-existing dependency representation of parts of the ATB and it will be interesting to compare the conversion accuracy using the different dependency representations, although we expect that there will not be any major differences in the representations. One other aspect of future work is to implement the algorithm in Wang and Zong (2010), using our own dependency representation, since this would allow a precise investigation of what the phrase structure parser is contributing as compared to our automatic conversion. We note that this work also experimented with dependency pa</context>
</contexts>
<marker>Habash, Roth, 2009</marker>
<rawString>Nizar Habash and Ryan Roth. 2009. CATiB: The Columbia Arabic Treebank. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 221–224, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Joakim Nivre</author>
</authors>
<title>A dependencydriven parser for German dependency and constituency representations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Parsing German,</booktitle>
<pages>47--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="12672" citStr="Hall and Nivre, 2008" startWordPosition="2177" endWordPosition="2180">correct some of the POS tag errors in the PTB (Manning, 2011). For example, if that has the (incorrect) tag DT in the complementizer position, it still receives the new POS tag P COMP. This procedure results in a set of 30 supertags, and Table 1 shows how they are partitioned into 14 projection types. These supertags and projection types are the basis of our DS-to-PS conversion, as discussed further in Section 2.2. We note here a brief comparison with earlier work on “hybrid” representations, which encode a PS representation inside a DS one, in order to convert from the latter to the former. (Hall and Nivre, 2008; Johan Hall and Nilsson, 2007; Johansson and Nugues, 2007). Our goal is very different. Instead of en1There are other details not discussed here. For example, we do not automatically assign a P NP supertag to the head child of an NP, since such a head can legitimately be, e.g, a JJ, in which case we make the supertag P ADJP, on the reasoning that it would be encoding “too much” to treat it as P NP. Instead, we rely on the DS and such labels as SBJ or OBJ to determine when to project it as NP in the converted PS. coding the phrase structure in the dependency tree via complex tags such as SBARQ</context>
</contexts>
<marker>Hall, Nivre, 2008</marker>
<rawString>Johan Hall and Joakim Nivre. 2008. A dependencydriven parser for German dependency and constituency representations. In Proceedings of the Workshop on Parsing German, pages 47–54, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Hybrid constituency-dependency parser for Swedish.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA,</booktitle>
<location>Tartu, Estonia.</location>
<contexts>
<context position="12702" citStr="Hall and Nilsson, 2007" startWordPosition="2183" endWordPosition="2186">errors in the PTB (Manning, 2011). For example, if that has the (incorrect) tag DT in the complementizer position, it still receives the new POS tag P COMP. This procedure results in a set of 30 supertags, and Table 1 shows how they are partitioned into 14 projection types. These supertags and projection types are the basis of our DS-to-PS conversion, as discussed further in Section 2.2. We note here a brief comparison with earlier work on “hybrid” representations, which encode a PS representation inside a DS one, in order to convert from the latter to the former. (Hall and Nivre, 2008; Johan Hall and Nilsson, 2007; Johansson and Nugues, 2007). Our goal is very different. Instead of en1There are other details not discussed here. For example, we do not automatically assign a P NP supertag to the head child of an NP, since such a head can legitimately be, e.g, a JJ, in which case we make the supertag P ADJP, on the reasoning that it would be encoding “too much” to treat it as P NP. Instead, we rely on the DS and such labels as SBJ or OBJ to determine when to project it as NP in the converted PS. coding the phrase structure in the dependency tree via complex tags such as SBARQ in Johansson and Nugues (2007</context>
</contexts>
<marker>Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre Johan Hall and Jens Nilsson. 2007. Hybrid constituency-dependency parser for Swedish. In Proceedings of NODALIDA, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for english.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA,</booktitle>
<location>Tartu, Estonia.</location>
<contexts>
<context position="4465" citStr="Johansson and Nugues (2007)" startWordPosition="715" endWordPosition="718"> to the Arabic treebank as well. We expect this to be successful because the ATB has some fundamental similarities to the PTB in spite of the language difference (Maamouri and Bies, 2004). As mentioned above, one goal in our DS to PS conversion work is to base it on a minimal DS representation. By “minimal”, we mean that it does not include information that is redundant, together with our conversion code, with the implicit information in the dependency structure itself. As discussed more in Section 2.1, we aim to make our dependency representation simpler than “hybrid” representations such as Johansson and Nugues (2007). The reason for our interest in this minimal representation is parsing. We do not want to require the parser to recover such a complex dependency representations, when it is, in fact, unnecessary, as we believe our approach shows. The benefit of this approach can only be seen when this line of work is extended to experiments with parsing and Arabic conversion. The work described here is just the first step in this process. A conversion scheme, such as ours, necessarily relies on some details of the annotation content in the DS and PS representations, and so our algorithm is not an algorithm d</context>
<context position="12731" citStr="Johansson and Nugues, 2007" startWordPosition="2187" endWordPosition="2190">ng, 2011). For example, if that has the (incorrect) tag DT in the complementizer position, it still receives the new POS tag P COMP. This procedure results in a set of 30 supertags, and Table 1 shows how they are partitioned into 14 projection types. These supertags and projection types are the basis of our DS-to-PS conversion, as discussed further in Section 2.2. We note here a brief comparison with earlier work on “hybrid” representations, which encode a PS representation inside a DS one, in order to convert from the latter to the former. (Hall and Nivre, 2008; Johan Hall and Nilsson, 2007; Johansson and Nugues, 2007). Our goal is very different. Instead of en1There are other details not discussed here. For example, we do not automatically assign a P NP supertag to the head child of an NP, since such a head can legitimately be, e.g, a JJ, in which case we make the supertag P ADJP, on the reasoning that it would be encoding “too much” to treat it as P NP. Instead, we rely on the DS and such labels as SBJ or OBJ to determine when to project it as NP in the converted PS. coding the phrase structure in the dependency tree via complex tags such as SBARQ in Johansson and Nugues (2007), we use a minimal represent</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In Proceedings of NODALIDA, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
<author>Owen Rambow</author>
</authors>
<title>A formalism for dependency grammar based on Tree Adjoining Grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Meaning-Text Theory,</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="32667" citStr="Joshi and Rambow, 2003" startWordPosition="5774" endWordPosition="5778">ing the results to PS, a further basis of comparison. Finally, we would like to stress that while we have used evalb for scoring the converting PS because it is the standard evaluation for PS work, it is a very insufficient standard for this work. As discussed at the end of Section 2, we have not included all the function tags or empty categories in our representation, a significant omission. We would like to expand our dependency representation to allow all the function tags and empty categories to be included in the converted PS. Our plan is to take our analogy to TAG more seriously (e.g., (Joshi and Rambow, 2003)) and use a label akin to adjunction to encode leftward (non-projective) movement in the tree, also using an appropriate dependency parser as well (Shen and Joshi, 2008). Acknowledgements This work was supported in part by the Defense Advanced Research Projects Agency, GALE Program Grant No. HR0011-06-1-0003. The views, opinions and/or findings contained in this article/presentation are those of the author/presenter and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Def</context>
</contexts>
<marker>Joshi, Rambow, 2003</marker>
<rawString>Aravind Joshi and Owen Rambow. 2003. A formalism for dependency grammar based on Tree Adjoining Grammar. In Proceedings of the Conference on Meaning-Text Theory, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
</authors>
<title>Developing an arabic treebank: Methods, guidelines, procedures, and tools.</title>
<date>2004</date>
<booktitle>In Ali Farghaly and Karine Megerdoomian, editors, COLING 2004 Computational Approaches to Arabic Script-based Languages,</booktitle>
<pages>2--9</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="4025" citStr="Maamouri and Bies, 2004" startWordPosition="642" endWordPosition="645">rabic, we are first focusing on 305 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 305–314, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics a conversion routine for the English PTB because it is so well-established and the results are easier to interpret. The intent is then to transfer this conversion algorithm work to the Arabic treebank as well. We expect this to be successful because the ATB has some fundamental similarities to the PTB in spite of the language difference (Maamouri and Bies, 2004). As mentioned above, one goal in our DS to PS conversion work is to base it on a minimal DS representation. By “minimal”, we mean that it does not include information that is redundant, together with our conversion code, with the implicit information in the dependency structure itself. As discussed more in Section 2.1, we aim to make our dependency representation simpler than “hybrid” representations such as Johansson and Nugues (2007). The reason for our interest in this minimal representation is parsing. We do not want to require the parser to recover such a complex dependency representatio</context>
</contexts>
<marker>Maamouri, Bies, 2004</marker>
<rawString>Mohamed Maamouri and Ann Bies. 2004. Developing an arabic treebank: Methods, guidelines, procedures, and tools. In Ali Farghaly and Karine Megerdoomian, editors, COLING 2004 Computational Approaches to Arabic Script-based Languages, pages 2–9, Geneva, Switzerland, August 28th. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Seth Kulick</author>
</authors>
<title>Upgrading and enhancing the Penn Arabic Treebank.</title>
<date>2011</date>
<booktitle>Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation.</booktitle>
<editor>In Joseph Olive, Caitlin Christianson, and John McCary, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="2764" citStr="Maamouri et al., 2011" startWordPosition="438" endWordPosition="441">able from the beginning, without a loss of information in either direction (Xia et al., 2009). Second, it is sometimes the case that it is convenient to do annotation in a dependency representation (e.g., if the annotators are already familiar with such a representation), though the treebank will in final form be either phrase-structure or multirepresentational (Xia et al., 2009). However, our concern is somewhat different. We are specifically interested in experimenting with dependency parsing of Arabic as a step in the annotation of the Arabic Treebank, which is a phrase structure treebank (Maamouri et al., 2011). Although we currently use a phrase structure parser in this annotation pipeline, there are advantages to the flexibility of being able to experiment with advances in parsing technology for dependency parsing. We would like to parse with a dependency representation of the data, and then convert the parser output to a phrase structure representation so that it can feed into the annotation pipeline. Therefore, in order to make use of dependency parsers, we need a conversion from dependency to phrase structure with very high accuracy, which is the goal of this paper. While one of our underlying </context>
</contexts>
<marker>Maamouri, Bies, Kulick, 2011</marker>
<rawString>Mohamed Maamouri, Ann Bies, and Seth Kulick. 2011. Upgrading and enhancing the Penn Arabic Treebank. In Joseph Olive, Caitlin Christianson, and John McCary, editors, Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
</authors>
<title>Part-of-speech tagging from 97% to 100%: Is it time for some linguistics?</title>
<date>2011</date>
<booktitle>Computational Linguistics and Intelligent Text Processing, 12th International Conference, CICLing 2011, Proceedings, Part I. Lecture Notes in Computer Science</booktitle>
<pages>6608</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<contexts>
<context position="12113" citStr="Manning, 2011" startWordPosition="2079" endWordPosition="2080">of the dependency structure. The second non-straightforward case1 is that of degenerate elementary trees, in which the “tree” is just the word itself, as for other, car, and generally. In such cases we default the supertag based on the original POS tag, and in some cases, the tree configuration. For example, a word with the JJ tag, such as other, would get the supertag P ADJP, with the RB tag such as generally the supertag P ADVP. We assign prenominal nouns such as car here the tag P PRENOM. Generating supertags in this way is a convenient way to correct some of the POS tag errors in the PTB (Manning, 2011). For example, if that has the (incorrect) tag DT in the complementizer position, it still receives the new POS tag P COMP. This procedure results in a set of 30 supertags, and Table 1 shows how they are partitioned into 14 projection types. These supertags and projection types are the basis of our DS-to-PS conversion, as discussed further in Section 2.2. We note here a brief comparison with earlier work on “hybrid” representations, which encode a PS representation inside a DS one, in order to convert from the latter to the former. (Hall and Nivre, 2008; Johan Hall and Nilsson, 2007; Johansson</context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Christopher Manning. 2011. Part-of-speech tagging from 97% to 100%: Is it time for some linguistics? In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, 12th International Conference, CICLing 2011, Proceedings, Part I. Lecture Notes in Computer Science 6608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational linguistics,</title>
<date>1993</date>
<pages>19--313</pages>
<contexts>
<context position="1276" citStr="Marcus et al., 1993" startWordPosition="185" endWordPosition="188">lines. The resulting system significantly outperforms previous work in such automatic conversion. We also achieve comparable results to a system using a phrase-structure parser for the conversion. A comparison with our system using either the part-of-speech tags or the supertags provides some indication of what the parser is contributing. 1 Introduction and Motivation Recent years have seen a significant increase in interest in dependency treebanks and dependency parsing. Since the standard training and test set for English parsing is a phrase structure (PS) treebank, the Penn Treebank (PTB) (Marcus et al., 1993; Marcus et al., 1994), the usual approach is to convert this to a dependency structure (DS) treebank, by means of various heuristics for identifying heads in a PS tree. The resulting DS representation is then used for training and parsing, with results reported on the DS representation. Our goal in this paper is to go in the reverse direction, from the DS to PS representation, by finding a minimal DS representation from which we can use an approximate version of the principles of the PTB guidelines to reconstruct the PS. Work in this conversion direction is somewhat less studied (Xia et al., </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of HLT.</booktitle>
<contexts>
<context position="1298" citStr="Marcus et al., 1994" startWordPosition="189" endWordPosition="193">system significantly outperforms previous work in such automatic conversion. We also achieve comparable results to a system using a phrase-structure parser for the conversion. A comparison with our system using either the part-of-speech tags or the supertags provides some indication of what the parser is contributing. 1 Introduction and Motivation Recent years have seen a significant increase in interest in dependency treebanks and dependency parsing. Since the standard training and test set for English parsing is a phrase structure (PS) treebank, the Penn Treebank (PTB) (Marcus et al., 1993; Marcus et al., 1994), the usual approach is to convert this to a dependency structure (DS) treebank, by means of various heuristics for identifying heads in a PS tree. The resulting DS representation is then used for training and parsing, with results reported on the DS representation. Our goal in this paper is to go in the reverse direction, from the DS to PS representation, by finding a minimal DS representation from which we can use an approximate version of the principles of the PTB guidelines to reconstruct the PS. Work in this conversion direction is somewhat less studied (Xia et al., 2009; Xia and Palmer, </context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Michael Collins</author>
</authors>
<date>2008</date>
<note>Evalb. http://nlp.cs.nyu.edu/evalb/.</note>
<contexts>
<context position="15515" citStr="Sekine and Collins, 2008" startWordPosition="2686" endWordPosition="2689">ome overlap between the five labels we use, as just described, and the PTB function tags, but in general we do not encode the full range of function tags in our representation, saving this for future work. More significantly, we also do not include empty categories and associated coindexing, which has the consequence that the dependency trees are projective. The reason we have not included these aspects in our representation and conversion yet is that we are focused here first on the evaluation for comparison with previous work, and the basis for this previous work is the usual evalb program (Sekine and Collins, 2008), which ignores function tags and empty categories. We return to this issue in the conclusion. 2.2 From Dependency to Phrase Structure There are two key aspects to the conversion from dependency to phrase structure. (1) We encode general conventions about annotation that are used throughout the annotation guidelines for the PTB. A common example is that of the “single-word” rule, in which a constituent consisting of just a single word is reduced to just that word, without the constituent bracketing, in many cases. (2) We use the set of supertags as the basis for defining projection-specific ru</context>
<context position="23536" citStr="Sekine and Collins, 2008" startWordPosition="4182" endWordPosition="4185">.1 96.7 23 Wang &amp; Zong ’10 95.9 96.3 96.1 USE-POS 94.8 95.7 95.3 USE-SUPER 96.2 97.3 96.7 24 USE-POS 94.0 94.7 94.4 USE-SUPER 95.9 97.1 96.5 Table 2: Results of dependency to phrase structure conversion. For our system, the results are presented in two ways, using either the gold part-of-speech tags (USEPOS) or our gold supertags (USE-SUPER). For purposes of comparison with Xia and Palmer (2001) and Xia et al. (2009), we also present the results for Section 00 using part-of-speech tags, but with an unlabeled evaluation (USE-POS-UNLABEL). structure trees, using the standard evalb scoring code (Sekine and Collins, 2008). Xia and Palmer (2001) defined three different algorithms for the conversion, utilizing different heuristics for how to build projection chains, and where to attach dependent subtrees. They reported results for their system for Section 00 of the PTB, and we include in Table 2 only their highest scoring algorithm. The system of Xia et al. (2009) uses conversion rules learned from Section 19, and then tested on Sections 00 and Section 22. We developed the algorithm using Section 24, and we also report results for Sections 00, 22, and 23, for comparison with previous work. We ran our system in t</context>
</contexts>
<marker>Sekine, Collins, 2008</marker>
<rawString>Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind Joshi</author>
</authors>
<title>LTAG dependency parsing with bidirectional incremental construction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>495--504</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="32836" citStr="Shen and Joshi, 2008" startWordPosition="5803" endWordPosition="5806">rd evaluation for PS work, it is a very insufficient standard for this work. As discussed at the end of Section 2, we have not included all the function tags or empty categories in our representation, a significant omission. We would like to expand our dependency representation to allow all the function tags and empty categories to be included in the converted PS. Our plan is to take our analogy to TAG more seriously (e.g., (Joshi and Rambow, 2003)) and use a label akin to adjunction to encode leftward (non-projective) movement in the tree, also using an appropriate dependency parser as well (Shen and Joshi, 2008). Acknowledgements This work was supported in part by the Defense Advanced Research Projects Agency, GALE Program Grant No. HR0011-06-1-0003. The views, opinions and/or findings contained in this article/presentation are those of the author/presenter and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. We would also like to thank Mohamed Maamouri, Colin Warner, Aravind Joshi, and Mitch Marcus for valuable conversations and feedback. (B) P VP premiered P ADVP here</context>
</contexts>
<marker>Shen, Joshi, 2008</marker>
<rawString>Libin Shen and Aravind Joshi. 2008. LTAG dependency parsing with bidirectional incremental construction. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 495– 504, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiguo Wang</author>
<author>Chengqing Zong</author>
</authors>
<title>Phrase structure parsing with dependency structure.</title>
<date>2010</date>
<booktitle>In COLING 2010: Posters,</booktitle>
<pages>1292--1300</pages>
<location>Beijing, China,</location>
<contexts>
<context position="25260" citStr="Wang and Zong (2010)" startWordPosition="4493" endWordPosition="4496">d the word y to its immediate right is its parent in the dependency tree and y has one of the verbal POS tags, then x receives the supertag P AUX, and otherwise P PP. Any word with the POS tag JJ, JJR, or JJS, receives the supertag P ADJP, and so on. The results for Xia and Palmer (2001) and Xia et al. (2009) were reported using an unlabeled version of evalb, so to compare properly we also report our results for Section 00 using an unlabeled evaluation of the run using the POS tags (USE-POS-UNLABEL), while all the other results use a labeled evaluation. We also compare our system with that of Wang and Zong (2010). Unlike the three other systems (including ours), this was not based on an automatic conversion from a gold dependency tree to phrase structure, but rather used the gold dependency tree as additional input for a phrase structure parser (the Berkeley parser). 4.1 Analysis While our system was developed using Section 24, the f-measure results for USE-SUPER are virtually identical across all four sections (96.4, 96.7, 96.7, 96.5). Interestingly, there is more variation in the 311 USE-POS results (95.0, 95.3, 95.3, 94.4). We take this to be an indication of a difference in the sections as to the </context>
<context position="27090" citStr="Wang and Zong (2010)" startWordPosition="4807" endWordPosition="4810">PER score, since the POS tag and dependency structure do not always, at least with our simple heuristics, lead to the gold supertag. For example, in the USEPOS run, according receives the incorrect supertag P VP, leading to an incorrect structure, while in the USE-SUPER run, it is able to use P PP, leading to the correct structure. However, even with the lower performance of USE-POS, it is well above the results reported in Xia et al. (2009) for Section 22, and even more so with the unlabeled evaluation of Section 00 compared to Xia and Palmer (2001) and Xia et al. (2009). The comparison with Wang and Zong (2010) for Section 23 (they did not report results for any other section) shows something very different, however. Their result, using a gold dependency tree together with the Berkeley parser, is above our USE-POS version and below our USE-SUPER version. Our interpretation of this is that it provides an indication of what the parser is providing on top of the gold dependency structure, which is roughly the same information that we have encoded in our DS to PS code. However, because the Wang and Zong (2010) system performs better than our USEPOS version, it is likely learning some of the nonstraightf</context>
<context position="31765" citStr="Wang and Zong (2010)" startWordPosition="5617" endWordPosition="5620">n, converting to phrase structure, and evaluating the resulting phrase structure representation as usual for parsing evaluation. We will also experiment with dependency parsing for the PTB dependency representation discussed in this paper. Habash and Roth (2009) discuss an already-existing dependency representation of parts of the ATB and it will be interesting to compare the conversion accuracy using the different dependency representations, although we expect that there will not be any major differences in the representations. One other aspect of future work is to implement the algorithm in Wang and Zong (2010), using our own dependency representation, since this would allow a precise investigation of what the phrase structure parser is contributing as compared to our automatic conversion. We note that this work also experimented with dependency parsing, and then automatically converting the results to PS, a further basis of comparison. Finally, we would like to stress that while we have used evalb for scoring the converting PS because it is the standard evaluation for PS work, it is a very insufficient standard for this work. As discussed at the end of Section 2, we have not included all the functi</context>
</contexts>
<marker>Wang, Zong, 2010</marker>
<rawString>Zhiguo Wang and Chengqing Zong. 2010. Phrase structure parsing with dependency structure. In COLING 2010: Posters, pages 1292–1300, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
</authors>
<title>Converting dependency structures to phrase structures.</title>
<date>2001</date>
<booktitle>In HLT-2001.</booktitle>
<contexts>
<context position="1903" citStr="Xia and Palmer, 2001" startWordPosition="295" endWordPosition="298">us et al., 1994), the usual approach is to convert this to a dependency structure (DS) treebank, by means of various heuristics for identifying heads in a PS tree. The resulting DS representation is then used for training and parsing, with results reported on the DS representation. Our goal in this paper is to go in the reverse direction, from the DS to PS representation, by finding a minimal DS representation from which we can use an approximate version of the principles of the PTB guidelines to reconstruct the PS. Work in this conversion direction is somewhat less studied (Xia et al., 2009; Xia and Palmer, 2001), but it is still an important topic for a number of reasons. First, because both DS and PS treebanks are of current interest, there is an increasing effort made to create multi-representational treebank resources with both DS and PS available from the beginning, without a loss of information in either direction (Xia et al., 2009). Second, it is sometimes the case that it is convenient to do annotation in a dependency representation (e.g., if the annotators are already familiar with such a representation), though the treebank will in final form be either phrase-structure or multirepresentation</context>
<context position="5505" citStr="Xia and Palmer (2001)" startWordPosition="900" endWordPosition="903">process. A conversion scheme, such as ours, necessarily relies on some details of the annotation content in the DS and PS representations, and so our algorithm is not an algorithm designed to take as input any arbitrary DS representation. However, the fundamentals of our dependency representation are not radically different than others - e.g. we make an auxiliary verb the child of the main verb, instead of the other way, but such choices can be adjusted for in the conversion. To evaluate the success of this conversion algorithm, we follow the same evaluation procedure as Xia et al. (2009) and Xia and Palmer (2001). We convert the PTB to a DS, and then use our algorithm to convert the DS back to a PS representation. The original PS and the converted-from-DS PS are then compared, in exactly the same way as parser output is compared with the original (gold) tree. We will show that our results in this area are a significant improvement above previous efforts. A key aspect of this work is that our DS-to-PS conversion encodes many of the properties of the PTB annotation guidelines (Bies et al., 1995), both globally and for specific XP projections. The PTB guidelines are built upon broad decisions about PS re</context>
<context position="21929" citStr="Xia and Palmer (2001)" startWordPosition="3888" endWordPosition="3891">ency representation of ...turn down the volume and close the curtains is shown in (4a) in Figure 7. The conjunct close the curtains is converted as a VP projection projecting to S. However, when the projection for turn is assembled, the code checks if the conjuncts are missing subjects, and if so, reduces the configuration to standard VP coordination, as in (4b). The COORD label is used to identify such structures for examination. 4 Results of Dependency to Phrase Structure Conversion To evaluate the correctness of conversion from dependency to phrase structure, we follow the same strategy as Xia and Palmer (2001) and Xia et al. (2009). We convert the phrase structure trees in the PTB to dependency structure and convert the dependency back to phrase structure. We then compare the original PTB trees with the newly-created phrase (3) (A) P VP absorbed (B) SINV P DET the DT the NN cost VBN been VP VBN absorbed P AUX had P NP-SBJ cost P VP been VBD had NP-SBJ VP 310 (B) VP P VP-COORD close P CC and P NP volume P PRT down VP P DET the P NP-OBJ curtains (4) (A) P VP turn and VP close NP-OBJ the curtains P DET the turn PRT down NP-OBJ the volume Figure 7: (4a) is the dependency representation of a coordinatio</context>
<context position="23309" citStr="Xia and Palmer (2001)" startWordPosition="4147" endWordPosition="4150">rec f 00 Xia &amp; Palmer ’01 86.2 88.7 87.5 Xia et al. ’09 91.8 89.2 90.5 USE-POS-UNLABEL 96.6 97.4 97.0 USE-POS 94.6 95.4 95.0 USE-SUPER 95.9 97.0 96.4 22 Xia et al. ’09 90.7 88.1 89.4 USE-POS 95.0 95.5 95.3 USE-SUPER 96.4 97.1 96.7 23 Wang &amp; Zong ’10 95.9 96.3 96.1 USE-POS 94.8 95.7 95.3 USE-SUPER 96.2 97.3 96.7 24 USE-POS 94.0 94.7 94.4 USE-SUPER 95.9 97.1 96.5 Table 2: Results of dependency to phrase structure conversion. For our system, the results are presented in two ways, using either the gold part-of-speech tags (USEPOS) or our gold supertags (USE-SUPER). For purposes of comparison with Xia and Palmer (2001) and Xia et al. (2009), we also present the results for Section 00 using part-of-speech tags, but with an unlabeled evaluation (USE-POS-UNLABEL). structure trees, using the standard evalb scoring code (Sekine and Collins, 2008). Xia and Palmer (2001) defined three different algorithms for the conversion, utilizing different heuristics for how to build projection chains, and where to attach dependent subtrees. They reported results for their system for Section 00 of the PTB, and we include in Table 2 only their highest scoring algorithm. The system of Xia et al. (2009) uses conversion rules lea</context>
<context position="24928" citStr="Xia and Palmer (2001)" startWordPosition="4435" endWordPosition="4438">E-POS) we use the gold POS tags, and not the supertags. Because our DS-to-PS algorithm is based on using the supertags to guide the conversion, the USE-POS runs work by using a few straightforward heuristics to guess the correct supertag from the POS tag and the dependency structure. For example, if a word x has the POS tag “TO” and the word y to its immediate right is its parent in the dependency tree and y has one of the verbal POS tags, then x receives the supertag P AUX, and otherwise P PP. Any word with the POS tag JJ, JJR, or JJS, receives the supertag P ADJP, and so on. The results for Xia and Palmer (2001) and Xia et al. (2009) were reported using an unlabeled version of evalb, so to compare properly we also report our results for Section 00 using an unlabeled evaluation of the run using the POS tags (USE-POS-UNLABEL), while all the other results use a labeled evaluation. We also compare our system with that of Wang and Zong (2010). Unlike the three other systems (including ours), this was not based on an automatic conversion from a gold dependency tree to phrase structure, but rather used the gold dependency tree as additional input for a phrase structure parser (the Berkeley parser). 4.1 Anal</context>
<context position="27026" citStr="Xia and Palmer (2001)" startWordPosition="4795" endWordPosition="4798">guous VBG. This is why the USE-POS score is lower than the USE-SUPER score, since the POS tag and dependency structure do not always, at least with our simple heuristics, lead to the gold supertag. For example, in the USEPOS run, according receives the incorrect supertag P VP, leading to an incorrect structure, while in the USE-SUPER run, it is able to use P PP, leading to the correct structure. However, even with the lower performance of USE-POS, it is well above the results reported in Xia et al. (2009) for Section 22, and even more so with the unlabeled evaluation of Section 00 compared to Xia and Palmer (2001) and Xia et al. (2009). The comparison with Wang and Zong (2010) for Section 23 (they did not report results for any other section) shows something very different, however. Their result, using a gold dependency tree together with the Berkeley parser, is above our USE-POS version and below our USE-SUPER version. Our interpretation of this is that it provides an indication of what the parser is providing on top of the gold dependency structure, which is roughly the same information that we have encoded in our DS to PS code. However, because the Wang and Zong (2010) system performs better than ou</context>
</contexts>
<marker>Xia, Palmer, 2001</marker>
<rawString>Fei Xia and Martha Palmer. 2001. Converting dependency structures to phrase structures. In HLT-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Owen Rambow</author>
<author>Rajesh Bhatt</author>
<author>Martha Palmer</author>
<author>Dipti Misra Sharma</author>
</authors>
<title>Towards a multirepresentational treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories.</booktitle>
<contexts>
<context position="1880" citStr="Xia et al., 2009" startWordPosition="291" endWordPosition="294">et al., 1993; Marcus et al., 1994), the usual approach is to convert this to a dependency structure (DS) treebank, by means of various heuristics for identifying heads in a PS tree. The resulting DS representation is then used for training and parsing, with results reported on the DS representation. Our goal in this paper is to go in the reverse direction, from the DS to PS representation, by finding a minimal DS representation from which we can use an approximate version of the principles of the PTB guidelines to reconstruct the PS. Work in this conversion direction is somewhat less studied (Xia et al., 2009; Xia and Palmer, 2001), but it is still an important topic for a number of reasons. First, because both DS and PS treebanks are of current interest, there is an increasing effort made to create multi-representational treebank resources with both DS and PS available from the beginning, without a loss of information in either direction (Xia et al., 2009). Second, it is sometimes the case that it is convenient to do annotation in a dependency representation (e.g., if the annotators are already familiar with such a representation), though the treebank will in final form be either phrase-structure</context>
<context position="5479" citStr="Xia et al. (2009)" startWordPosition="895" endWordPosition="898">he first step in this process. A conversion scheme, such as ours, necessarily relies on some details of the annotation content in the DS and PS representations, and so our algorithm is not an algorithm designed to take as input any arbitrary DS representation. However, the fundamentals of our dependency representation are not radically different than others - e.g. we make an auxiliary verb the child of the main verb, instead of the other way, but such choices can be adjusted for in the conversion. To evaluate the success of this conversion algorithm, we follow the same evaluation procedure as Xia et al. (2009) and Xia and Palmer (2001). We convert the PTB to a DS, and then use our algorithm to convert the DS back to a PS representation. The original PS and the converted-from-DS PS are then compared, in exactly the same way as parser output is compared with the original (gold) tree. We will show that our results in this area are a significant improvement above previous efforts. A key aspect of this work is that our DS-to-PS conversion encodes many of the properties of the PTB annotation guidelines (Bies et al., 1995), both globally and for specific XP projections. The PTB guidelines are built upon b</context>
<context position="21951" citStr="Xia et al. (2009)" startWordPosition="3893" endWordPosition="3896">turn down the volume and close the curtains is shown in (4a) in Figure 7. The conjunct close the curtains is converted as a VP projection projecting to S. However, when the projection for turn is assembled, the code checks if the conjuncts are missing subjects, and if so, reduces the configuration to standard VP coordination, as in (4b). The COORD label is used to identify such structures for examination. 4 Results of Dependency to Phrase Structure Conversion To evaluate the correctness of conversion from dependency to phrase structure, we follow the same strategy as Xia and Palmer (2001) and Xia et al. (2009). We convert the phrase structure trees in the PTB to dependency structure and convert the dependency back to phrase structure. We then compare the original PTB trees with the newly-created phrase (3) (A) P VP absorbed (B) SINV P DET the DT the NN cost VBN been VP VBN absorbed P AUX had P NP-SBJ cost P VP been VBD had NP-SBJ VP 310 (B) VP P VP-COORD close P CC and P NP volume P PRT down VP P DET the P NP-OBJ curtains (4) (A) P VP turn and VP close NP-OBJ the curtains P DET the turn PRT down NP-OBJ the volume Figure 7: (4a) is the dependency representation of a coordination structure, and the r</context>
<context position="23331" citStr="Xia et al. (2009)" startWordPosition="4152" endWordPosition="4155">86.2 88.7 87.5 Xia et al. ’09 91.8 89.2 90.5 USE-POS-UNLABEL 96.6 97.4 97.0 USE-POS 94.6 95.4 95.0 USE-SUPER 95.9 97.0 96.4 22 Xia et al. ’09 90.7 88.1 89.4 USE-POS 95.0 95.5 95.3 USE-SUPER 96.4 97.1 96.7 23 Wang &amp; Zong ’10 95.9 96.3 96.1 USE-POS 94.8 95.7 95.3 USE-SUPER 96.2 97.3 96.7 24 USE-POS 94.0 94.7 94.4 USE-SUPER 95.9 97.1 96.5 Table 2: Results of dependency to phrase structure conversion. For our system, the results are presented in two ways, using either the gold part-of-speech tags (USEPOS) or our gold supertags (USE-SUPER). For purposes of comparison with Xia and Palmer (2001) and Xia et al. (2009), we also present the results for Section 00 using part-of-speech tags, but with an unlabeled evaluation (USE-POS-UNLABEL). structure trees, using the standard evalb scoring code (Sekine and Collins, 2008). Xia and Palmer (2001) defined three different algorithms for the conversion, utilizing different heuristics for how to build projection chains, and where to attach dependent subtrees. They reported results for their system for Section 00 of the PTB, and we include in Table 2 only their highest scoring algorithm. The system of Xia et al. (2009) uses conversion rules learned from Section 19, </context>
<context position="24950" citStr="Xia et al. (2009)" startWordPosition="4440" endWordPosition="4443"> tags, and not the supertags. Because our DS-to-PS algorithm is based on using the supertags to guide the conversion, the USE-POS runs work by using a few straightforward heuristics to guess the correct supertag from the POS tag and the dependency structure. For example, if a word x has the POS tag “TO” and the word y to its immediate right is its parent in the dependency tree and y has one of the verbal POS tags, then x receives the supertag P AUX, and otherwise P PP. Any word with the POS tag JJ, JJR, or JJS, receives the supertag P ADJP, and so on. The results for Xia and Palmer (2001) and Xia et al. (2009) were reported using an unlabeled version of evalb, so to compare properly we also report our results for Section 00 using an unlabeled evaluation of the run using the POS tags (USE-POS-UNLABEL), while all the other results use a labeled evaluation. We also compare our system with that of Wang and Zong (2010). Unlike the three other systems (including ours), this was not based on an automatic conversion from a gold dependency tree to phrase structure, but rather used the gold dependency tree as additional input for a phrase structure parser (the Berkeley parser). 4.1 Analysis While our system </context>
<context position="26915" citStr="Xia et al. (2009)" startWordPosition="4775" endWordPosition="4778">es the gold supertag P PP, a more explicit representation of the word’s role in the structure than the ambiguous VBG. This is why the USE-POS score is lower than the USE-SUPER score, since the POS tag and dependency structure do not always, at least with our simple heuristics, lead to the gold supertag. For example, in the USEPOS run, according receives the incorrect supertag P VP, leading to an incorrect structure, while in the USE-SUPER run, it is able to use P PP, leading to the correct structure. However, even with the lower performance of USE-POS, it is well above the results reported in Xia et al. (2009) for Section 22, and even more so with the unlabeled evaluation of Section 00 compared to Xia and Palmer (2001) and Xia et al. (2009). The comparison with Wang and Zong (2010) for Section 23 (they did not report results for any other section) shows something very different, however. Their result, using a gold dependency tree together with the Berkeley parser, is above our USE-POS version and below our USE-SUPER version. Our interpretation of this is that it provides an indication of what the parser is providing on top of the gold dependency structure, which is roughly the same information that</context>
</contexts>
<marker>Xia, Rambow, Bhatt, Palmer, Sharma, 2009</marker>
<rawString>Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer, and Dipti Misra Sharma. 2009. Towards a multirepresentational treebank. In Proceedings of the Workshop on Treebanks and Linguistic Theories.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>