<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005055">
<title confidence="0.987141">
The Second Release of the RASP System
</title>
<author confidence="0.998575">
Ted Briscoe† John Carroll$ Rebecca Watson†
</author>
<affiliation confidence="0.855556">
†Computer Laboratory, University of Cambridge, Cambridge CB3 OFD, UK
firstname.lastname@cl.cam.ac.uk
$Department of Informatics, University of Sussex, Brighton BN1 9QH, UK
</affiliation>
<email confidence="0.996232">
J.A.Carroll@sussex.ac.uk
</email>
<sectionHeader confidence="0.993859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999770428571429">
We describe the new release of the RASP
(robust accurate statistical parsing) sys-
tem, designed for syntactic annotation of
free text. The new version includes a
revised and more semantically-motivated
output representation, an enhanced gram-
mar and part-of-speech tagger lexicon, and
a more flexible and semi-supervised train-
ing method for the structural parse ranking
model. We evaluate the released version
on the WSJ using a relational evaluation
scheme, and describe how the new release
allows users to enhance performance using
(in-domain) lexical information.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978958333333">
The first public release of the RASP system
(Briscoe &amp; Carroll, 2002) has been downloaded
by over 120 sites and used in diverse natural lan-
guage processing tasks, such as anaphora res-
olution, word sense disambiguation, identifying
rhetorical relations, resolving metonymy, detect-
ing compositionality in phrasal verbs, and diverse
applications, such as topic and sentiment classifi-
cation, text anonymisation, summarisation, infor-
mation extraction, and open domain question an-
swering. Briscoe &amp; Carroll (2002) give further de-
tails about the first release. Briscoe (2006) pro-
vides references and more information about ex-
tant use of RASP and fully describes the modifi-
cations discussed more briefly here.
The new release, which is free for all non-
commercial use&apos;, is designed to address several
weaknesses of the extant toolkit. Firstly, all mod-
ules have been incrementally improved to cover a
greater range of text types. Secondly, the part-of-
speech tagger lexicon has been semi-automatically
enhanced to better deal with rare or unseen be-
haviour of known words. Thirdly, better facil-
ities have been provided for user customisation.
</bodyText>
<footnote confidence="0.825663">
&apos;See http://www.informatics.susx.ac.uk/research/nlp/rasp/
for licence and download details.
</footnote>
<figure confidence="0.960293636363636">
raw text
1,
Tokeniser
1,
PoS Tagger
1,
Lemmatiser
1,
Parser/Grammar
1,
Parse Ranking Model
</figure>
<figureCaption confidence="0.999949">
Figure 1: RASP Pipeline
</figureCaption>
<bodyText confidence="0.99993825">
Fourthly, the grammatical relations output has
been redesigned to better support further process-
ing. Finally, the training and tuning of the parse
ranking model has been made more flexible.
</bodyText>
<sectionHeader confidence="0.591267" genericHeader="introduction">
2 Components of the System
</sectionHeader>
<bodyText confidence="0.9998832">
RASP is implemented as a series of modules writ-
ten in C and Common Lisp, which are pipelined,
working as a series of Unix-style filters. RASP
runs on Unix and is compatible with most C com-
pilers and Common Lisp implementations. The
public release includes Lisp and C executables for
common 32- and 64-bit architectures, shell scripts
for running and parameterising the system, docu-
mentation, and so forth. An overview of the sys-
tem is given in Figure 1.
</bodyText>
<subsectionHeader confidence="0.9665685">
2.1 Sentence Boundary Detection and
Tokenisation
</subsectionHeader>
<bodyText confidence="0.99993625">
The system is designed to take unannotated text or
transcribed (and punctuated) speech as input, and
not simply to run on pre-tokenised input such as
that typically found in corpora produced for NLP
purposes. Sentence boundary detection and to-
kenisation modules, implemented as a set of deter-
ministic finite-state rules in Flex (an open source
re-implementation of the original Unix Lex utility)
</bodyText>
<page confidence="0.978767">
77
</page>
<bodyText confidence="0.6551545">
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 77–80,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</bodyText>
<equation confidence="0.98085185">
dependent
�� � � �� � � � ����������
����
�������
��
ta arg mod det aux conj
arg
mod �
�� � � � � �� �����
�
subj
/ comp
/ �����
� �
obj pcomp clausal
ncsubj xsubjc ubj �
� � ��� �
�� � ��
�
dobj obj2 iobj xcomp ccomp
</equation>
<figureCaption confidence="0.998915">
Figure 2: The GR hierarchy
</figureCaption>
<bodyText confidence="0.973862695652174">
tagger implements the Forward-Backward algo-
rithm as well as the Viterbi algorithm, so users can
opt for tag thresholding rather than forced-choice
tagging (giving &gt;99% tag recall on DepBank, at
some cost to overall system speed). Recent exper-
iments suggest that this can
lead to a small gain
in parse accuracy as well as coverage (Watson,
2006).
� ���������
��
ncmod xmod cmod pmod
subj or dob
Vlachos
es already determined.
parsing if desired (e.g.
et al., 2006), as
well as, for example, handling of text with sen-
using a first-order
hidden markov
model (HMM) tagger implemented in C (Elwor-
thy, 1994) and trained on the manually-corrected
tagged versions of the Susanne, LOB and (sub-
set of) BNC corpora. The tagger has been aug-
mented with an unknown word model which per-
forms well under most circumstances. However,
known but rare words often caused problems as
tags for all realisations were rarely present. A se-
ries of manually developed rules has been semi-
automatically applied to the lexicon to amelio-
rate this problem by adding further tags with low
counts to rare words. The new tagger has an accu-
racy of just over 97% on the DepBank part of sec-
tion 23 of the Wall Street Journ
(‘bigram’)
al, suggesting that
this modification has resulted in competitive per-
78 formance on out-of-domain newspaper text. The
om lemmas in parses.
The terminals are featu-
ral descriptions of the preterminals, and the non-
terminals project information up the tree using
an X-bar scheme with 41 attributes with a maxi-
mum of 33 atomic values. Many of the ori
tags2.
ginal
</bodyText>
<subsectionHeader confidence="0.817208">
2.2 PoS and Punctuation Tagging
</subsectionHeader>
<bodyText confidence="0.985870157894737">
The tokenised text is tagged with one of 150
part-of-speech (PoS) and punctuation labels (de-
rived from the CLAWS tagset). This is done
and compiled into C, convert raw ASCII (or Uni-
code in UTF-8) data into a sequence of sentences
in which, for example punctuation tokens are sep-
arated from words by spaces, and so forth.
Since the first release this part of the system
has been incrementally improved to deal with a
greater variety of text types, and handle quo-
tation appropriately. Users are able to modify
the rules used and recompile the modules. All
RASP modules now accept XML mark up (with
certain hard-coded assumptions) so that data can
be pre-annotated—for example to identify named
entities—before being passed to the tokeniser, al-
lowing for more domain-dependent, potentially
multiword tokenisation and classification prior to
tence boundari
</bodyText>
<subsectionHeader confidence="0.983306">
2.3 Morphological Analysis
</subsectionHeader>
<bodyText confidence="0.999947642857143">
The morphological analyser is also implemented
in Flex, with about 1400 finite-state rules in-
corporating agreat deal of lexically exceptional
data. These rules are compiled into an efficient
C program encoding a deterministic finite state
transducer. The analyser takes a word form and
CLAWS tag and returns a lemma plus any inflec-
tional affixes. The type and token error rate of
the current system is less than 0.07% (Minnen,
Carroll and Pearce, 2001). The primary system-
internal value of morphological analysis is to en-
able later modules to use lexical information asso-
ciated with lemmas, and to facilitate further acqui-
sition of such information fr
</bodyText>
<subsectionHeader confidence="0.905277">
2.4 PoS and Punctuation Sequence Parsing
</subsectionHeader>
<bodyText confidence="0.995053647058824">
The manually-developed wide-coverage tag se-
quence grammar utilised in this version of the
parser consists of 689 unification-based phrase
structure rules (up from 400 in the first release).
The preterminals to this grammar are the PoS
and punctuation
ZThe relatively high level of detail in the tagset helps the
grammar writer to limit overgeneration and overacceptan
ce.
rules have been replaced with multiple more spe-
cific variants to increase precision. In addition,
coverage has been extended in various ways, no-
tably to cover quotation and word order permuta-
tions associated with direct and indirect quotation,
as is common in newspaper text. All rules now
have a rule-to-rule declarative specification of the
grammatical relations they license (see §2.6). Fi-
nally, around 20% of the rules have been manu-
ally identified as ‘marked’ in some way; this can
be exploited in customisation and in parse ranking.
Users can specify that certain rules should not be
used and so to some extent tune the parser to dif-
ferent genres without the need for retraining.
The current version of the grammar finds at least
one parse rooted in S for about 85% of the Susanne
corpus (used for grammar development), and most
of the remainder consists of phrasal fragments
marked as independent text sentences in passages
of dialogue. The coverage of our WSJ test data is
84%. In cases where there is no parse rooted in S,
the parser returns a connected sequence of partial
parses covering the input. The criteria are partial
parse probability and a preference for longer but
non-lexical combinations (Kiefer et al., 1999).
</bodyText>
<subsectionHeader confidence="0.935375">
2.5 Generalised LR Parser
</subsectionHeader>
<bodyText confidence="0.999988068965517">
A non-deterministic LALR(1) table is constructed
automatically from a CF ‘backbone’ compiled
from the feature-based grammar. The parser
builds a packed parse forest using this table to
guide the actions it performs. Probabilities are as-
sociated with subanalyses in the forest via those
associated with specific actions in cells of the LR
table (Inui et al., 1997). The n-best (i.e. most
probable) parses can be efficiently extracted by
unpacking subanalyses, following pointers to con-
tained subanalyses and choosing alternatives in or-
der of probabilistic ranking. This process back-
tracks occasionally since unifications are required
during the unpacking process and they occasion-
ally fail (see Oepen and Carroll, 2000).
The probabilities of actions in the LR table
are computed using bootstrapping methods which
utilise an unlabelled bracketing of the Susanne
Treebank (Watson et al., 2006). This makes the
system more easily retrainable after changes in the
grammar and opens up the possibility of quicker
tuning to in-domain data. In addition, the struc-
tural ranking induced by the parser can be re-
ranked using (in-domain) lexical data which pro-
vides conditional probability distributions for the
SUBCATegorisation attributes of the major lexi-
cal categories. Some generic data is supplied for
common verbs, but this can be augmented by user
supplied, possibly domain specific files.
</bodyText>
<subsectionHeader confidence="0.984502">
2.6 Grammatical Relations Output
</subsectionHeader>
<bodyText confidence="0.999987666666667">
The resulting set of ranked parses can be dis-
played, or passed on for further processing, in a
variety of formats which retain varying degrees of
information from the full derivations. We origi-
nally proposed transforming derivation trees into
a set of named grammatical relations (GRs), il-
lustrated as a subsumption hierarchy in Figure 2,
as a way of facilitating cross-system evaluation.
The revised GR scheme captures those aspects
of predicate-argument structure that the system is
able to recover and is the most stable and gram-
mar independent representation available. Revi-
sions include a treatment of coordination in which
the coordinator is the head in subsuming relations
to enable appropriate semantic inferences, and ad-
dition of a text adjunct (punctuation) relation to
the scheme.
Factoring rooted, directed graphs of GRs into a
set of bilexical dependencies makes it possible to
compute the transderivational support for a partic-
ular relation and thus compute a weighting which
takes account both of the probability of derivations
yielding a specific relation and of the proportion
of such derivations in the forest produced by the
parser. A weighted set of GRs from the parse for-
est is now computed efficiently using a variant of
the inside-outside algorithm (Watson et al., 2005).
</bodyText>
<sectionHeader confidence="0.996404" genericHeader="background">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9997882">
The new system has been evaluated using our re-
annotation of the PARC dependency bank (Dep-
Bank; King et al., 2003)—consisting of 560 sen-
tences chosen randomly from section 23 of the
Wall Street Journal—with grammatical relations
compatible with our system. Briscoe and Carroll
(2006) discuss issues raised by this reannotation.
Relations take the following form: (relation
subtype head dependent initial) where relation
specifies the type of relationship between the head
and dependent. The remaining subtype and ini-
tial slots encode additional specifications of the re-
lation type for some relations and the initial or un-
derlying logical relation of the grammatical sub-
ject in constructions such as passive. We deter-
</bodyText>
<page confidence="0.996578">
79
</page>
<bodyText confidence="0.999971678571429">
mine for each sentence the relations in the test set
which are correct at each level of the relational hi-
erarchy. A relation is correct if the head and de-
pendent slots are equal and if the other slots are
equal (if specified). If a relation is incorrect at
a given level in the hierarchy it may still match
for a subsuming relation (if the remaining slots all
match); for example, if a ncmod relation is mis-
labelled with zmod, it will be correct for all rela-
tions which subsume both ncmod and zmod, e.g.
mod. Similarly, the GR will be considered incor-
rect for zmod and all relations that subsume zmod
but not ncmod. Thus, the evaluation scheme cal-
culates unlabelled dependency accuracy at the de-
pendency (most general) level in the hierarchy.
The micro-averaged precision, recall and F1 score
are calculated from the counts for all relations in
the hierarchy. The macroaveraged scores are the
mean of the individual scores for each relation.
On the reannotated DepBank, the system
achieves a microaveraged F1 score of 76.3%
across all relations, using our new training method
(Watson et al., 2006). Briscoe and Carroll (2006)
show that the system has equivalent accuracy to
the PARC XLE parser when the morphosyntactic
features in the original DepBank gold standard are
taken into account. Figure 3 shows a breakdown
of the new system’s results by individual relation.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99728175">
Development has been partially funded by
the EPSRC RASP project (GR/N36462 and
GR/N36493) and greatly facilitated by Anna Ko-
rhonen, Diana McCarthy, Judita Preiss and An-
dreas Vlachos. Much of the system rests on ear-
lier work on the ANLT or associated tools by Bran
Boguraev, David Elworthy, Claire Grover, Kevin
Humphries, Guido Minnen, and Larry Piano.
</bodyText>
<sectionHeader confidence="0.993039" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.922063928571428">
Briscoe, E.J. (2006) An Introduction to Tag Sequence Gram-
mars and the RASP System Parser, University of Cam-
bridge, Computer Laboratory Technical Report 662.
Briscoe, E.J. and J. Carroll (2002) ‘Robust accurate statisti-
cal annotation of general text’, Proceedings of the 3rd Int.
Conf. on Language Resources and Evaluation (LREC’02),
Las Palmas, Gran Canaria, pp. 1499–1504.
Briscoe, E.J. and J. Carroll (2006) ‘Evaluating the Accu-
racy of an Unlexicalized Statistical Parser on the PARC
DepBank’, Proceedings of the COLING/ACL Conference,
Sydney, Australia.
Elworthy, D. (1994) ‘Does Baum-Welch re-estimation help
taggers?’, Proceedings of the 4th ACL Conference on Ap-
plied NLP, Stuttgart, Germany, pp. 53–58.
</reference>
<table confidence="0.993411275862069">
Relation Precision Recall Fl std GRs
dependent 79.76 77.49 78.61 10696
aux 93.33 91.00 92.15 400
conj 72.39 72.27 72.33 595
ta 42.61 51.37 46.58 292
det 87.73 90.48 89.09 1114
arg mod 79.18 75.47 77.28 8295
mod 74.43 67.78 70.95 3908
ncmod 75.72 69.94 72.72 3550
xmod 53.21 46.63 49.70 178
cmod 45.95 30.36 36.56 168
pmod 30.77 33.33 32.00 12
arg 77.42 76.45 76.94 4387
subj or dobj 82.36 74.51 78.24 3127
subj 78.55 66.91 72.27 1363
ncsubj 79.16 67.06 72.61 1354
xsubj 33.33 28.57 30.77 7
csubj 12.50 50.00 20.00 2
comp 75.89 79.53 77.67 3024
obj 79.49 79.42 79.46 2328
dobj 83.63 79.08 81.29 1764
obj2 23.08 30.00 26.09 20
iobj 70.77 76.10 73.34 544
clausal 60.98 74.40 67.02 672
xcomp 76.88 77.69 77.28 381
ccomp 46.44 69.42 55.55 291
pcomp 72.73 66.67 69.57 24
macroaverage 62.12 63.77 62.94
microaverage 77.66 74.98 76.29
</table>
<figureCaption confidence="0.987048">
Figure 3: Accuracy on DepBank
</figureCaption>
<reference confidence="0.999557064516129">
Inui, K., V. Sornlertlamvanich, H. Tanaka and T. Tokunaga
(1997) ‘A new formalization of probabilistic GLR pars-
ing’, Proceedings of the 5th International Workshop on
Parsing Technologies (IWPT’97), MIT, pp. 123–134.
Kiefer, B., H-U. Krieger, J. Carroll and R. Malouf (1999) ‘A
bag of useful techniques for efficient and robust parsing’,
Proceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, University of Mary-
land, pp. 473–480.
King, T.H., R. Crouch, S. Riezler, M. Dalrymple and R. Ka-
plan (2003) ‘The PARC700 Dependency Bank’, Proceed-
ings of the 4th International Workshop on Linguistically
Interpreted Corpora (LINC-03), Budapest, Hungary.
Minnen, G., J. Carroll and D. Pearce (2001) ‘Applied mor-
phological processing of English’, Natural Language En-
gineering, vol.7.3, 225–250.
Oepen, S. and J. Carroll (2000) ‘Ambiguity packing in
constraint-based parsing — practical results’, Proceedings
of the 1st Conference of the North American Chapter of the
Association for Computational Linguistics, Seattle, WA,
pp. 162–169.
Watson, R. (2006) ‘Part-of-speech tagging models for pars-
ing’, Proceedings of the 9th Annual CLUK Colloquium,
Open University, Milton Keynes, UK.
Watson, R., E.J. Briscoe and J. Carroll (2006) Semi-
supervised Training of a Statistical Parserfrom Unlabeled
Partially-bracketed Data, forthcoming.
Watson, R., J. Carroll and E.J. Briscoe (2005) ‘Efficient ex-
traction of grammatical relations’, Proceedings of the 9th
Int. Workshop on Parsing Technologies (IWPT’05), Van-
couver, Canada.
</reference>
<page confidence="0.99824">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.849987">
<title confidence="0.982699">The Second Release of the RASP System</title>
<address confidence="0.973221">Laboratory, University of Cambridge, Cambridge CB3 OFD, UK</address>
<email confidence="0.990294">firstname.lastname@cl.cam.ac.uk</email>
<address confidence="0.910885">of Informatics, University of Sussex, Brighton BN1 9QH, UK</address>
<email confidence="0.994328">J.A.Carroll@sussex.ac.uk</email>
<abstract confidence="0.9990444">We describe the new release of the RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text. The new version includes a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more flexible and semi-supervised training method for the structural parse ranking model. We evaluate the released version on the WSJ using a relational evaluation scheme, and describe how the new release allows users to enhance performance using (in-domain) lexical information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
</authors>
<title>An Introduction to Tag Sequence Grammars and the RASP System Parser,</title>
<date>2006</date>
<tech>Technical Report 662.</tech>
<institution>University of Cambridge, Computer Laboratory</institution>
<contexts>
<context position="1435" citStr="Briscoe (2006)" startWordPosition="201" endWordPosition="202">n-domain) lexical information. 1 Introduction The first public release of the RASP system (Briscoe &amp; Carroll, 2002) has been downloaded by over 120 sites and used in diverse natural language processing tasks, such as anaphora resolution, word sense disambiguation, identifying rhetorical relations, resolving metonymy, detecting compositionality in phrasal verbs, and diverse applications, such as topic and sentiment classification, text anonymisation, summarisation, information extraction, and open domain question answering. Briscoe &amp; Carroll (2002) give further details about the first release. Briscoe (2006) provides references and more information about extant use of RASP and fully describes the modifications discussed more briefly here. The new release, which is free for all noncommercial use&apos;, is designed to address several weaknesses of the extant toolkit. Firstly, all modules have been incrementally improved to cover a greater range of text types. Secondly, the part-ofspeech tagger lexicon has been semi-automatically enhanced to better deal with rare or unseen behaviour of known words. Thirdly, better facilities have been provided for user customisation. &apos;See http://www.informatics.susx.ac.u</context>
</contexts>
<marker>Briscoe, 2006</marker>
<rawString>Briscoe, E.J. (2006) An Introduction to Tag Sequence Grammars and the RASP System Parser, University of Cambridge, Computer Laboratory Technical Report 662.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text’,</title>
<date>2002</date>
<booktitle>Proceedings of the 3rd Int. Conf. on Language Resources and Evaluation (LREC’02), Las Palmas, Gran Canaria,</booktitle>
<pages>1499--1504</pages>
<contexts>
<context position="936" citStr="Briscoe &amp; Carroll, 2002" startWordPosition="128" endWordPosition="131">se of the RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text. The new version includes a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more flexible and semi-supervised training method for the structural parse ranking model. We evaluate the released version on the WSJ using a relational evaluation scheme, and describe how the new release allows users to enhance performance using (in-domain) lexical information. 1 Introduction The first public release of the RASP system (Briscoe &amp; Carroll, 2002) has been downloaded by over 120 sites and used in diverse natural language processing tasks, such as anaphora resolution, word sense disambiguation, identifying rhetorical relations, resolving metonymy, detecting compositionality in phrasal verbs, and diverse applications, such as topic and sentiment classification, text anonymisation, summarisation, information extraction, and open domain question answering. Briscoe &amp; Carroll (2002) give further details about the first release. Briscoe (2006) provides references and more information about extant use of RASP and fully describes the modificati</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Briscoe, E.J. and J. Carroll (2002) ‘Robust accurate statistical annotation of general text’, Proceedings of the 3rd Int. Conf. on Language Resources and Evaluation (LREC’02), Las Palmas, Gran Canaria, pp. 1499–1504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Evaluating the Accuracy of an Unlexicalized Statistical Parser on the PARC DepBank’,</title>
<date>2006</date>
<booktitle>Proceedings of the COLING/ACL Conference,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="11519" citStr="Briscoe and Carroll (2006)" startWordPosition="1837" endWordPosition="1840">compute a weighting which takes account both of the probability of derivations yielding a specific relation and of the proportion of such derivations in the forest produced by the parser. A weighted set of GRs from the parse forest is now computed efficiently using a variant of the inside-outside algorithm (Watson et al., 2005). 3 Evaluation The new system has been evaluated using our reannotation of the PARC dependency bank (DepBank; King et al., 2003)—consisting of 560 sentences chosen randomly from section 23 of the Wall Street Journal—with grammatical relations compatible with our system. Briscoe and Carroll (2006) discuss issues raised by this reannotation. Relations take the following form: (relation subtype head dependent initial) where relation specifies the type of relationship between the head and dependent. The remaining subtype and initial slots encode additional specifications of the relation type for some relations and the initial or underlying logical relation of the grammatical subject in constructions such as passive. We deter79 mine for each sentence the relations in the test set which are correct at each level of the relational hierarchy. A relation is correct if the head and dependent sl</context>
<context position="13080" citStr="Briscoe and Carroll (2006)" startWordPosition="2098" endWordPosition="2101">e.g. mod. Similarly, the GR will be considered incorrect for zmod and all relations that subsume zmod but not ncmod. Thus, the evaluation scheme calculates unlabelled dependency accuracy at the dependency (most general) level in the hierarchy. The micro-averaged precision, recall and F1 score are calculated from the counts for all relations in the hierarchy. The macroaveraged scores are the mean of the individual scores for each relation. On the reannotated DepBank, the system achieves a microaveraged F1 score of 76.3% across all relations, using our new training method (Watson et al., 2006). Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. Figure 3 shows a breakdown of the new system’s results by individual relation. Acknowledgements Development has been partially funded by the EPSRC RASP project (GR/N36462 and GR/N36493) and greatly facilitated by Anna Korhonen, Diana McCarthy, Judita Preiss and Andreas Vlachos. Much of the system rests on earlier work on the ANLT or associated tools by Bran Boguraev, David Elworthy, Claire Grover, Kevin Humphries, Guido Minnen, and</context>
</contexts>
<marker>Briscoe, Carroll, 2006</marker>
<rawString>Briscoe, E.J. and J. Carroll (2006) ‘Evaluating the Accuracy of an Unlexicalized Statistical Parser on the PARC DepBank’, Proceedings of the COLING/ACL Conference, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch re-estimation help taggers?’,</title>
<date>1994</date>
<booktitle>Proceedings of the 4th ACL Conference on Applied NLP,</booktitle>
<pages>53--58</pages>
<location>Stuttgart, Germany,</location>
<contexts>
<context position="4321" citStr="Elworthy, 1994" startWordPosition="680" endWordPosition="682"> GR hierarchy tagger implements the Forward-Backward algorithm as well as the Viterbi algorithm, so users can opt for tag thresholding rather than forced-choice tagging (giving &gt;99% tag recall on DepBank, at some cost to overall system speed). Recent experiments suggest that this can lead to a small gain in parse accuracy as well as coverage (Watson, 2006). � ��������� �� ncmod xmod cmod pmod subj or dob Vlachos es already determined. parsing if desired (e.g. et al., 2006), as well as, for example, handling of text with senusing a first-order hidden markov model (HMM) tagger implemented in C (Elworthy, 1994) and trained on the manually-corrected tagged versions of the Susanne, LOB and (subset of) BNC corpora. The tagger has been augmented with an unknown word model which performs well under most circumstances. However, known but rare words often caused problems as tags for all realisations were rarely present. A series of manually developed rules has been semiautomatically applied to the lexicon to ameliorate this problem by adding further tags with low counts to rare words. The new tagger has an accuracy of just over 97% on the DepBank part of section 23 of the Wall Street Journ (‘bigram’) al, s</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>Elworthy, D. (1994) ‘Does Baum-Welch re-estimation help taggers?’, Proceedings of the 4th ACL Conference on Applied NLP, Stuttgart, Germany, pp. 53–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Inui</author>
<author>V Sornlertlamvanich</author>
<author>H Tanaka</author>
<author>T Tokunaga</author>
</authors>
<title>A new formalization of probabilistic GLR parsing’,</title>
<date>1997</date>
<booktitle>Proceedings of the 5th International Workshop on Parsing Technologies (IWPT’97), MIT,</booktitle>
<pages>123--134</pages>
<contexts>
<context position="8874" citStr="Inui et al., 1997" startWordPosition="1426" endWordPosition="1429">re is no parse rooted in S, the parser returns a connected sequence of partial parses covering the input. The criteria are partial parse probability and a preference for longer but non-lexical combinations (Kiefer et al., 1999). 2.5 Generalised LR Parser A non-deterministic LALR(1) table is constructed automatically from a CF ‘backbone’ compiled from the feature-based grammar. The parser builds a packed parse forest using this table to guide the actions it performs. Probabilities are associated with subanalyses in the forest via those associated with specific actions in cells of the LR table (Inui et al., 1997). The n-best (i.e. most probable) parses can be efficiently extracted by unpacking subanalyses, following pointers to contained subanalyses and choosing alternatives in order of probabilistic ranking. This process backtracks occasionally since unifications are required during the unpacking process and they occasionally fail (see Oepen and Carroll, 2000). The probabilities of actions in the LR table are computed using bootstrapping methods which utilise an unlabelled bracketing of the Susanne Treebank (Watson et al., 2006). This makes the system more easily retrainable after changes in the gram</context>
</contexts>
<marker>Inui, Sornlertlamvanich, Tanaka, Tokunaga, 1997</marker>
<rawString>Inui, K., V. Sornlertlamvanich, H. Tanaka and T. Tokunaga (1997) ‘A new formalization of probabilistic GLR parsing’, Proceedings of the 5th International Workshop on Parsing Technologies (IWPT’97), MIT, pp. 123–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kiefer</author>
<author>H-U Krieger</author>
<author>J Carroll</author>
<author>R Malouf</author>
</authors>
<title>A bag of useful techniques for efficient and robust parsing’,</title>
<date>1999</date>
<booktitle>Proceedings of the 37th Annual Meeting of the Association</booktitle>
<pages>473--480</pages>
<institution>for Computational Linguistics, University of Maryland,</institution>
<contexts>
<context position="8483" citStr="Kiefer et al., 1999" startWordPosition="1365" endWordPosition="1368">xtent tune the parser to different genres without the need for retraining. The current version of the grammar finds at least one parse rooted in S for about 85% of the Susanne corpus (used for grammar development), and most of the remainder consists of phrasal fragments marked as independent text sentences in passages of dialogue. The coverage of our WSJ test data is 84%. In cases where there is no parse rooted in S, the parser returns a connected sequence of partial parses covering the input. The criteria are partial parse probability and a preference for longer but non-lexical combinations (Kiefer et al., 1999). 2.5 Generalised LR Parser A non-deterministic LALR(1) table is constructed automatically from a CF ‘backbone’ compiled from the feature-based grammar. The parser builds a packed parse forest using this table to guide the actions it performs. Probabilities are associated with subanalyses in the forest via those associated with specific actions in cells of the LR table (Inui et al., 1997). The n-best (i.e. most probable) parses can be efficiently extracted by unpacking subanalyses, following pointers to contained subanalyses and choosing alternatives in order of probabilistic ranking. This pro</context>
</contexts>
<marker>Kiefer, Krieger, Carroll, Malouf, 1999</marker>
<rawString>Kiefer, B., H-U. Krieger, J. Carroll and R. Malouf (1999) ‘A bag of useful techniques for efficient and robust parsing’, Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, University of Maryland, pp. 473–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H King</author>
<author>R Crouch</author>
<author>S Riezler</author>
<author>M Dalrymple</author>
<author>R Kaplan</author>
</authors>
<date>2003</date>
<booktitle>The PARC700 Dependency Bank’, Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03),</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="11350" citStr="King et al., 2003" startWordPosition="1813" endWordPosition="1816">ooted, directed graphs of GRs into a set of bilexical dependencies makes it possible to compute the transderivational support for a particular relation and thus compute a weighting which takes account both of the probability of derivations yielding a specific relation and of the proportion of such derivations in the forest produced by the parser. A weighted set of GRs from the parse forest is now computed efficiently using a variant of the inside-outside algorithm (Watson et al., 2005). 3 Evaluation The new system has been evaluated using our reannotation of the PARC dependency bank (DepBank; King et al., 2003)—consisting of 560 sentences chosen randomly from section 23 of the Wall Street Journal—with grammatical relations compatible with our system. Briscoe and Carroll (2006) discuss issues raised by this reannotation. Relations take the following form: (relation subtype head dependent initial) where relation specifies the type of relationship between the head and dependent. The remaining subtype and initial slots encode additional specifications of the relation type for some relations and the initial or underlying logical relation of the grammatical subject in constructions such as passive. We det</context>
</contexts>
<marker>King, Crouch, Riezler, Dalrymple, Kaplan, 2003</marker>
<rawString>King, T.H., R. Crouch, S. Riezler, M. Dalrymple and R. Kaplan (2003) ‘The PARC700 Dependency Bank’, Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03), Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>J Carroll</author>
<author>D Pearce</author>
</authors>
<title>Applied morphological processing of English’,</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<pages>225--250</pages>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Minnen, G., J. Carroll and D. Pearce (2001) ‘Applied morphological processing of English’, Natural Language Engineering, vol.7.3, 225–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>J Carroll</author>
</authors>
<title>Ambiguity packing in constraint-based parsing — practical results’,</title>
<date>2000</date>
<booktitle>Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>162--169</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="9229" citStr="Oepen and Carroll, 2000" startWordPosition="1477" endWordPosition="1480">rom the feature-based grammar. The parser builds a packed parse forest using this table to guide the actions it performs. Probabilities are associated with subanalyses in the forest via those associated with specific actions in cells of the LR table (Inui et al., 1997). The n-best (i.e. most probable) parses can be efficiently extracted by unpacking subanalyses, following pointers to contained subanalyses and choosing alternatives in order of probabilistic ranking. This process backtracks occasionally since unifications are required during the unpacking process and they occasionally fail (see Oepen and Carroll, 2000). The probabilities of actions in the LR table are computed using bootstrapping methods which utilise an unlabelled bracketing of the Susanne Treebank (Watson et al., 2006). This makes the system more easily retrainable after changes in the grammar and opens up the possibility of quicker tuning to in-domain data. In addition, the structural ranking induced by the parser can be reranked using (in-domain) lexical data which provides conditional probability distributions for the SUBCATegorisation attributes of the major lexical categories. Some generic data is supplied for common verbs, but this </context>
</contexts>
<marker>Oepen, Carroll, 2000</marker>
<rawString>Oepen, S. and J. Carroll (2000) ‘Ambiguity packing in constraint-based parsing — practical results’, Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics, Seattle, WA, pp. 162–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Watson</author>
</authors>
<title>Part-of-speech tagging models for parsing’,</title>
<date>2006</date>
<booktitle>Proceedings of the 9th Annual CLUK Colloquium,</booktitle>
<institution>Open University,</institution>
<location>Milton Keynes, UK.</location>
<contexts>
<context position="4064" citStr="Watson, 2006" startWordPosition="636" endWordPosition="637">mputational Linguistics dependent �� � � �� � � � ���������� ���� ������� �� ta arg mod det aux conj arg mod � �� � � � � �� ����� � subj / comp / ����� � � obj pcomp clausal ncsubj xsubjc ubj � � � ��� � �� � �� � dobj obj2 iobj xcomp ccomp Figure 2: The GR hierarchy tagger implements the Forward-Backward algorithm as well as the Viterbi algorithm, so users can opt for tag thresholding rather than forced-choice tagging (giving &gt;99% tag recall on DepBank, at some cost to overall system speed). Recent experiments suggest that this can lead to a small gain in parse accuracy as well as coverage (Watson, 2006). � ��������� �� ncmod xmod cmod pmod subj or dob Vlachos es already determined. parsing if desired (e.g. et al., 2006), as well as, for example, handling of text with senusing a first-order hidden markov model (HMM) tagger implemented in C (Elworthy, 1994) and trained on the manually-corrected tagged versions of the Susanne, LOB and (subset of) BNC corpora. The tagger has been augmented with an unknown word model which performs well under most circumstances. However, known but rare words often caused problems as tags for all realisations were rarely present. A series of manually developed rul</context>
</contexts>
<marker>Watson, 2006</marker>
<rawString>Watson, R. (2006) ‘Part-of-speech tagging models for parsing’, Proceedings of the 9th Annual CLUK Colloquium, Open University, Milton Keynes, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Watson</author>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Semisupervised Training of a Statistical Parserfrom Unlabeled Partially-bracketed Data, forthcoming.</title>
<date>2006</date>
<contexts>
<context position="9401" citStr="Watson et al., 2006" startWordPosition="1503" endWordPosition="1506">orest via those associated with specific actions in cells of the LR table (Inui et al., 1997). The n-best (i.e. most probable) parses can be efficiently extracted by unpacking subanalyses, following pointers to contained subanalyses and choosing alternatives in order of probabilistic ranking. This process backtracks occasionally since unifications are required during the unpacking process and they occasionally fail (see Oepen and Carroll, 2000). The probabilities of actions in the LR table are computed using bootstrapping methods which utilise an unlabelled bracketing of the Susanne Treebank (Watson et al., 2006). This makes the system more easily retrainable after changes in the grammar and opens up the possibility of quicker tuning to in-domain data. In addition, the structural ranking induced by the parser can be reranked using (in-domain) lexical data which provides conditional probability distributions for the SUBCATegorisation attributes of the major lexical categories. Some generic data is supplied for common verbs, but this can be augmented by user supplied, possibly domain specific files. 2.6 Grammatical Relations Output The resulting set of ranked parses can be displayed, or passed on for fu</context>
<context position="13052" citStr="Watson et al., 2006" startWordPosition="2094" endWordPosition="2097"> both ncmod and zmod, e.g. mod. Similarly, the GR will be considered incorrect for zmod and all relations that subsume zmod but not ncmod. Thus, the evaluation scheme calculates unlabelled dependency accuracy at the dependency (most general) level in the hierarchy. The micro-averaged precision, recall and F1 score are calculated from the counts for all relations in the hierarchy. The macroaveraged scores are the mean of the individual scores for each relation. On the reannotated DepBank, the system achieves a microaveraged F1 score of 76.3% across all relations, using our new training method (Watson et al., 2006). Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. Figure 3 shows a breakdown of the new system’s results by individual relation. Acknowledgements Development has been partially funded by the EPSRC RASP project (GR/N36462 and GR/N36493) and greatly facilitated by Anna Korhonen, Diana McCarthy, Judita Preiss and Andreas Vlachos. Much of the system rests on earlier work on the ANLT or associated tools by Bran Boguraev, David Elworthy, Claire Grover, Kevin </context>
</contexts>
<marker>Watson, Briscoe, Carroll, 2006</marker>
<rawString>Watson, R., E.J. Briscoe and J. Carroll (2006) Semisupervised Training of a Statistical Parserfrom Unlabeled Partially-bracketed Data, forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Watson</author>
<author>J Carroll</author>
<author>E J Briscoe</author>
</authors>
<title>Efficient extraction of grammatical relations’,</title>
<date>2005</date>
<booktitle>Proceedings of the 9th Int. Workshop on Parsing Technologies (IWPT’05),</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="11222" citStr="Watson et al., 2005" startWordPosition="1790" endWordPosition="1793">ations to enable appropriate semantic inferences, and addition of a text adjunct (punctuation) relation to the scheme. Factoring rooted, directed graphs of GRs into a set of bilexical dependencies makes it possible to compute the transderivational support for a particular relation and thus compute a weighting which takes account both of the probability of derivations yielding a specific relation and of the proportion of such derivations in the forest produced by the parser. A weighted set of GRs from the parse forest is now computed efficiently using a variant of the inside-outside algorithm (Watson et al., 2005). 3 Evaluation The new system has been evaluated using our reannotation of the PARC dependency bank (DepBank; King et al., 2003)—consisting of 560 sentences chosen randomly from section 23 of the Wall Street Journal—with grammatical relations compatible with our system. Briscoe and Carroll (2006) discuss issues raised by this reannotation. Relations take the following form: (relation subtype head dependent initial) where relation specifies the type of relationship between the head and dependent. The remaining subtype and initial slots encode additional specifications of the relation type for s</context>
</contexts>
<marker>Watson, Carroll, Briscoe, 2005</marker>
<rawString>Watson, R., J. Carroll and E.J. Briscoe (2005) ‘Efficient extraction of grammatical relations’, Proceedings of the 9th Int. Workshop on Parsing Technologies (IWPT’05), Vancouver, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>