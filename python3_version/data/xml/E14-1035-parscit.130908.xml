<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020764">
<title confidence="0.994862">
Dynamic Topic Adaptation for Phrase-based MT
</title>
<author confidence="0.999896">
Eva Hasler1, Phil Blunsom2, Philipp Koehn1, Barry Haddow1
</author>
<affiliation confidence="0.9989105">
1School of Informatics, University of Edinburgh
2Dept. of Computer Science, University of Oxford
</affiliation>
<sectionHeader confidence="0.98935" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961736842105">
Translating text from diverse sources
poses a challenge to current machine
translation systems which are rarely
adapted to structure beyond corpus level.
We explore topic adaptation on a diverse
data set and present a new bilingual vari-
ant of Latent Dirichlet Allocation to com-
pute topic-adapted, probabilistic phrase
translation features. We dynamically in-
fer document-specific translation proba-
bilities for test sets of unknown origin,
thereby capturing the effects of document
context on phrase translations. We show
gains of up to 1.26 BLEU over the base-
line and 1.04 over a domain adaptation
benchmark. We further provide an anal-
ysis of the domain-specific data and show
additive gains of our model in combination
with other types of topic-adapted features.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959913043478">
In statistical machine translation (SMT), there has
been a lot of interest in trying to incorporate in-
formation about the provenance of training exam-
ples in order to improve translations for specific
target domains. A popular approach are mixture
models (Foster and Kuhn, 2007) where each com-
ponent contains data from a specific genre or do-
main. Mixture models can be trained for cross-
domain adaption when the target domain is known
or for dynamic adaptation when the target domain
is inferred from the source text under translation.
More recent domain adaptation methods employ
corpus or instance weights to promote relevant
training examples (Matsoukas et al., 2009; Fos-
ter et al., 2010) or do more radical data selection
based on language model perplexity (Axelrod et
al., 2011). In this work, we are interested in the
dynamic adaptation case, which is challenging be-
cause we cannot tune our model towards any spe-
cific domain.
In previous literature, domains have often been
loosely defined in terms of corpora, for exam-
ple, news texts would be defined as belonging to
the news domain, ignoring the specific content of
news documents. It is often assumed that the data
within a domain is homogeneous in terms of style
and vocabulary, though that is not always true in
practice. The term topic on the other hand can
describe the thematic content of a document (e.g.
politics, economy, medicine) or a latent cluster in a
topic model. Topic modelling for machine transla-
tion aims to find a match between thematic context
and topic clusters. We view topic adaptation as
fine-grained domain adaptation with the implicit
assumption that there can be multiple distributions
over translations within the same data set. If these
distributions overlap, then we expect topic adapta-
tion to help separate them and yield better trans-
lations than an unadapted system. Topics can be
of varying granularity and are therefore a flexi-
ble means to structure data that is not uniform
enough to be modelled in its entirety. In recent
years there have been several attempts to integrat-
ing topical information into SMT either by learn-
ing better word alignments (Zhao and Xing, 2006),
by adapting translation features cross-domain (Su
et al., 2012), or by dynamically adapting lexical
weights (Eidelman et al., 2012) or adding sparse
topic features (Hasler et al., 2012).
We take a new approach to topic adaptation by
estimating probabilistic phrase translation features
in a completely Bayesian fashion. The motivation
is that automatically identifying topics in the train-
ing data can help to select the appropriate transla-
tion of a source phrase in the context of a docu-
ment. By adapting a system to automatically in-
duced topics we do not have to trust data from a
given domain to be uniform. We also overcome
the problem of defining the level of granularity for
domain adaptation. With more and more training
data automatically extracted from the web and lit-
tle knowledge about its content, we believe this
is an important area to focus on. Translation of
web sites is already a popular application for MT
systems and could be helped by dynamic model
adaptation. We present results on a mixed data
set of the TED corpus, parts of the Commoncrawl
corpus which contains crawled web data and parts
of the News Commentary corpus which contains
</bodyText>
<page confidence="0.976973">
328
</page>
<note confidence="0.9989115">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 328–337,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.98285">
Figure 1: Phrasal LDA model for inference on
training data.
</figureCaption>
<bodyText confidence="0.999910111111111">
documents about politics and economics. We be-
lieve that the broad range of this data set makes it a
suitable testbed for topic adaptation. We focus on
translation model adaptation to learn how words
and phrases translate in a given document-context
without knowing the origin of the document. By
learning translations over latent topics and com-
bining several topic-adapted features we achieve
improvements of more than 1 BLEU point.
</bodyText>
<sectionHeader confidence="0.989146" genericHeader="introduction">
2 Bilingual topic models over phrase
pairs
</sectionHeader>
<bodyText confidence="0.999982190476191">
Our model is based on LDA and infers topics
as distributions over phrase pairs instead of over
words. It is specific to machine translation in that
the conditional dependencies between source and
target phrases are modelled explicitly, and there-
fore we refer to it as phrasal LDA. Topic distribu-
tions learned on a training corpus are carried over
to tuning and test sets by running a modified in-
ference algorithm on the source side text of those
sets. Translation probabilities are adapted sepa-
rately to each source text under translation which
makes this a dynamic topic adaptation approach.
In the following we explain our approach to topic
modelling with the objective of estimating better
phrase translation probabilities for data sets that
exhibit a heterogeneous structure in terms of vo-
cabulary and style. The advantage from a mod-
elling point of view is that unlike with mixture
models, we avoid sparsity problems that would
arise if we treated documents or sets of documents
as domains and learned separate models for them.
</bodyText>
<subsectionHeader confidence="0.995244">
2.1 Latent Dirichlet Allocation (LDA)
</subsectionHeader>
<bodyText confidence="0.999336190476191">
LDA is a generative model that learns latent top-
ics in a document collection. In the original
formulation, topics are multinomial distributions
over words of the vocabulary and each docu-
ment is assigned a multinomial distribution over
topics (Blei et al., 2003). Our goal is to learn
topic-dependent phrase translation probabilities
and hence we modify this formulation by replac-
ing words with phrase pairs. This is straightfor-
ward when both source and target phrases are ob-
served but requires a modified inference approach
when only source phrases are observed in an un-
known test set. Different from standard LDA and
previous uses of LDA for MT, we define a bilin-
gual topic model that learns topic distributions
over phrase pairs. This allows us to model the
units of interest in a more principled way, without
the need to map per-word or per-sentence topics to
phrase pairs. Figure 1 shows a graphical represen-
tation of the following generative process.
For each of N documents in the collection
</bodyText>
<listItem confidence="0.999257636363636">
1. Choose topic distribution 9d — Dirichlet(a).
2. Choose the number of phrases pairs Pd in the
document, Pd — Poisson(�).
3. For every position di in the document corre-
sponding to a phrase pair pd,i of source and
target phrase si and ti1:
(a) Choose a topic zd,i — Multinomial(9d).
(b) Conditioned on topic zd,i, choose a
source phrase sd,i — Multinomial(tVzd,i).
(c) Conditioned on zd,i and sd,i, choose tar-
get phrase td,i — Multinomial(�sd,i,zd,i).
</listItem>
<bodyText confidence="0.994512818181818">
a, R and y are parameters of the Dirichlet dis-
tributions, which are asymmetric for k = 0. Our
inference algorithm is an implementation of col-
lapsed variational Bayes (CVB), with a first-order
Gaussian approximation (Teh et al., 2006). It has
been shown to be more accurate than standard VB
and to converge faster than collapsed Gibbs sam-
pling (Teh et al., 2006; Wang and Blunsom, 2013),
with little loss in accuracy. Because we have to
do inference over a large number of phrase pairs,
CVB is more practical than Gibbs sampling.
</bodyText>
<subsectionHeader confidence="0.999924">
2.2 Overview of training strategy
</subsectionHeader>
<bodyText confidence="0.999947583333333">
Ultimately, we want to learn translation probabil-
ities for all possible phrase pairs that apply to a
given test document during decoding. Therefore,
topic modelling operates on phrase pairs as they
will be seen during decoding. Given word-aligned
parallel corpora from several domains, we extract
lists of per-document phrase pairs produced by the
extraction algorithm in the Moses toolkit (Koehn
et al., 2007) which contain all phrase pairs consis-
tent with the word alignment. We run CVB on the
set of all training documents to learn latent topics
without providing information about the domains.
</bodyText>
<footnote confidence="0.951733">
1Parallel documents are modelled as bags of phrase pairs.
</footnote>
<page confidence="0.999102">
329
</page>
<bodyText confidence="0.999914571428571">
Using the trained model, CVB with modified in-
ference is run on all test documents with the set of
possible phrase translations that a decoder would
load from a phrase table before decoding. When
test inference has finished, we compute adapted
translation probabilities at the document-level by
marginalising over topics for each phrase pair.
</bodyText>
<sectionHeader confidence="0.99275" genericHeader="method">
3 Bilingual topic inference
</sectionHeader>
<subsectionHeader confidence="0.999643">
3.1 Inference on training documents
</subsectionHeader>
<bodyText confidence="0.999627214285714">
The aim of inference on the training data is to
find latent topics in the distributions over phrase
pairs in each document.This is done by repeatedly
visiting all phrase pair positions in all documents,
computing conditional topic probabilities and up-
dating counts. To bias the model to cluster stop
word phrases in one topic, we place an asymmet-
ric prior over the hyperparameters2 as described in
(Wallach et al., 2009) to make one of the topics a
priori more probable in every document. We use
a fixed-point update (Minka, 2012) to update the
hyperparameters after every iteration. For CVB
the conditional probability of topic zd,i given the
current state of all variables except zd,i is
</bodyText>
<equation confidence="0.999849375">
P(zd,i = k|z−(d,i),s,t,d,α,β,γ) ∝
(Eˆq[n−(d,i)
.,k,s,t ] + β)
(Eˆq[n−(d,i)
.,k,s,.
] + Ts · β)
·(Eˆq[n−(d,i)
d,k,. ] + α) (1)
</equation>
<bodyText confidence="0.999566954545454">
where s and t are all source and target phrases in
the collection. n−(d,i) −(d,i)
.,k,s,t , n.,k,s,. and n−(d,i)
d,k,. are cooc-
currence counts of topics with phrase pairs, source
phrases and documents respectively. Eˆq is the
expectation under the variational posterior and in
comparison to Gibbs sampling where the posterior
would otherwise look very similar, counts are re-
placed by their means. n−(d,i)
.,k,. is a topic occurrence
count, Ts is the number of possible target phrases
for a given source phrase and S is the total num-
ber of source phrases. By modelling phrase trans-
lation probabilities separately as P(ti|si,zi = k,..)
and P(si|zi = k,..), we can put different priors on
these distributions. For example, we want a sparse
distribution over target phrases for a given source
phrase and topic to express our translation prefer-
ence under each topic. The algorithm stops when
the variational posterior has converged for all doc-
uments or after a maximum of 100 iterations.
</bodyText>
<subsectionHeader confidence="0.966005">
3.2 Inference on tuning and test documents
</subsectionHeader>
<bodyText confidence="0.997767">
To compute translation probabilities for tuning
and test documents where target phrases are not
</bodyText>
<footnote confidence="0.620709">
2Omitted from the following equations for simplicity.
</footnote>
<bodyText confidence="0.9991075">
observed, the variational posterior is adapted as
shown in Equation 2
</bodyText>
<equation confidence="0.999861">
P(zd,i = k,ti,j|z−(d,i),s,t−(d,i),d,α,β,γ) ∝
(Eˆq[n−(d,i)
.,k,s,tj ] + β)
(Eˆq[n−(d,i)
.,k,s,. ] + Ts · β)
·(Eˆq[n−(d,i)
d,k,. ] + α) (2)
</equation>
<bodyText confidence="0.999917037037037">
which now computes the joint conditional prob-
ability of a topic k and a target phrase ti,j, given the
source phrase si and the test document d. There-
fore, the size of the support changes from K to
K ·Ts. While during training inference we compute
a distribution over topics for each source-target
pair, in test inference we can use the posterior to
marginalise out the topics and get a distribution
over target phrases for each source phrase.
We use the Moses decoder to produce lists of
translation options for each document in the tun-
ing and test sets. These lists comprise all phrase
pairs that will enter the search space at decod-
ing time. By default, only 20 target phrases per
source phrase are loaded from the phrase table,
so in order to allow for new phrase pairs to en-
ter the search space and for translation probabil-
ities to be computed more accurately, we allow
for up to 200 target phrases per source. For each
source sentence, we consider all possible phrase
segmentations and applicable target phrases. Un-
like in training, we do not iterate over all phrase
pairs in the list but over blocks of up to 200 target
phrases for a given source phrase. The algorithm
stops when all marginal translation probabilities
have converged though in practice we stopped ear-
lier to avoid overfitting.
</bodyText>
<subsectionHeader confidence="0.999616">
3.3 Phrase translation probabilities
</subsectionHeader>
<bodyText confidence="0.9999546">
After topic inference on the tuning and test data,
the forward translation probabilities P(t|s,d) are
computed. This is done separately for every doc-
ument d because we are interested in the trans-
lation probabilities that depend on the inferred
topic proportions for a given document. For ev-
ery document, we iterate over source positions pd,i
and use the current variational posterior to com-
pute P(ti,j|si,d) for all possible target phrases by
marginalizing over topics:
</bodyText>
<equation confidence="0.997354">
P(ti,j|si,d) = ∑kP(zi = k,ti,j|z−(d,i),s,t−(d,i),d)
</equation>
<bodyText confidence="0.9999485">
This is straightforward because during test in-
ference the variational posterior is normalised to
a distribution over topics and target phrases for
a given source phrase. If a source phrase oc-
curs multiple times in the same document, the
probabilities are averaged over all occurrences.
The inverse translation probabilities can be com-
puted analogously except that in cases where we
</bodyText>
<equation confidence="0.99436875">
(Eˆq[n−(d,i)
.,k,s,. ] +γ)
(Eˆq[n−(d,i)
.,k,. ] +S·γ)
(Eˆq[n−(d,i)
.,k,s,. ]+ γ)
(Eˆq[n−(d,i)
.,k,. ] + S · γ)
</equation>
<page confidence="0.931486">
330
</page>
<bodyText confidence="0.9963496">
do not have variational posteriors for a given pair
of source and target phrases, an approximation is
needed. We omit the results here since our exper-
iments so far did not indicate improvements with
the inverse features included.
</bodyText>
<sectionHeader confidence="0.766224" genericHeader="method">
4 More topic-adapted features
</sectionHeader>
<bodyText confidence="0.999915181818182">
Inspired by previous work on topic adaptation for
SMT, we add three additional topic-adapted fea-
tures to our model. All of these features make
use of the topic mixtures learned by our bilingual
topic model. The first feature is an adapted lexi-
cal weight, similar to the features in the work of
Eidelman et al. (2012). Our feature is different in
that we marginalise over topics to produce a single
adapted feature where v[k] is the kth element of a
document topic vector for document d and w(t|s,k)
is a topic-dependent word translation probability:
</bodyText>
<equation confidence="0.999157">
lex(¯t|¯s,d) =
1
{j|(i, j) ∈ a} E Ew(t|s,k)·v[k]
∀(i,j)∈a k
 |{z }
w(t|s)
</equation>
<bodyText confidence="0.999381166666667">
The second feature is a target unigram feature
similar to the lazy MDI adaptation of Ruiz and
Federico (2012). It includes an additional term
that measures the relevance of a target word wi by
comparing its document-specific probability Pdoc
to its probability under the asymmetric topic 0:
</bodyText>
<equation confidence="0.994496333333333">
· f( Pdoc(wi)
Ptopic0(wi))
 |{z }
relevance
(4)
, x &gt; 0 (5)
</equation>
<bodyText confidence="0.999659">
The third feature is a document similarity fea-
ture, similar to the semantic feature described by
Banchs and Costa-jussà (2011):
</bodyText>
<equation confidence="0.9880685">
docSimt = max(1 −JSD(vtrain doci,vtest doc)) (6)
i
</equation>
<bodyText confidence="0.9998966">
where vtrain_doci and vtest_doc are document topic
vector of training and test documents. Because
topic 0 captures phrase pairs that are common to
many documents, we exclude it from the topic
vectors before computing similarities.
</bodyText>
<subsectionHeader confidence="0.997486">
4.1 Feature combination
</subsectionHeader>
<bodyText confidence="0.99983775">
We tried integrating the four topic-adapted fea-
tures separately and in all possible combinations.
As we will see in the results section, while all fea-
tures improve over the baseline in isolation, the
adapted translation feature P(t|s,d) is the strongest
feature. For the features that have a counterpart in
the baseline model (p(t|s,d) and lex(t|s,d)), we ex-
perimented with either adding or replacing them in
</bodyText>
<table confidence="0.98751875">
Data Mixed CC NC TED
Train 354K (6450) 110K 103K 140K
Dev 2453 (39) 818 817 818
Test 5664 (112) 1892 1878 1894
</table>
<tableCaption confidence="0.987284">
Table 1: Number of sentence pairs and documents
</tableCaption>
<bodyText confidence="0.988915222222222">
(in brackets) in the French-English data sets. The
training data has 2.7M English words per domain.
the log-linear model. We found that while adding
the features worked well and yielded close to zero
weights for their baseline counterparts after tun-
ing, replacing them yielded better results in com-
bination with the other adapted features. We be-
lieve the reason could be that fewer phrase table
features in total are easier to optimise.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="method">
5 Experimental setup
</sectionHeader>
<subsectionHeader confidence="0.984169">
5.1 Data and baselines
</subsectionHeader>
<bodyText confidence="0.999935974358975">
Our experiments were carried out on a mixed
data set, containing the TED corpus (Cettolo et
al., 2012), parts of the News Commentary cor-
pus (NC) and parts of the Commoncrawl corpus
(CC) from the WMT13 shared task (Bojar et al.,
2013) as described in Table 1. We were guided
by two constraints in chosing our data set. 1) the
data has document boundaries and the content of
each document is assumed to be topically related,
2) there is some degree of topical variation within
each data set. In order to compare to domain adap-
tation approaches, we chose a setup with data from
different corpora. We want to abstract away from
adaptation effects that concern tuning of length
penalties and language models, so we use a mixed
tuning set containing data from all three domains
and train one language model on the concatenation
of (equally sized) target sides of the training data.
Word alignments are trained on the concatenation
of all training data and fixed for all models.
Our baseline (ALL) is a phrase-based French-
English system trained on the concatenation of
all parallel data. It was built with the Moses
toolkit (Koehn et al., 2007) using the 14 standard
core features including a 5gram language model.
Translation quality is evaluated on a large test set,
using the average feature weights of three optimi-
sation runs with PRO (Hopkins and May, 2011).
We use the mteval-v13a.pl script to compute case-
insensitive BLEU. As domain-aware benchmark
systems, we use the phrase table fill-up method
(FILLUP) of Bisazza et al. (2011) which pre-
serves the translation scores of phrases from the
IN model and the linear mixture models (LIN-
TM) of Sennrich (2012b) (both available in the
Moses toolkit). For both systems, we build sepa-
rate phrase tables for each domain and use a wrap-
per to decode tuning and test sets with domain-
specific tables. Both benchmarks have an advan-
</bodyText>
<equation confidence="0.989427454545455">
|t|
∏
i
(3)
trgUnigramst = |t |f( Pdoc(wi) )
∏ Pbaseline(wi)
i=1  |{z }
lazy MDI
2
f (x) = 1 + 1
x
</equation>
<page confidence="0.979755">
331
</page>
<table confidence="0.997082">
Model Mixed CC NC TED
IN 26.77 18.76 29.56 32.47
ALL 26.86 19.61 29.42 31.88
Table 2: BLEU of in-domain and baseline models.
Model Avg JSD
Ted-half vs Ted-full 0.07
CC-half vs CC-full 0.17
NC-half vs NC-full 0.09
Model Avg JSD Rank1-diff
Ted-IN vs ALL 0.15 10.8%
CC-IN vs ALL 0.17 18.4%
NC-IN vs ALL 0.13 13.3%
</table>
<tableCaption confidence="0.966598">
Table 3: Average JSD of IN vs. ALL models.
Rank1-diff: % PT entries where preferred transla-
tion changes.
</tableCaption>
<bodyText confidence="0.998418928571429">
tage over our model because they are aware of do-
main boundaries in the test set. Further, LIN-TM
adapts phrase table features in both translation di-
rections while we only adapt the forward features.
Table 2 shows BLEU scores of the baseline sys-
tem as well as the performance of three in-domain
models (IN) tuned under the same conditions. For
the IN models, every portion of the test set is de-
coded with a domain-specific model. Results on
the test set are broken down by domain but also
reported for the entire test set (mixed). For Ted
and NC, the in-domain models perform better than
ALL, while for CC the all-domain model improves
quite significantly over IN.
</bodyText>
<subsectionHeader confidence="0.998106">
5.2 General properties of the data sets
</subsectionHeader>
<bodyText confidence="0.999118034482759">
In this section we analyse some internal properties
of our three data sets that are relevant for adapta-
tion. All of the scores were computed on the sets
of source side tokens of the test set which were
limited to contain content words (nouns, verbs, ad-
jectives and adverbs). The test set was tagged with
the French TreeTagger (Schmid, 1994). The top of
Table 3 shows the average Jensen-Shannon diver-
gence (using log2, JSD E [0,1]) of each in-domain
model in comparison to the all-domain model,
which is an indicator of how much the distribu-
tions in the IN model change when adding out-of-
domain data. Likewise, Rank1-diff gives the per-
centage of word tokens in the test set where the
preferred translation according to p(e|f) changes
between IN and ALL. These are the words that
are most affected by adding data to the IN model.
Both numbers show that for Commoncrawl the IN
and ALL models differ more than in the other two
data sets. According to the JS divergence between
NC-IN and ALL, translation distibutions in the NC
phrase table are most similar to the ALL phrase
table. Table 4 shows the average JSD for each IN
model compared to a model trained on half of its
in-domain data. This score gives an idea of how
diverse a data set is, measured by comparing dis-
tributions over translations for source words in the
test set. According to this score, Commoncrawl
is the most diverse data set and Ted the most uni-
</bodyText>
<tableCaption confidence="0.791264">
Table 4: Average JSD of in-domain models
trained on half vs. all of the data.
</tableCaption>
<bodyText confidence="0.989174555555556">
form. Note however, that these divergence scores
do not provide information about the relative qual-
ity of the systems under comparison. For CC,
the ALL model yields a much higher BLEU score
than the IN model and it is likely that this is due to
noisy data in the CC corpus. In this case, the high
divergence is likely to mean that distributions are
corrected by out-of-domain data rather than being
shifted away from in-domain distributions.
</bodyText>
<subsectionHeader confidence="0.989913">
5.3 Topic-dependent decoding
</subsectionHeader>
<bodyText confidence="0.9999460625">
The phrase translation probabilities and additional
features described in the last two sections are used
as features in the log-linear translation model in
addition to the baseline translation features. When
combining all four adapted features, we replace
P(t|s) and lex(t|s) by their adapted counterparts.
We construct separate phrase tables for each doc-
ument in the development and test sets and use a
wrapper around the decoder to ensure that each in-
put document is paired with a configuration file
pointing to its document-specific translation table.
Documents are decoded in sequence so that only
one phrase table needs to be loaded at a time. Us-
ing the wrapped decoder we can run parameter op-
timisation (PRO) in the usual way to get one set of
tuned weights for all test documents.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999008">
In this section we present experimental results
with phrasal LDA. We show BLEU scores in com-
parison to a baseline system and two domain-
aware benchmark systems. We also evaluate
the adapted translation distributions by looking at
translation probabilities under specific topics and
inspect translations of ambiguous source words.
</bodyText>
<subsectionHeader confidence="0.999961">
6.1 Analyis of bilingual topic models
</subsectionHeader>
<bodyText confidence="0.998686666666667">
We experimented with different numbers of top-
ics for phrasal LDA. The diagrams in Figure 2
shows blocks of training and test documents in
each of the three domains for a model with 20 top-
ics. Darker shading means that documents have
a higher proportion of a particular topic in their
document-topic distribution. The first topic is the
one that was affected by the asymmetric prior and
inspecting its most probable phrase pairs showed
that it had ’collected’ a large number of stop word
phrases. This explains why it is the topic that
is most shared across documents and domains.
</bodyText>
<page confidence="0.996778">
332
</page>
<figureCaption confidence="0.999159">
Figure 2: Document-topic distributions for train-
ing (top) and test (bottom) documents, grouped by
domain and averaged into blocks for visualisation.
</figureCaption>
<figure confidence="0.9922313">
Topic 8 Topic 11
européenne → european crise → crisis
politiques → political taux → rate
politique → policy financière →financial
intérêts → interests monétaire → monetary
Topic 14 Topic 19
hôtel → hotel web → web
plage → beach utiliser → use
situé → located logiciel → software
chambres → bedrooms données → data
</figure>
<figureCaption confidence="0.999902">
Figure 3: Frequent phrase pairs in learned topics.
</figureCaption>
<bodyText confidence="0.999527176470588">
There is quite a clear horizontal separation be-
tween documents of different domains, for exam-
ple, topics 6, 8, 19 occur mostly in Ted, NC and
CC documents respectively. The overall structure
is very similar between training (top) and test (bot-
tom) documents, which shows that test inference
was successful in carrying over the information
learned on training documents. There is also some
degree of topic sharing across domains, for exam-
ple topics 4 and 15 occur in documents of all three
domains. Figure 3 shows examples of latent topics
found during inference on the training data. Topic
8 and 11 seem to be about politics and economy
and occur frequently in documents from the NC
corpus. Topic 14 contains phrases related to ho-
tels and topic 19 is about web and software, both
frequent themes in the CC corpus.
</bodyText>
<subsectionHeader confidence="0.999704">
6.2 Comparison according to BLEU
</subsectionHeader>
<bodyText confidence="0.9855538">
In Table 5 we compare our topic-adapted features
when added separately to the baseline phrase ta-
ble. The inclusion of each feature improves over
the concatenation baseline but the combination
of all four features gives the best overall results.
Though the relative performance differs slightly
for each domain portion in the test set, overall the
adapted lexical weight is the weakest feature and
the adapted translation probability is the strongest
feature. We also performed feature ablation tests
and found that no combination of features was su-
perior to combining all four features. This con-
firms that the gains of each feature lead to additive
improvements in the combined model.
In Table 6 we compare topic-adapted models
</bodyText>
<table confidence="0.998611666666667">
Model Mixed CC NC TED
lex(e|f,d) 26.99 19.93 29.34 32.19
trgUnigrams 27.15 19.90 29.54 32.50
docSim 27.22 20.11 29.63 32.40
p(e|f,d) 27.31 20.23 29.52 32.58
All features 27.67 20.40 30.04 33.08
</table>
<tableCaption confidence="0.995194">
Table 5: BLEU scores of pLDA features (50 top-
ics), separately and combined.
</tableCaption>
<table confidence="0.999763888888889">
Model Mixed CC NC TED
ALL 26.86 19.61 29.42 31.88
3 topics 26.95 19.83 29.46 32.02
5 topics *27.48 19.98 29.94 33.04
10 topics *27.65 20.34 29.99 33.14
20 topics *27.63 20.39 29.93 33.09
50 topics *27.67 20.40 30.04 33.08
100 topics *27.65 20.54 30.00 32.90
&gt;ALL +0.81 +0.93 +0.62 +1.26
</table>
<tableCaption confidence="0.981606333333333">
Table 6: BLEU scores of baseline and topic-
adapted systems (pLDA) with all 4 features and
largest improvements over baseline.
</tableCaption>
<bodyText confidence="0.997604171428571">
with varying numbers of topics to the concatena-
tion baseline. We see a consistent gain on all do-
mains when increasing the number of topics from
three to five and ten topics. This is evidence that
the number of domain labels is in fact smaller
than the number of underlying topics. The opti-
mal number of latent topics varies for each domain
and reflects our insights from section 5.2. The CC
domain was shown to be the most diverse and the
best performance on the CC portion of the test set
is achieved with 100 topics. Likewise, the TED
domain was shown to be least diverse and here
the best performance is achieved with only 10 top-
ics. The best performance on the entire test set is
achieved with 50 topics, which is also the optimal
number of topics for the NC domain. The bot-
ton row of the table indicates the relative improve-
ment of the best topic-adapted model per domain
over the ALL model. Using all four topic-adapted
features yields an improvement of 0.81 BLEU on
the mixed test set. The highest improvement on a
given domain is achieved for TED with an increase
of 1.26 BLEU. The smallest improvement is mea-
sured on the NC domain. This is in line with the
observation that distributions in the NC in-domain
table are most similar to the ALL table, therefore
we would expect the smallest improvement for do-
main or topic adaptation. We used bootstrap re-
sampling (Koehn, 2004) to measure significance
on the mixed test set and marked all statistically
significant results compared to the respective base-
lines with asterisk (*: p :5 0.01).
To demonstrate the benefit of topic adaptation
over more standard domain adaptation approaches
for a diverse data set, we show the performance
</bodyText>
<page confidence="0.997569">
333
</page>
<table confidence="0.999322833333333">
Model Mixed CC NC TED
FILLUP 27.12 19.36 29.78 32.71
LIN-TM 27.24 19.61 29.87 32.73
pLDA *27.67 20.40 30.04 33.08
&gt;FILLUP +0.55 +1.04 +0.26 +0.37
&gt;LIN-TM +0.43 +0.79 +0.17 +0.35
</table>
<tableCaption confidence="0.943644">
Table 7: Comparison of best pLDA system with
two domain-aware benchmark systems.
</tableCaption>
<table confidence="0.999772">
Model Mixed CC NC TED
LIN-LM
+ ALL 27.16 19.71 29.77 32.46
+ FILLUP 27.20 19.37 29.84 32.90
+ LIN-TM 27.34 19.59 29.92 33.02
+ pLDA *27.84 20.48 30.03 33.57
&gt;ALL +0.68 +0.77 +0.26 +1.11
</table>
<tableCaption confidence="0.97077475">
Table 8: Combination of all models with addi-
tional LM adaptation (pLDA: 50 topics).
of two state-of-the-art domain-adapted systems in
Table 7. Both FILLUP and LIN-TM improve over
</tableCaption>
<bodyText confidence="0.980642538461538">
the ALL model on the mixed test set, by 0.26 and
0.38 BLEU respectively. The largest improvement
is on TED while on the CC domain, FILLUP de-
creases in performance and LIN-TM yields no im-
provement either. This shows that relying on in-
domain distributions for adaptation to a noisy and
diverse domain like CC is problematic. The pLDA
model yields the largest improvement over the
domain-adapted systems on the CC test set, with
in increase of 1.04 BLEU over FILLUP and 0.79
over LIN-TM. The improvements on the other two
domains are smaller but consistent.
We also compare the best model from Table 6
to all other models in combination with linearly
interpolated language models (LIN-LM), interpo-
lated separately for each domain. Though the
improvements are slightly smaller than without
adapted language models, there is still a gain over
the concatenation baseline of 0.68 BLEU on the
mixed test set and similar improvements to before
over the benchmarks (on TED the improvements
are actually even larger). Thus, we have shown
that topic-adaptation is effective for test sets of
diverse documents and that we can achieve sub-
stantial improvements even in comparison with
domain-adapted translation and language models.
</bodyText>
<subsectionHeader confidence="0.999314">
6.3 Properties of adapted distributions and
topic-specific translations
</subsectionHeader>
<bodyText confidence="0.994805428571429">
The first column of Table 9 shows the average en-
tropy of phrase table entries in the adapted models
according to p(t|s,d) versus the all-domain model,
computed over source tokens in the test set that
are content words. The entropy decreases in the
adapted tables in all cases which is an indicator
that the distributions over translations of content
</bodyText>
<table confidence="0.998434285714286">
Set Model Avg entropy Avg perplexity
CC pLDA 3.74 9.21
NC ALL 3.99 10.13
TED pLDA 3.42 6.96
ALL 3.82 7.51
pLDA 3.33 9.17
ALL 4.00 9.71
</table>
<tableCaption confidence="0.998599">
Table 9: Average entropy of translation distribu-
tions and test set perplexity of the adapted model.
</tableCaption>
<equation confidence="0.944789583333333">
régime
topic 6 diet = 0.79 diet aids = 0.04
topic 8 regime* = 0.82 rule = 0.05
topic 19 restrictions = 0.53 diplomats = 0.10
noyau
topic 9 nucleus* = 0.89 core = 0.01
topic 11 core* = 0.93 inner = 0.03
topic 19 kernel = 0.58 core = 0.11
démon
topic 6 devil = 0.89 demon = 0.07
topic 8 demon* = 0.98 devil = 0.01
topic 19 daemon = 0.95 demon = 0.04
</equation>
<bodyText confidence="0.997579428571429">
Table 10: The two most probable translations of
régime, noyau and démon and probabilities under
different latent topics (*: preferred by ALL).
words have become more peaked. The second col-
umn shows the average perplexity of target tokens
in the test set which is a measure of how likely a
model is to produce words in the reference trans-
lation. We use the alignment information between
source and reference and therefore limit our anal-
ysis to pairs of aligned words, but nevertheless
this shows that the adapted translation distribu-
tions model the test set distributions better than the
baseline model. Therefore, the adapted distribu-
tions are not just more peaked but also more often
peaked towards the correct translation.
Table 10 shows examples of ambiguous French
words that have different preferred translations de-
pending on the latent topic. The word régime can
be translated as diet, regime and restrictions and
the model has learned that the probability over
translations changes when moving from one topic
to another (preferred translations under the ALL
model are marked with *). For example, the trans-
lation to diet is most probable under topic 6 and
the translation to regime which would occur in
a political context is most probable under topic
8. Topic 6 is most prominent among Ted docu-
ments while topic 8 is found most frequently in
News Commentary documents which have a high
percentage of politically related text. The French
word noyau can be translated to nucleus (physics),
core (generic) and kernel (IT) among other trans-
lations and the topics that exhibit these preferred
translations can be attributed to Ted (which con-
tains many talks about physics), NC and CC (with
</bodyText>
<page confidence="0.99676">
334
</page>
<table confidence="0.990544833333333">
Src: “il suffit d’éjecter le noyau et d’en insérer un autre, comme ce qu’on fait pour le clônage.”
BL: “it is the nucleus eject and insert another, like what we do to the clônage.”
pLDA: “he just eject the nucleus and insert another, like what we do to the clônage.” (nucleus = 0.77)
Ref: “you can just pop out the nucleus and pop in another one, and that’s what you’ve all heard about with cloning.”
Src: “pourtant ceci obligerait les contribuables des pays de ce noyau à fournir du capital au sud”
BL: “but this would force western taxpayers to provide the nucleus of capital in the south”
pLDA: “but this would force western taxpayers to provide the core of capital in the south” (core = 0.78)
Ref: “but this would unfairly force taxpayers in the core countries to provide capital to the south”
Src: “le noyau contient de nombreux pilotes, afin de fonctionner chez la plupart des utilisateurs.”
BL: “the nucleus contains many drivers, in order to work for most users.”
pLDA: “the kernel contains many drivers, to work for most users.” (kernel = 0.53)
Ref: “the precompiled kernel includes a lot of drivers, in order to work for most users.”
</table>
<figureCaption confidence="0.994554">
Figure 4: pLDA correctly translates noyau in test docs from Ted, NC and CC (adapted probabilities in
brackets). The baseline (nucleus = 0.27, core = 0.27, kernel = 0.23) translates all instances to nucleus.
</figureCaption>
<bodyText confidence="0.965333821428571">
many IT-related documents). The last example,
démon, has three frequent translations in English:
devil, demon and daemon. The last translation
refers to a computer process and would occur in an
IT context. The topic-phrase probabilities reveal
that its mostly likely translation as daemon occurs
under topic 19 which clusters IT-related phrase
pairs and is frequent in the CC corpus. These
examples show that our model can disambiguate
phrase translations using latent topics.
As another motivating example, in Figure 4 we
compare the output of our adapted models to the
output produced by the all-domain baseline for the
word noyau from Table 10. While the ALL base-
line translates each instance of noyau to nucleus,
the adapted model translates each instance differ-
ently depending on the inferred topic mixtures for
each document and always matches the reference
translation. The probabilities in brackets show
that the chosen translations were indeed the most
likely under the respective adapted model. While
the ALL model has a flat distribution over pos-
sible translations, the adapted models are peaked
towards the correct translation. This shows that
topic-specific translation probabilities are neces-
sary when the translation of a word shifts between
topics or domains and that peaked, adapted distri-
butions can lead to more correct translations.
</bodyText>
<sectionHeader confidence="0.999984" genericHeader="evaluation">
7 Related work
</sectionHeader>
<bodyText confidence="0.999987837209302">
There has been a lot of previous work using topic
information for SMT, most of it using monolin-
gual topic models. For example, Gong and Zhou
(2011) use the topical relevance of a target phrase,
computed using a mapping between source and
target side topics, as an additional feature in de-
coding. Axelrod et al. (2012) build topic-specific
translation models from the TED corpus and se-
lect topic-relevant data from the UN corpus to im-
prove coverage. Su et al. (2012) perform phrase
table adaptation in a setting where only monolin-
gual in-domain data and parallel out-of-domain
data are available. Eidelman et al. (2012) use
topic-dependent lexical weights as features in the
translation model, which is similar to our work
in that topic features are tuned towards useful-
ness of topic information and not towards a tar-
get domain. Hewavitharana et al. (2013) per-
form dynamic adaptation with monolingual top-
ics, encoding topic similarity between a conversa-
tion and training documents in an additional fea-
ture. This is similar to the work of Banchs and
Costa-jussà (2011), both of which inspired our
document similarity feature. Also related is the
work of Sennrich (2012a) who explore mixture-
modelling on unsupervised clusters for domain
adaptation and Chen et al. (2013) who compute
phrase pair features from vector space representa-
tions that capture domain similarity to a develop-
ment set. Both are cross-domain adaptation ap-
proaches, though. Instances of multilingual topic
models outside the field of MT include Boyd-
Graber and Blei (2009; Boyd-Graber and Resnik
(2010) who learn cross-lingual topic correspon-
dences (but do not learn conditional distributions
like our model does). In terms of model structure,
our model is similar to BiTAM (Zhao and Xing,
2006) which is an LDA-style model to learn topic-
based word alignments. The work of Carpuat and
Wu (2007) is similar to ours in spirit, but they pre-
dict the most probable translation in a context at
the token level while our adaptation operates at the
type level of a document.
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999964529411765">
We have presented a novel bilingual topic model
based on LDA and applied it to the task of transla-
tion model adaptation on a diverse French-English
data set. Our model infers topic distributions over
phrase pairs to compute document-specific trans-
lation probabilities and performs dynamic adap-
tation on test documents of unknown origin. We
have shown that our model outperforms a concate-
nation baseline and two domain-adapted bench-
mark systems with BLEU gains of up to 1.26 on
domain-specific test set portions and 0.81 overall.
We have also shown that a combination of topic-
adapted features performs better than each feature
in isolation and that these gains are additive. An
analysis of the data revealed that topic adaptation
compares most favourably to domain adaptation
when the domain in question is rather diverse.
</bodyText>
<page confidence="0.9986">
335
</page>
<sectionHeader confidence="0.998507" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999909125">
This work was supported by funding from the
Scottish Informatics and Computer Science Al-
liance (Eva Hasler) and funding from the Eu-
ropean Union Seventh Framework Programme
(FP7/2007-2013) under grant agreement 287658
(EU BRIDGE) and grant agreement 288769 (AC-
CEPT). Thanks to Chris Dyer for an initial discus-
sion about the phrasal LDA model.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999888427083334">
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP. Association
for Computational Linguistics.
Amittai Axelrod, Xiaodong He, Li Deng, Alex Acero,
and Mei-Yuh Hwang. 2012. New methods and
evaluation experiments on translating TED talks in
the IWSLT benchmark. In Proceedings of ICASSP.
IEEE.
Rafael E. Banchs and Marta R. Costa-jussà. 2011. A
semantic feature for statistical machine translation.
In Proceedings of the Fifth Workshop on Syntax,
Semantics and Structure in Statistical Translation,
SSST-5. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus Interpolation Methods for
Phrase-based SMT Adaptation. In Proceedings of
IWSLT.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation.
JMLR.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of WMT 2013. Asso-
ciation for Computational Linguistics.
Jordan Boyd-Graber and David Blei. 2009. Multilin-
gual Topic Models for Unaligned Text. In Proceed-
ings of the Twenty-Fifth Conference on Uncertainty
in Artificial Intelligence. AUAI Press.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
Sentiment Analysis Across Languages: Multilingual
Supervised Latent Dirichlet Allocation. In Proceed-
ings of EMNLP. Association for Computational Lin-
guistics.
Marine Carpuat and Dekai Wu. 2007. How phrase
sense disambiguation outperforms word sense dis-
ambiguation for SMT. In International Conference
on Theoretical and Methodological Issues in MT.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed
and translated talks. In Proceedings of EAMT.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in SMT. In
Proceedings of ACL. Association for Computational
Linguistics.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of ACL. Associa-
tion for Computational Linguistics.
G. Foster and R. Kuhn. 2007. Mixture-model adapta-
tion for SMT. In Proceedings of WMT. Association
for Computational Linguistics.
G. Foster, C. Goutte, and R. Kuhn. 2010. Discrimi-
native instance weighting for domain adaptation in
SMT. In Proceedings of EMNLP. Association for
Computational Linguistics.
Zhengxian Gong and Guodong Zhou. 2011. Employ-
ing topic modeling for SMT. In Proceedings of
IEEE (CSAE), volume 4.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse lexicalised features and topic adaptation for
SMT. In Proceedings of IWSLT.
S. Hewavitharana, D. Mehay, S. Ananthakrishnan, and
P. Natarajan. 2013. Incremental topic-based TM
adaptation for conversational SLT. In Proceedings
ofACL. Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for SMT. In ACL 2007: Demo and
poster sessions. Association for Computational Lin-
guistics.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proceedings
of EMNLP. Association for Computational Linguis-
tics.
S. Matsoukas, A. Rosti, and B. Zhang. 2009. Discrim-
inative corpus weight estimation for MT. In Pro-
ceedings of EMNLP. Association for Computational
Linguistics.
Thomas P Minka. 2012. Estimating a Dirichlet distri-
bution. Technical report.
Nick Ruiz and Marcello Federico. 2012. MDI Adap-
tation for the Lazy: Avoiding Normalization in LM
Adaptation for Lecture Translation. In Proceedings
of IWSLT.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
</reference>
<page confidence="0.988862">
336
</page>
<reference confidence="0.99927676">
Rico Sennrich. 2012a. Mixture-modeling with unsu-
pervised clusters for domain adaptation in SMT. In
Proceedings of EAMT.
Rico Sennrich. 2012b. Perplexity Minimization for
Translation Model Domain Adaptation in SMT. In
Proceedings of EACL. Association for Computa-
tional Linguistics.
J. Su, H. Wu, H. Wang, Y. Chen, X. Shi, H. Dong,
and Q. Liu. 2012. Translation model adaptation
for SMT with monolingual topic information. In
Proceedings ofACL. Association for Computational
Linguistics.
Yee Whye Teh, David Newman, and Max Welling.
2006. A collapsed variational Bayesian inference
algorithm for LDA. In Proceedings of NIPS.
Hanna M. Wallach, David M. Mimno, and Andrew Mc-
Callum. 2009. Rethinking LDA: Why priors matter.
In Proceedings of NIPS.
Pengyu Wang and Phil Blunsom. 2013. Collapsed
variational Bayesian inference for Hidden Markov
Models. In AISTATS, volume 31 of JMLR Proceed-
ings, pages 599–607.
Bing Zhao and Eric P. Xing. 2006. Bilingual topic ad-
mixture models for word alignment. In Proceedings
of ACL. Association for Computational Linguistics.
</reference>
<page confidence="0.998465">
337
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.616881">
<title confidence="0.999613">Dynamic Topic Adaptation for Phrase-based MT</title>
<author confidence="0.999278">Phil Philipp Barry</author>
<affiliation confidence="0.7118075">of Informatics, University of of Computer Science, University of Oxford</affiliation>
<abstract confidence="0.9982286">Translating text from diverse sources poses a challenge to current machine translation systems which are rarely adapted to structure beyond corpus level. We explore topic adaptation on a diverse data set and present a new bilingual variant of Latent Dirichlet Allocation to compute topic-adapted, probabilistic phrase translation features. We dynamically infer document-specific translation probabilities for test sets of unknown origin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU over the baseline and 1.04 over a domain adaptation benchmark. We further provide an analysis of the domain-specific data and show additive gains of our model in combination with other types of topic-adapted features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1768" citStr="Axelrod et al., 2011" startWordPosition="271" endWordPosition="274">er to improve translations for specific target domains. A popular approach are mixture models (Foster and Kuhn, 2007) where each component contains data from a specific genre or domain. Mixture models can be trained for crossdomain adaption when the target domain is known or for dynamic adaptation when the target domain is inferred from the source text under translation. More recent domain adaptation methods employ corpus or instance weights to promote relevant training examples (Matsoukas et al., 2009; Foster et al., 2010) or do more radical data selection based on language model perplexity (Axelrod et al., 2011). In this work, we are interested in the dynamic adaptation case, which is challenging because we cannot tune our model towards any specific domain. In previous literature, domains have often been loosely defined in terms of corpora, for example, news texts would be defined as belonging to the news domain, ignoring the specific content of news documents. It is often assumed that the data within a domain is homogeneous in terms of style and vocabulary, though that is not always true in practice. The term topic on the other hand can describe the thematic content of a document (e.g. politics, eco</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of EMNLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Li Deng</author>
<author>Alex Acero</author>
<author>Mei-Yuh Hwang</author>
</authors>
<title>New methods and evaluation experiments on translating TED talks in the IWSLT benchmark.</title>
<date>2012</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<publisher>IEEE.</publisher>
<contexts>
<context position="35326" citStr="Axelrod et al. (2012)" startWordPosition="5907" endWordPosition="5910">ranslations, the adapted models are peaked towards the correct translation. This shows that topic-specific translation probabilities are necessary when the translation of a word shifts between topics or domains and that peaked, adapted distributions can lead to more correct translations. 7 Related work There has been a lot of previous work using topic information for SMT, most of it using monolingual topic models. For example, Gong and Zhou (2011) use the topical relevance of a target phrase, computed using a mapping between source and target side topics, as an additional feature in decoding. Axelrod et al. (2012) build topic-specific translation models from the TED corpus and select topic-relevant data from the UN corpus to improve coverage. Su et al. (2012) perform phrase table adaptation in a setting where only monolingual in-domain data and parallel out-of-domain data are available. Eidelman et al. (2012) use topic-dependent lexical weights as features in the translation model, which is similar to our work in that topic features are tuned towards usefulness of topic information and not towards a target domain. Hewavitharana et al. (2013) perform dynamic adaptation with monolingual topics, encoding </context>
</contexts>
<marker>Axelrod, He, Deng, Acero, Hwang, 2012</marker>
<rawString>Amittai Axelrod, Xiaodong He, Li Deng, Alex Acero, and Mei-Yuh Hwang. 2012. New methods and evaluation experiments on translating TED talks in the IWSLT benchmark. In Proceedings of ICASSP. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael E Banchs</author>
<author>Marta R Costa-jussà</author>
</authors>
<title>A semantic feature for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, SSST-5. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="15123" citStr="Banchs and Costa-jussà (2011)" startWordPosition="2473" endWordPosition="2476"> for document d and w(t|s,k) is a topic-dependent word translation probability: lex(¯t|¯s,d) = 1 {j|(i, j) ∈ a} E Ew(t|s,k)·v[k] ∀(i,j)∈a k |{z } w(t|s) The second feature is a target unigram feature similar to the lazy MDI adaptation of Ruiz and Federico (2012). It includes an additional term that measures the relevance of a target word wi by comparing its document-specific probability Pdoc to its probability under the asymmetric topic 0: · f( Pdoc(wi) Ptopic0(wi)) |{z } relevance (4) , x &gt; 0 (5) The third feature is a document similarity feature, similar to the semantic feature described by Banchs and Costa-jussà (2011): docSimt = max(1 −JSD(vtrain doci,vtest doc)) (6) i where vtrain_doci and vtest_doc are document topic vector of training and test documents. Because topic 0 captures phrase pairs that are common to many documents, we exclude it from the topic vectors before computing similarities. 4.1 Feature combination We tried integrating the four topic-adapted features separately and in all possible combinations. As we will see in the results section, while all features improve over the baseline in isolation, the adapted translation feature P(t|s,d) is the strongest feature. For the features that have a </context>
<context position="36075" citStr="Banchs and Costa-jussà (2011)" startWordPosition="6028" endWordPosition="6031">e coverage. Su et al. (2012) perform phrase table adaptation in a setting where only monolingual in-domain data and parallel out-of-domain data are available. Eidelman et al. (2012) use topic-dependent lexical weights as features in the translation model, which is similar to our work in that topic features are tuned towards usefulness of topic information and not towards a target domain. Hewavitharana et al. (2013) perform dynamic adaptation with monolingual topics, encoding topic similarity between a conversation and training documents in an additional feature. This is similar to the work of Banchs and Costa-jussà (2011), both of which inspired our document similarity feature. Also related is the work of Sennrich (2012a) who explore mixturemodelling on unsupervised clusters for domain adaptation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of multilingual topic models outside the field of MT include BoydGraber and Blei (2009; Boyd-Graber and Resnik (2010) who learn cross-lingual topic correspondences (but do not learn conditional distributions like ou</context>
</contexts>
<marker>Banchs, Costa-jussà, 2011</marker>
<rawString>Rafael E. Banchs and Marta R. Costa-jussà. 2011. A semantic feature for statistical machine translation. In Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, SSST-5. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Nick Ruiz</author>
<author>Marcello Federico</author>
</authors>
<title>Fill-up versus Interpolation Methods for Phrase-based SMT Adaptation.</title>
<date>2011</date>
<booktitle>In Proceedings of IWSLT.</booktitle>
<contexts>
<context position="18010" citStr="Bisazza et al. (2011)" startWordPosition="2954" endWordPosition="2957">e concatenation of all training data and fixed for all models. Our baseline (ALL) is a phrase-based FrenchEnglish system trained on the concatenation of all parallel data. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5gram language model. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute caseinsensitive BLEU. As domain-aware benchmark systems, we use the phrase table fill-up method (FILLUP) of Bisazza et al. (2011) which preserves the translation scores of phrases from the IN model and the linear mixture models (LINTM) of Sennrich (2012b) (both available in the Moses toolkit). For both systems, we build separate phrase tables for each domain and use a wrapper to decode tuning and test sets with domainspecific tables. Both benchmarks have an advan|t| ∏ i (3) trgUnigramst = |t |f( Pdoc(wi) ) ∏ Pbaseline(wi) i=1 |{z } lazy MDI 2 f (x) = 1 + 1 x 331 Model Mixed CC NC TED IN 26.77 18.76 29.56 32.47 ALL 26.86 19.61 29.42 31.88 Table 2: BLEU of in-domain and baseline models. Model Avg JSD Ted-half vs Ted-full </context>
</contexts>
<marker>Bisazza, Ruiz, Federico, 2011</marker>
<rawString>Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus Interpolation Methods for Phrase-based SMT Adaptation. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>John Lafferty</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="6390" citStr="Blei et al., 2003" startWordPosition="1027" endWordPosition="1030">robabilities for data sets that exhibit a heterogeneous structure in terms of vocabulary and style. The advantage from a modelling point of view is that unlike with mixture models, we avoid sparsity problems that would arise if we treated documents or sets of documents as domains and learned separate models for them. 2.1 Latent Dirichlet Allocation (LDA) LDA is a generative model that learns latent topics in a document collection. In the original formulation, topics are multinomial distributions over words of the vocabulary and each document is assigned a multinomial distribution over topics (Blei et al., 2003). Our goal is to learn topic-dependent phrase translation probabilities and hence we modify this formulation by replacing words with phrase pairs. This is straightforward when both source and target phrases are observed but requires a modified inference approach when only source phrases are observed in an unknown test set. Different from standard LDA and previous uses of LDA for MT, we define a bilingual topic model that learns topic distributions over phrase pairs. This allows us to model the units of interest in a more principled way, without the need to map per-word or per-sentence topics t</context>
</contexts>
<marker>Blei, Ng, Jordan, Lafferty, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, Michael I. Jordan, and John Lafferty. 2003. Latent dirichlet allocation. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of WMT 2013. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="16713" citStr="Bojar et al., 2013" startWordPosition="2736" endWordPosition="2739">ain. the log-linear model. We found that while adding the features worked well and yielded close to zero weights for their baseline counterparts after tuning, replacing them yielded better results in combination with the other adapted features. We believe the reason could be that fewer phrase table features in total are easier to optimise. 5 Experimental setup 5.1 Data and baselines Our experiments were carried out on a mixed data set, containing the TED corpus (Cettolo et al., 2012), parts of the News Commentary corpus (NC) and parts of the Commoncrawl corpus (CC) from the WMT13 shared task (Bojar et al., 2013) as described in Table 1. We were guided by two constraints in chosing our data set. 1) the data has document boundaries and the content of each document is assumed to be topically related, 2) there is some degree of topical variation within each data set. In order to compare to domain adaptation approaches, we chose a setup with data from different corpora. We want to abstract away from adaptation effects that concern tuning of length penalties and language models, so we use a mixed tuning set containing data from all three domains and train one language model on the concatenation of (equally</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of WMT 2013. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
</authors>
<title>Multilingual Topic Models for Unaligned Text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence.</booktitle>
<publisher>AUAI Press.</publisher>
<marker>Boyd-Graber, Blei, 2009</marker>
<rawString>Jordan Boyd-Graber and David Blei. 2009. Multilingual Topic Models for Unaligned Text. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="36577" citStr="Boyd-Graber and Resnik (2010)" startWordPosition="6105" endWordPosition="6108">en a conversation and training documents in an additional feature. This is similar to the work of Banchs and Costa-jussà (2011), both of which inspired our document similarity feature. Also related is the work of Sennrich (2012a) who explore mixturemodelling on unsupervised clusters for domain adaptation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of multilingual topic models outside the field of MT include BoydGraber and Blei (2009; Boyd-Graber and Resnik (2010) who learn cross-lingual topic correspondences (but do not learn conditional distributions like our model does). In terms of model structure, our model is similar to BiTAM (Zhao and Xing, 2006) which is an LDA-style model to learn topicbased word alignments. The work of Carpuat and Wu (2007) is similar to ours in spirit, but they predict the most probable translation in a context at the token level while our adaptation operates at the type level of a document. 8 Conclusion We have presented a novel bilingual topic model based on LDA and applied it to the task of translation model adaptation on</context>
</contexts>
<marker>Boyd-Graber, Resnik, 2010</marker>
<rawString>Jordan Boyd-Graber and Philip Resnik. 2010. Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation. In Proceedings of EMNLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>How phrase sense disambiguation outperforms word sense disambiguation for SMT.</title>
<date>2007</date>
<booktitle>In International Conference on Theoretical and Methodological Issues in MT.</booktitle>
<contexts>
<context position="36869" citStr="Carpuat and Wu (2007)" startWordPosition="6154" endWordPosition="6157">ation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of multilingual topic models outside the field of MT include BoydGraber and Blei (2009; Boyd-Graber and Resnik (2010) who learn cross-lingual topic correspondences (but do not learn conditional distributions like our model does). In terms of model structure, our model is similar to BiTAM (Zhao and Xing, 2006) which is an LDA-style model to learn topicbased word alignments. The work of Carpuat and Wu (2007) is similar to ours in spirit, but they predict the most probable translation in a context at the token level while our adaptation operates at the type level of a document. 8 Conclusion We have presented a novel bilingual topic model based on LDA and applied it to the task of translation model adaptation on a diverse French-English data set. Our model infers topic distributions over phrase pairs to compute document-specific translation probabilities and performs dynamic adaptation on test documents of unknown origin. We have shown that our model outperforms a concatenation baseline and two dom</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. How phrase sense disambiguation outperforms word sense disambiguation for SMT. In International Conference on Theoretical and Methodological Issues in MT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>Wit3: Web inventory of transcribed and translated talks.</title>
<date>2012</date>
<booktitle>In Proceedings of EAMT.</booktitle>
<contexts>
<context position="16582" citStr="Cettolo et al., 2012" startWordPosition="2712" endWordPosition="2715">umber of sentence pairs and documents (in brackets) in the French-English data sets. The training data has 2.7M English words per domain. the log-linear model. We found that while adding the features worked well and yielded close to zero weights for their baseline counterparts after tuning, replacing them yielded better results in combination with the other adapted features. We believe the reason could be that fewer phrase table features in total are easier to optimise. 5 Experimental setup 5.1 Data and baselines Our experiments were carried out on a mixed data set, containing the TED corpus (Cettolo et al., 2012), parts of the News Commentary corpus (NC) and parts of the Commoncrawl corpus (CC) from the WMT13 shared task (Bojar et al., 2013) as described in Table 1. We were guided by two constraints in chosing our data set. 1) the data has document boundaries and the content of each document is assumed to be topically related, 2) there is some degree of topical variation within each data set. In order to compare to domain adaptation approaches, we chose a setup with data from different corpora. We want to abstract away from adaptation effects that concern tuning of length penalties and language models</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit3: Web inventory of transcribed and translated talks. In Proceedings of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
</authors>
<title>Vector space model for adaptation in SMT.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="36276" citStr="Chen et al. (2013)" startWordPosition="6059" endWordPosition="6062">ights as features in the translation model, which is similar to our work in that topic features are tuned towards usefulness of topic information and not towards a target domain. Hewavitharana et al. (2013) perform dynamic adaptation with monolingual topics, encoding topic similarity between a conversation and training documents in an additional feature. This is similar to the work of Banchs and Costa-jussà (2011), both of which inspired our document similarity feature. Also related is the work of Sennrich (2012a) who explore mixturemodelling on unsupervised clusters for domain adaptation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of multilingual topic models outside the field of MT include BoydGraber and Blei (2009; Boyd-Graber and Resnik (2010) who learn cross-lingual topic correspondences (but do not learn conditional distributions like our model does). In terms of model structure, our model is similar to BiTAM (Zhao and Xing, 2006) which is an LDA-style model to learn topicbased word alignments. The work of Carpuat and Wu (2007) is sim</context>
</contexts>
<marker>Chen, Kuhn, Foster, 2013</marker>
<rawString>Boxing Chen, Roland Kuhn, and George Foster. 2013. Vector space model for adaptation in SMT. In Proceedings of ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Topic models for dynamic translation model adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3281" citStr="Eidelman et al., 2012" startWordPosition="521" endWordPosition="524">er translations within the same data set. If these distributions overlap, then we expect topic adaptation to help separate them and yield better translations than an unadapted system. Topics can be of varying granularity and are therefore a flexible means to structure data that is not uniform enough to be modelled in its entirety. In recent years there have been several attempts to integrating topical information into SMT either by learning better word alignments (Zhao and Xing, 2006), by adapting translation features cross-domain (Su et al., 2012), or by dynamically adapting lexical weights (Eidelman et al., 2012) or adding sparse topic features (Hasler et al., 2012). We take a new approach to topic adaptation by estimating probabilistic phrase translation features in a completely Bayesian fashion. The motivation is that automatically identifying topics in the training data can help to select the appropriate translation of a source phrase in the context of a document. By adapting a system to automatically induced topics we do not have to trust data from a given domain to be uniform. We also overcome the problem of defining the level of granularity for domain adaptation. With more and more training data</context>
<context position="14340" citStr="Eidelman et al. (2012)" startWordPosition="2341" endWordPosition="2344">k,s,. ]+ γ) (Eˆq[n−(d,i) .,k,. ] + S · γ) 330 do not have variational posteriors for a given pair of source and target phrases, an approximation is needed. We omit the results here since our experiments so far did not indicate improvements with the inverse features included. 4 More topic-adapted features Inspired by previous work on topic adaptation for SMT, we add three additional topic-adapted features to our model. All of these features make use of the topic mixtures learned by our bilingual topic model. The first feature is an adapted lexical weight, similar to the features in the work of Eidelman et al. (2012). Our feature is different in that we marginalise over topics to produce a single adapted feature where v[k] is the kth element of a document topic vector for document d and w(t|s,k) is a topic-dependent word translation probability: lex(¯t|¯s,d) = 1 {j|(i, j) ∈ a} E Ew(t|s,k)·v[k] ∀(i,j)∈a k |{z } w(t|s) The second feature is a target unigram feature similar to the lazy MDI adaptation of Ruiz and Federico (2012). It includes an additional term that measures the relevance of a target word wi by comparing its document-specific probability Pdoc to its probability under the asymmetric topic 0: · </context>
<context position="35627" citStr="Eidelman et al. (2012)" startWordPosition="5955" endWordPosition="5958">ork There has been a lot of previous work using topic information for SMT, most of it using monolingual topic models. For example, Gong and Zhou (2011) use the topical relevance of a target phrase, computed using a mapping between source and target side topics, as an additional feature in decoding. Axelrod et al. (2012) build topic-specific translation models from the TED corpus and select topic-relevant data from the UN corpus to improve coverage. Su et al. (2012) perform phrase table adaptation in a setting where only monolingual in-domain data and parallel out-of-domain data are available. Eidelman et al. (2012) use topic-dependent lexical weights as features in the translation model, which is similar to our work in that topic features are tuned towards usefulness of topic information and not towards a target domain. Hewavitharana et al. (2013) perform dynamic adaptation with monolingual topics, encoding topic similarity between a conversation and training documents in an additional feature. This is similar to the work of Banchs and Costa-jussà (2011), both of which inspired our document similarity feature. Also related is the work of Sennrich (2012a) who explore mixturemodelling on unsupervised clus</context>
</contexts>
<marker>Eidelman, Boyd-Graber, Resnik, 2012</marker>
<rawString>Vladimir Eidelman, Jordan Boyd-Graber, and Philip Resnik. 2012. Topic models for dynamic translation model adaptation. In Proceedings of ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>R Kuhn</author>
</authors>
<title>Mixture-model adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of WMT. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1264" citStr="Foster and Kuhn, 2007" startWordPosition="188" endWordPosition="191">gin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU over the baseline and 1.04 over a domain adaptation benchmark. We further provide an analysis of the domain-specific data and show additive gains of our model in combination with other types of topic-adapted features. 1 Introduction In statistical machine translation (SMT), there has been a lot of interest in trying to incorporate information about the provenance of training examples in order to improve translations for specific target domains. A popular approach are mixture models (Foster and Kuhn, 2007) where each component contains data from a specific genre or domain. Mixture models can be trained for crossdomain adaption when the target domain is known or for dynamic adaptation when the target domain is inferred from the source text under translation. More recent domain adaptation methods employ corpus or instance weights to promote relevant training examples (Matsoukas et al., 2009; Foster et al., 2010) or do more radical data selection based on language model perplexity (Axelrod et al., 2011). In this work, we are interested in the dynamic adaptation case, which is challenging because w</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>G. Foster and R. Kuhn. 2007. Mixture-model adaptation for SMT. In Proceedings of WMT. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>C Goutte</author>
<author>R Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in SMT.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1676" citStr="Foster et al., 2010" startWordPosition="255" endWordPosition="259">erest in trying to incorporate information about the provenance of training examples in order to improve translations for specific target domains. A popular approach are mixture models (Foster and Kuhn, 2007) where each component contains data from a specific genre or domain. Mixture models can be trained for crossdomain adaption when the target domain is known or for dynamic adaptation when the target domain is inferred from the source text under translation. More recent domain adaptation methods employ corpus or instance weights to promote relevant training examples (Matsoukas et al., 2009; Foster et al., 2010) or do more radical data selection based on language model perplexity (Axelrod et al., 2011). In this work, we are interested in the dynamic adaptation case, which is challenging because we cannot tune our model towards any specific domain. In previous literature, domains have often been loosely defined in terms of corpora, for example, news texts would be defined as belonging to the news domain, ignoring the specific content of news documents. It is often assumed that the data within a domain is homogeneous in terms of style and vocabulary, though that is not always true in practice. The term</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>G. Foster, C. Goutte, and R. Kuhn. 2010. Discriminative instance weighting for domain adaptation in SMT. In Proceedings of EMNLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengxian Gong</author>
<author>Guodong Zhou</author>
</authors>
<title>Employing topic modeling for SMT.</title>
<date>2011</date>
<booktitle>In Proceedings of IEEE (CSAE),</booktitle>
<volume>4</volume>
<contexts>
<context position="35156" citStr="Gong and Zhou (2011)" startWordPosition="5878" endWordPosition="5881">in brackets show that the chosen translations were indeed the most likely under the respective adapted model. While the ALL model has a flat distribution over possible translations, the adapted models are peaked towards the correct translation. This shows that topic-specific translation probabilities are necessary when the translation of a word shifts between topics or domains and that peaked, adapted distributions can lead to more correct translations. 7 Related work There has been a lot of previous work using topic information for SMT, most of it using monolingual topic models. For example, Gong and Zhou (2011) use the topical relevance of a target phrase, computed using a mapping between source and target side topics, as an additional feature in decoding. Axelrod et al. (2012) build topic-specific translation models from the TED corpus and select topic-relevant data from the UN corpus to improve coverage. Su et al. (2012) perform phrase table adaptation in a setting where only monolingual in-domain data and parallel out-of-domain data are available. Eidelman et al. (2012) use topic-dependent lexical weights as features in the translation model, which is similar to our work in that topic features ar</context>
</contexts>
<marker>Gong, Zhou, 2011</marker>
<rawString>Zhengxian Gong and Guodong Zhou. 2011. Employing topic modeling for SMT. In Proceedings of IEEE (CSAE), volume 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hasler</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Sparse lexicalised features and topic adaptation for SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of IWSLT.</booktitle>
<contexts>
<context position="3335" citStr="Hasler et al., 2012" startWordPosition="530" endWordPosition="533">ibutions overlap, then we expect topic adaptation to help separate them and yield better translations than an unadapted system. Topics can be of varying granularity and are therefore a flexible means to structure data that is not uniform enough to be modelled in its entirety. In recent years there have been several attempts to integrating topical information into SMT either by learning better word alignments (Zhao and Xing, 2006), by adapting translation features cross-domain (Su et al., 2012), or by dynamically adapting lexical weights (Eidelman et al., 2012) or adding sparse topic features (Hasler et al., 2012). We take a new approach to topic adaptation by estimating probabilistic phrase translation features in a completely Bayesian fashion. The motivation is that automatically identifying topics in the training data can help to select the appropriate translation of a source phrase in the context of a document. By adapting a system to automatically induced topics we do not have to trust data from a given domain to be uniform. We also overcome the problem of defining the level of granularity for domain adaptation. With more and more training data automatically extracted from the web and little knowl</context>
</contexts>
<marker>Hasler, Haddow, Koehn, 2012</marker>
<rawString>Eva Hasler, Barry Haddow, and Philipp Koehn. 2012. Sparse lexicalised features and topic adaptation for SMT. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hewavitharana</author>
<author>D Mehay</author>
<author>S Ananthakrishnan</author>
<author>P Natarajan</author>
</authors>
<title>Incremental topic-based TM adaptation for conversational SLT.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="35864" citStr="Hewavitharana et al. (2013)" startWordPosition="5994" endWordPosition="5997">ource and target side topics, as an additional feature in decoding. Axelrod et al. (2012) build topic-specific translation models from the TED corpus and select topic-relevant data from the UN corpus to improve coverage. Su et al. (2012) perform phrase table adaptation in a setting where only monolingual in-domain data and parallel out-of-domain data are available. Eidelman et al. (2012) use topic-dependent lexical weights as features in the translation model, which is similar to our work in that topic features are tuned towards usefulness of topic information and not towards a target domain. Hewavitharana et al. (2013) perform dynamic adaptation with monolingual topics, encoding topic similarity between a conversation and training documents in an additional feature. This is similar to the work of Banchs and Costa-jussà (2011), both of which inspired our document similarity feature. Also related is the work of Sennrich (2012a) who explore mixturemodelling on unsupervised clusters for domain adaptation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of m</context>
</contexts>
<marker>Hewavitharana, Mehay, Ananthakrishnan, Natarajan, 2013</marker>
<rawString>S. Hewavitharana, D. Mehay, S. Ananthakrishnan, and P. Natarajan. 2013. Incremental topic-based TM adaptation for conversational SLT. In Proceedings ofACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="17835" citStr="Hopkins and May, 2011" startWordPosition="2927" endWordPosition="2930">containing data from all three domains and train one language model on the concatenation of (equally sized) target sides of the training data. Word alignments are trained on the concatenation of all training data and fixed for all models. Our baseline (ALL) is a phrase-based FrenchEnglish system trained on the concatenation of all parallel data. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5gram language model. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute caseinsensitive BLEU. As domain-aware benchmark systems, we use the phrase table fill-up method (FILLUP) of Bisazza et al. (2011) which preserves the translation scores of phrases from the IN model and the linear mixture models (LINTM) of Sennrich (2012b) (both available in the Moses toolkit). For both systems, we build separate phrase tables for each domain and use a wrapper to decode tuning and test sets with domainspecific tables. Both benchmarks have an advan|t| ∏ i (3) trgUnigramst = |t |f( Pdoc(wi) ) ∏ Pbaseline(wi) i=1 |{z } lazy MDI 2 f (x)</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of EMNLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for SMT.</title>
<date>2007</date>
<booktitle>In ACL</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="8552" citStr="Koehn et al., 2007" startWordPosition="1387" endWordPosition="1390">, 2006; Wang and Blunsom, 2013), with little loss in accuracy. Because we have to do inference over a large number of phrase pairs, CVB is more practical than Gibbs sampling. 2.2 Overview of training strategy Ultimately, we want to learn translation probabilities for all possible phrase pairs that apply to a given test document during decoding. Therefore, topic modelling operates on phrase pairs as they will be seen during decoding. Given word-aligned parallel corpora from several domains, we extract lists of per-document phrase pairs produced by the extraction algorithm in the Moses toolkit (Koehn et al., 2007) which contain all phrase pairs consistent with the word alignment. We run CVB on the set of all training documents to learn latent topics without providing information about the domains. 1Parallel documents are modelled as bags of phrase pairs. 329 Using the trained model, CVB with modified inference is run on all test documents with the set of possible phrase translations that a decoder would load from a phrase table before decoding. When test inference has finished, we compute adapted translation probabilities at the document-level by marginalising over topics for each phrase pair. 3 Biling</context>
<context position="17617" citStr="Koehn et al., 2007" startWordPosition="2891" endWordPosition="2894">n adaptation approaches, we chose a setup with data from different corpora. We want to abstract away from adaptation effects that concern tuning of length penalties and language models, so we use a mixed tuning set containing data from all three domains and train one language model on the concatenation of (equally sized) target sides of the training data. Word alignments are trained on the concatenation of all training data and fixed for all models. Our baseline (ALL) is a phrase-based FrenchEnglish system trained on the concatenation of all parallel data. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5gram language model. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute caseinsensitive BLEU. As domain-aware benchmark systems, we use the phrase table fill-up method (FILLUP) of Bisazza et al. (2011) which preserves the translation scores of phrases from the IN model and the linear mixture models (LINTM) of Sennrich (2012b) (both available in the Moses toolkit). For both systems, we build separate phras</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for SMT. In ACL 2007: Demo and poster sessions. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27436" citStr="Koehn, 2004" startWordPosition="4586" endWordPosition="4587">The botton row of the table indicates the relative improvement of the best topic-adapted model per domain over the ALL model. Using all four topic-adapted features yields an improvement of 0.81 BLEU on the mixed test set. The highest improvement on a given domain is achieved for TED with an increase of 1.26 BLEU. The smallest improvement is measured on the NC domain. This is in line with the observation that distributions in the NC in-domain table are most similar to the ALL table, therefore we would expect the smallest improvement for domain or topic adaptation. We used bootstrap resampling (Koehn, 2004) to measure significance on the mixed test set and marked all statistically significant results compared to the respective baselines with asterisk (*: p :5 0.01). To demonstrate the benefit of topic adaptation over more standard domain adaptation approaches for a diverse data set, we show the performance 333 Model Mixed CC NC TED FILLUP 27.12 19.36 29.78 32.71 LIN-TM 27.24 19.61 29.87 32.73 pLDA *27.67 20.40 30.04 33.08 &gt;FILLUP +0.55 +1.04 +0.26 +0.37 &gt;LIN-TM +0.43 +0.79 +0.17 +0.35 Table 7: Comparison of best pLDA system with two domain-aware benchmark systems. Model Mixed CC NC TED LIN-LM + </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Matsoukas</author>
<author>A Rosti</author>
<author>B Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for MT.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1654" citStr="Matsoukas et al., 2009" startWordPosition="251" endWordPosition="254">re has been a lot of interest in trying to incorporate information about the provenance of training examples in order to improve translations for specific target domains. A popular approach are mixture models (Foster and Kuhn, 2007) where each component contains data from a specific genre or domain. Mixture models can be trained for crossdomain adaption when the target domain is known or for dynamic adaptation when the target domain is inferred from the source text under translation. More recent domain adaptation methods employ corpus or instance weights to promote relevant training examples (Matsoukas et al., 2009; Foster et al., 2010) or do more radical data selection based on language model perplexity (Axelrod et al., 2011). In this work, we are interested in the dynamic adaptation case, which is challenging because we cannot tune our model towards any specific domain. In previous literature, domains have often been loosely defined in terms of corpora, for example, news texts would be defined as belonging to the news domain, ignoring the specific content of news documents. It is often assumed that the data within a domain is homogeneous in terms of style and vocabulary, though that is not always true</context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>S. Matsoukas, A. Rosti, and B. Zhang. 2009. Discriminative corpus weight estimation for MT. In Proceedings of EMNLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas P Minka</author>
</authors>
<title>Estimating a Dirichlet distribution.</title>
<date>2012</date>
<tech>Technical report.</tech>
<contexts>
<context position="9738" citStr="Minka, 2012" startWordPosition="1583" endWordPosition="1584">each phrase pair. 3 Bilingual topic inference 3.1 Inference on training documents The aim of inference on the training data is to find latent topics in the distributions over phrase pairs in each document.This is done by repeatedly visiting all phrase pair positions in all documents, computing conditional topic probabilities and updating counts. To bias the model to cluster stop word phrases in one topic, we place an asymmetric prior over the hyperparameters2 as described in (Wallach et al., 2009) to make one of the topics a priori more probable in every document. We use a fixed-point update (Minka, 2012) to update the hyperparameters after every iteration. For CVB the conditional probability of topic zd,i given the current state of all variables except zd,i is P(zd,i = k|z−(d,i),s,t,d,α,β,γ) ∝ (Eˆq[n−(d,i) .,k,s,t ] + β) (Eˆq[n−(d,i) .,k,s,. ] + Ts · β) ·(Eˆq[n−(d,i) d,k,. ] + α) (1) where s and t are all source and target phrases in the collection. n−(d,i) −(d,i) .,k,s,t , n.,k,s,. and n−(d,i) d,k,. are cooccurrence counts of topics with phrase pairs, source phrases and documents respectively. Eˆq is the expectation under the variational posterior and in comparison to Gibbs sampling where th</context>
</contexts>
<marker>Minka, 2012</marker>
<rawString>Thomas P Minka. 2012. Estimating a Dirichlet distribution. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Ruiz</author>
<author>Marcello Federico</author>
</authors>
<title>MDI Adaptation for the Lazy: Avoiding Normalization in LM Adaptation for Lecture Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of IWSLT.</booktitle>
<contexts>
<context position="14756" citStr="Ruiz and Federico (2012)" startWordPosition="2412" endWordPosition="2415"> model. All of these features make use of the topic mixtures learned by our bilingual topic model. The first feature is an adapted lexical weight, similar to the features in the work of Eidelman et al. (2012). Our feature is different in that we marginalise over topics to produce a single adapted feature where v[k] is the kth element of a document topic vector for document d and w(t|s,k) is a topic-dependent word translation probability: lex(¯t|¯s,d) = 1 {j|(i, j) ∈ a} E Ew(t|s,k)·v[k] ∀(i,j)∈a k |{z } w(t|s) The second feature is a target unigram feature similar to the lazy MDI adaptation of Ruiz and Federico (2012). It includes an additional term that measures the relevance of a target word wi by comparing its document-specific probability Pdoc to its probability under the asymmetric topic 0: · f( Pdoc(wi) Ptopic0(wi)) |{z } relevance (4) , x &gt; 0 (5) The third feature is a document similarity feature, similar to the semantic feature described by Banchs and Costa-jussà (2011): docSimt = max(1 −JSD(vtrain doci,vtest doc)) (6) i where vtrain_doci and vtest_doc are document topic vector of training and test documents. Because topic 0 captures phrase pairs that are common to many documents, we exclude it fro</context>
</contexts>
<marker>Ruiz, Federico, 2012</marker>
<rawString>Nick Ruiz and Marcello Federico. 2012. MDI Adaptation for the Lazy: Avoiding Normalization in LM Adaptation for Lecture Translation. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="19910" citStr="Schmid, 1994" startWordPosition="3301" endWordPosition="3302">sults on the test set are broken down by domain but also reported for the entire test set (mixed). For Ted and NC, the in-domain models perform better than ALL, while for CC the all-domain model improves quite significantly over IN. 5.2 General properties of the data sets In this section we analyse some internal properties of our three data sets that are relevant for adaptation. All of the scores were computed on the sets of source side tokens of the test set which were limited to contain content words (nouns, verbs, adjectives and adverbs). The test set was tagged with the French TreeTagger (Schmid, 1994). The top of Table 3 shows the average Jensen-Shannon divergence (using log2, JSD E [0,1]) of each in-domain model in comparison to the all-domain model, which is an indicator of how much the distributions in the IN model change when adding out-ofdomain data. Likewise, Rank1-diff gives the percentage of word tokens in the test set where the preferred translation according to p(e|f) changes between IN and ALL. These are the words that are most affected by adding data to the IN model. Both numbers show that for Commoncrawl the IN and ALL models differ more than in the other two data sets. Accord</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Mixture-modeling with unsupervised clusters for domain adaptation in SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of EAMT.</booktitle>
<contexts>
<context position="18134" citStr="Sennrich (2012" startWordPosition="2978" endWordPosition="2979"> the concatenation of all parallel data. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5gram language model. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute caseinsensitive BLEU. As domain-aware benchmark systems, we use the phrase table fill-up method (FILLUP) of Bisazza et al. (2011) which preserves the translation scores of phrases from the IN model and the linear mixture models (LINTM) of Sennrich (2012b) (both available in the Moses toolkit). For both systems, we build separate phrase tables for each domain and use a wrapper to decode tuning and test sets with domainspecific tables. Both benchmarks have an advan|t| ∏ i (3) trgUnigramst = |t |f( Pdoc(wi) ) ∏ Pbaseline(wi) i=1 |{z } lazy MDI 2 f (x) = 1 + 1 x 331 Model Mixed CC NC TED IN 26.77 18.76 29.56 32.47 ALL 26.86 19.61 29.42 31.88 Table 2: BLEU of in-domain and baseline models. Model Avg JSD Ted-half vs Ted-full 0.07 CC-half vs CC-full 0.17 NC-half vs NC-full 0.09 Model Avg JSD Rank1-diff Ted-IN vs ALL 0.15 10.8% CC-IN vs ALL 0.17 18.</context>
<context position="36175" citStr="Sennrich (2012" startWordPosition="6046" endWordPosition="6047"> parallel out-of-domain data are available. Eidelman et al. (2012) use topic-dependent lexical weights as features in the translation model, which is similar to our work in that topic features are tuned towards usefulness of topic information and not towards a target domain. Hewavitharana et al. (2013) perform dynamic adaptation with monolingual topics, encoding topic similarity between a conversation and training documents in an additional feature. This is similar to the work of Banchs and Costa-jussà (2011), both of which inspired our document similarity feature. Also related is the work of Sennrich (2012a) who explore mixturemodelling on unsupervised clusters for domain adaptation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of multilingual topic models outside the field of MT include BoydGraber and Blei (2009; Boyd-Graber and Resnik (2010) who learn cross-lingual topic correspondences (but do not learn conditional distributions like our model does). In terms of model structure, our model is similar to BiTAM (Zhao and Xing, 2006) whic</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012a. Mixture-modeling with unsupervised clusters for domain adaptation in SMT. In Proceedings of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Perplexity Minimization for Translation Model Domain Adaptation in SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18134" citStr="Sennrich (2012" startWordPosition="2978" endWordPosition="2979"> the concatenation of all parallel data. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5gram language model. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute caseinsensitive BLEU. As domain-aware benchmark systems, we use the phrase table fill-up method (FILLUP) of Bisazza et al. (2011) which preserves the translation scores of phrases from the IN model and the linear mixture models (LINTM) of Sennrich (2012b) (both available in the Moses toolkit). For both systems, we build separate phrase tables for each domain and use a wrapper to decode tuning and test sets with domainspecific tables. Both benchmarks have an advan|t| ∏ i (3) trgUnigramst = |t |f( Pdoc(wi) ) ∏ Pbaseline(wi) i=1 |{z } lazy MDI 2 f (x) = 1 + 1 x 331 Model Mixed CC NC TED IN 26.77 18.76 29.56 32.47 ALL 26.86 19.61 29.42 31.88 Table 2: BLEU of in-domain and baseline models. Model Avg JSD Ted-half vs Ted-full 0.07 CC-half vs CC-full 0.17 NC-half vs NC-full 0.09 Model Avg JSD Rank1-diff Ted-IN vs ALL 0.15 10.8% CC-IN vs ALL 0.17 18.</context>
<context position="36175" citStr="Sennrich (2012" startWordPosition="6046" endWordPosition="6047"> parallel out-of-domain data are available. Eidelman et al. (2012) use topic-dependent lexical weights as features in the translation model, which is similar to our work in that topic features are tuned towards usefulness of topic information and not towards a target domain. Hewavitharana et al. (2013) perform dynamic adaptation with monolingual topics, encoding topic similarity between a conversation and training documents in an additional feature. This is similar to the work of Banchs and Costa-jussà (2011), both of which inspired our document similarity feature. Also related is the work of Sennrich (2012a) who explore mixturemodelling on unsupervised clusters for domain adaptation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of multilingual topic models outside the field of MT include BoydGraber and Blei (2009; Boyd-Graber and Resnik (2010) who learn cross-lingual topic correspondences (but do not learn conditional distributions like our model does). In terms of model structure, our model is similar to BiTAM (Zhao and Xing, 2006) whic</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012b. Perplexity Minimization for Translation Model Domain Adaptation in SMT. In Proceedings of EACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Su</author>
<author>H Wu</author>
<author>H Wang</author>
<author>Y Chen</author>
<author>X Shi</author>
<author>H Dong</author>
<author>Q Liu</author>
</authors>
<title>Translation model adaptation for SMT with monolingual topic information.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3213" citStr="Su et al., 2012" startWordPosition="511" endWordPosition="514">mplicit assumption that there can be multiple distributions over translations within the same data set. If these distributions overlap, then we expect topic adaptation to help separate them and yield better translations than an unadapted system. Topics can be of varying granularity and are therefore a flexible means to structure data that is not uniform enough to be modelled in its entirety. In recent years there have been several attempts to integrating topical information into SMT either by learning better word alignments (Zhao and Xing, 2006), by adapting translation features cross-domain (Su et al., 2012), or by dynamically adapting lexical weights (Eidelman et al., 2012) or adding sparse topic features (Hasler et al., 2012). We take a new approach to topic adaptation by estimating probabilistic phrase translation features in a completely Bayesian fashion. The motivation is that automatically identifying topics in the training data can help to select the appropriate translation of a source phrase in the context of a document. By adapting a system to automatically induced topics we do not have to trust data from a given domain to be uniform. We also overcome the problem of defining the level of</context>
<context position="35474" citStr="Su et al. (2012)" startWordPosition="5932" endWordPosition="5935">he translation of a word shifts between topics or domains and that peaked, adapted distributions can lead to more correct translations. 7 Related work There has been a lot of previous work using topic information for SMT, most of it using monolingual topic models. For example, Gong and Zhou (2011) use the topical relevance of a target phrase, computed using a mapping between source and target side topics, as an additional feature in decoding. Axelrod et al. (2012) build topic-specific translation models from the TED corpus and select topic-relevant data from the UN corpus to improve coverage. Su et al. (2012) perform phrase table adaptation in a setting where only monolingual in-domain data and parallel out-of-domain data are available. Eidelman et al. (2012) use topic-dependent lexical weights as features in the translation model, which is similar to our work in that topic features are tuned towards usefulness of topic information and not towards a target domain. Hewavitharana et al. (2013) perform dynamic adaptation with monolingual topics, encoding topic similarity between a conversation and training documents in an additional feature. This is similar to the work of Banchs and Costa-jussà (2011</context>
</contexts>
<marker>Su, Wu, Wang, Chen, Shi, Dong, Liu, 2012</marker>
<rawString>J. Su, H. Wu, H. Wang, Y. Chen, X. Shi, H. Dong, and Q. Liu. 2012. Translation model adaptation for SMT with monolingual topic information. In Proceedings ofACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>David Newman</author>
<author>Max Welling</author>
</authors>
<title>A collapsed variational Bayesian inference algorithm for LDA.</title>
<date>2006</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="7812" citStr="Teh et al., 2006" startWordPosition="1266" endWordPosition="1269"> of phrases pairs Pd in the document, Pd — Poisson(�). 3. For every position di in the document corresponding to a phrase pair pd,i of source and target phrase si and ti1: (a) Choose a topic zd,i — Multinomial(9d). (b) Conditioned on topic zd,i, choose a source phrase sd,i — Multinomial(tVzd,i). (c) Conditioned on zd,i and sd,i, choose target phrase td,i — Multinomial(�sd,i,zd,i). a, R and y are parameters of the Dirichlet distributions, which are asymmetric for k = 0. Our inference algorithm is an implementation of collapsed variational Bayes (CVB), with a first-order Gaussian approximation (Teh et al., 2006). It has been shown to be more accurate than standard VB and to converge faster than collapsed Gibbs sampling (Teh et al., 2006; Wang and Blunsom, 2013), with little loss in accuracy. Because we have to do inference over a large number of phrase pairs, CVB is more practical than Gibbs sampling. 2.2 Overview of training strategy Ultimately, we want to learn translation probabilities for all possible phrase pairs that apply to a given test document during decoding. Therefore, topic modelling operates on phrase pairs as they will be seen during decoding. Given word-aligned parallel corpora from s</context>
</contexts>
<marker>Teh, Newman, Welling, 2006</marker>
<rawString>Yee Whye Teh, David Newman, and Max Welling. 2006. A collapsed variational Bayesian inference algorithm for LDA. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
<author>David M Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Rethinking LDA: Why priors matter.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="9628" citStr="Wallach et al., 2009" startWordPosition="1561" endWordPosition="1564">ence has finished, we compute adapted translation probabilities at the document-level by marginalising over topics for each phrase pair. 3 Bilingual topic inference 3.1 Inference on training documents The aim of inference on the training data is to find latent topics in the distributions over phrase pairs in each document.This is done by repeatedly visiting all phrase pair positions in all documents, computing conditional topic probabilities and updating counts. To bias the model to cluster stop word phrases in one topic, we place an asymmetric prior over the hyperparameters2 as described in (Wallach et al., 2009) to make one of the topics a priori more probable in every document. We use a fixed-point update (Minka, 2012) to update the hyperparameters after every iteration. For CVB the conditional probability of topic zd,i given the current state of all variables except zd,i is P(zd,i = k|z−(d,i),s,t,d,α,β,γ) ∝ (Eˆq[n−(d,i) .,k,s,t ] + β) (Eˆq[n−(d,i) .,k,s,. ] + Ts · β) ·(Eˆq[n−(d,i) d,k,. ] + α) (1) where s and t are all source and target phrases in the collection. n−(d,i) −(d,i) .,k,s,t , n.,k,s,. and n−(d,i) d,k,. are cooccurrence counts of topics with phrase pairs, source phrases and documents res</context>
</contexts>
<marker>Wallach, Mimno, McCallum, 2009</marker>
<rawString>Hanna M. Wallach, David M. Mimno, and Andrew McCallum. 2009. Rethinking LDA: Why priors matter. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pengyu Wang</author>
<author>Phil Blunsom</author>
</authors>
<title>Collapsed variational Bayesian inference for Hidden Markov Models.</title>
<date>2013</date>
<booktitle>In AISTATS, volume 31 of JMLR Proceedings,</booktitle>
<pages>599--607</pages>
<contexts>
<context position="7964" citStr="Wang and Blunsom, 2013" startWordPosition="1294" endWordPosition="1297">target phrase si and ti1: (a) Choose a topic zd,i — Multinomial(9d). (b) Conditioned on topic zd,i, choose a source phrase sd,i — Multinomial(tVzd,i). (c) Conditioned on zd,i and sd,i, choose target phrase td,i — Multinomial(�sd,i,zd,i). a, R and y are parameters of the Dirichlet distributions, which are asymmetric for k = 0. Our inference algorithm is an implementation of collapsed variational Bayes (CVB), with a first-order Gaussian approximation (Teh et al., 2006). It has been shown to be more accurate than standard VB and to converge faster than collapsed Gibbs sampling (Teh et al., 2006; Wang and Blunsom, 2013), with little loss in accuracy. Because we have to do inference over a large number of phrase pairs, CVB is more practical than Gibbs sampling. 2.2 Overview of training strategy Ultimately, we want to learn translation probabilities for all possible phrase pairs that apply to a given test document during decoding. Therefore, topic modelling operates on phrase pairs as they will be seen during decoding. Given word-aligned parallel corpora from several domains, we extract lists of per-document phrase pairs produced by the extraction algorithm in the Moses toolkit (Koehn et al., 2007) which conta</context>
</contexts>
<marker>Wang, Blunsom, 2013</marker>
<rawString>Pengyu Wang and Phil Blunsom. 2013. Collapsed variational Bayesian inference for Hidden Markov Models. In AISTATS, volume 31 of JMLR Proceedings, pages 599–607.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
</authors>
<title>Bilingual topic admixture models for word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3148" citStr="Zhao and Xing, 2006" startWordPosition="502" endWordPosition="505">We view topic adaptation as fine-grained domain adaptation with the implicit assumption that there can be multiple distributions over translations within the same data set. If these distributions overlap, then we expect topic adaptation to help separate them and yield better translations than an unadapted system. Topics can be of varying granularity and are therefore a flexible means to structure data that is not uniform enough to be modelled in its entirety. In recent years there have been several attempts to integrating topical information into SMT either by learning better word alignments (Zhao and Xing, 2006), by adapting translation features cross-domain (Su et al., 2012), or by dynamically adapting lexical weights (Eidelman et al., 2012) or adding sparse topic features (Hasler et al., 2012). We take a new approach to topic adaptation by estimating probabilistic phrase translation features in a completely Bayesian fashion. The motivation is that automatically identifying topics in the training data can help to select the appropriate translation of a source phrase in the context of a document. By adapting a system to automatically induced topics we do not have to trust data from a given domain to </context>
<context position="36770" citStr="Zhao and Xing, 2006" startWordPosition="6136" endWordPosition="6139">he work of Sennrich (2012a) who explore mixturemodelling on unsupervised clusters for domain adaptation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of multilingual topic models outside the field of MT include BoydGraber and Blei (2009; Boyd-Graber and Resnik (2010) who learn cross-lingual topic correspondences (but do not learn conditional distributions like our model does). In terms of model structure, our model is similar to BiTAM (Zhao and Xing, 2006) which is an LDA-style model to learn topicbased word alignments. The work of Carpuat and Wu (2007) is similar to ours in spirit, but they predict the most probable translation in a context at the token level while our adaptation operates at the type level of a document. 8 Conclusion We have presented a novel bilingual topic model based on LDA and applied it to the task of translation model adaptation on a diverse French-English data set. Our model infers topic distributions over phrase pairs to compute document-specific translation probabilities and performs dynamic adaptation on test documen</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>Bing Zhao and Eric P. Xing. 2006. Bilingual topic admixture models for word alignment. In Proceedings of ACL. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>