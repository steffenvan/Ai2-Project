<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.9964085">
Semantic Dependency Parsing of NomBank and PropBank:
An Efficient Integrated Approach via a Large-scale Feature Selection �
</title>
<author confidence="0.999566">
Hai Zhao( )†, Wenliang Chen({, _JCA)*, Chunyu Kit†(4941 )
</author>
<affiliation confidence="0.9822212">
†Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Avenue, Kowloon, Hong Kong, China
*Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.948797">
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
</address>
<email confidence="0.998248">
haizhao@cityu.edu.hk, chenwl@nict.go.jp
</email>
<sectionHeader confidence="0.993863" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999779375">
We present an integrated dependency-
based semantic role labeling system for
English from both NomBank and Prop-
Bank. By introducing assistant argument
labels and considering much more fea-
ture templates, two optimal feature tem-
plate sets are obtained through an effec-
tive feature selection procedure and help
construct a high performance single SRL
system. From the evaluations on the date
set of CoNLL-2008 shared task, the per-
formance of our system is quite close to
the state of the art. As to our knowl-
edge, this is the first integrated SRL sys-
tem that achieves a competitive perfor-
mance against previous pipeline systems.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978466666667">
We investigate the possibility to construct an effec-
tive integrated system for dependency-based se-
mantic role labeling (SRL) task. This means in
this work that a single system handles all these
sub-tasks, predicate identification/disambiguation
and argument identification/classification, regard-
less of whether the predicate is verbal or nominal.
Traditionally, a SRL task, either dependency
or constituent based, is implemented as two sub-
tasks, namely, argument identification and clas-
sification. If the predicate is unknown, then a
predicate identification or disambiguation subtask
should be additionally considered. A pipeline
framework is usually adopted to handle all these
sub-tasks. The reason to divide the whole task
</bodyText>
<footnote confidence="0.721502666666667">
This study is partially supported by CERG grant
9040861 (CityU 1318/03H), CityU Strategic Research Grant
7002037.
</footnote>
<bodyText confidence="0.9999115">
into multiple stages is two-fold, one is each sub-
task asks for its favorable features, the other is
at the consideration of computational efficiency.
Generally speaking, a joint system is slower than
a pipeline system in training. (Xue and Palmer,
2004) fount out that different features suited for
different sub-tasks of SRL, i.e. argument identifi-
cation and classification. The results from CoNLL
shared tasks in 2005 and 2008 (Carreras and Mar-
quez, 2005; Koomen et al., 2005; Surdeanu et al.,
2008; Johansson and Nugues, 2008), further show
that SRL pipeline may be one of the standard to
achieve a state-of-the-art performance in practice.
In the recent years, most works on SRL, includ-
ing two CoNLL shared task in 2004 and 2005,
focus on verbal predicates with the availability
of PropBank (Palmer et al., 2005). As a com-
plement to PropBank, NomBank (Meyers et al.,
2004) annotates nominal predicates and their cor-
responding semantic roles using similar semantic
framework as PropBank. Though SRL for nomi-
nal predicates offers more challenge, it draws rel-
atively little attention (Jiang and Ng, 2006).
(Pustejovsky et al., 2005) discussed the issue of
merging various treebanks, including PropBank,
NomBank, and others. The idea of merging these
two different treebanks was implemented in the
CoNLL-2008 shared task (Surdeanu et al., 2008).
However, few empirical studies support the ne-
cessity of an integrated learning strategy from
NomBank and PropBank. Though aiming at Chi-
nese SRL, (Xue, 2006) reported that their exper-
iments show that simply adding the verb data to
the training set of NomBank and extracting the
same features from the verb and noun instances
will hurt the overall performance. From the re-
sults of CoNLL-2008 shared task, the top system
by (Johansson and Nugues, 2008) also used two
</bodyText>
<page confidence="0.97875">
30
</page>
<note confidence="0.9966065">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 30–39,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999671828571429">
different subsystems to handle verbal and nominal
predicates, respectively.
Despite all the above facts, an integrated SRL
system still holds some sort of merits, being eas-
ier to implement, a single-stage feature selection
benefiting the whole system, an all-in-one model
outputting all required semantic role information
and so on.
The shared tasks at the CoNLL 2008 and 2009
are devoted to the joint learning of syntactic and
semantic dependencies, which show that SRL can
be well performed using only dependency syn-
tax input. Using data and evaluation settings
of the CoNLL-2008 shared task, this work will
only focus on semantic dependency parsing and
compares the best-performing SRL system in the
CoNLL-2009 shared Task (Zhao et al., 2009b)
with those in the CoNLL-2008 shared task (Sur-
deanu et al., 2008; Hajiˇc et al., 2009)1.
Aiming at main drawbacks of an integrated ap-
proach, two key techniques will be applied. 1)
Assistant argument labels are introduced for the
further improvement of argument pruning. This
helps the development of a fast and lightweight
SRL system. 2) Using a greedy feature selec-
tion algorithm, a large-scale feature engineering is
performed on a much larger feature template set
than that in previous work. This helps us find fea-
tures that may be of benefit to all SRL sub-tasks as
long as possible. As two optimal feature template
sets have been proven available, for the first time
we report that an integrated SRL system may pro-
vide a result close to the state-of-the-art achieved
by those SRL pipelines or individual systems for
some specific predicates.
</bodyText>
<sectionHeader confidence="0.988495" genericHeader="method">
2 Adaptive Argument Pruning
</sectionHeader>
<bodyText confidence="0.999911875">
A word-pair classification is used to formulate se-
mantic dependency parsing as in (Zhao and Kit,
2008). As for predicate identification or disam-
biguation, the first word is set as a virtual root
(which is virtually set before the beginning of the
sentence.) and the second as a predicate candi-
date. As for argument identification/classification,
the first word in a word pair is specified as a predi-
</bodyText>
<footnote confidence="0.995970125">
1CoNLL-2008 is an English-only task, while CoNLL-
2009 is a multilingual one. Though the English corpus in
CoNLL-2009 is almost identical to the corpus in the CoNLL-
2008 shared task evaluation, the latter holds more sophisti-
cated input structure as in (Surdeanu et al., 2008). The most
difference for these two tasks is that the identification of se-
mantic predicates is required in the task of CoNLL-2008 but
not in CoNLL-2009.
</footnote>
<bodyText confidence="0.999738735294118">
cate candidate and the second as an argument can-
didate. In either of case, the first word is called a
semantic head, and noted as p in our feature rep-
resentation, the second is called a semantic depen-
dent and noted as a.
Word pairs are collected for the classifier in
such order. The first word of the pair is set to the
virtual root at first, the second word is then spec-
ified as a predicate candidate. According to the
result that the predicate candidate is classified or
proven to be non-predicate, 1) the second word is
reset to next predicate candidate if the answer is
non-predicate, otherwise, 2) the first word of the
pair is reset to the predicate that is just determined,
and the second is set to every argument candidates
one by one. The classifier will scan the input sen-
tence from left to right to check if each word is a
true predicate.
Without any constraint, all word pairs in an in-
put sequence must be considered by the classifier,
leading to poor computational efficiency and un-
necessary performance loss. Thus, the training
sample for SRL task needs to be pruned properly.
We use a simple strategy to prune predicate can-
didates, namely, only verbs and nouns are chosen
in this case.
There are two paths to collect argument candi-
dates over the sequence. One is based on an input
syntactic dependency tree, the other is based on
a linear path of the sentence. As for the former
(hereafter it is referred to synPth), we continue to
use a dependency version of the pruning algorithm
of (Xue and Palmer, 2004). The pruning algorithm
is readdressed as the following.
</bodyText>
<listItem confidence="0.939755">
Initialization: Set the given predicate as the
current node;
(1) The current node and all of its syntactic
children are selected as argument candidates
(children are traversed from left to right.).
(2) Reset the current node to its syntactic head
and repeat step (1) until the root is reached.
</listItem>
<bodyText confidence="0.997894">
Note that this pruning algorithm is slightly dif-
ferent from that of (Xue and Palmer, 2004), the
predicate itself is also included in the argument
candidate list as the nominal predicate sometimes
takes itself as its argument.
The above pruning algorithm has been shown
effective. However, it is still inefficient for a SRL
</bodyText>
<page confidence="0.999784">
31
</page>
<bodyText confidence="0.999557291666667">
system that needs to tackle argument identifica-
tion/classification in a single stage. Assuming that
arguments trend to surround their predicate, an as-
sistant argument label ‘ NoMoreArgument’ is in-
troduced for further pruning. If an argument can-
didate in the above algorithm is assigned to such
a label, then the pruning algorithm will end im-
mediately. In training, this assistant label means
no more samples will be generated for the current
predicate, while in test, the decoder will not search
arguments any more. It will be seen that this adap-
tive technique more effectively prunes argument
candidates without missing more true arguments.
Along the linear path (hereafter referred to
linPth), the classifier will search all words before
and after the predicate. Similar to the pruning
algorithm for synPth, we also introduce two as-
sistant argument labels ‘ noLeft’ and ‘ noRight’
to adaptively prune words too far away from the
predicate.
To show how assistant argument labels actually
work, we give an example for linPth. Suppose an
input sequence with argument labels for a predi-
cate is
</bodyText>
<equation confidence="0.9251775">
a b c d e f g h .
A1 A0
</equation>
<bodyText confidence="0.999756">
Note that c and g are two boundary words as no
more arguments appear before or after them. After
two assistant argument labels are added, it will be
</bodyText>
<equation confidence="0.780341">
a b c d e f g h .
noLeft A1 A0 noRight
</equation>
<bodyText confidence="0.999853166666667">
Training samples will generated from c to g ac-
cording to the above sequence.
We use a Maximum Entropy classifier with a
tunable Gaussian prior as usual. Our implemen-
tation of the model adopts L-BFGS algorithm for
parameter optimization.
</bodyText>
<sectionHeader confidence="0.99905" genericHeader="method">
3 Feature Templates
</sectionHeader>
<subsectionHeader confidence="0.988029">
3.1 Elements for Feature Generation
</subsectionHeader>
<bodyText confidence="0.999921769230769">
Motivated by previous works, we carefully con-
sider those factors from a wide range of features
that can help semantic role labeling for both predi-
cate disambiguation, argument’s identification and
classification as the predicate is either verbal or
nominal. These works include (Gildea and Juraf-
sky, 2002; Carreras and Marquez, 2005; Koomen
et al., 2005; Marquez et al., 2005; Dang and
Palmer, 2005; Pradhan et al., 2005; Toutanova et
al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007;
Surdeanu et al., 2007; Johansson and Nugues,
2008; Che et al., 2008). Most feature templates
that we will adopt for this work will come from
various combinations or integrations of the follow-
ing basic elements.
Word Property. This type of elements include
word form (form and its split form, spForm)2,
lemma (lemma,spLemma), and part-of-speech tag
(pos, spPos), syntactic dependency label (dprel),
and semantic dependency label (semdprel)3.
Syntactic Connection. This includes syn-
tactic head (h), left(right) farthest(nearest) child
(lm, ln, rm, and rn), and high(low) support
verb or noun. We explain the last item, sup-
port verb(noun). From a given word to the
syntactic root along the syntactic tree, the first
verb/noun/preposition that is met is called as its
low support verb/noun/preposition, and the near-
est one to the root is called as its high support
verb/noun/preposition. The concept of support
verb was broadly used (Toutanova et al., 2005;
Xue, 2006; Jiang and Ng, 2006)4, we here extend
it to nouns and prepositions. In addition, we intro-
duce a slightly modified syntactic head, pphead,
it returns the left most sibling of a given word if
the word is headed by a preposition, otherwise it
returns the original head.
Path. There are two basic types of path between
the predicate and the argument candidates. One
is the linear path (linePath) in the sequence, the
other is the path in the syntactic parsing tree (dp-
Path). For the latter, we further divide it into four
sub-types with respect to the syntactic root, dp-
Path is the full path in the syntactic tree. Leading
two paths to the root from the predicate and the
argument, respectively, the common part of these
two paths will be dpPathShare. Assume that dp-
PathShare starts from a node r&apos;, then dpPathPred
is from the predicate to r&apos;, and dpPathArgu is from
the argument to r&apos;.
Family. Two types of children sets for the pred-
icate or argument candidate are considered, the
</bodyText>
<footnote confidence="0.973141625">
2In CoNLL-2008, Treebank tokens are split at the position
that a hyphen (-) or a forward slash (/) occurs. This leads to
two types of feature columns, non-split and split.
3Lemma and pos for either training or test are from auto-
matically pre-analyzed columns in the input files.
4Note that the meaning of support verb is slightly different
between (Toutanova et al., 2005) and (Xue, 2006; Jiang and
Ng, 2006)
</footnote>
<page confidence="0.99895">
32
</page>
<bodyText confidence="0.99787125">
first includes all syntactic children (children), the
second also includes all but excludes the left most
and the right most children (noFarChildren).
Concatenation of Elements. For all collected
elements according to linePath, children and so
on, we use three strategies to concatenate all those
strings to produce the feature value. The first is
seq, which concatenates all collected strings with-
out doing anything. The second is bag, which
removes all duplicated strings and sort the rest.
The third is noDup, which removes all duplicated
neighbored strings.
We address some other elements that are not in-
cluded by the above description as the following.
dpTreeRelation. It returns the relationship of a
and p in the input syntactic tree. The possible val-
ues for this feature include parent, sibling
etc.
isCurPred. It judges if a given word is the cur-
rent predicate. If the word is the predicate, then it
returns the predicate itself, otherwise it returns a
default value.
existCross. It judges if a forthcoming depen-
dency relation that is between a given word pair
may cause any cross with all existing dependency
relations.
distance. It counts the number of words along a
given path, either dpPath or linePath.
existSemdprel. It checks if the given argument
label for other predicates has been assigned to a
given word.
voice. This feature returns Active or Passive for
verbs, and a default value for nouns.
baseline. Two types of semantic role baseline
outputs are used for features from (Carreras and
Marquez, 2005)5. baseline Ax tags the head of
the first NP before the predicate as A0 and the
head of the first NP after the predicate as A1.
baseline Mod tags the dependant of the predicate
as AM-MOD as it is a modal verb.
We show some feature template examples de-
rived from the above mentioned items.
a.lm.lemma The lemma of the left most child of
the argument candidate.
p.h.dprel The dependant label of the syntactic
head of the predicate candidate.
p_1.pos+p.pos pos of the previous word of the
predicate and PoS of the predicate itself.
</bodyText>
<footnote confidence="0.514553333333333">
a:p|dpPath.lemma.bag Collect all lemmas
5These baseline rules were developed by Erik Tjong Kim
Sang, from the University of Antwerp, Belgium.
</footnote>
<bodyText confidence="0.987142375">
along the syntactic tree path from the argument
to the predicate, then removed all duplicated
ones and sort the rest, finally concatenate all as a
feature string.
a:p.highSupportNoun|linePath.dprel.seq Col-
lect all dependant labels along with the line path
from the argument to the high support noun of the
predicate, then concatenate all as a feature string.
</bodyText>
<subsectionHeader confidence="0.99989">
3.2 Feature Template Selection
</subsectionHeader>
<bodyText confidence="0.999989692307692">
Based on the above mentioned elements, 781 fea-
ture templates (hereafter the set of these templates
is referred to FT)6 are initially considered. Fea-
ture templates in this initial set are constructed in
a generalized way. For example, if we find that
a feature template a.lm.lemma was once used in
some existing work, then such three templates,
a.rm.lemma, a.rn.lemma, a.ln.lemma will be also
added into the set.
As an optimal feature template subset cannot be
expected to be extracted from so large a set by
hand, a greedy feature selection similar to that in
(Jiang and Ng, 2006; Ding and Chang, 2008) is ap-
plied. The detailed algorithm is described in Algo-
rithm 1. Assuming that the number of feature tem-
plates in a given set is n, the algorithm of (Ding
and Chang, 2008) requires O(n2) times of train-
ing/test routines, it cannot handle a set that con-
sists of hundreds of templates. As the time com-
plexity of Algorithm 1 is only O(n), it permits a
large scale feature selection accomplished by pay-
ing a reasonable time cost. Though the time com-
plexity of the algorithm given by (Jiang and Ng,
2006) is also linear, it should assume all feature
templates in the initial selected set ‘good’ enough
and handles other feature template candidates in a
strict incremental way. However, these two con-
straints are not easily satisfied in our case, while
Algorithm 1 may release these two constraints.
Choosing the first 1/10 templates in FT as
the initial selected set S, the feature selection is
performed for two argument candidate traverse
schemes, synPth and linPth, respectively. 4686
machine learning routines run for the former,
while 6248 routines for the latter. Two feature
template sets, FTsyn and FTlin, are obtained at
last. These two sets are given in Table 1-3. We see
that two sets share 30 identical feature templates
as in Table 1. FTsyn holds 51 different templates
</bodyText>
<footnote confidence="0.9912285">
6This set with detailed explanation will be available at our
website.
</footnote>
<page confidence="0.998451">
33
</page>
<tableCaption confidence="0.664706">
Table 1: Feature templates for both synPth and
linPth
</tableCaption>
<bodyText confidence="0.999956333333333">
as in Table 2 and FTlin holds 57 different tem-
plates as in Table 3. In these tables, the subscripts -
2(or -1) and 1(or 2) stand for the previous and next
words, respectively. For example, a.lm−1.lemma
returns the lemma of the previous word of the ar-
gument’s left most child.
</bodyText>
<sectionHeader confidence="0.998188" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.999940333333333">
After the predicate sense is disambiguated, an op-
timal argument structure for each predicate is de-
termined by the following maximal probability.
</bodyText>
<equation confidence="0.985807">
�Sp = argmax P�ai|ai−1, ai−2, ...), (1)
i
</equation>
<bodyText confidence="0.9989876">
where Sp is the argument structure, P�ai|ai−1...)
is the conditional probability to determine the la-
bel of the i-th argument candidate label. A beam
search algorithm is used to find the optimal argu-
ment structure.
</bodyText>
<sectionHeader confidence="0.99591" genericHeader="method">
5 Evaluation Results
</sectionHeader>
<bodyText confidence="0.853180818181818">
Our evaluation is performed on the standard
training/development/test corpus of CoNLL-2008
shared task. The data is derived by merging a de-
pendency version of the Penn Treebank with Prop-
Bank and NomBank. More details on the data are
Algorithm 1 Greedy Feature Selection
Input:
The set of all feature templates: FT
The set of selected feature templates: S0
Output:
The set of selected feature templates: S
</bodyText>
<sectionHeader confidence="0.325502" genericHeader="method">
Procedure:
</sectionHeader>
<bodyText confidence="0.5678448">
Let the counter i = 1
Let Si = S0 and C = FT − Si
while do
Train a model with features according to Si,
test on development set and the result is pi.
</bodyText>
<equation confidence="0.516655333333333">
Let Cr = null.
for each feature template fj in set Si do
Let S0 = Si − fj.
</equation>
<bodyText confidence="0.905388">
Train a model with features according to
S0, test on development set and the result
</bodyText>
<equation confidence="0.959455625">
is p0.
if p0 &gt; pi then
Cr = Cr + fj.
end if
end for
C=C+Cr
Si = Si − Cr
Let S0 i = Si
</equation>
<bodyText confidence="0.895277857142857">
Train a model with features according to S0i,
test on development set and the result is qi.
Let Cr = null
for each feature template fj in set C do
Let C0 = S0i + fj.
Train a model with features according to
C0, test on development set and the result
</bodyText>
<equation confidence="0.932185857142857">
is p0.
if p0 &gt; qi then
Cr=Cr+fj.
end if
end for
C = C − Cr
S0i = S0i + Cr
</equation>
<bodyText confidence="0.7317745">
if Si = Si−1(No feature templates are added
or removed) or, neither pi nor qi is larger than
</bodyText>
<figure confidence="0.988275286666667">
pi−1 and qi−1 then
Output S = argmaxp,,qz{Si, S0i} and the
algorithm ends.
else
Let i = i + 1, Si=Si−1 and C = FT − Si
end if
end while
p.lm.dprel
p.rm.dprel
p.spForm
p−1.spLemma
p.spLemma
p−1.spLemma+p.spLemma
p.spLemma + p1.spLemma
p.spLemma + p.h.spForm
p.spLemma + p.currentSense
p.lemma
p.lemma + p1.lemma
p−1.pos+p.pos
a.isCurPred.lemma
a−2.isCurPred.lemma + a−1.isCurPred.lemma
a.isCurPred.spLemma
a−1.isCurPred.spLemma + a.isCurPred.spLemma
a.isCurPred.spLemma + a1.isCurPred.spLemma
a.children.dprel.bag
a−1.spLemma + a.spLemma
a−1.spLemma + a.dprel
a−1.spLemma + a.dprel + a.h.spLemma
a.lm−1.spLemma
a.rm−1.dprel + a.spPos
a−1.lemma + a.dprel + a.h.lemma
a.lemma + p.lemma
a.pos + p.pos
a.spLemma + p.spLemma
a:p dpPath.dprel
a:p dpPathArgu.dprel
a:p dpPathPred.spPos
34
p.currentSense + a.spLemma
p.currentSense + a.spPos
p.voice + (a:p|direction)
p.rm.dprel
p.children.dprel.noDup
p.rm.form
p.lowSupportNoun.spForm
p.lowSupportProp:p|dpTreeRelation
p−2.form + p−1.form
p.voice
p.form + p.children.dprel.noDup
p.pos + p.dprel
p.spForm + p.children.dprel.bag
a.voice + (a:p|direction)
a−1.isCurPred.lemma
a1.isCurPred.lemma
a−1.isCurPred.lemma + a.isCurPred.lemma
a.isCurPred.lemma + a1.isCurPred.lemma
a1.isCurPred.spLemma
a−2.isCurPred.spLemma + a−1.isCurPred.spLemma
a.baseline Ax + a.voice + (a:p|direction)
a.baseline Mod
a.h.children.dprel.bag
a.lm.dprel + a.dprel
a.lm.dprel + a.pos
a.lm−1.lemma
a.lm.lemma
a.lm1.lemma
a.lm.pos + a.pos
a.lm.spForm
a.lm−1.spPos
a.lm.spPos
a.ln.dprel + a.pos
a.noFarChildren.spPos.bag + a.rm.spPos
a.children.spPos.seq + p.children.spPos.seq
a.rm.dprel + a.pos
a.rm−1.spPos
a.rm.spPos
a.rm1.spPos
a.rn.dprel + a.spPos
a.form
a.form + a1.form
a.form + a.pos
a−1.lemma
a−1.lemma + a.lemma
a−2.pos
a.spForm + a1.spForm
a.spForm + a.spPos
a.spLemma + a1.spLemma
a.spForm + a.children.spPos.seq
a.spForm + a.children.spPos.bag
a.spLemma + a.h.spForm
a.spLemma + a.pphead.spForm
a.existSemdprel A2
a:p|dpPathArgu.pos.seq
a:p|dpPathPred.dprel.seq
a:p|dpTreeRelation
Table 3: Feature templates only for linPth
p−1.lemma + p.lemma
p−2.pos
p.pos
p−2.spForm + p−1.spForm
p1.spForm
p.spForm + p.children.dprel.noDup
p.lm.spPos
p.spForm + p.lm.spPos
+ p.noFarChildren.spPos.bag + p.rm.spPos
p.dprel
p.children.dprel.bag
p.children.pos.seq
p.dprel = OBJ ? a
a.dprel
a−1.lemma + a1.lemma
a1.lemma
a−1.pos
a1.spPos
a.h.lemma
a.h.spLemma
a.pphead.lemma
a.pphead.spLemma
a.lm.dprel + a.spPos
a.rm−1.pos
a.spLemma + a.h.spPos
a.existSemdprel A1
a.dprel = OBJ ?
a.form + a.children.pos.seq
a.children.adv.bagb
a:p|linePath.distance
a:p|dpPath.distance
a:p|existCross
a:p|dpPath.dprel.bag
a:p|dpPathPred.dprel.bag
a:p|dpPath.spForm.seq
a:p|dpPathArgu.spForm.seq
a:p|dpPathPred.spForm.bag
a:p|dpPath.spLemma.seq
a:p|dpPathArgu.spLemma.seq
a:p|dpPathArgu.spLemma.bag
a:p|dpPathPred.spLemma.bag
a:p|dpPath.spPos.bag
a:p|dpPathPred.spPos.bag
(a:p|dpPath.dprel.seq) + p.spPos
(a:p|dpTreeRelation) + a.spPos
(a:p|dpTreeRelation) + p.spPos
(a.highSupportVerb:p|dpTreeRelation) + a.spPos
a.highSupportNoun:p|dpPath.dprel.seq
a.lowSupportVerb:p|dpPath.dprel.seq
a:p|linePath.spForm.bag
a:p|linePath.spLemma.bag
a:p|linePath.spLemma.seq
aThis feature checks if the dependant type is OBJ.
badv means all adverbs.
</figure>
<tableCaption confidence="0.75899">
Table 2: Feature templates only for synPth
</tableCaption>
<page confidence="0.998726">
35
</page>
<bodyText confidence="0.999969166666667">
in (Surdeanu et al., 2008). Note that CoNLL-2008
shared task is essentially a joint learning task for
both syntactic and semantic dependencies, how-
ever, we will focus on semantic part of this task.
The main semantic measure that we adopt is se-
mantic labeled F1 score (Sem-F1). In addition, the
macro labeled F1 scores (Macro-F1), which was
used for the ranking of the participating systems of
CoNLL-2008, the ratio between labeled F1 score
for semantic dependencies and the LAS for syn-
tactic dependencies (Sem-F1/LAS), are also given
for reference.
</bodyText>
<subsectionHeader confidence="0.999315">
5.1 Syntactic Dependency Parsers
</subsectionHeader>
<bodyText confidence="0.999989387096774">
We consider three types of syntactic information
to feed the SRL task. One is gold-standard syn-
tactic input, and other two are based on automati-
cally parsing results of two parsers, the state-of-
the-art syntactic parser described in (Johansson
and Nugues, 2008)7(it is referred to Johansson)
and an integrated parser described as the follow-
ing (referred to MSTME).
The parser is basically based on the MSTParser8
using all the features presented by (McDonald et
al., 2006) with projective parsing. Moreover, we
exploit three types of additional features to im-
prove the parser. 1) Chen et al. (2008) used fea-
tures derived from short dependency pairs based
on large-scale auto-parsed data to enhance depen-
dency parsing. Here, the same features are used,
though all dependency pairs rather than short de-
pendency pairs are extracted along with the de-
pendency direction from training data rather than
auto-parsed data. 2) Koo et al. (2008) presented
new features based on word clusters obtained from
large-scale unlabeled data and achieved large im-
provement for English and Czech. Here, the same
features are also used as word clusters are gen-
erated only from the training data. 3) Nivre and
McDonald (2008) presented an integrating method
to provide additional information for graph-based
and transition-based parsers. Here, we represent
features based on dependency relations predicted
by transition-based parsers for the MSTParer. For
the sake of efficiency, we use a fast transition-
</bodyText>
<footnote confidence="0.999331875">
7It is a 2-order maximum spanning tree parser with
pseudo-projective techniques. A syntactic-semantic rerank-
ing was performed to output the final results according to (Jo-
hansson and Nugues, 2008). However, only 1-best outputs of
the parser before reranking are used for our evaluation. Note
that the reranking may slightly improve the syntactic perfor-
mance according to (Johansson and Nugues, 2008).
8It’s freely available at http://mstparser.sourceforge.net.
</footnote>
<table confidence="0.999779733333333">
Parser Path Adaptive Pruning Coverage
Rate
/wo /w
Gold synPth 2.13M 1.05M 98.4%
(49.30%)
linPth 5.29M 1.57M 100.0%
(29.68%)
Johansson synPth 2.15M 1.06M 95.4%
(49.30%)
linPth 5.28M 1.57M 100.0%
(29.73%)
MSTME synPth 2.15M 1.06M 95.0%
(49.30%)
linPth 5.29M 1.57M 100.0%
(29.68%)
</table>
<tableCaption confidence="0.9953775">
Table 4: The number of training samples on argu-
ment candidates
</tableCaption>
<table confidence="0.998757666666667">
synPth+FT3,n linPth+FT�in
Syn-Parser LAS Sem Sem-Fl Sem Sem-Fl
Fl /LAS Fl /LAS
MSTME 88.39 80.53 91.10 79.83 90.31
Johansson 89.28 80.94 90.66 79.84 89.43
Gold 100.00 84.57 84.57 83.34 83.34
</table>
<tableCaption confidence="0.999482">
Table 5: Semantic Labeled F1
</tableCaption>
<bodyText confidence="0.887751666666667">
based parser based on maximum entropy as in
Zhao and Kit (2008). We still use the similar fea-
ture notations of that work.
</bodyText>
<subsectionHeader confidence="0.999663">
5.2 The Results
</subsectionHeader>
<bodyText confidence="0.99520948">
At first, we report the effectiveness of the proposed
adaptive argument pruning. The numbers of argu-
ment candidates are in Table 4. The statistics is
conducted on three different syntactic inputs. The
coverage rate in the table means the ratio of how
many true arguments are covered by the selected
pruning scheme. Note that the adaptive pruning
of argument candidates using assistant labels does
not change this rate. This ratio only depends on
which path, either synPth or UnPth, is chosen,
and how good the syntactic input is (if synPth
is the case). From the results, we see that more
than a half of argument candidates can be effec-
tively pruned for synPth and even 2/3 for UnPth.
As mentioned by (Pradhan et al., 2004), argument
identification plays a bottleneck role in improving
the performance of a SRL system. The effective-
ness of the proposed additional pruning techniques
may be seen as a significant improvement over the
original algorithm of (Xue and Palmer, 2004). The
results also indicate that such an assumption holds
that arguments trend to close with their predicate,
at either type of distance, syntactic or linear.
Based on different syntactic inputs, we obtain
different results on semantic dependency parsing
</bodyText>
<page confidence="0.995965">
36
</page>
<bodyText confidence="0.999967673469388">
as shown in Table 5. These results on differ-
ent syntactic inputs also give us a chance to ob-
serve how semantic performance varies according
to syntactic performance. The fact from the re-
sults is that the ratio Sem-F1/LAS becomes rela-
tively smaller as the syntactic input becomes bet-
ter. Though not so surprised, the results do show
that the argument traverse scheme synPth always
outperforms the other UnPth. The result of this
comparison partially shows that an integrated se-
mantic role labeler is sensitive to the order of how
argument candidates are traversed to some extent.
The performance given by synPth is com-
pared to some other systems that participated in
the CoNLL-2008 shared task. They were cho-
sen among the 20 participating systems either be-
cause they held better results (the first four partic-
ipants) or because they used some joint learning
techniques (Henderson et al., 2008). The results of
(Titov et al., 2009) that use the similar joint learn-
ing technique as (Henderson et al., 2008) are also
included9. Results of these evaluations on the test
set are in Table 6. Top three systems of CoNLL-
2008, (Johansson and Nugues, 2008; Ciaramita et
al., 2008; Che et al., 2008), used SRL pipelines.
In this work, we partially use the similar
techniques (synPth) for our participation in the
shared tasks of CoNLL-2008 and 2009 (Zhao and
Kit, 2008; Zhao et al., 2009b; Zhao et al., 2009a).
Here we report that all SRL sub-tasks are tackled
in one integrated model, while the predicate dis-
ambiguation sub-task was performed individually
in both of our previous systems. Therefore, this is
our first attempt at a full integrated SRL system.
(Titov et al., 2009) reported the best result by
using joint learning technique up to now. The
comparison indicates that our integrated system
outputs a result quite close to the state-of-the-art
by the pipeline system of (Johansson and Nugues,
2008) as the same syntactic structure input is
adopted. It is worth noting that our system actu-
ally competes with two independent sub-systems
of (Johansson and Nugues, 2008), one for verbal
predicates, the other for nominal predicates. In ad-
dition, the results of our system is obtained with-
out using additional joint learning technique like
syntactic-semantic reranking. It indicates that our
system is expected to obtain some further perfor-
mance improvement by using such techniques.
</bodyText>
<footnote confidence="0.978164">
9In addition, the work of (Henderson et al., 2008) and
(Titov et al., 2009) jointly considered syntactic and semantic
dependencies, that is significantly different from the others.
</footnote>
<sectionHeader confidence="0.994384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999995731707317">
We have described a dependency-based semantic
role labeling system for English from NomBank
and PropBank. From the evaluations, the result of
our system is quite close to the state of the art. As
to our knowledge, it is the first integrated SRL sys-
tem that achieves such a competitive performance
against previous pipeline systems.
According to the path that the word-pair classi-
fier traverses argument candidates, two integration
schemes are presented. Argument candidate prun-
ing and feature selection are performed on them,
respectively. These two schemes are more than
providing a trivial comparison. As assistant la-
beled are introduced to help further argument can-
didate pruning, and this techniques work well for
both schemes, it support the assumption that argu-
ments trend to surround their predicate. The pro-
posed feature selection procedure also work for
both schemes and output quite different two fea-
ture template sets, and either of the sets helps the
system obtain a competitive performance, this fact
suggests that the feature selection procedure is ro-
bust and effective, too.
Either of the presented integrated systems can
provide a competitive performance. This conclu-
sion about basic learning scheme for SRL is some
different from previous literatures. However, ac-
cording to our results, there does exist a ‘harmony’
feature template set that is helpful to both predi-
cate and argument identification/classification, or
SRL for both verbal and nominal predicates. We
attribute this different conclusion to two main fac-
tors, 1) much more feature templates (for example,
ten times more than those used by Xue et al.) than
previous that are considered for a successful fea-
ture engineering, 2) a maximum entropy classifier
makes it possible to accept so many various fea-
tures in one model. Note that maximum entropy is
not so sensitive to those (partially) overlapped fea-
tures, while SVM and other margin-based learners
are not so.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999402">
Our thanks give to Dr. Richard Johansson, who
kindly provided the syntactic output for his partic-
ipation in the CoNLL-2008 shared task.
</bodyText>
<page confidence="0.998447">
37
</page>
<table confidence="0.9998534">
Systemsa LAS Sem-F1 Macro Sem-F1 pred-F1b argu-F1c Verb-F1d Nomi-F1e
F1 /LAS
Johansson:2008*f 89.32 81.65 85.49 91.41 87.22 79.04 84.78 77.12
Ours:Johansson 89.28 80.94 85.12 90.66 86.57 78.30 83.66 76.93
Ours:MSTmE 88.39 80.53 84.93 91.10 86.80 77.60 82.77 77.23
Johansson:2008 89.32 80.37 84.86 89.98 85.40 78.02 84.45 74.32
Ciaramita:2008* 87.37 78.00 82.69 89.28 83.46 75.35 80.93 73.80
Che:2008 86.75 78.52 82.66 90.51 85.31 75.27 80.46 75.18
Zhao:2008* 87.68 76.75 82.24 87.53 78.52 75.93 78.81 73.59
Ciaramita:2008 86.60 77.50 82.06 89.49 83.46 74.56 80.15 73.17
Titov:2009 87.50 76.10 81.80 86.97 – – – –
Zhao:2008 86.66 76.16 81.44 87.88 78.26 75.18 77.67 73.28
Henderson:2008* 87.64 73.09 80.48 83.40 81.42 69.10 75.84 68.90
Henderson:2008 86.91 70.97 79.11 81.66 79.60 66.83 73.80 66.26
Ours:Gold 100.0 84.57 92.20 84.57 87.67 83.15 88.71 78.39
</table>
<tableCaption confidence="0.9426575">
aRanking according to Sem-F1
bLabeled F1 for predicate identification and classification
cLabeled F1 for argument identification and classification
dLabeled F1 for verbal predicates
eLabeled F1 for nominal predicates
f* means post-evaluation results, which are available at the official website of CoNLL-2008 shared task,
http://www.yr-bcn.es/dokuwiki/doku.php?id=conll2008:start.
Table 6: Comparison of the best existing systems
</tableCaption>
<sectionHeader confidence="0.995671" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999475257575758">
Xavier Carreras and Lluis Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of CoNLL-2005, pages 152–
164, Ann Arbor, Michigan, USA.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang
Li, Bing Qin, Ting Liu, and Sheng Li. 2008. A
cascaded syntactic and semantic dependency pars-
ing system. In Proceedings of CoNLL-2008, pages
238–242, Manchester, England, August.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchi-
moto, Yujie Zhang, and Hitoshi Isahara. 2008. De-
pendency parsing with short dependency relations
in unlabeled data. In Proceedings of IJCNLP-2008,
Hyderabad, India, January 8-10.
Massimiliano Ciaramita, Giuseppe Attardi, Felice
Dell’Orletta, and Mihai Surdeanu. 2008. Desrl: A
linear-time semantic role labeling system. In Pro-
ceedings of CoNLL-2008, pages 258–262, Manch-
ester, England, August.
Hoa Trang Dang and Martha Palmer. 2005. The role
of semantic roles in disambiguating verb senses. In
Proceedings ofACL-2005, pages 42–49, Ann Arbor,
USA.
Weiwei Ding and Baobao Chang. 2008. Improving
chinese semantic role classification with hierarchi-
cal feature selection strategy. In Proceedings of
EMNLP-2008, pages 324–323, Honolulu, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, pages 1–
18, Boulder, Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic depen-
dencies. In Proceedings of CoNLL-2008, pages
178–182, Manchester, England, August.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Seman-
tic role labeling of nombank: A maximum entropy
approach. In Proceedings of EMNLP-2006, pages
138–145, Sydney, Australia.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic–semantic analysis
with propbank and nombank. In Proceedings of
CoNLL-2008, page 183–187, Manchester, UK.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595–603,
Columbus, Ohio, USA, June.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen
tau Yih. 2005. Generalized inference with multi-
ple semantic role labeling systems. In Proceedings
of CoNLL-2005, pages 181–184, Ann Arbor, Michi-
gan, USA.
Chang Liu and Hwee Tou Ng. 2007. Learning pre-
dictive structures for semantic role labeling of nom-
bank. In Proceedings of ACL-2007, pages 208–215,
Prague, Czech.
</reference>
<page confidence="0.985359">
38
</page>
<reference confidence="0.999854148148148">
Lluis Marquez, Mihai Surdeanu, Pere Comas, and
Jordi Turmo. 2005. A robust combination strat-
egy for semantic role labeling. In Proceedings
of HLT/EMNLP-2005, page 644–651, Vancouver,
Canada.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings of
CoNLL-X, New York City, June.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In Proceedings of HLT/NAACL
Workshop on Frontiers in Corpus Annotation, pages
24–31, Boston, Massachusetts, USA, May 6.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950–958, Columbus, Ohio, June.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. 2004. Shallow
semantic parsing using support vector machines. In
Proceedings of HLT/NAACL-2004, pages 233–240,
Boston, Massachusetts, USA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic role labeling using different syntactic views.
In Proceedings of ACL-2005, pages 581–588, Ann
Arbor, USA.
James Pustejovsky, Adam Meyers, Martha Palmer, and
Massimo Poesio. 2005. Merging propbank, nom-
bank, timebank, penn discourse treebank and coref-
erence. In Proceedings of the Workshop on Frontiers
in Corpus Annotations II: Pie in the Sky, pages 5–12,
Ann Arbor, USA.
Mihai Surdeanu, Lluis Marquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for
semantic role labeling. Journal of Artificial Intelli-
gence Research, 29:105–151.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
CoNLL-2008, pages 159–177, Manchester, UK.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planarisation
for synchronous parsing of semantic and syntactic
dependencies. In IJCAI-2009, Pasadena, California,
USA.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL-2005, pages
589–596, Ann Arbor, USA.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP-2004, pages 88–94, Barcelona, Spain,
July 25-26.
Nianwen Xue. 2006. Semantic role labeling of nom-
inalized predicates in chinese. In Proceedings of
NAACL-2006, pages 431–438, New York City, USA,
June.
Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage max-
imum entropy models. In Proceeding of CoNLL-
2008, pages 203–207, Manchester, UK.
Hai Zhao, Wenliang Chen, Jun’ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009a. Multilin-
gual dependency learning: Exploiting rich features
for tagging syntactic and semantic dependencies. In
Proceedings of the 13th Conference on Computa-
tional Natural Language Learning (CoNLL-2009),
June 4-5, pages 61–66, Boulder, Colorado, USA.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009b. Multilingual dependency learning:
A huge feature engineering method to semantic de-
pendency parsing. In Proceedings of CoNLL-2009,
pages 55–60, Boulder, Colorado, USA.
</reference>
<page confidence="0.999531">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.756032">
<title confidence="0.9992975">Semantic Dependency Parsing of NomBank and PropBank: Efficient Integrated Approach via a Large-scale Feature Selection</title>
<author confidence="0.985109">Zhao</author>
<affiliation confidence="0.9398172">of Chinese, Translation and City University of Hong Tat Chee Avenue, Kowloon, Hong Kong, Infrastructure Group, MASTAR National Institute of Information and Communications</affiliation>
<address confidence="0.995851">3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan,</address>
<email confidence="0.983826">haizhao@cityu.edu.hk,chenwl@nict.go.jp</email>
<abstract confidence="0.999280705882353">We present an integrated dependencybased semantic role labeling system for English from both NomBank and Prop- Bank. By introducing assistant argument labels and considering much more feature templates, two optimal feature template sets are obtained through an effective feature selection procedure and help construct a high performance single SRL system. From the evaluations on the date set of CoNLL-2008 shared task, the performance of our system is quite close to the state of the art. As to our knowledge, this is the first integrated SRL system that achieves a competitive performance against previous pipeline systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis Marquez</author>
</authors>
<title>Introduction to the conll-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<pages>152--164</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="2464" citStr="Carreras and Marquez, 2005" startWordPosition="357" endWordPosition="361">ll these sub-tasks. The reason to divide the whole task This study is partially supported by CERG grant 9040861 (CityU 1318/03H), CityU Strategic Research Grant 7002037. into multiple stages is two-fold, one is each subtask asks for its favorable features, the other is at the consideration of computational efficiency. Generally speaking, a joint system is slower than a pipeline system in training. (Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws</context>
<context position="10545" citStr="Carreras and Marquez, 2005" startWordPosition="1714" endWordPosition="1717"> Training samples will generated from c to g according to the above sequence. We use a Maximum Entropy classifier with a tunable Gaussian prior as usual. Our implementation of the model adopts L-BFGS algorithm for parameter optimization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2, lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3. Syntact</context>
<context position="14574" citStr="Carreras and Marquez, 2005" startWordPosition="2384" endWordPosition="2387">ate, then it returns the predicate itself, otherwise it returns a default value. existCross. It judges if a forthcoming dependency relation that is between a given word pair may cause any cross with all existing dependency relations. distance. It counts the number of words along a given path, either dpPath or linePath. existSemdprel. It checks if the given argument label for other predicates has been assigned to a given word. voice. This feature returns Active or Passive for verbs, and a default value for nouns. baseline. Two types of semantic role baseline outputs are used for features from (Carreras and Marquez, 2005)5. baseline Ax tags the head of the first NP before the predicate as A0 and the head of the first NP after the predicate as A1. baseline Mod tags the dependant of the predicate as AM-MOD as it is a modal verb. We show some feature template examples derived from the above mentioned items. a.lm.lemma The lemma of the left most child of the argument candidate. p.h.dprel The dependant label of the syntactic head of the predicate candidate. p_1.pos+p.pos pos of the previous word of the predicate and PoS of the predicate itself. a:p|dpPath.lemma.bag Collect all lemmas 5These baseline rules were deve</context>
</contexts>
<marker>Carreras, Marquez, 2005</marker>
<rawString>Xavier Carreras and Lluis Marquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In Proceedings of CoNLL-2005, pages 152– 164, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Yuxuan Hu</author>
<author>Yongqiang Li</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>A cascaded syntactic and semantic dependency parsing system.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL-2008,</booktitle>
<pages>238--242</pages>
<location>Manchester, England,</location>
<contexts>
<context position="10765" citStr="Che et al., 2008" startWordPosition="1754" endWordPosition="1757">ization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2, lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3. Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm, and rn), and high(low) support verb or noun. We explain the last item, support verb(noun). From a given word to the syntac</context>
<context position="28497" citStr="Che et al., 2008" startWordPosition="4520" endWordPosition="4523"> extent. The performance given by synPth is compared to some other systems that participated in the CoNLL-2008 shared task. They were chosen among the 20 participating systems either because they held better results (the first four participants) or because they used some joint learning techniques (Henderson et al., 2008). The results of (Titov et al., 2009) that use the similar joint learning technique as (Henderson et al., 2008) are also included9. Results of these evaluations on the test set are in Table 6. Top three systems of CoNLL2008, (Johansson and Nugues, 2008; Ciaramita et al., 2008; Che et al., 2008), used SRL pipelines. In this work, we partially use the similar techniques (synPth) for our participation in the shared tasks of CoNLL-2008 and 2009 (Zhao and Kit, 2008; Zhao et al., 2009b; Zhao et al., 2009a). Here we report that all SRL sub-tasks are tackled in one integrated model, while the predicate disambiguation sub-task was performed individually in both of our previous systems. Therefore, this is our first attempt at a full integrated SRL system. (Titov et al., 2009) reported the best result by using joint learning technique up to now. The comparison indicates that our integrated sys</context>
</contexts>
<marker>Che, Li, Hu, Li, Qin, Liu, Li, 2008</marker>
<rawString>Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, and Sheng Li. 2008. A cascaded syntactic and semantic dependency parsing system. In Proceedings of CoNLL-2008, pages 238–242, Manchester, England, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Daisuke Kawahara</author>
<author>Kiyotaka Uchimoto</author>
<author>Yujie Zhang</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Dependency parsing with short dependency relations in unlabeled data.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP-2008,</booktitle>
<location>Hyderabad, India,</location>
<contexts>
<context position="24033" citStr="Chen et al. (2008)" startWordPosition="3801" endWordPosition="3804">c Dependency Parsers We consider three types of syntactic information to feed the SRL task. One is gold-standard syntactic input, and other two are based on automatically parsing results of two parsers, the state-ofthe-art syntactic parser described in (Johansson and Nugues, 2008)7(it is referred to Johansson) and an integrated parser described as the following (referred to MSTME). The parser is basically based on the MSTParser8 using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to improve the parser. 1) Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing. Here, the same features are used, though all dependency pairs rather than short dependency pairs are extracted along with the dependency direction from training data rather than auto-parsed data. 2) Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. Here, the same features are also used as word clusters are generated only from the training data. 3) Nivre and McDonald (2</context>
</contexts>
<marker>Chen, Kawahara, Uchimoto, Zhang, Isahara, 2008</marker>
<rawString>Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto, Yujie Zhang, and Hitoshi Isahara. 2008. Dependency parsing with short dependency relations in unlabeled data. In Proceedings of IJCNLP-2008, Hyderabad, India, January 8-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Giuseppe Attardi</author>
<author>Felice Dell’Orletta</author>
<author>Mihai Surdeanu</author>
</authors>
<title>Desrl: A linear-time semantic role labeling system.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL-2008,</booktitle>
<pages>258--262</pages>
<location>Manchester, England,</location>
<marker>Ciaramita, Attardi, Dell’Orletta, Surdeanu, 2008</marker>
<rawString>Massimiliano Ciaramita, Giuseppe Attardi, Felice Dell’Orletta, and Mihai Surdeanu. 2008. Desrl: A linear-time semantic role labeling system. In Proceedings of CoNLL-2008, pages 258–262, Manchester, England, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>The role of semantic roles in disambiguating verb senses.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL-2005,</booktitle>
<pages>42--49</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="10611" citStr="Dang and Palmer, 2005" startWordPosition="1726" endWordPosition="1729">uence. We use a Maximum Entropy classifier with a tunable Gaussian prior as usual. Our implementation of the model adopts L-BFGS algorithm for parameter optimization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2, lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3. Syntactic Connection. This includes syntactic head (h), left(right) farth</context>
</contexts>
<marker>Dang, Palmer, 2005</marker>
<rawString>Hoa Trang Dang and Martha Palmer. 2005. The role of semantic roles in disambiguating verb senses. In Proceedings ofACL-2005, pages 42–49, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Ding</author>
<author>Baobao Chang</author>
</authors>
<title>Improving chinese semantic role classification with hierarchical feature selection strategy.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP-2008,</booktitle>
<pages>324--323</pages>
<location>Honolulu, USA.</location>
<contexts>
<context position="16237" citStr="Ding and Chang, 2008" startWordPosition="2660" endWordPosition="2663">Template Selection Based on the above mentioned elements, 781 feature templates (hereafter the set of these templates is referred to FT)6 are initially considered. Feature templates in this initial set are constructed in a generalized way. For example, if we find that a feature template a.lm.lemma was once used in some existing work, then such three templates, a.rm.lemma, a.rn.lemma, a.ln.lemma will be also added into the set. As an optimal feature template subset cannot be expected to be extracted from so large a set by hand, a greedy feature selection similar to that in (Jiang and Ng, 2006; Ding and Chang, 2008) is applied. The detailed algorithm is described in Algorithm 1. Assuming that the number of feature templates in a given set is n, the algorithm of (Ding and Chang, 2008) requires O(n2) times of training/test routines, it cannot handle a set that consists of hundreds of templates. As the time complexity of Algorithm 1 is only O(n), it permits a large scale feature selection accomplished by paying a reasonable time cost. Though the time complexity of the algorithm given by (Jiang and Ng, 2006) is also linear, it should assume all feature templates in the initial selected set ‘good’ enough and </context>
</contexts>
<marker>Ding, Chang, 2008</marker>
<rawString>Weiwei Ding and Baobao Chang. 2008. Improving chinese semantic role classification with hierarchical feature selection strategy. In Proceedings of EMNLP-2008, pages 324–323, Honolulu, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="10517" citStr="Gildea and Jurafsky, 2002" startWordPosition="1709" endWordPosition="1713"> g h . noLeft A1 A0 noRight Training samples will generated from c to g according to the above sequence. We use a Maximum Entropy classifier with a tunable Gaussian prior as usual. Our implementation of the model adopts L-BFGS algorithm for parameter optimization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2, lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependenc</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
</authors>
<location>Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs</location>
<marker>Hajiˇc, Ciaramita, Johansson, </marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers M`arquez</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>1--18</pages>
<location>Boulder, Colorado, USA.</location>
<marker>M`arquez, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5, pages 1– 18, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
<author>Ivan Titov</author>
</authors>
<title>A latent variable model of synchronous parsing for syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL-2008,</booktitle>
<pages>178--182</pages>
<location>Manchester, England,</location>
<contexts>
<context position="28202" citStr="Henderson et al., 2008" startWordPosition="4467" endWordPosition="4470">t becomes better. Though not so surprised, the results do show that the argument traverse scheme synPth always outperforms the other UnPth. The result of this comparison partially shows that an integrated semantic role labeler is sensitive to the order of how argument candidates are traversed to some extent. The performance given by synPth is compared to some other systems that participated in the CoNLL-2008 shared task. They were chosen among the 20 participating systems either because they held better results (the first four participants) or because they used some joint learning techniques (Henderson et al., 2008). The results of (Titov et al., 2009) that use the similar joint learning technique as (Henderson et al., 2008) are also included9. Results of these evaluations on the test set are in Table 6. Top three systems of CoNLL2008, (Johansson and Nugues, 2008; Ciaramita et al., 2008; Che et al., 2008), used SRL pipelines. In this work, we partially use the similar techniques (synPth) for our participation in the shared tasks of CoNLL-2008 and 2009 (Zhao and Kit, 2008; Zhao et al., 2009b; Zhao et al., 2009a). Here we report that all SRL sub-tasks are tackled in one integrated model, while the predicat</context>
<context position="29739" citStr="Henderson et al., 2008" startWordPosition="4719" endWordPosition="4722">t quite close to the state-of-the-art by the pipeline system of (Johansson and Nugues, 2008) as the same syntactic structure input is adopted. It is worth noting that our system actually competes with two independent sub-systems of (Johansson and Nugues, 2008), one for verbal predicates, the other for nominal predicates. In addition, the results of our system is obtained without using additional joint learning technique like syntactic-semantic reranking. It indicates that our system is expected to obtain some further performance improvement by using such techniques. 9In addition, the work of (Henderson et al., 2008) and (Titov et al., 2009) jointly considered syntactic and semantic dependencies, that is significantly different from the others. 6 Conclusion We have described a dependency-based semantic role labeling system for English from NomBank and PropBank. From the evaluations, the result of our system is quite close to the state of the art. As to our knowledge, it is the first integrated SRL system that achieves such a competitive performance against previous pipeline systems. According to the path that the word-pair classifier traverses argument candidates, two integration schemes are presented. Ar</context>
</contexts>
<marker>Henderson, Merlo, Musillo, Titov, 2008</marker>
<rawString>James Henderson, Paola Merlo, Gabriele Musillo, and Ivan Titov. 2008. A latent variable model of synchronous parsing for syntactic and semantic dependencies. In Proceedings of CoNLL-2008, pages 178–182, Manchester, England, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Ping Jiang</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Semantic role labeling of nombank: A maximum entropy approach.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP-2006,</booktitle>
<pages>138--145</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3113" citStr="Jiang and Ng, 2006" startWordPosition="464" endWordPosition="467">u et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank. Though aiming at Chinese SRL, (Xue, 2006) reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance. </context>
<context position="10677" citStr="Jiang and Ng, 2006" startWordPosition="1738" endWordPosition="1741">ior as usual. Our implementation of the model adopts L-BFGS algorithm for parameter optimization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2, lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3. Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm, and rn), and high(low) support ver</context>
<context position="13047" citStr="Jiang and Ng, 2006" startWordPosition="2135" endWordPosition="2138">hare starts from a node r&apos;, then dpPathPred is from the predicate to r&apos;, and dpPathArgu is from the argument to r&apos;. Family. Two types of children sets for the predicate or argument candidate are considered, the 2In CoNLL-2008, Treebank tokens are split at the position that a hyphen (-) or a forward slash (/) occurs. This leads to two types of feature columns, non-split and split. 3Lemma and pos for either training or test are from automatically pre-analyzed columns in the input files. 4Note that the meaning of support verb is slightly different between (Toutanova et al., 2005) and (Xue, 2006; Jiang and Ng, 2006) 32 first includes all syntactic children (children), the second also includes all but excludes the left most and the right most children (noFarChildren). Concatenation of Elements. For all collected elements according to linePath, children and so on, we use three strategies to concatenate all those strings to produce the feature value. The first is seq, which concatenates all collected strings without doing anything. The second is bag, which removes all duplicated strings and sort the rest. The third is noDup, which removes all duplicated neighbored strings. We address some other elements tha</context>
<context position="16214" citStr="Jiang and Ng, 2006" startWordPosition="2656" endWordPosition="2659">string. 3.2 Feature Template Selection Based on the above mentioned elements, 781 feature templates (hereafter the set of these templates is referred to FT)6 are initially considered. Feature templates in this initial set are constructed in a generalized way. For example, if we find that a feature template a.lm.lemma was once used in some existing work, then such three templates, a.rm.lemma, a.rn.lemma, a.ln.lemma will be also added into the set. As an optimal feature template subset cannot be expected to be extracted from so large a set by hand, a greedy feature selection similar to that in (Jiang and Ng, 2006; Ding and Chang, 2008) is applied. The detailed algorithm is described in Algorithm 1. Assuming that the number of feature templates in a given set is n, the algorithm of (Ding and Chang, 2008) requires O(n2) times of training/test routines, it cannot handle a set that consists of hundreds of templates. As the time complexity of Algorithm 1 is only O(n), it permits a large scale feature selection accomplished by paying a reasonable time cost. Though the time complexity of the algorithm given by (Jiang and Ng, 2006) is also linear, it should assume all feature templates in the initial selected</context>
</contexts>
<marker>Jiang, Ng, 2006</marker>
<rawString>Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic role labeling of nombank: A maximum entropy approach. In Proceedings of EMNLP-2006, pages 138–145, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based syntactic–semantic analysis with propbank and nombank.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL-2008,</booktitle>
<pages>183--187</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="2537" citStr="Johansson and Nugues, 2008" startWordPosition="370" endWordPosition="373">rtially supported by CERG grant 9040861 (CityU 1318/03H), CityU Strategic Research Grant 7002037. into multiple stages is two-fold, one is each subtask asks for its favorable features, the other is at the consideration of computational efficiency. Generally speaking, a joint system is slower than a pipeline system in training. (Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2</context>
<context position="3803" citStr="Johansson and Nugues, 2008" startWordPosition="574" endWordPosition="577">ious treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank. Though aiming at Chinese SRL, (Xue, 2006) reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance. From the results of CoNLL-2008 shared task, the top system by (Johansson and Nugues, 2008) also used two 30 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 30–39, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP different subsystems to handle verbal and nominal predicates, respectively. Despite all the above facts, an integrated SRL system still holds some sort of merits, being easier to implement, a single-stage feature selection benefiting the whole system, an all-in-one model outputting all required semantic role information and so on. The shared tasks at the CoNLL 2008 and 2009 are devoted to the joint learning of syntactic and sema</context>
<context position="10746" citStr="Johansson and Nugues, 2008" startWordPosition="1750" endWordPosition="1753">lgorithm for parameter optimization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2, lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3. Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm, and rn), and high(low) support verb or noun. We explain the last item, support verb(noun). From a given</context>
<context position="23696" citStr="Johansson and Nugues, 2008" startWordPosition="3746" endWordPosition="3749"> measure that we adopt is semantic labeled F1 score (Sem-F1). In addition, the macro labeled F1 scores (Macro-F1), which was used for the ranking of the participating systems of CoNLL-2008, the ratio between labeled F1 score for semantic dependencies and the LAS for syntactic dependencies (Sem-F1/LAS), are also given for reference. 5.1 Syntactic Dependency Parsers We consider three types of syntactic information to feed the SRL task. One is gold-standard syntactic input, and other two are based on automatically parsing results of two parsers, the state-ofthe-art syntactic parser described in (Johansson and Nugues, 2008)7(it is referred to Johansson) and an integrated parser described as the following (referred to MSTME). The parser is basically based on the MSTParser8 using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to improve the parser. 1) Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing. Here, the same features are used, though all dependency pairs rather than short dependency pairs are extracted along with the dependency direct</context>
<context position="25111" citStr="Johansson and Nugues, 2008" startWordPosition="3965" endWordPosition="3969">ent for English and Czech. Here, the same features are also used as word clusters are generated only from the training data. 3) Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for the MSTParer. For the sake of efficiency, we use a fast transition7It is a 2-order maximum spanning tree parser with pseudo-projective techniques. A syntactic-semantic reranking was performed to output the final results according to (Johansson and Nugues, 2008). However, only 1-best outputs of the parser before reranking are used for our evaluation. Note that the reranking may slightly improve the syntactic performance according to (Johansson and Nugues, 2008). 8It’s freely available at http://mstparser.sourceforge.net. Parser Path Adaptive Pruning Coverage Rate /wo /w Gold synPth 2.13M 1.05M 98.4% (49.30%) linPth 5.29M 1.57M 100.0% (29.68%) Johansson synPth 2.15M 1.06M 95.4% (49.30%) linPth 5.28M 1.57M 100.0% (29.73%) MSTME synPth 2.15M 1.06M 95.0% (49.30%) linPth 5.29M 1.57M 100.0% (29.68%) Table 4: The number of training samples on argument candi</context>
<context position="28454" citStr="Johansson and Nugues, 2008" startWordPosition="4512" endWordPosition="4515">der of how argument candidates are traversed to some extent. The performance given by synPth is compared to some other systems that participated in the CoNLL-2008 shared task. They were chosen among the 20 participating systems either because they held better results (the first four participants) or because they used some joint learning techniques (Henderson et al., 2008). The results of (Titov et al., 2009) that use the similar joint learning technique as (Henderson et al., 2008) are also included9. Results of these evaluations on the test set are in Table 6. Top three systems of CoNLL2008, (Johansson and Nugues, 2008; Ciaramita et al., 2008; Che et al., 2008), used SRL pipelines. In this work, we partially use the similar techniques (synPth) for our participation in the shared tasks of CoNLL-2008 and 2009 (Zhao and Kit, 2008; Zhao et al., 2009b; Zhao et al., 2009a). Here we report that all SRL sub-tasks are tackled in one integrated model, while the predicate disambiguation sub-task was performed individually in both of our previous systems. Therefore, this is our first attempt at a full integrated SRL system. (Titov et al., 2009) reported the best result by using joint learning technique up to now. The c</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic–semantic analysis with propbank and nombank. In Proceedings of CoNLL-2008, page 183–187, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>595--603</pages>
<location>Columbus, Ohio, USA,</location>
<contexts>
<context position="24369" citStr="Koo et al. (2008)" startWordPosition="3855" endWordPosition="3858">described as the following (referred to MSTME). The parser is basically based on the MSTParser8 using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to improve the parser. 1) Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing. Here, the same features are used, though all dependency pairs rather than short dependency pairs are extracted along with the dependency direction from training data rather than auto-parsed data. 2) Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. Here, the same features are also used as word clusters are generated only from the training data. 3) Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for the MSTParer. For the sake of efficiency, we use a fast transition7It is a 2-order maximum spanning tree parser with ps</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of ACL-08: HLT, pages 595–603, Columbus, Ohio, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Koomen</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<title>Generalized inference with multiple semantic role labeling systems.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<pages>181--184</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="2485" citStr="Koomen et al., 2005" startWordPosition="362" endWordPosition="365">on to divide the whole task This study is partially supported by CERG grant 9040861 (CityU 1318/03H), CityU Strategic Research Grant 7002037. into multiple stages is two-fold, one is each subtask asks for its favorable features, the other is at the consideration of computational efficiency. Generally speaking, a joint system is slower than a pipeline system in training. (Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little at</context>
<context position="10566" citStr="Koomen et al., 2005" startWordPosition="1718" endWordPosition="1721">ated from c to g according to the above sequence. We use a Maximum Entropy classifier with a tunable Gaussian prior as usual. Our implementation of the model adopts L-BFGS algorithm for parameter optimization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2, lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3. Syntactic Connection. This i</context>
</contexts>
<marker>Koomen, Punyakanok, Roth, Yih, 2005</marker>
<rawString>Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005. Generalized inference with multiple semantic role labeling systems. In Proceedings of CoNLL-2005, pages 181–184, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Learning predictive structures for semantic role labeling of nombank.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-2007,</booktitle>
<pages>208--215</pages>
<location>Prague, Czech.</location>
<contexts>
<context position="10695" citStr="Liu and Ng, 2007" startWordPosition="1742" endWordPosition="1745">plementation of the model adopts L-BFGS algorithm for parameter optimization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2, lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3. Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm, and rn), and high(low) support verb or noun. We expl</context>
</contexts>
<marker>Liu, Ng, 2007</marker>
<rawString>Chang Liu and Hwee Tou Ng. 2007. Learning predictive structures for semantic role labeling of nombank. In Proceedings of ACL-2007, pages 208–215, Prague, Czech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis Marquez</author>
<author>Mihai Surdeanu</author>
<author>Pere Comas</author>
<author>Jordi Turmo</author>
</authors>
<title>A robust combination strategy for semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP-2005,</booktitle>
<pages>644--651</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="10588" citStr="Marquez et al., 2005" startWordPosition="1722" endWordPosition="1725">rding to the above sequence. We use a Maximum Entropy classifier with a tunable Gaussian prior as usual. Our implementation of the model adopts L-BFGS algorithm for parameter optimization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2, lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3. Syntactic Connection. This includes syntactic head</context>
</contexts>
<marker>Marquez, Surdeanu, Comas, Turmo, 2005</marker>
<rawString>Lluis Marquez, Mihai Surdeanu, Pere Comas, and Jordi Turmo. 2005. A robust combination strategy for semantic role labeling. In Proceedings of HLT/EMNLP-2005, page 644–651, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X,</booktitle>
<location>New York City,</location>
<contexts>
<context position="23907" citStr="McDonald et al., 2006" startWordPosition="3780" endWordPosition="3783">F1 score for semantic dependencies and the LAS for syntactic dependencies (Sem-F1/LAS), are also given for reference. 5.1 Syntactic Dependency Parsers We consider three types of syntactic information to feed the SRL task. One is gold-standard syntactic input, and other two are based on automatically parsing results of two parsers, the state-ofthe-art syntactic parser described in (Johansson and Nugues, 2008)7(it is referred to Johansson) and an integrated parser described as the following (referred to MSTME). The parser is basically based on the MSTParser8 using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to improve the parser. 1) Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing. Here, the same features are used, though all dependency pairs rather than short dependency pairs are extracted along with the dependency direction from training data rather than auto-parsed data. 2) Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Cze</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proceedings of CoNLL-X, New York City, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>The nombank project: An interim report.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL Workshop on Frontiers in Corpus Annotation,</booktitle>
<pages>24--31</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="2884" citStr="Meyers et al., 2004" startWordPosition="430" endWordPosition="433">unt out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank. Though aiming a</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The nombank project: An interim report. In Proceedings of HLT/NAACL Workshop on Frontiers in Corpus Annotation, pages 24–31, Boston, Massachusetts, USA, May 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>950--958</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="24637" citStr="Nivre and McDonald (2008)" startWordPosition="3899" endWordPosition="3902">1) Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing. Here, the same features are used, though all dependency pairs rather than short dependency pairs are extracted along with the dependency direction from training data rather than auto-parsed data. 2) Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. Here, the same features are also used as word clusters are generated only from the training data. 3) Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for the MSTParer. For the sake of efficiency, we use a fast transition7It is a 2-order maximum spanning tree parser with pseudo-projective techniques. A syntactic-semantic reranking was performed to output the final results according to (Johansson and Nugues, 2008). However, only 1-best outputs of the parser before reranking are used for our evaluation. Note that the reranking may slightl</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of ACL-08: HLT, pages 950–958, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="2824" citStr="Palmer et al., 2005" startWordPosition="419" endWordPosition="422">han a pipeline system in training. (Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL-2004,</booktitle>
<pages>233--240</pages>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="26798" citStr="Pradhan et al., 2004" startWordPosition="4240" endWordPosition="4243">nt candidates are in Table 4. The statistics is conducted on three different syntactic inputs. The coverage rate in the table means the ratio of how many true arguments are covered by the selected pruning scheme. Note that the adaptive pruning of argument candidates using assistant labels does not change this rate. This ratio only depends on which path, either synPth or UnPth, is chosen, and how good the syntactic input is (if synPth is the case). From the results, we see that more than a half of argument candidates can be effectively pruned for synPth and even 2/3 for UnPth. As mentioned by (Pradhan et al., 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of (Xue and Palmer, 2004). The results also indicate that such an assumption holds that arguments trend to close with their predicate, at either type of distance, syntactic or linear. Based on different syntactic inputs, we obtain different results on semantic dependency parsing 36 as shown in Table 5. These results on different syntactic inputs also give us a chance t</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Dan Jurafsky. 2004. Shallow semantic parsing using support vector machines. In Proceedings of HLT/NAACL-2004, pages 233–240, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Semantic role labeling using different syntactic views.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005,</booktitle>
<pages>581--588</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="10633" citStr="Pradhan et al., 2005" startWordPosition="1730" endWordPosition="1733"> Entropy classifier with a tunable Gaussian prior as usual. Our implementation of the model adopts L-BFGS algorithm for parameter optimization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2, lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3. Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Daniel Jurafsky. 2005. Semantic role labeling using different syntactic views. In Proceedings of ACL-2005, pages 581–588, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Adam Meyers</author>
<author>Martha Palmer</author>
<author>Massimo Poesio</author>
</authors>
<title>Merging propbank, nombank, timebank, penn discourse treebank and coreference.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky,</booktitle>
<pages>5--12</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="3141" citStr="Pustejovsky et al., 2005" startWordPosition="468" endWordPosition="471">son and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank. Though aiming at Chinese SRL, (Xue, 2006) reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance. From the results of CoNLL-20</context>
</contexts>
<marker>Pustejovsky, Meyers, Palmer, Poesio, 2005</marker>
<rawString>James Pustejovsky, Adam Meyers, Martha Palmer, and Massimo Poesio. 2005. Merging propbank, nombank, timebank, penn discourse treebank and coreference. In Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky, pages 5–12, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Lluis Marquez</author>
<author>Xavier Carreras</author>
<author>Pere R Comas</author>
</authors>
<title>Combination strategies for semantic role labeling.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>29--105</pages>
<contexts>
<context position="10718" citStr="Surdeanu et al., 2007" startWordPosition="1746" endWordPosition="1749">e model adopts L-BFGS algorithm for parameter optimization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2, lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3. Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm, and rn), and high(low) support verb or noun. We explain the last item, supp</context>
</contexts>
<marker>Surdeanu, Marquez, Carreras, Comas, 2007</marker>
<rawString>Mihai Surdeanu, Lluis Marquez, Xavier Carreras, and Pere R. Comas. 2007. Combination strategies for semantic role labeling. Journal of Artificial Intelligence Research, 29:105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL-2008,</booktitle>
<pages>159--177</pages>
<location>Manchester, UK.</location>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of CoNLL-2008, pages 159–177, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
</authors>
<title>Online graph planarisation for synchronous parsing of semantic and syntactic dependencies.</title>
<date>2009</date>
<booktitle>In IJCAI-2009,</booktitle>
<location>Pasadena, California, USA.</location>
<contexts>
<context position="28239" citStr="Titov et al., 2009" startWordPosition="4474" endWordPosition="4477">, the results do show that the argument traverse scheme synPth always outperforms the other UnPth. The result of this comparison partially shows that an integrated semantic role labeler is sensitive to the order of how argument candidates are traversed to some extent. The performance given by synPth is compared to some other systems that participated in the CoNLL-2008 shared task. They were chosen among the 20 participating systems either because they held better results (the first four participants) or because they used some joint learning techniques (Henderson et al., 2008). The results of (Titov et al., 2009) that use the similar joint learning technique as (Henderson et al., 2008) are also included9. Results of these evaluations on the test set are in Table 6. Top three systems of CoNLL2008, (Johansson and Nugues, 2008; Ciaramita et al., 2008; Che et al., 2008), used SRL pipelines. In this work, we partially use the similar techniques (synPth) for our participation in the shared tasks of CoNLL-2008 and 2009 (Zhao and Kit, 2008; Zhao et al., 2009b; Zhao et al., 2009a). Here we report that all SRL sub-tasks are tackled in one integrated model, while the predicate disambiguation sub-task was perform</context>
<context position="29764" citStr="Titov et al., 2009" startWordPosition="4724" endWordPosition="4727">-the-art by the pipeline system of (Johansson and Nugues, 2008) as the same syntactic structure input is adopted. It is worth noting that our system actually competes with two independent sub-systems of (Johansson and Nugues, 2008), one for verbal predicates, the other for nominal predicates. In addition, the results of our system is obtained without using additional joint learning technique like syntactic-semantic reranking. It indicates that our system is expected to obtain some further performance improvement by using such techniques. 9In addition, the work of (Henderson et al., 2008) and (Titov et al., 2009) jointly considered syntactic and semantic dependencies, that is significantly different from the others. 6 Conclusion We have described a dependency-based semantic role labeling system for English from NomBank and PropBank. From the evaluations, the result of our system is quite close to the state of the art. As to our knowledge, it is the first integrated SRL system that achieves such a competitive performance against previous pipeline systems. According to the path that the word-pair classifier traverses argument candidates, two integration schemes are presented. Argument candidate pruning </context>
</contexts>
<marker>Titov, Henderson, Merlo, Musillo, 2009</marker>
<rawString>Ivan Titov, James Henderson, Paola Merlo, and Gabriele Musillo. 2009. Online graph planarisation for synchronous parsing of semantic and syntactic dependencies. In IJCAI-2009, Pasadena, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005,</booktitle>
<pages>589--596</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="10657" citStr="Toutanova et al., 2005" startWordPosition="1734" endWordPosition="1737">th a tunable Gaussian prior as usual. Our implementation of the model adopts L-BFGS algorithm for parameter optimization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2, lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3. Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm, and rn), and h</context>
<context position="13011" citStr="Toutanova et al., 2005" startWordPosition="2128" endWordPosition="2131">will be dpPathShare. Assume that dpPathShare starts from a node r&apos;, then dpPathPred is from the predicate to r&apos;, and dpPathArgu is from the argument to r&apos;. Family. Two types of children sets for the predicate or argument candidate are considered, the 2In CoNLL-2008, Treebank tokens are split at the position that a hyphen (-) or a forward slash (/) occurs. This leads to two types of feature columns, non-split and split. 3Lemma and pos for either training or test are from automatically pre-analyzed columns in the input files. 4Note that the meaning of support verb is slightly different between (Toutanova et al., 2005) and (Xue, 2006; Jiang and Ng, 2006) 32 first includes all syntactic children (children), the second also includes all but excludes the left most and the right most children (noFarChildren). Concatenation of Elements. For all collected elements according to linePath, children and so on, we use three strategies to concatenate all those strings to produce the feature value. The first is seq, which concatenates all collected strings without doing anything. The second is bag, which removes all duplicated strings and sort the rest. The third is noDup, which removes all duplicated neighbored strings</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2005. Joint learning improves semantic role labeling. In Proceedings of ACL-2005, pages 589–596, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP-2004,</booktitle>
<pages>88--94</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2261" citStr="Xue and Palmer, 2004" startWordPosition="326" endWordPosition="329">tion and classification. If the predicate is unknown, then a predicate identification or disambiguation subtask should be additionally considered. A pipeline framework is usually adopted to handle all these sub-tasks. The reason to divide the whole task This study is partially supported by CERG grant 9040861 (CityU 1318/03H), CityU Strategic Research Grant 7002037. into multiple stages is two-fold, one is each subtask asks for its favorable features, the other is at the consideration of computational efficiency. Generally speaking, a joint system is slower than a pipeline system in training. (Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBan</context>
<context position="7939" citStr="Xue and Palmer, 2004" startWordPosition="1276" endWordPosition="1279">sequence must be considered by the classifier, leading to poor computational efficiency and unnecessary performance loss. Thus, the training sample for SRL task needs to be pruned properly. We use a simple strategy to prune predicate candidates, namely, only verbs and nouns are chosen in this case. There are two paths to collect argument candidates over the sequence. One is based on an input syntactic dependency tree, the other is based on a linear path of the sentence. As for the former (hereafter it is referred to synPth), we continue to use a dependency version of the pruning algorithm of (Xue and Palmer, 2004). The pruning algorithm is readdressed as the following. Initialization: Set the given predicate as the current node; (1) The current node and all of its syntactic children are selected as argument candidates (children are traversed from left to right.). (2) Reset the current node to its syntactic head and repeat step (1) until the root is reached. Note that this pruning algorithm is slightly different from that of (Xue and Palmer, 2004), the predicate itself is also included in the argument candidate list as the nominal predicate sometimes takes itself as its argument. The above pruning algor</context>
<context position="27052" citStr="Xue and Palmer, 2004" startWordPosition="4279" endWordPosition="4282">t candidates using assistant labels does not change this rate. This ratio only depends on which path, either synPth or UnPth, is chosen, and how good the syntactic input is (if synPth is the case). From the results, we see that more than a half of argument candidates can be effectively pruned for synPth and even 2/3 for UnPth. As mentioned by (Pradhan et al., 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of (Xue and Palmer, 2004). The results also indicate that such an assumption holds that arguments trend to close with their predicate, at either type of distance, syntactic or linear. Based on different syntactic inputs, we obtain different results on semantic dependency parsing 36 as shown in Table 5. These results on different syntactic inputs also give us a chance to observe how semantic performance varies according to syntactic performance. The fact from the results is that the ratio Sem-F1/LAS becomes relatively smaller as the syntactic input becomes better. Though not so surprised, the results do show that the a</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of EMNLP-2004, pages 88–94, Barcelona, Spain, July 25-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Semantic role labeling of nominalized predicates in chinese.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL-2006,</booktitle>
<pages>431--438</pages>
<location>New York City, USA,</location>
<contexts>
<context position="3510" citStr="Xue, 2006" startWordPosition="526" endWordPosition="527">al predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank. Though aiming at Chinese SRL, (Xue, 2006) reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance. From the results of CoNLL-2008 shared task, the top system by (Johansson and Nugues, 2008) also used two 30 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 30–39, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP different subsystems to handle verbal and nominal predicates, respectively. Despite all the above facts, an integrated SRL system still hol</context>
<context position="11660" citStr="Xue, 2006" startWordPosition="1894" endWordPosition="1895">spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3. Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm, and rn), and high(low) support verb or noun. We explain the last item, support verb(noun). From a given word to the syntactic root along the syntactic tree, the first verb/noun/preposition that is met is called as its low support verb/noun/preposition, and the nearest one to the root is called as its high support verb/noun/preposition. The concept of support verb was broadly used (Toutanova et al., 2005; Xue, 2006; Jiang and Ng, 2006)4, we here extend it to nouns and prepositions. In addition, we introduce a slightly modified syntactic head, pphead, it returns the left most sibling of a given word if the word is headed by a preposition, otherwise it returns the original head. Path. There are two basic types of path between the predicate and the argument candidates. One is the linear path (linePath) in the sequence, the other is the path in the syntactic parsing tree (dpPath). For the latter, we further divide it into four sub-types with respect to the syntactic root, dpPath is the full path in the synt</context>
<context position="13026" citStr="Xue, 2006" startWordPosition="2133" endWordPosition="2134">hat dpPathShare starts from a node r&apos;, then dpPathPred is from the predicate to r&apos;, and dpPathArgu is from the argument to r&apos;. Family. Two types of children sets for the predicate or argument candidate are considered, the 2In CoNLL-2008, Treebank tokens are split at the position that a hyphen (-) or a forward slash (/) occurs. This leads to two types of feature columns, non-split and split. 3Lemma and pos for either training or test are from automatically pre-analyzed columns in the input files. 4Note that the meaning of support verb is slightly different between (Toutanova et al., 2005) and (Xue, 2006; Jiang and Ng, 2006) 32 first includes all syntactic children (children), the second also includes all but excludes the left most and the right most children (noFarChildren). Concatenation of Elements. For all collected elements according to linePath, children and so on, we use three strategies to concatenate all those strings to produce the feature value. The first is seq, which concatenates all collected strings without doing anything. The second is bag, which removes all duplicated strings and sort the rest. The third is noDup, which removes all duplicated neighbored strings. We address so</context>
</contexts>
<marker>Xue, 2006</marker>
<rawString>Nianwen Xue. 2006. Semantic role labeling of nominalized predicates in chinese. In Proceedings of NAACL-2006, pages 431–438, New York City, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Parsing syntactic and semantic dependencies with two single-stage maximum entropy models.</title>
<date>2008</date>
<booktitle>In Proceeding of CoNLL2008,</booktitle>
<pages>203--207</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="5694" citStr="Zhao and Kit, 2008" startWordPosition="880" endWordPosition="883">re selection algorithm, a large-scale feature engineering is performed on a much larger feature template set than that in previous work. This helps us find features that may be of benefit to all SRL sub-tasks as long as possible. As two optimal feature template sets have been proven available, for the first time we report that an integrated SRL system may provide a result close to the state-of-the-art achieved by those SRL pipelines or individual systems for some specific predicates. 2 Adaptive Argument Pruning A word-pair classification is used to formulate semantic dependency parsing as in (Zhao and Kit, 2008). As for predicate identification or disambiguation, the first word is set as a virtual root (which is virtually set before the beginning of the sentence.) and the second as a predicate candidate. As for argument identification/classification, the first word in a word pair is specified as a predi1CoNLL-2008 is an English-only task, while CoNLL2009 is a multilingual one. Though the English corpus in CoNLL-2009 is almost identical to the corpus in the CoNLL2008 shared task evaluation, the latter holds more sophisticated input structure as in (Surdeanu et al., 2008). The most difference for these</context>
<context position="26000" citStr="Zhao and Kit (2008)" startWordPosition="4101" endWordPosition="4104">r Path Adaptive Pruning Coverage Rate /wo /w Gold synPth 2.13M 1.05M 98.4% (49.30%) linPth 5.29M 1.57M 100.0% (29.68%) Johansson synPth 2.15M 1.06M 95.4% (49.30%) linPth 5.28M 1.57M 100.0% (29.73%) MSTME synPth 2.15M 1.06M 95.0% (49.30%) linPth 5.29M 1.57M 100.0% (29.68%) Table 4: The number of training samples on argument candidates synPth+FT3,n linPth+FT�in Syn-Parser LAS Sem Sem-Fl Sem Sem-Fl Fl /LAS Fl /LAS MSTME 88.39 80.53 91.10 79.83 90.31 Johansson 89.28 80.94 90.66 79.84 89.43 Gold 100.00 84.57 84.57 83.34 83.34 Table 5: Semantic Labeled F1 based parser based on maximum entropy as in Zhao and Kit (2008). We still use the similar feature notations of that work. 5.2 The Results At first, we report the effectiveness of the proposed adaptive argument pruning. The numbers of argument candidates are in Table 4. The statistics is conducted on three different syntactic inputs. The coverage rate in the table means the ratio of how many true arguments are covered by the selected pruning scheme. Note that the adaptive pruning of argument candidates using assistant labels does not change this rate. This ratio only depends on which path, either synPth or UnPth, is chosen, and how good the syntactic input</context>
<context position="28666" citStr="Zhao and Kit, 2008" startWordPosition="4548" endWordPosition="4551">systems either because they held better results (the first four participants) or because they used some joint learning techniques (Henderson et al., 2008). The results of (Titov et al., 2009) that use the similar joint learning technique as (Henderson et al., 2008) are also included9. Results of these evaluations on the test set are in Table 6. Top three systems of CoNLL2008, (Johansson and Nugues, 2008; Ciaramita et al., 2008; Che et al., 2008), used SRL pipelines. In this work, we partially use the similar techniques (synPth) for our participation in the shared tasks of CoNLL-2008 and 2009 (Zhao and Kit, 2008; Zhao et al., 2009b; Zhao et al., 2009a). Here we report that all SRL sub-tasks are tackled in one integrated model, while the predicate disambiguation sub-task was performed individually in both of our previous systems. Therefore, this is our first attempt at a full integrated SRL system. (Titov et al., 2009) reported the best result by using joint learning technique up to now. The comparison indicates that our integrated system outputs a result quite close to the state-of-the-art by the pipeline system of (Johansson and Nugues, 2008) as the same syntactic structure input is adopted. It is w</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and semantic dependencies with two single-stage maximum entropy models. In Proceeding of CoNLL2008, pages 203–207, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>61--66</pages>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="4715" citStr="Zhao et al., 2009" startWordPosition="717" endWordPosition="720">em still holds some sort of merits, being easier to implement, a single-stage feature selection benefiting the whole system, an all-in-one model outputting all required semantic role information and so on. The shared tasks at the CoNLL 2008 and 2009 are devoted to the joint learning of syntactic and semantic dependencies, which show that SRL can be well performed using only dependency syntax input. Using data and evaluation settings of the CoNLL-2008 shared task, this work will only focus on semantic dependency parsing and compares the best-performing SRL system in the CoNLL-2009 shared Task (Zhao et al., 2009b) with those in the CoNLL-2008 shared task (Surdeanu et al., 2008; Hajiˇc et al., 2009)1. Aiming at main drawbacks of an integrated approach, two key techniques will be applied. 1) Assistant argument labels are introduced for the further improvement of argument pruning. This helps the development of a fast and lightweight SRL system. 2) Using a greedy feature selection algorithm, a large-scale feature engineering is performed on a much larger feature template set than that in previous work. This helps us find features that may be of benefit to all SRL sub-tasks as long as possible. As two opt</context>
<context position="28685" citStr="Zhao et al., 2009" startWordPosition="4552" endWordPosition="4555">se they held better results (the first four participants) or because they used some joint learning techniques (Henderson et al., 2008). The results of (Titov et al., 2009) that use the similar joint learning technique as (Henderson et al., 2008) are also included9. Results of these evaluations on the test set are in Table 6. Top three systems of CoNLL2008, (Johansson and Nugues, 2008; Ciaramita et al., 2008; Che et al., 2008), used SRL pipelines. In this work, we partially use the similar techniques (synPth) for our participation in the shared tasks of CoNLL-2008 and 2009 (Zhao and Kit, 2008; Zhao et al., 2009b; Zhao et al., 2009a). Here we report that all SRL sub-tasks are tackled in one integrated model, while the predicate disambiguation sub-task was performed individually in both of our previous systems. Therefore, this is our first attempt at a full integrated SRL system. (Titov et al., 2009) reported the best result by using joint learning technique up to now. The comparison indicates that our integrated system outputs a result quite close to the state-of-the-art by the pipeline system of (Johansson and Nugues, 2008) as the same syntactic structure input is adopted. It is worth noting that ou</context>
</contexts>
<marker>Zhao, Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>Hai Zhao, Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009a. Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5, pages 61–66, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Wenliang Chen</author>
<author>Chunyu Kit</author>
<author>Guodong Zhou</author>
</authors>
<title>Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL-2009,</booktitle>
<pages>55--60</pages>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="4715" citStr="Zhao et al., 2009" startWordPosition="717" endWordPosition="720">em still holds some sort of merits, being easier to implement, a single-stage feature selection benefiting the whole system, an all-in-one model outputting all required semantic role information and so on. The shared tasks at the CoNLL 2008 and 2009 are devoted to the joint learning of syntactic and semantic dependencies, which show that SRL can be well performed using only dependency syntax input. Using data and evaluation settings of the CoNLL-2008 shared task, this work will only focus on semantic dependency parsing and compares the best-performing SRL system in the CoNLL-2009 shared Task (Zhao et al., 2009b) with those in the CoNLL-2008 shared task (Surdeanu et al., 2008; Hajiˇc et al., 2009)1. Aiming at main drawbacks of an integrated approach, two key techniques will be applied. 1) Assistant argument labels are introduced for the further improvement of argument pruning. This helps the development of a fast and lightweight SRL system. 2) Using a greedy feature selection algorithm, a large-scale feature engineering is performed on a much larger feature template set than that in previous work. This helps us find features that may be of benefit to all SRL sub-tasks as long as possible. As two opt</context>
<context position="28685" citStr="Zhao et al., 2009" startWordPosition="4552" endWordPosition="4555">se they held better results (the first four participants) or because they used some joint learning techniques (Henderson et al., 2008). The results of (Titov et al., 2009) that use the similar joint learning technique as (Henderson et al., 2008) are also included9. Results of these evaluations on the test set are in Table 6. Top three systems of CoNLL2008, (Johansson and Nugues, 2008; Ciaramita et al., 2008; Che et al., 2008), used SRL pipelines. In this work, we partially use the similar techniques (synPth) for our participation in the shared tasks of CoNLL-2008 and 2009 (Zhao and Kit, 2008; Zhao et al., 2009b; Zhao et al., 2009a). Here we report that all SRL sub-tasks are tackled in one integrated model, while the predicate disambiguation sub-task was performed individually in both of our previous systems. Therefore, this is our first attempt at a full integrated SRL system. (Titov et al., 2009) reported the best result by using joint learning technique up to now. The comparison indicates that our integrated system outputs a result quite close to the state-of-the-art by the pipeline system of (Johansson and Nugues, 2008) as the same syntactic structure input is adopted. It is worth noting that ou</context>
</contexts>
<marker>Zhao, Chen, Kit, Zhou, 2009</marker>
<rawString>Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong Zhou. 2009b. Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing. In Proceedings of CoNLL-2009, pages 55–60, Boulder, Colorado, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>