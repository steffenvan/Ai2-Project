<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000232">
<sectionHeader confidence="0.6008135" genericHeader="abstract">
PREFACE
1 THE HISTORY OF THIS VOLUME
</sectionHeader>
<bodyText confidence="0.994520560439561">
User modeling is now approaching its second decade.
Its exact age, however, is difficult to determine. Was it
born in 1978, when Allen, Cohen, and Perrault started
to publish their seminal papers on dialog processing in
natural language systems based on Searle&apos;s theory of
speech acts and the speaker&apos;s intentions behind it? Or
was it in 1979, when Elain Rich reported on her
GRUNDY system, which contained an explicit repre-
sentation of what personality traits influence users&apos;
preferences of books to read, and built a model of its
current user by drawing assumptions as to the degree to
which these personality traits apply to the specific user,
for being able to give better advice?
There were predecessors to these systems, though,
which certainly do not deserve to be neglected. Hayes
and Rosner (1976) envisaged a dialog system that would
play the role of a curious party guest and try to find out
as much as possible about the beliefs of other party
guests. In the course of this fictitious dialog, the system
built a model of the dialog partner. In 1979, Power
published a report on a system dating back to 1974
which simulates a cooperative dialog between two
agents trying to achieve some simple real-world task.
Their dialog planning is based on a rudimentary model
of the beliefs and goals of the other agents.
The idea that, for understanding (the intentions be-
hind) natural language utterances, &amp;quot;it is necessary to
have a model of the beliefs of others&amp;quot; probably dates
back within Alto papers by Bruce (Bruce and Schmidt
1974, Bruce 1975). Its roots, however, can certainly be
traced far back within philosophy, at least to Leibniz
and Locke (see e.g., their epigone Christian Freyherr
von Wolff, 1712: &amp;quot;Wenn also zwey Personen mitei-
nander reden, und einer den andern verstehen soll; so
wird erfordert, 1. daB der, so da redet, bey einem jeden
Worte sich etwas gedencken konne: 2. daB der, so ihn
reden horet, eben dasjenige sich bey einem jeden Worte
gedenken kan, was der andere dencket.&amp;quot;).
Now, after 10 years of user modeling, it is certainly
acknowledged in artificial intelligence that, in order to
be capable of exhibiting pragmatically correct dialog
behavior, an Al system must include a model of the user
containing assumptions about his/her background
knowledge as well as his/her goals and plans in consult-
ing the system. Research in the field of user models
investigates how such assumptions can be automatically
created, represented, and exploited by the system in the
course of interaction with the user.
A dozen major and several more minor user modeling
systems have been designed and implemented in the last
decade, mostly in the context of natural language dialog
systems. The goal of UM86, the first international
workshop on user modeling, was to bring together the
researchers working on these projects, so that results
could be discussed and analyzed, and hopefully general
insights be found, that could prove useful for future
research. The meeting took place in Maria Laach, a
small village some 40 miles south of Bonn, West Ger-
many. Twenty-five prominent researchers had been
invited to participate. The pleasant setting of the con-
ference site close to the medieval abbey of Maria Laach
and the volcanic Lake Laach fostered a nice atmo-
sphere for intensive discussions and the exchange of
ideas until the early hours of the morning.
What were the direct results of the workshop? It was
agreed that a user model is &amp;quot;merely&amp;quot; a special case of
the more general concept of &amp;quot;agent model&amp;quot; (for further
details on all results mentioned here, see Kass and
Finin&apos;s survey paper in this volume). In certain conver-
sational settings, it will be necessary for a dialog system
to maintain both a model of its current user and of other
agents being talked about, though in most cases a model
of the user alone is sufficient. Several classification
criteria for user models have been proposed and dis-
cussed which are based on the nature of the task domain
and the conversational role that the system is supposed
to fill. No consensus could be reached in a lively
discussion on the relationship between user models and
so-called &amp;quot;discourse models&amp;quot;. Opinions ranged from
regarding these notions as being distinct both on a
conceptual and an implementational level to claiming
that user models subsume discourse models at both
levels.
It was agreed that two forms of publication should
result from the meeting: a number of papers that are
directly related to natural language should appear in a
special issue of the CL journal, and a more general
survey book on user modeling should be published by
Springer. Moreover, the discussion on the relationship
between user models and discourse models which
popped up at the workshop should be made available in
</bodyText>
<footnote confidence="0.8865506">
Copyright 1988 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided
that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To
copy otherwise, or to republish, requires a fee and/or specific permission.
0362-613X/ 88 /01000-0$03.00
Computational Linguistics, Volume 14, Number 3, September 1988 1
</footnote>
<subsectionHeader confidence="0.425629">
Kobsa and Wahlster Preface
</subsectionHeader>
<bodyText confidence="0.999174769230769">
printed form (see the discussion section in this issue,
and its special introduction).
All papers were subjected to an unusually extensive
reviewing procedure: Speakers had to distribute tenta-
tive versions of their papers long before the workshop,
which were then discussed at the meeting. Tape record-
ings of these discussions were made and distributed to
all speakers, and written reviews of each paper were
prepared by all participants. Selected speakers were
then invited to submit their revised and extended papers
to the special CL issue. The CL reviewers and the
editors added additional comments which the authors
had to take into account.
</bodyText>
<sectionHeader confidence="0.909444" genericHeader="method">
2 SURVEY OF THE CONTRIBUTIONS TO THIS ISSUE
</sectionHeader>
<bodyText confidence="0.99799805">
The volume starts with an excellent survey of the field
of user modeling in natural language dialog systems by
Robert Kass and Tim Finin. The authors present defi-
nitions for the concept of &amp;quot;user model&amp;quot; and the more
general concept of &amp;quot;agent model&amp;quot;. They make sugges-
tions as to what should be regarded as the content of a
user model, and list a number of dimensions along
which user models can be classified. Several techniques
are discussed as to how the system can acquire assump-
tions about the user&apos;s goals, plans, and beliefs in the
course of the dialog. Finally, for assessing the costs and
benefits of integrating a user model into an application
system to be developed, a number of relevant dimen-
sions are proposed along which interaction with the
system can be classified.
The order of the following papers reflects the proc-
essing sequence in user modeling systems, starting with
papers that focus on the recognition of beliefs and goals
of the user, and ending up with papers that focus on the
exploitation of these assumptions.
The contribution by Sandra Carberry is concerned
with the recognition of users&apos; plans in an ongoing dialog.
In her model, a number of domain-independent heuris-
tics are used for inferring candidate goals and plans
from user statements (these heuristics are extensions of
inference rules proposed by Allen and Perrault 1980,
Allen 1983). Focusing strategies are then employed for
selecting the candidate goals and plans most related to
the current dialog context and for integrating them into
a model of the user&apos;s actual plan. These strategies are
based on recent research on dialog organization and
focusing done, for example, by McKeown (1985a,b) and
Grosz and Sidner (1986).
Even before all its details have been determined, the
assumed user plan can be exploited for generating
expectations about dialog contributions of the user.
Carberry&apos;s model uses this potential for handling two
forms of linguistically problematic dialog contributions,
namely pragmatically ill-formed sentences and intersen-
tential ellipsis (only the former is described in this
contribution, however). Pragmatically ill-formed user
sentences, in her definition, are syntactically and se-
mantically correct, but do not fit the system&apos;s model
about the domain. Carberry discusses pragmatic ill-
formedness resulting from attribute confusions and
missing specifications in a user&apos;s question, and pro-
poses repair strategies based on the expectations from
the user model.
Up till now, nearly all plan recognition models have
assumed that the system&apos;s model of the user&apos;s plan is
perhaps incomplete, but never wrong. In a final section,
Carberry discusses how her analysis could be extended
to be able to detect clues indicating disparities between
the system&apos;s model of the user and the user&apos;s actual
goals and plans, reasoning on the system&apos;s model and
the system&apos;s domain knowledge to form hypotheses as
to the source of these disparities, responding to the user
and negotiating with him/her to isolate the errors, and
appropriately repairing the user model.
The advisor model of Alex Quilici, Michael Dyer,
and Margot Flowers presents an explanation-based ap-
proach to the problem of recognizing and responding to
user misconceptions. Their model assumes that a num-
ber of user beliefs and goals have been recognized
through analysis of the user&apos;s natural language reports
about problems in the use of UNIX, and entered into
the user model. The modeled beliefs concern applica-
bility conditions, enablements, and effects of UNIX
commands.
For each of the three belief types discerned, a
number of explanation patterns exist for determining
why beliefs of that type might not be held by the system.
For example, the system&apos;s disbelief in action Ap causes
state Sp can be explained by the beliefs Ap causes So
and Ao causes Sp, i.e., Ap has a different effect and Sp
is achieved by a different action. When a recognized
user belief is not shared by the system, the explanation
patterns pertaining to the specific belief type are
matched with the system&apos;s knowledge about UNIX in
order to find an explanation for why the system does not
hold this belief. The explanation thus discovered may
then be forwarded to the user.
A frequent type of user misconception is the misclas-
sification of objects (i.e., their subsumption under a
wrong superconcept in a concept hierarchy). Another
type is the belief that objects have certain attributes or
attribute values that they do not actually have. Kathleen
McCoy&apos;s contribution deals with how to behave after
such misconceptions have been detected and classified
as belonging to one of these categories. For each type of
user misconception, she proposes three strategies for
generating appropriate responses. The generated re-
sponse patterns are then transformed into natural lan-
guage responses using McDonald&apos;s (1980) MUMBLE
system.
In all of these strategies, the similarity of objects
plays an important role. Similarity of objects is defined
via the similarity of the attributes of these objects.
However, not all attributes are taken into account, but
only those which are salient in the current dialog
</bodyText>
<page confidence="0.925875">
2 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
<note confidence="0.493579">
Kobsa and Wahlster Preface
</note>
<bodyText confidence="0.999916986666667">
context. McCoy discerns three types of salience, each
stemming from a different source: static salience en-
coded directly in the generalization hierarchy, dynamic
salience gained by an attribute being explicitly men-
tioned in a discourse, and perspective salience gained
by viewing the domain objects through the active per-
spective.
Perspective salience turns out not to be a simple
substructure of the concept hierarchy, but to form an
independent structure (namely a set of highlighted
attributes), which is orthogonal to the concept hierar-
chy. Detailed examples from the field of financial advice
are given to illustrate the effect of perspective on
attribute salience and on one of the above-mentioned
response strategies. Suggestions are made as to what
influences the choice of a perspective in a given dialog.
As is the case with McCoy, Cecile Paris&apos; model also
assumes that inferences have already been drawn about
the experience of the user with respect to the domain of
discourse. These assumptions are represented by a list
of those items in the system&apos;s knowledge base which are
known to the user, and by information about whether
the user understands the basic concepts underlying the
domain of discourse.
In Paris&apos; model, these assumptions are exploited for
generating user-adapted object descriptions. The author
compared a number of encyclopedias for children and
adults and found that different types of descriptions are
used in each of these two text types, which are obvi-
ously aimed at two different types of readership: for
readers who can be expected to possess some back-
ground knowledge about the domain, objects or con-
cepts are described in terms of their subparts and the
properties of these subparts. This pattern of description
has been identified in studies on text organization by
McKeown (1985a,b), who called it a &amp;quot;constituency
schema&amp;quot;. Descriptions for &amp;quot;naive&amp;quot; readers, on the
other hand, essentially describe the processes associ-
ated with the operation of the object. This pattern is
called a &amp;quot;process schema&amp;quot; by Paris. The author ex-
plains her findings by assuming that readers with back-
ground knowledge—in contrast to &amp;quot;naive&amp;quot; readers—
can be expected to figure out how parts fit together to
form an object capable of performing a function. Pro-
viding such information would thus contradict Grice&apos;s
(1975) maxim of quantity and should therefore be
avoided.
Paris&apos; analysis suggests that the user&apos;s level of do-
main knowledge affects not only the amount, but also
the kind of information provided in descriptions. The
author suggests that, based on a user model, dialog
systems should also employ these two distinct strategies
for generating descriptions that are well-adapted to the
user. Since users are hardly ever either completely
naive nor completely expert, she also suggest that a
dialog system should combine both strategies, for ex-
ample in describing complex physical systems: When-
ever the user model indicates that the user possesses
local expertise about an involved object that is to be
described, the constituency schema can be employed;
otherwise the process schema must be used. Paris
outlines the points in each description schema at which
a switch to the other schema is possible.
The proposed principles have been implemented by
the author in TAILOR, a system that—roughly—takes a
request for an object description as its input, accesses
the user model described above, and generates a con-
ceptual representation of an object description that is
adapted to the user&apos;s level of expertise. After its con-
cepts have been replaced by lexemes, this description is
passed on to a generator that unifies it with a functional
grammar to produce English sentences. The operation
of the program is illustrated by a detailed example. In a
final section, Paris briefly presents heuristics that might
be used to acquire the user model preassumed by her.
</bodyText>
<sectionHeader confidence="0.758049" genericHeader="method">
SOME WORDS ABOUT THE AUTHORS
</sectionHeader>
<bodyText confidence="0.966045372881356">
Sandra Carberry received a B.A. degree in mathematics
from Cornell University and an M.S. degree in com-
puter science from Rice University, and was a Member
of Technical Staff at Bell Telephone Laboratories. She
is now an assistant professor of computer and informa-
tion sciences at the University of Delaware, where she
received her Ph.D. in 1985. Portions of her paper are
based on this dissertation research. Her current inter-
ests are robust models of plan recognition and the
application of user models to the generation of helpful
responses.
Timothy W. Finin is a technical director at the
UNISYS Paoli Research Center. He received a mas-
ter&apos;s degree in electrical engineering from MIT, and
M.Sc. and Ph.D. degrees in computer science from the
University of Illinois at Urbana-Champaign. He did
research at MIT in the area of robotics and computer
vision, and was an assistant professor in the Depart-
ment of Computer and Information Science at the
University of Pennsylvania from 1980 to 1987. His
current research interests include knowledge represen-
tation, expert systems, and computational linguistics, as
well as their applications to intelligent user interfaces.
Robert Kass just finished his Ph.D. studies at the
University of Pennsylvania. His dissertation concerned
the implicit acquisition of user models. He is now
working at the Center for Machine Intelligence in Ann
Arbor, Michigan.
Kathleen McCoy is an assistant professor of com-
puter and information sciences at the University of
Delaware. She received her Ph.D. in computer science
from the University of Pennsylvania in 1985. Portions of
her paper are based on this dissertation research. Her
current interests center on contextual effects of previ-
ous discourse on representation of user models and
generation of subsequent responses.
Alex Quilici is currently completing his Ph.D. studies
at the University of California, Los Angeles. Michael G.
Computational Linguistics, Volume 14, Number 3, September 1988 3
Kobsa and Wahlster Preface
Dyer and Margot Flowers did their graduate work at
Yale University before joining the faculty at UCLA.
Quilici&apos;s research concerns automatically detecting and
correcting plan-oriented misconceptions that can occur
in argumentative dialogs. The current paper summa-
rizes aspects of this research involving the misconcep-
tions of novice computer users. This work is part of
ongoing research programs by Dyer and Flowers in
argumentation, user modeling, language acquisition,
and connectionist models for language comprehension.
Cecile L. Paris received her bachelor&apos;s degree from
the University of California at Berkeley and her Ph.D.
in computer science from Columbia University. She is
now working at the Information Sciences Institute,
continuing her research on natural language generation
and user modeling. Her thesis work has focused on how
a system should tailor a response depending on how
much the user knows about the domain under consid-
eration. Her paper reports major results from this work.
</bodyText>
<sectionHeader confidence="0.986146" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.996865666666667">
The preparation of the Maria Laach workshop and the production of
this publication was supported by a number of individuals or institu-
tions, to which we would like to express our gratitude:
</bodyText>
<listItem confidence="0.943673333333333">
• The Collaborative Research Programme on Artificial Intelligence
and Knowledge-Based Systems (SFB 314) of the German Science
Foundation (DFG), which funded the workshop;
• Mark Line and Bernd Schafer, who prepared transcripts and
rendered the editors&apos; English intelligible even to native speakers;
• Bernd Nessen and Sokrates Evangelidis for their work as sound
engineers; and
• Doris Borchers, Gabriele Jacquinot, Johannes Reinert, and Parinaz
Mohammadzadeh for their administrative assistance.
</listItem>
<subsectionHeader confidence="0.30981">
The international user modeling community owes a lot to them.
</subsectionHeader>
<bodyText confidence="0.256651">
Alfred Kobsa, Wolfgang Wahlster
</bodyText>
<sectionHeader confidence="0.955272" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.994843387755102">
Allen, .1.1F. and Perrault, C.R. 1978 Participating in Dialogues: Under-
standing via Plan Deduction. AI-Memo 78-4, University of To-
ronto, Canada.
Allen, J.F. and Perrault, C.R. 1980 Analyzing Intention in Utterances.
Artificial Intelligence 15: 143-178.
Allen, J .F. 1983 Recognizing Intentions from Natural Language
Utterances. In Brady, M., and Berwick, R.C. (eds.) Computa-
tional Models of Discourse. MIT Press, Cambridge, MA.
Bruce, 13., and Schmidt, C.F. 1974 Episode Understanding and
Belief-Guided Parsing. NIH Report, CMB-TR-32, Department of
Computer Science, Rutgers University, New Brunswick, NJ.
Bruce, B. 1975 Belief Systems and Language Understanding. BBN
Report 2973 (Artificial Intelligence Report 21). Bolt, Beranek, and
Newman, Inc., Cambridge, MA.
Cohen, P.R. 1978 On Knowing What to Say: Planning Speech Acts.
TR-118, Department of Computer Science, University of Toronto,
Canada.
Cohen, P.R. and Perrault, C.R. 1979 Elements of a Plan-Based
Theory of Speech Acts. Cognitive Science 3: 177-212.
Grice, H .P. 1975 Logic and Conversation. In Cole, P. and Morgan, J.
(eds.) Syntax and Semantics 3: Speech Acts. Academic Press,
New York, NY.
Grosz, B.J. and Sidner, C.L. 1986 Attention, Intentions, and the
Structure of Discourse. Computational Linguistics 12: 175-204.
Hayes, P.J. and Rosner, M.A. 1976 ULLY: a Program for Handling
Conversations. In Proceedings of the 1976 AISB Summer Confer-
ence, Edinburgh, Scotland: 137-147.
McDonald, D.D. 1980 Natural Language Production as a Process of
Decision Making under Constraints. Ph.D. thesis, MIT, Cam-
bridge, MA.
McKeown, K.R. 1985a Discourse Strategies for Generating Natural-
Language Text. Artificial Intelligence 27: 1-41.
McKeown, K.R. 1985b Text Generation: Using Discourse Strategies
and Focus Constraints to Generate Natural-Language Text. Cam-
bridge University Press, Cambridge, England.
Perrault, C.R. and Allen, J.F. 1980 A Plan-Based Analysis of Indirect
Speech Acts. American Journal of Computational Linguistics 6:
67-182.
Power, R. 1974 A Computer Model of Conversation. Ph.D. thesis,
University of Edinburgh, Scotland.
Power, R. 1979 The Organisation of Purposeful Dialogues. Linguistics
17: 107-152.
Rich, E. 1979a Building and Exploiting User Models. Ph.D. thesis,
Department of Computer Science, Carnegie-Mellon University,
Pittsburgh, PA.
Rich, E. 1979b User Modeling via Stereotypes. Cognitive Science 3:
329-345.
NOTE
1. Congenial translation: &amp;quot;Whenever two personnes, then, shall
</reference>
<bodyText confidence="0.956397666666667">
speake one to the other, such that each understand the other, so is
it of necessity that, in the first instance, he, that personne just
speaking, shall, for each and every word, think a thing; and that, in
the second instance, he, that personne hearing that which be
spoken by the other, shall, for each and every word, think just that
thing which was thought by the other.&amp;quot;
</bodyText>
<page confidence="0.773406">
4 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.235641">
<note confidence="0.588074">PREFACE 1 THE HISTORY OF THIS VOLUME</note>
<abstract confidence="0.995661846153847">User modeling is now approaching its second decade. Its exact age, however, is difficult to determine. Was it born in 1978, when Allen, Cohen, and Perrault started to publish their seminal papers on dialog processing in natural language systems based on Searle&apos;s theory of speech acts and the speaker&apos;s intentions behind it? Or was it in 1979, when Elain Rich reported on her GRUNDY system, which contained an explicit representation of what personality traits influence users&apos; preferences of books to read, and built a model of its current user by drawing assumptions as to the degree to which these personality traits apply to the specific user, for being able to give better advice? There were predecessors to these systems, though, which certainly do not deserve to be neglected. Hayes and Rosner (1976) envisaged a dialog system that would play the role of a curious party guest and try to find out as much as possible about the beliefs of other party guests. In the course of this fictitious dialog, the system built a model of the dialog partner. In 1979, Power published a report on a system dating back to 1974 which simulates a cooperative dialog between two agents trying to achieve some simple real-world task. Their dialog planning is based on a rudimentary model of the beliefs and goals of the other agents. The idea that, for understanding (the intentions behind) natural language utterances, &amp;quot;it is necessary to have a model of the beliefs of others&amp;quot; probably dates back within Alto papers by Bruce (Bruce and Schmidt 1974, Bruce 1975). Its roots, however, can certainly be traced far back within philosophy, at least to Leibniz and Locke (see e.g., their epigone Christian Freyherr von Wolff, 1712: &amp;quot;Wenn also zwey Personen miteinander reden, und einer den andern verstehen soll; so wird erfordert, 1. daB der, so da redet, bey einem jeden Worte sich etwas gedencken konne: 2. daB der, so ihn reden horet, eben dasjenige sich bey einem jeden Worte gedenken kan, was der andere dencket.&amp;quot;). Now, after 10 years of user modeling, it is certainly acknowledged in artificial intelligence that, in order to capable of exhibiting correct behavior, an Al system must include a model of the user containing assumptions about his/her background knowledge as well as his/her goals and plans in consulting the system. Research in the field of user models investigates how such assumptions can be automatically created, represented, and exploited by the system in the course of interaction with the user. A dozen major and several more minor user modeling systems have been designed and implemented in the last decade, mostly in the context of natural language dialog systems. The goal of UM86, the first international workshop on user modeling, was to bring together the researchers working on these projects, so that results could be discussed and analyzed, and hopefully general insights be found, that could prove useful for future research. The meeting took place in Maria Laach, a small village some 40 miles south of Bonn, West Germany. Twenty-five prominent researchers had been invited to participate. The pleasant setting of the conference site close to the medieval abbey of Maria Laach and the volcanic Lake Laach fostered a nice atmosphere for intensive discussions and the exchange of ideas until the early hours of the morning. What were the direct results of the workshop? It was agreed that a user model is &amp;quot;merely&amp;quot; a special case of the more general concept of &amp;quot;agent model&amp;quot; (for further details on all results mentioned here, see Kass and Finin&apos;s survey paper in this volume). In certain conversational settings, it will be necessary for a dialog system to maintain both a model of its current user and of other agents being talked about, though in most cases a model of the user alone is sufficient. Several classification criteria for user models have been proposed and discussed which are based on the nature of the task domain and the conversational role that the system is supposed to fill. No consensus could be reached in a lively discussion on the relationship between user models and so-called &amp;quot;discourse models&amp;quot;. Opinions ranged from regarding these notions as being distinct both on a conceptual and an implementational level to claiming that user models subsume discourse models at both levels. It was agreed that two forms of publication should result from the meeting: a number of papers that are directly related to natural language should appear in a issue of the and a more general survey book on user modeling should be published by Springer. Moreover, the discussion on the relationship between user models and discourse models which popped up at the workshop should be made available in</abstract>
<note confidence="0.842017428571428">Copyright 1988 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided the copies are not made for direct commercial advantage and the and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/ 88 /01000-0$03.00 Computational Linguistics, Volume 14, Number 3, September 1988 1 Kobsa and Wahlster Preface printed form (see the discussion section in this issue,</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>1 1F Allen</author>
<author>C R Perrault</author>
</authors>
<title>Participating in Dialogues: Understanding via Plan Deduction. AI-Memo 78-4,</title>
<date>1978</date>
<institution>University of Toronto, Canada.</institution>
<marker>Allen, Perrault, 1978</marker>
<rawString>Allen, .1.1F. and Perrault, C.R. 1978 Participating in Dialogues: Understanding via Plan Deduction. AI-Memo 78-4, University of Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Allen</author>
<author>C R Perrault</author>
</authors>
<title>Analyzing Intention in Utterances.</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<volume>15</volume>
<pages>143--178</pages>
<contexts>
<context position="7346" citStr="Allen and Perrault 1980" startWordPosition="1212" endWordPosition="1215">teraction with the system can be classified. The order of the following papers reflects the processing sequence in user modeling systems, starting with papers that focus on the recognition of beliefs and goals of the user, and ending up with papers that focus on the exploitation of these assumptions. The contribution by Sandra Carberry is concerned with the recognition of users&apos; plans in an ongoing dialog. In her model, a number of domain-independent heuristics are used for inferring candidate goals and plans from user statements (these heuristics are extensions of inference rules proposed by Allen and Perrault 1980, Allen 1983). Focusing strategies are then employed for selecting the candidate goals and plans most related to the current dialog context and for integrating them into a model of the user&apos;s actual plan. These strategies are based on recent research on dialog organization and focusing done, for example, by McKeown (1985a,b) and Grosz and Sidner (1986). Even before all its details have been determined, the assumed user plan can be exploited for generating expectations about dialog contributions of the user. Carberry&apos;s model uses this potential for handling two forms of linguistically problemat</context>
</contexts>
<marker>Allen, Perrault, 1980</marker>
<rawString>Allen, J.F. and Perrault, C.R. 1980 Analyzing Intention in Utterances. Artificial Intelligence 15: 143-178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Allen</author>
</authors>
<title>Recognizing Intentions from Natural Language Utterances.</title>
<date>1983</date>
<booktitle>Computational Models of Discourse.</booktitle>
<editor>In Brady, M., and Berwick, R.C. (eds.)</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7359" citStr="Allen 1983" startWordPosition="1216" endWordPosition="1217"> can be classified. The order of the following papers reflects the processing sequence in user modeling systems, starting with papers that focus on the recognition of beliefs and goals of the user, and ending up with papers that focus on the exploitation of these assumptions. The contribution by Sandra Carberry is concerned with the recognition of users&apos; plans in an ongoing dialog. In her model, a number of domain-independent heuristics are used for inferring candidate goals and plans from user statements (these heuristics are extensions of inference rules proposed by Allen and Perrault 1980, Allen 1983). Focusing strategies are then employed for selecting the candidate goals and plans most related to the current dialog context and for integrating them into a model of the user&apos;s actual plan. These strategies are based on recent research on dialog organization and focusing done, for example, by McKeown (1985a,b) and Grosz and Sidner (1986). Even before all its details have been determined, the assumed user plan can be exploited for generating expectations about dialog contributions of the user. Carberry&apos;s model uses this potential for handling two forms of linguistically problematic dialog con</context>
</contexts>
<marker>Allen, 1983</marker>
<rawString>Allen, J .F. 1983 Recognizing Intentions from Natural Language Utterances. In Brady, M., and Berwick, R.C. (eds.) Computational Models of Discourse. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F Schmidt</author>
</authors>
<title>Episode Understanding and Belief-Guided Parsing.</title>
<date>1974</date>
<tech>NIH Report, CMB-TR-32,</tech>
<institution>Department of Computer Science, Rutgers University,</institution>
<location>New Brunswick, NJ.</location>
<contexts>
<context position="1576" citStr="Schmidt 1974" startWordPosition="266" endWordPosition="267">ut the beliefs of other party guests. In the course of this fictitious dialog, the system built a model of the dialog partner. In 1979, Power published a report on a system dating back to 1974 which simulates a cooperative dialog between two agents trying to achieve some simple real-world task. Their dialog planning is based on a rudimentary model of the beliefs and goals of the other agents. The idea that, for understanding (the intentions behind) natural language utterances, &amp;quot;it is necessary to have a model of the beliefs of others&amp;quot; probably dates back within Alto papers by Bruce (Bruce and Schmidt 1974, Bruce 1975). Its roots, however, can certainly be traced far back within philosophy, at least to Leibniz and Locke (see e.g., their epigone Christian Freyherr von Wolff, 1712: &amp;quot;Wenn also zwey Personen miteinander reden, und einer den andern verstehen soll; so wird erfordert, 1. daB der, so da redet, bey einem jeden Worte sich etwas gedencken konne: 2. daB der, so ihn reden horet, eben dasjenige sich bey einem jeden Worte gedenken kan, was der andere dencket.&amp;quot;). Now, after 10 years of user modeling, it is certainly acknowledged in artificial intelligence that, in order to be capable of exhibi</context>
</contexts>
<marker>Schmidt, 1974</marker>
<rawString>Bruce, 13., and Schmidt, C.F. 1974 Episode Understanding and Belief-Guided Parsing. NIH Report, CMB-TR-32, Department of Computer Science, Rutgers University, New Brunswick, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bruce</author>
</authors>
<title>Belief Systems and Language Understanding.</title>
<date>1975</date>
<tech>BBN Report 2973 (Artificial Intelligence Report 21).</tech>
<location>Bolt, Beranek, and Newman, Inc., Cambridge, MA.</location>
<contexts>
<context position="1589" citStr="Bruce 1975" startWordPosition="268" endWordPosition="269"> of other party guests. In the course of this fictitious dialog, the system built a model of the dialog partner. In 1979, Power published a report on a system dating back to 1974 which simulates a cooperative dialog between two agents trying to achieve some simple real-world task. Their dialog planning is based on a rudimentary model of the beliefs and goals of the other agents. The idea that, for understanding (the intentions behind) natural language utterances, &amp;quot;it is necessary to have a model of the beliefs of others&amp;quot; probably dates back within Alto papers by Bruce (Bruce and Schmidt 1974, Bruce 1975). Its roots, however, can certainly be traced far back within philosophy, at least to Leibniz and Locke (see e.g., their epigone Christian Freyherr von Wolff, 1712: &amp;quot;Wenn also zwey Personen miteinander reden, und einer den andern verstehen soll; so wird erfordert, 1. daB der, so da redet, bey einem jeden Worte sich etwas gedencken konne: 2. daB der, so ihn reden horet, eben dasjenige sich bey einem jeden Worte gedenken kan, was der andere dencket.&amp;quot;). Now, after 10 years of user modeling, it is certainly acknowledged in artificial intelligence that, in order to be capable of exhibiting pragmati</context>
</contexts>
<marker>Bruce, 1975</marker>
<rawString>Bruce, B. 1975 Belief Systems and Language Understanding. BBN Report 2973 (Artificial Intelligence Report 21). Bolt, Beranek, and Newman, Inc., Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
</authors>
<title>On Knowing What to Say: Planning Speech Acts.</title>
<date>1978</date>
<tech>TR-118,</tech>
<institution>Department of Computer Science, University of Toronto, Canada.</institution>
<marker>Cohen, 1978</marker>
<rawString>Cohen, P.R. 1978 On Knowing What to Say: Planning Speech Acts. TR-118, Department of Computer Science, University of Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>C R Perrault</author>
</authors>
<title>Elements of a Plan-Based Theory of Speech Acts.</title>
<date>1979</date>
<journal>Cognitive Science</journal>
<volume>3</volume>
<pages>177--212</pages>
<marker>Cohen, Perrault, 1979</marker>
<rawString>Cohen, P.R. and Perrault, C.R. 1979 Elements of a Plan-Based Theory of Speech Acts. Cognitive Science 3: 177-212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Logic and Conversation.</title>
<date>1975</date>
<editor>In Cole, P. and Morgan, J. (eds.)</editor>
<publisher>Academic Press,</publisher>
<location>New York, NY.</location>
<marker>Grice, 1975</marker>
<rawString>Grice, H .P. 1975 Logic and Conversation. In Cole, P. and Morgan, J. (eds.) Syntax and Semantics 3: Speech Acts. Academic Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<date>1986</date>
<journal>Attention, Intentions, and the Structure of Discourse. Computational Linguistics</journal>
<volume>12</volume>
<pages>175--204</pages>
<contexts>
<context position="7700" citStr="Grosz and Sidner (1986)" startWordPosition="1268" endWordPosition="1271">h the recognition of users&apos; plans in an ongoing dialog. In her model, a number of domain-independent heuristics are used for inferring candidate goals and plans from user statements (these heuristics are extensions of inference rules proposed by Allen and Perrault 1980, Allen 1983). Focusing strategies are then employed for selecting the candidate goals and plans most related to the current dialog context and for integrating them into a model of the user&apos;s actual plan. These strategies are based on recent research on dialog organization and focusing done, for example, by McKeown (1985a,b) and Grosz and Sidner (1986). Even before all its details have been determined, the assumed user plan can be exploited for generating expectations about dialog contributions of the user. Carberry&apos;s model uses this potential for handling two forms of linguistically problematic dialog contributions, namely pragmatically ill-formed sentences and intersentential ellipsis (only the former is described in this contribution, however). Pragmatically ill-formed user sentences, in her definition, are syntactically and semantically correct, but do not fit the system&apos;s model about the domain. Carberry discusses pragmatic illformedne</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, B.J. and Sidner, C.L. 1986 Attention, Intentions, and the Structure of Discourse. Computational Linguistics 12: 175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
<author>M A Rosner</author>
</authors>
<title>ULLY: a Program for Handling Conversations.</title>
<date>1976</date>
<booktitle>In Proceedings of the 1976 AISB Summer Conference,</booktitle>
<pages>137--147</pages>
<location>Edinburgh, Scotland:</location>
<contexts>
<context position="844" citStr="Hayes and Rosner (1976)" startWordPosition="136" endWordPosition="139">inal papers on dialog processing in natural language systems based on Searle&apos;s theory of speech acts and the speaker&apos;s intentions behind it? Or was it in 1979, when Elain Rich reported on her GRUNDY system, which contained an explicit representation of what personality traits influence users&apos; preferences of books to read, and built a model of its current user by drawing assumptions as to the degree to which these personality traits apply to the specific user, for being able to give better advice? There were predecessors to these systems, though, which certainly do not deserve to be neglected. Hayes and Rosner (1976) envisaged a dialog system that would play the role of a curious party guest and try to find out as much as possible about the beliefs of other party guests. In the course of this fictitious dialog, the system built a model of the dialog partner. In 1979, Power published a report on a system dating back to 1974 which simulates a cooperative dialog between two agents trying to achieve some simple real-world task. Their dialog planning is based on a rudimentary model of the beliefs and goals of the other agents. The idea that, for understanding (the intentions behind) natural language utterances</context>
</contexts>
<marker>Hayes, Rosner, 1976</marker>
<rawString>Hayes, P.J. and Rosner, M.A. 1976 ULLY: a Program for Handling Conversations. In Proceedings of the 1976 AISB Summer Conference, Edinburgh, Scotland: 137-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D McDonald</author>
</authors>
<title>Natural Language Production as a Process of Decision Making under Constraints.</title>
<date>1980</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT,</institution>
<location>Cambridge, MA.</location>
<marker>McDonald, 1980</marker>
<rawString>McDonald, D.D. 1980 Natural Language Production as a Process of Decision Making under Constraints. Ph.D. thesis, MIT, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K R McKeown</author>
</authors>
<title>1985a Discourse Strategies for Generating NaturalLanguage Text.</title>
<journal>Artificial Intelligence</journal>
<volume>27</volume>
<pages>1--41</pages>
<marker>McKeown, </marker>
<rawString>McKeown, K.R. 1985a Discourse Strategies for Generating NaturalLanguage Text. Artificial Intelligence 27: 1-41.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K R McKeown</author>
</authors>
<title>1985b Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural-Language Text.</title>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<marker>McKeown, </marker>
<rawString>McKeown, K.R. 1985b Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural-Language Text. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Perrault</author>
<author>J F Allen</author>
</authors>
<title>A Plan-Based Analysis of Indirect Speech Acts.</title>
<date>1980</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>6</volume>
<pages>67--182</pages>
<marker>Perrault, Allen, 1980</marker>
<rawString>Perrault, C.R. and Allen, J.F. 1980 A Plan-Based Analysis of Indirect Speech Acts. American Journal of Computational Linguistics 6: 67-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Power</author>
</authors>
<title>A Computer Model of Conversation.</title>
<date>1974</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh,</institution>
<location>Scotland.</location>
<marker>Power, 1974</marker>
<rawString>Power, R. 1974 A Computer Model of Conversation. Ph.D. thesis, University of Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Power</author>
</authors>
<title>The Organisation of Purposeful Dialogues.</title>
<date>1979</date>
<journal>Linguistics</journal>
<volume>17</volume>
<pages>107--152</pages>
<marker>Power, 1979</marker>
<rawString>Power, R. 1979 The Organisation of Purposeful Dialogues. Linguistics 17: 107-152.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Rich</author>
</authors>
<title>1979a Building and Exploiting User Models.</title>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Carnegie-Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<marker>Rich, </marker>
<rawString>Rich, E. 1979a Building and Exploiting User Models. Ph.D. thesis, Department of Computer Science, Carnegie-Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Rich</author>
</authors>
<title>1979b User Modeling via Stereotypes.</title>
<journal>Cognitive Science</journal>
<volume>3</volume>
<pages>329--345</pages>
<marker>Rich, </marker>
<rawString>Rich, E. 1979b User Modeling via Stereotypes. Cognitive Science 3: 329-345.</rawString>
</citation>
<citation valid="false">
<authors>
<author>NOTE</author>
</authors>
<title>Congenial translation: &amp;quot;Whenever two personnes,</title>
<note>then, shall</note>
<marker>NOTE, </marker>
<rawString>NOTE 1. Congenial translation: &amp;quot;Whenever two personnes, then, shall</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>