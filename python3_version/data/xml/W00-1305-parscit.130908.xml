<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.984148">
Topic Analysis Using a Finite Mixture Model
</title>
<author confidence="0.81955">
Hang Li and Kenji Yamanishi
</author>
<affiliation confidence="0.714305">
NEC Corporation
</affiliation>
<email confidence="0.97424">
{lihang,yamanisi}@ccm.cl.nec.co.jp
</email>
<sectionHeader confidence="0.994344" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99935">
We address the issue of &apos;topic analysis,&apos; by
which is determined a text&apos;s topic structure,
which indicates what topics are included in a
text, and how topics change within the text.
We propose a novel approach to this issue, one
based on statistical modeling and learning.
We represent topics by means of word clusters,
and employ a finite mixture model to repre-
sent a word distribution within a text. Our
experimental results indicate that our method
significantly outperforms a method that com-
bines existing techniques.
</bodyText>
<sectionHeader confidence="0.998433" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966521126761">
We consider here the issue of &apos;topic analysis,&apos;
by which is determined a text&apos;s topic struc-
ture, which indicates what topics are included
in a text and how topics change within the
text. Topic analysis consists of two main
tasks: topic identification and text segmen-
tation (based on topic changes).
Topic analysis is extremely useful in a vari-
ety of text processing applications. For exam-
ple, it can be used in the automatic indexing
of texts for purposes of information retrieval.
With it, one can understand what the main
topics and subtopics of a text are, and where
those subtopics lie within the text.
To the best of our knowledge, however, no
previous study has so far dealt with the topic
analysis problem in the above sense. The
most closely related are key word extraction
and text segmentation. A keyword extrac-
tion method (e.g., that using tf-idf (Salton
and Yang, 1973)) generally extracts from a
text key words which represent topics within
the text, but it does not conduct segmenta-
tion. A segmentation method (e.g., TextTil-
ing (Hearst, 1997)) generally segments a text
into blocks (paragraphs) in accord with topic
changes within the text, but it does not iden-
tify (or label) by itself the topics discussed in
each of the blocks.
The purpose of this paper is to provide a
single framework for conducting topic analy-
sis, i.e., performing both topic identification
and text segmentation.
The key characteristics of our framework
are 1) representing a topic by means of a clus-
ter of words that are closely related to the
topic, and 2) employing a stochastic model,
called a finite mixture model (e.g., (Everitt
and Hand, 1981)), to represent a word dis-
tribution within a text. The finite mixture
model has a hierarchical structure of probabil-
ity distributions. The first level is a probabil-
ity distribution of topics (topic distribution).
The second level consists of probability distri-
butions of words included within topics (word
distributions). These word distributions are
linearly combined to represent a word distri-
bution within a text, with the topic distribu-
tion being used as the coefficient vector. Here-
after we refer to a finite mixture model hav-
ing this structure as a stochastic topic model
(STM).
Before conducting topic analysis, we create
word clusters (topics) on the basis of word co-
occurrence in corpus data. We have devel-
oped a new method for word clustering using
stochastic complexity (or the MDL principle)
(Rissanen, 1996).
In topic analysis, we estimate a sequence
of STMs that would have given rise to a given
text, assuming that each block of a text is gen-
erated by an individual STM. We perform text
segmentation by detecting significant differ-
ences between STMs and perform topic iden-
tification by means of estimation of STMs.
With the results, we obtain the text&apos;s topic
structure which consists of segmented blocks
and their topics.
It is possible to perform topic analysis
by combining an existing word extraction
method (e.g., tf-idf) and an existing text seg-
</bodyText>
<page confidence="0.998498">
35
</page>
<bodyText confidence="0.99994645">
mentation method (e.g., TextTiling). Specif-
ically, one can extract key words from a text
using tf-idf, view these extracted key words
as topics, segment the text into blocks us-
ing TextTiling, and estimate the distribution
of topics (key words) within each block. Ex-
perimental results indicate, however, that our
method significantly outperforms such a com-
bined method in topic identification and out-
performs it in text segmentation, because it
utilizes word cluster information and employs
a well-defined probability framework.
Finite mixture models have been employed
in a number of text processing applications,
such as text classification (e.g., (Li and Ya-
manishi, 1997; Nigam et al., 2000)) and infor-
mation retrieval (e.g., (Hofmann, 1999)). As
will be discussed, however, our definition of a
finite mixture model and the way we use it
here differs significantly.
</bodyText>
<sectionHeader confidence="0.99568" genericHeader="method">
2 Stochastic Topic Model
</sectionHeader>
<subsectionHeader confidence="0.759541">
2.1 Topic
</subsectionHeader>
<bodyText confidence="0.9321972">
While the term &apos;topic&apos; is used in different ways
in different linguistic theories, we simply view
it here as a subject within a text. We rep-
resent a topic by means of a cluster of words
that are closely related to the topic, assum-
ing that a cluster has a seed word (or several
seed words) which indicates a topic. Figure 1
shows an example topic with the word &apos;trade&apos;
being the seed word.
trade: trade export import tariff trader GATT protectionist
</bodyText>
<figureCaption confidence="0.999156">
Figure 1: Example topic
</figureCaption>
<subsectionHeader confidence="0.998629">
2.2 Definition of STM
</subsectionHeader>
<bodyText confidence="0.996366846153846">
Let W denote a set of words, and K a set of
topics. We first define a distribution of topics
(clusters) P(k)P(k) = 1. Then, for
each topic k E K, we define a probability dis-
tribution of words P(w1k) : Ewe,&apos; P(wk) =
1. Here the value of P(w1k) will be zero if w is
not included in k. We next define a Stochas-
tic Topic Model (STM) as a finite mixture
model, which is a linear combination of the
word probability distributions P(wik), with
the topic distribution P(k) being used as the
coefficient vector. The probability of word to
in W is, then,
</bodyText>
<equation confidence="0.9131825">
P(w) = E P(k)P(wk) tvEW.
kEic
</equation>
<figureCaption confidence="0.984471">
Figure 2 depicts an example STM.
Figure 2: Example STM
</figureCaption>
<bodyText confidence="0.999858388888889">
For the purposes of statistical modeling, it
is advantageous to conceive of a text (i.e., a
word sequence) as having been generated by
some &apos;true&apos; STMs, which we then seek to esti-
mate as closely as possible. A text may have a
number of blocks, and each block is assumed
to be generated by an individual STM. The
STMs within a text are assumed to have the
same set of topics, but have different param-
eter values.
From the linguistic viewpoint, a text gener-
ally focuses on a single main topic, but it may
discuss different subtopics in different blocks.
While a text is discussing any one topic, it will
more frequently use words strongly related to
that topic. Hence, STM is a natural represen-
tation of statistical word occurrence based on
topics.
</bodyText>
<sectionHeader confidence="0.970943" genericHeader="method">
3 Word Clustering
</sectionHeader>
<bodyText confidence="0.9998911">
Before conducting topic analysis, we create
word clusters using a large data corpus. More
precisely, we treat all words in a vocabulary as
seed words, and for each seed word we collect
from the data those words which frequently
co-occur with it and group them into a cluster.
As one example, the word-cluster in Figure 1
has been constructed with the word &apos;trade&apos; as
the seed word.
We have developed a new method for reli-
ably collecting frequently co-occurring words
on the basis of stochastic complexity, or the
MDL principle. For a given data sequence
= xi ... x, and for a fixed probability
model M,1 the stochastic complexity of xm
relative to M, which we denote as SC(xm :
M), is defined as the least code length re-
quired to encode xm with M (Rissanen, 1996).
SC(&amp;quot; : M) can be interpreted as the amount
information included in xn relative to M. The
</bodyText>
<footnote confidence="0.737555666666667">
&apos;Here, we use &apos;model&apos; to refer to a-probability dis-
tribution which has specified parameters but unspeci-
fied parameter values.
</footnote>
<page confidence="0.99858">
36
</page>
<bodyText confidence="0.997614666666667">
MDL (Minimum Description Length) princi-
ple is a model selection criterion which asserts
that, for a given data sequence, the lower a
model&apos;s SC value, the greater its likelihood of
being a model which would have actually gen-
erated the data. MDL has many good prop-
erties as a criterion for model selection.&apos;
For a fixed seed word s, we take a word w as
a frequently co-occurring word if the presence
of s is a statistically significant indicator of
the presence of w.
Let a data sequence: (s1, w1), (s2, w2), •
(sm, wm) be given where (si, wi) denotes the
state of co-occurrence of words s and w in
the i-th text in the corpus data. Here, si E
{1, 0}, wi E {1,0},(i = 1,•• ,m), 1 denotes
the presence of a word, while 0 the absence
of it. We further denote sm = Si • sm, and
</bodyText>
<equation confidence="0.9980665">
77L
W W1 • • • WM •
</equation>
<bodyText confidence="0.96493925">
Then as in (Rissanen, 1996), the SC value of
Wm relative to a model I in which the presence
or absence of w is independent from those of
s (i.e., a Bernoulli model), is calculated as
</bodyText>
<equation confidence="0.997635">
SC(wm : I) = (-1+ –1log —m + log r,
m 2 2r
</equation>
<bodyText confidence="0.974015333333333">
where m+ denotes the number of l&apos;s in wm.
Here, log denotes the logarithm to the base
2, 7r the circular constant, and .11(z) &apos;V
</bodyText>
<equation confidence="0.707435666666667">
–z log z – (1– z)log(1 – z), when 0 &lt; z &lt;1;
de
H(z)f = 0, when z = 0 or z = 1.
</equation>
<bodyText confidence="0.999845222222222">
Let wms be the sequence of all wi&apos;s (wi E
Wm) such that its corresponding si is 1, where
ms denotes the number of l&apos;s in sm. Let tam&apos;s
be the sequence of all wi&apos;s (wi E iv&apos;) such that
its corresponding si is 0, where m-,, denotes
the number O&apos;s in sm. The SC value of iv&apos;
relative to a model D in which the presence
or absence of w is dependent on those of .5 is
then calculated as
</bodyText>
<subsubsectionHeader confidence="0.647936">
s
</subsubsectionHeader>
<bodyText confidence="0.942214333333333">
where 74 denotes the number of l&apos;s in iv&apos;. ,
and tv_.,+, the number of l&apos;s in tum-s.
We can then calculate
</bodyText>
<equation confidence="0.995894">
6SC = ÷(SC(wm : I) – SC(wm : D))
171w
1 lo, mwm-wr 1.
2rn 2m
(1)
</equation>
<bodyText confidence="0.996199666666667">
According to the MDL principle, the larger
the 5SC value, the more likely that the pres-
ence or absence of iv is dependent on those of
</bodyText>
<subsectionHeader confidence="0.374861">
S.
</subsectionHeader>
<bodyText confidence="0.9999355625">
Actually, we may think of a word w for
which the value of SSC is larger than a pre-
determined threshold 7 and P(wls) &gt; P(w)
is satisfied as that which occurs significantly
frequently with the seed word s.
Note that the word clustering process is
independent of topic analysis. While one
could employ other methods (e.g., (Hofmann,
1999)) here for word clustering, our clus-
tering algorithm is more efficient than con-
ventional ones. For example, Hofmann&apos;s is
of order 0(IDIIW12), while ours is only of
0(1131+ where IDI denotes the number
of texts and IWI the number of words. That
means that our method is more practical when
a large amount of text data is available.
</bodyText>
<sectionHeader confidence="0.995423" genericHeader="method">
4 Topic Analysis
</sectionHeader>
<subsectionHeader confidence="0.969386">
4.1 Input and Output
</subsectionHeader>
<bodyText confidence="0.983566789473684">
In topic analysis, we use STM to parse a
given text and output a topic structure which
consists of segmented blocks and their top-
ics. Figure 3 shows an example topic struc-
ture as output with our method. The text has
been segmented into five blocks, and to each
block, a number of topics having high prob-
ability values have been assigned (topics are
represented by their seed words). The topic
structure clearly represents what topics are in-
cluded in the text and how the topics change
within the text.
3Note that the quantity within in (1) is (em-
pirical) mutual information, which is an effective mea-
sure for word co-occurrence calculation (cf.,(Brown et
al., 1992)). When the sample size is small, mutual
information values tend to be undesirably large. The
quantity within {- • .} in (1) can help avoid this unde-
sirable tendency because its value will become large
</bodyText>
<figure confidence="0.933848">
3
4.2 Outline
Our topic analysis consists of three processes:
SC(wm : D) = (mail + I log -I- log r) a pre-process called &apos;topic spotting,&apos; text seg-
ms 2/r
(mrr
„n + log + log ) mentation, and topic identification. In topic
I 774;g. 7r ,
</figure>
<footnote confidence="0.991229">
2For an introduction to MDL, see (Li, 1998). when data size is small.
</footnote>
<page confidence="0.99692">
37
</page>
<figure confidence="0.982168771428572">
ASIA, EXPORTERS FEAR MAGI FROB U.S.-JAPal RIFT (25-BAR-1987)
block 0 trade-export-tariff-impert(0.12) Japan-lapanese(0.07) US(0.06)
0 Haunting trade friction betosen the U.S. and Japan has raised fears among many of Asia&apos;s exporting nations that the too could inflict ...
1 They told Reuter c rrrrr pendent&apos; in Asian capitals a U.S. move against Japan might boost protectionist sentiment in the U.S. and lead to ...
2 But some exporters said that while the conflict would hurt them in the long-run, in the short-term Tokyo&apos;s less might be their gain.
3 The U.S. Has said it till impose 300 mm n ears of tariffs on imports of Japanese electronics goods on April 17, in retaliation for Japan&apos;s ...
4 Unofficial Japanese estimates put the impact of the tariffs at 10 billion dlrs and spokesmen for major electronics firms said they would ...
6 &amp;quot;Bo wouldn&apos;t be able to do business,&amp;quot; said a spokesman for leading Japanese electronics firm Hatsushita Electric Industrial Co Ltd Olt.
6 &amp;quot;If the tariffs remain in place for any length of time beyond a foe months it will mean the complete erosion of exports (of goods subject
block 1 trade-expert-tariff-import(0.17) US(0.09) Taivan(0.06) dlrs(0.05)
7 In Taiwan, businessmen and officials are also vorried.
8 &amp;quot;We are aware of the seriousness of the U.S. throat against Japan because it serves as a warning to us,&amp;quot; said a senior Taiwanese trade ...
9 Taivan had a trade trade surplus of 15.6 billion dlrs last year, 95 pot of it with the U.S.
10 The surplus helped &apos;yell Taiwan&apos;s foreign exchange reserves to 53 billion dirs. among the norld&apos;s largest.
11 V. must quickly open our markets. remove trade barriers and cut import tariffs to allow imports of U.S. products, if ve want to defuse ...
12 A senior official of South Korea&apos;s trade promotion association said the trade dispute between the U.S. and Japan might also lead to ...
13 Last year South lore&amp; had a trade surplus of 7.1 billion dlrs with the U.S., up from 4.9 billion dire in 1985.
14 In lalaysia, trade officers and businessmen said tough corbs against Japan might allot hard-hit producers of semiconductors in third ...
block 2 Hong-Iong(0.16) trade-expert-tariff-import(0.10) US(0.06)
16 In Hong Kong, ober. newspapers have alleged Japan has been selling bolos-coot semiconductors, some electronics maaufacturers Our. ...
16 &amp;quot;That is a very short-term viev,&amp;quot; said Laureate Rills. director-general of the Federation of Hong tong Industry.
17 &amp;quot;If the ohole purpose is to prevent imports, on. day it will be extended to other sources. Ruch more serious for Hong long is the ...
18 The U.S. last year was Hong Kong&apos;s biggest export market, accounting for over 30 pet of domestically produced exports.
block 3 trade-expert-tariff-impert(0.14) Butten(0.00) Japan-Japanes.(0.07)
19 The Australian government is ataiting the outcome of trade talks between the U.S. and Japan with interest and concern, Industry ...
20 &amp;quot;This kind of deterioration in trade relatioas betveen two countries which are major trading partners of ours is a very .—
21 He said Australia&apos;s concerns centred on coal and Beef. Australia&apos;s too largest exports to Japan and also significant U.S.
22 leanohil. U.S.-Japan....diplematic manoeuvres to solve the trade stand-off continue.
block 4 lapan-Japaneae(0.12) measure(0.06) trade-expert-tariff-import(0.05)
23 Japan&apos;s ruling Liberal Democratic Party yesterday outlined a package of economic measures to boost the Japanese economy.
24 The measures proposed include a large supplementary budget and record public works spending in the first half of the financial year.
25 They also call for stepped-up spending as an emergency measure to stimulate the economy despite Prime Einister Yasnhiro lakaten.
26 Deputy U.S. trade Representative Michael Smith and Hakote Iuroda, Japan&apos;s deputy minister of International Trade and Industry (BITI),...
0-26: sentence if
(..): probability value
</figure>
<figureCaption confidence="0.999915">
Figure 3: Topic structure of text
</figureCaption>
<bodyText confidence="0.9999775">
spotting, we select topics discussed in a given
text. We can then construct STMs on the
basis of the topics. In text segmentation, we
segment the text on the basis of the STMs,
assuming that each block is generated by an
individual STM. In topic identification, we es-
timate the parameters of the STM for each
segmented block and select topics with high
probabilities for the block. In this way, we
obtain a topic structure for the text.
</bodyText>
<subsectionHeader confidence="0.99962">
4.3 Topic Spotting
</subsectionHeader>
<bodyText confidence="0.9991964">
In topic spotting, we first select key words
from a given text. We calculate what we call
the Shannon information of each word in the
text. The Shannon information of word w in
text t is defined as
</bodyText>
<equation confidence="0.922918">
I(w) = —N(w)log P(w),
</equation>
<bodyText confidence="0.999982483870968">
where N(w) denotes the frequency of w in t,
and P(w) the probability of the occurrence of
w as estimated from corpus data. I(w) may
be interpreted as the amount of information
represented by w. We select as key words the
top 1 words sorted in descending order of I.
While Shannon information is similar to
the tf-idf widely used in information retrieval
(e.g., (Salton and Yang, 1973)), the use of
Shannon information can be justified on the
basis of information theory, but that of tf-idf
cannot. Our preliminary experimental results
indicate that Shannon information performs
better than or at least as well as tf-idf in key
word extraction.4
From the results of word clustering, we next
select any cluster (topic) whose seed word is
included among the selected key words.
We next merge any two clusters if one of
their seed words is included in the other&apos;s clus-
ter. For example, when a cluster with seed
word &apos;trade&apos; contains the word &apos;import,&apos; and
a cluster with seed word &apos;import&apos; contains the
word &apos;trade,&apos; we merge the two. After two
such merges, we may obtain a relatively large
cluster with, for example, &apos;trade-import-tariff-
export&apos; as its seed words, as is shown in Fig-
ure 3. Figure 4 shows the merging algorithm.
In this way, we obtain the most conspicuous
and mutually independent topics discussed in
a given text.
</bodyText>
<subsectionHeader confidence="0.998663">
4.4 Text Segmentation
</subsectionHeader>
<bodyText confidence="0.999810333333333">
In segmentation, we first identify candidates
for points of segmentation within the given
text. When we assume a relatively short text
</bodyText>
<footnote confidence="0.961044">
4We will discuss it in the full version of the paper.
</footnote>
<page confidence="0.977743">
38
</page>
<equation confidence="0.9181855">
,Icn: clusters,
V = {{kibi = 1,2,— ,n}.
</equation>
<bodyText confidence="0.9090645">
For each cluster pair (k, k), if the seed
word of ki is included in ki and the seed
word of ki is included in k, then push
(k, k) into queue Q;
</bodyText>
<equation confidence="0.630202">
while (Q 0) {
</equation>
<bodyText confidence="0.934664333333333">
Remove the first element (k, k3) from Q;
if (ki and ki belong to different sets
W1,W2 in V)
</bodyText>
<equation confidence="0.85329">
Replace W1 and W2 in V with
U W2;
</equation>
<bodyText confidence="0.5366895">
For each element W of V, merge the
clusters in it.
</bodyText>
<figureCaption confidence="0.996511">
Figure 4: Algorithm: merge
</figureCaption>
<bodyText confidence="0.9964511">
for the purposes of our explanation here, all
sentence-ending periods will be candidates.
For each candidate, we create two pseudo-
texts, one consisting of the h sentences pre-
ceding it, and the other of the h sentences
following it (when fewer than h exist in any
-direction, we simply use those which do exist).
We use the EM algorithm ((Dempster et al.,
1977), cf., Figure 5) to separately estimate the
parameters of an STM from each of the two
pseudo texts. It is theoretically guaranteed
that the EM algorithm converges to a local
maximum of the likelihood. We next calculate
the similarity (i.e., essentially the converse no-
tion of distances) between the STM based
on the preceding pseudo-text, and the STM
based on the following pseudo-text. These
STMs are denoted, respectively, as PL(w) and
PR(w). The similarity between PL(w) and
PR(w) is defined as
</bodyText>
<equation confidence="0.9480445">
S(LI1R) = 1 E.Ew IPL(w) — PR(w)I
2
</equation>
<bodyText confidence="0.991690142857143">
The numerator is referred to in statistics as
variational distance and has good properties
as a distance between two probability dis-
tributions (cf., (Cover and Thomas, 1991),
p.299).
Figure 7 shows a graph of calculated simi-
larity values for each of the candidates in the
</bodyText>
<footnote confidence="0.688980666666667">
5We use similarity rather than distance here in or-
der to simplify comparison between our method and
TextTiling (Hearst, 1997).
</footnote>
<bodyText confidence="0.831549">
s: predetermined number.
For the /th iteration (1 = 1,- • • ,$),
we calculate
</bodyText>
<equation confidence="0.98922">
p(11-1)(kiw) = P(i)(k)P(1)(tulk)
EkEK P(i) (k)P(I)(wik)
p(1+1)(k) Ar(w)P(Ni+1)(kiw)
py+i)(wik) N(w)P(11-&apos;)(k1w)
EwEw No.opo+i)(kiw)
</equation>
<bodyText confidence="0.998965">
N(w) denotes the frequency of word w
in the data; N = EwEw N(w).
</bodyText>
<figureCaption confidence="0.998275">
Figure 5: EM algorithm
</figureCaption>
<bodyText confidence="0.893275666666667">
n: number of segmentation candidates,
S(i) i(i = 0 ... n): similarity score.
for (i = 1;i &lt; n– 1; i + +){
</bodyText>
<equation confidence="0.85786525">
if (S(i – 1) &gt; S(i) &amp; S(i 1) &gt; S(0){
j = i – 1;
while (j &gt; 0 &amp; SU – 1) &gt; S(j))
j– –;
P1 = S(j);
j = i 1;
whileU &lt; n&amp; SU + 1) &gt; S(j))
j + +;
P2 = S(j);
if (P1 – &gt; 9 &amp; P2 – &gt; 0)
Conduct segmentation at i.
}}
</equation>
<figureCaption confidence="0.997017">
Figure 6: Algorithm: segment
</figureCaption>
<bodyText confidence="0.9998752">
text shown in Figure 3. &apos;Valleys&apos; (i.e., low-
similarity values) in the graph suggest points
for reasonable segmentations. In actual prac-
tice, segmentation is performed for each valley
whose similarity values is lower to a predeter-
mined degree 0 than each of the values of its
left &apos;peak&apos; and right &apos;peak&apos; (cf., Figure 6) For
example, for the text in Figure 3, segmenta-
tion was performed at candidates (i.e., end of
sentences) 6, 14, 18, and 22, with 0 = 0.05.
</bodyText>
<subsectionHeader confidence="0.87195">
4.5 Topic Identification
</subsectionHeader>
<bodyText confidence="0.999865857142857">
After segmentation, we separately estimate
the parameters of the STM for each block,
again using the EM algorithm, and obtain
a topic (cluster) probability distribution for
each block. We then choose those topics (clus-
ters) in each block having high probability val-
ues. In this way, we construct a topic struc-
</bodyText>
<page confidence="0.998469">
39
</page>
<figureCaption confidence="0.996122">
Figure 7: Similarity values for segmentation
candidates
</figureCaption>
<bodyText confidence="0.999883555555556">
ture as in Figure 3 for the given text (topics
are here represented by their seed words).
We can view topics appearing in all the
blocks as main topics, and topics appearing
only in individual blocks as subtopics. In
the text in Figure 3, the topic represented
by seed-words &apos; trade- exp ort-t ariff-imp ort &apos; is
the main topic, and &apos;Japan-Japanese,&apos; Hong
Kong,&apos; etc., are subtopics.
</bodyText>
<sectionHeader confidence="0.997742" genericHeader="method">
5 Applications
</sectionHeader>
<bodyText confidence="0.9999909">
Our method can be used in a variety of text
processing applications.
For example, given a collection of texts
(e.g., home pages), we can automatically con-
struct an index of the texts on the basis of the
extracted topics. We can indicate which topic
is from which text or even which block of a
text. Furthermore, we can indicate which top-
ics are main topics of texts and which topics
are subtopics (e.g., by displaying main topics
in boldface, etc). In this way, users can get a
fair sense of the contents of the texts simply
by looking through the index. For a specific
text, users can get a rough sense of the con-
tent by looking at the topic structure as, for
example, it is shown in Figure 3.
Our method can also be useful for text min-
ing, text summarization, information extrac-
tion, and other text processing, which require
one to first analyze the structure of a text.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.976316">
To the best of our knowledge, no previous
study has so far dealt with topic identification
and text segmentation within a single frame-
work.
A widely used method for key word extrac-
tion calculates the tf-idf value of each word in
a text and uses those words having the largest
tf-idf values as key words for that text (e.g.,
(Salton and Yang, 1973)). One can view these
extracted key words as the topics of the text.
No keyword extraction method by itself, how-
ever, is able to conduct segmentation.
With respect to text segmentation, exist-
ing methods can be classified into two groups.
One is to divide a text into blocks (e.g.,
TextTiling (Hearst, 1997)), the other to di-
vide a stream of texts into its original texts
(e.g.,(Allan et al., 1998; Yamron et al., 1998;
Beeferman et al., 1999; Reynar, 1999)). The
former group generally employs unsupervised
learning, while the latter supervised one. No
existing segmentation method, however, has
attempted topic identification.
TextTiling creates for each segmentation
candidate two pseudo-texts, one preceding it
and the other following it, and calculates as
similarity the cosine value between the word
frequency vectors of the two pseudo texts. It
then conducts segmentation at valley points
in a similar way to that of our method. Since
the problem setting of TextTiling (in general
the former group) is most close to that of our
study, we use TextTiling for comparison in our
experiments.
Our method by its nature performs topic
identification and segmentation within a sin-
gle framework. While it is possible with a
combination of existing methods to extract
key words from a given text by using tf-idf,
view the extracted key words as topics, seg-
ment the text into blocks by employing Text-
Tiling, estimate distribution of topics in each
block, and identify topics having high prob-
abilities in each block. Our method outper:
forms such a combination (referred to here-
after as &apos;Com&apos;) for topic identification, be-
cause it utilizes word cluster information. It
also performs better than Corn in text seg-
mentation because it is based on a well-defined
probability framework. Most importantly is
that our method is able to output an easily
understandable topic structure, which has not
been proposed so far.
Note that topic analysis is different from
text classification (e.g., (Lewis et al., 1996; Li
and Yamanishi, 1999; Joachims, 1998; Weiss
et al., 1999; Nigam et al., 2000)). While text
classification uses a number of pre-determined
categories, topic analysis includes no notion
of category. The output of topic analysis is a
topic structure, while the output of text clas-
</bodyText>
<figure confidence="0.998980615384615">
0.35
&amp;quot;STM*
0.3
0.25
02
-.3 0.15
0.1
0.05
00 5
10 15
sentence number
20 25
_n-
</figure>
<page confidence="0.991752">
40
</page>
<bodyText confidence="0.997096411764706">
sification is a label representing a category.
Furthermore, text classification is generally
based on supervised learning, which uses la-
beled text data6. By way of contrast, topic
analysis is based on unsupervised learning,
which uses only unlabeled text data.
Finite mixture models have been used in
a variety of applications in text processing
(e.g., (Li and Yamanishi, 1997; Nigam et al.,
2000; Hofmann, 1999)), indicating that they
are essential to text processing. We should
note, however, that their definitions and the
ways they use them are different from those
for STM in this paper. For example, Li and
Yamanishi propose to employ in text classi-
fication a mixture model (Li and Yamanishi,
1997) defined over categories:
</bodyText>
<equation confidence="0.988482">
P(w1c) = E P (k1c)P (IWO, w E W, C E C,
keic
</equation>
<bodyText confidence="0.999748428571429">
where W denotes a set of words, and C a
set of categories. In their framework, a new
text d is assigned into a category c* such that
c* = arg maxcEc P(cld) is satisfied. Hofmann
proposes using in information retrieval a joint
distribution which he calls &apos;an aspect model,&apos;
__defined as (Hofmann, 1999)
</bodyText>
<equation confidence="0.987553">
P(w , d) = P(d)P(wid)
= P(d)EkeK P(Ickl)P(wIlc),
w E W,d E D
</equation>
<bodyText confidence="0.999991">
where D denotes a set of texts. Furthermore,
he proposes extracting in retrieval those texts
whose estimated word distributions P(udd)
are similar to the word distribution of a query.
</bodyText>
<sectionHeader confidence="0.994653" genericHeader="evaluation">
7 Experimental Results
</sectionHeader>
<bodyText confidence="0.9998834">
We have evaluated the performance of our
topic analysis method (STM) in terms of three
aspects: topic structure adequacy, text seg-
mentation accuracy, and topic identification
accuracy.
</bodyText>
<subsectionHeader confidence="0.997677">
7.1 Data Set
</subsectionHeader>
<bodyText confidence="0.9984004">
We know of no data available for the pur-
pose of evaluation of topic analysis. We thus
utilized Reuters news articles referred to as
&apos;Reuters-21578,&apos; which has been widely used
in text classification7. We used a prepared
</bodyText>
<footnote confidence="0.992305">
6An exception is the method proposed in (McCal-
lum and Nigam, 1999), which, instead of labeled texts,
uses unlabeled texts, pre-determined categories, and
keywords defined by humans for each category.
&apos;Available at http://www.research.att.com/iewis/.
</footnote>
<bodyText confidence="0.999912416666667">
split of the data &apos;Apte split,&apos; which consists
of 9603 texts for training and 3299 texts for
test. All of the texts had already been classi-
fied into 90 categories by human subjects.
For each text, we used the Oxford Learner&apos;s
Dictionary8 to conduct stemming, and re-
moved &apos;stop words&apos; (e.g., &apos;the,&apos; and&apos;) that we
had included on a previously prepared list.
The average length of a text was about 115
words. (We did not use phrases, however,
which would further improve experimental re-
sults.)
</bodyText>
<subsectionHeader confidence="0.999683">
7.2 Word Clustering
</subsectionHeader>
<bodyText confidence="0.999989857142857">
We conducted word clustering with 9603
training texts. 7340 individual words had a
total frequency of more than 5, and we used
them as seeds with which to collect frequently
co-occurring words. The threshold for clus-
tering 7 was set at 0.005, and this yielded
970 word clusters having more than one word
(i.e., not simply containing a seed word alone).
Note that the category labels of the training
texts need not be used in clustering.
We next conducted a topic analysis on all
the 3299 texts. The thresholds of 1, h, and 0
were set at 20, 3, and 0.05, respectively, on
the basis of preliminary experimental results.
</bodyText>
<subsectionHeader confidence="0.999732">
7.3 Topic Structure
</subsectionHeader>
<bodyText confidence="0.999562">
We looked at the topic structures of the 3299
texts obtained by our method to determine
how well they conformed to human intuition.
For topic identification in this experiment,
clusters in each block were sorted in descend-
ing order of their probabilities, and the top
7 seed words were extracted to represent the
topics of the block.
Figure 3 show results for the text with ID
14826; they generally agree well with human
intuition. The text has been segmented into
5 blocks and the topics of each block is rep-
resented by 7 seed words. The main topic is
represented by the seed-words &apos;trade-export-
tariff-import.&apos; The subtopics are represented
by &apos;Japan-Japanese,&apos; Taiwan,&amp;quot;Hong Kong,&apos;
etc. There were, however, a small number
of errors. For example, the text should also
have been segmented after sentences 11 and
13, but, due to limited sentence content, it was
not. Furthermore, assigning subtopic of &apos;But-
ton&apos; (from &apos;Mr. Button&apos;) into block 3 (due
to the high Shannon information value of the
word &apos;Button&apos;) was also undesirable.
</bodyText>
<footnote confidence="0.985617">
8Available at ftp://sable.ox.ac.uk.
</footnote>
<page confidence="0.999087">
41
</page>
<tableCaption confidence="0.6859525">
Table 1: 10 categories and their identification Table 2: Main topic identification results with
words
</tableCaption>
<subsectionHeader confidence="0.997264">
7.4 Main Topic Identification
</subsectionHeader>
<bodyText confidence="0.999859710526316">
We conducted an evaluation to determine
whether or not the main topics in the topic
structures obtained for the 3299 test texts
could be approximately matched with the la-
bels (categories) assigned to the test texts.
Note that here labels are used only for eval-
uation, not for training. This is in contrast
to the situation in most text classification ex-
periments, in which labels are generally used
both for training and for evaluation. It is not
particularly meaningful, then, to compare the
results for main topic identification obtained
here with those for text classification.
With STM, clusters in each block were
sorted in descending order of their probabil-
ities, and the top k seed words were extracted
to represent the topics of the block. Further-
more, a seed word appearing in all the blocks
of the text was considered to represent a main
topic. When a text had not been segmented
(i.e., has only one block), all top k seed words
were considered to represent main topics.
Table 1 lists the largest 10 categories in the
Reuters data. On the basis of the definition of
each of the 10 categories, we assigned based on
our intuition to each of them the identification
words that are listed in Table 1.
For the evaluation, when the seed words for
main topics contained at least one of the iden-
tification words, we considered our method to
have identified the corresponding main topic
equivalent to a human-determined category.
We then evaluated these in terms of preci-
sion and recall. Here, precision is defined as
the ratio of the number of decisions correctly
made to the total number of decisions made.
Recall is defined as the ratio of the number of
decisions correctly made to the total number
</bodyText>
<table confidence="0.977476714285714">
respect to 7 top words
category STM Corn
rec. pre. rec. pre.
earn 0.790 0.971 0.526 0.976
acq 0.245 0.854 0.184 0.841
money-fx 0.436 0.456 0.285 0.421
grain 0.322 0.750 0.174 0.650
crude 0.487 0.676 0.407 0.664
trade 0.667 0.473 0.590 0.356
interest 0.107 0.700 0.084 0.733
ship 0.247 0.957 0.270 0.828
wheat 0.620 0.936 0.408 0.967
corn 0.429 0.960 0.446 1.00
micro-average 0.515 0.824 0.365 0.774
</table>
<tableCaption confidence="0.909814">
Table 3: Main topic identification results with
respect to 5 top words
</tableCaption>
<table confidence="0.828150076923077">
category STM Corn
rec. pre. rec. pre.
earn 0.742 0.971 0.348 0.977
acq 0.184 0.868 0.120 0.869
money-fx 0.413 0.503 0.268 0.471
grain 0.295 0.759 0.121 0.600
crude 0.471 0.718 0.333 0.656
trade 0.479 0.505 0.513 0.403
interest 0.053 0.700 0.069 0.818
ship 0.169 1.000 0.180 0.762
wheat 0.577 0.953 0.282 0.952
corn 0.357 0.952 0.321 1.000
micro-average 0.461 0.850 0.257 0.767
</table>
<bodyText confidence="0.997871625">
of decisions which should have been correctly
made.
We also looked at the performance of Com
(cf., Section 6). For Corn, we extracted from a
text the key words with the 20 largest Shan-
non information values, segmented the text
using TextTiling, and extracted in each block
the key words having the largest k probabil-
ity values. Any key word extracted in all
blocks was considered to represent a main
topic. When the key words for main top-
ics contained at least one of the identification
words, we viewed that text as having the cor-
responding main topic.
Table 2 shows the results achieved with
STM and Com in the case of k 7.9 Table 3
</bodyText>
<footnote confidence="0.863463">
9For the definition of micro-averaging, see, for ex-
</footnote>
<bodyText confidence="0.978503590909091">
category
earn
acq
money-fx
grain
crude
trade
interest
ship
wheat
corn
identification words
earning, share, profit, dividend
acquisition, acquire, sell, buy
currency, dollar, yen, stg
grain, cereal, crop
oil, crude, gas
trade, export, import, tariff
interest &amp; rate
ship, vessel, ferry, tanker
wheat
corn, maize
</bodyText>
<page confidence="0.993275">
42
</page>
<table confidence="0.779938705882353">
Title: EGYPT BUYS PL 480 WHEAT FLOUR - U.S. TRADERS
&apos;Body: Egypt bought 126,723 tonnes of U.S. wheat flour in its PL
480 tender yesterday, trade sources said. The purchase included
51,880 tonnes for Ray shipment and 73,843 tonnes for lune shipment.
Price details sere not available.
Content fiords (Freq.): tonne(3) shipmen1(2) buy(1) detail(1)
Egypt(1) flour(1) inc1ude(1) Juno(1) PL(1) price(1) purchase(1)
source(1) trade(1) US(1) sheat(1)
Rey fiords (Shan. Inf.): tonne(17.3) shipment(15.3) PL(10.6) flour(9.8)
Egypt(9.3) detail(7.6) lune(7.2) wheat(6.8) purchase(6.6) source(6.5)
US(6.1) buy(6.0) inclnde(6.0) trade(6.3) price (6.1)
Cu. Topics (Prob.): tonne(0.17) shipment(0.11) price(0.06) lune(0.06)
include(0.06) parcbase(0.06) source(0.06)
SYS Topics (Prob.) : floor-sheat(0.16) tonne(0.12) shipment(0.11)
purchase-buy(0.11) Egypt(0.06)
Cluster: (flour-wheat: wheat tonne flour)
(purchase-buy: purchase buy)
</table>
<figureCaption confidence="0.996247">
Figure 8: Topic Identification Example
</figureCaption>
<bodyText confidence="0.996324227272727">
shows the results in the case of k = 5. The
comparison may be considered fair in that it
requires each of the two methods to provide
the same number of words to represent top-
ics. Results indicate that STM significantly
outperforms Com, particularly in terms of re-
call.
The main reason for the higher performance
achieved by STM is that it utilizes word clus-
ter information. Figure 8 shows topic analysis
results for the text with ID 15572 labeled with
&apos;wheat.&apos; The text contains only 15 content
words (word types), thus all of the 15 words
were extracted as key words and the text was
not segmented by either method. Corn was
unable to identify the main topic &apos;wheat,&apos; be-
cause the probability of each of the relevant
key words &apos;wheat&apos; and &apos;flour&apos; was low. In
contrast, STM successfully identified the topic
because the relevant key words were classified
into the same cluster, and its probability was
relatively high.
</bodyText>
<subsectionHeader confidence="0.8619175">
7.5 Segmentation and Subtopic
Identification
</subsectionHeader>
<bodyText confidence="0.999921666666667">
We collected the 50 longest test texts (re-
ferred to here as &apos;seed texts&apos;) from each of the
10 categories, and combined each with a test
text randomly selected from other categories
to produce 500 pseudo-texts. Placement of
the seed text within its pseudo-text (i.e., be-
fore or after the other text) was determined
randomly.
We used both STM and Corn to segment
each of the pseudo-texts into two blocks and
identify subtopics. Table 4 shows the segmen-
tation results for the two method evaluated
</bodyText>
<note confidence="0.449804">
ample, (Lewis and Ringuette, 1994).
</note>
<tableCaption confidence="0.990339">
Table 5: Subtopic identification results
</tableCaption>
<table confidence="0.994028071428572">
category of STM Com
seed text
rec. pre. rec. pre.
earn 0.430 0.945 0.324 0.973
acq 0.237 0.939 0.217 0.959
money-fx 0.585 0.950 0.533 0.961
grain 0.276 0.947 0.222 0.938
crude 0.572 0.979 0.557 0.990
trade 0.634 0.951 0.627 0.899
interest 0.211 0.937 0.136 1.000
ship 0.260 1.000 0.340 0.994
wheat 0.500 0.970 0.395 0.980
corn 0.317 1.000 0.441 0.882
Average 0.402 0.962 0.379 0.958
</table>
<bodyText confidence="0.999818">
in terms of recall, precision, and error prob-
ability. Table 5 shows the results of subtopic
identification as evaluated in terms of recall
and precision. Error probability is a metric
for evaluating segmentation results proposed
in (Allan et al., 1998; Beeferman et al., 1999).
It is defined here as the probability that a ran-
domly chosen pair of sentences a distance of k
sentence apart is incorrectly segmented.1°
Experimental results indicate that STM
outperforms Cora in both segmentation and
identification.11
</bodyText>
<sectionHeader confidence="0.999339" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999990555555555">
We have proposed a new method of topic
analysis that employs a finite mixture model,
referred to here as a stochastic topic model
(STM).
Topic analysis consists of two main tasks:
text segmentation and topic identification.
With topic analysis, one can obtain a topic
structure for a text.
Our method addresses topic analysis within
a single framework. It has the following novel
features: 1) it represents topics by means of
word clusters and employs a finite mixture
model (STM) to represent a word distribution
within a text; 2) it constructs topics on the
basis of corpus data before conducting topic
analysis; 3) it segments a text by detecting
significant differences between STMs; and 4)
it identifies topics by estimating parameters
</bodyText>
<footnote confidence="0.7016025">
10Here, k was set to 5 because the average length of
a text was about 10 sentences.
11We will discuss the results in the full version of
the paper.
</footnote>
<page confidence="0.999889">
43
</page>
<tableCaption confidence="0.998419">
Table 4: Text segmentation results
</tableCaption>
<bodyText confidence="0.9980593">
category of STM Corn
seed text
rec. pre. err. rec. pre. err.
earn 0.660 0.660 0.167 0.640 0.640 0.171
acq 0.820 0.820 0.059 0.740 0.740 0.085
money-fx 0.700 0.700 0.087 0.660 0.660 0.121
grain 0.700 0.700 0.074 0.660 0.660 0.076
crude 0.860 0.860 0.051 0.820 0.820 0.066
trade 0.800 0.800 0.072 0.800 0.800 0.081
interest 0.760 0.760 0.119 0.820 0.820 0.084
ship 0.837 0.854 0.074 0.816 0.833 0.084
wheat 0.760 0.760 0.075 0.640 0.640 0.130
Corn 0.625 0.625 0.147 0.650 0.650 0.105
Average 0.752 0.754 0.092 0.725 0.726 0.100
of STMs.
Experimental results indicate that our
method outperforms a method that combines
existing techniques. More specifically, it sig-
nificantly outperforms the combined method
in topic identification.
</bodyText>
<sectionHeader confidence="0.99875" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999128">
J. Allan, J. Carbonell, G. Doddington, J. Yam-
ron, and Y. Yang. 1998. Topic detection and
tracking pilot study: Final report. Proc. of the
DARPA Broadcast News Transcription and Un-
derstanding Workshop, pages 194-218.
D. Beeferman, A. Berger, and J. Lafferty.
1999. Statistical models for text segmentation.
Machi. Lrn., 34:177-210.
P. F. Brown, V. J. Della Pietra, P. V. deSouza,
J. C. Lai, and R. L. Mercer. 1992. Class-based
n-gram models of natural language. Comp.
Ling., 18(4):283-298.
T. M. Cover and J. A. Thomas. 1991. Elements of
Information Theory. John Wiley &amp; Sons Inc.,
New York.
A.P. Dempster, N.M. Laird, and D.B. Rubin.
1977. Maximum likelihood from incomplete
data via the em algorithm. Journ. of Roy. Stat.
Soci., Ser. B, 39(41-38.
B. Everitt and D. Hand. 1981. Finite Mixture Dis-
tributions. Chapman and Hall.
M. Hearst. 1997. Texttiling: Segmenting text
into multi-paragraph subtopic passages. Comp.
Ling., 23(433-64.
Thomas Hofmann. 1999. Probabilistic latent se-
mantic indexing. Proc. of SIGIR&apos;99, pages 50-
57.
T. Joachirns. 1998. Text categorization with sup-
port vector machines: Learning with many rel-
evant features. Proc. of ECML&apos;98.
D. D. Lewis and M. Ringuette. 1994. A compar-
ison of two learning algorithms for test catego-
rization. Proc. of 3rd Ann. Symp. on Doc. Ana.
and Info. Retr., pages 81-93.
D. D. Lewis, R. E. Schapire, J. P. Callan, and
R. Papka. 1996. Training algorithms for linear
text classifiers. Proc. of SIGIR&apos;96.
H. Li and K. Yamanishi. 1997. Document classi-
fication using a finite mixture model. Proc. of
ACL &apos;97, pages 39-47.
H. Li and K. Yamanishi. 1999. Text classification
using ESC-based stochastic decision lists. Proc.
of ACM-CIKM&apos;99, pages 122-130.
H. Li. 1998. A Probabilistic Approach to Lexical
Semantic Knowledge Acquisition and Structural
Disambiguation. Ph.D. Thesis, Univ. of Tokyo.
A. K. McCallum and K. Nigam. 1999. Text clas-
sification by bootstrapping with keywords, em
and shrinkage. Proc. of ACL&apos;99 Workshop Un-
supervised Learning in NLP.
K. Nigam, A. K. McCallum, S. Thrun, and
T. Mitchell. 2000. Text classification from
labeled and unlabeled documents using em.
Machi. Lrn., 39:103-134.
J. C. Reynar. 1999. Statistical models for topic
segmentation. Proc. of ACL &apos;99, pages 357-364.
J. Rissanen. 1996. Fisher information and
stochastic complexity. IEEE Trans. on Info.
Thry., 42(1):40-47.
G. Salton and C.S. Yang. 1973. On the speci-
fication of term values in automatic indexing.
Journ. of Doc., 29(4):351-372.
S. M. Weiss, C. Apte, F. Damerau, F. J. Oles,
T. Goetz, and T. Hampp. 1999. Maximiz-
ing text-mining performance. IEEE Intel. Sys.,
14(4):63-69.
J.P. Yamron, I. Carp, L. Gillick, S. Lowe, and
P. van Mulbregt. 1998. A Hidden Markov
Model approach to text segmentation and event
tracking. Proc. of ICASSP&apos;99, pages 333-336.
</reference>
<page confidence="0.999294">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.834221">
<title confidence="0.993064">Topic Analysis Using a Finite Mixture Model</title>
<author confidence="0.954473">Hang Li</author>
<author confidence="0.954473">Kenji</author>
<affiliation confidence="0.998618">NEC Corporation</affiliation>
<email confidence="0.88606">lihang@ccm.cl.nec.co.jp</email>
<email confidence="0.88606">yamanisi@ccm.cl.nec.co.jp</email>
<abstract confidence="0.999468769230769">We address the issue of &apos;topic analysis,&apos; by which is determined a text&apos;s topic structure, which indicates what topics are included in a text, and how topics change within the text. We propose a novel approach to this issue, one based on statistical modeling and learning. We represent topics by means of word clusters, and employ a finite mixture model to represent a word distribution within a text. Our experimental results indicate that our method significantly outperforms a method that combines existing techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>J Carbonell</author>
<author>G Doddington</author>
<author>J Yamron</author>
<author>Y Yang</author>
</authors>
<title>Topic detection and tracking pilot study: Final report.</title>
<date>1998</date>
<booktitle>Proc. of the DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>194--218</pages>
<contexts>
<context position="22624" citStr="Allan et al., 1998" startWordPosition="3964" endWordPosition="3967">gle framework. A widely used method for key word extraction calculates the tf-idf value of each word in a text and uses those words having the largest tf-idf values as key words for that text (e.g., (Salton and Yang, 1973)). One can view these extracted key words as the topics of the text. No keyword extraction method by itself, however, is able to conduct segmentation. With respect to text segmentation, existing methods can be classified into two groups. One is to divide a text into blocks (e.g., TextTiling (Hearst, 1997)), the other to divide a stream of texts into its original texts (e.g.,(Allan et al., 1998; Yamron et al., 1998; Beeferman et al., 1999; Reynar, 1999)). The former group generally employs unsupervised learning, while the latter supervised one. No existing segmentation method, however, has attempted topic identification. TextTiling creates for each segmentation candidate two pseudo-texts, one preceding it and the other following it, and calculates as similarity the cosine value between the word frequency vectors of the two pseudo texts. It then conducts segmentation at valley points in a similar way to that of our method. Since the problem setting of TextTiling (in general the forme</context>
<context position="35623" citStr="Allan et al., 1998" startWordPosition="6092" endWordPosition="6095">Com seed text rec. pre. rec. pre. earn 0.430 0.945 0.324 0.973 acq 0.237 0.939 0.217 0.959 money-fx 0.585 0.950 0.533 0.961 grain 0.276 0.947 0.222 0.938 crude 0.572 0.979 0.557 0.990 trade 0.634 0.951 0.627 0.899 interest 0.211 0.937 0.136 1.000 ship 0.260 1.000 0.340 0.994 wheat 0.500 0.970 0.395 0.980 corn 0.317 1.000 0.441 0.882 Average 0.402 0.962 0.379 0.958 in terms of recall, precision, and error probability. Table 5 shows the results of subtopic identification as evaluated in terms of recall and precision. Error probability is a metric for evaluating segmentation results proposed in (Allan et al., 1998; Beeferman et al., 1999). It is defined here as the probability that a randomly chosen pair of sentences a distance of k sentence apart is incorrectly segmented.1° Experimental results indicate that STM outperforms Cora in both segmentation and identification.11 8 Conclusions We have proposed a new method of topic analysis that employs a finite mixture model, referred to here as a stochastic topic model (STM). Topic analysis consists of two main tasks: text segmentation and topic identification. With topic analysis, one can obtain a topic structure for a text. Our method addresses topic analy</context>
</contexts>
<marker>Allan, Carbonell, Doddington, Yamron, Yang, 1998</marker>
<rawString>J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang. 1998. Topic detection and tracking pilot study: Final report. Proc. of the DARPA Broadcast News Transcription and Understanding Workshop, pages 194-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<journal>Machi. Lrn.,</journal>
<pages>34--177</pages>
<contexts>
<context position="22669" citStr="Beeferman et al., 1999" startWordPosition="3972" endWordPosition="3975">ey word extraction calculates the tf-idf value of each word in a text and uses those words having the largest tf-idf values as key words for that text (e.g., (Salton and Yang, 1973)). One can view these extracted key words as the topics of the text. No keyword extraction method by itself, however, is able to conduct segmentation. With respect to text segmentation, existing methods can be classified into two groups. One is to divide a text into blocks (e.g., TextTiling (Hearst, 1997)), the other to divide a stream of texts into its original texts (e.g.,(Allan et al., 1998; Yamron et al., 1998; Beeferman et al., 1999; Reynar, 1999)). The former group generally employs unsupervised learning, while the latter supervised one. No existing segmentation method, however, has attempted topic identification. TextTiling creates for each segmentation candidate two pseudo-texts, one preceding it and the other following it, and calculates as similarity the cosine value between the word frequency vectors of the two pseudo texts. It then conducts segmentation at valley points in a similar way to that of our method. Since the problem setting of TextTiling (in general the former group) is most close to that of our study, </context>
<context position="35648" citStr="Beeferman et al., 1999" startWordPosition="6096" endWordPosition="6099">re. rec. pre. earn 0.430 0.945 0.324 0.973 acq 0.237 0.939 0.217 0.959 money-fx 0.585 0.950 0.533 0.961 grain 0.276 0.947 0.222 0.938 crude 0.572 0.979 0.557 0.990 trade 0.634 0.951 0.627 0.899 interest 0.211 0.937 0.136 1.000 ship 0.260 1.000 0.340 0.994 wheat 0.500 0.970 0.395 0.980 corn 0.317 1.000 0.441 0.882 Average 0.402 0.962 0.379 0.958 in terms of recall, precision, and error probability. Table 5 shows the results of subtopic identification as evaluated in terms of recall and precision. Error probability is a metric for evaluating segmentation results proposed in (Allan et al., 1998; Beeferman et al., 1999). It is defined here as the probability that a randomly chosen pair of sentences a distance of k sentence apart is incorrectly segmented.1° Experimental results indicate that STM outperforms Cora in both segmentation and identification.11 8 Conclusions We have proposed a new method of topic analysis that employs a finite mixture model, referred to here as a stochastic topic model (STM). Topic analysis consists of two main tasks: text segmentation and topic identification. With topic analysis, one can obtain a topic structure for a text. Our method addresses topic analysis within a single frame</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>D. Beeferman, A. Berger, and J. Lafferty. 1999. Statistical models for text segmentation. Machi. Lrn., 34:177-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Comp. Ling.,</journal>
<pages>18--4</pages>
<contexts>
<context position="10742" citStr="Brown et al., 1992" startWordPosition="1908" endWordPosition="1911"> text and output a topic structure which consists of segmented blocks and their topics. Figure 3 shows an example topic structure as output with our method. The text has been segmented into five blocks, and to each block, a number of topics having high probability values have been assigned (topics are represented by their seed words). The topic structure clearly represents what topics are included in the text and how the topics change within the text. 3Note that the quantity within in (1) is (empirical) mutual information, which is an effective measure for word co-occurrence calculation (cf.,(Brown et al., 1992)). When the sample size is small, mutual information values tend to be undesirably large. The quantity within {- • .} in (1) can help avoid this undesirable tendency because its value will become large 3 4.2 Outline Our topic analysis consists of three processes: SC(wm : D) = (mail + I log -I- log r) a pre-process called &apos;topic spotting,&apos; text segms 2/r (mrr „n + log + log ) mentation, and topic identification. In topic I 774;g. 7r , 2For an introduction to MDL, see (Li, 1998). when data size is small. 37 ASIA, EXPORTERS FEAR MAGI FROB U.S.-JAPal RIFT (25-BAR-1987) block 0 trade-export-tariff-</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. 1992. Class-based n-gram models of natural language. Comp. Ling., 18(4):283-298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons Inc.,</publisher>
<location>New York.</location>
<contexts>
<context position="18877" citStr="Cover and Thomas, 1991" startWordPosition="3283" endWordPosition="3286">wo pseudo texts. It is theoretically guaranteed that the EM algorithm converges to a local maximum of the likelihood. We next calculate the similarity (i.e., essentially the converse notion of distances) between the STM based on the preceding pseudo-text, and the STM based on the following pseudo-text. These STMs are denoted, respectively, as PL(w) and PR(w). The similarity between PL(w) and PR(w) is defined as S(LI1R) = 1 E.Ew IPL(w) — PR(w)I 2 The numerator is referred to in statistics as variational distance and has good properties as a distance between two probability distributions (cf., (Cover and Thomas, 1991), p.299). Figure 7 shows a graph of calculated similarity values for each of the candidates in the 5We use similarity rather than distance here in order to simplify comparison between our method and TextTiling (Hearst, 1997). s: predetermined number. For the /th iteration (1 = 1,- • • ,$), we calculate p(11-1)(kiw) = P(i)(k)P(1)(tulk) EkEK P(i) (k)P(I)(wik) p(1+1)(k) Ar(w)P(Ni+1)(kiw) py+i)(wik) N(w)P(11-&apos;)(k1w) EwEw No.opo+i)(kiw) N(w) denotes the frequency of word w in the data; N = EwEw N(w). Figure 5: EM algorithm n: number of segmentation candidates, S(i) i(i = 0 ... n): similarity score.</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>T. M. Cover and J. A. Thomas. 1991. Elements of Information Theory. John Wiley &amp; Sons Inc., New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journ. of Roy. Stat. Soci., Ser. B,</journal>
<pages>39--41</pages>
<contexts>
<context position="18171" citStr="Dempster et al., 1977" startWordPosition="3167" endWordPosition="3170">cluded in k, then push (k, k) into queue Q; while (Q 0) { Remove the first element (k, k3) from Q; if (ki and ki belong to different sets W1,W2 in V) Replace W1 and W2 in V with U W2; For each element W of V, merge the clusters in it. Figure 4: Algorithm: merge for the purposes of our explanation here, all sentence-ending periods will be candidates. For each candidate, we create two pseudotexts, one consisting of the h sentences preceding it, and the other of the h sentences following it (when fewer than h exist in any -direction, we simply use those which do exist). We use the EM algorithm ((Dempster et al., 1977), cf., Figure 5) to separately estimate the parameters of an STM from each of the two pseudo texts. It is theoretically guaranteed that the EM algorithm converges to a local maximum of the likelihood. We next calculate the similarity (i.e., essentially the converse notion of distances) between the STM based on the preceding pseudo-text, and the STM based on the following pseudo-text. These STMs are denoted, respectively, as PL(w) and PR(w). The similarity between PL(w) and PR(w) is defined as S(LI1R) = 1 E.Ew IPL(w) — PR(w)I 2 The numerator is referred to in statistics as variational distance </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journ. of Roy. Stat. Soci., Ser. B, 39(41-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Everitt</author>
<author>D Hand</author>
</authors>
<title>Finite Mixture Distributions.</title>
<date>1981</date>
<publisher>Chapman and Hall.</publisher>
<contexts>
<context position="2307" citStr="Everitt and Hand, 1981" startWordPosition="377" endWordPosition="380">n method (e.g., TextTiling (Hearst, 1997)) generally segments a text into blocks (paragraphs) in accord with topic changes within the text, but it does not identify (or label) by itself the topics discussed in each of the blocks. The purpose of this paper is to provide a single framework for conducting topic analysis, i.e., performing both topic identification and text segmentation. The key characteristics of our framework are 1) representing a topic by means of a cluster of words that are closely related to the topic, and 2) employing a stochastic model, called a finite mixture model (e.g., (Everitt and Hand, 1981)), to represent a word distribution within a text. The finite mixture model has a hierarchical structure of probability distributions. The first level is a probability distribution of topics (topic distribution). The second level consists of probability distributions of words included within topics (word distributions). These word distributions are linearly combined to represent a word distribution within a text, with the topic distribution being used as the coefficient vector. Hereafter we refer to a finite mixture model having this structure as a stochastic topic model (STM). Before conducti</context>
</contexts>
<marker>Everitt, Hand, 1981</marker>
<rawString>B. Everitt and D. Hand. 1981. Finite Mixture Distributions. Chapman and Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Texttiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Comp. Ling.,</journal>
<pages>23--433</pages>
<contexts>
<context position="1725" citStr="Hearst, 1997" startWordPosition="280" endWordPosition="281">for purposes of information retrieval. With it, one can understand what the main topics and subtopics of a text are, and where those subtopics lie within the text. To the best of our knowledge, however, no previous study has so far dealt with the topic analysis problem in the above sense. The most closely related are key word extraction and text segmentation. A keyword extraction method (e.g., that using tf-idf (Salton and Yang, 1973)) generally extracts from a text key words which represent topics within the text, but it does not conduct segmentation. A segmentation method (e.g., TextTiling (Hearst, 1997)) generally segments a text into blocks (paragraphs) in accord with topic changes within the text, but it does not identify (or label) by itself the topics discussed in each of the blocks. The purpose of this paper is to provide a single framework for conducting topic analysis, i.e., performing both topic identification and text segmentation. The key characteristics of our framework are 1) representing a topic by means of a cluster of words that are closely related to the topic, and 2) employing a stochastic model, called a finite mixture model (e.g., (Everitt and Hand, 1981)), to represent a </context>
<context position="19101" citStr="Hearst, 1997" startWordPosition="3323" endWordPosition="3324">receding pseudo-text, and the STM based on the following pseudo-text. These STMs are denoted, respectively, as PL(w) and PR(w). The similarity between PL(w) and PR(w) is defined as S(LI1R) = 1 E.Ew IPL(w) — PR(w)I 2 The numerator is referred to in statistics as variational distance and has good properties as a distance between two probability distributions (cf., (Cover and Thomas, 1991), p.299). Figure 7 shows a graph of calculated similarity values for each of the candidates in the 5We use similarity rather than distance here in order to simplify comparison between our method and TextTiling (Hearst, 1997). s: predetermined number. For the /th iteration (1 = 1,- • • ,$), we calculate p(11-1)(kiw) = P(i)(k)P(1)(tulk) EkEK P(i) (k)P(I)(wik) p(1+1)(k) Ar(w)P(Ni+1)(kiw) py+i)(wik) N(w)P(11-&apos;)(k1w) EwEw No.opo+i)(kiw) N(w) denotes the frequency of word w in the data; N = EwEw N(w). Figure 5: EM algorithm n: number of segmentation candidates, S(i) i(i = 0 ... n): similarity score. for (i = 1;i &lt; n– 1; i + +){ if (S(i – 1) &gt; S(i) &amp; S(i 1) &gt; S(0){ j = i – 1; while (j &gt; 0 &amp; SU – 1) &gt; S(j)) j– –; P1 = S(j); j = i 1; whileU &lt; n&amp; SU + 1) &gt; S(j)) j + +; P2 = S(j); if (P1 – &gt; 9 &amp; P2 – &gt; 0) Conduct segmentati</context>
<context position="22534" citStr="Hearst, 1997" startWordPosition="3949" endWordPosition="3950">s study has so far dealt with topic identification and text segmentation within a single framework. A widely used method for key word extraction calculates the tf-idf value of each word in a text and uses those words having the largest tf-idf values as key words for that text (e.g., (Salton and Yang, 1973)). One can view these extracted key words as the topics of the text. No keyword extraction method by itself, however, is able to conduct segmentation. With respect to text segmentation, existing methods can be classified into two groups. One is to divide a text into blocks (e.g., TextTiling (Hearst, 1997)), the other to divide a stream of texts into its original texts (e.g.,(Allan et al., 1998; Yamron et al., 1998; Beeferman et al., 1999; Reynar, 1999)). The former group generally employs unsupervised learning, while the latter supervised one. No existing segmentation method, however, has attempted topic identification. TextTiling creates for each segmentation candidate two pseudo-texts, one preceding it and the other following it, and calculates as similarity the cosine value between the word frequency vectors of the two pseudo texts. It then conducts segmentation at valley points in a simila</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>M. Hearst. 1997. Texttiling: Segmenting text into multi-paragraph subtopic passages. Comp. Ling., 23(433-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>Proc. of SIGIR&apos;99,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="4419" citStr="Hofmann, 1999" startWordPosition="716" endWordPosition="717">ords as topics, segment the text into blocks using TextTiling, and estimate the distribution of topics (key words) within each block. Experimental results indicate, however, that our method significantly outperforms such a combined method in topic identification and outperforms it in text segmentation, because it utilizes word cluster information and employs a well-defined probability framework. Finite mixture models have been employed in a number of text processing applications, such as text classification (e.g., (Li and Yamanishi, 1997; Nigam et al., 2000)) and information retrieval (e.g., (Hofmann, 1999)). As will be discussed, however, our definition of a finite mixture model and the way we use it here differs significantly. 2 Stochastic Topic Model 2.1 Topic While the term &apos;topic&apos; is used in different ways in different linguistic theories, we simply view it here as a subject within a text. We represent a topic by means of a cluster of words that are closely related to the topic, assuming that a cluster has a seed word (or several seed words) which indicates a topic. Figure 1 shows an example topic with the word &apos;trade&apos; being the seed word. trade: trade export import tariff trader GATT prote</context>
<context position="9708" citStr="Hofmann, 1999" startWordPosition="1729" endWordPosition="1730"> l&apos;s in iv&apos;. , and tv_.,+, the number of l&apos;s in tum-s. We can then calculate 6SC = ÷(SC(wm : I) – SC(wm : D)) 171w 1 lo, mwm-wr 1. 2rn 2m (1) According to the MDL principle, the larger the 5SC value, the more likely that the presence or absence of iv is dependent on those of S. Actually, we may think of a word w for which the value of SSC is larger than a predetermined threshold 7 and P(wls) &gt; P(w) is satisfied as that which occurs significantly frequently with the seed word s. Note that the word clustering process is independent of topic analysis. While one could employ other methods (e.g., (Hofmann, 1999)) here for word clustering, our clustering algorithm is more efficient than conventional ones. For example, Hofmann&apos;s is of order 0(IDIIW12), while ours is only of 0(1131+ where IDI denotes the number of texts and IWI the number of words. That means that our method is more practical when a large amount of text data is available. 4 Topic Analysis 4.1 Input and Output In topic analysis, we use STM to parse a given text and output a topic structure which consists of segmented blocks and their topics. Figure 3 shows an example topic structure as output with our method. The text has been segmented </context>
<context position="24997" citStr="Hofmann, 1999" startWordPosition="4349" endWordPosition="4350">c analysis includes no notion of category. The output of topic analysis is a topic structure, while the output of text clas0.35 &amp;quot;STM* 0.3 0.25 02 -.3 0.15 0.1 0.05 00 5 10 15 sentence number 20 25 _n40 sification is a label representing a category. Furthermore, text classification is generally based on supervised learning, which uses labeled text data6. By way of contrast, topic analysis is based on unsupervised learning, which uses only unlabeled text data. Finite mixture models have been used in a variety of applications in text processing (e.g., (Li and Yamanishi, 1997; Nigam et al., 2000; Hofmann, 1999)), indicating that they are essential to text processing. We should note, however, that their definitions and the ways they use them are different from those for STM in this paper. For example, Li and Yamanishi propose to employ in text classification a mixture model (Li and Yamanishi, 1997) defined over categories: P(w1c) = E P (k1c)P (IWO, w E W, C E C, keic where W denotes a set of words, and C a set of categories. In their framework, a new text d is assigned into a category c* such that c* = arg maxcEc P(cld) is satisfied. Hofmann proposes using in information retrieval a joint distributio</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. Proc. of SIGIR&apos;99, pages 50-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachirns</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>Proc. of ECML&apos;98.</booktitle>
<marker>Joachirns, 1998</marker>
<rawString>T. Joachirns. 1998. Text categorization with support vector machines: Learning with many relevant features. Proc. of ECML&apos;98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>M Ringuette</author>
</authors>
<title>A comparison of two learning algorithms for test categorization.</title>
<date>1994</date>
<booktitle>Proc. of 3rd Ann. Symp. on Doc. Ana. and Info. Retr.,</booktitle>
<pages>81--93</pages>
<contexts>
<context position="34946" citStr="Lewis and Ringuette, 1994" startWordPosition="5982" endWordPosition="5985">me cluster, and its probability was relatively high. 7.5 Segmentation and Subtopic Identification We collected the 50 longest test texts (referred to here as &apos;seed texts&apos;) from each of the 10 categories, and combined each with a test text randomly selected from other categories to produce 500 pseudo-texts. Placement of the seed text within its pseudo-text (i.e., before or after the other text) was determined randomly. We used both STM and Corn to segment each of the pseudo-texts into two blocks and identify subtopics. Table 4 shows the segmentation results for the two method evaluated ample, (Lewis and Ringuette, 1994). Table 5: Subtopic identification results category of STM Com seed text rec. pre. rec. pre. earn 0.430 0.945 0.324 0.973 acq 0.237 0.939 0.217 0.959 money-fx 0.585 0.950 0.533 0.961 grain 0.276 0.947 0.222 0.938 crude 0.572 0.979 0.557 0.990 trade 0.634 0.951 0.627 0.899 interest 0.211 0.937 0.136 1.000 ship 0.260 1.000 0.340 0.994 wheat 0.500 0.970 0.395 0.980 corn 0.317 1.000 0.441 0.882 Average 0.402 0.962 0.379 0.958 in terms of recall, precision, and error probability. Table 5 shows the results of subtopic identification as evaluated in terms of recall and precision. Error probability is</context>
</contexts>
<marker>Lewis, Ringuette, 1994</marker>
<rawString>D. D. Lewis and M. Ringuette. 1994. A comparison of two learning algorithms for test categorization. Proc. of 3rd Ann. Symp. on Doc. Ana. and Info. Retr., pages 81-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>R E Schapire</author>
<author>J P Callan</author>
<author>R Papka</author>
</authors>
<title>Training algorithms for linear text classifiers.</title>
<date>1996</date>
<booktitle>Proc. of SIGIR&apos;96.</booktitle>
<contexts>
<context position="24225" citStr="Lewis et al., 1996" startWordPosition="4219" endWordPosition="4222">y employing TextTiling, estimate distribution of topics in each block, and identify topics having high probabilities in each block. Our method outper: forms such a combination (referred to hereafter as &apos;Com&apos;) for topic identification, because it utilizes word cluster information. It also performs better than Corn in text segmentation because it is based on a well-defined probability framework. Most importantly is that our method is able to output an easily understandable topic structure, which has not been proposed so far. Note that topic analysis is different from text classification (e.g., (Lewis et al., 1996; Li and Yamanishi, 1999; Joachims, 1998; Weiss et al., 1999; Nigam et al., 2000)). While text classification uses a number of pre-determined categories, topic analysis includes no notion of category. The output of topic analysis is a topic structure, while the output of text clas0.35 &amp;quot;STM* 0.3 0.25 02 -.3 0.15 0.1 0.05 00 5 10 15 sentence number 20 25 _n40 sification is a label representing a category. Furthermore, text classification is generally based on supervised learning, which uses labeled text data6. By way of contrast, topic analysis is based on unsupervised learning, which uses only </context>
</contexts>
<marker>Lewis, Schapire, Callan, Papka, 1996</marker>
<rawString>D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka. 1996. Training algorithms for linear text classifiers. Proc. of SIGIR&apos;96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>K Yamanishi</author>
</authors>
<title>Document classification using a finite mixture model.</title>
<date>1997</date>
<booktitle>Proc. of ACL &apos;97,</booktitle>
<pages>39--47</pages>
<contexts>
<context position="4348" citStr="Li and Yamanishi, 1997" startWordPosition="702" endWordPosition="706"> one can extract key words from a text using tf-idf, view these extracted key words as topics, segment the text into blocks using TextTiling, and estimate the distribution of topics (key words) within each block. Experimental results indicate, however, that our method significantly outperforms such a combined method in topic identification and outperforms it in text segmentation, because it utilizes word cluster information and employs a well-defined probability framework. Finite mixture models have been employed in a number of text processing applications, such as text classification (e.g., (Li and Yamanishi, 1997; Nigam et al., 2000)) and information retrieval (e.g., (Hofmann, 1999)). As will be discussed, however, our definition of a finite mixture model and the way we use it here differs significantly. 2 Stochastic Topic Model 2.1 Topic While the term &apos;topic&apos; is used in different ways in different linguistic theories, we simply view it here as a subject within a text. We represent a topic by means of a cluster of words that are closely related to the topic, assuming that a cluster has a seed word (or several seed words) which indicates a topic. Figure 1 shows an example topic with the word &apos;trade&apos; b</context>
<context position="24961" citStr="Li and Yamanishi, 1997" startWordPosition="4341" endWordPosition="4344"> a number of pre-determined categories, topic analysis includes no notion of category. The output of topic analysis is a topic structure, while the output of text clas0.35 &amp;quot;STM* 0.3 0.25 02 -.3 0.15 0.1 0.05 00 5 10 15 sentence number 20 25 _n40 sification is a label representing a category. Furthermore, text classification is generally based on supervised learning, which uses labeled text data6. By way of contrast, topic analysis is based on unsupervised learning, which uses only unlabeled text data. Finite mixture models have been used in a variety of applications in text processing (e.g., (Li and Yamanishi, 1997; Nigam et al., 2000; Hofmann, 1999)), indicating that they are essential to text processing. We should note, however, that their definitions and the ways they use them are different from those for STM in this paper. For example, Li and Yamanishi propose to employ in text classification a mixture model (Li and Yamanishi, 1997) defined over categories: P(w1c) = E P (k1c)P (IWO, w E W, C E C, keic where W denotes a set of words, and C a set of categories. In their framework, a new text d is assigned into a category c* such that c* = arg maxcEc P(cld) is satisfied. Hofmann proposes using in infor</context>
</contexts>
<marker>Li, Yamanishi, 1997</marker>
<rawString>H. Li and K. Yamanishi. 1997. Document classification using a finite mixture model. Proc. of ACL &apos;97, pages 39-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>K Yamanishi</author>
</authors>
<title>Text classification using ESC-based stochastic decision lists.</title>
<date>1999</date>
<booktitle>Proc. of ACM-CIKM&apos;99,</booktitle>
<pages>122--130</pages>
<contexts>
<context position="24249" citStr="Li and Yamanishi, 1999" startWordPosition="4223" endWordPosition="4226">ng, estimate distribution of topics in each block, and identify topics having high probabilities in each block. Our method outper: forms such a combination (referred to hereafter as &apos;Com&apos;) for topic identification, because it utilizes word cluster information. It also performs better than Corn in text segmentation because it is based on a well-defined probability framework. Most importantly is that our method is able to output an easily understandable topic structure, which has not been proposed so far. Note that topic analysis is different from text classification (e.g., (Lewis et al., 1996; Li and Yamanishi, 1999; Joachims, 1998; Weiss et al., 1999; Nigam et al., 2000)). While text classification uses a number of pre-determined categories, topic analysis includes no notion of category. The output of topic analysis is a topic structure, while the output of text clas0.35 &amp;quot;STM* 0.3 0.25 02 -.3 0.15 0.1 0.05 00 5 10 15 sentence number 20 25 _n40 sification is a label representing a category. Furthermore, text classification is generally based on supervised learning, which uses labeled text data6. By way of contrast, topic analysis is based on unsupervised learning, which uses only unlabeled text data. Fin</context>
</contexts>
<marker>Li, Yamanishi, 1999</marker>
<rawString>H. Li and K. Yamanishi. 1999. Text classification using ESC-based stochastic decision lists. Proc. of ACM-CIKM&apos;99, pages 122-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
</authors>
<title>A Probabilistic Approach to Lexical Semantic Knowledge Acquisition and Structural Disambiguation.</title>
<date>1998</date>
<tech>Ph.D. Thesis,</tech>
<institution>Univ. of Tokyo.</institution>
<contexts>
<context position="11223" citStr="Li, 1998" startWordPosition="2000" endWordPosition="2001">1) is (empirical) mutual information, which is an effective measure for word co-occurrence calculation (cf.,(Brown et al., 1992)). When the sample size is small, mutual information values tend to be undesirably large. The quantity within {- • .} in (1) can help avoid this undesirable tendency because its value will become large 3 4.2 Outline Our topic analysis consists of three processes: SC(wm : D) = (mail + I log -I- log r) a pre-process called &apos;topic spotting,&apos; text segms 2/r (mrr „n + log + log ) mentation, and topic identification. In topic I 774;g. 7r , 2For an introduction to MDL, see (Li, 1998). when data size is small. 37 ASIA, EXPORTERS FEAR MAGI FROB U.S.-JAPal RIFT (25-BAR-1987) block 0 trade-export-tariff-impert(0.12) Japan-lapanese(0.07) US(0.06) 0 Haunting trade friction betosen the U.S. and Japan has raised fears among many of Asia&apos;s exporting nations that the too could inflict ... 1 They told Reuter c rrrrr pendent&apos; in Asian capitals a U.S. move against Japan might boost protectionist sentiment in the U.S. and lead to ... 2 But some exporters said that while the conflict would hurt them in the long-run, in the short-term Tokyo&apos;s less might be their gain. 3 The U.S. Has said</context>
</contexts>
<marker>Li, 1998</marker>
<rawString>H. Li. 1998. A Probabilistic Approach to Lexical Semantic Knowledge Acquisition and Structural Disambiguation. Ph.D. Thesis, Univ. of Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
<author>K Nigam</author>
</authors>
<title>Text classification by bootstrapping with keywords, em and shrinkage.</title>
<date>1999</date>
<booktitle>Proc. of ACL&apos;99 Workshop Unsupervised Learning in NLP.</booktitle>
<contexts>
<context position="26414" citStr="McCallum and Nigam, 1999" startWordPosition="4589" endWordPosition="4593">g in retrieval those texts whose estimated word distributions P(udd) are similar to the word distribution of a query. 7 Experimental Results We have evaluated the performance of our topic analysis method (STM) in terms of three aspects: topic structure adequacy, text segmentation accuracy, and topic identification accuracy. 7.1 Data Set We know of no data available for the purpose of evaluation of topic analysis. We thus utilized Reuters news articles referred to as &apos;Reuters-21578,&apos; which has been widely used in text classification7. We used a prepared 6An exception is the method proposed in (McCallum and Nigam, 1999), which, instead of labeled texts, uses unlabeled texts, pre-determined categories, and keywords defined by humans for each category. &apos;Available at http://www.research.att.com/iewis/. split of the data &apos;Apte split,&apos; which consists of 9603 texts for training and 3299 texts for test. All of the texts had already been classified into 90 categories by human subjects. For each text, we used the Oxford Learner&apos;s Dictionary8 to conduct stemming, and removed &apos;stop words&apos; (e.g., &apos;the,&apos; and&apos;) that we had included on a previously prepared list. The average length of a text was about 115 words. (We did no</context>
</contexts>
<marker>McCallum, Nigam, 1999</marker>
<rawString>A. K. McCallum and K. Nigam. 1999. Text classification by bootstrapping with keywords, em and shrinkage. Proc. of ACL&apos;99 Workshop Unsupervised Learning in NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A K McCallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using em.</title>
<date>2000</date>
<journal>Machi. Lrn.,</journal>
<pages>39--103</pages>
<contexts>
<context position="4369" citStr="Nigam et al., 2000" startWordPosition="707" endWordPosition="710">ds from a text using tf-idf, view these extracted key words as topics, segment the text into blocks using TextTiling, and estimate the distribution of topics (key words) within each block. Experimental results indicate, however, that our method significantly outperforms such a combined method in topic identification and outperforms it in text segmentation, because it utilizes word cluster information and employs a well-defined probability framework. Finite mixture models have been employed in a number of text processing applications, such as text classification (e.g., (Li and Yamanishi, 1997; Nigam et al., 2000)) and information retrieval (e.g., (Hofmann, 1999)). As will be discussed, however, our definition of a finite mixture model and the way we use it here differs significantly. 2 Stochastic Topic Model 2.1 Topic While the term &apos;topic&apos; is used in different ways in different linguistic theories, we simply view it here as a subject within a text. We represent a topic by means of a cluster of words that are closely related to the topic, assuming that a cluster has a seed word (or several seed words) which indicates a topic. Figure 1 shows an example topic with the word &apos;trade&apos; being the seed word. t</context>
<context position="24306" citStr="Nigam et al., 2000" startWordPosition="4233" endWordPosition="4236">ify topics having high probabilities in each block. Our method outper: forms such a combination (referred to hereafter as &apos;Com&apos;) for topic identification, because it utilizes word cluster information. It also performs better than Corn in text segmentation because it is based on a well-defined probability framework. Most importantly is that our method is able to output an easily understandable topic structure, which has not been proposed so far. Note that topic analysis is different from text classification (e.g., (Lewis et al., 1996; Li and Yamanishi, 1999; Joachims, 1998; Weiss et al., 1999; Nigam et al., 2000)). While text classification uses a number of pre-determined categories, topic analysis includes no notion of category. The output of topic analysis is a topic structure, while the output of text clas0.35 &amp;quot;STM* 0.3 0.25 02 -.3 0.15 0.1 0.05 00 5 10 15 sentence number 20 25 _n40 sification is a label representing a category. Furthermore, text classification is generally based on supervised learning, which uses labeled text data6. By way of contrast, topic analysis is based on unsupervised learning, which uses only unlabeled text data. Finite mixture models have been used in a variety of applica</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell. 2000. Text classification from labeled and unlabeled documents using em. Machi. Lrn., 39:103-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
</authors>
<title>Statistical models for topic segmentation.</title>
<date>1999</date>
<booktitle>Proc. of ACL &apos;99,</booktitle>
<pages>357--364</pages>
<contexts>
<context position="22684" citStr="Reynar, 1999" startWordPosition="3976" endWordPosition="3977">lates the tf-idf value of each word in a text and uses those words having the largest tf-idf values as key words for that text (e.g., (Salton and Yang, 1973)). One can view these extracted key words as the topics of the text. No keyword extraction method by itself, however, is able to conduct segmentation. With respect to text segmentation, existing methods can be classified into two groups. One is to divide a text into blocks (e.g., TextTiling (Hearst, 1997)), the other to divide a stream of texts into its original texts (e.g.,(Allan et al., 1998; Yamron et al., 1998; Beeferman et al., 1999; Reynar, 1999)). The former group generally employs unsupervised learning, while the latter supervised one. No existing segmentation method, however, has attempted topic identification. TextTiling creates for each segmentation candidate two pseudo-texts, one preceding it and the other following it, and calculates as similarity the cosine value between the word frequency vectors of the two pseudo texts. It then conducts segmentation at valley points in a similar way to that of our method. Since the problem setting of TextTiling (in general the former group) is most close to that of our study, we use TextTili</context>
</contexts>
<marker>Reynar, 1999</marker>
<rawString>J. C. Reynar. 1999. Statistical models for topic segmentation. Proc. of ACL &apos;99, pages 357-364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Fisher information and stochastic complexity.</title>
<date>1996</date>
<journal>IEEE Trans. on Info. Thry.,</journal>
<pages>42--1</pages>
<contexts>
<context position="3127" citStr="Rissanen, 1996" startWordPosition="510" endWordPosition="511">ution). The second level consists of probability distributions of words included within topics (word distributions). These word distributions are linearly combined to represent a word distribution within a text, with the topic distribution being used as the coefficient vector. Hereafter we refer to a finite mixture model having this structure as a stochastic topic model (STM). Before conducting topic analysis, we create word clusters (topics) on the basis of word cooccurrence in corpus data. We have developed a new method for word clustering using stochastic complexity (or the MDL principle) (Rissanen, 1996). In topic analysis, we estimate a sequence of STMs that would have given rise to a given text, assuming that each block of a text is generated by an individual STM. We perform text segmentation by detecting significant differences between STMs and perform topic identification by means of estimation of STMs. With the results, we obtain the text&apos;s topic structure which consists of segmented blocks and their topics. It is possible to perform topic analysis by combining an existing word extraction method (e.g., tf-idf) and an existing text seg35 mentation method (e.g., TextTiling). Specifically, </context>
<context position="7232" citStr="Rissanen, 1996" startWordPosition="1227" endWordPosition="1228">s, and for each seed word we collect from the data those words which frequently co-occur with it and group them into a cluster. As one example, the word-cluster in Figure 1 has been constructed with the word &apos;trade&apos; as the seed word. We have developed a new method for reliably collecting frequently co-occurring words on the basis of stochastic complexity, or the MDL principle. For a given data sequence = xi ... x, and for a fixed probability model M,1 the stochastic complexity of xm relative to M, which we denote as SC(xm : M), is defined as the least code length required to encode xm with M (Rissanen, 1996). SC(&amp;quot; : M) can be interpreted as the amount information included in xn relative to M. The &apos;Here, we use &apos;model&apos; to refer to a-probability distribution which has specified parameters but unspecified parameter values. 36 MDL (Minimum Description Length) principle is a model selection criterion which asserts that, for a given data sequence, the lower a model&apos;s SC value, the greater its likelihood of being a model which would have actually generated the data. MDL has many good properties as a criterion for model selection.&apos; For a fixed seed word s, we take a word w as a frequently co-occurring wo</context>
</contexts>
<marker>Rissanen, 1996</marker>
<rawString>J. Rissanen. 1996. Fisher information and stochastic complexity. IEEE Trans. on Info. Thry., 42(1):40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C S Yang</author>
</authors>
<title>On the specification of term values in automatic indexing.</title>
<date>1973</date>
<journal>Journ. of Doc.,</journal>
<pages>29--4</pages>
<contexts>
<context position="1550" citStr="Salton and Yang, 1973" startWordPosition="250" endWordPosition="253"> segmentation (based on topic changes). Topic analysis is extremely useful in a variety of text processing applications. For example, it can be used in the automatic indexing of texts for purposes of information retrieval. With it, one can understand what the main topics and subtopics of a text are, and where those subtopics lie within the text. To the best of our knowledge, however, no previous study has so far dealt with the topic analysis problem in the above sense. The most closely related are key word extraction and text segmentation. A keyword extraction method (e.g., that using tf-idf (Salton and Yang, 1973)) generally extracts from a text key words which represent topics within the text, but it does not conduct segmentation. A segmentation method (e.g., TextTiling (Hearst, 1997)) generally segments a text into blocks (paragraphs) in accord with topic changes within the text, but it does not identify (or label) by itself the topics discussed in each of the blocks. The purpose of this paper is to provide a single framework for conducting topic analysis, i.e., performing both topic identification and text segmentation. The key characteristics of our framework are 1) representing a topic by means of</context>
<context position="16252" citStr="Salton and Yang, 1973" startWordPosition="2820" endWordPosition="2823">.3 Topic Spotting In topic spotting, we first select key words from a given text. We calculate what we call the Shannon information of each word in the text. The Shannon information of word w in text t is defined as I(w) = —N(w)log P(w), where N(w) denotes the frequency of w in t, and P(w) the probability of the occurrence of w as estimated from corpus data. I(w) may be interpreted as the amount of information represented by w. We select as key words the top 1 words sorted in descending order of I. While Shannon information is similar to the tf-idf widely used in information retrieval (e.g., (Salton and Yang, 1973)), the use of Shannon information can be justified on the basis of information theory, but that of tf-idf cannot. Our preliminary experimental results indicate that Shannon information performs better than or at least as well as tf-idf in key word extraction.4 From the results of word clustering, we next select any cluster (topic) whose seed word is included among the selected key words. We next merge any two clusters if one of their seed words is included in the other&apos;s cluster. For example, when a cluster with seed word &apos;trade&apos; contains the word &apos;import,&apos; and a cluster with seed word &apos;import</context>
<context position="22228" citStr="Salton and Yang, 1973" startWordPosition="3895" endWordPosition="3898">ntent by looking at the topic structure as, for example, it is shown in Figure 3. Our method can also be useful for text mining, text summarization, information extraction, and other text processing, which require one to first analyze the structure of a text. 6 Related Work To the best of our knowledge, no previous study has so far dealt with topic identification and text segmentation within a single framework. A widely used method for key word extraction calculates the tf-idf value of each word in a text and uses those words having the largest tf-idf values as key words for that text (e.g., (Salton and Yang, 1973)). One can view these extracted key words as the topics of the text. No keyword extraction method by itself, however, is able to conduct segmentation. With respect to text segmentation, existing methods can be classified into two groups. One is to divide a text into blocks (e.g., TextTiling (Hearst, 1997)), the other to divide a stream of texts into its original texts (e.g.,(Allan et al., 1998; Yamron et al., 1998; Beeferman et al., 1999; Reynar, 1999)). The former group generally employs unsupervised learning, while the latter supervised one. No existing segmentation method, however, has atte</context>
</contexts>
<marker>Salton, Yang, 1973</marker>
<rawString>G. Salton and C.S. Yang. 1973. On the specification of term values in automatic indexing. Journ. of Doc., 29(4):351-372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Weiss</author>
<author>C Apte</author>
<author>F Damerau</author>
<author>F J Oles</author>
<author>T Goetz</author>
<author>T Hampp</author>
</authors>
<title>Maximizing text-mining performance.</title>
<date>1999</date>
<journal>IEEE Intel. Sys.,</journal>
<pages>14--4</pages>
<contexts>
<context position="24285" citStr="Weiss et al., 1999" startWordPosition="4229" endWordPosition="4232">ach block, and identify topics having high probabilities in each block. Our method outper: forms such a combination (referred to hereafter as &apos;Com&apos;) for topic identification, because it utilizes word cluster information. It also performs better than Corn in text segmentation because it is based on a well-defined probability framework. Most importantly is that our method is able to output an easily understandable topic structure, which has not been proposed so far. Note that topic analysis is different from text classification (e.g., (Lewis et al., 1996; Li and Yamanishi, 1999; Joachims, 1998; Weiss et al., 1999; Nigam et al., 2000)). While text classification uses a number of pre-determined categories, topic analysis includes no notion of category. The output of topic analysis is a topic structure, while the output of text clas0.35 &amp;quot;STM* 0.3 0.25 02 -.3 0.15 0.1 0.05 00 5 10 15 sentence number 20 25 _n40 sification is a label representing a category. Furthermore, text classification is generally based on supervised learning, which uses labeled text data6. By way of contrast, topic analysis is based on unsupervised learning, which uses only unlabeled text data. Finite mixture models have been used in</context>
</contexts>
<marker>Weiss, Apte, Damerau, Oles, Goetz, Hampp, 1999</marker>
<rawString>S. M. Weiss, C. Apte, F. Damerau, F. J. Oles, T. Goetz, and T. Hampp. 1999. Maximizing text-mining performance. IEEE Intel. Sys., 14(4):63-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Yamron</author>
<author>I Carp</author>
<author>L Gillick</author>
<author>S Lowe</author>
<author>P van Mulbregt</author>
</authors>
<title>A Hidden Markov Model approach to text segmentation and event tracking.</title>
<date>1998</date>
<booktitle>Proc. of ICASSP&apos;99,</booktitle>
<pages>333--336</pages>
<marker>Yamron, Carp, Gillick, Lowe, van Mulbregt, 1998</marker>
<rawString>J.P. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt. 1998. A Hidden Markov Model approach to text segmentation and event tracking. Proc. of ICASSP&apos;99, pages 333-336.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>