<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006280">
<title confidence="0.985602">
Two Decades of Unsupervised POS induction: How far have we come?
</title>
<author confidence="0.986064">
Christos Christodoulopoulos
</author>
<affiliation confidence="0.9981505">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.995897">
christos.c@ed.ac.uk
</email>
<author confidence="0.992951">
Sharon Goldwater
</author>
<affiliation confidence="0.9984135">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.996339">
sgwater@inf.ed.ac.uk
</email>
<author confidence="0.996475">
Mark Steedman
</author>
<affiliation confidence="0.9984655">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.997027">
steedman@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.996777" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.75880576">
Part-of-speech (POS) induction is one of the
most popular tasks in research on unsuper-
vised NLP. Many different methods have been
proposed, yet comparisons are difficult to
make since there is little consensus on eval-
uation framework, and many papers evalu-
ate against only one or two competitor sys-
tems. Here we evaluate seven different POS
induction systems spanning nearly 20 years of
work, using a variety of measures. We show
that some of the oldest (and simplest) systems
stand up surprisingly well against more recent
approaches. Since most of these systems were
developed and tested using data from the WSJ
corpus, we compare their generalization abil-
ities by testing on both WSJ and the multi-
lingual Multext-East corpus. Finally, we in-
troduce the idea of evaluating systems based
on their ability to produce cluster prototypes
that are useful as input to a prototype-driven
learner. In most cases, the prototype-driven
learner outperforms the unsupervised system
used to initialize it, yielding state-of-the-art
results on WSJ and improvements on non-
English corpora.
</bodyText>
<sectionHeader confidence="0.999346" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999843295454545">
In recent years, unsupervised learning has become
a hot area in NLP, in large part due to the use of
sophisticated machine learning approaches which
promise to deliver better results than more tradi-
tional methods. Often the new approaches are tested
using part-of-speech (POS) tagging as an example
application, and usually they are shown to perform
better than one or another comparison system. How-
ever, it is difficult to draw overall conclusions about
the relative performance of unsupervised POS tag-
ging systems because of differences in evaluation
measures, and the fact that no paper includes di-
rect comparisons against more than a few other sys-
tems. In this paper, we attempt to remedy that
situation by providing a comprehensive evaluation
of seven different POS induction systems spanning
nearly 20 years of research. We focus specifically
on POS induction systems, where no prior knowl-
edge is available, in contrast to POS disambigua-
tion systems (Merialdo, 1994; Toutanova and John-
son, 2007; Naseem et al., 2009; Ravi and Knight,
2009; Smith and Eisner, 2005), which use a dic-
tionary to provide possible tags for some or all of
the words in the corpus, or prototype-driven sys-
tems (Haghighi and Klein, 2006), which use a small
set of prototypes for each tag class, but no dictio-
nary. Our motivation stems from another part of our
own research, in which we are trying to use NLP
systems on over 50 low-density languages (some of
them dead) where both tagged corpora and language
speakers are mostly unavailable. We therefore de-
sire to use these systems straight out of the box and
to know how well we can expect them to work.
One difficulty in evaluating POS induction sys-
tems is that there is no straightforward way to map
the clusters found by the algorithm onto the gold
standard tags; moreover, some systems are designed
to induce the number of clusters as well as their
contents, so the number of found clusters may not
match either the gold standard or that of another sys-
tem. Nevertheless, most recent papers have used
mapping-based performance measures (either one-
to-one or many-to-one accuracy). Here, we argue
that the entropy-based V-Measure (Rosenberg and
</bodyText>
<page confidence="0.969489">
575
</page>
<note confidence="0.8202085">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 575–584,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998960594594595">
Hirschberg, 2007) is more useful in many cases, be-
ing more stable across different numbers of found
and true clusters, and avoiding several of the prob-
lems with another commonly used entropy-based
measure, Variation of Information (Meilˇa, 2003).
Using V-Measure along with several other evalu-
ation measures, we compare the performance of the
different induction systems on both WSJ (the data on
which most systems were developed and tested) and
Multext East, a corpus of parallel texts in eight dif-
ferent languages. We find that for virtually all mea-
sures and datasets, older systems using relatively
simple models and algorithms (Brown et al., 1992;
Clark, 2003) work as well or better than systems
using newer and often far more sophisticated and
time-consuming machine learning methods (Gold-
water and Griffiths, 2007; Johnson, 2007; Graca et
al., 2009; Berg-Kirkpatrick et al., 2010). Thus, al-
though these newer methods have introduced po-
tentially useful machine learning techniques, they
should not be assumed to provide the best perfor-
mance for unsupervised POS induction.
In addition to our review and comparison, we in-
troduce a new way to both evaluate and potentially
improve a POS induction system. Our method is
based on the prototype-driven learning system of
Haghighi and Klein (2006), which achieves very
good performance by using a hand-selected list of
prototypes for each syntactic cluster. We instead use
the existing POS induction systems to induce proto-
types automatically, and evaluate the systems based
on the quality of their prototypes. We find that the
oldest system tested (Brown et al., 1992) produces
the best prototypes, and that using these prototypes
as input to Haghighi and Klein’s system yields state-
of-the-art performance on WSJ and improvements
on seven of the eight non-English corpora.
</bodyText>
<sectionHeader confidence="0.993472" genericHeader="method">
2 POS Induction Systems
</sectionHeader>
<bodyText confidence="0.996335744186046">
We describe each system only briefly; for details,
see the respective papers, cited below. Each system
outputs a set of syntactic clusters C; except where
noted, the target number of clusters |C |must be
specified as an input parameter. Since we are in-
terested in out-of-the-box performance, we use the
default parameter settings for each system, except
for |C|, which is varied in some of our experiments.
The systems are as follows:1
[brown]: Class-based n-grams (Brown et al.,
1992). This is the oldest and one of the simplest sys-
tems we tested. It uses a bigram model where each
word type is assigned to a latent class (a hard assign-
ment), and the probability of the corpus w1 ... wn
is computed as P(w1|c1) ∏n i=2 P(wi|ci)P(ci|ci−1),
where ci is the class of wi. The goal is to opti-
mize the probability of the corpus under this model.
The authors use an approximate search procedure:
greedy agglomerative hierarchical clustering fol-
lowed by a step in which individual word types are
considered for movement to a different class if this
improves the corpus probability.
[clark]: Class-based n-grams with morphology
(Clark, 2003). This system uses a similar model
to the previous one, and also clusters word types
(rather than tokens, as the rest of the systems do).
The main differences between the systems are that
clark uses a slightly different approximate search
procedure, and that he augments the probabilistic
model with a prior that prefers clusterings where
morphologically similar words are clustered to-
gether. The morphology component is implemented
as a single-order letter HMM.
[cw]: Chinese Whispers graph clustering (Bie-
mann, 2006). Unlike the other systems we consider,
this one induces the value of |C |rather than taking
it as an input parameter.2 The system uses a graph
clustering algorithm called Chinese Whispers that is
based on contextual similarity. The algorithm works
in two stages. The first clusters the most frequent
10,000 words (target words) based on their context
statistics, with contexts formed from the most fre-
quent 150-250 words (feature words) that appear ei-
</bodyText>
<footnote confidence="0.988424533333333">
1Implementations were obtained from:
brown: http://www.cs.berkeley.edu/∼pliang/
software/brown-cluster-1.2.zip (Percy Liang),
clark: http://www.cs.rhul.ac.uk/home/alexc/
pos2.tar.gz (Alex Clark),
cw: http://wortschatz.uni-leipzig.de/%7Ecbiemann/
software/jUnsupos1.0.zip (Chris Biemann),
bhmm, vbhmm, pr, feat: by request from the authors of the
respective papers.
2Another recent model that induces |C |is the Infinite HMM
(Van Gael et al., 2009). Unfortunately, we were unable to ob-
tain code for the IHMM in time to include it in our analysis.
Van Gael et al. (2009) report results of around 59% V-Measure
on WSJ, with 194 induced clusters, which is not as good as the
best system scores in Section 4.
</footnote>
<page confidence="0.998093">
576
</page>
<bodyText confidence="0.994087653061224">
ther to the left or right of a target word. The second
stage deals with medium and low frequency words
and uses pairwise similarity scores calculated by the
number of shared neighbors between two words in
a 4-word context window. The final clustering is
a combination of the clusters obtained in the two
stages. While the number of target words, feature
words, and window size are in principle parameters
of the algorithm, they are hard-coded in the imple-
mentation we used and we did not change them.
[bhmm]: Bayesian HMM with Gibbs sampling
(Goldwater and Griffiths, 2007). This system is
based on a standard HMM for POS tagging. It dif-
fers from the standard model by placing Dirichlet
priors over the multinomial parameters defining the
state-state and state-emission distributions, and uses
a collapsed Gibbs sampler to infer the hidden tags.
The Dirichlet hyperparameters a (which controls the
sparsity of the transition probabilities) and Q (which
controls the sparsity of the emission probabilities)
can be fixed or inferred. We used a bigram version
of this model with hyperparameter inference.
[vbhmm]: Bayesian HMM with variational
Bayes (Johnson, 2007). This system uses the
same bigram model as bhmm, but uses variational
Bayesian EM for inference. We fixed the a and Q
parameters to 0.1, values that appeared to be reason-
able based on Johnson (2007), and which were also
used by Graca et al. (2009).
[pr]: Sparsity posterior-regularization HMM
(Graca et al., 2009). The Bayesian approaches de-
scribed above encourage sparse state-state and state-
emission distributions only indirectly through the
Dirichlet priors. This system, while utilizing the
same bigram HMM, encourages sparsity directly
by constraining the posterior distributions using the
posterior regularization framework (Ganchev et al.,
2009). A parameter a controls the strengths of the
constraints (default = 25). Following Graca et al.
(2009), we set a = Q = 0.1.
[feat]: Feature-based HMM (Berg-Kirkpatrick
et al., 2010). This system uses a model that has the
structure of a standard HMM, but assumes that the
state-state and state-emission distributions are logis-
tic, rather than multinomial. The logistic distribu-
tions allow the model to incorporate local features
of the sort often used in discriminative models. The
default features are morphological, such as character
trigrams and capitalization.
</bodyText>
<sectionHeader confidence="0.994313" genericHeader="method">
3 Evaluation Measures
</sectionHeader>
<bodyText confidence="0.997185744186047">
One difficulty in comparing POS induction meth-
ods is in finding an appropriate evaluation measure.
Many different measures have been proposed over
the years, but there is still no consensus on which is
best. In addition, some measures with supposed the-
oretical advantages, such as Variation of Information
(VI) (Meilˇa, 2003) have had little empirical analy-
sis. Our goal in this section is to determine which
of these measures is most sensible for evaluating
the systems presented above. We first describe each
measure before presenting empirical results. Except
for VI, all measures range from 0 to 1, with higher
scores indicating better performance.
[many-to-1]: Many-to-one mapping accuracy
(also known as cluster purity) maps each cluster to
the gold standard tag that is most common for the
words in that cluster (henceforth, the preferred tag),
and then computes the proportion of words tagged
correctly. More than one cluster may be mapped to
the same gold standard tag. This is the most com-
monly used metric across the literature as it is in-
tuitive and creates a meaningful POS sequence out
of the cluster identifiers. However, it tends to yield
higher scores as ICI increases, making comparisons
difficult when |C |can vary.
[crossval]: Cross-validation accuracy (Gao and
Johnson, 2008) is intended to address the problem
with many-to-one accuracy which is that assigning
each word to its own class yields a perfect score. In
this measure, the first half of the corpus is used to
obtain the many-to-one mapping of clusters to tags,
and this mapping is used to compute the accuracy of
the clustering on the second half of the corpus.
[1-to-1]: One-to-one mapping accuracy
(Haghighi and Klein, 2006) constrains the mapping
from clusters to tags, so that at most one cluster can
be mapped to any tag. The mapping is performed
greedily. In general, as the number of clusters
increases, fewer clusters will be mapped to their
preferred tag and scores will decrease (especially
if the number of clusters is larger than the number
of tags, so that some clusters are unassigned and
receive zero credit). Again, this makes it difficult to
</bodyText>
<page confidence="0.981437">
577
</page>
<bodyText confidence="0.9963482">
compare solutions with different values of |C|.
[vi]: Variation of Information (Meilˇa, 2003) is
an information-theoretic measure that regards the
system output C and the gold standard tags T as two
separate clusterings, and evaluates the amount of in-
formation lost in going from C to T and the amount
of information gained, i.e., the sum of the condi-
tional entropy of each clustering conditioned on the
other. More formally, V I(C, T) = H(T|C) +
H(C|T) = H(C) + H(T) − 2I(C, T), where H(.)
is the entropy function and I(.) is the mutual infor-
mation. VI and other entropy-based measures have
been argued to be superior to accuracy-based mea-
sures such as those above, because they consider
not only the majority tag in each cluster, but also
whether the remainder of the cluster is more or less
homogeneous. Unlike the other measures we con-
sider, lower scores are better (since VI measures the
difference between clusterings in bits).
[vm]: V-Measure (Rosenberg and Hirschberg,
2007) is another entropy-based measure that is de-
signed to be analogous to F-measure, in that it is de-
fined as the weighted harmonic mean of two values,
homogeneity (h, the precision analogue) and com-
pleteness (c, the recall analogue):
</bodyText>
<equation confidence="0.8937849">
h = H(T |C)
c = 1
V M = −
H(T )
H(C|T )
1
−
H(C)
(1 + β)hc
(βh) + c
</equation>
<bodyText confidence="0.991504171428571">
As with F-measure, β is normally set to 1.
[vmb]: V-beta is an extension to V-Measure, pro-
posed by (Vlachos et al., 2009). They noted that
V-Measure favors clusterings where the number of
clusters |C |is larger than the number of POS tags
|T|. To address this issue the parameter β in equa-
tion 3 is set to |C|/|T |in order adjust the balance
between homogeneity and completeness.
[s-fscore]: Substitutable F-score (Frank et al.,
2009). One potential issue with all of the above mea-
sures is that they require a gold standard tagging to
compute. This is normally available during develop-
ment of a system, but if the system is deployed on a
novel language a gold standard may not be available.
In addition, there is the question of whether the gold
standard itself is “correct”. Recently, Frank et al.
(2009) proposed this novel evaluation measure that
requires no gold standard, instead using the concept
of substitutability to evaluate performance. Instead
of comparing the system’s clusters C to gold stan-
dard clusters T, they are compared to a set of clus-
ters S created from substitutable frames, i.e., clus-
ters of words that occur in the same syntactic en-
vironment. Ideally a substitutable frame would be
created by sentences differing in only one word (e.g.
“I want the blue ball.” and “I want the red ball.”)
and the resulting cluster would contain the words
that change (e.g. [blue, red]). However since it is
almost impossible to find these types of sentences
in real-world corpora, the authors use frames cre-
ated by two words appearing in the corpus with ex-
actly one word between (e.g. the —- ball). Once the
substitutable clusters have been created, they can be
used to calculate the Precision (SP), Recall (SR)
and F-score (SF) of the system’s clustering:
</bodyText>
<equation confidence="0.823155125">
SP = ∑sES ∑IEC |s n c|(|s n c |− 1)
SR =
SF =
∑IEC |c|(|c |− 1)
∑sES ∑IEC |s n c|(|s n c |− 1)
∑sES |s|(|s |− 1)
2 · SP · SR
SP + SR
</equation>
<subsectionHeader confidence="0.99669">
3.1 Empirical results
</subsectionHeader>
<bodyText confidence="0.999970533333333">
We mentioned a few strengths and weaknesses of
each evaluation method above; in this section we
present some empirical results to expand on these
claims. First, we examine the effects of varying |C|
on the behavior of the evaluation measures, while
keeping the number of gold standard tags the same
(|T |= 45). Results were obtained by training and
evaluating each system on the full WSJ portion of
the Penn Treebank corpus (Marcus et al., 1993). Fig-
ure 1 shows the results from the Brown system for
|C |ranging from 1 to 200; the same trends were ob-
served for all other systems.3 In addition, Table 1
provides results for the two extremes of |C |= 1 (all
words assigned to the same cluster) and |C |equal to
the size of the corpus (a single word per cluster), as
</bodyText>
<footnote confidence="0.9841586">
3The results reported in this paper are only a fraction
of the total from our experiments; given the number of
parameters, models and measures tested, we obtained over
15000 results. The full set of results can be found at
http://homepages.inf.ed.ac.uk/s0787820/pos/.
</footnote>
<page confidence="0.981255">
578
</page>
<figureCaption confidence="0.984547">
Figure 1: Scores for all evaluation measures as a function of the number of clusters returned [model:brown, corpus:wsj,
|C|:{1-200}, |T|:45]. The right-hand y-axis shows VI scores (lower is better); the left-hand y-axis shows percentage
scores for all other measures. The vertical line indicates |T|. Many-to-1 is invisible as it tracks crossval so closely.
</figureCaption>
<table confidence="0.996312375">
measure super random all single
many-to-1 97.85 13.97 13.97 100
crossval 97.59 13.98 13.98 0
1-to-1 97.86 2.42 13.97 0.01
vi 0.35 9.81 4.33 15.82
vm 95.98 0.02 0 35.42
vmb 95.98 0 0 99.99
s-fscore 7.53 0.50 0 0
</table>
<tableCaption confidence="0.938025">
Table 1: Baseline scores for the different evaluation mea-
sures on the WSJ corpus. For all measures except VI
higher is better.
</tableCaption>
<bodyText confidence="0.999863307692308">
well as two other baselines (a supervised tagging4
and a random clustering with |C |= 45).
These empirical results confirm that certain mea-
sures favor solutions with many clusters, while oth-
ers prefer fewer clusters. As expected, many-to-1
correlates positively with |C|, rising to almost 85%
with |C |= 200 and reaching 100% when the num-
ber of clusters is maximal (i.e., single). Recall that
crossval was proposed as a possible solution to this
problem, and it does solve the extreme case of sin-
gle, yielding 0% accuracy rather than 100%. How-
ever, it patterns just like many-to-1 for up to 200
clusters, suggesting that there is very little difference
</bodyText>
<footnote confidence="0.9312565">
4We used the Stanford Tagger trained on the WSJ corpus:
http://nlp.stanford.edu/software/tagger.shtml.
</footnote>
<bodyText confidence="0.998657407407407">
between the two for any reasonable number of clus-
ters, and we should be wary of using either one when
|C |may vary.
In contrast to these measures are 1-to-1 and vi: for
the most part, they yield worse performance (lower
1-to-1, higher vi) as |C |increases. However, in this
case the trend is not monotonic: there is an initial
improvement in performance before the decrease be-
gins. One might hope that the peak in performance
would occur when the number of clusters is approx-
imately equal to the number of gold standard tags;
however, the best performance for both 1-to-1 and
vi occurs with approximately 25-30 clusters, many
fewer than the gold standard 45.
Next we consider vm and vmb. Interestingly, al-
though vmb was proposed as a way to correct for the
supposed tendency of vm to increase with increas-
ing |C|, we find that vm is actually more stable than
vmb over different values of |C|. Thus, if the goal
is to compare systems producing different numbers
of clusters (especially important for systems that in-
duce the number of clusters), then vm seems more
appropriate than any of the above measures, which
are more standard in the literature.
Finally, we analyze the behavior of the gold-
standard-independent measure, s-fscore. On the
positive side, this measure assigns scores of 0 to the
</bodyText>
<page confidence="0.992415">
579
</page>
<bodyText confidence="0.999993517241379">
two extreme cases of all and single, and is relatively
stable across different values of |C |after an initial
increase. It assigns a lower score to the supervised
system than to brown, indicating that words in the
supervised clusters (which are very close to the gold
standard) are actually less substitutable than words
in the unsupervised clusters. This is probably due to
the fact that the gold standard encodes “pure” syn-
tactic classes, while substitutability also depends on
semantic characteristics (which tend to be picked up
by unsupervised clustering systems as well). An-
other potential problem with this measure is that it
has a very small dynamic range – while scores as
high as 1 are theoretically possible, in practice they
will never be achieved, and we see that the actual
range of scores observed are all under 20%.
We conclude that there is probably no single eval-
uation measure that is best for all purposes. If a gold
standard is available, then many-to-1 and 1-to-1 are
the most intuitive measures, but should not be used
when |C |is variable, and do not account for differ-
ences in the errors made. While vi has been popular
as an entropy-based alternative to address the latter
problem, its scores are not easy to interpret (being on
a scale of bits) and it still has the problem of incom-
parability across different |C|. Overall, vm seems to
be the best general-purpose measure that combines
an entropy-based score with an intuitive 0-1 scale
and stability over a wide range of |C|.
</bodyText>
<sectionHeader confidence="0.992783" genericHeader="method">
4 System comparison
</sectionHeader>
<bodyText confidence="0.999558888888889">
Having provided some intuitions about the behav-
ior of different evaluation methods, we move on to
evaluating the various systems presented in Section
2. We first present results for the same WSJ cor-
pus used above. However, because most of the sys-
tems were initially developed on this corpus, and
often evaluated only on it, there is a question of
whether their methods and/or hyperparameters are
overly specific to the domain or to the English lan-
guage. This is a particularly pertinent question since
a primary argument in favor of unsupervised sys-
tems is that they are easier to port to a new language
or domain than supervised systems. To address this
question, we evaluate all the systems as well on the
multilingual Multext East corpus (Erjavec, 2004),
without changing any of the parameter settings. |C|
was set to 45 for all of the experiments reported in
this section. Based on our assessment of evaluation
</bodyText>
<figureCaption confidence="0.9860565">
Figure 2: Performance of the different systems on WSJ,
using three different measures [|C|:45, |T|:45]
</figureCaption>
<bodyText confidence="0.384536">
pr e10 hrs.*
feat e40 hrs.*
</bodyText>
<tableCaption confidence="0.922389333333333">
Table 2: Runtimes for the different systems on WSJ
[|C|:45]. *pr and feat have multithreading implemen-
tations and ran on 16 cores.
</tableCaption>
<bodyText confidence="0.9998512">
measures above, we report VM scores as the most
reliable measure across different systems and clus-
ter set sizes; to facilitate comparisons with previous
papers, we also report many-to-one and one-to-one
accuracy.
</bodyText>
<subsectionHeader confidence="0.949709">
4.1 Results on WSJ
</subsectionHeader>
<bodyText confidence="0.999872785714286">
Figure 2 presents results for all seven systems, with
approximate runtimes shown in Table 2. While these
algorithms have not necessarily been optimized for
speed, there is a fairly clear distinction between the
older type-clustering models (brown, clark) and the
graph-based algorithm (cw) on the one hand, and
the newer machine-learning approaches (bhmm,
vbhmm, pr, feat) on the other, with the former be-
ing much faster to run. Despite their faster run-
times and less sophisticated methods, however, these
systems perform surprisingly well in comparison to
the latter group. Even the oldest and perhaps sim-
plest method (brown) outperforms the two BHMMs
and posterior regularization on all measures. Only
</bodyText>
<figure confidence="0.9557148">
system runtime
brown e10 min.
clark
cw
bhmm
vbhmm
e40 min.
e10 min.
e4 hrs.
e10 hrs.
</figure>
<page confidence="0.751924">
580
</page>
<figureCaption confidence="0.99532925">
Figure 3: VM scores for the different systems on English
Multext-East and WSJ-S corpora [ C :45, T :{14,17}]
Figure 4: VM scores for the different systems on the eight
Multext-East corpora [ C :45, T :14]
</figureCaption>
<bodyText confidence="0.999753823529412">
the very latest approach (feat) rivals clark, show-
ing slightly better performance on two of the three
measures (clark: 71.2, 53.8, 65.5 on many-to-one,
one-to-one, VM; feat: 73.9, 53.3, 67.7). The cw
system returns a total of 568 clusters on this data set,
so the many-to-one and one-to-one measures are not
strictly comparable to the other systems; on VM this
system achieves middling performance.
We note that the two best-performing systems,
clark and feat, are also the only two to use mor-
phological information. Since the clustering algo-
rithms used by brown and clark are quite similar,
the difference in performance between the two can
probably be attributed to the extra information pro-
vided by the morphology. This supports the (unsur-
prising) conclusion that incorporating morphologi-
cal features is generally helpful for POS induction.
</bodyText>
<subsectionHeader confidence="0.92437">
4.2 Results on other corpora
</subsectionHeader>
<bodyText confidence="0.999843813953489">
We now examine whether either the relative or ab-
solute performance of the different systems holds up
when tested on a variety of different languages. For
these experiments, we used the 1984 portion of the
Multext-East corpus (�7k sentences), which contains
parallel translations of Orwell’s 1984 in 8 different
languages: Bulgarian[bg], Czech[cs], Estonian[et],
Hungarian[hu], Romanian[ro], Slovene[sl], Ser-
bian[sr] and English[en]. We also included a 7k
sentence version of the WSJ corpus [wsj-s] to help
differentiate effects of corpus size from those of do-
main/language. For the WSJ corpora we experi-
mented with two standardly used tagsets: the orig-
inal PTB 45-tag gold standard and a coarser set of
17 tags previously used by several researchers work-
ing on unsupervised POS tagging (Smith and Eis-
ner, 2005; Goldwater and Griffiths, 2007; Johnson,
2007). For the Multext-East corpus only a coarse 14-
tag tagset was available.5 Finally, to facilitate direct
comparisons of genre while controlling for the size
of both the corpus and the tag set, we also created a
further collapsed 13-tag set for WSJ.6
Figure 3 illustrates the abilities of the different
systems to generalize across different genres of En-
glish text. Comparing the results for the Multext-
East English corpus and the small WSJ corpus with
13 tags (i.e., controlling as much as possible for cor-
pus size and number of gold standard tags), we see
that despite being developed on WSJ, the systems
actually perform better on Multext-East. This is en-
couraging, since it suggests that the methods and
hyperparameters of the algorithms are not strongly
tied to WSJ. It also suggests that Multext-East is in
some sense an easier corpus than WSJ. Indeed, the
distribution of vocabulary items supports this view:
the 100 most frequent words account for 48% of
the WSJ corpus, but 57% of the 1984 novel. It is
also worth pointing out that, although previous re-
searchers have reduced the 45-tag WSJ set to 17 tags
in order to create an easier task for unsupervised
learning (and to decrease training time), reducing
the tag set further to 13 tags actually decreases per-
formance, since some distinctions found by the sys-
</bodyText>
<footnote confidence="0.9726505">
5Out of the 14 tags only 11 are shared across all languages.
For details c.f. Appendix B in (Naseem et al., 2009).
6We tried to make the meanings of the tags as similar as
possible between the two corpora; we had to create 13 rather
than 14 WSJ tags for this reason. Our 13-tag set can be found
athttp://homepages.inf.ed.ac.uk/s0787820/pos/.
</footnote>
<page confidence="0.997002">
581
</page>
<bodyText confidence="0.999970486486486">
tems (e.g., between different types of punctuation)
are collapsed in the gold standard.
Figure 4 gives the results of the different systems
on the various languages.7 Not surprisingly, all the
algorithms perform best on English, often by a wide
margin, suggesting that they are indeed tuned bet-
ter towards English syntax and/or morphology. One
might expect that the two systems with morpho-
logical features (clark and feat) would show less
difference between English and some of the other
languages (all of which have complex morphology)
than the other systems. However, although clark
and feat (along with Brown) are the best perform-
ing systems overall, they don’t show any particular
benefit for the morphologically complex languages.$
One difference between the Multext-East results
and the WSJ results is that on Multext-East, clark
clearly outperforms all the other systems. This is
true for both the English and non-English corpora,
despite the similar performance of clark and feat
on (English) WSJ. This suggests that feat benefits
more from the larger corpus size of WSJ. For the
other languages clark may be benefiting from some-
what more general morphological features; feat cur-
rently contains suffix features but no prefix features
(although these could be added).
Overall, our experiments on multiple languages
support our earlier claim that many of the newer
POS induction systems are not as successful as the
older methods. Moreover, these experiments under-
score the importance of testing unsupervised sys-
tems on multiple languages and domains, since both
the absolute and relative performance of systems
may change on different data sets. Ideally, some of
the corpora should be held out as unseen test data
if an effective argument is to be made regarding the
language- or domain-generality of the system.
</bodyText>
<sectionHeader confidence="0.874121" genericHeader="method">
5 Learning from induced prototypes
</sectionHeader>
<bodyText confidence="0.999855983050848">
We now introduce a final novel method of evaluat-
ing POS induction systems and potentially improv-
ing their performance as well. Our idea is based
Some results are missing because not all of the corpora
were successfully processed by all of the systems.
$It can be argued that lemmatization would have given a sig-
nificant gain to the performance of the systems in these lan-
guages. Although lemmatization information was included in
the corpus we chose not to use it, maintaining the fully unsu-
pervised nature of this task.
on the prototype-driven learning model of Haghighi
and Klein (2006). This model is unsupervised, but
requires as input a handful of prototypes (canonical
examples) for each word class. The system uses a
log-linear model with features that include the pro-
totype lists as well as morphological features (the
same ones used in feat). Using the most frequent
words in each gold standard class as prototypes, the
authors report 80.5% accuracy (both many-to-one
and one-to-one) on WSJ, considerably higher than
any of the induction systems seen here. This raises
two questions: If we wish to induce prototypes with-
out a tagged corpus or language-specific knowledge,
which induction system will provide the best pro-
totypes (i.e., most similar to the gold standard pro-
totypes)? And, can we use the induced prototypes
as input to the prototype-driven model (h&amp;k) to
achieve better performance than the system the pro-
totypes were extracted from?
To explore these questions, we implemented a
simple heuristic method for inducing prototypes
from the output C of a POS induction system by
selecting a few frequent words in each cluster that
are the most similar to other words in the cluster and
also the most dissimilar to the words in other clus-
ters. For each cluster ci E C, we retain as candi-
date prototypes the words whose frequency in ci is
at least 90% as high as the word with the highest fre-
quency (in ci). This yields about 20-30 candidates
from each cluster. For each of these, we compute
its average similarity S to the other candidates in its
cluster, and the average dissimilarity D to the candi-
dates in other clusters. Similarity is computed using
the method described by Haghighi and Klein (2006),
which uses SVD on word context vectors and cosine
similarity. Dissimilarity between a pair of words is
computed as one minus the similarity. Finally we
compute the average M = 0.5(S + D), sort the
words by their M scores, and keep as prototypes
the top ten words with M &gt; 0.25 * maxim(M).
The cutoff threshold results in some clusters having
less than ten prototypes, which is appropriate since
some gold standard categories have very few mem-
bers (e.g., punctuation, determiners).
Using this method, we first tested the various
base+proto systems on the WSJ corpus. Results
in Table 3 show that the brown system produces
the best prototypes. Although not as good as
using prototypes from the gold standard (h&amp;k),
</bodyText>
<page confidence="0.991071">
582
</page>
<table confidence="0.999853875">
system many-to-1 1-to-1 vm
brown 76.1(8.3) 60.7(10.6) 68.8(5.8)
clark 74.5(3.3) 62.1(8.3) 68.6(3.0)
bhmm 71.8(8.6) 56.5(15.0) 65.7(9.5)
vbhmm 68.1(17.9) 67.2(20.7) 67.5(18.3)
pr 71.6(9.2) 60.2(17.0) 67.2(12.4)
feat 69.8(-4.1) 52.0(-1.3) 63.1(-4.6)
h&amp;k 80.2 80.2 75.2
</table>
<tableCaption confidence="0.936468">
Table 3: Scores on WSJ for our prototype-based POS in-
duction system, with prototypes extracted from each of
the existing systems [ICI:45,ITI:45]. Numbers in paren-
theses are the improvement over the same system without
using the prototype step. Scores in bold indicate the best
performance (improvement) in each column. h&amp;k uses
gold standard prototypes.
</tableCaption>
<table confidence="0.998328636363636">
corpus brown clark
wsj 68.8(5.8) 68.5(3.0)
wsj-s 62.3(2.7) 67.5(3.6)
en 58.5(1.6) 57.9(-3.3)
bg 53.7(2.3) 50.2(-7.1)
cs 49.9(5.0) 48.0(-4.0)
et 45.8(4.9) 44.4(-1.9)
hu 45.8(0.1) 47.0(-5.7)
ro 53.2(0.8) 52.7(-3.3)
sl 51.2(2.9) 51.7(-4.6)
sr 48.0(2.8) 46.4(-4.9)
</table>
<tableCaption confidence="0.888427666666667">
Table 4: VM scores for brown+proto and clark+proto
on all corpora. Numbers in parentheses indicate improve-
ment over the base systems.
</tableCaption>
<bodyText confidence="0.999913047619048">
brown+proto yields a large improvement over
brown, and the highest performance of any system
tested so far. In fact, the brown+proto scores are, to
our knowledge, the best reported results for an un-
supervised POS induction system on WSJ.
Next, we evaluated the two best-performing
+proto systems on Multext-East, as shown in Ta-
ble 4. We see that brown again yields the best
prototypes, and again yields improvements when
used as brown+proto (although the improvements
are not as large as those on WSJ). Interestingly,
clark+proto actually performs worse than clark on
the multilingual data, showing that although induced
prototypes can in principle improve the performance
of a system, not all systems will benefit in all situ-
ations. This suggests a need for additional investi-
gation to determine what properties of an existing
induction system allow it to produce useful proto-
types with the current method and/or to develop a
specialized system specifically targeted towards in-
ducing useful prototypes.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99999096969697">
In this paper, we have attempted to provide a more
comprehensive review and comparison of evaluation
measures and systems for POS induction than has
been done before. We pointed out that most of the
commonly used evaluation measures are sensitive to
the number of induced clusters, and suggested that
V-measure (which is less sensitive) should be used
as an alternative or in conjunction with the standard
measures. With regard to the systems themselves,
we found that many of the newer approaches actu-
ally perform worse than older methods that are both
simpler and faster. The newer systems have intro-
duced potentially important machine learning tools,
but are not necessarily better suited to the POS in-
duction task specifically.
Since portability is a distinguishing feature for un-
supervised models, we have stressed the importance
of testing the systems on corpora that were not used
in their development, and especially on different lan-
guages. We found that on non-English languages,
Clark’s (2003) system performed best.
Finally, we introduced the idea of evaluating in-
duction systems based on their ability to produce
useful cluster prototypes. We found that the old-
est system (Brown et al., 1992) yielded the best
prototypes, and that using these prototypes gave
state-of-the-art performance on WSJ, as well as im-
provements on nearly all of the non-English corpora.
These promising results suggest a new direction for
future research: improving POS induction by de-
veloping methods targeted towards extracting better
prototypes, rather than focusing on improving clus-
tering of the entire data set.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99939625">
We thank Mark Johnson, Kuzman Ganchev, and
Taylor Berg-Kirkpatrick for providing the imple-
mentations of their models, as well as Stella
Frank, Tom Kwiatkowski, Luke Zettlemoyer and the
anonymous reviewers for their comments and sug-
gestions. This work was supported by an EPSRC
graduate Fellowship, and by ERC Advanced Fellow-
ship 249520 GRAMPLUS.
</bodyText>
<page confidence="0.998315">
583
</page>
<sectionHeader confidence="0.998349" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999657804597701">
Taylor Berg-Kirkpatrick, Alexandre B. Cˆot´e, John DeN-
ero, and Dan Klein. 2010. Painless unsupervised
learning with features. In Proceedings of NAACL
2010, pages 582–590, Los Angeles, California, June.
Chris Biemann. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In Proceed-
ings of COLING ACL 2006, pages 7–12, Morristown,
NJ, USA.
Peter F. Brown, Vincent J. Della Pietra, Peter V. Desouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467–479.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of EACL 2003, pages 59–66,
Morristown, NJ, USA.
Tomaˇz Erjavec. 2004. MULTEXT-East Version 3:
Multilingual Morphosyntactic Specifications, Lexi-
cons and Corpora. In Fourth International Conference
on Language Resources and Evaluation, LREC’04,
page In print, Paris. ELRA.
Stella Frank, Sharon Goldwater, and Frank Keller. 2009.
Evaluating models of syntactic category acquisition
without using a gold standard. In Proceedings of
CogSci09, July.
Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and
Ben Taskar. 2009. Posterior regularization for struc-
tured latent variable models. Technical report, Univer-
sity of Pennsylvania.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
bayesian estimators for unsupervised hidden markov
model pos taggers. In Proceedings of EMNLP 2008,
pages 344–352, Morristown, NJ, USA.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of ACL 2007, pages 744–751,
Prague, Czech Republic, June.
Joao Graca, Kuzman Ganchev, Ben Taskar, and Fernando
Pereira. 2009. Posterior vs parameter sparsity in latent
variable models. In Y. Bengio, D. Schuurmans, J. Laf-
ferty, C. K. I. Williams, and A. Culotta, editors, Ad-
vances in Neural Information Processing Systems 22,
pages 664–672.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
NAACL 2006, pages 320–327, Morristown, NJ, USA.
Mark Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers? In Proceedings of EMNLP-CoNLL
2007, pages 296–305, Prague, Czech Republic, June.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguistics,
19(2):331–330.
Marina Meilˇa. 2003. Comparing clusterings by the vari-
ation of information. In Learning Theory and Kernel
Machines, pages 173–187.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20(2):155–
172.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. Journal ofAr-
tificial Intelligence Research, 36:341–385.
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-IJCNLP 2009, pages 504–512, Sun-
tec, Singapore, August.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of EMNLP-
CoNLL 2007, pages 410–420.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: training log-linear models on unlabeled data.
In Proceedings of ACL 2005, pages 354–362, Morris-
town, NJ, USA.
K. Toutanova and M. Johnson. 2007. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In Proceedings of NIPS 2007.
Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahra-
mani. 2009. The infinite HMM for unsupervised PoS
tagging. In Proceedings of EMLNP 2009, pages 678–
687, Singapore, August.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. 2009. Unsupervised and constrained dirichlet
process mixture models for verb clustering. In Pro-
ceedings of GEMS 2009, pages 74–82, Morristown,
NJ, USA.
</reference>
<page confidence="0.998582">
584
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.279007">
<title confidence="0.989382">Two Decades of Unsupervised POS induction: How far have we come?</title>
<author confidence="0.89136">Christos</author>
<affiliation confidence="0.9964565">School of University of</affiliation>
<email confidence="0.959897">christos.c@ed.ac.uk</email>
<author confidence="0.946549">Sharon</author>
<affiliation confidence="0.9984005">School of University of</affiliation>
<email confidence="0.969065">sgwater@inf.ed.ac.uk</email>
<author confidence="0.995917">Mark</author>
<affiliation confidence="0.999087">School of University of</affiliation>
<email confidence="0.989013">steedman@inf.ed.ac.uk</email>
<abstract confidence="0.99983324">Part-of-speech (POS) induction is one of the most popular tasks in research on unsupervised NLP. Many different methods have been proposed, yet comparisons are difficult to make since there is little consensus on evaluation framework, and many papers evaluate against only one or two competitor systems. Here we evaluate seven different POS induction systems spanning nearly 20 years of work, using a variety of measures. We show that some of the oldest (and simplest) systems stand up surprisingly well against more recent approaches. Since most of these systems were developed and tested using data from the WSJ corpus, we compare their generalization abilities by testing on both WSJ and the multilingual Multext-East corpus. Finally, we introduce the idea of evaluating systems based on their ability to produce cluster prototypes that are useful as input to a prototype-driven learner. In most cases, the prototype-driven learner outperforms the unsupervised system used to initialize it, yielding state-of-the-art results on WSJ and improvements on non-</abstract>
<intro confidence="0.364785">English corpora.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre B Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL 2010,</booktitle>
<pages>582--590</pages>
<location>Los Angeles, California,</location>
<marker>Berg-Kirkpatrick, Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre B. Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Proceedings of NAACL 2010, pages 582–590, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Unsupervised part-of-speech tagging employing efficient graph clustering.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING ACL</booktitle>
<pages>7--12</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7299" citStr="Biemann, 2006" startWordPosition="1159" endWordPosition="1161">this improves the corpus probability. [clark]: Class-based n-grams with morphology (Clark, 2003). This system uses a similar model to the previous one, and also clusters word types (rather than tokens, as the rest of the systems do). The main differences between the systems are that clark uses a slightly different approximate search procedure, and that he augments the probabilistic model with a prior that prefers clusterings where morphologically similar words are clustered together. The morphology component is implemented as a single-order letter HMM. [cw]: Chinese Whispers graph clustering (Biemann, 2006). Unlike the other systems we consider, this one induces the value of |C |rather than taking it as an input parameter.2 The system uses a graph clustering algorithm called Chinese Whispers that is based on contextual similarity. The algorithm works in two stages. The first clusters the most frequent 10,000 words (target words) based on their context statistics, with contexts formed from the most frequent 150-250 words (feature words) that appear ei1Implementations were obtained from: brown: http://www.cs.berkeley.edu/∼pliang/ software/brown-cluster-1.2.zip (Percy Liang), clark: http://www.cs.r</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann. 2006. Unsupervised part-of-speech tagging employing efficient graph clustering. In Proceedings of COLING ACL 2006, pages 7–12, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V Desouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="4451" citStr="Brown et al., 1992" startWordPosition="700" endWordPosition="703"> in many cases, being more stable across different numbers of found and true clusters, and avoiding several of the problems with another commonly used entropy-based measure, Variation of Information (Meilˇa, 2003). Using V-Measure along with several other evaluation measures, we compare the performance of the different induction systems on both WSJ (the data on which most systems were developed and tested) and Multext East, a corpus of parallel texts in eight different languages. We find that for virtually all measures and datasets, older systems using relatively simple models and algorithms (Brown et al., 1992; Clark, 2003) work as well or better than systems using newer and often far more sophisticated and time-consuming machine learning methods (Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010). Thus, although these newer methods have introduced potentially useful machine learning techniques, they should not be assumed to provide the best performance for unsupervised POS induction. In addition to our review and comparison, we introduce a new way to both evaluate and potentially improve a POS induction system. Our method is based on the prototype-driv</context>
<context position="6134" citStr="Brown et al., 1992" startWordPosition="967" endWordPosition="970"> system yields stateof-the-art performance on WSJ and improvements on seven of the eight non-English corpora. 2 POS Induction Systems We describe each system only briefly; for details, see the respective papers, cited below. Each system outputs a set of syntactic clusters C; except where noted, the target number of clusters |C |must be specified as an input parameter. Since we are interested in out-of-the-box performance, we use the default parameter settings for each system, except for |C|, which is varied in some of our experiments. The systems are as follows:1 [brown]: Class-based n-grams (Brown et al., 1992). This is the oldest and one of the simplest systems we tested. It uses a bigram model where each word type is assigned to a latent class (a hard assignment), and the probability of the corpus w1 ... wn is computed as P(w1|c1) ∏n i=2 P(wi|ci)P(ci|ci−1), where ci is the class of wi. The goal is to optimize the probability of the corpus under this model. The authors use an approximate search procedure: greedy agglomerative hierarchical clustering followed by a step in which individual word types are considered for movement to a different class if this improves the corpus probability. [clark]: Cl</context>
<context position="35350" citStr="Brown et al., 1992" startWordPosition="5765" endWordPosition="5768">ystems have introduced potentially important machine learning tools, but are not necessarily better suited to the POS induction task specifically. Since portability is a distinguishing feature for unsupervised models, we have stressed the importance of testing the systems on corpora that were not used in their development, and especially on different languages. We found that on non-English languages, Clark’s (2003) system performed best. Finally, we introduced the idea of evaluating induction systems based on their ability to produce useful cluster prototypes. We found that the oldest system (Brown et al., 1992) yielded the best prototypes, and that using these prototypes gave state-of-the-art performance on WSJ, as well as improvements on nearly all of the non-English corpora. These promising results suggest a new direction for future research: improving POS induction by developing methods targeted towards extracting better prototypes, rather than focusing on improving clustering of the entire data set. Acknowledgments We thank Mark Johnson, Kuzman Ganchev, and Taylor Berg-Kirkpatrick for providing the implementations of their models, as well as Stella Frank, Tom Kwiatkowski, Luke Zettlemoyer and th</context>
</contexts>
<marker>Brown, Pietra, Desouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. Desouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL 2003,</booktitle>
<pages>59--66</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4465" citStr="Clark, 2003" startWordPosition="704" endWordPosition="705">g more stable across different numbers of found and true clusters, and avoiding several of the problems with another commonly used entropy-based measure, Variation of Information (Meilˇa, 2003). Using V-Measure along with several other evaluation measures, we compare the performance of the different induction systems on both WSJ (the data on which most systems were developed and tested) and Multext East, a corpus of parallel texts in eight different languages. We find that for virtually all measures and datasets, older systems using relatively simple models and algorithms (Brown et al., 1992; Clark, 2003) work as well or better than systems using newer and often far more sophisticated and time-consuming machine learning methods (Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010). Thus, although these newer methods have introduced potentially useful machine learning techniques, they should not be assumed to provide the best performance for unsupervised POS induction. In addition to our review and comparison, we introduce a new way to both evaluate and potentially improve a POS induction system. Our method is based on the prototype-driven learning sy</context>
<context position="6781" citStr="Clark, 2003" startWordPosition="1080" endWordPosition="1081"> simplest systems we tested. It uses a bigram model where each word type is assigned to a latent class (a hard assignment), and the probability of the corpus w1 ... wn is computed as P(w1|c1) ∏n i=2 P(wi|ci)P(ci|ci−1), where ci is the class of wi. The goal is to optimize the probability of the corpus under this model. The authors use an approximate search procedure: greedy agglomerative hierarchical clustering followed by a step in which individual word types are considered for movement to a different class if this improves the corpus probability. [clark]: Class-based n-grams with morphology (Clark, 2003). This system uses a similar model to the previous one, and also clusters word types (rather than tokens, as the rest of the systems do). The main differences between the systems are that clark uses a slightly different approximate search procedure, and that he augments the probabilistic model with a prior that prefers clusterings where morphologically similar words are clustered together. The morphology component is implemented as a single-order letter HMM. [cw]: Chinese Whispers graph clustering (Biemann, 2006). Unlike the other systems we consider, this one induces the value of |C |rather t</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proceedings of EACL 2003, pages 59–66, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomaˇz Erjavec</author>
</authors>
<title>MULTEXT-East Version 3: Multilingual Morphosyntactic Specifications, Lexicons and Corpora.</title>
<date>2004</date>
<booktitle>In Fourth International Conference on Language Resources and Evaluation, LREC’04,</booktitle>
<pages>page</pages>
<location>Paris. ELRA.</location>
<contexts>
<context position="22274" citStr="Erjavec, 2004" startWordPosition="3660" endWordPosition="3661">Section 2. We first present results for the same WSJ corpus used above. However, because most of the systems were initially developed on this corpus, and often evaluated only on it, there is a question of whether their methods and/or hyperparameters are overly specific to the domain or to the English language. This is a particularly pertinent question since a primary argument in favor of unsupervised systems is that they are easier to port to a new language or domain than supervised systems. To address this question, we evaluate all the systems as well on the multilingual Multext East corpus (Erjavec, 2004), without changing any of the parameter settings. |C| was set to 45 for all of the experiments reported in this section. Based on our assessment of evaluation Figure 2: Performance of the different systems on WSJ, using three different measures [|C|:45, |T|:45] pr e10 hrs.* feat e40 hrs.* Table 2: Runtimes for the different systems on WSJ [|C|:45]. *pr and feat have multithreading implementations and ran on 16 cores. measures above, we report VM scores as the most reliable measure across different systems and cluster set sizes; to facilitate comparisons with previous papers, we also report man</context>
</contexts>
<marker>Erjavec, 2004</marker>
<rawString>Tomaˇz Erjavec. 2004. MULTEXT-East Version 3: Multilingual Morphosyntactic Specifications, Lexicons and Corpora. In Fourth International Conference on Language Resources and Evaluation, LREC’04, page In print, Paris. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stella Frank</author>
<author>Sharon Goldwater</author>
<author>Frank Keller</author>
</authors>
<title>Evaluating models of syntactic category acquisition without using a gold standard.</title>
<date>2009</date>
<booktitle>In Proceedings of CogSci09,</booktitle>
<contexts>
<context position="14720" citStr="Frank et al., 2009" startWordPosition="2359" endWordPosition="2362">hted harmonic mean of two values, homogeneity (h, the precision analogue) and completeness (c, the recall analogue): h = H(T |C) c = 1 V M = − H(T ) H(C|T ) 1 − H(C) (1 + β)hc (βh) + c As with F-measure, β is normally set to 1. [vmb]: V-beta is an extension to V-Measure, proposed by (Vlachos et al., 2009). They noted that V-Measure favors clusterings where the number of clusters |C |is larger than the number of POS tags |T|. To address this issue the parameter β in equation 3 is set to |C|/|T |in order adjust the balance between homogeneity and completeness. [s-fscore]: Substitutable F-score (Frank et al., 2009). One potential issue with all of the above measures is that they require a gold standard tagging to compute. This is normally available during development of a system, but if the system is deployed on a novel language a gold standard may not be available. In addition, there is the question of whether the gold standard itself is “correct”. Recently, Frank et al. (2009) proposed this novel evaluation measure that requires no gold standard, instead using the concept of substitutability to evaluate performance. Instead of comparing the system’s clusters C to gold standard clusters T, they are com</context>
</contexts>
<marker>Frank, Goldwater, Keller, 2009</marker>
<rawString>Stella Frank, Sharon Goldwater, and Frank Keller. 2009. Evaluating models of syntactic category acquisition without using a gold standard. In Proceedings of CogSci09, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao Grac¸a</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2009</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2009</marker>
<rawString>Kuzman Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and Ben Taskar. 2009. Posterior regularization for structured latent variable models. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mark Johnson</author>
</authors>
<title>A comparison of bayesian estimators for unsupervised hidden markov model pos taggers.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>344--352</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="12157" citStr="Gao and Johnson, 2008" startWordPosition="1910" endWordPosition="1913">one mapping accuracy (also known as cluster purity) maps each cluster to the gold standard tag that is most common for the words in that cluster (henceforth, the preferred tag), and then computes the proportion of words tagged correctly. More than one cluster may be mapped to the same gold standard tag. This is the most commonly used metric across the literature as it is intuitive and creates a meaningful POS sequence out of the cluster identifiers. However, it tends to yield higher scores as ICI increases, making comparisons difficult when |C |can vary. [crossval]: Cross-validation accuracy (Gao and Johnson, 2008) is intended to address the problem with many-to-one accuracy which is that assigning each word to its own class yields a perfect score. In this measure, the first half of the corpus is used to obtain the many-to-one mapping of clusters to tags, and this mapping is used to compute the accuracy of the clustering on the second half of the corpus. [1-to-1]: One-to-one mapping accuracy (Haghighi and Klein, 2006) constrains the mapping from clusters to tags, so that at most one cluster can be mapped to any tag. The mapping is performed greedily. In general, as the number of clusters increases, fewe</context>
</contexts>
<marker>Gao, Johnson, 2008</marker>
<rawString>Jianfeng Gao and Mark Johnson. 2008. A comparison of bayesian estimators for unsupervised hidden markov model pos taggers. In Proceedings of EMNLP 2008, pages 344–352, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL 2007,</booktitle>
<pages>744--751</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4621" citStr="Goldwater and Griffiths, 2007" startWordPosition="725" endWordPosition="729">based measure, Variation of Information (Meilˇa, 2003). Using V-Measure along with several other evaluation measures, we compare the performance of the different induction systems on both WSJ (the data on which most systems were developed and tested) and Multext East, a corpus of parallel texts in eight different languages. We find that for virtually all measures and datasets, older systems using relatively simple models and algorithms (Brown et al., 1992; Clark, 2003) work as well or better than systems using newer and often far more sophisticated and time-consuming machine learning methods (Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010). Thus, although these newer methods have introduced potentially useful machine learning techniques, they should not be assumed to provide the best performance for unsupervised POS induction. In addition to our review and comparison, we introduce a new way to both evaluate and potentially improve a POS induction system. Our method is based on the prototype-driven learning system of Haghighi and Klein (2006), which achieves very good performance by using a hand-selected list of prototypes for each syntactic cluster. We instead u</context>
<context position="9032" citStr="Goldwater and Griffiths, 2007" startWordPosition="1425" endWordPosition="1428"> not as good as the best system scores in Section 4. 576 ther to the left or right of a target word. The second stage deals with medium and low frequency words and uses pairwise similarity scores calculated by the number of shared neighbors between two words in a 4-word context window. The final clustering is a combination of the clusters obtained in the two stages. While the number of target words, feature words, and window size are in principle parameters of the algorithm, they are hard-coded in the implementation we used and we did not change them. [bhmm]: Bayesian HMM with Gibbs sampling (Goldwater and Griffiths, 2007). This system is based on a standard HMM for POS tagging. It differs from the standard model by placing Dirichlet priors over the multinomial parameters defining the state-state and state-emission distributions, and uses a collapsed Gibbs sampler to infer the hidden tags. The Dirichlet hyperparameters a (which controls the sparsity of the transition probabilities) and Q (which controls the sparsity of the emission probabilities) can be fixed or inferred. We used a bigram version of this model with hyperparameter inference. [vbhmm]: Bayesian HMM with variational Bayes (Johnson, 2007). This syst</context>
<context position="25638" citStr="Goldwater and Griffiths, 2007" startWordPosition="4197" endWordPosition="4200">t-East corpus (�7k sentences), which contains parallel translations of Orwell’s 1984 in 8 different languages: Bulgarian[bg], Czech[cs], Estonian[et], Hungarian[hu], Romanian[ro], Slovene[sl], Serbian[sr] and English[en]. We also included a 7k sentence version of the WSJ corpus [wsj-s] to help differentiate effects of corpus size from those of domain/language. For the WSJ corpora we experimented with two standardly used tagsets: the original PTB 45-tag gold standard and a coarser set of 17 tags previously used by several researchers working on unsupervised POS tagging (Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Johnson, 2007). For the Multext-East corpus only a coarse 14- tag tagset was available.5 Finally, to facilitate direct comparisons of genre while controlling for the size of both the corpus and the tag set, we also created a further collapsed 13-tag set for WSJ.6 Figure 3 illustrates the abilities of the different systems to generalize across different genres of English text. Comparing the results for the MultextEast English corpus and the small WSJ corpus with 13 tags (i.e., controlling as much as possible for corpus size and number of gold standard tags), we see that despite being develope</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully bayesian approach to unsupervised part-of-speech tagging. In Proceedings of ACL 2007, pages 744–751, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao Graca</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
<author>Fernando Pereira</author>
</authors>
<title>Posterior vs parameter sparsity in latent variable models.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems 22,</booktitle>
<pages>664--672</pages>
<editor>In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors,</editor>
<contexts>
<context position="4656" citStr="Graca et al., 2009" startWordPosition="732" endWordPosition="735">a, 2003). Using V-Measure along with several other evaluation measures, we compare the performance of the different induction systems on both WSJ (the data on which most systems were developed and tested) and Multext East, a corpus of parallel texts in eight different languages. We find that for virtually all measures and datasets, older systems using relatively simple models and algorithms (Brown et al., 1992; Clark, 2003) work as well or better than systems using newer and often far more sophisticated and time-consuming machine learning methods (Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010). Thus, although these newer methods have introduced potentially useful machine learning techniques, they should not be assumed to provide the best performance for unsupervised POS induction. In addition to our review and comparison, we introduce a new way to both evaluate and potentially improve a POS induction system. Our method is based on the prototype-driven learning system of Haghighi and Klein (2006), which achieves very good performance by using a hand-selected list of prototypes for each syntactic cluster. We instead use the existing POS induction syste</context>
<context position="9869" citStr="Graca et al. (2009)" startWordPosition="1560" endWordPosition="1563">es a collapsed Gibbs sampler to infer the hidden tags. The Dirichlet hyperparameters a (which controls the sparsity of the transition probabilities) and Q (which controls the sparsity of the emission probabilities) can be fixed or inferred. We used a bigram version of this model with hyperparameter inference. [vbhmm]: Bayesian HMM with variational Bayes (Johnson, 2007). This system uses the same bigram model as bhmm, but uses variational Bayesian EM for inference. We fixed the a and Q parameters to 0.1, values that appeared to be reasonable based on Johnson (2007), and which were also used by Graca et al. (2009). [pr]: Sparsity posterior-regularization HMM (Graca et al., 2009). The Bayesian approaches described above encourage sparse state-state and stateemission distributions only indirectly through the Dirichlet priors. This system, while utilizing the same bigram HMM, encourages sparsity directly by constraining the posterior distributions using the posterior regularization framework (Ganchev et al., 2009). A parameter a controls the strengths of the constraints (default = 25). Following Graca et al. (2009), we set a = Q = 0.1. [feat]: Feature-based HMM (Berg-Kirkpatrick et al., 2010). This system</context>
</contexts>
<marker>Graca, Ganchev, Taskar, Pereira, 2009</marker>
<rawString>Joao Graca, Kuzman Ganchev, Ben Taskar, and Fernando Pereira. 2009. Posterior vs parameter sparsity in latent variable models. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 664–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL 2006,</booktitle>
<pages>320--327</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2641" citStr="Haghighi and Klein, 2006" startWordPosition="406" endWordPosition="409">cludes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know how well we can expect them to work. One difficulty in evaluating POS induction systems is that there is no straightforward way to map the clusters found by the algorithm onto the gold standard tags; moreover, some syst</context>
<context position="5098" citStr="Haghighi and Klein (2006)" startWordPosition="802" endWordPosition="805"> well or better than systems using newer and often far more sophisticated and time-consuming machine learning methods (Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010). Thus, although these newer methods have introduced potentially useful machine learning techniques, they should not be assumed to provide the best performance for unsupervised POS induction. In addition to our review and comparison, we introduce a new way to both evaluate and potentially improve a POS induction system. Our method is based on the prototype-driven learning system of Haghighi and Klein (2006), which achieves very good performance by using a hand-selected list of prototypes for each syntactic cluster. We instead use the existing POS induction systems to induce prototypes automatically, and evaluate the systems based on the quality of their prototypes. We find that the oldest system tested (Brown et al., 1992) produces the best prototypes, and that using these prototypes as input to Haghighi and Klein’s system yields stateof-the-art performance on WSJ and improvements on seven of the eight non-English corpora. 2 POS Induction Systems We describe each system only briefly; for details</context>
<context position="12568" citStr="Haghighi and Klein, 2006" startWordPosition="1980" endWordPosition="1983"> POS sequence out of the cluster identifiers. However, it tends to yield higher scores as ICI increases, making comparisons difficult when |C |can vary. [crossval]: Cross-validation accuracy (Gao and Johnson, 2008) is intended to address the problem with many-to-one accuracy which is that assigning each word to its own class yields a perfect score. In this measure, the first half of the corpus is used to obtain the many-to-one mapping of clusters to tags, and this mapping is used to compute the accuracy of the clustering on the second half of the corpus. [1-to-1]: One-to-one mapping accuracy (Haghighi and Klein, 2006) constrains the mapping from clusters to tags, so that at most one cluster can be mapped to any tag. The mapping is performed greedily. In general, as the number of clusters increases, fewer clusters will be mapped to their preferred tag and scores will decrease (especially if the number of clusters is larger than the number of tags, so that some clusters are unassigned and receive zero credit). Again, this makes it difficult to 577 compare solutions with different values of |C|. [vi]: Variation of Information (Meilˇa, 2003) is an information-theoretic measure that regards the system output C </context>
<context position="29762" citStr="Haghighi and Klein (2006)" startWordPosition="4874" endWordPosition="4877">. 5 Learning from induced prototypes We now introduce a final novel method of evaluating POS induction systems and potentially improving their performance as well. Our idea is based Some results are missing because not all of the corpora were successfully processed by all of the systems. $It can be argued that lemmatization would have given a significant gain to the performance of the systems in these languages. Although lemmatization information was included in the corpus we chose not to use it, maintaining the fully unsupervised nature of this task. on the prototype-driven learning model of Haghighi and Klein (2006). This model is unsupervised, but requires as input a handful of prototypes (canonical examples) for each word class. The system uses a log-linear model with features that include the prototype lists as well as morphological features (the same ones used in feat). Using the most frequent words in each gold standard class as prototypes, the authors report 80.5% accuracy (both many-to-one and one-to-one) on WSJ, considerably higher than any of the induction systems seen here. This raises two questions: If we wish to induce prototypes without a tagged corpus or language-specific knowledge, which i</context>
<context position="31394" citStr="Haghighi and Klein (2006)" startWordPosition="5150" endWordPosition="5153">y selecting a few frequent words in each cluster that are the most similar to other words in the cluster and also the most dissimilar to the words in other clusters. For each cluster ci E C, we retain as candidate prototypes the words whose frequency in ci is at least 90% as high as the word with the highest frequency (in ci). This yields about 20-30 candidates from each cluster. For each of these, we compute its average similarity S to the other candidates in its cluster, and the average dissimilarity D to the candidates in other clusters. Similarity is computed using the method described by Haghighi and Klein (2006), which uses SVD on word context vectors and cosine similarity. Dissimilarity between a pair of words is computed as one minus the similarity. Finally we compute the average M = 0.5(S + D), sort the words by their M scores, and keep as prototypes the top ten words with M &gt; 0.25 * maxim(M). The cutoff threshold results in some clusters having less than ten prototypes, which is appropriate since some gold standard categories have very few members (e.g., punctuation, determiners). Using this method, we first tested the various base+proto systems on the WSJ corpus. Results in Table 3 show that the</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of NAACL 2006, pages 320–327, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL 2007,</booktitle>
<pages>296--305</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2424" citStr="Johnson, 2007" startWordPosition="368" endWordPosition="370"> system. However, it is difficult to draw overall conclusions about the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know ho</context>
<context position="4636" citStr="Johnson, 2007" startWordPosition="730" endWordPosition="731">ormation (Meilˇa, 2003). Using V-Measure along with several other evaluation measures, we compare the performance of the different induction systems on both WSJ (the data on which most systems were developed and tested) and Multext East, a corpus of parallel texts in eight different languages. We find that for virtually all measures and datasets, older systems using relatively simple models and algorithms (Brown et al., 1992; Clark, 2003) work as well or better than systems using newer and often far more sophisticated and time-consuming machine learning methods (Goldwater and Griffiths, 2007; Johnson, 2007; Graca et al., 2009; Berg-Kirkpatrick et al., 2010). Thus, although these newer methods have introduced potentially useful machine learning techniques, they should not be assumed to provide the best performance for unsupervised POS induction. In addition to our review and comparison, we introduce a new way to both evaluate and potentially improve a POS induction system. Our method is based on the prototype-driven learning system of Haghighi and Klein (2006), which achieves very good performance by using a hand-selected list of prototypes for each syntactic cluster. We instead use the existing</context>
<context position="9621" citStr="Johnson, 2007" startWordPosition="1516" endWordPosition="1517">ater and Griffiths, 2007). This system is based on a standard HMM for POS tagging. It differs from the standard model by placing Dirichlet priors over the multinomial parameters defining the state-state and state-emission distributions, and uses a collapsed Gibbs sampler to infer the hidden tags. The Dirichlet hyperparameters a (which controls the sparsity of the transition probabilities) and Q (which controls the sparsity of the emission probabilities) can be fixed or inferred. We used a bigram version of this model with hyperparameter inference. [vbhmm]: Bayesian HMM with variational Bayes (Johnson, 2007). This system uses the same bigram model as bhmm, but uses variational Bayesian EM for inference. We fixed the a and Q parameters to 0.1, values that appeared to be reasonable based on Johnson (2007), and which were also used by Graca et al. (2009). [pr]: Sparsity posterior-regularization HMM (Graca et al., 2009). The Bayesian approaches described above encourage sparse state-state and stateemission distributions only indirectly through the Dirichlet priors. This system, while utilizing the same bigram HMM, encourages sparsity directly by constraining the posterior distributions using the post</context>
<context position="25654" citStr="Johnson, 2007" startWordPosition="4201" endWordPosition="4202">which contains parallel translations of Orwell’s 1984 in 8 different languages: Bulgarian[bg], Czech[cs], Estonian[et], Hungarian[hu], Romanian[ro], Slovene[sl], Serbian[sr] and English[en]. We also included a 7k sentence version of the WSJ corpus [wsj-s] to help differentiate effects of corpus size from those of domain/language. For the WSJ corpora we experimented with two standardly used tagsets: the original PTB 45-tag gold standard and a coarser set of 17 tags previously used by several researchers working on unsupervised POS tagging (Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Johnson, 2007). For the Multext-East corpus only a coarse 14- tag tagset was available.5 Finally, to facilitate direct comparisons of genre while controlling for the size of both the corpus and the tag set, we also created a further collapsed 13-tag set for WSJ.6 Figure 3 illustrates the abilities of the different systems to generalize across different genres of English text. Comparing the results for the MultextEast English corpus and the small WSJ corpus with 13 tags (i.e., controlling as much as possible for corpus size and number of gold standard tags), we see that despite being developed on WSJ, the sy</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Proceedings of EMNLP-CoNLL 2007, pages 296–305, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="16651" citStr="Marcus et al., 1993" startWordPosition="2705" endWordPosition="2708">ystem’s clustering: SP = ∑sES ∑IEC |s n c|(|s n c |− 1) SR = SF = ∑IEC |c|(|c |− 1) ∑sES ∑IEC |s n c|(|s n c |− 1) ∑sES |s|(|s |− 1) 2 · SP · SR SP + SR 3.1 Empirical results We mentioned a few strengths and weaknesses of each evaluation method above; in this section we present some empirical results to expand on these claims. First, we examine the effects of varying |C| on the behavior of the evaluation measures, while keeping the number of gold standard tags the same (|T |= 45). Results were obtained by training and evaluating each system on the full WSJ portion of the Penn Treebank corpus (Marcus et al., 1993). Figure 1 shows the results from the Brown system for |C |ranging from 1 to 200; the same trends were observed for all other systems.3 In addition, Table 1 provides results for the two extremes of |C |= 1 (all words assigned to the same cluster) and |C |equal to the size of the corpus (a single word per cluster), as 3The results reported in this paper are only a fraction of the total from our experiments; given the number of parameters, models and measures tested, we obtained over 15000 results. The full set of results can be found at http://homepages.inf.ed.ac.uk/s0787820/pos/. 578 Figure 1:</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):331–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meilˇa</author>
</authors>
<title>Comparing clusterings by the variation of information.</title>
<date>2003</date>
<booktitle>In Learning Theory and Kernel Machines,</booktitle>
<pages>173--187</pages>
<marker>Meilˇa, 2003</marker>
<rawString>Marina Meilˇa. 2003. Comparing clusterings by the variation of information. In Learning Theory and Kernel Machines, pages 173–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<pages>172</pages>
<contexts>
<context position="2395" citStr="Merialdo, 1994" startWordPosition="364" endWordPosition="365">than one or another comparison system. However, it is difficult to draw overall conclusions about the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight </context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155– 172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Benjamin Snyder</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Multilingual part-of-speech tagging: Two unsupervised approaches.</title>
<date>2009</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>36--341</pages>
<contexts>
<context position="2445" citStr="Naseem et al., 2009" startWordPosition="371" endWordPosition="374">r, it is difficult to draw overall conclusions about the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know how well we can expect </context>
<context position="27086" citStr="Naseem et al., 2009" startWordPosition="4445" endWordPosition="4448">some sense an easier corpus than WSJ. Indeed, the distribution of vocabulary items supports this view: the 100 most frequent words account for 48% of the WSJ corpus, but 57% of the 1984 novel. It is also worth pointing out that, although previous researchers have reduced the 45-tag WSJ set to 17 tags in order to create an easier task for unsupervised learning (and to decrease training time), reducing the tag set further to 13 tags actually decreases performance, since some distinctions found by the sys5Out of the 14 tags only 11 are shared across all languages. For details c.f. Appendix B in (Naseem et al., 2009). 6We tried to make the meanings of the tags as similar as possible between the two corpora; we had to create 13 rather than 14 WSJ tags for this reason. Our 13-tag set can be found athttp://homepages.inf.ed.ac.uk/s0787820/pos/. 581 tems (e.g., between different types of punctuation) are collapsed in the gold standard. Figure 4 gives the results of the different systems on the various languages.7 Not surprisingly, all the algorithms perform best on English, often by a wide margin, suggesting that they are indeed tuned better towards English syntax and/or morphology. One might expect that the t</context>
</contexts>
<marker>Naseem, Snyder, Eisenstein, Barzilay, 2009</marker>
<rawString>Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and Regina Barzilay. 2009. Multilingual part-of-speech tagging: Two unsupervised approaches. Journal ofArtificial Intelligence Research, 36:341–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP 2009,</booktitle>
<pages>504--512</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2468" citStr="Ravi and Knight, 2009" startWordPosition="375" endWordPosition="378"> draw overall conclusions about the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know how well we can expect them to work. One diffi</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In Proceedings of ACL-IJCNLP 2009, pages 504–512, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL</booktitle>
<pages>410--420</pages>
<contexts>
<context position="13987" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="2221" endWordPosition="2224">tional entropy of each clustering conditioned on the other. More formally, V I(C, T) = H(T|C) + H(C|T) = H(C) + H(T) − 2I(C, T), where H(.) is the entropy function and I(.) is the mutual information. VI and other entropy-based measures have been argued to be superior to accuracy-based measures such as those above, because they consider not only the majority tag in each cluster, but also whether the remainder of the cluster is more or less homogeneous. Unlike the other measures we consider, lower scores are better (since VI measures the difference between clusterings in bits). [vm]: V-Measure (Rosenberg and Hirschberg, 2007) is another entropy-based measure that is designed to be analogous to F-measure, in that it is defined as the weighted harmonic mean of two values, homogeneity (h, the precision analogue) and completeness (c, the recall analogue): h = H(T |C) c = 1 V M = − H(T ) H(C|T ) 1 − H(C) (1 + β)hc (βh) + c As with F-measure, β is normally set to 1. [vmb]: V-beta is an extension to V-Measure, proposed by (Vlachos et al., 2009). They noted that V-Measure favors clusterings where the number of clusters |C |is larger than the number of POS tags |T|. To address this issue the parameter β in equation 3 is se</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of EMNLPCoNLL 2007, pages 410–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL 2005,</booktitle>
<pages>354--362</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2493" citStr="Smith and Eisner, 2005" startWordPosition="379" endWordPosition="382">ns about the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know how well we can expect them to work. One difficulty in evaluating POS i</context>
<context position="25607" citStr="Smith and Eisner, 2005" startWordPosition="4192" endWordPosition="4196">84 portion of the Multext-East corpus (�7k sentences), which contains parallel translations of Orwell’s 1984 in 8 different languages: Bulgarian[bg], Czech[cs], Estonian[et], Hungarian[hu], Romanian[ro], Slovene[sl], Serbian[sr] and English[en]. We also included a 7k sentence version of the WSJ corpus [wsj-s] to help differentiate effects of corpus size from those of domain/language. For the WSJ corpora we experimented with two standardly used tagsets: the original PTB 45-tag gold standard and a coarser set of 17 tags previously used by several researchers working on unsupervised POS tagging (Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Johnson, 2007). For the Multext-East corpus only a coarse 14- tag tagset was available.5 Finally, to facilitate direct comparisons of genre while controlling for the size of both the corpus and the tag set, we also created a further collapsed 13-tag set for WSJ.6 Figure 3 illustrates the abilities of the different systems to generalize across different genres of English text. Comparing the results for the MultextEast English corpus and the small WSJ corpus with 13 tags (i.e., controlling as much as possible for corpus size and number of gold standard tags), we </context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: training log-linear models on unlabeled data. In Proceedings of ACL 2005, pages 354–362, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>M Johnson</author>
</authors>
<title>A Bayesian LDAbased model for semi-supervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of NIPS</booktitle>
<contexts>
<context position="2424" citStr="Toutanova and Johnson, 2007" startWordPosition="366" endWordPosition="370">her comparison system. However, it is difficult to draw overall conclusions about the relative performance of unsupervised POS tagging systems because of differences in evaluation measures, and the fact that no paper includes direct comparisons against more than a few other systems. In this paper, we attempt to remedy that situation by providing a comprehensive evaluation of seven different POS induction systems spanning nearly 20 years of research. We focus specifically on POS induction systems, where no prior knowledge is available, in contrast to POS disambiguation systems (Merialdo, 1994; Toutanova and Johnson, 2007; Naseem et al., 2009; Ravi and Knight, 2009; Smith and Eisner, 2005), which use a dictionary to provide possible tags for some or all of the words in the corpus, or prototype-driven systems (Haghighi and Klein, 2006), which use a small set of prototypes for each tag class, but no dictionary. Our motivation stems from another part of our own research, in which we are trying to use NLP systems on over 50 low-density languages (some of them dead) where both tagged corpora and language speakers are mostly unavailable. We therefore desire to use these systems straight out of the box and to know ho</context>
</contexts>
<marker>Toutanova, Johnson, 2007</marker>
<rawString>K. Toutanova and M. Johnson. 2007. A Bayesian LDAbased model for semi-supervised part-of-speech tagging. In Proceedings of NIPS 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Andreas Vlachos</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>The infinite HMM for unsupervised PoS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of EMLNP 2009,</booktitle>
<pages>678--687</pages>
<marker>Van Gael, Vlachos, Ghahramani, 2009</marker>
<rawString>Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahramani. 2009. The infinite HMM for unsupervised PoS tagging. In Proceedings of EMLNP 2009, pages 678– 687, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
<author>Anna Korhonen</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Unsupervised and constrained dirichlet process mixture models for verb clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of GEMS 2009,</booktitle>
<pages>74--82</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="14407" citStr="Vlachos et al., 2009" startWordPosition="2307" endWordPosition="2310">more or less homogeneous. Unlike the other measures we consider, lower scores are better (since VI measures the difference between clusterings in bits). [vm]: V-Measure (Rosenberg and Hirschberg, 2007) is another entropy-based measure that is designed to be analogous to F-measure, in that it is defined as the weighted harmonic mean of two values, homogeneity (h, the precision analogue) and completeness (c, the recall analogue): h = H(T |C) c = 1 V M = − H(T ) H(C|T ) 1 − H(C) (1 + β)hc (βh) + c As with F-measure, β is normally set to 1. [vmb]: V-beta is an extension to V-Measure, proposed by (Vlachos et al., 2009). They noted that V-Measure favors clusterings where the number of clusters |C |is larger than the number of POS tags |T|. To address this issue the parameter β in equation 3 is set to |C|/|T |in order adjust the balance between homogeneity and completeness. [s-fscore]: Substitutable F-score (Frank et al., 2009). One potential issue with all of the above measures is that they require a gold standard tagging to compute. This is normally available during development of a system, but if the system is deployed on a novel language a gold standard may not be available. In addition, there is the ques</context>
</contexts>
<marker>Vlachos, Korhonen, Ghahramani, 2009</marker>
<rawString>Andreas Vlachos, Anna Korhonen, and Zoubin Ghahramani. 2009. Unsupervised and constrained dirichlet process mixture models for verb clustering. In Proceedings of GEMS 2009, pages 74–82, Morristown, NJ, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>