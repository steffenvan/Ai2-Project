<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.043593">
<title confidence="0.996156">
Answer Extraction, Semantic Clustering, and Extractive Summarization
for Clinical Question Answering
</title>
<author confidence="0.998882">
Dina Demner-Fushman&apos;,3 and Jimmy Lin&apos;,2,3
</author>
<affiliation confidence="0.99867525">
&apos;Department of Computer Science
2College of Information Studies
3Institute for Advanced Computer Studies
University of Maryland
</affiliation>
<address confidence="0.880631">
College Park, MD 20742, USA
</address>
<email confidence="0.999095">
demner@cs.umd.edu, jimmylin@umd.edu
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99982055">
This paper presents a hybrid approach
to question answering in the clinical
domain that combines techniques from
summarization and information retrieval.
We tackle a frequently-occurring class of
questions that takes the form “What is
the best drug treatment for X?” Starting
from an initial set of MEDLINE citations,
our system first identifies the drugs un-
der study. Abstracts are then clustered us-
ing semantic classes from the UMLS on-
tology. Finally, a short extractive sum-
mary is generated for each abstract to pop-
ulate the clusters. Two evaluations—a
manual one focused on short answers and
an automatic one focused on the support-
ing abstracts—demonstrate that our sys-
tem compares favorably to PubMed, the
search system most widely used by physi-
cians today.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951292682927">
Complex information needs can rarely be ad-
dressed by single documents, but rather require the
integration of knowledge from multiple sources.
This suggests that modern information retrieval
systems, which excel at producing ranked lists of
documents sorted by relevance, may not be suffi-
cient to provide users with a good overview of the
“information landscape”.
Current question answering systems aspire to
address this shortcoming by gathering relevant
“facts” from multiple documents in response to
information needs. The so-called “definition”
or “other” questions at recent TREC evalua-
tions (Voorhees, 2005) serve as good examples:
“good answers” to these questions include inter-
esting “nuggets” about a particular person, organi-
zation, entity, or event.
The importance of cross-document information
synthesis has not escaped the attention of other re-
searchers. The last few years have seen a conver-
gence between the question answering and sum-
marization communities (Amig´o et al., 2004), as
highlighted by the shift from generic to query-
focused summaries in the 2005 DUC evalua-
tion (Dang, 2005). Despite a focus on document
ranking, different techniques for organizing search
results have been explored by information retrieval
researchers, as exemplified by techniques based on
clustering (Hearst and Pedersen, 1996; Dumais et
al., 2001; Lawrie and Croft, 2003).
Our work, which is situated in the domain of
clinical medicine, lies at the intersection of ques-
tion answering, information retrieval, and summa-
rization. We employ answer extraction to identify
short answers, semantic clustering to group sim-
ilar results, and extractive summarization to pro-
duce supporting evidence. This paper describes
how each of these capabilities contributes to an in-
formation system tailored to the requirements of
physicians. Two separate evaluations demonstrate
the effectiveness of our approach.
</bodyText>
<sectionHeader confidence="0.992603" genericHeader="introduction">
2 Clinical Information Needs
</sectionHeader>
<bodyText confidence="0.999821555555556">
Although the need to answer questions related
to patient care has been well documented (Cov-
ell et al., 1985; Gorman et al., 1994; Ely et al.,
1999), studies have shown that existing search sys-
tems, e.g., PubMed, the U.S. National Library of
Medicine’s search engine, are often unable to sup-
ply physicians with clinically-relevant answers in
a timely manner (Gorman et al., 1994; Cham-
bliss and Conley, 1996). Clinical information
</bodyText>
<page confidence="0.9725">
841
</page>
<note confidence="0.6662325">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 841–848,
Sydney, July 2006. c�2006 Association for Computational Linguistics
Disease: Chronic Prostatitis
► anti-microbial
</note>
<listItem confidence="0.990306909090909">
1. [temafloxacin] Treatment of chronic bacterial prostatitis with temafloxacin. Temafloxacin 400 mg b.i.d. adminis-
tered orally for 28 days represents a safe and effective treatment for chronic bacterial prostatitis.
2. [ofloxacin] Ofloxacin in the management of complicated urinary tract infections, including prostatitis. In chronic
bacterial prostatitis, results to date suggest that ofloxacin may be more effective clinically and as effective micro-
biologically as carbenicillin.
3. ...
► Alpha-adrenergic blocking agent
1. [terazosine] Terazosin therapy for chronic prostatitis/chronic pelvic pain syndrome: a randomized, placebo con-
trolled trial. CONCLUSIONS: Terazosin proved superior to placebo for patients with chronic prostatitis/chronic
pelvic pain syndrome who had not received alpha-blockers previously.
2. ...
</listItem>
<tableCaption confidence="0.991914">
Table 1: System response to the question “What is the best drug treatment for chronic prostatitis?”
</tableCaption>
<bodyText confidence="0.999588097560976">
systems for decision support represent a poten-
tially high-impact application. From a research
perspective, the clinical domain is attractive be-
cause substantial knowledge has already been cod-
ified in the Unified Medical Language System
(UMLS) (Lindberg et al., 1993). The 2004 version
of the UMLS Metathesaurus contains information
about over 1 million biomedical concepts and 5
million concept names. This and related resources
allow us to explore knowledge-based techniques
with substantially less upfront investment.
Naturally, physicians have a wide spectrum of
information needs, ranging from questions about
the selection of treatment options to questions
about legal issues. To make the retrieval problem
more tractable, we focus on a subset of therapy
questions taking the form “What is the best drug
treatment for X?”, where X can be any number of
diseases. We have chosen to tackle this class of
questions because studies of physicians’ behavior
in natural settings have revealed that such ques-
tions occur quite frequently (Ely et al., 1999). By
leveraging the natural distribution of clinical in-
formation needs, we can make the greatest impact
with the least effort.
Our research follows the principles of evidence-
based medicine (EBM) (Sackett et al., 2000),
which provides a well-defined model to guide the
process of clinical question answering. EBM is
a widely-accepted paradigm for medical practice
that involves the explicit use of current best ev-
idence, i.e., high-quality patient-centered clinical
research reported in the primary medical literature,
to make decisions about patient care. As shown
by previous work (Cogdill and Moore, 1997; De
Groote and Dorsch, 2003), citations from the
MEDLINE database (maintained by the U.S. Na-
tional Library of Medicine) serve as a good source
of clinical evidence. As a result of these findings,
our work focuses on MEDLINE abstracts as the
source for answers.
</bodyText>
<sectionHeader confidence="0.965997" genericHeader="method">
3 Question Answering Approach
</sectionHeader>
<bodyText confidence="0.998083954545455">
Conflicting desiderata shape the characteristics of
“answers” to clinical questions. On the one hand,
conciseness is paramount. Physicians are always
under time pressure when making decisions, and
information overload is a serious concern. Fur-
thermore, we ultimately envision deploying ad-
vanced retrieval systems in portable packages such
as PDAs to serve as tools in bedside interac-
tions (Hauser et al., 2004). The small form factor
of such devices limits the amount of text that can
be displayed. However, conciseness exists in ten-
sion with completeness. For physicians, the im-
plications of making potentially life-altering deci-
sions mean that all evidence must be carefully ex-
amined in context. For example, the efficacy of a
drug is always framed in the context of a specific
sample population, over a set duration, at some
fixed dosage, etc. A physician simply cannot rec-
ommend a particular course of action without con-
sidering all these factors.
Our approach seeks to balance conciseness and
completeness by providing hierarchical and inter-
</bodyText>
<page confidence="0.995083">
842
</page>
<bodyText confidence="0.999776">
active “answers” that support multiple levels of
drill-down. A partial example is shown in Fig-
ure 1. Top-level answers to “What is the best drug
treatment for X?” consist of categories of drugs
that may be of interest to the physician. Each cat-
egory is associated with a cluster of abstracts from
MEDLINE about that particular treatment option.
Drilling down into a cluster, the physician is pre-
sented with extractive summaries of abstracts that
outline the clinical findings. To obtain more detail,
the physician can pull up the complete abstract
text, and finally the electronic version of the en-
tire article (if available). In the example shown in
Figure 1, the physician can see that two classes of
drugs (anti-microbial and alpha-adrenergic block-
ing agent) are relevant for the disease “chronic
prostatitis”. Drilling down into the first cluster, the
physician can see summarized evidence for two
specific types of anti-microbials (temafloxacin and
ofloxacin) extracted from MEDLINE abstracts.
Three major capabilities are required to produce
the “answers” described above. First, the system
must accurately identify the drugs under study in
an abstract. Second, the system must group ab-
stracts based on these substances in a meaningful
way. Third, the system must generate short sum-
maries of the clinical findings. We describe a clin-
ical question answering system that implements
exactly these capabilities (answer extraction, se-
mantic clustering, and extractive summarization).
</bodyText>
<sectionHeader confidence="0.992599" genericHeader="method">
4 System Implementation
</sectionHeader>
<bodyText confidence="0.999997375">
Our work is primarily concerned with synthesiz-
ing coherent answers from a set of search results—
the actual source of these results is not important.
For convenience, we employ MEDLINE citations
retrieved by the PubMed search engine (which
also serves as a baseline for comparison). Given
an initial set of citations, answer generation pro-
ceeds in three phases, described below.
</bodyText>
<subsectionHeader confidence="0.99884">
4.1 Answer Extraction
</subsectionHeader>
<bodyText confidence="0.9997477">
Given a set of abstracts, our system first identi-
fies the drugs under study; these later become the
short answers. In the parlance of evidence-based
medicine, drugs fall into the category of “interven-
tions”, which encompasses everything from surgi-
cal procedures to diagnostic tests.
Our extractor for interventions relies on
MetaMap (Aronson, 2001), a program that au-
tomatically identifies entities corresponding to
UMLS concepts. UMLS has an extensive cov-
erage of drugs, falling under the semantic type
PHARMACOLOGICAL SUBSTANCE and a few oth-
ers. All such entities are identified as candidates
and each is scored based on a number of features:
its position in the abstract, its frequency of occur-
rence, etc. A separate evaluation on a blind test
set demonstrates that our extractor is able to accu-
rately recognize the interventions in a MEDLINE
abstract; see details in (Demner-Fushman and Lin,
2005; Demner-Fushman and Lin, 2006 in press).
</bodyText>
<subsectionHeader confidence="0.998246">
4.2 Semantic Clustering
</subsectionHeader>
<bodyText confidence="0.999959823529412">
Retrieved MEDLINE citations are organized into
semantic clusters based on the main interventions
identified in the abstract text. We employed a
variant of the hierarchical agglomerative cluster-
ing algorithm (Zhao and Karypis, 2002) that uti-
lizes semantic relationships within UMLS to com-
pute similarities between interventions.
Iteratively, we group abstracts whose interven-
tions fall under a common ancestor, i.e., a hyper-
nym. The more generic ancestor concept (i.e., the
class of drugs) is then used as the cluster label.
The process repeats until no new clusters can be
formed. In order to preserve granularity at the
level of practical clinical interest, the tops of the
UMLS hierarchy were truncated; for example, the
MeSH category “Chemical and Drugs” is too gen-
eral to be useful. This process was manually per-
formed during system development. We decided
to allow an abstract to appear in multiple clusters
if more than one intervention was identified, e.g.,
if the abstract compared the efficacy of two treat-
ments. Once the clusters have been formed, all
citations are then sorted in the order of the origi-
nal PubMed results, with the most abstract UMLS
concept as the cluster label. Clusters themselves
are sorted in decreasing size under the assumption
that more clinical research is devoted to more per-
tinent types of drugs.
Returning to the example in Figure 1, the ab-
stracts about temafloxacin and ofloxacin were
clustered together because both drugs are hy-
ponyms of anti-microbials within the UMLS on-
tology. As can be seen, this semantic resource pro-
vides a powerful tool for organizing search results.
</bodyText>
<subsectionHeader confidence="0.998172">
4.3 Extractive Summarization
</subsectionHeader>
<bodyText confidence="0.999044666666667">
For each MEDLINE citation, our system gener-
ates a short extractive summary consisting of three
elements: the main intervention (which is usu-
</bodyText>
<page confidence="0.991098">
843
</page>
<bodyText confidence="0.999966714285714">
ally more specific than the cluster label); the ti-
tle of the abstract; and the top-scoring outcome
sentence. The “outcome”, another term from
evidence-based medicine, asserts the clinical find-
ings of a study, and is typically found towards
the end of a MEDLINE abstract. In our case,
outcome sentences state the efficacy of a drug in
treating a particular disease. Previously, we have
built an outcome extractor capable of identifying
such sentences in MEDLINE abstracts using su-
pervised machine learning techniques (Demner-
Fushman and Lin, 2005; Demner-Fushman and
Lin, 2006 in press). Evaluation on a blind held-
out test set shows high classification accuracy.
</bodyText>
<sectionHeader confidence="0.993137" genericHeader="method">
5 Evaluation Methodology
</sectionHeader>
<bodyText confidence="0.999991">
Given that our work draws from QA, IR, and sum-
marization, a proper evaluation that captures the
salient characteristics of our system proved to be
quite challenging. Overall, evaluation can be de-
composed into two separate components: locating
a suitable resource to serve as ground truth and
leveraging it to assess system responses.
It is not difficult to find disease-specific pharma-
cology resources. We employed Clinical Evidence
(CE), a periodic report created by the British Med-
ical Journal (BMJ) Publishing Group that summa-
rizes the best known drugs for a few dozen dis-
eases. Note that the existence of such secondary
sources does not obviate the need for automated
systems because they are perpetually falling out of
date due to rapid advances in medicine. Further-
more, such reports are currently created by highly-
experienced physicians, which is an expensive and
time-consuming process.
For each disease, CE classifies drugs into one of
six categories: beneficial, likely beneficial, trade-
offs (i.e., may have adverse side effects), un-
known, unlikely beneficial, and harmful. Included
with each entry is a list of references—citations
consulted by the editors in compiling the resource.
Although the completeness of the drugs enumer-
ated in CE is questionable, it nevertheless can be
viewed as “authoritative”.
</bodyText>
<subsectionHeader confidence="0.856095">
5.1 Previous Work
</subsectionHeader>
<bodyText confidence="0.999966875">
How can we leverage a resource such as CE to as-
sess the responses generated by our system? A
survey of evaluation methodologies reveals short-
comings in existing techniques.
Answers to factoid questions are automatically
scored using regular expression patterns (Lin,
2005). In our application, this is inadequate
for many reasons: there is rarely an exact string
match between system output and drugs men-
tioned in CE, primarily due to synonymy (for ex-
ample, alpha-adrenergic blocking agent and α-
blocker refer to the same class of drugs) and on-
tological mismatch (for example, CE might men-
tion beta-agonists, while a retrieved abstract dis-
cusses formoterol, which is a specific represen-
tative of beta-agonists). Furthermore, while this
evaluation method can tell us if the drugs proposed
by the system are “good”, it cannot measure how
well the answer is supported by MEDLINE cita-
tions; recall that answer justification is important
for physicians.
The nugget evaluation methodology (Voorhees,
2005) developed for scoring answers to com-
plex questions is not suitable for our task, since
there is no coherent notion of an “answer text”
that the user reads end–to–end. Furthermore, it
is unclear what exactly a “nugget” in this case
would be. For similar reasons, methodologies for
summarization evaluation are also of little help.
Typically, system-generated summaries are either
evaluated manually by humans (which is expen-
sive and time-consuming) or automatically using
a metric such as ROUGE, which compares sys-
tem output against a number of reference sum-
maries. The interactive nature of our answers vio-
lates the assumption that systems’ responses are
static text segments. Furthermore, it is unclear
what exactly should go into a reference summary,
because physicians may want varying amounts of
detail depending on familiarity with the disease
and patient-specific factors.
Evaluation methodologies from information re-
trieval are also inappropriate. User studies have
previously been employed to examine the effect
of categorized search results. However, they often
conflate the effectiveness of the interface with that
of the underlying algorithms. For example, Du-
mais et al. (2001) found significant differences in
task performance based on different ways of using
purely presentational devices such as mouseovers,
expandable lists, etc. While interface design is
clearly important, it is not the focus of our work.
Clustering techniques have also been evaluated
in the same manner as text classification algo-
rithms, in terms of precision, recall, etc. based
on some ground truth (Zhao and Karypis, 2002).
</bodyText>
<page confidence="0.990419">
844
</page>
<bodyText confidence="0.999979052631579">
This, however, assumes the existence of stable,
invariant categories, which is not the case since
our output clusters are query-specific. Although
it may be possible to manually create “reference
clusters”, we lack sufficient resources to develop
such a data set. Furthermore, it is unclear if suffi-
cient interannotator agreement can be obtained to
support meaningful evaluation.
Ultimately, we devised two separate evaluations
to assess the quality of our system output based
on the techniques discussed above. The first is
a manual evaluation focused on the cluster labels
(i.e., drug categories), based on a factoid QA eval-
uation methodology. The second is an automatic
evaluation of the retrieved abstracts using ROUGE,
drawing elements from summarization evaluation.
Details of the evaluation setup and results are pre-
ceded by a description of the test collection we
created from CE.
</bodyText>
<subsectionHeader confidence="0.99929">
5.2 Test Collection
</subsectionHeader>
<bodyText confidence="0.999991581395349">
We were able to mine the June 2004 edition of
Clinical Evidence to create a test collection for
system evaluation. We randomly selected thirty
diseases, generating a development set of five
questions and a test set of twenty-five questions.
Some examples include: acute asthma, chronic
prostatitis, community acquired pneumonia, and
erectile dysfunction. CE listed an average of 11.3
interventions per disease; of those, 2.3 on average
were marked as beneficial and 1.9 as likely benefi-
cial. On average, there were 48.4 references asso-
ciated with each disease, representing the articles
consulted during the compilation of CE itself. Of
those, 34.7 citations on average appeared in MED-
LINE; we gathered all these abstracts, which serve
as the reference summaries for our ROUGE-based
automatic evaluation.
Since the focus of our work is not on retrieval al-
gorithms per se, we employed PubMed to fetch an
initial set of MEDLINE citations and performed
answer synthesis using those results. The PubMed
citations also serve as a baseline, since it repre-
sents a system commonly used by physicians.
In order to obtain the best possible set of ci-
tations, the first author (an experienced PubMed
searcher), manually formulated queries, taking
advantage of MeSH (Medical Subject Headings)
terms when available. MeSH terms are controlled
vocabulary concepts assigned manually by trained
medical indexers (based on the full text of the ar-
ticles), and encode a substantial amount of knowl-
edge about the contents of the citation. PubMed
allows searches on MeSH terms, which usually
yield accurate results. In addition, we limited re-
trieved citations to those that have the MeSH head-
ing “drug therapy” and those that describe a clin-
ical trial (another metadata field). Finally, we re-
stricted the date range of the queries so that ab-
stracts published after our version of CE were ex-
cluded. Although the query formulation process
currently requires a human, we envision automat-
ing this step using a template-based approach in
the future.
</bodyText>
<sectionHeader confidence="0.993999" genericHeader="method">
6 System Evaluation
</sectionHeader>
<bodyText confidence="0.999979666666667">
We adapted existing techniques to evaluate our
system in two separate ways: a factoid-style man-
ual evaluation focused on short answers and an
automatic evaluation with ROUGE using CE-cited
abstracts as the reference summaries. The setup
and results for both are detailed below.
</bodyText>
<subsectionHeader confidence="0.997341">
6.1 Manual Evaluation of Short Answers
</subsectionHeader>
<bodyText confidence="0.999995464285714">
In our manual evaluation, system outputs were as-
sessed as if they were answers to factoid ques-
tions. We gathered three different sets of answers.
For the baseline, we used the main intervention
from each of the first three PubMed citations. For
our test condition, we considered the three largest
clusters, taking the main intervention from the first
abstract in each cluster. This yields three drugs
that are at the same level of ontological granularity
as those extracted from the unclustered PubMed
citations. For our third condition, we assumed the
existence of an oracle which selects the three best
clusters (as determined by the first author, a med-
ical doctor). From each of these three clusters,
we extracted the main intervention of the first ab-
stracts. This oracle condition represents an achiev-
able upper bound with a human in the loop. Physi-
cians are highly-trained professionals that already
have significant domain knowledge. Faced with a
small number of choices, it is likely that they will
be able to select the most promising cluster, even
if they did not previously know it.
This preparation yielded up to nine drug names,
three from each experimental condition. For short,
we refer to these as PubMed, Cluster, and Oracle,
respectively. After blinding the source of the drugs
and removing duplicates, each short answer was
presented to the first author for evaluation. Since
</bodyText>
<page confidence="0.997258">
845
</page>
<table confidence="0.9997922">
Clinical Evidence Physician
B LB T U UB H N Good Okay Bad
PubMed 0.200 0.213 0.160 0.053 0.000 0.013 0.360 0.600 0.227 0.173
Cluster 0.387 0.173 0.173 0.027 0.000 0.000 0.240 0.827 0.133 0.040
Oracle 0.400 0.200 0.133 0.093 0.013 0.000 0.160 0.893 0.093 0.013
</table>
<tableCaption confidence="0.936253">
Table 2: Manual evaluation of short answers: distribution of system answers with respect to CE cat-
egories (left side) and with respect to the assessor’s own expertise (right side). (Key: B=beneficial,
LB=likely beneficial, T=tradeoffs, U=unknown, UB=unlikely beneficial, H=harmful, N=not in CE)
</tableCaption>
<bodyText confidence="0.999280536585366">
the assessor had no idea from which condition an
answer came, this process guarded against asses-
sor bias.
Each answer was evaluated in two different
ways: first, with respect to the ground truth in CE,
and second, using the assessor’s own medical ex-
pertise. In the first set of judgments, the asses-
sor determined which of the six categories (ben-
eficial, likely beneficial, tradeoffs, unknown, un-
likely beneficial, harmful) the system answer be-
longed to, based on the CE recommendations. As
we have discussed previously, a human (with suf-
ficient domain knowledge) is required to perform
this matching due to synonymy and differences in
ontological granularity. However, note that the as-
sessor only considered the drug name when mak-
ing this categorization. In the second set of judg-
ments, the assessor separately determined if the
short answer was “good”, “okay” (marginal), or
“bad” based both on CE and her own experience,
taking into account the abstract title and the top-
scoring outcome sentence (and if necessary, the
entire abstract text).
Results of this manual evaluation are presented
in Table 2, which shows the distribution of judg-
ments for the three experimental conditions. For
baseline PubMed, 20% of the examined drugs fell
in the beneficial category; the values are 39% for
the Cluster condition and 40% for the Oracle con-
dition. In terms of short answers, our system
returns approximately twice as many beneficial
drugs as the baseline, a marked increase in answer
accuracy. Note that a large fraction of the drugs
evaluated were not found in CE at all, which pro-
vides an estimate of its coverage. In terms of the
assessor’s own judgments, 60% of PubMed short
answers were found to be “good”, compared to
83% and 89% for the Cluster and Oracle condi-
tions, respectively. From a factoid QA point of
view, we can conclude that our system outper-
forms the PubMed baseline.
</bodyText>
<subsectionHeader confidence="0.99969">
6.2 Automatic Evaluation of Abstracts
</subsectionHeader>
<bodyText confidence="0.999907435897436">
A major limitation of the factoid-based evaluation
methodology is that it does not measure the qual-
ity of the abstracts from which the short answers
were extracted. Since we lacked the necessary
resources to manually gather abstract-level judg-
ments for evaluation, we sought an alternative.
Fortunately, CE can be leveraged to assess the
“goodness” of abstracts automatically. We assume
that references cited in CE are examples of high
quality abstracts, since they were used in gener-
ating the drug recommendations. Following stan-
dard assumptions made in summarization evalu-
ation, we considered abstracts that are similar in
content with these “reference abstracts” to also be
“good” (i.e., relevant). Similarity in content can
be quantified with ROUGE.
Since physicians demand high precision, we as-
sess the cumulative relevance after the first, sec-
ond, and third abstract that the clinician is likely
to have examined (where the relevance for each
individual abstract is given by its ROUGE-1 pre-
cision score). For the baseline PubMed condition,
the examined abstracts simply correspond to the
first three hits in the result set. For our test system,
we developed three different orderings. The first,
which we term cluster round-robin, selects the first
abstract from the top three clusters (by size). The
second, which we term oracle cluster order, selects
three abstracts from the best cluster, assuming the
existence of an oracle that informs the system. The
third, which we term oracle round-robin, selects
the first abstract from each of the three best clus-
ters (also determined by an oracle).
Results of this evaluation are shown in Table 3.
The columns show the cumulative relevance (i.e.,
ROUGE score) after examining the first, second,
and third abstract, under the different ordering
conditions. To determine statistical significance,
we applied the Wilcoxon signed-rank test, the
</bodyText>
<page confidence="0.996202">
846
</page>
<table confidence="0.9992862">
Rank 1 Rank 2 Rank 3
PubMed Ranked List 0.170 0.349 0.523
Cluster Round-Robin 0.181 (+6.3%)° 0.356 (+2.1%)° 0.526 (+0.5%)°
Oracle Cluster Order 0.206 (+21.5%)° 0.392 (+12.6%)° 0.597 (+14.0%)•
Oracle Round-Robin 0.206 (+21.5%)° 0.396 (+13.6%)° 0.586 (+11.9%)•
</table>
<tableCaption confidence="0.802242">
Table 3: Cumulative relevance after examining the first, second, and third abstracts, according to different
orderings. (° denotes n.s., ° denotes sig. at 0.90, • denotes sig. at 0.95)
</tableCaption>
<bodyText confidence="0.998950166666667">
standard non-parametric test for applications of
this type. Due to the relatively small test set (only
25 questions), the increase in cumulative relevance
exhibited by the cluster round-robin condition is
not statistically significant. However, differences
for the oracle conditions were significant.
</bodyText>
<sectionHeader confidence="0.996655" genericHeader="discussions">
7 Discussion and Related Work
</sectionHeader>
<bodyText confidence="0.999736289473684">
According to two separate evaluations, it appears
that our system outperforms the PubMed baseline.
However, our approach provides more advantages
over a linear result set that are not highlighted in
these evaluations. Although difficult to quantify,
categorized results provide an overview of the in-
formation landscape that is difficult to acquire by
simply browsing a ranked list—user studies of cat-
egorized search have affirmed its value (Hearst
and Pedersen, 1996; Dumais et al., 2001). One
main advantage we see in our application is bet-
ter “redundancy management”. With a ranked list,
the physician may be forced to browse through
multiple redundant abstracts that discuss the same
or similar drugs to get a sense of the different
treatment options. With our cluster-based ap-
proach, however, potentially redundant informa-
tion is grouped together, since interventions dis-
cussed in a particular cluster are ontologically re-
lated through UMLS. The physician can examine
different clusters for a broad overview, or peruse
multiple abstracts within a cluster for a more thor-
ough review of the evidence. Our cluster-based
system is able to support both types of behaviors.
This work demonstrates the value of semantic
resources in the question answering process, since
our approach makes extensive use of the UMLS
ontology in all phases of answer synthesis. The
coverage of individual drugs, as well as the rela-
tionship between different types of drugs within
UMLS enables both answer extraction and seman-
tic clustering. As detailed in (Demner-Fushman
and Lin, 2006 in press), UMLS-based features are
also critical in the identification of clinical out-
comes, on which our extractive summaries are
based. As a point of comparison, we also im-
plemented a purely term-based approach to clus-
tering PubMed citations. The results are so inco-
herent that a formal evaluation would prove to be
meaningless. Semantic relations between drugs,
as captured in UMLS, provide an effective method
for organizing results—these relations cannot be
captured by keyword content alone. Furthermore,
term-based approaches suffer from the cluster la-
beling problem: it is difficult to automatically gen-
erate a short heading that describes cluster content.
Nevertheless, there are a number of assump-
tions behind our work that are worth pointing
out. First, we assume a high quality initial re-
sult set. Since the class of questions we examine
translates naturally into accurate PubMed queries
that can make full use of human-assigned MeSH
terms, the overall quality of the initial citations
can be assured. Related work in retrieval algo-
rithms (Demner-Fushman and Lin, 2006 in press)
shows that accurate relevance scoring of MED-
LINE citations in response to more general clin-
ical questions is possible.
Second, our system does not actually perform
semantic processing to determine the efficacy of a
drug: it only recognizes “topics” and outcome sen-
tences that state clinical findings. Since the sys-
tem by default orders the clusters based on size, it
implicitly equates “most popular drug” with “best
drug”. Although this assumption is false, we have
observed in practice that more-studied drugs are
more likely to be beneficial.
In contrast with the genomics domain, which
has received much attention from both the IR and
NLP communities, retrieval systems for the clin-
ical domain represent an underexplored area of
research. Although individual components that
attempt to operationalize principles of evidence-
based medicine do exist (Mendonc¸a and Cimino,
2001; Niu and Hirst, 2004), complete end–to-
end clinical question answering systems are dif-
</bodyText>
<page confidence="0.990154">
847
</page>
<bodyText confidence="0.999860166666667">
ficult to find. Within the context of the PERSI-
VAL project (McKeown et al., 2003), researchers
at Columbia have developed a system that lever-
ages patient records to rerank search results. Since
the focus is on personalized summaries, this work
can be viewed as complementary to our own.
</bodyText>
<sectionHeader confidence="0.994297" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999684">
The primary contribution of this work is the de-
velopment of a clinical question answering system
that caters to the unique requirements of physi-
cians, who demand both conciseness and com-
pleteness. These competing factors can be bal-
anced in a system’s response by providing mul-
tiple levels of drill-down that allow the informa-
tion space to be viewed at different levels of gran-
ularity. We have chosen to implement these capa-
bilities through answer extraction, semantic clus-
tering, and extractive summarization. Two sepa-
rate evaluations demonstrate that our system out-
performs the PubMed baseline, illustrating the ef-
fectiveness of a hybrid approach that leverages se-
mantic resources.
</bodyText>
<sectionHeader confidence="0.99798" genericHeader="acknowledgments">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.995317333333333">
This work was supported in part by the U.S. Na-
tional Library of Medicine. The second author
thanks Esther and Kiri for their loving support.
</bodyText>
<sectionHeader confidence="0.998905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999858277777778">
E. Amig´o, J. Gonzalo, V. Peinado, A. Pe˜nas, and
F. Verdejo. 2004. An empirical study of informa-
tion synthesis task. In ACL 2004.
A. Aronson. 2001. Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: The MetaMap
program. In AMIA 2001.
M. Chambliss and J. Conley. 1996. Answering clinical
questions. The Journal of Family Practice, 43:140–
144.
K. Cogdill and M. Moore. 1997. First-year medi-
cal students’ information needs and resource selec-
tion: Responses to a clinical scenario. Bulletin of
the Medical Library Association, 85(1):51–54.
D. Covell, G. Uman, and P. Manning. 1985. Informa-
tion needs in office practice: Are they being met?
Annals of Internal Medicine, 103(4):596–599.
H. Dang. 2005. Overview of DUC 2005. In DUC
2005 Workshop at HLT/EMNLP 2005.
S. De Groote and J. Dorsch. 2003. Measuring use
patterns of online journals and databases. Journal of
the Medical Library Association, 91(2):231–240.
D. Demner-Fushman and J. Lin. 2005. Knowledge ex-
traction for clinical question answering: Preliminary
results. In AAAI 2005 Workshop on QA in Restricted
Domains.
D. Demner-Fushman and J. Lin. 2006, in press. An-
swering clinical questions with knowledge-based
and statistical techniques. Comp. Ling.
S. Dumais, E. Cutrell, and H. Chen. 2001. Optimizing
search by showing results in context. In CHI 2001.
J. Ely, J. Osheroff, M. Ebell, G. Bergus, B. Levy,
M. Chambliss, and E. Evans. 1999. Analysis of
questions asked by family doctors regarding patient
care. BMJ, 319:358–361.
P. Gorman, J. Ash, and L. Wykoff. 1994. Can pri-
mary care physicians’ questions be answered using
the medical journal literature? Bulletin of the Medi-
cal Library Association, 82(2):140–146, April.
S. Hauser, D. Demner-Fushman, G. Ford, and
G. Thoma. 2004. PubMed on Tap: Discovering
design principles for online information delivery to
handheld computers. In MEDINFO 2004.
M. Hearst and J. Pedersen. 1996. Reexaming the clus-
ter hypothesis: Scatter/gather on retrieval results. In
SIGIR 1996.
D. Lawrie and W. Croft. 2003. Generating hierarchical
summaries for Web searches. In SIGIR 2003.
J. Lin. 2005. Evaluation of resources for question an-
swering evaluation. In SIGIR 2005.
D. Lindberg, B. Humphreys, and A. McCray. 1993.
The Unified Medical Language System. Methods of
Information in Medicine, 32(4):281–291.
K. McKeown, N. Elhadad, and V. Hatzivassiloglou.
2003. Leveraging a common representation for per-
sonalized search and summarization in a medical
digital library. In JCDL 2003.
E. Mendonc¸a and J. Cimino. 2001. Building a knowl-
edge base to support a digital library. In MEDINFO
2001.
Y. Niu and G. Hirst. 2004. Analysis of semantic
classes in medical text for question answering. In
ACL 2004 Workshop on QA in Restricted Domains.
David Sackett, Sharon Straus, W. Richardson, William
Rosenberg, and R. Haynes. 2000. Evidence-
Based Medicine: How to Practice and Teach EBM.
Churchill Livingstone, second edition.
E. Voorhees. 2005. Using question series to eval-
uate question answering system effectiveness. In
HLT/EMNLP 2005.
Y. Zhao and G. Karypis. 2002. Evaluation of hierar-
chical clustering algorithms for document datasets.
In CIKM 2002.
</reference>
<page confidence="0.997225">
848
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.699646">
<title confidence="0.996177">Answer Extraction, Semantic Clustering, and Extractive Summarization for Clinical Question Answering</title>
<author confidence="0.943734">Jimmy</author>
<affiliation confidence="0.87074325">of Computer Science of Information Studies for Advanced Computer Studies University of Maryland</affiliation>
<address confidence="0.999987">College Park, MD 20742, USA</address>
<email confidence="0.999966">demner@cs.umd.edu,jimmylin@umd.edu</email>
<abstract confidence="0.997436666666667">This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval. We tackle a frequently-occurring class of questions that takes the form “What is best drug treatment for Starting from an initial set of MEDLINE citations, our system first identifies the drugs under study. Abstracts are then clustered using semantic classes from the UMLS ontology. Finally, a short extractive summary is generated for each abstract to populate the clusters. Two evaluations—a manual one focused on short answers and an automatic one focused on the supporting abstracts—demonstrate that our system compares favorably to PubMed, the search system most widely used by physicians today.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Amig´o</author>
<author>J Gonzalo</author>
<author>V Peinado</author>
<author>A Pe˜nas</author>
<author>F Verdejo</author>
</authors>
<title>An empirical study of information synthesis task.</title>
<date>2004</date>
<booktitle>In ACL</booktitle>
<marker>Amig´o, Gonzalo, Peinado, Pe˜nas, Verdejo, 2004</marker>
<rawString>E. Amig´o, J. Gonzalo, V. Peinado, A. Pe˜nas, and F. Verdejo. 2004. An empirical study of information synthesis task. In ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Aronson</author>
</authors>
<title>Effective mapping of biomedical text to the UMLS Metathesaurus: The MetaMap program.</title>
<date>2001</date>
<booktitle>In AMIA</booktitle>
<contexts>
<context position="9898" citStr="Aronson, 2001" startWordPosition="1485" endWordPosition="1486">not important. For convenience, we employ MEDLINE citations retrieved by the PubMed search engine (which also serves as a baseline for comparison). Given an initial set of citations, answer generation proceeds in three phases, described below. 4.1 Answer Extraction Given a set of abstracts, our system first identifies the drugs under study; these later become the short answers. In the parlance of evidence-based medicine, drugs fall into the category of “interventions”, which encompasses everything from surgical procedures to diagnostic tests. Our extractor for interventions relies on MetaMap (Aronson, 2001), a program that automatically identifies entities corresponding to UMLS concepts. UMLS has an extensive coverage of drugs, falling under the semantic type PHARMACOLOGICAL SUBSTANCE and a few others. All such entities are identified as candidates and each is scored based on a number of features: its position in the abstract, its frequency of occurrence, etc. A separate evaluation on a blind test set demonstrates that our extractor is able to accurately recognize the interventions in a MEDLINE abstract; see details in (Demner-Fushman and Lin, 2005; Demner-Fushman and Lin, 2006 in press). 4.2 Se</context>
</contexts>
<marker>Aronson, 2001</marker>
<rawString>A. Aronson. 2001. Effective mapping of biomedical text to the UMLS Metathesaurus: The MetaMap program. In AMIA 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chambliss</author>
<author>J Conley</author>
</authors>
<title>Answering clinical questions.</title>
<date>1996</date>
<journal>The Journal of Family Practice,</journal>
<volume>43</volume>
<pages>144</pages>
<contexts>
<context position="3448" citStr="Chambliss and Conley, 1996" startWordPosition="511" endWordPosition="515">scribes how each of these capabilities contributes to an information system tailored to the requirements of physicians. Two separate evaluations demonstrate the effectiveness of our approach. 2 Clinical Information Needs Although the need to answer questions related to patient care has been well documented (Covell et al., 1985; Gorman et al., 1994; Ely et al., 1999), studies have shown that existing search systems, e.g., PubMed, the U.S. National Library of Medicine’s search engine, are often unable to supply physicians with clinically-relevant answers in a timely manner (Gorman et al., 1994; Chambliss and Conley, 1996). Clinical information 841 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 841–848, Sydney, July 2006. c�2006 Association for Computational Linguistics Disease: Chronic Prostatitis ► anti-microbial 1. [temafloxacin] Treatment of chronic bacterial prostatitis with temafloxacin. Temafloxacin 400 mg b.i.d. administered orally for 28 days represents a safe and effective treatment for chronic bacterial prostatitis. 2. [ofloxacin] Ofloxacin in the management of complicated urinary tract infections, including prostatitis. In chro</context>
</contexts>
<marker>Chambliss, Conley, 1996</marker>
<rawString>M. Chambliss and J. Conley. 1996. Answering clinical questions. The Journal of Family Practice, 43:140– 144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Cogdill</author>
<author>M Moore</author>
</authors>
<title>First-year medical students’ information needs and resource selection: Responses to a clinical scenario.</title>
<date>1997</date>
<journal>Bulletin of the Medical Library Association,</journal>
<volume>85</volume>
<issue>1</issue>
<contexts>
<context position="6298" citStr="Cogdill and Moore, 1997" startWordPosition="922" endWordPosition="925">Ely et al., 1999). By leveraging the natural distribution of clinical information needs, we can make the greatest impact with the least effort. Our research follows the principles of evidencebased medicine (EBM) (Sackett et al., 2000), which provides a well-defined model to guide the process of clinical question answering. EBM is a widely-accepted paradigm for medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature, to make decisions about patient care. As shown by previous work (Cogdill and Moore, 1997; De Groote and Dorsch, 2003), citations from the MEDLINE database (maintained by the U.S. National Library of Medicine) serve as a good source of clinical evidence. As a result of these findings, our work focuses on MEDLINE abstracts as the source for answers. 3 Question Answering Approach Conflicting desiderata shape the characteristics of “answers” to clinical questions. On the one hand, conciseness is paramount. Physicians are always under time pressure when making decisions, and information overload is a serious concern. Furthermore, we ultimately envision deploying advanced retrieval sys</context>
</contexts>
<marker>Cogdill, Moore, 1997</marker>
<rawString>K. Cogdill and M. Moore. 1997. First-year medical students’ information needs and resource selection: Responses to a clinical scenario. Bulletin of the Medical Library Association, 85(1):51–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Covell</author>
<author>G Uman</author>
<author>P Manning</author>
</authors>
<title>Information needs in office practice: Are they being met?</title>
<date>1985</date>
<journal>Annals of Internal Medicine,</journal>
<volume>103</volume>
<issue>4</issue>
<contexts>
<context position="3149" citStr="Covell et al., 1985" startWordPosition="462" endWordPosition="466">ain of clinical medicine, lies at the intersection of question answering, information retrieval, and summarization. We employ answer extraction to identify short answers, semantic clustering to group similar results, and extractive summarization to produce supporting evidence. This paper describes how each of these capabilities contributes to an information system tailored to the requirements of physicians. Two separate evaluations demonstrate the effectiveness of our approach. 2 Clinical Information Needs Although the need to answer questions related to patient care has been well documented (Covell et al., 1985; Gorman et al., 1994; Ely et al., 1999), studies have shown that existing search systems, e.g., PubMed, the U.S. National Library of Medicine’s search engine, are often unable to supply physicians with clinically-relevant answers in a timely manner (Gorman et al., 1994; Chambliss and Conley, 1996). Clinical information 841 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 841–848, Sydney, July 2006. c�2006 Association for Computational Linguistics Disease: Chronic Prostatitis ► anti-microbial 1. [temafloxacin] Treatment of </context>
</contexts>
<marker>Covell, Uman, Manning, 1985</marker>
<rawString>D. Covell, G. Uman, and P. Manning. 1985. Information needs in office practice: Are they being met? Annals of Internal Medicine, 103(4):596–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2005</date>
<booktitle>In DUC 2005 Workshop at HLT/EMNLP</booktitle>
<contexts>
<context position="2222" citStr="Dang, 2005" startWordPosition="328" endWordPosition="329"> response to information needs. The so-called “definition” or “other” questions at recent TREC evaluations (Voorhees, 2005) serve as good examples: “good answers” to these questions include interesting “nuggets” about a particular person, organization, entity, or event. The importance of cross-document information synthesis has not escaped the attention of other researchers. The last few years have seen a convergence between the question answering and summarization communities (Amig´o et al., 2004), as highlighted by the shift from generic to queryfocused summaries in the 2005 DUC evaluation (Dang, 2005). Despite a focus on document ranking, different techniques for organizing search results have been explored by information retrieval researchers, as exemplified by techniques based on clustering (Hearst and Pedersen, 1996; Dumais et al., 2001; Lawrie and Croft, 2003). Our work, which is situated in the domain of clinical medicine, lies at the intersection of question answering, information retrieval, and summarization. We employ answer extraction to identify short answers, semantic clustering to group similar results, and extractive summarization to produce supporting evidence. This paper des</context>
</contexts>
<marker>Dang, 2005</marker>
<rawString>H. Dang. 2005. Overview of DUC 2005. In DUC 2005 Workshop at HLT/EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S De Groote</author>
<author>J Dorsch</author>
</authors>
<title>Measuring use patterns of online journals and databases.</title>
<date>2003</date>
<journal>Journal of the Medical Library Association,</journal>
<volume>91</volume>
<issue>2</issue>
<marker>De Groote, Dorsch, 2003</marker>
<rawString>S. De Groote and J. Dorsch. 2003. Measuring use patterns of online journals and databases. Journal of the Medical Library Association, 91(2):231–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Demner-Fushman</author>
<author>J Lin</author>
</authors>
<title>Knowledge extraction for clinical question answering: Preliminary results.</title>
<date>2005</date>
<booktitle>In AAAI 2005 Workshop on QA in Restricted Domains.</booktitle>
<contexts>
<context position="10450" citStr="Demner-Fushman and Lin, 2005" startWordPosition="1573" endWordPosition="1576">tic tests. Our extractor for interventions relies on MetaMap (Aronson, 2001), a program that automatically identifies entities corresponding to UMLS concepts. UMLS has an extensive coverage of drugs, falling under the semantic type PHARMACOLOGICAL SUBSTANCE and a few others. All such entities are identified as candidates and each is scored based on a number of features: its position in the abstract, its frequency of occurrence, etc. A separate evaluation on a blind test set demonstrates that our extractor is able to accurately recognize the interventions in a MEDLINE abstract; see details in (Demner-Fushman and Lin, 2005; Demner-Fushman and Lin, 2006 in press). 4.2 Semantic Clustering Retrieved MEDLINE citations are organized into semantic clusters based on the main interventions identified in the abstract text. We employed a variant of the hierarchical agglomerative clustering algorithm (Zhao and Karypis, 2002) that utilizes semantic relationships within UMLS to compute similarities between interventions. Iteratively, we group abstracts whose interventions fall under a common ancestor, i.e., a hypernym. The more generic ancestor concept (i.e., the class of drugs) is then used as the cluster label. The proces</context>
</contexts>
<marker>Demner-Fushman, Lin, 2005</marker>
<rawString>D. Demner-Fushman and J. Lin. 2005. Knowledge extraction for clinical question answering: Preliminary results. In AAAI 2005 Workshop on QA in Restricted Domains.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Demner-Fushman</author>
<author>J Lin</author>
</authors>
<title>in press. Answering clinical questions with knowledge-based and statistical techniques.</title>
<date>2006</date>
<journal>Comp. Ling.</journal>
<contexts>
<context position="10480" citStr="Demner-Fushman and Lin, 2006" startWordPosition="1577" endWordPosition="1580">nterventions relies on MetaMap (Aronson, 2001), a program that automatically identifies entities corresponding to UMLS concepts. UMLS has an extensive coverage of drugs, falling under the semantic type PHARMACOLOGICAL SUBSTANCE and a few others. All such entities are identified as candidates and each is scored based on a number of features: its position in the abstract, its frequency of occurrence, etc. A separate evaluation on a blind test set demonstrates that our extractor is able to accurately recognize the interventions in a MEDLINE abstract; see details in (Demner-Fushman and Lin, 2005; Demner-Fushman and Lin, 2006 in press). 4.2 Semantic Clustering Retrieved MEDLINE citations are organized into semantic clusters based on the main interventions identified in the abstract text. We employed a variant of the hierarchical agglomerative clustering algorithm (Zhao and Karypis, 2002) that utilizes semantic relationships within UMLS to compute similarities between interventions. Iteratively, we group abstracts whose interventions fall under a common ancestor, i.e., a hypernym. The more generic ancestor concept (i.e., the class of drugs) is then used as the cluster label. The process repeats until no new cluster</context>
<context position="12879" citStr="Demner-Fushman and Lin, 2006" startWordPosition="1959" endWordPosition="1962">hree elements: the main intervention (which is usu843 ally more specific than the cluster label); the title of the abstract; and the top-scoring outcome sentence. The “outcome”, another term from evidence-based medicine, asserts the clinical findings of a study, and is typically found towards the end of a MEDLINE abstract. In our case, outcome sentences state the efficacy of a drug in treating a particular disease. Previously, we have built an outcome extractor capable of identifying such sentences in MEDLINE abstracts using supervised machine learning techniques (DemnerFushman and Lin, 2005; Demner-Fushman and Lin, 2006 in press). Evaluation on a blind heldout test set shows high classification accuracy. 5 Evaluation Methodology Given that our work draws from QA, IR, and summarization, a proper evaluation that captures the salient characteristics of our system proved to be quite challenging. Overall, evaluation can be decomposed into two separate components: locating a suitable resource to serve as ground truth and leveraging it to assess system responses. It is not difficult to find disease-specific pharmacology resources. We employed Clinical Evidence (CE), a periodic report created by the British Medical </context>
<context position="28322" citStr="Demner-Fushman and Lin, 2006" startWordPosition="4389" endWordPosition="4392">ugh UMLS. The physician can examine different clusters for a broad overview, or peruse multiple abstracts within a cluster for a more thorough review of the evidence. Our cluster-based system is able to support both types of behaviors. This work demonstrates the value of semantic resources in the question answering process, since our approach makes extensive use of the UMLS ontology in all phases of answer synthesis. The coverage of individual drugs, as well as the relationship between different types of drugs within UMLS enables both answer extraction and semantic clustering. As detailed in (Demner-Fushman and Lin, 2006 in press), UMLS-based features are also critical in the identification of clinical outcomes, on which our extractive summaries are based. As a point of comparison, we also implemented a purely term-based approach to clustering PubMed citations. The results are so incoherent that a formal evaluation would prove to be meaningless. Semantic relations between drugs, as captured in UMLS, provide an effective method for organizing results—these relations cannot be captured by keyword content alone. Furthermore, term-based approaches suffer from the cluster labeling problem: it is difficult to autom</context>
</contexts>
<marker>Demner-Fushman, Lin, 2006</marker>
<rawString>D. Demner-Fushman and J. Lin. 2006, in press. Answering clinical questions with knowledge-based and statistical techniques. Comp. Ling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>E Cutrell</author>
<author>H Chen</author>
</authors>
<title>Optimizing search by showing results in context.</title>
<date>2001</date>
<booktitle>In CHI</booktitle>
<contexts>
<context position="2465" citStr="Dumais et al., 2001" startWordPosition="360" endWordPosition="363">on, organization, entity, or event. The importance of cross-document information synthesis has not escaped the attention of other researchers. The last few years have seen a convergence between the question answering and summarization communities (Amig´o et al., 2004), as highlighted by the shift from generic to queryfocused summaries in the 2005 DUC evaluation (Dang, 2005). Despite a focus on document ranking, different techniques for organizing search results have been explored by information retrieval researchers, as exemplified by techniques based on clustering (Hearst and Pedersen, 1996; Dumais et al., 2001; Lawrie and Croft, 2003). Our work, which is situated in the domain of clinical medicine, lies at the intersection of question answering, information retrieval, and summarization. We employ answer extraction to identify short answers, semantic clustering to group similar results, and extractive summarization to produce supporting evidence. This paper describes how each of these capabilities contributes to an information system tailored to the requirements of physicians. Two separate evaluations demonstrate the effectiveness of our approach. 2 Clinical Information Needs Although the need to an</context>
<context position="16516" citStr="Dumais et al. (2001)" startWordPosition="2521" endWordPosition="2525"> interactive nature of our answers violates the assumption that systems’ responses are static text segments. Furthermore, it is unclear what exactly should go into a reference summary, because physicians may want varying amounts of detail depending on familiarity with the disease and patient-specific factors. Evaluation methodologies from information retrieval are also inappropriate. User studies have previously been employed to examine the effect of categorized search results. However, they often conflate the effectiveness of the interface with that of the underlying algorithms. For example, Dumais et al. (2001) found significant differences in task performance based on different ways of using purely presentational devices such as mouseovers, expandable lists, etc. While interface design is clearly important, it is not the focus of our work. Clustering techniques have also been evaluated in the same manner as text classification algorithms, in terms of precision, recall, etc. based on some ground truth (Zhao and Karypis, 2002). 844 This, however, assumes the existence of stable, invariant categories, which is not the case since our output clusters are query-specific. Although it may be possible to ma</context>
<context position="27246" citStr="Dumais et al., 2001" startWordPosition="4219" endWordPosition="4222">on is not statistically significant. However, differences for the oracle conditions were significant. 7 Discussion and Related Work According to two separate evaluations, it appears that our system outperforms the PubMed baseline. However, our approach provides more advantages over a linear result set that are not highlighted in these evaluations. Although difficult to quantify, categorized results provide an overview of the information landscape that is difficult to acquire by simply browsing a ranked list—user studies of categorized search have affirmed its value (Hearst and Pedersen, 1996; Dumais et al., 2001). One main advantage we see in our application is better “redundancy management”. With a ranked list, the physician may be forced to browse through multiple redundant abstracts that discuss the same or similar drugs to get a sense of the different treatment options. With our cluster-based approach, however, potentially redundant information is grouped together, since interventions discussed in a particular cluster are ontologically related through UMLS. The physician can examine different clusters for a broad overview, or peruse multiple abstracts within a cluster for a more thorough review of</context>
</contexts>
<marker>Dumais, Cutrell, Chen, 2001</marker>
<rawString>S. Dumais, E. Cutrell, and H. Chen. 2001. Optimizing search by showing results in context. In CHI 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ely</author>
<author>J Osheroff</author>
<author>M Ebell</author>
<author>G Bergus</author>
<author>B Levy</author>
<author>M Chambliss</author>
<author>E Evans</author>
</authors>
<title>Analysis of questions asked by family doctors regarding patient care.</title>
<date>1999</date>
<journal>BMJ,</journal>
<pages>319--358</pages>
<contexts>
<context position="3189" citStr="Ely et al., 1999" startWordPosition="471" endWordPosition="474">rsection of question answering, information retrieval, and summarization. We employ answer extraction to identify short answers, semantic clustering to group similar results, and extractive summarization to produce supporting evidence. This paper describes how each of these capabilities contributes to an information system tailored to the requirements of physicians. Two separate evaluations demonstrate the effectiveness of our approach. 2 Clinical Information Needs Although the need to answer questions related to patient care has been well documented (Covell et al., 1985; Gorman et al., 1994; Ely et al., 1999), studies have shown that existing search systems, e.g., PubMed, the U.S. National Library of Medicine’s search engine, are often unable to supply physicians with clinically-relevant answers in a timely manner (Gorman et al., 1994; Chambliss and Conley, 1996). Clinical information 841 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 841–848, Sydney, July 2006. c�2006 Association for Computational Linguistics Disease: Chronic Prostatitis ► anti-microbial 1. [temafloxacin] Treatment of chronic bacterial prostatitis with temaf</context>
<context position="5692" citStr="Ely et al., 1999" startWordPosition="831" endWordPosition="834"> us to explore knowledge-based techniques with substantially less upfront investment. Naturally, physicians have a wide spectrum of information needs, ranging from questions about the selection of treatment options to questions about legal issues. To make the retrieval problem more tractable, we focus on a subset of therapy questions taking the form “What is the best drug treatment for X?”, where X can be any number of diseases. We have chosen to tackle this class of questions because studies of physicians’ behavior in natural settings have revealed that such questions occur quite frequently (Ely et al., 1999). By leveraging the natural distribution of clinical information needs, we can make the greatest impact with the least effort. Our research follows the principles of evidencebased medicine (EBM) (Sackett et al., 2000), which provides a well-defined model to guide the process of clinical question answering. EBM is a widely-accepted paradigm for medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature, to make decisions about patient care. As shown by previous work (Cogdill and Moore</context>
</contexts>
<marker>Ely, Osheroff, Ebell, Bergus, Levy, Chambliss, Evans, 1999</marker>
<rawString>J. Ely, J. Osheroff, M. Ebell, G. Bergus, B. Levy, M. Chambliss, and E. Evans. 1999. Analysis of questions asked by family doctors regarding patient care. BMJ, 319:358–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Gorman</author>
<author>J Ash</author>
<author>L Wykoff</author>
</authors>
<title>Can primary care physicians’ questions be answered using the medical journal literature?</title>
<date>1994</date>
<journal>Bulletin of the Medical Library Association,</journal>
<volume>82</volume>
<issue>2</issue>
<contexts>
<context position="3170" citStr="Gorman et al., 1994" startWordPosition="467" endWordPosition="470">ine, lies at the intersection of question answering, information retrieval, and summarization. We employ answer extraction to identify short answers, semantic clustering to group similar results, and extractive summarization to produce supporting evidence. This paper describes how each of these capabilities contributes to an information system tailored to the requirements of physicians. Two separate evaluations demonstrate the effectiveness of our approach. 2 Clinical Information Needs Although the need to answer questions related to patient care has been well documented (Covell et al., 1985; Gorman et al., 1994; Ely et al., 1999), studies have shown that existing search systems, e.g., PubMed, the U.S. National Library of Medicine’s search engine, are often unable to supply physicians with clinically-relevant answers in a timely manner (Gorman et al., 1994; Chambliss and Conley, 1996). Clinical information 841 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 841–848, Sydney, July 2006. c�2006 Association for Computational Linguistics Disease: Chronic Prostatitis ► anti-microbial 1. [temafloxacin] Treatment of chronic bacterial pro</context>
</contexts>
<marker>Gorman, Ash, Wykoff, 1994</marker>
<rawString>P. Gorman, J. Ash, and L. Wykoff. 1994. Can primary care physicians’ questions be answered using the medical journal literature? Bulletin of the Medical Library Association, 82(2):140–146, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hauser</author>
<author>D Demner-Fushman</author>
<author>G Ford</author>
<author>G Thoma</author>
</authors>
<title>PubMed on Tap: Discovering design principles for online information delivery to handheld computers.</title>
<date>2004</date>
<booktitle>In MEDINFO</booktitle>
<contexts>
<context position="7000" citStr="Hauser et al., 2004" startWordPosition="1031" endWordPosition="1034">the U.S. National Library of Medicine) serve as a good source of clinical evidence. As a result of these findings, our work focuses on MEDLINE abstracts as the source for answers. 3 Question Answering Approach Conflicting desiderata shape the characteristics of “answers” to clinical questions. On the one hand, conciseness is paramount. Physicians are always under time pressure when making decisions, and information overload is a serious concern. Furthermore, we ultimately envision deploying advanced retrieval systems in portable packages such as PDAs to serve as tools in bedside interactions (Hauser et al., 2004). The small form factor of such devices limits the amount of text that can be displayed. However, conciseness exists in tension with completeness. For physicians, the implications of making potentially life-altering decisions mean that all evidence must be carefully examined in context. For example, the efficacy of a drug is always framed in the context of a specific sample population, over a set duration, at some fixed dosage, etc. A physician simply cannot recommend a particular course of action without considering all these factors. Our approach seeks to balance conciseness and completeness</context>
</contexts>
<marker>Hauser, Demner-Fushman, Ford, Thoma, 2004</marker>
<rawString>S. Hauser, D. Demner-Fushman, G. Ford, and G. Thoma. 2004. PubMed on Tap: Discovering design principles for online information delivery to handheld computers. In MEDINFO 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
<author>J Pedersen</author>
</authors>
<title>Reexaming the cluster hypothesis: Scatter/gather on retrieval results.</title>
<date>1996</date>
<booktitle>In SIGIR</booktitle>
<contexts>
<context position="2444" citStr="Hearst and Pedersen, 1996" startWordPosition="356" endWordPosition="359">ts” about a particular person, organization, entity, or event. The importance of cross-document information synthesis has not escaped the attention of other researchers. The last few years have seen a convergence between the question answering and summarization communities (Amig´o et al., 2004), as highlighted by the shift from generic to queryfocused summaries in the 2005 DUC evaluation (Dang, 2005). Despite a focus on document ranking, different techniques for organizing search results have been explored by information retrieval researchers, as exemplified by techniques based on clustering (Hearst and Pedersen, 1996; Dumais et al., 2001; Lawrie and Croft, 2003). Our work, which is situated in the domain of clinical medicine, lies at the intersection of question answering, information retrieval, and summarization. We employ answer extraction to identify short answers, semantic clustering to group similar results, and extractive summarization to produce supporting evidence. This paper describes how each of these capabilities contributes to an information system tailored to the requirements of physicians. Two separate evaluations demonstrate the effectiveness of our approach. 2 Clinical Information Needs Al</context>
<context position="27224" citStr="Hearst and Pedersen, 1996" startWordPosition="4215" endWordPosition="4218">cluster round-robin condition is not statistically significant. However, differences for the oracle conditions were significant. 7 Discussion and Related Work According to two separate evaluations, it appears that our system outperforms the PubMed baseline. However, our approach provides more advantages over a linear result set that are not highlighted in these evaluations. Although difficult to quantify, categorized results provide an overview of the information landscape that is difficult to acquire by simply browsing a ranked list—user studies of categorized search have affirmed its value (Hearst and Pedersen, 1996; Dumais et al., 2001). One main advantage we see in our application is better “redundancy management”. With a ranked list, the physician may be forced to browse through multiple redundant abstracts that discuss the same or similar drugs to get a sense of the different treatment options. With our cluster-based approach, however, potentially redundant information is grouped together, since interventions discussed in a particular cluster are ontologically related through UMLS. The physician can examine different clusters for a broad overview, or peruse multiple abstracts within a cluster for a m</context>
</contexts>
<marker>Hearst, Pedersen, 1996</marker>
<rawString>M. Hearst and J. Pedersen. 1996. Reexaming the cluster hypothesis: Scatter/gather on retrieval results. In SIGIR 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lawrie</author>
<author>W Croft</author>
</authors>
<title>Generating hierarchical summaries for Web searches.</title>
<date>2003</date>
<booktitle>In SIGIR</booktitle>
<contexts>
<context position="2490" citStr="Lawrie and Croft, 2003" startWordPosition="364" endWordPosition="367">ity, or event. The importance of cross-document information synthesis has not escaped the attention of other researchers. The last few years have seen a convergence between the question answering and summarization communities (Amig´o et al., 2004), as highlighted by the shift from generic to queryfocused summaries in the 2005 DUC evaluation (Dang, 2005). Despite a focus on document ranking, different techniques for organizing search results have been explored by information retrieval researchers, as exemplified by techniques based on clustering (Hearst and Pedersen, 1996; Dumais et al., 2001; Lawrie and Croft, 2003). Our work, which is situated in the domain of clinical medicine, lies at the intersection of question answering, information retrieval, and summarization. We employ answer extraction to identify short answers, semantic clustering to group similar results, and extractive summarization to produce supporting evidence. This paper describes how each of these capabilities contributes to an information system tailored to the requirements of physicians. Two separate evaluations demonstrate the effectiveness of our approach. 2 Clinical Information Needs Although the need to answer questions related to</context>
</contexts>
<marker>Lawrie, Croft, 2003</marker>
<rawString>D. Lawrie and W. Croft. 2003. Generating hierarchical summaries for Web searches. In SIGIR 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
</authors>
<title>Evaluation of resources for question answering evaluation.</title>
<date>2005</date>
<booktitle>In SIGIR</booktitle>
<contexts>
<context position="10450" citStr="Lin, 2005" startWordPosition="1575" endWordPosition="1576">actor for interventions relies on MetaMap (Aronson, 2001), a program that automatically identifies entities corresponding to UMLS concepts. UMLS has an extensive coverage of drugs, falling under the semantic type PHARMACOLOGICAL SUBSTANCE and a few others. All such entities are identified as candidates and each is scored based on a number of features: its position in the abstract, its frequency of occurrence, etc. A separate evaluation on a blind test set demonstrates that our extractor is able to accurately recognize the interventions in a MEDLINE abstract; see details in (Demner-Fushman and Lin, 2005; Demner-Fushman and Lin, 2006 in press). 4.2 Semantic Clustering Retrieved MEDLINE citations are organized into semantic clusters based on the main interventions identified in the abstract text. We employed a variant of the hierarchical agglomerative clustering algorithm (Zhao and Karypis, 2002) that utilizes semantic relationships within UMLS to compute similarities between interventions. Iteratively, we group abstracts whose interventions fall under a common ancestor, i.e., a hypernym. The more generic ancestor concept (i.e., the class of drugs) is then used as the cluster label. The proces</context>
<context position="12849" citStr="Lin, 2005" startWordPosition="1957" endWordPosition="1958">isting of three elements: the main intervention (which is usu843 ally more specific than the cluster label); the title of the abstract; and the top-scoring outcome sentence. The “outcome”, another term from evidence-based medicine, asserts the clinical findings of a study, and is typically found towards the end of a MEDLINE abstract. In our case, outcome sentences state the efficacy of a drug in treating a particular disease. Previously, we have built an outcome extractor capable of identifying such sentences in MEDLINE abstracts using supervised machine learning techniques (DemnerFushman and Lin, 2005; Demner-Fushman and Lin, 2006 in press). Evaluation on a blind heldout test set shows high classification accuracy. 5 Evaluation Methodology Given that our work draws from QA, IR, and summarization, a proper evaluation that captures the salient characteristics of our system proved to be quite challenging. Overall, evaluation can be decomposed into two separate components: locating a suitable resource to serve as ground truth and leveraging it to assess system responses. It is not difficult to find disease-specific pharmacology resources. We employed Clinical Evidence (CE), a periodic report c</context>
<context position="14599" citStr="Lin, 2005" startWordPosition="2229" endWordPosition="2230">ffs (i.e., may have adverse side effects), unknown, unlikely beneficial, and harmful. Included with each entry is a list of references—citations consulted by the editors in compiling the resource. Although the completeness of the drugs enumerated in CE is questionable, it nevertheless can be viewed as “authoritative”. 5.1 Previous Work How can we leverage a resource such as CE to assess the responses generated by our system? A survey of evaluation methodologies reveals shortcomings in existing techniques. Answers to factoid questions are automatically scored using regular expression patterns (Lin, 2005). In our application, this is inadequate for many reasons: there is rarely an exact string match between system output and drugs mentioned in CE, primarily due to synonymy (for example, alpha-adrenergic blocking agent and α- blocker refer to the same class of drugs) and ontological mismatch (for example, CE might mention beta-agonists, while a retrieved abstract discusses formoterol, which is a specific representative of beta-agonists). Furthermore, while this evaluation method can tell us if the drugs proposed by the system are “good”, it cannot measure how well the answer is supported by MED</context>
</contexts>
<marker>Lin, 2005</marker>
<rawString>J. Lin. 2005. Evaluation of resources for question answering evaluation. In SIGIR 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lindberg</author>
<author>B Humphreys</author>
<author>A McCray</author>
</authors>
<title>The Unified Medical Language System.</title>
<date>1993</date>
<journal>Methods of Information in Medicine,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="4907" citStr="Lindberg et al., 1993" startWordPosition="709" endWordPosition="712">c prostatitis/chronic pelvic pain syndrome: a randomized, placebo controlled trial. CONCLUSIONS: Terazosin proved superior to placebo for patients with chronic prostatitis/chronic pelvic pain syndrome who had not received alpha-blockers previously. 2. ... Table 1: System response to the question “What is the best drug treatment for chronic prostatitis?” systems for decision support represent a potentially high-impact application. From a research perspective, the clinical domain is attractive because substantial knowledge has already been codified in the Unified Medical Language System (UMLS) (Lindberg et al., 1993). The 2004 version of the UMLS Metathesaurus contains information about over 1 million biomedical concepts and 5 million concept names. This and related resources allow us to explore knowledge-based techniques with substantially less upfront investment. Naturally, physicians have a wide spectrum of information needs, ranging from questions about the selection of treatment options to questions about legal issues. To make the retrieval problem more tractable, we focus on a subset of therapy questions taking the form “What is the best drug treatment for X?”, where X can be any number of diseases.</context>
</contexts>
<marker>Lindberg, Humphreys, McCray, 1993</marker>
<rawString>D. Lindberg, B. Humphreys, and A. McCray. 1993. The Unified Medical Language System. Methods of Information in Medicine, 32(4):281–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>N Elhadad</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Leveraging a common representation for personalized search and summarization in a medical digital library.</title>
<date>2003</date>
<booktitle>In JCDL</booktitle>
<contexts>
<context position="30463" citStr="McKeown et al., 2003" startWordPosition="4725" endWordPosition="4728">ough this assumption is false, we have observed in practice that more-studied drugs are more likely to be beneficial. In contrast with the genomics domain, which has received much attention from both the IR and NLP communities, retrieval systems for the clinical domain represent an underexplored area of research. Although individual components that attempt to operationalize principles of evidencebased medicine do exist (Mendonc¸a and Cimino, 2001; Niu and Hirst, 2004), complete end–toend clinical question answering systems are dif847 ficult to find. Within the context of the PERSIVAL project (McKeown et al., 2003), researchers at Columbia have developed a system that leverages patient records to rerank search results. Since the focus is on personalized summaries, this work can be viewed as complementary to our own. 8 Conclusion The primary contribution of this work is the development of a clinical question answering system that caters to the unique requirements of physicians, who demand both conciseness and completeness. These competing factors can be balanced in a system’s response by providing multiple levels of drill-down that allow the information space to be viewed at different levels of granulari</context>
</contexts>
<marker>McKeown, Elhadad, Hatzivassiloglou, 2003</marker>
<rawString>K. McKeown, N. Elhadad, and V. Hatzivassiloglou. 2003. Leveraging a common representation for personalized search and summarization in a medical digital library. In JCDL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mendonc¸a</author>
<author>J Cimino</author>
</authors>
<title>Building a knowledge base to support a digital library.</title>
<date>2001</date>
<booktitle>In MEDINFO</booktitle>
<marker>Mendonc¸a, Cimino, 2001</marker>
<rawString>E. Mendonc¸a and J. Cimino. 2001. Building a knowledge base to support a digital library. In MEDINFO 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Niu</author>
<author>G Hirst</author>
</authors>
<title>Analysis of semantic classes in medical text for question answering.</title>
<date>2004</date>
<booktitle>In ACL 2004 Workshop on QA in Restricted Domains.</booktitle>
<contexts>
<context position="30314" citStr="Niu and Hirst, 2004" startWordPosition="4700" endWordPosition="4703">e clinical findings. Since the system by default orders the clusters based on size, it implicitly equates “most popular drug” with “best drug”. Although this assumption is false, we have observed in practice that more-studied drugs are more likely to be beneficial. In contrast with the genomics domain, which has received much attention from both the IR and NLP communities, retrieval systems for the clinical domain represent an underexplored area of research. Although individual components that attempt to operationalize principles of evidencebased medicine do exist (Mendonc¸a and Cimino, 2001; Niu and Hirst, 2004), complete end–toend clinical question answering systems are dif847 ficult to find. Within the context of the PERSIVAL project (McKeown et al., 2003), researchers at Columbia have developed a system that leverages patient records to rerank search results. Since the focus is on personalized summaries, this work can be viewed as complementary to our own. 8 Conclusion The primary contribution of this work is the development of a clinical question answering system that caters to the unique requirements of physicians, who demand both conciseness and completeness. These competing factors can be bala</context>
</contexts>
<marker>Niu, Hirst, 2004</marker>
<rawString>Y. Niu and G. Hirst. 2004. Analysis of semantic classes in medical text for question answering. In ACL 2004 Workshop on QA in Restricted Domains.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Sackett</author>
<author>Sharon Straus</author>
<author>W Richardson</author>
<author>William Rosenberg</author>
<author>R Haynes</author>
</authors>
<title>EvidenceBased Medicine: How to Practice and Teach EBM. Churchill Livingstone,</title>
<date>2000</date>
<note>second edition.</note>
<contexts>
<context position="5909" citStr="Sackett et al., 2000" startWordPosition="865" endWordPosition="868">s to questions about legal issues. To make the retrieval problem more tractable, we focus on a subset of therapy questions taking the form “What is the best drug treatment for X?”, where X can be any number of diseases. We have chosen to tackle this class of questions because studies of physicians’ behavior in natural settings have revealed that such questions occur quite frequently (Ely et al., 1999). By leveraging the natural distribution of clinical information needs, we can make the greatest impact with the least effort. Our research follows the principles of evidencebased medicine (EBM) (Sackett et al., 2000), which provides a well-defined model to guide the process of clinical question answering. EBM is a widely-accepted paradigm for medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature, to make decisions about patient care. As shown by previous work (Cogdill and Moore, 1997; De Groote and Dorsch, 2003), citations from the MEDLINE database (maintained by the U.S. National Library of Medicine) serve as a good source of clinical evidence. As a result of these findings, our work focus</context>
</contexts>
<marker>Sackett, Straus, Richardson, Rosenberg, Haynes, 2000</marker>
<rawString>David Sackett, Sharon Straus, W. Richardson, William Rosenberg, and R. Haynes. 2000. EvidenceBased Medicine: How to Practice and Teach EBM. Churchill Livingstone, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Using question series to evaluate question answering system effectiveness.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP</booktitle>
<contexts>
<context position="1734" citStr="Voorhees, 2005" startWordPosition="251" endWordPosition="252">on Complex information needs can rarely be addressed by single documents, but rather require the integration of knowledge from multiple sources. This suggests that modern information retrieval systems, which excel at producing ranked lists of documents sorted by relevance, may not be sufficient to provide users with a good overview of the “information landscape”. Current question answering systems aspire to address this shortcoming by gathering relevant “facts” from multiple documents in response to information needs. The so-called “definition” or “other” questions at recent TREC evaluations (Voorhees, 2005) serve as good examples: “good answers” to these questions include interesting “nuggets” about a particular person, organization, entity, or event. The importance of cross-document information synthesis has not escaped the attention of other researchers. The last few years have seen a convergence between the question answering and summarization communities (Amig´o et al., 2004), as highlighted by the shift from generic to queryfocused summaries in the 2005 DUC evaluation (Dang, 2005). Despite a focus on document ranking, different techniques for organizing search results have been explored by </context>
<context position="15327" citStr="Voorhees, 2005" startWordPosition="2344" endWordPosition="2345">utput and drugs mentioned in CE, primarily due to synonymy (for example, alpha-adrenergic blocking agent and α- blocker refer to the same class of drugs) and ontological mismatch (for example, CE might mention beta-agonists, while a retrieved abstract discusses formoterol, which is a specific representative of beta-agonists). Furthermore, while this evaluation method can tell us if the drugs proposed by the system are “good”, it cannot measure how well the answer is supported by MEDLINE citations; recall that answer justification is important for physicians. The nugget evaluation methodology (Voorhees, 2005) developed for scoring answers to complex questions is not suitable for our task, since there is no coherent notion of an “answer text” that the user reads end–to–end. Furthermore, it is unclear what exactly a “nugget” in this case would be. For similar reasons, methodologies for summarization evaluation are also of little help. Typically, system-generated summaries are either evaluated manually by humans (which is expensive and time-consuming) or automatically using a metric such as ROUGE, which compares system output against a number of reference summaries. The interactive nature of our answ</context>
</contexts>
<marker>Voorhees, 2005</marker>
<rawString>E. Voorhees. 2005. Using question series to evaluate question answering system effectiveness. In HLT/EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhao</author>
<author>G Karypis</author>
</authors>
<title>Evaluation of hierarchical clustering algorithms for document datasets.</title>
<date>2002</date>
<booktitle>In CIKM</booktitle>
<contexts>
<context position="10747" citStr="Zhao and Karypis, 2002" startWordPosition="1615" endWordPosition="1618">tified as candidates and each is scored based on a number of features: its position in the abstract, its frequency of occurrence, etc. A separate evaluation on a blind test set demonstrates that our extractor is able to accurately recognize the interventions in a MEDLINE abstract; see details in (Demner-Fushman and Lin, 2005; Demner-Fushman and Lin, 2006 in press). 4.2 Semantic Clustering Retrieved MEDLINE citations are organized into semantic clusters based on the main interventions identified in the abstract text. We employed a variant of the hierarchical agglomerative clustering algorithm (Zhao and Karypis, 2002) that utilizes semantic relationships within UMLS to compute similarities between interventions. Iteratively, we group abstracts whose interventions fall under a common ancestor, i.e., a hypernym. The more generic ancestor concept (i.e., the class of drugs) is then used as the cluster label. The process repeats until no new clusters can be formed. In order to preserve granularity at the level of practical clinical interest, the tops of the UMLS hierarchy were truncated; for example, the MeSH category “Chemical and Drugs” is too general to be useful. This process was manually performed during s</context>
<context position="16939" citStr="Zhao and Karypis, 2002" startWordPosition="2587" endWordPosition="2590"> employed to examine the effect of categorized search results. However, they often conflate the effectiveness of the interface with that of the underlying algorithms. For example, Dumais et al. (2001) found significant differences in task performance based on different ways of using purely presentational devices such as mouseovers, expandable lists, etc. While interface design is clearly important, it is not the focus of our work. Clustering techniques have also been evaluated in the same manner as text classification algorithms, in terms of precision, recall, etc. based on some ground truth (Zhao and Karypis, 2002). 844 This, however, assumes the existence of stable, invariant categories, which is not the case since our output clusters are query-specific. Although it may be possible to manually create “reference clusters”, we lack sufficient resources to develop such a data set. Furthermore, it is unclear if sufficient interannotator agreement can be obtained to support meaningful evaluation. Ultimately, we devised two separate evaluations to assess the quality of our system output based on the techniques discussed above. The first is a manual evaluation focused on the cluster labels (i.e., drug categor</context>
</contexts>
<marker>Zhao, Karypis, 2002</marker>
<rawString>Y. Zhao and G. Karypis. 2002. Evaluation of hierarchical clustering algorithms for document datasets. In CIKM 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>