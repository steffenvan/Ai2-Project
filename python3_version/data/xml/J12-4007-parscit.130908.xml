<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004458">
<figure confidence="0.943392777777778">
Book Reviews
Discourse Processing
Manfred Stede
University of Potsdam
Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited
by Graeme Hirst, volume 15), 2011, ix+155 pp; paperbound, ISBN 978-1-60845-734-2,
$40.00; ebook, ISBN 978-1-60845-735-9, $30.00 or by subscription
Reviewed by
Bonnie Webber
</figure>
<affiliation confidence="0.518641">
University of Edinburgh
</affiliation>
<bodyText confidence="0.993029727272727">
Discourse is coming in from the cold. After years of being ignored by researchers in
other areas of computational linguistics and language technology, many of these same
researchers are beginning to think that their own work could benefit from treating text
as more than just a bag of sentences. That is, they are beginning to think that discourse
offers some low-hanging fruit—achievable improvements in system performance that
exploit either aspects of text structure or the context that text establishes and uses for
efficient referring and/or predicational expressions.
This new monograph on Discourse Processing by Manfred Stede both reflects this
new zeitgeist and provides an introduction to discourse for researchers in computational
linguistics or language technology with little or no background in the area. This clear
and timely monograph consists of a brief introduction to discourse, a meaty chapter on
each of the three aspects of discourse processing that hold most promise for language
technology, and a brief conclusion on where discourse research might go in the future.
I will go through the three major chapters, and then make some general remarks.
Chapter 2
Chapter 2 addresses two distinct types of large-scale discourse structure: structure that
follows from a text belonging to a particular genre, and structure that follows from
the topic (or topic mix) of a text. The genre of a text affects features such as style
and register. What is relevant here is structure that genre may confer on a text. Stede
suggests that some, but not all, texts inherit large-scale structure from their genre,
calling some unstructured, some structured, and some semi-structured. As a reader, I
did not find this distinction useful, because all text that belongs to a genre seems to
get some large-scale structure from it. On the other hand, all or part of this structure
might simply not be manifest in the kind of lexico-syntactic features that automated
systems regularly rely on for text segmentation. As a case in point, although Stede offers
the text Suffering (used as a running example throughout the book) as an example of
unstructured text, like other instances of Comments in the Talk of the Town section of
the New Yorker magazine, its large-scale structure comprises a “hook” aimed at getting
the reader’s attention, followed by a short essay that concludes with a serious point.
Although ways of attracting a reader’s attention may not have specific lexico-syntactic
features, it might still be possible to recognize the transition between “hook” and essay,
and essay structure itself is what ETS’s eRater system (Burstein and Chodorow 2010)
aims to recognize and evaluate.
</bodyText>
<footnote confidence="0.35272">
© 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 4
</footnote>
<bodyText confidence="0.991569522727273">
This first half of Chapter 2 focuses on the genre-based structure of scientific texts
and of film reviews. Here researchers have already shown that language technologies
such as information extraction and sentiment analysis benefit from taking such structure
into account, so this is entirely appropriate for the book’s target audience. More on
genre-based functional structure and its use in producing structured biomedical
abstracts can be found in the recent survey of research on discourse structure and
language technology by Webber, Egg, and Kordoni (2012).
The second half of Chapter 2 discusses large-scale discourse structure associated
with patterns of topics. Such structure is often found in expository writing such as
encyclopedia articles and travel pieces. Here, changing patterns of content words corre-
late well with changes in topic, rendering them useful for the many approaches to text
segmentation that are well-described in this half of the chapter. Because the discussion
here of probabilistic models for topic segmentation is rather short, the reader who wants
to know more should consult the excellent survey of topic segmentation methods by
Purver (2011).
Chapter 3
Chapter 3, entitled Coreference Resolution, addresses more than this, dealing with the
resolution of other expressions whose reduction is licensed by the discourse context,
such as bridging reference and “other” reference, which Halliday and Hasan (1976)
call comparative reference because it occurs with comparative forms such as “larger
fish” and “a more impressive poodle,” as well as with “other,” “another,” and “such.”
Stede justifies inclusion of this chapter for two reasons—the close connection between
coreference resolution and topic segmentation and the benefits to text analysis provided
by having its pronouns resolved. But another reason must be the link mentioned earlier
between text and context: Discourse creates the context in which context-reduced
expressions make sense, so it falls naturally within the tasks of discourse processing
to resolve them, either through modeling context explicitly or through the use of
proxies.
The chapter starts with an overview of coreference and anaphora that covers both
their forms and their functions. This is followed by an important section on corpus
annotation (Section 3.2), included because (as Stede notes) what has been annotated and
why it has been annotated strongly determines what expressions are resolved and how.
This section identifies many of the problems in coreference annotation that have been
raised in the literature, but recognizes that research has to make use of the resources
that exist and not just the resources it wants. Several of these are indicated at the end
of the section, reminding one that it would have been useful to have some pointers in
Chapter 2 to corpora available for genre-based segmentation (such as Liakata’s ART
corpus)1 or for topic-based segmentation.
Stede then links the current chapter to the previous one through a discussion of
entity-based coherence (Section 3.3) and then discusses how to identify when a pronoun
or definite noun phrase should be treated as anaphoric (Section 3.4) as groundwork
for discussion of anaphora resolution (Sections 3.5–3.7). Missing from the discussion of
detecting non-anaphoric (pleonastic) pronouns is mention of Bergsma’s recent system
NADA for doing this (Bergsma and Yarowsky 2011).2
</bodyText>
<footnote confidence="0.9927235">
1 Downloadable from http://www.aber.ac.uk/en/ns/research/cb/projects/art/art-corpus/
2 Downloadable from http://code.google.com/p/nada-nonref-pronoun-detector/
</footnote>
<page confidence="0.981341">
918
</page>
<subsectionHeader confidence="0.893341">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.995072625">
The discussion of anaphora resolution covers rule-based approaches to resolving
nominal anaphora (Section 3.5) and then supervised machine learning methods for
anaphora resolution (Section 3.6). The latter follows the structure (albeit not the con-
tent) of Ng’s survey (2010), in discussing mention-pair models, and then entity-mention
models. Whereas Ng then discusses ranking models, including his cluster ranker (Rahman
and Ng 2009), which is conceptually similar to the Lappin and Leass (1994) approach
described in Section 3.5, Stede discusses a range of more recent models, most of which
are subsequent to Ng’s survey.
Section 3.8 surveys methods evaluating coreference resolution and some of the
known problems in doing so. A good complement to this is Byron’s too-little-known
discussion of problems in the consistent reporting of such results (Byron 2001). Chap-
ter 3 concludes with a section on Recent Trends, which would also have been useful in
Chapter 2.
Chapter 4
The fourth and longest chapter deals with semantic or pragmatically oriented coherence
relations that hold between adjacent text spans or discourse units. Whereas the previous
two chapters were essentially theory-neutral, the presentation in Chapter 4 largely
reflects the perspective of Rhetorical Structure Theory (Mann and Thompson 1988). RST
takes a text to be a sequence of elementary discourse units that comprise the leaves of
a tree structure of coherence relations between recursively defined discourse units. RST
also assumes that one of the arguments to a coherence relation may be more important
to the speaker’s purpose than the other, calling the former the nucleus and the latter,
the satellite.
This RST framework dictates the structure of the chapter: Following an introductory
section that explains and motivates coherence relations, each subsequent section consid-
ers the next task in an RST analysis—segmenting a text into elementary discourse units
(Section 4.2), recognizing which (adjacent) units stand in a coherence relation and what
(single) relation holds between them (Section 4.3), and finally, inducing the overall tree
structure of coherence relations that hold between recursively defined discourse units
(Section 4.4). All these tasks are well described, both from a theoretical perspective and
in terms of automated procedures for carrying them out. Coverage of relevant work is
very high.
Where the reader may get confused, however, is that a good proportion of the
more recent work on identifying coherence relations does not fall within the framework
of RST, and thus doesn’t adhere to several of its assumptions—in particular, that a
text is divisible into a covering sequence of elementary discourse units, that only one
relation can hold between discourse units, that the arguments to a coherence relation
must be adjacent, that one argument to a coherence relation may intrinsically convey
information that is more important to the speaker’s purpose than the other, and that
coherence relations impose an overall tree structure on a text in terms of recursively
defined discourse units.
Although Chapter 4 discusses the Penn Discourse TreeBank (Prasad et al. 2008) and
its “somewhat modest annotations” (page 126), the discussion is framed in terms of
RST tasks, whereas the assumptions underlying the Penn Discourse TreeBank reflect its
concerns with a quite different set of tasks involved in recognizing coherence relations.
The first task requires finding evidence for a coherence relation (in the form of a
discourse connective such as a coordinating or subordinating conjunction or a discourse
adverbial, or in the form of sentence adjacency) and then determining (1) if the evidence
</bodyText>
<page confidence="0.993828">
919
</page>
<note confidence="0.490173">
Computational Linguistics Volume 38, Number 4
</note>
<bodyText confidence="0.999304444444445">
does indeed signal a coherence relation, given that evidence is often ambiguous; (2) if
it does, what constitutes its arguments; and (3) what is its sense. Although Chapter 4
covers some of this work (Dinesh et al. 2005; Wellner and Pustejovsky 2007; Elwell
and Baldridge 2008; Pitler and Nenkova 2009; Prasad, Joshi, and Webber 2010), its
appearance within the context of a discussion of RST-tasks may lead to some confusion.
Chapter 4 concludes with a brief discussion of some important open issues regard-
ing coherence relations, including problems with associating a large text span with
a single recursive structure of coherence relations and problems with inter-annotator
agreement.
</bodyText>
<sectionHeader confidence="0.652784" genericHeader="abstract">
Summary
</sectionHeader>
<bodyText confidence="0.9999673125">
For its intended audience, this monograph will serve as a compact, readable intro-
duction to the subject of discourse processing. The relevant phenomena are presented
clearly, as are many of the computational methods for dealing with them. What readers
won’t get is criteria for choosing among the methods or an understanding of what each
method is good for. This problem may reflect the absence of comparable performance
results and useful error analyses in the original publications, however.
Also missing from the monograph is discussion of applications of discourse pro-
cessing, and pointers to more of the resources available to researchers interested in
discourse structure. This is where the additional resources I have mentioned may prove
complementary.
Finally, a plea to the series editor: Monographs such as this one really need an index.
Some monographs in the series have one, whereas others (like this one) don’t. Because
the series appears in both electronic and physical format, one could excuse the former
not having an explicit index, since in most cases, one can get away with the basic search
facility in the Adobe Reader. Nothing similar is available for the nicely sized physical
monographs. Their authors should be strongly encouraged to provide them.
</bodyText>
<sectionHeader confidence="0.987764" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.975709638297872">
Bergsma, Shane and David Yarowsky. 2011.
Nada: A robust system for non-referential
pronoun detection. In Proceedings of
DAARC,12 pages, Faro.
Burstein, Jill and Martin Chodorow. 2010.
Progress and new directions in technology
for automated essay evaluation.
In R. Kaplan, editor, The Oxford Handbook
of Applied Linguistics. Oxford University
Press, 2nd edition, pages 487–497.
Byron, Donna. 2001. The uncommon
denominator: A proposal for consistent
reporting of pronoun resolution results.
Computational Linguistics, 27(4):569–577.
Dinesh, Nikhil, Alan Lee, Eleni Miltsakaki,
Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2005. Attribution
and the (non-)alignment of syntactic
and discourse arguments of connectives.
In ACL Workshop on Frontiers in Corpus
Annotation, pages 29–36, Ann Arbor, MI.
Elwell, Robert and Jason Baldridge.
2008. Discourse connective argument
identification with connective specific
rankers. In Proceedings of the IEEE
Conference on Semantic Computing,
8 pages, Santa Clara, CA.
Halliday, Michael and Ruqaiya Hasan.
1976. Cohesion in English. Longman.
Lappin, Shalom and Herbert Leass. 1994.
An algorithm for pronominal anaphora
resolution. Computational Linguistics,
20(4):535–561.
Mann, William and Sandra Thompson.
1988. Rhetorical structure theory: Toward
a functional theory of text organization.
Text, 8(3):243–281.
Ng, Vincent. 2010. Supervised noun
phrase coreference research: The
first 15 years. In Proceedings of the
48th Annual Meeting of the Association
for Computational Linguistics,
pages 1396–1411, Uppsala.
Pitler, Emily and Ani Nenkova. 2009. Using
syntax to disambiguate explicit discourse
connectives in text. In ACL-IJCNLP ’09:
Proceedings of the 47th Meeting of the
</reference>
<page confidence="0.98587">
920
</page>
<reference confidence="0.964582624999999">
Book Reviews
Association for Computational Linguistics
and the 4th International Joint Conference on
Natural Language Processing, pages 13–16,
Singapore.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, Livio Robaldo, Aravind
Joshi, and Bonnie Webber. 2008. The Penn
Discourse TreeBank 2.0. In Proceedings,
6th International Conference on Language
Resources and Evaluation, pages 2961–2968,
Marrakech.
Prasad, Rashmi, Aravind Joshi, and Bonnie
Webber. 2010. Exploiting scope for shallow
discourse parsing. In Proceedings of the
7th International Conference on Language
Resources and Evaluation (LREC 2010),
pages 2076–2083, Malta.
Purver, Matthew. 2011. Topic segmentation.
In Gokhan Tur and Renato de Mori,
editors, Spoken Language Understanding:
This book review was edited by Pierre Isabelle.
Systems for Extracting Semantic
Information from Speech, Chapter 11.
Wiley, Hoboken, NJ.
Rahman, Altaf and Vincent Ng. 2009.
Supervised models for coreference
resolution. In Proceedings of the 2009
Conference on Empirical Methods in
Natural Language Processing,
pages 968–977, Singapore.
Webber, Bonnie, Markus Egg, and
Valia Kordoni. 2012. Discourse
structure and language technology.
Natural Language Engineering,
doi:10.1017/S1351324911000337.
Wellner, Ben and James Pustejovsky. 2007.
Automatically identifying the arguments
to discourse connectives. In Proceedings of
the 2007 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 92–101, Prague.
Bonnie Webber is a Professor of Informatics at Edinburgh University. She received both her MSc
and PhD from Harvard University. She is a Fellow of the Royal Society of Edinburgh and of
the American Association for Artificial Intelligence. Both her early and her recent research have
focused on computational approaches to discourse and question answering. In between, she
has carried out research on animation from instructions, medical decision support systems, and
biomedical text processing. Webber’s e-mail address is bonnie.webber@ed.ac.uk.
</reference>
<page confidence="0.997539">
921
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.087924">
<title confidence="0.996528">Book Reviews Discourse Processing</title>
<author confidence="0.998363">Manfred Stede</author>
<affiliation confidence="0.987344">University of Potsdam</affiliation>
<note confidence="0.511228">Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by Graeme Hirst, volume 15), 2011, ix+155 pp; paperbound, ISBN 978-1-60845-734-2, $40.00; ebook, ISBN 978-1-60845-735-9, $30.00 or by subscription Reviewed by</note>
<author confidence="0.997606">Bonnie Webber</author>
<affiliation confidence="0.9965">University of Edinburgh</affiliation>
<abstract confidence="0.992648714285714">Discourse is coming in from the cold. After years of being ignored by researchers in other areas of computational linguistics and language technology, many of these same researchers are beginning to think that their own work could benefit from treating text as more than just a bag of sentences. That is, they are beginning to think that discourse offers some low-hanging fruit—achievable improvements in system performance that exploit either aspects of text structure or the context that text establishes and uses for efficient referring and/or predicational expressions.</abstract>
<intro confidence="0.784706">new monograph on Processing Manfred Stede both reflects this</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>David Yarowsky</author>
</authors>
<title>Nada: A robust system for non-referential pronoun detection.</title>
<date>2011</date>
<booktitle>In Proceedings of DAARC,12</booktitle>
<pages>pages,</pages>
<location>Faro.</location>
<contexts>
<context position="6556" citStr="Bergsma and Yarowsky 2011" startWordPosition="1001" endWordPosition="1004">eful to have some pointers in Chapter 2 to corpora available for genre-based segmentation (such as Liakata’s ART corpus)1 or for topic-based segmentation. Stede then links the current chapter to the previous one through a discussion of entity-based coherence (Section 3.3) and then discusses how to identify when a pronoun or definite noun phrase should be treated as anaphoric (Section 3.4) as groundwork for discussion of anaphora resolution (Sections 3.5–3.7). Missing from the discussion of detecting non-anaphoric (pleonastic) pronouns is mention of Bergsma’s recent system NADA for doing this (Bergsma and Yarowsky 2011).2 1 Downloadable from http://www.aber.ac.uk/en/ns/research/cb/projects/art/art-corpus/ 2 Downloadable from http://code.google.com/p/nada-nonref-pronoun-detector/ 918 Book Reviews The discussion of anaphora resolution covers rule-based approaches to resolving nominal anaphora (Section 3.5) and then supervised machine learning methods for anaphora resolution (Section 3.6). The latter follows the structure (albeit not the content) of Ng’s survey (2010), in discussing mention-pair models, and then entity-mention models. Whereas Ng then discusses ranking models, including his cluster ranker (Rahma</context>
</contexts>
<marker>Bergsma, Yarowsky, 2011</marker>
<rawString>Bergsma, Shane and David Yarowsky. 2011. Nada: A robust system for non-referential pronoun detection. In Proceedings of DAARC,12 pages, Faro.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
<author>Martin Chodorow</author>
</authors>
<title>Progress and new directions in technology for automated essay evaluation.</title>
<date>2010</date>
<contexts>
<context position="3006" citStr="Burstein and Chodorow 2010" startWordPosition="465" endWordPosition="468">though Stede offers the text Suffering (used as a running example throughout the book) as an example of unstructured text, like other instances of Comments in the Talk of the Town section of the New Yorker magazine, its large-scale structure comprises a “hook” aimed at getting the reader’s attention, followed by a short essay that concludes with a serious point. Although ways of attracting a reader’s attention may not have specific lexico-syntactic features, it might still be possible to recognize the transition between “hook” and essay, and essay structure itself is what ETS’s eRater system (Burstein and Chodorow 2010) aims to recognize and evaluate. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 4 This first half of Chapter 2 focuses on the genre-based structure of scientific texts and of film reviews. Here researchers have already shown that language technologies such as information extraction and sentiment analysis benefit from taking such structure into account, so this is entirely appropriate for the book’s target audience. More on genre-based functional structure and its use in producing structured biomedical abstracts can be found in the recent survey of </context>
</contexts>
<marker>Burstein, Chodorow, 2010</marker>
<rawString>Burstein, Jill and Martin Chodorow. 2010. Progress and new directions in technology for automated essay evaluation.</rawString>
</citation>
<citation valid="false">
<booktitle>The Oxford Handbook of Applied Linguistics.</booktitle>
<pages>487--497</pages>
<editor>In R. Kaplan, editor,</editor>
<publisher>Oxford University Press,</publisher>
<note>2nd edition,</note>
<marker></marker>
<rawString>In R. Kaplan, editor, The Oxford Handbook of Applied Linguistics. Oxford University Press, 2nd edition, pages 487–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Byron</author>
</authors>
<title>The uncommon denominator: A proposal for consistent reporting of pronoun resolution results.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="7599" citStr="Byron 2001" startWordPosition="1142" endWordPosition="1143"> of Ng’s survey (2010), in discussing mention-pair models, and then entity-mention models. Whereas Ng then discusses ranking models, including his cluster ranker (Rahman and Ng 2009), which is conceptually similar to the Lappin and Leass (1994) approach described in Section 3.5, Stede discusses a range of more recent models, most of which are subsequent to Ng’s survey. Section 3.8 surveys methods evaluating coreference resolution and some of the known problems in doing so. A good complement to this is Byron’s too-little-known discussion of problems in the consistent reporting of such results (Byron 2001). Chapter 3 concludes with a section on Recent Trends, which would also have been useful in Chapter 2. Chapter 4 The fourth and longest chapter deals with semantic or pragmatically oriented coherence relations that hold between adjacent text spans or discourse units. Whereas the previous two chapters were essentially theory-neutral, the presentation in Chapter 4 largely reflects the perspective of Rhetorical Structure Theory (Mann and Thompson 1988). RST takes a text to be a sequence of elementary discourse units that comprise the leaves of a tree structure of coherence relations between recur</context>
</contexts>
<marker>Byron, 2001</marker>
<rawString>Byron, Donna. 2001. The uncommon denominator: A proposal for consistent reporting of pronoun resolution results. Computational Linguistics, 27(4):569–577.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>Attribution and the (non-)alignment of syntactic and discourse arguments of connectives.</title>
<date>2005</date>
<booktitle>In ACL Workshop on Frontiers in Corpus Annotation,</booktitle>
<pages>29--36</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="10711" citStr="Dinesh et al. 2005" startWordPosition="1623" endWordPosition="1626">cerns with a quite different set of tasks involved in recognizing coherence relations. The first task requires finding evidence for a coherence relation (in the form of a discourse connective such as a coordinating or subordinating conjunction or a discourse adverbial, or in the form of sentence adjacency) and then determining (1) if the evidence 919 Computational Linguistics Volume 38, Number 4 does indeed signal a coherence relation, given that evidence is often ambiguous; (2) if it does, what constitutes its arguments; and (3) what is its sense. Although Chapter 4 covers some of this work (Dinesh et al. 2005; Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler and Nenkova 2009; Prasad, Joshi, and Webber 2010), its appearance within the context of a discussion of RST-tasks may lead to some confusion. Chapter 4 concludes with a brief discussion of some important open issues regarding coherence relations, including problems with associating a large text span with a single recursive structure of coherence relations and problems with inter-annotator agreement. Summary For its intended audience, this monograph will serve as a compact, readable introduction to the subject of discourse proces</context>
</contexts>
<marker>Dinesh, Lee, Miltsakaki, Prasad, Joshi, Webber, 2005</marker>
<rawString>Dinesh, Nikhil, Alan Lee, Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2005. Attribution and the (non-)alignment of syntactic and discourse arguments of connectives. In ACL Workshop on Frontiers in Corpus Annotation, pages 29–36, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Elwell</author>
<author>Jason Baldridge</author>
</authors>
<title>Discourse connective argument identification with connective specific rankers.</title>
<date>2008</date>
<booktitle>In Proceedings of the IEEE Conference on Semantic Computing,</booktitle>
<volume>8</volume>
<pages>pages,</pages>
<location>Santa Clara, CA.</location>
<contexts>
<context position="10768" citStr="Elwell and Baldridge 2008" startWordPosition="1631" endWordPosition="1634"> in recognizing coherence relations. The first task requires finding evidence for a coherence relation (in the form of a discourse connective such as a coordinating or subordinating conjunction or a discourse adverbial, or in the form of sentence adjacency) and then determining (1) if the evidence 919 Computational Linguistics Volume 38, Number 4 does indeed signal a coherence relation, given that evidence is often ambiguous; (2) if it does, what constitutes its arguments; and (3) what is its sense. Although Chapter 4 covers some of this work (Dinesh et al. 2005; Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler and Nenkova 2009; Prasad, Joshi, and Webber 2010), its appearance within the context of a discussion of RST-tasks may lead to some confusion. Chapter 4 concludes with a brief discussion of some important open issues regarding coherence relations, including problems with associating a large text span with a single recursive structure of coherence relations and problems with inter-annotator agreement. Summary For its intended audience, this monograph will serve as a compact, readable introduction to the subject of discourse processing. The relevant phenomena are presented clearly, as ar</context>
</contexts>
<marker>Elwell, Baldridge, 2008</marker>
<rawString>Elwell, Robert and Jason Baldridge. 2008. Discourse connective argument identification with connective specific rankers. In Proceedings of the IEEE Conference on Semantic Computing, 8 pages, Santa Clara, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<note>Cohesion in English. Longman.</note>
<contexts>
<context position="4579" citStr="Halliday and Hasan (1976)" startWordPosition="698" endWordPosition="701">te well with changes in topic, rendering them useful for the many approaches to text segmentation that are well-described in this half of the chapter. Because the discussion here of probabilistic models for topic segmentation is rather short, the reader who wants to know more should consult the excellent survey of topic segmentation methods by Purver (2011). Chapter 3 Chapter 3, entitled Coreference Resolution, addresses more than this, dealing with the resolution of other expressions whose reduction is licensed by the discourse context, such as bridging reference and “other” reference, which Halliday and Hasan (1976) call comparative reference because it occurs with comparative forms such as “larger fish” and “a more impressive poodle,” as well as with “other,” “another,” and “such.” Stede justifies inclusion of this chapter for two reasons—the close connection between coreference resolution and topic segmentation and the benefits to text analysis provided by having its pronouns resolved. But another reason must be the link mentioned earlier between text and context: Discourse creates the context in which context-reduced expressions make sense, so it falls naturally within the tasks of discourse processin</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday, Michael and Ruqaiya Hasan. 1976. Cohesion in English. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
<author>Herbert Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="7232" citStr="Lappin and Leass (1994)" startWordPosition="1083" endWordPosition="1086">search/cb/projects/art/art-corpus/ 2 Downloadable from http://code.google.com/p/nada-nonref-pronoun-detector/ 918 Book Reviews The discussion of anaphora resolution covers rule-based approaches to resolving nominal anaphora (Section 3.5) and then supervised machine learning methods for anaphora resolution (Section 3.6). The latter follows the structure (albeit not the content) of Ng’s survey (2010), in discussing mention-pair models, and then entity-mention models. Whereas Ng then discusses ranking models, including his cluster ranker (Rahman and Ng 2009), which is conceptually similar to the Lappin and Leass (1994) approach described in Section 3.5, Stede discusses a range of more recent models, most of which are subsequent to Ng’s survey. Section 3.8 surveys methods evaluating coreference resolution and some of the known problems in doing so. A good complement to this is Byron’s too-little-known discussion of problems in the consistent reporting of such results (Byron 2001). Chapter 3 concludes with a section on Recent Trends, which would also have been useful in Chapter 2. Chapter 4 The fourth and longest chapter deals with semantic or pragmatically oriented coherence relations that hold between adjac</context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>Lappin, Shalom and Herbert Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Mann</author>
<author>Sandra Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="8052" citStr="Mann and Thompson 1988" startWordPosition="1208" endWordPosition="1211">e of the known problems in doing so. A good complement to this is Byron’s too-little-known discussion of problems in the consistent reporting of such results (Byron 2001). Chapter 3 concludes with a section on Recent Trends, which would also have been useful in Chapter 2. Chapter 4 The fourth and longest chapter deals with semantic or pragmatically oriented coherence relations that hold between adjacent text spans or discourse units. Whereas the previous two chapters were essentially theory-neutral, the presentation in Chapter 4 largely reflects the perspective of Rhetorical Structure Theory (Mann and Thompson 1988). RST takes a text to be a sequence of elementary discourse units that comprise the leaves of a tree structure of coherence relations between recursively defined discourse units. RST also assumes that one of the arguments to a coherence relation may be more important to the speaker’s purpose than the other, calling the former the nucleus and the latter, the satellite. This RST framework dictates the structure of the chapter: Following an introductory section that explains and motivates coherence relations, each subsequent section considers the next task in an RST analysis—segmenting a text int</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>Mann, William and Sandra Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Supervised noun phrase coreference research: The first 15 years.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1396--1411</pages>
<location>Uppsala.</location>
<marker>Ng, 2010</marker>
<rawString>Ng, Vincent. 2010. Supervised noun phrase coreference research: The first 15 years. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1396–1411, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Using syntax to disambiguate explicit discourse connectives in text.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP ’09: Proceedings of the 47th Meeting of the Book Reviews</booktitle>
<contexts>
<context position="10793" citStr="Pitler and Nenkova 2009" startWordPosition="1635" endWordPosition="1638">elations. The first task requires finding evidence for a coherence relation (in the form of a discourse connective such as a coordinating or subordinating conjunction or a discourse adverbial, or in the form of sentence adjacency) and then determining (1) if the evidence 919 Computational Linguistics Volume 38, Number 4 does indeed signal a coherence relation, given that evidence is often ambiguous; (2) if it does, what constitutes its arguments; and (3) what is its sense. Although Chapter 4 covers some of this work (Dinesh et al. 2005; Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler and Nenkova 2009; Prasad, Joshi, and Webber 2010), its appearance within the context of a discussion of RST-tasks may lead to some confusion. Chapter 4 concludes with a brief discussion of some important open issues regarding coherence relations, including problems with associating a large text span with a single recursive structure of coherence relations and problems with inter-annotator agreement. Summary For its intended audience, this monograph will serve as a compact, readable introduction to the subject of discourse processing. The relevant phenomena are presented clearly, as are many of the computation</context>
</contexts>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>Pitler, Emily and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In ACL-IJCNLP ’09: Proceedings of the 47th Meeting of the Book Reviews</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing,</booktitle>
<pages>13--16</pages>
<marker></marker>
<rawString>Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing, pages 13–16, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0. In</title>
<date>2008</date>
<booktitle>Proceedings, 6th International Conference on Language Resources and Evaluation,</booktitle>
<pages>2961--2968</pages>
<location>Marrakech.</location>
<contexts>
<context position="9916" citStr="Prasad et al. 2008" startWordPosition="1497" endWordPosition="1500">k of RST, and thus doesn’t adhere to several of its assumptions—in particular, that a text is divisible into a covering sequence of elementary discourse units, that only one relation can hold between discourse units, that the arguments to a coherence relation must be adjacent, that one argument to a coherence relation may intrinsically convey information that is more important to the speaker’s purpose than the other, and that coherence relations impose an overall tree structure on a text in terms of recursively defined discourse units. Although Chapter 4 discusses the Penn Discourse TreeBank (Prasad et al. 2008) and its “somewhat modest annotations” (page 126), the discussion is framed in terms of RST tasks, whereas the assumptions underlying the Penn Discourse TreeBank reflect its concerns with a quite different set of tasks involved in recognizing coherence relations. The first task requires finding evidence for a coherence relation (in the form of a discourse connective such as a coordinating or subordinating conjunction or a discourse adverbial, or in the form of sentence adjacency) and then determining (1) if the evidence 919 Computational Linguistics Volume 38, Number 4 does indeed signal a coh</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Prasad, Rashmi, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0. In Proceedings, 6th International Conference on Language Resources and Evaluation, pages 2961–2968, Marrakech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>Exploiting scope for shallow discourse parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC 2010),</booktitle>
<pages>2076--2083</pages>
<marker>Prasad, Joshi, Webber, 2010</marker>
<rawString>Prasad, Rashmi, Aravind Joshi, and Bonnie Webber. 2010. Exploiting scope for shallow discourse parsing. In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC 2010), pages 2076–2083, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
</authors>
<title>Topic segmentation. In Gokhan Tur and Renato de Mori, editors, Spoken Language Understanding: This book review was edited by Pierre Isabelle.</title>
<date>2011</date>
<booktitle>Systems for Extracting Semantic Information from Speech, Chapter 11.</booktitle>
<publisher>Wiley,</publisher>
<location>Hoboken, NJ.</location>
<contexts>
<context position="4313" citStr="Purver (2011)" startWordPosition="662" endWordPosition="663">e second half of Chapter 2 discusses large-scale discourse structure associated with patterns of topics. Such structure is often found in expository writing such as encyclopedia articles and travel pieces. Here, changing patterns of content words correlate well with changes in topic, rendering them useful for the many approaches to text segmentation that are well-described in this half of the chapter. Because the discussion here of probabilistic models for topic segmentation is rather short, the reader who wants to know more should consult the excellent survey of topic segmentation methods by Purver (2011). Chapter 3 Chapter 3, entitled Coreference Resolution, addresses more than this, dealing with the resolution of other expressions whose reduction is licensed by the discourse context, such as bridging reference and “other” reference, which Halliday and Hasan (1976) call comparative reference because it occurs with comparative forms such as “larger fish” and “a more impressive poodle,” as well as with “other,” “another,” and “such.” Stede justifies inclusion of this chapter for two reasons—the close connection between coreference resolution and topic segmentation and the benefits to text analy</context>
</contexts>
<marker>Purver, 2011</marker>
<rawString>Purver, Matthew. 2011. Topic segmentation. In Gokhan Tur and Renato de Mori, editors, Spoken Language Understanding: This book review was edited by Pierre Isabelle. Systems for Extracting Semantic Information from Speech, Chapter 11. Wiley, Hoboken, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Supervised models for coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>968--977</pages>
<contexts>
<context position="7170" citStr="Rahman and Ng 2009" startWordPosition="1073" endWordPosition="1076">2011).2 1 Downloadable from http://www.aber.ac.uk/en/ns/research/cb/projects/art/art-corpus/ 2 Downloadable from http://code.google.com/p/nada-nonref-pronoun-detector/ 918 Book Reviews The discussion of anaphora resolution covers rule-based approaches to resolving nominal anaphora (Section 3.5) and then supervised machine learning methods for anaphora resolution (Section 3.6). The latter follows the structure (albeit not the content) of Ng’s survey (2010), in discussing mention-pair models, and then entity-mention models. Whereas Ng then discusses ranking models, including his cluster ranker (Rahman and Ng 2009), which is conceptually similar to the Lappin and Leass (1994) approach described in Section 3.5, Stede discusses a range of more recent models, most of which are subsequent to Ng’s survey. Section 3.8 surveys methods evaluating coreference resolution and some of the known problems in doing so. A good complement to this is Byron’s too-little-known discussion of problems in the consistent reporting of such results (Byron 2001). Chapter 3 concludes with a section on Recent Trends, which would also have been useful in Chapter 2. Chapter 4 The fourth and longest chapter deals with semantic or prag</context>
</contexts>
<marker>Rahman, Ng, 2009</marker>
<rawString>Rahman, Altaf and Vincent Ng. 2009. Supervised models for coreference resolution. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 968–977, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Markus Egg</author>
<author>Valia Kordoni</author>
</authors>
<date>2012</date>
<booktitle>Discourse structure and language technology. Natural Language Engineering, doi:10.1017/S1351324911000337.</booktitle>
<marker>Webber, Egg, Kordoni, 2012</marker>
<rawString>Webber, Bonnie, Markus Egg, and Valia Kordoni. 2012. Discourse structure and language technology. Natural Language Engineering, doi:10.1017/S1351324911000337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Wellner</author>
<author>James Pustejovsky</author>
</authors>
<title>Automatically identifying the arguments to discourse connectives.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>92--101</pages>
<location>Prague.</location>
<contexts>
<context position="10741" citStr="Wellner and Pustejovsky 2007" startWordPosition="1627" endWordPosition="1630">ifferent set of tasks involved in recognizing coherence relations. The first task requires finding evidence for a coherence relation (in the form of a discourse connective such as a coordinating or subordinating conjunction or a discourse adverbial, or in the form of sentence adjacency) and then determining (1) if the evidence 919 Computational Linguistics Volume 38, Number 4 does indeed signal a coherence relation, given that evidence is often ambiguous; (2) if it does, what constitutes its arguments; and (3) what is its sense. Although Chapter 4 covers some of this work (Dinesh et al. 2005; Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler and Nenkova 2009; Prasad, Joshi, and Webber 2010), its appearance within the context of a discussion of RST-tasks may lead to some confusion. Chapter 4 concludes with a brief discussion of some important open issues regarding coherence relations, including problems with associating a large text span with a single recursive structure of coherence relations and problems with inter-annotator agreement. Summary For its intended audience, this monograph will serve as a compact, readable introduction to the subject of discourse processing. The relevant phenomena a</context>
</contexts>
<marker>Wellner, Pustejovsky, 2007</marker>
<rawString>Wellner, Ben and James Pustejovsky. 2007. Automatically identifying the arguments to discourse connectives. In Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 92–101, Prague.</rawString>
</citation>
<citation valid="false">
<title>Bonnie Webber is a Professor of Informatics at Edinburgh University. She received both her MSc and PhD from Harvard University. She is a Fellow of the Royal Society of Edinburgh and of the American Association for Artificial Intelligence. Both her early and her recent research have focused on computational approaches to discourse and question answering. In between, she has carried out research on animation from instructions, medical decision support systems, and biomedical text processing. Webber’s e-mail address is bonnie.webber@ed.ac.uk.</title>
<marker></marker>
<rawString>Bonnie Webber is a Professor of Informatics at Edinburgh University. She received both her MSc and PhD from Harvard University. She is a Fellow of the Royal Society of Edinburgh and of the American Association for Artificial Intelligence. Both her early and her recent research have focused on computational approaches to discourse and question answering. In between, she has carried out research on animation from instructions, medical decision support systems, and biomedical text processing. Webber’s e-mail address is bonnie.webber@ed.ac.uk.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>