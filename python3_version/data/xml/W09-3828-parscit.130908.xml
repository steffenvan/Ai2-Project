<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.968879">
Effective Analysis of Causes and Inter-dependencies of Parsing Errors
</title>
<author confidence="0.999809">
Tadayoshi Hara1 Yusuke Miyao1 Jun’ichi Tsujii1,2,3
</author>
<affiliation confidence="0.959314">
1Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, JAPAN
2School of Computer Science, University of Manchester
</affiliation>
<address confidence="0.594708">
3NaCTeM (National Center for Text Mining)
</address>
<email confidence="0.995745">
{harasan,yusuke,tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.982817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981076923077">
In this paper, we propose two methods for
analyzing errors in parsing. One is to clas-
sify errors into categories which grammar
developers can easily associate with de-
fects in grammar or a parsing model and
thus its improvement. The other is to
discover inter-dependencies among errors,
and thus grammar developers can focus on
errors which are crucial for improving the
performance of a parsing model.
The first method uses patterns of er-
rors to associate them with categories of
causes for those errors, such as errors in
scope determination of coordination, PP-
attachment, identification of antecedent of
relative clauses, etc. On the other hand,
the second method, which is based on re-
parsing with one of observed errors cor-
rected, assesses inter-dependencies among
errors by examining which other errors
were to be corrected as a result if a spe-
cific error was corrected.
Experiments show that these two meth-
ods are complementary and by being com-
bined, they can provide useful clues as to
how to improve a given grammar.
</bodyText>
<sectionHeader confidence="0.99517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999810519230769">
In any kind of complex systems, analyzing causes
of errors is a crucial step for improving its perfor-
mance. In recent sophisticated parsing technolo-
gies, the step of error analysis has been becoming
more and more convoluted and time-consuming,
if not impossible. While common performance
evaluation measures such as F-values are useful to
compare the performance of systems or evaluate
improvement of a system, they hardly give useful
clues as to how to improve a system. Evaluation
measures usually assume uniform units such as the
number of correctly or incorrectly recognized con-
stituent boundaries and their labels, or in a similar
vein, dependency links among words and their la-
bels, and then compute single values such as the F-
value. These values do not give any insights as to
where the weaknesses exist in a parsing model. As
a result, the improvement process takes the form
of time consuming trial-error cycles.
Once grammar developers know the actual dis-
tribution of errors across different categories such
as PP-attachment, complement/adjunct distinc-
tion, gerund/participle distinction, etc., they can
think of focused and systematic improvement of
a parsing model.
Another problem of the F-value in terms of
uniform units is that it does not take inter-
dependencies among errors into consideration. In
particular, for parsers based on grammar for-
malisms such as LFG (Kaplan and Bresnan, 1995),
HPSG (Pollard and Sag, 1994), or CCG (Steed-
man, 2000), units (eg. single predicate-argument
links) are inter-related through hierarchical struc-
tures and structure sharing assumed by these for-
malisms. Single errors are inherently propagated
to other sets of errors. This is also the case, though
to a lesser extent, for parsing models in which
shallow parsing is followed by another component
for semantic label assignment.
In order to address these two issues, we propose
two methods in this paper. One is to recognize
cause categories of errors and the other is to cap-
ture inter-dependencies among errors. The former
method defines various patterns of errors to iden-
tify categories of error causes. The latter method
re-parses a sentence with a single target error cor-
rected, and regards the errors which are corrected
in re-parse as errors dependent on the target.
Although these two methods are implemented
for a specific parser using HPSG (Miyao and Tsu-
jii, 2005; Ninomiya et al., 2006), the same ideas
can be applied to any type of parsing models.
</bodyText>
<page confidence="0.356329">
180
</page>
<note confidence="0.907485333333333">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 180–191,
Paris, October 2009. c�2009 Association for Computational Linguistics
Sentence: John has come
</note>
<figure confidence="0.748219">
Predicative event 1:
Predicate-argument relations
</figure>
<figureCaption confidence="0.919294">
Figure 1: Predicate-argument relations
Figure 2: Representation of predicate-argument
relations
</figureCaption>
<equation confidence="0.966818">
has : aux_2args
come : verb_1arg
</equation>
<bodyText confidence="0.999958090909091">
In the following, Section 2 introduces a parser
and its evaluation metrics, Section 3 illustrates dif-
ficulties in analyzing parsing errors based on com-
mon evaluation measures, and Section 4 proposes
the two methods for effective error analysis. Sec-
tion 5 presents experimental results which show
how our methods work for analyzing actual pars-
ing errors. Section 6 and Section 7 illustrate fur-
ther application of these methods to related topics.
Section 8 summarizes this research and indicates
some of future directions.
</bodyText>
<sectionHeader confidence="0.370994" genericHeader="introduction">
2 A parser and its evaluation
</sectionHeader>
<bodyText confidence="0.9977174">
A parser is a system which interprets given sen-
tences in terms of structures derived from syn-
tactic or in some cases semantic viewpoints, and
structures constructed as a result are used as es-
sential information for various tasks of natural lan-
guage processing such as information extraction,
machine translation, and so on.
In this paper, we address issues involved in im-
proving the performance of a parser which pro-
duces structural representations deeper than sur-
face constituent structures. Such a parser is called
a “deep parser.” In many deep parsers, the output
structure is defined by a linguistics-based gram-
mar framework such as CFG, CCG (Steedman,
2000), LFG (Kaplan and Bresnan, 1995) or HPSG
</bodyText>
<table confidence="0.964864285714286">
Abbr. Full Abbr. Full
aux auxiliary conj conjunction
prep prepositional lgs logical subject
verb verb app apposition
coord coordination relative relative
det determiner Narg(s) takes N arguments
adj adjunction mod modifies a word
</table>
<tableCaption confidence="0.996622">
Table 1: Descriptions for predicate types
</tableCaption>
<bodyText confidence="0.99954556097561">
(Pollard and Sag, 1994). Alternatively, some deep
parsing models assume staged processing in which
a stage of shallow parsing is followed by a stage of
semantic role labeling, which assigns labels indi-
cating semantic relationships between predicates
and their arguments. In either case, we assume a
parser to produce a single “deep” structural rep-
resentation for a given sentence, which is chosen
from a set of possible interpretations as the most
probable one by a disambiguation model.
For evaluation of the performance of a parser,
various metrics have been introduced according
to the structure captured by a given grammar
formalism or a system of semantic labels. In
most cases, instead of examining correctness for
a whole structure, a parser is evaluated in terms of
the F-value which shows how correctly it recog-
nizes relationships among words and assigns “la-
bels” to the relationships in the structure. In this
paper, we assume a certain type of “predicate-
argument relation.”
In this measurement, a structure given for a
sentence is decomposed into a set of predicative
words and their arguments. A predicate takes
other words as its arguments. In our representa-
tion, the arguments are labeled by semantically
neutral labels such as ARGn(n = 1...5) and
MOD. In this representation, a basic unit is a
triplet, such as
&lt;Predicate:PredicateType,
ArgumentLabel,
Argument&gt;,
where “Predicate” and “Argument” are surface
words. As shown in the examples in Section 4,
“PredicateType” bears extra information concern-
ing the syntactic construction in which the triplet
is embedded. ARG1-ARG5 express relations be-
tween a Head and its complement, while MOD ex-
presses a relation between an Adjunct and its mod-
ifiee. Since all dependency relations are expressed
by triplets, triplets contain not only semantic de-
</bodyText>
<figure confidence="0.996612659090909">
Predicate
Word: has
Grammatical nature: auxiliary
# of arguments: 2
Predicative event 2:
Predicate
Word: come
Grammatical nature: verb
# of arguments: 1
Argument 2
come
John
Argument 1
Argument 1
John
John
ARG1 ARG2
ARG1
181
Correct answer: I saw a girl with a telescope
Correct (75%):
Parser output:
Error (25%):
I saw a girl with a telescope
I saw a girl with a telescope
I saw a girl with a telescope
ARG1 ARG2
ARG1 ARG2
ARG1 ARG2
ARG1 ARG2 ARG2
ARG1 ARG2
Compare
ARG1 ARG2
ARG1
ARG1
ARG2
Analysis 1: (Possible) ARG1
Analysis 2: (Impossible)
Error:
ARG1
They completed the sale of it to him for $1,000
ARG1
ARG1
They completed the sale of it to him for $1,000
</figure>
<figureCaption confidence="0.964747">
Figure 4: Sketch of error propagation
</figureCaption>
<figure confidence="0.983879090909091">
ARG1
They completed the sale of it to him for $1,000
Can each error occur independently?
ARG1
ARG1
ARG1
ARG1
ARG1
ARG1
Conflict
ARG1
</figure>
<figureCaption confidence="0.96026">
Figure 3: An example of parsing performance
evaluations
</figureCaption>
<bodyText confidence="0.999086136363637">
pendencies but also many dependencies which are
essentially syntactic in nature. Figure 1 shows an
example used in Miyao and Tsujii (2005) and Ni-
nomiya et al. (2006).
This example shows predicate-argument rela-
tions for “John has come.” There are two pred-
icates in this sentence, “has” and ”come”. The
word “has”, which is used as an auxiliary verb,
takes two words, “John” and “come”, as its ar-
guments, and therefore two triplets of predicate-
argument relation, &lt;has ARG1 John&gt; and &lt;has
ARG2 come&gt;. As for the predicative word
“come”, we have one triplet &lt;come ARG1 John&gt;.
Note that, in this HPSG analysis, the auxiliary
verb “has” is analyzed in such a way that it takes
one NP as subject and one VP as complement,
and that the subject of the auxiliary verb is shared
by the verb (“come”) in VP as its subject (Fig-
ure 2). The fact that “has” in this sentence is an
auxiliary verb is indicated by the “PredicateType”,
aux 2args. A “PredicateType” consists of a type
and the number of arguments it takes (Table 1).
</bodyText>
<sectionHeader confidence="0.930566" genericHeader="method">
3 Difficulties in analyzing parsing errors
</sectionHeader>
<bodyText confidence="0.994046696969697">
Figure 3 shows an example of the evaluation of
the parser based on these predicate-argument rela-
tions. Note that the predicate types are abbreviated
in this figure. In the sentence “I saw a girl with a
telescope”, there should be four triplets for the two
predicates, ”saw” and “with,” each of which takes
Figure 5: Parsing errors around one relative clause
attachment
two arguments. Although the parser output does
indeed contain four triplets, the first argument of
“with” is not the correct one. Thus, this output is
erroneous, with the F-value of 75%.
While the F-value thus computed is fine for cap-
turing the performance of a parser, it does not offer
any help for improving its performance.
First, because it does not give any indica-
tion on what portion of erroneous triplets are in
PP-attachment, complement/adjunct distinction,
gerund/participle distinction, etc., one cannot de-
termine which part of a parsing model should be
improved. In order to identify error categories, we
have to manually compare a parsing output with
a correct parse and classify them. Consider again
the example in Figure 3. We can easily observe
that “ARG1” of predicate “with” was mistaken. In
this case, the word linked via “ARG1” represents
a modifiee of the prepositional phrase, and thereby
we conclude that the error is in PP-attachment.
While the process looks straightforward for this
simple sentence and error, to perform such a man-
ual inspection for all sentences and more complex
types of errors is costly, and becomes inhibitive
when the size of a test set of sentences is realisti-
</bodyText>
<figure confidence="0.663210142857143">
ARG1
The book on the shelf which I read yesterday
ARG1
ARG2
Error:
ARG2
182
</figure>
<bodyText confidence="0.993695833333333">
cally large.
Another problem with the F-value is that it ig-
nores inter-dependencies among errors. Since the
F-value does not consider inter-dependencies, one
cannot determine which errors are more crucial
than others in terms of the performance of the sys-
tem as a whole.
A simple example of inter-dependency is shown
in Figure 4. “ARG1” of “for” and “to” were mis-
taken by a parser, both of which can be classified
as PP-attachments as in Figure 3. However, the
two errors are not independent. The former error
can occur by itself (Analysis 1) while the latter
cannot because of the structural conflict with the
former (Analysis 2). The occurrence of the latter
error thus forces the former.
Moreover, inter-dependency in a deep parser
based on linguistics-based formalisms can be
complicated. Error propagation is ingrained in
grammar itself. Consider Figure 5. In this exam-
ple, a wrong decision on the antecedent of a rela-
tive clause results in a wrong triplet of the predi-
cate in the embedded clause with the antecedent.
That is, the two erroneous triplets, one of the
“ARG1” of “which” and the other of the “ARG2”
of “read,” were caused by a single wrong deci-
sion of the antecedent of a relative clause. Such
a propagation of errors can be even more compli-
cated, for example, when the predicate in the rela-
tive clause is a control verb.
In the following section we propose two meth-
ods for analyzing errors. Although both meth-
ods are implemented for the specific parser Enju
(Miyao and Tsujii, 2005; Ninomiya et al., 2006),
the same ideas can be implemented for any parsing
model.
</bodyText>
<sectionHeader confidence="0.996801" genericHeader="method">
4 Methods for effective error analysis
</sectionHeader>
<subsectionHeader confidence="0.999308">
4.1 Recognizing categories of error causes
</subsectionHeader>
<bodyText confidence="0.9980104">
While the Enju parser produces rich feature struc-
tures as output, the performance is evaluated by
the F-value in terms of basic units of predicate-
argment structure. As we illustrated in Section 2,
the basic unit is a triplet in the following form.
&lt;Predicate:PredicateType,
ArgumentLabel,
Argument&gt;
We illustrated in Section 2 how we can identify
errors in PP-attachment simply by examining a
</bodyText>
<figure confidence="0.462235">
(Patterns of correct answer and parser output can be interchanged)
Example:
</figure>
<figureCaption confidence="0.533534">
Figure 6: Pattern for “To-infinitive for modi-
fier/argument of verb”
</figureCaption>
<bodyText confidence="0.999489285714286">
triplet produced by the parser with the correspond-
ing triplet in the gold standard parse.
However, in more complex cases, we have to
consider a set of mismatched triplets collectively
in order to map errors to meaningful error causes.
The following are typical examples of error causes
and pattern rules which identify them.
</bodyText>
<listItem confidence="0.9885765">
(1) Interpretation of Infinitival Clauses as Adjunct
or Complement
</listItem>
<bodyText confidence="0.846331714285714">
Two different types of interpretations of the in-
finitival clauses are explicitly indicated by “Predi-
cateType.” Consider the following two sentences.
(a) [Infinitival clause as an adjunct of the main
clause]
The car was designed (by John) to use it for
business trips.
</bodyText>
<listItem confidence="0.922397">
(b) [Infinitival clause as an argument of catena-
tive verb]
</listItem>
<bodyText confidence="0.9912264">
The car is designed to run fast.
In both sentences, “to” is treated as a predicate to
represent the infinitival clauses in triplets. How-
ever, Enju marks the “PredicateType” of (a) as
“aux-mod-2args,” while it marks the predicate
simply as “aux-2args” in (b). Furthermore, the
linkage between the main clause and the infinitival
clause is treated differently. In (a), the infinitival
clause takes the main clause with relation MOD,
while in (b) the main clause takes the infinitival
</bodyText>
<figure confidence="0.969792020618557">
Pattern:
to :
aux_2args
Correct output:
[ verb, ] ...
to : aux_mod_2args
[ verb2 ]
ARG1
Unknown subject
MOD
ARG2
ARG1
Parser output:
[ verb, ] ...
[ verb2 ]
ARG3
Correct answer:
Parser output:
The car was designed
The car was designed
Unknown subject
ARG3
to :
to : aux_mod_2args
MOD ARG2
aux_2args
ARG1
ARG1
use it for ...
use it for ...
183
(Patterns of correct answer and parser output can be interchanged)
Example:
(Patterns of correct answer and parser output can be interchanged)
Example:
Pattern:
Correct output:
ARG1
MOD
The customers walk the door
in
expecting: verb_mod_3args
Parser output:
Not exist
ARG1 (MOD)
expecting: verb_3args
The customers walk the door
in
ARG2
ARG3
you to have
a package for them
ARG2 ARG3
a package for them
you to have
Pattern:
...
Correct output:
[ verb, ] ...
lgs_1arg
ARG1
ARG1
...
Parser output:
[ verb, ] ...
prep_2args
ARG1
ARG1
Unknown subject
released
Friends ...
in ...September
Correct answer:
A 50-state study
by : lgs_1arg
prep_2args
by :
ARG1
ARG1
ARG1
Parser output:
A 50-state study released
in September
...
ARG1
Unknown subject
ARG1
Friends ...
Correct answer:
...
Parser output:
[ gerund ]: verb_mod_Narg(s)
...
[
gerund ]: verb_Narg(s)
...
...
</figure>
<figureCaption confidence="0.8182605">
Figure 7: Pattern for “Gerund acts as modifier or
not”
</figureCaption>
<bodyText confidence="0.99842575">
clause as ARG3. Furthermore, in the catenative
verb interpretation of “designed”, the deep object
(the surface subject in this example) fills ARG1
of the verb in the infinitival clause (complement),
while in the adjunct interpretation, the deep sub-
ject which is missing in this sentence occupies
the same role. Consequently, a single erroneous
choice between these two interpretations results in
a set of mismatched triplets.
We recognize such a set of mismatched triplets
by a pattern rule (Figure 6) and map them to this
type of error cause.
(2) Interpretation of Gerund-Participle interpreta-
tions
A treatment similar to (1) is taken for different
interpretations of Gerund. Interpretation as Ad-
junct of a main clause is signaled by the “Predi-
cateType” verb-mod-*, while an interpretation as
a modifier of a noun is represented by the “Predi-
cateType” verb (Figure 7).
</bodyText>
<listItem confidence="0.769805">
(3) Interpretation of “by”
</listItem>
<bodyText confidence="0.999812625">
A prepositional phrase with “by” in a passive
clause can be interpreted as a deep subject, while
the same phrase can be interpreted as an ordinary
PP phrase that is used as an adjunct. The first in-
terpretation is marked by the “PredicateType” lgs
(logical subject) which takes only one argument.
The relationship between the passivized verb and
the deep subject is captured by ARG1 which goes
</bodyText>
<figureCaption confidence="0.8106455">
Figure 8: Pattern for “Subject for passive sentence
or not”
</figureCaption>
<figure confidence="0.818209">
Example:
</figure>
<figureCaption confidence="0.995862">
Figure 9: Pattern for “Relative clause attachment”
</figureCaption>
<bodyText confidence="0.999103285714286">
from the verb to the noun phrase. On the other
hand, in the interpretation as an ordinary PP, the
preposition as predicate links the main verb and
NP via ARG1 and ARG2, respectively (Figure 8).
Again, a set of mismatched triplets should be
mapped to a single cause of errors via a pattern
rule.
</bodyText>
<listItem confidence="0.777961">
(4) Antecedent of a Relative Clause
</listItem>
<bodyText confidence="0.995933333333333">
This type of error is manifested by two mis-
matched triplets with different predicates. This is
because a wrong choice of antecedent for a rela-
tive clause results in a wrong link for the trace of
the relative clause.
Since a relative clause pronoun is treated as a
</bodyText>
<figure confidence="0.848127590909091">
Pattern:
...
relative_1arg
...
Parser output: ARG1/2
ARG1
Error
Correct answer: ARG2
the shelf
The book on
which: relative_1arg
I read yesterday
ARG1
Parser output: ARG2
Error ARG1
the shelf
which : relative_1arg
I read yesterday
The book on
184
Cause categories Patterns
[Argument selection]
</figure>
<table confidence="0.99666485">
Prepositional attachment ARG1 ofprep *
Adjunction attachment ARG1 of adj *
Conjunction attachment ARG1 of conj *
Head selection for noun phrase ARG1 of det *
Coordination ARG1/2 of coord *
[Predicate type selection]
Preposition/Adjunction prep * —adj *
Gerund acts as modifier/not verb mod Narg(s)
— verb Narg(s)
Coordination/conjunction coord * — conj *
# of arguments for preposition prep Marg(s)
—prep Narg(s)
Adjunction/adjunctive noun adj * — noun *
[More structural errors]
To-infinitive for see Figure 6
modifier/argument of verb
Subject for passive sentence/not see Figure 8
[Others]
Comma any error around “,”
Relative clause attachment see Figure 9
</table>
<tableCaption confidence="0.997601">
Table 2: Defined patterns for cause categories
</tableCaption>
<bodyText confidence="0.999961">
predicate which takes the antecedent as its single
argument, identification of error type can be done
simply by looking at ARG1. However, since the
errors usually propagate to the triplets that contain
their traces, we have to map them together to the
single error (Figure 9).
Table 2 shows the errors across different types
which our current version of pattern rules can
identify.
</bodyText>
<subsectionHeader confidence="0.6528565">
4.2 Capturing inter-dependencies among
errors
</subsectionHeader>
<bodyText confidence="0.987942545454546">
Some inter-dependencies among erroneous
triplets are ingrained in grammar, such as the case
of antecedent of a relative clause in (4) in Section
4.1. Some are caused by general constraints such
as the projection principle in dependency structure
(Figure 4 in Section 2).
Regardless of causes of dependencies, to recog-
nize inter-dependencies among errors is a crucial
step of effective error analysis.
Our method consists of the following four steps:
[Step 1] Re-parsing a target sentence: A given sen-
tence is re-parsed under the condition where an er-
ror is forcibly corrected.
[Step 2] Forming a network of inter-dependencies
of errors: By comparing the new parse result (a
set of triplets) with the initial parse result, this
step creates a directed graph of errors in the ini-
Figure 10: Schema of capturing inter-
dependencies
tial parse. A directed link shows that correction of
the error in the starting node produces a new parse
result in which the error in the receiving node of
the link disappears.
[Step 3] Forming groups of inter-dependent errors:
This step recognizes a group of inter-dependent er-
rors which forms a directed circle in the network
created by [Step 2].
[Step 4] Forming a network of error propagation:
This step creates a new network by reducing each
of inter-dependent error groups of [Step 3] to a sin-
gle node.
Figure 10 illustrates how these steps work. In
this example, while “today” should modify the
noun phrase “our work force”, the initial parse
wrongly takes “today” as the head noun of the
whole noun phrase. As a result, there are five er-
rors; three wrong outputs, “ARG2” of “on” (Er-
ror 1), “ARG1” of “our” (Error 2) and “ARG1”
of “work” (Error 3). There is an extra triplet for
“ARG1” of “force” (Error 4), and a triplet for
“ARG1” of “today” (Error 5) is missing (Figure
10 (a)).
Figure 10 (b) shows inter-dependencies among
the errors recognized by [Step 2], and Figure 10
</bodyText>
<figure confidence="0.98403970886076">
Re-parse a sentence under the condition where
each error is forcibly corrected
(b)
Inter-dependency among errors:
re-parse
3
,
,
Correct
3
Correct
Correct
Correct
,
,
2
,
,
2
4
Correct
2
,
,
,
disappear
disappear
disappear
disappear
disappear
re-parse
re-parse
re-parse
re-parse
2
1
1
1
1
1
2
3
4
5
4
4
4
3
3
,
,
Form inter-dependent error groups and
error propagation network
(c) Resultant network:
Propagation
5
1
2
3
4
Inter-dependent error group Inter-dependent error group
(a)
It has no bearing
Errors:
1
ARG2
ARG2 ARG1
on
2
ARG1
our work force today
3
ARG1 ARG1
ARG1
4
ARG1
5
185
Cause categories of errors # of
</figure>
<table confidence="0.979928347826087">
Errors Locations
Classified 2,078 1,671
[Argument selection]
Prepositional attachment 579 579
Adjunction attachment 261 261
Conjunction attachment 43 40
Head selection for noun phrase 30 30
Coordination 202 184
[Predicate type selection]
Preposition/Adjunction 108 54
Gerund acts as modifier/not 84 31
Coordination/conjunction 54 27
# of arguments for preposition 51 17
Adjunction/adjunctive noun 13 13
[More structural errors]
To-infinitive for 120 22
modifier/argument of verb
Subject for passive sentence/not 8 3
[Others]
Comma 444 372
Relative clause attachment 102 38
Unclassified 2,631
Total 4,709 I
</table>
<tableCaption confidence="0.999919">
Table 3: Errors classified into cause categories
</tableCaption>
<bodyText confidence="0.9882562">
(c) shows what the resultant network looks like.
An inter-dependent error group of 1, 2, 3 and 4 is
recognized by [Step 3] and represented as a single
node. Error 5 is propagated to this node in the final
network.
</bodyText>
<sectionHeader confidence="0.998704" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9998225">
We applied our methods to the analyses of actual
errors produced by Enju. This version of Enju was
trained on the Penn Treebank (Marcus et al., 1994)
Section 2-21.
</bodyText>
<subsectionHeader confidence="0.995587">
5.1 Observation of identified cause categories
</subsectionHeader>
<bodyText confidence="0.999943294117647">
We first parsed sentences in PTB Section 22, and
based on the observation of errors, we defined the
patterns in Section 4. We then parsed sentences in
Section 0. The errors in Section 0 were mapped to
error cause categories by the pattern rules created
for Section 22.
Table 3 summarizes the distribution across the
causes of errors. The left and right numbers in the
table show the number of erroneous triplets clas-
sified into the categories and the frequency of the
patterns matched, respectively. The table shows
that, with the 14 pattern rules, we successfully ob-
served 1,671 hits and 2,078 erroneous triplets are
dealt with by these hits. This amounts to more
than 40% erroneous triplets (2,078/4,709). Since
this was the first attempt, we expect the coverage
can be easily improved by adding new patterns.
</bodyText>
<table confidence="0.996584">
Evaluated sentences (erroneous) 1,811 (1,009)
Errors (Correctable) 4,709 (3,085)
Inter-dependent error groups 1,978
Correction propagations 501
F-score (LP/LR) 90.69 (90.78/90.59)
</table>
<tableCaption confidence="0.999337">
Table 4: Summary of inter-dependencies
</tableCaption>
<figure confidence="0.997437166666666">
1500
1000
500
0
1 2 3 4 5 6 7 8 9 10
Size of inter—dependent error group
</figure>
<figureCaption confidence="0.9880405">
Figure 11: Frequency of each size of inter-
dependent error group
</figureCaption>
<bodyText confidence="0.999980379310345">
From the table, we can observe that a signif-
icant portion of errors is covered by simple types
of error causes such as PP-attachment and Adjunct
attachment. They are simple in the sense that the
number of erroneous triplets treated and the fre-
quency of the pattern application coincide. How-
ever, their conceived significance may be over-
rated. These simple types may constitute parts of
more complex error causes. Furthermore, since
pattern rules for these simple causes are easy to
prepare and have already been covered by the cur-
rent version, most of the remaining 60% of the er-
roneous triplets are likely to require patterns for
more complex causes.
On the other hand, patterns for complex causes
collect more erroneous triplets once they are fired.
This tendency is more noticeable in structural pat-
terns of errors. For example, in “To-infinitive for
modifier/argument of verb,” there were only 22
hits for the pattern, while the number of erroneous
triplets is 120. This implies five triplets per hit.
This is because, in a deep parser, a wrong choice
between adjunct or complement interpretations of
a to-infinitival clause affects the interpretation of
implicit arguments in the clause through control.
Though expected, such detailed observations show
how differences between shallow and deep parsers
may affect evaluation methods and the methods of
analyzing errors.
</bodyText>
<subsectionHeader confidence="0.999835">
5.2 Observation of inter-dependencies
</subsectionHeader>
<bodyText confidence="0.999694">
In the inter-dependency experiments we per-
formed, some errors could not be forcibly cor-
rected by our method. This was because the parser
</bodyText>
<figure confidence="0.992221487804878">
186
(a)
Sentence:
ARG1
show :
verb_2args
later ...
up decades
Inter-dependent error group (a):
Correction propagation from (a) to (b)
Correct answer:
ARG2
... thought she could
Parser output:
... thought she could
ARG2
ARG1 ARG2
ARG1 ARG2
ARG2
help : verb_2args
help :
ARG2
aux_2args
save ...
save ...
Inter-dependent error group (b):
... she could help save :
Parser output:
... she could help
Correct answer:
ARG1
ARG1
verb_2args
verb_3args
ARG2
her teaching certificate .
ARG2
ARG3
ARG1
save :
her teaching certificate .
</figure>
<figureCaption confidence="0.99276">
Figure 13: Correction propagation between ob-
tained inter-dependent error groups
</figureCaption>
<figure confidence="0.999665695652174">
Inter-dependent error group (a):
Inter-dependent error group (b):
Inter-dependent error group (c):
Inter-dependent error group (d):
Sentence:
) She says she offered Mrs. Yeargi b a quiet resignation and
(a)
( certificate .
save her teaching
thought she could help
The asbestos fiber , crocidolite , is unusually resilient once
(b)
it enters the lungs , with even brief exposures to it causing
(c) (d)
symptoms that show up decades later , researchers said .
... fiber
Correct answer:
crocidolite ...
ARG1 ARG2
... fiber
, :app_2args
crocidolite ...
Parser output:
, :
coorrl_2args
ARG1 ARG2
ARG1
ARG1
... is usually resilient ... the lungs
with ...
,
ARG1
ARG1
Correct answer:
Parser output:
... symptoms that
ARG1
ARG2
... symptoms that show :
verb_1arg
later ...
up decades
ARG1
ARG1
... it causing symptoms that show up decades later ...
ARG1
</figure>
<figureCaption confidence="0.99955">
Figure 12: Obtained inter-dependent error groups
</figureCaption>
<bodyText confidence="0.999873781818182">
we use prunes less probable parse substructures
during parsing. In some cases, even if we gave a
large positive value to a triplet which should be in-
cluded in the final parse, parsing paths which can
contain the triplet were pruned before. In this re-
search, we ignored such errors as “uncorrectable”
ones, and focused on the remaining “correctable”
errors.
Table 4 shows a summary of the analysis. As the
previous experiment, Enju produced 4,709 errors
for Section 0, of which 3,085 were correctable. By
applying the method illustrated in Section 4.2, we
obtained 1,978 inter-dependent error groups and
501 correction propagation relationships among
the groups.
Figure 11 shows the frequency of the size of
inter-dependent error groups. About half of the
groups contain only single errors which could
have only one-way correction propagations with
other errors or were completely independent of
other errors.
Figure 12 shows an example of the extracted
inter-dependent error groups. For the sentence
shown at the top, Enju gave seven errors. By ap-
plying the method in Section 4.2, these errors were
grouped into four inter-dependent error groups (a)
to (d), and no correction propagations were de-
tected among them. Group (a) contains two errors
on the comma’s local behavior as apposition or co-
ordination. Group (b) contains the errors on the
words which give almost the same attachment be-
haviors. Group (c) contains the errors on whether
the verb “show” took “decades” as its object or
not. Group (d) contains an error on the attachment
of the adverb “later”. Regardless of the overlap
of the regions in the sentence for (c) and (d), our
approach successfully grouped the errors into two
independent groups. The method shows that the
errors in each group are inter-dependent, but er-
rors in one group are independent of those in an-
other. This enables us to concentrate on each of
the co-occurring error groups separately, without
minding the errors in other groups.
Figure 13 shows another example. In this ex-
ample, eight errors for a sentence were classified
into two inter-dependent error groups (a) and (b).
Moreover, it shows that the correction of group (a)
results in correction of group (b).
The errors in group (a) were related to the
choice as to whether “help” had an auxiliary or
a pure verbal role. The errors in group (b) were
related with the choice as to whether “save” took
only one object (“her teaching certificate”) or two
objects (“her” and “teaching certificate”). Be-
tween group (a) and (b), no “structural” con-
</bodyText>
<figure confidence="0.984252017857143">
Sentence:
(a)
Inter-dependent error group (a):
ARG1
It invests heavily in ... securities overseas :
...
adj_1arg
Cause categories:
Comma,
Relative clause attachment
Cause categories:
Coordination (fragment)
Head selection for noun phrase
... president ... of this U.S. sales and :
...
coord_2args
Inter-dependent error group (b):
Figure 14: Combining our two methods (1)
Cause categories:
Adjunction attachment
Inter-dependent error group (a):
Inter-dependent error group (b):
Correction propagation from (a) to (b)
... is currently waiving management fees , which
ARG1
ARG1
ARG1
ARG1
boosts ...
ARG1
ARG1
ARG2
ARG1 ARG1 ARG1 ARG2
ARG2
of this :
ARG1
det_1arg
ARG1
U.S. sales
and : coord_2args
ARG2
marketing arm
ARG1
ARG1
ARG1
It invests heavily in dollar-denominated securities overseas and
(b)
is currently waiving management fees , which boosts
its yield .
Sentence:
Clark J. Vitulli was named senior vice president and general
(a)
manager
auto Maker Mazda Motor Corp .
(b)
of this U.S. sales
</figure>
<figureCaption confidence="0.544576">
and marketing arm of Japanese
Cause categories: Coordination (fragment)
</figureCaption>
<bodyText confidence="0.998803545454546">
flict could arise when correcting only each of the
groups. We could then hypothesize that the cor-
rection propagation between the two groups were
caused by the disambiguation model.
By dividing the errors into minimal units and
clarifying the effects of correcting a target error,
we can conclude that the inter-dependent group
(a) should be handled first for effective improve-
ment of the parser. In such a way, obtained inter-
dependencies among errors can suggest an effec-
tive strategy for parser improvement.
</bodyText>
<subsectionHeader confidence="0.992556">
5.3 Combination of the two methods
</subsectionHeader>
<bodyText confidence="0.9999744">
By combining the two methods described in Sec-
tion 4.1 and 4.2, we can see how each error cause
affects the performance of a parser. The results
are summarized in Table 5. The leftmost column
in the table shows the numbers of errors in terms
of triplets, which are the same as the leftmost col-
umn in Table 3.
The “independence rate” shows the ratio of er-
roneous triplets in the category which are not af-
fected by correction of other erroneous triplets. On
the other hand, the “correction effect” shows how
many erroneous triplets would be corrected if one
of the erroneous triplets in the category was cor-
rected. These two columns are computed by using
the error propagation network constructed in Sec-
tion 4.2. That is, by using the network we obtain
the number of erroneous triplets to be corrected if
a given erroneous triplet in the category was cor-
rected, sum up these numbers and then calculate
the average number of expected side-effect correc-
</bodyText>
<figureCaption confidence="0.898196">
Figure 15: Combining our two methods (2)
</figureCaption>
<bodyText confidence="0.999682655172414">
tion per erroneous triplet in the category.
Figure 14 shows an example of independent er-
rors. For the sentence at the top, the parser pro-
duced four errors. The method in Section 4.2
successfully discovered two inter-dependent error
groups (a) and (b). There was no error propaga-
tion relation between the two groups. On the other
hand, the method in Section 4.1 associated all of
these four errors with the categories of “Adjunc-
tion attachment,” “Comma” and “Relative clause
attachment,” and the error for the “Adjunction at-
tachment” corresponds to the inter-dependent er-
ror group (a). Because this group is not a receiving
node of any propagation in the network, the error
is regarded as an “independent” one.
Independent errors mean that, if a new parsing
model could correct them, the correction would
not be destroyed by other errors which remain in
the new parsing model.
The correction effect shows the opposite and
desirable effect of the nature of the dependency
among errors which the propagation network rep-
resents. This means that, if one of erroneous
triplets in the category was corrected, the correc-
tion would be amplified through propagation, and
as a result other errors would also be corrected.
We show an example of the correction effect in
Figure 15. In the figure, the parser had six errors;
three false outputs for ARG1 of “and,” “this” and
</bodyText>
<page confidence="0.5027">
188
</page>
<table confidence="0.997848190476191">
Cause categories of errors # of errors Independence Correction Expected range
rate (%) effect (%) of error correction
[Argument selection]
Prepositional attachment 579 74.8 144.3 625.0 - 835.5
Adjunction attachment 261 56.6 179.6 265.3 - 468.8
Conjunction attachment 43 36.4 239.4 37.5 - 102.9
Head selection for noun phrase 30 0.0 381.8 0.0 - 114.5
Coordination 202 42.5 221.2 189.9 - 446.8
[Predicate type selection]
Preposition/Adjunction 108 41.7 158.3 71.3 - 171.0
Gerund acts as modifier/not 84 46.2 159.0 61.7 - 133.0
Coordination/conjunction 54 44.4 169.4 40.6 - 91.5
# of arguments for preposition 51 95.8 108.3 52.9 - 55.2
Adjunction/adjunctive noun 13 75.0 125.0 12.2 - 16.3
[More structural errors]
To-infinitive for 120 36.0 116.0 50.1 - 139.2
modifier/argument of verb
Subject for passive sentence/not 8 37.5 112.5 3.4 - 9.0
[Others]
Comma 444 39.5 194.4 341.0 - 863.1
Relative clause attachment 102 32.1 141.7 46.4 - 144.5
</table>
<tableCaption confidence="0.999768">
Table 5: Correction propagations between errors for each cause category and the other errors
</tableCaption>
<bodyText confidence="0.999971558823529">
“U.S.,” two false outputs for ARG2 of “of” and
“and,” and a missing output for ARG1 of “sales.”
Our method for inter-dependencies classified these
errors into two inter-dependent error groups (a)
and (b), and extracted an correction propagation
from (a) to (b). Our method for cause categories,
on the other hand, associated two errors of “and”
with the category “Coordination” and one error of
“this” with the category “Head selection for noun
phrase.” When we correct an error in the interde-
pendent error group (a), the correction leads to not
only correction of the other errors in (a) but also
correction of the error in (b) via correction prop-
agation from (a) to (b). Therefore, a correction
effect of an error in group (a) results in 6.0.
On the basis of the above considerations, we es-
timated the range of the effect which an error cor-
rection in each category has. The minimum of ex-
pected correction range in Table 5 is given by the
product of the number of erroneous triplets in the
category, the independence rate and the correction
effect. On the other hand, the maximum is given
by the product of the number of erroneous triplets
in the category and the correction effect. This as-
sumes that all corrections made in the category are
not cancelled by other errors, while the figure in
the minimum are based on the assumption that all
corrections made in the category, except for the in-
dependent ones, are cancelled by other errors.
Table 5 would thus suggest which categories
should be resolved with high priority, from three
points of view: the number of errors in the cat-
egory, the number of independent errors, and the
correction effect.
</bodyText>
<sectionHeader confidence="0.979732" genericHeader="method">
6 Further applications of our methods
</sectionHeader>
<bodyText confidence="0.999926285714286">
In this section, as an example of the further ap-
plication of our methods, we attempt to analyze
parsing behaviors in domain adaptation from the
viewpoints of error cause categories.
In Hara et al. (2007), we proposed a method for
adapting Enju to a target domain, and then suc-
ceeded in improving the parser performance for
the GENIA corpus (Kim et al., 2003), a biomed-
ical domain. Table 6 summarizes the parsing re-
sults for three types of settings respectively: pars-
ing PTB with Enju (“Enju for PTB”), parsing GE-
NIA with Enju (“Enju for GENIA”), and parsing
GENIA with the adapted model (“Adapted for GE-
NIA”). We then analyzed the performance transi-
tion among these settings from the viewpoint of
the cause categories given in Section 4.1 (Table 7).
In order to compare the error frequencies among
different settings, we took the percentage of target
errors in all of the evaluated triplets. The signed
values between the two settings show how much
the errors increased when moving from the left set-
tings to the right ones.
When we focus on the transition from “Enju
for PTB” to “Enju for GENIA,” we can observe
that the change in the domain resulted in a dif-
ferent distribution of error causes. The errors for
most categories increased, and in particular, the er-
rors for “Prepositional attachment” and “Coordi-
</bodyText>
<page confidence="0.579916">
189
</page>
<table confidence="0.9993542">
Enju for PTB Enju for GENIA Adapted for GENIA
Evaluated sentences 1,811 842 842
Evaluated triplets 44,934 22,230 22,230
Errors 4,709 3,120 2,229
F-score (LP/LR) 90.69 (90.78/90.59) 87.41 (87.60/87.22) 90.93 (91.10/90.76)
</table>
<tableCaption confidence="0.957374">
Table 6: Summary of parsing performances for domain and model variations
</tableCaption>
<table confidence="0.997959666666667">
Cause categories of errors Rate of errors against total examined relations in test set (%)
Enju for PTB −→ Enju for GENIA −→ Adapted for GENIA
Classified 4.62 +2.60% 7.22 −1.80&amp; 5.42
[Argument selection]
Prepositional attachment 1.29 +0.93% 2.22 −0.64&amp; 1.58
Adjunction attachment 0.58 +0.38% 0.96 −0.20&amp; 0.76
Conjunction attachment 0.10 −0.04&amp; 0.06 −0.04&amp; 0.02
Head selection for noun phrase 0.07 +0.17% 0.24 −0.06&amp; 0.18
Coordination 0.45 +0.59% 1.04 −0.25&amp; 0.79
[Predicate type selection]
Preposition/Adjunction 0.24 +0.08% 0.32 −0.06&amp; 0.26
Gerund acts as modifier/not 0.19 −0.07&amp; 0.12 +0.01% 0.13
Coordination/conjunction 0.12 ±0.00→ 0.12 −0.07&amp; 0.05
# of arguments for preposition 0.11 −0.02&amp; 0.09 ±0.00&amp; 0.09
Adjunction/adjunctive noun 0.03 +0.19% 0.22 −0.08&amp; 0.14
[More structural errors]
To-infinitive for 0.27 +0.02% 0.29 −0.09&amp; 0.20
modifier/argument of verb
Subject for passive sentence/not 0.02 +0.34% 0.36 +0.01% 0.37
[Others]
Comma 0.99 −0.03&amp; 0.96 −0.31&amp; 0.65
Relative clause attachment 0.23 +0.05% 0.28 −0.03&amp; 0.25
Unclassified 5.86 +0.96% 6.82 −2.22&amp; 4.60
Total (Classified + Unclassified) 10.48 +3.56% 14.04 −4.01&amp; 10.03
</table>
<tableCaption confidence="0.999674">
Table 7: Error distributions for domain and model variations
</tableCaption>
<bodyText confidence="0.9999803">
nation” increased remarkably. On the other hand,
the transition from “Enju for GENIA” to “Adapted
for GENIA” shows that their adaptation method
succeeded in reducing the errors for most cate-
gories to some extent. However, for “Preposi-
tional attachment,” “Coordination,” and “Subject
for passive sentence or not,” there were still no-
ticeable gaps in error distribution between “Enju
for PTB” and “Adapted for GENIA.” This would
mean that the adapted model requires further per-
formance improvement if we expect the same level
of performances for those categories as the parser
originally obtained in PTB.
We could thus capture some biases of cause
categories which occur in domain transition or
in domain adaptation, which would not be clari-
fied by F-score evaluation methods. With inter-
dependencies given by the method described in
Section 4.2, the above analysis would be useful
for effectively exploring further adaptation.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="method">
7 Related works
</sectionHeader>
<bodyText confidence="0.999975961538462">
Although there have been many researchers who
analyzed errors in their own systems in the experi-
ments, there has been little research which focused
on error analysis itself.
In the field of parsing, McDonald and Nivre
(2007) compared parsing errors between graph-
based and transition-based parsers. They consid-
ered accuracy transitions from various points of
view, and the obtained statistical data suggested
that error propagation seemed to occur in the
graph structures of parsing outputs. Our research
proceeded one step further and attempted to re-
veal the nature of the propagations. In examin-
ing the combination of the two types of parsing,
they utilized approaches similar to our method for
capturing inter-dependencies of errors. They al-
lowed a parser to give only structures produced by
the parsers and utilized the ideas for evaluating the
parser’s potentials, whereas we utilized it for ob-
serving error propagations.
Dredze et al. (2007) showed that many of the
parsing errors in domain adaptation tasks may
come from inconsistencies between the annota-
tions of training resources. This would sug-
gest that just error comparisons without consider-
ing the inconsistencies could lead to a misunder-
</bodyText>
<page confidence="0.594932">
190
</page>
<bodyText confidence="0.9999775">
standing of what happens in domain transitions.
The summarized error cause categories and inter-
dependencies given by our methods would be use-
ful clues for extracting such domain-dependent er-
ror phenomena.
When we look into other research areas in nat-
ural language processing, Gim´enez and M`arquez
(2008) proposed an automatic error analysis ap-
proach in machine translation (MT) technologies.
They developed a metric set which could capture
features in MT outputs at different linguistic lev-
els with different levels of granularity. Like we
considered parsing systems, they explored ways to
resolve costly and rewardless error analysis in the
MT field. One of their objectives was to enable
researchers to easily obtain detailed linguistic re-
ports on the behavior of their systems, and to con-
centrate on analyses for the system improvements.
</bodyText>
<sectionHeader confidence="0.99539" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99999104">
We proposed two methods for analyzing parsing
errors. One is to assign errors to cause categories,
and the other is to capture inter-dependencies
among errors. The first method defines error pat-
terns to identify cause categories and then asso-
ciates errors involved in the patterns with the cor-
responding categories. The second method re-
parses a sentence with a target error corrected, and
regards errors corrected together as dependent on
the target.
In our experiments with an HPSG parser, we
successfully associated more than 40% of the er-
rors with 14 cause categories, and captured 1,978
inter-dependent error groups. Moreover, the com-
bination of our methods gave a more detailed error
analysis for effective improvement of the parser.
In our future work, we would give more pat-
tern rules for classifying a large percentage of er-
rors into cause categories, and incorporate uncor-
rectable errors into inter-dependency analysis. Af-
ter improving the analytical facilities of our indi-
vidual methods, we would explore the possibil-
ity of combining the methods for obtaining more
powerful and detailed clues on how to improve
parsing performance.
</bodyText>
<sectionHeader confidence="0.996563" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9968765">
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan).
</bodyText>
<sectionHeader confidence="0.996263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999801222222223">
Mark Dredze, John Blitzer, Partha Pratim Talukdar,
Kuzman Ganchev, Jo˜ao V. Grac¸a, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proceedings of the
CoNLL Shared Task Session of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 1051–1055.
Jes´us Gim´enez and Lluis M`arquez. 2008. Towards het-
erogeneous automatic mt error analysis. In Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC’08), pages 1894–1901.
Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of 10th International Confer-
ence on Parsing Technologies (IWPT 2007), pages
11–22.
Ronald M. Kaplan and Joan Bresnan. 1995. Lexical-
functional grammar: A formal system for gram-
matical representation. Formal Issues in Lexical-
Functional Grammar, pages 29–130.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun’ichi Tsujii. 2003. GENIA corpus - a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19(suppl. 1):i180–i182.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert Macintyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
ARPA Human Language Technology Workshop.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122–131.
Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL), pages 83–90.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 155–163.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Mark Steedman. 2000. The Syntactic Process. THE
MIT Press.
</reference>
<page confidence="0.928073">
191
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.288239">
<title confidence="0.999146">Effective Analysis of Causes and Inter-dependencies of Parsing Errors</title>
<author confidence="0.868799">Yusuke</author>
<affiliation confidence="0.817614">of Computer Science, University of</affiliation>
<address confidence="0.635014">Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033,</address>
<affiliation confidence="0.44086">of Computer Science, University of (National Center for Text</affiliation>
<abstract confidence="0.998001962962963">In this paper, we propose two methods for analyzing errors in parsing. One is to classify errors into categories which grammar developers can easily associate with defects in grammar or a parsing model and thus its improvement. The other is to discover inter-dependencies among errors, and thus grammar developers can focus on errors which are crucial for improving the performance of a parsing model. The first method uses patterns of errors to associate them with categories of causes for those errors, such as errors in scope determination of coordination, PPattachment, identification of antecedent of relative clauses, etc. On the other hand, the second method, which is based on reparsing with one of observed errors corrected, assesses inter-dependencies among errors by examining which other errors were to be corrected as a result if a specific error was corrected. Experiments show that these two methods are complementary and by being combined, they can provide useful clues as to how to improve a given grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>John Blitzer</author>
<author>Partha Pratim Talukdar</author>
<author>Kuzman Ganchev</author>
<author>Jo˜ao V Grac¸a</author>
<author>Fernando Pereira</author>
</authors>
<title>Frustratingly hard domain adaptation for dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1051--1055</pages>
<marker>Dredze, Blitzer, Talukdar, Ganchev, Grac¸a, Pereira, 2007</marker>
<rawString>Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman Ganchev, Jo˜ao V. Grac¸a, and Fernando Pereira. 2007. Frustratingly hard domain adaptation for dependency parsing. In Proceedings of the CoNLL Shared Task Session of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1051–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Towards heterogeneous automatic mt error analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<pages>1894--1901</pages>
<marker>Gim´enez, M`arquez, 2008</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2008. Towards heterogeneous automatic mt error analysis. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), pages 1894–1901.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadayoshi Hara</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluating impact of re-training a lexical disambiguation model on domain adaptation of an hpsg parser.</title>
<date>2007</date>
<booktitle>In Proceedings of 10th International Conference on Parsing Technologies (IWPT</booktitle>
<pages>11--22</pages>
<contexts>
<context position="36520" citStr="Hara et al. (2007)" startWordPosition="5998" endWordPosition="6001"> while the figure in the minimum are based on the assumption that all corrections made in the category, except for the independent ones, are cancelled by other errors. Table 5 would thus suggest which categories should be resolved with high priority, from three points of view: the number of errors in the category, the number of independent errors, and the correction effect. 6 Further applications of our methods In this section, as an example of the further application of our methods, we attempt to analyze parsing behaviors in domain adaptation from the viewpoints of error cause categories. In Hara et al. (2007), we proposed a method for adapting Enju to a target domain, and then succeeded in improving the parser performance for the GENIA corpus (Kim et al., 2003), a biomedical domain. Table 6 summarizes the parsing results for three types of settings respectively: parsing PTB with Enju (“Enju for PTB”), parsing GENIA with Enju (“Enju for GENIA”), and parsing GENIA with the adapted model (“Adapted for GENIA”). We then analyzed the performance transition among these settings from the viewpoint of the cause categories given in Section 4.1 (Table 7). In order to compare the error frequencies among diffe</context>
</contexts>
<marker>Hara, Miyao, Tsujii, 2007</marker>
<rawString>Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Evaluating impact of re-training a lexical disambiguation model on domain adaptation of an hpsg parser. In Proceedings of 10th International Conference on Parsing Technologies (IWPT 2007), pages 11–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexicalfunctional grammar: A formal system for grammatical representation. Formal Issues in LexicalFunctional Grammar,</title>
<date>1995</date>
<pages>29--130</pages>
<contexts>
<context position="2811" citStr="Kaplan and Bresnan, 1995" startWordPosition="435" endWordPosition="438">as to where the weaknesses exist in a parsing model. As a result, the improvement process takes the form of time consuming trial-error cycles. Once grammar developers know the actual distribution of errors across different categories such as PP-attachment, complement/adjunct distinction, gerund/participle distinction, etc., they can think of focused and systematic improvement of a parsing model. Another problem of the F-value in terms of uniform units is that it does not take interdependencies among errors into consideration. In particular, for parsers based on grammar formalisms such as LFG (Kaplan and Bresnan, 1995), HPSG (Pollard and Sag, 1994), or CCG (Steedman, 2000), units (eg. single predicate-argument links) are inter-related through hierarchical structures and structure sharing assumed by these formalisms. Single errors are inherently propagated to other sets of errors. This is also the case, though to a lesser extent, for parsing models in which shallow parsing is followed by another component for semantic label assignment. In order to address these two issues, we propose two methods in this paper. One is to recognize cause categories of errors and the other is to capture inter-dependencies among</context>
<context position="5484" citStr="Kaplan and Bresnan, 1995" startWordPosition="855" endWordPosition="858">uctures derived from syntactic or in some cases semantic viewpoints, and structures constructed as a result are used as essential information for various tasks of natural language processing such as information extraction, machine translation, and so on. In this paper, we address issues involved in improving the performance of a parser which produces structural representations deeper than surface constituent structures. Such a parser is called a “deep parser.” In many deep parsers, the output structure is defined by a linguistics-based grammar framework such as CFG, CCG (Steedman, 2000), LFG (Kaplan and Bresnan, 1995) or HPSG Abbr. Full Abbr. Full aux auxiliary conj conjunction prep prepositional lgs logical subject verb verb app apposition coord coordination relative relative det determiner Narg(s) takes N arguments adj adjunction mod modifies a word Table 1: Descriptions for predicate types (Pollard and Sag, 1994). Alternatively, some deep parsing models assume staged processing in which a stage of shallow parsing is followed by a stage of semantic role labeling, which assigns labels indicating semantic relationships between predicates and their arguments. In either case, we assume a parser to produce a </context>
</contexts>
<marker>Kaplan, Bresnan, 1995</marker>
<rawString>Ronald M. Kaplan and Joan Bresnan. 1995. Lexicalfunctional grammar: A formal system for grammatical representation. Formal Issues in LexicalFunctional Grammar, pages 29–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Yuka Teteisi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>GENIA corpus - a semantically annotated corpus for bio-textmining.</title>
<date>2003</date>
<journal>Bioinformatics,</journal>
<volume>19</volume>
<pages>1--180</pages>
<contexts>
<context position="36675" citStr="Kim et al., 2003" startWordPosition="6026" endWordPosition="6029">her errors. Table 5 would thus suggest which categories should be resolved with high priority, from three points of view: the number of errors in the category, the number of independent errors, and the correction effect. 6 Further applications of our methods In this section, as an example of the further application of our methods, we attempt to analyze parsing behaviors in domain adaptation from the viewpoints of error cause categories. In Hara et al. (2007), we proposed a method for adapting Enju to a target domain, and then succeeded in improving the parser performance for the GENIA corpus (Kim et al., 2003), a biomedical domain. Table 6 summarizes the parsing results for three types of settings respectively: parsing PTB with Enju (“Enju for PTB”), parsing GENIA with Enju (“Enju for GENIA”), and parsing GENIA with the adapted model (“Adapted for GENIA”). We then analyzed the performance transition among these settings from the viewpoint of the cause categories given in Section 4.1 (Table 7). In order to compare the error frequencies among different settings, we took the percentage of target errors in all of the evaluated triplets. The signed values between the two settings show how much the error</context>
</contexts>
<marker>Kim, Ohta, Teteisi, Tsujii, 2003</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun’ichi Tsujii. 2003. GENIA corpus - a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl. 1):i180–i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert Macintyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="22821" citStr="Marcus et al., 1994" startWordPosition="3740" endWordPosition="3743">uctural errors] To-infinitive for 120 22 modifier/argument of verb Subject for passive sentence/not 8 3 [Others] Comma 444 372 Relative clause attachment 102 38 Unclassified 2,631 Total 4,709 I Table 3: Errors classified into cause categories (c) shows what the resultant network looks like. An inter-dependent error group of 1, 2, 3 and 4 is recognized by [Step 3] and represented as a single node. Error 5 is propagated to this node in the final network. 5 Experiments We applied our methods to the analyses of actual errors produced by Enju. This version of Enju was trained on the Penn Treebank (Marcus et al., 1994) Section 2-21. 5.1 Observation of identified cause categories We first parsed sentences in PTB Section 22, and based on the observation of errors, we defined the patterns in Section 4. We then parsed sentences in Section 0. The errors in Section 0 were mapped to error cause categories by the pattern rules created for Section 22. Table 3 summarizes the distribution across the causes of errors. The left and right numbers in the table show the number of erroneous triplets classified into the categories and the frequency of the patterns matched, respectively. The table shows that, with the 14 patt</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, Macintyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert Macintyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>122--131</pages>
<contexts>
<context position="40288" citStr="McDonald and Nivre (2007)" startWordPosition="6589" endWordPosition="6592">for those categories as the parser originally obtained in PTB. We could thus capture some biases of cause categories which occur in domain transition or in domain adaptation, which would not be clarified by F-score evaluation methods. With interdependencies given by the method described in Section 4.2, the above analysis would be useful for effectively exploring further adaptation. 7 Related works Although there have been many researchers who analyzed errors in their own systems in the experiments, there has been little research which focused on error analysis itself. In the field of parsing, McDonald and Nivre (2007) compared parsing errors between graphbased and transition-based parsers. They considered accuracy transitions from various points of view, and the obtained statistical data suggested that error propagation seemed to occur in the graph structures of parsing outputs. Our research proceeded one step further and attempted to reveal the nature of the propagations. In examining the combination of the two types of parsing, they utilized approaches similar to our method for capturing inter-dependencies of errors. They allowed a parser to give only structures produced by the parsers and utilized the i</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 122–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>83--90</pages>
<contexts>
<context position="3779" citStr="Miyao and Tsujii, 2005" startWordPosition="591" endWordPosition="595">hallow parsing is followed by another component for semantic label assignment. In order to address these two issues, we propose two methods in this paper. One is to recognize cause categories of errors and the other is to capture inter-dependencies among errors. The former method defines various patterns of errors to identify categories of error causes. The latter method re-parses a sentence with a single target error corrected, and regards the errors which are corrected in re-parse as errors dependent on the target. Although these two methods are implemented for a specific parser using HPSG (Miyao and Tsujii, 2005; Ninomiya et al., 2006), the same ideas can be applied to any type of parsing models. 180 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 180–191, Paris, October 2009. c�2009 Association for Computational Linguistics Sentence: John has come Predicative event 1: Predicate-argument relations Figure 1: Predicate-argument relations Figure 2: Representation of predicate-argument relations has : aux_2args come : verb_1arg In the following, Section 2 introduces a parser and its evaluation metrics, Section 3 illustrates difficulties in analyzing parsing errors b</context>
<context position="8600" citStr="Miyao and Tsujii (2005)" startWordPosition="1363" endWordPosition="1366">RG1 ARG2 ARG1 ARG2 ARG1 ARG2 ARG1 ARG2 ARG2 ARG1 ARG2 Compare ARG1 ARG2 ARG1 ARG1 ARG2 Analysis 1: (Possible) ARG1 Analysis 2: (Impossible) Error: ARG1 They completed the sale of it to him for $1,000 ARG1 ARG1 They completed the sale of it to him for $1,000 Figure 4: Sketch of error propagation ARG1 They completed the sale of it to him for $1,000 Can each error occur independently? ARG1 ARG1 ARG1 ARG1 ARG1 ARG1 Conflict ARG1 Figure 3: An example of parsing performance evaluations pendencies but also many dependencies which are essentially syntactic in nature. Figure 1 shows an example used in Miyao and Tsujii (2005) and Ninomiya et al. (2006). This example shows predicate-argument relations for “John has come.” There are two predicates in this sentence, “has” and ”come”. The word “has”, which is used as an auxiliary verb, takes two words, “John” and “come”, as its arguments, and therefore two triplets of predicateargument relation, &lt;has ARG1 John&gt; and &lt;has ARG2 come&gt;. As for the predicative word “come”, we have one triplet &lt;come ARG1 John&gt;. Note that, in this HPSG analysis, the auxiliary verb “has” is analyzed in such a way that it takes one NP as subject and one VP as complement, and that the subject of</context>
<context position="12671" citStr="Miyao and Tsujii, 2005" startWordPosition="2061" endWordPosition="2064"> a wrong decision on the antecedent of a relative clause results in a wrong triplet of the predicate in the embedded clause with the antecedent. That is, the two erroneous triplets, one of the “ARG1” of “which” and the other of the “ARG2” of “read,” were caused by a single wrong decision of the antecedent of a relative clause. Such a propagation of errors can be even more complicated, for example, when the predicate in the relative clause is a control verb. In the following section we propose two methods for analyzing errors. Although both methods are implemented for the specific parser Enju (Miyao and Tsujii, 2005; Ninomiya et al., 2006), the same ideas can be implemented for any parsing model. 4 Methods for effective error analysis 4.1 Recognizing categories of error causes While the Enju parser produces rich feature structures as output, the performance is evaluated by the F-value in terms of basic units of predicateargment structure. As we illustrated in Section 2, the basic unit is a triplet in the following form. &lt;Predicate:PredicateType, ArgumentLabel, Argument&gt; We illustrated in Section 2 how we can identify errors in PP-attachment simply by examining a (Patterns of correct answer and parser out</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL), pages 83–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Takuya Matsuzaki</author>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Extremely lexicalized models for accurate and fast HPSG parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>155--163</pages>
<contexts>
<context position="3803" citStr="Ninomiya et al., 2006" startWordPosition="596" endWordPosition="599">ed by another component for semantic label assignment. In order to address these two issues, we propose two methods in this paper. One is to recognize cause categories of errors and the other is to capture inter-dependencies among errors. The former method defines various patterns of errors to identify categories of error causes. The latter method re-parses a sentence with a single target error corrected, and regards the errors which are corrected in re-parse as errors dependent on the target. Although these two methods are implemented for a specific parser using HPSG (Miyao and Tsujii, 2005; Ninomiya et al., 2006), the same ideas can be applied to any type of parsing models. 180 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 180–191, Paris, October 2009. c�2009 Association for Computational Linguistics Sentence: John has come Predicative event 1: Predicate-argument relations Figure 1: Predicate-argument relations Figure 2: Representation of predicate-argument relations has : aux_2args come : verb_1arg In the following, Section 2 introduces a parser and its evaluation metrics, Section 3 illustrates difficulties in analyzing parsing errors based on common evaluatio</context>
<context position="8627" citStr="Ninomiya et al. (2006)" startWordPosition="1368" endWordPosition="1372"> ARG1 ARG2 ARG2 ARG1 ARG2 Compare ARG1 ARG2 ARG1 ARG1 ARG2 Analysis 1: (Possible) ARG1 Analysis 2: (Impossible) Error: ARG1 They completed the sale of it to him for $1,000 ARG1 ARG1 They completed the sale of it to him for $1,000 Figure 4: Sketch of error propagation ARG1 They completed the sale of it to him for $1,000 Can each error occur independently? ARG1 ARG1 ARG1 ARG1 ARG1 ARG1 Conflict ARG1 Figure 3: An example of parsing performance evaluations pendencies but also many dependencies which are essentially syntactic in nature. Figure 1 shows an example used in Miyao and Tsujii (2005) and Ninomiya et al. (2006). This example shows predicate-argument relations for “John has come.” There are two predicates in this sentence, “has” and ”come”. The word “has”, which is used as an auxiliary verb, takes two words, “John” and “come”, as its arguments, and therefore two triplets of predicateargument relation, &lt;has ARG1 John&gt; and &lt;has ARG2 come&gt;. As for the predicative word “come”, we have one triplet &lt;come ARG1 John&gt;. Note that, in this HPSG analysis, the auxiliary verb “has” is analyzed in such a way that it takes one NP as subject and one VP as complement, and that the subject of the auxiliary verb is shar</context>
<context position="12695" citStr="Ninomiya et al., 2006" startWordPosition="2065" endWordPosition="2068"> antecedent of a relative clause results in a wrong triplet of the predicate in the embedded clause with the antecedent. That is, the two erroneous triplets, one of the “ARG1” of “which” and the other of the “ARG2” of “read,” were caused by a single wrong decision of the antecedent of a relative clause. Such a propagation of errors can be even more complicated, for example, when the predicate in the relative clause is a control verb. In the following section we propose two methods for analyzing errors. Although both methods are implemented for the specific parser Enju (Miyao and Tsujii, 2005; Ninomiya et al., 2006), the same ideas can be implemented for any parsing model. 4 Methods for effective error analysis 4.1 Recognizing categories of error causes While the Enju parser produces rich feature structures as output, the performance is evaluated by the F-value in terms of basic units of predicateargment structure. As we illustrated in Section 2, the basic unit is a triplet in the following form. &lt;Predicate:PredicateType, ArgumentLabel, Argument&gt; We illustrated in Section 2 how we can identify errors in PP-attachment simply by examining a (Patterns of correct answer and parser output can be interchanged)</context>
</contexts>
<marker>Ninomiya, Matsuzaki, Tsuruoka, Miyao, Tsujii, 2006</marker>
<rawString>Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2006. Extremely lexicalized models for accurate and fast HPSG parsing. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 155–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl J Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="2841" citStr="Pollard and Sag, 1994" startWordPosition="440" endWordPosition="443">in a parsing model. As a result, the improvement process takes the form of time consuming trial-error cycles. Once grammar developers know the actual distribution of errors across different categories such as PP-attachment, complement/adjunct distinction, gerund/participle distinction, etc., they can think of focused and systematic improvement of a parsing model. Another problem of the F-value in terms of uniform units is that it does not take interdependencies among errors into consideration. In particular, for parsers based on grammar formalisms such as LFG (Kaplan and Bresnan, 1995), HPSG (Pollard and Sag, 1994), or CCG (Steedman, 2000), units (eg. single predicate-argument links) are inter-related through hierarchical structures and structure sharing assumed by these formalisms. Single errors are inherently propagated to other sets of errors. This is also the case, though to a lesser extent, for parsing models in which shallow parsing is followed by another component for semantic label assignment. In order to address these two issues, we propose two methods in this paper. One is to recognize cause categories of errors and the other is to capture inter-dependencies among errors. The former method def</context>
<context position="5788" citStr="Pollard and Sag, 1994" startWordPosition="900" endWordPosition="903">ng the performance of a parser which produces structural representations deeper than surface constituent structures. Such a parser is called a “deep parser.” In many deep parsers, the output structure is defined by a linguistics-based grammar framework such as CFG, CCG (Steedman, 2000), LFG (Kaplan and Bresnan, 1995) or HPSG Abbr. Full Abbr. Full aux auxiliary conj conjunction prep prepositional lgs logical subject verb verb app apposition coord coordination relative relative det determiner Narg(s) takes N arguments adj adjunction mod modifies a word Table 1: Descriptions for predicate types (Pollard and Sag, 1994). Alternatively, some deep parsing models assume staged processing in which a stage of shallow parsing is followed by a stage of semantic role labeling, which assigns labels indicating semantic relationships between predicates and their arguments. In either case, we assume a parser to produce a single “deep” structural representation for a given sentence, which is chosen from a set of possible interpretations as the most probable one by a disambiguation model. For evaluation of the performance of a parser, various metrics have been introduced according to the structure captured by a given gram</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>THE MIT Press.</publisher>
<contexts>
<context position="2866" citStr="Steedman, 2000" startWordPosition="446" endWordPosition="448"> the improvement process takes the form of time consuming trial-error cycles. Once grammar developers know the actual distribution of errors across different categories such as PP-attachment, complement/adjunct distinction, gerund/participle distinction, etc., they can think of focused and systematic improvement of a parsing model. Another problem of the F-value in terms of uniform units is that it does not take interdependencies among errors into consideration. In particular, for parsers based on grammar formalisms such as LFG (Kaplan and Bresnan, 1995), HPSG (Pollard and Sag, 1994), or CCG (Steedman, 2000), units (eg. single predicate-argument links) are inter-related through hierarchical structures and structure sharing assumed by these formalisms. Single errors are inherently propagated to other sets of errors. This is also the case, though to a lesser extent, for parsing models in which shallow parsing is followed by another component for semantic label assignment. In order to address these two issues, we propose two methods in this paper. One is to recognize cause categories of errors and the other is to capture inter-dependencies among errors. The former method defines various patterns of </context>
<context position="5452" citStr="Steedman, 2000" startWordPosition="852" endWordPosition="853">tences in terms of structures derived from syntactic or in some cases semantic viewpoints, and structures constructed as a result are used as essential information for various tasks of natural language processing such as information extraction, machine translation, and so on. In this paper, we address issues involved in improving the performance of a parser which produces structural representations deeper than surface constituent structures. Such a parser is called a “deep parser.” In many deep parsers, the output structure is defined by a linguistics-based grammar framework such as CFG, CCG (Steedman, 2000), LFG (Kaplan and Bresnan, 1995) or HPSG Abbr. Full Abbr. Full aux auxiliary conj conjunction prep prepositional lgs logical subject verb verb app apposition coord coordination relative relative det determiner Narg(s) takes N arguments adj adjunction mod modifies a word Table 1: Descriptions for predicate types (Pollard and Sag, 1994). Alternatively, some deep parsing models assume staged processing in which a stage of shallow parsing is followed by a stage of semantic role labeling, which assigns labels indicating semantic relationships between predicates and their arguments. In either case, </context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. THE MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>