<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015281">
<title confidence="0.973498">
Vocabulary Choice as an Indicator of Perspective
</title>
<author confidence="0.949049">
Beata Beigman Klebanov, Eyal Beigman, Daniel Diermeier
</author>
<affiliation confidence="0.958282">
Northwestern University and Washington University in St. Louis
</affiliation>
<email confidence="0.994128">
beata,d-diermeier@northwestern.edu,beigman@wustl.edu
</email>
<sectionHeader confidence="0.993788" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999928357142857">
We establish the following characteris-
tics of the task of perspective classifi-
cation: (a) using term frequencies in a
document does not improve classification
achieved with absence/presence features;
(b) for datasets allowing the relevant com-
parisons, a small number of top features is
found to be as effective as the full feature
set and indispensable for the best achieved
performance, testifying to the existence
of perspective-specific keywords. We re-
late our findings to research on word fre-
quency distributions and to discourse ana-
lytic studies of perspective.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965">
We address the task of perspective classification.
Apart from the spatial sense not considered here,
perspective can refer to an agent’s role (doctor vs
patient in a dialogue), or understood as “a par-
ticular way of thinking about something, espe-
cially one that is influenced by one’s beliefs or
experiences,” stressing the manifestation of one’s
broader perspective in some specific issue, or “the
state of one’s ideas, the facts known to one, etc.,
in having a meaningful interrelationship,” stress-
ing the meaningful connectedness of one’s stances
and pronouncements on possibly different issues.1
Accordingly, one can talk about, say, opinion
on a particular proposed legislation on abortion
within pro-choice or pro-life perspectives; in this
case, perspective essentially boils down to opi-
nion in a particular debate. Holding the issue con-
stant but relaxing the requirement of a debate on a
specific document, we can consider writings from
pro- and con- perspective, in, for example, the
death penalty controversy over a course of a period
of time. Relaxing the issue specificity somewhat,
</bodyText>
<footnote confidence="0.530025">
1Google English Dictionary, Dictionary.com
</footnote>
<bodyText confidence="0.999859517241379">
one can talk about perspectives of people on two
sides of a conflict; this is not opposition or sup-
port for any particular proposal, but ideas about
a highly related cluster of issues, such as Israeli
and Palestinian perspectives on the conflict in all
its manifestations. Zooming out even further, one
can talk about perspectives due to certain life con-
tingencies, such as being born and raised in a par-
ticular culture, region, religion, or political tradi-
tion, such perspectives manifesting themselves in
certain patterns of discourse on a wide variety of
issues, for example, views on political issues in the
Middle East from Arab vs Western observers.
In this article, we consider perspective at all
the four levels of abstraction. We apply the same
types of models to all, in order to discover any
common properties of perspective classification.
We contrast it with text categorization and with
opinion classification by employing models rou-
tinely used for such tasks. Specifically, we con-
sider models that use term frequencies as features
(usually found to be superior for text categoriza-
tion) and models that use term absence/presence
(usually found to be superior for opinion classi-
fication). We motivate our hypothesis that pre-
sence/absence features would be as good as or
better than frequencies, and test it experimentally.
Secondly, we investigate the question of feature
redundancy often observed in text categorization.
</bodyText>
<sectionHeader confidence="0.965014" genericHeader="method">
2 Vocabulary Selection
</sectionHeader>
<bodyText confidence="0.9998961">
A line of inquiry going back at least to Zipf strives
to characterize word frequency distributions in
texts and corpora; see Baayen (2001) for a sur-
vey. One of the findings in this literature is that
a multinomial (called “urn model” by Baayen)
is not a good model for word frequency distri-
butions. Among the many proposed remedies
(Baayen, 2001; Jansche, 2003; Baroni and Evert,
2007; Bhat and Sproat, 2009), we would like to
draw attention to the following insight articulated
</bodyText>
<page confidence="0.980303">
253
</page>
<note confidence="0.5061025">
Proceedings of the ACL 2010 Conference Short Papers, pages 253–257,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999910880952381">
most clearly in Jansche (2003). Estimation is im-
proved if texts are construed as being generated by
two processes, one choosing which words would
appear at all in the text, and then, for words that
have been chosen to appear, how many times they
would in fact appear. Jansche (2003) describes a
two-stage generation process: (1) Toss a z-biased
coin; if it comes up heads, generate 0; if it comes
up tails, (2) generate according to F(B), where
F(B) is a negative binomial distribution and z is a
parameter controlling the extent of zero-inflation.
The postulation of two separate processes is
effective for predicting word frequencies, but is
there any meaning to the two processes? The first
process of deciding on the vocabulary, or word
types, for the text – what is its function? Jansche
(2003) suggests that the zero-inflation component
takes care of the multitude of vocabulary words
that are not “on topic” for the given text, including
taboo words, technical jargon, proper names. This
implies that words that are chosen to appear are
all “on topic”. Indeed, text segmentation studies
show that tracing recurrence of words in a text
permits topical segmentation (Hearst, 1997; Hoey,
1991). Yet, if a person compares abortion to infan-
ticide – are we content with describing this word
as being merely “on topic,” that is, having a certain
probability of occurrence once the topic of abor-
tion comes up? In fact, it is only likely to occur
if the speaker holds a pro-life perspective, while a
pro-choicer would avoid this term.
We therefore hypothesize that the choice of vo-
cabulary is not only a matter of topic but also
of perspective, while word recurrence has mainly
to do with the topical composition of the text.
Therefore, tracing word frequencies is not going to
be effective for perspective classification beyond
noting the mere presence/absence of words, dif-
ferently from the findings in text categorization,
where frequency-based features usually do better
than boolean features for sufficiently large voca-
bulary sizes (McCallum and Nigam, 1998).
</bodyText>
<sectionHeader confidence="0.99726" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999636173076923">
Partial Birth Abortion (PBA) debates: We use
transcripts of the debates on Partial Birth Abor-
tion Ban Act on the floors of the US House and
Senate in 104-108 Congresses (1995-2003). Simi-
lar legislation was proposed multiple times, passed
the legislatures, and, after having initially been ve-
toed by President Clinton, was signed into law
by President Bush in 2003. We use data from
278 legislators, with 669 speeches in all. We
take only one speech per speaker per year; since
many serve multiple years, each speaker is repre-
sented with 1 to 5 speeches. We perform 10-fold
cross-validation splitting by speakers, so that all
speeches by the same speaker are assigned to the
same fold and testing is always inter-speaker.
When deriving the label for perspective, it is im-
portant to differentiate between a particular leg-
islation and a pro-choice / pro-life perspective.
A pro-choice person might still support the bill:
“I am pro-choice, but believe late-term abortions
are wrong. Abortion is a very personal decision
and a woman’s right to choose whether to ter-
minate a pregnancy subject to the restrictions of
Roe v. Wade must be protected. In my judgment,
however, the use of this particular procedure can-
not be justified.” (Rep. Shays, R-CT, 2003). To
avoid inconsistency between vote and perspective,
we use data from pro-choice and pro-life non-
governmental organizations, NARAL and NRLC,
that track legislators’ votes on abortion-related
bills, showing the percentage of times a legislator
supported the side the organization deems consis-
tent with its perspective. We removed 22 legisla-
tors with a mixed record, that is, those who gave
20-60% support to one of the positions.2
Death Penalty (DP) blogs: We use University
of Maryland Death Penalty Corpus (Greene and
Resnik, 2009) of 1085 texts from a number of pro-
and anti-death penalty websites. We report 4-fold
cross-validation (DP-4) using the folds in Greene
and Resnik (2009), where training and testing data
come from different websites for each of the sides,
as well as 10-fold cross-validation performance on
the entire corpus, irrespective of the site.3
Bitter Lemons (BL): We use the GUEST part
of the BitterLemons corpus (Lin et al., 2006), con-
taining 296 articles published in 2001-2005 on
http://www.bitterlemons.org by more than 200 dif-
ferent Israeli and Palestinian writers on issues re-
lated to the conflict.
Bitter Lemons International (BL-I): We col-
lected 150 documents each by a different per-
</bodyText>
<footnote confidence="0.998360625">
2Ratings are from: http://www.OnTheIssues.org/. We fur-
ther excluded data from Rep. James Moran, D-VA, as he
changed his vote over the years. For legislators rated by nei-
ther NRLC nor NARAL, we assumed the vote aligns with the
perspective.
3The 10-fold setting yields almost perfect performance
likely due to site-specific features beyond perspective per se,
hence we do not use this setting in subsequent experiments.
</footnote>
<page confidence="0.996252">
254
</page>
<bodyText confidence="0.999677083333333">
son from either Arab or Western perspectives
on Middle Eastern affairs in 2003-2009 from
http://www.bitterlemons-international.org/. The
writers and interviewees on this site are usually
former diplomats or government officials, aca-
demics, journalists, media and political analysts.4
The specific issues cover a broad spectrum, includ-
ing public life, politics, wars and conflicts, educa-
tion, trade relations in and between countries like
Lebanon, Jordan, Iraq, Egypt, Yemen, Morocco,
Saudi Arabia, as well as their relations with the
US and members of the European Union.
</bodyText>
<subsectionHeader confidence="0.999839">
3.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.996741142857143">
We are interested in perspective manifestations
using common English vocabulary. To avoid the
possibility that artifacts such as names of senators
or states drive the classification, we use as features
words that contain only lowercase letters, possibly
hyphenated. No stemming is performed, and no
stopwords are excluded.5
</bodyText>
<tableCaption confidence="0.99774">
Table 1: Summary of corpora
</tableCaption>
<table confidence="0.9996158">
Data #Docs #Features # CV folds
PBA 669 9.8 K 10
BL 296 10 K 10
BL-I 150 9 K 10
DP 1085 25 K 4
</table>
<sectionHeader confidence="0.992845" genericHeader="method">
4 Models
</sectionHeader>
<bodyText confidence="0.999918933333333">
For generative models, we use two versions
of Naive Bayes models termed multi-variate
Bernoulli (here, NB-BOOL) and multinomial (here,
NB-COUNT), respectively, in McCallum and
Nigam (1998) study of event models for text cate-
gorization. The first records presence/absence of a
word in a text, while the second records the num-
ber of occurrences. McCallum and Nigam (1998)
found NB-COUNT to do better than NB-BOOL for
sufficiently large vocabulary sizes for text catego-
rization by topic. For discriminative models, we
use linear SVM, with presence-absence, norma-
lized frequency, and tfidf feature weighting. Both
types of models are commonly used for text clas-
sification tasks. For example, Lin et al. (2006) use
</bodyText>
<footnote confidence="0.845924166666666">
4We excluded Israeli, Turkish, Iranian, Pakistani writers
as not clearly representing either perspective.
5We additionally removed words containing support, op-
pos, sustain, overrid from the PBA data, in order not to in-
flate the performance on perspective classification due to the
explicit reference to the upcoming vote.
</footnote>
<bodyText confidence="0.9677622">
NB-COUNT and SVM-NORMF for perspective clas-
sification; Pang et al. (2002) consider most and
Yu et al. (2008) all of the above for related tasks
of movie review and political party classification.
We use SVM&amp;quot;ght (Joachims, 1999) for SVM and
WEKA toolkit (Witten and Frank, 2005; Hall et
al., 2009) for both version of Naive Bayes. Param-
eter optimization for all SVM models is performed
using grid search on the training data separately
for each partition into train and test data.6
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.962239111111111">
Table 2 summarizes the cross-validation results for
the four datasets discussed above. Notably, the
SVM-BOOL model is either the best or not signif-
icantly different from the best performing model,
although the competitors use more detailed textual
information, namely, the count of each word’s ap-
pearance in the text, either raw (NB-COUNT), nor-
malized (SVM-NORMF), or combined with docu-
ment frequency (SVM-TFIDF).
</bodyText>
<tableCaption confidence="0.894199333333333">
Table 2: Classification accuracy. Scores sig-
nificantly different from the best performance
(p2t&lt;0.05 on paired t-test) are given an asterisk.
</tableCaption>
<table confidence="0.999527142857143">
Data NB SVM
BOOL COUNT BOOL NORMF TFIDF
PBA *0.93 0.96 0.96 0.96 0.97
DP-4 0.82 0.82 0.83 0.82 0.727
DP-10 *0.88 *0.93 0.98 *0.97 *0.97
BL 0.89 0.88 0.89 0.86 0.84
BL-I 0.68 0.66 0.73 0.65 0.65
</table>
<bodyText confidence="0.993432117647059">
We conclude that there is no evidence for the
relevance of the frequency composition of the
text for perspective classification, for all levels of
venue- and topic-control, from the tightest (PBA
debates) to the loosest (Western vs Arab authors
on Middle Eastern affairs). This result is a clear
indication that perspective classification is quite
different from text categorization by topic, where
count-based features usually perform better than
boolean features. On the other hand, we have not
6Parameter c controlling the trade-off between errors
on training data and margin is−optimized for all datasets,
with the grid c = {10−6, 105, ... , 105}. On the DP
data parameter j controlling penalties for misclassification
of positive and negative cases is optimized as well (j =
{10−2, 10−1, ... , 102}), since datasets are unbalanced (for
example, there is a fold with 27%-73% split).
</bodyText>
<footnote confidence="0.99711775">
7Here SVM-TFIDF is doing somewhat better than SVM-
BOOL on one of the folds and much worse on two other folds;
paired t-test with just 4 pairs of observations does not detect
a significant difference.
</footnote>
<page confidence="0.997461">
255
</page>
<bodyText confidence="0.999960166666667">
observed that boolean features are reliably better
than count-based features, as reported for the sen-
timent classification task in the movie review do-
main (Pang et al., 2002).
We note the low performance on BL-I, which
could testify to a low degree of lexical consolida-
tion in the Arab vs Western perspectives (more on
this below). It is also possible that the small size of
BL-I leads to overfitting and low accuracies. How-
ever, PBA subset with only 151 items (only 2002
and 2003 speeches) is still 96% classifiable, so size
alone does not explain low BL-I performance.
</bodyText>
<sectionHeader confidence="0.567543" genericHeader="conclusions">
6 Consolidation of perspective
</sectionHeader>
<bodyText confidence="0.987250761904762">
We explore feature redundancy in perspective
classification.We first investigate retention of only
N best features, then elimination thereof. As a
proxy of feature quality, we use the weight as-
signed to the feature by the SVM-BOOL model
based on the training data. Thus, to get the per-
formance with N best features, we take the 2
highest and lowest weight features, for the posi-
tive and negative classes, respectively, and retrain
SVM-BOOL with these features only.8
Table 3: Consolidation of perspective. Nbest
shows the smallest N and its proportion out of
all features for which the performance of SVM-
BOOL with only the best N features is not sig-
nificantly inferior (pit&gt;0.1) to that of the full
feature set. No-Nbest shows the largest num-
ber N for which a model without N best fea-
tures is not significantly inferior to the full model.
N={50,100,150,..., 1000}; for DP and BL-I, ad-
ditionally N={1050,1100,..., 1500}; for PBA, ad-
ditionally N={10, 20, 30, 40}.
</bodyText>
<table confidence="0.999202333333333">
Data Nbest No-Nbest
N % N %
PBA 250 2.6% 10 &lt;1%
BL 500 4.9% 100 &lt;1%
DP 100 &lt;1% 1250 5.2%
BL-I 200 2.2% 950 11%
</table>
<bodyText confidence="0.999769857142857">
We observe that it is generally sufficient to use
a small percentage of the available words to ob-
tain the same classification accuracy as with the
full feature set, even in high-accuracy cases such
as PBA and BL. The effectiveness of a small
subset of features is consistent with the observa-
tion in the discourse analysis studies that rivals
</bodyText>
<footnote confidence="0.5250915">
8We experimented with the mutual information based fea-
ture selection as well, with generally worse results.
</footnote>
<bodyText confidence="0.999987862745098">
in long-lasting controversies tend to consolidate
their vocabulary and signal their perspective with
certain stigma words and banner words, that is,
specific keywords used by a discourse commu-
nity to implicate adversaries and to create sym-
pathy with own perspective, respectively (Teubert,
2001). Thus, in abortion debates, using infanti-
cide as a synonym for abortion is a pro-life stigma.
Note that this does not mean the rest of the fea-
tures are not informative for classification, only
that they are redundant with respect to a small per-
centage of top weight features.
When N best features are eliminated, perfor-
mance goes down significantly with even smaller
N for PBA and BL datasets. Thus, top features
are not only effective, they are also crucial for ac-
curate classification, as their discrimination capa-
city is not replicated by any of the other vocabu-
lary words. This finding is consistent with Lin
and Hauptmann (2006) study of perspective vs
topic classification: While topical differences be-
tween two corpora are manifested in difference in
distributions of great many words, they observed
little perspective-based variation in distributions
of most words, apart from certain words that are
preferentially used by adherents of one or the other
perspective on the given topic.
For DP and BL-I datasets, the results seem
to suggest perspectives with more diffused key-
word distribution (No-NBest figures are higher).
We note, however, that feature redundancy exper-
iments are confounded in these cases by either a
low power of the paired t-test with only 4 pairs
(DP) or by a high variance in performance among
the 10 folds (BL-I), both of which lead to nume-
rically large discrepancy in performance that is not
deemed significant, making it easy to “match” the
full set performance with small-N best features as
well as without large-N best features. Better com-
parisons are needed in order to verify the hypo-
thesis of low consolidation.
In future work, we plan to experiment with ad-
ditional features. For example, Greene and Resnik
(2009) reported higher classification accuracies
for the DP-4 data using syntactic frames in which
a selected group of words appeared, rather than
mere presence/absence of the words. Another di-
rection is exploring words as members of seman-
tic fields – while word use might be insufficiently
consistent within a perspective, selection of a se-
mantic domain might show better consistency.
</bodyText>
<page confidence="0.996732">
256
</page>
<sectionHeader confidence="0.988225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99961215625">
Herald Baayen. 2001. Word frequency distributions.
Dordrecht: Kluwer.
Marco Baroni and Stefan Evert. 2007. Words
and Echoes: Assessing and Mitigating the Non-
Randomness Problem in Word Frequency Distribu-
tion Modeling. In Proceedings of the ACL, pages
904–911, Prague, Czech Republic.
Suma Bhat and Richard Sproat. 2009. Knowing the
Unseen: Estimating Vocabulary Size over Unseen
Samples. In Proceedings of the ACL, pages 109–
117, Suntec, Singapore, August.
Stephan Greene and Philip Resnik. 2009. More
than Words: Syntactic Packaging and Implicit Sen-
timent. In Proceedings of HLT-NAACL, pages 503–
511, Boulder, CO, June.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringe, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33–64.
Michael Hoey. 1991. Patterns of Lexis in Text. Oxford
University Press.
Martin Jansche. 2003. Parametric Models of Linguis-
tic Count Data. In Proceedings of the ACL, pages
288–295, Sapporo, Japan, July.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Wei-Hao Lin and Alexander Hauptmann. 2006. Are
these documents written from different perspec-
tives? A test of different perspectives based on sta-
tistical distribution divergence. In Proceedings of
the ACL, pages 1057–1064, Morristown, NJ, USA.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? Identifying perspectives at the document and
sentence levels. In Proceedings of CoNLL, pages
109–116, Morristown, NJ, USA.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text clas-
sification. In Proceedings of AAAI-98 Workshop
on Learning for Text Categorization, pages 41–48,
Madison, WI, July.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In Proceedings of
EMNLP, Philadelphia, PA, July.
Wolfgang Teubert. 2001. A Province of a Federal
Superstate, Ruled by an Unelected Bureaucracy –
Keywords of the Euro-Sceptic Discourse in Britain.
In Andreas Musolff, Colin Good, Petra Points, and
Ruth Wittlinger, editors, Attitudes towards Europe:
Language in the unification process, pages 45–86.
Ashgate Publishing Ltd, Hants, England.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, 2 edition.
Bei Yu, Stefan Kaufmann, and Daniel Diermeier.
2008. Classifying party affiliation from political
speech. Journal of Information Technology and Pol-
itics, 5(1):33–48.
</reference>
<page confidence="0.997183">
257
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.932654">
<title confidence="0.997651">Vocabulary Choice as an Indicator of Perspective</title>
<author confidence="0.982623">Beata Beigman Klebanov</author>
<author confidence="0.982623">Eyal Beigman</author>
<author confidence="0.982623">Daniel Diermeier</author>
<affiliation confidence="0.964318">Northwestern University and Washington University in St. Louis</affiliation>
<abstract confidence="0.998414">We establish the following characteristics of the task of perspective classification: (a) using term frequencies in a document does not improve classification achieved with absence/presence features; (b) for datasets allowing the relevant comparisons, a small number of top features is found to be as effective as the full feature set and indispensable for the best achieved performance, testifying to the existence of perspective-specific keywords. We relate our findings to research on word frequency distributions and to discourse analytic studies of perspective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Herald Baayen</author>
</authors>
<title>Word frequency distributions.</title>
<date>2001</date>
<publisher>Kluwer.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="3544" citStr="Baayen (2001)" startWordPosition="539" endWordPosition="540">ally, we consider models that use term frequencies as features (usually found to be superior for text categorization) and models that use term absence/presence (usually found to be superior for opinion classification). We motivate our hypothesis that presence/absence features would be as good as or better than frequencies, and test it experimentally. Secondly, we investigate the question of feature redundancy often observed in text categorization. 2 Vocabulary Selection A line of inquiry going back at least to Zipf strives to characterize word frequency distributions in texts and corpora; see Baayen (2001) for a survey. One of the findings in this literature is that a multinomial (called “urn model” by Baayen) is not a good model for word frequency distributions. Among the many proposed remedies (Baayen, 2001; Jansche, 2003; Baroni and Evert, 2007; Bhat and Sproat, 2009), we would like to draw attention to the following insight articulated 253 Proceedings of the ACL 2010 Conference Short Papers, pages 253–257, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics most clearly in Jansche (2003). Estimation is improved if texts are construed as being generated by two </context>
</contexts>
<marker>Baayen, 2001</marker>
<rawString>Herald Baayen. 2001. Word frequency distributions. Dordrecht: Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Stefan Evert</author>
</authors>
<title>Words and Echoes: Assessing and Mitigating the NonRandomness Problem in Word Frequency Distribution Modeling.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>904--911</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3790" citStr="Baroni and Evert, 2007" startWordPosition="580" endWordPosition="583">pothesis that presence/absence features would be as good as or better than frequencies, and test it experimentally. Secondly, we investigate the question of feature redundancy often observed in text categorization. 2 Vocabulary Selection A line of inquiry going back at least to Zipf strives to characterize word frequency distributions in texts and corpora; see Baayen (2001) for a survey. One of the findings in this literature is that a multinomial (called “urn model” by Baayen) is not a good model for word frequency distributions. Among the many proposed remedies (Baayen, 2001; Jansche, 2003; Baroni and Evert, 2007; Bhat and Sproat, 2009), we would like to draw attention to the following insight articulated 253 Proceedings of the ACL 2010 Conference Short Papers, pages 253–257, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics most clearly in Jansche (2003). Estimation is improved if texts are construed as being generated by two processes, one choosing which words would appear at all in the text, and then, for words that have been chosen to appear, how many times they would in fact appear. Jansche (2003) describes a two-stage generation process: (1) Toss a z-biased coin;</context>
</contexts>
<marker>Baroni, Evert, 2007</marker>
<rawString>Marco Baroni and Stefan Evert. 2007. Words and Echoes: Assessing and Mitigating the NonRandomness Problem in Word Frequency Distribution Modeling. In Proceedings of the ACL, pages 904–911, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suma Bhat</author>
<author>Richard Sproat</author>
</authors>
<title>Knowing the Unseen: Estimating Vocabulary Size over Unseen Samples.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>109--117</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="3814" citStr="Bhat and Sproat, 2009" startWordPosition="584" endWordPosition="587">bsence features would be as good as or better than frequencies, and test it experimentally. Secondly, we investigate the question of feature redundancy often observed in text categorization. 2 Vocabulary Selection A line of inquiry going back at least to Zipf strives to characterize word frequency distributions in texts and corpora; see Baayen (2001) for a survey. One of the findings in this literature is that a multinomial (called “urn model” by Baayen) is not a good model for word frequency distributions. Among the many proposed remedies (Baayen, 2001; Jansche, 2003; Baroni and Evert, 2007; Bhat and Sproat, 2009), we would like to draw attention to the following insight articulated 253 Proceedings of the ACL 2010 Conference Short Papers, pages 253–257, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics most clearly in Jansche (2003). Estimation is improved if texts are construed as being generated by two processes, one choosing which words would appear at all in the text, and then, for words that have been chosen to appear, how many times they would in fact appear. Jansche (2003) describes a two-stage generation process: (1) Toss a z-biased coin; if it comes up heads, g</context>
</contexts>
<marker>Bhat, Sproat, 2009</marker>
<rawString>Suma Bhat and Richard Sproat. 2009. Knowing the Unseen: Estimating Vocabulary Size over Unseen Samples. In Proceedings of the ACL, pages 109– 117, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>More than Words: Syntactic Packaging and Implicit Sentiment.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>503--511</pages>
<location>Boulder, CO,</location>
<contexts>
<context position="7894" citStr="Greene and Resnik, 2009" startWordPosition="1247" endWordPosition="1250">wever, the use of this particular procedure cannot be justified.” (Rep. Shays, R-CT, 2003). To avoid inconsistency between vote and perspective, we use data from pro-choice and pro-life nongovernmental organizations, NARAL and NRLC, that track legislators’ votes on abortion-related bills, showing the percentage of times a legislator supported the side the organization deems consistent with its perspective. We removed 22 legislators with a mixed record, that is, those who gave 20-60% support to one of the positions.2 Death Penalty (DP) blogs: We use University of Maryland Death Penalty Corpus (Greene and Resnik, 2009) of 1085 texts from a number of proand anti-death penalty websites. We report 4-fold cross-validation (DP-4) using the folds in Greene and Resnik (2009), where training and testing data come from different websites for each of the sides, as well as 10-fold cross-validation performance on the entire corpus, irrespective of the site.3 Bitter Lemons (BL): We use the GUEST part of the BitterLemons corpus (Lin et al., 2006), containing 296 articles published in 2001-2005 on http://www.bitterlemons.org by more than 200 different Israeli and Palestinian writers on issues related to the conflict. Bitt</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. More than Words: Syntactic Packaging and Implicit Sentiment. In Proceedings of HLT-NAACL, pages 503– 511, Boulder, CO, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringe</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="11373" citStr="Hall et al., 2009" startWordPosition="1793" endWordPosition="1796">4We excluded Israeli, Turkish, Iranian, Pakistani writers as not clearly representing either perspective. 5We additionally removed words containing support, oppos, sustain, overrid from the PBA data, in order not to inflate the performance on perspective classification due to the explicit reference to the upcoming vote. NB-COUNT and SVM-NORMF for perspective classification; Pang et al. (2002) consider most and Yu et al. (2008) all of the above for related tasks of movie review and political party classification. We use SVM&amp;quot;ght (Joachims, 1999) for SVM and WEKA toolkit (Witten and Frank, 2005; Hall et al., 2009) for both version of Naive Bayes. Parameter optimization for all SVM models is performed using grid search on the training data separately for each partition into train and test data.6 5 Results Table 2 summarizes the cross-validation results for the four datasets discussed above. Notably, the SVM-BOOL model is either the best or not significantly different from the best performing model, although the competitors use more detailed textual information, namely, the count of each word’s appearance in the text, either raw (NB-COUNT), normalized (SVM-NORMF), or combined with document frequency (SVM</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringe, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringe, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>TextTiling: Segmenting Text into Multi-Paragraph Subtopic Passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="5224" citStr="Hearst, 1997" startWordPosition="814" endWordPosition="815">separate processes is effective for predicting word frequencies, but is there any meaning to the two processes? The first process of deciding on the vocabulary, or word types, for the text – what is its function? Jansche (2003) suggests that the zero-inflation component takes care of the multitude of vocabulary words that are not “on topic” for the given text, including taboo words, technical jargon, proper names. This implies that words that are chosen to appear are all “on topic”. Indeed, text segmentation studies show that tracing recurrence of words in a text permits topical segmentation (Hearst, 1997; Hoey, 1991). Yet, if a person compares abortion to infanticide – are we content with describing this word as being merely “on topic,” that is, having a certain probability of occurrence once the topic of abortion comes up? In fact, it is only likely to occur if the speaker holds a pro-life perspective, while a pro-choicer would avoid this term. We therefore hypothesize that the choice of vocabulary is not only a matter of topic but also of perspective, while word recurrence has mainly to do with the topical composition of the text. Therefore, tracing word frequencies is not going to be effec</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti Hearst. 1997. TextTiling: Segmenting Text into Multi-Paragraph Subtopic Passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Hoey</author>
</authors>
<title>Patterns of Lexis in Text.</title>
<date>1991</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="5237" citStr="Hoey, 1991" startWordPosition="816" endWordPosition="817">sses is effective for predicting word frequencies, but is there any meaning to the two processes? The first process of deciding on the vocabulary, or word types, for the text – what is its function? Jansche (2003) suggests that the zero-inflation component takes care of the multitude of vocabulary words that are not “on topic” for the given text, including taboo words, technical jargon, proper names. This implies that words that are chosen to appear are all “on topic”. Indeed, text segmentation studies show that tracing recurrence of words in a text permits topical segmentation (Hearst, 1997; Hoey, 1991). Yet, if a person compares abortion to infanticide – are we content with describing this word as being merely “on topic,” that is, having a certain probability of occurrence once the topic of abortion comes up? In fact, it is only likely to occur if the speaker holds a pro-life perspective, while a pro-choicer would avoid this term. We therefore hypothesize that the choice of vocabulary is not only a matter of topic but also of perspective, while word recurrence has mainly to do with the topical composition of the text. Therefore, tracing word frequencies is not going to be effective for pers</context>
</contexts>
<marker>Hoey, 1991</marker>
<rawString>Michael Hoey. 1991. Patterns of Lexis in Text. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Jansche</author>
</authors>
<title>Parametric Models of Linguistic Count Data.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>288--295</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="3766" citStr="Jansche, 2003" startWordPosition="578" endWordPosition="579">motivate our hypothesis that presence/absence features would be as good as or better than frequencies, and test it experimentally. Secondly, we investigate the question of feature redundancy often observed in text categorization. 2 Vocabulary Selection A line of inquiry going back at least to Zipf strives to characterize word frequency distributions in texts and corpora; see Baayen (2001) for a survey. One of the findings in this literature is that a multinomial (called “urn model” by Baayen) is not a good model for word frequency distributions. Among the many proposed remedies (Baayen, 2001; Jansche, 2003; Baroni and Evert, 2007; Bhat and Sproat, 2009), we would like to draw attention to the following insight articulated 253 Proceedings of the ACL 2010 Conference Short Papers, pages 253–257, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics most clearly in Jansche (2003). Estimation is improved if texts are construed as being generated by two processes, one choosing which words would appear at all in the text, and then, for words that have been chosen to appear, how many times they would in fact appear. Jansche (2003) describes a two-stage generation process: (</context>
</contexts>
<marker>Jansche, 2003</marker>
<rawString>Martin Jansche. 2003. Parametric Models of Linguistic Count Data. In Proceedings of the ACL, pages 288–295, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods -Support Vector Learning.</booktitle>
<editor>In B. Schlkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11304" citStr="Joachims, 1999" startWordPosition="1782" endWordPosition="1783">for text classification tasks. For example, Lin et al. (2006) use 4We excluded Israeli, Turkish, Iranian, Pakistani writers as not clearly representing either perspective. 5We additionally removed words containing support, oppos, sustain, overrid from the PBA data, in order not to inflate the performance on perspective classification due to the explicit reference to the upcoming vote. NB-COUNT and SVM-NORMF for perspective classification; Pang et al. (2002) consider most and Yu et al. (2008) all of the above for related tasks of movie review and political party classification. We use SVM&amp;quot;ght (Joachims, 1999) for SVM and WEKA toolkit (Witten and Frank, 2005; Hall et al., 2009) for both version of Naive Bayes. Parameter optimization for all SVM models is performed using grid search on the training data separately for each partition into train and test data.6 5 Results Table 2 summarizes the cross-validation results for the four datasets discussed above. Notably, the SVM-BOOL model is either the best or not significantly different from the best performing model, although the competitors use more detailed textual information, namely, the count of each word’s appearance in the text, either raw (NB-COU</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. Schlkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods -Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Are these documents written from different perspectives? A test of different perspectives based on statistical distribution divergence.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>1057--1064</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16462" citStr="Lin and Hauptmann (2006)" startWordPosition="2640" endWordPosition="2643"> abortion debates, using infanticide as a synonym for abortion is a pro-life stigma. Note that this does not mean the rest of the features are not informative for classification, only that they are redundant with respect to a small percentage of top weight features. When N best features are eliminated, performance goes down significantly with even smaller N for PBA and BL datasets. Thus, top features are not only effective, they are also crucial for accurate classification, as their discrimination capacity is not replicated by any of the other vocabulary words. This finding is consistent with Lin and Hauptmann (2006) study of perspective vs topic classification: While topical differences between two corpora are manifested in difference in distributions of great many words, they observed little perspective-based variation in distributions of most words, apart from certain words that are preferentially used by adherents of one or the other perspective on the given topic. For DP and BL-I datasets, the results seem to suggest perspectives with more diffused keyword distribution (No-NBest figures are higher). We note, however, that feature redundancy experiments are confounded in these cases by either a low po</context>
</contexts>
<marker>Lin, Hauptmann, 2006</marker>
<rawString>Wei-Hao Lin and Alexander Hauptmann. 2006. Are these documents written from different perspectives? A test of different perspectives based on statistical distribution divergence. In Proceedings of the ACL, pages 1057–1064, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Which side are you on? Identifying perspectives at the document and sentence levels.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>109--116</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8316" citStr="Lin et al., 2006" startWordPosition="1316" endWordPosition="1319">lators with a mixed record, that is, those who gave 20-60% support to one of the positions.2 Death Penalty (DP) blogs: We use University of Maryland Death Penalty Corpus (Greene and Resnik, 2009) of 1085 texts from a number of proand anti-death penalty websites. We report 4-fold cross-validation (DP-4) using the folds in Greene and Resnik (2009), where training and testing data come from different websites for each of the sides, as well as 10-fold cross-validation performance on the entire corpus, irrespective of the site.3 Bitter Lemons (BL): We use the GUEST part of the BitterLemons corpus (Lin et al., 2006), containing 296 articles published in 2001-2005 on http://www.bitterlemons.org by more than 200 different Israeli and Palestinian writers on issues related to the conflict. Bitter Lemons International (BL-I): We collected 150 documents each by a different per2Ratings are from: http://www.OnTheIssues.org/. We further excluded data from Rep. James Moran, D-VA, as he changed his vote over the years. For legislators rated by neither NRLC nor NARAL, we assumed the vote aligns with the perspective. 3The 10-fold setting yields almost perfect performance likely due to site-specific features beyond pe</context>
<context position="10750" citStr="Lin et al. (2006)" startWordPosition="1695" endWordPosition="1698">rnoulli (here, NB-BOOL) and multinomial (here, NB-COUNT), respectively, in McCallum and Nigam (1998) study of event models for text categorization. The first records presence/absence of a word in a text, while the second records the number of occurrences. McCallum and Nigam (1998) found NB-COUNT to do better than NB-BOOL for sufficiently large vocabulary sizes for text categorization by topic. For discriminative models, we use linear SVM, with presence-absence, normalized frequency, and tfidf feature weighting. Both types of models are commonly used for text classification tasks. For example, Lin et al. (2006) use 4We excluded Israeli, Turkish, Iranian, Pakistani writers as not clearly representing either perspective. 5We additionally removed words containing support, oppos, sustain, overrid from the PBA data, in order not to inflate the performance on perspective classification due to the explicit reference to the upcoming vote. NB-COUNT and SVM-NORMF for perspective classification; Pang et al. (2002) consider most and Yu et al. (2008) all of the above for related tasks of movie review and political party classification. We use SVM&amp;quot;ght (Joachims, 1999) for SVM and WEKA toolkit (Witten and Frank, 2</context>
</contexts>
<marker>Lin, Wilson, Wiebe, Hauptmann, 2006</marker>
<rawString>Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann. 2006. Which side are you on? Identifying perspectives at the document and sentence levels. In Proceedings of CoNLL, pages 109–116, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for Naive Bayes text classification.</title>
<date>1998</date>
<booktitle>In Proceedings of AAAI-98 Workshop on Learning for Text Categorization,</booktitle>
<pages>41--48</pages>
<location>Madison, WI,</location>
<contexts>
<context position="6101" citStr="McCallum and Nigam, 1998" startWordPosition="955" endWordPosition="958">to occur if the speaker holds a pro-life perspective, while a pro-choicer would avoid this term. We therefore hypothesize that the choice of vocabulary is not only a matter of topic but also of perspective, while word recurrence has mainly to do with the topical composition of the text. Therefore, tracing word frequencies is not going to be effective for perspective classification beyond noting the mere presence/absence of words, differently from the findings in text categorization, where frequency-based features usually do better than boolean features for sufficiently large vocabulary sizes (McCallum and Nigam, 1998). 3 Data Partial Birth Abortion (PBA) debates: We use transcripts of the debates on Partial Birth Abortion Ban Act on the floors of the US House and Senate in 104-108 Congresses (1995-2003). Similar legislation was proposed multiple times, passed the legislatures, and, after having initially been vetoed by President Clinton, was signed into law by President Bush in 2003. We use data from 278 legislators, with 669 speeches in all. We take only one speech per speaker per year; since many serve multiple years, each speaker is represented with 1 to 5 speeches. We perform 10-fold cross-validation s</context>
<context position="10233" citStr="McCallum and Nigam (1998)" startWordPosition="1612" endWordPosition="1615">n perspective manifestations using common English vocabulary. To avoid the possibility that artifacts such as names of senators or states drive the classification, we use as features words that contain only lowercase letters, possibly hyphenated. No stemming is performed, and no stopwords are excluded.5 Table 1: Summary of corpora Data #Docs #Features # CV folds PBA 669 9.8 K 10 BL 296 10 K 10 BL-I 150 9 K 10 DP 1085 25 K 4 4 Models For generative models, we use two versions of Naive Bayes models termed multi-variate Bernoulli (here, NB-BOOL) and multinomial (here, NB-COUNT), respectively, in McCallum and Nigam (1998) study of event models for text categorization. The first records presence/absence of a word in a text, while the second records the number of occurrences. McCallum and Nigam (1998) found NB-COUNT to do better than NB-BOOL for sufficiently large vocabulary sizes for text categorization by topic. For discriminative models, we use linear SVM, with presence-absence, normalized frequency, and tfidf feature weighting. Both types of models are commonly used for text classification tasks. For example, Lin et al. (2006) use 4We excluded Israeli, Turkish, Iranian, Pakistani writers as not clearly repre</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for Naive Bayes text classification. In Proceedings of AAAI-98 Workshop on Learning for Text Categorization, pages 41–48, Madison, WI, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Philadelphia, PA,</location>
<contexts>
<context position="11150" citStr="Pang et al. (2002)" startWordPosition="1754" endWordPosition="1757">r discriminative models, we use linear SVM, with presence-absence, normalized frequency, and tfidf feature weighting. Both types of models are commonly used for text classification tasks. For example, Lin et al. (2006) use 4We excluded Israeli, Turkish, Iranian, Pakistani writers as not clearly representing either perspective. 5We additionally removed words containing support, oppos, sustain, overrid from the PBA data, in order not to inflate the performance on perspective classification due to the explicit reference to the upcoming vote. NB-COUNT and SVM-NORMF for perspective classification; Pang et al. (2002) consider most and Yu et al. (2008) all of the above for related tasks of movie review and political party classification. We use SVM&amp;quot;ght (Joachims, 1999) for SVM and WEKA toolkit (Witten and Frank, 2005; Hall et al., 2009) for both version of Naive Bayes. Parameter optimization for all SVM models is performed using grid search on the training data separately for each partition into train and test data.6 5 Results Table 2 summarizes the cross-validation results for the four datasets discussed above. Notably, the SVM-BOOL model is either the best or not significantly different from the best per</context>
<context position="13582" citStr="Pang et al., 2002" startWordPosition="2151" endWordPosition="2154">105}. On the DP data parameter j controlling penalties for misclassification of positive and negative cases is optimized as well (j = {10−2, 10−1, ... , 102}), since datasets are unbalanced (for example, there is a fold with 27%-73% split). 7Here SVM-TFIDF is doing somewhat better than SVMBOOL on one of the folds and much worse on two other folds; paired t-test with just 4 pairs of observations does not detect a significant difference. 255 observed that boolean features are reliably better than count-based features, as reported for the sentiment classification task in the movie review domain (Pang et al., 2002). We note the low performance on BL-I, which could testify to a low degree of lexical consolidation in the Arab vs Western perspectives (more on this below). It is also possible that the small size of BL-I leads to overfitting and low accuracies. However, PBA subset with only 151 items (only 2002 and 2003 speeches) is still 96% classifiable, so size alone does not explain low BL-I performance. 6 Consolidation of perspective We explore feature redundancy in perspective classification.We first investigate retention of only N best features, then elimination thereof. As a proxy of feature quality,</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. In Proceedings of EMNLP, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Teubert</author>
</authors>
<title>A Province of a Federal Superstate, Ruled by an Unelected Bureaucracy</title>
<date>2001</date>
<booktitle>Keywords of the Euro-Sceptic Discourse in Britain. In</booktitle>
<pages>45--86</pages>
<editor>Andreas Musolff, Colin Good, Petra Points, and Ruth Wittlinger, editors,</editor>
<publisher>Ashgate Publishing Ltd, Hants,</publisher>
<location>England.</location>
<contexts>
<context position="15828" citStr="Teubert, 2001" startWordPosition="2533" endWordPosition="2534">acy as with the full feature set, even in high-accuracy cases such as PBA and BL. The effectiveness of a small subset of features is consistent with the observation in the discourse analysis studies that rivals 8We experimented with the mutual information based feature selection as well, with generally worse results. in long-lasting controversies tend to consolidate their vocabulary and signal their perspective with certain stigma words and banner words, that is, specific keywords used by a discourse community to implicate adversaries and to create sympathy with own perspective, respectively (Teubert, 2001). Thus, in abortion debates, using infanticide as a synonym for abortion is a pro-life stigma. Note that this does not mean the rest of the features are not informative for classification, only that they are redundant with respect to a small percentage of top weight features. When N best features are eliminated, performance goes down significantly with even smaller N for PBA and BL datasets. Thus, top features are not only effective, they are also crucial for accurate classification, as their discrimination capacity is not replicated by any of the other vocabulary words. This finding is consis</context>
</contexts>
<marker>Teubert, 2001</marker>
<rawString>Wolfgang Teubert. 2001. A Province of a Federal Superstate, Ruled by an Unelected Bureaucracy – Keywords of the Euro-Sceptic Discourse in Britain. In Andreas Musolff, Colin Good, Petra Points, and Ruth Wittlinger, editors, Attitudes towards Europe: Language in the unification process, pages 45–86. Ashgate Publishing Ltd, Hants, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques.</booktitle>
<volume>2</volume>
<pages>edition.</pages>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="11353" citStr="Witten and Frank, 2005" startWordPosition="1789" endWordPosition="1792">, Lin et al. (2006) use 4We excluded Israeli, Turkish, Iranian, Pakistani writers as not clearly representing either perspective. 5We additionally removed words containing support, oppos, sustain, overrid from the PBA data, in order not to inflate the performance on perspective classification due to the explicit reference to the upcoming vote. NB-COUNT and SVM-NORMF for perspective classification; Pang et al. (2002) consider most and Yu et al. (2008) all of the above for related tasks of movie review and political party classification. We use SVM&amp;quot;ght (Joachims, 1999) for SVM and WEKA toolkit (Witten and Frank, 2005; Hall et al., 2009) for both version of Naive Bayes. Parameter optimization for all SVM models is performed using grid search on the training data separately for each partition into train and test data.6 5 Results Table 2 summarizes the cross-validation results for the four datasets discussed above. Notably, the SVM-BOOL model is either the best or not significantly different from the best performing model, although the competitors use more detailed textual information, namely, the count of each word’s appearance in the text, either raw (NB-COUNT), normalized (SVM-NORMF), or combined with doc</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, 2 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bei Yu</author>
<author>Stefan Kaufmann</author>
<author>Daniel Diermeier</author>
</authors>
<title>Classifying party affiliation from political speech.</title>
<date>2008</date>
<journal>Journal of Information Technology and Politics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="11185" citStr="Yu et al. (2008)" startWordPosition="1761" endWordPosition="1764">r SVM, with presence-absence, normalized frequency, and tfidf feature weighting. Both types of models are commonly used for text classification tasks. For example, Lin et al. (2006) use 4We excluded Israeli, Turkish, Iranian, Pakistani writers as not clearly representing either perspective. 5We additionally removed words containing support, oppos, sustain, overrid from the PBA data, in order not to inflate the performance on perspective classification due to the explicit reference to the upcoming vote. NB-COUNT and SVM-NORMF for perspective classification; Pang et al. (2002) consider most and Yu et al. (2008) all of the above for related tasks of movie review and political party classification. We use SVM&amp;quot;ght (Joachims, 1999) for SVM and WEKA toolkit (Witten and Frank, 2005; Hall et al., 2009) for both version of Naive Bayes. Parameter optimization for all SVM models is performed using grid search on the training data separately for each partition into train and test data.6 5 Results Table 2 summarizes the cross-validation results for the four datasets discussed above. Notably, the SVM-BOOL model is either the best or not significantly different from the best performing model, although the competi</context>
</contexts>
<marker>Yu, Kaufmann, Diermeier, 2008</marker>
<rawString>Bei Yu, Stefan Kaufmann, and Daniel Diermeier. 2008. Classifying party affiliation from political speech. Journal of Information Technology and Politics, 5(1):33–48.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>