<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.975805">
Selecting the Most Highly Correlated Pairs within a Large Vocabulary
</title>
<author confidence="0.996931">
Kyoji Umemura
</author>
<affiliation confidence="0.999834">
Department of Computer Science
Toyoahshi University of Technology
</affiliation>
<email confidence="0.997895">
umemura@tutics.tut.ac.jp
</email>
<sectionHeader confidence="0.995615" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999455">
Occurence patterns of words in documents
can be expressed as binary vectors. When
two vectors are similar, the two words cor-
responding to the vectors may have some
implicit relationship with each other. We
call these two words a correlated pair.
This report describes a method for obtain-
ing the most highly correlated pairs of a
given size. In practice, the method re-
quires computation time,
and memory space, where is the
number of documents or records. Since
this does not depend on the size of the
vocabulary under analysis, it is possible
to compute correlations between all the
words in a corpus.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.960773686567164">
In order to find relationships between words in a
large corpus or between labels in a large database,
we may use a distance measure between the binary
vectors of dimensions, where is the number of
documents or records, and the th element is 1 if the
th document/record contains the word or the label,
or 0 otherwise.
There are several distance measures suitable
for this purpose, such as the mutual informa-
tion(Church and Hanks, 1990), the dice coeffi-
cient(Manning and Schueutze 8.5, 1999), the phi
coefficient(Manning and Schuetze 5.3.3, 1999), the
cosine measure(Manning and Schueutze 8.5, 1999)
and the confidence(Arrawal and Srikant, 1995).
There are also special functions for certain applica-
tions, such as then complimentary similarity mea-
sure (CSM)(Hagita and Sawaki, 1995) which is
known as to be suitable for cases with a noisy pat-
tern.
All of these five measures can be obtained from a
simple contingency table. This table has four num-
bers for each word/label and word/label . The
first number is the number of documents/records
that have both and . We define this number as
. The second number is the number of doc-
uments/records that have but not . We define this
number as . The third number is the num-
ber of documents/records that do not have but do
have . We define this number as . The
fourth and the last number is the number of docu-
ments/records that have neither nor . We define
this number as
.
An obvious method to obtain the most highly re-
lated pairs is to calculate , , ,for all
pairs of words/labels, compute the similarity for all
pairs and then select pairs of the highest values. Let
be the number of possible words/labels, and
be the total number of documents/records in a cor-
pus/database. This method requires memory
space and computation time. However,
its use is only feasible if is smaller than . When
is larger than ten thousand, execution of this pro-
cedure becomes difficult.
The method described here is based on the obser-
vation that there is an upper boundary to the number
of different words in one document. The assumption
of such a boundary could even made of a large scale
dice coefficient
corpus. For example, a collective corpus of a news-
paper may become larger and larger, but the length
of each article is stable. It is not likely that one arti-
cle would contain thousands of different words.
In view of this observation and the assumption,
this method is effective for obtaining the most highly
correlated pairs in a large corpus, and uses
memory space, and computation
time.
2 Notations
Several notations are introduced in this section to de-
scribe the method. Assuming a corpus C, which is a
set of sets of words, values are assigned as follows.
: documents (elements of the corpus).
, , :label (elements of a document).
: is placed after in the alphabetical
order.
: the total number of documents.
</bodyText>
<sectionHeader confidence="0.33737" genericHeader="method">
3 Problem Definition
</sectionHeader>
<bodyText confidence="0.954197558823529">
When the corpus of a set of sets of labels is provided,
and the function of a pair of labels to the
number in the following form is also provided, we
will obtain : the set of pairs of a given size that
satisfies the following condition.
where
The following are examples of .
cosine function
confidence
: the number of documents that con-
tain and contain .
pairwise mutual information
: the number of documents that con-
tain but not .
: the number of documents that contain
.
: the number of documents that con-
tains but not .
phi coefficient
: the number of documents that cone-
tain neither nor .
complementary similarity measure
Implementation of a program that requires
memory space and computation time
is easy. A program of this type could be used to
calculate , ,, and for all pairs of and
, and could then provide the most highly correlated
pairs. However, compuation with this method is not
feasible when is large.
For example, in order to calculate the most highly
correlated words within a newspaper over several
years of publication, becomes roughly , and
becomes . The amount of computation time is
then increased to .
</bodyText>
<sectionHeader confidence="0.619411" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.95101">
In terms of the actual data, the number of correlated
pairs is usually much smaller than the number of un-
correlated pairs. Moreover, most of the uncorrelated
pairs usually satisfy the condition:
</bodyText>
<listItem confidence="0.955809333333333">
Proof:
1. is equivalent to .
2. By definition, the sum of ,,
, and always represents the to-
tal number of documents.
3. Similarly, the sum of and
is the number of documents
that contain . This equals .
4. Similarly, the sum of and
is the number of documents
that contain . This equals .
5. These four equations make it possi-
</listItem>
<bodyText confidence="0.996973142857143">
ble to express , ,and by
and .
These formulas indicate that the number of re-
quired two-dimensional tables is not four, but just
one. In other words, if we create a table of
and one variable for , we can obtain ,
,
</bodyText>
<sectionHeader confidence="0.771552" genericHeader="method">
6 The memory requirement for
</sectionHeader>
<bodyText confidence="0.961796206896552">
Let be the maximum number of different
words/labels in one document. The following prop-
erty exists in .
,
and are not of interest. This method takes this fact
into account. Moreover, it also uses the relationship
between and to make the
computation feasible.
, and
.
5 Relationship between and
Proofs of the following equations are provided be-
low.
This relationship indicates that if the table
is stored using tuples of where
, the required memory space is .
Tuples where are not necessary be-
cause we know that when the tuple
for does not exist in memory.
This estimation is pessimistic. The actual size of
the tuples will be smaller than , since not all
documents will have different words/labels.
7 Obtaining , and
The algorithm to obtain , and is straight-
forward. First, the corpus must be trasformed into a
set of sets of words/labels. Since this is a set form,
there are no duplications of the words/labels of one
document. In the following program, the hashtable
returns 0 for a non-existent item.
</bodyText>
<listItem confidence="0.998864333333333">
(1) Let DFA be empty hashtable.
(2) Let DF be empty hashtable
(3) Let N be 0
</listItem>
<bodyText confidence="0.956297125">
The cosine measure, the dice coefficient, and pair-
wise mutual information have property 1 and prop-
erty 2 as defined below. This implies that the value
for where is actually the mini-
mum value of all . Therefore, the first part of
the total ordered sequence of is the sorted list
of where . The rest is an arbitary
order of pairs where .
</bodyText>
<figureCaption confidence="0.9606585">
Property 1: the value is not negative.
Property 2: when , the value is .
</figureCaption>
<bodyText confidence="0.999496125">
The phi coefficient and the complementary simi-
larity measure have the following properties 1, 2 and
3. Therefore, the first part of the total ordered se-
quence where the value is positive, is equal to first
part of the sorted list where and the
value is positive. Moreover, this list contains all
pairs that have a positive correlation. This list is long
enough for the actual application.
</bodyText>
<equation confidence="0.5027275">
Property 1: when , the value is nega-
tive.
Property 2: when and are not correlated, the
(6)  |For each word in D
(7)  |assign the word to X
(8)   ||For each word in D
(9)   ||assign the word to Y
(10)     DFA(X, Y)=DFA(X,Y)+1
(11)   ||end of loop
(12)  |end of loop
</equation>
<listItem confidence="0.374181">
(13) end of loop
</listItem>
<bodyText confidence="0.9846804">
The computation time for this program is less than
. Since is independent from , the compu-
tation time is . Again, is a pessimistic
estimation, since not all documents will have dif-
ferent words/labels.
</bodyText>
<sectionHeader confidence="0.554606" genericHeader="method">
8 Selecting Pairs
</sectionHeader>
<bodyText confidence="0.91632052173913">
Even though , ,, and can be obtained
in constant time after preprocessing, there are
values to consider to obtain the best N correlated
pairs. Fortunately, many of the functions thatare
usable as indicators of correlation and, at least, all
five functions, return a lower value than the known
threshold if .
Property 3: when and tend to appear at the
same time, the estimated value is positive.
It should be recalled that the number of pairs
where is less than . The sorted
list is obtained in com-
putation time, where is the maximum number of
different words/labels in one document. Since is
constant, it becomes , even if the
size ofvocabulary is very large.
It is true that for the given some fixed vocabu-
lary of size , might be larger than as we
increase the size of corpus. Fortunately, the actual
memory consumption of this procedure also have
the upper bound of , and we will not loose
any memory space. When is not fixed and may
become very large compare to as is the case for
</bodyText>
<footnote confidence="0.586947">
proper nouns, is smaller than .
9 Case study of a Newspaper Corpus
</footnote>
<bodyText confidence="0.98218225">
The computation time of the baseline system is
where is the distinct number of labels in the
The left side of the formula equals the
total number of all pairs of words/labels.
</bodyText>
<table confidence="0.710403888888889">
This cannot exceed .
(4) For each document, assign it to D
(5)  |N = N + 1
estimated value is .
time(sec.) speed(sec./doc)
1000 2.4
3000 7.8
10000 21.1
30000 60.9
</table>
<tableCaption confidence="0.8684765">
Table 1: The actual execution time shows a linear
relationship to the size of input data.
</tableCaption>
<bodyText confidence="0.999374052631579">
corpus. When we analyzed labels of names of places
in a newspaper over the course of one year, this cor-
pus consisted of about 60,000 documents. The place
names totalled 1902 after morphological analysis.
The maximum number of names in one document
was 142, and the average in one document was 4.02.
In this case, the method described here, was much
more efficient than the baseline system.
Table 1 shows the actual execution time of the
program in the appendix, changing the length of the
corpus. This program computes similarity values for
all pairs of words where . It indicates that
the execution time is linear.
Our observation shows that even if the corpus
were extended year by year, which is the maxi-
mum number of different words in one document is
stable, even though the total number of words would
increase with the ongoing addition of proper nouns
and new concepts.
</bodyText>
<sectionHeader confidence="0.695777" genericHeader="method">
10 For a large corpus
</sectionHeader>
<bodyText confidence="0.999319785714286">
Although the program in the appendix cannot be ap-
plied to a corpus larger than memory size, we can
obtain a table of using sequential access to file.
The program in the appendix stores every pair in
memory. The space requirement of may
seem too great to hold in memory. However, se-
quential file can be used to obtain the table, as
follows. Although the computation time for is
rather than , the total compu-
tation time remains the same because computation
of is required to select pairs in both
cases.
Consider the following data. Each line corre-
sponds to one document.
</bodyText>
<equation confidence="0.6961705">
a b
a c
x y
x
x y z
a b c
</equation>
<bodyText confidence="0.884752">
When the pairs of words in each document are
recorded, the following file is obtained. Note that
since , it is not necessary to
record pairs where . This reduces the memory
requirement.
</bodyText>
<figure confidence="0.994574909090909">
a a
a b
b b
a a
a c
c c
x x
x y
y y
x x
x x
x y
x z
y y
y z
z z
a a
a b
a c
b b
b c
c c
</figure>
<bodyText confidence="0.978305">
Using the merge sort algorithm which can sort a
large file using sequential access only, the file can
be sorted in computation time. Af-
ter sorting in alphabetical order, same pairs come
together. Then, the pairs can be counted with se-
quential access, thereby providing the table. An
example of this table fllows:
</bodyText>
<figure confidence="0.9619655">
a a 3
a b 2
a c 2
b b 2
b c 1
c c 2
</figure>
<equation confidence="0.507611666666667">
x x 3
x y 2
x z 1
y y 2
y z 1
z z 1
</equation>
<bodyText confidence="0.991929416666667">
It should be noted that the table can be obtained
easily by extracting lines in which letter of the first
column and that of the second column are the same,
since . The table can usually be
stored in memory since it is a one dimensional array.
After storing in memory, similarity can be com-
puted line by line. The following example uses the
phi coefficient. The first column is the coefficient,
followed by , , ,, and . Since the phi
coefficient is reflective, the value where
is not required. When the function is not symmetric,
and can be exchanged at the same
</bodyText>
<table confidence="0.998323461538462">
time.
0.544705 3 0 0 3 a a
0.384900 2 1 0 3 a b
0.384900 2 1 0 3 a c
0.624695 2 0 0 4 b b
0.156174 1 1 1 3 b c
0.624695 2 0 0 4 c c
0.544705 3 0 0 3 x x
0.384900 2 1 0 3 x y
0.242536 1 2 0 3 x z
0.624695 2 0 0 4 y y
0.392232 1 1 0 4 y z
0.674200 1 0 0 5 z z
</table>
<bodyText confidence="0.9957734">
The ordered list can be obtained by sorting this
table with the first column. This example shows that
pairs where , such as or ,
do not add any overhead to either memory or com-
putation time.
</bodyText>
<subsectionHeader confidence="0.844112">
11 Comparison with Apriori
</subsectionHeader>
<bodyText confidence="0.9998255">
There is a well known algorithm for forming a list of
related items termed Apriori(Arrawal and Srikant,
1995). Apriori lists all relationship using confi-
dence, where is larger than a specified
value. Using Apriori, the threshold can be spec-
ified in order to reduce computation, whereas with
the proposed method, there is no way to adjust this
threshold. This implies that Apriori may be faster
than our algorithm in terms of confidence. However,
since Apriori uses the property of confidence to re-
duce computation, it cannot be used for other func-
tions, unlike the proposed method which can employ
many standard functions, at least the five measures
used here including confidence.
</bodyText>
<subsectionHeader confidence="0.882905">
12 Correlation of All Substrings
</subsectionHeader>
<bodyText confidence="0.996197423076923">
When computing correlations of all substrings in a
corpus, can be as large as . Since the
memory space requirement and computation time
does not depend on , this method can be used to
generate a list of the most hightly correlated sub-
strings of any length. In fact, in some cases, may
be too large to compute.
The Yamamoto-Church method(Yamamoto and
Church, 2001) allows for the creation of a ta-
ble using memory space and
computation time, where represents all substrings
in a given corpus. Yamamoto’s method shows that
although there may be kinds of
substrings in a corpus, there is occurence
patterns (or sets of substrings which have same oc-
curence pattern) at most. The computational cost
is greatly reduced if we deal with each pattern in-
stead of each substring. Although the order of com-
putional complexity does not depend on , differs
whether the pattern is used or not. We have also de-
veloped a system using the pattern which actually re-
duces the cost of computation. Although the number
of is still problematic even using the Yamamoto-
Church method, and although the computation cost
is much larger than using words, the program runs
much faster than the simple method.
</bodyText>
<sectionHeader confidence="0.963797" genericHeader="method">
13 Conclusion
</sectionHeader>
<bodyText confidence="0.999700266666667">
This paper describes a method for selecting cor-
related pairs in memory space and
computation time, where is the num-
ber of documents in a corpus, provided that there
is an upper boundary in the number of different
words/labels in one document/record. We have ob-
served that a corpus usually has this kind of upper
boundary, and have shown that we can uses a se-
quential file for most of our memory requirements.
This method is useful not only for confidence but
also for other functions whose values are decided by
, , , .Examples of these functions are
mutual information, the dice coefficient, the confi-
dence measure, the phi coefficient and the compli-
mentary similarity measure.
</bodyText>
<sectionHeader confidence="0.998504" genericHeader="conclusions">
References
</sectionHeader>
<reference confidence="0.997326035714286">
K. W. Church and P. Hanks 1990 Word association
norms, mutual information and lexicography Compu-
tational Linguistics, 16(1):22–29
R. Agrawal and R. Srikant 1995 Mining of association
rules between sets of items in large databases. In Pro-
ceedings of the ACM SIGMOD Conference on Man-
agement of Data:94–105
N. Hagita and M. Sawaki: 1995 Robust recognition
of degraded machine-printedcharacters using com-
plimentary similarity measure and error-correction
learning Proceedings of the SPIE - The International
Society for Optical Engineering 2442:234–244
U. M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth The
KDD Process for Extracting Useful Knowledge from
Volumes of Data Communications of the ACM,
39(11):27–34,
Christopher D. Manning and Hinrich Schuetze, 1999
Chapter 8.5, Semantic Similarity Foundations of
statistical natural language processing:294–303, The
MIT Press
Christopher D. Manning and Hinrich Schuetze, 1999
Chapter 5.3.3, Pearson’s chi-square test Founda-
tions of statistical natural language processing:169–
172, The MIT Press
Mikio Yamamoto and Kenneth W. Church 2001 Using
Suffix Arrrays to Compute Term Frequency and Docu-
ment Frequency for All Substring in a Corpus Com-
putational Linguistics, 27(1):1–30, The MIT Press.
</reference>
<sectionHeader confidence="0.722918" genericHeader="references">
Appendix
</sectionHeader>
<reference confidence="0.862099076923077">
Sample of DATA
1992.01.01.00000043 Takarazuka Tokyo
1992.01.01.00000046 Okinawa Yatushiro
1992.01.01.00000048 Hiroshima Kurihara Onomichi Yokohama
1992.01.01.00000049 Tokyo
1992.01.01.00000050 Ichihara Tokyo
1992.01.01.00000051 Aizuwakamatu Fukushima Tokyo
1992.01.01.00000056 Matumoto Sahara Utsunomiya
1992.01.01.00000065 Tokyo
1992.01.01.00000066 Aomori Shimokita
Sample Program(csm.awk)
1 # Definition of similarity -
2 # Complimentary Similarity Measure.
</reference>
<figure confidence="0.992333225806452">
3 function f(a, b, c, d) {
4 return (a * d - b * c) / sqrt((a + c) * (b + d));
5 }
6 # Foreach line, count up both df(x) and dfa(x, y).
7 { for(i=2; i&lt;= NF; i++) {
8 df[$i]++;
9 for(j=i;j&lt;= NF; j++) {
10 dfa[$i, $j]++;
11 }
12 }
13 }
14 # For all (x, y) where dfa(x, y)&lt;&gt;0, get the value.
15 END{
16 for(k in dfa) {
17 split(k, x, SUBSEP);
18 if(x[1] != x[2]) {
19 a = dfa[k];
20 b = df[x[1]] - a;
21 c = df[x[2]] - a;
22 d = NR - a - b - c;
23 r = f(a, b, c, d);
24 printf(&amp;quot;%10.6f\t%s\t%s\n&amp;quot;,
25 r, x[1], x[2]);
26 r = f(a, c, b, d);
27 printf(&amp;quot;%10.6f\t%s\t%s\n&amp;quot;,
28 r, x[2], x[1]);
29 }
30 }
31 }
Usage:
$ awk -f csm.awk &lt; mai.txt  |sort -nr  |head -20
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.803705">
<title confidence="0.999872">Selecting the Most Highly Correlated Pairs within a Large Vocabulary</title>
<author confidence="0.974801">Kyoji</author>
<affiliation confidence="0.9912295">Department of Computer Toyoahshi University of</affiliation>
<email confidence="0.856504">umemura@tutics.tut.ac.jp</email>
<abstract confidence="0.998618294117647">Occurence patterns of words in documents can be expressed as binary vectors. When two vectors are similar, the two words corresponding to the vectors may have some implicit relationship with each other. We call these two words a correlated pair. This report describes a method for obtaining the most highly correlated pairs of a size. In practice, the method requires computation time, and memory space, where is the number of documents or records. Since this does not depend on the size of the vocabulary under analysis, it is possible to compute correlations between all the words in a corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography Computational Linguistics,</title>
<date>1990</date>
<journal>Volumes of Data Communications of the ACM,</journal>
<booktitle>In Proceedings of the ACM SIGMOD Conference on Management of Data:94–105</booktitle>
<volume>16</volume>
<issue>1</issue>
<publisher>MIT Press</publisher>
<contexts>
<context position="1234" citStr="Church and Hanks, 1990" startWordPosition="197" endWordPosition="201">r of documents or records. Since this does not depend on the size of the vocabulary under analysis, it is possible to compute correlations between all the words in a corpus. 1 Introduction In order to find relationships between words in a large corpus or between labels in a large database, we may use a distance measure between the binary vectors of dimensions, where is the number of documents or records, and the th element is 1 if the th document/record contains the word or the label, or 0 otherwise. There are several distance measures suitable for this purpose, such as the mutual information(Church and Hanks, 1990), the dice coefficient(Manning and Schueutze 8.5, 1999), the phi coefficient(Manning and Schuetze 5.3.3, 1999), the cosine measure(Manning and Schueutze 8.5, 1999) and the confidence(Arrawal and Srikant, 1995). There are also special functions for certain applications, such as then complimentary similarity measure (CSM)(Hagita and Sawaki, 1995) which is known as to be suitable for cases with a noisy pattern. All of these five measures can be obtained from a simple contingency table. This table has four numbers for each word/label and word/label . The first number is the number of documents/rec</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString> K. W. Church and P. Hanks 1990 Word association norms, mutual information and lexicography Computational Linguistics, 16(1):22–29 R. Agrawal and R. Srikant 1995 Mining of association rules between sets of items in large databases. In Proceedings of the ACM SIGMOD Conference on Management of Data:94–105 N. Hagita and M. Sawaki: 1995 Robust recognition of degraded machine-printedcharacters using complimentary similarity measure and error-correction learning Proceedings of the SPIE - The International Society for Optical Engineering 2442:234–244 U. M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth The KDD Process for Extracting Useful Knowledge from Volumes of Data Communications of the ACM, 39(11):27–34, Christopher D. Manning and Hinrich Schuetze, 1999 Chapter 8.5, Semantic Similarity Foundations of statistical natural language processing:294–303, The MIT Press Christopher D. Manning and Hinrich Schuetze, 1999 Chapter 5.3.3, Pearson’s chi-square test Foundations of statistical natural language processing:169– 172, The MIT Press Mikio Yamamoto and Kenneth W. Church 2001 Using Suffix Arrrays to Compute Term Frequency and Document Frequency for All Substring in a Corpus Computational Linguistics, 27(1):1–30, The MIT Press. Sample of DATA</rawString>
</citation>
<citation valid="false">
<pages>01--01</pages>
<location>Takarazuka Tokyo</location>
<marker>1992.</marker>
<rawString>01.01.00000043 Takarazuka Tokyo</rawString>
</citation>
<citation valid="false">
<pages>01--01</pages>
<location>Okinawa Yatushiro</location>
<marker>1992.</marker>
<rawString>01.01.00000046 Okinawa Yatushiro</rawString>
</citation>
<citation valid="false">
<pages>01--01</pages>
<location>Hiroshima Kurihara Onomichi Yokohama</location>
<marker>1992.</marker>
<rawString>01.01.00000048 Hiroshima Kurihara Onomichi Yokohama</rawString>
</citation>
<citation valid="false">
<pages>01--01</pages>
<location>Tokyo</location>
<marker>1992.</marker>
<rawString>01.01.00000049 Tokyo</rawString>
</citation>
<citation valid="false">
<pages>01--01</pages>
<location>Ichihara Tokyo</location>
<marker>1992.</marker>
<rawString>01.01.00000050 Ichihara Tokyo</rawString>
</citation>
<citation valid="false">
<booktitle>01.01.00000051 Aizuwakamatu</booktitle>
<location>Fukushima Tokyo</location>
<marker>1992.</marker>
<rawString>01.01.00000051 Aizuwakamatu Fukushima Tokyo</rawString>
</citation>
<citation valid="false">
<institution>01.01.00000056 Matumoto Sahara Utsunomiya</institution>
<marker>1992.</marker>
<rawString>01.01.00000056 Matumoto Sahara Utsunomiya</rawString>
</citation>
<citation valid="false">
<pages>01--01</pages>
<location>Tokyo</location>
<marker>1992.</marker>
<rawString>01.01.00000065 Tokyo</rawString>
</citation>
<citation valid="false">
<booktitle>01.01.00000066 Aomori Shimokita Sample Program(csm.awk) 1 # Definition of similarity -2 # Complimentary Similarity Measure.</booktitle>
<marker>1992.</marker>
<rawString>01.01.00000066 Aomori Shimokita Sample Program(csm.awk) 1 # Definition of similarity -2 # Complimentary Similarity Measure.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>