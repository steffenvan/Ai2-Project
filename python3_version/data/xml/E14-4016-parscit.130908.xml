<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000573">
<title confidence="0.9984015">
Deterministic Word Segmentation
Using Maximum Matching with Fully Lexicalized Rules
</title>
<author confidence="0.871922">
Manabu Sassano
</author>
<affiliation confidence="0.792652">
Yahoo Japan Corporation
</affiliation>
<address confidence="0.504629">
Midtown Tower, 9-7-1 Akasaka, Minato-ku, Tokyo 107-6211, Japan
</address>
<email confidence="0.625112">
msassano@yahoo-corp.jp
</email>
<sectionHeader confidence="0.993663" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998888125">
We present a fast algorithm of word seg-
mentation that scans an input sentence
in a deterministic manner just one time.
The algorithm is based on simple max-
imum matching which includes execu-
tion of fully lexicalized transformational
rules. Since the process of rule match-
ing is incorporated into dictionary lookup,
fast segmentation is achieved. We eval-
uated the proposed method on word seg-
mentation of Japanese. Experimental re-
sults show that our segmenter runs consid-
erably faster than the state-of-the-art sys-
tems and yields a practical accuracy when
a more accurate segmenter or an annotated
corpus is available.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999332586956522">
The aim of this study is to improve the speed
of word segmentation. Applications for many
Asian languages including Chinese and Japanese
require word segmentation. Such languages do not
have explicit word delimiters such as white spaces.
Word segmentation is often needed before every
task of fundamental text processing such as count-
ing words, searching for words, indexing docu-
ments, and extracting words. Therefore, the per-
formance of word segmentation is crucial for these
languages. Take for instance, information retrieval
(IR) systems for documents in Japanese. It typi-
cally uses a morphological analyzer&apos; to tokenize
the content of the documents. One of the most
time consuming tasks in IR systems is indexing,
which uses morphological analysis intensively.
Major approaches to Japanese morphological
analysis (MA) are based on methods of finding
&apos; Japanese has a conjugation system in morphology and
does not put white spaces between words. Therefore, we
have to do morphological analysis in order to segment a given
sentence into words and give an associated part-of-speech
(POS) tag to each word. In the main stream of the research
of Japanese language processing, morphological analysis has
meant to be a joint task of segmentation and POS tagging.
the best sequence of words along with their part-
of-speech tags using a dictionary where they use
the Viterbi search (e.g., (Nagata, 1994), (Kudo et
al., 2004)). However, computation cost of mod-
ern MA systems is mainly attributed to the Viterbi
search as Kaji et al. (2010) point out.
One of methods of improving the speed of MA
or word segmentation will be to avoid or reduce
the Viterbi search. We can avoid this by using
maximum matching in the case of word segmenta-
tion. Since there are many applications such as IR
and text classification, where part-of-speech tags
are not mandatory, in this paper we focus on word
segmentation and adopt maximum matching for it.
However, maximum matching for Japanese word
segmentation is rarely used these days because the
segmentation accuracy is not good enough and the
accuracy of MA is much higher. In this paper we
investigate to improve the accuracy of maximum-
matching based word segmentation while keeping
speedy processing.
</bodyText>
<sectionHeader confidence="0.957064" genericHeader="method">
2 Segmentation Algorithm
</sectionHeader>
<bodyText confidence="0.999876285714286">
Our algorithm is basically based on maximum
matching, or longest matching (Nagata, 1997). Al-
though maximum matching is very simple and
easy to implement, a segmenter with this algo-
rithm is not sufficiently accurate. For the pur-
pose of improving the segmentation accuracy, sev-
eral methods that can be combined with maximum
matching have been examined. In previous studies
(Palmer, 1997; Hockenmaier and Brew, 1998), the
combination of maximum matching and character-
based transformational rules has been investigated
for Chinese. They have reported promising results
in terms of accuracy and have not mentioned the
running time of their methods, which might sup-
posedly be very slow because we have to scan an
input sentence many times to apply learned trans-
formational rules.
In order to avoid such heavy post processing,
we simplify the type of rules and incorporate the
process of applying rules into a single process of
maximum matching for dictionary lookup. We
</bodyText>
<page confidence="0.992128">
79
</page>
<note confidence="0.6882345">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 79–83,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.877663133333333">
Input: ci: sentence which is represented as a char-
acter sequence. N: the number of characters in a
given sentence. t: dictionary, the data structure of
which should be a trie. oj: map an ID j to a single
word or a sequence of words. hj: total length of
oj.
Function: Lookup(t, c, i, N): search the dic-
tionary t for the substring c starting at the position
i up to N by using maximum matching. This re-
turns the ID of the entry in the dictionary t when it
matches and otherwise returns −1.
procedure Segment(c, N, t)
var i: index of the character sequence c
var j: ID of the entry in the trie dictionary t
begin
</bodyText>
<equation confidence="0.951745222222222">
i ← 1
while (i ≤ N) do begin
j = Lookup(t, c, i, N)
if (j = −1) then
{ unknown word as a single character }
print ci ; i = i + 1
else
print oj ; i = i + hj
{ if oj is a sequence of words, print each
</equation>
<bodyText confidence="0.8166106">
word in the sequence with a delimiter.
Otherwise print oj as a single token. }
endif
print delimiter
{ delimiter will be a space or something. }
</bodyText>
<listItem confidence="0.516295">
end
end
</listItem>
<figureCaption confidence="0.971140333333333">
Figure 1: Algorithm of word segmentation with
maximum matching that incorporates execution of
transformational rules.
</figureCaption>
<bodyText confidence="0.999620083333333">
show in Figure 1 the pseudo code of the algorithm
of word segmentation using maximum matching,
where the combination of maximum matching and
execution of simplified transformational rules is
realized. If each of the data oj in Figure 1 is a sin-
gle token, the algorithm which is presented here is
identical with segmentation by maximum match-
ing.
We use the following types of transformational
rules: c0c1...cl_1cl → w0...wm where ci is a char-
acter and wj is a word (or morpheme). Below are
sample rules for Japanese word segmentation:
</bodyText>
<listItem confidence="0.996370666666667">
• はないか (ha-na-i-ka) → は (ha; topic-
marker) ない (na-i; “does not exist”) か (ka;
“or”) 2
• 大工学部 (dai-ko-gaku-bu) → 大 (dai; “uni-
versity”) 工学部 (ko-gaku-bu; “the faculty of
engineering”)
</listItem>
<bodyText confidence="0.99987935483871">
Note that the form of the left hand side of the rule
is the sequence of characters, not the sequence of
words. Due to this simplification, we can combine
dictionary lookup by maximum matching with ex-
ecution of transformational rules and make them
into a single process. In other words, if we find
a sequence of characters of the left hand side of a
certain rule, then we write out the right hand side
of the rule immediately. This construction enables
us to naturally incorporate execution (or appli-
cation) of transformational rules into dictionary-
lookup, i.e., maximum matching.
Although the algorithm in Figure 1 does not
specify the algorithm or the implementation of
function Lookup(), a trie is suitable for the struc-
ture of the dictionary. It is known that an effi-
cient implementation of a trie is realized by using
a double-array structure (Aoe, 1989), which en-
ables us to look up a given key at the O(n) cost,
where n is the length of the key. In this case the
computation cost of the algorithm of Figure 1 is
O(n).
We can see in Figure 1 that the Viterbi search
is not executed and the average number of dictio-
nary lookups is fewer than the number of char-
acters of an input sentence because the average
length of words is longer than one. This contrasts
with Viterbi-based algorithms of word segmenta-
tion or morphological analysis that always require
dictionary lookup at each character position in a
sentence.
</bodyText>
<sectionHeader confidence="0.886586" genericHeader="method">
3 Learning Transformational Rules
</sectionHeader>
<subsectionHeader confidence="0.999658">
3.1 Framework of Learning
</subsectionHeader>
<bodyText confidence="0.999982">
The algorithm in Figure 1 can be combined with
rules learned from a reference corpus as well as
hand-crafted rules. We used here a modified ver-
sion of Brill’s error-driven transformation-based
learning (TBL) (Brill, 1995) for rule learning.
In our system, an initial system is a word seg-
menter that uses maximum matching with a given
</bodyText>
<footnote confidence="0.763726">
2 If we use simple maximum matching, i.e., with no trans-
formational rules, to segment the samples here, we will get
wrong segmentations as follows: はないか → はな (ha-na;
</footnote>
<equation confidence="0.85288625">
“flower”) いか (i-ka; “squid”), 大工学部 → 大工 (dai-ku;
“carpenter”) 学部 (gaku-bu; “faculty”).
80
0 00
w0w1 ··· wn → w0w1 ··· wm
L w00w01 · · · w0n → L w0w1 · · · wm
w00w01 · · · w0n R → w0w1 · · · wm R
Lw00w01 ···w0n R → Lw0w1 ···wm R
</equation>
<tableCaption confidence="0.979028">
Table 1: Rule templates for error-driven learning
</tableCaption>
<bodyText confidence="0.96623425">
word list and words which occur in a given ref-
erence corpus (a training corpus). Our segmenter
treats an unknown word, which is not in the dictio-
nary, as a one-character word as shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999273">
3.2 Generating Candidate Rules
</subsectionHeader>
<bodyText confidence="0.999920333333333">
In order to generate candidate rules, first we
compare the output of the current system with
the reference corpus and extract the differences
(Tashiro et al., 1994) as rules that have the follow-
ing form: L w00w01 · · · w0n R → L w0w1 · · · wm R
where w00w01 · · · w0n is a word sequence in the sys-
tem output and w0w1 · · · wm is a word sequence
in the reference corpus and L is a word in the left
context and R is a word in the right context. After
this extraction process, we generate four lexical-
ized rules from each extracted rule by using the
templates defined in Table 1.
</bodyText>
<subsectionHeader confidence="0.999562">
3.3 Learning Rules
</subsectionHeader>
<bodyText confidence="0.999269928571429">
In order to reduce huge computation when learn-
ing a rule at each iteration of TBL, we use some
heuristic strategy. The heuristic score h is defined
as: h = f ∗ (n + m) where f is a frequency of
the rule in question and n is the number of words
in w00w01 · · · w0n and m is the number of words in
w0w1 · · · wm. After sorting the generated rules as-
sociated with the score h, we apply each candidate
rule in decreasing order of h and compute the error
reduction. If we get positive reduction, we obtain
this rule and incorporate it into the current dictio-
nary and then proceed to the next iteration. If we
do not find any rules that reduce errors, we termi-
nate the learning process.
</bodyText>
<sectionHeader confidence="0.999776" genericHeader="evaluation">
4 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.998831">
4.1 Corpora and an Initial Word List
</subsectionHeader>
<bodyText confidence="0.999989636363636">
In our experiments for Japanese we used the Kyoto
University Text Corpus Version 4 (we call it KC4)
(Kurohashi and Nagao, 2003), which includes
newspaper articles, and 470M Japanese sentences
(Kawahara and Kurohashi, 2006), which is com-
piled from the Web. For training, we used two
sets of the corpus. The first set is the articles on
January 1st through 8th (7,635 sentences) of KC4.
The second one is 320,000 sentences that are se-
lected from the 470M Web corpus. Note that the
Web corpus is not annotated and we use it after
word segmentation is given by JUMAN 6.0 (Kuro-
hashi and Kawahara, 2007). The test data is a set
of sentences in the articles on January 9th (1,220
sentences). The articles on January 10th were used
for development.
We used all the words in the dictionary of JU-
MAN 6.0 as an initial word list. The number of
the words in the dictionary is 542,061. They are
generated by removing the grammatical informa-
tion such as part-of-speech tags from the entries in
the original dictionary of JUMAN 6.0.
</bodyText>
<subsectionHeader confidence="0.528526">
4.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999598424242424">
Segmentation Performance We used word
based F-measure and character-wise accuracy to
evaluate the segmentation performance.
Table 2 shows comparison of various systems
including ours. It is natural that since our sys-
tem uses only fully lexicalized rules and does not
use any generalized rules, it achieves a moderate
performance. However, by using the Web cor-
pus that contains 320,000 sentences, it yields an
F-measure of near 0.96, which is at the same level
as the F-measure of HMMs (baseline) in (Kudo et
al., 2004, Table 3). We will discuss how we can
improve it in a later section.
Segmentation Speed Table 3 shows comparison
of the segmentation speed of various systems for
320,000 sentences of the Web corpus. Since, in
general, such comparison is heavily dependent on
the implementation of the systems, we have to be
careful for drawing any conclusion. However, we
can see that our system, which does not use the
Viterbi search, achieved considerably higher pro-
cessing speed than other systems.
Further Improvement The method that we
have presented so far is based on lexicalized rules.
That is, we do not have any generalized rules. The
system does not recognize an unknown English
word as a single token because most of such words
are not in the dictionary and then are split into sin-
gle letters. Similarly, a number that does not ap-
pear in the training corpus is split into digits.
It is possible to improve the presented method
by incorporating relatively simple post-processing
that concatenates Arabic numerals, numerals in
</bodyText>
<page confidence="0.99782">
81
</page>
<table confidence="0.986338833333333">
System # of Sent. F-measure Char. Acc. # of Rules
JUMAN 6.0 NA 0.9821 0.9920 NA
MeCab 0.98 w/ jumandic 7,958 0.9861 0.9939 NA
Ours w/o training corpus 0 0.8474 0.9123 0
Ours w/ KC4 7,635 0.9470 0.9693 2228
w/ Web320K 320,000 0.9555 0.9769 24267
</table>
<tableCaption confidence="0.988021">
Table 2: Performance summary of various systems and configurations. Jumandic for MeCab (Kudo et
al., 2004) is stemmed from the dictionary of JUMAN.
</tableCaption>
<table confidence="0.834014">
System (Charset Encoding) Model/Algorithm Time (sec.)
JUMAN 6.0 (EUC-JP) Markov model w/ hand-tuned costs 161.09
MeCab 0.98 (UTF-8) w/ jumandic CRFs 13.71
KyTea 0.3.3 (UTF-8) w/ jumandic Pointwise prediction w/ SVM 188.01
Ours (UTF-8) Maximum matching w/ rules 3.22
</table>
<tableCaption confidence="0.98564">
Table 3: Running time on the Web320K corpus. We used a PC (Intel Xeon 2.33 GHz with 8GB memory
on FreeBSD 6.3). The model for segmentation of KyTea (Neubig et al., 2011) in our experiments is
trained with the word list of JUMAN on KC4 (see in Section 4.1).
</tableCaption>
<table confidence="0.9721793">
System F-measure
JUMAN 6.0 0.9821
MeCab 0.98 w/ jumandic 0.9861
KyTea 0.3.3 w/ jumandic 0.9789
MEMMs (Uchimoto et al., 2001) 0.9644
HMMs (Kudo et al., 2004, Table 3) 0.9622
Ours w/ KC4 0.9470
Ours w/ KC4 + post-proc. 0.9680
Ours w/ Web320K 0.9555
Ours w/ Web320K + post-proc. 0.9719
</table>
<tableCaption confidence="0.870197">
Table 4: Performance comparison to other sys-
tems.
</tableCaption>
<bodyText confidence="0.997485615384615">
kanji3, Latin characters, and katakana4 ones. This
type of post processing is commonly used in
Japanese morphological analysis. JUMAN and
MeCab have a similar mechanism and use it.
As an additional experiment, we incorporated
this post processing into our segmenter and mea-
sured the performance. The result is shown in Ta-
ble 4. The segmenter with the post processing
yields an F-measure of 0.9719 when it is trained
on the 320k Web corpus. We observed that the
performance gap between state-of-the-art systems
such as JUMAN and MeCab and ours becomes
smaller. Additional computation time was +10%
</bodyText>
<footnote confidence="0.953316">
3 Kanji in Japanese, or hanzi in Chinese, is a ideographic
script. Kanji means Chinese characters.
4 Katakana is one of the phonetic scripts used in Japanese.
It is mainly used to denote loan words and onomatopoeias.
Such type of words are very productive and are often un-
known words in Japanese language processing.
</footnote>
<bodyText confidence="0.9998715">
for the post processing and this means the seg-
menter with the post processing is still much faster
than other sophisticated MA systems. Many ap-
plications which have to process a huge amount of
documents would gain the benefits from our pro-
posed methods.
</bodyText>
<sectionHeader confidence="0.999984" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999962">
The use of transformational rules for improving
word segmentation as well as morphological anal-
ysis is not new. It is found in previous work (Papa-
georgiou, 1994; Palmer, 1997; Hockenmaier and
Brew, 1998; Gao et al., 2004). However, their ap-
proaches require the Viterbi search and/or a heavy
post process such as cascaded transformation in
order to rewrite the output of the base segmenter.
This leads to slow execution and systems that in-
corporate such approaches have much higher cost
of computation than ours.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998057">
We have proposed a new combination of maxi-
mum matching and fully lexicalized transforma-
tional rules. The proposed method allows us to
carry out considerably faster word segmentation
with a practically reasonable accuracy. We have
evaluated the effectiveness of our method on cor-
pora in Japanese. The experimental results show
that we can combine our methods with either
an existing morphological analyzer or a human-
edited training corpus.
</bodyText>
<page confidence="0.998283">
82
</page>
<sectionHeader confidence="0.99609" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993141328125">
Jun-Ichi Aoe. 1989. An efficient digital search al-
gorithm by using a double-array structure. IEEE
Transactions on Software Engineering, 15(9):1066–
1077.
Eric Brill. 1995. Transformation-based error driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543–565.
Jianfeng Gao, Andi Wu, Cheng-Ning Huang,
Hong qiao Li, Xinsong Xia, and Hauwei Qin.
2004. Adaptive Chinese word segmentation. In
Proc. ofACL-2004, pages 462–469.
Julia Hockenmaier and Chris Brew. 1998. Error-driven
learning of Chinese word segmentation. In Proc. of
PACLIC 12, pages 218–229.
Nobuhiro Kaji, Yasuhiro Fujiwara, Naoki Yoshinaga,
and Masaru Kitsuregawa. 2010. Efficient staggered
decoding for sequence labeling. In Proc. of ACL
2010, pages 485–494.
Daisuke Kawahara and Sadao Kurohashi. 2006.
Case frame compilation from the web using high-
performance computing. In Proc. of LREC 2006,
pages 1344–1347.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying conditional random fields to
Japanese morphological analysis. In Proc. of
EMNLP 2004, pages 230–237.
Sadao Kurohashi and Daisuke Kawahara. 2007.
JUMAN (a User-Extensible Morphological An-
alyzer for Japanese). http://nlp.ist.
i.kyoto-u.ac.jp/index.php?JUMAN,
http://nlp.ist.i.kyoto-u.ac.jp/EN/
index.php?JUMAN.
Sadao Kurohashi and Makoto Nagao. 2003. Build-
ing a Japanese parsed corpus. In Anne Abeille, edi-
tor, Treebanks: Building and Using Parsed Corpora,
pages 249–260. Kluwer Academic Publishers.
Masaaki Nagata. 1994. A stochastic Japanese morpho-
logical analyzer using a forward-DP backward-A*
n-best search algorithm. In Proc. of COLING-94,
pages 201–207.
Masaaki Nagata. 1997. A self-organizing Japanese
word segmenter using heuristic word identification
and re-estimation. In Proc. of WVLC-5, pages 203–
215.
Graham Neubig, Yosuke Nagata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
Japanese morphological analysis. In Proc. of ACL-
2011.
David D. Palmer. 1997. A trainable rule-based algo-
rithm for word segmentation. In Proc. ofACL-1997,
pages 321–328.
Constantine P. Papageorgiou. 1994. Japanese word
segmentation by hidden Markov model. In Proc. of
HLT-1994, pages 283–288.
Toshihisa Tashiro, Noriyoshi Uratani, and Tsuyoshi
Morimoto. 1994. Restructuring tagged corpora with
morpheme adjustment rules. In Proc. of COLING-
1994, pages 569–573.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The unknown word problem: a morpho-
logical analysis of Japanese using maximum entropy
aided by a dictionary. In Proc. of EMNLP 2001,
pages 91–99.
</reference>
<page confidence="0.999235">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.506488">
<title confidence="0.910839333333333">Deterministic Word Segmentation Using Maximum Matching with Fully Lexicalized Rules Manabu</title>
<author confidence="0.825311">Yahoo Japan Midtown Tower</author>
<email confidence="0.970692">msassano@yahoo-corp.jp</email>
<abstract confidence="0.997749529411765">We present a fast algorithm of word segmentation that scans an input sentence in a deterministic manner just one time. The algorithm is based on simple maximum matching which includes execution of fully lexicalized transformational rules. Since the process of rule matching is incorporated into dictionary lookup, fast segmentation is achieved. We evaluated the proposed method on word segmentation of Japanese. Experimental results show that our segmenter runs considerably faster than the state-of-the-art systems and yields a practical accuracy when a more accurate segmenter or an annotated corpus is available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jun-Ichi Aoe</author>
</authors>
<title>An efficient digital search algorithm by using a double-array structure.</title>
<date>1989</date>
<journal>IEEE Transactions on Software Engineering,</journal>
<volume>15</volume>
<issue>9</issue>
<pages>1077</pages>
<contexts>
<context position="6935" citStr="Aoe, 1989" startWordPosition="1154" endWordPosition="1155">nto a single process. In other words, if we find a sequence of characters of the left hand side of a certain rule, then we write out the right hand side of the rule immediately. This construction enables us to naturally incorporate execution (or application) of transformational rules into dictionarylookup, i.e., maximum matching. Although the algorithm in Figure 1 does not specify the algorithm or the implementation of function Lookup(), a trie is suitable for the structure of the dictionary. It is known that an efficient implementation of a trie is realized by using a double-array structure (Aoe, 1989), which enables us to look up a given key at the O(n) cost, where n is the length of the key. In this case the computation cost of the algorithm of Figure 1 is O(n). We can see in Figure 1 that the Viterbi search is not executed and the average number of dictionary lookups is fewer than the number of characters of an input sentence because the average length of words is longer than one. This contrasts with Viterbi-based algorithms of word segmentation or morphological analysis that always require dictionary lookup at each character position in a sentence. 3 Learning Transformational Rules 3.1 </context>
</contexts>
<marker>Aoe, 1989</marker>
<rawString>Jun-Ichi Aoe. 1989. An efficient digital search algorithm by using a double-array structure. IEEE Transactions on Software Engineering, 15(9):1066– 1077.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="7778" citStr="Brill, 1995" startWordPosition="1300" endWordPosition="1301">d the average number of dictionary lookups is fewer than the number of characters of an input sentence because the average length of words is longer than one. This contrasts with Viterbi-based algorithms of word segmentation or morphological analysis that always require dictionary lookup at each character position in a sentence. 3 Learning Transformational Rules 3.1 Framework of Learning The algorithm in Figure 1 can be combined with rules learned from a reference corpus as well as hand-crafted rules. We used here a modified version of Brill’s error-driven transformation-based learning (TBL) (Brill, 1995) for rule learning. In our system, an initial system is a word segmenter that uses maximum matching with a given 2 If we use simple maximum matching, i.e., with no transformational rules, to segment the samples here, we will get wrong segmentations as follows: はないか → はな (ha-na; “flower”) いか (i-ka; “squid”), 大工学部 → 大工 (dai-ku; “carpenter”) 学部 (gaku-bu; “faculty”). 80 0 00 w0w1 ··· wn → w0w1 ··· wm L w00w01 · · · w0n → L w0w1 · · · wm w00w01 · · · w0n R → w0w1 · · · wm R Lw00w01 ···w0n R → Lw0w1 ···wm R Table 1: Rule templates for error-driven learning word list and words which occur in a given </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Andi Wu</author>
</authors>
<title>Cheng-Ning Huang, Hong qiao Li, Xinsong Xia, and Hauwei Qin.</title>
<date>2004</date>
<booktitle>In Proc. ofACL-2004,</booktitle>
<pages>462--469</pages>
<marker>Gao, Wu, 2004</marker>
<rawString>Jianfeng Gao, Andi Wu, Cheng-Ning Huang, Hong qiao Li, Xinsong Xia, and Hauwei Qin. 2004. Adaptive Chinese word segmentation. In Proc. ofACL-2004, pages 462–469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Chris Brew</author>
</authors>
<title>Error-driven learning of Chinese word segmentation.</title>
<date>1998</date>
<booktitle>In Proc. of PACLIC 12,</booktitle>
<pages>218--229</pages>
<contexts>
<context position="3520" citStr="Hockenmaier and Brew, 1998" startWordPosition="550" endWordPosition="553">y is not good enough and the accuracy of MA is much higher. In this paper we investigate to improve the accuracy of maximummatching based word segmentation while keeping speedy processing. 2 Segmentation Algorithm Our algorithm is basically based on maximum matching, or longest matching (Nagata, 1997). Although maximum matching is very simple and easy to implement, a segmenter with this algorithm is not sufficiently accurate. For the purpose of improving the segmentation accuracy, several methods that can be combined with maximum matching have been examined. In previous studies (Palmer, 1997; Hockenmaier and Brew, 1998), the combination of maximum matching and characterbased transformational rules has been investigated for Chinese. They have reported promising results in terms of accuracy and have not mentioned the running time of their methods, which might supposedly be very slow because we have to scan an input sentence many times to apply learned transformational rules. In order to avoid such heavy post processing, we simplify the type of rules and incorporate the process of applying rules into a single process of maximum matching for dictionary lookup. We 79 Proceedings of the 14th Conference of the Euro</context>
<context position="15126" citStr="Hockenmaier and Brew, 1998" startWordPosition="2603" endWordPosition="2606">is mainly used to denote loan words and onomatopoeias. Such type of words are very productive and are often unknown words in Japanese language processing. for the post processing and this means the segmenter with the post processing is still much faster than other sophisticated MA systems. Many applications which have to process a huge amount of documents would gain the benefits from our proposed methods. 5 Related Work The use of transformational rules for improving word segmentation as well as morphological analysis is not new. It is found in previous work (Papageorgiou, 1994; Palmer, 1997; Hockenmaier and Brew, 1998; Gao et al., 2004). However, their approaches require the Viterbi search and/or a heavy post process such as cascaded transformation in order to rewrite the output of the base segmenter. This leads to slow execution and systems that incorporate such approaches have much higher cost of computation than ours. 6 Conclusion We have proposed a new combination of maximum matching and fully lexicalized transformational rules. The proposed method allows us to carry out considerably faster word segmentation with a practically reasonable accuracy. We have evaluated the effectiveness of our method on co</context>
</contexts>
<marker>Hockenmaier, Brew, 1998</marker>
<rawString>Julia Hockenmaier and Chris Brew. 1998. Error-driven learning of Chinese word segmentation. In Proc. of PACLIC 12, pages 218–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Yasuhiro Fujiwara</author>
<author>Naoki Yoshinaga</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Efficient staggered decoding for sequence labeling.</title>
<date>2010</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>485--494</pages>
<contexts>
<context position="2382" citStr="Kaji et al. (2010)" startWordPosition="366" endWordPosition="369">ot put white spaces between words. Therefore, we have to do morphological analysis in order to segment a given sentence into words and give an associated part-of-speech (POS) tag to each word. In the main stream of the research of Japanese language processing, morphological analysis has meant to be a joint task of segmentation and POS tagging. the best sequence of words along with their partof-speech tags using a dictionary where they use the Viterbi search (e.g., (Nagata, 1994), (Kudo et al., 2004)). However, computation cost of modern MA systems is mainly attributed to the Viterbi search as Kaji et al. (2010) point out. One of methods of improving the speed of MA or word segmentation will be to avoid or reduce the Viterbi search. We can avoid this by using maximum matching in the case of word segmentation. Since there are many applications such as IR and text classification, where part-of-speech tags are not mandatory, in this paper we focus on word segmentation and adopt maximum matching for it. However, maximum matching for Japanese word segmentation is rarely used these days because the segmentation accuracy is not good enough and the accuracy of MA is much higher. In this paper we investigate </context>
</contexts>
<marker>Kaji, Fujiwara, Yoshinaga, Kitsuregawa, 2010</marker>
<rawString>Nobuhiro Kaji, Yasuhiro Fujiwara, Naoki Yoshinaga, and Masaru Kitsuregawa. 2010. Efficient staggered decoding for sequence labeling. In Proc. of ACL 2010, pages 485–494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Case frame compilation from the web using highperformance computing.</title>
<date>2006</date>
<booktitle>In Proc. of LREC</booktitle>
<pages>1344--1347</pages>
<contexts>
<context position="10128" citStr="Kawahara and Kurohashi, 2006" startWordPosition="1755" endWordPosition="1758">ules associated with the score h, we apply each candidate rule in decreasing order of h and compute the error reduction. If we get positive reduction, we obtain this rule and incorporate it into the current dictionary and then proceed to the next iteration. If we do not find any rules that reduce errors, we terminate the learning process. 4 Experiments and Discussion 4.1 Corpora and an Initial Word List In our experiments for Japanese we used the Kyoto University Text Corpus Version 4 (we call it KC4) (Kurohashi and Nagao, 2003), which includes newspaper articles, and 470M Japanese sentences (Kawahara and Kurohashi, 2006), which is compiled from the Web. For training, we used two sets of the corpus. The first set is the articles on January 1st through 8th (7,635 sentences) of KC4. The second one is 320,000 sentences that are selected from the 470M Web corpus. Note that the Web corpus is not annotated and we use it after word segmentation is given by JUMAN 6.0 (Kurohashi and Kawahara, 2007). The test data is a set of sentences in the articles on January 9th (1,220 sentences). The articles on January 10th were used for development. We used all the words in the dictionary of JUMAN 6.0 as an initial word list. The</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006. Case frame compilation from the web using highperformance computing. In Proc. of LREC 2006, pages 1344–1347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Appliying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>230--237</pages>
<contexts>
<context position="2268" citStr="Kudo et al., 2004" startWordPosition="346" endWordPosition="349">logical analysis (MA) are based on methods of finding &apos; Japanese has a conjugation system in morphology and does not put white spaces between words. Therefore, we have to do morphological analysis in order to segment a given sentence into words and give an associated part-of-speech (POS) tag to each word. In the main stream of the research of Japanese language processing, morphological analysis has meant to be a joint task of segmentation and POS tagging. the best sequence of words along with their partof-speech tags using a dictionary where they use the Viterbi search (e.g., (Nagata, 1994), (Kudo et al., 2004)). However, computation cost of modern MA systems is mainly attributed to the Viterbi search as Kaji et al. (2010) point out. One of methods of improving the speed of MA or word segmentation will be to avoid or reduce the Viterbi search. We can avoid this by using maximum matching in the case of word segmentation. Since there are many applications such as IR and text classification, where part-of-speech tags are not mandatory, in this paper we focus on word segmentation and adopt maximum matching for it. However, maximum matching for Japanese word segmentation is rarely used these days because</context>
<context position="11468" citStr="Kudo et al., 2004" startWordPosition="1989" endWordPosition="1992">of-speech tags from the entries in the original dictionary of JUMAN 6.0. 4.2 Results and Discussion Segmentation Performance We used word based F-measure and character-wise accuracy to evaluate the segmentation performance. Table 2 shows comparison of various systems including ours. It is natural that since our system uses only fully lexicalized rules and does not use any generalized rules, it achieves a moderate performance. However, by using the Web corpus that contains 320,000 sentences, it yields an F-measure of near 0.96, which is at the same level as the F-measure of HMMs (baseline) in (Kudo et al., 2004, Table 3). We will discuss how we can improve it in a later section. Segmentation Speed Table 3 shows comparison of the segmentation speed of various systems for 320,000 sentences of the Web corpus. Since, in general, such comparison is heavily dependent on the implementation of the systems, we have to be careful for drawing any conclusion. However, we can see that our system, which does not use the Viterbi search, achieved considerably higher processing speed than other systems. Further Improvement The method that we have presented so far is based on lexicalized rules. That is, we do not hav</context>
<context position="12843" citStr="Kudo et al., 2004" startWordPosition="2221" endWordPosition="2224">n are split into single letters. Similarly, a number that does not appear in the training corpus is split into digits. It is possible to improve the presented method by incorporating relatively simple post-processing that concatenates Arabic numerals, numerals in 81 System # of Sent. F-measure Char. Acc. # of Rules JUMAN 6.0 NA 0.9821 0.9920 NA MeCab 0.98 w/ jumandic 7,958 0.9861 0.9939 NA Ours w/o training corpus 0 0.8474 0.9123 0 Ours w/ KC4 7,635 0.9470 0.9693 2228 w/ Web320K 320,000 0.9555 0.9769 24267 Table 2: Performance summary of various systems and configurations. Jumandic for MeCab (Kudo et al., 2004) is stemmed from the dictionary of JUMAN. System (Charset Encoding) Model/Algorithm Time (sec.) JUMAN 6.0 (EUC-JP) Markov model w/ hand-tuned costs 161.09 MeCab 0.98 (UTF-8) w/ jumandic CRFs 13.71 KyTea 0.3.3 (UTF-8) w/ jumandic Pointwise prediction w/ SVM 188.01 Ours (UTF-8) Maximum matching w/ rules 3.22 Table 3: Running time on the Web320K corpus. We used a PC (Intel Xeon 2.33 GHz with 8GB memory on FreeBSD 6.3). The model for segmentation of KyTea (Neubig et al., 2011) in our experiments is trained with the word list of JUMAN on KC4 (see in Section 4.1). System F-measure JUMAN 6.0 0.9821 M</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Appliying conditional random fields to Japanese morphological analysis. In Proc. of EMNLP 2004, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Daisuke Kawahara</author>
</authors>
<date>2007</date>
<note>JUMAN (a User-Extensible Morphological Analyzer for Japanese). http://nlp.ist. i.kyoto-u.ac.jp/index.php?JUMAN, http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?JUMAN.</note>
<contexts>
<context position="10503" citStr="Kurohashi and Kawahara, 2007" startWordPosition="1825" endWordPosition="1829">orpora and an Initial Word List In our experiments for Japanese we used the Kyoto University Text Corpus Version 4 (we call it KC4) (Kurohashi and Nagao, 2003), which includes newspaper articles, and 470M Japanese sentences (Kawahara and Kurohashi, 2006), which is compiled from the Web. For training, we used two sets of the corpus. The first set is the articles on January 1st through 8th (7,635 sentences) of KC4. The second one is 320,000 sentences that are selected from the 470M Web corpus. Note that the Web corpus is not annotated and we use it after word segmentation is given by JUMAN 6.0 (Kurohashi and Kawahara, 2007). The test data is a set of sentences in the articles on January 9th (1,220 sentences). The articles on January 10th were used for development. We used all the words in the dictionary of JUMAN 6.0 as an initial word list. The number of the words in the dictionary is 542,061. They are generated by removing the grammatical information such as part-of-speech tags from the entries in the original dictionary of JUMAN 6.0. 4.2 Results and Discussion Segmentation Performance We used word based F-measure and character-wise accuracy to evaluate the segmentation performance. Table 2 shows comparison of </context>
</contexts>
<marker>Kurohashi, Kawahara, 2007</marker>
<rawString>Sadao Kurohashi and Daisuke Kawahara. 2007. JUMAN (a User-Extensible Morphological Analyzer for Japanese). http://nlp.ist. i.kyoto-u.ac.jp/index.php?JUMAN, http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?JUMAN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Building a Japanese parsed corpus.</title>
<date>2003</date>
<booktitle>In Anne Abeille, editor, Treebanks: Building and Using Parsed Corpora,</booktitle>
<pages>249--260</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="10033" citStr="Kurohashi and Nagao, 2003" startWordPosition="1743" endWordPosition="1746">00w01 · · · w0n and m is the number of words in w0w1 · · · wm. After sorting the generated rules associated with the score h, we apply each candidate rule in decreasing order of h and compute the error reduction. If we get positive reduction, we obtain this rule and incorporate it into the current dictionary and then proceed to the next iteration. If we do not find any rules that reduce errors, we terminate the learning process. 4 Experiments and Discussion 4.1 Corpora and an Initial Word List In our experiments for Japanese we used the Kyoto University Text Corpus Version 4 (we call it KC4) (Kurohashi and Nagao, 2003), which includes newspaper articles, and 470M Japanese sentences (Kawahara and Kurohashi, 2006), which is compiled from the Web. For training, we used two sets of the corpus. The first set is the articles on January 1st through 8th (7,635 sentences) of KC4. The second one is 320,000 sentences that are selected from the 470M Web corpus. Note that the Web corpus is not annotated and we use it after word segmentation is given by JUMAN 6.0 (Kurohashi and Kawahara, 2007). The test data is a set of sentences in the articles on January 9th (1,220 sentences). The articles on January 10th were used for</context>
</contexts>
<marker>Kurohashi, Nagao, 2003</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 2003. Building a Japanese parsed corpus. In Anne Abeille, editor, Treebanks: Building and Using Parsed Corpora, pages 249–260. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A stochastic Japanese morphological analyzer using a forward-DP backward-A* n-best search algorithm.</title>
<date>1994</date>
<booktitle>In Proc. of COLING-94,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="2247" citStr="Nagata, 1994" startWordPosition="344" endWordPosition="345"> Japanese morphological analysis (MA) are based on methods of finding &apos; Japanese has a conjugation system in morphology and does not put white spaces between words. Therefore, we have to do morphological analysis in order to segment a given sentence into words and give an associated part-of-speech (POS) tag to each word. In the main stream of the research of Japanese language processing, morphological analysis has meant to be a joint task of segmentation and POS tagging. the best sequence of words along with their partof-speech tags using a dictionary where they use the Viterbi search (e.g., (Nagata, 1994), (Kudo et al., 2004)). However, computation cost of modern MA systems is mainly attributed to the Viterbi search as Kaji et al. (2010) point out. One of methods of improving the speed of MA or word segmentation will be to avoid or reduce the Viterbi search. We can avoid this by using maximum matching in the case of word segmentation. Since there are many applications such as IR and text classification, where part-of-speech tags are not mandatory, in this paper we focus on word segmentation and adopt maximum matching for it. However, maximum matching for Japanese word segmentation is rarely us</context>
</contexts>
<marker>Nagata, 1994</marker>
<rawString>Masaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-DP backward-A* n-best search algorithm. In Proc. of COLING-94, pages 201–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A self-organizing Japanese word segmenter using heuristic word identification and re-estimation.</title>
<date>1997</date>
<booktitle>In Proc. of WVLC-5,</booktitle>
<pages>203--215</pages>
<contexts>
<context position="3195" citStr="Nagata, 1997" startWordPosition="500" endWordPosition="501">. Since there are many applications such as IR and text classification, where part-of-speech tags are not mandatory, in this paper we focus on word segmentation and adopt maximum matching for it. However, maximum matching for Japanese word segmentation is rarely used these days because the segmentation accuracy is not good enough and the accuracy of MA is much higher. In this paper we investigate to improve the accuracy of maximummatching based word segmentation while keeping speedy processing. 2 Segmentation Algorithm Our algorithm is basically based on maximum matching, or longest matching (Nagata, 1997). Although maximum matching is very simple and easy to implement, a segmenter with this algorithm is not sufficiently accurate. For the purpose of improving the segmentation accuracy, several methods that can be combined with maximum matching have been examined. In previous studies (Palmer, 1997; Hockenmaier and Brew, 1998), the combination of maximum matching and characterbased transformational rules has been investigated for Chinese. They have reported promising results in terms of accuracy and have not mentioned the running time of their methods, which might supposedly be very slow because </context>
</contexts>
<marker>Nagata, 1997</marker>
<rawString>Masaaki Nagata. 1997. A self-organizing Japanese word segmenter using heuristic word identification and re-estimation. In Proc. of WVLC-5, pages 203– 215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Yosuke Nagata</author>
<author>Shinsuke Mori</author>
</authors>
<title>Pointwise prediction for robust, adaptable Japanese morphological analysis.</title>
<date>2011</date>
<booktitle>In Proc. of ACL2011.</booktitle>
<contexts>
<context position="13320" citStr="Neubig et al., 2011" startWordPosition="2299" endWordPosition="2302"> Web320K 320,000 0.9555 0.9769 24267 Table 2: Performance summary of various systems and configurations. Jumandic for MeCab (Kudo et al., 2004) is stemmed from the dictionary of JUMAN. System (Charset Encoding) Model/Algorithm Time (sec.) JUMAN 6.0 (EUC-JP) Markov model w/ hand-tuned costs 161.09 MeCab 0.98 (UTF-8) w/ jumandic CRFs 13.71 KyTea 0.3.3 (UTF-8) w/ jumandic Pointwise prediction w/ SVM 188.01 Ours (UTF-8) Maximum matching w/ rules 3.22 Table 3: Running time on the Web320K corpus. We used a PC (Intel Xeon 2.33 GHz with 8GB memory on FreeBSD 6.3). The model for segmentation of KyTea (Neubig et al., 2011) in our experiments is trained with the word list of JUMAN on KC4 (see in Section 4.1). System F-measure JUMAN 6.0 0.9821 MeCab 0.98 w/ jumandic 0.9861 KyTea 0.3.3 w/ jumandic 0.9789 MEMMs (Uchimoto et al., 2001) 0.9644 HMMs (Kudo et al., 2004, Table 3) 0.9622 Ours w/ KC4 0.9470 Ours w/ KC4 + post-proc. 0.9680 Ours w/ Web320K 0.9555 Ours w/ Web320K + post-proc. 0.9719 Table 4: Performance comparison to other systems. kanji3, Latin characters, and katakana4 ones. This type of post processing is commonly used in Japanese morphological analysis. JUMAN and MeCab have a similar mechanism and use it</context>
</contexts>
<marker>Neubig, Nagata, Mori, 2011</marker>
<rawString>Graham Neubig, Yosuke Nagata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable Japanese morphological analysis. In Proc. of ACL2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
</authors>
<title>A trainable rule-based algorithm for word segmentation.</title>
<date>1997</date>
<booktitle>In Proc. ofACL-1997,</booktitle>
<pages>321--328</pages>
<contexts>
<context position="3491" citStr="Palmer, 1997" startWordPosition="548" endWordPosition="549">tation accuracy is not good enough and the accuracy of MA is much higher. In this paper we investigate to improve the accuracy of maximummatching based word segmentation while keeping speedy processing. 2 Segmentation Algorithm Our algorithm is basically based on maximum matching, or longest matching (Nagata, 1997). Although maximum matching is very simple and easy to implement, a segmenter with this algorithm is not sufficiently accurate. For the purpose of improving the segmentation accuracy, several methods that can be combined with maximum matching have been examined. In previous studies (Palmer, 1997; Hockenmaier and Brew, 1998), the combination of maximum matching and characterbased transformational rules has been investigated for Chinese. They have reported promising results in terms of accuracy and have not mentioned the running time of their methods, which might supposedly be very slow because we have to scan an input sentence many times to apply learned transformational rules. In order to avoid such heavy post processing, we simplify the type of rules and incorporate the process of applying rules into a single process of maximum matching for dictionary lookup. We 79 Proceedings of th</context>
<context position="15098" citStr="Palmer, 1997" startWordPosition="2601" endWordPosition="2602"> Japanese. It is mainly used to denote loan words and onomatopoeias. Such type of words are very productive and are often unknown words in Japanese language processing. for the post processing and this means the segmenter with the post processing is still much faster than other sophisticated MA systems. Many applications which have to process a huge amount of documents would gain the benefits from our proposed methods. 5 Related Work The use of transformational rules for improving word segmentation as well as morphological analysis is not new. It is found in previous work (Papageorgiou, 1994; Palmer, 1997; Hockenmaier and Brew, 1998; Gao et al., 2004). However, their approaches require the Viterbi search and/or a heavy post process such as cascaded transformation in order to rewrite the output of the base segmenter. This leads to slow execution and systems that incorporate such approaches have much higher cost of computation than ours. 6 Conclusion We have proposed a new combination of maximum matching and fully lexicalized transformational rules. The proposed method allows us to carry out considerably faster word segmentation with a practically reasonable accuracy. We have evaluated the effec</context>
</contexts>
<marker>Palmer, 1997</marker>
<rawString>David D. Palmer. 1997. A trainable rule-based algorithm for word segmentation. In Proc. ofACL-1997, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantine P Papageorgiou</author>
</authors>
<title>Japanese word segmentation by hidden Markov model.</title>
<date>1994</date>
<booktitle>In Proc. of HLT-1994,</booktitle>
<pages>283--288</pages>
<contexts>
<context position="15084" citStr="Papageorgiou, 1994" startWordPosition="2598" endWordPosition="2600">etic scripts used in Japanese. It is mainly used to denote loan words and onomatopoeias. Such type of words are very productive and are often unknown words in Japanese language processing. for the post processing and this means the segmenter with the post processing is still much faster than other sophisticated MA systems. Many applications which have to process a huge amount of documents would gain the benefits from our proposed methods. 5 Related Work The use of transformational rules for improving word segmentation as well as morphological analysis is not new. It is found in previous work (Papageorgiou, 1994; Palmer, 1997; Hockenmaier and Brew, 1998; Gao et al., 2004). However, their approaches require the Viterbi search and/or a heavy post process such as cascaded transformation in order to rewrite the output of the base segmenter. This leads to slow execution and systems that incorporate such approaches have much higher cost of computation than ours. 6 Conclusion We have proposed a new combination of maximum matching and fully lexicalized transformational rules. The proposed method allows us to carry out considerably faster word segmentation with a practically reasonable accuracy. We have evalu</context>
</contexts>
<marker>Papageorgiou, 1994</marker>
<rawString>Constantine P. Papageorgiou. 1994. Japanese word segmentation by hidden Markov model. In Proc. of HLT-1994, pages 283–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshihisa Tashiro</author>
<author>Noriyoshi Uratani</author>
<author>Tsuyoshi Morimoto</author>
</authors>
<title>Restructuring tagged corpora with morpheme adjustment rules.</title>
<date>1994</date>
<booktitle>In Proc. of COLING1994,</booktitle>
<pages>569--573</pages>
<contexts>
<context position="8727" citStr="Tashiro et al., 1994" startWordPosition="1478" endWordPosition="1481">“carpenter”) 学部 (gaku-bu; “faculty”). 80 0 00 w0w1 ··· wn → w0w1 ··· wm L w00w01 · · · w0n → L w0w1 · · · wm w00w01 · · · w0n R → w0w1 · · · wm R Lw00w01 ···w0n R → Lw0w1 ···wm R Table 1: Rule templates for error-driven learning word list and words which occur in a given reference corpus (a training corpus). Our segmenter treats an unknown word, which is not in the dictionary, as a one-character word as shown in Figure 1. 3.2 Generating Candidate Rules In order to generate candidate rules, first we compare the output of the current system with the reference corpus and extract the differences (Tashiro et al., 1994) as rules that have the following form: L w00w01 · · · w0n R → L w0w1 · · · wm R where w00w01 · · · w0n is a word sequence in the system output and w0w1 · · · wm is a word sequence in the reference corpus and L is a word in the left context and R is a word in the right context. After this extraction process, we generate four lexicalized rules from each extracted rule by using the templates defined in Table 1. 3.3 Learning Rules In order to reduce huge computation when learning a rule at each iteration of TBL, we use some heuristic strategy. The heuristic score h is defined as: h = f ∗ (n + m) </context>
</contexts>
<marker>Tashiro, Uratani, Morimoto, 1994</marker>
<rawString>Toshihisa Tashiro, Noriyoshi Uratani, and Tsuyoshi Morimoto. 1994. Restructuring tagged corpora with morpheme adjustment rules. In Proc. of COLING1994, pages 569–573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>The unknown word problem: a morphological analysis of Japanese using maximum entropy aided by a dictionary.</title>
<date>2001</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>91--99</pages>
<contexts>
<context position="13532" citStr="Uchimoto et al., 2001" startWordPosition="2336" endWordPosition="2339"> Model/Algorithm Time (sec.) JUMAN 6.0 (EUC-JP) Markov model w/ hand-tuned costs 161.09 MeCab 0.98 (UTF-8) w/ jumandic CRFs 13.71 KyTea 0.3.3 (UTF-8) w/ jumandic Pointwise prediction w/ SVM 188.01 Ours (UTF-8) Maximum matching w/ rules 3.22 Table 3: Running time on the Web320K corpus. We used a PC (Intel Xeon 2.33 GHz with 8GB memory on FreeBSD 6.3). The model for segmentation of KyTea (Neubig et al., 2011) in our experiments is trained with the word list of JUMAN on KC4 (see in Section 4.1). System F-measure JUMAN 6.0 0.9821 MeCab 0.98 w/ jumandic 0.9861 KyTea 0.3.3 w/ jumandic 0.9789 MEMMs (Uchimoto et al., 2001) 0.9644 HMMs (Kudo et al., 2004, Table 3) 0.9622 Ours w/ KC4 0.9470 Ours w/ KC4 + post-proc. 0.9680 Ours w/ Web320K 0.9555 Ours w/ Web320K + post-proc. 0.9719 Table 4: Performance comparison to other systems. kanji3, Latin characters, and katakana4 ones. This type of post processing is commonly used in Japanese morphological analysis. JUMAN and MeCab have a similar mechanism and use it. As an additional experiment, we incorporated this post processing into our segmenter and measured the performance. The result is shown in Table 4. The segmenter with the post processing yields an F-measure of 0</context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 2001</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 2001. The unknown word problem: a morphological analysis of Japanese using maximum entropy aided by a dictionary. In Proc. of EMNLP 2001, pages 91–99.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>