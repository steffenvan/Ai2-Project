<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.9971355">
Maximum Entropy based Rule Selection Model for
Syntax-based Statistical Machine Translation
</title>
<author confidence="0.991696">
Qun Liu1 and Zhongjun He1n and Yang Liu1 and Shouxun Lin1
</author>
<affiliation confidence="0.995532">
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
Beijing, 100190, China
2Graduate University of Chinese Academy of Sciences
</affiliation>
<address confidence="0.611767">
Beijing, 100049, China
</address>
<email confidence="0.97631">
{liuqun,zjhe,yliu,sxlin}@ict.ac.cn
</email>
<figure confidence="0.481212">
Abstract
NP
NP
</figure>
<bodyText confidence="0.999055071428571">
This paper proposes a novel maximum en-
tropy based rule selection (MERS) model
for syntax-based statistical machine transla-
tion (SMT). The MERS model combines lo-
cal contextual information around rules and
information of sub-trees covered by variables
in rules. Therefore, our model allows the de-
coder to perform context-dependent rule se-
lection during decoding. We incorporate the
MERS model into a state-of-the-art linguis-
tically syntax-based SMT model, the tree-
to-string alignment template model. Experi-
ments show that our approach achieves signif-
icant improvements over the baseline system.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999333117647059">
Syntax-based statistical machine translation (SMT)
models (Liu et al., 2006; Galley et al., 2006; Huang
et al., 2006) capture long distance reorderings by us-
ing rules with structural and linguistical information
as translation knowledge. Typically, a translation
rule consists of a source-side and a target-side. How-
ever, the source-side of a rule usually corresponds
to multiple target-sides in multiple rules. Therefore,
during decoding, the decoder should select a correct
target-side for a source-side. We call this rule selec-
tion.
Rule selection is of great importance to syntax-
based SMT systems. Comparing with word selec-
tion in word-based SMT and phrase selection in
phrase-based SMT, rule selection is more generic
and important. This is because that a rule not only
contains terminals (words or phrases), but also con-
</bodyText>
<page confidence="0.996945">
89
</page>
<figure confidence="0.636306">
DNP NPB DNP NPB
</figure>
<figureCaption confidence="0.999936">
Figure 1: Example of translation rules
</figureCaption>
<bodyText confidence="0.999955173913044">
tains nonterminals and structural information. Ter-
minals indicate lexical translations, while nontermi-
nals and structural information can capture short or
long distance reorderings. See rules in Figure 1 for
illustration. These two rules share the same syntactic
tree on the source side. However, on the target side,
either the translations for terminals or the phrase re-
orderings for nonterminals are quite different. Dur-
ing decoding, when a rule is selected and applied to a
source text, both lexical translations (for terminals)
and reorderings (for nonterminals) are determined.
Therefore, rule selection affects both lexical transla-
tion and phrase reordering.
However, most of the current syntax-based sys-
tems ignore contextual information when they se-
lecting rules during decoding, especially the infor-
mation of sub-trees covered by nonterminals. For
example, the information of X, and X, is not
recorded when the rules in Figure 1 extracted from
the training examples in Figure 2. This makes the
decoder hardly distinguish the two rules. Intuitively,
information of sub-trees covered by nonterminals as
well as contextual information of rules are believed
</bodyText>
<figure confidence="0.89153">
NP
NN
NP
NN
NN
NN
DEG
DEG
X 1
X 2
X 1
X 2
X 1 X 2
levels
X 2 standard
of X 1
</figure>
<note confidence="0.83314">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89–97,
Honolulu, October 2008. c�2008 Association for Computational Linguistics
NP NP
</note>
<figureCaption confidence="0.998584">
Figure 2: Training examples for rules in Figure 1
</figureCaption>
<bodyText confidence="0.9949236">
to be helpful for rule selection.
Recent research showed that contextual infor-
mation can help perform word or phrase selec-
tion. Carpuat and Wu (2007b) and Chan et
al. (2007) showed improvents by integrating word-
sense-disambiguation (WSD) system into a phrase-
based (Koehn, 2004) and a hierarchical phrase-
based (Chiang, 2005) SMT system, respectively.
Similar to WSD, Carpuat and Wu (2007a) used con-
textual information to solve the ambiguity prob-
lem for phrases. They integrated a phrase-sense-
disambiguation (PSD) model into a phrase-based
SMT system and achieved improvements.
In this paper, we propose a novel solution for
rule selection for syntax-based SMT. We use the
maximum entropy approach to combine rich con-
textual information around a rule and the informa-
tion of sub-trees covered by nonterminals in a rule.
For each ambiguous source-side of translation rules,
a maximum entropy based rule selection (MERS)
model is built. Thus the MERS models can help the
decoder to perform a context-dependent rule selec-
tion.
Comparing with WSD (or PSD), there are some
advantages of our approach:
</bodyText>
<listItem confidence="0.9995061">
• Our approach resolves ambiguity for rules with
multi-level syntactic structure, while WSD re-
solves ambiguity for strings that have no struc-
tures;
• Our approach can help the decoder perform
both lexical selection and phrase reorderings,
while WSD can help the decoder only perform
lexical selection;
• Our method takes WSD as a special case, since
a rule may only consists of terminals.
</listItem>
<bodyText confidence="0.999590375">
In our previous work (He et al., 2008), we re-
ported improvements by integrating a MERS model
into a formally syntax-based SMT model, the hier-
archical phrase-based model (Chiang, 2005). In this
paper, we incorporate the MERS model into a state-
of-the-art linguistically syntax-based SMT model,
the tree-to-string alignment template (TAT) model
(Liu et al., 2006). The basic differences are:
</bodyText>
<listItem confidence="0.990028142857143">
• The MERS model here combines rich informa-
tion of source syntactic tree as features since
the translation model is linguistically syntax-
based. He et al. (2008) did not use this in-
formation.
• In this paper, we build MERS models for all
ambiguous source-sides, including lexicalized
(source-side which only contains terminals),
partially lexicalized (source-side which con-
tains both terminals and nonterminals), and un-
lexicalized (source-side which only contains
nonterminals). He et al. (2008) only built
MERS models for partially lexicalized source-
sides.
</listItem>
<bodyText confidence="0.999872714285714">
In the TAT model, a TAT can be considered as a
translation rule which describes correspondence be-
tween source syntactic tree and target string. TAT
can capture linguistically motivated reorderings at
short or long distance. Experiments show that by
incorporating MERS model, the baseline system
achieves statistically significant improvement.
This paper is organized as follows: Section 2
reviews the TAT model; Section 3 introduces the
MERS model and describes feature definitions; Sec-
tion 4 demonstrates a method to incorporate the
MERS model into the translation model; Section 5
reports and analyzes experimental results; Section 6
gives conclusions.
</bodyText>
<sectionHeader confidence="0.97225" genericHeader="method">
2 Baseline System
</sectionHeader>
<bodyText confidence="0.999401166666667">
Our baseline system is Lynx (Liu et al., 2006),
which is a linguistically syntax-based SMT system.
For translating a source sentence f1 = f1...f�...fi,
Lynx firstly employs a parser to produce a source
syntactic tree T(f1 ), and then uses the source
syntactic tree as the input to search translations:
</bodyText>
<table confidence="0.375403222222222">
DNP NPB DNP NPB
industrial products manufacturing
X, :NP DEG
X, :NN NN
levels
X, :NP DEG
overall standard
X,:NN NN
of the match
</table>
<page confidence="0.722435">
90
</page>
<equation confidence="0.986519">
(1) !eI1 = argmaxe,,1Pr(eI1|fJ1 )
= argmaxe,,1Pr(T(fJ1 )|fJ1 )Pr(eI 1|T(fJ1 ))
</equation>
<bodyText confidence="0.984462363636364">
In doing this, Lynx uses tree-to-string alignment
template to build relationship between source syn-
tactic tree and target string. A TAT is actually a
translation rule: the source-side is a parser tree with
leaves consisting of words and nonterminals, the
target-side is a target string consisting of words and
nonterminals.
TAT can be learned from word-aligned, source-
parsed parallel corpus. Figure 4 shows three types
of TATs extracted from the training example in Fig-
ure 3: lexicalized (the left), partially lexicalized
(the middle), unlexicalized (the right). Lexicalized
TAT contains only terminals, which is similar to
phrase-to-phrase translation in phrase-based model
except that it is constrained by a syntactic tree on the
source-side. Partially lexicalized TAT contains both
terminals and non-terminals, which can be used for
both lexical translation and phrase reordering. Un-
lexicalized TAT contains only nonterminals and can
only be used for phrase reordering.
Lynx builds translation model in a log-linear
framework (Och and Ney, 2002):
</bodyText>
<figure confidence="0.526966">
IP
</figure>
<figureCaption confidence="0.981574">
Figure 3: Word-aligned, source-parsed training example.
</figureCaption>
<figure confidence="0.724372">
NN NPB NPB
</figure>
<figureCaption confidence="0.9983545">
Figure 4: TATs learned from the training example in Fig-
ure 3.
</figureCaption>
<bodyText confidence="0.9982032">
are no features in Lynx that can capture contextual
information during decoding, except for the n-gram
language model which considers the left and right
neighboring n-1 target words. But this information
it very limited.
</bodyText>
<figure confidence="0.996540037037037">
NPB
NN NN NN
VP
VV VPB
VV
The incomes
of city and
village resident
continued to
grow
NN
NN
NN NN
NN
NN
X ,
X a
X ,
X,
city and
village
incomes of
X,
resident
X,
X,
X a
</figure>
<listItem confidence="0.873866090909091">
(2) P(eI1|T (fJ1 )) = 3 The Maximum Entropy based Rule
exp[&amp;quot;m Amhm(eI1, T(fJ1 ))] Selection Model
&amp;quot;e, exp[&amp;quot;m Amhm(eI1,T(fJ1 ))]
Following features are used:
• Translation probabilities: P(!e |T!) and P( T!|!e);
• Lexical weights: Pw(!e |T!) and Pw(
T!|!e);
• TAT penalty: exp(1), which is analogous to
phrase penalty in phrase-based model;
• Language model Plm(eI1);
• Word penalty I.
</listItem>
<bodyText confidence="0.9915882">
In Lynx, rule selection mainly depends on trans-
lation probabilities and lexical weights. These four
scores describe how well a source tree links to a tar-
get string, which are estimated on the training cor-
pus according to occurrence times of !e and T!. There
</bodyText>
<subsectionHeader confidence="0.997468">
3.1 The model
</subsectionHeader>
<bodyText confidence="0.9998682">
In this paper, we focus on using contextual infor-
mation to help the TAT model perform context-
dependent rule selection. We consider the rule se-
lection task as a multi-class classification task: for
a source syntactic tree T!, each corresponding target
string !e is a label. Thus during decoding, when a
TAT ( T!, !e&apos;) is selected, T! is classified into label !e&apos;,
actually.
A good way to solve the classification problem is
the maximum entropy approach:
</bodyText>
<equation confidence="0.571053333333333">
(3) Prs(!e |T!,T(Xk)) =
exp[&amp;quot;i Aihi(!e, C( T!),T(Xk))]
&amp;quot;!e, exp[&amp;quot;i Aihi(!e&apos;, C( T!), T(Xk))]
</equation>
<page confidence="0.976758">
91
</page>
<figure confidence="0.9954348">
tigøao
NN
gøongy`e
NN
de
chÿanpÿõn
VP
VV
DNP
X 1 :NP
DEG
(a) Lexical Features
NPB
NN
zhiz`ao
</figure>
<bodyText confidence="0.999608227272727">
where T and a are the source tree and target string of
a TAT, respectively. hi is a binary feature functions
and Ai is the feature weight of hi. C( T) defines local
contextual information of T�. Xk is a nonterminal in
the source tree T, where k is an index. T (Xk) is the
source sub-tree covered by Xk.
The advantage of the MERS model is that it uses
rich contextual information to compute posterior
probability for e given T. However, the transla-
tion probabilities and lexical weights in Lynx ignore
these information.
Note that for each ambiguous source tree, we
build a MERS model. That means, if there are
N source trees extracted from the training corpus
are ambiguous (the source tree which corresponds
to multiple translations), thus for each ambiguous
source tree Ti (i = 1, ..., N), a MERS model Mi
(i = 1, ..., N) is built. Since a source tree may cor-
respond to several hundreds of target translations at
most, the feature space of a MERS model is not pro-
hibitively large. Thus the complexity for training a
MERS model is low.
</bodyText>
<subsectionHeader confidence="0.99593">
3.2 Feature Definition
</subsectionHeader>
<bodyText confidence="0.99993025">
Let (T, e) be a translation rule in the TAT model.
We use f( T�) to represent the source phrase covered
by T�. To build a MERS model for the source tree T,
we explore various features listed below.
</bodyText>
<listItem confidence="0.864496">
1. Lexical Features (LF)
</listItem>
<bodyText confidence="0.997239666666667">
These features are defined on source words.
Specifically, there are two kinds of lexical fea-
tures: external features f−1 and f+1, which
are the source words immediately to the left
and right of f ( T), respectively; internal fea-
tures fL(T(Xk)) and fR(T(Xk)), which are
the left most and right most boundary words of
the source phrase covered by T(Xk), respec-
tively.
See Figure 5 (a) for illustration. In
this example, f−1=tigøao, f+1=zhiz`ao,
fL(T(X1))=gøongy`e, fR(T (X1))=chÿanpin.
</bodyText>
<sectionHeader confidence="0.53147" genericHeader="method">
2. Parts-of-speech (POS) Features (POSF)
</sectionHeader>
<bodyText confidence="0.982779">
These features are the POS tags of the source
words defined in the lexical features: P−1,
P+1, PL(T(Xk)), PR(T(Xk)) are the POS
tags of f−1, f+1, fL(T (Xk)), fR(T (Xk)), re-
</bodyText>
<figure confidence="0.9943305">
VP DNP NPB
VV
X 1 :NP
DEG
NN
tigøao NN NN de zhiz`ao
gøongy`e chÿanpin
(b) POS Features
DNP NP
DNP
X 1 :NP DEG
de
(c) Span Feature (d) Parent Feature
X 1 :NP
2 words
DEG
de
NP
DNP NPB
X 1 :NP DEG
de
(e) Sibling Feature
</figure>
<figureCaption confidence="0.913563333333333">
Figure 5: Illustration of features of the MERS model. The
source tree of the TAT is ( DNP(NP X 1 ) (DEG de)).
Gray nodes denote information included in the feature.
</figureCaption>
<page confidence="0.971858">
92
</page>
<bodyText confidence="0.964334">
spectively. POS tags can generalize over all
training examples.
Figure 5 (b) shows POS features. P_1=VV,
P+1=NN, PL(T (X1))=NN, PR(T (X1))=NN.
</bodyText>
<listItem confidence="0.890462">
3. Span Features (SPF)
</listItem>
<bodyText confidence="0.9978895">
These features are the length of the source
phrase f(T(Xk)) covered by T(Xk). In Liu’s
TAT model, the knowledge learned from a short
span can be used for a larger span. This is not
reliable. Thus we use span features to allow the
MERS model to learn a preference for short or
large span.
In Figure 5 (c), the span of X , is 2.
</bodyText>
<listItem confidence="0.864742">
4. Parent Feature (PF) !
</listItem>
<bodyText confidence="0.9967715">
The parent node of T in the parser tree of the
source sentence. The same source sub-tree may
have different parent nodes in different training
examples. Therefore, this feature may provide
information for distinguishing source sub-trees.
Figure 5 (d) shows that the parent is a NP node.
</bodyText>
<listItem confidence="0.621461">
5. Sibling Features (SBF) !
</listItem>
<bodyText confidence="0.99997632">
The siblings of the root of T. This feature con-
siders neighboring nodes which share the same
parent node.
In Figure 5 (e), the source tree has one sibling
node NPB.
Those features make use of rich information
around a rule, including the contextual information
of a rule and the information of sub-trees covered
by nonterminals. They are never used in Liu’s TAT
model.
Figure 5 shows features for a partially lexicalized
source tree. Furthermore, we also build MERS mod-
els for lexicalized and unlexicalized source trees.
Note that for lexicalized tree, features do not include
the information of sub-trees since there is no nonter-
minals.
The features can be easily obtained by modify-
ing the TAT extraction algorithm described in (Liu
et al., 2006). When a TAT is extracted from a
word-aligned, source-parsed parallel sentence, we
just record the contextual features and the features of
the sub-trees. Then we use the toolkit implemented
by Zhang (2004) to train MERS models for the am-
biguous source syntactic trees separately. We set the
iteration number to 100 and Gaussian prior to 1.
</bodyText>
<sectionHeader confidence="0.83334" genericHeader="method">
4 Integrating the MERS Models into the
Translation Model
</sectionHeader>
<bodyText confidence="0.986565333333333">
We integrate the MERS models into the TAT model
during the translation of each source sentence. Thus
the MERS models can help the decoder perform
context-dependent rule selection during decoding.
For integration, we add two new features into the
log-linear translation model:
</bodyText>
<listItem confidence="0.9213176">
• P�s(!e� T! , T(Xk)). This feature is computed by
the MERS model according to equation (3),
which gives a probability that the model select-
ing a target-side !e given an ambiguous source-
side T! , considering rich contextual informa-
tion.
• Pap = exp(1). During decoding, if a source
tree has multiple translations, this feature is set
to exp(1), otherwise it is set to exp(0). Since
the MERS models are only built for ambiguous
</listItem>
<bodyText confidence="0.934623166666667">
source trees, the first feature P,s(!e� T!, T(Xk))
for non-ambiguous source tree will be set to
1.0. Therefore, the decoder will prefer to
use non-ambiguous TATs. However, non-
ambiguous TATs usually occur only once in the
training corpus, which are not reliable. Thus
we use this feature to reward ambiguous TATs.
The advantage of our integration is that we need
not change the main decoding algorithm of Lynx.
Furthermore, the weights of the new features can be
trained together with other features of the translation
model.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.96949">
5.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999973125">
We carry out experiments on Chinese-to-English
translation. The training corpus is the FBIS cor-
pus, which contains 239k sentence pairs with 6.9M
Chinese words and 8.9M English words. For the
language model, we use SRI Language Modeling
Toolkit (Stolcke, 2002) with modified Kneser-Ney
smoothing (Chen and Goodman, 1998) to train two
tri-gram language models on the English portion of
</bodyText>
<page confidence="0.997108">
93
</page>
<table confidence="0.969824833333333">
Type No. of No. of No. of ambiguous % ambiguous
TATs source trees source trees
Lexicalized 333,077 16,367 14,380 87.86
Partially Lexicalized 342,767 38,497 28,397 73.76
Unlexicalized 83,024 7,384 5,991 81.13
Total 758,868 62,248 48,768 78.34
</table>
<tableCaption confidence="0.997671">
Table 1: Statistical information of TATs filtered by test sets of NIST MT 2003 and 2005.
</tableCaption>
<table confidence="0.9990395">
System Features
P(!e |T! ) P( T! |!e) Pw(!e |T!) Pw( T!|!e) lm1 lm2 TP WP P, AP
Lynx 0.210 0.016 0.081 0.051 0.171 0.013 -0.055 0.403 - -
+MERS 0.031 0.008 0.020 0.080 0.152 0.014 0.027 0.270 0.194 0.207
</table>
<tableCaption confidence="0.997337666666667">
Table 2: Feature weights obtained by minimum error rate training on the development set. The first 8 features are used
by Lynx. TP=TAT penalty, WP=word penalty, AP=ambiguous TAT penalty. Note that in fact, the positive weight for
WP and AP indicate a reward.
</tableCaption>
<bodyText confidence="0.99989825">
the training corpus and the Xinhua portion of the Gi-
gaword corpus, respectively. NIST MT 2002 test set
is used as the development set. NIST MT 2003 and
NIST MT 2005 test sets are used as the test sets.
The translation quality is evaluated by BLEU met-
ric (Papineni et al., 2002), as calculated by mteval-
v11b.pl with case-insensitive matching of n-grams,
where n = 4.
</bodyText>
<subsectionHeader confidence="0.994529">
5.2 Training
</subsectionHeader>
<bodyText confidence="0.999934565217392">
To train the translation model, we first run GIZA++
(Och and Ney, 2000) to obtain word alignment in
both translation directions. Then the word alignment
is refined by performing “grow-diag-final” method
(Koehn et al., 2003). We use a Chinese parser de-
veloped by Deyi Xiong (Xiong et al., 2005) to parse
the Chinese sentences of the training corpus.
Our TAT extraction algorithm is similar to Liu et
al. (2006), except that we make some tiny modifica-
tions to extract contextual features for MERS mod-
els. To extract TAT, we set the maximum height of
the source sub-tree to h = 3, the maximum number
of direct descendants of a node of sub-tree to c = 5.
See (Liu et al., 2006) for specific definitions of these
parameters.
Table 1 shows statistical information of TATs
which are filtered by the two test sets. For each type
(lexicalized, partially lexicalized, unlexicalized) of
TATs, a great portion of the source trees are am-
biguous. The number of ambiguous source trees ac-
counts for 78.34% of the total source trees. This in-
dicates that the TAT model faces serious rule selec-
tion problem during decoding.
</bodyText>
<subsectionHeader confidence="0.843731">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.998843416666667">
We use Lynx as the baseline system. Then the
MERS models are incorporated into Lynx, and
the system is called Lynx+MERS. To run the
decoder, Lynx and Lynx+MERS share the same
settings: tatTable-limit=30, tatTable-threshold=0,
stack-limit=100, stack-threshold=0.00001. The
meanings of the pruning parameters are the same to
Liu et al. (2006).
We perform minimum error rate training (Och,
2003) to tune the feature weights for the log-linear
model to maximize the systems’s BLEU score on the
development set. The weights are shown in Table 2.
These weights are then used to run Lynx and
Lynx+MERS on the test sets. Table 3 shows the
results. Lynx obtains BLEU scores of 26.15 on
NIST03 and 26.09 on NIST05. Using all features
described in Section 3.2, Lynx+MERS finally ob-
tains BLEU scores of 27.05 on NIST03 and 27.28
on NIST05. The absolute improvements is 0.90
and 1.19, respectively. Using the sign-test described
by Collins et al. (2005), both improvements are
statistically significant at p &lt; 0.01. Moreover,
Lynx+MERS also achieves higher n-gram preci-
sions than Lynx.
</bodyText>
<page confidence="0.99756">
94
</page>
<table confidence="0.9998205">
Test Set System BLEU-4 Individual n-gram precisions
1 2 3 4
NIST03 Lynx 26.15 71.62 35.64 18.64 9.82
+MERS 27.05 72.00 36.72 19.51 10.37
NIST05 Lynx 26.09 70.39 35.12 18.53 10.11
+MERS 27.28 71.16 36.19 19.62 10.95
</table>
<tableCaption confidence="0.999456">
Table 3: BLEU-4 scores (case-insensitive) on the test sets.
</tableCaption>
<subsectionHeader confidence="0.999518">
5.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999741567567567">
The baseline system only uses four features for
rule selection: the translation probabilities P(e |T�)
and P( T|e); and the lexical weights P,,,(e |T�) and
P,,,(T |e). These features are estimated on the train-
ing corpus by the maximum likelihood approach,
which does not allow the decoder to perform a con-
text dependent rule selection. Although Lynx uses
language model as feature, the n-gram language
model only considers the left and right n-1 neigh-
boring target words.
The MERS models combines rich contextual in-
formation as features to help the decoder perform
rule selection. Table 4 shows the effect of different
feature sets. We test two classes of feature sets: the
single feature (the top four rows of Table 4) and the
combination of features (the bottom five rows of Ta-
ble 4). For the single feature set, the POS tags are
the most useful and stable features. Using this fea-
ture, Lynx+MERS achieves improvements on both
the test sets. The reason is that POS tags can be gen-
eralized over all training examples, which can alle-
viate the data sparseness problem.
Although we find that some single features may
hurt the BLEU score, they are useful in combina-
tion of features. This is because one of the strengths
of the maximum entropy model is that it can in-
corporate various features to perform classification.
Therefore, using all features defined in Section 3.2,
we obtain statistically significant improvements (the
last row of Table 4). In order to know how the
MERS models improve translation quality, we in-
spect the 1-best outputs of Lynx and Lynx+MERS.
We find that the first way that the MERS models help
the decoder is that they can perform better selection
for words or phrases, similar to the effect of WSD
or PSD. This is because that lexicalized and partially
lexicalized TAT contains terminals. Considering the
</bodyText>
<table confidence="0.9999015">
Feature Sets NIST03 NIST05
LF 26.12 26.32
POSF 26.36 26.21
PF 26.17 25.90
SBF 26.47 26.08
LF+POSF 26.61 26.59
LF+POSF+SPF 26.70 26.44
LF+POSF+PF 26.81 26.56
LF+POSF+SBF 26.68 26.89
LF+POSF+SPF+PF+SBF 27.05 27.28
</table>
<tableCaption confidence="0.998893">
Table 4: BLEU-4 scores on different feature sets.
</tableCaption>
<bodyText confidence="0.743648">
following examples:
</bodyText>
<listItem confidence="0.983079833333333">
• Source:
• Reference: Malta is located in southern Eu-
rope
• Lynx: Malta in southern Europe
• Lynx+MERS: Malta is located in southern Eu-
rope
</listItem>
<bodyText confidence="0.997141785714286">
Here the Chinese word “ ” is incor-
rectly translated into “in” by the baseline system.
Lynx+MERS produces the correct translation “is lo-
cated in”. That is because, the MERS model consid-
ers more contextual information for rule selection.
In the MERS model, P,.3(in |) = 0.09, which is
smaller than P,.3(is located in |) = 0.14. There-
fore, the MERS model prefers the translation “is lo-
cated in”. Note that here the source tree (VV )
is lexicalized, and the role of the MERS model is
actually the same as WSD.
The second way that the MERS models help the
decoder is that they can perform better phrase re-
orderings. Considering the following examples:
</bodyText>
<page confidence="0.98101">
95
</page>
<figure confidence="0.944795666666667">
• Source: [ ]t [ ]2
...
NP
</figure>
<listItem confidence="0.977886333333333">
• Reference: According to its [development
strategy]2 [in the Chinese market]t ...
• Lynx: Accordance with [the Chinese market]t
[development strategy]2 ...
• Lynx+MERS: According to the [development
strategy]2 [in the Chinese market]t
</listItem>
<bodyText confidence="0.999976916666667">
The syntactic tree of the Chinese phrase “
” is shown in Figure 6. How-
ever, there are two TATs which can be applied to the
source tree, as shown in Figure 7. The baseline sys-
tem selects the left TAT and produces a monotone
translation of the subtrees “X 1 :PP” and “X 2 :NPB”.
However, Lynx+MERS uses the right TAT and per-
forms correct phrase reordering by swapping the two
source phrases. Here the source tree is partially lex-
icalized, and both the contextual information and
the information of sub-trees covered by nontermi-
nals are considered by the MERS model.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999965291666667">
In this paper, we propose a maximum entropy based
rule selection model for syntax-based SMT. We
use two kinds information as features: the local-
contextual information of a rule, the information of
sub-trees matched by nonterminals in a rule. During
decoding, these features allow the decoder to per-
form a context-dependent rule selection. However,
this information is never used in most of the current
syntax-based SMT models.
The advantage of the MERS model is that it can
help the decoder not only perform lexical selection,
but also phrase reorderings. We demonstrate one
way to incorporate the MERS models into a state-
of-the-art linguistically syntax-based SMT model,
the tree-to-string alignment model. Experiments
show that by incorporating the MERS models, the
baseline system achieves statistically significant im-
provements.
We find that rich contextual information can im-
prove translation quality for a syntax-based SMT
system. In future, we will explore more sophisti-
cated features for the MERS model. Moreover, we
will test the performance of the MERS model on
large scale corpus.
</bodyText>
<sectionHeader confidence="0.997363" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999917666666667">
We would like to thank Yajuan Lv for her valuable
suggestions. This work was supported by the Na-
tional Natural Science Foundation of China (NO.
60573188 and 60736014), and the High Technology
Research and Development Program of China (NO.
2006AA010108).
</bodyText>
<sectionHeader confidence="0.995796" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.8989155">
Marine Carpuat and Dekai Wu. 2007a. How phrase
sense disambiguation outperforms word sense disam-
biguation for statistical machine translation. In 11th
Conference on Theoretical and Methodological Issues
in Machine Translation, pages 43–52.
Marine Carpuat and Dekai Wu. 2007b. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of EMNLP-CoNLL 2007,
pages 61–72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
</reference>
<figure confidence="0.97123625">
DNP
NPB
of
in Chinese market
</figure>
<figureCaption confidence="0.978586">
Figure 6: Syntactic tree of the source phrase “
</figureCaption>
<figure confidence="0.9995001">
”
.
NP
NP
DNP
NPB
DNP
NPB
PP DEG
development strategy
</figure>
<figureCaption confidence="0.995188">
Figure 7: TATs which can be used for the source phrase
</figureCaption>
<figure confidence="0.992636769230769">
“ ”.
X 2
X 1
PP
DEG
X 2
X 1
PP
DEG
X 1
X 2
X 2
X 1
</figure>
<page confidence="0.966319">
96
</page>
<reference confidence="0.999558253521127">
Meeting of the Association for Computational Linguis-
tics, pages 33–40.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263–270.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Proc.
of ACL05, pages 531–540.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL 2006, pages 961–968.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 321–328.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT-NAACL 2003, pages 127–133.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115–124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics, pages
609–616.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440–447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295–302.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160–167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311–318.
Andreas Stolcke. 2002. Srilm – an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken language Processing,
volume 2, pages 901–904.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the penn chinese tree-
bank with semantic knowledge. In Proceedings of
IJCNLP 2005, pages 70–81.
Le Zhang. 2004. Maximum entropy model-
ing toolkit for python and c++. available at
http://homepages.inf.ed.ac.uk/s0450736/maxent too-
lkit.html.
</reference>
<page confidence="0.99969">
97
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.445537">
<title confidence="0.765394">Maximum Entropy based Rule Selection Model Syntax-based Statistical Machine Translation</title>
<affiliation confidence="0.954074">Laboratory of Intelligent Information Institute of Computing Technology, Chinese Academy of</affiliation>
<address confidence="0.987719">Beijing, 100190,</address>
<affiliation confidence="0.967259">University of Chinese Academy of</affiliation>
<address confidence="0.991056">Beijing, 100049,</address>
<abstract confidence="0.998845470588235">NP NP This paper proposes a novel maximum entropy based rule selection (MERS) model for syntax-based statistical machine translation (SMT). The MERS model combines local contextual information around rules and information of sub-trees covered by variables in rules. Therefore, our model allows the decoder to perform context-dependent rule selection during decoding. We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the treeto-string alignment template model. Experiments show that our approach achieves significant improvements over the baseline system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>How phrase sense disambiguation outperforms word sense disambiguation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In 11th Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>43--52</pages>
<contexts>
<context position="3525" citStr="Carpuat and Wu (2007" startWordPosition="536" endWordPosition="539"> the decoder hardly distinguish the two rules. Intuitively, information of sub-trees covered by nonterminals as well as contextual information of rules are believed NP NN NP NN NN NN DEG DEG X 1 X 2 X 1 X 2 X 1 X 2 levels X 2 standard of X 1 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89–97, Honolulu, October 2008. c�2008 Association for Computational Linguistics NP NP Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007a. How phrase sense disambiguation outperforms word sense disambiguation for statistical machine translation. In 11th Conference on Theoretical and Methodological Issues in Machine Translation, pages 43–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL</booktitle>
<pages>61--72</pages>
<contexts>
<context position="3525" citStr="Carpuat and Wu (2007" startWordPosition="536" endWordPosition="539"> the decoder hardly distinguish the two rules. Intuitively, information of sub-trees covered by nonterminals as well as contextual information of rules are believed NP NN NP NN NN NN DEG DEG X 1 X 2 X 1 X 2 X 1 X 2 levels X 2 standard of X 1 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89–97, Honolulu, October 2008. c�2008 Association for Computational Linguistics NP NP Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007b. Improving statistical machine translation using word sense disambiguation. In Proceedings of EMNLP-CoNLL 2007, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="3550" citStr="Chan et al. (2007)" startWordPosition="541" endWordPosition="544">uish the two rules. Intuitively, information of sub-trees covered by nonterminals as well as contextual information of rules are believed NP NN NP NN NN NN DEG DEG X 1 X 2 X 1 X 2 X 1 X 2 levels X 2 standard of X 1 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89–97, Honolulu, October 2008. c�2008 Association for Computational Linguistics NP NP Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule and the information of s</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University Center for Research in Computing Technology.</institution>
<contexts>
<context position="15776" citStr="Chen and Goodman, 1998" startWordPosition="2575" endWordPosition="2578">e not reliable. Thus we use this feature to reward ambiguous TATs. The advantage of our integration is that we need not change the main decoding algorithm of Lynx. Furthermore, the weights of the new features can be trained together with other features of the translation model. 5 Experiments 5.1 Corpus We carry out experiments on Chinese-to-English translation. The training corpus is the FBIS corpus, which contains 239k sentence pairs with 6.9M Chinese words and 8.9M English words. For the language model, we use SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998) to train two tri-gram language models on the English portion of 93 Type No. of No. of No. of ambiguous % ambiguous TATs source trees source trees Lexicalized 333,077 16,367 14,380 87.86 Partially Lexicalized 342,767 38,497 28,397 73.76 Unlexicalized 83,024 7,384 5,991 81.13 Total 758,868 62,248 48,768 78.34 Table 1: Statistical information of TATs filtered by test sets of NIST MT 2003 and 2005. System Features P(!e |T! ) P( T! |!e) Pw(!e |T!) Pw( T!|!e) lm1 lm2 TP WP P, AP Lynx 0.210 0.016 0.081 0.051 0.171 0.013 -0.055 0.403 - - +MERS 0.031 0.008 0.020 0.080 0.152 0.014 0.027 0.270 0.194 0.2</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="3700" citStr="Chiang, 2005" startWordPosition="564" endWordPosition="565"> DEG DEG X 1 X 2 X 1 X 2 X 1 X 2 levels X 2 standard of X 1 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89–97, Honolulu, October 2008. c�2008 Association for Computational Linguistics NP NP Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule and the information of sub-trees covered by nonterminals in a rule. For each ambiguous source-side of translation rules, a maximum entropy based rule selection (MERS) model i</context>
<context position="5042" citStr="Chiang, 2005" startWordPosition="780" endWordPosition="781">ere are some advantages of our approach: • Our approach resolves ambiguity for rules with multi-level syntactic structure, while WSD resolves ambiguity for strings that have no structures; • Our approach can help the decoder perform both lexical selection and phrase reorderings, while WSD can help the decoder only perform lexical selection; • Our method takes WSD as a special case, since a rule may only consists of terminals. In our previous work (He et al., 2008), we reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: • The MERS model here combines rich information of source syntactic tree as features since the translation model is linguistically syntaxbased. He et al. (2008) did not use this information. • In this paper, we build MERS models for all ambiguous source-sides, including lexicalized (source-side which only contains terminals), partially lexicalized (source-side which contains both terminals a</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL05,</booktitle>
<pages>531--540</pages>
<contexts>
<context position="19074" citStr="Collins et al. (2005)" startWordPosition="3136" endWordPosition="3139"> to Liu et al. (2006). We perform minimum error rate training (Och, 2003) to tune the feature weights for the log-linear model to maximize the systems’s BLEU score on the development set. The weights are shown in Table 2. These weights are then used to run Lynx and Lynx+MERS on the test sets. Table 3 shows the results. Lynx obtains BLEU scores of 26.15 on NIST03 and 26.09 on NIST05. Using all features described in Section 3.2, Lynx+MERS finally obtains BLEU scores of 27.05 on NIST03 and 27.28 on NIST05. The absolute improvements is 0.90 and 1.19, respectively. Using the sign-test described by Collins et al. (2005), both improvements are statistically significant at p &lt; 0.01. Moreover, Lynx+MERS also achieves higher n-gram precisions than Lynx. 94 Test Set System BLEU-4 Individual n-gram precisions 1 2 3 4 NIST03 Lynx 26.15 71.62 35.64 18.64 9.82 +MERS 27.05 72.00 36.72 19.51 10.37 NIST05 Lynx 26.09 70.39 35.12 18.53 10.11 +MERS 27.28 71.16 36.19 19.62 10.95 Table 3: BLEU-4 scores (case-insensitive) on the test sets. 5.4 Analysis The baseline system only uses four features for rule selection: the translation probabilities P(e |T�) and P( T|e); and the lexical weights P,,,(e |T�) and P,,,(T |e). These fe</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>M. Collins, P. Koehn, and I. Kucerova. 2005. Clause restructuring for statistical machine translation. In Proc. of ACL05, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>961--968</pages>
<contexts>
<context position="1119" citStr="Galley et al., 2006" startWordPosition="152" endWordPosition="155">tax-based statistical machine translation (SMT). The MERS model combines local contextual information around rules and information of sub-trees covered by variables in rules. Therefore, our model allows the decoder to perform context-dependent rule selection during decoding. We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the treeto-string alignment template model. Experiments show that our approach achieves significant improvements over the baseline system. 1 Introduction Syntax-based statistical machine translation (SMT) models (Liu et al., 2006; Galley et al., 2006; Huang et al., 2006) capture long distance reorderings by using rules with structural and linguistical information as translation knowledge. Typically, a translation rule consists of a source-side and a target-side. However, the source-side of a rule usually corresponds to multiple target-sides in multiple rules. Therefore, during decoding, the decoder should select a correct target-side for a source-side. We call this rule selection. Rule selection is of great importance to syntaxbased SMT systems. Comparing with word selection in word-based SMT and phrase selection in phrase-based SMT, rule</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-ACL 2006, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Improving statistical machine translation using lexicalized rule selection.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>321--328</pages>
<contexts>
<context position="4897" citStr="He et al., 2008" startWordPosition="756" endWordPosition="759">tion (MERS) model is built. Thus the MERS models can help the decoder to perform a context-dependent rule selection. Comparing with WSD (or PSD), there are some advantages of our approach: • Our approach resolves ambiguity for rules with multi-level syntactic structure, while WSD resolves ambiguity for strings that have no structures; • Our approach can help the decoder perform both lexical selection and phrase reorderings, while WSD can help the decoder only perform lexical selection; • Our method takes WSD as a special case, since a rule may only consists of terminals. In our previous work (He et al., 2008), we reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: • The MERS model here combines rich information of source syntactic tree as features since the translation model is linguistically syntaxbased. He et al. (2008) did not use this information. • In this paper, we build MERS models for all ambiguous so</context>
</contexts>
<marker>He, Liu, Lin, 2008</marker>
<rawString>Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Improving statistical machine translation using lexicalized rule selection. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="1140" citStr="Huang et al., 2006" startWordPosition="156" endWordPosition="159"> machine translation (SMT). The MERS model combines local contextual information around rules and information of sub-trees covered by variables in rules. Therefore, our model allows the decoder to perform context-dependent rule selection during decoding. We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the treeto-string alignment template model. Experiments show that our approach achieves significant improvements over the baseline system. 1 Introduction Syntax-based statistical machine translation (SMT) models (Liu et al., 2006; Galley et al., 2006; Huang et al., 2006) capture long distance reorderings by using rules with structural and linguistical information as translation knowledge. Typically, a translation rule consists of a source-side and a target-side. However, the source-side of a rule usually corresponds to multiple target-sides in multiple rules. Therefore, during decoding, the decoder should select a correct target-side for a source-side. We call this rule selection. Rule selection is of great importance to syntaxbased SMT systems. Comparing with word selection in word-based SMT and phrase selection in phrase-based SMT, rule selection is more ge</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>127--133</pages>
<contexts>
<context position="17240" citStr="Koehn et al., 2003" startWordPosition="2825" endWordPosition="2828">dicate a reward. the training corpus and the Xinhua portion of the Gigaword corpus, respectively. NIST MT 2002 test set is used as the development set. NIST MT 2003 and NIST MT 2005 test sets are used as the test sets. The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mtevalv11b.pl with case-insensitive matching of n-grams, where n = 4. 5.2 Training To train the translation model, we first run GIZA++ (Och and Ney, 2000) to obtain word alignment in both translation directions. Then the word alignment is refined by performing “grow-diag-final” method (Koehn et al., 2003). We use a Chinese parser developed by Deyi Xiong (Xiong et al., 2005) to parse the Chinese sentences of the training corpus. Our TAT extraction algorithm is similar to Liu et al. (2006), except that we make some tiny modifications to extract contextual features for MERS models. To extract TAT, we set the maximum height of the source sub-tree to h = 3, the maximum number of direct descendants of a node of sub-tree to c = 5. See (Liu et al., 2006) for specific definitions of these parameters. Table 1 shows statistical information of TATs which are filtered by the two test sets. For each type (l</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="3654" citStr="Koehn, 2004" startWordPosition="557" endWordPosition="558">ation of rules are believed NP NN NP NN NN NN DEG DEG X 1 X 2 X 1 X 2 X 1 X 2 levels X 2 standard of X 1 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89–97, Honolulu, October 2008. c�2008 Association for Computational Linguistics NP NP Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule and the information of sub-trees covered by nonterminals in a rule. For each ambiguous source-side of translation rules, a maxim</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1098" citStr="Liu et al., 2006" startWordPosition="148" endWordPosition="151">ERS) model for syntax-based statistical machine translation (SMT). The MERS model combines local contextual information around rules and information of sub-trees covered by variables in rules. Therefore, our model allows the decoder to perform context-dependent rule selection during decoding. We incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model, the treeto-string alignment template model. Experiments show that our approach achieves significant improvements over the baseline system. 1 Introduction Syntax-based statistical machine translation (SMT) models (Liu et al., 2006; Galley et al., 2006; Huang et al., 2006) capture long distance reorderings by using rules with structural and linguistical information as translation knowledge. Typically, a translation rule consists of a source-side and a target-side. However, the source-side of a rule usually corresponds to multiple target-sides in multiple rules. Therefore, during decoding, the decoder should select a correct target-side for a source-side. We call this rule selection. Rule selection is of great importance to syntaxbased SMT systems. Comparing with word selection in word-based SMT and phrase selection in p</context>
<context position="5219" citStr="Liu et al., 2006" startWordPosition="804" endWordPosition="807"> no structures; • Our approach can help the decoder perform both lexical selection and phrase reorderings, while WSD can help the decoder only perform lexical selection; • Our method takes WSD as a special case, since a rule may only consists of terminals. In our previous work (He et al., 2008), we reported improvements by integrating a MERS model into a formally syntax-based SMT model, the hierarchical phrase-based model (Chiang, 2005). In this paper, we incorporate the MERS model into a stateof-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al., 2006). The basic differences are: • The MERS model here combines rich information of source syntactic tree as features since the translation model is linguistically syntaxbased. He et al. (2008) did not use this information. • In this paper, we build MERS models for all ambiguous source-sides, including lexicalized (source-side which only contains terminals), partially lexicalized (source-side which contains both terminals and nonterminals), and unlexicalized (source-side which only contains nonterminals). He et al. (2008) only built MERS models for partially lexicalized sourcesides. In the TAT mod</context>
<context position="6524" citStr="Liu et al., 2006" startWordPosition="1000" endWordPosition="1003"> source syntactic tree and target string. TAT can capture linguistically motivated reorderings at short or long distance. Experiments show that by incorporating MERS model, the baseline system achieves statistically significant improvement. This paper is organized as follows: Section 2 reviews the TAT model; Section 3 introduces the MERS model and describes feature definitions; Section 4 demonstrates a method to incorporate the MERS model into the translation model; Section 5 reports and analyzes experimental results; Section 6 gives conclusions. 2 Baseline System Our baseline system is Lynx (Liu et al., 2006), which is a linguistically syntax-based SMT system. For translating a source sentence f1 = f1...f�...fi, Lynx firstly employs a parser to produce a source syntactic tree T(f1 ), and then uses the source syntactic tree as the input to search translations: DNP NPB DNP NPB industrial products manufacturing X, :NP DEG X, :NN NN levels X, :NP DEG overall standard X,:NN NN of the match 90 (1) !eI1 = argmaxe,,1Pr(eI1|fJ1 ) = argmaxe,,1Pr(T(fJ1 )|fJ1 )Pr(eI 1|T(fJ1 )) In doing this, Lynx uses tree-to-string alignment template to build relationship between source syntactic tree and target string. A TA</context>
<context position="13807" citStr="Liu et al., 2006" startWordPosition="2255" endWordPosition="2258">rce tree has one sibling node NPB. Those features make use of rich information around a rule, including the contextual information of a rule and the information of sub-trees covered by nonterminals. They are never used in Liu’s TAT model. Figure 5 shows features for a partially lexicalized source tree. Furthermore, we also build MERS models for lexicalized and unlexicalized source trees. Note that for lexicalized tree, features do not include the information of sub-trees since there is no nonterminals. The features can be easily obtained by modifying the TAT extraction algorithm described in (Liu et al., 2006). When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees. Then we use the toolkit implemented by Zhang (2004) to train MERS models for the ambiguous source syntactic trees separately. We set the iteration number to 100 and Gaussian prior to 1. 4 Integrating the MERS Models into the Translation Model We integrate the MERS models into the TAT model during the translation of each source sentence. Thus the MERS models can help the decoder perform context-dependent rule selection during decoding. For int</context>
<context position="17426" citStr="Liu et al. (2006)" startWordPosition="2859" endWordPosition="2862">ets are used as the test sets. The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mtevalv11b.pl with case-insensitive matching of n-grams, where n = 4. 5.2 Training To train the translation model, we first run GIZA++ (Och and Ney, 2000) to obtain word alignment in both translation directions. Then the word alignment is refined by performing “grow-diag-final” method (Koehn et al., 2003). We use a Chinese parser developed by Deyi Xiong (Xiong et al., 2005) to parse the Chinese sentences of the training corpus. Our TAT extraction algorithm is similar to Liu et al. (2006), except that we make some tiny modifications to extract contextual features for MERS models. To extract TAT, we set the maximum height of the source sub-tree to h = 3, the maximum number of direct descendants of a node of sub-tree to c = 5. See (Liu et al., 2006) for specific definitions of these parameters. Table 1 shows statistical information of TATs which are filtered by the two test sets. For each type (lexicalized, partially lexicalized, unlexicalized) of TATs, a great portion of the source trees are ambiguous. The number of ambiguous source trees accounts for 78.34% of the total source</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="17088" citStr="Och and Ney, 2000" startWordPosition="2803" endWordPosition="2806">e first 8 features are used by Lynx. TP=TAT penalty, WP=word penalty, AP=ambiguous TAT penalty. Note that in fact, the positive weight for WP and AP indicate a reward. the training corpus and the Xinhua portion of the Gigaword corpus, respectively. NIST MT 2002 test set is used as the development set. NIST MT 2003 and NIST MT 2005 test sets are used as the test sets. The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mtevalv11b.pl with case-insensitive matching of n-grams, where n = 4. 5.2 Training To train the translation model, we first run GIZA++ (Och and Ney, 2000) to obtain word alignment in both translation directions. Then the word alignment is refined by performing “grow-diag-final” method (Koehn et al., 2003). We use a Chinese parser developed by Deyi Xiong (Xiong et al., 2005) to parse the Chinese sentences of the training corpus. Our TAT extraction algorithm is similar to Liu et al. (2006), except that we make some tiny modifications to extract contextual features for MERS models. To extract TAT, we set the maximum height of the source sub-tree to h = 3, the maximum number of direct descendants of a node of sub-tree to c = 5. See (Liu et al., 200</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="8038" citStr="Och and Ney, 2002" startWordPosition="1234" endWordPosition="1237">racted from the training example in Figure 3: lexicalized (the left), partially lexicalized (the middle), unlexicalized (the right). Lexicalized TAT contains only terminals, which is similar to phrase-to-phrase translation in phrase-based model except that it is constrained by a syntactic tree on the source-side. Partially lexicalized TAT contains both terminals and non-terminals, which can be used for both lexical translation and phrase reordering. Unlexicalized TAT contains only nonterminals and can only be used for phrase reordering. Lynx builds translation model in a log-linear framework (Och and Ney, 2002): IP Figure 3: Word-aligned, source-parsed training example. NN NPB NPB Figure 4: TATs learned from the training example in Figure 3. are no features in Lynx that can capture contextual information during decoding, except for the n-gram language model which considers the left and right neighboring n-1 target words. But this information it very limited. NPB NN NN NN VP VV VPB VV The incomes of city and village resident continued to grow NN NN NN NN NN NN X , X a X , X, city and village incomes of X, resident X, X, X a (2) P(eI1|T (fJ1 )) = 3 The Maximum Entropy based Rule exp[&amp;quot;m Amhm(eI1, T(fJ1</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="18526" citStr="Och, 2003" startWordPosition="3043" endWordPosition="3044">f the source trees are ambiguous. The number of ambiguous source trees accounts for 78.34% of the total source trees. This indicates that the TAT model faces serious rule selection problem during decoding. 5.3 Results We use Lynx as the baseline system. Then the MERS models are incorporated into Lynx, and the system is called Lynx+MERS. To run the decoder, Lynx and Lynx+MERS share the same settings: tatTable-limit=30, tatTable-threshold=0, stack-limit=100, stack-threshold=0.00001. The meanings of the pruning parameters are the same to Liu et al. (2006). We perform minimum error rate training (Och, 2003) to tune the feature weights for the log-linear model to maximize the systems’s BLEU score on the development set. The weights are shown in Table 2. These weights are then used to run Lynx and Lynx+MERS on the test sets. Table 3 shows the results. Lynx obtains BLEU scores of 26.15 on NIST03 and 26.09 on NIST05. Using all features described in Section 3.2, Lynx+MERS finally obtains BLEU scores of 27.05 on NIST03 and 27.28 on NIST05. The absolute improvements is 0.90 and 1.19, respectively. Using the sign-test described by Collins et al. (2005), both improvements are statistically significant at</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="16915" citStr="Papineni et al., 2002" startWordPosition="2774" endWordPosition="2777">13 -0.055 0.403 - - +MERS 0.031 0.008 0.020 0.080 0.152 0.014 0.027 0.270 0.194 0.207 Table 2: Feature weights obtained by minimum error rate training on the development set. The first 8 features are used by Lynx. TP=TAT penalty, WP=word penalty, AP=ambiguous TAT penalty. Note that in fact, the positive weight for WP and AP indicate a reward. the training corpus and the Xinhua portion of the Gigaword corpus, respectively. NIST MT 2002 test set is used as the development set. NIST MT 2003 and NIST MT 2005 test sets are used as the test sets. The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mtevalv11b.pl with case-insensitive matching of n-grams, where n = 4. 5.2 Training To train the translation model, we first run GIZA++ (Och and Ney, 2000) to obtain word alignment in both translation directions. Then the word alignment is refined by performing “grow-diag-final” method (Koehn et al., 2003). We use a Chinese parser developed by Deyi Xiong (Xiong et al., 2005) to parse the Chinese sentences of the training corpus. Our TAT extraction algorithm is similar to Liu et al. (2006), except that we make some tiny modifications to extract contextual features for MERS mod</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="15716" citStr="Stolcke, 2002" startWordPosition="2569" endWordPosition="2570">ly occur only once in the training corpus, which are not reliable. Thus we use this feature to reward ambiguous TATs. The advantage of our integration is that we need not change the main decoding algorithm of Lynx. Furthermore, the weights of the new features can be trained together with other features of the translation model. 5 Experiments 5.1 Corpus We carry out experiments on Chinese-to-English translation. The training corpus is the FBIS corpus, which contains 239k sentence pairs with 6.9M Chinese words and 8.9M English words. For the language model, we use SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998) to train two tri-gram language models on the English portion of 93 Type No. of No. of No. of ambiguous % ambiguous TATs source trees source trees Lexicalized 333,077 16,367 14,380 87.86 Partially Lexicalized 342,767 38,497 28,397 73.76 Unlexicalized 83,024 7,384 5,991 81.13 Total 758,868 62,248 48,768 78.34 Table 1: Statistical information of TATs filtered by test sets of NIST MT 2003 and 2005. System Features P(!e |T! ) P( T! |!e) Pw(!e |T!) Pw( T!|!e) lm1 lm2 TP WP P, AP Lynx 0.210 0.016 0.081 0.051 0.171 0.013 -0.055 0.403 - - +ME</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken language Processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
<author>Yueliang Qian</author>
</authors>
<title>Parsing the penn chinese treebank with semantic knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<pages>70--81</pages>
<contexts>
<context position="17310" citStr="Xiong et al., 2005" startWordPosition="2839" endWordPosition="2842">aword corpus, respectively. NIST MT 2002 test set is used as the development set. NIST MT 2003 and NIST MT 2005 test sets are used as the test sets. The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mtevalv11b.pl with case-insensitive matching of n-grams, where n = 4. 5.2 Training To train the translation model, we first run GIZA++ (Och and Ney, 2000) to obtain word alignment in both translation directions. Then the word alignment is refined by performing “grow-diag-final” method (Koehn et al., 2003). We use a Chinese parser developed by Deyi Xiong (Xiong et al., 2005) to parse the Chinese sentences of the training corpus. Our TAT extraction algorithm is similar to Liu et al. (2006), except that we make some tiny modifications to extract contextual features for MERS models. To extract TAT, we set the maximum height of the source sub-tree to h = 3, the maximum number of direct descendants of a node of sub-tree to c = 5. See (Liu et al., 2006) for specific definitions of these parameters. Table 1 shows statistical information of TATs which are filtered by the two test sets. For each type (lexicalized, partially lexicalized, unlexicalized) of TATs, a great por</context>
</contexts>
<marker>Xiong, Li, Liu, Lin, Qian, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian. 2005. Parsing the penn chinese treebank with semantic knowledge. In Proceedings of IJCNLP 2005, pages 70–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Zhang</author>
</authors>
<title>Maximum entropy modeling toolkit for python and c++. available at http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.</title>
<date>2004</date>
<marker>Le Zhang, 2004</marker>
<rawString>Le Zhang. 2004. Maximum entropy modeling toolkit for python and c++. available at http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>