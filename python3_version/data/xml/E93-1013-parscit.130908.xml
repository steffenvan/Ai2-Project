<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.926005">
LFG Semantics via Constraints
</title>
<author confidence="0.924318">
Mary Dalrymple John Lamping Vijay Saraswat
</author>
<email confidence="0.717144">
{dalrymple, lamping, saraswat}@parc.xerox.com
</email>
<address confidence="0.534294">
Xerox PARC
3333 Coyote Hill Road
Palo Alto, CA 94304 USA
</address>
<sectionHeader confidence="0.968735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999903642857143">
Semantic theories of natural language as-
sociate meanings with utterances by pro-
viding meanings for lexical items and
rules for determining the meaning of
larger units given the meanings of their
parts. Traditionally, meanings are com-
bined via function composition, which
works well when constituent structure
trees are used to guide semantic com-
position. More recently, the functional
structure of LFG has been used to pro-
vide the syntactic information necessary
for constraining derivations of meaning
in a cross-linguistically uniform format.
It has been difficult, however, to recon-
cile this approach with the combination
of meanings by function composition. In
contrast to compositional approaches, we
present a deductive approach to assem-
bling meanings, based on reasoning with
constraints, which meshes well with the
unordered nature of information in the
functional structure. Our use of linear
logic as a &apos;glue&apos; for assembling meanings
also allows for a coherent treatment of
modification as well as of the LFG re-
quirements of completeness and coher-
ence.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999799">
In languages like English, the substantial scaffold-
ing provided by surface constituent structure trees is
often a useful guide for semantic composition, and
the A-calculus is a convenient formalism for assem-
bling the semantics along that scaffolding [Montague,
1974]. This is because the derivation of the mean-
ing of a phrase can often be viewed as mirroring the
surface constituent structure of the English phrase.
The sentence Bill kissed Hillary has the surface con-
stituent structure indicated by the bracketing in 1:
</bodyText>
<equation confidence="0.722762">
(1) [s [NP Bill] [vp kissed [NP Hillary]]]
</equation>
<bodyText confidence="0.999646580645161">
The verb is viewed as bearing a close syntactic rela-
tion to the object and forming a constituent with it;
this constituent then combines with the subject of
the sentence. Similarly, the meaning of the verb can
be viewed as a two-place function which is applied
first to the object, then to the subject, producing the
meaning of the sentence.
However, this approach is not as natural for lan-
guages whose surface structure does not resemble
that of English. For instance, a problem is presented
by VSO languages such as Irish [McCloskey, 1979].
To preserve the hypothesis that surface constituent
structure provides the proper scaffolding for seman-
tic interpretation in VS0 languages, one of two as-
sumptions must be made. One must assume either
that semantic composition is nonuniform across lan-
guages (leading to loss of explanatory power), or that
semantic composition proceeds not with reference to
surface syntactic structure, but instead with refer-
ence to a more abstract (English-like) constituent
structure representation. This second hypothesis
seems to us to render vacuous the claim that surface
constituent structure is useful in semantic composi-
tion.
Further problems are encountered in the seman-
tic analysis of a free word order language such as
Warlpiri [Simpson, 1983; Simpson, 1991], where sur-
face constituent structure does not always give rise to
units that are semantically coherent or useful. Here,
an argument of a verb may not even appear as a
single unit at surface constituent structure; further,
</bodyText>
<page confidence="0.999215">
97
</page>
<bodyText confidence="0.997185059322034">
arguments of a verb may appear in various different
places in the string. In such cases, the appeal to an
order of composition different from that of English
is particularly unattractive, since different orders of
composition would be needed for each possible word
order sequence.
The observation that surface constituent struc-
ture does not always provide the optimal set of con-
stituents or hierarchical structure to guide semantic
interpretation has led to efforts to use a more ab-
stract, cross-linguistically uniform structure to guide
semantic composition. As originally proposed by
Kaplan and Bresnan [1982] and Halvorsen [1983], the
functional structure or f-structure of LFG is a rep-
resentation of such a structure. However, as noted
by Halvorsen [1983] and Reyle [1988], the A-calculus
is not a very natural tool for combining meanings
of f-structure constituents. The problem is that the
sub constituents of an f-structure are not assumed to
be ordered, and so the fixed order of combination of a
functor with its arguments imposed by the A-calculus
is no longer an advantage; in fact, it becomes a disad-
vantage, since an artificial ordering must be imposed
on the composition of meanings. Furthermore, the
components of the f-structure may be not only com-
plements but also modifiers, which contribute to the
final semantics in a very different way.
Related approaches. In an effort to solve the
problem of the order-dependence imposed by stan-
dard versions of the A-calculus, Reyle [1988] pro-
poses to extend the A-calculus to reduce its sequen-
tial bias, assembling meanings by an enhanced ap-
plication mechanism. However, it is not clear how
Reyle&apos;s system can be extended to treat modification
or complex predicates, phenomena which our use of
linear logic allows us to handle.
Another means of overcoming the problem of the
order-dependence of the A-calculus is to adopt se-
mantic terms whose structure resembles f-structures
[Fenstad et at., 1985; Pollard and Sag, 1987;
Halvorsen and Kaplan, 1988]. On these approaches,
attribute-value matrices are used to encode seman-
tic information, allowing the syntactic and semantic
representations to be built up simultaneously and
in the same order-independent manner. However,
when expressions of the A-calculus are replaced with
attribute-value matrices, other problems arise: in
particular, it is not clear how to view such attribute-
value matrices as formulas, since issues such as the
representation of variable binding and scope are not
treated precisely.
These problems have been noted, and remedies
have been proposed. Sometimes, for example, an
algorithm is given which globally examines a se-
mantic attribute-value matrix representation to con-
struct a sentence in a well-defined logic; for in-
stance, Halvorsen [1983] presents an approach in
which attribute-value matrices are translated into
formulas of intensional logic. However, the compu-
tation involved is concerned with manipulating these
representations in procedural ways: it is hard to see
how these procedural mechanisms translate to mean-
ing preserving manipulations on the formulas that
the matrices represent. In sum, such approaches
tend to sacrifice the semantic precision, and declara-
tive simplicity of logical approaches (e.g. A-calculus
based approaches), and seem difficult to extend gen-
erally or motivate convincingly.
Our approach. Our approach shares the order-
independent features of approaches that represent
semantic information using attribute-value matrices,
while still allowing a well-defined treatment of vari-
able binding and scope. We do this by identifying
(1) a language of meanings and (2) a language for
assembling meanings.
In principle, (1) can be any logic (e.g., Montague&apos;s
higher-order logic); for the purposes of this paper all
we need is the language of first-order terms. Because
we assemble the meaning out of semantically pre-
cise components, our approach shares the precision
of the A-calculus based approaches. For example, the
assembled meaning has precise variable binding and
scoping.
We take (2) to be a fragment of first-order (linear)
logic carefully chosen for its computational proper-
ties, as discussed below. In contrast to using the
A-calculus to combine fragments of meaning via or-
dered applications, we combine fragments of mean-
ing through unordered conjunction, and implication.
Rather than using A-reduction to simplify mean-
ings, we rely on deduction, as advocated by Pereira
[1990; 1991].
The elements of the f-structure provide an un-
ordered set of constraints, expressed in the logic,
governing how the semantics can fit together. Con-
straints for combining lexically-provided meanings
can be encoded in lexical items, as instructions for
combining several arguments into a result.&apos;
In effect, then, our approach uses first order logic
as the &apos;glue&apos; with which semantic representations are
assembled. Once all the constraints are assembled,
deduction in the logic is used to infer the mean-
ing of the entire structure. Throughout this process
we maintain a sharp distinction between assertions
about the meaning (the glue) and the meaning itself.
To better capture some linguistic properties, we
make use of first order linear logic as the glue with
which meanings are assembled [Girard, 19871.2 One
&apos;Constraints may also be provided as rules govern-
ing particular configurations. Such rules are applicable
when properties not of individual lexical items in the con-
struction but of the construction as a whole are responsi-
ble for its interpretation; these cases include the seman-
tics of relative clauses. We will not discuss examples of
configurationally-defined rules in this paper.
2Specifically, we make use only of the tensor fragment
of linear logic. The fragment is closed under conjunction,
universal quantification and implication (with atomic an-
</bodyText>
<page confidence="0.997901">
98
</page>
<bodyText confidence="0.999940785714286">
way of thinking about linear logic is that it intro-
duces accounting of premises and conclusions, so that
deductions consume their premises to generate their
conclusions. It turns out that this property of linear
logic nicely captures the LFG requirements of co-
herence and consistency, and additionally provides
a natural way to handle modifiers: a modifier con-
sumes the unmodified meaning of the structure it
modifies and produces from it a new, modified mean-
ing.
In the following, we first illustrate our approach
by discussing a simple example, and then present
more complex examples showing how modifiers and
valence changing operations are handled.
</bodyText>
<sectionHeader confidence="0.803583" genericHeader="method">
2 Theoretical preliminaries
</sectionHeader>
<bodyText confidence="0.995696881188119">
In the following, we describe two linguistic assump-
tions that underlie this work. First, we assume that
various aspects of linguistic structure (phonological,
syntactic, semantic, and other aspects) are formally
represented as projections and are related to one an-
other by means of functional correspondences. We
also assume that the relation between the thematic
roles of a verb and the grammatical functions that
realize them are specified by means of mapping prin-
ciples which apply postlexically.
Projections. We adopt the projection architec-
ture proposed by Kaplan [1987] and Halvorsen and
Kaplan [1988] to relate f-structures to representa-
tions of their meaning: f-structures are put in func-
tional correspondence with semantic representations,
similar to the correspondence between nodes of the
constituent structure tree and f-structures. The se-
mantic projection of an f-structure, written with a
subscript cr, is a representation of the meaning of
that f-structure.
Thus, the notation &apos;/,&apos; in the lexical entries given
in Figure 1 stands for the semantic projection of the
f-structure &apos;1&apos;; similarly, &apos;(1 suBi),&apos; is the seman-
tic projection of (1 SUBJ). The equation ic,= Bill
indicates that the semantic projection of T, the f-
structure introduced by the NP Bill, is Bill. The
lexical entry for Hillary is analogous. When a lexical
entry is used, the metavariable `r is instantiated and
replaced with an actual variable corresponding to an
f-structure f [Kaplan and Bresnan, 1982, page 183].
Similarly, the metavariable `I,&apos; is instantiated to a
logic variable corresponding to the meaning of the
f-structure. In other words, the equation Tu= Bill
is instantiated as fTh, = Bill for some logic variable
frig-
tecedents). It arises from transferring to linear logic the
ideas underlying the concurrent constraint programming
scheme of Saraswat [1989] — an explicit formulation for
the higher-order version of the linear concurrent con-
straint programming scheme is given in Saraswat and
Lincoln [1992]. A nice tutorial introduction to linear logic
itself may be found in Scedrov [1990].
We have used the multiplicative conjunction 0 and
linear implication —o connectives of linear logic,
rather than the analogous conjunction A and impli-
cation of classical logic. For the present, we can
think of the linear and classical connectives as being
identical. Similarly, the of course connective &apos;!&apos; of
linear logic can be ignored for now. Below, we will
discuss respects in which the linear logic connectives
have properties that are crucially different from their
counterparts in classical logics.
Mapping principles. We follow Bresnan and
Kanerva [1989], Alsina [1993], Butt [1993] and oth-
ers in assuming that verbs specify an association be-
tween each of their arguments and a particular the-
matic role, and that mapping principles associate
these thematic roles with surface grammatical func-
tions; this assumption, while not necessary for the
treatment of simple examples such as the one dis-
cussed in Section 3, is linguistically well-motivated
and enables us to provide a nice treatment of com-
plex predicates, to be discussed in Section 5.
The lexical entry for kiss specifies the denotation
of (T PRED): it requires two arguments which we
will label agent and theme. Mapping principles en-
sure that each of these arguments is associated with
some grammatical function: here, the SUBJ of kiss
(Bill) is interpreted as the agent, and the OBJ of kiss
(Hillary) is interpreted as the theme. The specific
mapping principles that we assume are given in Fig-
ure 2.
The function of the mapping principles is to spec-
ify the set of possible associations between gram-
matical functions and thematic roles. This is done
by means of implication. Grammatical functions al-
ways appear on the left side of a mapping princi-
ple implication, and the thematic roles with which
those grammatical functions are associated appear
on the right side. Mapping principle (1), for exam-
ple, relates the thematic roles of agent and theme
designated by a two-argument verb like kiss to the
grammatical functions that realize these arguments:
it states that if a SUBJ and an OBJ are present, this
permits the deduction that the thematic role of agent
is associated with the SUBJ and the thematic role of
theme is associated with the OBJ. (Other associa-
tions are encoded by means of other mapping prin-
ciples; the mapping principles given in Figure 2 en-
codes only two of the possibilities.)
We make implicit appeal to an independently-
given, fully-worked-out theory of argument mapping,
from which mapping principles such as those given
in Figure 2 can be shown to follow. It is impor-
tant to note that we do not intend any claims about
the correctness of the specific details of the map-
ping principles given in Figure 2; rather, our claim
is that mapping principles should be of the general
form illustrated there, specifying possible relations
between thematic roles and grammatical functions.
In particular, no theoretical significance should be
</bodyText>
<page confidence="0.917394">
99
</page>
<figure confidence="0.447532">
Bill NP (t PRED) = &apos;BILL&apos;
= Bill
kissed v (1 PRED) = &apos;KISS&apos;
V X ,Y. agent((i PRED), , X) 0 theme((i PRED)a , Y) –o Ta= kiss(X,Y)
Hillary NP (1 PRED) = &apos;HILLARY&apos;
ia = Hillary
</figure>
<figureCaption confidence="0.982794">
Figure 1: Lexical entries for Bill, kissed, Hillary
</figureCaption>
<figure confidence="0.766001666666667">
!(Vf, X ,Y. ((f sus.i)c = X) 0 ((f OBJ), = Y) —o agent((f PRED)a, X) 0 theMe((f PRED),, Y))
KV f , X ,Y, Z. ((f SUBJ), = X) 0 ((f oBJ),, = Y) 0 ((f 08.12)0 = Z) —o
permitter((f PRED)ar , X) 0 agent((f PRED)a , Z) 0 theme((f PRED)a , Y))
</figure>
<figureCaption confidence="0.999631">
Figure 2: Argument mapping principles (Premises.)
bill: (f2, = Bill) (UI, Modus Ponens.)
hillary: (f3,, = Hillary) (UI, Modus Ponens.)
kiss : (VX, Y. agent(fia , X) 0 theme(fi, ,Y) –o ha = kiss(X,Y))
mappingl : (VX, Y. (f,, = X) 0 (f,,, = Y) —0 agent(flo , X) 0 theme(fio ,Y)))
(bill® hillary 0 kissed 0 mappingl)
–o agent(fia , Bill) 0 theme(fi, , Hillary) 0 kissed
–0 /1,7 = kiss(Bill, Hillary)
Figure 3: Derivation of Bill kissed Hillary
</figureCaption>
<bodyText confidence="0.999956714285714">
attached to the choice of thematic role labels used
here; for the verb kiss, for example, labels such as
&apos;kisser&apos; and &apos;kissed&apos; would do as well. We require
only that the thematic roles designated in the lexical
entries of individual verbs are specified in enough de-
tail for mapping principles such as those illustrated
in Figure 2 to apply successfully.
</bodyText>
<sectionHeader confidence="0.9802525" genericHeader="method">
3 A simple example of semantic
composition
</sectionHeader>
<figureCaption confidence="0.951368">
Consider sentence 2 and the lexical entries given in
Figure 1:
</figureCaption>
<bodyText confidence="0.4259315">
(2) Bill kissed Hillary.
The f-structure for (2) is:
</bodyText>
<table confidence="0.976418">
PRED fi :`KLSS&apos;
f4: SUBJ f2: [ PRED &apos;BILL&apos;] 1
OBJ f3: [ PRED &apos;HILLARY&apos;]
</table>
<bodyText confidence="0.999043482758621">
The meaning associated with the f-structure may be
derived by logical deduction, as shown in Figure 3.3
3An alternative derivation, not using mapping princi-
ples, is also possible. In that case, the lexical entry for
kissed would require a SUBJ and an OBJ rather than an
agent and a theme, and the derivation would proceed in
The first three lines contain the information con-
tributed by the lexical entries for Bill, Hillary, and
kissed, abbreviated as bill, hillary, and kissed. The
verb kissed requires two pieces of information, an
agent and a theme, in no particular order, to produce
a meaning for the sentence, f4,. The mapping prin-
ciple needed for associating the syntactic arguments
of transitive verbs with the agent/theme argument
structure is given on the fourth line and abbreviated
as mappingl. Mapping principles are assumed to
be a part of the background theory, rather than being
introduced by particular lexical items. Each map-
ping principle can, then, be used as many or as few
times as necessary.
The premises—i.e., the lexical entries and map-
ping principle—are restated as the first step of the
derivation, labeled &apos;Premises&apos;. The second step is
derived from the premises by Universal Instantia-
tion and Modus Ponens. The last step is then de-
rived from this result by Universal Instantiation and
Modus Ponens.
To summarize: a variable is introduced for the
meaning corresponding to each f-structure in the
</bodyText>
<equation confidence="0.8686104">
this way:
((.f2a = Bill)
0(130. = Hillary)
0(VX, Y. f2g = X 0 fur = Y —0 Acr = kiss(X,Y)))
–o li,, = kiss(Bill, Hillary)
</equation>
<page confidence="0.853056">
100
</page>
<bodyText confidence="0.999678726027398">
syntactic representation. These variables form the
scaffolding that guides the assembly of the meaning.
Further information is then introduced: information
associated with each lexical entry is made available,
as are all the mapping rules. Once all this informa-
tion is present, we look for a logical deduction of a
meaning of the sentence from that information.
The use of linear logic provides certain advantages,
since it allows us to capture the intuition that lexical
items and phrases contribute uniquely to the mean-
ing of a sentence. As noted by Klein and Sag [1985,
page 172]:
Translation rules in Montague semantics
have the property that the translation of
each component of a complex expression
occurs exactly once in the translation of
the whole. ... That is to say, we do not
want the set S [of semantic representa-
tions of a phrase] to contain all meaning-
ful expressions of IL which can be built
up from the elements of 5, but only those
which use each element exactly once.
Similar observations underlie the work of Lambek
[1958] on categorial grammars and the recent work
of van Benthem [1991] and others on dynamic logics.
It is this &apos;resource-conscious&apos; property of natural
language semantics - a meaning is used once and
once only in a semantic derivation - that linear logic
allows us to capture. The basic insight underlying
linear logic is to treat logical formulas as finite re-
sources, which are consumed in the process of de-
duction. This gives rise to a notion of linear impli-
cation -o which is resource-conscious: the formula
A -o B can be thought of as an action that can con-
sume (one copy of) A to produce (one copy of) B.
Thus, the formula A0(A -o B) linearly implies B —
but not A0 B (because the deduction consumes A),
and not (A -o B) B (because the linear implica-
tion is also consumed in doing the deduction). The
resource consciousness not only disallows arbitrary
duplication of formulas, but also arbitrary deletion
of formulas. This causes the notion of conjunction we
use (0) to be sensitive to the multiplicity of formu-
las: A0 A is not equivalent to A (the former has two
copies of the formula A). For example, the formula
A0 A0 (A -o B) does linearly imply A0 B (there is
still one A left over) — but does not linearly imply B
(there must still be one A present). Thus, linear logic
checks that a formula is used once and only once in
a deduction, reflecting the resource-consciousness of
natural language semantics. Finally, linear logic has
an of course connective ! which turns off accounting
for its formula. That is, !A linearly implies an arbi-
trary number copies of A, including none. We use
this connective on the background theory of map-
ping principles to indicate that they are not subject
to accounting; they can be used as often or seldom
as necessary.
A primary advantage of the use of linear logic is
that it enables a clean semantic definition of com-
pleteness and coherence.4 In the present setting, the
feature structure f corresponding to the utterance
is associated with the (0) conjunction 0 of all the
formulas associated with the lexical items in the ut-
terance. The conjunction is said to be complete and
coherent if Th -o fc, = t (for some term t),
where Th is the background theory containing, e.g.,
the mapping principles. Each t is to be thought of
as a valid meaning for the sentence. This guarantees
that the entries are used exactly once in building up
the denotation of the utterance: no syntactic or se-
mantic requirements may be left unfulfilled, and no
meaning may remain unused.
</bodyText>
<sectionHeader confidence="0.998596" genericHeader="method">
4 Modification
</sectionHeader>
<bodyText confidence="0.9997576">
Another primary advantage of the use of linear logic
&apos;glue&apos; in the derivation of meanings of sentences is
that it enables a clear treatment of modification.
Consider the following sentence, containing the sen-
tential modifier obviously:
</bodyText>
<listItem confidence="0.792413">
(4) Bill obviously kissed Hillary.
</listItem>
<bodyText confidence="0.994259">
We make the standard assumption that the verb
kissed is the main syntactic predicate of this sen-
tence. The following is the f-structure for example
4:
</bodyText>
<table confidence="0.86361375">
PRED : &apos;KISS&apos;
SUBJ f2 [ PRED &apos;BILL&apos;
ft: OBJ f3 : [ PRED HILLARY&apos;
MODS { fs : [ PRED &apos;OBVIOUSLY&apos;
</table>
<bodyText confidence="0.997041">
We also assume that the meaning of the sentence can
be represented by the following formula:
</bodyText>
<equation confidence="0.489894">
(6) obviously(kiss(Bill, Hillary))
</equation>
<bodyText confidence="0.999261666666667">
It is clear that there is a &apos;mismatch&apos; of sorts between
the syntactic representation and the meaning of the
sentence; syntactically, the verb is the main functor,
while the main semantic functor is the adverb.5
Consider now the lexical entry for obviously given
in Figure 4. The semantic equation associated with
</bodyText>
<footnote confidence="0.507986076923077">
4`An f-structure is locally complete if and only if it
contains all the governable grammatical functions that
its predicate governs. An f-structure is complete if and
only if all its subsidiary f-structures are locally complete.
An f-structure is locally coherent if and only if all the gov-
ernable grammatical functions that it contains are gov-
erned by a local predicate. An f-structure is coherent if
and only if all its subsidiary f-structures are locally co-
herent.&apos; [Kaplan and Bresnan, 1982, pages 211-212]
5The related phenomenon of head switching, discussed
in connection with machine translation by Kaplan et al.
[1989] and Kaplan and Wedekind [1993], is also amenable
to treatment along the lines presented here.
</footnote>
<page confidence="0.965649">
101
</page>
<equation confidence="0.973397666666667">
Bill NP PRED) =
iu = Bill
obviously ADV PRED) = &apos;OBVIOUSLY&apos;
VP. (mops t)„ = P —0 (MODS
kissed V PRED)= &apos;KISS&apos;
VX, Y. agent((i PRED),, , X) (g) theme((i PRED)e , 17) —0 IQ= kiss(X,Y)
Hillary NP PRED) = &apos;HILLARY&apos;
= Hillary
1), = obviously(P)
</equation>
<figureCaption confidence="0.753184">
Figure 4: Lexical entries for Bill, obviously, kissed, Hillary
</figureCaption>
<equation confidence="0.99326">
bill: (f2, = Bill)
hillary : (f3, = Hillary)
kiss : (VX, Y. agent(ficr, X) theme(fia ,Y) ha = kiSS(X, Y))
obviously: (VP. f.to = P —0 fia = obviously(P))
mappingl : (VX, Y. (Lc, = X) 0 (ho = Y) —o agent(fif, , X) theme(fi, ,Y)))
(bill hillary 0 kissed® obviously mapping].)
—o agent(fi,, Bill) theme(fi, , Hillary) 0 kissed obviously
- = kiss(Bill, Hillary) 0 obviously
- = obviously(kiss(Bill, Hillary))
</equation>
<figureCaption confidence="0.999565">
Figure 5: Derivation of Bill obviously kissed Hillary
</figureCaption>
<bodyText confidence="0.944438452380952">
(Premises.)
(UI, Modus Ponens.)
(UI, Modus Ponens.)
(UI, Modus Ponens.)
obviously makes use of &apos;inside-out functional uncer-
tainty&apos; [Halvorsen and Kaplan, 1988]. The expres-
sion (mops 1) denotes an f-structure through which
there is a path MODS leading to T. For example, if
is the f-structure labeled A above, then (mops T)
is the f-structure labeled h, and (mops Oa is the
semantic projection of 14. Thus, the lexical entry for
obviously specifies the semantic representation of the
f-structure that it modifies, an f-structure in which
it is properly contained.
Recall that linear logic enables a coherent notion of
consumption and production of meanings. We claim
that the semantic function of adverbs (and, indeed,
of modifiers in general) is to consume the meaning of
the structure they modify, producing a new, modified
meaning. Note in particular that the meaning of
the modified structure, (mops Oa, appears on both
sides of —o ; the unmodified meaning is consumed,
and the modified meaning is produced.
The derivation of the meaning of example 4 is
shown in Figure 5. The first part of the derivation is
the same as the derivation shown in Figure 3 for the
sentence Bill kissed Hillary. The crucial difference
is the presence of information introduced by obvi-
ously, shown in the fourth line and abbreviated as
obviously. In the last step in the derivation, the
linear implication introduced by obviously consumes
the previous value for f4, and produces the new and
final value.
By using linear logic, each step of the derivation
keeps track of what &apos;resources&apos; have been consumed
by linear implications. As mentioned above, the
value for fur is a meaning for this sentence only if
there is no other information left. Thus, the deriva-
tion could not stop at the next to last step, because
the linear implication introduced by obviously was
still left. The final step provides the only complete
and coherent meaning derivable for the utterance.
</bodyText>
<sectionHeader confidence="0.965014" genericHeader="method">
5 Valence-changing operations
</sectionHeader>
<bodyText confidence="0.881385352941176">
We have seen that modifiers can be treated as &apos;con-
suming&apos; the meaning of the structure that they mod-
ify, producing a new, modified meaning. A simi-
lar, although syntactically more complex, case arises
with complex predicates, as Butt [1990; 1993] shows.
Butt discusses the &apos;permissive construction&apos; in
Urdu, illustrated in 7:
(7) Hillary-ne diyaa [vp Bill-ko xat
Hillary-ERG let Bill-DAT letter-NOM
likhne
write-PART
&apos;Hillary let Bill write a letter.&apos;
She shows that although the permissive construction
is seemingly biclausal, it actually involves a com-
plex predicate: a syntactically monoclausal predicate
formed in the presence of the verb diyaa &apos;let&apos;. In
the case at hand, the presence of diyaa requires an
</bodyText>
<page confidence="0.991532">
102
</page>
<figure confidence="0.995013857142857">
Hillary NP PRED) = &apos;HILLARY&apos;
• Hillary
Bill NP (1 PRED) = &apos;BILL&apos;
• = Bill
xat N PRED) = &apos;LETTER&apos;
• = letter
likhne V (T PRED)= &apos;WRITE&apos;
</figure>
<figureCaption confidence="0.813299666666667">
VX, Y. agent((i PRED)a , X) theme((r PRED)a, Y) —o 1Q write(X,Y)
diyaa V VX, P. permittergi PRED),,, )0® I,„= P —o t„,= let(X, P)
Figure 6: Lexical entries for Hillary, Bill, xat, likhne, diyaa
</figureCaption>
<equation confidence="0.825606181818182">
hillary : (f2,, = Hillary)
bill: (h, = Bill)
letter: (fio. = letter)
write: (VX, Y. agent(h,, X) 0 theme(fic ,Y) —o f5,= write(X,Y))
let : (VX, P. permitter(f5,, X) 0 kc = P —0 = f5, = let(X, P)
mapping2: (VX, Y, Z. (f2, = X) 0 (f3, = Y) (ha = —0
permitter(fia, X) 0 agent( ,Y) 0 theme(fia , Z))
(bill hillary 0 letter 0 write 0 let 0 mapping2)
—o permitter(fic, , Hillary) 0 agent(fia, Bill) 0 theme(ficr, letter) 0 write 0 let
—o permitter(fi, Hillary) 0 let (A, = write(Bill, letter))
—o f5, = let(H Wary, (write(Bill , letter))
</equation>
<figureCaption confidence="0.999735">
Figure 7: Derivation of Hillary let Bill write a letter
</figureCaption>
<bodyText confidence="0.863706272727273">
(Premises.)
(UI, Modus Ponens.)
(UI, Modus Ponens.)
(UI, Modus Ponens.)
additional argument which we will label &apos;permitter&apos;,
in addition to the arguments required by the verb
likhne &apos;write&apos;. In general, the verb diyaa &apos;let&apos; modi-
fies the argument structure of the verb with which it
combines, requiring in addition to the original inven-
tory of arguments the presence of a permitter. The
f-structure for example 7 is:
</bodyText>
<table confidence="0.95329075">
(8) f5: PRED `LET(WRITE)&apos;
SUBJ 12: [ PRED &apos;HILLARY&apos;]
OBJ2 f3: [ PRED
OBJ f4 : [ PRED &apos;LETTER&apos;
</table>
<bodyText confidence="0.987979771428572">
As Butt points out, the verbs participating in the
formation of the permissive construction need not
form a syntactic constituent; in example 7, the verbs
likhne and diyaa are not even next to each other.
This shows that complex predicate formation can-
not be analyzed as taking place in the lexicon; a
method of dynamically creating a complex predicate
in the syntax is needed. That is, sentences such as 7
have, in essence, two syntactic heads, which dynami-
cally combine to produce a single syntactic argument
structure.
We claim that the function of a verb such as per-
missive diyaa is somewhat analogous to that of a
modifier: diyaa consumes the meaning of the origi-
nal verb and its arguments, producing a new permis-
sive meaning and requiring an additional argument,
the permitter. Mapping principles apply to this new,
augmented argument structure to associate the new
thematic argument structure with the appropriate
set of syntactic roles. We illustrate the derivation of
the meaning of example 7 in Figure 7.
The lexical entries necessary for example 7 can be
found in Figure 6. The instantiated information from
these lexical entries appears in the first five lines of
Figure 7. Mapping principle (2) in Figure 2, ab-
breviated as mapping2, links the permitter, agent,
and theme of the (derived) argument structure to the
syntactic arguments of a permissive construction; the
mapping principle is given in the sixth line of Figure
7.6
The premises of the derivation are, as above, infor-
mation given by lexical entries and the mapping prin-
ciple. By means of mapping principle mapping2, in-
formation about the possible array of thematic roles
required by the complex predicate let-write can be
</bodyText>
<footnote confidence="0.900003">
6Recall that in our framework, all the mapping prin-
ciples are present to be used as needed. In the derivation
of the meaning of example 7, shown in Figure 7, we have
omnisciently provided the one that will be needed.
</footnote>
<page confidence="0.998683">
103
</page>
<bodyText confidence="0.99666615">
derived; this step uses Universal Instantiation and
Modus Ponens.
Next, a (preliminary) meaning for f-structure
write(Bill,letter), is derived by Universal Instan-
tiation and Modus Ponens. At this point, the re-
quirements imposed by diyaa &apos;let&apos;, labeled let, are
met: a permitter (Hillary) is present, and a com-
plete meaning for f-structure A has been produced.
These meanings can be consumed, and a new mean-
ing produced, as represented in the final line of the
derivation. Again, this meaning is the only one avail-
able, since completeness and coherence obtains only
when all requirements are fulfilled and no extra infor-
mation remains. As with the case of modifiers, the
final step provides the only complete and coherent
meaning derivable for the utterance.
Notice that the meaning of the complex predi-
cate is not derived by composition of verb meanings:
the permissive verb diyaa does not combine with
the verb likhne &apos;write&apos; to form a new verb mean-
ing. Instead, permissive diyaa requires a (prelim-
inary) sentence meaning, write(Bill,letter) in the
example above, in addition to the presence of a per-
mitter argument.
More generally, this approach treats linguistic phe-
nomena such as modification and complex predicate
formation function by operating on semantic enti-
ties that have combined with all of their arguments,
producing a modified meaning and (in the case of
complex predicate formation) introducing further ar-
guments. While it would be possible to extend our
approach to operate on semantic entities that have
not combined with all their arguments, we have not
yet encountered a compelling reason to do so. Our
current restriction is not so confining as it might ap-
pear; most operations that can be performed on se-
mantic entities that have not combined with all their
arguments have analogues that operate on fully com-
bined entities. In further research, we plan to explore
this characteristic of our analysis more fully.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999988524590164">
Our approach results in a somewhat different view of
semantic composition, compared to A-calculus based
approaches. First of all, notice that both in A-
calculus based approaches and in our approach, there
is not only a semantic level of meanings of utterances
and phrases, but also a glue level or composition level
responsible for assembling semantic level meanings of
constituents to get a meaning for an entire utterance.
In A-calculus based approaches, the semantic level
is higher order intensional logic. The composition
level is the rules, often not stated in any formal sys-
tem, that say what pattern of applications to do to
assemble the constituent meanings. The composition
level relies on function application in the semantic
level to assemble meanings. This forces some confla-
tion of the levels, because it is using a semantic level
operation, application, to carry out a composition
level task. It requires functions at the semantic level
whose primary purpose is to allow the composition
level to combine meanings via application. For exam-
ple, in order for the composition level to work right,
the semantic level meaning of a transitive verb must
be a function of two arguments, rather. than a rela-
tion. This rather artificial requirement is a symptom
of some of the work of the composition level being
done at the semantic level.
Our approach, on the other hand, better segre-
gates the two levels of meaning, because the com-
position level uses its own mechanism (substitution)
to assemble semantic level meanings, rather than re-
lying on semantic level operations. Thus, the linear
logic operations of the composition level don&apos;t appear
at the semantic level and the classical operations of
the semantic level don&apos;t appear at the composition
leve1.7
Our system also expresses the composition level
rules in a formal system, first order linear logic. The
composition rules are expressed by relations in the
lexical entries and the mapping rules. There is no
separate process of deciding how the meanings of
lexical entries will be combined; the relations they
establish, together with some background facts, just
imply the high level meaning. All the necessary
connections between phrases are made at the com-
position level when lexical entries are instantiated,
through the shared variables of the sigma projec-
tions. From then on, logical inference at the com-
position level assembles the semantic level meaning.
These examples illustrate the capability of our
framework to handle the combination of predi-
cates with their arguments, modification, and arity-
affecting operations. The use of linear logic provides
a simple treatment of the requirements of complete-
ness and consistency and of complex predicates. Fur-
ther, our deduction framework enables us to use lin-
ear logic to state such operations in a formally well-
defined and tractable manner.
In future work, we plan to explore more fully the
semantics of modification, and to pursue the addi-
tion of a type system to the logic to treat quantifiers
analogously to Pereira [1990; 1991].
</bodyText>
<sectionHeader confidence="0.998742" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99995">
We are grateful to Ron Kaplan, Stanley Peters, John
Maxwell, Joan Bresnan, and Stuart Shieber for help-
ful comments on earlier versions of this paper. We
would particularly like to thank Fernando Pereira for
extensive and very helpful discussion of the issues
presented here.
</bodyText>
<footnote confidence="0.936722285714286">
7This separation is not a necessary consequence of
using deduction to assemble meanings; the composition
logic could call for semantic level operations. But we
have so far been able to maintain the separation, and the
question of whether the separation can be maintained
seems to be linguistically interesting and worthy of fur-
ther pursuit.
</footnote>
<page confidence="0.998401">
104
</page>
<sectionHeader confidence="0.995346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999948971428572">
[Alsina, 1993] Alex Alsina. Predicate Composition: A
Theory of Syntactic Function Alternations. PhD the-
sis, Stanford University, 1993.
[Bresnan and Kanerva, 1989] Joan Bresnan and
Jonni M. Kanerva. Locative inversion in Chicheiva:
A case study of factorization in grammar. Linguistic
Inquiry, 20(1):1-50, 1989. Also in E. Wehrli and T.
Stowell, eds., Syntax and Semantics 26: Syntax and
the Lexicon. New York: Academic Press.
[Butt et al., 1990] Miriam Butt, Michio Isoda, and Pe-
ter Sells. Complex predicates in LFG. MS, Stanford
University, 1990.
[Butt, 1993] Miriam Butt. The Structure of Complex
Predicates. PhD thesis, Stanford University, 1993. In
preparation.
[Fenstad et al., 1985] Jens Erik Fenstad, Per-Kristian
Halvorsen, Tore Langholm, and Johan van Benthem.
Equations, schemata and situations: A framework
for linguistic semantics. Technical Report 29, Center
for the Study of Language and Information, Stanford
University, 1985.
[Girard, 1987] J.-Y. Girard. Linear logic. Theoretical
Computer Science, 45:1-102, 1987.
[Halvorsen and Kaplan, 1988] Per-Kristian Halvorsen
and Ronald M. Kaplan. Projections and semantic de-
scription in Lexical-Functional Grammar. In Proceed-
ings of the International Conference on Fifth Gener-
ation Computer Systems, pages 1116-1122, Tokyo,
Japan, 1988. Institute for New Generation Systems.
[Halvorsen, 1983] Per-Kristian Halvorsen. Semantics
for Lexical-Functional Grammar. Linguistic Inquiry,
14(4):567-615, 1983.
[Kaplan and Bresnan, 1982] Ronald M. Kaplan and
Joan Bresnan. Lexical-Functional Grammar: A
formal system for grammatical representation. In
Joan Bresnan, editor, The Mental Representation
of Grammatical Relations, pages 173-281. The MIT
Press, Cambridge, MA, 1982.
[Kaplan and Wedekind, 1993] Ronald M. Kaplan and
Jürgen Wedekind. Restriction and correspondence-
based translation. In Proceedings of the Sixth Meet-
ing of the European ACL, University of Utrecht, April
1993. European Chapter of the Association for Com-
putational Linguistics.
[Kaplan et al., 1989] Ronald M. Kaplan, Klaus Netter,
Jurgen Wedekind, and Annie Zaenen. Translation
by structural correspondences. In Proceedings of the
Fourth Meeting of the European ACL, pages 272-
281, University of Manchester, April 1989. European
Chapter of the Association for Computational Lin-
guistics.
[Kaplan, 1987] Ronald M. Kaplan. Three seductions
of computational psycholinguistics. In Peter White-
lock, Harold Somers, Paul Bennett, Rod Johnson,
and Mary McGee Wood, editors, Linguistic The-
ory and Computer Applications, pages 149-188. Aca-
demic Press, London, 1987.
[Klein and Sag, 1985] Ewan Klein and Ivan A. Sag.
Type-driven translation. Linguistics and Philosophy,
8:163-201, 1985.
[Lambek, 1958] Joachim Lambek. The mathematics of
sentence structure. American Mathematical Monthly,
65:154-170, 1958.
[McCloskey, 1979] James McCloskey. Transformational
syntax and model theoretic semantics : a case study
in modern Irish. D. Reidel, Dordrecht, 1979.
[Montague, 1974] Richard Montague. Formal Philoso-
phy. Yale University Press, New Haven, 1974. Rich-
mond Thomason, editor.
[Pereira, 1990] Fernando C. N. Pereira. Categorial se-
mantics and scoping. Computational Linguistics,
16(1):1-10, 1990.
[Pereira, 1991] Fernando C. N. Pereira. Semantic inter-
pretation as higher-order deduction. In Jan van Eijck,
editor, Logics in Al: European Workshop JELIA &apos;90,
pages 78-96, Amsterdam, Holland, 1991. Springer-
Verlag.
[Pollard and Sag, 1987] Carl Pollard and Ivan A. Sag.
Information-Based Syntax and Semantics, Volume I.
Number 13 in CSLI Lecture Notes. CSLI/The Uni-
versity of Chicago Press, Stanford University, 1987.
[Reyle, 1988] Uwe Reyle. Compositional semantics for
LFG. In Uwe Reyle and Christian Rohrer, editors,
Natural language parsing and linguistic theories. D.
Reidel, Dordrecht, 1988.
[Saraswat and Lincoln, 1992] Vijay A. Saraswat and
Patrick Lincoln. Higher-order, linear concurrent con-
straint programming. Technical report, Xerox Palo
Alto Research Center, August 1992.
[Saraswat, 1989] Vijay A. Saraswat. Concurrent
Constraint Programming Languages. PhD thesis,
Carnegie-Mellon University, 1989. To appear, Doc-
toral Dissertation Award and Logic Programming Se-
ries, MIT Press, 1993.
[Scedrov, 1990] A. Scedrov. A brief guide to linear logic.
Bulletin of the European Assoc. for Theoretical Com-
puter Science, 41:154-165, June 1990.
[Simpson, 1983] Jane Simpson. Aspects of Warlpiri
Morphology and Syntax. PhD thesis, MIT, 1983.
[Simpson, 1991] Jane Simpson. Warlpiri Morpho-
Syntax. Kluwer Academic Publishers, Dordrecht,
1991.
[van Benthem, 1991] Johan van Benthem. Language
in Action: Categories, Lambdas and Dynamic Logic.
North-Holland, Amsterdam, 1991.
</reference>
<page confidence="0.999016">
105
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.661153">
<title confidence="0.998796">LFG Semantics via Constraints</title>
<author confidence="0.999228">Mary Dalrymple John Lamping Vijay Saraswat</author>
<email confidence="0.993142">dalrymple@parc.xerox.com</email>
<email confidence="0.993142">lamping@parc.xerox.com</email>
<email confidence="0.993142">saraswat@parc.xerox.com</email>
<affiliation confidence="0.769734">Xerox PARC</affiliation>
<address confidence="0.993788">3333 Coyote Hill Road Palo Alto, CA 94304 USA</address>
<abstract confidence="0.995403275862069">Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts. Traditionally, meanings are combined via function composition, which works well when constituent structure trees are used to guide semantic com- More recently, the LFG has been used to provide the syntactic information necessary for constraining derivations of meaning in a cross-linguistically uniform format. It has been difficult, however, to reconcile this approach with the combination of meanings by function composition. In contrast to compositional approaches, we present a deductive approach to assembling meanings, based on reasoning with constraints, which meshes well with the unordered nature of information in the structure. Our use of a &apos;glue&apos; for assembling meanings also allows for a coherent treatment of modification as well as of the LFG requirements of completeness and coherence.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alex Alsina</author>
</authors>
<title>Predicate Composition: A Theory of Syntactic Function Alternations. PhD thesis,</title>
<date>1993</date>
<institution>Stanford University,</institution>
<marker>[Alsina, 1993]</marker>
<rawString>Alex Alsina. Predicate Composition: A Theory of Syntactic Function Alternations. PhD thesis, Stanford University, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>Jonni M Kanerva</author>
</authors>
<title>Locative inversion in Chicheiva: A case study of factorization in grammar.</title>
<date>1989</date>
<booktitle>Linguistic Inquiry,</booktitle>
<pages>20--1</pages>
<editor>in E. Wehrli and T. Stowell, eds.,</editor>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<note>Also</note>
<marker>[Bresnan and Kanerva, 1989]</marker>
<rawString>Joan Bresnan and Jonni M. Kanerva. Locative inversion in Chicheiva: A case study of factorization in grammar. Linguistic Inquiry, 20(1):1-50, 1989. Also in E. Wehrli and T. Stowell, eds., Syntax and Semantics 26: Syntax and the Lexicon. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Butt</author>
<author>Michio Isoda</author>
<author>Peter Sells</author>
</authors>
<date>1990</date>
<booktitle>Complex predicates in LFG. MS,</booktitle>
<institution>Stanford University,</institution>
<marker>[Butt et al., 1990]</marker>
<rawString>Miriam Butt, Michio Isoda, and Peter Sells. Complex predicates in LFG. MS, Stanford University, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Butt</author>
</authors>
<title>The Structure of Complex Predicates.</title>
<date>1993</date>
<tech>PhD thesis,</tech>
<institution>Stanford University,</institution>
<note>In preparation.</note>
<marker>[Butt, 1993]</marker>
<rawString>Miriam Butt. The Structure of Complex Predicates. PhD thesis, Stanford University, 1993. In preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Erik Fenstad</author>
<author>Per-Kristian Halvorsen</author>
<author>Tore Langholm</author>
<author>Johan van Benthem</author>
</authors>
<title>Equations, schemata and situations: A framework for linguistic semantics.</title>
<date>1985</date>
<tech>Technical Report 29,</tech>
<institution>Center for the Study of Language and Information, Stanford University,</institution>
<marker>[Fenstad et al., 1985]</marker>
<rawString>Jens Erik Fenstad, Per-Kristian Halvorsen, Tore Langholm, and Johan van Benthem. Equations, schemata and situations: A framework for linguistic semantics. Technical Report 29, Center for the Study of Language and Information, Stanford University, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-Y Girard</author>
</authors>
<title>Linear logic.</title>
<date>1987</date>
<journal>Theoretical Computer Science,</journal>
<pages>45--1</pages>
<marker>[Girard, 1987]</marker>
<rawString>J.-Y. Girard. Linear logic. Theoretical Computer Science, 45:1-102, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Per-Kristian Halvorsen</author>
<author>Ronald M Kaplan</author>
</authors>
<title>Projections and semantic description in Lexical-Functional Grammar.</title>
<date>1988</date>
<booktitle>In Proceedings of the International Conference on Fifth Generation Computer Systems,</booktitle>
<pages>1116--1122</pages>
<location>Tokyo, Japan,</location>
<marker>[Halvorsen and Kaplan, 1988]</marker>
<rawString>Per-Kristian Halvorsen and Ronald M. Kaplan. Projections and semantic description in Lexical-Functional Grammar. In Proceedings of the International Conference on Fifth Generation Computer Systems, pages 1116-1122, Tokyo, Japan, 1988. Institute for New Generation Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Per-Kristian Halvorsen</author>
</authors>
<title>Semantics for Lexical-Functional Grammar. Linguistic Inquiry,</title>
<date>1983</date>
<pages>14--4</pages>
<marker>[Halvorsen, 1983]</marker>
<rawString>Per-Kristian Halvorsen. Semantics for Lexical-Functional Grammar. Linguistic Inquiry, 14(4):567-615, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexical-Functional Grammar: A formal system for grammatical representation.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<pages>173--281</pages>
<editor>In Joan Bresnan, editor,</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA,</location>
<marker>[Kaplan and Bresnan, 1982]</marker>
<rawString>Ronald M. Kaplan and Joan Bresnan. Lexical-Functional Grammar: A formal system for grammatical representation. In Joan Bresnan, editor, The Mental Representation of Grammatical Relations, pages 173-281. The MIT Press, Cambridge, MA, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Jürgen Wedekind</author>
</authors>
<title>Restriction and correspondencebased translation.</title>
<date>1993</date>
<booktitle>In Proceedings of the Sixth Meeting of the European ACL,</booktitle>
<institution>University of Utrecht,</institution>
<marker>[Kaplan and Wedekind, 1993]</marker>
<rawString>Ronald M. Kaplan and Jürgen Wedekind. Restriction and correspondencebased translation. In Proceedings of the Sixth Meeting of the European ACL, University of Utrecht, April 1993. European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Klaus Netter</author>
<author>Jurgen Wedekind</author>
<author>Annie Zaenen</author>
</authors>
<title>Translation by structural correspondences.</title>
<date>1989</date>
<booktitle>In Proceedings of the Fourth Meeting of the European ACL,</booktitle>
<pages>272--281</pages>
<institution>University of Manchester,</institution>
<marker>[Kaplan et al., 1989]</marker>
<rawString>Ronald M. Kaplan, Klaus Netter, Jurgen Wedekind, and Annie Zaenen. Translation by structural correspondences. In Proceedings of the Fourth Meeting of the European ACL, pages 272-281, University of Manchester, April 1989. European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
</authors>
<title>Three seductions of computational psycholinguistics.</title>
<date>1987</date>
<booktitle>Linguistic Theory and Computer Applications,</booktitle>
<pages>149--188</pages>
<editor>In Peter Whitelock, Harold Somers, Paul Bennett, Rod Johnson, and Mary McGee Wood, editors,</editor>
<publisher>Academic Press,</publisher>
<location>London,</location>
<marker>[Kaplan, 1987]</marker>
<rawString>Ronald M. Kaplan. Three seductions of computational psycholinguistics. In Peter Whitelock, Harold Somers, Paul Bennett, Rod Johnson, and Mary McGee Wood, editors, Linguistic Theory and Computer Applications, pages 149-188. Academic Press, London, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ewan Klein</author>
<author>Ivan A Sag</author>
</authors>
<title>Type-driven translation.</title>
<date>1985</date>
<journal>Linguistics and Philosophy,</journal>
<pages>8--163</pages>
<marker>[Klein and Sag, 1985]</marker>
<rawString>Ewan Klein and Ivan A. Sag. Type-driven translation. Linguistics and Philosophy, 8:163-201, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Lambek</author>
</authors>
<title>The mathematics of sentence structure.</title>
<date>1958</date>
<journal>American Mathematical Monthly,</journal>
<pages>65--154</pages>
<marker>[Lambek, 1958]</marker>
<rawString>Joachim Lambek. The mathematics of sentence structure. American Mathematical Monthly, 65:154-170, 1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James McCloskey</author>
</authors>
<title>Transformational syntax and model theoretic semantics : a case study in modern Irish.</title>
<date>1979</date>
<location>D. Reidel, Dordrecht,</location>
<marker>[McCloskey, 1979]</marker>
<rawString>James McCloskey. Transformational syntax and model theoretic semantics : a case study in modern Irish. D. Reidel, Dordrecht, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>Formal Philosophy.</title>
<date>1974</date>
<editor>Richmond Thomason, editor.</editor>
<publisher>Yale University Press,</publisher>
<location>New Haven,</location>
<marker>[Montague, 1974]</marker>
<rawString>Richard Montague. Formal Philosophy. Yale University Press, New Haven, 1974. Richmond Thomason, editor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
</authors>
<title>Categorial semantics and scoping.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--1</pages>
<marker>[Pereira, 1990]</marker>
<rawString>Fernando C. N. Pereira. Categorial semantics and scoping. Computational Linguistics, 16(1):1-10, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
</authors>
<title>Semantic interpretation as higher-order deduction.</title>
<date>1991</date>
<booktitle>Logics in Al: European Workshop JELIA &apos;90,</booktitle>
<pages>78--96</pages>
<editor>In Jan van Eijck, editor,</editor>
<publisher>SpringerVerlag.</publisher>
<location>Amsterdam, Holland,</location>
<marker>[Pereira, 1991]</marker>
<rawString>Fernando C. N. Pereira. Semantic interpretation as higher-order deduction. In Jan van Eijck, editor, Logics in Al: European Workshop JELIA &apos;90, pages 78-96, Amsterdam, Holland, 1991. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Information-Based Syntax and Semantics, Volume I.</title>
<date>1987</date>
<journal>Number</journal>
<volume>13</volume>
<institution>University of Chicago Press, Stanford University,</institution>
<note>in CSLI Lecture Notes. CSLI/The</note>
<marker>[Pollard and Sag, 1987]</marker>
<rawString>Carl Pollard and Ivan A. Sag. Information-Based Syntax and Semantics, Volume I. Number 13 in CSLI Lecture Notes. CSLI/The University of Chicago Press, Stanford University, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uwe Reyle</author>
</authors>
<title>Compositional semantics for LFG.</title>
<date>1988</date>
<editor>In Uwe Reyle and Christian Rohrer, editors,</editor>
<location>Dordrecht,</location>
<marker>[Reyle, 1988]</marker>
<rawString>Uwe Reyle. Compositional semantics for LFG. In Uwe Reyle and Christian Rohrer, editors, Natural language parsing and linguistic theories. D. Reidel, Dordrecht, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vijay A Saraswat</author>
<author>Patrick Lincoln</author>
</authors>
<title>Higher-order, linear concurrent constraint programming.</title>
<date>1992</date>
<tech>Technical report,</tech>
<institution>Xerox Palo Alto Research Center,</institution>
<marker>[Saraswat and Lincoln, 1992]</marker>
<rawString>Vijay A. Saraswat and Patrick Lincoln. Higher-order, linear concurrent constraint programming. Technical report, Xerox Palo Alto Research Center, August 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vijay A Saraswat</author>
</authors>
<title>Concurrent Constraint Programming Languages.</title>
<date>1989</date>
<booktitle>Doctoral Dissertation Award and Logic Programming Series,</booktitle>
<tech>PhD thesis,</tech>
<publisher>MIT Press,</publisher>
<institution>Carnegie-Mellon University,</institution>
<note>To appear,</note>
<marker>[Saraswat, 1989]</marker>
<rawString>Vijay A. Saraswat. Concurrent Constraint Programming Languages. PhD thesis, Carnegie-Mellon University, 1989. To appear, Doctoral Dissertation Award and Logic Programming Series, MIT Press, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Scedrov</author>
</authors>
<title>A brief guide to linear logic.</title>
<date>1990</date>
<journal>Bulletin of the European Assoc. for Theoretical Computer Science,</journal>
<pages>41--154</pages>
<marker>[Scedrov, 1990]</marker>
<rawString>A. Scedrov. A brief guide to linear logic. Bulletin of the European Assoc. for Theoretical Computer Science, 41:154-165, June 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Simpson</author>
</authors>
<title>Aspects of Warlpiri Morphology and Syntax.</title>
<date>1983</date>
<tech>PhD thesis, MIT,</tech>
<marker>[Simpson, 1983]</marker>
<rawString>Jane Simpson. Aspects of Warlpiri Morphology and Syntax. PhD thesis, MIT, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Simpson</author>
</authors>
<title>Warlpiri MorphoSyntax.</title>
<date>1991</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht,</location>
<marker>[Simpson, 1991]</marker>
<rawString>Jane Simpson. Warlpiri MorphoSyntax. Kluwer Academic Publishers, Dordrecht, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan van Benthem</author>
</authors>
<title>Language in Action: Categories, Lambdas and Dynamic Logic.</title>
<date>1991</date>
<publisher>North-Holland,</publisher>
<location>Amsterdam,</location>
<marker>[van Benthem, 1991]</marker>
<rawString>Johan van Benthem. Language in Action: Categories, Lambdas and Dynamic Logic. North-Holland, Amsterdam, 1991.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>