<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000182">
<title confidence="0.961325">
Faster Parsing by Supertagger Adaptation
</title>
<author confidence="0.8558615">
Jonathan K. Kummerfelda Jessika Roesnerb Tim Dawborna James Haggertya
James R. Currana* Stephen Clarkc*
</author>
<affiliation confidence="0.994567">
School of Information Technologiesa Department of Computer Scienceb Computer Laboratoryc
University of Sydney University of Texas at Austin University of Cambridge
</affiliation>
<note confidence="0.612165">
NSW 2006, Australia Austin, TX, USA Cambridge CB3 0FD, UK
</note>
<email confidence="0.980735">
james@it.usyd.edu.aua* stephen.clark@cl.cam.ac.ukc*
</email>
<sectionHeader confidence="0.993495" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999784">
We propose a novel self-training method
for a parser which uses a lexicalised gram-
mar and supertagger, focusing on increas-
ing the speed of the parser rather than
its accuracy. The idea is to train the su-
pertagger on large amounts of parser out-
put, so that the supertagger can learn to
supply the supertags that the parser will
eventually choose as part of the highest-
scoring derivation. Since the supertag-
ger supplies fewer supertags overall, the
parsing speed is increased. We demon-
strate the effectiveness of the method us-
ing a CCG supertagger and parser, obtain-
ing significant speed increases on newspa-
per text with no loss in accuracy. We also
show that the method can be used to adapt
the CCG parser to new domains, obtain-
ing accuracy and speed improvements for
Wikipedia and biomedical text.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959446428572">
In many NLP tasks and applications, e.g. distribu-
tional similarity (Curran, 2004) and question an-
swering (Dumais et al., 2002), large volumes of
text and detailed syntactic information are both
critical for high performance. To avoid a trade-
off between these two, we need to increase parsing
speed, but without losing accuracy.
Parsing with lexicalised grammar formalisms,
such as Lexicalised Tree Adjoining Grammar and
Combinatory Categorial Grammar (CCG; Steed-
man, 2000), can be made more efficient using a
supertagger. Bangalore and Joshi (1999) call su-
pertagging almost parsing because of the signifi-
cant reduction in ambiguity which occurs once the
supertags have been assigned.
In this paper, we focus on the CCG parser and
supertagger described in Clark and Curran (2007).
Since the CCG lexical category set used by the su-
pertagger is much larger than the Penn Treebank
POS tag set, the accuracy of supertagging is much
lower than POS tagging; hence the CCG supertag-
ger assigns multiple supertags1 to a word, when
the local context does not provide enough infor-
mation to decide on the correct supertag.
The supertagger feeds lexical categories to the
parser, and the two interact, sometimes using mul-
tiple passes over a sentence. If a spanning analy-
sis cannot be found by the parser, the number of
lexical categories supplied by the supertagger is
increased. The supertagger-parser interaction in-
fluences speed in two ways: first, the larger the
lexical ambiguity, the more derivations the parser
must consider; second, each further pass is as
costly as parsing a whole extra sentence.
Our goal is to increase parsing speed without
loss of accuracy. The technique we use is a form
of self-training, in which the output of the parser is
used to train the supertagger component. The ex-
isting literature on self-training reports mixed re-
sults. Clark et al. (2003) were unable to improve
the accuracy of POS tagging using self-training.
In contrast, McClosky et al. (2006a) report im-
proved accuracy through self-training for a two-
stage parser and re-ranker.
Here our goal is not to improve accuracy, only
to maintain it, which we achieve through an adap-
tive supertagger. The adaptive supertagger pro-
duces lexical categories that the parser would have
used in the final derivation when using the base-
line model. However, it does so with much lower
ambiguity levels, and potentially during an ear-
lier pass, which means sentences are parsed faster.
By increasing the ambiguity level of the adaptive
models to match the baseline system, we can also
slightly increase supertagging accuracy, which can
lead to higher parsing accuracy.
</bodyText>
<footnote confidence="0.636551">
1We use supertag and lexical category interchangeably.
</footnote>
<page confidence="0.964639">
345
</page>
<note confidence="0.944349">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345–355,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999881">
Using the parser to generate training data also
has the advantage that it is not a domain specific
process. Previous work has shown that parsers
typically perform poorly outside of their train-
ing domain (Gildea, 2001). Using a newspaper-
trained parser, we constructed new training sets for
Wikipedia and biomedical text. These were used
to create new supertagging models adapted to the
different domains.
The self-training method of adapting the su-
pertagger to suit the parser increased parsing speed
by more than 50% across all three domains, with-
out loss of accuracy. Using an adapted supertagger
with ambiguity levels tuned to match the baseline
system, we were also able to increase F-score on
labelled grammatical relations by 0.75%.
</bodyText>
<sectionHeader confidence="0.987657" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9999035">
Many statistical parsers use two stages: a tag-
ging stage that labels each word with its gram-
matical role, and a parsing stage that uses the tags
to form a parse tree. Lexicalised grammars typ-
ically contain a much smaller set of rules than
phrase-structure grammars, relying on tags (su-
pertags) that contain a more detailed description
of each word’s role in the sentence. This leads to
much larger tag sets, and shifts a large proportion
of the search for an optimal derivation to the tag-
ging component of the parser.
Figure 1 gives two sentences and their CCG
derivations, showing how some of the syntactic
ambiguity is transferred to the supertagging com-
ponent in a lexicalised grammar. Note that the
lexical category assigned to with is different in
each case, reflecting the fact that the prepositional
phrase attaches differently. Either we need a tag-
ging model that can resolve this ambiguity, or both
lexical categories must be supplied to the parser
which can then attempt to resolve the ambiguity
by eventually selecting between them.
</bodyText>
<subsectionHeader confidence="0.991444">
2.1 Supertagging
</subsectionHeader>
<bodyText confidence="0.999952333333333">
Supertaggers typically use standard linear-time
tagging algorithms, and only consider words in the
local context when assigning a supertag. The C&amp;C
supertagger is similar to the Ratnaparkhi (1996)
tagger, using features based on words and POS
tags in a five-word window surrounding the target
word, and defining a local probability distribution
over supertags for each word in the sentence, given
the previous two supertags. The Viterbi algorithm
</bodyText>
<equation confidence="0.9207129">
I ate pizza with cutlery
NP (S\NP)INP NP ((S\NP)\(S\NP))1NP NP
&gt; &gt;
S\NP (S\NP)\(S\NP)
I ate pizza with anchovies
NP (S\NP)INP NP (NP\NP)INP NP
&gt;
&lt;
&gt;
&lt;
</equation>
<figureCaption confidence="0.997035">
Figure 1: Two CCG derivations with PP ambiguity.
</figureCaption>
<bodyText confidence="0.999778743589744">
can be used to find the most probable supertag se-
quence. Alternatively the Forward-Backward al-
gorithm can be used to efficiently sum over all se-
quences, giving a probability distribution over su-
pertags for each word which is conditional only on
the input sentence.
Supertaggers can be made accurate enough for
wide coverage parsing using multi-tagging (Chen
et al., 1999), in which more than one supertag
can be assigned to a word; however, as more su-
pertags are supplied by the supertagger, parsing
efficiency decreases (Chen et al., 2002), demon-
strating the influence of lexical ambiguity on pars-
ing complexity (Sarkar et al., 2000).
Clark and Curran (2004) applied supertagging
to CCG, using a flexible multi-tagging approach.
The supertagger assigns to a word all lexical cate-
gories whose probabilities are within some factor,
Q, of the most probable category for that word.
When the supertagger is integrated with the C&amp;C
parser, several progressively lower Q values are
considered. If a sentence is not parsed on one
pass then the parser attempts to parse the sentence
again with a lower Q value, using a larger set of
categories from the supertagger. Since most sen-
tences are parsed at the first level (in which the av-
erage number of supertags assigned to each word
is only slightly greater than one), this provides
some of the speed benefit of single tagging, but
without loss of coverage (Clark and Curran, 2004).
Supertagging has since been effectively applied
to other formalisms, such as HPSG (Blunsom and
Baldwin, 2006; Zhang et al., 2009), and as an in-
formation source for tasks such as Statistical Ma-
chine Translation (Hassan et al., 2007). The use
of parser output for supertagger training has been
explored for LTAG by Sarkar (2007). However, the
focus of that work was on improving parser and
supertagger accuracy rather than speed.
</bodyText>
<figure confidence="0.7283846">
&lt;
S\NP
&lt;
S
NP\NP
NP
S\NP
S
346
Previously , watch imports were denied such duty-free treatment
S/S �N/N N (S[dcl]\NP)/(S[pss]\NP) (S[pss]\NP)/NP NP/NP N/N N
N N (S[dcl]\NP)/NP S[pss]\NP (N/N)/(N/N)
S[adj]\NP (S[dcl]\NP)/(S[adj]\NP) (S[pss]\NP)/NP N/N
(S[pt]\NP)/NP
(S[dcl]\NP)/NP
</figure>
<figureCaption confidence="0.605574">
Figure 2: An example sentence and the sets of categories assigned by the supertagger. The first category
in each column is correct and the categories used by the parser are marked in bold. The correct category
for watch is included here, for expository purposes, but in fact was not provided by the supertagger.
</figureCaption>
<subsectionHeader confidence="0.998945">
2.2 Semi-supervised training
</subsectionHeader>
<bodyText confidence="0.999028411764706">
Previous exploration of semi-supervised training
in NLP has focused on improving accuracy, often
for the case where only small amounts of manually
labelled training data are available. One approach
is co-training, in which two models with indepen-
dent views of the data iteratively inform each other
by labelling extra training data. Sarkar (2001) ap-
plied co-training to LTAG parsing, in which the su-
pertagger and parser provide the two views. Steed-
man et al. (2003) extended the method to a variety
of parser pairs.
Another method is to use a re-ranker (Collins
and Koo, 2002) on the output of a system to gener-
ate new training data. Like co-training, this takes
advantage of a different view of the data, but the
two views are not independent as the re-ranker is
limited to the set of options produced by the sys-
tem. This method has been used effectively to
improve parsing performance on newspaper text
(McClosky et al., 2006a), as well as adapting a
Penn Treebank parser to a new domain (McClosky
et al., 2006b).
As well as using independent views of data to
generate extra training data, multiple views can be
used to provide constraints at test time. Holling-
shead and Roark (2007) improved the accuracy
of a parsing pipeline by using the output of later
stages to constrain earlier stages.
The only work we are aware of that uses self-
training to improve the efficiency of parsers is van
Noord (2009), who adopts a similar idea to the
one in this paper for improving the efficiency of
a Dutch parser based on a manually constructed
HPSG grammar.
</bodyText>
<sectionHeader confidence="0.987484" genericHeader="method">
3 Adaptive Supertagging
</sectionHeader>
<bodyText confidence="0.997010111111111">
The purpose of the supertagger is to cut down the
search space for the parser by reducing the set of
categories that must be considered for each word.
A perfect supertagger would assign the correct cat-
egory to every word. CCG supertaggers are about
92% accurate when assigning a single lexical cate-
gory to each word (Clark and Curran, 2004). This
is not accurate enough for wide coverage parsing
and so a multi-tagging approach is used instead.
In the final derivation, the parser uses one category
from each set, and it is important to note that hav-
ing the correct category in the set does not guaran-
tee that the parser will use it.
Figure 2 gives an example sentence and the sets
of lexical categories supplied by the supertagger,
for a particular value of 0.2 The usual target of
the supertagging task is to produce the top row of
categories in Figure 2, the correct categories. We
propose a new task that instead aims for the cat-
egories the parser will use, which are marked in
bold for this case. The purpose of this new task is
to improve speed.
The reason speed will be improved is that we
can construct models that will constrain the set of
possible derivations more than the baseline model.
We can construct these models because we can
obtain much more of our target output, parser-
annotated sentences, than we could for the gold-
standard supertagging task.
The new target data will contain tagging errors,
and so supertagging accuracy measured against
the correct categories may decrease. If we ob-
tained perfect accuracy on our new task then we
would be removing all of the categories not cho-
sen by the parser. However, parsing accuracy will
not decrease since the parser will still receive the
categories it would have used, and will therefore
be able to form the same highest-scoring deriva-
tion (and hence will choose it).
To test this idea we parsed millions of sentences
2Two of the categories for such have been left out for
reasons of space, and the correct category for watch has been
included for expository reasons. The fact that the supertagger
does not supply this category is the reason that the parser does
not analyse the sentence correctly.
</bodyText>
<page confidence="0.992191">
347
</page>
<bodyText confidence="0.999978125">
in three domains, producing new data annotated
with the categories that the parser used with the
baseline model. We constructed new supertagging
models that are adapted to suit the parser by train-
ing on the combination of these sets and the stan-
dard training corpora. We applied standard evalu-
ation metrics for speed and accuracy, and explored
the source of the changes in parsing performance.
</bodyText>
<sectionHeader confidence="0.99593" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999431">
In this work, we consider three domains: news-
wire, Wikipedia text and biomedical text.
</bodyText>
<subsectionHeader confidence="0.996756">
4.1 Training and accuracy evaluation
</subsectionHeader>
<bodyText confidence="0.999941027027027">
We have used Sections 02-21 of CCGbank (Hock-
enmaier and Steedman, 2007), the CCG version of
the Penn Treebank (Marcus et al., 1993), as train-
ing data for the newspaper domain. Sections 00
and 23 were used for development and test eval-
uation. A further 113,346,430 tokens (4,566,241
sentences) of raw data from the Wall Street Jour-
nal section of the North American News Corpus
(Graff, 1995) were parsed to produce the training
data for adaptation. This text was tokenised us-
ing the C&amp;C tools tokeniser and parsed using our
baseline models. For the smaller training sets, sen-
tences from 1988 were used as they would be most
similar in style to the evaluation corpus. In all ex-
periments the sentences from 1989 were excluded
to ensure no overlap occurred with CCGbank.
As Wikipedia text we have used 794,024,397
tokens (51,673,069 sentences) from Wikipedia ar-
ticles. This text was processed in the same way as
the NANC data to produce parser-annotated train-
ing data. For supertagger evaluation, one thousand
sentences were manually annotated with CCG lex-
ical categories and POS tags. For parser evalua-
tion, three hundred of these sentences were man-
ually annotated with DepBank grammatical rela-
tions (King et al., 2003) in the style of Briscoe
and Carroll (2006). Both sets of annotations were
produced by manually correcting the output of the
baseline system. The annotation was performed
by Stephen Clark and Laura Rimell.
For the biomedical domain we have used sev-
eral different resources. As gold standard data for
supertagger evaluation we have used supertagged
GENIA data (Kim et al., 2003), annotated by
Rimell and Clark (2008). For parsing evalua-
tion, grammatical relations from the BioInfer cor-
pus were used (Pyysalo et al., 2007), with the
</bodyText>
<subsectionHeader confidence="0.524497">
Source Sentence Length Corpus %
</subsectionHeader>
<table confidence="0.9921255">
Range Average Variance
0-4 3.26 0.64 1.2
5-20 14.04 17.41 39.2
News 21-40 28.76 29.27 49.4
41-250 49.73 86.73 10.2
All 24.83 152.15 100.0
0-4 2.81 0.60 22.4
5-20 11.64 21.56 48.9
Wiki 21-40 28.02 28.48 24.3
41-250 49.69 77.70 4.5
All 15.33 154.57 100.0
0-4 2.98 0.75 0.9
5-20 14.54 15.14 41.3
Bio 21-40 28.49 29.34 48.0
41-250 49.17 68.34 9.8
All 24.53 139.35 100.0
</table>
<tableCaption confidence="0.848256333333333">
Table 1: Statistics for sentences in the supertagger
training data. Sentences containing more than 250
tokens were not included in our data sets.
</tableCaption>
<bodyText confidence="0.999436083333333">
same post-processing process as Rimell and Clark
(2009) to convert the C&amp;C parser output to Stan-
ford format grammatical relations (de Marneffe
et al., 2006). For adaptive training we have
used 1,900,618,859 tokens (76,739,723 sentences)
from the MEDLINE abstracts tokenised by McIn-
tosh and Curran (2008). These sentences were
POS-tagged and parsed twice, once as for the
newswire and Wikipedia data, and then again, us-
ing the bio-specific models developed by Rimell
and Clark (2009). Statistics for the sentences in
the training sets are given in Table 1.
</bodyText>
<subsectionHeader confidence="0.998749">
4.2 Speed evaluation data
</subsectionHeader>
<bodyText confidence="0.999989454545455">
For speed evaluation we held out three sets of sen-
tences from each domain-specific corpus. Specif-
ically, we used 30,000, 4,000 and 2,000 unique
sentences of length 5-20, 21-40 and 41-250 tokens
respectively. Speeds on these length controlled
sets were combined to calculate an overall pars-
ing speed for the text in each domain. Note that
more than 20% of the Wikipedia sentences were
less than five words in length and the overall dis-
tribution is skewed towards shorter sentences com-
pared to the other corpora.
</bodyText>
<sectionHeader confidence="0.999012" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999984428571429">
We used the hybrid parsing model described in
Clark and Curran (2007), and the Viterbi decoder
to find the highest-scoring derivation. The multi-
pass supertagger-parser interaction was also used.
The test data was excluded from training data
for the supertagger for all of the newswire and
Wikipedia models. For the biomedical models ten-
</bodyText>
<page confidence="0.993767">
348
</page>
<bodyText confidence="0.996629">
fold cross validation was used. The accuracy of
supertagging is measured by multi-tagging at the
first Q level and considering a word correct if the
correct tag is amongst any of the assigned tags.
For the biomedical parser evaluation we have
used the parsing model and grammatical relation
conversion script from Rimell and Clark (2009).
Our timing measurements are calculated in two
ways. Overall times were measured using the C&amp;C
parser’s timers. Individual sentence measurements
were made using the Intel timing registers, since
standard methods are not accurate enough for the
short time it takes to parse a single sentence.
To check whether changes were statistically sig-
nificant we applied the test described by Chinchor
(1995). This measures the probability that two sets
of responses are drawn from the same distribution,
where a score below 0.05 is considered significant.
Models were trained on an Intel Core2Duo
3GHz with 4GB of RAM. The evaluation was per-
formed on a dual quad-core Intel Xeon 2.27GHz
with 16GB of RAM.
</bodyText>
<subsectionHeader confidence="0.997908">
5.1 Tagging ambiguity optimisation
</subsectionHeader>
<bodyText confidence="0.9999791">
The number of lexical categories assigned to a
word by the CCG supertagger depends on the prob-
abilities calculated for each category and the Q
level being used. Each lexical category with a
probability within a factor of Q of the most prob-
able category is included. This means that the
choice of Q level determines the tagging ambigu-
ity, and so has great influence on parsing speed, ac-
curacy and coverage. Also, the tagging ambiguity
produced by a Q level will vary between models.
A more confident model will have a more peaked
distribution of category probabilities for a word,
and therefore need a smaller Q value to assign the
same number of categories.
Additionally, the C&amp;C parser uses multiple Q
levels. The first pass over a sentence is at a high Q
level, resulting in a low tagging ambiguity. If the
categories assigned are too restrictive to enable a
spanning analysis, the system makes another pass
with a lower Q level, resulting in a higher tagging
ambiguity. A maximum of five passes are made,
with the Q levels varying from 0.075 to 0.001.
We have taken two approaches to choosing Q
levels. When the aim of an experiment is to im-
prove speed, we use the system’s default Q levels.
While this choice means a more confident model
will assign fewer tags, this simply reflects the fact
that the model is more confident. It should pro-
duce similar accuracy results, but with lower am-
biguity, which will lead to higher speed.
For accuracy optimisation experiments we tune
the Q levels to produce the same average tagging
ambiguity as the baseline model on Section 00 of
CCGbank. Accuracy depends heavily on the num-
ber of categories supplied, so the new models are
at an accuracy disadvantage if they propose fewer
categories. By matching the ambiguity of the de-
fault model, we can increase accuracy at the cost
of some of the speed improvements the new mod-
els obtain.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999968388888889">
We have performed four primary sets of exper-
iments to explore the ability of an adaptive su-
pertagger to improve parsing speed or accuracy. In
the first two experiments, we explore performance
on the newswire domain, which is the source of
training data for the parsing model and the base-
line supertagging model. In the second set of ex-
periments, we train on a mixture of gold standard
newswire data and parser-annotated data from the
target domain.
In both cases we perform two experiments. The
first aimed to improve speed, keeping the Q levels
the same. This should lead to an increase in speed
as the extra training data means the models are
more confident and so have lower ambiguity than
the baseline model for a given Q value. The second
experiment aimed to improve accuracy, tuning the
Q levels as described in the previous section.
</bodyText>
<subsectionHeader confidence="0.983389">
6.1 Newswire speed improvement
</subsectionHeader>
<bodyText confidence="0.9999904375">
In our first experiment, we trained supertagger
models using Generalised Iterative Scaling (GIS)
(Darroch and Ratcliff, 1972), the limited mem-
ory BFGS method (BFGS) (Nocedal and Wright,
1999), the averaged perceptron (Collins, 2002),
and the margin infused relaxed algorithm (MIRA)
(Crammer and Singer, 2003). Note that these
are all alternative methods for estimating the lo-
cal log-linear probability distributions used by the
Ratnaparkhi-style tagger. We do not use global
tagging models as in Lafferty et al. (2001) or
Collins (2002). The training data consisted of Sec-
tions 02–21 of CCGbank and progressively larger
quantities of parser-annotated NANC data – from
zero to four million extra sentences. The results of
these tests are presented in Table 2.
</bodyText>
<page confidence="0.991225">
349
</page>
<table confidence="0.999915">
Ambiguity (%) Tagging Accuracy (%) F-score Speed (sents / sec)
Data 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m
Baseline 1.27 96.34 85.46 39.6
BFGS 1.27 1.23 1.19 1.18 96.33 96.18 95.95 95.93 85.45 85.51 85.57 85.68 39.8 49.6 71.8 60.0
GIS 1.28 1.24 1.21 1.20 96.44 96.27 96.09 96.11 85.44 85.46 85.58 85.62 37.4 44.1 51.3 54.1
MIRA 1.30 1.24 1.17 1.13 96.44 96.14 95.56 95.18 85.44 85.40 85.38 85.42 34.1 44.8 60.2 73.3
</table>
<tableCaption confidence="0.88841">
Table 2: Speed improvements on newswire, using various amounts of parser-annotated NANC data.
</tableCaption>
<table confidence="0.999824272727273">
Sentences Av. Time Change (ms) Total Time Change (s)
Sentence length 5-20 21-40 41-250 5-20 21-40 41-250 5-20 21-40 41-250
Lower tag amb. 1166 333 281 -7.54 -71.42 -183.23 -1.1 -29 -26
Earlier pass Same tag amb. 248 38 8 -2.94 -27.08 -108.28 -0.095 -1.3 -0.44
Higher tag amb. 530 33 14 -5.84 -32.25 -44.10 -0.40 -1.3 -0.31
Lower tag amb. 19288 3120 1533 -1.13 -5.18 -38.05 -2.8 -20 -30
Same pass Same tag amb. 7285 259 35 -0.29 0.94 24.57 -0.28 0.30 0.44
Higher tag amb. 1133 101 24 -0.25 2.70 8.09 -0.037 0.34 0.099
Lower tag amb. 334 114 104 0.90 7.60 -46.34 0.039 1.1 -2.5
Later pass Same tag amb. 14 1 0 1.06 4.26 n/a 0.0019 0.0053 0.0
Higher tag amb. 2 1 1 -0.13 26.43 308.03 -3.4e-05 0.033 0.16
</table>
<tableCaption confidence="0.996318">
Table 3: Breakdown of the source of changes in speed. The test sentences are divided into nine sets
</tableCaption>
<bodyText confidence="0.988405446808511">
based on the change in parsing behaviour between the baseline model and a model trained using MIRA,
Sections 02-21 of CCGbank and 4,000,000 NANC sentences.
Using the default Q levels we found that the
perceptron-trained models lost accuracy, disqual-
ifying them from this test. The BFGS, GIS and
MIRA models produced mixed results, but no
statistically significant decrease in accuracy, and
as the amount of parser-annotated data was in-
creased, parsing speed increased by up to 85%.
To determine the source of the speed improve-
ment we considered the times recorded by the tim-
ing registers. In Table 3, we have aggregated these
measurements based on the change in the pass at
which the sentence is parsed, and how the tag-
ging ambiguity changes on that pass. For sen-
tences parsed on two different passes the ambigu-
ity comparison is at the earlier pass. The “Total
Time Change” section of the table is the change in
parsing time for sentences of that type when pars-
ing ten thousand sentences from the corpus. This
takes into consideration the actual distribution of
sentence lengths in the corpus.
Several effects can be observed in these re-
sults. 72% of sentences are parsed on the same
pass, but with lower tag ambiguity (5th row in Ta-
ble 3). This provides 44% of the speed improve-
ment. Three to six times as many sentences are
parsed on an earlier pass than are parsed on a later
pass. This means the sentences parsed later have
very little effect on the overall speed. At the same
time, the average gain for sentences parsed earlier
is almost always larger than the average cost for
sentences parsed later. These effects combine to
produce a particularly large improvement for the
sentences parsed at an earlier pass. In fact, despite
making up only 7% of sentences in the set, those
parsed earlier with lower ambiguity provide 50%
of the speed improvement.
It is also interesting to note the changes for sen-
tences parsed on the same pass, with the same
ambiguity. We may expect these sentences to be
parsed in approximately the same amount of time,
and this is the case for the short set, but not for the
two larger sets, where we see an increase in pars-
ing time. This suggests that the categories being
supplied are more productive, leading to a larger
set of possible derivations.
</bodyText>
<subsectionHeader confidence="0.999257">
6.2 Newswire accuracy optimised
</subsectionHeader>
<bodyText confidence="0.999584266666667">
Any decrease in tagging ambiguity will generally
lead to a decrease in accuracy. The parser uses a
more sophisticated algorithm with global knowl-
edge of the sentence and so we would expect it
to be better at choosing categories than the su-
pertagger. Unlike the supertagger it will exclude
categories that cannot be used in a derivation. In
the previous section, we saw that training the su-
pertagger on parser output allowed us to develop
models that produced the same categories, despite
lower tagging ambiguity. Since they were trained
on the categories the parser was able to use in
derivations, these models should also now be pro-
viding categories that are more likely to be useful.
This leads us to our second experiment, opti-
</bodyText>
<page confidence="0.992457">
350
</page>
<table confidence="0.999460428571428">
Tagging Accuracy (%) F-score Speed (sents / sec)
NANC sents 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m
Baseline 96.34 85.46 39.6
BFGS 96.33 96.42 96.42 96.66 85.45 85.55 85.64 85.98 39.5 43.7 43.9 42.7
GIS 96.34 96.43 96.53 96.62 85.36 85.47 85.84 85.87 39.1 41.4 41.7 42.6
Perceptron 95.82 95.99 96.30 - 85.28 85.39 85.64 - 45.9 48.0 45.2 -
MIRA 96.23 96.29 96.46 96.63 85.47 85.45 85.55 85.84 37.7 41.4 41.4 42.9
</table>
<tableCaption confidence="0.979324">
Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.
</tableCaption>
<table confidence="0.998322166666667">
Train Corpus Ambiguity Tag. Acc. F-score Speed (sents / sec)
News Wiki Bio News Wiki Bio News Wiki Bio News Wiki Bio
Baseline 1.267 1.317 1.281 96.34 94.52 90.70 85.46 80.8 75.0 39.6 50.9 35.1
News 1.126 1.151 1.130 95.18 93.56 90.07 85.42 81.2 75.2 73.3 83.9 60.3
Wiki 1.147 1.154 1.129 95.06 93.52 90.03 84.70 81.4 75.5 62.4 73.9 58.7
Bio 1.134 1.146 1.114 94.66 93.15 89.88 84.23 80.7 75.9 66.2 90.4 59.3
</table>
<tableCaption confidence="0.821237">
Table 5: Cross-corpus speed improvement, models trained with MIRA and 4,000,000 sentences. The
highlighted values are the top speed for each evaluation set and results that are statistically indistinguish-
able from it.
</tableCaption>
<bodyText confidence="0.999526411764706">
mising accuracy on newswire. We used the same
models as in the previous experiment, but tuned
the Q levels as described in Section 5.1.
Comparing Tables 2 and 4 we can see the in-
fluence of Q level choice, and therefore tagging
ambiguity. When the default Q values were used
ambiguity dropped consistently as more parser-
annotated data was used, and category accuracy
dropped in the same way. Tuning the Q levels to
match ambiguity produces the opposite trend.
Interestingly, while the decrease in supertag ac-
curacy in the previous experiment did not translate
into a decrease in F-score, the increase in tag accu-
racy here does translate into an increase in F-score.
This indicates that the supertagger is adapting to
suit the parser. In the previous experiment, the
supertagger was still providing the categories the
parser would have used with the baseline supertag-
ging model, but it provided fewer other categories.
Since the parser is not a perfect supertagger these
other categories may in fact have been incorrect,
and so supertagger accuracy goes down, without
changing parsing results. Here we have allowed
the supertagger to assign extra categories, which
will only increase its accuracy.
The increase in F-score has two sources. First,
our supertagger is more accurate, and so the parser
is more likely to receive category sets that can be
combined into the correct derivation. Also, the su-
pertagger has been trained on categories that the
parser is able to use in derivations, which means
they are more productive.
As Table 6 shows, this change translates into an
improvement of up to 0.75% in F-score on Section
</bodyText>
<table confidence="0.999450333333333">
Model Tag. Acc. F-score Speed
(%) (%) (sents/sec)
Baseline 96.51 85.20 39.6
GIS, 4,000k NANC 96.83 85.95 42.6
BFGS, 4,000k NANC 96.91 85.90 42.7
MIRA, 4,000k NANC 96.84 85.79 42.9
</table>
<tableCaption confidence="0.804168">
Table 6: Evaluation of top models on Section 23 of
CCGbank. All changes in F-score are statistically
significant.
</tableCaption>
<bodyText confidence="0.97276475">
23 of CCGbank. All of the new models in the table
make a statistically significant improvement over
the baseline.
It is also interesting to note that the results in
Tables 2, 4 and 6, are similar for all of the train-
ing algorithms. However, the training times differ
considerably. For all four algorithms the training
time is proportional to the amount of data, but the
GIS and BFGS models trained on only CCGbank
took 4,500 and 4,200 seconds to train, while the
equivalent perceptron and MIRA models took 90
and 95 seconds to train.
</bodyText>
<subsectionHeader confidence="0.999636">
6.3 Annotation method comparison
</subsectionHeader>
<bodyText confidence="0.999972636363636">
To determine whether these improvements were
dependent on the annotations being produced
by the parser we performed a set of tests with
supertagger, rather than parser, annotated data.
Three extra training sets were created by annotat-
ing newswire sentences with supertags using the
baseline supertagging model. One set used the
one-best tagger, and two were produced using the
most probable tag for each word out of the set sup-
plied by the multi-tagger, with variations in the Q
value and dictionary cutoff for the two sets.
</bodyText>
<page confidence="0.994495">
351
</page>
<table confidence="0.997381666666667">
Train Corpus Ambiguity Tag. Acc. F-score Speed (sents / sec)
Wiki Bio News Wiki Bio News Wiki Bio News Wiki Bio
Baseline 1.317 1.281 96.34 94.52 90.70 85.46 80.8 75.0 39.6 50.9 35.1
News 1.331 1.322 96.53 94.86 91.32 85.84 80.1 75.2 41.8 32.6 31.4
Wiki 1.293 1.251 96.28 94.79 91.08 85.02 81.7 75.8 40.4 37.2 37.2
Bio 1.287 1.195 96.15 94.28 91.03 84.95 80.6 76.1 39.2 52.9 26.2
</table>
<tableCaption confidence="0.972748">
Table 7: Cross-corpus accuracy optimisation, models trained with GIS and 400,000 sentences.
</tableCaption>
<table confidence="0.9998385">
Annotation method Tag. Acc. F-score
Baseline 96.34 85.46
Parser 96.46 85.55
One-best super 95.94 85.24
Multi-tagger a 95.91 84.98
Multi-tagger b 96.00 84.99
</table>
<tableCaption confidence="0.765978">
Table 8: Comparison of annotation methods for
extra data. The multi-taggers used Q values 0.075
and 0.001, and dictionary cutoffs 20 and 150, for
taggers a and b respectively.
</tableCaption>
<table confidence="0.9994012">
Corpus Speed (sents / sec)
Sent length 5-20 21-40 41-250
News 242 44.8 8.24
Wiki 224 42.0 6.10
Bio 268 41.5 6.48
</table>
<tableCaption confidence="0.9545005">
Table 9: Cross-corpus speed for the baseline
model on data sets balanced on sentence length.
</tableCaption>
<bodyText confidence="0.99987475">
As Table 8 shows, in all cases the use of
supertagger-annotated data led to poorer perfor-
mance than the baseline system, while the use of
parser-annotated data led to an improvement in F-
score. The parser has access to a range of infor-
mation that the supertagger does not, producing a
different view of the data that the supertagger can
productively learn from.
</bodyText>
<subsectionHeader confidence="0.99912">
6.4 Cross-domain speed improvement
</subsectionHeader>
<bodyText confidence="0.999984323529411">
When applying parsers out of domain they are typ-
ically slower and less accurate (Gildea, 2001). In
this experiment, we attempt to increase speed on
out-of-domain data. Note that for some of the re-
sults presented here it may appear that the C&amp;C
parser does not lose speed when out of domain,
since the Wikipedia and biomedical corpora con-
tain shorter sentences on average than the news
corpus. However, by testing on balanced sets it
is clear that speed does decrease, particularly for
longer sentences, as shown in Table 9.
For our domain adaptation development ex-
periments, we considered a collection of differ-
ent models; here we only present results for the
best set of models. For speed improvement these
were MIRA models trained on 4,000,000 parser-
annotated sentences from the target domain.
As Table 5 shows, this training method pro-
duces models adapted to the new domain. In par-
ticular, note that models trained on Wikipedia or
the biomedical data produce lower F-scores3 than
the baseline on newswire. Meanwhile, on the
target domain they are adapted to, these models
achieve a higher F-score and parse sentences at
least 45% faster than the baseline.
The changes in tagging ambiguity and accuracy
also show that adaptation has occurred. In all
cases, the new models have lower tagging ambi-
guity, and lower supertag accuracy. However, on
the corpus of the extra data, the performance of
the adapted models is comparable to the baseline
model, which means the parser is probably still be
receiving the same categories that it used from the
sets provided by the baseline system.
</bodyText>
<subsectionHeader confidence="0.999679">
6.5 Cross-domain accuracy optimised
</subsectionHeader>
<bodyText confidence="0.99982865">
The ambiguity tuning method used to improve ac-
curacy on the newspaper domain can also be ap-
plied to the models trained on other domains. In
Table 7, we have tested models trained using GIS
and 400,000 sentences of parsed target-domain
text, with Q levels tuned to match ambiguity with
the baseline.
As for the newspaper domain, we observe in-
creased supertag accuracy and F-score. Also, in
almost every case the new models perform worse
than the baseline on domains other than the one
they were trained on.
In some cases the models in Table 7 are less ac-
curate than those in Table 5. This is because as
well as optimising the Q levels we have changed
training methods. All of the training methods were
tried, but only the method with the best results in
newswire is included here, which for F-score when
trained on 400,000 sentences was GIS.
The accuracy presented so far for the biomedi-
</bodyText>
<footnote confidence="0.99488675">
3Note that the F-scores for Wikipedia and biomedical text
are reported to only three significant figures as only 300 and
500 sentences respectively were available for parser evalua-
tion.
</footnote>
<page confidence="0.985945">
352
</page>
<table confidence="0.997887125">
Train Corpus F-score
Rimell and Clark (2009) 81.5
Baseline 80.7
CCGbank + Genia 81.5
+ Newswire 81.9
+ Wikipedia 82.2
+ Biomedical 81.7
+ R&amp;C annotated Bio 82.3
</table>
<tableCaption confidence="0.982693">
Table 10: Performance comparison for models us-
</tableCaption>
<bodyText confidence="0.904223785714286">
ing extra gold standard biomedical data. Models
were trained with GIS and 4,000,000 extra sen-
tences, and are tested using a POS-tagger trained
on biomedical data.
cal model is considerably lower than that reported
by Rimell and Clark (2009). This is because no
gold standard biomedical training data was used
in our experiments. Table 10 shows the results of
adding Rimell and Clark’s gold standard biomedi-
cal supertag data and using their biomedical POS-
tagger. The table also shows how accuracy can be
further improved by adding our parser-annotated
data from the biomedical domain as well as the
additional gold standard data.
</bodyText>
<sectionHeader confidence="0.998376" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99997621875">
This work has demonstrated that an adapted su-
pertagger can improve parsing speed and accu-
racy. The purpose of the supertagger is to re-
duce the search space for the parser. By train-
ing the supertagger on parser output, we allow the
parser to reach the derivation it would have found,
sooner. This approach also enables domain adap-
tation, improving speed and accuracy outside the
original domain of the parser.
The perceptron-based algorithms used in this
work are also able to function online, modifying
the model weights after each sentence is parsed.
This could be used to construct a system that con-
tinuously adapts to the domain it is parsing.
By training on parser-annotated NANC data
we constructed models that were adapted to the
newspaper-trained parser. The fastest model
parsed sentences 1.85 times as fast and was as
accurate as the baseline system. Adaptive train-
ing is also an effective method of improving per-
formance on other domains. Models trained on
parser-annotated Wikipedia text and MEDLINE
text had improved performance on these target do-
mains, in terms of both speed and accuracy. Op-
timising for speed or accuracy can be achieved by
modifying the Q levels used by the supertagger,
which controls the lexical category ambiguity at
each level used by the parser.
The result is an accurate and efficient wide-
coverage CCG parser that can be easily adapted for
NLP applications in new domains without manu-
ally annotating data.
</bodyText>
<sectionHeader confidence="0.994731" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999611111111111">
We thank the reviewers for their helpful feed-
back. This work was supported by Australian Re-
search Council Discovery grants DP0665973 and
DP1097291, the Capital Markets Cooperative Re-
search Centre, and a University of Sydney Merit
Scholarship. Part of the work was completed at the
Johns Hopkins University Summer Workshop and
(partially) supported by National Science Founda-
tion Grant Number IIS-0833652.
</bodyText>
<sectionHeader confidence="0.998837" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998642909090909">
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237–265.
Phil Blunsom and Timothy Baldwin. 2006. Multi-
lingual deep lexical acquisition for HPSGs via su-
pertagging. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 164–171, Sydney, Australia.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the Poster Ses-
sion of the 21st International Conference on Com-
putational Linguistics, Sydney, Australia.
John Chen, Srinivas Bangalore, and Vijay K. Shanker.
1999. New models for improving supertag disam-
biguation. In Proceedings of the Ninth Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 188–195, Bergen, Nor-
way.
John Chen, Srinivas Bangalore, Michael Collins, and
Owen Rambow. 2002. Reranking an n-gram su-
pertagger. In Proceedings of the 6th International
Workshop on Tree Adjoining Grammars and Related
Frameworks, pages 259–268, Venice, Italy.
Nancy Chinchor. 1995. Statistical significance
of MUC-6 results. In Proceedings of the Sixth
Message Understanding Conference, pages 39–43,
Columbia, MD, USA.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of the 20th International Con-
ference on Computational Linguistics, pages 282–
288, Geneva, Switzerland.
</reference>
<page confidence="0.995898">
353
</page>
<reference confidence="0.998746018518519">
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.
Stephen Clark, James R. Curran, and Miles Osborne.
2003. Bootstrapping POS-taggers using unlabelled
data. In Proceedings of the seventh Conference on
Natural Language Learning, pages 49–55, Edmon-
ton, Canada.
Michael Collins and Terry Koo. 2002. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25–69.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1–8, Philadel-
phia, PA, USA.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951–991.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
John N. Darroch and David Ratcliff. 1972. General-
ized iterative scaling for log-linear models. The An-
nals of Mathematical Statistics, 43(5):1470–1480.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449–54,
Genoa, Italy.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andrew Ng. 2002. Web question answering: Is
more always better? In Proceedings of the 25th In-
ternational ACMSIGIR Conference on Research and
Development, Tampere, Finland.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Pro-
cessing, Pittsburgh, PA, USA.
David Graff. 1995. North American News Text Cor-
pus. LDC95T21. Linguistic Data Consortium.
Philadelphia, PA, USA.
Hany Hassan, Khalil Sima’an, and Andy Way. 2007.
Supertagged phrase-based statistical machine trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
288–295, Prague, Czech Republic.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.
Kristy Hollingshead and Brian Roark. 2007. Pipeline
iteration. In Proceedings of the 45th Meeting of the
Association for Computational Linguistics, pages
952–959, Prague, Czech Republic.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun’ichi Tsujii. 2003. GENIA corpus - a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19(1):180–182.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The
PARC 700 Dependency Bank. In Proceedings of
the 4th International Workshop on Linguistically In-
terpreted Corpora, Budapest, Hungary.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282–289, San Francisco, CA, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006a. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, Brook-
lyn, NY, USA.
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for parser
adaptation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 337–344, Sydney, Aus-
tralia.
Tara McIntosh and James R. Curran. 2008. Weighted
mutual exclusion bootstrapping for domain inde-
pendent lexicon and template acquisition. In Pro-
ceedings of the Australasian Language Technology
Workshop, Hobart, Australia.
Jorge Nocedal and Stephen J. Wright. 1999. Numeri-
cal Optimization. Springer.
Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-
tri Haverinen, Juho Heimonen, and Tapio Salakoski.
2007. On the unification of syntactic annotations
under the Stanford dependency scheme: a case study
on bioinfer and GENIA. In Proceedings of the ACL
workshop on biological, translational, and clinical
language processing, pages 25–32, Prague, Czech
Republic.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the 1996 Con-
ference on Empirical Methods in Natural Language
Processing, pages 133–142, Philadelphia, PA, USA.
</reference>
<page confidence="0.989436">
354
</page>
<reference confidence="0.999523558139535">
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
475–484, Honolulu, HI, USA.
Laura Rimell and Stephen Clark. 2009. Port-
ing a lexicalized-grammar parser to the biomedi-
cal domain. Journal of Biomedical Informatics,
42(5):852–865.
Anoop Sarkar, Fel Xia, and Aravind K. Joshi. 2000.
Some experiments on indicators of parsing com-
plexity for lexicalized grammars. In Proceedings of
the COLING Workshop on Efficiency in Large-scale
Parsing Systems, pages 37–42, Luxembourg.
Anoop Sarkar. 2001. Applying co-training methods
to statistical parsing. In Proceedings of the Second
Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, pages 1–8,
Pittsburgh, PA, USA.
Anoop Sarkar. 2007. Combining supertagging and
lexicalized tree-adjoining grammar parsing. In
Srinivas Bangalore and Aravind Joshi, editors, Com-
plexity of Lexical Descriptions and its Relevance to
Natural Language Processing: A Supertagging Ap-
proach. MIT Press, Boston, MA, USA.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Stephen Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the 10th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 331–338, Budapest, Hun-
gary.
Geertjan van Noord. 2009. Learning efficient parsing.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 817–825. Association for Com-
putational Linguistics.
Yao-zhong Zhang, Takuya Matsuzaki, and Jun’ichi
Tsujii. 2009. HPSG supertagging: A sequence la-
beling view. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies, pages
210–213, Paris, France.
</reference>
<page confidence="0.999007">
355
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.718268">
<title confidence="0.999881">Faster Parsing by Supertagger Adaptation</title>
<author confidence="0.9404345">K Jessika Tim James R Stephen</author>
<affiliation confidence="0.998649">of Information Department of Computer Computer University of Sydney University of Texas at Austin University of Cambridge</affiliation>
<address confidence="0.977388">NSW 2006, Australia Austin, TX, USA Cambridge CB3 0FD, UK</address>
<abstract confidence="0.990523857142857">We propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy. The idea is to train the supertagger on large amounts of parser output, so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highestscoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method usa and parser, obtaining significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="1789" citStr="Bangalore and Joshi (1999)" startWordPosition="272" endWordPosition="275">racy and speed improvements for Wikipedia and biomedical text. 1 Introduction In many NLP tasks and applications, e.g. distributional similarity (Curran, 2004) and question answering (Dumais et al., 2002), large volumes of text and detailed syntactic information are both critical for high performance. To avoid a tradeoff between these two, we need to increase parsing speed, but without losing accuracy. Parsing with lexicalised grammar formalisms, such as Lexicalised Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG; Steedman, 2000), can be made more efficient using a supertagger. Bangalore and Joshi (1999) call supertagging almost parsing because of the significant reduction in ambiguity which occurs once the supertags have been assigned. In this paper, we focus on the CCG parser and supertagger described in Clark and Curran (2007). Since the CCG lexical category set used by the supertagger is much larger than the Penn Treebank POS tag set, the accuracy of supertagging is much lower than POS tagging; hence the CCG supertagger assigns multiple supertags1 to a word, when the local context does not provide enough information to decide on the correct supertag. The supertagger feeds lexical categori</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Timothy Baldwin</author>
</authors>
<title>Multilingual deep lexical acquisition for HPSGs via supertagging.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>164--171</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="8109" citStr="Blunsom and Baldwin, 2006" startWordPosition="1301" endWordPosition="1304">egrated with the C&amp;C parser, several progressively lower Q values are considered. If a sentence is not parsed on one pass then the parser attempts to parse the sentence again with a lower Q value, using a larger set of categories from the supertagger. Since most sentences are parsed at the first level (in which the average number of supertags assigned to each word is only slightly greater than one), this provides some of the speed benefit of single tagging, but without loss of coverage (Clark and Curran, 2004). Supertagging has since been effectively applied to other formalisms, such as HPSG (Blunsom and Baldwin, 2006; Zhang et al., 2009), and as an information source for tasks such as Statistical Machine Translation (Hassan et al., 2007). The use of parser output for supertagger training has been explored for LTAG by Sarkar (2007). However, the focus of that work was on improving parser and supertagger accuracy rather than speed. &lt; S\NP &lt; S NP\NP NP S\NP S 346 Previously , watch imports were denied such duty-free treatment S/S �N/N N (S[dcl]\NP)/(S[pss]\NP) (S[pss]\NP)/NP NP/NP N/N N N N (S[dcl]\NP)/NP S[pss]\NP (N/N)/(N/N) S[adj]\NP (S[dcl]\NP)/(S[adj]\NP) (S[pss]\NP)/NP N/N (S[pt]\NP)/NP (S[dcl]\NP)/NP </context>
</contexts>
<marker>Blunsom, Baldwin, 2006</marker>
<rawString>Phil Blunsom and Timothy Baldwin. 2006. Multilingual deep lexical acquisition for HPSGs via supertagging. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 164–171, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank.</title>
<date>2006</date>
<booktitle>In Proceedings of the Poster Session of the 21st International Conference on Computational Linguistics,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="14575" citStr="Briscoe and Carroll (2006)" startWordPosition="2396" endWordPosition="2399"> style to the evaluation corpus. In all experiments the sentences from 1989 were excluded to ensure no overlap occurred with CCGbank. As Wikipedia text we have used 794,024,397 tokens (51,673,069 sentences) from Wikipedia articles. This text was processed in the same way as the NANC data to produce parser-annotated training data. For supertagger evaluation, one thousand sentences were manually annotated with CCG lexical categories and POS tags. For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the Source Sentence Length Corpus % Range Average Variance 0-4 3.26 0.64 1.2 5-20 14.04 17.41 39.2 News 21-40 28.76 </context>
</contexts>
<marker>Briscoe, Carroll, 2006</marker>
<rawString>Ted Briscoe and John Carroll. 2006. Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank. In Proceedings of the Poster Session of the 21st International Conference on Computational Linguistics, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>Srinivas Bangalore</author>
<author>Vijay K Shanker</author>
</authors>
<title>New models for improving supertag disambiguation.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>188--195</pages>
<location>Bergen,</location>
<contexts>
<context position="6949" citStr="Chen et al., 1999" startWordPosition="1108" endWordPosition="1111"> previous two supertags. The Viterbi algorithm I ate pizza with cutlery NP (S\NP)INP NP ((S\NP)\(S\NP))1NP NP &gt; &gt; S\NP (S\NP)\(S\NP) I ate pizza with anchovies NP (S\NP)INP NP (NP\NP)INP NP &gt; &lt; &gt; &lt; Figure 1: Two CCG derivations with PP ambiguity. can be used to find the most probable supertag sequence. Alternatively the Forward-Backward algorithm can be used to efficiently sum over all sequences, giving a probability distribution over supertags for each word which is conditional only on the input sentence. Supertaggers can be made accurate enough for wide coverage parsing using multi-tagging (Chen et al., 1999), in which more than one supertag can be assigned to a word; however, as more supertags are supplied by the supertagger, parsing efficiency decreases (Chen et al., 2002), demonstrating the influence of lexical ambiguity on parsing complexity (Sarkar et al., 2000). Clark and Curran (2004) applied supertagging to CCG, using a flexible multi-tagging approach. The supertagger assigns to a word all lexical categories whose probabilities are within some factor, Q, of the most probable category for that word. When the supertagger is integrated with the C&amp;C parser, several progressively lower Q values</context>
</contexts>
<marker>Chen, Bangalore, Shanker, 1999</marker>
<rawString>John Chen, Srinivas Bangalore, and Vijay K. Shanker. 1999. New models for improving supertag disambiguation. In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics, pages 188–195, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>Srinivas Bangalore</author>
<author>Michael Collins</author>
<author>Owen Rambow</author>
</authors>
<title>Reranking an n-gram supertagger.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks,</booktitle>
<pages>259--268</pages>
<location>Venice, Italy.</location>
<contexts>
<context position="7118" citStr="Chen et al., 2002" startWordPosition="1137" endWordPosition="1140">INP NP (NP\NP)INP NP &gt; &lt; &gt; &lt; Figure 1: Two CCG derivations with PP ambiguity. can be used to find the most probable supertag sequence. Alternatively the Forward-Backward algorithm can be used to efficiently sum over all sequences, giving a probability distribution over supertags for each word which is conditional only on the input sentence. Supertaggers can be made accurate enough for wide coverage parsing using multi-tagging (Chen et al., 1999), in which more than one supertag can be assigned to a word; however, as more supertags are supplied by the supertagger, parsing efficiency decreases (Chen et al., 2002), demonstrating the influence of lexical ambiguity on parsing complexity (Sarkar et al., 2000). Clark and Curran (2004) applied supertagging to CCG, using a flexible multi-tagging approach. The supertagger assigns to a word all lexical categories whose probabilities are within some factor, Q, of the most probable category for that word. When the supertagger is integrated with the C&amp;C parser, several progressively lower Q values are considered. If a sentence is not parsed on one pass then the parser attempts to parse the sentence again with a lower Q value, using a larger set of categories from</context>
</contexts>
<marker>Chen, Bangalore, Collins, Rambow, 2002</marker>
<rawString>John Chen, Srinivas Bangalore, Michael Collins, and Owen Rambow. 2002. Reranking an n-gram supertagger. In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks, pages 259–268, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<title>Statistical significance of MUC-6 results.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference,</booktitle>
<pages>39--43</pages>
<location>Columbia, MD, USA.</location>
<contexts>
<context position="17787" citStr="Chinchor (1995)" startWordPosition="2920" endWordPosition="2921">idering a word correct if the correct tag is amongst any of the assigned tags. For the biomedical parser evaluation we have used the parsing model and grammatical relation conversion script from Rimell and Clark (2009). Our timing measurements are calculated in two ways. Overall times were measured using the C&amp;C parser’s timers. Individual sentence measurements were made using the Intel timing registers, since standard methods are not accurate enough for the short time it takes to parse a single sentence. To check whether changes were statistically significant we applied the test described by Chinchor (1995). This measures the probability that two sets of responses are drawn from the same distribution, where a score below 0.05 is considered significant. Models were trained on an Intel Core2Duo 3GHz with 4GB of RAM. The evaluation was performed on a dual quad-core Intel Xeon 2.27GHz with 16GB of RAM. 5.1 Tagging ambiguity optimisation The number of lexical categories assigned to a word by the CCG supertagger depends on the probabilities calculated for each category and the Q level being used. Each lexical category with a probability within a factor of Q of the most probable category is included. T</context>
</contexts>
<marker>Chinchor, 1995</marker>
<rawString>Nancy Chinchor. 1995. Statistical significance of MUC-6 results. In Proceedings of the Sixth Message Understanding Conference, pages 39–43, Columbia, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>282--288</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="7237" citStr="Clark and Curran (2004)" startWordPosition="1156" endWordPosition="1159">e supertag sequence. Alternatively the Forward-Backward algorithm can be used to efficiently sum over all sequences, giving a probability distribution over supertags for each word which is conditional only on the input sentence. Supertaggers can be made accurate enough for wide coverage parsing using multi-tagging (Chen et al., 1999), in which more than one supertag can be assigned to a word; however, as more supertags are supplied by the supertagger, parsing efficiency decreases (Chen et al., 2002), demonstrating the influence of lexical ambiguity on parsing complexity (Sarkar et al., 2000). Clark and Curran (2004) applied supertagging to CCG, using a flexible multi-tagging approach. The supertagger assigns to a word all lexical categories whose probabilities are within some factor, Q, of the most probable category for that word. When the supertagger is integrated with the C&amp;C parser, several progressively lower Q values are considered. If a sentence is not parsed on one pass then the parser attempts to parse the sentence again with a lower Q value, using a larger set of categories from the supertagger. Since most sentences are parsed at the first level (in which the average number of supertags assigned</context>
<context position="10964" citStr="Clark and Curran, 2004" startWordPosition="1779" endWordPosition="1782">es. The only work we are aware of that uses selftraining to improve the efficiency of parsers is van Noord (2009), who adopts a similar idea to the one in this paper for improving the efficiency of a Dutch parser based on a manually constructed HPSG grammar. 3 Adaptive Supertagging The purpose of the supertagger is to cut down the search space for the parser by reducing the set of categories that must be considered for each word. A perfect supertagger would assign the correct category to every word. CCG supertaggers are about 92% accurate when assigning a single lexical category to each word (Clark and Curran, 2004). This is not accurate enough for wide coverage parsing and so a multi-tagging approach is used instead. In the final derivation, the parser uses one category from each set, and it is important to note that having the correct category in the set does not guarantee that the parser will use it. Figure 2 gives an example sentence and the sets of lexical categories supplied by the supertagger, for a particular value of 0.2 The usual target of the supertagging task is to produce the top row of categories in Figure 2, the correct categories. We propose a new task that instead aims for the categories</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. The importance of supertagging for wide-coverage CCG parsing. In Proceedings of the 20th International Conference on Computational Linguistics, pages 282– 288, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="2019" citStr="Clark and Curran (2007)" startWordPosition="310" endWordPosition="313">etailed syntactic information are both critical for high performance. To avoid a tradeoff between these two, we need to increase parsing speed, but without losing accuracy. Parsing with lexicalised grammar formalisms, such as Lexicalised Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG; Steedman, 2000), can be made more efficient using a supertagger. Bangalore and Joshi (1999) call supertagging almost parsing because of the significant reduction in ambiguity which occurs once the supertags have been assigned. In this paper, we focus on the CCG parser and supertagger described in Clark and Curran (2007). Since the CCG lexical category set used by the supertagger is much larger than the Penn Treebank POS tag set, the accuracy of supertagging is much lower than POS tagging; hence the CCG supertagger assigns multiple supertags1 to a word, when the local context does not provide enough information to decide on the correct supertag. The supertagger feeds lexical categories to the parser, and the two interact, sometimes using multiple passes over a sentence. If a spanning analysis cannot be found by the parser, the number of lexical categories supplied by the supertagger is increased. The supertag</context>
<context position="16782" citStr="Clark and Curran (2007)" startWordPosition="2760" endWordPosition="2763"> 1. 4.2 Speed evaluation data For speed evaluation we held out three sets of sentences from each domain-specific corpus. Specifically, we used 30,000, 4,000 and 2,000 unique sentences of length 5-20, 21-40 and 41-250 tokens respectively. Speeds on these length controlled sets were combined to calculate an overall parsing speed for the text in each domain. Note that more than 20% of the Wikipedia sentences were less than five words in length and the overall distribution is skewed towards shorter sentences compared to the other corpora. 5 Evaluation We used the hybrid parsing model described in Clark and Curran (2007), and the Viterbi decoder to find the highest-scoring derivation. The multipass supertagger-parser interaction was also used. The test data was excluded from training data for the supertagger for all of the newswire and Wikipedia models. For the biomedical models ten348 fold cross validation was used. The accuracy of supertagging is measured by multi-tagging at the first Q level and considering a word correct if the correct tag is amongst any of the assigned tags. For the biomedical parser evaluation we have used the parsing model and grammatical relation conversion script from Rimell and Clar</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
<author>Miles Osborne</author>
</authors>
<title>Bootstrapping POS-taggers using unlabelled data.</title>
<date>2003</date>
<booktitle>In Proceedings of the seventh Conference on Natural Language Learning,</booktitle>
<pages>49--55</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="3108" citStr="Clark et al. (2003)" startWordPosition="493" endWordPosition="496">ng analysis cannot be found by the parser, the number of lexical categories supplied by the supertagger is increased. The supertagger-parser interaction influences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence. Our goal is to increase parsing speed without loss of accuracy. The technique we use is a form of self-training, in which the output of the parser is used to train the supertagger component. The existing literature on self-training reports mixed results. Clark et al. (2003) were unable to improve the accuracy of POS tagging using self-training. In contrast, McClosky et al. (2006a) report improved accuracy through self-training for a twostage parser and re-ranker. Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adaptive supertagger. The adaptive supertagger produces lexical categories that the parser would have used in the final derivation when using the baseline model. However, it does so with much lower ambiguity levels, and potentially during an earlier pass, which means sentences are parsed faster. By increasing the </context>
</contexts>
<marker>Clark, Curran, Osborne, 2003</marker>
<rawString>Stephen Clark, James R. Curran, and Miles Osborne. 2003. Bootstrapping POS-taggers using unlabelled data. In Proceedings of the seventh Conference on Natural Language Learning, pages 49–55, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="9626" citStr="Collins and Koo, 2002" startWordPosition="1543" endWordPosition="1546">the supertagger. 2.2 Semi-supervised training Previous exploration of semi-supervised training in NLP has focused on improving accuracy, often for the case where only small amounts of manually labelled training data are available. One approach is co-training, in which two models with independent views of the data iteratively inform each other by labelling extra training data. Sarkar (2001) applied co-training to LTAG parsing, in which the supertagger and parser provide the two views. Steedman et al. (2003) extended the method to a variety of parser pairs. Another method is to use a re-ranker (Collins and Koo, 2002) on the output of a system to generate new training data. Like co-training, this takes advantage of a different view of the data, but the two views are not independent as the re-ranker is limited to the set of options produced by the system. This method has been used effectively to improve parsing performance on newspaper text (McClosky et al., 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al., 2006b). As well as using independent views of data to generate extra training data, multiple views can be used to provide constraints at test time. Hollingshead and Roa</context>
</contexts>
<marker>Collins, Koo, 2002</marker>
<rawString>Michael Collins and Terry Koo. 2002. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="21108" citStr="Collins, 2002" startWordPosition="3489" endWordPosition="3490">. The first aimed to improve speed, keeping the Q levels the same. This should lead to an increase in speed as the extra training data means the models are more confident and so have lower ambiguity than the baseline model for a given Q value. The second experiment aimed to improve accuracy, tuning the Q levels as described in the previous section. 6.1 Newswire speed improvement In our first experiment, we trained supertagger models using Generalised Iterative Scaling (GIS) (Darroch and Ratcliff, 1972), the limited memory BFGS method (BFGS) (Nocedal and Wright, 1999), the averaged perceptron (Collins, 2002), and the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003). Note that these are all alternative methods for estimating the local log-linear probability distributions used by the Ratnaparkhi-style tagger. We do not use global tagging models as in Lafferty et al. (2001) or Collins (2002). The training data consisted of Sections 02–21 of CCGbank and progressively larger quantities of parser-annotated NANC data – from zero to four million extra sentences. The results of these tests are presented in Table 2. 349 Ambiguity (%) Tagging Accuracy (%) F-score Speed (sents / sec) Data 0</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 1–8, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="21184" citStr="Crammer and Singer, 2003" startWordPosition="3498" endWordPosition="3501">. This should lead to an increase in speed as the extra training data means the models are more confident and so have lower ambiguity than the baseline model for a given Q value. The second experiment aimed to improve accuracy, tuning the Q levels as described in the previous section. 6.1 Newswire speed improvement In our first experiment, we trained supertagger models using Generalised Iterative Scaling (GIS) (Darroch and Ratcliff, 1972), the limited memory BFGS method (BFGS) (Nocedal and Wright, 1999), the averaged perceptron (Collins, 2002), and the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003). Note that these are all alternative methods for estimating the local log-linear probability distributions used by the Ratnaparkhi-style tagger. We do not use global tagging models as in Lafferty et al. (2001) or Collins (2002). The training data consisted of Sections 02–21 of CCGbank and progressively larger quantities of parser-annotated NANC data – from zero to four million extra sentences. The results of these tests are presented in Table 2. 349 Ambiguity (%) Tagging Accuracy (%) F-score Speed (sents / sec) Data 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m Baseline 1.27 96.</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1322" citStr="Curran, 2004" startWordPosition="203" endWordPosition="204"> to supply the supertags that the parser will eventually choose as part of the highestscoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtaining significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text. 1 Introduction In many NLP tasks and applications, e.g. distributional similarity (Curran, 2004) and question answering (Dumais et al., 2002), large volumes of text and detailed syntactic information are both critical for high performance. To avoid a tradeoff between these two, we need to increase parsing speed, but without losing accuracy. Parsing with lexicalised grammar formalisms, such as Lexicalised Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG; Steedman, 2000), can be made more efficient using a supertagger. Bangalore and Joshi (1999) call supertagging almost parsing because of the significant reduction in ambiguity which occurs once the supertags have been assigne</context>
<context position="7237" citStr="Curran (2004)" startWordPosition="1158" endWordPosition="1159"> sequence. Alternatively the Forward-Backward algorithm can be used to efficiently sum over all sequences, giving a probability distribution over supertags for each word which is conditional only on the input sentence. Supertaggers can be made accurate enough for wide coverage parsing using multi-tagging (Chen et al., 1999), in which more than one supertag can be assigned to a word; however, as more supertags are supplied by the supertagger, parsing efficiency decreases (Chen et al., 2002), demonstrating the influence of lexical ambiguity on parsing complexity (Sarkar et al., 2000). Clark and Curran (2004) applied supertagging to CCG, using a flexible multi-tagging approach. The supertagger assigns to a word all lexical categories whose probabilities are within some factor, Q, of the most probable category for that word. When the supertagger is integrated with the C&amp;C parser, several progressively lower Q values are considered. If a sentence is not parsed on one pass then the parser attempts to parse the sentence again with a lower Q value, using a larger set of categories from the supertagger. Since most sentences are parsed at the first level (in which the average number of supertags assigned</context>
<context position="10964" citStr="Curran, 2004" startWordPosition="1781" endWordPosition="1782">ly work we are aware of that uses selftraining to improve the efficiency of parsers is van Noord (2009), who adopts a similar idea to the one in this paper for improving the efficiency of a Dutch parser based on a manually constructed HPSG grammar. 3 Adaptive Supertagging The purpose of the supertagger is to cut down the search space for the parser by reducing the set of categories that must be considered for each word. A perfect supertagger would assign the correct category to every word. CCG supertaggers are about 92% accurate when assigning a single lexical category to each word (Clark and Curran, 2004). This is not accurate enough for wide coverage parsing and so a multi-tagging approach is used instead. In the final derivation, the parser uses one category from each set, and it is important to note that having the correct category in the set does not guarantee that the parser will use it. Figure 2 gives an example sentence and the sets of lexical categories supplied by the supertagger, for a particular value of 0.2 The usual target of the supertagging task is to produce the top row of categories in Figure 2, the correct categories. We propose a new task that instead aims for the categories</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John N Darroch</author>
<author>David Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>43</volume>
<issue>5</issue>
<contexts>
<context position="21001" citStr="Darroch and Ratcliff, 1972" startWordPosition="3471" endWordPosition="3474">f gold standard newswire data and parser-annotated data from the target domain. In both cases we perform two experiments. The first aimed to improve speed, keeping the Q levels the same. This should lead to an increase in speed as the extra training data means the models are more confident and so have lower ambiguity than the baseline model for a given Q value. The second experiment aimed to improve accuracy, tuning the Q levels as described in the previous section. 6.1 Newswire speed improvement In our first experiment, we trained supertagger models using Generalised Iterative Scaling (GIS) (Darroch and Ratcliff, 1972), the limited memory BFGS method (BFGS) (Nocedal and Wright, 1999), the averaged perceptron (Collins, 2002), and the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003). Note that these are all alternative methods for estimating the local log-linear probability distributions used by the Ratnaparkhi-style tagger. We do not use global tagging models as in Lafferty et al. (2001) or Collins (2002). The training data consisted of Sections 02–21 of CCGbank and progressively larger quantities of parser-annotated NANC data – from zero to four million extra sentences. The results of thes</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>John N. Darroch and David Ratcliff. 1972. Generalized iterative scaling for log-linear models. The Annals of Mathematical Statistics, 43(5):1470–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>449--54</pages>
<location>Genoa, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 449–54, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Dumais</author>
<author>Michele Banko</author>
<author>Eric Brill</author>
<author>Jimmy Lin</author>
<author>Andrew Ng</author>
</authors>
<title>Web question answering: Is more always better?</title>
<date>2002</date>
<booktitle>In Proceedings of the 25th International ACMSIGIR Conference on Research and Development,</booktitle>
<location>Tampere, Finland.</location>
<contexts>
<context position="1367" citStr="Dumais et al., 2002" startWordPosition="209" endWordPosition="212">er will eventually choose as part of the highestscoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtaining significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text. 1 Introduction In many NLP tasks and applications, e.g. distributional similarity (Curran, 2004) and question answering (Dumais et al., 2002), large volumes of text and detailed syntactic information are both critical for high performance. To avoid a tradeoff between these two, we need to increase parsing speed, but without losing accuracy. Parsing with lexicalised grammar formalisms, such as Lexicalised Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG; Steedman, 2000), can be made more efficient using a supertagger. Bangalore and Joshi (1999) call supertagging almost parsing because of the significant reduction in ambiguity which occurs once the supertags have been assigned. In this paper, we focus on the CCG parser </context>
</contexts>
<marker>Dumais, Banko, Brill, Lin, Ng, 2002</marker>
<rawString>Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, and Andrew Ng. 2002. Web question answering: Is more always better? In Proceedings of the 25th International ACMSIGIR Conference on Research and Development, Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="4336" citStr="Gildea, 2001" startWordPosition="686" endWordPosition="687"> of the adaptive models to match the baseline system, we can also slightly increase supertagging accuracy, which can lead to higher parsing accuracy. 1We use supertag and lexical category interchangeably. 345 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345–355, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Using the parser to generate training data also has the advantage that it is not a domain specific process. Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). Using a newspapertrained parser, we constructed new training sets for Wikipedia and biomedical text. These were used to create new supertagging models adapted to the different domains. The self-training method of adapting the supertagger to suit the parser increased parsing speed by more than 50% across all three domains, without loss of accuracy. Using an adapted supertagger with ambiguity levels tuned to match the baseline system, we were also able to increase F-score on labelled grammatical relations by 0.75%. 2 Background Many statistical parsers use two stages: a tagging stage that labe</context>
<context position="31673" citStr="Gildea, 2001" startWordPosition="5300" endWordPosition="5301">4 42.0 6.10 Bio 268 41.5 6.48 Table 9: Cross-corpus speed for the baseline model on data sets balanced on sentence length. As Table 8 shows, in all cases the use of supertagger-annotated data led to poorer performance than the baseline system, while the use of parser-annotated data led to an improvement in Fscore. The parser has access to a range of information that the supertagger does not, producing a different view of the data that the supertagger can productively learn from. 6.4 Cross-domain speed improvement When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). In this experiment, we attempt to increase speed on out-of-domain data. Note that for some of the results presented here it may appear that the C&amp;C parser does not lose speed when out of domain, since the Wikipedia and biomedical corpora contain shorter sentences on average than the news corpus. However, by testing on balanced sets it is clear that speed does decrease, particularly for longer sentences, as shown in Table 9. For our domain adaptation development experiments, we considered a collection of different models; here we only present results for the best set of models. For speed impr</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>North American News Text Corpus.</title>
<date>1995</date>
<booktitle>LDC95T21. Linguistic Data Consortium.</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="13706" citStr="Graff, 1995" startWordPosition="2254" endWordPosition="2255">rics for speed and accuracy, and explored the source of the changes in parsing performance. 4 Data In this work, we consider three domains: newswire, Wikipedia text and biomedical text. 4.1 Training and accuracy evaluation We have used Sections 02-21 of CCGbank (Hockenmaier and Steedman, 2007), the CCG version of the Penn Treebank (Marcus et al., 1993), as training data for the newspaper domain. Sections 00 and 23 were used for development and test evaluation. A further 113,346,430 tokens (4,566,241 sentences) of raw data from the Wall Street Journal section of the North American News Corpus (Graff, 1995) were parsed to produce the training data for adaptation. This text was tokenised using the C&amp;C tools tokeniser and parsed using our baseline models. For the smaller training sets, sentences from 1988 were used as they would be most similar in style to the evaluation corpus. In all experiments the sentences from 1989 were excluded to ensure no overlap occurred with CCGbank. As Wikipedia text we have used 794,024,397 tokens (51,673,069 sentences) from Wikipedia articles. This text was processed in the same way as the NANC data to produce parser-annotated training data. For supertagger evaluatio</context>
</contexts>
<marker>Graff, 1995</marker>
<rawString>David Graff. 1995. North American News Text Corpus. LDC95T21. Linguistic Data Consortium. Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hany Hassan</author>
<author>Khalil Sima’an</author>
<author>Andy Way</author>
</authors>
<title>Supertagged phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>288--295</pages>
<location>Prague, Czech Republic.</location>
<marker>Hassan, Sima’an, Way, 2007</marker>
<rawString>Hany Hassan, Khalil Sima’an, and Andy Way. 2007. Supertagged phrase-based statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288–295, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="13388" citStr="Hockenmaier and Steedman, 2007" startWordPosition="2196" endWordPosition="2200">nalyse the sentence correctly. 347 in three domains, producing new data annotated with the categories that the parser used with the baseline model. We constructed new supertagging models that are adapted to suit the parser by training on the combination of these sets and the standard training corpora. We applied standard evaluation metrics for speed and accuracy, and explored the source of the changes in parsing performance. 4 Data In this work, we consider three domains: newswire, Wikipedia text and biomedical text. 4.1 Training and accuracy evaluation We have used Sections 02-21 of CCGbank (Hockenmaier and Steedman, 2007), the CCG version of the Penn Treebank (Marcus et al., 1993), as training data for the newspaper domain. Sections 00 and 23 were used for development and test evaluation. A further 113,346,430 tokens (4,566,241 sentences) of raw data from the Wall Street Journal section of the North American News Corpus (Graff, 1995) were parsed to produce the training data for adaptation. This text was tokenised using the C&amp;C tools tokeniser and parsed using our baseline models. For the smaller training sets, sentences from 1988 were used as they would be most similar in style to the evaluation corpus. In all</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristy Hollingshead</author>
<author>Brian Roark</author>
</authors>
<title>Pipeline iteration.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>952--959</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="10235" citStr="Hollingshead and Roark (2007)" startWordPosition="1650" endWordPosition="1654">llins and Koo, 2002) on the output of a system to generate new training data. Like co-training, this takes advantage of a different view of the data, but the two views are not independent as the re-ranker is limited to the set of options produced by the system. This method has been used effectively to improve parsing performance on newspaper text (McClosky et al., 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al., 2006b). As well as using independent views of data to generate extra training data, multiple views can be used to provide constraints at test time. Hollingshead and Roark (2007) improved the accuracy of a parsing pipeline by using the output of later stages to constrain earlier stages. The only work we are aware of that uses selftraining to improve the efficiency of parsers is van Noord (2009), who adopts a similar idea to the one in this paper for improving the efficiency of a Dutch parser based on a manually constructed HPSG grammar. 3 Adaptive Supertagging The purpose of the supertagger is to cut down the search space for the parser by reducing the set of categories that must be considered for each word. A perfect supertagger would assign the correct category to e</context>
</contexts>
<marker>Hollingshead, Roark, 2007</marker>
<rawString>Kristy Hollingshead and Brian Roark. 2007. Pipeline iteration. In Proceedings of the 45th Meeting of the Association for Computational Linguistics, pages 952–959, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Yuka Teteisi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>GENIA corpus - a semantically annotated corpus for bio-textmining.</title>
<date>2003</date>
<journal>Bioinformatics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="14909" citStr="Kim et al., 2003" startWordPosition="2449" endWordPosition="2452">gger evaluation, one thousand sentences were manually annotated with CCG lexical categories and POS tags. For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the Source Sentence Length Corpus % Range Average Variance 0-4 3.26 0.64 1.2 5-20 14.04 17.41 39.2 News 21-40 28.76 29.27 49.4 41-250 49.73 86.73 10.2 All 24.83 152.15 100.0 0-4 2.81 0.60 22.4 5-20 11.64 21.56 48.9 Wiki 21-40 28.02 28.48 24.3 41-250 49.69 77.70 4.5 All 15.33 154.57 100.0 0-4 2.98 0.75 0.9 5-20 14.54 15.14 41.3 Bio 21-40 28.49 29.34 48.0 41-250 49.17 68.34 9.8 All 24.53 139.35 100.0 Table 1: Statistics for sentences in the superta</context>
</contexts>
<marker>Kim, Ohta, Teteisi, Tsujii, 2003</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun’ichi Tsujii. 2003. GENIA corpus - a semantically annotated corpus for bio-textmining. Bioinformatics, 19(1):180–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tracy H King</author>
<author>Richard Crouch</author>
<author>Stefan Riezler</author>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
</authors>
<title>The PARC 700 Dependency Bank.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="14532" citStr="King et al., 2003" startWordPosition="2388" endWordPosition="2391">ed as they would be most similar in style to the evaluation corpus. In all experiments the sentences from 1989 were excluded to ensure no overlap occurred with CCGbank. As Wikipedia text we have used 794,024,397 tokens (51,673,069 sentences) from Wikipedia articles. This text was processed in the same way as the NANC data to produce parser-annotated training data. For supertagger evaluation, one thousand sentences were manually annotated with CCG lexical categories and POS tags. For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the Source Sentence Length Corpus % Range Average Variance 0-4 3.26 0.64 </context>
</contexts>
<marker>King, Crouch, Riezler, Dalrymple, Kaplan, 2003</marker>
<rawString>Tracy H. King, Richard Crouch, Stefan Riezler, Mary Dalrymple, and Ronald M. Kaplan. 2003. The PARC 700 Dependency Bank. In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="21394" citStr="Lafferty et al. (2001)" startWordPosition="3531" endWordPosition="3534">rove accuracy, tuning the Q levels as described in the previous section. 6.1 Newswire speed improvement In our first experiment, we trained supertagger models using Generalised Iterative Scaling (GIS) (Darroch and Ratcliff, 1972), the limited memory BFGS method (BFGS) (Nocedal and Wright, 1999), the averaged perceptron (Collins, 2002), and the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003). Note that these are all alternative methods for estimating the local log-linear probability distributions used by the Ratnaparkhi-style tagger. We do not use global tagging models as in Lafferty et al. (2001) or Collins (2002). The training data consisted of Sections 02–21 of CCGbank and progressively larger quantities of parser-annotated NANC data – from zero to four million extra sentences. The results of these tests are presented in Table 2. 349 Ambiguity (%) Tagging Accuracy (%) F-score Speed (sents / sec) Data 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m Baseline 1.27 96.34 85.46 39.6 BFGS 1.27 1.23 1.19 1.18 96.33 96.18 95.95 95.93 85.45 85.51 85.57 85.68 39.8 49.6 71.8 60.0 GIS 1.28 1.24 1.21 1.20 96.44 96.27 96.09 96.11 85.44 85.46 85.58 85.62 37.4 44.1 51.3 54.1 MIRA 1.30 1</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="13448" citStr="Marcus et al., 1993" startWordPosition="2208" endWordPosition="2211"> annotated with the categories that the parser used with the baseline model. We constructed new supertagging models that are adapted to suit the parser by training on the combination of these sets and the standard training corpora. We applied standard evaluation metrics for speed and accuracy, and explored the source of the changes in parsing performance. 4 Data In this work, we consider three domains: newswire, Wikipedia text and biomedical text. 4.1 Training and accuracy evaluation We have used Sections 02-21 of CCGbank (Hockenmaier and Steedman, 2007), the CCG version of the Penn Treebank (Marcus et al., 1993), as training data for the newspaper domain. Sections 00 and 23 were used for development and test evaluation. A further 113,346,430 tokens (4,566,241 sentences) of raw data from the Wall Street Journal section of the North American News Corpus (Graff, 1995) were parsed to produce the training data for adaptation. This text was tokenised using the C&amp;C tools tokeniser and parsed using our baseline models. For the smaller training sets, sentences from 1988 were used as they would be most similar in style to the evaluation corpus. In all experiments the sentences from 1989 were excluded to ensure</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Brooklyn, NY, USA.</location>
<contexts>
<context position="3215" citStr="McClosky et al. (2006" startWordPosition="510" endWordPosition="513">s increased. The supertagger-parser interaction influences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence. Our goal is to increase parsing speed without loss of accuracy. The technique we use is a form of self-training, in which the output of the parser is used to train the supertagger component. The existing literature on self-training reports mixed results. Clark et al. (2003) were unable to improve the accuracy of POS tagging using self-training. In contrast, McClosky et al. (2006a) report improved accuracy through self-training for a twostage parser and re-ranker. Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adaptive supertagger. The adaptive supertagger produces lexical categories that the parser would have used in the final derivation when using the baseline model. However, it does so with much lower ambiguity levels, and potentially during an earlier pass, which means sentences are parsed faster. By increasing the ambiguity level of the adaptive models to match the baseline system, we can also slightly increase supertag</context>
<context position="9977" citStr="McClosky et al., 2006" startWordPosition="1606" endWordPosition="1609">lling extra training data. Sarkar (2001) applied co-training to LTAG parsing, in which the supertagger and parser provide the two views. Steedman et al. (2003) extended the method to a variety of parser pairs. Another method is to use a re-ranker (Collins and Koo, 2002) on the output of a system to generate new training data. Like co-training, this takes advantage of a different view of the data, but the two views are not independent as the re-ranker is limited to the set of options produced by the system. This method has been used effectively to improve parsing performance on newspaper text (McClosky et al., 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al., 2006b). As well as using independent views of data to generate extra training data, multiple views can be used to provide constraints at test time. Hollingshead and Roark (2007) improved the accuracy of a parsing pipeline by using the output of later stages to constrain earlier stages. The only work we are aware of that uses selftraining to improve the efficiency of parsers is van Noord (2009), who adopts a similar idea to the one in this paper for improving the efficiency of a Dutch parser based on a manually con</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006a. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Brooklyn, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>337--344</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3215" citStr="McClosky et al. (2006" startWordPosition="510" endWordPosition="513">s increased. The supertagger-parser interaction influences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence. Our goal is to increase parsing speed without loss of accuracy. The technique we use is a form of self-training, in which the output of the parser is used to train the supertagger component. The existing literature on self-training reports mixed results. Clark et al. (2003) were unable to improve the accuracy of POS tagging using self-training. In contrast, McClosky et al. (2006a) report improved accuracy through self-training for a twostage parser and re-ranker. Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adaptive supertagger. The adaptive supertagger produces lexical categories that the parser would have used in the final derivation when using the baseline model. However, it does so with much lower ambiguity levels, and potentially during an earlier pass, which means sentences are parsed faster. By increasing the ambiguity level of the adaptive models to match the baseline system, we can also slightly increase supertag</context>
<context position="9977" citStr="McClosky et al., 2006" startWordPosition="1606" endWordPosition="1609">lling extra training data. Sarkar (2001) applied co-training to LTAG parsing, in which the supertagger and parser provide the two views. Steedman et al. (2003) extended the method to a variety of parser pairs. Another method is to use a re-ranker (Collins and Koo, 2002) on the output of a system to generate new training data. Like co-training, this takes advantage of a different view of the data, but the two views are not independent as the re-ranker is limited to the set of options produced by the system. This method has been used effectively to improve parsing performance on newspaper text (McClosky et al., 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al., 2006b). As well as using independent views of data to generate extra training data, multiple views can be used to provide constraints at test time. Hollingshead and Roark (2007) improved the accuracy of a parsing pipeline by using the output of later stages to constrain earlier stages. The only work we are aware of that uses selftraining to improve the efficiency of parsers is van Noord (2009), who adopts a similar idea to the one in this paper for improving the efficiency of a Dutch parser based on a manually con</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006b. Reranking and self-training for parser adaptation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 337–344, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tara McIntosh</author>
<author>James R Curran</author>
</authors>
<title>Weighted mutual exclusion bootstrapping for domain independent lexicon and template acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<location>Hobart, Australia.</location>
<contexts>
<context position="15910" citStr="McIntosh and Curran (2008)" startWordPosition="2613" endWordPosition="2617"> 28.02 28.48 24.3 41-250 49.69 77.70 4.5 All 15.33 154.57 100.0 0-4 2.98 0.75 0.9 5-20 14.54 15.14 41.3 Bio 21-40 28.49 29.34 48.0 41-250 49.17 68.34 9.8 All 24.53 139.35 100.0 Table 1: Statistics for sentences in the supertagger training data. Sentences containing more than 250 tokens were not included in our data sets. same post-processing process as Rimell and Clark (2009) to convert the C&amp;C parser output to Stanford format grammatical relations (de Marneffe et al., 2006). For adaptive training we have used 1,900,618,859 tokens (76,739,723 sentences) from the MEDLINE abstracts tokenised by McIntosh and Curran (2008). These sentences were POS-tagged and parsed twice, once as for the newswire and Wikipedia data, and then again, using the bio-specific models developed by Rimell and Clark (2009). Statistics for the sentences in the training sets are given in Table 1. 4.2 Speed evaluation data For speed evaluation we held out three sets of sentences from each domain-specific corpus. Specifically, we used 30,000, 4,000 and 2,000 unique sentences of length 5-20, 21-40 and 41-250 tokens respectively. Speeds on these length controlled sets were combined to calculate an overall parsing speed for the text in each d</context>
</contexts>
<marker>McIntosh, Curran, 2008</marker>
<rawString>Tara McIntosh and James R. Curran. 2008. Weighted mutual exclusion bootstrapping for domain independent lexicon and template acquisition. In Proceedings of the Australasian Language Technology Workshop, Hobart, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
<author>Stephen J Wright</author>
</authors>
<title>Numerical Optimization.</title>
<date>1999</date>
<publisher>Springer.</publisher>
<contexts>
<context position="21067" citStr="Nocedal and Wright, 1999" startWordPosition="3482" endWordPosition="3485">get domain. In both cases we perform two experiments. The first aimed to improve speed, keeping the Q levels the same. This should lead to an increase in speed as the extra training data means the models are more confident and so have lower ambiguity than the baseline model for a given Q value. The second experiment aimed to improve accuracy, tuning the Q levels as described in the previous section. 6.1 Newswire speed improvement In our first experiment, we trained supertagger models using Generalised Iterative Scaling (GIS) (Darroch and Ratcliff, 1972), the limited memory BFGS method (BFGS) (Nocedal and Wright, 1999), the averaged perceptron (Collins, 2002), and the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003). Note that these are all alternative methods for estimating the local log-linear probability distributions used by the Ratnaparkhi-style tagger. We do not use global tagging models as in Lafferty et al. (2001) or Collins (2002). The training data consisted of Sections 02–21 of CCGbank and progressively larger quantities of parser-annotated NANC data – from zero to four million extra sentences. The results of these tests are presented in Table 2. 349 Ambiguity (%) Tagging Accura</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>Jorge Nocedal and Stephen J. Wright. 1999. Numerical Optimization. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Filip Ginter</author>
<author>Veronika Laippala</author>
<author>Katri Haverinen</author>
<author>Juho Heimonen</author>
<author>Tapio Salakoski</author>
</authors>
<title>On the unification of syntactic annotations under the Stanford dependency scheme: a case study on bioinfer and GENIA.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL workshop on biological, translational, and clinical language processing,</booktitle>
<pages>25--32</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="15052" citStr="Pyysalo et al., 2007" startWordPosition="2472" endWordPosition="2475">red of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the Source Sentence Length Corpus % Range Average Variance 0-4 3.26 0.64 1.2 5-20 14.04 17.41 39.2 News 21-40 28.76 29.27 49.4 41-250 49.73 86.73 10.2 All 24.83 152.15 100.0 0-4 2.81 0.60 22.4 5-20 11.64 21.56 48.9 Wiki 21-40 28.02 28.48 24.3 41-250 49.69 77.70 4.5 All 15.33 154.57 100.0 0-4 2.98 0.75 0.9 5-20 14.54 15.14 41.3 Bio 21-40 28.49 29.34 48.0 41-250 49.17 68.34 9.8 All 24.53 139.35 100.0 Table 1: Statistics for sentences in the supertagger training data. Sentences containing more than 250 tokens were not included in our data sets. same post-processing process as Rimell and Cl</context>
</contexts>
<marker>Pyysalo, Ginter, Laippala, Haverinen, Heimonen, Salakoski, 2007</marker>
<rawString>Sampo Pyysalo, Filip Ginter, Veronika Laippala, Katri Haverinen, Juho Heimonen, and Tapio Salakoski. 2007. On the unification of syntactic annotations under the Stanford dependency scheme: a case study on bioinfer and GENIA. In Proceedings of the ACL workshop on biological, translational, and clinical language processing, pages 25–32, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy partof-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the 1996 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="6127" citStr="Ratnaparkhi (1996)" startWordPosition="975" endWordPosition="976"> supertagging component in a lexicalised grammar. Note that the lexical category assigned to with is different in each case, reflecting the fact that the prepositional phrase attaches differently. Either we need a tagging model that can resolve this ambiguity, or both lexical categories must be supplied to the parser which can then attempt to resolve the ambiguity by eventually selecting between them. 2.1 Supertagging Supertaggers typically use standard linear-time tagging algorithms, and only consider words in the local context when assigning a supertag. The C&amp;C supertagger is similar to the Ratnaparkhi (1996) tagger, using features based on words and POS tags in a five-word window surrounding the target word, and defining a local probability distribution over supertags for each word in the sentence, given the previous two supertags. The Viterbi algorithm I ate pizza with cutlery NP (S\NP)INP NP ((S\NP)\(S\NP))1NP NP &gt; &gt; S\NP (S\NP)\(S\NP) I ate pizza with anchovies NP (S\NP)INP NP (NP\NP)INP NP &gt; &lt; &gt; &lt; Figure 1: Two CCG derivations with PP ambiguity. can be used to find the most probable supertag sequence. Alternatively the Forward-Backward algorithm can be used to efficiently sum over all sequenc</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy partof-speech tagger. In Proceedings of the 1996 Conference on Empirical Methods in Natural Language Processing, pages 133–142, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Adapting a lexicalized-grammar parser to contrasting domains.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>475--484</pages>
<location>Honolulu, HI, USA.</location>
<contexts>
<context position="14947" citStr="Rimell and Clark (2008)" startWordPosition="2455" endWordPosition="2458">ntences were manually annotated with CCG lexical categories and POS tags. For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the Source Sentence Length Corpus % Range Average Variance 0-4 3.26 0.64 1.2 5-20 14.04 17.41 39.2 News 21-40 28.76 29.27 49.4 41-250 49.73 86.73 10.2 All 24.83 152.15 100.0 0-4 2.81 0.60 22.4 5-20 11.64 21.56 48.9 Wiki 21-40 28.02 28.48 24.3 41-250 49.69 77.70 4.5 All 15.33 154.57 100.0 0-4 2.98 0.75 0.9 5-20 14.54 15.14 41.3 Bio 21-40 28.49 29.34 48.0 41-250 49.17 68.34 9.8 All 24.53 139.35 100.0 Table 1: Statistics for sentences in the supertagger training data. Sentences containi</context>
</contexts>
<marker>Rimell, Clark, 2008</marker>
<rawString>Laura Rimell and Stephen Clark. 2008. Adapting a lexicalized-grammar parser to contrasting domains. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 475–484, Honolulu, HI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Porting a lexicalized-grammar parser to the biomedical domain.</title>
<date>2009</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>42</volume>
<issue>5</issue>
<contexts>
<context position="15662" citStr="Rimell and Clark (2009)" startWordPosition="2576" endWordPosition="2579">et al., 2007), with the Source Sentence Length Corpus % Range Average Variance 0-4 3.26 0.64 1.2 5-20 14.04 17.41 39.2 News 21-40 28.76 29.27 49.4 41-250 49.73 86.73 10.2 All 24.83 152.15 100.0 0-4 2.81 0.60 22.4 5-20 11.64 21.56 48.9 Wiki 21-40 28.02 28.48 24.3 41-250 49.69 77.70 4.5 All 15.33 154.57 100.0 0-4 2.98 0.75 0.9 5-20 14.54 15.14 41.3 Bio 21-40 28.49 29.34 48.0 41-250 49.17 68.34 9.8 All 24.53 139.35 100.0 Table 1: Statistics for sentences in the supertagger training data. Sentences containing more than 250 tokens were not included in our data sets. same post-processing process as Rimell and Clark (2009) to convert the C&amp;C parser output to Stanford format grammatical relations (de Marneffe et al., 2006). For adaptive training we have used 1,900,618,859 tokens (76,739,723 sentences) from the MEDLINE abstracts tokenised by McIntosh and Curran (2008). These sentences were POS-tagged and parsed twice, once as for the newswire and Wikipedia data, and then again, using the bio-specific models developed by Rimell and Clark (2009). Statistics for the sentences in the training sets are given in Table 1. 4.2 Speed evaluation data For speed evaluation we held out three sets of sentences from each domain</context>
<context position="17390" citStr="Rimell and Clark (2009)" startWordPosition="2857" endWordPosition="2860">d Curran (2007), and the Viterbi decoder to find the highest-scoring derivation. The multipass supertagger-parser interaction was also used. The test data was excluded from training data for the supertagger for all of the newswire and Wikipedia models. For the biomedical models ten348 fold cross validation was used. The accuracy of supertagging is measured by multi-tagging at the first Q level and considering a word correct if the correct tag is amongst any of the assigned tags. For the biomedical parser evaluation we have used the parsing model and grammatical relation conversion script from Rimell and Clark (2009). Our timing measurements are calculated in two ways. Overall times were measured using the C&amp;C parser’s timers. Individual sentence measurements were made using the Intel timing registers, since standard methods are not accurate enough for the short time it takes to parse a single sentence. To check whether changes were statistically significant we applied the test described by Chinchor (1995). This measures the probability that two sets of responses are drawn from the same distribution, where a score below 0.05 is considered significant. Models were trained on an Intel Core2Duo 3GHz with 4GB</context>
<context position="34319" citStr="Rimell and Clark (2009)" startWordPosition="5746" endWordPosition="5749">ined on. In some cases the models in Table 7 are less accurate than those in Table 5. This is because as well as optimising the Q levels we have changed training methods. All of the training methods were tried, but only the method with the best results in newswire is included here, which for F-score when trained on 400,000 sentences was GIS. The accuracy presented so far for the biomedi3Note that the F-scores for Wikipedia and biomedical text are reported to only three significant figures as only 300 and 500 sentences respectively were available for parser evaluation. 352 Train Corpus F-score Rimell and Clark (2009) 81.5 Baseline 80.7 CCGbank + Genia 81.5 + Newswire 81.9 + Wikipedia 82.2 + Biomedical 81.7 + R&amp;C annotated Bio 82.3 Table 10: Performance comparison for models using extra gold standard biomedical data. Models were trained with GIS and 4,000,000 extra sentences, and are tested using a POS-tagger trained on biomedical data. cal model is considerably lower than that reported by Rimell and Clark (2009). This is because no gold standard biomedical training data was used in our experiments. Table 10 shows the results of adding Rimell and Clark’s gold standard biomedical supertag data and using the</context>
</contexts>
<marker>Rimell, Clark, 2009</marker>
<rawString>Laura Rimell and Stephen Clark. 2009. Porting a lexicalized-grammar parser to the biomedical domain. Journal of Biomedical Informatics, 42(5):852–865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
<author>Fel Xia</author>
<author>Aravind K Joshi</author>
</authors>
<title>Some experiments on indicators of parsing complexity for lexicalized grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the COLING Workshop on Efficiency in Large-scale Parsing Systems,</booktitle>
<pages>37--42</pages>
<contexts>
<context position="7212" citStr="Sarkar et al., 2000" startWordPosition="1152" endWordPosition="1155"> find the most probable supertag sequence. Alternatively the Forward-Backward algorithm can be used to efficiently sum over all sequences, giving a probability distribution over supertags for each word which is conditional only on the input sentence. Supertaggers can be made accurate enough for wide coverage parsing using multi-tagging (Chen et al., 1999), in which more than one supertag can be assigned to a word; however, as more supertags are supplied by the supertagger, parsing efficiency decreases (Chen et al., 2002), demonstrating the influence of lexical ambiguity on parsing complexity (Sarkar et al., 2000). Clark and Curran (2004) applied supertagging to CCG, using a flexible multi-tagging approach. The supertagger assigns to a word all lexical categories whose probabilities are within some factor, Q, of the most probable category for that word. When the supertagger is integrated with the C&amp;C parser, several progressively lower Q values are considered. If a sentence is not parsed on one pass then the parser attempts to parse the sentence again with a lower Q value, using a larger set of categories from the supertagger. Since most sentences are parsed at the first level (in which the average num</context>
</contexts>
<marker>Sarkar, Xia, Joshi, 2000</marker>
<rawString>Anoop Sarkar, Fel Xia, and Aravind K. Joshi. 2000. Some experiments on indicators of parsing complexity for lexicalized grammars. In Proceedings of the COLING Workshop on Efficiency in Large-scale Parsing Systems, pages 37–42, Luxembourg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="9396" citStr="Sarkar (2001)" startWordPosition="1503" endWordPosition="1504">upertagger. The first category in each column is correct and the categories used by the parser are marked in bold. The correct category for watch is included here, for expository purposes, but in fact was not provided by the supertagger. 2.2 Semi-supervised training Previous exploration of semi-supervised training in NLP has focused on improving accuracy, often for the case where only small amounts of manually labelled training data are available. One approach is co-training, in which two models with independent views of the data iteratively inform each other by labelling extra training data. Sarkar (2001) applied co-training to LTAG parsing, in which the supertagger and parser provide the two views. Steedman et al. (2003) extended the method to a variety of parser pairs. Another method is to use a re-ranker (Collins and Koo, 2002) on the output of a system to generate new training data. Like co-training, this takes advantage of a different view of the data, but the two views are not independent as the re-ranker is limited to the set of options produced by the system. This method has been used effectively to improve parsing performance on newspaper text (McClosky et al., 2006a), as well as adap</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, pages 1–8, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Combining supertagging and lexicalized tree-adjoining grammar parsing.</title>
<date>2007</date>
<booktitle>In Srinivas Bangalore and Aravind Joshi, editors, Complexity of Lexical Descriptions and its Relevance to Natural Language Processing: A Supertagging Approach.</booktitle>
<publisher>MIT Press,</publisher>
<location>Boston, MA, USA.</location>
<contexts>
<context position="8327" citStr="Sarkar (2007)" startWordPosition="1341" endWordPosition="1342">from the supertagger. Since most sentences are parsed at the first level (in which the average number of supertags assigned to each word is only slightly greater than one), this provides some of the speed benefit of single tagging, but without loss of coverage (Clark and Curran, 2004). Supertagging has since been effectively applied to other formalisms, such as HPSG (Blunsom and Baldwin, 2006; Zhang et al., 2009), and as an information source for tasks such as Statistical Machine Translation (Hassan et al., 2007). The use of parser output for supertagger training has been explored for LTAG by Sarkar (2007). However, the focus of that work was on improving parser and supertagger accuracy rather than speed. &lt; S\NP &lt; S NP\NP NP S\NP S 346 Previously , watch imports were denied such duty-free treatment S/S �N/N N (S[dcl]\NP)/(S[pss]\NP) (S[pss]\NP)/NP NP/NP N/N N N N (S[dcl]\NP)/NP S[pss]\NP (N/N)/(N/N) S[adj]\NP (S[dcl]\NP)/(S[adj]\NP) (S[pss]\NP)/NP N/N (S[pt]\NP)/NP (S[dcl]\NP)/NP Figure 2: An example sentence and the sets of categories assigned by the supertagger. The first category in each column is correct and the categories used by the parser are marked in bold. The correct category for watc</context>
</contexts>
<marker>Sarkar, 2007</marker>
<rawString>Anoop Sarkar. 2007. Combining supertagging and lexicalized tree-adjoining grammar parsing. In Srinivas Bangalore and Aravind Joshi, editors, Complexity of Lexical Descriptions and its Relevance to Natural Language Processing: A Supertagging Approach. MIT Press, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Miles Osborne</author>
<author>Anoop Sarkar</author>
<author>Stephen Clark</author>
<author>Rebecca Hwa</author>
<author>Julia Hockenmaier</author>
<author>Paul Ruhlen</author>
<author>Stephen Baker</author>
<author>Jeremiah Crim</author>
</authors>
<title>Bootstrapping statistical parsers from small datasets.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>331--338</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="9515" citStr="Steedman et al. (2003)" startWordPosition="1522" endWordPosition="1526">old. The correct category for watch is included here, for expository purposes, but in fact was not provided by the supertagger. 2.2 Semi-supervised training Previous exploration of semi-supervised training in NLP has focused on improving accuracy, often for the case where only small amounts of manually labelled training data are available. One approach is co-training, in which two models with independent views of the data iteratively inform each other by labelling extra training data. Sarkar (2001) applied co-training to LTAG parsing, in which the supertagger and parser provide the two views. Steedman et al. (2003) extended the method to a variety of parser pairs. Another method is to use a re-ranker (Collins and Koo, 2002) on the output of a system to generate new training data. Like co-training, this takes advantage of a different view of the data, but the two views are not independent as the re-ranker is limited to the set of options produced by the system. This method has been used effectively to improve parsing performance on newspaper text (McClosky et al., 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al., 2006b). As well as using independent views of data to gen</context>
</contexts>
<marker>Steedman, Osborne, Sarkar, Clark, Hwa, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Stephen Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 331–338, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geertjan van Noord</author>
</authors>
<title>Learning efficient parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>817--825</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>van Noord, 2009</marker>
<rawString>Geertjan van Noord. 2009. Learning efficient parsing. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 817–825. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yao-zhong Zhang</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>HPSG supertagging: A sequence labeling view.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies,</booktitle>
<pages>210--213</pages>
<location>Paris, France.</location>
<contexts>
<context position="8130" citStr="Zhang et al., 2009" startWordPosition="1305" endWordPosition="1308">, several progressively lower Q values are considered. If a sentence is not parsed on one pass then the parser attempts to parse the sentence again with a lower Q value, using a larger set of categories from the supertagger. Since most sentences are parsed at the first level (in which the average number of supertags assigned to each word is only slightly greater than one), this provides some of the speed benefit of single tagging, but without loss of coverage (Clark and Curran, 2004). Supertagging has since been effectively applied to other formalisms, such as HPSG (Blunsom and Baldwin, 2006; Zhang et al., 2009), and as an information source for tasks such as Statistical Machine Translation (Hassan et al., 2007). The use of parser output for supertagger training has been explored for LTAG by Sarkar (2007). However, the focus of that work was on improving parser and supertagger accuracy rather than speed. &lt; S\NP &lt; S NP\NP NP S\NP S 346 Previously , watch imports were denied such duty-free treatment S/S �N/N N (S[dcl]\NP)/(S[pss]\NP) (S[pss]\NP)/NP NP/NP N/N N N N (S[dcl]\NP)/NP S[pss]\NP (N/N)/(N/N) S[adj]\NP (S[dcl]\NP)/(S[adj]\NP) (S[pss]\NP)/NP N/N (S[pt]\NP)/NP (S[dcl]\NP)/NP Figure 2: An example </context>
</contexts>
<marker>Zhang, Matsuzaki, Tsujii, 2009</marker>
<rawString>Yao-zhong Zhang, Takuya Matsuzaki, and Jun’ichi Tsujii. 2009. HPSG supertagging: A sequence labeling view. In Proceedings of the 11th International Conference on Parsing Technologies, pages 210–213, Paris, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>