<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.111240">
<title confidence="0.970261">
SPARSAR: An Expressive Poetry Reader
</title>
<author confidence="0.99701">
Rodolfo Delmonte &amp; Anton Maria Prati
</author>
<affiliation confidence="0.946573">
artment of Language Studies &amp; Department of Computer Scie
Ca’ Foscari University - 30123, Venezia, Italy
</affiliation>
<email confidence="0.994023">
delmont@unive.it
</email>
<sectionHeader confidence="0.993795" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997235">
We present SPARSAR, a system for the auto-
matic analysis of poetry(and text) style which
makes use of NLP tools like tokenizers, sen-
tence splitters, NER (Name Entity Recogni-
tion) tools, and taggers. In addition the system
adds syntactic and semantic structural analysis
and prosodic modeling. We do a dependency
mapping to analyse the verbal complex and de-
termine Discourse Structure. Another impor-
tant component of the system is a phonological
parser to account for OOVWs, in the process
of grapheme to phoneme conversion of the
poem. We also measure the prosody of the
poem by associating mean durational values in
msecs to each syllable from a database of syl-
lable durations; to account for missing sylla-
bles we built a syllable parser with the aim to
evaluate durational values for any possible syl-
lable structure. A fundamental component for
the production of emotions is the one that per-
forms affective and sentiment analysis. This is
done on a line by line basis. Lines associated to
specific emotions are then marked to be pro-
nounced with special care for the final module
of the system, which is reponsible for the pro-
duction of expressive reading by a TTS modu-
le, in our case the one made available by Apple
on their computers. Expressive reading is al-
lowed by the possibility to interact with the
TTS.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981384615385">
We present SPARSAR, a system for poetry (and
text) style analysis by means of parameters de-
rived from deep poem (and text) analysis. We
use our system for deep text understanding called
VENSES(XXX,2005) for that aim. SPAR-
SAR(XXX,2013a) works on top of the output
provided by VENSES and is organized in three
main modules which can be used also to analyse
similarities between couples of poems by the
same or different poet and similarities between
collections of poems by a couple of poets. In ad-
dition to what is usually needed to compute text
level semantic and pragmatic features, poetry
introduces a number of additional layers of
meaning by means of metrical and rhyming de-
vices. For these reasons more computation is
required in order to assess and evaluate the level
of complexity that a poem objectively contains.
We use prosodic durational parameters from a
database of English syllables we produced for a
prosodic speech recognizer (XXX,1990). These
parameters are used to evaluate objective pre-
sumed syllable and feet prosodic distribution at
line level. The sum of all of these data is then
used to create a parameterized version of the po-
em to be read by a TTS, with an appropriate ex-
pressivity. Expressive reading is generated by
combining syntactic, semantic, lexical and pro-
sodic information. It is a well-known fact that
TTS systems are unable to produce utterances
with appropriate prosody(van Santen et
al.,2003)1. Besides the general problems related
to TTS reading normal texts, when a poem is
inputted to the TTS the result is worsened by the
internal rules which compute stanza boundaries
as sentence delimiters. So every time there are
continuations or enjambements from one stanza
to the next the TTS will not be able to see it, and
will produce a long pause. The TTS is also blind
to line boundaries. More importantly, the TTS
reads every sentence with the same tone, thus
contributing an unpleasant repeated overall bor-
ing sense which does not correspond to the con-
tents read. This is why sentiment analysis can be
of help, together with semantic processing at dis-
course level.
As regards affective or emotional reading, then,
the prosody of current TTS systems is neutral,
and generally uses flat intonation contours. Pro-
ducing “expressive” prosody will require mo-
difying rhythm, stress patterns and intonation as
described in section 4(see Kao &amp; Jurafsky,2012).
</bodyText>
<footnote confidence="0.9949828">
1 as he puts it, “The wrong words are emphasized, phrase
boundaries are not appropriately indicated, and there is no
prosodic structure for longer stretches of speech. As a result,
comprehension is difficult and the overall listening expe-
rience is disconcerting...” (ibid.,1657).
</footnote>
<page confidence="0.987037">
73
</page>
<note confidence="0.7766035">
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 73–76,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.996269339622642">
The paper is organized as follows: here below a
subsection contains a short state of the art limited
though to latest publications; section 2 shortly
presents SPARSAR; section 3 is dedicated to
Prosody, Rhyming and Metrical Structure; a
short state of the art of expressive reading is pre-
sented in section 4, which is devoted to TextTo-
Speech and parameters induction from the analy-
sis. Eventually we present an evaluation, a con-
clusion and work for the future.
2 PARSAR - Automatic Analysis of Po-
etic Structure and Rhythm with Syn-
tax, Semantics and Phonology
SPARSAR[8] produces a deep analysis of each
poem at different levels: it works at sentence le-
vel at first, than at line level and finally at stanza
level. The structure of the system is organized as
follows: at first syntactic, semantic and gramma-
tical functions are evaluated. Then the poem is
translated into a phonetic form preserving its vi-
sual structure and its subdivision into lines and
stanzas. Phonetically translated words are asso-
ciated to mean duration values taking into ac-
count position in the word and stress. Taking into
account syntactic and semantic information, we
then proceed to “demote” word stress of depen-
dent or functional words. At the end of the analy-
sis of the poem, the system can measure the fol-
lowing parameters: mean verse length in terms of
msec. and in number of feet. The latter is derived
by a line and stanza representation of metrical
structure. More on this topic below.
Another important component of the analysis of
rhythm is constituted by the algorithm that
measures and evaluates rhyme schemes at stanza
level and then the overall rhyming structure at
poem level. As regards syntax, we build chunks
and dependency structures. To complete our
work, we introduce semantics at two levels. On
the one hand, we isolate verbal complex in order
to verify propositional properties, like presence
of negation, computing factuality from a
crosscheck with modality, aspectuality – that we
derive from our lexica – and tense. We also clas-
sify referring espressions by distinguishing con-
crete from abstract nouns, identifying highly am-
biguous from singleton concepts (from number
of possible meanings from WordNet and other
similar repositories). Eventually, we carry out a
sentiment analysis of every poem, thus contribu-
ting a three-way classification: neutral, negative,
positive that can be used as a powerful tool for
expressive purposes.
</bodyText>
<sectionHeader confidence="0.983381" genericHeader="method">
3 Rhetoric Devices, Metrical and Pro-
</sectionHeader>
<subsectionHeader confidence="0.53822">
sodic Structure
</subsectionHeader>
<bodyText confidence="0.999993844444445">
The second module takes care of rhetorical de-
vices, metrical structure and prosodic structure.
This time the file is read on a line by line level
by simply collecting strings in a sequence and
splitting lines at each newline character. In a
subsequent loop, whenever two newlines charac-
ters are met, a stanza is computed. In order to
compute rhetorical and prosodic structure we
need to transform each word into its phonetic
counterpart, by accessing the transcriptions
available in the CMU dictionary. The Carnegie
Mellon Pronouncing Dictionary is freely avai-
lable online and includes American English pro-
nunciation2. We had available a syllable parser
which was used to build the VESD database of
English syllables (XXX, 1999a) (Venice English
Syllable Database) to be used in the Prosodic
Module of SLIM, a system for prosodic self-
learning activities(XXX,2010), which we use
whenever we have a failure of our pronunciation
dictionary which covers some 170,000 entries.
Remaining problems to be solved are related to
ambiguous homographs like “import” (verb) and
“import” (noun) and are treated on the basis of
their lexical category derived from previous tag-
ging; and Out Of Vocabulary Words (OOVW). If
a word is not found in the dictionary, we try dif-
ferent capitalizations, as well as breaking apart
hyphenated words, and then we check with sim-
ple heuristics, differences in spelling determined
by British vs. American pronunciation. Then we
proceed by morphological decomposition, split-
ting at first the word from its prefix and if that
still does not work, its derivational suffix. As a
last resource, we use an orthographically based
version of the same dictionary to try and match
the longest possible string in coincidence with
our OOVW. Some words we had to reconstruct
are: wayfare, gangrened, krog, copperplate,
splendor, filmy, seraphic, unstarred, shrive, slip-
stream, fossicking, unplotted, corpuscle, thither,
wraiths, etc. In some cases, the problem that
made the system fail was the syllable which was
not available in our database of syllable dura-
tions, VESD3. This problem has been coped with
</bodyText>
<footnote confidence="0.942252285714286">
2 It is available online at
&lt;http://www.speech.cs.cmu.edu/cgi-bin/cmudict/&gt;.
3 In VESD, syllables have been collected from WSJCAM,
the Cambridge version of the continuous speech recognition
corpus produced from the Wall Street Journal, distributed
by the Linguistic Data Consortium (LDC). We worked on a
subset of 4165 sentences, with 70,694 words which consti-
</footnote>
<page confidence="0.997917">
74
</page>
<bodyText confidence="0.999965833333333">
by launching the syllable parser and then compu-
ting durations from the component phonemes, or
from the closest similar syllable available in the
database. We only had to add 12 new syllables
for a set of approximately 500 poems that we
computed to test the system.
</bodyText>
<subsectionHeader confidence="0.9986685">
3.1 Computing Metrical Structure and
Rhyming Scheme
</subsectionHeader>
<bodyText confidence="0.992872735294118">
Any poem can be characterized by its rhythm
which is also revealing of the poet&apos;s peculiar
style. In turn, the poem&apos;s rhythm is based mainly
on two elements: meter, that is distribution of
stressed and unstressed syllables in the verse,
presence of rhyming and other poetic devices
like alliteration, assonance, consonance, en-
jambements, etc. which contribute to poetic form
at stanza level.
We follow Hayward (1991) to mark a poetic
foot by a numerical sequence that is an alterna-
tion of 0/1: “0” for unstressed and “1” for stres-
sed syllables. The sequence of these sings makes
up the foot and depending on number of feet one
can speak of iambic, trochaic, anapestic, dactylic,
etc. poetic style. But then we deepen our analysis
by considering stanzas as structural units in
which rhyming plays an essential role. Secondly
we implement a prosodic acoustic measure to get
a precise definition of rhythm. Syllables are not
just any combination of sounds, and their internal
structure is fundamental to the nature of the poet-
ic rhythm that will ensue. The use of duration has
allowed our system to produce a model of a poet-
ry reader that we implement by speech synthesis.
To this aim we assume that syllable acoustic
identity changes as a function of three parame-
ters:
- internal structure in terms of onset and rhyme
which is characterized by number consonants,
consonant clusters, vowel or diphthong
- position in the word, whether beginning, end or
middle
- primary stress, secondary stress or unstressed
</bodyText>
<sectionHeader confidence="0.975457" genericHeader="method">
4 TTS and Modeling Poetry Reading
</sectionHeader>
<bodyText confidence="0.997208230769231">
The other important part of the work regards us-
ing the previous analyses to produce intelligible,
tute half of the total number of words in the corpus amoun-
ting to 133,080. We ended up with 113,282 syllables and
287,734 phones. The final typology is made up of 44 pho-
nes, 4393 syllable types and 11,712 word types. From word-
level and phoneme-level transcriptions we produced sylla-
bles automatically by means of a syllable parser. The result
was then checked manually.
correct, appropriate and possibly pleasant or
catchy poetry reading by a TextToSpeech sys-
tem. In fact, the intention was more ambitious
and was producing an “expressive” reading of a
poem in the sense also intended by work reported
in Ovesdotter &amp; Sprout(2005), Ovesdotter(2005),
Scherer(2003). In Ovesdotter &amp; Sprout(2005),
the authors present work on fairy tales, intended
to use positive vs negative classification of sen-
tences to produce a better reading. To that aim
they used a machine learning approach, based on
the manual annotation of some 185 children sto-
ries4. They reported accuracy results around 63%
and F-score around 70%, which they explain
may be due to a very low interannotator agree-
ment, and to the fact that the dataset was too
small. In Ovesdotter(2005) the author presents
work on the perception of emotion based again
on fairy tales reading by human readers. The ex-
periment had the goal of checking the validity of
the association of acoustic parameters to emotion
types. Global acoustic features included F0, in-
tensity, speech rate in number of words, feet,
syllables per minute, fluency, i.e. number of
pauses or silences. The results show some con-
tradictory data for ANGRY state, but fully com-
pliant data for HAPPY5. These data must be re-
garded as tendencies and are confirmed by ex-
periments reported also in Scherer(2003) and
Schršder(2001). However, it must be underlined
that all researchers confirm the importance of
semantic content, that is the meaning as a means
for transmitting affective states.
The TTS we are now referring to is the one
freely available under Mac OSX in Apple’s de-
vices. In fact, the output of our system can be
used to record .wav or .mpeg files that can then
be played by any sound player program. The in-
formation made available by the system is suffi-
ciently deep to allow for Mac TTS interactive
program to adapt the text to be read and model it
4 Features used to learn to distinguish “emotional” from
“neutral” sentences, include (ibid., 582): first sentence in the
story; direct speech; thematic story type (animal tale, ordi-
nary folk-tale, jokes and anecdotes); interrogative and ex-
clamative punctuation marks; sentence length in words;
ranges of story progress; percent of semantic words (JJ, N,
V, RB); V count in sentence, excluding participles; positive
and negative words; WordNet emotion words; interjections
and affective words; content BOW: N,V,JJ,RB words by
POS.
5 In particular, “angry” was associated with “decreased F0”
and “decreased speech rate”, but also an increased “paus-
ing”. On the contrary, “happy” showed an “increased F0,
intensity, pausing” but a “decreased speech rate”. “Happy”
is similar to “surprised”, while “angry” is similar to “sad”.
</bodyText>
<page confidence="0.997305">
75
</page>
<bodyText confidence="0.99921543902439">
accurately. We used the internal commands
which can modify sensibly the content of the text
to be read. The voices now available are pleasant
and highly intelligible. We produced a set of
rules that take into account a number of essential
variables and parameter to be introduced in the
file to be read. Parameters that can be modified
include: Duration as Speaking Rate; Intonation
from first word marked to a Reset mark; Silence
introduced as Durational value; Emphasis at
word level increasing Pitch; Volume from first
word marked to a Reset mark, increasing intensi-
ty. We discovered that Apple’s TTS makes mis-
takes when reading some specific words, which
we then had to input to the system in a phonetic
format, using the TUNE modality.
The rules address the following information:
- the title
- the first and last line of the poem
- a word is one of the phonetically spelled out words
- a word is the last word of a sentence and is followed
by an exclamation/interrogative mark
- a word is a syntactic head (either at constituency or
dependency level)
- a word is a quantifier, or marks the beginning of a
quantified expression
- a word is a SUBJect head
- a word marks the end of a line and is (not) followed
by punctuation
- a word is the first word of a line and coincides with
a new stanza and is preceded by punctuation
- a line is part of a sentence which is a frozen or a
formulaic expression with specific pragmatic content
specifically encoded
- a line is part of a sentence that introduces new Top-
ic, a Change, Foreground Relevance as computed by
semantics and discourse relations
- a line is part of a sentence and is dependent in Dis-
course Structure and its Move is Down or Same Level
- a discourse marker indicates the beginning of a sub-
ordinate clause
</bodyText>
<sectionHeader confidence="0.9848325" genericHeader="method">
5 Evaluation, Conclusion and Future
Work
</sectionHeader>
<bodyText confidence="0.966406428571429">
We have done a manual evaluation by analysing
a randomly chosen sample of 50 poems out of
the 500 analysed by the system. The evaluation
has been made by a secondary school teacher of
English literature, expert in poetry6. We asked
the teacher to verify the following four levels of
analysis: 1. phonetic translation; 2. syllable divi-
sion; 3. feet grouping; 4. metrical rhyming struc-
ture. Results show a percentage of error which is
6 I here acknowledge the contribution of XXX and thank her
for the effort.
around 5% as a whole, in the four different levels
of analysis. A first prototype has been presented
in(XXX,2013a), and improvements have been
done since then; but more work is needed to tune
prosodic parameters for expressivity rendering
both at intonational and rhythmic level. The most
complex element to control seems to be varia-
tions at discourse structure which are responsible
for continuation intonational patterns vs. begin-
ning of a new contour.
</bodyText>
<sectionHeader confidence="0.992311" genericHeader="method">
Reference
</sectionHeader>
<reference confidence="0.999291435897436">
XXX. 1999. &amp;quot;Prosodic Modeling for Syllable Structures
from the VESD - Venice English Syllable Database&amp;quot;,
in Atti 9° Convegno GFS-AIA, Venezia, 161-168.
XXX. 2008. &amp;quot;Speech Synthesis for Language Tutoring Sys-
tems&amp;quot;, in V.Melissa Holland &amp; F.Pete Fisher(eds.),
(2008), The Path of Speech Technologies in Computer
Assisted Language Learning, Routledge - Taylor and
Francis Group-, New York, 123-150.
XXX, 2010. &amp;quot;Prosodic tools for language learning&amp;quot;, Inter-
national Journal of Speech Technology. 12(4):161-
184.
XXX, 2013a. SPARSAR: a System for Poetry Automatic
Rhythm and Style AnalyzeR, SLATE 2013, Demon-
stration Track.
XXX. 2005. &amp;quot;VENSES – a Linguistically-Based System for
Semantic Evaluation&amp;quot;, in J. Qui–onero-Candela et
al.(eds.), 2005. Machine Learning Challenges. LNCS,
Springer, Berlin, 344-371.
M. Hayward. 1991. &amp;quot;A connectionist model of poetic me-
ter&amp;quot;. Poetics, 20, 303-317.
Justine Kao and Dan Jurafsky. 2012. &amp;quot;A Computational
Analysis of Style, Affect, and Imagery in Contempo-
rary Poetry&amp;quot;. in Proc. NAACL Workshop on Computa-
tional Linguistics for Literature.
Cecilia Ovesdotter Alm, Richard Sproat, 2005. &amp;quot;Emotional
sequencing and development in fairy tales&amp;quot;, In Procee-
dings of the First International Conference on Affective
Computing and Intelligent Interaction, ACII ’05.
Cecilia Ovesdotter Alm, 2005. &amp;quot;Emotions from text: Ma-
chine learning for text-based emotion prediction&amp;quot;, In
Proceedings of HLT/EMNLP, 347-354.
Jan van Santen, Lois Black, Gilead Cohen, Alexander Kain,
Esther Klabbers,Taniya Mishra, Jacques de Villiers, and
Xiaochuan Niu. 2003. &amp;quot;Applications of Computer Gene-
rated Expressive Speech for Communication Disor-
ders&amp;quot;, in Proc. Eurospeech, Geneva, 1657-1660.
K. R. Scherer. 2003. “Vocal communication of emotions: a
review of research paradigms”, Speech Communication,
40(1-2):227-256.
</reference>
<page confidence="0.991756">
76
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.801437">
<title confidence="0.999421">SPARSAR: An Expressive Poetry Reader</title>
<author confidence="0.999935">Rodolfo Delmonte</author>
<author confidence="0.999935">Anton Maria Prati</author>
<affiliation confidence="0.999573">artment of Language Studies &amp; Department of Computer</affiliation>
<address confidence="0.99874">Ca’ Foscari University - 30123, Venezia, Italy</address>
<email confidence="0.999076">delmont@unive.it</email>
<abstract confidence="0.992470612903226">We present SPARSAR, a system for the automatic analysis of poetry(and text) style which makes use of NLP tools like tokenizers, sentence splitters, NER (Name Entity Recognition) tools, and taggers. In addition the system adds syntactic and semantic structural analysis and prosodic modeling. We do a dependency mapping to analyse the verbal complex and determine Discourse Structure. Another important component of the system is a phonological parser to account for OOVWs, in the process of grapheme to phoneme conversion of the poem. We also measure the prosody of the poem by associating mean durational values in msecs to each syllable from a database of syllable durations; to account for missing syllables we built a syllable parser with the aim to evaluate durational values for any possible syllable structure. A fundamental component for the production of emotions is the one that performs affective and sentiment analysis. This is done on a line by line basis. Lines associated to specific emotions are then marked to be pronounced with special care for the final module of the system, which is reponsible for the production of expressive reading by a TTS module, in our case the one made available by Apple on their computers. Expressive reading is allowed by the possibility to interact with the TTS.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Prosodic Modeling for Syllable Structures from the VESD - Venice English Syllable Database&amp;quot;,</title>
<date>1999</date>
<booktitle>in Atti 9° Convegno GFS-AIA, Venezia,</booktitle>
<pages>161--168</pages>
<marker>1999</marker>
<rawString>XXX. 1999. &amp;quot;Prosodic Modeling for Syllable Structures from the VESD - Venice English Syllable Database&amp;quot;, in Atti 9° Convegno GFS-AIA, Venezia, 161-168.</rawString>
</citation>
<citation valid="true">
<title>Speech Synthesis for Language Tutoring Systems&amp;quot;,</title>
<date>2008</date>
<booktitle>in V.Melissa Holland &amp; F.Pete Fisher(eds.), (2008), The Path of Speech Technologies in Computer Assisted Language Learning, Routledge -</booktitle>
<pages>123--150</pages>
<publisher>Taylor and Francis Group-,</publisher>
<location>New York,</location>
<marker>2008</marker>
<rawString>XXX. 2008. &amp;quot;Speech Synthesis for Language Tutoring Systems&amp;quot;, in V.Melissa Holland &amp; F.Pete Fisher(eds.), (2008), The Path of Speech Technologies in Computer Assisted Language Learning, Routledge - Taylor and Francis Group-, New York, 123-150.</rawString>
</citation>
<citation valid="true">
<title>Prosodic tools for language learning&amp;quot;,</title>
<date>2010</date>
<journal>International Journal of Speech Technology.</journal>
<pages>12--4</pages>
<marker>2010</marker>
<rawString>XXX, 2010. &amp;quot;Prosodic tools for language learning&amp;quot;, International Journal of Speech Technology. 12(4):161-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>2013a</author>
</authors>
<title>SPARSAR: a System for Poetry Automatic Rhythm and Style AnalyzeR,</title>
<date>2013</date>
<institution>Demonstration Track.</institution>
<location>SLATE</location>
<marker>2013a, 2013</marker>
<rawString>XXX, 2013a. SPARSAR: a System for Poetry Automatic Rhythm and Style AnalyzeR, SLATE 2013, Demonstration Track.</rawString>
</citation>
<citation valid="true">
<title>VENSES – a Linguistically-Based System for Semantic Evaluation&amp;quot;,</title>
<date>2005</date>
<booktitle>Machine Learning Challenges. LNCS,</booktitle>
<pages>344--371</pages>
<editor>in J. Qui–onero-Candela et al.(eds.),</editor>
<publisher>Springer,</publisher>
<location>Berlin,</location>
<contexts>
<context position="11983" citStr="(2005)" startWordPosition="1949" endWordPosition="1949">umber of words in the corpus amounting to 133,080. We ended up with 113,282 syllables and 287,734 phones. The final typology is made up of 44 phones, 4393 syllable types and 11,712 word types. From wordlevel and phoneme-level transcriptions we produced syllables automatically by means of a syllable parser. The result was then checked manually. correct, appropriate and possibly pleasant or catchy poetry reading by a TextToSpeech system. In fact, the intention was more ambitious and was producing an “expressive” reading of a poem in the sense also intended by work reported in Ovesdotter &amp; Sprout(2005), Ovesdotter(2005), Scherer(2003). In Ovesdotter &amp; Sprout(2005), the authors present work on fairy tales, intended to use positive vs negative classification of sentences to produce a better reading. To that aim they used a machine learning approach, based on the manual annotation of some 185 children stories4. They reported accuracy results around 63% and F-score around 70%, which they explain may be due to a very low interannotator agreement, and to the fact that the dataset was too small. In Ovesdotter(2005) the author presents work on the perception of emotion based again on fairy tales re</context>
</contexts>
<marker>2005</marker>
<rawString>XXX. 2005. &amp;quot;VENSES – a Linguistically-Based System for Semantic Evaluation&amp;quot;, in J. Qui–onero-Candela et al.(eds.), 2005. Machine Learning Challenges. LNCS, Springer, Berlin, 344-371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hayward</author>
</authors>
<title>A connectionist model of poetic meter&amp;quot;.</title>
<date>1991</date>
<journal>Poetics,</journal>
<volume>20</volume>
<pages>303--317</pages>
<contexts>
<context position="10131" citStr="Hayward (1991)" startWordPosition="1634" endWordPosition="1635">syllable available in the database. We only had to add 12 new syllables for a set of approximately 500 poems that we computed to test the system. 3.1 Computing Metrical Structure and Rhyming Scheme Any poem can be characterized by its rhythm which is also revealing of the poet&apos;s peculiar style. In turn, the poem&apos;s rhythm is based mainly on two elements: meter, that is distribution of stressed and unstressed syllables in the verse, presence of rhyming and other poetic devices like alliteration, assonance, consonance, enjambements, etc. which contribute to poetic form at stanza level. We follow Hayward (1991) to mark a poetic foot by a numerical sequence that is an alternation of 0/1: “0” for unstressed and “1” for stressed syllables. The sequence of these sings makes up the foot and depending on number of feet one can speak of iambic, trochaic, anapestic, dactylic, etc. poetic style. But then we deepen our analysis by considering stanzas as structural units in which rhyming plays an essential role. Secondly we implement a prosodic acoustic measure to get a precise definition of rhythm. Syllables are not just any combination of sounds, and their internal structure is fundamental to the nature of t</context>
</contexts>
<marker>Hayward, 1991</marker>
<rawString>M. Hayward. 1991. &amp;quot;A connectionist model of poetic meter&amp;quot;. Poetics, 20, 303-317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justine Kao</author>
<author>Dan Jurafsky</author>
</authors>
<title>A Computational Analysis of Style, Affect, and Imagery in Contemporary Poetry&amp;quot;.</title>
<date>2012</date>
<booktitle>in Proc. NAACL Workshop on Computational Linguistics for Literature.</booktitle>
<marker>Kao, Jurafsky, 2012</marker>
<rawString>Justine Kao and Dan Jurafsky. 2012. &amp;quot;A Computational Analysis of Style, Affect, and Imagery in Contemporary Poetry&amp;quot;. in Proc. NAACL Workshop on Computational Linguistics for Literature.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecilia Ovesdotter Alm</author>
<author>Richard Sproat</author>
</authors>
<title>Emotional sequencing and development in fairy tales&amp;quot;,</title>
<date>2005</date>
<booktitle>In Proceedings of the First International Conference on Affective Computing and Intelligent Interaction, ACII ’05.</booktitle>
<marker>Alm, Sproat, 2005</marker>
<rawString>Cecilia Ovesdotter Alm, Richard Sproat, 2005. &amp;quot;Emotional sequencing and development in fairy tales&amp;quot;, In Proceedings of the First International Conference on Affective Computing and Intelligent Interaction, ACII ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecilia Ovesdotter Alm</author>
</authors>
<title>Emotions from text: Machine learning for text-based emotion prediction&amp;quot;,</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>347--354</pages>
<marker>Alm, 2005</marker>
<rawString>Cecilia Ovesdotter Alm, 2005. &amp;quot;Emotions from text: Machine learning for text-based emotion prediction&amp;quot;, In Proceedings of HLT/EMNLP, 347-354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan van Santen</author>
<author>Lois Black</author>
<author>Gilead Cohen</author>
<author>Alexander Kain</author>
<author>Esther Klabbers</author>
<author>Taniya Mishra</author>
<author>Jacques de Villiers</author>
<author>Xiaochuan Niu</author>
</authors>
<title>Applications of Computer Generated Expressive Speech for Communication Disorders&amp;quot;, in</title>
<date>2003</date>
<booktitle>Proc. Eurospeech,</booktitle>
<pages>1657--1660</pages>
<location>Geneva,</location>
<marker>van Santen, Black, Cohen, Kain, Klabbers, Mishra, de Villiers, Niu, 2003</marker>
<rawString>Jan van Santen, Lois Black, Gilead Cohen, Alexander Kain, Esther Klabbers,Taniya Mishra, Jacques de Villiers, and Xiaochuan Niu. 2003. &amp;quot;Applications of Computer Generated Expressive Speech for Communication Disorders&amp;quot;, in Proc. Eurospeech, Geneva, 1657-1660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R Scherer</author>
</authors>
<title>Vocal communication of emotions: a review of research paradigms”,</title>
<date>2003</date>
<journal>Speech Communication,</journal>
<pages>40--1</pages>
<marker>Scherer, 2003</marker>
<rawString>K. R. Scherer. 2003. “Vocal communication of emotions: a review of research paradigms”, Speech Communication, 40(1-2):227-256.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>