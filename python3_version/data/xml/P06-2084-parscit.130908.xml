<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.991481">
Combining Association Measures for Collocation Extraction
</title>
<author confidence="0.978355">
Pavel Pecina and Pavel Schlesinger
</author>
<affiliation confidence="0.8401625">
Institute of Formal and Applied Linguistics
Charles University, Prague, Czech Republic
</affiliation>
<email confidence="0.981084">
{pecina,schlesinger}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.982227" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960058823529">
We introduce the possibility of combining
lexical association measures and present
empirical results of several methods em-
ployed in automatic collocation extrac-
tion. First, we present a comprehensive
summary overview of association mea-
sures and their performance on manu-
ally annotated data evaluated by precision-
-recall graphs and mean average precision.
Second, we describe several classification
methods for combining association mea-
sures, followed by their evaluation and
comparison with individual measures. Fi-
nally, we propose a feature selection algo-
rithm significantly reducing the number of
combined measures with only a small per-
formance degradation.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941098039216">
Lexical association measures are mathematical
formulas determining the strength of association
between two or more words based on their occur-
rences and cooccurrences in a text corpus. They
have a wide spectrum of applications in the field
of natural language processing and computational
linguistics such as automatic collocation extrac-
tion (Manning and Schütze, 1999), bilingual word
alignment (Mihalcea and Pedersen, 2003) or de-
pendency parsing. A number of various associa-
tion measures were introduced in the last decades.
An overview of the most widely used techniques
is given e.g. in Manning and Schütze (1999) or
Pearce (2002). Several researchers also attempted
to compare existing methods and suggest differ-
ent evaluation schemes, e.g Kita (1994) and Evert
(2001). A comprehensive study of statistical as-
pects of word cooccurrences can be found in Evert
(2004) or Krenn (2000).
In this paper we present a novel approach to au-
tomatic collocation extraction based on combin-
ing multiple lexical association measures. We also
address the issue of the evaluation of association
measures by precision-recall graphs and mean av-
erage precision scores. Finally, we propose a step-
wise feature selection algorithm that reduces the
number of combined measures needed with re-
spect to performance on held-out data.
The term collocation has both linguistic and
lexicographic character. It has various definitions
but none of them is widely accepted. We adopt
the definition from Choueka (1988) who defines
a collocational expression as “a syntactic and se-
mantic unit whose exact and unambiguous mean-
ing or connotation cannot be derived directly from
the meaning or connotation of its components”.
This notion of collocation is relatively wide and
covers a broad range of lexical phenomena such as
idioms, phrasal verbs, light verb compounds, tech-
nological expressions, proper names, and stock
phrases. Our motivation originates from machine
translation: we want to capture all phenomena that
may require special treatment in translation.
Experiments presented in this paper were per-
formed on Czech data and our attention was re-
stricted to two-word (bigram) collocations – pri-
marily for the limited scalability of some meth-
ods to higher-order n-grams and also for the rea-
son that experiments with longer word expressions
would require processing of much larger corpus to
obtain enough evidence of the observed events.
</bodyText>
<sectionHeader confidence="0.970393" genericHeader="method">
2 Reference data
</sectionHeader>
<bodyText confidence="0.99994325">
The first step in our work was to create a refer-
ence data set. Krenn (2000) suggests that col-
location extraction methods should be evaluated
against a reference set of collocations manually
extracted from the full candidate data from a cor-
pus. To avoid the experiments to be biased by
underlying data preprocessing (part-of-speech tag-
ging, lemmatization, and parsing), we extracted
the reference data from morphologically and syn-
tactically annotated Prague Dependency Treebank
2.0 containing about 1.5 million words annotated
on analytical layer (PDT 2.0, 2006). A corpus of
this size is certainly not sufficient for real-world
applications but we found it adequate for our eval-
uation purposes – a larger corpus would have made
the manual collocation extraction task infeasible.
</bodyText>
<page confidence="0.981473">
651
</page>
<note confidence="0.725055">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 651–658,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999832217391305">
Dependency trees from the corpus were broken
down into dependency bigrams consisting of lem-
mas of the head word and its modifier, their part-
-of-speech pattern, and dependency type. From
87 980 sentences containing 1504 847 words, we
obtained a total of 635 952 different dependency
bigrams types. Only 26 450 of them occur in the
data more than five times. The less frequent bi-
grams do not meet the requirement of sufficient
evidence of observations needed by some meth-
ods used in this work (they assume normal dis-
tribution of observations and become unreliable
when dealing with rare events) and were not in-
cluded in the evaluation. We, however, must
agree with Moore (2004) arguing that these cases
comprise majority of all the data (the Zipfian
phenomenon) and thus should not be excluded
from real-world applications. Finally, we filtered
out all bigrams having such part-of-speech pat-
terns that never form a collocation (conjunction–
preposition, preposition–pronoun, etc.) and ob-
tained a list consisting of 12 232 dependency bi-
grams, further called collocation candidates.
</bodyText>
<subsectionHeader confidence="0.988201">
2.1 Manual annotation
</subsectionHeader>
<bodyText confidence="0.999915833333333">
The list of collocation candidates was manually
processed by three trained linguists in parallel and
independently with the aim of identifying colloca-
tions as defined by Choueka. To simplify and clar-
ify the work they were instructed to select those
bigrams that can be assigned to these categories:
</bodyText>
<table confidence="0.57779">
* idiomatic expressions
- studená válka (cold war)
- visí otazník (question mark is hanging — open question)
* technical terms
ˇredseda vlády (prime minister)
- oˇcitý svˇedek (eye witness)
* support verb constructions
- mít pravdu (to be right)
- uˇcinit rozhodnutí (make decision)
* names of persons, locations, and other entities
- Pražský hrad (Prague Castle)
- ˇCervený kˇríž (Red Cross)
* stock phrases
</table>
<tableCaption confidence="0.377442">
- zásadní problém (major problem)
- konec roku (end of the year)
</tableCaption>
<bodyText confidence="0.99992015">
The first (expected) observation was that the in-
terannotator agreement among all the categories
was rather poor: the Cohen’s n between annota-
tors ranged from 0.29 to 0.49, which demonstrates
that the notion of collocation is very subjective,
domain-specific, and somewhat vague. The reason
that three annotators were used was to get a more
precise and objective idea about what can be con-
sidered a collocation by combining outcomes from
multiple annotators. Only those bigrams that all
three annotators independently recognized as col-
locations (of any type) were considered true collo-
cations. The reference data set contains 2 557 such
bigrams, which is 20.9% of all. n between these
two categories reanged from 0.52 to 0.58.
The data was split into six stratified samples.
Five folds were used for five-fold cross validation
and average performance estimation. The remain-
ing one fold was put aside and used as held-out
data in experiments described in Section 5.
</bodyText>
<sectionHeader confidence="0.968333" genericHeader="method">
3 Association measures
</sectionHeader>
<bodyText confidence="0.994301153846154">
In the context of collocation extraction, lexical as-
sociation measures are formulas determining the
degree of association between collocation com-
ponents. They compute an association score for
each collocation candidate extracted from a cor-
pus. The scores indicate the potential for a can-
didate to be a collocation. They can be used for
ranking (candidates with high scores at the top),
or for classification (by setting a threshold and dis-
carding all bigrams below this threshold).
If some words occur together more often than
by chance, then this may be evidence that they
have a special function that is not simply explained
as a result of their combination (Manning and
Schütze, 1999). This property is known in linguis-
tics as non-compositionality. We think of a cor-
pus as a randomly generated sequence of words
that is viewed as a sequence of word pairs (de-
pendency bigrams in our case). Occurrence fre-
quencies and marginal frequencies are used in sev-
eral association measures that reflect how much
the word cooccurrence is accidental. Such mea-
sures include: estimation of joint and conditional
bigram probabilities (Table 1, 1–3), mutual infor-
mation and derived measures (4–9), statistical tests
of independence (10–14), likelihood measures (15–
16), and various other heuristic association mea-
sures and coefficients (17–55) originating in differ-
ent research fields.
By determining the entropy of the immediate
context of a word sequence (words immediately
preceding or following the bigram), the associa-
tion measures (56–60) rank collocations according
to the assumption that they occur as (syntactic)
units in a (information-theoretically) noisy envi-
ronment (Shimohata et al., 1997). By comparing
empirical contexts of a word sequence and of its
components (open-class words occurring within
- p
</bodyText>
<page confidence="0.976832">
652
</page>
<listItem confidence="0.9114464">
# Name Formula # Name Formula
1. Joint probability P(xy) 47. Gini index max[P(x∗)(P(y|x)2+P(¯y|x)2)−P(∗y)2
?2.Conditional probability P(y|x) 48. Confidence +P(¯x∗)(P(y|¯x)2+P(¯y|¯x)2)−P(∗¯y)2,
3. Reverse conditional prob. P(x|y) 49.Laplace P(∗y)(P(x|y)2+P(¯x|y)2)−P(x∗)2
4. Pointwise mutual inform. log P (xy) 50. Conviction +P(∗¯y)(P(x|¯y)2+P(¯x|¯y)2)−P(¯x∗)2]
5. Mutual dependency (MD) 51.Piatersky-Shapiro max[P(y|x), P(x|y)]
6. Log frequency biased MD 52.Certainityfactor NP (xy)+1 NP (xy)+1
7. Normalized expectation 53. Added value (AV)
8. Mutual expectation 54 Co
?9. Salience 54. Collective
</listItem>
<table confidence="0.84739830882353">
’ 2 ?55.Klosgen
10.Pearsons χ test
11. Fisher’s exact test
12.t test
13.z score
14.Poison significance measure
15.Log likelihood ratio
16. Squared log likelihood ratio
P (x∗)P (∗y)
logs �)p�2
P (x∗ y)
)2
P∗y y)
log +log P(x
P(x)P (∗y) ma[ NP(x*)+2 , NP (∗y)+2 ]
2f(xy) P (x∗)P (∗y) P (¯x∗)P (∗y)1
f( x∗)+f(∗y) max[ P (x¯y) P (¯xy) J
2f P(xy)−P(x∗)P(∗y)
P(xy)
f(x∗)+f(
f(∗ v�
log
P (s()p�∗v�•logf(xy)
(fY - fij) 1-P 1−P
max[P(y|x)−P(∗y), P(x|y)−P(x∗)]
Pi,j fij
f &apos; f (2 &apos; P(x* )P(V)+P (¯x∗)P (∗y) ·
N! f(xv) f(xv&apos;) f(xv) f(2f,) 1−P (x∗)P (∗y)−P (¯x∗)P (∗y)
f(xy)− ˆf(xy)
√f(xy)(1−(f(xy)/N)) 1−P (xy)−P (¯x¯y)
f(xy)−ˆf(xy)
pP(xy) ·AV
√ ˆf(xy)(1−( ˆf(xy)/N)) Context measures: −P w P(w|Cxy) logP(w|Cxy)
ˆf(xy)−f(xy)log ˆf(xy)+logf(xy)! ?56. Context entropy −Pw P(w|Cl xy) logP(w|Cl xy)
logN ?57.Left context entropy −P w P (w|Crxy) logP (w|Crxy)
58. Right context entropy P(x∗) logP(x∗)
59. Left context divergence −P wP(w|Cl xy) logP(w|Clxy)
60. Right context divergence P(∗y) logP(∗y)
61. Cross entropy −PwP(w|Cr xy) logP(w|Cr xy)
62. Reverse cross entropy −PwP(w|Cx) log P(w|Cy)
63. Intersection measure −P wP(w|Cy) log P(w|Cx)
?64. Euclidean norm 2|Cx∩Cy |
65. Cosine norm
? 66.L1 norm
67. Confusion probability
?68. Reverse confusion prob.
?69. Jensen-Shannon diverg.
?70. Cosine of pointwise MI
71. KL divergence
72. Reverse KL divergence
?73. Skew divergence
74. Reverse skew divergence
75. Phrase word coocurrence
76. Word association
Cosine context similarity:
?77.in boolean vector space
78. in tf vector space
79. in tf·idf vector space
Dice context similarity:
80. in boolean vector space
81. in tf vector space
82. in tf·idf vector space
1
−2P fij
i,jfijlog ˆfij
logfij2
−2Pi,j ˆfij
Association coefficients: a
</table>
<figure confidence="0.998597196078431">
17. Russel-Rao
18. Sokal-Michiner
19. Rogers-Tanimoto
20.Hamann
21. Third Sokal-Sneath
22.Jaccard
?23.First Kulczynsky
24. Second Sokal-Sneath
25. Second Kulczynski
?26. Fourth Sokal-Sneath
?27. Odds ratio
28.Yulle’s ω
29. Yulle’s Q
30.Driver-Kroeber
31. Fifth Sokal-Sneath
32. Pearson
33. Baroni-Urbani
?34. Braun-Blanquet
?
35. Simpson
36. Michael
37. Mountford
38.Fager
39. Unigram subtuples
40.U cost
41.S cost
42.R cost
43. T combined cost
44. Phi
45. Kappa
46.J measure
a+b+c+d
a+d
a+b+c+d
a+d
a+2b+2c+d
(a+d)−(b+c)
a+b+c+d
b+c
a+d
a
a+b+c
a
|Cx|+|Cy|
b+c qPw(P(w|Cx)−P(w Cy)) 2
a P w P (w|Cx)P (w|Cy)
a+2(b+c)
1 a a
P w P (w|Cx)2·P w P (w|Cy)2
Pw |P (w|Cx) − P (w|Cy)
(x|Cw)P (w)
2 ( a+b + a+c )
4
(a+b + aac++ d+b + d+c )
abd
√ad−√bc
PwP ((x|C.)P
Pw Cw
√ad+√bc 1
ad−bc (p(w|Cx) ||1 (p(w|Cx)+p(w|Cy)))
2[D2
ad+bc +D(p(w|Cy)||12(p(w|Cx)+p(w|Cy)))]
a Pw MI(w,x)MI(w,y)
√(a+b)(a+c)
ad
√(a+b)(a+c)(d+b)(d+c) √Pw MI(w,x)2·√Pw MI(w,y)2
ad−bc P P (w|Cx) log P (w|Cx)
√(a+b)(a+c)(d+b)(d+c) w P (w|Cy )
a+√ad P (w |Cy)
P(w |Cy) log
Pw P (w|Cx)
D(p(w|Cx)||α(w|Cy)+(1−α)p(w|Cx))
D(p(w|Cy)||αp(w|Cx)+(1−α)p(w|Cy))
1 f(x|Cxy) f(y|Cxy)
a+b+c+ ad
√
a
max(a+b,a+c)
a
min(a+b,a+c)
4(ad−bc)
2 ( f(xy) + f(xy) )
1 f(x |Cy)−f(xy) f(y|Cx)−f(xy)
(a+d)2+(b+c)2
2a
2 ( f (xy) + f (xy) )
2bc+ab+ac
a 1
− 2 max(b, c)
2 (cos(cx,cxy)+cos(cy,cxy))
P xiyi
√(a+b)(a+c)
log ad 111 cz= (zi); cos(cx,cy) = √P xi2·√P yi2
q1 zi =δ(f(wi|Cz))
zi =f(wi|Cz)
zi = f(wi|Cz)· N df(wi)= |{x : wi2Cx}|
bc −3.29 a + b + c + d
log(1+ min(b,c)+a
max(b,c)+a )
log(1+ min(b,c) 2
a+1 )−1
a a
</figure>
<table confidence="0.786415516129032">
log(1+ a+b )·log(1+ a+c ) df(wi);
1
√U ×S×R 2 (dice(cx,cxy)+dice(cy,cxy))
P (xy)−P (x∗)P (∗y) 2 P xiyi
cz=(zi); dice(cx,cy)=
√P (x∗)P (∗y)(1−P (x∗))(1−P (∗y)) P xi2+P yi2
P (xy)+P (¯x¯y)−P (x∗)P (∗y)−P (¯x∗)P (∗¯y) zi =δ(f(wi|Cz))
zi =f(wi|Cz)
z=f(wi|Cz)· N df(wi)=|{x:w|
i i2Cx}
1−P (x∗)P (∗y)−P (¯x∗)P (∗¯y)
P(y|x) P(¯y|x)
max[P(xy)log P (∗y) +P(x¯y)log P (∗¯y) ,
P(xy)log P (x|y) P (¯x|y)
P (x∗) +P(¯xy)log P (¯x∗) ] df(wi);
a=f(xy) b=f(x¯y) f(x∗)
c=f(¯xy) d=f(¯x¯y) f(¯x∗)
f(∗y) f(∗¯y) N
A contingency table contains observed frequencies and marginal frequencies for a bigram
xy; w¯ stands for any word except w; ∗ stands for any word; N is a total number of bi-
grams. The table cells are sometimes referred to as fij. Statistical tests of independence
work with contingency tables of expected frequencies ˆf(xy) = f(x∗)f(∗y)/N.
Cw empirical context of w
empirical context of xy
left immediate context of xy
right immediate context of xy
Cxy
Cl
xy
Cr
xy
</table>
<tableCaption confidence="0.9984485">
Table 1: Lexical association measures used for bigram collocation extraction.
?denotes those selected by the model reduction algorithm discussed in Section 5.
</tableCaption>
<page confidence="0.987341">
653
</page>
<figure confidence="0.9899595">
0.0 0.2 0.4 0.6 0.8 1.0
Recall
</figure>
<figureCaption confidence="0.998642666666667">
Figure 1: Vertical averaging of precision-recall curves. Thin
curves represent individual non-averaged curves obtained by
Pointwise mutual information (4) on five data folds.
</figureCaption>
<bodyText confidence="0.99989825">
a specified context window), the association mea-
sures rank collocations according to the assump-
tion that semantically non-compositional expres-
sions typically occur as (semantic) units in differ-
ent contexts than their components (Zhai, 1997).
Measures (61–74) have information theory back-
ground and measures (75–82) are adopted from the
field of information retrieval.
</bodyText>
<subsectionHeader confidence="0.980675">
3.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.999982866666667">
Collocation extraction can be viewed as classifi-
cation into two categories. By setting a threshold,
any association measure becomes a binary clas-
sifier: bigrams with higher association scores fall
into one class (collocations), the rest into the other
class (non-collocations). Performance of such
classifiers can be measured for example by accu-
racy – fraction of correct predictions. However,
the proportion of the two classes in our case is far
from equal and we want to distinguish classifier
performance between them. In this case, several
authors, e.g. Evert (2001), suggest using precision
–fraction of positive predictions correct and re-
call – fraction of positives correctly predicted. The
higher the scores the better the classification is.
</bodyText>
<subsectionHeader confidence="0.995361">
3.2 Precision-recall curves
</subsectionHeader>
<bodyText confidence="0.999920416666667">
Since choosing a classification threshold depends
primarily on the intended application and there is
no principled way of finding it (Inkpen and Hirst,
2002), we can measure performance of associa-
tion measures by precision–recall scores within
the entire interval of possible threshold values. In
this manner, individual association measures can
be thoroughly compared by their two-dimensional
precision-recall curves visualizing the quality of
ranking without committing to a classification
threshold. The closer the curve stays to the top
and right, the better the ranking procedure is.
</bodyText>
<table confidence="0.893413">
Pointwise mutual information (4)
Pearson’s test (10)
z score (13)
Cosine context similarity in boolean vector space (77)
Unigram subtuple measure (39)
0.0 0.2 0.4 0.6 0.8 1.0
Recall
</table>
<figureCaption confidence="0.997849">
Figure 2: Crossvalidated and averaged precision-recall
curves of selected association measures (numbers in brack-
ets refer to Table 1).
</figureCaption>
<bodyText confidence="0.999814916666667">
Precision-recall curves are very sensitive to data
(see Figure 1). In order to obtain a good esti-
mate of their shapes cross validation and averag-
ing are necessary: all cross-validation folds with
scores for each instance are combined and a single
curve is drawn. Averaging can be done in three
ways: vertical –fixing recall, averaging precision,
horizontal –fixing precision, averaging recall, and
combined –fixing threshold, averaging both preci-
sion and recall (Fawcett, 2003). Vertical averag-
ing, as illustrated in Figure 1, worked reasonably
well in our case and was used in all experiments.
</bodyText>
<subsectionHeader confidence="0.998885">
3.3 Mean average precision
</subsectionHeader>
<bodyText confidence="0.99872572">
Visual comparison of precision-recall curves is
a powerfull evaluation tool in many research fields
(e.g. information retrieval). However, it has a seri-
ous weakness. One can easily compare two curves
that never cross one another. The curve that pre-
dominates another one within the entire interval
of recall seems obviously better. When this is not
the case, the judgment is not so obvious. Also
significance tests on the curves are problematic.
Only well-defined one-dimensional quality mea-
sures can rank evaluated methods by their per-
formance. We adopt such a measure from in-
formation retrieval (Hull, 1993). For each cross-
-validation data fold we define average precision
(AP) as the expected value of precision for all pos-
sible values of recall (assuming uniform distribu-
tion) and mean average precision (MAP) as a mean
of this measure computed for each data fold. Sig-
nificance testing in this case can be realized by
paired t-test or by more appropriate nonparametric
paired Wilcoxon test.
Due to the unreliable precision scores for low
recall and their fast changes for high recall, esti-
mation of AP should be limited only to some nar-
rower recall interval, e.g. (0.1,0.9)
</bodyText>
<figure confidence="0.970471959183674">
Precision
0.2 0.4 0.6 0.8 1.0
Unaveraged precision curve
Averaged precison curve
Average precision
0.2 0.4 0.6 0.8 1.0
654
Mean average precision
0.2 0.3 0.4 0.5 0.6 0.7
38
30
29
22
45
20
48
44
49
41
40
77
76
73
26
72
53
81
51
57
67
18
11
12
79
4
5
6
73
1
2
8
3
6
9
7
4
5
11
18
16
15
82
19
13
80
77
10
17
58
78
12
81
65
61
68
14
25
77
38
30
5
4
29
22
45
20
18
6
76
48
44
73
26
11
72
53
49
41
40
81
12
51
79
57
67
59
76
43
21
33
23
24
28
29
37
31
32
38
44
48
50
54
34
20
45
22
63
27
42
30
39
57
51
60
75
55
47
69
52
49
53
72
26
71
66
74
67
62
79
36
46
56
40
41
35
64
70
</figure>
<figureCaption confidence="0.9980472">
Figure 3: a) Mean average precision of all association measures in descending order. Methods are referred by numbers
from Table 1. The solid points correspond to measures selected by the model reduction algorithm from Section 5. b) Visu-
alization of p-values from the significance tests of difference between each method pair (order is the same for both graphs). The
darker points correspond to p-values greater than α=0.1 and indicate methods with statistically indistinguishable performance
(measured by paired Wilcoxon test on values of average precision obtained from five independent data folds).
</figureCaption>
<subsectionHeader confidence="0.96631">
3.4 Experiments and results
</subsectionHeader>
<bodyText confidence="0.999917021276596">
In the initial experiments, we implemented all 82
association measures from Table 1, processed all
morphologically and syntactically annotated sen-
tences from PDT 2.0, and computed scores of all
the association measures for each dependency bi-
gram in the reference data. For each associa-
tion measure and each of the five evaluation data
folds, we computed precision-recall scores and
drew an averaged precision-recall curve. Curves
of some well-performing methods are depicted in
Figure 2. Next, for each association measure and
each data fold, we estimated scores of average pre-
cision on narrower recall interval (0.1,0.9), com-
puted mean average precision, ranked the asso-
ciation measures according to MAP in descend-
ing order, and result depicted in Figure 3 a). Fi-
nally, we applied a paired Wilcoxon test, detected
measures with statistically indistinguishable per-
formance, and visualized this information in Fig-
ure 3 b).
A baseline system ranking bigrams randomly
operates with average precision of 20.9%. The
best performing method for collocation extrac-
tion measured by mean average precision is co-
sine context similarity in boolean vector space (77)
(MAP 66.49%) followed by other 16 associa-
tion measures with nearly identical performance
(Figure 3 a). They include some popular meth-
ods well-known to perform reliably in this task,
such as pointwise mutual information (4), Pear-
son’s x2 test (10), z score (13), odds ratio (27), or
squared log likelihood ratio (16).
The interesting point to note is that, in terms
of MAP, context similarity measures, e.g. (77),
slightly outperform measures based on simple oc-
curence frequencies, e.g. (39). In a more thorough
comparison by percision-recall curves, we observe
that the former very significantly predominates the
latter in the first half of the recall interval and vice
versa in the second half (Figure 2). This is a case
where the MAP is not a sufficient metric for com-
parison of association measure performance. It is
also worth pointing out that even if two methods
have the same precision-recall curves the actual bi-
gram rank order can be very different. Existence
of such non-correlated (in terms of ranking) mea-
sures will be essential in the following sections.
</bodyText>
<sectionHeader confidence="0.954284" genericHeader="method">
4 Combining association measures
</sectionHeader>
<bodyText confidence="0.9997825">
Each collocation candidate xi can be described by
the feature vector xi = (xi1,... , xi82)T consisting
of 82 association scores from Table 1 and assigned
a label yi E 10, 11 which indicates whether the
bigram is considered to be a collocation (y = 1)
or not (y = 0). We look for a ranker function
f(x) —* R that determines the strength of lexical
association between components of bigram x and
hence has the character of an association measure.
This allows us to compare it with other association
measures by the same means of precision-recall
curves and mean average precision. Further, we
present several classification methods and demon-
strate how they can be employed for ranking, i.e.
what function can be used as a ranker. For refer-
ences see Venables and Ripley (2002).
</bodyText>
<subsectionHeader confidence="0.998333">
4.1 Linear logistic regression
</subsectionHeader>
<bodyText confidence="0.999241333333333">
An additive model for binary response is repre-
sented by a generalized linear model (GLM) in
a form of logistic regression:
</bodyText>
<equation confidence="0.997989">
logit(π) = Q0 + Q1x1 + ... + Qpxp
</equation>
<page confidence="0.996973">
655
</page>
<table confidence="0.995088391304348">
Average precision
0.2 0.4 0.6 0.8 1.0
method AP MAP
R=20 R=50 R=80 R=(0.1,0.9) +
NNet (5 units) 89.56 82.74 70.11 80.81 21.53
NNet (3 units) 89.41 81.99 69.64 79.71 19.88
NNet (2 units) 86.92 81.68 68.33 78.77 18.47
SVM (linear) 85.72 79.49 63.86 75.66 13.79
LDA 84.72 77.18 62.90 75.11 12.96
SVM (quadratic) 84.29 79.54 64.24 74.53 12.09
NNet (1 unit) 77.98 76.83 66.75 73.25 10.17
GLM 82.45 76.26 58.61 71.88 8.11
Cosine similarity (77) 80.94 68.90 50.54 66.49 0.00
Unigram subtuples (39) 74.55 67.49 55.16 65.74 -
Neural network (5 units)
Support vector machine (linear)
Linear discriminant analysis
Neural network (1 unit)
Linear logistic regression
Cosine context similarity in boolean vector space (77)
Unigram subtuple measure (39)
0.0 0.2 0.4 0.6 0.8 1.0
Recall
</table>
<tableCaption confidence="0.99422825">
Table 2: Performance of methods combining all association
measures: average precision (AP) for fixed recall values and
mean average precision (MAP) on the narrower recall interval
with relative improvement in the last column (values in %).
</tableCaption>
<bodyText confidence="0.9999617">
where logit(π) = log(π/(1−π)) is a canonical link
function for odds-ratio and π E (0, 1) is a con-
ditional probability for positive response given
a vector x. The estimation of β0 and O is done
by maximum likelihood method which is solved
by the iteratively reweighted least squares algo-
rithm. The ranker function in this case is defined
as the predicted value F, or equivalently (due to
the monotonicity of logit link function) as the lin-
ear combination �β0 + �OTx.
</bodyText>
<subsectionHeader confidence="0.994747">
4.2 Linear discriminant analysis
</subsectionHeader>
<bodyText confidence="0.8959975">
The basic idea of Fisher’s linear discriminant anal-
ysis (LDA) is to find a one-dimensional projection
defined by a vector c so that for the projected com-
bination cTx the ratio of the between variance B
to the within variance W is maximized:
cTBc
max
�
cTW c
After projection, cTx can be directly used as ranker.
</bodyText>
<subsectionHeader confidence="0.981579">
4.3 Support vector machines
</subsectionHeader>
<bodyText confidence="0.9999784">
For technical reason, let us now change the labels
yiE {-1,+1}. The goal in support vector machines
(SVM) is to estimate a function f(x)=β0+OTx and
find a classifier y(x) = sign(f(x)) which can be
solved through the following convex optimization:
</bodyText>
<equation confidence="0.940763">
[1−yi(β0 + OT xi)]++ λ2||O||2
</equation>
<bodyText confidence="0.99471225">
with λ as a regularization parameter. The hinge
loss function L(y,f(x)) = [1− yf(x)]+ is active
only for positive values (i.e. bad predictions) and
therefore is very suitable for ranking models with
&amp; +�OTx as a ranker function. Setting the regu-
larization parameter λ is crucial for both the es-
timators β0, O and further classification (or rank-
ing). As an alternative to a often inappropriate grid
</bodyText>
<figureCaption confidence="0.998694">
Figure 4: Precision-recall curves of selected methods com-
bining all association measures compared with curves of two
best measures employed individually on the same data sets.
</figureCaption>
<bodyText confidence="0.9996234">
search, Hastie (2004) proposed an effective algo-
rithm which fits the entire SVM regularization path
[β0(λ),O(λ)] and gave us the option to choose the
optimal value of λ. As an objective function we
used total amount of loss on training data.
</bodyText>
<subsectionHeader confidence="0.973655">
4.4 Neural networks
</subsectionHeader>
<bodyText confidence="0.999865333333333">
Assuming the most common model of neural net-
works (NNet) with one hidden layer, the aim is to
find inner weights wjh and outer weights whi for
</bodyText>
<equation confidence="0.893776">
yi=φ0(α0 + Ewhiφh(αh + E wjhxj))
</equation>
<bodyText confidence="0.999985461538461">
where h ranges over units in the hidden layer. Ac-
tivation functions φh and function φ0 are fixed.
Typically, φh is taken to be the logistic function
φh(z) = exp(z)/(1 + exp(z)) and φ0 to be the
indicator function φ0(z) = I(z &gt; A) with A as
a classification threshold. For ranking we simply
set φ0(z) = z. Parameters of neural networks are
estimated by the backpropagation algorithm. The
loss function can be based either on least squares
or maximum likehood. To avoid problems with
convergence of the algorithm we used the former
one. The tuning parameter of a classifier is then
the number of units in the hidden layer.
</bodyText>
<subsectionHeader confidence="0.945141">
4.5 Experiments and results
</subsectionHeader>
<bodyText confidence="0.9998875">
To avoid incommensurability of association mea-
sures in our experiments, we used a common pre-
processing technique for multivariate standardiza-
tion: we centered values of each association mea-
sure towards zero and scaled them to unit variance.
Precision-recall curves of all methods were ob-
tained by vertical averaging in five-fold cross val-
idation on the same reference data as in the ear-
lier experiments. Mean average precision was
computed from average precision values estimated
</bodyText>
<figure confidence="0.723371714285714">
En
i=1
min
p0ep
656
Average precision
0.2 0.4 0.6 0.8 1.0
</figure>
<bodyText confidence="0.999971823529412">
on the recall interval (0.1,0.9). In each cross-
-validation step, four folds were used for training
and one fold for testing.
All methods performed very well in compari-
son with individual measures. The best result was
achieved by a neural network with five units in the
hidden layer with 80.81% MAP, which is 21.53%
relative improvement compared to the best indi-
vidual associaton measure. More complex mod-
els, such as neural networks with more than five
units in the hidden layer and support vector ma-
chines with higher order polynomial kernels, were
highly overfitted on the training data folds and bet-
ter results were achieved by simpler models. De-
tailed results of all experiment are given in Ta-
ble 2 and precision-recall curves of selected meth-
ods depicted in Figure 4.
</bodyText>
<sectionHeader confidence="0.992217" genericHeader="method">
5 Model reduction
</sectionHeader>
<bodyText confidence="0.9999670625">
Combining association measures by any of the
presented methods is reasonable and helps in the
collocation extraction task. However, the combi-
nation models are too complex in number of pre-
dictors used. Some association measures are very
similar (analytically or empirically) and as predic-
tors perhaps even redundant. Such measures have
no use in the models, make their training harder,
and should be excluded. Principal component
analysis applied to the evaluation data showed that
95% of its total variance is explained by only 17
principal components and 99.9% is explained by
42 of them. This gives us the idea that we should
be able to significantly reduce the number of vari-
ables in our models with no (or relativelly small)
degradation in their performance.
</bodyText>
<subsectionHeader confidence="0.989668">
5.1 The algorithm
</subsectionHeader>
<bodyText confidence="0.9999108">
A straightforward, but in our case hardly feasible,
approach is an exhaustive search through the space
of all possible subsets of all association measures.
Another option is a heuristic step-wise algorithm
iteratively removing one variable at a time until
some stopping criterion is met. Such algorithms
are not very robust, they are sensitive to data and
generally not very recommended. However, we
tried to avoid these problems by initializing our
step-wise algorithm by clustering similar variables
and choosing one predictor from each cluster as
a representative of variables with the same contri-
bution to the model. Thus we remove the highly
corelated predictors and continue with the step-
-wise procedure.
</bodyText>
<table confidence="0.974410166666667">
NNet (5 units) with 82 predictors
NNet (5 units) with 42 predictors
NNet (5 units) with 17 predictors
NNet (5 units) with 7 predictors
Cosine context similarity in boolean vector space (77)
Unigram subtuple measure (39)
</table>
<figure confidence="0.508195">
0.0 0.2 0.4 0.6 0.8 1.0
Recall
</figure>
<figureCaption confidence="0.997006333333333">
Figure 5: Precision-recall curves of four NNet models from
the model reduction process with different number of predic-
tors compared with curves of two best individual methods.
</figureCaption>
<bodyText confidence="0.9999728125">
The algorithm starts with the hierarchical clus-
tering of variables in order to group those with
a similar contribution to the model, measured by
the absolute value of Pearson’s correlation coef�-
cient. After 82−d iterations, variables are grouped
into d non-empty clusters and one representative
from each cluster is selected as a predictor into the
initial model. This selection is based on individual
predictor performance on held-out data.
Then, the algorithm continues with d predictors
in the initial model and in each iteration removes
a predictor causing minimal degradation of perfor-
mance measured by MAP on held-out data. The
algorithm stops when the difference becomes sig-
nificant – either statistically (by paired Wilcoxon
test) or practically (set by a human).
</bodyText>
<subsectionHeader confidence="0.984542">
5.2 Experiments and results
</subsectionHeader>
<bodyText confidence="0.999890818181818">
We performed the model reduction experiment on
the neural network with five units in the hidden
layer (the best performing combination method).
The similarity matrix for hierarchical clustering
was computed on the held-out data and parame-
ter d (number of initial predictors) was experimen-
tally set to 60. In each iteration of the algorithm,
we used four data folds (out of the five used in pre-
vious experiments) for fitting the models and the
held-out fold to measure the performance of these
models and to select the variable to be removed.
The new model was cross-validated on the same
five data-folds as in the previous experiments.
Precision-recall curves for some intermediate
models are shown in Figure 5. We can conclude
that we were able to reduce the NNet model to
about 17 predictors without statistically signifi-
cant difference in performance. The correspond-
ing association measures are marked in Table 1
and highlighted in Figure 3a). They include mea-
sures from the entire range of individual mean av-
erage precision values.
</bodyText>
<page confidence="0.997812">
657
</page>
<sectionHeader confidence="0.997963" genericHeader="conclusions">
6 Conclusions and discussion
</sectionHeader>
<bodyText confidence="0.999991020408163">
We created and manually annotated a reference
data set consisting of 12 232 Czech dependency
bigrams. 20.9% of them were agreed to be a col-
location by three annotators. We implemented 82
association measures, employed them for collo-
cation extraction and evaluated them against the
reference data set by averaged precision-recall
curves and mean average precision in five-fold
cross validation. The best result was achieved by
a method measuring cosine context similarity in
boolean vector space with mean average precision
of 66.49%.
We exploit the fact that different subgroups of
collocations have different sensitivity to certain
association measures and showed that combining
these measures aids in collocation extraction. All
investigated methods significantly outperformed
individual association measures. The best results
were achieved by a simple neural network with
five units in the hidden layer. Its mean average
precision was 80.81% which is 21.53% relative
improvement with respect to the best individual
measure. Using more complex neural networks or
a quadratic separator in support vector machines
led to overtraining and did not improve the perfor-
mace on test data.
We proposed a stepwise feature selection algo-
rithm reducing the number of predictors in com-
bination models and tested it with the neural net-
work. We were able to reduce the number of its
variables from 82 to 17 without significant degra-
dation of its performance.
No attempt in our work has been made to select
the “best universal method” for combining associ-
ation measures nor to elicit the “best association
measures” for collocation extraction. These tasks
depend heavily on data, language, and notion of
collocation itself. We demonstrated that combin-
ing association measures is meaningful and im-
proves precission and recall of the extraction pro-
cedure and full performance improvement can be
achieved by a relatively small number of measures
combined.
Preliminary results of our research were already
published in Pecina (2005). In the current work,
we used a new version of the Prague Dependecy
Treebank (PDT 2.0, 2006) and the reference data
was improved by additional manual anotation by
two linguists.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999274">
This work has been supported by the Ministry of
Education of the Czech Republic, projects MSM
0021620838 and LC 536. We would like to thank
our advisor Jan Hajiˇc, our colleagues, and anony-
mous reviewers for their valuable comments.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994807925925926">
Y. Choueka. 1988. Looking for needles in a haystack or lo-
cating interesting collocational expressions in large textual
databases. In Proceedings of the RIAO.
S. Evert and B. Krenn. 2001. Methods for the qualitative
evaluation of lexical association measures. In Proceedings
of the 39th Annual Meeting of the ACL, Toulouse, France.
S. Evert. 2004. The Statistics of Word Cooccurrences: Word
Pairs and Collocations. Ph.D. thesis, Univ. of Stuttgart.
T. Fawcett. 2003. ROC graphs: Notes and practical con-
siderations for data mining researchers. Technical report,
HPL-2003-4. HP Laboratories, Palo Alto, CA.
T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. 2004. The
entire regularization path for the support vector machine.
Journal of Machine Learning Research, 5.
D. Hull. 1993. Using statistical testing in the evaluation of
retrieval experiments. In Proceedings of the 16th annual
international ACM SIGIR conference on Research and de-
velopment in information retrieval, New York, NY.
D. Inkpen and G. Hirst. 2002. Acquiring collocations for
lexical choice between near synonyms. In SIGLEX Work-
shop on Unsupervised Lexical Acquisition, 40th meeting
of the ACL, Philadelphia.
K. Kita, Y. Kato, T. Omoto, and Y. Yano. 1994. A compar-
ative study of automatic extraction of collocations from
corpora: Mutual information vs. cost criteria. Journal of
Natural Language Processing.
B. Krenn. 2000. The Usual Suspects: Data-Oriented Models
for Identification and Representation of Lexical Colloca-
tions. Ph.D. thesis, Saarland University.
C. D. Manning and H. Schtitze. 1999. Foundations of Statis-
tical Natural Language Processing. The MIT Press, Cam-
bridge, Massachusetts.
R. Mihalcea and T. Pedersen. 2003. An evaluation exercise
for word alignment. In Proceedings of HLT-NAACL Work-
shop, Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, Edmonton, Alberta.
R. C. Moore. 2004. On log-likelihood-ratios and the signif-
icance of rare events. In Proceedings of the 2004 Confer-
ence on EMNLP, Barcelona, Spain.
D. Pearce. 2002. A comparative evaluation of collocation ex-
traction techniques. In Third International Conference on
language Resources and Evaluation, Las Palmas, Spain.
P. Pecina. 2005. An extensive empirical study of colloca-
tion extraction methods. In Proceedings of the ACL 2005
Student Research Workshop, Ann Arbor, USA.
S. Shimohata, T. Sugio, and J. Nagata. 1997. Retrieving col-
locations by co-occurrences and word order constraints.
In Proc. of the 35th Meeting ofACL/EACL, Madrid, Spain.
W. N. Venables and B. D. Ripley. 2002. Modern Applied
Statistics with S. 4th ed. Springer Verlag, New York.
C. Zhai. 1997. Exploiting context to identify lexical atoms:
A statistical view of linguistic context. In International
and Interdisciplinary Conf. on Modeling and Using Context.
PDT 2.0. 2006. http://ufal.mff.cuni.cz/pdt2.0/.
</reference>
<page confidence="0.995967">
658
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.679644">
<title confidence="0.999904">Combining Association Measures for Collocation Extraction</title>
<author confidence="0.999863">Pecina Schlesinger</author>
<affiliation confidence="0.882575">Institute of Formal and Applied Linguistics Charles University, Prague, Czech Republic</affiliation>
<email confidence="0.97958">pecina@ufal.mff.cuni.cz</email>
<email confidence="0.97958">schlesinger@ufal.mff.cuni.cz</email>
<abstract confidence="0.994840611111111">We introduce the possibility of combining lexical association measures and present empirical results of several methods employed in automatic collocation extraction. First, we present a comprehensive summary overview of association measures and their performance on manually annotated data evaluated by precision- -recall graphs and mean average precision. Second, we describe several classification methods for combining association measures, followed by their evaluation and comparison with individual measures. Finally, we propose a feature selection algorithm significantly reducing the number of combined measures with only a small performance degradation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Choueka</author>
</authors>
<title>Looking for needles in a haystack or locating interesting collocational expressions in large textual databases.</title>
<date>1988</date>
<booktitle>In Proceedings of the RIAO.</booktitle>
<contexts>
<context position="2389" citStr="Choueka (1988)" startWordPosition="347" endWordPosition="348">Krenn (2000). In this paper we present a novel approach to automatic collocation extraction based on combining multiple lexical association measures. We also address the issue of the evaluation of association measures by precision-recall graphs and mean average precision scores. Finally, we propose a stepwise feature selection algorithm that reduces the number of combined measures needed with respect to performance on held-out data. The term collocation has both linguistic and lexicographic character. It has various definitions but none of them is widely accepted. We adopt the definition from Choueka (1988) who defines a collocational expression as “a syntactic and semantic unit whose exact and unambiguous meaning or connotation cannot be derived directly from the meaning or connotation of its components”. This notion of collocation is relatively wide and covers a broad range of lexical phenomena such as idioms, phrasal verbs, light verb compounds, technological expressions, proper names, and stock phrases. Our motivation originates from machine translation: we want to capture all phenomena that may require special treatment in translation. Experiments presented in this paper were performed on C</context>
</contexts>
<marker>Choueka, 1988</marker>
<rawString>Y. Choueka. 1988. Looking for needles in a haystack or locating interesting collocational expressions in large textual databases. In Proceedings of the RIAO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Evert</author>
<author>B Krenn</author>
</authors>
<title>Methods for the qualitative evaluation of lexical association measures.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the ACL,</booktitle>
<location>Toulouse, France.</location>
<marker>Evert, Krenn, 2001</marker>
<rawString>S. Evert and B. Krenn. 2001. Methods for the qualitative evaluation of lexical association measures. In Proceedings of the 39th Annual Meeting of the ACL, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Evert</author>
</authors>
<title>The Statistics of Word Cooccurrences: Word Pairs and Collocations.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. of Stuttgart.</institution>
<contexts>
<context position="1771" citStr="Evert (2004)" startWordPosition="251" endWordPosition="252">e processing and computational linguistics such as automatic collocation extraction (Manning and Schütze, 1999), bilingual word alignment (Mihalcea and Pedersen, 2003) or dependency parsing. A number of various association measures were introduced in the last decades. An overview of the most widely used techniques is given e.g. in Manning and Schütze (1999) or Pearce (2002). Several researchers also attempted to compare existing methods and suggest different evaluation schemes, e.g Kita (1994) and Evert (2001). A comprehensive study of statistical aspects of word cooccurrences can be found in Evert (2004) or Krenn (2000). In this paper we present a novel approach to automatic collocation extraction based on combining multiple lexical association measures. We also address the issue of the evaluation of association measures by precision-recall graphs and mean average precision scores. Finally, we propose a stepwise feature selection algorithm that reduces the number of combined measures needed with respect to performance on held-out data. The term collocation has both linguistic and lexicographic character. It has various definitions but none of them is widely accepted. We adopt the definition f</context>
</contexts>
<marker>Evert, 2004</marker>
<rawString>S. Evert. 2004. The Statistics of Word Cooccurrences: Word Pairs and Collocations. Ph.D. thesis, Univ. of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fawcett</author>
</authors>
<title>ROC graphs: Notes and practical considerations for data mining researchers.</title>
<date>2003</date>
<tech>Technical report, HPL-2003-4. HP Laboratories,</tech>
<location>Palo Alto, CA.</location>
<contexts>
<context position="16670" citStr="Fawcett, 2003" startWordPosition="2565" endWordPosition="2566">l Figure 2: Crossvalidated and averaged precision-recall curves of selected association measures (numbers in brackets refer to Table 1). Precision-recall curves are very sensitive to data (see Figure 1). In order to obtain a good estimate of their shapes cross validation and averaging are necessary: all cross-validation folds with scores for each instance are combined and a single curve is drawn. Averaging can be done in three ways: vertical –fixing recall, averaging precision, horizontal –fixing precision, averaging recall, and combined –fixing threshold, averaging both precision and recall (Fawcett, 2003). Vertical averaging, as illustrated in Figure 1, worked reasonably well in our case and was used in all experiments. 3.3 Mean average precision Visual comparison of precision-recall curves is a powerfull evaluation tool in many research fields (e.g. information retrieval). However, it has a serious weakness. One can easily compare two curves that never cross one another. The curve that predominates another one within the entire interval of recall seems obviously better. When this is not the case, the judgment is not so obvious. Also significance tests on the curves are problematic. Only well-</context>
</contexts>
<marker>Fawcett, 2003</marker>
<rawString>T. Fawcett. 2003. ROC graphs: Notes and practical considerations for data mining researchers. Technical report, HPL-2003-4. HP Laboratories, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>S Rosset</author>
<author>R Tibshirani</author>
<author>J Zhu</author>
</authors>
<title>The entire regularization path for the support vector machine.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>5</volume>
<marker>Hastie, Rosset, Tibshirani, Zhu, 2004</marker>
<rawString>T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. 2004. The entire regularization path for the support vector machine. Journal of Machine Learning Research, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hull</author>
</authors>
<title>Using statistical testing in the evaluation of retrieval experiments.</title>
<date>1993</date>
<booktitle>In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="17423" citStr="Hull, 1993" startWordPosition="2685" endWordPosition="2686">on Visual comparison of precision-recall curves is a powerfull evaluation tool in many research fields (e.g. information retrieval). However, it has a serious weakness. One can easily compare two curves that never cross one another. The curve that predominates another one within the entire interval of recall seems obviously better. When this is not the case, the judgment is not so obvious. Also significance tests on the curves are problematic. Only well-defined one-dimensional quality measures can rank evaluated methods by their performance. We adopt such a measure from information retrieval (Hull, 1993). For each cross-validation data fold we define average precision (AP) as the expected value of precision for all possible values of recall (assuming uniform distribution) and mean average precision (MAP) as a mean of this measure computed for each data fold. Significance testing in this case can be realized by paired t-test or by more appropriate nonparametric paired Wilcoxon test. Due to the unreliable precision scores for low recall and their fast changes for high recall, estimation of AP should be limited only to some narrower recall interval, e.g. (0.1,0.9) Precision 0.2 0.4 0.6 0.8 1.0 U</context>
</contexts>
<marker>Hull, 1993</marker>
<rawString>D. Hull. 1993. Using statistical testing in the evaluation of retrieval experiments. In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Inkpen</author>
<author>G Hirst</author>
</authors>
<title>Acquiring collocations for lexical choice between near synonyms.</title>
<date>2002</date>
<booktitle>In SIGLEX Workshop on Unsupervised Lexical Acquisition, 40th meeting of the ACL,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="15444" citStr="Inkpen and Hirst, 2002" startWordPosition="2381" endWordPosition="2384">n be measured for example by accuracy – fraction of correct predictions. However, the proportion of the two classes in our case is far from equal and we want to distinguish classifier performance between them. In this case, several authors, e.g. Evert (2001), suggest using precision –fraction of positive predictions correct and recall – fraction of positives correctly predicted. The higher the scores the better the classification is. 3.2 Precision-recall curves Since choosing a classification threshold depends primarily on the intended application and there is no principled way of finding it (Inkpen and Hirst, 2002), we can measure performance of association measures by precision–recall scores within the entire interval of possible threshold values. In this manner, individual association measures can be thoroughly compared by their two-dimensional precision-recall curves visualizing the quality of ranking without committing to a classification threshold. The closer the curve stays to the top and right, the better the ranking procedure is. Pointwise mutual information (4) Pearson’s test (10) z score (13) Cosine context similarity in boolean vector space (77) Unigram subtuple measure (39) 0.0 0.2 0.4 0.6 0</context>
</contexts>
<marker>Inkpen, Hirst, 2002</marker>
<rawString>D. Inkpen and G. Hirst. 2002. Acquiring collocations for lexical choice between near synonyms. In SIGLEX Workshop on Unsupervised Lexical Acquisition, 40th meeting of the ACL, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kita</author>
<author>Y Kato</author>
<author>T Omoto</author>
<author>Y Yano</author>
</authors>
<title>A comparative study of automatic extraction of collocations from corpora: Mutual information vs. cost criteria.</title>
<date>1994</date>
<journal>Journal of Natural Language Processing.</journal>
<marker>Kita, Kato, Omoto, Yano, 1994</marker>
<rawString>K. Kita, Y. Kato, T. Omoto, and Y. Yano. 1994. A comparative study of automatic extraction of collocations from corpora: Mutual information vs. cost criteria. Journal of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Krenn</author>
</authors>
<title>The Usual Suspects: Data-Oriented Models for Identification and Representation of Lexical Collocations.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Saarland University.</institution>
<contexts>
<context position="1787" citStr="Krenn (2000)" startWordPosition="254" endWordPosition="255"> computational linguistics such as automatic collocation extraction (Manning and Schütze, 1999), bilingual word alignment (Mihalcea and Pedersen, 2003) or dependency parsing. A number of various association measures were introduced in the last decades. An overview of the most widely used techniques is given e.g. in Manning and Schütze (1999) or Pearce (2002). Several researchers also attempted to compare existing methods and suggest different evaluation schemes, e.g Kita (1994) and Evert (2001). A comprehensive study of statistical aspects of word cooccurrences can be found in Evert (2004) or Krenn (2000). In this paper we present a novel approach to automatic collocation extraction based on combining multiple lexical association measures. We also address the issue of the evaluation of association measures by precision-recall graphs and mean average precision scores. Finally, we propose a stepwise feature selection algorithm that reduces the number of combined measures needed with respect to performance on held-out data. The term collocation has both linguistic and lexicographic character. It has various definitions but none of them is widely accepted. We adopt the definition from Choueka (198</context>
<context position="3405" citStr="Krenn (2000)" startWordPosition="510" endWordPosition="511">ses. Our motivation originates from machine translation: we want to capture all phenomena that may require special treatment in translation. Experiments presented in this paper were performed on Czech data and our attention was restricted to two-word (bigram) collocations – primarily for the limited scalability of some methods to higher-order n-grams and also for the reason that experiments with longer word expressions would require processing of much larger corpus to obtain enough evidence of the observed events. 2 Reference data The first step in our work was to create a reference data set. Krenn (2000) suggests that collocation extraction methods should be evaluated against a reference set of collocations manually extracted from the full candidate data from a corpus. To avoid the experiments to be biased by underlying data preprocessing (part-of-speech tagging, lemmatization, and parsing), we extracted the reference data from morphologically and syntactically annotated Prague Dependency Treebank 2.0 containing about 1.5 million words annotated on analytical layer (PDT 2.0, 2006). A corpus of this size is certainly not sufficient for real-world applications but we found it adequate for our e</context>
</contexts>
<marker>Krenn, 2000</marker>
<rawString>B. Krenn. 2000. The Usual Suspects: Data-Oriented Models for Identification and Representation of Lexical Collocations. Ph.D. thesis, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schtitze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Manning, Schtitze, 1999</marker>
<rawString>C. D. Manning and H. Schtitze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>T Pedersen</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL Workshop, Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,</booktitle>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="1326" citStr="Mihalcea and Pedersen, 2003" startWordPosition="177" endWordPosition="180">and comparison with individual measures. Finally, we propose a feature selection algorithm significantly reducing the number of combined measures with only a small performance degradation. 1 Introduction Lexical association measures are mathematical formulas determining the strength of association between two or more words based on their occurrences and cooccurrences in a text corpus. They have a wide spectrum of applications in the field of natural language processing and computational linguistics such as automatic collocation extraction (Manning and Schütze, 1999), bilingual word alignment (Mihalcea and Pedersen, 2003) or dependency parsing. A number of various association measures were introduced in the last decades. An overview of the most widely used techniques is given e.g. in Manning and Schütze (1999) or Pearce (2002). Several researchers also attempted to compare existing methods and suggest different evaluation schemes, e.g Kita (1994) and Evert (2001). A comprehensive study of statistical aspects of word cooccurrences can be found in Evert (2004) or Krenn (2000). In this paper we present a novel approach to automatic collocation extraction based on combining multiple lexical association measures. W</context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>R. Mihalcea and T. Pedersen. 2003. An evaluation exercise for word alignment. In Proceedings of HLT-NAACL Workshop, Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
</authors>
<title>On log-likelihood-ratios and the significance of rare events.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on EMNLP,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="4939" citStr="Moore (2004)" startWordPosition="750" endWordPosition="751">cy bigrams consisting of lemmas of the head word and its modifier, their part-of-speech pattern, and dependency type. From 87 980 sentences containing 1504 847 words, we obtained a total of 635 952 different dependency bigrams types. Only 26 450 of them occur in the data more than five times. The less frequent bigrams do not meet the requirement of sufficient evidence of observations needed by some methods used in this work (they assume normal distribution of observations and become unreliable when dealing with rare events) and were not included in the evaluation. We, however, must agree with Moore (2004) arguing that these cases comprise majority of all the data (the Zipfian phenomenon) and thus should not be excluded from real-world applications. Finally, we filtered out all bigrams having such part-of-speech patterns that never form a collocation (conjunction– preposition, preposition–pronoun, etc.) and obtained a list consisting of 12 232 dependency bigrams, further called collocation candidates. 2.1 Manual annotation The list of collocation candidates was manually processed by three trained linguists in parallel and independently with the aim of identifying collocations as defined by Chou</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>R. C. Moore. 2004. On log-likelihood-ratios and the significance of rare events. In Proceedings of the 2004 Conference on EMNLP, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pearce</author>
</authors>
<title>A comparative evaluation of collocation extraction techniques.</title>
<date>2002</date>
<booktitle>In Third International Conference on language Resources and Evaluation,</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="1535" citStr="Pearce (2002)" startWordPosition="215" endWordPosition="216">measures are mathematical formulas determining the strength of association between two or more words based on their occurrences and cooccurrences in a text corpus. They have a wide spectrum of applications in the field of natural language processing and computational linguistics such as automatic collocation extraction (Manning and Schütze, 1999), bilingual word alignment (Mihalcea and Pedersen, 2003) or dependency parsing. A number of various association measures were introduced in the last decades. An overview of the most widely used techniques is given e.g. in Manning and Schütze (1999) or Pearce (2002). Several researchers also attempted to compare existing methods and suggest different evaluation schemes, e.g Kita (1994) and Evert (2001). A comprehensive study of statistical aspects of word cooccurrences can be found in Evert (2004) or Krenn (2000). In this paper we present a novel approach to automatic collocation extraction based on combining multiple lexical association measures. We also address the issue of the evaluation of association measures by precision-recall graphs and mean average precision scores. Finally, we propose a stepwise feature selection algorithm that reduces the numb</context>
</contexts>
<marker>Pearce, 2002</marker>
<rawString>D. Pearce. 2002. A comparative evaluation of collocation extraction techniques. In Third International Conference on language Resources and Evaluation, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pecina</author>
</authors>
<title>An extensive empirical study of collocation extraction methods.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL 2005 Student Research Workshop,</booktitle>
<location>Ann Arbor, USA.</location>
<marker>Pecina, 2005</marker>
<rawString>P. Pecina. 2005. An extensive empirical study of collocation extraction methods. In Proceedings of the ACL 2005 Student Research Workshop, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shimohata</author>
<author>T Sugio</author>
<author>J Nagata</author>
</authors>
<title>Retrieving collocations by co-occurrences and word order constraints.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th Meeting ofACL/EACL,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="8814" citStr="Shimohata et al., 1997" startWordPosition="1356" endWordPosition="1359">imation of joint and conditional bigram probabilities (Table 1, 1–3), mutual information and derived measures (4–9), statistical tests of independence (10–14), likelihood measures (15– 16), and various other heuristic association measures and coefficients (17–55) originating in different research fields. By determining the entropy of the immediate context of a word sequence (words immediately preceding or following the bigram), the association measures (56–60) rank collocations according to the assumption that they occur as (syntactic) units in a (information-theoretically) noisy environment (Shimohata et al., 1997). By comparing empirical contexts of a word sequence and of its components (open-class words occurring within - p 652 # Name Formula # Name Formula 1. Joint probability P(xy) 47. Gini index max[P(x∗)(P(y|x)2+P(¯y|x)2)−P(∗y)2 ?2.Conditional probability P(y|x) 48. Confidence +P(¯x∗)(P(y|¯x)2+P(¯y|¯x)2)−P(∗¯y)2, 3. Reverse conditional prob. P(x|y) 49.Laplace P(∗y)(P(x|y)2+P(¯x|y)2)−P(x∗)2 4. Pointwise mutual inform. log P (xy) 50. Conviction +P(∗¯y)(P(x|¯y)2+P(¯x|¯y)2)−P(¯x∗)2] 5. Mutual dependency (MD) 51.Piatersky-Shapiro max[P(y|x), P(x|y)] 6. Log frequency biased MD 52.Certainityfactor NP (xy</context>
</contexts>
<marker>Shimohata, Sugio, Nagata, 1997</marker>
<rawString>S. Shimohata, T. Sugio, and J. Nagata. 1997. Retrieving collocations by co-occurrences and word order constraints. In Proc. of the 35th Meeting ofACL/EACL, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Venables</author>
<author>B D Ripley</author>
</authors>
<date>2002</date>
<booktitle>Modern Applied Statistics with S. 4th</booktitle>
<editor>ed.</editor>
<publisher>Springer Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="22218" citStr="Venables and Ripley (2002)" startWordPosition="3533" endWordPosition="3536">signed a label yi E 10, 11 which indicates whether the bigram is considered to be a collocation (y = 1) or not (y = 0). We look for a ranker function f(x) —* R that determines the strength of lexical association between components of bigram x and hence has the character of an association measure. This allows us to compare it with other association measures by the same means of precision-recall curves and mean average precision. Further, we present several classification methods and demonstrate how they can be employed for ranking, i.e. what function can be used as a ranker. For references see Venables and Ripley (2002). 4.1 Linear logistic regression An additive model for binary response is represented by a generalized linear model (GLM) in a form of logistic regression: logit(π) = Q0 + Q1x1 + ... + Qpxp 655 Average precision 0.2 0.4 0.6 0.8 1.0 method AP MAP R=20 R=50 R=80 R=(0.1,0.9) + NNet (5 units) 89.56 82.74 70.11 80.81 21.53 NNet (3 units) 89.41 81.99 69.64 79.71 19.88 NNet (2 units) 86.92 81.68 68.33 78.77 18.47 SVM (linear) 85.72 79.49 63.86 75.66 13.79 LDA 84.72 77.18 62.90 75.11 12.96 SVM (quadratic) 84.29 79.54 64.24 74.53 12.09 NNet (1 unit) 77.98 76.83 66.75 73.25 10.17 GLM 82.45 76.26 58.61 7</context>
</contexts>
<marker>Venables, Ripley, 2002</marker>
<rawString>W. N. Venables and B. D. Ripley. 2002. Modern Applied Statistics with S. 4th ed. Springer Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zhai</author>
</authors>
<title>Exploiting context to identify lexical atoms: A statistical view of linguistic context.</title>
<date>1997</date>
<booktitle>In International and Interdisciplinary Conf. on Modeling and Using Context. PDT 2.0.</booktitle>
<note>http://ufal.mff.cuni.cz/pdt2.0/.</note>
<contexts>
<context position="14366" citStr="Zhai, 1997" startWordPosition="2222" endWordPosition="2223">y Table 1: Lexical association measures used for bigram collocation extraction. ?denotes those selected by the model reduction algorithm discussed in Section 5. 653 0.0 0.2 0.4 0.6 0.8 1.0 Recall Figure 1: Vertical averaging of precision-recall curves. Thin curves represent individual non-averaged curves obtained by Pointwise mutual information (4) on five data folds. a specified context window), the association measures rank collocations according to the assumption that semantically non-compositional expressions typically occur as (semantic) units in different contexts than their components (Zhai, 1997). Measures (61–74) have information theory background and measures (75–82) are adopted from the field of information retrieval. 3.1 Evaluation Collocation extraction can be viewed as classification into two categories. By setting a threshold, any association measure becomes a binary classifier: bigrams with higher association scores fall into one class (collocations), the rest into the other class (non-collocations). Performance of such classifiers can be measured for example by accuracy – fraction of correct predictions. However, the proportion of the two classes in our case is far from equal</context>
</contexts>
<marker>Zhai, 1997</marker>
<rawString>C. Zhai. 1997. Exploiting context to identify lexical atoms: A statistical view of linguistic context. In International and Interdisciplinary Conf. on Modeling and Using Context. PDT 2.0. 2006. http://ufal.mff.cuni.cz/pdt2.0/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>