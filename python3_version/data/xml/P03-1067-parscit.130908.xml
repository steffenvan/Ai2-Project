<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000302">
<title confidence="0.7583255">
Using model-theoretic semantic interpretation to guide statistical
parsing and word recognition in a spoken language interface*
</title>
<author confidence="0.993501">
William Schuler
</author>
<affiliation confidence="0.998386">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.966503">
200 S. 33rd Street
Philadelphia, PA 19104
</address>
<email confidence="0.999125">
schuler@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.983194" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999514">
This paper describes an extension of the se-
mantic grammars used in conventional sta-
tistical spoken language interfaces to allow
the probabilities of derived analyses to be
conditioned on the meanings or denotations
of input utterances in the context of an
interface&apos;s underlying application environ-
ment or world model. Since these denota-
tions will be used to guide disambiguation
in interactive applications, they must be ef-
ficiently shared among the many possible
analyses that may be assigned to an input
utterance. This paper therefore presents a
formal restriction on the scope of variables
in a semantic grammar which guarantees
that the denotations of all possible analy-
ses of an input utterance can be calculated
in polynomial time, without undue con-
straints on the expressivity of the derived
semantics. Empirical tests show that this
model-theoretic interpretation yields a sta-
tistically significant improvement on stan-
dard measures of parsing accuracy over a
baseline grammar not conditioned on deno-
tations.
</bodyText>
<sectionHeader confidence="0.997735" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.971312968253969">
The development of speaker-independent mixed-
initiative speech interfaces, in which users not only
answer questions but also ask questions and give
instructions, is currently limited by the perfor-
mance of language models based largely on word co-
occurrences. Even under ideal circumstances, with
large application-specific corpora on which to train,
*The author would like to thank David Chiang, Karin
Kipper, and three anonymous reviewers for particularly
helpful comments on this material. This work was sup-
ported by NSF grant EIA 0224417.
conventional language models are not sufficiently
predictive to correctly analyze a wide variety of in-
puts from a wide variety of speakers, such as might
be encountered in a general-purpose interface for di-
recting robots, office assistants, or other agents with
complex capabilities. Such tasks may involve unla-
beled objects that must be precisely described, and a
wider range of actions than a standard database in-
terface would require (which also must be precisely
described), introducing a great deal of ambiguity into
input processing.
This paper therefore explores the use of a statis-
tical model of language conditioned on the mean-
ings or denotations of input utterances in the context
of an interface&apos;s underlying application environment
or world model. This use of model-theoretic inter-
pretation represents an important extension to the
`semantic grammars&apos; used in existing statistical spo-
ken language interfaces, which rely on co-occurrences
among lexically-determined semantic classes and slot
fillers (Miller et al., 1996), in that the probability
of an analysis is now also conditioned on the exis-
tence of denoted entities and relations in the world
model. The advantage of the interpretation-based
disambiguation advanced here is that the probabil-
ity of generating, for example, the noun phrase `the
lemon next to the safe&apos; can be more reliably esti-
mated from the frequency with which noun phrases
have non-empty denotations — given the fact that
`the lemon next to the safe&apos; does indeed denote some-
thing in the world model — than it can from the rel-
atively sparse co-occurrences of frame labels such as
LEMON and NEXT-TO, or of NEXT-TO and SAFE.
Since there are exponentially many word strings
attributable to any utterance, and an exponential
(Catalan-order) number of possible parse tree anal-
yses attributable to any string of words, this use
of model-theoretic interpretation for disambiguation
must involve some kind of sharing of partial results
between competing analyses if interpretation is to be
performed on large numbers of possible analyses in a
practical interactive application. This paper there-
fore also presents a formal restriction on the scope of
variables in a semantic grammar (without untoward
constraints on the expressivity of the derived seman-
tics) which guarantees that the denotations of all
possible analyses of an input utterance can be calcu-
lated in polynomial time. Empirical tests show that
this use of model-theoretic interpretation in disam-
biguation yields a statistically significant improve-
ment on standard measures of parsing accuracy over
a baseline grammar not conditioned on denotations.
</bodyText>
<sectionHeader confidence="0.989265" genericHeader="method">
2 Model-theoretic interpretation
</sectionHeader>
<bodyText confidence="0.999990482758621">
In order to determine whether a user&apos;s directions de-
note entities and relations that exist in the world
model — and of course, in order to execute those
directions once they are disambiguated — it is nec-
essary to precisely represent the meanings of input
utterances.
Semantic grammars of the sort employed in cur-
rent spoken language interfaces for flight reservation
tasks (Miller et al., 1996; Seneff et al., 1998) asso-
ciate fragments of logical — typically relational alge-
bra — expressions with recursive transition networks
encoding lexicalized rules in a context-free grammar
(the independent probabilities of these rules can then
be estimated from a training corpus and multiplied
together to give a probability for any given analysis).
In flight reservation systems, these associated seman-
tic expressions usually designate entities through a
fixed set of constant symbols used as proper names
(e.g. for cities and numbered flights); but in applica-
tions with unlabeled (perhaps visually-represented)
environments, entities must be described by pred-
icating one or more modifiers over some variable,
narrowing the set of potential referents by specify-
ing colors, spatial locations, etc., until only the de-
sired entity or entities remain. A semantic grammar
for interacting with this kind of unlabeled environ-
ment might contain the following rules, using vari-
ables x1... xn (over entities in the world model) in
the associated logical expressions:
</bodyText>
<equation confidence="0.9624075">
VP VP PP : Ax1... xn. $1(x1... xm)A
$2(x1, xm+1... xn)
VP hold NP : Ax1. Hold(Agent, x1) A $2(x1)
NP a glass : Ax1. Glass(x1)
PP under NP : Ax1x2. Under(x1, x2) A $2(x2)
NP the faucet : Ax1. Faucet(x1)
</equation>
<bodyText confidence="0.99270375">
in which m and n are integers and 0 G m G n. Each
lambda expression Ax1... xn. 0 indicates a function
from a tuple of entities (e1... en� to a truth value
defined by the remainder of the expression 0 (sub-
</bodyText>
<equation confidence="0.9991976">
VP — &gt;VP PP
Ax1::: xn=2: $1(x1::: xm=1) A $2(x1; xm+1::: xn)
{(f1; g1); (f2; g2);::: }
PP —&gt; under NP
lx1x2: U(x1; x2) A $2(x2)
{(f1; g1); (f2; g2);::: }
hold NP —&gt; ... under NP —&gt; ...
{g1; g2; ::: }
Ax1: G(x1)
{f1; f2; ::: }
</equation>
<figure confidence="0.6375955">
1x1: F(x1)
a glass the faucet
</figure>
<figureCaption confidence="0.9472434">
Figure 1: Semantic grammar derivation showing the
associated semantics and denotation of each con-
stituent. Entire rules are shown at each step in the
derivation in order to make the semantic associations
explicit.
</figureCaption>
<bodyText confidence="0.9997704">
stituting e1... en for x1... xn), which denotes a set of
tuples satisfying 0, drawn from En (where E is the
set of entities in the world model).
The pseudo-variables $1, $2, ... in this notation in-
dicate the sites at which the semantic expressions
associated with each rule&apos;s nonterminal symbols are
to compose (the numbers correspond to the relative
positions of the symbols on the right hand side of
each rule, numbered from left to right). Semantic ex-
pressions for complete sentences are then formed by
composing the sub-expressions associated with each
rule at the appropriate sites.1
Figure 1 shows the above rules assembled in a
derivation of the sentence `hold a glass under the
faucet.&apos; The denotation annotated beneath each con-
stituent is simply the set of variable assignments
(for each free variable) that satisfy the constituent&apos;s
semantics. These denotations exactly capture the
meaning (in a given world model) of the assem-
bled semantic expressions dominated by each con-
stituent, regardless of how many sub-expressions are
subsumed by that constituent, and can therefore be
shared among competing analyses in lieu of the se-
mantic expression itself, as a partial result in model-
theoretic interpretation.
</bodyText>
<subsectionHeader confidence="0.995937">
2.1 Variable scope
</subsectionHeader>
<bodyText confidence="0.850046">
Note, however, that the adjunction of the preposi-
tional phrase modifier `under the faucet&apos; adds an-
other free variable (x2) to the semantics of the verb
1This use of pseudo-variables is intended to resemble
that of the unix program `yacc,&apos; which has a similar pur-
pose (associating syntax with semantics in constructing
compilers for programming languages).
</bodyText>
<equation confidence="0.590413">
VP —&gt; hold NP
Ax1: H(A; x1) A $2(x1)
{g1; g2; ::: }
</equation>
<table confidence="0.953659857142857">
m VP —&gt; VP PP VP —&gt; VP
Ax1... xn=1. $I(x1... xm=0) n $2(x1, xm+1... xn) Ax2... xn=1. Qx1. $1(x1... xn)
{()}
VP —&gt; hold NP PP —&gt; under NP m VP —&gt; VP PP
Qx1. H(A, x1) n $2(x1) Ax1. Qx2. U(x1, x2) n $2(x2) Ax1... xn=1. $I(x1... xm=1) n $2(x1, xm+1... xn)
{91, 92, ... }
hold NP —&gt; ... under NP —&gt; ...
</table>
<figureCaption confidence="0.882526571428571">
Figure 2: Derivation with minimal scoping. The VP —&gt; hold NP PP —&gt; PP
variable xl in the semantic expression associated Ax1. H(A,x1) n $2(x1) Ax2... xn. Qx1. $1(x1... xn)
with the prepositional phrase `under the faucet&apos; can- {91, 92, ... } {91, 92, ... }
not be identified with the variable in the verb phrase.
hold NP —&gt; ... PP —&gt; under NP
Ax1x2. U(x2, x1) n $2(x1)
{(f1, 91), (f2, 92), ... }
</figureCaption>
<bodyText confidence="0.999989025641026">
phrase, and therefore another factor of jEj to the car-
dinality of its denotation. Moreover, under this kind
of global scoping, if additional prepositional phrases
are adjoined, they would each contribute yet an-
other free variable, increasing the complexity of the
denotation by an additional factor of jEj, making
the shared interpretation of such structures poten-
tially exponential on the length of the input. This
proliferation of free variables means that the vari-
ables introduced by the noun phrases in an utter-
ance, such as `hold a glass under the faucet,&apos; can-
not all be given global scope, as in Figure 1. On
the other hand, the variables introduced by quanti-
fied noun phrases cannot be bound as soon as the
noun phrases are composed, as in Figure 2, because
these variables may need to be used in modifiers
composed in subsequent (higher) rule applications.
Fortunately, if these non-immediate variable scop-
ing arrangements are expressed structurally, as dom-
inance relationships in the elementary tree struc-
tures of some grammar, then a structural restriction
on this grammar can be enforced that preserves as
many non-immediate scoping arrangements as possi-
ble while still preventing an unbounded proliferation
of free variables.
The correct scoping arrangements (e.g. for the sen-
tence `hold a glass under the faucet,&apos; shown Fig-
ure 3) can be expressed using ordered sets of parse
rules grouped together in such a way as to allow
other structural material to intervene. In this case,
a group would include a rule for composing a verb
and a noun phrase with some associated predicate,
and one or more rules for binding each of the pred-
icate&apos;s variables in a quantifier somewhere above it
(thereby ensuring that these rules always occur to-
gether with the quantifier rules dominating the pred-
icate rule), while still allowing rules adjoining prepo-
sitional phrase modifiers to apply in between them
(so that variables in their associated predicates can
</bodyText>
<equation confidence="0.472476">
under NP —&gt; ...
</equation>
<figureCaption confidence="0.996902">
Figure 3: Derivation with desired scoping.
</figureCaption>
<bodyText confidence="0.9999459">
be bound by the same quantifiers).
These `grouped rules&apos; can be formalized using a
tree-rewriting system whose elementary trees can
subsume several ordered CFG rule applications (or
steps in a context-free derivation), as shown in Fig-
ure 4. Each such elementary tree contains a rule
(node) associated with a logical predicate and rules
(nodes) associated with quantifiers binding each of
the predicate&apos;s variables. These trees are then com-
posed by rewriting operations (dotted lines), which
split them up and either insert them between or iden-
tify them with (if demarcated with dashed lines) the
rules in another elementary tree — in this case, the
elementary tree anchored by the word `under.&apos; These
trees are considered elementary in order to exclude
the possibility of generating derivations that contain
unbound variables or quantifiers over unused vari-
ables, which would have no intuitive meaning. The
composition operations will be presented in further
detail in Section 2.2.
</bodyText>
<subsectionHeader confidence="0.986614">
2.2 Semantic composition as tree-rewriting
</subsectionHeader>
<bodyText confidence="0.999986833333333">
A general class of rewriting systems can be defined
using sets of allowable expansions of some type of
object to incorporate zero or more other instances
of the same type of object, each of which is simi-
larly expandable. Such a system can generate ar-
bitrarily complex structure by recursively expand-
ing or `rewriting&apos; each new object, concluding with a
set of zero-expansions at the frontier. For example,
a context-free grammar may be cast as a rewriting
system whose objects are strings, and whose allow-
able expansions are its grammar productions, each
of which expands or rewrites a certain string as a set
</bodyText>
<table confidence="0.738406">
Q —&gt; a N —&gt; faucet
Ax1. Faucet(x1)
1
PP —&gt; PP
Ax2... xn. Qx1. $1(x1... xn)
PP — &gt;P NP m
Ax1... xn. $1(x1... xn) A $2(x1)
PP —&gt; PP
Ax2... xn. Qx1. $1(x1... xn)
NP — &gt;Q N
$2
2
Figure 4: Complete elementary tree for `under&apos; showing argument insertion sites.
V —&gt; hold NP —&gt; ...
Ax1. Hold(A, x1) . . .
VP —&gt; hold NP m
Ax1... xn. $1(x1... xn) A $2(x1)
m VP —&gt; VP PP
Ax1... xn. $1(x1... xm) A $2(x1, xm+1... xn)
VP —&gt; VP
Ax2... xn. Qx1. $1(x1... xn)
VP —&gt; VP
Ax2... xn. Qx1. $1(x1... xn)
1
VP
1
P —&gt; under
Ax1x2. Under(x2, x1)
NP
</table>
<page confidence="0.475317">
2
</page>
<bodyText confidence="0.9974349">
of zero or more sub-strings arranged around certain
`elementary&apos; strings contributing terminal symbols.
A class of tree-rewriting systems can similarly
be defined as rewriting systems whose objects are
trees, and whose allowable expansions are produc-
tions (similar to context-free productions), each of
which rewrite a tree A as some function f applied
to zero or more sub-trees A1; ::: As; s &gt; 0 arranged
around some `elementary&apos; tree structure defined by
f (Pollard, 1984; Weir, 1988):
</bodyText>
<equation confidence="0.96362">
A—� f(A1;::: As) (1)
</equation>
<bodyText confidence="0.999882727272727">
This elementary tree structure can be used to ex-
press the dominance relationship between a logical
predicate and the quantifiers that bind its variables
(which must be preserved in any meaningful derived
structure); but in order to allow the same instance
of a quantifier to bind variables in more than one
predicate, the rewriting productions of such a se-
mantic tree-rewriting system must allow expanded
subtrees to identify parts of their structure (specifi-
cally, the parts containing quantifiers) with parts of
each other&apos;s structure, and with that of their host
elementary tree.
In particular, a rewriting production in such a sys-
tem would rewrite a tree A as an elementary tree a0
with a set of sub-trees A1; ::: As inserted into it, each
of which is first partitioned into a set of contiguous
components (in order to isolate particular quantifier
nodes and other kinds of sub-structure) using a tree
partition function g at some sequence of split points
Oi1,... #ici), which are node addresses in Ai (the first
of which simply specifies the root).2 The resulting se-
quence of partitioned components of each expanded
</bodyText>
<footnote confidence="0.677521">
2The node addresses encode a path from the root of
</footnote>
<bodyText confidence="0.992323">
sub-tree are then inserted into a0 at a correspond-
ing sequence of insertion site addresses ��i1,... �ici�
defined by the rewriting function f:
</bodyText>
<equation confidence="0.994160333333333">
f(A1;::: As) _
�0���11,... �1c1�; g#11,... #1c1(A1)] :::
���s1,... �scs�; g#s1,... #scs (As)](2)
</equation>
<bodyText confidence="0.998114">
Since each address can only host a single inserted
component, any components from different sub-tree
arguments of f that are assigned to the same inser-
tion site address are constrained to be identical in or-
der for the production to apply. Additionally, some
addresses may be `pre-filled&apos; as part of the elemen-
tary structure defined in f, and therefore may also
be identified with components of sub-tree arguments
of f that are inserted at the same address.
Figure 4 shows the set of insertion sites (designated
with boxed indices) for each argument of an elemen-
tary tree anchored by `under.&apos; The sites labeled 1 ,
associated with the first argument sub-tree (in this
case, the tree anchored by `hold&apos;), indicate that it is
composed by partitioning it into three components,
each dominating or dominated by the others, the low-
est of which is inserted at the terminal node labeled
`VP,&apos; the middle of which is identified with a pre-
filled component (delimited by dashed lines), con-
taining the quantifier node labeled `VP —� VP,&apos; and
the uppermost of which (empty in the figure) is in-
serted at the root, while preserving the relative dom-
inance relationships among the nodes in both trees.
Similarly, sites labeled 2 , associated with the sec-
ond argument sub-tree (for the noun phrase comple-
the tree in which every address g�i specifies the ith child
of the node at the end of path g.
ment to the preposition), indicate that it is composed
by partitioning it into two components — again, one
dominating the other — the lowest of which is inserted
at the terminal node labeled `NP,&apos; and the uppermost
of which is identified with another pre-filled compo-
nent containing the quantifier node labeled `PP —�
PP,&apos; again preserving the relative dominance rela-
tionships among the nodes in both trees.
</bodyText>
<subsectionHeader confidence="0.968136">
2.3 Shared interpretation
</subsectionHeader>
<bodyText confidence="0.9997905">
Recall the problem of unbounded variable prolif-
eration described in Section 2.1. The advantage
of using a tree-rewriting system to model semantic
composition is that such systems allow the appli-
cation of well-studied restrictions to limit their re-
cursive capacity to generate structural descriptions
(in this case, to limit the unbounded overlapping
of quantifier-variable dependencies that can produce
unlimited numbers of free variables at certain steps in
a derivation), without limiting the multi-level struc-
ture of their elementary trees, used here for captur-
ing the well-formedness constraint that a predicate
be dominated by its variables&apos; quantifiers.
One such restriction, based on the regular form
restriction defined for tree adjoining grammars
(Rogers, 1994), prohibits a grammar from allowing
any cycle of elementary trees, each intervening inside
a spine (a path connecting the insertion sites of any
argument) of the next. This restriction is defined
below:
</bodyText>
<construct confidence="0.997101266666667">
Definition 2.1 Let a spine in an elementary tree be
the path of nodes (or object-level rule applications)
connecting all insertion site addresses of the same
argument.
Definition 2.2 A grammar G is in regular form if a
directed acyclic graph (V, E) can be drawn with ver-
tices VH, VA 2 V corresponding to partitioned ele-
mentary trees of G (partitioned as described above),
and directed edges (VH, VA) 2 E C_ V x V from each
vertex VH, corresponding to a partitioned elementary
tree that can host an argument, to each vertex VA,
corresponding to a partitioned elementary tree that
can function as its argument, whose partition inter-
sects its spine at any place other than the top node
in the spine.
</construct>
<bodyText confidence="0.999000064516129">
This restriction ensures that there will be no un-
bounded `pumping&apos; of intervening tree structure in
any derivation, so there will never be an unbounded
amount of unrecognized tree structure to keep track
of at any step in a bottom-up parse, so the number
of possible descriptions of each sub-span of the in-
put will be bounded by some constant. It is called a
`regular form&apos; restriction because it ensures that the
set of root-to-leaf paths in any derived structure will
form a regular language.
A CKY-style parser can now be built that rec-
ognizes each context-free rule in an elementary tree
from the bottom up, storing in order the unrecog-
nized rules that lie above it in the elementary tree
(as well as any remaining rules from any composed
sub-trees) as a kind of promissory note. The fact
that any regular-form grammar has a regular path
set means that only a finite number of states will be
required to keep track of this promised, unrecognized
structure in a bottom-up traversal, so the parser will
have the usual O(n3) complexity (times a constant
equal to the finite number of possible unrecognized
structures).
Moreover, since the parser can recognize any string
derivable by such a grammar, it can create a shared
forest representation of every possible analysis of a
given input by annotating every possible applica-
tion of parse rules that could be used in the deriva-
tion of each constituent (Billot and Lang, 1989).
This polynomial-sized shared forest representation
can then be interpreted determine which constituents
denote entities and relations in the world model, in
order to allow model-theoretic semantic information
to guide disambiguation decisions in parsing.
Finally, the regular form restriction also has the
important effect of ensuring that the number of un-
recognized quantifier nodes at any step in a bottom-
up analysis — and therefore the number of free vari-
ables in any word or phrase constituent of a parse — is
also bounded by some constant, which limits the size
of any constituent&apos;s denotation to a polynomial or-
der of E, the number of entities in the environment.
The interpretation of any shared forest derived by
this kind of regular-form tree-rewriting system can
therefore be calculated in worst-case polynomial time
on E.
A denotation-annotated shared forest for the noun
phrase `the girl with the hat behind the counter&apos; is
shown in Figure 5, using the noun and preposition
trees from Figure 4, with alternative applications of
parse rules represented as circles below each derived
constituent. This shared structure subsumes two
competing analyses: one containing the noun phrase
`the girl with the hat&apos;, denoting the entity g1, and
the other containing the noun phrase `the hat be-
hind the counter&apos;, which does not denote anything
in the world model. Assuming that noun phrases
rarely occur with empty denotations in the training
data, the parse containing the phrase `the girl with
the hat&apos; will be preferred, because there is indeed a
girl with a hat in the world model.
This formalism has similarities with two ex-
</bodyText>
<table confidence="0.709129333333333">
NP ! girl P ! with NP ! hat P ! behind NP ! counter
Ax1: Girl(x1) Ax1x2: With(x2; x1) Ax1: Hat(x1) Ax1x2: Behind(x2; x1) Ax1: Counter(x1)
fg1; g2; g3g fhh1; g1i;hh2; b1ig fh1; h2; h3; h4g fhc1; g1ig fc1; c2g
</table>
<figureCaption confidence="0.982464">
Figure 5: Shared forest for `the girl with the hat behind the counter.&apos;
</figureCaption>
<equation confidence="0.976828090909091">
NP ! NP PP
~x1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn)
; or fg1g
PP ! PP
Ax2::: xn=2: Qx1: $1(x1::: xn)
;
PP ! P NP ~x1::: xn=2: $1(x1::: xn) ^ $2(x1)
NP ! NP PP ;
~x1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn)
fg1g NP ! NP PP
~x1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn)
;
PP ! PP
Ax2::: xn=2: Qx1: $1(x1::: xn)
fg1g
PP ! P NP ~x1::: xn=2: $1(x1::: xn) ^ $2(x1)
fhc1; g1ig
PP ! PP
Ax2::: xn=2: Qx1: $1(x1::: xn)
fg1; b1g
PP ! PNP ~x1::: xn=2: $1(x1::: xn) ^ $2(x1)
fhh1; g1i;hh2; b1ig
</equation>
<bodyText confidence="0.9996704">
tensions of tree-adjoining grammar (Joshi, 1985),
namely multi-component tree adjoining grammar
(Becker et al., 1991) and description tree substitu-
tion grammar (Rambow et al., 1995), and indeed
represents something of a combination of the two:
</bodyText>
<listItem confidence="0.922516363636364">
1. Like description tree substitution grammars,
but unlike multi-component TAGs, it allows
trees to be partitioned into any desired set of
contiguous components during composition,
2. Like multi-component TAGs, but unlike descrip-
tion tree substitution grammars, it allows the
specification of particular insertion sites within
elementary trees, and
3. Unlike both, it allow duplication of structure
(which is used for merging quantifiers from dif-
ferent elementary trees).
</listItem>
<bodyText confidence="0.999887121212121">
The use of lambda calculus functions to define de-
composable meanings for input sentences draws on
traditions of Church (1940) and Montague (1973),
but this approach differs from the Montagovian sys-
tem by introducing explicit limits on computational
complexity (in order to allow tractable disambigua-
tion).
This approach to semantics is very similar to that
described by Shieber (1994), in which syntactic and
semantic expressions are assembled synchronously
using paired tree-adjoining grammars with isomor-
phic derivations, except that in this approach the
derived structures are isomorphic as well, hence the
reduction of synchronous tree pairs to semantically-
annotated syntax trees. This isomorphism restric-
tion on derived trees reduces the number of quantifier
scoping configurations that can be assigned to any
given input (most of which are unlikely to be used
in a practical application), but its relative parsimony
allows syntactically ambiguous inputs to be seman-
tically interpreted in a shared forest representation
in worst-case polynomial time. The interleaving of
semantic evaluation and parsing for the purpose of
disambiguation also has much in common with that
of Dowding et al. (1994), except that in this case,
constituents are not only semantically type-checked,
but are also fully interpreted each time they are pro-
posed. There are also commonalities between the un-
derspecified semantic representation of structurally-
ambiguous elementary tree constituents in a shared
forest and the underspecified semantic representa-
tion of (e.g. quantifier) scope ambiguity described
by Reyle (1993).3
</bodyText>
<sectionHeader confidence="0.997141" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999984256410256">
The contribution of this model-theoretic semantic in-
formation toward disambiguation was evaluated on
a set of directions to animated agents collected in a
controlled but spatially complex 3-D simulated en-
vironment (of children running a lemonade stand).
In order to avoid priming them towards particu-
lar linguistic constructions, subjects were shown un-
narrated animations of computer-simulated agents
performing different tasks in this environment (pick-
ing fruit, operating a juicer, and exchanging lemon-
ade for money), which were described only as the `de-
sired behavior&apos; of each agent. The subjects were then
asked to direct the agents, using their own words, to
perform the desired behaviors as shown.
340 utterances were collected and annotated with
brackets and elementary tree node addresses as de-
scribed in Section 2.2, for use as training data and
as gold standard data in testing. Some sample direc-
tions are shown in Figure 6. Most elementary trees
were extracted, with some simplifications for parsing
efficiency, from an existing broad-coverage grammar
resource (XTAG Research Group, 1998), but some
elementary trees for multi-word expressions had to
be created anew. In all, a complete annotation of this
corpus required a grammar of 68 elementary trees
and a lexicon of 288 lexicalizations (that is, words
or sets of words with indivisible semantics, forming
the anchors of a given elementary tree). Each lex-
icalization was then assigned a semantic expression
describing the intended geometric relation or class of
objects in the simulated 3-D environment.4
The interface was tested on the first 100 collected
utterances, and the parsing model was trained on
the remaining utterances. The presence or absence
of a denotation of each constituent was added to the
label of each constituent in the denotation-sensitive
parsing model (for example, statistics were collected
for the frequency of `NP:— —� NP:+ PP:+&apos; events,
meaning a noun phrase that does not denote any-
</bodyText>
<footnote confidence="0.958520375">
3Denotation of competing applications of parse rules
can be unioned (though this effectively treats ambiguity
as a form of disjunction), or stored separately to some
finitie beam (though some globally preferable but locally
dispreferred structures would be lost).
4Here it was assumed that the intention of the user
was to direct the agent to perform the actions shown in
the `desired behavior&apos; animation.
</footnote>
<bodyText confidence="0.890942166666667">
Walk towards the tree where you see a yellow lemon
on the ground.
Pick up the lemon.
Place the lemon in the pool.
Take the dollar bill from the person in front of you.
Walk to the left towards the big black cube.
</bodyText>
<figureCaption confidence="0.936685">
Figure 6: Sample utterances from collected corpus.
</figureCaption>
<bodyText confidence="0.998742297297297">
thing in the environment expands to a noun phrase
and a prepositional phrase that do have a denota-
tion in the environment), whereas the baseline sys-
tem used a parsing model conditioned on only con-
stituent labels (for example, `NP —� NP PP&apos; events).
The entire word lattice output of the speech recog-
nizer was fed directly into the parser, so as to al-
low the model-theoretic semantic information to be
brought to bear on word recognition ambiguity as
well as on structural ambiguity in parsing.
Since any derivation of elementary trees uniquely
defines a semantic expression at each node, the task
of evaluating this kind of semantic analysis is reduced
to the familiar task of evaluating a the accuracy of
a labeled bracketing (labeled with elementary tree
names and node addresses). Here, the standard mea-
sures of labeled precision and recall are used. Note
that there may be multiple possible bracketings for
each gold standard tree in a given word lattice that
differ only in the start and end frames of the com-
ponent words. Since neither the baseline nor test
parsing models are sensitive to the start and end
frames of the component words, the gold standard
bracketing is simply assumed to use the most likely
frame segmentation in the word lattice that yields
the correct word sequence.
The results of the experiment are summarized
below. The environment-based model shows a
statistically significant (p&lt;.05) improvement of 3
points in labeled recall, a 12% reduction in error.
Most of the improvement can be attributed to the
denotation-sensitive parser dispreferring noun phrase
constituents with mis-attached modifiers, which do
not denote anything in the world model.
Model LR LP
baseline model 82% 78%
baseline + denotation bit 85% 81%
</bodyText>
<sectionHeader confidence="0.997959" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999394">
This paper has described an extension of the seman-
tic grammars used in conventional spoken language
interfaces to allow the probabilities of derived anal-
yses to be conditioned on the results of a model-
theoretic interpretation. In particular, a formal re-
striction was presented on the scope of variables in a
semantic grammar which guarantees that the deno-
tations of all possible analyses of an input utterance
can be calculated in polynomial time, without un-
due constraints on the expressivity of the derived
semantics. Empirical tests show that this model-
theoretic interpretation yields a statistically signif-
icant improvement on standard measures of parsing
accuracy over a baseline grammar not conditioned
on denotations.
</bodyText>
<sectionHeader confidence="0.998527" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998483470588236">
Tilman Becker, Aravind Joshi, and Owen Rambow.
1991. Long distance scrambling and tree adjoining
grammars. In Fifth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL&apos;91), pages 21{26.
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 2 �h Annual Meeting of the Association
for Computational Linguistics (ACL &apos;89), pages
143{151.
Alonzo Church. 1940. A formulation of the sim-
ple theory of types. Journal of Symbolic Logic,
5(2):56-68.
John Dowding, Robert Moore, Francois Andery, and
Douglas Moran. 1994. Interleaving syntax and se-
mantics in an efficient bottom-up parser. In Pro-
ceedings of the 32nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL&apos;94).
Aravind K. Joshi. 1985. How much context sensi-
tivity is necessary for characterizing structural de-
scriptions: Tree adjoining grammars. In L. Kart-
tunen D. Dowty and A. Zwicky, editors, Natu-
ral language parsing: Psychological, computational
and theoretical perspectives, pages 206{250. Cam-
bridge University Press, Cambridge, U.K.
Scott Miller, David Stallard, Robert Bobrow, and
Richard Schwartz. 1996. A fully statistical ap-
proach to natural language interfaces. In Pro-
ceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL&apos;96),
pages 55{61.
Richard Montague. 1973. The proper treatment
of quantification in ordinary English. In J. Hin-
tikka, J.M.E. Moravcsik, and P. Suppes, editors,
Approaches to Natural Langauge, pages 221{242.
D. Riedel, Dordrecht. Reprinted in R. H. Thoma-
son ed., Formal Philosophy, Yale University Press,
1994.
Carl Pollard. 1984. Generalized phrase structure
grammars, head grammars and natural langauge.
Ph.D. thesis, Stanford University.
Owen Rambow, David Weir, and K. Vijay-Shanker.
1995. D-tree grammars. In Proceedings of the 33rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL &apos;95).
Uwe Reyle. 1993. Dealing with ambiguities by un-
derspecification: Construction, representation and
deduction. Journal of Semantics, 10:123{179.
James Rogers. 1994. Capturing CFLs with tree ad-
joining grammars. In Proceedings of the 32nd An-
nual Meeting of the Association for Computational
Linguistics (ACL &apos;94).
Stephanie Seneff, Ed Hurley, Raymond Lau, Chris-
tine Pao, Philipp Schmid, and Victor Zue. 1998.
GALAXY-II: a reference architecture for conversa-
tional system development. In Proceedings of the
5th International Conference on Spoken Language
Processing (ICSLP &apos;98), Sydney, Australia.
Stuart M. Shieber. 1994. Restricting the weak-
generative capability of synchronous tree adjoining
grammars. Computational Intelligence, 10(4).
David Weir. 1988. Characterizing mildly context-
sensitive grammar formalisms. Ph.D. thesis, De-
partment of Computer and Information Science,
University of Pennsylvania.
XTAG Research Group. 1998. A lexicalized tree
adjoining grammar for english. Technical report,
IRCS, University of Pennsylvania.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.816476">
<title confidence="0.9665145">Using model-theoretic semantic interpretation to guide statistical and word recognition in a spoken language</title>
<author confidence="0.999978">William Schuler</author>
<affiliation confidence="0.9996325">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.9991865">200 S. 33rd Street Philadelphia, PA 19104</address>
<email confidence="0.999474">schuler@linc.cis.upenn.edu</email>
<abstract confidence="0.995277846153846">This paper describes an extension of the semantic grammars used in conventional statistical spoken language interfaces to allow the probabilities of derived analyses to be on the meanings or of input utterances in the context of an interface&apos;s underlying application environor Since these denotations will be used to guide disambiguation in interactive applications, they must be efficiently shared among the many possible analyses that may be assigned to an input utterance. This paper therefore presents a formal restriction on the scope of variables in a semantic grammar which guarantees that the denotations of all possible analyses of an input utterance can be calculated in polynomial time, without undue constraints on the expressivity of the derived semantics. Empirical tests show that this model-theoretic interpretation yields a statistically significant improvement on standard measures of parsing accuracy over a baseline grammar not conditioned on denotations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tilman Becker</author>
<author>Aravind Joshi</author>
<author>Owen Rambow</author>
</authors>
<title>Long distance scrambling and tree adjoining grammars.</title>
<date>1991</date>
<booktitle>In Fifth Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;91),</booktitle>
<pages>21--26</pages>
<contexts>
<context position="22736" citStr="Becker et al., 1991" startWordPosition="3766" endWordPosition="3769">ter.&apos; NP ! NP PP ~x1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn) ; or fg1g PP ! PP Ax2::: xn=2: Qx1: $1(x1::: xn) ; PP ! P NP ~x1::: xn=2: $1(x1::: xn) ^ $2(x1) NP ! NP PP ; ~x1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn) fg1g NP ! NP PP ~x1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn) ; PP ! PP Ax2::: xn=2: Qx1: $1(x1::: xn) fg1g PP ! P NP ~x1::: xn=2: $1(x1::: xn) ^ $2(x1) fhc1; g1ig PP ! PP Ax2::: xn=2: Qx1: $1(x1::: xn) fg1; b1g PP ! PNP ~x1::: xn=2: $1(x1::: xn) ^ $2(x1) fhh1; g1i;hh2; b1ig tensions of tree-adjoining grammar (Joshi, 1985), namely multi-component tree adjoining grammar (Becker et al., 1991) and description tree substitution grammar (Rambow et al., 1995), and indeed represents something of a combination of the two: 1. Like description tree substitution grammars, but unlike multi-component TAGs, it allows trees to be partitioned into any desired set of contiguous components during composition, 2. Like multi-component TAGs, but unlike description tree substitution grammars, it allows the specification of particular insertion sites within elementary trees, and 3. Unlike both, it allow duplication of structure (which is used for merging quantifiers from different elementary trees). T</context>
</contexts>
<marker>Becker, Joshi, Rambow, 1991</marker>
<rawString>Tilman Becker, Aravind Joshi, and Owen Rambow. 1991. Long distance scrambling and tree adjoining grammars. In Fifth Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;91), pages 21{26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvie Billot</author>
<author>Bernard Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of the 2 �h Annual Meeting of the Association for Computational Linguistics (ACL &apos;89),</booktitle>
<pages>143--151</pages>
<contexts>
<context position="20211" citStr="Billot and Lang, 1989" startWordPosition="3329" endWordPosition="3332">gular-form grammar has a regular path set means that only a finite number of states will be required to keep track of this promised, unrecognized structure in a bottom-up traversal, so the parser will have the usual O(n3) complexity (times a constant equal to the finite number of possible unrecognized structures). Moreover, since the parser can recognize any string derivable by such a grammar, it can create a shared forest representation of every possible analysis of a given input by annotating every possible application of parse rules that could be used in the derivation of each constituent (Billot and Lang, 1989). This polynomial-sized shared forest representation can then be interpreted determine which constituents denote entities and relations in the world model, in order to allow model-theoretic semantic information to guide disambiguation decisions in parsing. Finally, the regular form restriction also has the important effect of ensuring that the number of unrecognized quantifier nodes at any step in a bottomup analysis — and therefore the number of free variables in any word or phrase constituent of a parse — is also bounded by some constant, which limits the size of any constituent&apos;s denotation</context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>Sylvie Billot and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the 2 �h Annual Meeting of the Association for Computational Linguistics (ACL &apos;89), pages 143{151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alonzo Church</author>
</authors>
<title>A formulation of the simple theory of types.</title>
<date>1940</date>
<journal>Journal of Symbolic Logic,</journal>
<pages>5--2</pages>
<contexts>
<context position="23460" citStr="Church (1940)" startWordPosition="3875" endWordPosition="3876">ion of the two: 1. Like description tree substitution grammars, but unlike multi-component TAGs, it allows trees to be partitioned into any desired set of contiguous components during composition, 2. Like multi-component TAGs, but unlike description tree substitution grammars, it allows the specification of particular insertion sites within elementary trees, and 3. Unlike both, it allow duplication of structure (which is used for merging quantifiers from different elementary trees). The use of lambda calculus functions to define decomposable meanings for input sentences draws on traditions of Church (1940) and Montague (1973), but this approach differs from the Montagovian system by introducing explicit limits on computational complexity (in order to allow tractable disambiguation). This approach to semantics is very similar to that described by Shieber (1994), in which syntactic and semantic expressions are assembled synchronously using paired tree-adjoining grammars with isomorphic derivations, except that in this approach the derived structures are isomorphic as well, hence the reduction of synchronous tree pairs to semanticallyannotated syntax trees. This isomorphism restriction on derived </context>
</contexts>
<marker>Church, 1940</marker>
<rawString>Alonzo Church. 1940. A formulation of the simple theory of types. Journal of Symbolic Logic, 5(2):56-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Dowding</author>
<author>Robert Moore</author>
<author>Francois Andery</author>
<author>Douglas Moran</author>
</authors>
<title>Interleaving syntax and semantics in an efficient bottom-up parser.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL&apos;94).</booktitle>
<contexts>
<context position="24535" citStr="Dowding et al. (1994)" startWordPosition="4033" endWordPosition="4036"> isomorphic as well, hence the reduction of synchronous tree pairs to semanticallyannotated syntax trees. This isomorphism restriction on derived trees reduces the number of quantifier scoping configurations that can be assigned to any given input (most of which are unlikely to be used in a practical application), but its relative parsimony allows syntactically ambiguous inputs to be semantically interpreted in a shared forest representation in worst-case polynomial time. The interleaving of semantic evaluation and parsing for the purpose of disambiguation also has much in common with that of Dowding et al. (1994), except that in this case, constituents are not only semantically type-checked, but are also fully interpreted each time they are proposed. There are also commonalities between the underspecified semantic representation of structurallyambiguous elementary tree constituents in a shared forest and the underspecified semantic representation of (e.g. quantifier) scope ambiguity described by Reyle (1993).3 3 Evaluation The contribution of this model-theoretic semantic information toward disambiguation was evaluated on a set of directions to animated agents collected in a controlled but spatially c</context>
</contexts>
<marker>Dowding, Moore, Andery, Moran, 1994</marker>
<rawString>John Dowding, Robert Moore, Francois Andery, and Douglas Moran. 1994. Interleaving syntax and semantics in an efficient bottom-up parser. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL&apos;94).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>How much context sensitivity is necessary for characterizing structural descriptions: Tree adjoining grammars.</title>
<date>1985</date>
<booktitle>Natural language parsing: Psychological, computational and theoretical perspectives,</booktitle>
<pages>206--250</pages>
<editor>In L. Karttunen D. Dowty and A. Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, U.K.</location>
<contexts>
<context position="22667" citStr="Joshi, 1985" startWordPosition="3759" endWordPosition="3760">e 5: Shared forest for `the girl with the hat behind the counter.&apos; NP ! NP PP ~x1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn) ; or fg1g PP ! PP Ax2::: xn=2: Qx1: $1(x1::: xn) ; PP ! P NP ~x1::: xn=2: $1(x1::: xn) ^ $2(x1) NP ! NP PP ; ~x1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn) fg1g NP ! NP PP ~x1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn) ; PP ! PP Ax2::: xn=2: Qx1: $1(x1::: xn) fg1g PP ! P NP ~x1::: xn=2: $1(x1::: xn) ^ $2(x1) fhc1; g1ig PP ! PP Ax2::: xn=2: Qx1: $1(x1::: xn) fg1; b1g PP ! PNP ~x1::: xn=2: $1(x1::: xn) ^ $2(x1) fhh1; g1i;hh2; b1ig tensions of tree-adjoining grammar (Joshi, 1985), namely multi-component tree adjoining grammar (Becker et al., 1991) and description tree substitution grammar (Rambow et al., 1995), and indeed represents something of a combination of the two: 1. Like description tree substitution grammars, but unlike multi-component TAGs, it allows trees to be partitioned into any desired set of contiguous components during composition, 2. Like multi-component TAGs, but unlike description tree substitution grammars, it allows the specification of particular insertion sites within elementary trees, and 3. Unlike both, it allow duplication of structure (whic</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Aravind K. Joshi. 1985. How much context sensitivity is necessary for characterizing structural descriptions: Tree adjoining grammars. In L. Karttunen D. Dowty and A. Zwicky, editors, Natural language parsing: Psychological, computational and theoretical perspectives, pages 206{250. Cambridge University Press, Cambridge, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>David Stallard</author>
<author>Robert Bobrow</author>
<author>Richard Schwartz</author>
</authors>
<title>A fully statistical approach to natural language interfaces.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL&apos;96),</booktitle>
<pages>55--61</pages>
<contexts>
<context position="2901" citStr="Miller et al., 1996" startWordPosition="428" endWordPosition="431"> interface would require (which also must be precisely described), introducing a great deal of ambiguity into input processing. This paper therefore explores the use of a statistical model of language conditioned on the meanings or denotations of input utterances in the context of an interface&apos;s underlying application environment or world model. This use of model-theoretic interpretation represents an important extension to the `semantic grammars&apos; used in existing statistical spoken language interfaces, which rely on co-occurrences among lexically-determined semantic classes and slot fillers (Miller et al., 1996), in that the probability of an analysis is now also conditioned on the existence of denoted entities and relations in the world model. The advantage of the interpretation-based disambiguation advanced here is that the probability of generating, for example, the noun phrase `the lemon next to the safe&apos; can be more reliably estimated from the frequency with which noun phrases have non-empty denotations — given the fact that `the lemon next to the safe&apos; does indeed denote something in the world model — than it can from the relatively sparse co-occurrences of frame labels such as LEMON and NEXT-T</context>
<context position="4935" citStr="Miller et al., 1996" startWordPosition="751" endWordPosition="754">model-theoretic interpretation in disambiguation yields a statistically significant improvement on standard measures of parsing accuracy over a baseline grammar not conditioned on denotations. 2 Model-theoretic interpretation In order to determine whether a user&apos;s directions denote entities and relations that exist in the world model — and of course, in order to execute those directions once they are disambiguated — it is necessary to precisely represent the meanings of input utterances. Semantic grammars of the sort employed in current spoken language interfaces for flight reservation tasks (Miller et al., 1996; Seneff et al., 1998) associate fragments of logical — typically relational algebra — expressions with recursive transition networks encoding lexicalized rules in a context-free grammar (the independent probabilities of these rules can then be estimated from a training corpus and multiplied together to give a probability for any given analysis). In flight reservation systems, these associated semantic expressions usually designate entities through a fixed set of constant symbols used as proper names (e.g. for cities and numbered flights); but in applications with unlabeled (perhaps visually-r</context>
</contexts>
<marker>Miller, Stallard, Bobrow, Schwartz, 1996</marker>
<rawString>Scott Miller, David Stallard, Robert Bobrow, and Richard Schwartz. 1996. A fully statistical approach to natural language interfaces. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL&apos;96), pages 55{61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>The proper treatment of quantification in ordinary English.</title>
<date>1973</date>
<booktitle>Approaches to Natural Langauge,</booktitle>
<pages>221--242</pages>
<editor>In J. Hintikka, J.M.E. Moravcsik, and P. Suppes, editors,</editor>
<publisher>Yale University Press,</publisher>
<contexts>
<context position="23480" citStr="Montague (1973)" startWordPosition="3878" endWordPosition="3879"> Like description tree substitution grammars, but unlike multi-component TAGs, it allows trees to be partitioned into any desired set of contiguous components during composition, 2. Like multi-component TAGs, but unlike description tree substitution grammars, it allows the specification of particular insertion sites within elementary trees, and 3. Unlike both, it allow duplication of structure (which is used for merging quantifiers from different elementary trees). The use of lambda calculus functions to define decomposable meanings for input sentences draws on traditions of Church (1940) and Montague (1973), but this approach differs from the Montagovian system by introducing explicit limits on computational complexity (in order to allow tractable disambiguation). This approach to semantics is very similar to that described by Shieber (1994), in which syntactic and semantic expressions are assembled synchronously using paired tree-adjoining grammars with isomorphic derivations, except that in this approach the derived structures are isomorphic as well, hence the reduction of synchronous tree pairs to semanticallyannotated syntax trees. This isomorphism restriction on derived trees reduces the nu</context>
</contexts>
<marker>Montague, 1973</marker>
<rawString>Richard Montague. 1973. The proper treatment of quantification in ordinary English. In J. Hintikka, J.M.E. Moravcsik, and P. Suppes, editors, Approaches to Natural Langauge, pages 221{242. D. Riedel, Dordrecht. Reprinted in R. H. Thomason ed., Formal Philosophy, Yale University Press, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
</authors>
<title>Generalized phrase structure grammars, head grammars and natural langauge.</title>
<date>1984</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="13853" citStr="Pollard, 1984" startWordPosition="2273" endWordPosition="2274"> $2(x1, xm+1... xn) VP —&gt; VP Ax2... xn. Qx1. $1(x1... xn) VP —&gt; VP Ax2... xn. Qx1. $1(x1... xn) 1 VP 1 P —&gt; under Ax1x2. Under(x2, x1) NP 2 of zero or more sub-strings arranged around certain `elementary&apos; strings contributing terminal symbols. A class of tree-rewriting systems can similarly be defined as rewriting systems whose objects are trees, and whose allowable expansions are productions (similar to context-free productions), each of which rewrite a tree A as some function f applied to zero or more sub-trees A1; ::: As; s &gt; 0 arranged around some `elementary&apos; tree structure defined by f (Pollard, 1984; Weir, 1988): A—� f(A1;::: As) (1) This elementary tree structure can be used to express the dominance relationship between a logical predicate and the quantifiers that bind its variables (which must be preserved in any meaningful derived structure); but in order to allow the same instance of a quantifier to bind variables in more than one predicate, the rewriting productions of such a semantic tree-rewriting system must allow expanded subtrees to identify parts of their structure (specifically, the parts containing quantifiers) with parts of each other&apos;s structure, and with that of their hos</context>
</contexts>
<marker>Pollard, 1984</marker>
<rawString>Carl Pollard. 1984. Generalized phrase structure grammars, head grammars and natural langauge. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>David Weir</author>
<author>K Vijay-Shanker</author>
</authors>
<title>D-tree grammars.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL &apos;95).</booktitle>
<contexts>
<context position="22800" citStr="Rambow et al., 1995" startWordPosition="3776" endWordPosition="3779">) ; or fg1g PP ! PP Ax2::: xn=2: Qx1: $1(x1::: xn) ; PP ! P NP ~x1::: xn=2: $1(x1::: xn) ^ $2(x1) NP ! NP PP ; ~x1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn) fg1g NP ! NP PP ~x1::: xn=1: $1(x1::: xm=1) ^ $2(x1; xm+1::: xn) ; PP ! PP Ax2::: xn=2: Qx1: $1(x1::: xn) fg1g PP ! P NP ~x1::: xn=2: $1(x1::: xn) ^ $2(x1) fhc1; g1ig PP ! PP Ax2::: xn=2: Qx1: $1(x1::: xn) fg1; b1g PP ! PNP ~x1::: xn=2: $1(x1::: xn) ^ $2(x1) fhh1; g1i;hh2; b1ig tensions of tree-adjoining grammar (Joshi, 1985), namely multi-component tree adjoining grammar (Becker et al., 1991) and description tree substitution grammar (Rambow et al., 1995), and indeed represents something of a combination of the two: 1. Like description tree substitution grammars, but unlike multi-component TAGs, it allows trees to be partitioned into any desired set of contiguous components during composition, 2. Like multi-component TAGs, but unlike description tree substitution grammars, it allows the specification of particular insertion sites within elementary trees, and 3. Unlike both, it allow duplication of structure (which is used for merging quantifiers from different elementary trees). The use of lambda calculus functions to define decomposable meani</context>
</contexts>
<marker>Rambow, Weir, Vijay-Shanker, 1995</marker>
<rawString>Owen Rambow, David Weir, and K. Vijay-Shanker. 1995. D-tree grammars. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL &apos;95).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uwe Reyle</author>
</authors>
<title>Dealing with ambiguities by underspecification: Construction, representation and deduction.</title>
<date>1993</date>
<journal>Journal of Semantics,</journal>
<pages>10--123</pages>
<contexts>
<context position="24938" citStr="Reyle (1993)" startWordPosition="4092" endWordPosition="4093">in a shared forest representation in worst-case polynomial time. The interleaving of semantic evaluation and parsing for the purpose of disambiguation also has much in common with that of Dowding et al. (1994), except that in this case, constituents are not only semantically type-checked, but are also fully interpreted each time they are proposed. There are also commonalities between the underspecified semantic representation of structurallyambiguous elementary tree constituents in a shared forest and the underspecified semantic representation of (e.g. quantifier) scope ambiguity described by Reyle (1993).3 3 Evaluation The contribution of this model-theoretic semantic information toward disambiguation was evaluated on a set of directions to animated agents collected in a controlled but spatially complex 3-D simulated environment (of children running a lemonade stand). In order to avoid priming them towards particular linguistic constructions, subjects were shown unnarrated animations of computer-simulated agents performing different tasks in this environment (picking fruit, operating a juicer, and exchanging lemonade for money), which were described only as the `desired behavior&apos; of each agen</context>
</contexts>
<marker>Reyle, 1993</marker>
<rawString>Uwe Reyle. 1993. Dealing with ambiguities by underspecification: Construction, representation and deduction. Journal of Semantics, 10:123{179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Rogers</author>
</authors>
<title>Capturing CFLs with tree adjoining grammars.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL &apos;94).</booktitle>
<contexts>
<context position="17885" citStr="Rogers, 1994" startWordPosition="2931" endWordPosition="2932"> is that such systems allow the application of well-studied restrictions to limit their recursive capacity to generate structural descriptions (in this case, to limit the unbounded overlapping of quantifier-variable dependencies that can produce unlimited numbers of free variables at certain steps in a derivation), without limiting the multi-level structure of their elementary trees, used here for capturing the well-formedness constraint that a predicate be dominated by its variables&apos; quantifiers. One such restriction, based on the regular form restriction defined for tree adjoining grammars (Rogers, 1994), prohibits a grammar from allowing any cycle of elementary trees, each intervening inside a spine (a path connecting the insertion sites of any argument) of the next. This restriction is defined below: Definition 2.1 Let a spine in an elementary tree be the path of nodes (or object-level rule applications) connecting all insertion site addresses of the same argument. Definition 2.2 A grammar G is in regular form if a directed acyclic graph (V, E) can be drawn with vertices VH, VA 2 V corresponding to partitioned elementary trees of G (partitioned as described above), and directed edges (VH, V</context>
</contexts>
<marker>Rogers, 1994</marker>
<rawString>James Rogers. 1994. Capturing CFLs with tree adjoining grammars. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL &apos;94).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Seneff</author>
<author>Ed Hurley</author>
<author>Raymond Lau</author>
<author>Christine Pao</author>
<author>Philipp Schmid</author>
<author>Victor Zue</author>
</authors>
<title>GALAXY-II: a reference architecture for conversational system development.</title>
<date>1998</date>
<booktitle>In Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP &apos;98),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="4957" citStr="Seneff et al., 1998" startWordPosition="755" endWordPosition="758">pretation in disambiguation yields a statistically significant improvement on standard measures of parsing accuracy over a baseline grammar not conditioned on denotations. 2 Model-theoretic interpretation In order to determine whether a user&apos;s directions denote entities and relations that exist in the world model — and of course, in order to execute those directions once they are disambiguated — it is necessary to precisely represent the meanings of input utterances. Semantic grammars of the sort employed in current spoken language interfaces for flight reservation tasks (Miller et al., 1996; Seneff et al., 1998) associate fragments of logical — typically relational algebra — expressions with recursive transition networks encoding lexicalized rules in a context-free grammar (the independent probabilities of these rules can then be estimated from a training corpus and multiplied together to give a probability for any given analysis). In flight reservation systems, these associated semantic expressions usually designate entities through a fixed set of constant symbols used as proper names (e.g. for cities and numbered flights); but in applications with unlabeled (perhaps visually-represented) environmen</context>
</contexts>
<marker>Seneff, Hurley, Lau, Pao, Schmid, Zue, 1998</marker>
<rawString>Stephanie Seneff, Ed Hurley, Raymond Lau, Christine Pao, Philipp Schmid, and Victor Zue. 1998. GALAXY-II: a reference architecture for conversational system development. In Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP &apos;98), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Restricting the weakgenerative capability of synchronous tree adjoining grammars.</title>
<date>1994</date>
<journal>Computational Intelligence,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="23719" citStr="Shieber (1994)" startWordPosition="3914" endWordPosition="3915">stitution grammars, it allows the specification of particular insertion sites within elementary trees, and 3. Unlike both, it allow duplication of structure (which is used for merging quantifiers from different elementary trees). The use of lambda calculus functions to define decomposable meanings for input sentences draws on traditions of Church (1940) and Montague (1973), but this approach differs from the Montagovian system by introducing explicit limits on computational complexity (in order to allow tractable disambiguation). This approach to semantics is very similar to that described by Shieber (1994), in which syntactic and semantic expressions are assembled synchronously using paired tree-adjoining grammars with isomorphic derivations, except that in this approach the derived structures are isomorphic as well, hence the reduction of synchronous tree pairs to semanticallyannotated syntax trees. This isomorphism restriction on derived trees reduces the number of quantifier scoping configurations that can be assigned to any given input (most of which are unlikely to be used in a practical application), but its relative parsimony allows syntactically ambiguous inputs to be semantically inter</context>
</contexts>
<marker>Shieber, 1994</marker>
<rawString>Stuart M. Shieber. 1994. Restricting the weakgenerative capability of synchronous tree adjoining grammars. Computational Intelligence, 10(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Weir</author>
</authors>
<title>Characterizing mildly contextsensitive grammar formalisms.</title>
<date>1988</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="13866" citStr="Weir, 1988" startWordPosition="2275" endWordPosition="2276"> xn) VP —&gt; VP Ax2... xn. Qx1. $1(x1... xn) VP —&gt; VP Ax2... xn. Qx1. $1(x1... xn) 1 VP 1 P —&gt; under Ax1x2. Under(x2, x1) NP 2 of zero or more sub-strings arranged around certain `elementary&apos; strings contributing terminal symbols. A class of tree-rewriting systems can similarly be defined as rewriting systems whose objects are trees, and whose allowable expansions are productions (similar to context-free productions), each of which rewrite a tree A as some function f applied to zero or more sub-trees A1; ::: As; s &gt; 0 arranged around some `elementary&apos; tree structure defined by f (Pollard, 1984; Weir, 1988): A—� f(A1;::: As) (1) This elementary tree structure can be used to express the dominance relationship between a logical predicate and the quantifiers that bind its variables (which must be preserved in any meaningful derived structure); but in order to allow the same instance of a quantifier to bind variables in more than one predicate, the rewriting productions of such a semantic tree-rewriting system must allow expanded subtrees to identify parts of their structure (specifically, the parts containing quantifiers) with parts of each other&apos;s structure, and with that of their host elementary </context>
</contexts>
<marker>Weir, 1988</marker>
<rawString>David Weir. 1988. Characterizing mildly contextsensitive grammar formalisms. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>XTAG Research Group</author>
</authors>
<title>A lexicalized tree adjoining grammar for english.</title>
<date>1998</date>
<tech>Technical report, IRCS,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="26053" citStr="Group, 1998" startWordPosition="4262" endWordPosition="4263"> exchanging lemonade for money), which were described only as the `desired behavior&apos; of each agent. The subjects were then asked to direct the agents, using their own words, to perform the desired behaviors as shown. 340 utterances were collected and annotated with brackets and elementary tree node addresses as described in Section 2.2, for use as training data and as gold standard data in testing. Some sample directions are shown in Figure 6. Most elementary trees were extracted, with some simplifications for parsing efficiency, from an existing broad-coverage grammar resource (XTAG Research Group, 1998), but some elementary trees for multi-word expressions had to be created anew. In all, a complete annotation of this corpus required a grammar of 68 elementary trees and a lexicon of 288 lexicalizations (that is, words or sets of words with indivisible semantics, forming the anchors of a given elementary tree). Each lexicalization was then assigned a semantic expression describing the intended geometric relation or class of objects in the simulated 3-D environment.4 The interface was tested on the first 100 collected utterances, and the parsing model was trained on the remaining utterances. Th</context>
</contexts>
<marker>Group, 1998</marker>
<rawString>XTAG Research Group. 1998. A lexicalized tree adjoining grammar for english. Technical report, IRCS, University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>