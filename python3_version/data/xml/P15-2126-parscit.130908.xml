<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.043692">
<title confidence="0.991868">
Model Adaptation for Personalized Opinion Analysis
</title>
<author confidence="0.974632">
Mohammad Al Boni1, Keira Qi Zhou1, Hongning Wang2, and Matthew S. Gerber1
</author>
<affiliation confidence="0.999392333333333">
1Department of Systems and Information Engineering
2Department of Computer Science
1,2University of Virginia, USA
</affiliation>
<email confidence="0.997373">
1,21ma2sm,qz4aq,hw5x,msg8ul@virginia.edu
</email>
<sectionHeader confidence="0.993872" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999838888888889">
Humans are idiosyncratic and variable: to-
wards the same topic, they might hold dif-
ferent opinions or express the same opin-
ion in various ways. It is hence impor-
tant to model opinions at the level of in-
dividual users; however it is impractical
to estimate independent sentiment classi-
fication models for each user with limited
data. In this paper, we adopt a model-
based transfer learning solution – using
linear transformations over the parame-
ters of a generic model – for personalized
opinion analysis. Extensive experimental
results on a large collection of Amazon
reviews confirm our method significantly
outperformed a user-independent generic
opinion model as well as several state-of-
the-art transfer learning algorithms.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999912125">
The proliferation of user-generated opinionated
text data has fueled great interest in opinion analy-
sis (Pang and Lee, 2008; Liu, 2012). Understand-
ing opinions expressed by a population of users
has value in a wide spectrum of areas, including
social network analysis (Bodendorf and Kaiser,
2009), business intelligence (Gamon et al., 2005),
marketing analysis (Jansen et al., 2009), person-
alized recommendation (Yang et al., 2013) and
many more.
Most of the existing opinion analysis research
focuses on population-level analyses, i.e., predict-
ing opinions based on models estimated from a
collection of users. The underlying assumption is
that users are homogeneous in the way they ex-
press opinions. Nevertheless, different users may
use the same words to express distinct opinions.
For example, the word “expensive” tends to be
associated with negative sentiment in general, al-
though some users may use it to describe their sat-
isfaction with a product’s quality. Failure to rec-
ognize this difference across users will inevitably
lead to inaccurate understanding of opinions.
However, due to the limited availability of user-
specific opinionated data, it is impractical to es-
timate independent models for each user. In this
work, we propose a transfer learning based solu-
tion, named LinAdapt, to address this challenge.
Instead of estimating independent classifiers for
each user, we start from a generic model and adapt
it toward individual users based on their own opin-
ionated text data. In particular, our key assump-
tion is that the adaptation can be achieved via a set
of linear transformations over the generic model’s
parameters. When we have sufficient observations
for a particular user, the transformations will push
the adapted model towards the user’s personalized
model; otherwise, it will back off to the generic
model. Empirical evaluations on a large collection
of Amazon reviews verify the effectiveness of the
proposed solution: it significantly outperformed a
user-independent generic model as well as several
state-of-the-art transfer learning algorithms.
Our contribution is two-fold: 1) we enable ef-
ficient personalization of opinion analysis via a
transfer learning approach, and 2) the proposed so-
lution is general and applicable to any linear model
for user opinion analysis.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999581142857143">
Sentiment Analysis refers to the process of iden-
tifying subjective information in source materials
(Pang and Lee, 2008; Liu, 2012). Typical tasks in-
clude: 1) classifying textual documents into posi-
tive and negative polarity categories, (Dave et al.,
2003; Kim and Hovy, 2004); 2) identifying textual
topics and their associated opinions (Wang et al.,
2010; Jo and Oh, 2011); and 3) opinion summa-
rization (Hu and Liu, 2004; Ku et al., 2006). Ap-
proaches for these tasks focus on population-level
opinion analyses, in which one model is shared
across all users. Little effort has been devoted
to personalized opinion analyses, where each user
has a particular model, due to the absence of user-
</bodyText>
<page confidence="0.917868">
769
</page>
<bodyText confidence="0.954176551724138">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 769–774,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
specific opinion data for model estimation.
Transfer Learning aims to help improve pre-
dictive models by using knowledge from different
but related problems (Pan and Yang, 2010). In
the opinion mining community, transfer learning
is used primarily for domain adaptation. Blitzer
et al. (2006) proposed structural correspondence
learning to identify the correspondences among
features between different domains via the concept
of pivot features. Pan et al. (2010) propose a spec-
tral feature alignment algorithm to align domain-
specific sentiment words from different domains
for sentiment categorization. By assuming that
users tend to express consistent opinions towards
the same topic over time, Guerra et al. (2011) ap-
plied instance-based transfer learning for real time
sentiment analysis.
Our method is inspired by a personalized rank-
ing model adaptation method developed by Wang
et al. (2013). To the best of our knowledge, our
work is the first to estimate user-level classifiers
for opinion analysis. By adapting a generic opin-
ion classification model for each user, heterogene-
ity among their expressions of opinions can be
captured and it help us understand users’ opinions
at a finer granularity.
</bodyText>
<sectionHeader confidence="0.9938305" genericHeader="method">
3 Linear Transformation Based Model
Adaptation
</sectionHeader>
<bodyText confidence="0.99748995">
Given a generic sentiment classification model y =
fs(x), we aim at finding an optimal adapted model
y = fu(x) for user u, such that fu(x) best cap-
tures u’s opinion in his/her generated textual doc-
uments Du ={xd, yd}|D|
d=1, where xd is the feature
vector for document d, yd is the sentiment class
label (e.g., positive v.s., negative). To achieve so,
we assume that such adaptation can be performed
via a series of linear transformations on fs(x)’s
model parameter ws. This assumption is general
and can be applied to a wide variety of sentiment
classifiers, e.g., logistic regression and linear sup-
port vector machines, as long as they have a linear
core function. Therefore, we name our proposed
method as LinAdapt. In this paper, we focus on
logistic regression (Pang et al., 2002); but the pro-
posed procedures can be easily adopted for many
other classifiers (Wang et al., 2013).
Our global model y =fs(x) can be written as,
</bodyText>
<equation confidence="0.998095333333333">
1
Ps(yd = 1|xd) = (1)
1 + e−wsTxd
</equation>
<bodyText confidence="0.9946335">
where ws are the linear coefficients for the corre-
sponding document features.
Standard linear transformations, i.e., scaling,
shifting and rotation, can be encoded via a V x
</bodyText>
<equation confidence="0.998181666666667">
(V + 1) matrix Au for each user u as:
0 ... ... ...
0 0 ... ... aug(V ) bug(V )
</equation>
<bodyText confidence="0.999686461538461">
where V is the total number of features.
However, the above transformation introduces
O(V 2) free parameters, which are even more than
the number of free parameters required to estimate
a new logistic regression model. Following the so-
lution proposed by Wang et al. (2013), we further
assume the transformations can be performed in a
group-wise manner to reduce the size of param-
eters in adaptation. The intuition behind this as-
sumption is that features that share similar contri-
butions to the classification model are more likely
to be adapted in the same way. Another advantage
of feature grouping is that the feedback informa-
tion will be propagated through the features in the
same group while adaptation; hence the features
that are not observed in the adaptation data can
also be updated properly.
We denote g(·) as the feature grouping function,
which maps V original features to K groups, and
auk, buk and cu k as the scaling, shifting and rotation
operations over ws in group k for user u. In addi-
tion, rotation is only performed for the features in
the same group, and it is assumed to be symmetric,
i.e., cuk,ij = cuk,ji, where g(i) = k and g(j) = k.
As a result, the personalized classification model
fu(x) after adaptation can be written as,
</bodyText>
<equation confidence="0.998242333333333">
1
P u(yd = 1|xd) = (2)
1 + e−(Au ˜ws)Txd
</equation>
<bodyText confidence="0.998388444444445">
where ˜ws = (ws, 1) to accommodate the shifting
operation.
The optimal transformation matrix Au for user
u can be estimated by maximum likelihood esti-
mation based on user u’s own opinionated docu-
ment collection Du. To avoid overfitting, we pe-
nalize the transformation which increases the dis-
crepancy between the adapted model and global
model by the following regularization term,
</bodyText>
<equation confidence="0.899567">
(au k − 1)2 − σ 2
E E cuk,ij 2, (3)
i,g(i)=k jOi,g(j)=k
</equation>
<bodyText confidence="0.997002333333333">
where q, Q and E are trade-off parameters control-
ling the balance among shifting, scaling and rota-
tion operations in adaptation.
</bodyText>
<equation confidence="0.995707727272727">
�
� � � � � � �
au g(1) cu g(1),12 cu g(1),13 0 0 bu g(1)
cu g(2),21 au g(2) cug(2),23 ... 0 bug(2)
cug(3),31 cug(3),32 aug(3) ..
.. ..bug(3)
..
..
. .
1
R(Au) = −η
2
− E K
2 E
k=1
K
E
k=1
buk2
K
E
k=1
</equation>
<page confidence="0.940767">
770
</page>
<bodyText confidence="0.970977583333333">
Combining the newly introduced regularization
term for Au and log-likelihood function for logis-
tic regression, we get the following optimization
problem to estimate the adaptation parameters,
max L(Au) = LLR(Du; Pu) + R(Au) (4)
A-
where LLR(Du; Pu) is the log-likelihood of logis-
tic regression on collection Du, and Pu is defined
in Eq (2).
Gradient-based method is used to optimize
Eq (4), in which the gradient for auk, bukand cu k
can be calculated as,
</bodyText>
<sectionHeader confidence="0.994715" genericHeader="evaluation">
4 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.999991142857143">
We performed empirical evaluations of the pro-
posed LinAdapt algorithm on a large collection of
product review documents. We compared our ap-
proach with several state-of-the-art transfer learn-
ing algorithms. In the following, we will first in-
troduce the evaluation corpus and baselines, and
then discuss our experimental findings.
</bodyText>
<subsectionHeader confidence="0.998958">
4.1 Data Collection and Baselines
</subsectionHeader>
<bodyText confidence="0.99999725">
We used a corpus of Amazon reviews provided
on Stanford SNAP website by McAuley and
Leskovec. (2013). We performed simple data pre-
processing: 1) annotated the reviews with ratings
greater than 3 stars (out of total 5 stars) as positive,
and others as negative; 2) removed duplicate re-
views; 3) removed reviewers who have more than
1,000 reviews or more than 90% positive or neg-
ative reviews; 4) chronologically ordered the re-
views in each user. We extracted unigrams and bi-
grams to construct bag-of-words feature represen-
tations for the review documents. Standard stop-
word removal (Lewis et al., 2004) and Porter stem-
ming (Willett, 2006) were applied. Chi-square
and information gain (Yang and Pedersen, 1997)
were used for feature selection and the union of
the resulting selected features are used in the fi-
nal controlled vocabulary. The resulting evalua-
tion data set contains 32,930 users, 281,813 posi-
tive reviews, and 81,522 negative reviews, where
each review is represented with 5,000 text features
with TF-IDF as the feature value.
Our first baseline is an instance-based adapta-
tion method (Brighton and Mellish, 2002). The k-
nearest neighbors of each testing review document
are found from the shared training set for person-
alized model training. As a result, for each test-
ing case, we are estimating an independent clas-
sification model. We denote this method as “Re-
Train.” The second baseline builds on the model-
based adaptation method developed by Geng et
al. (2012). For each user, it enforces the adapted
model to be close to the global model via an ad-
ditional L2 regularization when training the per-
sonalized model. But the full set of parameters in
logistic regression need to estimated during adap-
tation. We denote this method as “Reg-LR.”
In our experiments, all model adaptation is per-
formed in an online fashion: we first applied the
up-to-date classification model on the given test-
ing document; evaluated the model’s performance
with ground-truth; and used the feedback to up-
date the model. Because the class distribution of
our evaluation data set is highly skewed (77.5%
positive), it is important to evaluate the adapted
models’ performance on both classes. In the fol-
lowing comparisons, we report the average F-1
measure of both positive and negative classes.
</bodyText>
<subsectionHeader confidence="0.999264">
4.2 Comparison of Adaptation Performance
</subsectionHeader>
<bodyText confidence="0.999985">
First we need to estimate a global model for adap-
tation. A typical approach is to collect a portion
of historical reviews from each user to construct a
shared training corpus (Wang et al., 2013). How-
ever, this setting is problematic: it already exploits
information from every user and does not reflect
the reality that some (new) users might not exist
when training the global model. In our experi-
ment, we isolated a group of random users for
global model training. In addition, since there are
multiple categories in this review collection, such
as book, movies, electronics, etc, and each user
might discuss various categories, it is infeasible
to balance the coverage of different categories in
global model training by only selecting the users.
As a result, we vary the number of reviews in each
domain from the selected training users to estimate
the global model. We started with 1000 reviews
from the top 5 categories (Movies &amp; TV, Books,
Music, Home &amp; Kitchen, and Video Games), then
evaluated the global model on 10,000 testing users
which consist of three groups: light users with 2 to
10 reviews, medium users with 11 to 50 reviews,
and heavy users with 51 to 200 reviews. After each
evaluation run, we added an extra 1000 reviews
and repeated the training and evaluation.
</bodyText>
<figure confidence="0.998579464285714">
Dtd
E
d=1
=
&amp;L(Au)
&amp;ak
s
wi xdi}−η(ak − 1)
E
fYd[1 − P(Yd|xd)]
i,g(i)=k
Dtd
E
d=1
=
i,g(i)=k
&amp;L(Au)
&amp;bk
xdi}−vbk
E
fYd[1 − P(Yd|xd)]
Dtd
E
d=1
=
&amp;L(Au)
&amp;Ck,ij
fYd[1 − P(Yd|xd)]wsjxdi}−ECk,ij
</figure>
<page confidence="0.982634">
771
</page>
<tableCaption confidence="0.818329">
Table 1: Global model training with varying size
of training corpus.
</tableCaption>
<table confidence="0.9996242">
Model Metric 1000 2000 3000 4000 5000
Global Pos F1 0.741 0.737 0.738 0.734 0.729
Neg F1 0.106 0.126 0.125 0.132 0.159
LinAdapt Pos F1 0.694 0.693 0.692 0.694 0.696
Neg F1 0.299 0.299 0.296 0.299 0.304
</table>
<tableCaption confidence="0.980431">
Table 2: Effect of feature grouping in LinAdapt.
</tableCaption>
<table confidence="0.996623625">
Method Metric 100 200 400 800 1000
Rand Pos F1 0.691 0.692 0.696 0.686 0.681
Neg F1 0.295 0.298 0.300 0.322 0.322
SVD Pos F1 0.691 0.698 0.704 0.697 0.696
Neg F1 0.298 0.302 0.300 0.322 0.334
Pos F1 0.701 0.702 0.705 0.700 0.696
Cross
Neg F1 0.298 0.299 0.303 0.328 0.331
</table>
<bodyText confidence="0.99997097826087">
To understand the effect of global model train-
ing in model adaptation, we also included the per-
formance of LinAdapt, which only used shifting
and scaling operations and Cross feature group-
ing method with k = 400 (detailed feature group-
ing method will be discussed in the next exper-
iment). Table 1 shows the performance of the
global model and LinAdapt with respect to differ-
ent training corpus size. We found that the global
model converged very quickly with around 5,000
reviews, and this gives the best compromise for
both positive and negative classes in both global
and adaptaed model. Therefore, we will use this
global model for later adaptation experiments.
We then investigated the effect of feature group-
ing in LinAdapt. We employed the feature group-
ing methods of SVD and Cross developed by
Wang et al. (2013). A random feature grouping
method is included to validate the necessity of
proper feature grouping. We varied the number
of feature groups from 100 to 1000, and evaluated
the adapted models using the same 10,000 testing
users from the previous experiment. As shown in
Table 2, Cross provided the best adaptation per-
formance and random is the worse; a moderate
group size balances performance between positive
and negative classes. For the remaining experi-
ments, we use the Cross grouping with k = 400
in LinAdapt. In this group setting, we found that
the average number of features per group is 12.47
while the median is 12, which means that features
are normally distributed across different groups.
Next, we investigated the effect of differ-
ent linear operations in LinAdapt, and com-
pared LinAdapt against the baselines. We started
LinAdapt with only the shifting operation, and
then included scaling and rotation. To validate
the necessity of personalizing sentiment classifica-
tion models, we also included the global model’s
performance in Figure 1. In particular, to under-
stand the longitudinal effect of personalized model
adaptation, we only used the heavy users (4,021
users) in this experiment. The results indicate
that the adapted models outperformed the global
model in identifying the negative class; while the
global model performs the best in recognizing pos-
itive reviews. This is due to the heavily biased
class distribution in our collection: global model
puts great emphasis on the positive reviews; while
the adaptation methods give equal weights to both
positive and negative reviews. In particular, in
LinAdapt, scaling and shifting operations lead to
satisfactory adaptation performance for the nega-
tive class with only 15 reviews; while rotation is
essential for recognizing the positive class.
To better understand the improvement of model
adaptation against the global model in different
types of users, we decomposed the performance
gain of different adaptation methods. For this ex-
periment, we used all the 10,000 testing users:
we used the first 50% of the reviews from each
user for adaptation and the rest for testing. Ta-
ble 3 shows the performance gain of different al-
gorithms under light, medium and heavy users.
For the heavy and medium users, which only
consist 0.1% and 35% of the total population in
our data set, our adaptation model achieved the
best improvement against the global model com-
pared with Reg-LR and ReTrain. For the light
users, who cover 64.9% of the total population,
LinAdapt was able to improve the performance
against the global model for the negative class, but
Reg-LR and ReTrain had attained higher perfor-
mance. For the positive class, none of those adap-
tation methods can improve over the global model
although they provide a very close performance (in
LinAdapt, the differences are not significant). The
significant improvement in negative class predic-
tion from model adaptation is encouraging con-
sidering the biased distribution of classes, which
results in poor performance in the global model.
The above improved classification performance
indicates the adapted model captures the hetero-
geneity in expressing opinions across users. To
verify this, we investigated textual features whose
sentiment polarities are most/least frequently up-
dated across users. We computed the variance of
the absolute difference between the learned feature
weights in LinAdapt and global model. High vari-
ance indicates the word’s sentiment polarity fre-
quently changes across different users. But there
are two reasons for a low variance: first, a rare
</bodyText>
<page confidence="0.982803">
772
</page>
<figure confidence="0.99919062745098">
F−Measure
F−Measure
# adaption documents
# adaption documents
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10 20 30 40 50 60 70 80 90
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
2 4 6 8 10
Zoomed Part
shifting
shifting+scaling
shifting+scaling+rotation
Global
ReTrain
Reg−LR
100
100
10 20 30 40 50 60 70 80 90
0.4
0.35
0.3
0.25
0.2
0.15
shifting
shifting+scaling
shifting+scaling+rotation
Global
ReTrain
Reg−LR
0
0.1
0.05
(a) Positive F-1 measure (b) Negative F-1 measure
</figure>
<figureCaption confidence="0.999574">
Figure 1: Online adaptation performance comparisons.
</figureCaption>
<tableCaption confidence="0.9792">
Table 3: User-level performance gain over global
model from ReTrain, Reg-LR and LinAdapt.
</tableCaption>
<table confidence="0.998196454545455">
Method User Class Pos F1 Neg F1
Heavy -0.092 0.155*
ReTrain Medium -0.095 0.235*
Light -0.157* 0.255*
Heavy -0.010 0.109*
Reg-LR Medium -0.005 0.206*
Light -0.060 0.232*
Heavy -0.046 0.248*
LinAdapt Medium -0.049 0.235*
Light -0.091 0.117*
* p-value&lt; 0.05 with paired t-test.
</table>
<tableCaption confidence="0.8870296">
Table 4: Top 10 words with the highest and lowest
variance of learned polarity in LinAdapt.
Variance Features
Table 5: Learned sentiment polarity range of three
typical words in LinAdapt.
</tableCaption>
<table confidence="0.9606568">
Feature Range Global Used as Used as
Weight Positive Negative
Experience [-0.231,0.232] 0.002 3348 1503
Good [-0.170,0.816] 0.032 8438 1088
Money [-0.439,0.074] -0.013 646 6238
</table>
<bodyText confidence="0.9987206">
timent across users. Table 5 shows the detailed
range of learned polarity for three typical opin-
ion words in 10,000 users. This result indicates
LinAdapt well captures the fact that users express
opinions differently even with the same words.
</bodyText>
<sectionHeader confidence="0.989387" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.980124357142857">
waste good attempt
Highest money return save
poor worst annoy
lover correct pure
Lowest care the product odd
sex evil less than
word that is not used by many users; second, a
word is being used frequently, yet, with the same
polarity. We are only interested in the second case.
Therefore, for each word, we compute its user fre-
quency (UF), i.e., how many unique users used
this word in their reviews. Then, we selected 1000
most popular features by UF, and ranked them ac-
cording to the variance of learned sentiment polar-
ities. Table 4 shows the top ten features with the
highest and lowest polarity variance.
We inspected the learned weights in the adapted
models in each user from LinAdapt, and found
the words like waste, poor, and good share the
same sentiment polarity as in the global model
but different magnitudes; while words like money,
instead, and return are almost neutral in global
model, but vary across the personalized models.
On the other hand, words such as care, sex, evil,
pure, and correct constantly carry the same sen-
In this paper, we developed a transfer learning
based solution for personalized opinion mining.
Linear transformations of scaling, shifting and ro-
tation are exploited to adapt a global sentiment
classification model for each user. Empirical
evaluations based on a large collection of opin-
ionated review documents confirm that the pro-
posed method effectively models personal opin-
ions. By analyzing the variance of the learned
feature weights, we are able to discover words
that hold different polarities across users, which
indicates our model captures the fact that users
express opinions differently even with the same
words. In the future, we plan to further explore
this linear transformation based adaptation from
different perspectives, e.g., sharing adaptation op-
erations across users or review categories.
</bodyText>
<sectionHeader confidence="0.999287" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9911272">
This research was funded in part by grant
W911NF-10-2-0051 from the United States Army
Research Laboratory. Also, Hongning Wang is
partially supported by the Yahoo Academic Career
Enhancement Award.
</bodyText>
<page confidence="0.998027">
773
</page>
<sectionHeader confidence="0.990138" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999847398148148">
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 confer-
ence on empirical methods in natural language pro-
cessing, pages 120–128. Association for Computa-
tional Linguistics.
Freimut Bodendorf and Carolin Kaiser. 2009. Detect-
ing opinion leaders and trends in online social net-
works. In Proceedings of the 2nd ACM workshop on
Social web search and mining, pages 65–68. ACM.
Henry Brighton and Chris Mellish. 2002. Advances
in instance selection for instance-based learning al-
gorithms. Data mining and knowledge discovery,
6(2):153–172.
Pedro Henrique Calais Guerra, Adriano Veloso, Wag-
ner Meira Jr, and Virg´ılio Almeida. 2011. From
bias to opinion: a transfer-learning approach to real-
time sentiment analysis. In Proceedings of the 17th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 150–158.
ACM.
Kushal Dave, Steve Lawrence, and David M Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In
Proceedings of the 12th international conference on
World Wide Web, pages 519–528. ACM.
Michael Gamon, Anthony Aue, Simon Corston-Oliver,
and Eric Ringger. 2005. Pulse: Mining customer
opinions from free text. In Advances in Intelligent
Data Analysis VI, pages 121–132. Springer.
Bo Geng, Yichen Yang, Chao Xu, and Xian-Sheng
Hua. 2012. Ranking model adaptation for domain-
specific search. Knowledge and Data Engineering,
IEEE Transactions on, 24(4):745–758.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.
Bernard J Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter power: Tweets as
electronic word of mouth. Journal of the Ameri-
can society for information science and technology,
60(11):2169–2188.
Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of the fourth ACM international con-
ference on Web search and data mining, pages 815–
824. ACM.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1367. Association for Computa-
tional Linguistics.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006. Opinion extraction, summarization and track-
ing in news and blog corpora. In AAAI spring
symposium: Computational approaches to analyz-
ing weblogs, volume 100107.
David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. Smart stopword list.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1–167.
Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In Proceedings of the 7th
ACM conference on Recommender systems, pages
165–172. ACM.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. Knowledge and Data Engineer-
ing, IEEE Transactions on, 22(10):1345–1359.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain sen-
timent classification via spectral feature alignment.
In Proceedings of the 19th international conference
on World wide web, pages 751–760. ACM.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data:
a rating regression approach. In Proceedings of
the 16th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 783–
792. ACM.
Hongning Wang, Xiaodong He, Ming-Wei Chang,
Yang Song, Ryen W White, and Wei Chu. 2013.
Personalized ranking model adaptation for web
search. In Proceedings of the 36th international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 323–332. ACM.
Peter Willett. 2006. The porter stemming algorithm:
then and now. Program, 40(3):219–223.
Yiming Yang and Jan O Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In ICML, volume 97, pages 412–420.
Dingqi Yang, Daqing Zhang, Zhiyong Yu, and Zhu
Wang. 2013. A sentiment-enhanced personalized
location recommendation system. In Proceedings of
the 24th ACM Conference on Hypertext and Social
Media, pages 119–128. ACM.
</reference>
<page confidence="0.998378">
774
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872291">
<title confidence="0.99989">Model Adaptation for Personalized Opinion Analysis</title>
<author confidence="0.992768">Al Keira Qi Hongning</author>
<author confidence="0.992768">S Matthew</author>
<affiliation confidence="0.938492333333333">of Systems and Information of Computer of Virginia,</affiliation>
<abstract confidence="0.998873842105263">Humans are idiosyncratic and variable: towards the same topic, they might hold different opinions or express the same opinion in various ways. It is hence important to model opinions at the level of individual users; however it is impractical to estimate independent sentiment classification models for each user with limited data. In this paper, we adopt a modelbased transfer learning solution – using linear transformations over the parameters of a generic model – for personalized opinion analysis. Extensive experimental results on a large collection of Amazon reviews confirm our method significantly outperformed a user-independent generic opinion model as well as several state-ofthe-art transfer learning algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 conference on empirical methods in natural language processing,</booktitle>
<pages>120--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4607" citStr="Blitzer et al. (2006)" startWordPosition="696" endWordPosition="699">has a particular model, due to the absence of user769 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 769–774, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics specific opinion data for model estimation. Transfer Learning aims to help improve predictive models by using knowledge from different but related problems (Pan and Yang, 2010). In the opinion mining community, transfer learning is used primarily for domain adaptation. Blitzer et al. (2006) proposed structural correspondence learning to identify the correspondences among features between different domains via the concept of pivot features. Pan et al. (2010) propose a spectral feature alignment algorithm to align domainspecific sentiment words from different domains for sentiment categorization. By assuming that users tend to express consistent opinions towards the same topic over time, Guerra et al. (2011) applied instance-based transfer learning for real time sentiment analysis. Our method is inspired by a personalized ranking model adaptation method developed by Wang et al. (2</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120–128. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freimut Bodendorf</author>
<author>Carolin Kaiser</author>
</authors>
<title>Detecting opinion leaders and trends in online social networks.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2nd ACM workshop on Social web search and mining,</booktitle>
<pages>65--68</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1326" citStr="Bodendorf and Kaiser, 2009" startWordPosition="192" endWordPosition="195"> transformations over the parameters of a generic model – for personalized opinion analysis. Extensive experimental results on a large collection of Amazon reviews confirm our method significantly outperformed a user-independent generic opinion model as well as several state-ofthe-art transfer learning algorithms. 1 Introduction The proliferation of user-generated opinionated text data has fueled great interest in opinion analysis (Pang and Lee, 2008; Liu, 2012). Understanding opinions expressed by a population of users has value in a wide spectrum of areas, including social network analysis (Bodendorf and Kaiser, 2009), business intelligence (Gamon et al., 2005), marketing analysis (Jansen et al., 2009), personalized recommendation (Yang et al., 2013) and many more. Most of the existing opinion analysis research focuses on population-level analyses, i.e., predicting opinions based on models estimated from a collection of users. The underlying assumption is that users are homogeneous in the way they express opinions. Nevertheless, different users may use the same words to express distinct opinions. For example, the word “expensive” tends to be associated with negative sentiment in general, although some user</context>
</contexts>
<marker>Bodendorf, Kaiser, 2009</marker>
<rawString>Freimut Bodendorf and Carolin Kaiser. 2009. Detecting opinion leaders and trends in online social networks. In Proceedings of the 2nd ACM workshop on Social web search and mining, pages 65–68. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Brighton</author>
<author>Chris Mellish</author>
</authors>
<title>Advances in instance selection for instance-based learning algorithms. Data mining and knowledge discovery,</title>
<date>2002</date>
<pages>6--2</pages>
<contexts>
<context position="10824" citStr="Brighton and Mellish, 2002" startWordPosition="1755" endWordPosition="1758">words feature representations for the review documents. Standard stopword removal (Lewis et al., 2004) and Porter stemming (Willett, 2006) were applied. Chi-square and information gain (Yang and Pedersen, 1997) were used for feature selection and the union of the resulting selected features are used in the final controlled vocabulary. The resulting evaluation data set contains 32,930 users, 281,813 positive reviews, and 81,522 negative reviews, where each review is represented with 5,000 text features with TF-IDF as the feature value. Our first baseline is an instance-based adaptation method (Brighton and Mellish, 2002). The knearest neighbors of each testing review document are found from the shared training set for personalized model training. As a result, for each testing case, we are estimating an independent classification model. We denote this method as “ReTrain.” The second baseline builds on the modelbased adaptation method developed by Geng et al. (2012). For each user, it enforces the adapted model to be close to the global model via an additional L2 regularization when training the personalized model. But the full set of parameters in logistic regression need to estimated during adaptation. We den</context>
</contexts>
<marker>Brighton, Mellish, 2002</marker>
<rawString>Henry Brighton and Chris Mellish. 2002. Advances in instance selection for instance-based learning algorithms. Data mining and knowledge discovery, 6(2):153–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Henrique Calais Guerra</author>
<author>Adriano Veloso</author>
<author>Wagner Meira Jr</author>
<author>Virg´ılio Almeida</author>
</authors>
<title>From bias to opinion: a transfer-learning approach to realtime sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>150--158</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5031" citStr="Guerra et al. (2011)" startWordPosition="757" endWordPosition="760">odels by using knowledge from different but related problems (Pan and Yang, 2010). In the opinion mining community, transfer learning is used primarily for domain adaptation. Blitzer et al. (2006) proposed structural correspondence learning to identify the correspondences among features between different domains via the concept of pivot features. Pan et al. (2010) propose a spectral feature alignment algorithm to align domainspecific sentiment words from different domains for sentiment categorization. By assuming that users tend to express consistent opinions towards the same topic over time, Guerra et al. (2011) applied instance-based transfer learning for real time sentiment analysis. Our method is inspired by a personalized ranking model adaptation method developed by Wang et al. (2013). To the best of our knowledge, our work is the first to estimate user-level classifiers for opinion analysis. By adapting a generic opinion classification model for each user, heterogeneity among their expressions of opinions can be captured and it help us understand users’ opinions at a finer granularity. 3 Linear Transformation Based Model Adaptation Given a generic sentiment classification model y = fs(x), we aim</context>
</contexts>
<marker>Guerra, Veloso, Jr, Almeida, 2011</marker>
<rawString>Pedro Henrique Calais Guerra, Adriano Veloso, Wagner Meira Jr, and Virg´ılio Almeida. 2011. From bias to opinion: a transfer-learning approach to realtime sentiment analysis. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 150–158. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th international conference on World Wide Web,</booktitle>
<pages>519--528</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3600" citStr="Dave et al., 2003" startWordPosition="541" endWordPosition="544">nificantly outperformed a user-independent generic model as well as several state-of-the-art transfer learning algorithms. Our contribution is two-fold: 1) we enable efficient personalization of opinion analysis via a transfer learning approach, and 2) the proposed solution is general and applicable to any linear model for user opinion analysis. 2 Related Work Sentiment Analysis refers to the process of identifying subjective information in source materials (Pang and Lee, 2008; Liu, 2012). Typical tasks include: 1) classifying textual documents into positive and negative polarity categories, (Dave et al., 2003; Kim and Hovy, 2004); 2) identifying textual topics and their associated opinions (Wang et al., 2010; Jo and Oh, 2011); and 3) opinion summarization (Hu and Liu, 2004; Ku et al., 2006). Approaches for these tasks focus on population-level opinion analyses, in which one model is shared across all users. Little effort has been devoted to personalized opinion analyses, where each user has a particular model, due to the absence of user769 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processin</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of the 12th international conference on World Wide Web, pages 519–528. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Anthony Aue</author>
<author>Simon Corston-Oliver</author>
<author>Eric Ringger</author>
</authors>
<title>Pulse: Mining customer opinions from free text.</title>
<date>2005</date>
<booktitle>In Advances in Intelligent Data Analysis VI,</booktitle>
<pages>121--132</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1370" citStr="Gamon et al., 2005" startWordPosition="198" endWordPosition="201">del – for personalized opinion analysis. Extensive experimental results on a large collection of Amazon reviews confirm our method significantly outperformed a user-independent generic opinion model as well as several state-ofthe-art transfer learning algorithms. 1 Introduction The proliferation of user-generated opinionated text data has fueled great interest in opinion analysis (Pang and Lee, 2008; Liu, 2012). Understanding opinions expressed by a population of users has value in a wide spectrum of areas, including social network analysis (Bodendorf and Kaiser, 2009), business intelligence (Gamon et al., 2005), marketing analysis (Jansen et al., 2009), personalized recommendation (Yang et al., 2013) and many more. Most of the existing opinion analysis research focuses on population-level analyses, i.e., predicting opinions based on models estimated from a collection of users. The underlying assumption is that users are homogeneous in the way they express opinions. Nevertheless, different users may use the same words to express distinct opinions. For example, the word “expensive” tends to be associated with negative sentiment in general, although some users may use it to describe their satisfaction </context>
</contexts>
<marker>Gamon, Aue, Corston-Oliver, Ringger, 2005</marker>
<rawString>Michael Gamon, Anthony Aue, Simon Corston-Oliver, and Eric Ringger. 2005. Pulse: Mining customer opinions from free text. In Advances in Intelligent Data Analysis VI, pages 121–132. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Geng</author>
<author>Yichen Yang</author>
<author>Chao Xu</author>
<author>Xian-Sheng Hua</author>
</authors>
<title>Ranking model adaptation for domainspecific search. Knowledge and Data Engineering,</title>
<date>2012</date>
<journal>IEEE Transactions on,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="11174" citStr="Geng et al. (2012)" startWordPosition="1815" endWordPosition="1818">on data set contains 32,930 users, 281,813 positive reviews, and 81,522 negative reviews, where each review is represented with 5,000 text features with TF-IDF as the feature value. Our first baseline is an instance-based adaptation method (Brighton and Mellish, 2002). The knearest neighbors of each testing review document are found from the shared training set for personalized model training. As a result, for each testing case, we are estimating an independent classification model. We denote this method as “ReTrain.” The second baseline builds on the modelbased adaptation method developed by Geng et al. (2012). For each user, it enforces the adapted model to be close to the global model via an additional L2 regularization when training the personalized model. But the full set of parameters in logistic regression need to estimated during adaptation. We denote this method as “Reg-LR.” In our experiments, all model adaptation is performed in an online fashion: we first applied the up-to-date classification model on the given testing document; evaluated the model’s performance with ground-truth; and used the feedback to update the model. Because the class distribution of our evaluation data set is high</context>
</contexts>
<marker>Geng, Yang, Xu, Hua, 2012</marker>
<rawString>Bo Geng, Yichen Yang, Chao Xu, and Xian-Sheng Hua. 2012. Ranking model adaptation for domainspecific search. Knowledge and Data Engineering, IEEE Transactions on, 24(4):745–758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3767" citStr="Hu and Liu, 2004" startWordPosition="570" endWordPosition="573">ficient personalization of opinion analysis via a transfer learning approach, and 2) the proposed solution is general and applicable to any linear model for user opinion analysis. 2 Related Work Sentiment Analysis refers to the process of identifying subjective information in source materials (Pang and Lee, 2008; Liu, 2012). Typical tasks include: 1) classifying textual documents into positive and negative polarity categories, (Dave et al., 2003; Kim and Hovy, 2004); 2) identifying textual topics and their associated opinions (Wang et al., 2010; Jo and Oh, 2011); and 3) opinion summarization (Hu and Liu, 2004; Ku et al., 2006). Approaches for these tasks focus on population-level opinion analyses, in which one model is shared across all users. Little effort has been devoted to personalized opinion analyses, where each user has a particular model, due to the absence of user769 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 769–774, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics specific opinion data for model estimation. Transfe</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Mimi Zhang</author>
<author>Kate Sobel</author>
<author>Abdur Chowdury</author>
</authors>
<title>Twitter power: Tweets as electronic word of mouth.</title>
<date>2009</date>
<journal>Journal of the American</journal>
<pages>60--11</pages>
<contexts>
<context position="1412" citStr="Jansen et al., 2009" startWordPosition="204" endWordPosition="207">Extensive experimental results on a large collection of Amazon reviews confirm our method significantly outperformed a user-independent generic opinion model as well as several state-ofthe-art transfer learning algorithms. 1 Introduction The proliferation of user-generated opinionated text data has fueled great interest in opinion analysis (Pang and Lee, 2008; Liu, 2012). Understanding opinions expressed by a population of users has value in a wide spectrum of areas, including social network analysis (Bodendorf and Kaiser, 2009), business intelligence (Gamon et al., 2005), marketing analysis (Jansen et al., 2009), personalized recommendation (Yang et al., 2013) and many more. Most of the existing opinion analysis research focuses on population-level analyses, i.e., predicting opinions based on models estimated from a collection of users. The underlying assumption is that users are homogeneous in the way they express opinions. Nevertheless, different users may use the same words to express distinct opinions. For example, the word “expensive” tends to be associated with negative sentiment in general, although some users may use it to describe their satisfaction with a product’s quality. Failure to recog</context>
</contexts>
<marker>Jansen, Zhang, Sobel, Chowdury, 2009</marker>
<rawString>Bernard J Jansen, Mimi Zhang, Kate Sobel, and Abdur Chowdury. 2009. Twitter power: Tweets as electronic word of mouth. Journal of the American society for information science and technology, 60(11):2169–2188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohan Jo</author>
<author>Alice H Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the fourth ACM international conference on Web search and data mining,</booktitle>
<pages>815--824</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3719" citStr="Jo and Oh, 2011" startWordPosition="561" endWordPosition="564">s. Our contribution is two-fold: 1) we enable efficient personalization of opinion analysis via a transfer learning approach, and 2) the proposed solution is general and applicable to any linear model for user opinion analysis. 2 Related Work Sentiment Analysis refers to the process of identifying subjective information in source materials (Pang and Lee, 2008; Liu, 2012). Typical tasks include: 1) classifying textual documents into positive and negative polarity categories, (Dave et al., 2003; Kim and Hovy, 2004); 2) identifying textual topics and their associated opinions (Wang et al., 2010; Jo and Oh, 2011); and 3) opinion summarization (Hu and Liu, 2004; Ku et al., 2006). Approaches for these tasks focus on population-level opinion analyses, in which one model is shared across all users. Little effort has been devoted to personalized opinion analyses, where each user has a particular model, due to the absence of user769 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 769–774, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics spe</context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Yohan Jo and Alice H Oh. 2011. Aspect and sentiment unification model for online review analysis. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 815– 824. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>1367</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="3621" citStr="Kim and Hovy, 2004" startWordPosition="545" endWordPosition="548">rmed a user-independent generic model as well as several state-of-the-art transfer learning algorithms. Our contribution is two-fold: 1) we enable efficient personalization of opinion analysis via a transfer learning approach, and 2) the proposed solution is general and applicable to any linear model for user opinion analysis. 2 Related Work Sentiment Analysis refers to the process of identifying subjective information in source materials (Pang and Lee, 2008; Liu, 2012). Typical tasks include: 1) classifying textual documents into positive and negative polarity categories, (Dave et al., 2003; Kim and Hovy, 2004); 2) identifying textual topics and their associated opinions (Wang et al., 2010; Jo and Oh, 2011); and 3) opinion summarization (Hu and Liu, 2004; Ku et al., 2006). Approaches for these tasks focus on population-level opinion analyses, in which one model is shared across all users. Little effort has been devoted to personalized opinion analyses, where each user has a particular model, due to the absence of user769 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pag</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th international conference on Computational Linguistics, page 1367. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lun-Wei Ku</author>
<author>Yu-Ting Liang</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Opinion extraction, summarization and tracking in news and blog corpora. In AAAI spring symposium: Computational approaches to analyzing weblogs,</title>
<date>2006</date>
<volume>volume</volume>
<pages>100107</pages>
<contexts>
<context position="3785" citStr="Ku et al., 2006" startWordPosition="574" endWordPosition="577">ation of opinion analysis via a transfer learning approach, and 2) the proposed solution is general and applicable to any linear model for user opinion analysis. 2 Related Work Sentiment Analysis refers to the process of identifying subjective information in source materials (Pang and Lee, 2008; Liu, 2012). Typical tasks include: 1) classifying textual documents into positive and negative polarity categories, (Dave et al., 2003; Kim and Hovy, 2004); 2) identifying textual topics and their associated opinions (Wang et al., 2010; Jo and Oh, 2011); and 3) opinion summarization (Hu and Liu, 2004; Ku et al., 2006). Approaches for these tasks focus on population-level opinion analyses, in which one model is shared across all users. Little effort has been devoted to personalized opinion analyses, where each user has a particular model, due to the absence of user769 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 769–774, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics specific opinion data for model estimation. Transfer Learning aims to</context>
</contexts>
<marker>Ku, Liang, Chen, 2006</marker>
<rawString>Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006. Opinion extraction, summarization and tracking in news and blog corpora. In AAAI spring symposium: Computational approaches to analyzing weblogs, volume 100107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Yiming Yang</author>
<author>Tony G Rose</author>
<author>Fan Li</author>
</authors>
<date>2004</date>
<note>Smart stopword list.</note>
<contexts>
<context position="10299" citStr="Lewis et al., 2004" startWordPosition="1673" endWordPosition="1676">ion and Baselines We used a corpus of Amazon reviews provided on Stanford SNAP website by McAuley and Leskovec. (2013). We performed simple data preprocessing: 1) annotated the reviews with ratings greater than 3 stars (out of total 5 stars) as positive, and others as negative; 2) removed duplicate reviews; 3) removed reviewers who have more than 1,000 reviews or more than 90% positive or negative reviews; 4) chronologically ordered the reviews in each user. We extracted unigrams and bigrams to construct bag-of-words feature representations for the review documents. Standard stopword removal (Lewis et al., 2004) and Porter stemming (Willett, 2006) were applied. Chi-square and information gain (Yang and Pedersen, 1997) were used for feature selection and the union of the resulting selected features are used in the final controlled vocabulary. The resulting evaluation data set contains 32,930 users, 281,813 positive reviews, and 81,522 negative reviews, where each review is represented with 5,000 text features with TF-IDF as the feature value. Our first baseline is an instance-based adaptation method (Brighton and Mellish, 2002). The knearest neighbors of each testing review document are found from the</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. 2004. Smart stopword list.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1165" citStr="Liu, 2012" startWordPosition="169" endWordPosition="170"> sentiment classification models for each user with limited data. In this paper, we adopt a modelbased transfer learning solution – using linear transformations over the parameters of a generic model – for personalized opinion analysis. Extensive experimental results on a large collection of Amazon reviews confirm our method significantly outperformed a user-independent generic opinion model as well as several state-ofthe-art transfer learning algorithms. 1 Introduction The proliferation of user-generated opinionated text data has fueled great interest in opinion analysis (Pang and Lee, 2008; Liu, 2012). Understanding opinions expressed by a population of users has value in a wide spectrum of areas, including social network analysis (Bodendorf and Kaiser, 2009), business intelligence (Gamon et al., 2005), marketing analysis (Jansen et al., 2009), personalized recommendation (Yang et al., 2013) and many more. Most of the existing opinion analysis research focuses on population-level analyses, i.e., predicting opinions based on models estimated from a collection of users. The underlying assumption is that users are homogeneous in the way they express opinions. Nevertheless, different users may</context>
<context position="3476" citStr="Liu, 2012" startWordPosition="524" endWordPosition="525">pirical evaluations on a large collection of Amazon reviews verify the effectiveness of the proposed solution: it significantly outperformed a user-independent generic model as well as several state-of-the-art transfer learning algorithms. Our contribution is two-fold: 1) we enable efficient personalization of opinion analysis via a transfer learning approach, and 2) the proposed solution is general and applicable to any linear model for user opinion analysis. 2 Related Work Sentiment Analysis refers to the process of identifying subjective information in source materials (Pang and Lee, 2008; Liu, 2012). Typical tasks include: 1) classifying textual documents into positive and negative polarity categories, (Dave et al., 2003; Kim and Hovy, 2004); 2) identifying textual topics and their associated opinions (Wang et al., 2010; Jo and Oh, 2011); and 3) opinion summarization (Hu and Liu, 2004; Ku et al., 2006). Approaches for these tasks focus on population-level opinion analyses, in which one model is shared across all users. Little effort has been devoted to personalized opinion analyses, where each user has a particular model, due to the absence of user769 Proceedings of the 53rd Annual Meeti</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian McAuley</author>
<author>Jure Leskovec</author>
</authors>
<title>Hidden factors and hidden topics: understanding rating dimensions with review text.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th ACM conference on Recommender systems,</booktitle>
<pages>165--172</pages>
<publisher>ACM.</publisher>
<marker>McAuley, Leskovec, 2013</marker>
<rawString>Julian McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics: understanding rating dimensions with review text. In Proceedings of the 7th ACM conference on Recommender systems, pages 165–172. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Qiang Yang</author>
</authors>
<title>A survey on transfer learning.</title>
<date>2010</date>
<journal>Knowledge and Data Engineering, IEEE Transactions on,</journal>
<volume>22</volume>
<issue>10</issue>
<contexts>
<context position="4492" citStr="Pan and Yang, 2010" startWordPosition="679" endWordPosition="682">del is shared across all users. Little effort has been devoted to personalized opinion analyses, where each user has a particular model, due to the absence of user769 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 769–774, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics specific opinion data for model estimation. Transfer Learning aims to help improve predictive models by using knowledge from different but related problems (Pan and Yang, 2010). In the opinion mining community, transfer learning is used primarily for domain adaptation. Blitzer et al. (2006) proposed structural correspondence learning to identify the correspondences among features between different domains via the concept of pivot features. Pan et al. (2010) propose a spectral feature alignment algorithm to align domainspecific sentiment words from different domains for sentiment categorization. By assuming that users tend to express consistent opinions towards the same topic over time, Guerra et al. (2011) applied instance-based transfer learning for real time senti</context>
</contexts>
<marker>Pan, Yang, 2010</marker>
<rawString>Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. Knowledge and Data Engineering, IEEE Transactions on, 22(10):1345–1359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Xiaochuan Ni</author>
<author>Jian-Tao Sun</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
</authors>
<title>Cross-domain sentiment classification via spectral feature alignment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>751--760</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4777" citStr="Pan et al. (2010)" startWordPosition="719" endWordPosition="722">Conference on Natural Language Processing (Short Papers), pages 769–774, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics specific opinion data for model estimation. Transfer Learning aims to help improve predictive models by using knowledge from different but related problems (Pan and Yang, 2010). In the opinion mining community, transfer learning is used primarily for domain adaptation. Blitzer et al. (2006) proposed structural correspondence learning to identify the correspondences among features between different domains via the concept of pivot features. Pan et al. (2010) propose a spectral feature alignment algorithm to align domainspecific sentiment words from different domains for sentiment categorization. By assuming that users tend to express consistent opinions towards the same topic over time, Guerra et al. (2011) applied instance-based transfer learning for real time sentiment analysis. Our method is inspired by a personalized ranking model adaptation method developed by Wang et al. (2013). To the best of our knowledge, our work is the first to estimate user-level classifiers for opinion analysis. By adapting a generic opinion classification model for </context>
</contexts>
<marker>Pan, Ni, Sun, Yang, Chen, 2010</marker>
<rawString>Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang, and Zheng Chen. 2010. Cross-domain sentiment classification via spectral feature alignment. In Proceedings of the 19th international conference on World wide web, pages 751–760. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and trends in information retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="1153" citStr="Pang and Lee, 2008" startWordPosition="165" endWordPosition="168">estimate independent sentiment classification models for each user with limited data. In this paper, we adopt a modelbased transfer learning solution – using linear transformations over the parameters of a generic model – for personalized opinion analysis. Extensive experimental results on a large collection of Amazon reviews confirm our method significantly outperformed a user-independent generic opinion model as well as several state-ofthe-art transfer learning algorithms. 1 Introduction The proliferation of user-generated opinionated text data has fueled great interest in opinion analysis (Pang and Lee, 2008; Liu, 2012). Understanding opinions expressed by a population of users has value in a wide spectrum of areas, including social network analysis (Bodendorf and Kaiser, 2009), business intelligence (Gamon et al., 2005), marketing analysis (Jansen et al., 2009), personalized recommendation (Yang et al., 2013) and many more. Most of the existing opinion analysis research focuses on population-level analyses, i.e., predicting opinions based on models estimated from a collection of users. The underlying assumption is that users are homogeneous in the way they express opinions. Nevertheless, differe</context>
<context position="3464" citStr="Pang and Lee, 2008" startWordPosition="520" endWordPosition="523">he generic model. Empirical evaluations on a large collection of Amazon reviews verify the effectiveness of the proposed solution: it significantly outperformed a user-independent generic model as well as several state-of-the-art transfer learning algorithms. Our contribution is two-fold: 1) we enable efficient personalization of opinion analysis via a transfer learning approach, and 2) the proposed solution is general and applicable to any linear model for user opinion analysis. 2 Related Work Sentiment Analysis refers to the process of identifying subjective information in source materials (Pang and Lee, 2008; Liu, 2012). Typical tasks include: 1) classifying textual documents into positive and negative polarity categories, (Dave et al., 2003; Kim and Hovy, 2004); 2) identifying textual topics and their associated opinions (Wang et al., 2010; Jo and Oh, 2011); and 3) opinion summarization (Hu and Liu, 2004; Ku et al., 2006). Approaches for these tasks focus on population-level opinion analyses, in which one model is shared across all users. Little effort has been devoted to personalized opinion analyses, where each user has a particular model, due to the absence of user769 Proceedings of the 53rd </context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6352" citStr="Pang et al., 2002" startWordPosition="976" endWordPosition="979"> his/her generated textual documents Du ={xd, yd}|D| d=1, where xd is the feature vector for document d, yd is the sentiment class label (e.g., positive v.s., negative). To achieve so, we assume that such adaptation can be performed via a series of linear transformations on fs(x)’s model parameter ws. This assumption is general and can be applied to a wide variety of sentiment classifiers, e.g., logistic regression and linear support vector machines, as long as they have a linear core function. Therefore, we name our proposed method as LinAdapt. In this paper, we focus on logistic regression (Pang et al., 2002); but the proposed procedures can be easily adopted for many other classifiers (Wang et al., 2013). Our global model y =fs(x) can be written as, 1 Ps(yd = 1|xd) = (1) 1 + e−wsTxd where ws are the linear coefficients for the corresponding document features. Standard linear transformations, i.e., scaling, shifting and rotation, can be encoded via a V x (V + 1) matrix Au for each user u as: 0 ... ... ... 0 0 ... ... aug(V ) bug(V ) where V is the total number of features. However, the above transformation introduces O(V 2) free parameters, which are even more than the number of free parameters re</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongning Wang</author>
<author>Yue Lu</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Latent aspect rating analysis on review text data: a rating regression approach.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>783--792</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3701" citStr="Wang et al., 2010" startWordPosition="557" endWordPosition="560"> learning algorithms. Our contribution is two-fold: 1) we enable efficient personalization of opinion analysis via a transfer learning approach, and 2) the proposed solution is general and applicable to any linear model for user opinion analysis. 2 Related Work Sentiment Analysis refers to the process of identifying subjective information in source materials (Pang and Lee, 2008; Liu, 2012). Typical tasks include: 1) classifying textual documents into positive and negative polarity categories, (Dave et al., 2003; Kim and Hovy, 2004); 2) identifying textual topics and their associated opinions (Wang et al., 2010; Jo and Oh, 2011); and 3) opinion summarization (Hu and Liu, 2004; Ku et al., 2006). Approaches for these tasks focus on population-level opinion analyses, in which one model is shared across all users. Little effort has been devoted to personalized opinion analyses, where each user has a particular model, due to the absence of user769 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 769–774, Beijing, China, July 26-31, 2015. c�2015 Association for Computation</context>
</contexts>
<marker>Wang, Lu, Zhai, 2010</marker>
<rawString>Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. Latent aspect rating analysis on review text data: a rating regression approach. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 783– 792. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongning Wang</author>
<author>Xiaodong He</author>
<author>Ming-Wei Chang</author>
<author>Yang Song</author>
<author>Ryen W White</author>
<author>Wei Chu</author>
</authors>
<title>Personalized ranking model adaptation for web search.</title>
<date>2013</date>
<booktitle>In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>323--332</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5211" citStr="Wang et al. (2013)" startWordPosition="785" endWordPosition="788"> et al. (2006) proposed structural correspondence learning to identify the correspondences among features between different domains via the concept of pivot features. Pan et al. (2010) propose a spectral feature alignment algorithm to align domainspecific sentiment words from different domains for sentiment categorization. By assuming that users tend to express consistent opinions towards the same topic over time, Guerra et al. (2011) applied instance-based transfer learning for real time sentiment analysis. Our method is inspired by a personalized ranking model adaptation method developed by Wang et al. (2013). To the best of our knowledge, our work is the first to estimate user-level classifiers for opinion analysis. By adapting a generic opinion classification model for each user, heterogeneity among their expressions of opinions can be captured and it help us understand users’ opinions at a finer granularity. 3 Linear Transformation Based Model Adaptation Given a generic sentiment classification model y = fs(x), we aim at finding an optimal adapted model y = fu(x) for user u, such that fu(x) best captures u’s opinion in his/her generated textual documents Du ={xd, yd}|D| d=1, where xd is the fea</context>
<context position="6450" citStr="Wang et al., 2013" startWordPosition="993" endWordPosition="996">ent d, yd is the sentiment class label (e.g., positive v.s., negative). To achieve so, we assume that such adaptation can be performed via a series of linear transformations on fs(x)’s model parameter ws. This assumption is general and can be applied to a wide variety of sentiment classifiers, e.g., logistic regression and linear support vector machines, as long as they have a linear core function. Therefore, we name our proposed method as LinAdapt. In this paper, we focus on logistic regression (Pang et al., 2002); but the proposed procedures can be easily adopted for many other classifiers (Wang et al., 2013). Our global model y =fs(x) can be written as, 1 Ps(yd = 1|xd) = (1) 1 + e−wsTxd where ws are the linear coefficients for the corresponding document features. Standard linear transformations, i.e., scaling, shifting and rotation, can be encoded via a V x (V + 1) matrix Au for each user u as: 0 ... ... ... 0 0 ... ... aug(V ) bug(V ) where V is the total number of features. However, the above transformation introduces O(V 2) free parameters, which are even more than the number of free parameters required to estimate a new logistic regression model. Following the solution proposed by Wang et al.</context>
<context position="12217" citStr="Wang et al., 2013" startWordPosition="1987" endWordPosition="1990">ing document; evaluated the model’s performance with ground-truth; and used the feedback to update the model. Because the class distribution of our evaluation data set is highly skewed (77.5% positive), it is important to evaluate the adapted models’ performance on both classes. In the following comparisons, we report the average F-1 measure of both positive and negative classes. 4.2 Comparison of Adaptation Performance First we need to estimate a global model for adaptation. A typical approach is to collect a portion of historical reviews from each user to construct a shared training corpus (Wang et al., 2013). However, this setting is problematic: it already exploits information from every user and does not reflect the reality that some (new) users might not exist when training the global model. In our experiment, we isolated a group of random users for global model training. In addition, since there are multiple categories in this review collection, such as book, movies, electronics, etc, and each user might discuss various categories, it is infeasible to balance the coverage of different categories in global model training by only selecting the users. As a result, we vary the number of reviews i</context>
<context position="14914" citStr="Wang et al. (2013)" startWordPosition="2456" endWordPosition="2459">th k = 400 (detailed feature grouping method will be discussed in the next experiment). Table 1 shows the performance of the global model and LinAdapt with respect to different training corpus size. We found that the global model converged very quickly with around 5,000 reviews, and this gives the best compromise for both positive and negative classes in both global and adaptaed model. Therefore, we will use this global model for later adaptation experiments. We then investigated the effect of feature grouping in LinAdapt. We employed the feature grouping methods of SVD and Cross developed by Wang et al. (2013). A random feature grouping method is included to validate the necessity of proper feature grouping. We varied the number of feature groups from 100 to 1000, and evaluated the adapted models using the same 10,000 testing users from the previous experiment. As shown in Table 2, Cross provided the best adaptation performance and random is the worse; a moderate group size balances performance between positive and negative classes. For the remaining experiments, we use the Cross grouping with k = 400 in LinAdapt. In this group setting, we found that the average number of features per group is 12.4</context>
</contexts>
<marker>Wang, He, Chang, Song, White, Chu, 2013</marker>
<rawString>Hongning Wang, Xiaodong He, Ming-Wei Chang, Yang Song, Ryen W White, and Wei Chu. 2013. Personalized ranking model adaptation for web search. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval, pages 323–332. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Willett</author>
</authors>
<title>The porter stemming algorithm: then and now.</title>
<date>2006</date>
<journal>Program,</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="10335" citStr="Willett, 2006" startWordPosition="1681" endWordPosition="1682">zon reviews provided on Stanford SNAP website by McAuley and Leskovec. (2013). We performed simple data preprocessing: 1) annotated the reviews with ratings greater than 3 stars (out of total 5 stars) as positive, and others as negative; 2) removed duplicate reviews; 3) removed reviewers who have more than 1,000 reviews or more than 90% positive or negative reviews; 4) chronologically ordered the reviews in each user. We extracted unigrams and bigrams to construct bag-of-words feature representations for the review documents. Standard stopword removal (Lewis et al., 2004) and Porter stemming (Willett, 2006) were applied. Chi-square and information gain (Yang and Pedersen, 1997) were used for feature selection and the union of the resulting selected features are used in the final controlled vocabulary. The resulting evaluation data set contains 32,930 users, 281,813 positive reviews, and 81,522 negative reviews, where each review is represented with 5,000 text features with TF-IDF as the feature value. Our first baseline is an instance-based adaptation method (Brighton and Mellish, 2002). The knearest neighbors of each testing review document are found from the shared training set for personalize</context>
</contexts>
<marker>Willett, 2006</marker>
<rawString>Peter Willett. 2006. The porter stemming algorithm: then and now. Program, 40(3):219–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In ICML,</booktitle>
<volume>97</volume>
<pages>412--420</pages>
<contexts>
<context position="10407" citStr="Yang and Pedersen, 1997" startWordPosition="1689" endWordPosition="1692">eskovec. (2013). We performed simple data preprocessing: 1) annotated the reviews with ratings greater than 3 stars (out of total 5 stars) as positive, and others as negative; 2) removed duplicate reviews; 3) removed reviewers who have more than 1,000 reviews or more than 90% positive or negative reviews; 4) chronologically ordered the reviews in each user. We extracted unigrams and bigrams to construct bag-of-words feature representations for the review documents. Standard stopword removal (Lewis et al., 2004) and Porter stemming (Willett, 2006) were applied. Chi-square and information gain (Yang and Pedersen, 1997) were used for feature selection and the union of the resulting selected features are used in the final controlled vocabulary. The resulting evaluation data set contains 32,930 users, 281,813 positive reviews, and 81,522 negative reviews, where each review is represented with 5,000 text features with TF-IDF as the feature value. Our first baseline is an instance-based adaptation method (Brighton and Mellish, 2002). The knearest neighbors of each testing review document are found from the shared training set for personalized model training. As a result, for each testing case, we are estimating </context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O Pedersen. 1997. A comparative study on feature selection in text categorization. In ICML, volume 97, pages 412–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingqi Yang</author>
<author>Daqing Zhang</author>
<author>Zhiyong Yu</author>
<author>Zhu Wang</author>
</authors>
<title>A sentiment-enhanced personalized location recommendation system.</title>
<date>2013</date>
<booktitle>In Proceedings of the 24th ACM Conference on Hypertext and Social Media,</booktitle>
<pages>119--128</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1461" citStr="Yang et al., 2013" startWordPosition="211" endWordPosition="214">n of Amazon reviews confirm our method significantly outperformed a user-independent generic opinion model as well as several state-ofthe-art transfer learning algorithms. 1 Introduction The proliferation of user-generated opinionated text data has fueled great interest in opinion analysis (Pang and Lee, 2008; Liu, 2012). Understanding opinions expressed by a population of users has value in a wide spectrum of areas, including social network analysis (Bodendorf and Kaiser, 2009), business intelligence (Gamon et al., 2005), marketing analysis (Jansen et al., 2009), personalized recommendation (Yang et al., 2013) and many more. Most of the existing opinion analysis research focuses on population-level analyses, i.e., predicting opinions based on models estimated from a collection of users. The underlying assumption is that users are homogeneous in the way they express opinions. Nevertheless, different users may use the same words to express distinct opinions. For example, the word “expensive” tends to be associated with negative sentiment in general, although some users may use it to describe their satisfaction with a product’s quality. Failure to recognize this difference across users will inevitably</context>
</contexts>
<marker>Yang, Zhang, Yu, Wang, 2013</marker>
<rawString>Dingqi Yang, Daqing Zhang, Zhiyong Yu, and Zhu Wang. 2013. A sentiment-enhanced personalized location recommendation system. In Proceedings of the 24th ACM Conference on Hypertext and Social Media, pages 119–128. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>