<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003192">
<title confidence="0.996546">
Jointly optimizing a two-step conditional random field model for machine
transliteration and its fast decoding algorithm
</title>
<author confidence="0.998668">
Dong Yang, Paul Dixon and Sadaoki Furui
</author>
<affiliation confidence="0.9998195">
Department of Computer Science
Tokyo Institute of Technology
</affiliation>
<address confidence="0.56164">
Tokyo 152-8552 Japan
</address>
<email confidence="0.999456">
{raymond,dixonp,furui}@furui.cs.titech.ac.jp
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999887625">
This paper presents a joint optimization
method of a two-step conditional random
field (CRF) model for machine transliter-
ation and a fast decoding algorithm for
the proposed method. Our method lies in
the category of direct orthographical map-
ping (DOM) between two languages with-
out using any intermediate phonemic map-
ping. In the two-step CRF model, the first
CRF segments an input word into chunks
and the second one converts each chunk
into one unit in the target language. In this
paper, we propose a method to jointly op-
timize the two-step CRFs and also a fast
algorithm to realize it. Our experiments
show that the proposed method outper-
forms the well-known joint source channel
model (JSCM) and our proposed fast al-
gorithm decreases the decoding time sig-
nificantly. Furthermore, combination of
the proposed method and the JSCM gives
further improvement, which outperforms
state-of-the-art results in terms of top-1 ac-
curacy.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998483076923077">
There are more than 6000 languages in the world
and 10 languages of them have more than 100 mil-
lion native speakers. With the information revolu-
tion and globalization, systems that support mul-
tiple language processing and spoken language
translation become urgent demands. The transla-
tion of named entities from alphabetic to syllabary
language is usually performed through translitera-
tion, which tries to preserve the pronunciation in
the original language.
For example, in Chinese, foreign words are
written with Chinese characters; in Japanese, for-
eign words are usually written with special char-
</bodyText>
<figure confidence="0.9076302">
Source Name Target Name Note
G o o g l e 䈋 SE English-to-Chinese
gu ge Chinese Romanized writing
G o o g l e 䉫䊷 䉫 䊦 English-to-Japanese
guu gu ru Japanese Romanized writing
</figure>
<figureCaption confidence="0.999981">
Figure 1: Transliteration examples
</figureCaption>
<bodyText confidence="0.999351454545454">
acters called Katakana; examples are given in Fig-
ure 1.
An intuitive transliteration method (Knight and
Graehl, 1998; Oh et al., 2006) is to firstly convert
a source word into phonemes, then find the corre-
sponding phonemes in the target language, and fi-
nally convert them to the target language’s written
system. There are two reasons why this method
does not work well: first, the named entities have
diverse origins and this makes the grapheme-to-
phoneme conversion very difficult; second, the
transliteration is usually not only determined by
the pronunciation, but also affected by how they
are written in the original language.
Direct orthographical mapping (DOM), which
performs the transliteration between two lan-
guages directly without using any intermediate
phonemic mapping, is recently gaining more at-
tention in the transliteration research community,
and it is also the “Standard Run” of the “NEWS
2009 Machine Transliteration Shared Task” (Li et
al., 2009). In this paper, we try to make our system
satisfy the standard evaluation condition, which
requires that the system uses the provided parallel
corpus (without pronunciation) only, and cannot
use any other bilingual or monolingual resources.
The source channel and joint source channel
models (JSCMs) (Li et al., 2004) have been pro-
posed for DOM, which try to model P (T |S) and
P(T, S) respectively, where T and S denote the
words in the target and source languages. Ekbal
et al. (2006) modified the JSCM to incorporate
different context information into the model for
</bodyText>
<page confidence="0.974413">
275
</page>
<note confidence="0.505679">
Proceedings of the ACL 2010 Conference Short Papers, pages 275–280,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999405">
Indian languages. In the “NEWS 2009 Machine
Transliteration Shared Task”, a new two-step CRF
model for transliteration task has been proposed
(Yang et al., 2009), in which the first step is to
segment a word in the source language into char-
acter chunks and the second step is to perform a
context-dependent mapping from each chunk into
one written unit in the target language.
In this paper, we propose to jointly optimize a
two-step CRF model. We also propose a fast de-
coding algorithm to speed up the joint search. The
rest of this paper is organized as follows: Sec-
tion 2 explains the two-step CRF method, fol-
lowed by Section 3 which describes our joint opti-
mization method and its fast decoding algorithm;
Section 4 introduces a rapid implementation of a
JSCM system in the weighted finite state trans-
ducer (WFST) framework; and the last section
reports the experimental results and conclusions.
Although our method is language independent, we
use an English-to-Chinese transliteration task in
all the explanations and experiments.
</bodyText>
<sectionHeader confidence="0.992488" genericHeader="method">
2 Two-step CRF method
</sectionHeader>
<subsectionHeader confidence="0.854524">
2.1 CRF introduction
</subsectionHeader>
<bodyText confidence="0.929740222222222">
A chain-CRF (Lafferty et al., 2001) is an undi-
rected graphical model which assigns a probability
to a label sequence L = l1l2 ... lT, given an input
sequence C = c1c2 ... cT. CRF training is usually
performed through the L-BFGS algorithm (Wal-
lach, 2002) and decoding is performed by the
Viterbi algorithm. We formalize machine translit-
eration as a CRF tagging problem, as shown in
Figure 2.
</bodyText>
<listItem confidence="0.997144">
• Single unit features: C−2, C−1, C0, C1, C2
• Combination features: C−1C0, C0C1
</listItem>
<bodyText confidence="0.999570857142857">
Here, C0 is the current character, C−1 and C1 de-
note the previous and next characters, and C−2 and
C2 are the characters located two positions to the
left and right of C0.
One limitation of their work is that only top-1
segmentation is output to the following CRF con-
verter.
</bodyText>
<subsectionHeader confidence="0.999544">
2.3 CRF converter
</subsectionHeader>
<bodyText confidence="0.984141">
Similar to the CRF segmenter, the CRF converter
has the format shown in Figure 2.
For this CRF, Yang et al. (2009) used the fol-
lowing features:
</bodyText>
<listItem confidence="0.937025">
• Single unit features: CK−1, CK0, CK1
• Combination features: CK−1CK0,
CK0CK1
</listItem>
<bodyText confidence="0.957193666666667">
where CK represents the source language chunk,
and the subscript notation is the same as the CRF
segmenter.
</bodyText>
<sectionHeader confidence="0.9749575" genericHeader="method">
3 Joint optimization and its fast decoding
algorithm
</sectionHeader>
<subsectionHeader confidence="0.999851">
3.1 Joint optimization
</subsectionHeader>
<bodyText confidence="0.99737625">
We denote a word in the source language by S, a
segmentation of S by A, and a word in the target
langauge by T. Our goal is to find the best word T�
in the target language which maximizes the prob-
ability P(T|S).
Yang et al. (2009) used only the best segmen-
tation in the first CRF and the best output in the
second CRF, which is equivalent to
</bodyText>
<equation confidence="0.9360476">
A� = arg max P(A|S)
A
T i m o t h y 3% A
T/B i/N m/B o/N t/B h/N y/N
Ti/ mo/3% thy/A
</equation>
<figureCaption confidence="0.97846">
Figure 2: An pictorial description of a CRF seg- T = arg max P(T |S, A), (1)
menter and a CRF converter T
</figureCaption>
<subsectionHeader confidence="0.999048">
2.2 CRF segmenter
</subsectionHeader>
<bodyText confidence="0.999712466666667">
In the CRF, a feature function describes a co-
occurrence relation, and it is usually a binary func-
tion, taking the value 1 when both an observa-
tion and a label transition are observed. Yang et
al. (2009) used the following features in the seg-
mentation tool:
where P(A|S) and P(T|S,A) represent two
CRFs respectively. This method considers the seg-
mentation and the conversion as two independent
steps. A major limitation is that, if the segmenta-
tion from the first step is wrong, the error propa-
gates to the second step, and the error is very dif-
ficult to recover.
In this paper, we propose a new method to
jointly optimize the two-step CRF, which can be
</bodyText>
<page confidence="0.932412">
276
</page>
<equation confidence="0.951263666666667">
written as:
T� = arg max P(T|S)
T
J:
= arg max P(T, A|S)
T A
= arg maxJ:P(A|S)P(T|S,A)
T A
(2)
</equation>
<bodyText confidence="0.999550166666667">
The joint optimization considers all the segmen-
tation possibilities and sums the probability over
all the alternative segmentations which generate
the same output. It considers the segmentation and
conversion in a unified framework and is robust to
segmentation errors.
</bodyText>
<subsectionHeader confidence="0.991867">
3.2 N-best approximation
</subsectionHeader>
<bodyText confidence="0.999975576923077">
In the process of finding the best output using
Equation 2, a dynamic programming algorithm for
joint decoding of the segmentation and conversion
is possible, but the implementation becomes very
complicated. Another direction is to divide the de-
coding into two steps of segmentation and conver-
sion, which is this paper’s method. However, exact
inference by listing all possible candidates explic-
itly and summing over all possible segmentations
is intractable, because of the exponential computa-
tion complexity with the source word’s increasing
length.
In the segmentation step, the number of possible
segmentations is 2N, where N is the length of the
source word and 2 is the size of the tagging set. In
the conversion step, the number of possible candi-
dates is MN′, where N′ is the number of chunks
from the 1st step and M is the size of the tagging
set. M is usually large, e.g., about 400 in Chinese
and 50 in Japanese, and it is impossible to list all
the candidates.
Our analysis shows that beyond the 10th candi-
date, almost all the probabilities of the candidates
in both steps drop below 0.01. Therefore we de-
cided to generate top-10 results for both steps to
approximate the Equation 2.
</bodyText>
<subsectionHeader confidence="0.997138">
3.3 Fast decoding algorithm
</subsectionHeader>
<bodyText confidence="0.998832863636364">
As introduced in the previous subsection, in the
whole decoding process we have to perform n-best
CRF decoding in the segmentation step and 10 n-
best CRF decoding in the second CRF. Is it really
necessary to perform the second CRF for all the
segmentations? The answer is “No” for candidates
with low probabilities. Here we propose a no-loss
fast decoding algorithm for deciding when to stop
performing the second CRF decoding.
Suppose we have a list of segmentation candi-
dates which are generated by the 1st CRF, ranked
by probabilities P(A|S) in descending order A :
A1, A2,..., AN and we are performing the 2nd
CRF decoding starting from A1. Up to Ak,
we get a list of candidates T : T1, T2, ..., TL,
ranked by probabilities in descending order. If
we can guarantee that, even performing the 2nd
CRF decoding for all the remaining segmentations
Ak+1, Ak+2, ..., AN, the top 1 candidate does not
change, then we can stop decoding.
We can show that the following formula is the
stop condition:
</bodyText>
<equation confidence="0.983434333333333">
k
Pk(T1|S) − Pk(T2|S) &gt; 1 − P(Aj|S). (3)
j=1
</equation>
<bodyText confidence="0.999590636363636">
The meaning of this formula is that the prob-
ability of all the remaining candidates is smaller
than the probability difference between the best
and the second best candidates; on the other hand,
even if all the remaining probabilities are added to
the second best candidate, it still cannot overturn
the top candidate. The mathematical proof is pro-
vided in Appendix A.
The stop condition here has no approximation
nor pre-defined assumption, and it is a no-loss fast
decoding algorithm.
</bodyText>
<sectionHeader confidence="0.979333" genericHeader="method">
4 Rapid development of a JSCM system
</sectionHeader>
<bodyText confidence="0.964828333333333">
The JSCM represents how the source words and
target names are generated simultaneously (Li et
al., 2004):
</bodyText>
<equation confidence="0.89636175">
P(S, T) = P(s1, s2, ..., sk, t1, t2, ..., tk)
= P(&lt; s, t &gt;1, &lt; s, t &gt;2, ..., &lt; s, t &gt;k)
P(&lt; s, t &gt;k  |&lt; s, t &gt;k−1
1 ) (4)
</equation>
<bodyText confidence="0.999899125">
where S = (s1, s2, ..., sk) is a word in the source
langauge and T = (t1, t2, ..., tk) is a word in the
target language.
The training parallel data without alignment is
first aligned by a Viterbi version EM algorithm (Li
et al., 2004).
The decoding problem in JSCM can be written
as:
</bodyText>
<equation confidence="0.9984868">
T = arg max P(S, T). (5)
T
K
= H
k=1
</equation>
<page confidence="0.981974">
277
</page>
<bodyText confidence="0.999819785714286">
After the alignments are generated, we use the
MITLM toolkit (Hsu and Glass, 2008) to build a
trigram model with modified Kneser-Ney smooth-
ing. We then convert the n-gram to a WFST
M (Sproat et al., 2000; Caseiro et al., 2002). To al-
low transliteration from a sequence of characters,
a second WFST T is constructed. The input word
is converted to an acceptor I, and it is then com-
bined with T and M according to O = I o T o M
where o denotes the composition operator. The
n–best paths are extracted by projecting the out-
put, removing the epsilon labels and applying the
n-shortest paths algorithm with determinization in
the OpenFst Toolkit (Allauzen et al., 2007).
</bodyText>
<sectionHeader confidence="0.999791" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9998715">
We use several metrics from (Li et al., 2009) to
measure the performance of our system.
</bodyText>
<listItem confidence="0.998969285714286">
1. Top-1 ACC: word accuracy of the top-1 can-
didate
2. Mean F-score: fuzziness in the top-1 candi-
date, how close the top-1 candidate is to the refer-
ence
3. MRR: mean reciprocal rank, 1/MRR tells ap-
proximately the average rank of the correct result
</listItem>
<subsectionHeader confidence="0.984346">
5.1 Comparison with the baseline and JSCM
</subsectionHeader>
<bodyText confidence="0.99915425">
We use the training, development and test sets of
NEWS 2009 data for English-to-Chinese in our
experiments as detailed in Table 1. This is a paral-
lel corpus without alignment.
</bodyText>
<table confidence="0.992521">
Training data Development data Test data
31961 2896 2896
</table>
<tableCaption confidence="0.99986">
Table 1: Corpus size (number of word pairs)
</tableCaption>
<bodyText confidence="0.9983363">
We compare the proposed decoding method
with the baseline which uses only the best candi-
dates in both CRF steps, and also with the well
known JSCM. As we can see in Table 2, the pro-
posed method improves the baseline top-1 ACC
from 0.670 to 0.708, and it works as well as, or
even better than the well known JSCM in all the
three measurements.
Our experiments also show that the decoding
time can be reduced significantly via using our fast
decoding algorithm. As we have explained, with-
out fast decoding, we need 11 CRF n-best decod-
ing for each word; the number can be reduced to
3.53 (1 “the first CRF”+2.53 “the second CRF”)
via the fast decoding algorithm.
We should notice that the decoding time is sig-
nificantly shorter than the training time. While
testing takes minutes on a normal PC, the train-
ing of the CRF converter takes up to 13 hours on
an 8-core (8*3G Hz) server.
</bodyText>
<table confidence="0.9983828">
Measure Top-1 Mean MRR
ACC F-score
Baseline 0.670 0.869 0.750
Joint optimization 0.708 0.885 0.789
JSCM 0.706 0.882 0.789
</table>
<tableCaption confidence="0.9710025">
Table 2: Comparison of the proposed decoding
method with the previous method and the JSCM
</tableCaption>
<subsectionHeader confidence="0.995903">
5.2 Further improvement
</subsectionHeader>
<bodyText confidence="0.999869833333333">
We tried to combine the two-step CRF model and
the JSCM. From the two-step CRF model we get
the conditional probability PCRF (T |S) and from
the JSCM we get the joint probability P(S, T).
The conditional probability of PJSCM(T |S) can
be calculuated as follows:
</bodyText>
<equation confidence="0.9908772">
P(T, S) P (T, S)
PJSCM(T |S) = P (S) = �T P (T, S). (6)
They are used in our combination method as:
P(T|S) = λPCRF(T|S) + (1 − λ)PJSCM(T|S)
(7)
</equation>
<bodyText confidence="0.9973726">
where λ denotes the interpolation weight (λ is set
by development data in this paper).
As we can see in Table 3, the linear combination
of two sytems further improves the top-1 ACC to
0.720, and it has outperformed the best reported
“Standard Run” (Li et al., 2009) result 0.717. (The
reported best “Standard Run” result 0.731 used
target language phoneme information, which re-
quires a monolingual dictionary; as a result it is
not a standard run.)
</bodyText>
<table confidence="0.999536857142857">
Measure Top-1 Mean MRR
ACC F-score
Baseline+JSCM 0.713 0.883 0.794
Joint optimization 0.720 0.888 0.797
+ JSCM
state-of-the-art 0.717 0.890 0.785
(Li et al., 2009)
</table>
<tableCaption confidence="0.998287">
Table 3: Model combination results
</tableCaption>
<sectionHeader confidence="0.938849" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.834895333333333">
In this paper we have presented our new joint
optimization method for a two-step CRF model
and its fast decoding algorithm. The proposed
</bodyText>
<page confidence="0.99114">
278
</page>
<bodyText confidence="0.999960214285714">
method improved the system significantly and out-
performed the JSCM. Combining the proposed
method with JSCM, the performance was further
improved.
In future work we are planning to combine our
system with multilingual systems. Also we want
to make use of acoustic information in machine
transliteration. We are currently investigating dis-
criminative training as a method to further im-
prove the JSCM. Another issue of our two-step
CRF method is that the training complexity in-
creases quadratically according to the size of the
label set, and how to reduce the training time needs
more research.
</bodyText>
<sectionHeader confidence="0.988568" genericHeader="acknowledgments">
Appendix A. Proof of Equation 3
</sectionHeader>
<bodyText confidence="0.8958025">
The CRF segmentation provides a list of segmen-
tations: A : A1, A2,..., AN, with conditional
probabilities P(A1|S), P(A2|S), ..., P(AN |S).
If Equation 3 holds, then for Vi =� 1,
</bodyText>
<equation confidence="0.990674789473684">
k
Pk(T1|S) &gt; Pk(T2|S) + (1 − P(Aj|S))
j=1
k
&gt; Pk(Ti|S) + (1 − P(Aj|S))
j=1
N
= Pk(Ti|S) + P(Aj|S)
j=k+1
&gt; Pk(Ti|S)
N
+ P(Aj|S)P(Ti|S, Aj)
j=k+1
= P(Ti|S) (9)
Therefore, P(T1|S) &gt; P(Ti|S)(i =� 1), and T1
maximizes the probability P(T |S).
N
P(Aj|S) = 1.
j=1
</equation>
<bodyText confidence="0.832636454545455">
The CRF conversion, given a segmenta-
tion Ai, provides a list of transliteration out-
put T1, T2, ..., TM, with conditional probabilities
P(T1|S, Ai),P(T2|S, Ai),...,P(TM |S, Ai).
In our fast decoding algorithm, we start per-
forming the CRF conversion from A1, then A2,
and then A3, etc. Up to Ak, we get a list of can-
didates T : T1, T2,..., TL, ranked by probabili-
ties Pk(T |S) in descending order. The probability
Pk(Ti|S)(l = 1, 2, ..., L) is accumulated probabil-
ity of P(Ti|S) over A1, A2,..., Ak, calculated by:
</bodyText>
<equation confidence="0.976709666666667">
k
Pk(T�|S) = P(Aj|S)P(Ti|S, Aj)
j=1
</equation>
<bodyText confidence="0.999648333333333">
If we continue performing the CRF conversion
to cover all N (N &gt; k) segmentations, eventually
we will get:
</bodyText>
<equation confidence="0.969840142857143">
N
P(T�|S) = P(Aj|S)P(Ti|S, Aj)
j=1
k
&gt; P(Aj|S)P(Ti|S, Aj)
j=1
= Pk(T�|S) (8)
</equation>
<page confidence="0.997227">
279
</page>
<sectionHeader confidence="0.998342" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999874666666667">
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut and Mehryar Mohri 2007. OpenFst: A
General and Efficient Weighted Finite-State Trans-
ducer Library. Proceedings of the Ninth Interna-
tional Conference on Implementation and Applica-
tion of Automata, (CIAA), pages 11-23.
Diamantino Caseiro, Isabel Trancosoo, Luis Oliveira
and Ceu Viana 2002. Grapheme-to-phone using fi-
nite state transducers. Proceedings IEEE Workshop
on Speech Synthesis.
Asif Ekbal, Sudip Kumar Naskar and Sivaji Bandy-
opadhyay. 2006. A modified joint source-channel
model for transliteration, Proceedings of the COL-
ING/ACL, pages 191-198.
Bo-June Hsu and James Glass 2008. Iterative Lan-
guage Model Estimation: Efficient Data Structure
&amp; Algorithms. Proceedings Interspeech, pages 841-
844.
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration, Association for Computational Lin-
guistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data., Proceedings of International Confer-
ence on Machine Learning, pages 282-289.
Haizhou Li, Min Zhang and Jian Su. 2004. A joint
source-channel model for machine transliteration,
Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics.
Haizhou Li, A. Kumaran, Vladimir Pervouchine and
Min Zhang 2009. Report of NEWS 2009 Ma-
chine Transliteration Shared Task, Proceedings of
the 2009 Named Entities Workshop: Shared Task on
Transliteration (NEWS 2009), pages 1-18
Jong-Hoon Oh, Key-Sun Choi and Hitoshi Isahara.
2006. A comparison of different machine transliter-
ation models, Journal of Artificial Intelligence Re-
search, 27, pages 119-151.
Richard Sproat 2000. Corpus-Based Methods and
Hand-Built Methods. Proceedings of International
Conference on Spoken Language Processing, pages
426-428.
Andrew J. Viterbi 1967. Error Bounds for Convolu-
tional Codes and an Asymptotically Optimum De-
coding Algorithm. IEEE Transactions on Informa-
tion Theory, Volume IT-13, pages 260-269.
Hanna Wallach 2002. Efficient Training of Condi-
tional Random Fields. M. Thesis, University of Ed-
inburgh.
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oon-
ishi, Masanobu Nakamura and Sadaoki Furui 2009.
Combining a Two-step Conditional Random Field
Model and a Joint Source Channel Model for Ma-
chine Transliteration, Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009), pages 72-75
</reference>
<page confidence="0.997122">
280
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.705151">
<title confidence="0.9954865">Jointly optimizing a two-step conditional random field model for machine transliteration and its fast decoding algorithm</title>
<author confidence="0.999634">Dong Yang</author>
<author confidence="0.999634">Paul Dixon</author>
<author confidence="0.999634">Sadaoki Furui</author>
<affiliation confidence="0.99997">Department of Computer Science Tokyo Institute of Technology</affiliation>
<address confidence="0.956666">Tokyo 152-8552 Japan</address>
<abstract confidence="0.98967464">This paper presents a joint optimization method of a two-step conditional random field (CRF) model for machine transliteration and a fast decoding algorithm for the proposed method. Our method lies in the category of direct orthographical mapping (DOM) between two languages without using any intermediate phonemic mapping. In the two-step CRF model, the first CRF segments an input word into chunks and the second one converts each chunk into one unit in the target language. In this paper, we propose a method to jointly optimize the two-step CRFs and also a fast algorithm to realize it. Our experiments show that the proposed method outperforms the well-known joint source channel model (JSCM) and our proposed fast algorithm decreases the decoding time significantly. Furthermore, combination of the proposed method and the JSCM gives further improvement, which outperforms state-of-the-art results in terms of top-1 accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
</authors>
<title>Wojciech Skut and Mehryar Mohri</title>
<date>2007</date>
<booktitle>Proceedings of the Ninth International Conference on Implementation and Application of Automata, (CIAA),</booktitle>
<pages>11--23</pages>
<contexts>
<context position="11595" citStr="Allauzen et al., 2007" startWordPosition="1991" endWordPosition="1994"> the MITLM toolkit (Hsu and Glass, 2008) to build a trigram model with modified Kneser-Ney smoothing. We then convert the n-gram to a WFST M (Sproat et al., 2000; Caseiro et al., 2002). To allow transliteration from a sequence of characters, a second WFST T is constructed. The input word is converted to an acceptor I, and it is then combined with T and M according to O = I o T o M where o denotes the composition operator. The n–best paths are extracted by projecting the output, removing the epsilon labels and applying the n-shortest paths algorithm with determinization in the OpenFst Toolkit (Allauzen et al., 2007). 5 Experiments We use several metrics from (Li et al., 2009) to measure the performance of our system. 1. Top-1 ACC: word accuracy of the top-1 candidate 2. Mean F-score: fuzziness in the top-1 candidate, how close the top-1 candidate is to the reference 3. MRR: mean reciprocal rank, 1/MRR tells approximately the average rank of the correct result 5.1 Comparison with the baseline and JSCM We use the training, development and test sets of NEWS 2009 data for English-to-Chinese in our experiments as detailed in Table 1. This is a parallel corpus without alignment. Training data Development data </context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut and Mehryar Mohri 2007. OpenFst: A General and Efficient Weighted Finite-State Transducer Library. Proceedings of the Ninth International Conference on Implementation and Application of Automata, (CIAA), pages 11-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diamantino Caseiro</author>
</authors>
<title>Isabel Trancosoo, Luis Oliveira and Ceu</title>
<date>2002</date>
<booktitle>Proceedings IEEE Workshop on Speech Synthesis.</booktitle>
<location>Viana</location>
<marker>Caseiro, 2002</marker>
<rawString>Diamantino Caseiro, Isabel Trancosoo, Luis Oliveira and Ceu Viana 2002. Grapheme-to-phone using finite state transducers. Proceedings IEEE Workshop on Speech Synthesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asif Ekbal</author>
</authors>
<title>Sudip Kumar Naskar and Sivaji Bandyopadhyay.</title>
<date>2006</date>
<booktitle>Proceedings of the COLING/ACL,</booktitle>
<pages>191--198</pages>
<marker>Ekbal, 2006</marker>
<rawString>Asif Ekbal, Sudip Kumar Naskar and Sivaji Bandyopadhyay. 2006. A modified joint source-channel model for transliteration, Proceedings of the COLING/ACL, pages 191-198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo-June Hsu</author>
<author>James Glass</author>
</authors>
<title>Iterative Language Model Estimation: Efficient Data Structure &amp; Algorithms.</title>
<date>2008</date>
<booktitle>Proceedings Interspeech,</booktitle>
<pages>841--844</pages>
<contexts>
<context position="11013" citStr="Hsu and Glass, 2008" startWordPosition="1885" endWordPosition="1888">sents how the source words and target names are generated simultaneously (Li et al., 2004): P(S, T) = P(s1, s2, ..., sk, t1, t2, ..., tk) = P(&lt; s, t &gt;1, &lt; s, t &gt;2, ..., &lt; s, t &gt;k) P(&lt; s, t &gt;k |&lt; s, t &gt;k−1 1 ) (4) where S = (s1, s2, ..., sk) is a word in the source langauge and T = (t1, t2, ..., tk) is a word in the target language. The training parallel data without alignment is first aligned by a Viterbi version EM algorithm (Li et al., 2004). The decoding problem in JSCM can be written as: T = arg max P(S, T). (5) T K = H k=1 277 After the alignments are generated, we use the MITLM toolkit (Hsu and Glass, 2008) to build a trigram model with modified Kneser-Ney smoothing. We then convert the n-gram to a WFST M (Sproat et al., 2000; Caseiro et al., 2002). To allow transliteration from a sequence of characters, a second WFST T is constructed. The input word is converted to an acceptor I, and it is then combined with T and M according to O = I o T o M where o denotes the composition operator. The n–best paths are extracted by projecting the output, removing the epsilon labels and applying the n-shortest paths algorithm with determinization in the OpenFst Toolkit (Allauzen et al., 2007). 5 Experiments We</context>
</contexts>
<marker>Hsu, Glass, 2008</marker>
<rawString>Bo-June Hsu and James Glass 2008. Iterative Language Model Estimation: Efficient Data Structure &amp; Algorithms. Proceedings Interspeech, pages 841-844.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<institution>Machine Transliteration, Association for Computational Linguistics.</institution>
<contexts>
<context position="2167" citStr="Knight and Graehl, 1998" startWordPosition="334" endWordPosition="337">tities from alphabetic to syllabary language is usually performed through transliteration, which tries to preserve the pronunciation in the original language. For example, in Chinese, foreign words are written with Chinese characters; in Japanese, foreign words are usually written with special charSource Name Target Name Note G o o g l e 䈋 SE English-to-Chinese gu ge Chinese Romanized writing G o o g l e 䉫䊷 䉫 䊦 English-to-Japanese guu gu ru Japanese Romanized writing Figure 1: Transliteration examples acters called Katakana; examples are given in Figure 1. An intuitive transliteration method (Knight and Graehl, 1998; Oh et al., 2006) is to firstly convert a source word into phonemes, then find the corresponding phonemes in the target language, and finally convert them to the target language’s written system. There are two reasons why this method does not work well: first, the named entities have diverse origins and this makes the grapheme-tophoneme conversion very difficult; second, the transliteration is usually not only determined by the pronunciation, but also affected by how they are written in the original language. Direct orthographical mapping (DOM), which performs the transliteration between two </context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine Transliteration, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.,</title>
<date>2001</date>
<booktitle>Proceedings of International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="4858" citStr="Lafferty et al., 2001" startWordPosition="767" endWordPosition="770">eed up the joint search. The rest of this paper is organized as follows: Section 2 explains the two-step CRF method, followed by Section 3 which describes our joint optimization method and its fast decoding algorithm; Section 4 introduces a rapid implementation of a JSCM system in the weighted finite state transducer (WFST) framework; and the last section reports the experimental results and conclusions. Although our method is language independent, we use an English-to-Chinese transliteration task in all the explanations and experiments. 2 Two-step CRF method 2.1 CRF introduction A chain-CRF (Lafferty et al., 2001) is an undirected graphical model which assigns a probability to a label sequence L = l1l2 ... lT, given an input sequence C = c1c2 ... cT. CRF training is usually performed through the L-BFGS algorithm (Wallach, 2002) and decoding is performed by the Viterbi algorithm. We formalize machine transliteration as a CRF tagging problem, as shown in Figure 2. • Single unit features: C−2, C−1, C0, C1, C2 • Combination features: C−1C0, C0C1 Here, C0 is the current character, C−1 and C1 denote the previous and next characters, and C−2 and C2 are the characters located two positions to the left and righ</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data., Proceedings of International Conference on Machine Learning, pages 282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source-channel model for machine transliteration,</title>
<date>2004</date>
<booktitle>Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3337" citStr="Li et al., 2004" startWordPosition="516" endWordPosition="519">hich performs the transliteration between two languages directly without using any intermediate phonemic mapping, is recently gaining more attention in the transliteration research community, and it is also the “Standard Run” of the “NEWS 2009 Machine Transliteration Shared Task” (Li et al., 2009). In this paper, we try to make our system satisfy the standard evaluation condition, which requires that the system uses the provided parallel corpus (without pronunciation) only, and cannot use any other bilingual or monolingual resources. The source channel and joint source channel models (JSCMs) (Li et al., 2004) have been proposed for DOM, which try to model P (T |S) and P(T, S) respectively, where T and S denote the words in the target and source languages. Ekbal et al. (2006) modified the JSCM to incorporate different context information into the model for 275 Proceedings of the ACL 2010 Conference Short Papers, pages 275–280, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Indian languages. In the “NEWS 2009 Machine Transliteration Shared Task”, a new two-step CRF model for transliteration task has been proposed (Yang et al., 2009), in which the first step is to </context>
<context position="10483" citStr="Li et al., 2004" startWordPosition="1766" endWordPosition="1769">of this formula is that the probability of all the remaining candidates is smaller than the probability difference between the best and the second best candidates; on the other hand, even if all the remaining probabilities are added to the second best candidate, it still cannot overturn the top candidate. The mathematical proof is provided in Appendix A. The stop condition here has no approximation nor pre-defined assumption, and it is a no-loss fast decoding algorithm. 4 Rapid development of a JSCM system The JSCM represents how the source words and target names are generated simultaneously (Li et al., 2004): P(S, T) = P(s1, s2, ..., sk, t1, t2, ..., tk) = P(&lt; s, t &gt;1, &lt; s, t &gt;2, ..., &lt; s, t &gt;k) P(&lt; s, t &gt;k |&lt; s, t &gt;k−1 1 ) (4) where S = (s1, s2, ..., sk) is a word in the source langauge and T = (t1, t2, ..., tk) is a word in the target language. The training parallel data without alignment is first aligned by a Viterbi version EM algorithm (Li et al., 2004). The decoding problem in JSCM can be written as: T = arg max P(S, T). (5) T K = H k=1 277 After the alignments are generated, we use the MITLM toolkit (Hsu and Glass, 2008) to build a trigram model with modified Kneser-Ney smoothing. We then </context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang and Jian Su. 2004. A joint source-channel model for machine transliteration, Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
</authors>
<title>Vladimir Pervouchine and Min Zhang</title>
<date>2009</date>
<booktitle>Report of NEWS 2009 Machine Transliteration Shared Task, Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS</booktitle>
<pages>1--18</pages>
<marker>Li, Kumaran, 2009</marker>
<rawString>Haizhou Li, A. Kumaran, Vladimir Pervouchine and Min Zhang 2009. Report of NEWS 2009 Machine Transliteration Shared Task, Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 1-18</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Key-Sun Choi</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A comparison of different machine transliteration models,</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>27</volume>
<pages>119--151</pages>
<contexts>
<context position="2185" citStr="Oh et al., 2006" startWordPosition="338" endWordPosition="341"> syllabary language is usually performed through transliteration, which tries to preserve the pronunciation in the original language. For example, in Chinese, foreign words are written with Chinese characters; in Japanese, foreign words are usually written with special charSource Name Target Name Note G o o g l e 䈋 SE English-to-Chinese gu ge Chinese Romanized writing G o o g l e 䉫䊷 䉫 䊦 English-to-Japanese guu gu ru Japanese Romanized writing Figure 1: Transliteration examples acters called Katakana; examples are given in Figure 1. An intuitive transliteration method (Knight and Graehl, 1998; Oh et al., 2006) is to firstly convert a source word into phonemes, then find the corresponding phonemes in the target language, and finally convert them to the target language’s written system. There are two reasons why this method does not work well: first, the named entities have diverse origins and this makes the grapheme-tophoneme conversion very difficult; second, the transliteration is usually not only determined by the pronunciation, but also affected by how they are written in the original language. Direct orthographical mapping (DOM), which performs the transliteration between two languages directly</context>
</contexts>
<marker>Oh, Choi, Isahara, 2006</marker>
<rawString>Jong-Hoon Oh, Key-Sun Choi and Hitoshi Isahara. 2006. A comparison of different machine transliteration models, Journal of Artificial Intelligence Research, 27, pages 119-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
</authors>
<title>Corpus-Based Methods and Hand-Built Methods.</title>
<date>2000</date>
<booktitle>Proceedings of International Conference on Spoken Language Processing,</booktitle>
<pages>426--428</pages>
<marker>Sproat, 2000</marker>
<rawString>Richard Sproat 2000. Corpus-Based Methods and Hand-Built Methods. Proceedings of International Conference on Spoken Language Processing, pages 426-428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Viterbi</author>
</authors>
<title>Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>13</volume>
<pages>260--269</pages>
<marker>Viterbi, 1967</marker>
<rawString>Andrew J. Viterbi 1967. Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm. IEEE Transactions on Information Theory, Volume IT-13, pages 260-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
</authors>
<title>Efficient Training of Conditional Random Fields.</title>
<date>2002</date>
<tech>M. Thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="5076" citStr="Wallach, 2002" startWordPosition="808" endWordPosition="810">4 introduces a rapid implementation of a JSCM system in the weighted finite state transducer (WFST) framework; and the last section reports the experimental results and conclusions. Although our method is language independent, we use an English-to-Chinese transliteration task in all the explanations and experiments. 2 Two-step CRF method 2.1 CRF introduction A chain-CRF (Lafferty et al., 2001) is an undirected graphical model which assigns a probability to a label sequence L = l1l2 ... lT, given an input sequence C = c1c2 ... cT. CRF training is usually performed through the L-BFGS algorithm (Wallach, 2002) and decoding is performed by the Viterbi algorithm. We formalize machine transliteration as a CRF tagging problem, as shown in Figure 2. • Single unit features: C−2, C−1, C0, C1, C2 • Combination features: C−1C0, C0C1 Here, C0 is the current character, C−1 and C1 denote the previous and next characters, and C−2 and C2 are the characters located two positions to the left and right of C0. One limitation of their work is that only top-1 segmentation is output to the following CRF converter. 2.3 CRF converter Similar to the CRF segmenter, the CRF converter has the format shown in Figure 2. For th</context>
</contexts>
<marker>Wallach, 2002</marker>
<rawString>Hanna Wallach 2002. Efficient Training of Conditional Random Fields. M. Thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Yang</author>
<author>Paul Dixon</author>
<author>Yi-Cheng Pan</author>
</authors>
<title>Tasuku Oonishi, Masanobu Nakamura and Sadaoki Furui 2009. Combining a Two-step Conditional Random Field Model and a Joint Source Channel Model for Machine Transliteration,</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS</booktitle>
<pages>72--75</pages>
<contexts>
<context position="3905" citStr="Yang et al., 2009" startWordPosition="608" endWordPosition="611">nt source channel models (JSCMs) (Li et al., 2004) have been proposed for DOM, which try to model P (T |S) and P(T, S) respectively, where T and S denote the words in the target and source languages. Ekbal et al. (2006) modified the JSCM to incorporate different context information into the model for 275 Proceedings of the ACL 2010 Conference Short Papers, pages 275–280, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Indian languages. In the “NEWS 2009 Machine Transliteration Shared Task”, a new two-step CRF model for transliteration task has been proposed (Yang et al., 2009), in which the first step is to segment a word in the source language into character chunks and the second step is to perform a context-dependent mapping from each chunk into one written unit in the target language. In this paper, we propose to jointly optimize a two-step CRF model. We also propose a fast decoding algorithm to speed up the joint search. The rest of this paper is organized as follows: Section 2 explains the two-step CRF method, followed by Section 3 which describes our joint optimization method and its fast decoding algorithm; Section 4 introduces a rapid implementation of a JS</context>
<context position="5702" citStr="Yang et al. (2009)" startWordPosition="920" endWordPosition="923">oding is performed by the Viterbi algorithm. We formalize machine transliteration as a CRF tagging problem, as shown in Figure 2. • Single unit features: C−2, C−1, C0, C1, C2 • Combination features: C−1C0, C0C1 Here, C0 is the current character, C−1 and C1 denote the previous and next characters, and C−2 and C2 are the characters located two positions to the left and right of C0. One limitation of their work is that only top-1 segmentation is output to the following CRF converter. 2.3 CRF converter Similar to the CRF segmenter, the CRF converter has the format shown in Figure 2. For this CRF, Yang et al. (2009) used the following features: • Single unit features: CK−1, CK0, CK1 • Combination features: CK−1CK0, CK0CK1 where CK represents the source language chunk, and the subscript notation is the same as the CRF segmenter. 3 Joint optimization and its fast decoding algorithm 3.1 Joint optimization We denote a word in the source language by S, a segmentation of S by A, and a word in the target langauge by T. Our goal is to find the best word T� in the target language which maximizes the probability P(T|S). Yang et al. (2009) used only the best segmentation in the first CRF and the best output in the </context>
</contexts>
<marker>Yang, Dixon, Pan, 2009</marker>
<rawString>Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oonishi, Masanobu Nakamura and Sadaoki Furui 2009. Combining a Two-step Conditional Random Field Model and a Joint Source Channel Model for Machine Transliteration, Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 72-75</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>