<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.958186">
A Just-In-Time Keyword Extraction from Meeting Transcripts
</title>
<author confidence="0.99598">
Hyun-Je Song Junho Go Seong-Bae Park Se-Young Park
</author>
<affiliation confidence="0.994918">
School of Computer Science and Engineering
Kyungpook National University
</affiliation>
<address confidence="0.648378">
Daegu, Korea
</address>
<email confidence="0.996826">
{hjsong,jhgo,sbpark,sypark}@sejong.knu.ac.kr
</email>
<sectionHeader confidence="0.994979" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999136705882353">
In a meeting, it is often desirable to extract
keywords from each utterance as soon as it is
spoken. Thus, this paper proposes a just-in-
time keyword extraction from meeting tran-
scripts. The proposed method considers two
major factors that make it different from key-
word extraction from normal texts. The first
factor is the temporal history of preceding ut-
terances that grants higher importance to re-
cent utterances than old ones, and the sec-
ond is topic relevance that forces only the pre-
ceding utterances relevant to the current utter-
ance to be considered in keyword extraction.
Our experiments on two data sets in English
and Korean show that the consideration of the
factors results in performance improvement in
keyword extraction from meeting transcripts.
</bodyText>
<sectionHeader confidence="0.998111" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999990458333334">
A meeting is generally accomplished by a number
of participants and a wide range of subjects are dis-
cussed. Therefore, it would be helpful to meeting
participants to provide them with some additional
information related to the current subject. For in-
stance, assume that a participant is discussing a spe-
cific topic with other participants at a meeting. The
summary of previous meetings on the topic is then
one of the most important resources for her discus-
sion.
In order to provide information on a topic to par-
ticipants, keywords should be first generated for the
topic since keywords are often representatives of a
topic. A number of techniques have been proposed
for automatic keyword extraction (Frank et al., 1999;
Turney, 2000; Mihalcea and Tarau, 2004; Wan et al.,
2007), and they are designed to extract keywords
from a written document. However, they are not
suitable for meeting transcripts. In a meeting, it is
often desirable to extract keywords at the time at
which a new utterance is made for just-in-time ser-
vice of additional information. Otherwise, the ex-
tracted keywords become just the important words
at the end of the meeting.
Two key factors for just-in-time keyword extrac-
tion from meeting transcripts are time of preceding
utterances and topic of current utterance. First, cur-
rent utterance is affected by temporal history of pre-
ceding utterances. That is, when a new utterance
is made it is likely to be related more closely with
latest utterances than old ones. Second, the preced-
ing utterances which carry similar topics to current
utterance are more important than irrelevant utter-
ances. Since a meeting consists of several topics,
the utterances that have nothing to do with current
utterance are inappropriate as a history of the cur-
rent utterance.
This paper proposes a graph-based keyword ex-
traction to reflect these factors. The proposed
method represents an utterance as a graph of which
nodes are candidate keywords. The preceding utter-
ances are also expressed as a history graph in which
the weight of an edge is the temporal importance
of the keywords connected by the edge. To reflect
the temporal history of utterances, forgetting curve
(Wozniak, 1999) is adopted in updating the weights
of edges in the history graph. It expresses effectively
not only the reciprocal relation between memory re-
</bodyText>
<page confidence="0.42881">
888
</page>
<note confidence="0.731922">
Proceedings of NAACL-HLT 2013, pages 888–896,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99891324137931">
tention and time, but also active recall that makes
frequent words more consequential in keyword ex-
traction. Then, a subgraph that is relevant to the
current utterance is derived from the history graph,
and used as an actual history of the current utterance.
The keywords of the current utterance are extracted
by TextRank (Mihalcea and Tarau, 2004) from the
merged graph of the current utterance and the his-
tory graphs.
The proposed method is evaluated with two kinds
of data sets: the National Assembly transcripts
in Korean and the ICSI meeting corpus (Janin et
al., 2003) in English. The experimental results
show that it outperforms both the TFIDF frame-
work (Frank et al., 1999; Liu et al., 2009) and the
PageRank-based graph model (Wan et al., 2007).
One thing to note is that the proposed method im-
proves even the supervised methods that do not re-
flect utterance time and topic relevance for the ICSI
corpus. This proves that it is critical to consider time
and content of utterances simultaneously in keyword
extraction from meeting transcripts.
The rest of the paper is organized as follows. Sec-
tion 2 reviews the related studies on keyword extrac-
tion. Section 3 explains the overall process of the
proposed method, and Section 4 addresses its de-
tailed description how to reflect meeting character-
istics. Experimental results are presented in Section
5. Finally, Section 6 draws some conclusions.
</bodyText>
<sectionHeader confidence="0.999771" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997552428571429">
Keyword extraction has been of interest for a long
time in various fields such as information retrieval,
document clustering, summarization, and so on.
Thus, there have been many studies on automatic
keyword extraction. The frequency-based key-
word extraction with TFIDF weighting (Frank et al.,
1999) and the graph-based keyword extraction (Mi-
halcea and Tarau, 2004) are two base models for this
task. Many studies recently tried to extend them by
incorporating specific information such as linguistic
knowledge (Hulth, 2003), web-based resource (Tur-
ney, 2003), and semantic knowledge (Chen et al.,
2010). As a result, they show good performance on
written text. However, it is difficult to use them di-
rectly for spoken genres, since spoken genres have
significantly different characteristics from written
text.
There have been a few studies focused on key-
word extraction from spoken genres. Among them,
the extraction from meetings has attracted more con-
cern, since the need for grasping important points
of a meeting or an opinion of each participant has
increased. The studies on meetings focused on
the exterior features of meeting dialogues such as
unstructured and ill-formed sentences. Liu et al.
(2009) used some knowledge sources such as Part-
of-Speech (POS) filtering, word clustering, and sen-
tence salience to reflect dialogue features, and they
found out that a simple TFIDF-based keyword ex-
traction using these knowledge sources works rea-
sonably well. They also extended their work by
adopting various features such as decision making
sentence features, speech-related features, and sum-
mary features that reflect meeting transcripts better
(Liu et al., 2011). Chen et al. (2010) extracted key-
words from spoken course lectures. In this study,
they considered prosodic information from HKT
forced alignment and topics in a lecture generated
by Probabilistic Latent Semantic Analysis (pLSA).
These studies focused on the exterior characteris-
tics of spoken genres, since they assumed that entire
scripts are given in advance and then they extracted
keywords that best describe the scripts. However, to
the best of our knowledge, there is no previous study
considered time of utterances which is an intrinsic
element of spoken genres.
The relevance between current utterance and pre-
ceding utterances is also a critical feature in keyword
extraction from meeting transcripts. The study that
considers this relevance explicitly is CollabRank
proposed by Wan and Xiao (2008). This is collabo-
rative approach to extract keywords in a document.
In this study, it is assumed that a few neighbor doc-
uments close to a current document can help extract
keywords. Therefore, they applied a clustering al-
gorithm to a document set and then extracted words
that are reinforced by the documents within a clus-
ter. However, this method also does not consider the
utterance time, since it is designed to extract key-
words from normal documents. As a result, if it is
applied to meeting transcripts, all preceding utter-
ances would affect the current utterance uniformly,
which leads to a poor performance.
</bodyText>
<figure confidence="0.6644925">
889
Expanded graph (G4)
</figure>
<figureCaption confidence="0.999832">
Figure 1: The overall process of the just-in-time keyword extraction from meeting transcripts.
</figureCaption>
<figure confidence="0.988541888888889">
Merge
Keywords
Filter
Current
utterance
Current
utterance
graph (G1)
Expand
Subgraph
extraction
History
graph (G2)
Subgraph (G3)
Keyword
extraction
Keyword
graph (G5)
</figure>
<sectionHeader confidence="0.6407695" genericHeader="method">
3 Just-In-Time Keyword Extraction for a
Meeting
</sectionHeader>
<bodyText confidence="0.995324552631579">
Figure 1 depicts the overall process of extracting
keywords from an utterance as soon as it is spo-
ken. We represent all the components in a meeting
as graphs. This is because graphs are effective to ex-
press the relationship between words, and the graph
operations that are required for keyword extraction
are also efficiently performed. That is, whenever an
utterance is spoken, it is represented as a graph (G1)
of which nodes are the potential keywords in the ut-
terance. This graph is named as current utterance
graph.
The summary of all preceding utterances is also
represented as a history graph (G2). We assume that
only the preceding utterances that are directly re-
lated with the current utterance are important for ex-
tracting keywords from the current utterance. There-
fore, a subgraph of G2 that maximally covers the
current utterance graph (G1) is extracted. This sub-
graph is labeled as G3 in Figure 1. Then, the current
utterance graph G1 is expanded by merging it and
G3. This expanded graph (G4) is a combined rep-
resentation of the current and preceding utterances,
and then the keywords of the current utterance is ex-
tracted from this graph. The keywords are so-called
hub nodes of G4.
After keywords are extracted from the current ut-
terance, the current utterance becomes a part of the
history graph for the next utterance. For this, the
extracted keywords are also represented as a graph
(G5), and it is merged into the current history G2.
This merged graph becomes a new history graph
for the next utterance. In merging two graphs, the
weight of each edge in G2 is updated to reflect the
temporal history. If an edge is connecting two nouns
from an old utterance, its weight becomes small. In
the same way, the weights for the edges from recent
utterances get large. The weights of the edges from
G5 are 1, the largest possible value.
</bodyText>
<sectionHeader confidence="0.984175" genericHeader="method">
4 Graph Representation and Weight
Update
</sectionHeader>
<subsectionHeader confidence="0.9747245">
4.1 Current Utterance Graph and History
Graph
</subsectionHeader>
<bodyText confidence="0.971369105263158">
Current utterance graph is a graph-representation of
the current utterance. When current utterance con-
sists of m words, we first extract the potential key-
890
words from the current utterance. Since all words
within the current utterance are not keywords, some
words are filtered out. For this filtering out, we fol-
low the POS filtering approach proposed by Liu et
al. (2009). This approach filters out non-keywords
using a stop-word list and POS tags of the words.
Assume that n words remain after the filtering out,
where n &lt; m. These n words become the vertices
of the current utterance graph.
Formally, the current utterance graph G1 =
(V1, E1) is an undirected graph, where |V1 |= n.
E1 is a set of edges and each edge implies that the
nouns connected by the edge co-occur within a win-
dow sized W. For each e1 ijE E1 that connects nodes
v1i and v1j , its weight is given by
</bodyText>
<equation confidence="0.79764525">
�
= 1 if vZ &amp;v� cooccur within the window,
w1ij (1)
0 otherwise.
</equation>
<bodyText confidence="0.999988586206896">
In a meeting, preceding utterances affect the cur-
rent utterance. We assume that only the keywords
of preceding utterances are effective. Therefore, the
history graph G2 = (V2, E2) is an undirected graph
of keywords in the preceding utterances. That is,
all vertices in V2 are keywords extracted from one
or more previous utterances, and the edge between
two keywords implies that they co-occurred at least
once. Every edge in E2 has a weight that represents
its temporal importance.
The history graph is updated whenever keywords
are extracted from a new utterance. This is because
the current utterance becomes a part of the history
graph for the next utterance. As a history, old ut-
terances are less important than recent ones. Thus,
the temporal importance should decrease gradually
according to the passage of time. In addition, the
keywords which occur frequently at a meeting are
more important than those mentioned just once or
twice. Since the frequently-mentioned keywords are
normally major topics of the meeting, their influence
should last for a long time.
To model these characteristics, the forgetting
curve (Wozniak, 1999) is adopted in updating the
history graph. It models the decline of memory re-
tention in time. Figure 2 shows a typical represen-
tation of the forgetting curve. The X-axis of this
figure is time and the Y-axis is memory retention.
As shown in this figure, memory retention of new
</bodyText>
<figure confidence="0.607485">
Time
</figure>
<figureCaption confidence="0.989772">
Figure 2: Memory retention according to time.
</figureCaption>
<bodyText confidence="0.99987725">
information decreases gradually by the exponential
nature of forgetting. However, whenever the infor-
mation is repeated, it is recalled longer. This is for-
mulated as
</bodyText>
<equation confidence="0.967783">
R = e−
</equation>
<bodyText confidence="0.969153">
where R is memory retention, t is time, and 5 is the
relative strength of memory.
Based on the forgetting curve, the weight of each
edge e2 ijE E2 between keywords v2iand v2j is set as
</bodyText>
<equation confidence="0.999195">
t
w2ij = exp f(vz,vj) , (2)
</equation>
<bodyText confidence="0.999958714285714">
where t is the elapse of utterance time and f(vi, vj)
is the frequency that vi and vj co-occur from the
beginning of the meeting to now. According to
this equation, the temporal importance between key-
words decreases gradually as time passes by, but the
keyword relations repeated during the meeting are
remembered for a long time in the history graph.
</bodyText>
<subsectionHeader confidence="0.710577">
4.2 Keyword Extraction by Merging Current
Utterance and History Graphs
</subsectionHeader>
<bodyText confidence="0.9979901">
All words within the history graph are not equally
important in extracting keywords from the current
utterance. In general, many participants discuss a
wide range of topics in a meeting. Therefore, some
preceding utterances that shares topics with the cur-
rent utterance are more significant. We assume that
the preceding utterances that contain the nouns in
the current utterance share topics with the current
utterance. Thus, only a subgraph of G2 that contain
words in G1 is relevant for keyword extraction from
</bodyText>
<figure confidence="0.878202">
G1.
Memory
Retention
t
S
,
891
</figure>
<bodyText confidence="0.999060888888889">
Given the current utterance graph G1 = (V1, E1)
and the history graph G2 = (V2, E2), the relevant
graph G3 = (V3, E3) is a subgraph of G2. Here,
V3 = (V1nV2)Uadjacency(V1) and adjacency(V1)
is a set of vertices from G2 which are directly con-
nected to the words in V1. That is, V3 contains
the words of G1 and their direct neighbor words in
G2. E3 is a subset of E2. Only the edges that ap-
pear in E2 are included in E3. The weight w3 ijof
each e3 ijE E3 is also borrowed from G2. That is,
w3 ij= w2ij. Therefore, G3 is a 1-walk subgraph1 of
G2 in which words in G1 and their neighbor words
appear.
The keywords of the current utterance should re-
flect the relevant history as well as the current utter-
ance itself. For this purpose, G1 is expanded with
respect to G3. The expanded graph G4 = (V4, E4)
of G1 is defined as
</bodyText>
<equation confidence="0.9998485">
V4 = V1 U V3,
E4 = E1 U E3.
</equation>
<bodyText confidence="0.945033166666667">
For each edge e4ij E E4, its weight w4 ijis determined
to be the larger value between w1ij and w3 ijif it ap-
pears in both G1 and G3. When it appears in only
one of the graphs, w4ij is set to be the weight of its
corresponding graph. That is,
max(w1ij, w3ij) if e4 ij E E1 and e4 ij E E3,
</bodyText>
<equation confidence="0.94059975">
w � if e4 E E1 and e4 E/ E3,
ij
w3 otherwise.
ij
</equation>
<bodyText confidence="0.99994034883721">
From this expanded graph G4, the keywords are
extracted by TextRank (Mihalcea and Tarau, 2004).
TextRank is an unsupervised graph-based method
for keyword extraction. It singles out the key ver-
tices of a graph by providing a ranking mechanism.
In order to rank the vertices, it computes the score
of each vertex v4i E V4 by
where 0 &lt; d &lt; 1 is a damping factor and adj(vi)
denotes vi’s neighbors. Finally, the words whose
score is larger than a specific threshold θ are cho-
sen as keywords. Especially when the current utter-
ance is the first utterance of a meeting, the history
graph does not exist. In this case, the current utter-
ance graph becomes the expanded graph (G4 = G1),
and keywords are extracted from the current utter-
ance graph.
The proposed method extracts keywords when-
ever an utterance is spoken. Thus, it tries to extract
keywords even if the current utterance is not related
to the topics of a meeting or is too short. However,
if the current utterance is irrelevant to the meeting,
it has just a few connections with other previous ut-
terances, and thus the potential keywords in this ut-
terance are apt to have a low score. The proposed
method, however, does not select the words whose
score is smaller than the threshold θ as keywords.
As a result, it extracts only the relevant keywords
during the meeting.
Since the keywords for the current utterance
should be the history for the next utterance, they
have to be reflected into the history graph. There-
fore, a keyword graph G5 = (V5, E5) is constructed
from the keywords. Here, V5 is a set of keywords
extracted from G4, and E5 is a subset of E4 that
corresponds to V5. The weights of edges in E5 are
same with those in E4. That is, w5 ij= w4ij. The key-
word graph G5 is then merged into the history graph
G2 in the same way that G1 and G3 are merged. As
stated above, the weights of the edges in the history
graph G2 are updated by Equation (2). Therefore,
before merging G5 and G2, all weights of G2 are
updated by increasing t as t + 1 to reflect temporal
importance of preceding utterances.
</bodyText>
<equation confidence="0.555075555555556">
�
�� �
���
w4ij =
� w•4 5 Experiments
S(v4i ) = (1 − d) + d � • / \
vJ4 Eadj(v4� ) 3z 4 S(Vp,
Ev4 Eadj (v,4) wj k
(3)
</equation>
<bodyText confidence="0.862665454545455">
1If a m-walk subgraph (m &gt; 1) is used, more affluent his-
tory is used. However, this graph contains some words irrel-
evant to the current utterance. According to our experiments,
1-walk subgraph outperforms other m-walk subgraphs where
m &gt; 1. In addition, extracting G3 becomes expensive for large
m.
The proposed method is evaluated with two kinds of
data sets: the National Assembly transcripts in Ko-
rean and the ICSI meeting corpus in English. Both
data sets are the records of meetings that are manu-
ally dictated by human transcribers.
</bodyText>
<page confidence="0.88728">
892
</page>
<tableCaption confidence="0.999797">
Table 1: Simple statistics of the National Assembly transcripts
</tableCaption>
<table confidence="0.755234333333333">
No. of utterances the first meeting the second meeting
Average No. of words per utterance 1,280 573
7.22 10.17
</table>
<subsectionHeader confidence="0.926791">
5.1 National Assembly Transcripts in Korean
</subsectionHeader>
<bodyText confidence="0.999954888888889">
The first corpus used to evaluate our method is the
National Assembly transcripts2. This corpus is ob-
tained from the Knowledge Management System
of the National Assembly of KoreaIt is transcribed
from the 305th assembly record of the Knowledge
Economy Committee in 2012. Table 1 summa-
rizes simple statistics of the National Assembly tran-
scripts. The 305th assembly record actually consists
of two meetings. The first meeting contains 1,280
utterances and the second has 573 utterances. The
average number of words per utterance in the first
meeting is 7.22 while the second meeting contains
10.17 words per utterance on average. The second
meeting transcript is used as a development data set
to determine window size W of Equation (1), the
damping factor d of Equation (3), and the threshold
0. For all experiments below, d is set 0.85, W is 10,
and 0 is 0.28. The remaining first meeting transcript
is used as a data set to extract keywords since this
transcript contains more utterances. Only nouns are
considered as potential keywords. That is, only the
words whose POS tag is NNG (common noun) or
NNP (proper noun) can be a keyword.
Three annotators are engaged to extract keywords
manually for each utterance in the first meeting
transcript, since the Knowledge Management Sys-
tem does not provide the keywords3. The aver-
age number of keywords per utterance is 2.58. To
see the inter-judge agreement among the annotators,
the Kappa coefficient (Carletta, 1996) was investi-
gated. The kappa agreement of the National Assem-
bly transcript is 0.31 that falls under the category of
‘Fair’. Even though all congressmen in the transcript
belong to the same committee, they discussed vari-
ous topics at the meeting. As a result, the keywords
are difficult to be agreed unanimously by all three
</bodyText>
<footnote confidence="0.7434514">
2The data set is available: http://ml.knu.ac.kr/
dataset/keywordextraction.html
3A guideline was given to the annotators that keywords must
be a single word and the maximum number of keywords per
utterance is five.
</footnote>
<bodyText confidence="0.999971585365854">
annotators. Therefore, in this paper the words that
are recommended by more than two annotators are
chosen as keywords.
The evaluation is done with two metics: F-
measure and the weighted relative score (WRS).
Since the previous work by Liu et al. (2009) re-
ported only F-measure and WRS, F-measure instead
of precision/recall are used for the comparison with
their method. The weighted relative score is de-
rived from Pyramid metric (Nenkova and Passon-
neau, 2004). When a keyword extraction system
generates keywords which many annotators agree,
a higher score is given to it. On the other hand, a
lower score is given if fewer annotators agree.
The proposed method is compared with two base-
line models to see its relative performance. One is
the frequency-based keyword extraction with TFIDF
weighting (Frank et al., 1999) and the other is Tex-
tRank in which the weight of edges is mutual in-
formation between vertices (Wan et al., 2007). In
TFIDF, each utterance is considered as a document,
and thus all utterances including the current one
are regarded as whole documents. The frequency-
based TFIDF chooses top-K words according to
their TFIDF value from the set of words appearing in
the meeting transcript. Since the human annotators
are restricted to extract up to five keywords, the key-
word extraction systems including our method are
also requested to select top-5 keywords when more
than five keywords are produced.
In order to see the effect of preceding utterances in
baseline models, the performances are measured ac-
cording to the number of preceding utterances used.
Figure 3 shows the results. The X-axis of this fig-
ure is the number of preceding utterances and the Y-
axis represents F-measures. As shown in this figure,
the performance of the baseline models improves
monotonically at first as the number of preceding
utterances increases. However, the performance im-
provement stops when many preceding utterances
are involved, and the performance begins to drop
</bodyText>
<page confidence="0.890828">
893
</page>
<figureCaption confidence="0.972115">
Figure 3: The performance of baseline models according
to the number of preceding utterances
</figureCaption>
<tableCaption confidence="0.979553">
Table 2: The experimental results on the National Assem-
bly transcripts
</tableCaption>
<table confidence="0.87209475">
Methods F-measure WRS
TextRank 0.478 0.387
TFIDF 0.481 0.394
Proposed method 0.533 0.421
</table>
<bodyText confidence="0.998854428571428">
when too many utterances are considered. The per-
formance of TextRank model drops from 20 preced-
ing utterances, while that of TFIDF model begins to
drops at 50 utterances. When too many preceding
utterances are taken into account, it is highly pos-
sible that some of their topics are irrelevant to the
current utterance, which leads to performance drop.
Table 2 compares our method with the baseline
models on the National Assembly transcripts. The
performances of baseline models are obtained when
they show the best performance for various number
of preceding utterances. TextRank model achieves
F-measure of 0.478 and weighted relative score of
0.387, while TFIDF reports its best F-measure of
0.481 and weighted relative score of 0.394. Thus,
the difference between TFIDF and TextRank is not
significant. However, F-measure and weighted rel-
ative score of the proposed method are 0.533 and
0.421 respectively, and they are much higher than
those of baseline models. In addition, our method
achieves precision of 0.543 and recall of 0.523 and
</bodyText>
<tableCaption confidence="0.999062">
Table 3: The importance of temporal history
</tableCaption>
<bodyText confidence="0.942230739130435">
F-measure WRS
With Temporal History 0.533 0.421
Without Temporal History 0.518 0.413
this is much higher performance than TextRank
whose precision is just 0.510. Since the proposed
method uses, as history, the preceding utterances
relevant to the current utterance, its performance is
kept high even if whole utterances are used. There-
fore, it could be inferred that it is important to adopt
only the relevant history in keyword extraction from
meeting transcripts.
One of the key factors of our method is the tem-
poral history. Its importance is given in Table 3. As
explained above, the temporal history is achieved by
Equation (2). Thus, the proposed model does not
reflect the temporal importance of preceding utter-
ances if w = 1 always. That is, under w = 1,
old utterances are regarded as important as recent ut-
terances. Without temporal history, F-measure and
weighted relative score are just 0.518 and 0.413 re-
spectively. These poor performances prove the im-
portance of the temporal history in keyword extrac-
tion from meeting transcripts.
</bodyText>
<subsectionHeader confidence="0.995348">
5.2 ICSI Meeting Corpus in English
</subsectionHeader>
<bodyText confidence="0.99984825">
The proposed method is also evaluated on the ICSI
meeting corpus (Janin et al., 2003) which consists of
naturally occurring meetings recordings. This cor-
pus is widely used for summarizing and extracting
keywords of meetings. We followed all the exper-
imental settings proposed by Liu et al. (2009) for
this corpus. Among 26 meeting transcripts chosen
by Liu et al. from 161 transcripts of the ICSI meet-
ing corpus, 6 transcripts are used as development
data and the remaining transcripts are used as data
to extract keywords. The parameters for the ICSI
meeting corpus are set to be d = 0.85, W = 10,
and 0 = 0.20. Each meeting of the corpus consists
of several topic segments, and every topic segment
contains three sets of keywords that are annotated by
three annotators. Up to five keywords are annotated
for a topic segment.
Table 4 shows simple statistics of the ICSI meet-
ing data. Total number of topic segments in the 26
meetings is originally 201, but some of them do not
</bodyText>
<page confidence="0.926065">
894
</page>
<tableCaption confidence="0.999992">
Table 4: Simple statistics of the ICSI meeting data
Table 6: The effect of considering topic relevance
</tableCaption>
<figure confidence="0.968991476190476">
# of meetings
Information
Value
26
201
140
260
Average # of words per utterance
7.22
Methods
With topic relevance
Without topic relevance
F-measure
0.334
0.291
# of topic segments
# of topic segments used actually
Average # of utterances per topic segment
WRS
0.533
0.458
</figure>
<tableCaption confidence="0.992459">
Table 5: The experimental results on the ICSI corpus
</tableCaption>
<sectionHeader confidence="0.712279" genericHeader="method">
Methods
</sectionHeader>
<bodyText confidence="0.999200918918919">
TFIDF-Liu
TextRank-Liu
ME model
Proposed method
have keywords. Such segments are discarded, and
the remaining 140 topic segments are actually used.
The average number of utterances in a topic segment
is 260 and the average number of words per utter-
ance is 7.22.
Unlike the National Assembly transcripts, the
keywords of the ICSI meeting corpus are annotated
at the topic segment level, not the utterance level.
Therefore, the proposed method which extracts key-
words at the utterance level can not be applied di-
rectly to this corpus. In order to obtain keywords
for a topic segment with the proposed method, the
keywords are first extracted from each utterance in
the segment by the proposed method and then they
are all accumulated. The proposed method extracts
keywords for a topic segment from these accumu-
lated utterance-level keywords as follows. Assume
that a topic segment consists of l utterances. Since
our method can extract up to 5 keywords for each
utterance, the number of keywords for the segment
can reach to 5 · l. From these keywords, we select
top-5 keywords ranked by Equation (3).
The proposed method is compared with three pre-
vious studies. The first two are the methods pro-
posed by Liu et al. (2009) One is the frequency-
based method of TFIDF weighting with the fea-
tures such as POS filtering, word clustering, and sen-
tence salience score, and the other is the graph-based
method with POS filtering. The last method is a
maximum entropy model applied to this task (Liu
et al., 2008). Note that the maximum entropy is a
supervised learning model.
Table 5 summarizes the comparison results. As
shown in this table, the proposed method outper-
forms all previous methods. Our method achieves
precision of 0.311 and recall of 0.361, and thus
the F-score is 0.334. The weight relative score
of the proposed method is 0.533. This is the im-
provement of up to 0.044 in F-measure and 0.129
in weighted relative score over other unsupervised
methods (TFIDF-Liu and TextRank-Liu). It should
be also noted that the proposed method outperforms
even the supervised method (ME model). The differ-
ence between our method and the maximum entropy
model in weighted relative score is 0.132.
One possible variant of the proposed method for
the ICSI corpus is to simply merge the current utter-
ance graph (G1) with the history graph (G2) rather
than to extract keywords from each utterance. Af-
ter the current utterance graph of the last utterance
in a topic segment is merged into the history graph,
the keywords for the segment are extracted from the
history graph. This variant and the proposed method
both rely on the temporal history, but the difference
is that the history graph of the variant accumulates
all information within the topic segment. Thus, the
keywords extracted from the history graph by this
variant are those without consideration of topic rel-
evance.
Table 6 compares the proposed method with the
variant. The performance of the variant is higher
than those of TFIDF-Liu and TextRank-Liu. This
proves the importance of the temporal history in
keyword extraction from meeting transcripts. How-
ever, the proposed method still outperforms the vari-
ant, and it demonstrates the importance of topic rel-
evance. Therefore, it can be concluded that the con-
sideration of temporal history and topic relevance
is critical in keyword extraction from meeting tran-
scripts.
</bodyText>
<figure confidence="0.957708363636364">
F-measure
0.290
0.277
0.312
0.334
WRS
0.404
0.380
0.401
0.533
895
</figure>
<sectionHeader confidence="0.948876" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999981576923077">
In this paper, we have proposed a just-in-time key-
word extraction from meeting transcripts. Whenever
an utterance is spoken, the proposed method extracts
keywords from the utterance that best describe the
utterance. Based on the graph representation of all
components in a meeting, the proposed method ex-
tracts keywords by TextRank with some graph oper-
ations.
Temporal history and topic of the current utter-
ance are two major factors especially in keyword ex-
traction from meeting transcripts. This is because re-
cent utterances are more important than old ones and
only the preceding utterances of which topic is rele-
vant to the current utterance are important. To model
the temporal importance of the preceding utterances,
the concept of forgetting curve is used in updating
the history graph of preceding utterances. In addi-
tion, the subgraph of the history graph that shares
words appearing in the current utterance graph is
used to extract keywords rather than whole history
graph. The proposed method was evaluated with the
National Assembly transcripts and the ICSI meeting
corpus. According to our experimental results on
these data sets, the performance of keyword extrac-
tion is improved when we consider temporal history
and topic relevance.
</bodyText>
<sectionHeader confidence="0.998648" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99475">
This research was supported by the Converging Re-
search Center Program funded by the Ministry of
Education, Science and Technology (2012K001342)
</bodyText>
<sectionHeader confidence="0.998153" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999941787878788">
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249–254.
Yun-Nung Chen, Yu Huang, Sheng-Yi Kong, , and Lin-
Shan Lee. 2010. Automatic key term extraction from
spoken course lectures using branching entropy and
prosodic/semantic features. In Proceedings of IEEE
Workshop on Spoken Language Technology, pages
265–270.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceedings
of the 18th International Joint Conference on Artificial
intelligence, pages 668–671.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Proceed-
ings of International Conference on Empirical Meth-
ods in Natural Language Processing, pages 216–223.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin, Thilo
Pfau, Elizabeth Shriberg, Andreas Stolcke, and Chuck
Wooters. 2003. The icsi meeting corpus. In Proceed-
ings of International Conference on Acoustics, Speech,
and Signal Processing, pages 364–367.
Fei Liu, Feifan Liu, and Yang Liu. 2008. Automatic key-
word extraction for the meeting corpus using super-
vised approach and bigram expansion. In Proceedings
of IEEE Spoken Language Technology, pages 181–
184.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu. 2009.
Unsupervised approaches for automatic keyword ex-
traction using meeting transcripts. In Proceedings of
Annual Conference of the North American Chapter of
the ACL, pages 620–628.
Fei Liu, Feifan Liu, and Yang Liu. 2011. A super-
vised framework for keyword extraction from meeting
transcripts. IEEE Transactions on Audio, Speech, and
Language Processing, 19(3):538–548.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of International
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 404–411.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings of Annual Conference of the
North American Chapter of the ACL, pages 145–152.
Peter D. Turney. 2000. Learning algorithms for
keyphrase extraction. Information Retrieval, 2:303–
336.
Peter D. Turney. 2003. Coherent keyphrase extrac-
tion via web mining. In Proceedings of the 18th In-
ternational Joint Conference on Artificial intelligence,
pages 434–439.
Xiaojun Wan and Jianguo Xiao. 2008. Collabrank:
Towards a collaborative approach to single-document
keyphrase extraction. In Proceedings of International
Conference on Computational Linguistics, pages 969–
976.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. To-
wards an iterative reinforcement approach for simulta-
neous document summarization and keyword extrac-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 552–
559.
Robert H. Wozniak. 1999. Classics in Psychology,
1855–1914: Historical Essays. Thoemmes Press.
</reference>
<page confidence="0.954025">
896
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.237556">
<title confidence="0.999937">A Just-In-Time Keyword Extraction from Meeting Transcripts</title>
<author confidence="0.997638">Hyun-Je Song Junho Go Seong-Bae Park Se-Young Park</author>
<affiliation confidence="0.632584">School of Computer Science and Kyungpook National Daegu,</affiliation>
<abstract confidence="0.999083">In a meeting, it is often desirable to extract keywords from each utterance as soon as it is spoken. Thus, this paper proposes a just-intime keyword extraction from meeting transcripts. The proposed method considers two major factors that make it different from keyword extraction from normal texts. The first factor is the temporal history of preceding utterances that grants higher importance to recent utterances than old ones, and the second is topic relevance that forces only the preceding utterances relevant to the current utterance to be considered in keyword extraction. Our experiments on two data sets in English and Korean show that the consideration of the factors results in performance improvement in keyword extraction from meeting transcripts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="19630" citStr="Carletta, 1996" startWordPosition="3329" endWordPosition="3330"> 0 is 0.28. The remaining first meeting transcript is used as a data set to extract keywords since this transcript contains more utterances. Only nouns are considered as potential keywords. That is, only the words whose POS tag is NNG (common noun) or NNP (proper noun) can be a keyword. Three annotators are engaged to extract keywords manually for each utterance in the first meeting transcript, since the Knowledge Management System does not provide the keywords3. The average number of keywords per utterance is 2.58. To see the inter-judge agreement among the annotators, the Kappa coefficient (Carletta, 1996) was investigated. The kappa agreement of the National Assembly transcript is 0.31 that falls under the category of ‘Fair’. Even though all congressmen in the transcript belong to the same committee, they discussed various topics at the meeting. As a result, the keywords are difficult to be agreed unanimously by all three 2The data set is available: http://ml.knu.ac.kr/ dataset/keywordextraction.html 3A guideline was given to the annotators that keywords must be a single word and the maximum number of keywords per utterance is five. annotators. Therefore, in this paper the words that are recom</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Nung Chen</author>
<author>Yu Huang</author>
<author>Sheng-Yi Kong</author>
</authors>
<title>Automatic key term extraction from spoken course lectures using branching entropy and prosodic/semantic features.</title>
<date>2010</date>
<booktitle>In Proceedings of IEEE Workshop on Spoken Language Technology,</booktitle>
<pages>265--270</pages>
<contexts>
<context position="5519" citStr="Chen et al., 2010" startWordPosition="879" endWordPosition="882"> Related Work Keyword extraction has been of interest for a long time in various fields such as information retrieval, document clustering, summarization, and so on. Thus, there have been many studies on automatic keyword extraction. The frequency-based keyword extraction with TFIDF weighting (Frank et al., 1999) and the graph-based keyword extraction (Mihalcea and Tarau, 2004) are two base models for this task. Many studies recently tried to extend them by incorporating specific information such as linguistic knowledge (Hulth, 2003), web-based resource (Turney, 2003), and semantic knowledge (Chen et al., 2010). As a result, they show good performance on written text. However, it is difficult to use them directly for spoken genres, since spoken genres have significantly different characteristics from written text. There have been a few studies focused on keyword extraction from spoken genres. Among them, the extraction from meetings has attracted more concern, since the need for grasping important points of a meeting or an opinion of each participant has increased. The studies on meetings focused on the exterior features of meeting dialogues such as unstructured and ill-formed sentences. Liu et al. </context>
</contexts>
<marker>Chen, Huang, Kong, 2010</marker>
<rawString>Yun-Nung Chen, Yu Huang, Sheng-Yi Kong, , and LinShan Lee. 2010. Automatic key term extraction from spoken course lectures using branching entropy and prosodic/semantic features. In Proceedings of IEEE Workshop on Spoken Language Technology, pages 265–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Gordon W Paynter</author>
<author>Ian H Witten</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial intelligence,</booktitle>
<pages>668--671</pages>
<contexts>
<context position="1746" citStr="Frank et al., 1999" startWordPosition="271" endWordPosition="274">ussed. Therefore, it would be helpful to meeting participants to provide them with some additional information related to the current subject. For instance, assume that a participant is discussing a specific topic with other participants at a meeting. The summary of previous meetings on the topic is then one of the most important resources for her discussion. In order to provide information on a topic to participants, keywords should be first generated for the topic since keywords are often representatives of a topic. A number of techniques have been proposed for automatic keyword extraction (Frank et al., 1999; Turney, 2000; Mihalcea and Tarau, 2004; Wan et al., 2007), and they are designed to extract keywords from a written document. However, they are not suitable for meeting transcripts. In a meeting, it is often desirable to extract keywords at the time at which a new utterance is made for just-in-time service of additional information. Otherwise, the extracted keywords become just the important words at the end of the meeting. Two key factors for just-in-time keyword extraction from meeting transcripts are time of preceding utterances and topic of current utterance. First, current utterance is </context>
<context position="4176" citStr="Frank et al., 1999" startWordPosition="666" endWordPosition="669">ds more consequential in keyword extraction. Then, a subgraph that is relevant to the current utterance is derived from the history graph, and used as an actual history of the current utterance. The keywords of the current utterance are extracted by TextRank (Mihalcea and Tarau, 2004) from the merged graph of the current utterance and the history graphs. The proposed method is evaluated with two kinds of data sets: the National Assembly transcripts in Korean and the ICSI meeting corpus (Janin et al., 2003) in English. The experimental results show that it outperforms both the TFIDF framework (Frank et al., 1999; Liu et al., 2009) and the PageRank-based graph model (Wan et al., 2007). One thing to note is that the proposed method improves even the supervised methods that do not reflect utterance time and topic relevance for the ICSI corpus. This proves that it is critical to consider time and content of utterances simultaneously in keyword extraction from meeting transcripts. The rest of the paper is organized as follows. Section 2 reviews the related studies on keyword extraction. Section 3 explains the overall process of the proposed method, and Section 4 addresses its detailed description how to r</context>
<context position="20989" citStr="Frank et al., 1999" startWordPosition="3548" endWordPosition="3551">(WRS). Since the previous work by Liu et al. (2009) reported only F-measure and WRS, F-measure instead of precision/recall are used for the comparison with their method. The weighted relative score is derived from Pyramid metric (Nenkova and Passonneau, 2004). When a keyword extraction system generates keywords which many annotators agree, a higher score is given to it. On the other hand, a lower score is given if fewer annotators agree. The proposed method is compared with two baseline models to see its relative performance. One is the frequency-based keyword extraction with TFIDF weighting (Frank et al., 1999) and the other is TextRank in which the weight of edges is mutual information between vertices (Wan et al., 2007). In TFIDF, each utterance is considered as a document, and thus all utterances including the current one are regarded as whole documents. The frequencybased TFIDF chooses top-K words according to their TFIDF value from the set of words appearing in the meeting transcript. Since the human annotators are restricted to extract up to five keywords, the keyword extraction systems including our method are also requested to select top-5 keywords when more than five keywords are produced. </context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl Gutwin, and Craig G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. In Proceedings of the 18th International Joint Conference on Artificial intelligence, pages 668–671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Hulth</author>
</authors>
<title>Improved automatic keyword extraction given more linguistic knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of International Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="5440" citStr="Hulth, 2003" startWordPosition="869" endWordPosition="870"> are presented in Section 5. Finally, Section 6 draws some conclusions. 2 Related Work Keyword extraction has been of interest for a long time in various fields such as information retrieval, document clustering, summarization, and so on. Thus, there have been many studies on automatic keyword extraction. The frequency-based keyword extraction with TFIDF weighting (Frank et al., 1999) and the graph-based keyword extraction (Mihalcea and Tarau, 2004) are two base models for this task. Many studies recently tried to extend them by incorporating specific information such as linguistic knowledge (Hulth, 2003), web-based resource (Turney, 2003), and semantic knowledge (Chen et al., 2010). As a result, they show good performance on written text. However, it is difficult to use them directly for spoken genres, since spoken genres have significantly different characteristics from written text. There have been a few studies focused on keyword extraction from spoken genres. Among them, the extraction from meetings has attracted more concern, since the need for grasping important points of a meeting or an opinion of each participant has increased. The studies on meetings focused on the exterior features </context>
</contexts>
<marker>Hulth, 2003</marker>
<rawString>Anette Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of International Conference on Empirical Methods in Natural Language Processing, pages 216–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Janin</author>
<author>Don Baron</author>
<author>Jane Edwards</author>
<author>Dan Ellis</author>
<author>David Gelbart</author>
<author>Nelson Morgan</author>
<author>Barbara Peskin</author>
<author>Thilo Pfau</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Chuck Wooters</author>
</authors>
<title>The icsi meeting corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>364--367</pages>
<contexts>
<context position="4069" citStr="Janin et al., 2003" startWordPosition="648" endWordPosition="651">3 Association for Computational Linguistics tention and time, but also active recall that makes frequent words more consequential in keyword extraction. Then, a subgraph that is relevant to the current utterance is derived from the history graph, and used as an actual history of the current utterance. The keywords of the current utterance are extracted by TextRank (Mihalcea and Tarau, 2004) from the merged graph of the current utterance and the history graphs. The proposed method is evaluated with two kinds of data sets: the National Assembly transcripts in Korean and the ICSI meeting corpus (Janin et al., 2003) in English. The experimental results show that it outperforms both the TFIDF framework (Frank et al., 1999; Liu et al., 2009) and the PageRank-based graph model (Wan et al., 2007). One thing to note is that the proposed method improves even the supervised methods that do not reflect utterance time and topic relevance for the ICSI corpus. This proves that it is critical to consider time and content of utterances simultaneously in keyword extraction from meeting transcripts. The rest of the paper is organized as follows. Section 2 reviews the related studies on keyword extraction. Section 3 exp</context>
<context position="24651" citStr="Janin et al., 2003" startWordPosition="4141" endWordPosition="4144">iven in Table 3. As explained above, the temporal history is achieved by Equation (2). Thus, the proposed model does not reflect the temporal importance of preceding utterances if w = 1 always. That is, under w = 1, old utterances are regarded as important as recent utterances. Without temporal history, F-measure and weighted relative score are just 0.518 and 0.413 respectively. These poor performances prove the importance of the temporal history in keyword extraction from meeting transcripts. 5.2 ICSI Meeting Corpus in English The proposed method is also evaluated on the ICSI meeting corpus (Janin et al., 2003) which consists of naturally occurring meetings recordings. This corpus is widely used for summarizing and extracting keywords of meetings. We followed all the experimental settings proposed by Liu et al. (2009) for this corpus. Among 26 meeting transcripts chosen by Liu et al. from 161 transcripts of the ICSI meeting corpus, 6 transcripts are used as development data and the remaining transcripts are used as data to extract keywords. The parameters for the ICSI meeting corpus are set to be d = 0.85, W = 10, and 0 = 0.20. Each meeting of the corpus consists of several topic segments, and every</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>Adam Janin, Don Baron, Jane Edwards, Dan Ellis, David Gelbart, Nelson Morgan, Barbara Peskin, Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke, and Chuck Wooters. 2003. The icsi meeting corpus. In Proceedings of International Conference on Acoustics, Speech, and Signal Processing, pages 364–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Feifan Liu</author>
<author>Yang Liu</author>
</authors>
<title>Automatic keyword extraction for the meeting corpus using supervised approach and bigram expansion.</title>
<date>2008</date>
<booktitle>In Proceedings of IEEE Spoken Language Technology,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="27486" citStr="Liu et al., 2008" startWordPosition="4628" endWordPosition="4631">ts of l utterances. Since our method can extract up to 5 keywords for each utterance, the number of keywords for the segment can reach to 5 · l. From these keywords, we select top-5 keywords ranked by Equation (3). The proposed method is compared with three previous studies. The first two are the methods proposed by Liu et al. (2009) One is the frequencybased method of TFIDF weighting with the features such as POS filtering, word clustering, and sentence salience score, and the other is the graph-based method with POS filtering. The last method is a maximum entropy model applied to this task (Liu et al., 2008). Note that the maximum entropy is a supervised learning model. Table 5 summarizes the comparison results. As shown in this table, the proposed method outperforms all previous methods. Our method achieves precision of 0.311 and recall of 0.361, and thus the F-score is 0.334. The weight relative score of the proposed method is 0.533. This is the improvement of up to 0.044 in F-measure and 0.129 in weighted relative score over other unsupervised methods (TFIDF-Liu and TextRank-Liu). It should be also noted that the proposed method outperforms even the supervised method (ME model). The difference</context>
</contexts>
<marker>Liu, Liu, Liu, 2008</marker>
<rawString>Fei Liu, Feifan Liu, and Yang Liu. 2008. Automatic keyword extraction for the meeting corpus using supervised approach and bigram expansion. In Proceedings of IEEE Spoken Language Technology, pages 181– 184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifan Liu</author>
<author>Deana Pennell</author>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>Unsupervised approaches for automatic keyword extraction using meeting transcripts.</title>
<date>2009</date>
<booktitle>In Proceedings of Annual Conference of the North American Chapter of the ACL,</booktitle>
<pages>620--628</pages>
<contexts>
<context position="4195" citStr="Liu et al., 2009" startWordPosition="670" endWordPosition="673">l in keyword extraction. Then, a subgraph that is relevant to the current utterance is derived from the history graph, and used as an actual history of the current utterance. The keywords of the current utterance are extracted by TextRank (Mihalcea and Tarau, 2004) from the merged graph of the current utterance and the history graphs. The proposed method is evaluated with two kinds of data sets: the National Assembly transcripts in Korean and the ICSI meeting corpus (Janin et al., 2003) in English. The experimental results show that it outperforms both the TFIDF framework (Frank et al., 1999; Liu et al., 2009) and the PageRank-based graph model (Wan et al., 2007). One thing to note is that the proposed method improves even the supervised methods that do not reflect utterance time and topic relevance for the ICSI corpus. This proves that it is critical to consider time and content of utterances simultaneously in keyword extraction from meeting transcripts. The rest of the paper is organized as follows. Section 2 reviews the related studies on keyword extraction. Section 3 explains the overall process of the proposed method, and Section 4 addresses its detailed description how to reflect meeting char</context>
<context position="6125" citStr="Liu et al. (2009)" startWordPosition="976" endWordPosition="979"> al., 2010). As a result, they show good performance on written text. However, it is difficult to use them directly for spoken genres, since spoken genres have significantly different characteristics from written text. There have been a few studies focused on keyword extraction from spoken genres. Among them, the extraction from meetings has attracted more concern, since the need for grasping important points of a meeting or an opinion of each participant has increased. The studies on meetings focused on the exterior features of meeting dialogues such as unstructured and ill-formed sentences. Liu et al. (2009) used some knowledge sources such as Partof-Speech (POS) filtering, word clustering, and sentence salience to reflect dialogue features, and they found out that a simple TFIDF-based keyword extraction using these knowledge sources works reasonably well. They also extended their work by adopting various features such as decision making sentence features, speech-related features, and summary features that reflect meeting transcripts better (Liu et al., 2011). Chen et al. (2010) extracted keywords from spoken course lectures. In this study, they considered prosodic information from HKT forced ali</context>
<context position="10653" citStr="Liu et al. (2009)" startWordPosition="1715" endWordPosition="1718">mes small. In the same way, the weights for the edges from recent utterances get large. The weights of the edges from G5 are 1, the largest possible value. 4 Graph Representation and Weight Update 4.1 Current Utterance Graph and History Graph Current utterance graph is a graph-representation of the current utterance. When current utterance consists of m words, we first extract the potential key890 words from the current utterance. Since all words within the current utterance are not keywords, some words are filtered out. For this filtering out, we follow the POS filtering approach proposed by Liu et al. (2009). This approach filters out non-keywords using a stop-word list and POS tags of the words. Assume that n words remain after the filtering out, where n &lt; m. These n words become the vertices of the current utterance graph. Formally, the current utterance graph G1 = (V1, E1) is an undirected graph, where |V1 |= n. E1 is a set of edges and each edge implies that the nouns connected by the edge co-occur within a window sized W. For each e1 ijE E1 that connects nodes v1i and v1j , its weight is given by � = 1 if vZ &amp;v� cooccur within the window, w1ij (1) 0 otherwise. In a meeting, preceding utteran</context>
<context position="20421" citStr="Liu et al. (2009)" startWordPosition="3456" endWordPosition="3459">ng to the same committee, they discussed various topics at the meeting. As a result, the keywords are difficult to be agreed unanimously by all three 2The data set is available: http://ml.knu.ac.kr/ dataset/keywordextraction.html 3A guideline was given to the annotators that keywords must be a single word and the maximum number of keywords per utterance is five. annotators. Therefore, in this paper the words that are recommended by more than two annotators are chosen as keywords. The evaluation is done with two metics: Fmeasure and the weighted relative score (WRS). Since the previous work by Liu et al. (2009) reported only F-measure and WRS, F-measure instead of precision/recall are used for the comparison with their method. The weighted relative score is derived from Pyramid metric (Nenkova and Passonneau, 2004). When a keyword extraction system generates keywords which many annotators agree, a higher score is given to it. On the other hand, a lower score is given if fewer annotators agree. The proposed method is compared with two baseline models to see its relative performance. One is the frequency-based keyword extraction with TFIDF weighting (Frank et al., 1999) and the other is TextRank in wh</context>
<context position="24862" citStr="Liu et al. (2009)" startWordPosition="4174" endWordPosition="4177">1, old utterances are regarded as important as recent utterances. Without temporal history, F-measure and weighted relative score are just 0.518 and 0.413 respectively. These poor performances prove the importance of the temporal history in keyword extraction from meeting transcripts. 5.2 ICSI Meeting Corpus in English The proposed method is also evaluated on the ICSI meeting corpus (Janin et al., 2003) which consists of naturally occurring meetings recordings. This corpus is widely used for summarizing and extracting keywords of meetings. We followed all the experimental settings proposed by Liu et al. (2009) for this corpus. Among 26 meeting transcripts chosen by Liu et al. from 161 transcripts of the ICSI meeting corpus, 6 transcripts are used as development data and the remaining transcripts are used as data to extract keywords. The parameters for the ICSI meeting corpus are set to be d = 0.85, W = 10, and 0 = 0.20. Each meeting of the corpus consists of several topic segments, and every topic segment contains three sets of keywords that are annotated by three annotators. Up to five keywords are annotated for a topic segment. Table 4 shows simple statistics of the ICSI meeting data. Total numbe</context>
<context position="27204" citStr="Liu et al. (2009)" startWordPosition="4578" endWordPosition="4581">od, the keywords are first extracted from each utterance in the segment by the proposed method and then they are all accumulated. The proposed method extracts keywords for a topic segment from these accumulated utterance-level keywords as follows. Assume that a topic segment consists of l utterances. Since our method can extract up to 5 keywords for each utterance, the number of keywords for the segment can reach to 5 · l. From these keywords, we select top-5 keywords ranked by Equation (3). The proposed method is compared with three previous studies. The first two are the methods proposed by Liu et al. (2009) One is the frequencybased method of TFIDF weighting with the features such as POS filtering, word clustering, and sentence salience score, and the other is the graph-based method with POS filtering. The last method is a maximum entropy model applied to this task (Liu et al., 2008). Note that the maximum entropy is a supervised learning model. Table 5 summarizes the comparison results. As shown in this table, the proposed method outperforms all previous methods. Our method achieves precision of 0.311 and recall of 0.361, and thus the F-score is 0.334. The weight relative score of the proposed </context>
</contexts>
<marker>Liu, Pennell, Liu, Liu, 2009</marker>
<rawString>Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu. 2009. Unsupervised approaches for automatic keyword extraction using meeting transcripts. In Proceedings of Annual Conference of the North American Chapter of the ACL, pages 620–628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Feifan Liu</author>
<author>Yang Liu</author>
</authors>
<title>A supervised framework for keyword extraction from meeting transcripts.</title>
<date>2011</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="6585" citStr="Liu et al., 2011" startWordPosition="1045" endWordPosition="1048">has increased. The studies on meetings focused on the exterior features of meeting dialogues such as unstructured and ill-formed sentences. Liu et al. (2009) used some knowledge sources such as Partof-Speech (POS) filtering, word clustering, and sentence salience to reflect dialogue features, and they found out that a simple TFIDF-based keyword extraction using these knowledge sources works reasonably well. They also extended their work by adopting various features such as decision making sentence features, speech-related features, and summary features that reflect meeting transcripts better (Liu et al., 2011). Chen et al. (2010) extracted keywords from spoken course lectures. In this study, they considered prosodic information from HKT forced alignment and topics in a lecture generated by Probabilistic Latent Semantic Analysis (pLSA). These studies focused on the exterior characteristics of spoken genres, since they assumed that entire scripts are given in advance and then they extracted keywords that best describe the scripts. However, to the best of our knowledge, there is no previous study considered time of utterances which is an intrinsic element of spoken genres. The relevance between curren</context>
</contexts>
<marker>Liu, Liu, Liu, 2011</marker>
<rawString>Fei Liu, Feifan Liu, and Yang Liu. 2011. A supervised framework for keyword extraction from meeting transcripts. IEEE Transactions on Audio, Speech, and Language Processing, 19(3):538–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of International Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="1786" citStr="Mihalcea and Tarau, 2004" startWordPosition="277" endWordPosition="280">ful to meeting participants to provide them with some additional information related to the current subject. For instance, assume that a participant is discussing a specific topic with other participants at a meeting. The summary of previous meetings on the topic is then one of the most important resources for her discussion. In order to provide information on a topic to participants, keywords should be first generated for the topic since keywords are often representatives of a topic. A number of techniques have been proposed for automatic keyword extraction (Frank et al., 1999; Turney, 2000; Mihalcea and Tarau, 2004; Wan et al., 2007), and they are designed to extract keywords from a written document. However, they are not suitable for meeting transcripts. In a meeting, it is often desirable to extract keywords at the time at which a new utterance is made for just-in-time service of additional information. Otherwise, the extracted keywords become just the important words at the end of the meeting. Two key factors for just-in-time keyword extraction from meeting transcripts are time of preceding utterances and topic of current utterance. First, current utterance is affected by temporal history of precedin</context>
<context position="3843" citStr="Mihalcea and Tarau, 2004" startWordPosition="609" endWordPosition="612">) is adopted in updating the weights of edges in the history graph. It expresses effectively not only the reciprocal relation between memory re888 Proceedings of NAACL-HLT 2013, pages 888–896, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics tention and time, but also active recall that makes frequent words more consequential in keyword extraction. Then, a subgraph that is relevant to the current utterance is derived from the history graph, and used as an actual history of the current utterance. The keywords of the current utterance are extracted by TextRank (Mihalcea and Tarau, 2004) from the merged graph of the current utterance and the history graphs. The proposed method is evaluated with two kinds of data sets: the National Assembly transcripts in Korean and the ICSI meeting corpus (Janin et al., 2003) in English. The experimental results show that it outperforms both the TFIDF framework (Frank et al., 1999; Liu et al., 2009) and the PageRank-based graph model (Wan et al., 2007). One thing to note is that the proposed method improves even the supervised methods that do not reflect utterance time and topic relevance for the ICSI corpus. This proves that it is critical t</context>
<context position="5281" citStr="Mihalcea and Tarau, 2004" startWordPosition="842" endWordPosition="846">Section 3 explains the overall process of the proposed method, and Section 4 addresses its detailed description how to reflect meeting characteristics. Experimental results are presented in Section 5. Finally, Section 6 draws some conclusions. 2 Related Work Keyword extraction has been of interest for a long time in various fields such as information retrieval, document clustering, summarization, and so on. Thus, there have been many studies on automatic keyword extraction. The frequency-based keyword extraction with TFIDF weighting (Frank et al., 1999) and the graph-based keyword extraction (Mihalcea and Tarau, 2004) are two base models for this task. Many studies recently tried to extend them by incorporating specific information such as linguistic knowledge (Hulth, 2003), web-based resource (Turney, 2003), and semantic knowledge (Chen et al., 2010). As a result, they show good performance on written text. However, it is difficult to use them directly for spoken genres, since spoken genres have significantly different characteristics from written text. There have been a few studies focused on keyword extraction from spoken genres. Among them, the extraction from meetings has attracted more concern, since</context>
<context position="15318" citStr="Mihalcea and Tarau, 2004" startWordPosition="2562" endWordPosition="2565">e relevant history as well as the current utterance itself. For this purpose, G1 is expanded with respect to G3. The expanded graph G4 = (V4, E4) of G1 is defined as V4 = V1 U V3, E4 = E1 U E3. For each edge e4ij E E4, its weight w4 ijis determined to be the larger value between w1ij and w3 ijif it appears in both G1 and G3. When it appears in only one of the graphs, w4ij is set to be the weight of its corresponding graph. That is, max(w1ij, w3ij) if e4 ij E E1 and e4 ij E E3, w � if e4 E E1 and e4 E/ E3, ij w3 otherwise. ij From this expanded graph G4, the keywords are extracted by TextRank (Mihalcea and Tarau, 2004). TextRank is an unsupervised graph-based method for keyword extraction. It singles out the key vertices of a graph by providing a ranking mechanism. In order to rank the vertices, it computes the score of each vertex v4i E V4 by where 0 &lt; d &lt; 1 is a damping factor and adj(vi) denotes vi’s neighbors. Finally, the words whose score is larger than a specific threshold θ are chosen as keywords. Especially when the current utterance is the first utterance of a meeting, the history graph does not exist. In this case, the current utterance graph becomes the expanded graph (G4 = G1), and keywords are</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Proceedings of International Conference on Empirical Methods in Natural Language Processing, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Proceedings of Annual Conference of the North American Chapter of the ACL,</booktitle>
<pages>145--152</pages>
<contexts>
<context position="20629" citStr="Nenkova and Passonneau, 2004" startWordPosition="3488" endWordPosition="3492">r/ dataset/keywordextraction.html 3A guideline was given to the annotators that keywords must be a single word and the maximum number of keywords per utterance is five. annotators. Therefore, in this paper the words that are recommended by more than two annotators are chosen as keywords. The evaluation is done with two metics: Fmeasure and the weighted relative score (WRS). Since the previous work by Liu et al. (2009) reported only F-measure and WRS, F-measure instead of precision/recall are used for the comparison with their method. The weighted relative score is derived from Pyramid metric (Nenkova and Passonneau, 2004). When a keyword extraction system generates keywords which many annotators agree, a higher score is given to it. On the other hand, a lower score is given if fewer annotators agree. The proposed method is compared with two baseline models to see its relative performance. One is the frequency-based keyword extraction with TFIDF weighting (Frank et al., 1999) and the other is TextRank in which the weight of edges is mutual information between vertices (Wan et al., 2007). In TFIDF, each utterance is considered as a document, and thus all utterances including the current one are regarded as whole</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings of Annual Conference of the North American Chapter of the ACL, pages 145–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Learning algorithms for keyphrase extraction. Information Retrieval,</title>
<date>2000</date>
<pages>2--303</pages>
<contexts>
<context position="1760" citStr="Turney, 2000" startWordPosition="275" endWordPosition="276"> would be helpful to meeting participants to provide them with some additional information related to the current subject. For instance, assume that a participant is discussing a specific topic with other participants at a meeting. The summary of previous meetings on the topic is then one of the most important resources for her discussion. In order to provide information on a topic to participants, keywords should be first generated for the topic since keywords are often representatives of a topic. A number of techniques have been proposed for automatic keyword extraction (Frank et al., 1999; Turney, 2000; Mihalcea and Tarau, 2004; Wan et al., 2007), and they are designed to extract keywords from a written document. However, they are not suitable for meeting transcripts. In a meeting, it is often desirable to extract keywords at the time at which a new utterance is made for just-in-time service of additional information. Otherwise, the extracted keywords become just the important words at the end of the meeting. Two key factors for just-in-time keyword extraction from meeting transcripts are time of preceding utterances and topic of current utterance. First, current utterance is affected by te</context>
</contexts>
<marker>Turney, 2000</marker>
<rawString>Peter D. Turney. 2000. Learning algorithms for keyphrase extraction. Information Retrieval, 2:303– 336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Coherent keyphrase extraction via web mining.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial intelligence,</booktitle>
<pages>434--439</pages>
<contexts>
<context position="5475" citStr="Turney, 2003" startWordPosition="873" endWordPosition="875">ly, Section 6 draws some conclusions. 2 Related Work Keyword extraction has been of interest for a long time in various fields such as information retrieval, document clustering, summarization, and so on. Thus, there have been many studies on automatic keyword extraction. The frequency-based keyword extraction with TFIDF weighting (Frank et al., 1999) and the graph-based keyword extraction (Mihalcea and Tarau, 2004) are two base models for this task. Many studies recently tried to extend them by incorporating specific information such as linguistic knowledge (Hulth, 2003), web-based resource (Turney, 2003), and semantic knowledge (Chen et al., 2010). As a result, they show good performance on written text. However, it is difficult to use them directly for spoken genres, since spoken genres have significantly different characteristics from written text. There have been a few studies focused on keyword extraction from spoken genres. Among them, the extraction from meetings has attracted more concern, since the need for grasping important points of a meeting or an opinion of each participant has increased. The studies on meetings focused on the exterior features of meeting dialogues such as unstru</context>
</contexts>
<marker>Turney, 2003</marker>
<rawString>Peter D. Turney. 2003. Coherent keyphrase extraction via web mining. In Proceedings of the 18th International Joint Conference on Artificial intelligence, pages 434–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>Collabrank: Towards a collaborative approach to single-document keyphrase extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics,</booktitle>
<pages>969--976</pages>
<contexts>
<context position="7393" citStr="Wan and Xiao (2008)" startWordPosition="1169" endWordPosition="1172">bilistic Latent Semantic Analysis (pLSA). These studies focused on the exterior characteristics of spoken genres, since they assumed that entire scripts are given in advance and then they extracted keywords that best describe the scripts. However, to the best of our knowledge, there is no previous study considered time of utterances which is an intrinsic element of spoken genres. The relevance between current utterance and preceding utterances is also a critical feature in keyword extraction from meeting transcripts. The study that considers this relevance explicitly is CollabRank proposed by Wan and Xiao (2008). This is collaborative approach to extract keywords in a document. In this study, it is assumed that a few neighbor documents close to a current document can help extract keywords. Therefore, they applied a clustering algorithm to a document set and then extracted words that are reinforced by the documents within a cluster. However, this method also does not consider the utterance time, since it is designed to extract keywords from normal documents. As a result, if it is applied to meeting transcripts, all preceding utterances would affect the current utterance uniformly, which leads to a poo</context>
</contexts>
<marker>Wan, Xiao, 2008</marker>
<rawString>Xiaojun Wan and Jianguo Xiao. 2008. Collabrank: Towards a collaborative approach to single-document keyphrase extraction. In Proceedings of International Conference on Computational Linguistics, pages 969– 976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
<author>Jianguo Xiao</author>
</authors>
<title>Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>552--559</pages>
<contexts>
<context position="1805" citStr="Wan et al., 2007" startWordPosition="281" endWordPosition="284">s to provide them with some additional information related to the current subject. For instance, assume that a participant is discussing a specific topic with other participants at a meeting. The summary of previous meetings on the topic is then one of the most important resources for her discussion. In order to provide information on a topic to participants, keywords should be first generated for the topic since keywords are often representatives of a topic. A number of techniques have been proposed for automatic keyword extraction (Frank et al., 1999; Turney, 2000; Mihalcea and Tarau, 2004; Wan et al., 2007), and they are designed to extract keywords from a written document. However, they are not suitable for meeting transcripts. In a meeting, it is often desirable to extract keywords at the time at which a new utterance is made for just-in-time service of additional information. Otherwise, the extracted keywords become just the important words at the end of the meeting. Two key factors for just-in-time keyword extraction from meeting transcripts are time of preceding utterances and topic of current utterance. First, current utterance is affected by temporal history of preceding utterances. That </context>
<context position="4249" citStr="Wan et al., 2007" startWordPosition="679" endWordPosition="682">vant to the current utterance is derived from the history graph, and used as an actual history of the current utterance. The keywords of the current utterance are extracted by TextRank (Mihalcea and Tarau, 2004) from the merged graph of the current utterance and the history graphs. The proposed method is evaluated with two kinds of data sets: the National Assembly transcripts in Korean and the ICSI meeting corpus (Janin et al., 2003) in English. The experimental results show that it outperforms both the TFIDF framework (Frank et al., 1999; Liu et al., 2009) and the PageRank-based graph model (Wan et al., 2007). One thing to note is that the proposed method improves even the supervised methods that do not reflect utterance time and topic relevance for the ICSI corpus. This proves that it is critical to consider time and content of utterances simultaneously in keyword extraction from meeting transcripts. The rest of the paper is organized as follows. Section 2 reviews the related studies on keyword extraction. Section 3 explains the overall process of the proposed method, and Section 4 addresses its detailed description how to reflect meeting characteristics. Experimental results are presented in Sec</context>
<context position="21102" citStr="Wan et al., 2007" startWordPosition="3570" endWordPosition="3573">/recall are used for the comparison with their method. The weighted relative score is derived from Pyramid metric (Nenkova and Passonneau, 2004). When a keyword extraction system generates keywords which many annotators agree, a higher score is given to it. On the other hand, a lower score is given if fewer annotators agree. The proposed method is compared with two baseline models to see its relative performance. One is the frequency-based keyword extraction with TFIDF weighting (Frank et al., 1999) and the other is TextRank in which the weight of edges is mutual information between vertices (Wan et al., 2007). In TFIDF, each utterance is considered as a document, and thus all utterances including the current one are regarded as whole documents. The frequencybased TFIDF chooses top-K words according to their TFIDF value from the set of words appearing in the meeting transcript. Since the human annotators are restricted to extract up to five keywords, the keyword extraction systems including our method are also requested to select top-5 keywords when more than five keywords are produced. In order to see the effect of preceding utterances in baseline models, the performances are measured according to</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 552– 559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert H Wozniak</author>
</authors>
<date>1999</date>
<booktitle>Classics in Psychology, 1855–1914: Historical Essays.</booktitle>
<publisher>Thoemmes Press.</publisher>
<contexts>
<context position="3219" citStr="Wozniak, 1999" startWordPosition="514" endWordPosition="515"> important than irrelevant utterances. Since a meeting consists of several topics, the utterances that have nothing to do with current utterance are inappropriate as a history of the current utterance. This paper proposes a graph-based keyword extraction to reflect these factors. The proposed method represents an utterance as a graph of which nodes are candidate keywords. The preceding utterances are also expressed as a history graph in which the weight of an edge is the temporal importance of the keywords connected by the edge. To reflect the temporal history of utterances, forgetting curve (Wozniak, 1999) is adopted in updating the weights of edges in the history graph. It expresses effectively not only the reciprocal relation between memory re888 Proceedings of NAACL-HLT 2013, pages 888–896, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics tention and time, but also active recall that makes frequent words more consequential in keyword extraction. Then, a subgraph that is relevant to the current utterance is derived from the history graph, and used as an actual history of the current utterance. The keywords of the current utterance are extracted by TextRank (M</context>
<context position="12359" citStr="Wozniak, 1999" startWordPosition="2010" endWordPosition="2011">ywords are extracted from a new utterance. This is because the current utterance becomes a part of the history graph for the next utterance. As a history, old utterances are less important than recent ones. Thus, the temporal importance should decrease gradually according to the passage of time. In addition, the keywords which occur frequently at a meeting are more important than those mentioned just once or twice. Since the frequently-mentioned keywords are normally major topics of the meeting, their influence should last for a long time. To model these characteristics, the forgetting curve (Wozniak, 1999) is adopted in updating the history graph. It models the decline of memory retention in time. Figure 2 shows a typical representation of the forgetting curve. The X-axis of this figure is time and the Y-axis is memory retention. As shown in this figure, memory retention of new Time Figure 2: Memory retention according to time. information decreases gradually by the exponential nature of forgetting. However, whenever the information is repeated, it is recalled longer. This is formulated as R = e− where R is memory retention, t is time, and 5 is the relative strength of memory. Based on the forg</context>
</contexts>
<marker>Wozniak, 1999</marker>
<rawString>Robert H. Wozniak. 1999. Classics in Psychology, 1855–1914: Historical Essays. Thoemmes Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>