<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027903">
<note confidence="0.958565">
Proceedings of HLT-NAACL 2003
Demonstrations , pp. 21-22
Edmonton, May-June 2003
</note>
<title confidence="0.990536">
Automatically Discovering Word Senses
</title>
<author confidence="0.996976">
Patrick Pantel and Dekang Lin
</author>
<affiliation confidence="0.998081">
Department of Computing Science
University of Alberta
</affiliation>
<address confidence="0.940365">
Edmonton, Alberta T6G 2E8 Canada
</address>
<email confidence="0.998328">
{ppantel, lindek}@cs.ualberta.ca
</email>
<sectionHeader confidence="0.998585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99944525">
We will demonstrate the output of a distribu-
tional clustering algorithm called Clustering
by Committee that automatically discovers
word senses from text1.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.976075866666667">
Using word senses versus word forms is useful in many
applications such as information retrieval (Voorhees
1998), machine translation (Hutchins and Sommers
1992), and question-answering (Pasca and Harabagiu
2001).
The Distributional Hypothesis (Harris 1985) states
that words that occur in the same contexts tend to be
similar. There have been many approaches to compute
the similarity between words based on their distribution
in a corpus (Hindle 1990; Landauer and Dumais 1997;
Lin 1998). The output of these programs is a ranked list
of similar words to each word. For example, Lin’s
approach outputs the following similar words for wine
and suit:
wine: beer, white wine, red wine,
</bodyText>
<construct confidence="0.775583857142857">
Chardonnay, champagne, fruit, food,
coffee, juice, Cabernet, cognac,
vinegar, Pinot noir, milk, vodka,...
suit: lawsuit, jacket, shirt, pant, dress,
case, sweater, coat, trouser, claim,
business suit, blouse, skirt, litiga-
tion, ...
</construct>
<bodyText confidence="0.9988978">
The similar words of wine represent the meaning of
wine. However, the similar words of suit represent a
mixture of its clothing and litigation senses. Such lists
of similar words do not distinguish between the
multiple senses of polysemous words.
</bodyText>
<footnote confidence="0.819847">
1 The demonstration is currently available online at
www.cs.ualberta.ca/~lindek/demos/wordcluster.htm.
</footnote>
<bodyText confidence="0.999296666666667">
We will demonstrate the output of a distributional
clustering algorithm called Clustering by Committee
(CBC) that discovers word senses automatically from
text. Each cluster that a word belongs to corresponds to
a sense of the word. The following is a sample output
from our algorithm:
</bodyText>
<figure confidence="0.773493066666667">
(suit
0.39 (blouse, slack, legging, sweater)
0.20 (lawsuit, allegation, case, charge)
)
(plant
0.41 (plant, factory, facility,
refinery)
0.20 (shrub, ground cover, perennial,
bulb)
)
(heart
0.27 (kidney, bone marrow, marrow,
liver)
0.17 (psyche, consciousness, soul, mind)
)
</figure>
<bodyText confidence="0.9997372">
Each entry shows the clusters to which the head-
word belongs along with its similarity to the cluster.
The lists of words are the top-4 most similar members
to the cluster centroid. Each cluster corresponds to a
sense of the headword.
</bodyText>
<sectionHeader confidence="0.991222" genericHeader="method">
2 Feature Representation
</sectionHeader>
<bodyText confidence="0.999862538461538">
Following (Lin 1998), we represent each word by a
feature vector. Each feature corresponds to a context in
which the word occurs. For example, “sip __” is a verb-
object context. If the word wine occurred in this
context, the context is a feature of wine. These features
are obtained by parsing a large corpus using Minipar
(Lin 1994), a broad-coverage English parser. The value
of the feature is the pointwise mutual information
(Manning and Schütze 1999) between the feature and
the word. Let c be a context and Fc(w) be the frequency
count of a word w occurring in context c. The pointwise
mutual information, miw,c, between c and w is defined
as:
</bodyText>
<equation confidence="0.73024625">
Fc ( w) References
miw c ,=
Fi
∑
</equation>
<bodyText confidence="0.99902175">
where N is the total frequency counts of all words and
their contexts. We compute the similarity between two
words wi and wj using the cosine coefficient (Salton and
McGill 1983) of their mutual information vectors:
</bodyText>
<sectionHeader confidence="0.972208" genericHeader="method">
3 Clustering by Committee
</sectionHeader>
<bodyText confidence="0.999989363636364">
CBC finds clusters by first discovering the underlying
structure of the data. It does this by searching for sets
of representative elements for each cluster, which we
refer to as committees. The goal is to find committees
that unambiguously describe the (unknown) target
classes. By carefully choosing committee members, the
features of the centroid tend to be the more typical
features of the target class. For example, our system
chose the following committee members to compute
the centroid of the state cluster: Illinois, Michigan,
Minnesota, Iowa, Wisconsin, Indiana, Nebraska and
Vermont. States like Washington and New York are not
part of the committee because they are polysemous.
The centroid of a cluster is constructed by averaging
the feature vectors of the committee members.
CBC consists of three phases. Phase I computes
each element’s top-k similar elements. In Phase II, we
do a first pass through the data and discover the
committees. The goal is that we form tight committees
(high intra-cluster similarity) that are dissimilar from
one another (low inter-cluster similarity) and that cover
the whole similarity space. The method is based on
finding sub-clusters in the top-similar elements of every
given element.
In the final phase of the algorithm, each word is
assigned to its most similar clusters (represented by a
committee). Suppose a word w is assigned to a cluster
c. We then remove from w its features that intersect
with the features in c. Intuitively, this removes the c
sense from w, allowing CBC to discover the less
frequent senses of a word and to avoid discovering
duplicate senses. The word w is then assigned to its
next most similar cluster and the process is repeated.
</bodyText>
<sectionHeader confidence="0.997821" genericHeader="method">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.8669372">
We will demonstrate the senses discovered by CBC for
54,685 words on the 3GB ACQUAINT corpus. CBC
discovered 24,497 polysemous words.
Harris, Z. 1985. Distributional structure. In: Katz, J. J.
(ed.) The Philosophy of Linguistics. New York:
</bodyText>
<reference confidence="0.97933">
Oxford University Press. pp. 26–47.
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90. pp.
268–275. Pittsburgh, PA.
Hutchins, J. and Sommers, H. 1992. Introduction to
Machine Translation. Academic Press.
Landauer, T. K., and Dumais, S. T. 1997. A solution to
Plato&apos;s problem: The Latent Semantic Analysis
theory of the acquisition, induction, and representa-
tion of knowledge. Psychological Review, 104:211–
240.
Lin, D. 1994. Principar - an efficient, broad-coverage,
principle-based parser. In Proceedings of COLING-
94. pp. 42–48. Kyoto, Japan.
Lin, D. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL-98.
pp. 768–774. Montreal, Canada.
Manning, C. D. and Schütze, H. 1999. Foundations of
Statistical Natural Language Processing. MIT Press.
Pasca, M. and Harabagiu, S. 2001. The informative role
of WordNet in Open-Domain Question Answering.
In Proceedings of NAACL-01 Workshop on WordNet
and Other Lexical Resources. pp. 138–143.
Pittsburgh, PA.
Salton, G. and McGill, M. J. 1983. Introduction to
Modern Information Retrieval. McGraw Hill.
Voorhees, E. M. 1998. Using WordNet for text
retrieval. In WordNet: An Electronic Lexical
Database, edited by C. Fellbaum. pp. 285–303. MIT
Press.
</reference>
<figure confidence="0.994129923076923">
i
j
×
N N
sim(wi, wj)= c
∑miwic2 × ∑
c c
∑ miwic × miwjc
2
mi w c
j
N
( w) ∑Fc(j)
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.399988">
<note confidence="0.974664333333333">Proceedings of HLT-NAACL 2003 Demonstrations , pp. 21-22 Edmonton, May-June 2003</note>
<title confidence="0.99104">Automatically Discovering Word Senses</title>
<author confidence="0.845658">Pantel</author>
<affiliation confidence="0.999712">Department of Computing University of</affiliation>
<address confidence="0.972241">Edmonton, Alberta T6G 2E8 Canada</address>
<email confidence="0.997349">ppantel@cs.ualberta.ca</email>
<email confidence="0.997349">lindek@cs.ualberta.ca</email>
<abstract confidence="0.9047396">We will demonstrate the output of a distributional clustering algorithm called Clustering by Committee that automatically discovers senses from</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<pages>26--47</pages>
<institution>Oxford University Press.</institution>
<marker></marker>
<rawString>Oxford University Press. pp. 26–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicateargument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL-90.</booktitle>
<pages>268--275</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="901" citStr="Hindle 1990" startWordPosition="124" endWordPosition="125">nstrate the output of a distributional clustering algorithm called Clustering by Committee that automatically discovers word senses from text1. 1 Introduction Using word senses versus word forms is useful in many applications such as information retrieval (Voorhees 1998), machine translation (Hutchins and Sommers 1992), and question-answering (Pasca and Harabagiu 2001). The Distributional Hypothesis (Harris 1985) states that words that occur in the same contexts tend to be similar. There have been many approaches to compute the similarity between words based on their distribution in a corpus (Hindle 1990; Landauer and Dumais 1997; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin’s approach outputs the following similar words for wine and suit: wine: beer, white wine, red wine, Chardonnay, champagne, fruit, food, coffee, juice, Cabernet, cognac, vinegar, Pinot noir, milk, vodka,... suit: lawsuit, jacket, shirt, pant, dress, case, sweater, coat, trouser, claim, business suit, blouse, skirt, litigation, ... The similar words of wine represent the meaning of wine. However, the similar words of suit represent a mixture of its clothing and lit</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Hindle, D. 1990. Noun classification from predicateargument structures. In Proceedings of ACL-90. pp. 268–275. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hutchins</author>
<author>H Sommers</author>
</authors>
<title>Introduction to Machine Translation.</title>
<date>1992</date>
<publisher>Academic Press.</publisher>
<marker>Hutchins, Sommers, 1992</marker>
<rawString>Hutchins, J. and Sommers, H. 1992. Introduction to Machine Translation. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato&apos;s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<pages>240</pages>
<contexts>
<context position="927" citStr="Landauer and Dumais 1997" startWordPosition="126" endWordPosition="129">utput of a distributional clustering algorithm called Clustering by Committee that automatically discovers word senses from text1. 1 Introduction Using word senses versus word forms is useful in many applications such as information retrieval (Voorhees 1998), machine translation (Hutchins and Sommers 1992), and question-answering (Pasca and Harabagiu 2001). The Distributional Hypothesis (Harris 1985) states that words that occur in the same contexts tend to be similar. There have been many approaches to compute the similarity between words based on their distribution in a corpus (Hindle 1990; Landauer and Dumais 1997; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin’s approach outputs the following similar words for wine and suit: wine: beer, white wine, red wine, Chardonnay, champagne, fruit, food, coffee, juice, Cabernet, cognac, vinegar, Pinot noir, milk, vodka,... suit: lawsuit, jacket, shirt, pant, dress, case, sweater, coat, trouser, claim, business suit, blouse, skirt, litigation, ... The similar words of wine represent the meaning of wine. However, the similar words of suit represent a mixture of its clothing and litigation senses. Such lists</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, T. K., and Dumais, S. T. 1997. A solution to Plato&apos;s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104:211– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Principar - an efficient, broad-coverage, principle-based parser.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING94.</booktitle>
<pages>42--48</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="2868" citStr="Lin 1994" startWordPosition="430" endWordPosition="431">ss, soul, mind) ) Each entry shows the clusters to which the headword belongs along with its similarity to the cluster. The lists of words are the top-4 most similar members to the cluster centroid. Each cluster corresponds to a sense of the headword. 2 Feature Representation Following (Lin 1998), we represent each word by a feature vector. Each feature corresponds to a context in which the word occurs. For example, “sip __” is a verbobject context. If the word wine occurred in this context, the context is a feature of wine. These features are obtained by parsing a large corpus using Minipar (Lin 1994), a broad-coverage English parser. The value of the feature is the pointwise mutual information (Manning and Schütze 1999) between the feature and the word. Let c be a context and Fc(w) be the frequency count of a word w occurring in context c. The pointwise mutual information, miw,c, between c and w is defined as: Fc ( w) References miw c ,= Fi ∑ where N is the total frequency counts of all words and their contexts. We compute the similarity between two words wi and wj using the cosine coefficient (Salton and McGill 1983) of their mutual information vectors: 3 Clustering by Committee CBC find</context>
</contexts>
<marker>Lin, 1994</marker>
<rawString>Lin, D. 1994. Principar - an efficient, broad-coverage, principle-based parser. In Proceedings of COLING94. pp. 42–48. Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL-98.</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="938" citStr="Lin 1998" startWordPosition="130" endWordPosition="131">clustering algorithm called Clustering by Committee that automatically discovers word senses from text1. 1 Introduction Using word senses versus word forms is useful in many applications such as information retrieval (Voorhees 1998), machine translation (Hutchins and Sommers 1992), and question-answering (Pasca and Harabagiu 2001). The Distributional Hypothesis (Harris 1985) states that words that occur in the same contexts tend to be similar. There have been many approaches to compute the similarity between words based on their distribution in a corpus (Hindle 1990; Landauer and Dumais 1997; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin’s approach outputs the following similar words for wine and suit: wine: beer, white wine, red wine, Chardonnay, champagne, fruit, food, coffee, juice, Cabernet, cognac, vinegar, Pinot noir, milk, vodka,... suit: lawsuit, jacket, shirt, pant, dress, case, sweater, coat, trouser, claim, business suit, blouse, skirt, litigation, ... The similar words of wine represent the meaning of wine. However, the similar words of suit represent a mixture of its clothing and litigation senses. Such lists of similar</context>
<context position="2556" citStr="Lin 1998" startWordPosition="374" endWordPosition="375">. The following is a sample output from our algorithm: (suit 0.39 (blouse, slack, legging, sweater) 0.20 (lawsuit, allegation, case, charge) ) (plant 0.41 (plant, factory, facility, refinery) 0.20 (shrub, ground cover, perennial, bulb) ) (heart 0.27 (kidney, bone marrow, marrow, liver) 0.17 (psyche, consciousness, soul, mind) ) Each entry shows the clusters to which the headword belongs along with its similarity to the cluster. The lists of words are the top-4 most similar members to the cluster centroid. Each cluster corresponds to a sense of the headword. 2 Feature Representation Following (Lin 1998), we represent each word by a feature vector. Each feature corresponds to a context in which the word occurs. For example, “sip __” is a verbobject context. If the word wine occurred in this context, the context is a feature of wine. These features are obtained by parsing a large corpus using Minipar (Lin 1994), a broad-coverage English parser. The value of the feature is the pointwise mutual information (Manning and Schütze 1999) between the feature and the word. Let c be a context and Fc(w) be the frequency count of a word w occurring in context c. The pointwise mutual information, miw,c, be</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING/ACL-98. pp. 768–774. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schütze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2990" citStr="Manning and Schütze 1999" startWordPosition="446" endWordPosition="449"> the cluster. The lists of words are the top-4 most similar members to the cluster centroid. Each cluster corresponds to a sense of the headword. 2 Feature Representation Following (Lin 1998), we represent each word by a feature vector. Each feature corresponds to a context in which the word occurs. For example, “sip __” is a verbobject context. If the word wine occurred in this context, the context is a feature of wine. These features are obtained by parsing a large corpus using Minipar (Lin 1994), a broad-coverage English parser. The value of the feature is the pointwise mutual information (Manning and Schütze 1999) between the feature and the word. Let c be a context and Fc(w) be the frequency count of a word w occurring in context c. The pointwise mutual information, miw,c, between c and w is defined as: Fc ( w) References miw c ,= Fi ∑ where N is the total frequency counts of all words and their contexts. We compute the similarity between two words wi and wj using the cosine coefficient (Salton and McGill 1983) of their mutual information vectors: 3 Clustering by Committee CBC finds clusters by first discovering the underlying structure of the data. It does this by searching for sets of representative</context>
</contexts>
<marker>Manning, Schütze, 1999</marker>
<rawString>Manning, C. D. and Schütze, H. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>S Harabagiu</author>
</authors>
<title>The informative role of WordNet in Open-Domain Question Answering.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL-01 Workshop on WordNet and Other Lexical Resources.</booktitle>
<pages>138--143</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="661" citStr="Pasca and Harabagiu 2001" startWordPosition="84" endWordPosition="87">trations , pp. 21-22 Edmonton, May-June 2003 Automatically Discovering Word Senses Patrick Pantel and Dekang Lin Department of Computing Science University of Alberta Edmonton, Alberta T6G 2E8 Canada {ppantel, lindek}@cs.ualberta.ca Abstract We will demonstrate the output of a distributional clustering algorithm called Clustering by Committee that automatically discovers word senses from text1. 1 Introduction Using word senses versus word forms is useful in many applications such as information retrieval (Voorhees 1998), machine translation (Hutchins and Sommers 1992), and question-answering (Pasca and Harabagiu 2001). The Distributional Hypothesis (Harris 1985) states that words that occur in the same contexts tend to be similar. There have been many approaches to compute the similarity between words based on their distribution in a corpus (Hindle 1990; Landauer and Dumais 1997; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin’s approach outputs the following similar words for wine and suit: wine: beer, white wine, red wine, Chardonnay, champagne, fruit, food, coffee, juice, Cabernet, cognac, vinegar, Pinot noir, milk, vodka,... suit: lawsuit, jacket</context>
</contexts>
<marker>Pasca, Harabagiu, 2001</marker>
<rawString>Pasca, M. and Harabagiu, S. 2001. The informative role of WordNet in Open-Domain Question Answering. In Proceedings of NAACL-01 Workshop on WordNet and Other Lexical Resources. pp. 138–143. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw Hill.</publisher>
<contexts>
<context position="3396" citStr="Salton and McGill 1983" startWordPosition="523" endWordPosition="526">feature of wine. These features are obtained by parsing a large corpus using Minipar (Lin 1994), a broad-coverage English parser. The value of the feature is the pointwise mutual information (Manning and Schütze 1999) between the feature and the word. Let c be a context and Fc(w) be the frequency count of a word w occurring in context c. The pointwise mutual information, miw,c, between c and w is defined as: Fc ( w) References miw c ,= Fi ∑ where N is the total frequency counts of all words and their contexts. We compute the similarity between two words wi and wj using the cosine coefficient (Salton and McGill 1983) of their mutual information vectors: 3 Clustering by Committee CBC finds clusters by first discovering the underlying structure of the data. It does this by searching for sets of representative elements for each cluster, which we refer to as committees. The goal is to find committees that unambiguously describe the (unknown) target classes. By carefully choosing committee members, the features of the centroid tend to be the more typical features of the target class. For example, our system chose the following committee members to compute the centroid of the state cluster: Illinois, Michigan, </context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G. and McGill, M. J. 1983. Introduction to Modern Information Retrieval. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Using WordNet for text retrieval. In WordNet: An Electronic Lexical Database,</title>
<date>1998</date>
<journal>edited by C. Fellbaum.</journal>
<pages>285--303</pages>
<publisher>MIT Press.</publisher>
<marker>Voorhees, 1998</marker>
<rawString>Voorhees, E. M. 1998. Using WordNet for text retrieval. In WordNet: An Electronic Lexical Database, edited by C. Fellbaum. pp. 285–303. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>