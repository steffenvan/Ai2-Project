<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000230">
<title confidence="0.990267">
Relative Rank Statistics for Dialog Analysis
</title>
<author confidence="0.939487">
Juan M. Huerta
</author>
<affiliation confidence="0.744894">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.956327">
1101 Kitchawan Road
Yorktown Heights, NY 10598
</address>
<email confidence="0.998924">
huerta@us.ibm.com
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998739444444445">
We introduce the relative rank differential sta-
tistic which is a non-parametric approach to
document and dialog analysis based on word
frequency rank-statistics. We also present a
simple method to establish semantic saliency in
dialog, documents, and dialog segments using
these word frequency rank statistics. Applica-
tions of our technique include the dynamic
tracking of topic and semantic evolution in a
dialog, topic detection, automatic generation of
document tags, and new story or event detec-
tion in conversational speech and text. Our ap-
proach benefits from the robustness, simplicity
and efficiency of non-parametric and rank
based approaches and consistently outper-
formed term-frequency and TF-IDF cosine dis-
tance approaches in several experiments con-
ducted.
</bodyText>
<sectionHeader confidence="0.989955" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.99992262264151">
Existing research in dialog analysis has focused on
several specific problems including dialog act de-
tection (e.g., Byron and Heeman 1998), segmenta-
tion and chunking (e.g., Hearst 1993), topic detec-
tion (e.g., Zimmerman et al 2005), distillation and
summarization (e.g., Mishne et al 2005) etc. The
breath of this research reflects the increasing im-
portance that dialog analysis has for multiple do-
mains and applications. While historically, dialog
analysis research has initially leveraged the corre-
sponding techniques originally intended for textual
document analysis, techniques tailored specifically
for dialog processing eventually should be able to
address the sparseness, noise, and time considera-
tions intrinsic to dialog and conversations.
The approach proposed in this paper focuses on the
relative change of rank ordering of words occur-
ring in a conversation according to their frequen-
cies. Our approach emphasizes relatively improb-
able terms by focusing on terms that are relatively
unlikely to appear frequently and thus weighting
their change in rank more once they are observed.
Our technique achieves this in a non-parametric
fashion without explicitly computing probabilities,
without the assumption of an underlying distribu-
tion, and without the computation of likelihoods.
In general, non-parametric approaches to data
analysis are well known and present several attrac-
tive characteristics (as a general reference see Hol-
lander and Wolfe 1999). Non-parametric ap-
proaches require few assumptions about the data
analyzed and can present computational advan-
tages over parametric approaches especially when
the underlying distributions of the data are not
normal. In specific, our approach uses rank order
statistics of word-feature frequencies to compute a
relative rank-differential statistic.
This paper is organized as follows: in Section 2 we
introduce and describe our basic approach (the
relative rank differential RRD function and its
sorted list). In Section 3 we address the temporal
nature of dialogs and describe considerations to
dynamically update the RRD statistics in an on-
line fashion especially for the case of shifting tem-
poral windows of analysis. In Section 4 we relate
the RRD approach to relevant existing and previ-
ous dialog and text analysis approaches. In Section
5 we illustrate the usefulness of our metric by ana-
lyzing a set of conversations in various ways using
the RRD. Specifically, in that section we will em-
pirically demonstrate its robustness to noise and
data sparseness compared to the popular term fre-
quency and TF-IDF cosine distance approaches in
</bodyText>
<page confidence="0.975408">
965
</page>
<note confidence="0.9623085">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 965–972,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.989840333333333">
a dialog classification task. And finally, in Section
6, we present some concluding remarks and future
directions
</bodyText>
<sectionHeader confidence="0.930107" genericHeader="introduction">
2 The Relative Rank Differential
</sectionHeader>
<bodyText confidence="0.997901857142857">
Let { 1 , 2 , 3 ...}
du = d u d u d u denote the ranked
dictionary of a language (i.e., the ordered list of
words sorted in decreasing order of frequency).
The superscript u denotes that this ranked list is
based on the universal language. Specifically, the
word diu is the th
</bodyText>
<equation confidence="0.503419">
i entry in u
</equation>
<bodyText confidence="0.99903125">
d if its frequency of
occurrence in the language denoted by ( u)
f di is
larger than ( u )
f dj for every j where i &lt; j (for
notational simplicity we assume that no two words
share the same frequency). In the case where want
to relax this assumption we simply allow i &lt; j
</bodyText>
<equation confidence="0.8739565">
when ( ) ( u )
f d i = f d as long as u
u di precedes
j
</equation>
<bodyText confidence="0.957421259259259">
dju lexicographically, or under any other desired
precedence criteria. For u
d we assume that
f (diu) &gt; 0 for every entry (i.e., each word has
been observed at least once in the language).
Similarly, let now { 1 , 2 , 3 ...}
dS = d S d S d S de-
note the corresponding ranked dictionary for a dia-
log, or dialog segment, S (ordered, as in the
case of the language dictionary, in decreasing order
of frequency)1. The superscript S denotes that this
ranked list is based on the dialog S. The word S
di
is the th
i entry in s
d if its frequency of occurrence
in the conversation segment S denoted by ( S )
f di
is larger than ( S)
f dj for every j where i &lt; j .
In this case we allow f (di S) &gt;_ 0 for every i so
that the cardinality of u
d is the same as s
d .
Let rd (w) denote the rank of word w in the
ranked dictionary d so that, for example,
r du i
</bodyText>
<equation confidence="0.9403215">
d ( ) = .
u i
</equation>
<bodyText confidence="0.9876554">
Based now on a dialog segment and a universal
language, any given word w will be associated
with a rank in u
d (the universal ranked dictionary)
and a rank in s
d , the dialog segment ranked dic-
tionary.
Let us define now for every word the relative
rank differential (RRD) function or statistic2 given
by:
</bodyText>
<equation confidence="0.43145">
cds,du (w)
</equation>
<bodyText confidence="0.999847548387097">
The relative rank-differential is the ratio of the
absolute difference (or change) in rank between the
word’s original position in the universal dictionary
and the segment s. The exponent a in the de-
nominator allows us to emphasize or deemphasize
changes in terms according to their position or rank
in the language (or universal) dictionary. Typically
we will want to increase the denominator’s value
(i.e., deemphasize) for terms that have very low
frequency (and their rank value in the universal
dictionary is large) so that only relatively big
changes in rank will result in substantial values of
this function.
When alpha is zero, the RRD focuses on every
word identically as we consider only the absolute
change in rank. For alpha equal to 1.0 the relative
change in rank gets scaled down linearly according
to its rank, while for alphas larger than 1.0 the nu-
merator will scale down or reduce to a larger extent
the value of relative rank differential for words that
have large rank value.
Based on each word’s relative rank differential
we can compute the ranked list of words sorted in
decreasing order by their corresponding value of
relative rank differential. Let this sorted list of
words be denoted by R(d u , d S) = {w1, w2 ,...} . So
that c(wi) is larger than c(wj) 3 for every j
where i &lt; j.
We now provide some intuition on the ranked
RRD lists and the RRD function. The ranked
dictionary of a language contains information
</bodyText>
<figure confidence="0.994792">
-
s
u
(
w)
w)
(
rd
rd
(rdu (w)y
</figure>
<footnote confidence="0.9173749">
1 We only consider at this point the case in which both speak-
ers’ parts in a dialog interaction are considered jointly (i.e.,
single channel), however, our method can be easily extended
to separate conversation channels. Also, for simplicity we
consider at this point only words (or phrases) as features.
2 For brevity, we refer to the Relative Rank Differential of a
word given two utterances as a statistic. It is not, strictly
speaking, a metric or a distance, but rather a function.
3 For simplicity, c is written without subscripts when these are
apparent from the context.
</footnote>
<page confidence="0.996802">
966
</page>
<bodyText confidence="0.998313176470588">
about the frequency of all words in a language (i.e.,
across the universe of conversations) while the
segment counterpart pertains a single conversation
or segment thereof. The relative rank differential
tells us how different a word is ranked in a
conversation segment from the universal language,
but this difference is normalized by the universal
rank of the word. Intuitively, and especially when
alpha equals 1.0, the RRD denotes some sort of
percent change in rank. This also means that this
function is less sensitive to small changes in
frequency in the case of frequent words and to
small changes in rank in case of infrequent words.
Finally, the sorted list R(d u, dS) contains in order
of importance the most relatively salient terms of a
dialog segment, as measured by relative changes or
differences in rank.
</bodyText>
<sectionHeader confidence="0.982661" genericHeader="method">
3 Collecting Rank Statistics
</sectionHeader>
<bodyText confidence="0.992987766666667">
We now discuss how to extend the metrics de-
scribed in the previous section to consider finite-
time sliding windows of analysis, that is, we de-
scribe how to update rank statistics, specifically the
ranked lists and relative rank differential informa-
tion for every feature in an on-line fashion. This is
useful when tracking the evolution of single dia-
logs, when focusing the analysis to span shorter
regions, as well as to supporting dynamic real-time
analytics of large number of dialogs.
To approach this, we decompose the word
events (words as they occur in time) into arriving
and departing events. An arriving event at time t is
a word that is covered by the analysis window at
its specific time as the finite length window slides
in time, and a departing word at time t is a feature
that stops falling within the window of analysis.
For simplicity, and without loss of generality, we
now assume that we are performing the analysis in
real time and that the sliding window of analysis
spans from current time t back to (t-T), where T is
the length of the window of analysis.
An arriving word at current time t falls into our
current window of analysis and thus needs to be
processed. To account for these events efficiently,
we need a new structure: the temporal event FIFO
list (i.e., a queue where events get registered) that
keeps track of events as they arrive in time. As an
event (word wt) arrives it is registered and proc-
essed as follows:
</bodyText>
<listItem confidence="0.771058">
1. Find the corresponding identifier of wt in
the universal ranked dictionary and add it
as u
</listItem>
<bodyText confidence="0.87687525">
di at the end of the temporal event list
together with its time stamp.
2. The corresponding entry in s
d , the ranked
segment dictionary, is located through an
index list that maps diu ® dk and the
segment frequency associated is incre-
mented ( ) = ( s ) + 1
</bodyText>
<figure confidence="0.614967666666667">
f dk f d
s
k
</figure>
<listItem confidence="0.997337631578947">
3. Verify if the rank of the feature needs to be
updated in the segment rank list. In other
words evaluate whether f (dk−1) &gt; f (ds
k )
still holds true after the update. If this is
not true then shift feature up in the rank list
(to a higher rank) and shift down the
predecessor feature in the rank list. In this
single shift-up-down operation, update the
index list and the value of k.
4. For every feature shifted down in 3 down
re-compute the relative rank differential
RRD function and verify if its position
needs to be modified in R(d u, dS) (a sec-
ond index list is needed to compute this ef-
ficiently).
5. Repeat step 3 iteratively until feature is not
able to push up any further in the ranked
list.
</listItem>
<bodyText confidence="0.994270428571428">
The process for dealing with departing events is
quite similar to the arriving process just described.
Of course, as the analysis window slides in time, it
is necessary to keep track of the temporal event
FIFO list to make sure that the events at the top are
removed as soon as they fall out of the analysis
window. The process is then:
</bodyText>
<listItem confidence="0.929411166666667">
1. The departing event is identified and its
corresponding identifier in the universal
ranked dictionary u
di is removed from the
top of the temporal event list.
2. Its location in ds the ranked segment dic-
tionary is located through the index list.
The corresponding segment frequency as-
sociated is decreased as follows:
f(dk)= f(dk)−1.
3. Verify if the rank of the feature needs to be
updated in the segment rank list. In other
</listItem>
<page confidence="0.990544">
967
</page>
<bodyText confidence="0.988300571428571">
words evaluate if f (dk+1) &lt; f (dk ) still
holds true after the update. If not shift fea-
ture down in rank (to a lower rank, denot-
ing less frequent occurrence) and shift the
successor feature up in the rank list. In this
single shift up-down operation, update the
index list and the value of k.
</bodyText>
<listItem confidence="0.994871">
4. For every feature shifted up in step 3 re-
compute the relative rank differential and
verify if its location needs to be modified
in R(du,dS)
5. Repeat step 3 until the feature is not able to
shift down any further in the ranked list.
</listItem>
<bodyText confidence="0.999089222222222">
The procedures just described are efficiently
implementable as they simply identify entries in
rank lists through index lists, update values by in-
crementing and decrementing variables, and per-
formed some localized and limited re-sorting. Ad-
ditionally, simple operations like adding data at the
end and removing data at the beginning of the
FIFO list are needed making it altogether computa-
tionally inexpensive.
</bodyText>
<sectionHeader confidence="0.999702" genericHeader="method">
4 Related Techniques
</sectionHeader>
<bodyText confidence="0.999979755102041">
Our work relates to several existing techniques as
follows. Many techniques of text and dialog analy-
sis utilize a word frequency vector based approach
(e.g., Chu-Carroll et al 1999) in which lexical fea-
tures counts (term frequencies) are used to popu-
late the vector. Sometimes the term frequency is
normalized by document size and weighted by the
inverse document frequency (TF-IDF). The TF-
IDF and TF metrics are the base of other ap-
proaches like discriminative classification (Kuo
and Lee 2003; and Li and Huerta 2004), Text Till-
ing or topic chains (Hearst 1993; Zechner 2001),
and latent semantic indexing (Landauer et al 1998).
Ultimately, these types of approaches are the foun-
dation of complex classification and document un-
derstanding systems which use these features to-
gether with possibly more sophisticated classifica-
tion algorithms (e.g., D’Avanzo et al 2007).
When using TF and TF-IDF approaches, it is im-
portant to notice that by normalizing the term fre-
quency by the document length, TF-based ap-
proaches are effectively equivalent to estimation of
a multinomial distribution. The variance of the es-
timate will be larger as the number of observations
decreases. Recently, approaches that explicitly es-
tablish this parametric assumption and perform
parameter inference have been presented in (Blei et
al 2003). This work is an example of the potential
complexity associated when performing parameter
inference.
The area of adaptation of frequency parameters
for ASR, specifically the work of (Church 2000), is
relevant to our work in the sense that both ap-
proaches emphasize the importance of and present
a method to update the lexical or semantic feature
statistics on-line.
In the area of non-parametric processing of dialog
and text, the work of (Huffaker et al 2006), is very
close to the work in this paper as it deals with non-
parametric statistics of the word frequencies (rank
of occurrences) and uses the Spearman’s Correla-
tion Coefficient. Our work differs from this ap-
proach in two ways: first, the Relative Rank Dif-
ferential tells us about the relative change in rank
(while SCC focuses in the absolute change) and
secondly, from the ranked RDD list, we can iden-
tify the saliency of each term (as opposed to sim-
ply computing the overall similarity between two
passages).
</bodyText>
<sectionHeader confidence="0.999357" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999931545454545">
In order to illustrate the application of the RRD
statistic, we conducted two sets of experiments
based on conversations recorded in a large cus-
tomer contact center for an American car manufac-
turer. In the first group of experiments we took a
corpus of 258 hand transcribed dialogs and con-
ducted classification experiments using the basic
RRD statistic as feature. We compared its per-
formance against term frequency and TF-IDF
based cosine distance approaches. The second set
of experiments is based on ASR transcribed speech
and for this we used a second corpus consisting of
a set of 44 conversations spanning over 3 hours of
conversational speech.
In the first set of experiments we intend to illus-
trate two things: first the usefulness of RRD as a
feature in terms of representational accuracy and
second, its robustness to noise and data sparseness
compared to other popular features. In the second
set of experiments we illustrate the versatility and
potential of our technique to be applied in dialog-
oriented analysis.
</bodyText>
<page confidence="0.997433">
968
</page>
<sectionHeader confidence="0.770595" genericHeader="conclusions">
5.1 RRD for Dialog matching
</sectionHeader>
<bodyText confidence="0.999873269230769">
For this set of experiments we used a corpus of 258
hand transcribed conversations. Each dialog was
treated like a single document. Using the set of
dialogs we constructed different query vectors and
affected these queries using various noise condi-
tions, and then we utilized these vectors to perform
a simple document query classification experiment.
We measured the cosine distance between the
noisy query vector and the document vector of
each document in this corpus. A noisy query is
constructing by adding zero mean additive gaus-
sian noise to the query vector with amplitude pro-
portional to the value of a parameter N and with
floor value of zero to avoid negative valued fea-
tures. We allow, in these experiments, for counts to
have non-integer values; as the dialog becomes
larger, the Gaussian assumption holds true due to
the Central Limit Theorem, independently of the
actual underlying distribution of the noise source.
This distortion is intended to mimic the variation
between two similar dialogs (or utterances) that are
essentially similar, except for a additive zero mean
random changes. A good statistic should be able to
show robustness to these types of distortions. A
correct match is counted when the closest match
for each query is the generating document.
</bodyText>
<table confidence="0.999248">
N=0.0 N=.05 N=0.1 N=0.2 N=0.4
TF- 99.6 98.0 84.9 60.0 32.5
cosine
TF- 99.6 99.6 97.3 88.0 67.4
IDF
cosine
RRD- 99.6 99.6 97.6 91.8 70.9
dot
</table>
<tableCaption confidence="0.993172">
Table 1. Query match accuracy for 3 features un-
</tableCaption>
<bodyText confidence="0.986246821428571">
der several query noise conditions.
Table 1 shows the percent correct matches for the
TF, TF-IDF and Relative Rank Differential fea-
tures, under various levels of query noise. As we
can see, in clean conditions the accuracy of the 3
features is quite high but as the noise conditions
increase the accuracy of the 3 techniques decreases
substantially. However, the TF feature is much
more sensitive to noise than the other two tech-
niques. We can see that our technique is better than
both TF and TF-IDF in noisy conditions.
We also conducted experiments to test the com-
parative robustness or the RRD feature to query
data sparseness. To measure this, we evaluated the
accuracy in query-document match when using a
random subset of the document as query. Figure 1
show the results of this experiment using the RRD
feature, the Term Frequency, and the TF-IDF fea-
ture vectors. We can see that with as little as 5% of
the document size as query, the RRD achieves
close to 90% accuracy while the TF-IDF feature
needs up to 20% to achieve the same performance,
and the TF counts only need close to 70%.
These results empirically demonstrate that RRD
statistics are more robust to noise and to term cov-
erage sparseness than TF and TF-IDF.
Figure1. Query match accuracy for 3 feature types
under various query data sparseness conditions
</bodyText>
<subsectionHeader confidence="0.999216">
5.2 ASR Based experiments
</subsectionHeader>
<bodyText confidence="0.999964190476191">
For the experiments of this section we used 44 dia-
logs. Manual transcriptions for these 44 conversa-
tions were obtained in order to evaluate the speech
recognition accuracy. While we could have used
the manual transcripts to perform the analysis, the
results reported here are based on the recognition
output. The reason for using ASR transcripts as
opposed to human transcription is that we wanted
to evaluate how useful our approach would be in a
real ASR based solution dealing with large
amounts of noisy data at this level of ASR error.
Each dialog was recorded in two separate channels
(one for the agent and one for the customer) and
automatically transcribed separately using a large
vocabulary two-stage automatic speech recognizer
system. In the first stage, a speaker independent
recognition pass is performed after which the re-
sulting hypothesis is used to compensate and adapt
feature and models. Using the adapted feature and
models the second stage recognition is performed.
After recognition, the single best hypothesis with
</bodyText>
<page confidence="0.996625">
969
</page>
<bodyText confidence="0.98893298">
time stamps for the agent and customer are weaved
back together.
The overall Word Error Rate is about 24% and var-
ies significantly between the set of agents and the
set of customers (the set of agents being more ac-
curate).
The universal dictionary we used consists exclu-
sively of the words occurring in the corpus which
total 2046 unique words. Call length ranged from
just less than 1 minute to more than 20 minutes
with most of the calls lasting between 2 and 3 min-
utes. The corpus consists of close to 30k tokens
and does not distinguish between agent channel
and customer channel. A universal dictionary of
ranked words is built from the set of dialogs and
each dialog is treated as a segment.
Dialog Tagging and Topic Saliency
In this analysis we look at complete dialogs. A use-
ful application of the methods we describe in this
work is to identify and separate calls that are inter-
esting from non-interesting calls4, furthermore, one
could also be interested in singling out which spe-
cific terms make this dialog salient. An application
of this approach is the automatic generation of tags
(e.g., social-network style of document tagging). In
our approach, we will identify calls whose top en-
tries in their sorted relative rank differential lists
are above a certain threshold and deem these calls
as semantically salient.
We now describe in detail how an interesting call
can be distinguished from a non-interesting call
using the relative rank differential statistic.
Figure 2 below shows the ranked dictionary
dS = d S d S d S (i.e., the universal rank id’s
{ 1 , 2 , 3 ...}
as a function of their observed ranks) and Figure 3
shows the plot of the sorted relative rank differen-
tial list R(du, dS) for when the segment corre-
sponds to an interesting call (as defined above).
The chosen call, specifically shows as topic AIR-
BAG deployment in the context of a car accident.
Specifically, Figure 2 shows the corresponding
rank in the universal ranked dictionary versus the
rank in the dialog or segment. We can see that the
4 For the purpose of this work, we simply define as an inter-
esting call a call that deals with an infrequent or rare topic
which influences the distribution of keywords and key-phrases.
Examples of calls in our domain meeting this criterion are
calls dealing with accidents and airbags.
right-most part of the plot is largely monotonic,
meaning that most entries of lesser frequency occur
in the same ranked order both in the universal and
the specific dialog (including zero times for the
segment), while a subset across the whole range of
the universal dictionary were substantially relo-
cated up in the rank (i.e., occurred more frequently
in the dialog than in the language). If the plot was a
single straight line each word would have the same
rank both in the language and in the dialog.
We argue that while the terms of interest lie in that
subset of interest in the graph (the terms whose
rank increased substantially), not all of those words
are equally interesting or important and rather than
simply looking at absolute changes in rank we fo-
cus on the relative-rank differential RRD metric.
Thus, Figure 3 shows the sorted values of the rela-
tive rank differential list (with α = 1 . 3 ). The top
entries and their rank in the universal dictionary (in
parentheses) are: AIRBAGS (253), AS (55),
FRONT (321), DEPLOY (369), SIDE (279), AC-
CIDENT (687). As we can see, the top entries are
distributed across a broad range of ranks in the
universal dictionary and relate to the topic of the
conversation, which from the top ranked entries are
evidently the deployment of front and side airbags
during an accident, and thus, for this call were able
to identify its semantic saliency from the corpus of
conversations.
Other interesting or salient calls also showed a
similar this profile in the RRD curve.
The question now is what the behavior of our ap-
proach for uninteresting calls is. We repeated the
procedure above for a call which we deemed se-
mantically un-interesting (i.e., dealing with a
common topic like call transfer and other routine
procedures). Figure 4 shows the sorted relative
rank differential values and, especially when com-
pared with Figure 2, we see a large monotonic
component on the higher ranked terms and not so
marked discontinuities in the low and mid-range
part of the curve.
We computed the relative rank differential RRD
metric for each feature similarly as with the inter-
esting call, and ranked the words based on these
values. The distribution of the ranked values is
shown in Figure 5. The resulting words with top
values are CLEAR (1113), INFORMATION (122)
BUYING (1941), and CLEARLY (1910). From
these words we cannot really tell what is the spe-
cific topic of the conversation is as easily as with
</bodyText>
<page confidence="0.988646">
970
</page>
<bodyText confidence="0.997859">
the interesting call. More importantly, we can now
compare Figures 3 and 5 and see that the highest
relative rank differential value of the top entry in
Figure 3 (larger than 10) is significantly larger than
the largest relative rank differential value in Figure
5 (just above 7) reflecting the fact that the relative
rank differential metric could be a useful parameter
in evaluating semantic saliency of a segment using
a static threshold. As an interesting point, con-
ceivably the highly ranked features based on RRD
could reflect language utilization idiosyncrasies.
</bodyText>
<figureCaption confidence="0.833917">
Figure 4. Ranked dictionary entry vs Universal
Rank for a non-salient call
Figure 2. Ranked dictionary entry vs Universal
Rank for a salient call
Figure 5. Sorted relative rank differential values
of R(u S
d , d) for a non-interesting (semantically
non-salient) call.
</figureCaption>
<figure confidence="0.607725">
6 Conclusions
</figure>
<figureCaption confidence="0.998058">
Figure 3. Sorted relative rank differential values
</figureCaption>
<bodyText confidence="0.994672470588235">
of R(u S
d , d) for a semantically salient call.
In this paper we presented a novel non parametric
rank-statistics based method for the semantic
analysis of conversations and dialogs. Our method
is implementable in segment-based or dialog-based
modalities, as well as in batch form and in on-line
or dynamic form. Applications of our method in-
clude topic detection, event tracking, story/topic
monitoring, new-event detection, summarization,
information filtering, etc. Because our approach is
based on non-parametric statistics it has favorable
intrinsic benefits, like making no assumptions
about the underlying data, which makes it suitable
for the use of both lexical semantic features as well
as classifier-based semantic features. Furthermore,
our approach could, in the future, benefit from
</bodyText>
<page confidence="0.994672">
971
</page>
<bodyText confidence="0.999919807692308">
classical non-parametric approaches like block-
treatment analysis etc.
We demonstrated that our approach is as effective
in query classification as TF and TF-IDF in low
noise and no noise (i.e., distortion) conditions, and
consistently better than those techniques in noisy
conditions. We also found RRD to be more robust
to query data sparseness than TF and TF-IDF.
These results provide a motivation to combine our
statistic with other techniques like topic chains,
textilling, latent semantic indexing, and discrimi-
nant classification approaches; specifically RRD
could replace TF and TF-IDF based features.
Future work could focus on applying ranking sta-
tistics to techniques for mining and tracking tem-
poral and time-changing parameters in conjunction
with techniques like (Agrawal and Srikant 1995;
Pratt 2001; Last et al 2001).
Another area of possible future work is the detec-
tion and separation of multiple underlying trends in
dialogs. Our approach is also suited for the analy-
sis of large streams of real time conversations, and
this is a very important area of focus as presently
more and more conversational data gets generated
through channels like chat, mobile telephony, VoIP
etc.
</bodyText>
<sectionHeader confidence="0.999107" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999707555555556">
Agrawal R. and Srikant R. 1995. Mining Sequential
Patterns. In Proc. of the 11th Int&apos;l Conference on
Data Engineering, Taipei, Taiwan.
Berger, A. L., Pietra, V. J., and Pietra, S. A. 1996. A
maximum entropy approach to natural language
processing. Comp. Linguist. 22, 1
Blei D., Ng A., and Jordan M. 2003. Latent Dirichlet
allocation. J. of Machine Learning Research
Byron, D. K. and Heeman, P. A. 1998. Discourse
Marker Use in Task-Oriented Spoken Dialog.
TR664, University of Rochester.
Chu-Carroll, J, and Carpenter R. 1999. Vector-Based
Natural Language Call Routing. Journal of Computa-
tional Linguistics, 25(30), pp. 361-388
Church, K. 2000. Empirical estimates of adaptation:
The chance of two Noriega &apos;s is closer to p/2 than p2.
In Coling
D&apos;Avanzo E., Elia A., Kuflik T., Vietri S. 2007. Univer-
sity of Salerno, LAKE System at DUC 2007, Proc.
Document Understanding Conference
Hearst, M. 1993. TextTiling: A Quantitative Approach
to Discourse Segmentation, Technical Report
UCB:S2K-93-24, Berkeley, CA
Hollander &amp; Wolfe 1999. Nonparametric Statistical
Methods, Second Edition, John Wiley and Sons
Huffaker, D., Jorgensen, J., Iacobelli, F., Tepper, P., &amp;
Cassell, J. 2006. Computational Measures for Lan-
guage Similarity across Time in Online Communities.
Workshop on ACTS at HLT-NAACL, New York
City, NY.
Klinkenberg R. and Renz I. 1998. Adaptive information
filtering: Learning in the presence of concept drifts.
In Learning for Text Categorization, Menlo Park
Kuo H.-K.J. and Lee C. H. Discriminative training of
natural language call routers, IEEE Transactions on
Speech and Audio Processing, Volume 11, Issue 1,
Jan 2003 Page(s): 24 - 35.
Landauer T., Foltz P. W., and Laham D. Introduction to
Latent Semantic Analysis Discourse Processes 25,
1998.
Last M., Klein Y., and Kandel A., Knowledge Discov-
ery in Time Series Databases IEEE Trans. on Sys-
tems, Man, and Cybernetics 31B(2001).
Li X. and Huerta J.M., Discriminative Training of
Compound word based Multinomial Classifiers for
Speech Routing Proc. ICSLP 2004
Mishne, G., Carmel, D., Hoory, R., Roytman, A., and
Soffer, A. 2005. Automatic analysis of call-center
conversations. In Proc. of the 14th ACM interna-
tional Conference on information and Knowledge.
Pratt K. B. Locating patterns in discrete time series.
Master&apos;s thesis, Computer Science and Engineering,
University of South Florida, 2001.
Stanley K. O. Learning concept drift with a committee
of decision trees. Comp. Science Dept., University of
Texas-Austin. TR AI-03-302, 2003.
Zechner K. Automatic Summarization of Spoken Dia-
logues in Unrestricted Domains. PhD thesis, LTI,
CMU, 2001
Zimmermann M., Liu Y., E. Shriberg, and A. Stolcke,
Toward Joint Segmentation and Classification of
Dialog Acts in Multiparty Meetings MLMI work-
shop, 2005
</reference>
<page confidence="0.997757">
972
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000236">
<title confidence="0.999867">Relative Rank Statistics for Dialog Analysis</title>
<author confidence="0.999966">M Juan</author>
<affiliation confidence="0.994699">IBM T.J. Watson Research</affiliation>
<address confidence="0.7112705">1101 Kitchawan Yorktown Heights, NY</address>
<email confidence="0.998795">huerta@us.ibm.com</email>
<abstract confidence="0.999093293269232">We introduce the relative rank differential statistic which is a non-parametric approach to document and dialog analysis based on word frequency rank-statistics. We also present a simple method to establish semantic saliency in dialog, documents, and dialog segments using these word frequency rank statistics. Applications of our technique include the dynamic tracking of topic and semantic evolution in a dialog, topic detection, automatic generation of document tags, and new story or event detection in conversational speech and text. Our approach benefits from the robustness, simplicity and efficiency of non-parametric and rank based approaches and consistently outperformed term-frequency and TF-IDF cosine distance approaches in several experiments conducted. 1 Background Existing research in dialog analysis has focused on several specific problems including dialog act detection (e.g., Byron and Heeman 1998), segmentation and chunking (e.g., Hearst 1993), topic detection (e.g., Zimmerman et al 2005), distillation and summarization (e.g., Mishne et al 2005) etc. The breath of this research reflects the increasing importance that dialog analysis has for multiple domains and applications. While historically, dialog analysis research has initially leveraged the corresponding techniques originally intended for textual document analysis, techniques tailored specifically for dialog processing eventually should be able to address the sparseness, noise, and time considerations intrinsic to dialog and conversations. The approach proposed in this paper focuses on the relative change of rank ordering of words occurring in a conversation according to their frequen- Our approach emphasizes relatively improbby focusing on terms that are relatively unlikely to appear frequently and thus weighting their change in rank more once they are observed. Our technique achieves this in a non-parametric fashion without explicitly computing probabilities, without the assumption of an underlying distribution, and without the computation of likelihoods. In general, non-parametric approaches to data analysis are well known and present several attractive characteristics (as a general reference see Hollander and Wolfe 1999). Non-parametric approaches require few assumptions about the data analyzed and can present computational advantages over parametric approaches especially when the underlying distributions of the data are not normal. In specific, our approach uses rank order statistics of word-feature frequencies to compute a relative rank-differential statistic. This paper is organized as follows: in Section 2 we introduce and describe our basic approach (the relative rank differential RRD function and its sorted list). In Section 3 we address the temporal nature of dialogs and describe considerations to dynamically update the RRD statistics in an online fashion especially for the case of shifting temporal windows of analysis. In Section 4 we relate the RRD approach to relevant existing and previous dialog and text analysis approaches. In Section 5 we illustrate the usefulness of our metric by analyzing a set of conversations in various ways using the RRD. Specifically, in that section we will empirically demonstrate its robustness to noise and data sparseness compared to the popular term frequency and TF-IDF cosine distance approaches in 965 of the 2008 Conference on Empirical Methods in Natural Language pages 965–972, October Association for Computational Linguistics a dialog classification task. And finally, in Section 6, we present some concluding remarks and future directions 2 The Relative Rank Differential 2, 3...} ud ud udenote the dictionary of a language (i.e., the ordered list of words sorted in decreasing order of frequency). superscript that this ranked list is on the Specifically, the is the th in u its frequency of in the language denoted by is than u) for every notational simplicity we assume that no two words share the same frequency). In the case where want relax this assumption we simply allow ) ( u) d d long as uprecedes j lexicographically, or under any other desired criteria. For assume that every entry (i.e., each word has been observed at least once in the language). let now 2, 3...} Sd Sd S note the corresponding ranked dictionary for a diaor dialog segment, as in the case of the language dictionary, in decreasing order The superscript S denotes that this list is based on the dialog The word S the th in s its frequency of occurrence the conversation segment by S) larger than for every this case we allow every the cardinality of u the same as the rank of word the dictionary that, for example, i d( ) Based now on a dialog segment and a universal any given word be associated a rank in u universal ranked dictionary) a rank in s the dialog segment ranked dictionary. Let us define now for every word the relative differential (RRD) function or given by: The relative rank-differential is the ratio of the absolute difference (or change) in rank between the word’s original position in the universal dictionary the segment s. The exponent the denominator allows us to emphasize or deemphasize changes in terms according to their position or rank in the language (or universal) dictionary. Typically we will want to increase the denominator’s value (i.e., deemphasize) for terms that have very low frequency (and their rank value in the universal dictionary is large) so that only relatively big changes in rank will result in substantial values of this function. When alpha is zero, the RRD focuses on every word identically as we consider only the absolute change in rank. For alpha equal to 1.0 the relative change in rank gets scaled down linearly according to its rank, while for alphas larger than 1.0 the numerator will scale down or reduce to a larger extent the value of relative rank differential for words that have large rank value. Based on each word’s relative rank differential we can compute the ranked list of words sorted in decreasing order by their corresponding value of relative rank differential. Let this sorted list of be denoted by u, ,...} So larger than 3for every We now provide some intuition on the ranked RRD lists and the RRD function. The ranked dictionary of a language contains information s u ( ( only consider at this point the case in which both speakers’ parts in a dialog interaction are considered jointly (i.e., single channel), however, our method can be easily extended to separate conversation channels. Also, for simplicity we consider at this point only words (or phrases) as features. brevity, we refer to the Relative Rank Differential of a given two utterances as a It is not, strictly a metric or a distance, but rather a simplicity, c is written without subscripts when these are from the 966 about the frequency of all words in a language (i.e., across the universe of conversations) while the segment counterpart pertains a single conversation or segment thereof. The relative rank differential tells us how different a word is ranked in a conversation segment from the universal language, but this difference is normalized by the universal rank of the word. Intuitively, and especially when alpha equals 1.0, the RRD denotes some sort of percent change in rank. This also means that this function is less sensitive to small changes in frequency in the case of frequent words and to small changes in rank in case of infrequent words. the sorted list in order of importance the most relatively salient terms of a dialog segment, as measured by relative changes or differences in rank. 3 Collecting Rank Statistics We now discuss how to extend the metrics described in the previous section to consider finitetime sliding windows of analysis, that is, we describe how to update rank statistics, specifically the ranked lists and relative rank differential information for every feature in an on-line fashion. This is useful when tracking the evolution of single dialogs, when focusing the analysis to span shorter regions, as well as to supporting dynamic real-time analytics of large number of dialogs. approach this, we decompose the as they occur in time) into arriving departing events. An arriving event at time a word that is covered by the analysis window at its specific time as the finite length window slides time, and a departing word at time a feature that stops falling within the window of analysis. For simplicity, and without loss of generality, we now assume that we are performing the analysis in real time and that the sliding window of analysis from current time to the length of the window of analysis. arriving word at current time into our current window of analysis and thus needs to be processed. To account for these events efficiently, need a new structure: the temporal event list (i.e., a queue where events get registered) that keeps track of events as they arrive in time. As an (word arrives it is registered and processed as follows: Find the corresponding identifier of in the universal ranked dictionary and add it u at the end of the temporal event list together with its time stamp. The corresponding entry in s the ranked segment dictionary, is located through an list that maps ® the segment frequency associated is incre- ) s) f d s k 3. Verify if the rank of the feature needs to be updated in the segment rank list. In other evaluate whether still holds true after the update. If this is not true then shift feature up in the rank list (to a higher rank) and shift down the predecessor feature in the rank list. In this single shift-up-down operation, update the list and the value of 4. For every feature shifted down in 3 down re-compute the relative rank differential RRD function and verify if its position to be modified in second index list is needed to compute this efficiently). 5. Repeat step 3 iteratively until feature is not able to push up any further in the ranked list. The process for dealing with departing events is quite similar to the arriving process just described. Of course, as the analysis window slides in time, it is necessary to keep track of the temporal event FIFO list to make sure that the events at the top are removed as soon as they fall out of the analysis window. The process is then: 1. The departing event is identified and its corresponding identifier in the universal dictionary u is removed from the top of the temporal event list. Its location in the ranked segment dictionary is located through the index list. The corresponding segment frequency associated is decreased as follows: 3. Verify if the rank of the feature needs to be updated in the segment rank list. In other 967 evaluate if holds true after the update. If not shift feature down in rank (to a lower rank, denoting less frequent occurrence) and shift the successor feature up in the rank list. In this single shift up-down operation, update the index list and the value of k. 4. For every feature shifted up in step 3 recompute the relative rank differential and verify if its location needs to be modified 5. Repeat step 3 until the feature is not able to shift down any further in the ranked list. The procedures just described are efficiently implementable as they simply identify entries in rank lists through index lists, update values by incrementing and decrementing variables, and performed some localized and limited re-sorting. Additionally, simple operations like adding data at the end and removing data at the beginning of the FIFO list are needed making it altogether computationally inexpensive. 4 Related Techniques Our work relates to several existing techniques as follows. Many techniques of text and dialog analysis utilize a word frequency vector based approach (e.g., Chu-Carroll et al 1999) in which lexical features counts (term frequencies) are used to populate the vector. Sometimes the term frequency is normalized by document size and weighted by the inverse document frequency (TF-IDF). The TF- IDF and TF metrics are the base of other approaches like discriminative classification (Kuo and Lee 2003; and Li and Huerta 2004), Text Tilling or topic chains (Hearst 1993; Zechner 2001), and latent semantic indexing (Landauer et al 1998). Ultimately, these types of approaches are the foundation of complex classification and document understanding systems which use these features together with possibly more sophisticated classification algorithms (e.g., D’Avanzo et al 2007). When using TF and TF-IDF approaches, it is important to notice that by normalizing the term frequency by the document length, TF-based approaches are effectively equivalent to estimation of a multinomial distribution. The variance of the estimate will be larger as the number of observations Recently, approaches that explicitly establish this parametric assumption and perform parameter inference have been presented in (Blei et al 2003). This work is an example of the potential complexity associated when performing parameter inference. The area of adaptation of frequency parameters for ASR, specifically the work of (Church 2000), is relevant to our work in the sense that both approaches emphasize the importance of and present a method to update the lexical or semantic feature statistics on-line. In the area of non-parametric processing of dialog and text, the work of (Huffaker et al 2006), is very close to the work in this paper as it deals with nonparametric statistics of the word frequencies (rank of occurrences) and uses the Spearman’s Correlation Coefficient. Our work differs from this approach in two ways: first, the Relative Rank Differential tells us about the relative change in rank (while SCC focuses in the absolute change) and secondly, from the ranked RDD list, we can identify the saliency of each term (as opposed to simply computing the overall similarity between two passages). 5 Experiments In order to illustrate the application of the RRD statistic, we conducted two sets of experiments based on conversations recorded in a large customer contact center for an American car manufacturer. In the first group of experiments we took a corpus of 258 hand transcribed dialogs and conducted classification experiments using the basic RRD statistic as feature. We compared its performance against term frequency and TF-IDF based cosine distance approaches. The second set of experiments is based on ASR transcribed speech and for this we used a second corpus consisting of a set of 44 conversations spanning over 3 hours of conversational speech. In the first set of experiments we intend to illustrate two things: first the usefulness of RRD as a feature in terms of representational accuracy and second, its robustness to noise and data sparseness compared to other popular features. In the second set of experiments we illustrate the versatility and potential of our technique to be applied in dialogoriented analysis. 968 5.1 RRD for Dialog matching For this set of experiments we used a corpus of 258 hand transcribed conversations. Each dialog was treated like a single document. Using the set of dialogs we constructed different query vectors and affected these queries using various noise conditions, and then we utilized these vectors to perform a simple document query classification experiment. We measured the cosine distance between the noisy query vector and the document vector of each document in this corpus. A noisy query is constructing by adding zero mean additive gaussian noise to the query vector with amplitude proto the value of a parameter with floor value of zero to avoid negative valued features. We allow, in these experiments, for counts to have non-integer values; as the dialog becomes larger, the Gaussian assumption holds true due to the Central Limit Theorem, independently of the actual underlying distribution of the noise source. This distortion is intended to mimic the variation between two similar dialogs (or utterances) that are essentially similar, except for a additive zero mean random changes. A good statistic should be able to show robustness to these types of distortions. A correct match is counted when the closest match for each query is the generating document. N=0.0 N=.05 N=0.1 N=0.2 N=0.4 TFcosine 99.6 98.0 84.9 60.0 32.5 TF- IDF cosine 99.6 99.6 97.3 88.0 67.4 RRDdot 99.6 99.6 97.6 91.8 70.9 1. match accuracy for 3 features under several query noise conditions. Table 1 shows the percent correct matches for the TF, TF-IDF and Relative Rank Differential features, under various levels of query noise. As we can see, in clean conditions the accuracy of the 3 features is quite high but as the noise conditions increase the accuracy of the 3 techniques decreases substantially. However, the TF feature is much more sensitive to noise than the other two techniques. We can see that our technique is better than both TF and TF-IDF in noisy conditions. We also conducted experiments to test the comparative robustness or the RRD feature to query data sparseness. To measure this, we evaluated the accuracy in query-document match when using a the document as query. Figure 1 show the results of this experiment using the RRD feature, the Term Frequency, and the TF-IDF feature vectors. We can see that with as little as 5% of the document size as query, the RRD achieves close to 90% accuracy while the TF-IDF feature needs up to 20% to achieve the same performance, and the TF counts only need close to 70%. These results empirically demonstrate that RRD statistics are more robust to noise and to term coverage sparseness than TF and TF-IDF. match accuracy for 3 feature types under various query data sparseness conditions 5.2 ASR Based experiments For the experiments of this section we used 44 dialogs. Manual transcriptions for these 44 conversations were obtained in order to evaluate the speech recognition accuracy. While we could have used the manual transcripts to perform the analysis, the results reported here are based on the recognition output. The reason for using ASR transcripts as opposed to human transcription is that we wanted to evaluate how useful our approach would be in a real ASR based solution dealing with large amounts of noisy data at this level of ASR error. Each dialog was recorded in two separate channels (one for the agent and one for the customer) and automatically transcribed separately using a large vocabulary two-stage automatic speech recognizer system. In the first stage, a speaker independent recognition pass is performed after which the resulting hypothesis is used to compensate and adapt feature and models. Using the adapted feature and models the second stage recognition is performed. After recognition, the single best hypothesis with 969 time stamps for the agent and customer are weaved back together. The overall Word Error Rate is about 24% and varies significantly between the set of agents and the set of customers (the set of agents being more accurate). The universal dictionary we used consists exclusively of the words occurring in the corpus which total 2046 unique words. Call length ranged from just less than 1 minute to more than 20 minutes with most of the calls lasting between 2 and 3 minutes. The corpus consists of close to 30k tokens and does not distinguish between agent channel and customer channel. A universal dictionary of ranked words is built from the set of dialogs and each dialog is treated as a segment. Dialog Tagging and Topic Saliency In this analysis we look at complete dialogs. A useful application of the methods we describe in this work is to identify and separate calls that are interfrom non-interesting furthermore, one could also be interested in singling out which specific terms make this dialog salient. An application of this approach is the automatic generation of tags (e.g., social-network style of document tagging). In our approach, we will identify calls whose top entries in their sorted relative rank differential lists are above a certain threshold and deem these calls as semantically salient. We now describe in detail how an interesting call can be distinguished from a non-interesting call using the relative rank differential statistic. Figure 2 below shows the ranked dictionary Sd Sd S(i.e., the universal rank id’s 2, 3...} as a function of their observed ranks) and Figure 3 the plot of the sorted relative rank differenlist when the segment corresponds to an interesting call (as defined above). The chosen call, specifically shows as topic AIR- BAG deployment in the context of a car accident. Specifically, Figure 2 shows the corresponding rank in the universal ranked dictionary versus the rank in the dialog or segment. We can see that the the purpose of this work, we simply define as an intercall call that deals with an infrequent or rare topic which influences the distribution of keywords and key-phrases. Examples of calls in our domain meeting this criterion are calls dealing with accidents and airbags. right-most part of the plot is largely monotonic, meaning that most entries of lesser frequency occur in the same ranked order both in the universal and the specific dialog (including zero times for the segment), while a subset across the whole range of the universal dictionary were substantially relocated up in the rank (i.e., occurred more frequently in the dialog than in the language). If the plot was a single straight line each word would have the same rank both in the language and in the dialog. We argue that while the terms of interest lie in that subset of interest in the graph (the terms whose rank increased substantially), not all of those words are equally interesting or important and rather than simply looking at absolute changes in rank we focus on the relative-rank differential RRD metric. Thus, Figure 3 shows the sorted values of the relarank differential list (with . 3 The top entries and their rank in the universal dictionary (in parentheses) are: AIRBAGS (253), AS (55), FRONT (321), DEPLOY (369), SIDE (279), AC- CIDENT (687). As we can see, the top entries are distributed across a broad range of ranks in the universal dictionary and relate to the topic of the conversation, which from the top ranked entries are evidently the deployment of front and side airbags during an accident, and thus, for this call were able to identify its semantic saliency from the corpus of conversations. Other interesting or salient calls also showed a similar this profile in the RRD curve. The question now is what the behavior of our approach for uninteresting calls is. We repeated the procedure above for a call which we deemed semantically un-interesting (i.e., dealing with a common topic like call transfer and other routine procedures). Figure 4 shows the sorted relative rank differential values and, especially when compared with Figure 2, we see a large monotonic component on the higher ranked terms and not so marked discontinuities in the low and mid-range part of the curve. We computed the relative rank differential RRD metric for each feature similarly as with the interesting call, and ranked the words based on these values. The distribution of the ranked values is shown in Figure 5. The resulting words with top values are CLEAR (1113), INFORMATION (122) BUYING (1941), and CLEARLY (1910). From these words we cannot really tell what is the specific topic of the conversation is as easily as with 970 the interesting call. More importantly, we can now compare Figures 3 and 5 and see that the highest relative rank differential value of the top entry in Figure 3 (larger than 10) is significantly larger than the largest relative rank differential value in Figure 5 (just above 7) reflecting the fact that the relative rank differential metric could be a useful parameter in evaluating semantic saliency of a segment using a static threshold. As an interesting point, conceivably the highly ranked features based on RRD could reflect language utilization idiosyncrasies. 4. dictionary entry vs Universal Rank for a non-salient call 2. dictionary entry vs Universal Rank for a salient call 5. relative rank differential values ofS a non-interesting (semantically non-salient) call. 6 Conclusions 3. relative rank differential values ofS a semantically salient call. In this paper we presented a novel non parametric rank-statistics based method for the semantic analysis of conversations and dialogs. Our method is implementable in segment-based or dialog-based modalities, as well as in batch form and in on-line or dynamic form. Applications of our method include topic detection, event tracking, story/topic monitoring, new-event detection, summarization, information filtering, etc. Because our approach is based on non-parametric statistics it has favorable intrinsic benefits, like making no assumptions about the underlying data, which makes it suitable for the use of both lexical semantic features as well as classifier-based semantic features. Furthermore, our approach could, in the future, benefit from 971 classical non-parametric approaches like blocktreatment analysis etc. We demonstrated that our approach is as effective in query classification as TF and TF-IDF in low noise and no noise (i.e., distortion) conditions, and consistently better than those techniques in noisy conditions. We also found RRD to be more robust to query data sparseness than TF and TF-IDF. These results provide a motivation to combine our statistic with other techniques like topic chains, textilling, latent semantic indexing, and discriminant classification approaches; specifically RRD could replace TF and TF-IDF based features. Future work could focus on applying ranking statistics to techniques for mining and tracking temporal and time-changing parameters in conjunction with techniques like (Agrawal and Srikant 1995; Pratt 2001; Last et al 2001). Another area of possible future work is the detection and separation of multiple underlying trends in dialogs. Our approach is also suited for the analysis of large streams of real time conversations, and this is a very important area of focus as presently more and more conversational data gets generated through channels like chat, mobile telephony, VoIP etc.</abstract>
<note confidence="0.789163052631579">References R. and Srikant R. 1995. Sequential In Proc. of the 11th Int&apos;l Conference on Data Engineering, Taipei, Taiwan. A. L., Pietra, V. J., and Pietra, S. A. 1996. maximum entropy approach to natural language Comp. Linguist. 22, 1 D., Ng A., and Jordan M. 2003. Dirichlet J. of Machine Learning Research D. K. and Heeman, P. A. 1998. Marker Use in Task-Oriented Spoken Dialog. TR664, University of Rochester. J, and Carpenter R. 1999. Language Call Journal of Computational Linguistics, 25(30), pp. 361-388 K. 2000. estimates of adaptation: chance of two Noriega &apos;s is closer to p/2 than In Coling E., Elia A., Kuflik T., Vietri S. 2007. Univerof Salerno, LAKE System at DUC 2007, Document Understanding Conference M. 1993. A Quantitative Approach Discourse Technical Report UCB:S2K-93-24, Berkeley, CA &amp; Wolfe 1999. Statistical Edition, John Wiley and Sons Huffaker, D., Jorgensen, J., Iacobelli, F., Tepper, P., &amp; J. 2006. Measures for Language Similarity across Time in Online Communities. Workshop on ACTS at HLT-NAACL, New York City, NY. R. and Renz I. 1998. information Learning in the presence of concept In Learning for Text Categorization, Menlo Park H.-K.J. and Lee C. H. training of language call routers, Transactions on Speech and Audio Processing, Volume 11, Issue 1, Jan 2003 Page(s): 24 - 35. T., Foltz P. W., and Laham D. to Semantic Analysis Processes 25, 1998. M., Klein Y., and Kandel A., Discovin Time Series Databases Trans. on Systems, Man, and Cybernetics 31B(2001). X. and Huerta J.M., Training of Compound word based Multinomial Classifiers for Routing ICSLP 2004 Mishne, G., Carmel, D., Hoory, R., Roytman, A., and A. 2005. analysis of call-center In of the 14th ACM international Conference on information and Knowledge. K. B. patterns in discrete time Master&apos;s thesis, Computer Science and Engineering, University of South Florida, 2001. K. O. concept drift with a committee decision Comp. Science Dept., University of Texas-Austin. TR AI-03-302, 2003.</note>
<author confidence="0.301741">K Summarization of Spoken Dia-</author>
<affiliation confidence="0.601839">in Unrestricted PhD thesis, LTI,</affiliation>
<address confidence="0.884362">CMU, 2001</address>
<author confidence="0.929348">M Zimmermann</author>
<author confidence="0.929348">Y Liu</author>
<author confidence="0.929348">E Shriberg</author>
<author confidence="0.929348">A Stolcke</author>
<title confidence="0.448155">Toward Joint Segmentation and Classification of</title>
<author confidence="0.455968">Acts in Multiparty Meetings work-</author>
<address confidence="0.7614635">shop, 2005 972</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Agrawal</author>
<author>R Srikant</author>
</authors>
<title>Mining Sequential Patterns.</title>
<date>1995</date>
<booktitle>In Proc. of the 11th Int&apos;l Conference on Data Engineering,</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Agrawal, Srikant, 1995</marker>
<rawString>Agrawal R. and Srikant R. 1995. Mining Sequential Patterns. In Proc. of the 11th Int&apos;l Conference on Data Engineering, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>V J Pietra</author>
<author>S A Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Comp. Linguist.</journal>
<volume>22</volume>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, A. L., Pietra, V. J., and Pietra, S. A. 1996. A maximum entropy approach to natural language processing. Comp. Linguist. 22, 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>J. of Machine Learning Research</journal>
<contexts>
<context position="14159" citStr="Blei et al 2003" startWordPosition="2446" endWordPosition="2449">ification and document understanding systems which use these features together with possibly more sophisticated classification algorithms (e.g., D’Avanzo et al 2007). When using TF and TF-IDF approaches, it is important to notice that by normalizing the term frequency by the document length, TF-based approaches are effectively equivalent to estimation of a multinomial distribution. The variance of the estimate will be larger as the number of observations decreases. Recently, approaches that explicitly establish this parametric assumption and perform parameter inference have been presented in (Blei et al 2003). This work is an example of the potential complexity associated when performing parameter inference. The area of adaptation of frequency parameters for ASR, specifically the work of (Church 2000), is relevant to our work in the sense that both approaches emphasize the importance of and present a method to update the lexical or semantic feature statistics on-line. In the area of non-parametric processing of dialog and text, the work of (Huffaker et al 2006), is very close to the work in this paper as it deals with nonparametric statistics of the word frequencies (rank of occurrences) and uses </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei D., Ng A., and Jordan M. 2003. Latent Dirichlet allocation. J. of Machine Learning Research</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Byron</author>
<author>P A Heeman</author>
</authors>
<date>1998</date>
<booktitle>Discourse Marker Use in Task-Oriented Spoken Dialog. TR664,</booktitle>
<institution>University of Rochester.</institution>
<contexts>
<context position="1086" citStr="Byron and Heeman 1998" startWordPosition="155" endWordPosition="158"> statistics. Applications of our technique include the dynamic tracking of topic and semantic evolution in a dialog, topic detection, automatic generation of document tags, and new story or event detection in conversational speech and text. Our approach benefits from the robustness, simplicity and efficiency of non-parametric and rank based approaches and consistently outperformed term-frequency and TF-IDF cosine distance approaches in several experiments conducted. 1 Background Existing research in dialog analysis has focused on several specific problems including dialog act detection (e.g., Byron and Heeman 1998), segmentation and chunking (e.g., Hearst 1993), topic detection (e.g., Zimmerman et al 2005), distillation and summarization (e.g., Mishne et al 2005) etc. The breath of this research reflects the increasing importance that dialog analysis has for multiple domains and applications. While historically, dialog analysis research has initially leveraged the corresponding techniques originally intended for textual document analysis, techniques tailored specifically for dialog processing eventually should be able to address the sparseness, noise, and time considerations intrinsic to dialog and conv</context>
</contexts>
<marker>Byron, Heeman, 1998</marker>
<rawString>Byron, D. K. and Heeman, P. A. 1998. Discourse Marker Use in Task-Oriented Spoken Dialog. TR664, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chu-Carroll</author>
<author>R Carpenter</author>
</authors>
<title>Vector-Based Natural Language Call Routing.</title>
<date>1999</date>
<journal>Journal of Computational Linguistics,</journal>
<volume>25</volume>
<issue>30</issue>
<pages>361--388</pages>
<marker>Chu-Carroll, Carpenter, 1999</marker>
<rawString>Chu-Carroll, J, and Carpenter R. 1999. Vector-Based Natural Language Call Routing. Journal of Computational Linguistics, 25(30), pp. 361-388</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>Empirical estimates of adaptation: The chance of two Noriega &apos;s is closer to p/2 than p2. In Coling</title>
<date>2000</date>
<contexts>
<context position="14355" citStr="Church 2000" startWordPosition="2477" endWordPosition="2478">es, it is important to notice that by normalizing the term frequency by the document length, TF-based approaches are effectively equivalent to estimation of a multinomial distribution. The variance of the estimate will be larger as the number of observations decreases. Recently, approaches that explicitly establish this parametric assumption and perform parameter inference have been presented in (Blei et al 2003). This work is an example of the potential complexity associated when performing parameter inference. The area of adaptation of frequency parameters for ASR, specifically the work of (Church 2000), is relevant to our work in the sense that both approaches emphasize the importance of and present a method to update the lexical or semantic feature statistics on-line. In the area of non-parametric processing of dialog and text, the work of (Huffaker et al 2006), is very close to the work in this paper as it deals with nonparametric statistics of the word frequencies (rank of occurrences) and uses the Spearman’s Correlation Coefficient. Our work differs from this approach in two ways: first, the Relative Rank Differential tells us about the relative change in rank (while SCC focuses in the </context>
</contexts>
<marker>Church, 2000</marker>
<rawString>Church, K. 2000. Empirical estimates of adaptation: The chance of two Noriega &apos;s is closer to p/2 than p2. In Coling</rawString>
</citation>
<citation valid="true">
<authors>
<author>E D&apos;Avanzo</author>
<author>A Elia</author>
<author>T Kuflik</author>
<author>S Vietri</author>
</authors>
<date>2007</date>
<booktitle>University of Salerno, LAKE System at DUC 2007, Proc. Document Understanding Conference</booktitle>
<marker>D&apos;Avanzo, Elia, Kuflik, Vietri, 2007</marker>
<rawString>D&apos;Avanzo E., Elia A., Kuflik T., Vietri S. 2007. University of Salerno, LAKE System at DUC 2007, Proc. Document Understanding Conference</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>TextTiling: A Quantitative Approach to Discourse Segmentation,</title>
<date>1993</date>
<tech>Technical Report UCB:S2K-93-24,</tech>
<location>Berkeley, CA</location>
<contexts>
<context position="1133" citStr="Hearst 1993" startWordPosition="164" endWordPosition="165">namic tracking of topic and semantic evolution in a dialog, topic detection, automatic generation of document tags, and new story or event detection in conversational speech and text. Our approach benefits from the robustness, simplicity and efficiency of non-parametric and rank based approaches and consistently outperformed term-frequency and TF-IDF cosine distance approaches in several experiments conducted. 1 Background Existing research in dialog analysis has focused on several specific problems including dialog act detection (e.g., Byron and Heeman 1998), segmentation and chunking (e.g., Hearst 1993), topic detection (e.g., Zimmerman et al 2005), distillation and summarization (e.g., Mishne et al 2005) etc. The breath of this research reflects the increasing importance that dialog analysis has for multiple domains and applications. While historically, dialog analysis research has initially leveraged the corresponding techniques originally intended for textual document analysis, techniques tailored specifically for dialog processing eventually should be able to address the sparseness, noise, and time considerations intrinsic to dialog and conversations. The approach proposed in this paper </context>
<context position="13401" citStr="Hearst 1993" startWordPosition="2332" endWordPosition="2333">omputationally inexpensive. 4 Related Techniques Our work relates to several existing techniques as follows. Many techniques of text and dialog analysis utilize a word frequency vector based approach (e.g., Chu-Carroll et al 1999) in which lexical features counts (term frequencies) are used to populate the vector. Sometimes the term frequency is normalized by document size and weighted by the inverse document frequency (TF-IDF). The TFIDF and TF metrics are the base of other approaches like discriminative classification (Kuo and Lee 2003; and Li and Huerta 2004), Text Tilling or topic chains (Hearst 1993; Zechner 2001), and latent semantic indexing (Landauer et al 1998). Ultimately, these types of approaches are the foundation of complex classification and document understanding systems which use these features together with possibly more sophisticated classification algorithms (e.g., D’Avanzo et al 2007). When using TF and TF-IDF approaches, it is important to notice that by normalizing the term frequency by the document length, TF-based approaches are effectively equivalent to estimation of a multinomial distribution. The variance of the estimate will be larger as the number of observations</context>
</contexts>
<marker>Hearst, 1993</marker>
<rawString>Hearst, M. 1993. TextTiling: A Quantitative Approach to Discourse Segmentation, Technical Report UCB:S2K-93-24, Berkeley, CA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hollander</author>
<author>Wolfe</author>
</authors>
<title>Nonparametric Statistical Methods, Second Edition,</title>
<date>1999</date>
<publisher>John Wiley and Sons</publisher>
<contexts>
<context position="2410" citStr="Hollander and Wolfe 1999" startWordPosition="348" endWordPosition="352">ords occurring in a conversation according to their frequencies. Our approach emphasizes relatively improbable terms by focusing on terms that are relatively unlikely to appear frequently and thus weighting their change in rank more once they are observed. Our technique achieves this in a non-parametric fashion without explicitly computing probabilities, without the assumption of an underlying distribution, and without the computation of likelihoods. In general, non-parametric approaches to data analysis are well known and present several attractive characteristics (as a general reference see Hollander and Wolfe 1999). Non-parametric approaches require few assumptions about the data analyzed and can present computational advantages over parametric approaches especially when the underlying distributions of the data are not normal. In specific, our approach uses rank order statistics of word-feature frequencies to compute a relative rank-differential statistic. This paper is organized as follows: in Section 2 we introduce and describe our basic approach (the relative rank differential RRD function and its sorted list). In Section 3 we address the temporal nature of dialogs and describe considerations to dyna</context>
</contexts>
<marker>Hollander, Wolfe, 1999</marker>
<rawString>Hollander &amp; Wolfe 1999. Nonparametric Statistical Methods, Second Edition, John Wiley and Sons</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Huffaker</author>
<author>J Jorgensen</author>
<author>F Iacobelli</author>
<author>P Tepper</author>
<author>J Cassell</author>
</authors>
<title>Computational Measures for Language Similarity across Time in Online Communities.</title>
<date>2006</date>
<booktitle>Workshop on ACTS at HLT-NAACL,</booktitle>
<location>New York City, NY.</location>
<contexts>
<context position="14620" citStr="Huffaker et al 2006" startWordPosition="2521" endWordPosition="2524">s decreases. Recently, approaches that explicitly establish this parametric assumption and perform parameter inference have been presented in (Blei et al 2003). This work is an example of the potential complexity associated when performing parameter inference. The area of adaptation of frequency parameters for ASR, specifically the work of (Church 2000), is relevant to our work in the sense that both approaches emphasize the importance of and present a method to update the lexical or semantic feature statistics on-line. In the area of non-parametric processing of dialog and text, the work of (Huffaker et al 2006), is very close to the work in this paper as it deals with nonparametric statistics of the word frequencies (rank of occurrences) and uses the Spearman’s Correlation Coefficient. Our work differs from this approach in two ways: first, the Relative Rank Differential tells us about the relative change in rank (while SCC focuses in the absolute change) and secondly, from the ranked RDD list, we can identify the saliency of each term (as opposed to simply computing the overall similarity between two passages). 5 Experiments In order to illustrate the application of the RRD statistic, we conducted </context>
</contexts>
<marker>Huffaker, Jorgensen, Iacobelli, Tepper, Cassell, 2006</marker>
<rawString>Huffaker, D., Jorgensen, J., Iacobelli, F., Tepper, P., &amp; Cassell, J. 2006. Computational Measures for Language Similarity across Time in Online Communities. Workshop on ACTS at HLT-NAACL, New York City, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Klinkenberg</author>
<author>I Renz</author>
</authors>
<title>Adaptive information filtering: Learning in the presence of concept drifts. In Learning for Text Categorization,</title>
<date>1998</date>
<location>Menlo Park</location>
<marker>Klinkenberg, Renz, 1998</marker>
<rawString>Klinkenberg R. and Renz I. 1998. Adaptive information filtering: Learning in the presence of concept drifts. In Learning for Text Categorization, Menlo Park</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-K J Kuo</author>
<author>C H Lee</author>
</authors>
<title>Discriminative training of natural language call routers,</title>
<date>2003</date>
<booktitle>IEEE Transactions on Speech and Audio Processing, Volume 11, Issue 1,</booktitle>
<pages>24--35</pages>
<contexts>
<context position="13333" citStr="Kuo and Lee 2003" startWordPosition="2317" endWordPosition="2320"> data at the beginning of the FIFO list are needed making it altogether computationally inexpensive. 4 Related Techniques Our work relates to several existing techniques as follows. Many techniques of text and dialog analysis utilize a word frequency vector based approach (e.g., Chu-Carroll et al 1999) in which lexical features counts (term frequencies) are used to populate the vector. Sometimes the term frequency is normalized by document size and weighted by the inverse document frequency (TF-IDF). The TFIDF and TF metrics are the base of other approaches like discriminative classification (Kuo and Lee 2003; and Li and Huerta 2004), Text Tilling or topic chains (Hearst 1993; Zechner 2001), and latent semantic indexing (Landauer et al 1998). Ultimately, these types of approaches are the foundation of complex classification and document understanding systems which use these features together with possibly more sophisticated classification algorithms (e.g., D’Avanzo et al 2007). When using TF and TF-IDF approaches, it is important to notice that by normalizing the term frequency by the document length, TF-based approaches are effectively equivalent to estimation of a multinomial distribution. The v</context>
</contexts>
<marker>Kuo, Lee, 2003</marker>
<rawString>Kuo H.-K.J. and Lee C. H. Discriminative training of natural language call routers, IEEE Transactions on Speech and Audio Processing, Volume 11, Issue 1, Jan 2003 Page(s): 24 - 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<date>1998</date>
<booktitle>Introduction to Latent Semantic Analysis Discourse Processes 25,</booktitle>
<contexts>
<context position="13468" citStr="Landauer et al 1998" startWordPosition="2340" endWordPosition="2343">relates to several existing techniques as follows. Many techniques of text and dialog analysis utilize a word frequency vector based approach (e.g., Chu-Carroll et al 1999) in which lexical features counts (term frequencies) are used to populate the vector. Sometimes the term frequency is normalized by document size and weighted by the inverse document frequency (TF-IDF). The TFIDF and TF metrics are the base of other approaches like discriminative classification (Kuo and Lee 2003; and Li and Huerta 2004), Text Tilling or topic chains (Hearst 1993; Zechner 2001), and latent semantic indexing (Landauer et al 1998). Ultimately, these types of approaches are the foundation of complex classification and document understanding systems which use these features together with possibly more sophisticated classification algorithms (e.g., D’Avanzo et al 2007). When using TF and TF-IDF approaches, it is important to notice that by normalizing the term frequency by the document length, TF-based approaches are effectively equivalent to estimation of a multinomial distribution. The variance of the estimate will be larger as the number of observations decreases. Recently, approaches that explicitly establish this par</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Landauer T., Foltz P. W., and Laham D. Introduction to Latent Semantic Analysis Discourse Processes 25, 1998.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Last</author>
<author>Y Klein</author>
<author>A Kandel</author>
</authors>
<title>Knowledge Discovery</title>
<booktitle>in Time Series Databases IEEE Trans. on Systems, Man, and Cybernetics 31B(2001).</booktitle>
<marker>Last, Klein, Kandel, </marker>
<rawString>Last M., Klein Y., and Kandel A., Knowledge Discovery in Time Series Databases IEEE Trans. on Systems, Man, and Cybernetics 31B(2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>J M Huerta</author>
</authors>
<title>Discriminative Training of Compound word based Multinomial Classifiers for Speech Routing</title>
<date>2004</date>
<booktitle>Proc. ICSLP</booktitle>
<contexts>
<context position="13358" citStr="Li and Huerta 2004" startWordPosition="2322" endWordPosition="2325"> of the FIFO list are needed making it altogether computationally inexpensive. 4 Related Techniques Our work relates to several existing techniques as follows. Many techniques of text and dialog analysis utilize a word frequency vector based approach (e.g., Chu-Carroll et al 1999) in which lexical features counts (term frequencies) are used to populate the vector. Sometimes the term frequency is normalized by document size and weighted by the inverse document frequency (TF-IDF). The TFIDF and TF metrics are the base of other approaches like discriminative classification (Kuo and Lee 2003; and Li and Huerta 2004), Text Tilling or topic chains (Hearst 1993; Zechner 2001), and latent semantic indexing (Landauer et al 1998). Ultimately, these types of approaches are the foundation of complex classification and document understanding systems which use these features together with possibly more sophisticated classification algorithms (e.g., D’Avanzo et al 2007). When using TF and TF-IDF approaches, it is important to notice that by normalizing the term frequency by the document length, TF-based approaches are effectively equivalent to estimation of a multinomial distribution. The variance of the estimate w</context>
</contexts>
<marker>Li, Huerta, 2004</marker>
<rawString>Li X. and Huerta J.M., Discriminative Training of Compound word based Multinomial Classifiers for Speech Routing Proc. ICSLP 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mishne</author>
<author>D Carmel</author>
<author>R Hoory</author>
<author>A Roytman</author>
<author>A Soffer</author>
</authors>
<title>Automatic analysis of call-center conversations.</title>
<date>2005</date>
<booktitle>In Proc. of the 14th ACM international Conference on information and Knowledge.</booktitle>
<contexts>
<context position="1237" citStr="Mishne et al 2005" startWordPosition="178" endWordPosition="181">of document tags, and new story or event detection in conversational speech and text. Our approach benefits from the robustness, simplicity and efficiency of non-parametric and rank based approaches and consistently outperformed term-frequency and TF-IDF cosine distance approaches in several experiments conducted. 1 Background Existing research in dialog analysis has focused on several specific problems including dialog act detection (e.g., Byron and Heeman 1998), segmentation and chunking (e.g., Hearst 1993), topic detection (e.g., Zimmerman et al 2005), distillation and summarization (e.g., Mishne et al 2005) etc. The breath of this research reflects the increasing importance that dialog analysis has for multiple domains and applications. While historically, dialog analysis research has initially leveraged the corresponding techniques originally intended for textual document analysis, techniques tailored specifically for dialog processing eventually should be able to address the sparseness, noise, and time considerations intrinsic to dialog and conversations. The approach proposed in this paper focuses on the relative change of rank ordering of words occurring in a conversation according to their </context>
</contexts>
<marker>Mishne, Carmel, Hoory, Roytman, Soffer, 2005</marker>
<rawString>Mishne, G., Carmel, D., Hoory, R., Roytman, A., and Soffer, A. 2005. Automatic analysis of call-center conversations. In Proc. of the 14th ACM international Conference on information and Knowledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K B Pratt</author>
</authors>
<title>Locating patterns in discrete time series.</title>
<date>2001</date>
<tech>Master&apos;s thesis,</tech>
<institution>Computer Science and Engineering, University of South Florida,</institution>
<marker>Pratt, 2001</marker>
<rawString>Pratt K. B. Locating patterns in discrete time series. Master&apos;s thesis, Computer Science and Engineering, University of South Florida, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K O Stanley</author>
</authors>
<title>Learning concept drift with a committee of decision trees.</title>
<date>2003</date>
<tech>TR AI-03-302,</tech>
<institution>Comp. Science Dept., University of Texas-Austin.</institution>
<marker>Stanley, 2003</marker>
<rawString>Stanley K. O. Learning concept drift with a committee of decision trees. Comp. Science Dept., University of Texas-Austin. TR AI-03-302, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
</authors>
<title>Automatic Summarization of Spoken Dialogues in Unrestricted Domains.</title>
<date>2001</date>
<tech>PhD thesis, LTI, CMU,</tech>
<contexts>
<context position="13416" citStr="Zechner 2001" startWordPosition="2334" endWordPosition="2335">y inexpensive. 4 Related Techniques Our work relates to several existing techniques as follows. Many techniques of text and dialog analysis utilize a word frequency vector based approach (e.g., Chu-Carroll et al 1999) in which lexical features counts (term frequencies) are used to populate the vector. Sometimes the term frequency is normalized by document size and weighted by the inverse document frequency (TF-IDF). The TFIDF and TF metrics are the base of other approaches like discriminative classification (Kuo and Lee 2003; and Li and Huerta 2004), Text Tilling or topic chains (Hearst 1993; Zechner 2001), and latent semantic indexing (Landauer et al 1998). Ultimately, these types of approaches are the foundation of complex classification and document understanding systems which use these features together with possibly more sophisticated classification algorithms (e.g., D’Avanzo et al 2007). When using TF and TF-IDF approaches, it is important to notice that by normalizing the term frequency by the document length, TF-based approaches are effectively equivalent to estimation of a multinomial distribution. The variance of the estimate will be larger as the number of observations decreases. Rec</context>
</contexts>
<marker>Zechner, 2001</marker>
<rawString>Zechner K. Automatic Summarization of Spoken Dialogues in Unrestricted Domains. PhD thesis, LTI, CMU, 2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zimmermann</author>
<author>Y Liu</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<date>2005</date>
<booktitle>Toward Joint Segmentation and Classification of Dialog Acts in Multiparty Meetings MLMI workshop,</booktitle>
<marker>Zimmermann, Liu, Shriberg, Stolcke, 2005</marker>
<rawString>Zimmermann M., Liu Y., E. Shriberg, and A. Stolcke, Toward Joint Segmentation and Classification of Dialog Acts in Multiparty Meetings MLMI workshop, 2005</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>