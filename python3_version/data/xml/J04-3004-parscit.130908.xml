<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994017">
Understanding the Yarowsky Algorithm
</title>
<author confidence="0.999164">
Steven Abney∗
</author>
<affiliation confidence="0.994152">
University of Michigan
</affiliation>
<bodyText confidence="0.973671833333333">
Many problems in computational linguistics are well suited for bootstrapping (semisupervised
learning) techniques. The Yarowsky algorithm is a well-known bootstrapping algorithm, but
it is not mathematically well understood. This article analyzes it as optimizing an objective
function. More specifically, a number of variants of the Yarowsky algorithm (though not the
original algorithm itself) are shown to optimize either likelihood or a closely related objective
function K.
</bodyText>
<sectionHeader confidence="0.998163" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999807142857143">
Bootstrapping, or semisupervised learning, has become an important topic in com-
putational linguistics. For many language-processing tasks, there are an abundance
of unlabeled data, but labeled data are lacking and too expensive to create in large
quantities, making bootstrapping techniques desirable.
The Yarowsky (1995) algorithm was one of the first bootstrapping algorithms to be-
come widely known in computational linguistics. In brief, it consists of two loops. The
“inner loop” or base learner is a supervised learning algorithm. Specifically, Yarowsky
uses a simple decision list learner that considers rules of the form “If instance x con-
tains feature f, then predict label j” and selects those rules whose precision on the
training data is highest.
The “outer loop” is given a seed set of rules to start with. In each iteration, it uses
the current set of rules to assign labels to unlabeled data. It selects those instances
regarding which the base learner’s predictions are most confident and constructs a
labeled training set from them. It then calls the inner loop to construct a new classifier
(that is, a new set of rules), and the cycle repeats.
An alternative algorithm, co-training (Blum and Mitchell 1998), has subsequently
become more popular, perhaps in part because it has proven amenable to theoretical
analysis (Dasgupta, Littman, and McAllester 2001), in contrast to the Yarowsky al-
gorithm, which is as yet mathematically poorly understood. The current article aims
to rectify this lack of understanding, increasing the attractiveness of the Yarowsky
algorithm as an alternative to co-training. The Yarowsky algorithm does have the ad-
vantage of placing less of a restriction on the data sets it can be applied to. Co-training
requires data attributes to be separable into two views that are conditionally indepen-
dent given the target label; the Yarowsky algorithm makes no such assumption about
its data.
In previous work, I did propose an assumption about the data called precision
independence, under which the Yarowsky algorithm could be shown effective (Ab-
ney 2002). That assumption is ultimately unsatisfactory, however, not only because it
</bodyText>
<note confidence="0.8827166">
∗ 4080 Frieze Bldg., 105 S. State Street, Ann Arbor, MI 48109-1285. E-mail: abney.umich.edu.
Submission received: 26 August 2003; Revised submission received: 21 December 2003; Accepted for
publication: 10 February 2004
© 2004 Association for Computational Linguistics
Computational Linguistics Volume 30, Number 3
</note>
<tableCaption confidence="0.994289">
Table 1
</tableCaption>
<note confidence="0.5342025">
The Yarowsky algorithm variants. Y-1/DL-EM reduces H; the others
reduce K.
Y-1/DL-EM-Λ EM inner loop that uses labeled examples only
Y-1/DL-EM-X EM inner loop that uses all examples
</note>
<table confidence="0.9212532">
Y-1/DL-1-R Near-original Yarowsky inner loop, no smoothing
Y-1/DL-1-VS Near-original Yarowsky inner loop, “variable smoothing”
YS-P Sequential update, “antismoothing”
YS-R Sequential update, no smoothing
YS-FS Sequential update, original Yarowsky smoothing
</table>
<bodyText confidence="0.999797212121212">
restricts the data sets on which the algorithm can be shown effective, but also for ad-
ditional internal reasons. A detailed discussion would take us too far afield here, but
suffice it to say that precision independence is a property that it would be preferable
not to assume, but rather to derive from more basic properties of a data set, and that
closer empirical study shows that precision independence fails to be satisfied in some
data sets on which the Yarowsky algorithm is effective.
This article proposes a different approach. Instead of making assumptions about
the data, it views the Yarowsky algorithm as optimizing an objective function. We will
show that several variants of the algorithm (though not the algorithm in precisely its
original form) optimize either negative log likelihood H or an alternative objective
function, K, that imposes an upper bound on H.
Ideally, we would like to show that the Yarowsky algorithm minimizes H. Un-
fortunately, we are not able to do so. But we are able to show that a variant of the
Yarowsky algorithm, which we call Y-1/DL-EM, decreases H in each iteration. It com-
bines the outer loop of the Yarowsky algorithm with a different inner loop based on
the expectation-maximization (EM) algorithm.
A second proposed variant of the Yarowsky algorithm, Y-1/DL-1, has the advan-
tage that its inner loop is very similar to the original Yarowsky inner loop, unlike
Y-1/DL-EM, whose inner loop bears little resemblance to the original. Y-1/DL-1 has
the disadvantage that it does not directly reduce H, but we show that it does reduce
the alternative objective function K.
We also consider a third variant, YS. It differs from Y-1/DL-EM and Y-1/DL-1
in that it updates sequentially (adding a single rule in each iteration), rather than in
parallel (updating all rules in each iteration). Besides having the intrinsic interest of
sequential update, YS can be proven effective when using exactly the same smoothing
method as used in the original Yarowsky algorithm, in contrast to Y-1/DL-1, which
uses either no smoothing or a nonstandard “variable smoothing.” YS is proven to
decrease K.
The Yarowsky algorithm variants that we consider are summarized in Table 1. To
the extent that these variants capture the essence of the original algorithm, we have
a better formal understanding of its effectiveness. Even if the variants are deemed to
depart substantially from the original algorithm, we have at least obtained a family
of new bootstrapping algorithms that are mathematically understood.
</bodyText>
<sectionHeader confidence="0.991342" genericHeader="method">
2. The Generic Yarowsky Algorithm
</sectionHeader>
<subsectionHeader confidence="0.964772">
2.1 The Original Algorithm Y-0
</subsectionHeader>
<bodyText confidence="0.998723">
The original Yarowsky algorithm, which we refer to as Y-0, is given in table 2. It is
an iterative algorithm. One begins with a seed set Λ(0) of labeled examples and a
</bodyText>
<page confidence="0.995033">
366
</page>
<note confidence="0.937588">
Abney Understanding the Yarowsky Algorithm
</note>
<tableCaption confidence="0.96343">
Table 2
</tableCaption>
<figure confidence="0.797802916666667">
The generic Yarowsky algorithm (Y-0)
(1) Given: examples X, and initial labeling Y(0)
(2) For t ∈ {0, 1, ...}
(2.1) Train classifier on labeled examples (A(t), Y(t)), where A(t) = {x ∈ X|Y(t) =� ⊥}
The resulting classifier predicts label j for example x with probability 7r(t+1)
x (j)
(2.2) For each example x ∈ X:
(2.2.1) Set yˆ = arg maxj 7r(t+1)
� x (j)
�
�
(2.3) If Y(t+1) = Y(t), stop
</figure>
<bodyText confidence="0.994855333333333">
set V(0) of unlabeled examples. At each iteration, a classifier is constructed from the
labeled examples; then the classifier is applied to the unlabeled examples to create a
new labeled set.
To discuss the algorithm formally, we require some notation. We assume first a set
of examples X and a feature set Fx for each x E X. The set of examples with feature f
is Xf. Note that x E Xf if and only if f E Fx.
We also require a series of labelings Y(t), where t represents the iteration number.
We write Y(t) xfor the label of example x under labeling Y(t). An unlabeled example is
one for which Y(t)
</bodyText>
<equation confidence="0.758396">
x is undefined, in which case we write Y(t)
x = 1. We write V(t) for
</equation>
<bodyText confidence="0.999235">
the set of unlabeled examples and Λ(t) for the set of labeled examples. It will also be
useful to have a notation for the set of examples with label j:
</bodyText>
<equation confidence="0.993991333333333">
Λ(t)
j =_ {x E X�Y(t)
x = j =� 1}
</equation>
<bodyText confidence="0.943033428571429">
Note that Λ(t) is the disjoint union of the sets Λ(t)
j . When t is clear from context, we
drop the superscript (t) and write simply Λj, V, Yx, etc.
At the risk of ambiguity, we will also sometimes write Λf for the set of labeled
examples with feature f, trusting to the index to discriminate between Λf (labeled
examples with feature f) and Λj (labeled examples with label j). We always use f and
g to represent features and j and k to represent labels. The reader may wish to refer
to Table 3, which summarizes notation used throughout the article.
In each iteration, the Yarowsky algorithm uses a supervised learner to train a clas-
sifier on the labeled examples. Let us call this supervised learner the base learning
algorithm; it is a function from (X, Y(t)) to a classifier 7r drawn from a space of classi-
fiers Π. It is assumed that the classifier makes confidence-weighted predictions. That
is, the classifier defines a scoring function 7r(x,j), and the predicted label for example
x is
yˆ =_ arg max 7r(x,j) (1)
j
Ties are broken arbitrarily. Technically, we assume a fixed order over labels and define
the maximization as returning the first label in the ordering, in case of a tie.
It will be convenient to assume that the scoring function is nonnegative and
bounded, in which case we can normalize it to make 7r(x,j) a conditional distribution
over labels j for a given example x. Henceforward, we write 7rx(j) instead of 7r(x,j),
</bodyText>
<equation confidence="0.992389857142857">
(2.2.2) Set Y(t+1)
x =
Y(0)
x if x ∈ A(0)
yˆ if 7r(t+1)
x (ˆy) &gt; ζ
⊥ otherwise
</equation>
<page confidence="0.995771">
367
</page>
<figure confidence="0.672214096774194">
Computational Linguistics Volume 30, Number 3
Table 3
Summary of notation.
X set of examples, both labeled and unlabeled
Y the current labeling; Y(t) is the labeling at iteration t
Λ the (current) set of labeled examples
V the (current) set of unlabeled examples
x an example index
f, g feature indices
j, k label indices
Fx the features of example x
Yx the label of example x; value is undefined (1) if x is unlabeled
Xf, Λf, Vf examples, labeled examples, unlabeled examples that have feature f
Λj, Λfj examples with label j, examples with feature f and label j
M the number of features of a given example: |Fx |(cf. equation (12))
L the number of labels
Ox(j) labeling distribution (equation (5))
7rx(j) prediction distribution (equation (12); except for DL-0, which uses equation (11))
Bfj score for rule f --+ j; we view Bf as the prediction distribution off
yˆ label that maximizes 7rx(j) for given x (equation (1)
[[Φ]] truth value of Φ: value is 0 or 1
H objective function, negative log-likelihood (equation (6))
H(p) entropy of distribution p
H(p||q) cross entropy: − Ex p(x) logq(x) (cf. equations (2) and (3))
K objective function, upper bound on H (equation (20))
qf(j) precision of rule f --+ j (equation (9))
˜qf (j) smoothed precision (equation (10))
ˆqf (j) “peaked” precision (equation (25))
j† the label that maximizes precision qf(j) for a given feature f (equation (26))
j* the label that maximizes rule score Bfj for a given feature f (equation (28))
U(�) uniform distribution
</figure>
<bodyText confidence="0.997965894736842">
understanding 7rx to be a probability distribution over labels j. We call this distribution
the prediction distribution of the classifier on example x.
To complete an iteration of the Yarowsky algorithm, one recomputes labels for
examples. Specifically, the label yˆ is assigned to example x if the score 7rx(ˆy) exceeds a
threshold C, called the labeling threshold. The new labeled set A(t+1) contains all ex-
amples for which 7rx(ˆy) &gt; C. Relabeling applies only to examples in V(0). The labels for
examples in A(0) are indelible, because A(0) constitutes the original manually labeled
data, as opposed to data that have been labeled by the learning algorithm itself.
The algorithm continues until convergence. The particular base learning algorithm
that Yarowsky uses is deterministic, in the sense that the classifier induced is a deter-
ministic function of the labeled data. Hence, the algorithm is known to have converged
at whatever point the labeling remains unchanged.
Note that the algorithm as stated leaves the base learning algorithm unspecified.
We can distinguish between the generic Yarowsky algorithm Y-0, for which the base
learning algorithm is an open parameter, and the specific Yarowsky algorithm, which
includes a specification of the base learner. Informally, we call the generic algorithm
the outer loop and the base learner the inner loop of the specific Yarowsky algorithm.
The base learner that Yarowsky assumes is a decision list induction algorithm. We
postpone discussion of it until Section 3.
</bodyText>
<page confidence="0.990168">
368
</page>
<bodyText confidence="0.54295">
Abney Understanding the Yarowsky Algorithm
</bodyText>
<subsectionHeader confidence="0.99956">
2.2 An Objective Function
</subsectionHeader>
<bodyText confidence="0.999230625">
Machine learning algorithms are typically designed to optimize some objective func-
tion that represents a formal measure of performance. The maximum-likelihood crite-
rion is the most commonly used objective function. Suppose we have a set of examples
Λ, with labels Yx for x E Λ, and a parametric family of models πθ such that π(j|x;θ)
represents the probability of assigning label j to example x, according to the model.
The likelihood of θ is the probability of the full data set according to the model, viewed
as a function of θ, and the maximum-likelihood criterion instructs us to choose the
parameter settings θˆ that maximize likelihood, or equivalently, log-likelihood:
</bodyText>
<equation confidence="0.998866333333333">
l(θ) = log � π(Yx|x;θ)
xEA
�= log π(Yx|x; θ)
xEA
�= ~ [[j = Yx]]logπ(j|x;θ)
xEA j
</equation>
<bodyText confidence="0.968026666666667">
(The notation [[Φ]] represents the truth value of the proposition Φ; it is one if Φ is true
and zero otherwise.)
Let us define
</bodyText>
<equation confidence="0.986854">
φx(j) = [[j = Yx]] for x E Λ
</equation>
<bodyText confidence="0.973264666666667">
Note that φx satisfies the formal requirements of a probability distribution over labels
j: Specifically, it is a point distribution with all its mass concentrated on Yx. We call it
the labeling distribution. Now we can write
</bodyText>
<equation confidence="0.993689">
l(θ) = � � φx(j) logπ(j|x;θ)
xEA j
�= − H(φx||πx) (2)
xEA
</equation>
<bodyText confidence="0.998776666666667">
In (2) we have written πx for the distribution π(·|x; θ), leaving the dependence on θ
implicit. We have also used the nonstandard notation H(p||q) for what is sometimes
called cross entropy. It is easy to verify that
</bodyText>
<equation confidence="0.83531">
H(p||q) = H(p) + D(p||q) (3)
where H(p) is the entropy of p and D is Kullback-Leibler divergence. Note that when
p is a point distribution, H(p) = 0 and hence H(p||q) = D(p||q). In particular:
l(θ) = − � D(φx||πx) (4)
xEA
</equation>
<bodyText confidence="0.9991015">
Thus when, as here, φx is a point distribution, we can restate the maximum-likelihood
criterion as instructing us to choose the model that minimizes the total divergence
between the empirical labeling distributions φx and the model’s prediction distribu-
tions πx.
To extend l(θ) to unlabeled examples, we need only observe that unlabeled exam-
ples are ones about whose labels the data provide no information. Accordingly, we
</bodyText>
<page confidence="0.993623">
369
</page>
<note confidence="0.217953">
Computational Linguistics Volume 30, Number 3
</note>
<bodyText confidence="0.9880675">
revise the definition of ox to treat unlabeled examples as ones whose labeling distribu-
tion is the maximally uncertain distribution, which is to say, the uniform distribution:
</bodyText>
<equation confidence="0.992832833333333">
(1) _ ( Ql= Yx]] for x E Λ (5)
x tfor x E V
L
where L is the number of labels. Equivalently:
ox(j) =[[x E Λj]] + [[x E V]] 1
L
</equation>
<bodyText confidence="0.9967285">
When we replace Λ with X, expressions (2) and (4) are no longer equivalent; we
must use (2). Since H(ox||7rx) = H(ox) + D(ox||7rx), and H(ox) is minimized when x is
labeled, minimizing H(ox||7rx) forces one to label unlabeled examples. On labeled ex-
amples, H(ox||7rx) = D(ox||7rx), and D(ox||7rx) is minimized when the labels of examples
agree with the predictions of the model.
In short, we adopt as objective function
</bodyText>
<equation confidence="0.9977915">
�H == H(ox||7rx) = −l(o,θ) (6)
xEX
</equation>
<bodyText confidence="0.997902">
We seek to minimize H.
</bodyText>
<subsectionHeader confidence="0.999655">
2.3 The Modified Algorithm Y-1
</subsectionHeader>
<bodyText confidence="0.9996935">
We can show that a modified version of the Yarowsky algorithm finds a local minimum
of H. Two modifications are necessary:
</bodyText>
<listItem confidence="0.8437155">
• The labeling function Y is recomputed in each iteration as before, but
with the constraint that an example once labeled stays labeled. The label
may change, but a labeled example cannot become unlabeled again.
• We eliminate the threshold ζ or (equivalently) fix it at 1/L. As a result,
</listItem>
<bodyText confidence="0.996576571428571">
the only examples that remain unlabeled after the labeling step are those
for which 7rx is the uniform distribution. The problem with an arbitrary
threshold is that it prevents the algorithm from converging to a
minimum of H. A threshold that gradually decreases to 1/L would also
address the problem but would complicate the analysis.
The modified algorithm, Y-1, is given in Table 4.
To obtain a proof, it will be necessary to make an assumption about the supervised
</bodyText>
<equation confidence="0.74988025">
classifier 7r(t+1) induced by the base learner in step 2.1 of the algorithm. A natural as-
sumption is that the base learner chooses 7r(t+1) so as to minimize ExEΛ(t) D(o(t)
x ||7r(t+1)
x ).
</equation>
<bodyText confidence="0.8618465">
A weaker assumption will suffice, however. We assume that the base learner reduces
divergence, if possible. That is, we assume
</bodyText>
<equation confidence="0.9986835">
�∆DΛ == D(oXt)||7rXt+1)) − E D(oXt)||7rXt)) &lt; 0 (7)
xEΛ(t) xEΛ(t)
</equation>
<bodyText confidence="0.706920333333333">
with equality only if there is no classifier 7r(t+1) E Π that makes ∆DΛ &lt; 0. Note
that any learning algorithm that minimizes ExEΛ(t) D(oXt)||7rXt+1)) satisfies the weaker
assumption (7), inasmuch as the option of setting 7r(t+1)
</bodyText>
<equation confidence="0.826068">
x = 7r(t)
x is always available.
</equation>
<page confidence="0.989413">
370
</page>
<note confidence="0.776037">
Abney Understanding the Yarowsky Algorithm
</note>
<tableCaption confidence="0.981727">
Table 4
</tableCaption>
<bodyText confidence="0.816512">
The modified generic Yarowsky algorithm (Y-1).
</bodyText>
<listItem confidence="0.9896975">
(1) Given: X, Y(0)
(2) For t E f0, 1, ...}
(2.1) Train classifier on (A(t), Y(t)); result is π(t+1)
(2.2) For each example x E X:
</listItem>
<equation confidence="0.9898776">
(2.2.1) Set yˆ = arg maxj π(t+1)
 x (j)


(2.3) If Y(t+1) = Y(t), stop
</equation>
<bodyText confidence="0.7812525">
We also consider a somewhat stronger assumption, namely, that the base learner
reduces divergence over all examples, not just over labeled examples:
</bodyText>
<equation confidence="0.9988585">
�∆DX ≡ D(φXt)||πXt+1)) −11 D(φXt)||πXt)) ≤ 0 (8)
x∈X x∈X
</equation>
<bodyText confidence="0.999613333333333">
If a base learning algorithm satisfies (8), the proof of theorem 1 is shorter; but (7) is
the more natural condition for a base learner to satisfy.
We can now state the main theorem of this section.
</bodyText>
<subsectionHeader confidence="0.64737">
Theorem 1
</subsectionHeader>
<bodyText confidence="0.998819333333333">
If the base learning algorithm satisfies (7) or (8), algorithm Y-1 decreases H at each
iteration until it reaches a critical point of H.
We require the following lemma in order to prove the theorem:
</bodyText>
<subsectionHeader confidence="0.642277">
Lemma 1
</subsectionHeader>
<bodyText confidence="0.954665">
For all distributions p
</bodyText>
<equation confidence="0.988584333333333">
1
H(p) ≥ log
maxj p(j)
</equation>
<bodyText confidence="0.956101">
with equality iff p is the uniform distribution.
</bodyText>
<subsectionHeader confidence="0.870557">
Proof
</subsectionHeader>
<bodyText confidence="0.970335">
By definition, for all k:
</bodyText>
<equation confidence="0.9923695">
p(k) ≤ maxp(j)
j
1 1
log p(k) ≥ log maxj p(j)
Since this is true for all k, it is true if we take the expectation with respect to p:
1 � p(k) log 1
p(k) log p(k) ≥ maxj p(j)
k
1
H(p) ≥ log maxj p(j)
(2.2.2) Set Y(t+1)
x =
Y(0) xif x E A(0)
yˆ if x E A(t) V π(t+1)
x (ˆy) &gt; 1/L
L otherwise
E
k
</equation>
<page confidence="0.979698">
371
</page>
<note confidence="0.214744">
Computational Linguistics Volume 30, Number 3
</note>
<bodyText confidence="0.996383">
We have equality only if p(k) = maxj p(j) for all k, that is, only if p is the uniform
distribution.
We now prove the theorem.
</bodyText>
<subsectionHeader confidence="0.732357">
Proof of Theorem 1
</subsectionHeader>
<bodyText confidence="0.833545">
The algorithm produces a sequence of labelings 0(0),0(1),... and a sequence of clas-
</bodyText>
<equation confidence="0.9757104">
sifiers 7r(1), 7r(2), .... The classifier 7r(t+1) is trained on 0(t), and the labeling 0(t+1) is
created using 7r(t+1).
Recall that
H =E [H(0x) + D(0xjj7rx)]
x∈X
</equation>
<bodyText confidence="0.995801375">
In the training step (2.1) of the algorithm, we hold 0 fixed and change 7r, and in the
labeling step (2.2), we hold 7r fixed and change 0. We will show that the training step
minimizes H as a function of 7r, and the labeling step minimizes H as a function of 0
except in examples in which it is at a critical point of H. Hence, H is nonincreasing in
each iteration of the algorithm and is strictly decreasing unless (0(t),7r(t)) is a critical
point of H.
Let us consider the labeling step first. In this step, 7r is held constant, but 0 (pos-
sibly) changes, and we have
</bodyText>
<equation confidence="0.994188">
OH=E OH(x)
x∈X
</equation>
<bodyText confidence="0.538907">
where
</bodyText>
<equation confidence="0.990495333333333">
OH(x) _— H(0Xt+1)jj7rXt+1)) − H(0(t)
x jj7r(t+1)
x )
</equation>
<bodyText confidence="0.837788925925926">
We can show that OH is nonpositive if we can show that OH(x) is nonpositive for all
x.
We can guarantee that OH(x) &lt; 0 if 0(t+1) minimizes H(pjj7rx (t+1)) viewed as a
function of p. By definition:
E 1
H(pjj7r(t+1) pj log 7rx (j)
x ) = (t+1)
j
We wish to find the distribution p that minimizes H(pjj7r(t+1)
x ). Clearly, we accomplish
that by placing all the mass of p in pj∗, where j* minimizes −log 7rx(t+1) (j). If there
is more than one minimizer, H(pjj7rx (t+1)) is minimized by any distribution p that dis-
tributes all its mass among the minimizers of − log 7r(t+1)
x (j). Observe further that
arg min 1 = arg max 7rx (j)
j log 7rx (j) j (t+1)
(t+1) = yˆ
That is, we minimize H(pjj7r(t+1)
x ) by setting pj = [[j = ˆy]], which is to say, by labeling
x as predicted by 7r(t+1). That is how algorithm Y-1 defines 0(t+1)
x for all examples
x E A(t+1) whose labels are modifiable (that is, excluding x E A(0)).
Note that 0(t+1)
x does not minimize H(pjj7r(t+1)
x ) for examples x E V(t+1), that is, for
examples x that remain unlabeled at t + 1. However, in algorithm Y-1, any example
that is unlabeled at t + 1 is necessarily also unlabeled at t, so for any such example,
</bodyText>
<page confidence="0.955543">
372
</page>
<bodyText confidence="0.9758173">
Abney Understanding the Yarowsky Algorithm
∆H(x) = 0. Hence, if any label changes in the labeling step, H decreases, and if no
label changes, H remains unchanged; in either case, H does not increase.
We can show further that even for examples x ∈ V(t+1), the labeling distribution
φ(t+1)
x assigned by Y-1 represents a critical point of H. For any example x ∈ V(t+1),
the prediction distribution π(t+1)
x is the uniform distribution (otherwise Y-1 would
have labeled x). Hence the divergence between φ(t+1) and π(t+1) is zero, and thus at
a minimum. It would be possible to decrease H(φ(t+1)
</bodyText>
<equation confidence="0.8981438">
x ||π(t+1)
x ) by decreasing H(φ(t+1)
x )
at the cost of an increase in D(φ(t+1)
x ||π(t+1)
</equation>
<bodyText confidence="0.997545076923077">
x ), but all directions of motion (all ways of
selecting labels to receive increased probability mass) are equally good. That is to say,
the gradient of H is zero; we are at a critical point.
Essentially, we have reached a saddle point. We have minimized H with respect
to φx(j) along those dimensions with a nonzero gradient. Along the remaining dimen-
sions, we are actually at a local maximum, but without a gradient to choose a direction
of descent.
Now let us consider the algorithm’s training step (2.1). In this step, φ is held
constant, so the change in H is equal to the change in D—recall that H(φ||π) = H(φ) +
D(φ||π). By the hypothesis of the theorem, there are two cases: The base learner satisfies
either (7) or (8). If it satisfies (8), the base learner minimizes D as a function of π, hence
it follows immediately that it minimizes H as a function of π.
Suppose instead that the base learner satisfies (7). We can express H as
</bodyText>
<equation confidence="0.9959915">
�H = H(φx) + � D(φx||πx) + � D(φx||πx)
x∈X x∈Λ(t) x∈V(t)
</equation>
<bodyText confidence="0.964366833333333">
In the training step, the first term remains constant. The second term decreases, by
hypothesis. But the third term may increase. However, we can show that any increase
in the third term is more than offset in the labeling step.
Consider an arbitrary example x in V(t). Since it is unlabeled at time t, we know
that φ(t)
x is the uniform distribution u:
</bodyText>
<equation confidence="0.965116333333333">
1
u(j) = L
Moreover, π(t)
</equation>
<bodyText confidence="0.997023666666667">
x must also be the uniform distribution; otherwise example x would
have been labeled in a previous iteration. Therefore the value of H(x) = H(φx||πx) at
the beginning of iteration t is H0:
</bodyText>
<equation confidence="0.927875">
�H0 = φ(t) ~= 1 = H(u)
j x (j)log 1 j u(j) log u(j)
πx(t) (j)
After the training step, the value is H1:
�H1 = φ(t)
j x (j) log 1
π(t +1) (j)
x
</equation>
<bodyText confidence="0.978939666666667">
If πx remains unchanged in the training step, then the new distribution π(t+1)
x , like the
old one, is the uniform distribution, and the example remains unlabeled. Hence there
is no change in H, and in particular, H is nonincreasing, as desired. On the other hand,
if πx does change, then the new distribution π(t+1)
x is nonuniform, and the example is
</bodyText>
<page confidence="0.989358">
373
</page>
<note confidence="0.21306">
Computational Linguistics Volume 30, Number 3
</note>
<bodyText confidence="0.980476">
labeled in the labeling step. Hence the value of H(x) at the end of the iteration, after
the labeling step, is H2:
</bodyText>
<equation confidence="0.98290925">
�H2 = φ(t+1) 1
j x (j) log 1 =log πx (ˆy)
πx (j) (t+1)
(t+1)
</equation>
<bodyText confidence="0.880263666666667">
By Lemma 1, H2 &lt; H(u); hence H2 &lt; H0.
As we observed above, H1 &gt; H0, but if we consider the change overall, we find
that the increase in the training step is more than offset in the labeling step:
</bodyText>
<equation confidence="0.996949">
OH(x) = H2 − H1 + H1 − H0 &lt; 0
</equation>
<sectionHeader confidence="0.982115" genericHeader="method">
3. The Specific Yarowsky Algorithm
</sectionHeader>
<subsectionHeader confidence="0.999912">
3.1 The Original Decision List Induction Algorithm DL-0
</subsectionHeader>
<bodyText confidence="0.959491785714286">
When one speaks of the Yarowsky algorithm, one often has in mind not just the generic
algorithm Y-0 (or Y-1), but an algorithm whose specification includes the particular
choice of base learning algorithm made by Yarowsky. Specifically, Yarowsky’s base
learner constructs a decision list, that is, a list of rules of form f → j, where f is a
feature and j is a label, with score θfj. A rule f → j matches example x if x possesses
the feature f. The label predicted for a given example x is the label of the highest
scoring rule that matches x.
Yarowsky uses smoothed precision for rule scoring. As the name suggests,
smoothed precision ˜qf (j) is a smoothed version of (raw) precision qf (j), which is the
probability that rule f → j is correct given that it matches
qf (j) ≡ { 1%L /  |Af  |if otherwise (9 )
where Af is the set of labeled examples that possess feature f, and Afj is the set of
labeled examples with feature f and label j.
Smoothed precision ˜q(j|f; c) is defined as follows:
</bodyText>
<equation confidence="0.998472">
|Afj |+c ˜q(j|f ; �) ≡(10)
|Af |+ Le
</equation>
<bodyText confidence="0.998124">
We also write ˜qf (j) when c is clear from context.
Yarowsky defines a rule’s score to be its smoothed precision:
</bodyText>
<equation confidence="0.876489">
θfj = ˜qf (j)
</equation>
<bodyText confidence="0.9035647">
Anticipating later needs, we will also consider raw precision as an alternative: θfj =
qf(j). Both raw and smoothed precision have the properties of a conditional probability
distribution. Generally, we view θfj as a conditional distribution over labels j for a fixed
feature f.
Yarowsky defines the confidence of the decision list to be the score of the highest-
scoring rule that matches the instance being classified. This is equivalent to defining
πx(j) ∝ max θfj (11)
f ∈Fx
(Recall that Fx is the set of features of x.) Since the classifier’s prediction for x is
defined, in equation (1), to be the label that maximizes πx(j), definition (11) implies
</bodyText>
<page confidence="0.990147">
374
</page>
<note confidence="0.723628">
Abney Understanding the Yarowsky Algorithm
</note>
<tableCaption confidence="0.949965">
Table 5
</tableCaption>
<bodyText confidence="0.949221">
The decision list induction algorithm DL-0. The value accumulated in N[f, j] is |Afj|, and the
value accumulated in Z[f] is |Af|.
</bodyText>
<listItem confidence="0.936674166666667">
(0) Given: a fixed value for E &gt; 0
Initialize arrays N[f, j] = 0, Z[f] = 0 for all f, j
(1) For each example x E A
(1.1) Let j be the label of x
(1.2) Increment N[f, j], Z[f], for each feature f of x
(2) For each feature f and label j
</listItem>
<equation confidence="0.891362666666667">
(2.1) Set θfj = N�f,j���
Z�f��L�
(*) Define πx(j) a maxf∈Fx θfj
</equation>
<bodyText confidence="0.998787692307692">
that the classifier’s prediction is the label of the highest-scoring rule matching x, as
desired.
We have written a in (11) rather than = because maximizing Ofj across f E Fx for
each label j will not in general yield a probability distribution over labels—though the
scores will be positive and bounded, and hence normalizable. Considering only the
final predicted label yˆ for a given example x, the normalization will have no effect,
inasmuch as all scores Ofj being compared will be scaled in the same way.
As characterized by Yarowsky, a decision list contains only those rules f → j whose
score ˜qf(j) exceeds the labeling threshold C. This can be seen purely as an efficiency
measure. Including rules whose score falls below the labeling threshold will have no
effect on the classifier’s predictions, as the threshold will be applied when the classifier
is applied to examples. For this reason, we do not prune the list. That is, we represent
a decision list as a set of parameters 1Ofj}, one for every possible rule f → j in the cross
product of the set of features and the set of labels.
The decision list induction algorithm used by Yarowsky is summarized in Table 5;
we refer to it as DL-0. Note that the step labeled (*) is not actually a step of the
induction algorithm but rather specifies how the decision list is used to compute a
prediction distribution 7rx for a given example x.
Unfortunately, we cannot prove anything about DL-0 as it stands. In particular,
we are unable to show that DL-0 reduces divergence between prediction and labeling
distributions (7). In the next section, we describe an alternative decision list induc-
tion algorithm, DL-EM, that does satisfy (7); hence we can apply Theorem 1 to the
combination Y-1/DL-EM to show that it reduces H. However, a disadvantage of DL-
EM is that it does not resemble the algorithm DL-0 used by Yarowsky. We return in
section 3.4 to a close variant of DL-0 called DL-1 and show that though it does not
directly reduce H, it does reduce the upper bound K.
</bodyText>
<subsectionHeader confidence="0.999671">
3.2 The Decision List Induction Algorithm DL-EM
</subsectionHeader>
<bodyText confidence="0.999705125">
The algorithm DL-EM is a special case of the EM algorithm. We consider two versions
of the algorithm: DL-EM-A and DL-EM-X. They differ in that DL-EM-A is trained on
labeled examples only, whereas DL-EM-X is trained on both labeled and unlabeled
examples. However, the basic outline of the algorithm is the same for both.
First, the DL-EM algorithms do not assume Yarowsky’s definition of 7r, given in
(11). As discussed above, the parameters Ofj can be thought of as defining a prediction
distribution Of (j) over labels j for each feature f. Hence equation (11) specifies how the
prediction distributions Of for the features of example x are to be combined to yield a
</bodyText>
<page confidence="0.9861">
375
</page>
<note confidence="0.209019">
Computational Linguistics Volume 30, Number 3
</note>
<bodyText confidence="0.960904">
prediction distribution πx for x. Instead of combining distributions by maximizing θfj
across f ∈ Fx as in equation (11), DL-EM takes a mixture of the θf:
</bodyText>
<equation confidence="0.956965666666667">
1 � θfj (12)
πx(j) = m
f EFx
</equation>
<bodyText confidence="0.959626090909091">
Here m = |Fx |is the number of features that x possesses; for the sake of simplicity, we
assume that all examples have the same number of features. Since θf is a probability
distribution for each f, and since any convex combination of distributions is also a
distribution, it follows that πx as defined in (12) is a probability distribution.
The two definitions for πx(j), (11) and (12), will often have the same mode ˆy, but
that is guaranteed only in the rather severely restricted case of two features and two
labels. Under definition (11), the prediction is determined entirely by the strongest
θf, whereas definition (12) permits a bloc of weaker θf to outvote the strongest one.
Yarowsky explicitly wished to avoid the possibility of such interactions. Nonetheless,
definition (12), used by DL-EM, turns out to make analysis of other base learners more
manageable, and we will assume it henceforth, not only for DL-EM, but also for the
algorithms DL-1 and YS discussed in subsequent sections.
DL-EM also differs from DL-0 in that DL-EM does not construct a classifier “from
scratch” but rather seeks to improve on a previous classifier. In the context of the
Yarowsky algorithm, the previous classifier is the one from the previous iteration of
the outer loop. We write θold
fj for the parameters and πold
x for the prediction distributions
of the previous classifier.
Conceptually, DL-EM considers the label j assigned to an example x to be gen-
erated by choosing a feature f ∈ Fx and then assigning the label j according to the
feature’s prediction distribution θf (j). The choice of feature f is a hidden variable. The
degree to which an example labeled j is imputed to feature f is determined by the old
distribution:
[[f old r{ ∈ Fx]] qld [[f ∈ Fx]](&apos;m θold
v I x,j) E=
g [[g ∈ Fx]]θgjd πold V)
One can think of πold(f |x, j) either as the posterior probability that feature f was re-
sponsible for the label j, or as the portion of the labeled example (x, j) that is imputed
to feature f. We also write πold
xj (f) as a synonym for πold(f |x, j). The new estimate θfj is
obtained by summing imputed occurrences of (f, j) and normalizing across labels. For
DL-EM-Λ, this takes the form
</bodyText>
<equation confidence="0.87570425">
ExEΛj πold(f|x, j)
θfj = E ExEΛk πold(f|x,k)
k
The algorithm is summarized in Table 6.
</equation>
<bodyText confidence="0.9993655">
The second version of the algorithm, DL-EM-X, is summarized in Table 7. It is like
DL-EM-Λ, except that it uses the update rule
</bodyText>
<equation confidence="0.823058">
θ — ExEΛj πold(f|x,j) + L ExEV πold(f|x,j) /13
</equation>
<bodyText confidence="0.900697">
fl Ek [ExEΛk πold (f  |x, k) + L ExEV πol d (f |x, k)] l )
Update rule (13) includes unlabeled examples as well as labeled examples. Concep-
tually, it divides each unlabeled example equally among the labels, then divides the
resulting fractional labeled example among the example’s features.
</bodyText>
<page confidence="0.993901">
376
</page>
<note confidence="0.786731">
Abney Understanding the Yarowsky Algorithm
</note>
<tableCaption confidence="0.872263">
Table 6
</tableCaption>
<figure confidence="0.978390444444444">
DL-EM-A decision list induction algorithm.
(0) Initialize N[f, j] = 0 for all f, j
(1) For each example x labeled j
(1.1) Let Z = Eg∈Fx θold
gj
(1.2) For each f ∈ Fx, increment N[f, j] by 1 Zθold
fj
(2) For each feature f
(2.1) Let Z = Ej N[f, j]
(2.2) For each label j, set θfj = 1ZN[f,j]
Table 7
DL-EM-X decision list induction algorithm.
(0) Initialize N[f, j] = 0 and U[f , j] = 0, for all f, j
(1) For each example x labeled j
(1.1) Let Z = E g∈Fx θold
gj
(1.2) For each f ∈ Fx, increment N[f, j] by 1 Zθold
fj
(2) For each unlabeled example x
(2.1) Let Z = E g∈Fx θold
gj
(2.2) For each f ∈ Fx, increment U[f ,j] by 1 Zθold
fj
(3) For each feature f
(3.1) Let Z = Ej(N[f,j] + 1L U[f,j])
(3.2) For each label j, set θ= 1 (N[f,j] + 1 U[f,j])
fj
</figure>
<bodyText confidence="0.999337">
We note that both variants of the DL-EM algorithm constitute a single iteration
of an EM-like algorithm. A single iteration suffices to prove the following theorem,
though multiple iterations would also be effective:
</bodyText>
<subsectionHeader confidence="0.46338">
Theorem 2
</subsectionHeader>
<bodyText confidence="0.994325333333333">
The classifier produced by the DL-EM-Λ algorithm satisfies equation (7), and the clas-
sifier produced by the DL-EM-X algorithm satisfies equation (8).
Combining Theorems 1 and 2 yields the following corollary:
</bodyText>
<sectionHeader confidence="0.791255" genericHeader="method">
Corollary
</sectionHeader>
<bodyText confidence="0.960402444444445">
The Yarowsky algorithm Y-1, using DL-EM-Λ or DL-EM-X as its base learning algo-
rithm, decreases H at each iteration until it reaches a critical point of H.
Proof of Theorem 2
Let Bold represent the parameter values at the beginning of the call to DL-EM, let B
represent a family of free variables that we will optimize, and let 7rold and 7r be the
corresponding prediction distributions. The labeling distribution O is fixed. For any
set of examples α, let ∆Dα be the change in Ex∈α D(Ox||7rx) resulting from the change
in B. We are obviously particularly interested in two cases: that in which α is the set
of all examples X (for DL-EM-X) and that in which α is the set of labeled examples
</bodyText>
<page confidence="0.982681">
377
</page>
<table confidence="0.989869263157895">
Computational Linguistics Volume 30, Number 3
Λ (for DL-EM-Λ). In either case, we will show that ∆Dα &lt; 0, with equality only if no
choice of θ decreases D.
We first derive an expression for −∆Dα that we will put to use shortly:
�−∆Dα = [D(φx||πxold) − D(φx||πx)I
x∈α � �
�= H(φx||πold
x∈α x ) − H(φx) − H(φx||πx) + H(φx)
�= ~ φx (j) [log πx (j) − log πxold (j)] (14)
x∈α j
The EM algorithm is based on the fact that divergence is non-negative, and strictly
positive if the distributions compared are not identical:
� 11 φx(j)D(πold
0 &lt; x∈α xj ||πxj)
j
�= 11 � πold
j x∈α φx(j) xj (f )
f ∈Fx πold
xj (f ) log πxj(f )
~= � � � θold �
j x∈α φx(j) πold
f ∈Fx xj (f ) log x (j) � πx(j)
fj
πold θfj
which yields the inequality
~ E φx (j) [log πx (j) − log πxold (j) ] &gt; E E � � �
j x∈α j x∈α φx(j) πold
f ∈Fx xj (f ) log θfj − log θold
fj
By (14), this can be written as E � � �
� x∈α φx(j) πold
− ∆Dα &gt; f ∈Fx xj (f ) log θfj − log θold (15)
j fj
Since θold
fj is constant, by maximizing
~ E � πold
j x∈α φx(j) xj (f ) log θfj (16)
f ∈Fx
</table>
<bodyText confidence="0.804292666666667">
we maximize a lower bound on −∆Dα. It is easy to see that −∆Dα is bounded above
by zero: we simply set θfj = θold
fj . Since divergence is zero only if the two distributions
are identical, we have strict inequality in (15) unless the best choice for θ is θold, in
which case no choice of θ makes ∆Dα &lt; 0.
It remains to show that DL-EM computes the parameter set θ that maximizes (16).
We wish to maximize (16) under the constraints that the values {θfj} for fixed f sum to
unity across choices of j, so we apply Lagrange’s method. We express the constraints
in the form
</bodyText>
<equation confidence="0.582503">
Cf = 0
</equation>
<bodyText confidence="0.928874">
where
</bodyText>
<equation confidence="0.658969">
Cf � � θfj − 1
j
</equation>
<page confidence="0.968881">
378
</page>
<bodyText confidence="0.871976">
Abney Understanding the Yarowsky Algorithm
We seek a solution to the family of equations that results from expressing the gradient
of (16) as a linear combination of the gradients of the constraints:
</bodyText>
<equation confidence="0.998175">
∂ � � � ∂Cf
φx(k) πold
xk (g) log θgk = λf (17)
∂θfj ∂θfj
k x∈α g∈Fx
</equation>
<bodyText confidence="0.9288175">
We derive an expression for the derivative on the left-hand side:
Similarly for the right-hand side:
</bodyText>
<equation confidence="0.7641675">
∂ � � � �
φx(k) πold
xk (g) log θgk = φx(j)πold
xj (f )
∂θfj
k x∈α g∈Fx x∈X ∩α
</equation>
<table confidence="0.971134166666667">
f θfj
1
Substituting these into equation (17): ∂Cf = 1
∂θfj
� φx(j)πold
x∈Xf ∩α xj (f ) 1 = λf
θfj
� φx(j)πold
θfj = xj (f ) 1(18)
x∈Xf∩α λf
Using the constraint Cf = 0 and solving for λf:
~ � φx(j)πold
j x∈Xf ∩α xj (f ) 1 − 1 = 0
λf
� E φx(j)πold
λf = x∈Xf∩α xj (f )
j
Substituting back into (18):
</table>
<equation confidence="0.892165111111111">
Ex∈Xf∩α φx(j)πold
xj (f )
θfj =EkEx∈Xf∩α φx(k)πold
xk (f) (19)
If we consider the case where α is the set of all examples and expand φx in (19),
we obtain
 
1 θfj = Z 7rxj d (f) + L πxjd(f)
2E x∈Vf
</equation>
<bodyText confidence="0.788769428571429">
where Z normalizes θf. It is not hard to see that this is the update rule that DL-EM-X
computes, using the intermediate values:
� πold
N[f , j] = xj (f )
x∈Λfj
� πold
U[f ,j] = xj (f)
</bodyText>
<page confidence="0.638023">
x∈Vf
379
</page>
<figure confidence="0.618495666666667">
Computational Linguistics Volume 30, Number 3
If we consider the case where α is the set of labeled examples and expand φx in (19),
we obtain
1 � πold
θfj = Z xj (f )
xEΛfj
</figure>
<bodyText confidence="0.9979658">
This is the update rule that DL-EM-Λ computes. Thus we see that DL-EM-X reduces
DX, and DL-EM-Λ reduces DΛ.
We note in closing that DL-EM-X can be simplified when used with algorithm Y-1,
inasmuch as it is known that θfj = 1/L for all (f, j), where f E Fx for some x E V. Then
the expression for U[f,j] simplifies as follows:
</bodyText>
<equation confidence="0.994243125">
E πold
xEVf xj (f )
� �
�1/L
=
xEV EgEFx 1/L
= |Vf|
m
</equation>
<bodyText confidence="0.999935666666667">
The dependence on j disappears, so we can replace U[f,j] with U[f] in algorithm
DL-EM-X, delete step 2.1, and replace step 2.2 with the statement “For each f E Fx,
increment U[f] by 1/m.”
</bodyText>
<subsectionHeader confidence="0.998527">
3.3 The Objective Function K
</subsectionHeader>
<bodyText confidence="0.9997995">
Y-1/DL-EM is the only variation on the Yarowsky algorithm that we can show to
reduce negative log-likelihood, H. The variants that we discuss in the remainder of
the article, Y-1/DL-1 and YS, reduce an alternative objective function, K, which we
now define.
The value K (or, more precisely, the value K/m) is an upper bound on H, which
we derive using Jensen’s inequality, as follows:
</bodyText>
<equation confidence="0.988283666666667">
� ~ � 1 θgj
H = − j φxj log m
xEX gEFx
� � � 1 log θgj
&lt; − j φxj m
xEX gEFx
</equation>
<bodyText confidence="0.7432965">
We define = 1~ E H(φx||θg)
m xEX gEFx
</bodyText>
<equation confidence="0.9893085">
�K =- E H(φx||θg) (20)
xEX gEFx
</equation>
<bodyText confidence="0.9982224">
By minimizing K, we minimize an upper bound on H. Moreover, it is in principle
possible to reduce K to zero. Since H(φx||θg) = H(φx) + D(φx||θg), K is reduced to zero
if all examples are labeled, each feature concentrates its prediction distribution in a
single label, and the label of every example agrees with the prediction of every feature
it possesses. In this limiting case, any minimizer of K is also a minimizer of H.
</bodyText>
<page confidence="0.994793">
380
</page>
<note confidence="0.855791">
Abney Understanding the Yarowsky Algorithm
</note>
<tableCaption confidence="0.958332">
Table 8
</tableCaption>
<figure confidence="0.595243875">
The decision list induction algorithm DL-1-R.
(0) Initialize N[f, j] = 0, Z[f] = 0 for all f, j
(1) For each example-label pair (x, j)
(1.1) For each feature f ∈ Fx, increment N[f, j], Z[f]
(2) For each feature f and label j
(2.1) Set Bfj = N[f,j]
Z[f]
(*) Define 7rx(j) = m Ef∈Fx Bfj
</figure>
<tableCaption confidence="0.937885">
Table 9
</tableCaption>
<bodyText confidence="0.926479">
The decision list induction algorithm DL-1-VS.
</bodyText>
<listItem confidence="0.967165666666667">
(0) Initialize N[f, j] = 0, Z[f] = 0, U[f] = 0 for all f, j
(1) For each example-label pair (x, j)
(1.1) For each feature f ∈ Fx, increment N[f, j], Z[f]
(2) For each unlabeled example x
(2.1) For each feature f ∈ Fx, increment U[f]
(3) For each feature f and label j
(3.1) Set e = U[f]/L
(3.2) Set Bfj = V1+UV]
(*) Define 7rx(j) = m Ef∈Fx Bfj
</listItem>
<bodyText confidence="0.999985142857143">
We hasten to add a proviso: It is not possible to reduce K to zero for all data sets.
The following provides a necessary and sufficient condition for being able to do so.
Consider an undirected bipartite graph G whose nodes are examples and features.
There is an edge between example x and feature f just in case f is a feature of x.
Define examples x1 and x2 to be neighbors if they both belong to the same connected
component of G. K is reducible to zero if and only if x1 and x2 have the same label
according to Y(0), for all pairs of neighbors x1 and x2 in Λ(0).
</bodyText>
<subsectionHeader confidence="0.965636">
3.4 Algorithm DL-1
</subsectionHeader>
<bodyText confidence="0.9992945">
We consider two variants of DL-0, called DL-1-R and DL-1-VS. They differ from DL-0
in two ways. First, the DL-1 algorithms assume the “mean” definition of 7rx given in
equation (12) rather than the “max” definition of equation (11). This is not actually a
difference in the induction algorithm itself, but in the way the decision list is used to
construct a prediction distribution 7rx.
Second, the DL-1 algorithms use update rules that differ from the smoothed pre-
cision of DL-0. DL-1-R (Table 8) uses raw precision instead of smoothed precision.
DL-1-VS (Table 9) uses smoothed precision, but unlike DL-0, DL-1-VS does not use
a fixed smoothing constant e; rather e varies from feature to feature. Specifically, in
computing the score Ofj, DL-1-VS uses |Vf |/L as its value for e.
The value of e used by DL-1-VS can be expressed in another way that will prove
useful. Let us define
</bodyText>
<equation confidence="0.97292375">
|Λf |
p(Λ|f) ≡ |Xf|
|Vf |
p(V|f ) ≡ |Xf|
</equation>
<page confidence="0.990906">
381
</page>
<figure confidence="0.779695">
Computational Linguistics Volume 30, Number 3
Lemma 2
The parameter values {θfj} computed by DL-1-VS can be expressed as
</figure>
<equation confidence="0.718277">
θfj = p(Λ|f)qf(j) + p(V|f)u(j) (21)
</equation>
<bodyText confidence="0.804667">
where u(j) is the uniform distribution over labels.
</bodyText>
<subsectionHeader confidence="0.603102">
Proof
</subsectionHeader>
<bodyText confidence="0.8751258">
If |Λf |= 0, then p(Λ|f) = 0 and θfj = u(j). Further, N[f,j] = Z[f] = 0, so DL-1-VS
computes θfj = u(j), and the lemma is proved. Hence we need only consider the case
|Λf |&gt; 0.
First we show that smoothed precision can be expressed as a convex combination
of raw precision (9) and the uniform distribution. Define δ = c/|Λf|. Then:
</bodyText>
<equation confidence="0.999868625">
|Λfj |+ �
˜qf(j) = |Λf |+ L
|Λfj|/|Λf |+ δ
= 1 + Lδ
1 Lδ δ
=1 + Lδ qf (j) + 1 + Lδ �Lδ
1 Lδ
= 1 + Lδ qf (j) + 1 + Lδ u(j) (22)
</equation>
<bodyText confidence="0.8924325">
Now we show that the mixing coefficient 1/(1 + Lδ) of (22) is the same as the mixing
coefficient p(Λ|f) of the lemma, when c = |Vf|/L as in step 3.1 of DL-1-VS:
</bodyText>
<equation confidence="0.997953555555556">
e = |Vf |p(V|f)
= L � p(Λ|f)
|Λf|
L
1
Lδ =
p(Λ|f )
1 = p(Λ|f)
1 + Lδ
</equation>
<bodyText confidence="0.9047568">
The main theorem of this section (Theorem 5) is that the specific Yarowsky al-
gorithm Y-1/DL-1 decreases K in each iteration until it reaches a critical point. It is
proved as a corollary of two theorems. The first (Theorem 3) shows that DL-1 min-
imizes K as a function of θ, holding φ constant, and the second (Theorem 4) shows
that Y-1 decreases K as a function of φ, holding θ constant. More precisely, DL-1-R
minimizes K over labeled examples Λ, and DL-1-VS minimizes K over all examples
X. Either is sufficient for Y-1 to be effective.
Theorem 3
DL-1 minimizes K as a function of θ, holding φ constant. Specifically, DL-1-R minimizes
K over labeled examples Λ, and DL-1-VS minimizes K over all examples X.
</bodyText>
<subsectionHeader confidence="0.489291">
Proof
</subsectionHeader>
<bodyText confidence="0.995701">
We wish to minimize K as a function of θ under the constraints
</bodyText>
<equation confidence="0.986442">
Cf -� θfj − 1 = 0
j
− 1
</equation>
<page confidence="0.989202">
382
</page>
<bodyText confidence="0.909708">
Abney Understanding the Yarowsky Algorithm
for each f. As before, to minimize K under the constraints Cf = 0, we express the
gradient of K as a linear combination of the gradients of the constraints and solve the
resulting system of equations:
</bodyText>
<equation confidence="0.954617">
(23)
∂θfj
</equation>
<bodyText confidence="0.999742">
First we derive expressions for the derivatives of Cf and K. The variable α represents
the set of examples over which we are minimizing K:
</bodyText>
<equation confidence="0.986887">
∂Cf = 1
∂θfj
∂ E E E φxk log θgk
∂θfj x∈αg∈Fx k
�= −
x∈Xf∩α
</equation>
<bodyText confidence="0.647931">
We substitute these expressions into (23) and solve for θfj:
</bodyText>
<equation confidence="0.9061061875">
�− 1
x∈Xf∩α φxj = λf
θfj
θfj = − � φxj/λf
x∈Xf∩α
Substituting the latter expression into the equation for Cf = 0 and solving for f yields
 
E −φxj/λf = 1
x∈Xf∩α
−|Xf n α |= λf
Substituting this back into the expression for θfj gives us
|Xf n α |x∈Xf (24)
a φxj
If α = Λ, we have
1θfj = |Λf |[[x E Λj]]
= qf (j)
</equation>
<bodyText confidence="0.9462615">
This is the update computed by DL-1-R, showing that DL-1-R computes the parameter
values 1θfj} that minimize K over the labeled examples Λ.
</bodyText>
<figure confidence="0.821052692307692">
∂K
= λf
∂θfj
∂Cf
∂K
∂θfj
1
φxj θfj
~
j
θfj =
1
x∈Λf
</figure>
<page confidence="0.778534">
383
</page>
<note confidence="0.265465">
Computational Linguistics Volume 30, Number 3
</note>
<equation confidence="0.8872342">
If α = X, we have
θfj = |1|XY[[x ∈ Λj]] +|Xf 1|x∈Vf YL
|Λfj ||Vf |1
|Λf |+ |Xf |·L
= p(Λ|f) · qf(j) + p(V|f) · u(j)
</equation>
<bodyText confidence="0.9956885">
By Lemma 2, this is the update computed by DL-1-VS, hence DL-1-VS minimizes K
over the complete set of examples X.
</bodyText>
<subsectionHeader confidence="0.441006">
Theorem 4
</subsectionHeader>
<bodyText confidence="0.916705">
If the base learner decreases K over X or over Λ, where the prediction distribution is
computed as
</bodyText>
<equation confidence="0.989757333333333">
1 ~
πx(j) = m
f ∈Fx
</equation>
<bodyText confidence="0.994188">
then algorithm Y-1 decreases K at each iteration until it reaches a critical point, con-
sidering K as a function of φ with θ held constant.
</bodyText>
<subsectionHeader confidence="0.859688">
Proof
</subsectionHeader>
<bodyText confidence="0.999146">
The proof has the same structure as the proof of Theorem 1, so we give only a sketch
here. We minimize K as a function of φ by minimizing it for each example separately:
</bodyText>
<equation confidence="0.949875">
K(x) = � H(φx||θg)
g∈Fx
</equation>
<table confidence="0.769547714285714">
�= � 1
j φxj log θgj
g∈Fx
To minimize K(x), we choose φxj so as to concentrate all mass in
�arg min 1
j log = arg max πx(j)
g∈Fx θgj j
</table>
<bodyText confidence="0.9969245">
This is the labeling rule used by Y-1.
If the base learner minimizes over Λ only, rather than X, it can be shown that any
increase in K on unlabeled examples is compensated for in the labeling step, as in the
proof of Theorem 1.
</bodyText>
<subsectionHeader confidence="0.613501">
Theorem 5
</subsectionHeader>
<bodyText confidence="0.999598">
The specific Yarowsky algorithms Y-1/DL-1-R and Y-1/DL-1-VS decrease K at each
iteration until they reach a critical point.
</bodyText>
<subsectionHeader confidence="0.85005">
Proof
</subsectionHeader>
<bodyText confidence="0.957775">
Immediate from Theorems 3 and 4.
</bodyText>
<sectionHeader confidence="0.98438" genericHeader="method">
4. Sequential Algorithms
</sectionHeader>
<subsectionHeader confidence="0.995847">
4.1 The Family YS
</subsectionHeader>
<bodyText confidence="0.993228">
The Yarowsky algorithm variants we have considered up to now do “parallel” updates
in the sense that the parameters {θfj} are completely recomputed at each iteration. In
</bodyText>
<equation confidence="0.6406075">
=
|Λf|
|Xf  |·
θfj
</equation>
<page confidence="0.991939">
384
</page>
<bodyText confidence="0.973407944444445">
Abney Understanding the Yarowsky Algorithm
this section, we consider a family YS of “sequential” variants of the Yarowsky al-
gorithm, in which a single feature is selected for update at each iteration. The YS
algorithms resemble the “Yarowsky-Cautious” algorithm of Collins &amp; Singer (1999),
though they differ from that algorithm in that they update a single feature in each iter-
ation, rather than a small set of features, as in Yarowsky-Cautious. The YS algorithms
are intended to be as close to the Y-1/DL-1 algorithm as is consonant with single-
feature updates. The YS algorithms differ from one another, and from Y-1/DL-1, in
the choice of update rule. An interesting range of update rules work in the sequential
setting. In particular, smoothed precision with fixed e, as in the original algorithm
Y-0/DL-0, works in the sequential setting, though with a proviso that will be spelled
out later.
Instead of an initial labeled set, there is an initial classifier consisting of a set of
selected features S0 and initial parameter set B(0) with B(0) fj= 1/L for all f V S0. At
each iteration, one feature is selected to be added to the selected set. A feature, once
selected, remains in the selected set. It is permissible for a feature to be selected more
than once; this permits us to continue reducing K even after all features have been
selected. In short, there is a sequence of selected features f0,f1, ... , and
</bodyText>
<equation confidence="0.417293">
St+1 = St U {ft}
</equation>
<bodyText confidence="0.959111666666667">
The parameters for the selected feature are also updated. At iteration t, the pa-
rameters Bgj, with g = ft, may be modified, but all other parameters remain constant.
That is:
</bodyText>
<equation confidence="0.718983666666667">
B(t+1)
gj = B(t)
gj for g =� ft
It follows that, for all t:
1
L for g V St
</equation>
<bodyText confidence="0.999990823529412">
However, parameters for features in S0 may not be modified, inasmuch as they play
the role of manually labeled data.
In each iteration, one selects a feature ft and computes (or recomputes) the predic-
tion distribution Bft for the selected feature ft. Then labels are recomputed as follows.
Recall that yˆ = arg maxj 7rx(j), where we continue to assume 7rx(j) to have the “mix-
ture” definition (equation (12)). The label of example x is set to yˆ if any feature of x
belongs to St+1. In particular, all previously labeled examples continue to be labeled
(though their labels may change), and any unlabeled examples possessing feature ft
become labeled.
The algorithm is summarized in Table 10. It is actually an algorithm schema;
the definition for “update” needs to be supplied. We consider three different update
functions: one that uses raw precision as its prediction distribution, one that uses
smoothed precision, and one that goes in the opposite direction, using what we might
call “peaked precision.” As we have seen, smoothed precision can be expressed as a
mixture of raw precision and the uniform (i.e., maximum-entropy) distribution (22).
Peaked precision ˆq(f) mixes in a certain amount of the point (i.e., minimum-entropy)
distribution that has all its mass on the label that maximizes raw precision:
</bodyText>
<equation confidence="0.926828">
ˆqf (j) = p(Λ(t)V)qf(j) + p(V(t)lf)[[j = j†j(25)
</equation>
<bodyText confidence="0.878571">
where
</bodyText>
<equation confidence="0.994634">
j† = argmaxqf(j) (26)
j
B(t) =
gj
</equation>
<page confidence="0.996425">
385
</page>
<table confidence="0.395692">
Computational Linguistics Volume 30, Number 3
</table>
<tableCaption confidence="0.822667">
Table 10
</tableCaption>
<figure confidence="0.934368692307692">
The sequential algorithm YS.
(0) Given: S(0), θ(0), with θ(0)
gj = 1/L for g E� S(0)
(1) Initialization
(1.1) Set S = S(0), θ = θ(0)
(1.2) For each example x E X
If x possesses a feature in S(0), set Yx = ˆy, else set Yx = L
(2) Loop:
(2.1) Choose a feature f E� S(0) such that Af =� 0 and θf =� qf
If there is none, stop
(2.2) Add f to S
(2.3) For each label j, set θfj = update(f, j)
(2.4) For each example x possessing a feature in S, set Yx = yˆ
</figure>
<bodyText confidence="0.965131">
Note that peaked precision involves a variable amount of “peaking”; the mixing pa-
rameters depend on the relative proportions of labeled and unlabeled examples. Note
also that j† is a function of f, though we do not explicitly represent that dependence.
The three instantiations of algorithm YS that we consider are
</bodyText>
<equation confidence="0.973423333333333">
YS-P (“peaked”) θfj = ˆqf (j)
YS-R (“raw”) θfj = qf (j)
YS-FS (“fixed smoothing”) θfj = ˜qf (j)
</equation>
<bodyText confidence="0.99996525">
We will show that the first two algorithms reduce K in each iteration. We will show
that the third algorithm, YS-FS, reduces K in iterations in which ft is a new feature,
not previously selected. Unfortunately, we are unable to show that YS-FS reduces K
when ft is a previously selected feature. This suggests employing a mixed algorithm
in which smoothed precision is used for new features but raw or peaked precision is
used for previously selected features.
A final issue with the algorithm schema YS concerns the selection of features
in step 2.1. The schema as stated does not specify which feature is to be selected.
In essence, the manner in which rules are selected does not matter, as long as one
selects rules that have room for improvement, in the sense that the current prediction
distribution θf differs from raw precision qf. (The justification for this choice is given
in Theorem 9.) The theorems in the following sections show that K decreases in each
iteration, so long as any such rule can be found.
One could choose greedily by choosing the feature that maximizes gain G (equa-
tion (27)), though in the next section we give lower bounds for G that are rather more
easily computed (Theorems 6 and 7).
</bodyText>
<subsectionHeader confidence="0.99225">
4.2 Gain
</subsectionHeader>
<bodyText confidence="0.999884714285714">
From this point on, we consider a single iteration of the YS algorithm and discard the
variable t. We write θold and φold for the parameter set and labeling at the beginning of
the iteration, and we write simply θ and φ for the new parameter set and new label-
ing. The set A (respectively, V) represents the examples that are labeled (respectively,
unlabeled) at the beginning of the iteration. The selected feature is f.
We wish to choose a prediction distribution for f so as to guarantee that K decreases
in each iteration. The gain in the current iteration is
</bodyText>
<equation confidence="0.945321">
G =E E rH(φxld||θ�ld) − H(φx||θg)] (27)
x∈X g∈Fx
</equation>
<page confidence="0.987182">
386
</page>
<bodyText confidence="0.8749945">
Abney Understanding the Yarowsky Algorithm
Gain is the negative change in K; it is positive when K decreases.
In considering the reduction in K from (φold, θold) to (φ, θ), it will be convenient to
consider the following intermediate values:
</bodyText>
<table confidence="0.483279375">
where �K0 = E H(φold
xEX gEFx x ||θold
�K1 = E g )
xEX gEFx H(ψx||θold
�K2 = E g )
xEX gEFx H(ψx||θg)
�K3 = E H(φx||θg)
xEX gEFx
</table>
<equation confidence="0.6165284">
[[j = j*]] if x E Vf
ψxj = I φol
xj
otherwise
and
θfj (28)
j* - arg max
j
One should note that
•
</equation>
<bodyText confidence="0.543638">
θf is the new prediction distribution for the candidate f; θgj = θold
gj for
</bodyText>
<listItem confidence="0.958502333333333">
g =�f.
• φ is the new label distribution, after relabeling. It is defined as
~ [j =ˆy(x)]] if x E Λ U Xf
</listItem>
<equation confidence="0.936548">
φxj = (29)
1 otherwise
L
</equation>
<listItem confidence="0.9964112">
• for x E Vf, the only selected feature at t + 1 is f, hence j* = yˆ for such
examples. It follows that ψ and φ agree on examples in Vf. They also
agree on examples that are unlabeled at t + 1, assigning them the
uniform label distribution. If ψ and φ differ, it is only on old labeled
examples (Λ) that need to be relabeled, given the addition of f.
</listItem>
<bodyText confidence="0.9732555">
The gain G can be represented as the sum of three intermediate gains, correspond-
ing to the intermediate values just defined:
</bodyText>
<equation confidence="0.8984352">
G = GV + Gθ + GΛ (30)
where
GV = K0 − K1
Gθ = K1 − K2
GΛ = K2 − K3
</equation>
<bodyText confidence="0.995819">
The gain GV intuitively represents the gain that is attributable to labeling previously
unlabeled examples in accordance with the predictions of θ. The gain Gθ represents the
gain that is attributable to changing the values θfj, where f is the selected feature. The
</bodyText>
<page confidence="0.983491">
387
</page>
<note confidence="0.464597">
Computational Linguistics Volume 30, Number 3
</note>
<bodyText confidence="0.951189888888889">
gain GΛ represents the gain that is attributable to changing the labels of previously
labeled examples to make labels agree with the predictions of the new model θ. The
gain Gθ corresponds to step 2.3 of algorithm YS, in which θ is changed but φ is held
constant; and the combined GV and GΛ gains correspond to step 2.4 of algorithm YS,
in which φ is changed while holding θ constant.
In the remainder of this section, we derive two lower bounds for G. In following
sections, we show that the updates YS-P, YS-R, and YS-FS guarantee that the lower
bounds given below are non-negative, and hence that G is non-negative.
Lemma 3
</bodyText>
<equation confidence="0.642752">
GV = 0
Proof
</equation>
<bodyText confidence="0.979705666666666">
We show that K remains unchanged if we substitute ψ for φold in K0. The only property
of ψ that we need is that it agrees with φold on previously labeled examples.
Since ψx = φold
x for x ∈ Λ, we need only consider examples in V. Since these
examples are unlabeled at the beginning of the iteration, none of their features have
been selected, hence θold = 1/L for all their features g. Hence
</bodyText>
<figure confidence="0.990192333333333">
gj
11 K1 = − 11 11 ψxj log θold
x∈V g∈Fx j gj
11
g∈Fx
11 = −
x∈V
� �
�11φold �log 1
xj L
j
� �
�11ψxj �log 1
L
j
x11
11
g∈Fx
11 = − 11 11 φold
x∈V g∈Fx j xj log θgj
old = K0
</figure>
<bodyText confidence="0.963218125">
(Note that ψxj is not in general equal to (hold, but ~j ψxj and ~j Old both equal 1.) This
&apos;Xjshows that K0 = K1, and hence that GV = 0.
Lemma 4
GΛ ≥ 0.
We must show that relabeling old labeled examples—that is, setting φx(j) = [[j = ˆyjfor x ∈ Λ—does not increase K. The proof has the same structure as the proof of
Theorem 1 and is omitted.
Lemma 5
Gθ is equal to
</bodyText>
<equation confidence="0.621155">
|Λf |
H(qf ||θfld) − H(qf  ||θf)~ + |Vf |~log L − log θf1∗ ~ (31)
</equation>
<bodyText confidence="0.6076595">
Proof
By definition, Gθ = K1−K2, and K1 and K2 are identical everywhere except on examples
</bodyText>
<page confidence="0.983871">
388
</page>
<bodyText confidence="0.4063985">
Abney Understanding the Yarowsky Algorithm
in Xf. Hence
</bodyText>
<equation confidence="0.85679">
EGθ = E [H(ψx ||θgld) − H(ψx ||θg)]
x∈Xf g∈Fx
</equation>
<bodyText confidence="0.810598">
We divide this sum into three partial sums:
</bodyText>
<equation confidence="0.906614857142857">
Gθ = A + B + C (32)
EA = [H(ψx||θyld) − H(ψx ||θf )]
x∈Af
EB = [H(ψx ||θfld) − H(ψx ||θf )]
x∈Vf
EC = E [H(ψx ||θgld) − H(ψx ||θg)]
x∈Xf g54f∈Fx
</equation>
<bodyText confidence="0.827071">
We consider each partial sum separately:
</bodyText>
<equation confidence="0.96192395">
EA = [H(ψx ||θfld) − H(ψx ||θf)]
x∈Af
E= − E ψxk [log θld − log θfk]
x∈Af k
E= − E [[x ∈ Ak]] [log θVd − logθ fk]
x∈Af k
E= − |Afk  |[log θId − logθ fk]
k
= −|Af  |E qf (k) [log IId− logθ fk]
k
= |Af |[H(qf ||θfld) − H(qf||θf)] (33)
EB = [H(ψx ||θfld) − H(ψx ||θf)]
x∈Vf
E= − E II — logo
x∈Vf k ψxk [log J ]
E= − E [[k = j∗]] [log Id− logθ fk]
x∈Vf k
Vf  |I log θad − log θf�∗ J
L fj∗
Vf  |I lfj
</equation>
<bodyText confidence="0.8998426">
og L − log θ1 ∗ J (34)
The justification for the last step is a bit subtle. If f is a new feature, not previously
selected, then θold
fk = 1/L for all k, and the substitution is valid. On the other hand, if
f is a previously selected feature, then |Vf  |= 0, and even though the substitution of
</bodyText>
<page confidence="0.989975">
389
</page>
<figure confidence="0.890475520833334">
Computational Linguistics Volume 30, Number 3
1/L for θold
fj∗ may not be valid, it is innocuous.
EC = E [H(ψx ||θgold) − H(ψx||θg)]
x∈Xf g54f∈Fx [H(ψx||θgld) − H(ψx ||θgld)]
E= E
x∈Xf g54f∈Fx
= 0 (35)
Combining (32), (33), (34), and (35) yields the lemma.
Theorem 6
G is bounded below by (31).
Proof
Combining (30) with Lemmas 3, 4, and 5.
Theorem 7
G is bounded below by
|Af |[H(qf  ||θfld) − H(qf ||θf)]
Proof
The theorem follows immediately from Theorem 6 if we can show that
logL − log θf1∗ &gt; 0
Observe first that logL = H(u). (Recall that u(j) = 1/L is the uniform distribution over
labels.) By Lemma 1, we know that
H(u) − log θf1∗ &gt; H(u) − H(θf)
&gt; 0
The latter follows because the uniform distribution maximizes entropy.
Theorem 8
G is bounded below by
|Af |[D(qf ||θfld) − D(qf||θf)]
Proof
Immediate from Theorem 7 and the fact that
H(qf||θold
f ) − H(qf ||θf) = H(qf) + D(qf||θold
f ) − H(qf) − D(qf||θf)
= D(qf||θold
f ) − D(qf||θf)
Theorem 9
If θold
f =� qf, then there is a choice of θf that yields strictly positive gain.
390
Abney Understanding the Yarowsky Algorithm
Proof
If θold
f =� qf, then
D(qf ||θold
f ) &gt; 0
Setting θf = qf has the result that
|Λf |[D(qf||θfld) − D(qf||θf)] = |Λf|D(qf||θold
f ) &gt; 0
Hence G &gt; 0 by Theorem 8.
</figure>
<subsectionHeader confidence="0.986707">
4.3 Algorithm YS-P
</subsectionHeader>
<bodyText confidence="0.999919">
We now use the results of the previous section to show that the algorithm YS-P is
correct in the sense that it reduces K in every iteration.
</bodyText>
<subsectionHeader confidence="0.350674">
Theorem 10
</subsectionHeader>
<bodyText confidence="0.942526">
In each iteration of algorithm YS-P, K decreases.
</bodyText>
<subsectionHeader confidence="0.456503">
Proof
</subsectionHeader>
<bodyText confidence="0.997762666666667">
We wish to show that G &gt; 0. By Theorem 6, that is true if expression (31) is positive.
By Theorem 9, there exist choices for θf that make (31) positive, hence in particular,
we guarantee G &gt; 0 by maximizing (31). We maximize (31) by minimizing
</bodyText>
<equation confidence="0.982734571428572">
1
|Λf |H(qf ||θf) + |Vf  |log (36)
θfj*
Since
H(qf ||θf) = H(qf) + D(qf ||θf)
we minimize (36) by minimizing
|Λf|D(qf||θf) + |Vf |log θ1*(37)
</equation>
<bodyText confidence="0.987209375">
Both terms are nonnegative. The first term is zero if θf = qf. The second term is zero
for any distribution that concentrates all its mass in a single label j*; it is symmetric
in all choices of j* and decreases monotonically as θfj* approaches one. Hence, the
minimum of (37) will have j* equal to the mode of qf, though it may be more peaked
than qf, at the cost of an increase in the first term, but offset by a decrease in the
second term.
Recall that j† = argmaxj qf (j). By the reasoning of the previous paragraph, we
know that j† = j* at the minimum of (37). Hence we can minimize (37) by minimizing
</bodyText>
<equation confidence="0.802365">
|Λf |D(qf ||θf ) − |Vf  |E [[k = j†]] log θfk (38)
k
</equation>
<bodyText confidence="0.968302">
We compute the gradient: 1
</bodyText>
<equation confidence="0.8547905">
∂θfj [|Λf|D(qf||θf) − |Vf |E[[k = j†]]logθfkJ k
∂θfj [|Λf|H(qf||θf) − |Λf|H(qf) − |Vf|E[[k = j†]]logθfkl k
</equation>
<page confidence="0.979884">
391
</page>
<subsectionHeader confidence="0.218315">
Computational Linguistics Volume 30, Number 3
</subsectionHeader>
<bodyText confidence="0.4992255">
as |Λf|H(qf||θf) −∂θfj|Vf |EQk=l†]]logθfk
= −  |Λf  |aafj E qf (k) log θfk −  |Vf  |aafj E [[k = j†]] log θfk
</bodyText>
<equation confidence="0.9544134">
k k
= −|Λf  |∂ qf (j) log θfj − |Vf  |∂ [[j = j†]] log θfj
∂θfj ∂θfj
= −|Λf|qf(j) 1 − |Vf|[[j = j†]] 1 (39)
θfj θfj
</equation>
<bodyText confidence="0.9999445">
As before, the derivative of the constraint Cf = 0 is one, and we minimize (38) under
the constraint by solving
</bodyText>
<equation confidence="0.998489">
−|Λf|qf(j) 1 − |Vf|[[j = j†]] 1 = λ
θfj θfj
θfj = (−|Λf|qf(j) − |Vf|[[j = j†]]) /λ (40)
</equation>
<bodyText confidence="0.561417">
Substituting into the constraint gives us
</bodyText>
<equation confidence="0.99837825">
E (−|Λf|qf(j) − |Vf|[[j = j†]]) /λ = 1
j
−|Λf |− |Vf |= λ
−|Xf |= λ
</equation>
<bodyText confidence="0.85149">
Substituting this back into (40) yields:
</bodyText>
<equation confidence="0.977986">
θfj = p(Λ|f)qf(j) + p(V|f)[[j = j†]] (41)
</equation>
<bodyText confidence="0.995055">
That is, the maximizing solution is peaked precision, which is the update rule for YS-P.
</bodyText>
<subsectionHeader confidence="0.917383">
4.4 Algorithm YS-R
</subsectionHeader>
<bodyText confidence="0.9995855">
We now show that YS-R also decreases K in each iteration. In fact, it has essentially
already been proven.
</bodyText>
<subsectionHeader confidence="0.462615">
Theorem 11
</subsectionHeader>
<bodyText confidence="0.787227">
Algorithm YS-R decreases K in each iteration.
</bodyText>
<subsectionHeader confidence="0.578416">
Proof
</subsectionHeader>
<bodyText confidence="0.872320666666667">
In the proof of Theorem 9, we showed that the choice
θf = qf
yields strictly positive gain. This is the update rule used by YS-R.
</bodyText>
<subsectionHeader confidence="0.580579">
4.5 Algorithm YS-FS
</subsectionHeader>
<bodyText confidence="0.99790725">
The original Yarowsky algorithm YS-0/DL-0 used smoothed precision with fixed c
as update rule. We have been unsuccessful at justifying this choice of update rule
in general. However, we are able at least to show that it does decrease K when the
selected feature is a new feature, not previously selected.
</bodyText>
<page confidence="0.9793">
392
</page>
<figure confidence="0.5194875">
Abney Understanding the Yarowsky Algorithm
Theorem 12
Algorithm YS-FS has positive gain in each iteration in which the selected feature has
not been previously selected.
Proof
By Theorem 7, gain is positive if
H(qf ||θold
f ) &gt; H(qf ||θf ) (42)
</figure>
<bodyText confidence="0.983031333333333">
By the assumption that the selected feature f has not been previously selected, θold
f is
the uniform distribution u, and the left-hand side of (42) is equal to H(qf ||u). It is easy
to verify that H(p||u) = H(u) for any distribution p; hence the left-hand side of (42) is
equal to H(u). Further, YS-FS uses smoothed precision as update rule, θf = ˜qf, so (42)
can be rewritten as
</bodyText>
<equation confidence="0.841698">
H(u) &gt; H(qf ||˜qf)
</equation>
<bodyText confidence="0.999971333333333">
This condition does not hold trivially, inasmuch as cross entropy, like divergence, is
unbounded. But we can show that it holds in this particular case.
We derive an upper bound for H(qf ||˜qf):
</bodyText>
<equation confidence="0.783285">
H(qf ||˜qf) = − � qf (j) log ˜qf (j)
j
</equation>
<bodyText confidence="0.9577618">
~= − qf(j)log 11 1 qf (j) 1 + Leu(j)]
j
� � 1 �
≤ − qf (j) 1 + L log qf (j) + L
j 1 + L log u(j)
</bodyText>
<equation confidence="0.94677625">
1 L
= H(qf) + H(qf ||u)
1 + L 1 + L
1 L
= H(qf) + H(u) (43)
1 + L 1 + L�Lc
Observe that
H(u) &gt; H(qf) + H(u) (44)
1 + L 1 + L
� �
1 − L H(u) &gt;
1 + L
1 H(qf)
1+Le
iff
H(u) &gt; H(qf)
</equation>
<bodyText confidence="0.925731222222222">
We know that H(u) ≥ H(qf) because the uniform distribution maximizes entropy. We
know that the inequality is strict by the following reasoning. Since f is a new feature,
θold
f = u. Because of the restriction on step 2.1 in algorithm YS, θold
f =� qf, hence qf =~ u,
and H(u) is strictly greater than H(qf).
Hence (44) is true, and combining (44) with (43), we have shown (42) to be true,
proving the theorem.
iff
</bodyText>
<page confidence="0.993492">
393
</page>
<figure confidence="0.356241">
Computational Linguistics Volume 30, Number 3
</figure>
<sectionHeader confidence="0.772024" genericHeader="method">
5. Minimization of Feature Entropy
</sectionHeader>
<bodyText confidence="0.999814692307692">
At the beginning of the article, the co-training algorithm was mentioned as an alterna-
tive to the Yarowsky algorithm. There is in fact a connection between co-training and
the Yarowsky algorithm. In the original co-training paper (Blum and Mitchell 1998), it
was suggested that the algorithm be understood as seeking to maximize agreement on
unlabeled data between classifiers trained on two different “views” of the data. Subse-
quent work (Dasgupta, Littman, and McAllester 2001) has proven a direct connection
between classifier error and such cross-view agreement on unlabeled data.
In the current context, there is also justification for pursuing agreement on unla-
beled data. However, the Yarowsky algorithm does not assume the existence of two
conditionally independent views of the data. Rather, there is a motivation for seeking
agreement on unlabeled data between arbitrary pairs of features.
Recall that our original objective function, H, can be expressed as the sum of an
entropy term and a divergence term:
</bodyText>
<equation confidence="0.9869175">
H =E [H(Ox) + D(Ox||7rx)
x∈X
</equation>
<bodyText confidence="0.9940528">
As D(Ox||7rx) becomes small and H(7rx) becomes small, H(Ox) necessarily also becomes
small; hence we can limit H by limiting H(7rx) and D(Ox||7rx). Intuitively, we wish to
reduce the uncertainty of the model’s predictions, while also improving the fit between
the model’s predictions and the known labels.
Let us focus now on the uncertainty of the model’s predictions:
</bodyText>
<equation confidence="0.8783116">
�H(7rx) = − 7rx(j) log 7rx(j)
j
�= − � �
j �E1
7rx(j) log mBgj �
</equation>
<table confidence="0.803134416666667">
g∈Fx
E E7rx(j) 1 log Bgj
≤ − g∈Fx m
j
�= − � �
j �� 1 �E 1
mBfj m log Bgj
f∈Fx g∈Fx
1 � m2 � � Bfj log Bgj
f ∈Fx g∈Fx j
= 1 � m2 � H(Bf||Bg)
f ∈Fx g∈Fx
</table>
<figure confidence="0.909432">
1 � � [H(Bf) + D(Bf ||Bg)
= g∈Fx
m2
f ∈Fx
= 1EmE 1 � � D(Bf ||Bg) (45)
f ∈Fx H(Bf) + m2 g∈Fx
f ∈Fx
</figure>
<bodyText confidence="0.992122666666667">
In other words, by decreasing the uncertainty of the prediction distributions of indi-
vidual features and simultaneously increasing the agreement among features (that is,
decreasing their pairwise divergence), we decrease an upper bound on H(7rx). This
</bodyText>
<page confidence="0.969208">
394
</page>
<bodyText confidence="0.918773">
Abney Understanding the Yarowsky Algorithm
motivates interfeature agreement without recourse to an assumption of independent
views.
</bodyText>
<sectionHeader confidence="0.997805" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.999976888888889">
In this article, we have presented a number of variants of the Yarowsky algorithm,
and we have shown that they optimize natural objective functions. We considered
first the modified generic Yarowsky algorithm Y-1 and showed that it minimizes the
objective function H (which is equivalent to maximizing likelihood), provided that its
base learner reduces H.
We then considered three families of specific Yarowsky-like algorithms. The
Y-1/DL-EM algorithms (Y-1/DL-EM-Λ and Y-1/DL-EM-X) minimize H but have the
disadvantage that the DL-EM base learner has no similarity to Yarowsky’s original base
learner. A much better approximation to Yarowsky’s original base learner is provided
by DL-1, and the Y-1/DL-1 algorithms (Y-1/DL-1-R and Y-1/DL-1-VS) were shown to
minimize the objective function K, an upper bound for H. Finally, the YS algorithms
(YS-P, YS-R, and YS-FS) are sequential variants, reminiscent of the Yarowsky-Cautious
algorithm of Collins and Singer; we showed that they minimize K.
To the extent that these algorithms capture the essence of the original Yarowsky
algorithm, they provide a formal understanding of Yarowsky’s approach. Even if they
are deemed to diverge too much from the original to cast light on its workings, they
at least represent a new family of bootstrapping algorithms with solid mathematical
foundations.
</bodyText>
<sectionHeader confidence="0.994926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999861464285714">
Abney, Steven. 2002. Bootstrapping. In
Proceedings of 40th Annual Meeting of the
Association for Computational Linguistics
(ACL), Philadelphia, pages 360–367.
Blum, Avrim and Tom Mitchell. 1998.
Combining labeled and unlabeled data
with co-training. In Proceedings of the 11th
Annual Conference on Computational
Learning Theory (COLT), pages 92–100.
Morgan Kaufmann, San Francisco.
Collins, Michael and Yoram Singer. 1999.
Unsupervised models for named entity
classification. In Proceedings of Empirical
Methods in Natural Language Processing
(EMNLP), College Park, MD,
pages 100–110.
Dasgupta, Sanjoy, Michael Littman, and
David McAllester. 2001. PAC
generalization bounds for co-training. In
Proceedings of Advances in Neural
Information Processing Systems 14 (NIPS),
Vancouver, British Columbia, Canada.
Yarowsky, David. 1995. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual
Meeting of the Association for Computational
Linguistics, Cambridge, MA, pages
189–196.
</reference>
<page confidence="0.999118">
395
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.670567">
<title confidence="0.902252">Understanding the Yarowsky Algorithm</title>
<affiliation confidence="0.97055">University of Michigan</affiliation>
<abstract confidence="0.9508742">Many problems in computational linguistics are well suited for bootstrapping (semisupervised learning) techniques. The Yarowsky algorithm is a well-known bootstrapping algorithm, but it is not mathematically well understood. This article analyzes it as optimizing an objective function. More specifically, a number of variants of the Yarowsky algorithm (though not the original algorithm itself) are shown to optimize either likelihood or a closely related objective</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<date>2002</date>
<booktitle>Bootstrapping. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>360--367</pages>
<location>Philadelphia,</location>
<contexts>
<context position="2663" citStr="Abney 2002" startWordPosition="404" endWordPosition="406">le aims to rectify this lack of understanding, increasing the attractiveness of the Yarowsky algorithm as an alternative to co-training. The Yarowsky algorithm does have the advantage of placing less of a restriction on the data sets it can be applied to. Co-training requires data attributes to be separable into two views that are conditionally independent given the target label; the Yarowsky algorithm makes no such assumption about its data. In previous work, I did propose an assumption about the data called precision independence, under which the Yarowsky algorithm could be shown effective (Abney 2002). That assumption is ultimately unsatisfactory, however, not only because it ∗ 4080 Frieze Bldg., 105 S. State Street, Ann Arbor, MI 48109-1285. E-mail: abney.umich.edu. Submission received: 26 August 2003; Revised submission received: 21 December 2003; Accepted for publication: 10 February 2004 © 2004 Association for Computational Linguistics Computational Linguistics Volume 30, Number 3 Table 1 The Yarowsky algorithm variants. Y-1/DL-EM reduces H; the others reduce K. Y-1/DL-EM-Λ EM inner loop that uses labeled examples only Y-1/DL-EM-X EM inner loop that uses all examples Y-1/DL-1-R Near-or</context>
</contexts>
<marker>Abney, 2002</marker>
<rawString>Abney, Steven. 2002. Bootstrapping. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, pages 360–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT),</booktitle>
<pages>92--100</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="1793" citStr="Blum and Mitchell 1998" startWordPosition="268" endWordPosition="271">onsiders rules of the form “If instance x contains feature f, then predict label j” and selects those rules whose precision on the training data is highest. The “outer loop” is given a seed set of rules to start with. In each iteration, it uses the current set of rules to assign labels to unlabeled data. It selects those instances regarding which the base learner’s predictions are most confident and constructs a labeled training set from them. It then calls the inner loop to construct a new classifier (that is, a new set of rules), and the cycle repeats. An alternative algorithm, co-training (Blum and Mitchell 1998), has subsequently become more popular, perhaps in part because it has proven amenable to theoretical analysis (Dasgupta, Littman, and McAllester 2001), in contrast to the Yarowsky algorithm, which is as yet mathematically poorly understood. The current article aims to rectify this lack of understanding, increasing the attractiveness of the Yarowsky algorithm as an alternative to co-training. The Yarowsky algorithm does have the advantage of placing less of a restriction on the data sets it can be applied to. Co-training requires data attributes to be separable into two views that are conditio</context>
<context position="60607" citStr="Blum and Mitchell 1998" startWordPosition="11208" endWordPosition="11211">ng reasoning. Since f is a new feature, θold f = u. Because of the restriction on step 2.1 in algorithm YS, θold f =� qf, hence qf =~ u, and H(u) is strictly greater than H(qf). Hence (44) is true, and combining (44) with (43), we have shown (42) to be true, proving the theorem. iff 393 Computational Linguistics Volume 30, Number 3 5. Minimization of Feature Entropy At the beginning of the article, the co-training algorithm was mentioned as an alternative to the Yarowsky algorithm. There is in fact a connection between co-training and the Yarowsky algorithm. In the original co-training paper (Blum and Mitchell 1998), it was suggested that the algorithm be understood as seeking to maximize agreement on unlabeled data between classifiers trained on two different “views” of the data. Subsequent work (Dasgupta, Littman, and McAllester 2001) has proven a direct connection between classifier error and such cross-view agreement on unlabeled data. In the current context, there is also justification for pursuing agreement on unlabeled data. However, the Yarowsky algorithm does not assume the existence of two conditionally independent views of the data. Rather, there is a motivation for seeking agreement on unlabe</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Blum, Avrim and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT), pages 92–100. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), College Park, MD,</booktitle>
<pages>100--110</pages>
<contexts>
<context position="44796" citStr="Collins &amp; Singer (1999)" startWordPosition="8157" endWordPosition="8160">rease K at each iteration until they reach a critical point. Proof Immediate from Theorems 3 and 4. 4. Sequential Algorithms 4.1 The Family YS The Yarowsky algorithm variants we have considered up to now do “parallel” updates in the sense that the parameters {θfj} are completely recomputed at each iteration. In = |Λf| |Xf |· θfj 384 Abney Understanding the Yarowsky Algorithm this section, we consider a family YS of “sequential” variants of the Yarowsky algorithm, in which a single feature is selected for update at each iteration. The YS algorithms resemble the “Yarowsky-Cautious” algorithm of Collins &amp; Singer (1999), though they differ from that algorithm in that they update a single feature in each iteration, rather than a small set of features, as in Yarowsky-Cautious. The YS algorithms are intended to be as close to the Y-1/DL-1 algorithm as is consonant with singlefeature updates. The YS algorithms differ from one another, and from Y-1/DL-1, in the choice of update rule. An interesting range of update rules work in the sequential setting. In particular, smoothed precision with fixed e, as in the original algorithm Y-0/DL-0, works in the sequential setting, though with a proviso that will be spelled o</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Collins, Michael and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), College Park, MD, pages 100–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjoy Dasgupta</author>
<author>Michael Littman</author>
<author>David McAllester</author>
</authors>
<title>PAC generalization bounds for co-training.</title>
<date>2001</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems 14 (NIPS),</booktitle>
<location>Vancouver, British Columbia, Canada.</location>
<marker>Dasgupta, Littman, McAllester, 2001</marker>
<rawString>Dasgupta, Sanjoy, Michael Littman, and David McAllester. 2001. PAC generalization bounds for co-training. In Proceedings of Advances in Neural Information Processing Systems 14 (NIPS), Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="890" citStr="Yarowsky (1995)" startWordPosition="119" endWordPosition="120">ically well understood. This article analyzes it as optimizing an objective function. More specifically, a number of variants of the Yarowsky algorithm (though not the original algorithm itself) are shown to optimize either likelihood or a closely related objective function K. 1. Introduction Bootstrapping, or semisupervised learning, has become an important topic in computational linguistics. For many language-processing tasks, there are an abundance of unlabeled data, but labeled data are lacking and too expensive to create in large quantities, making bootstrapping techniques desirable. The Yarowsky (1995) algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics. In brief, it consists of two loops. The “inner loop” or base learner is a supervised learning algorithm. Specifically, Yarowsky uses a simple decision list learner that considers rules of the form “If instance x contains feature f, then predict label j” and selects those rules whose precision on the training data is highest. The “outer loop” is given a seed set of rules to start with. In each iteration, it uses the current set of rules to assign labels to unlabeled data. It selects tho</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, David. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, Cambridge, MA, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>