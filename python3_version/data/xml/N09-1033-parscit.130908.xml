<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001609">
<title confidence="0.975181">
Semi-Automatic Entity Set Refinement
</title>
<author confidence="0.832103">
Vishnu Vyas and Patrick Pantel
</author>
<affiliation confidence="0.750349">
Yahoo! Labs
</affiliation>
<address confidence="0.895445">
Santa Clara, CA 95054
</address>
<email confidence="0.91439">
{vishnu,ppantel}@yahoo-inc.com
</email>
<sectionHeader confidence="0.998166" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999501">
State of the art set expansion algorithms pro-
duce varying quality expansions for different
entity types. Even for the highest quality ex-
pansions, errors still occur and manual re-
finements are necessary for most practical
uses. In this paper, we propose algorithms to
aide this refinement process, greatly reducing
the amount of manual labor required. The me-
thods rely on the fact that most expansion er-
rors are systematic, often stemming from the
fact that some seed elements are ambiguous.
Using our methods, empirical evidence shows
that average R-precision over random entity
sets improves by 26% to 51% when given
from 5 to 10 manually tagged errors. Both
proposed refinement models have linear time
complexity in set size allowing for practical
online use in set expansion systems.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991812258064516">
Sets of named entities are extremely useful in a
variety of natural language and information re-
trieval tasks. For example, companies such as Ya-
hoo! and Google maintain sets of named entities
such as cities, products and celebrities to improve
search engine relevance.
Manually creating and maintaining large sets of
named entities is expensive and laborious. In re-
sponse, many automatic and semi-automatic me-
thods of creating sets of named entities have been
proposed, some are supervised (Zhou and Su,
2001), unsupervised (Pantel and Lin 2002, Nadeau
et al. 2006), and others semi-supervised (Kozareva
et al. 2008). Semi-supervised approaches are often
used in practice since they allow for targeting spe-
cific entity classes such as European Cities and
French Impressionist Painters. Methods differ in
complexity from simple ones using lexico-
syntactic patterns (Hearst 1992) to more compli-
cated techniques based on distributional similarity
(Paşca 2007a).
Even for state of the art methods, expansion er-
rors inevitably occur and manual refinements are
necessary for most practical uses requiring high
precision (such as for query interpretation at com-
mercial search engines). Looking at expansions
from state of the art systems such as GoogleSets1 ,
we found systematic errors such as those resulting
from ambiguous seed instances. For example, con-
sider the following seed instances for the target set
Roman Gods:
</bodyText>
<keyword confidence="0.4840845">
Minerva, Neptune, Baccus, Juno,
Apollo
</keyword>
<bodyText confidence="0.997473166666667">
GoogleSet’s expansion as well others employing
distributional expansion techniques consists of a
mishmash of Roman Gods and celestial bodies,
originating most likely from the fact that Neptune
is both a Roman God and a Planet. Below is an
excerpt of the GoogleSet expansion:
</bodyText>
<construct confidence="0.425567333333333">
Mars, Venus, *Moon, Mercury,
*asteroid, Jupiter, *Earth,
*comet, *Sonne, *Sun, ...
</construct>
<bodyText confidence="0.99959825">
The inherent semantic similarity between the errors
can be leveraged to quickly clean up the expan-
sion. For example, given a manually tagged error
“asteroid”, a distributional similarity thesaurus
</bodyText>
<footnote confidence="0.977688">
1 http://labs.google.com/sets
</footnote>
<page confidence="0.899594">
290
</page>
<note confidence="0.8954815">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 290–298,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999906264705882">
such as (Lin 1998)2 can identify comet as similar to
asteroid and therefore potentially also as an error.
This method has its limitations since a manually
tagged error such as Earth would correctly remove
Moon and Sun, but it would also incorrectly re-
move Mars, Venus and Jupiter since they are also
similar to Earth3.
In this paper, we propose two algorithms to im-
prove the precision of automatically expanded enti-
ty sets by using minimal human negative
judgments. The algorithms leverage the fact that
set expansion errors are systematically caused by
ambiguous seed instances which attract incorrect
instances of an unintended entity type. We use dis-
tributional similarity and sense feature modeling to
identify such unintended entity types in order to
quickly clean up errors with minimal manual labor.
We show empirical evidence that average R-
precision over random entity sets improves by 26%
to 51% when given from 5 to 10 manually tagged
errors. Both proposed refinement models have li-
near time complexity in set size allowing for prac-
tical online use in set expansion systems.
The remainder of this paper is organized as fol-
lows. In the next section we review related work
and position our contribution within its landscape.
Section 3 presents our task of dynamically model-
ing the similarity of a set of words and describes
algorithms for refining sets of named entities. The
datasets and our evaluation methodology used to
perform our experiments are presented in Section 4
and in Section 5 we describe experimental results.
Finally, we conclude with some discussion and
future work.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999642111111111">
There is a large body of work for automatically
building sets of named entities using various tech-
niques including supervised, unsupervised and
semi-supervised methods. Supervised techniques
use large amounts of training data to detect and
classify entities into coarse grained classes such as
People, Organizations, and Places (Bunescu and
Mooney 2004; Etzioni et al. 2005). On the other
hand, unsupervised methods require no training
</bodyText>
<footnote confidence="0.9982412">
2 See http://demo.patrickpantel.com/ for a demonstration of
the distributional thesaurus.
3 In practice, this problem is rare since most terms that are
similar in one of their senses tend not to be similar in their
other senses.
</footnote>
<bodyText confidence="0.999944588235294">
data and rely on approaches such as clustering,
targeted patterns and co-occurrences to extract sets
of entities (Pantel and Lin 2002; Downey et al.
2007).
Semi-supervised approaches are often used in
practice since they allow for targeting specific enti-
ty classes. These methods rely on a small set of
seed examples to extract sets of entities. They ei-
ther are based on distributional approaches or em-
ploy lexico-syntactic patterns to expand a small set
of seeds to a larger set of candidate expansions.
Some methods such as (Riloff and Shepherd 1997;
Riloff and Jones 1999; Banko et al. 2007;Paşca
2007a) use lexico-syntactic patterns to expand a
set of seeds from web text and query logs. Others
such as (Paşca et al. 2006; Paşca 2007b; Paşca and
Durme 2008) use distributional approaches. Wang
and Cohen (2007) use structural cues in semi-
structured text to expand sets of seed elements. In
all methods however, expansion errors inevitably
occur. This paper focuses on the task of post
processing any such system’s expansion output
using minimal human judgments in order to re-
move expansion errors.
Using user feedback to improve a system’s per-
formance is a common theme within many infor-
mation retrieval and machine learning tasks. One
form of user feedback is active learning (Cohn et
al. 1994), where one or more classifiers are used to
focus human annotation efforts on the most benefi-
cial test cases. Active learning has been successful-
ly applied to various natural language tasks such as
parsing (Tang et al. 2001), POS tagging (Dagan
and Engelson 1995) and providing large amounts
of annotations for common natural language
processing tasks such as word sense disambigua-
tion (Banko and Brill 2001). Relevance feedback is
another popular feedback paradigm commonly
used in information retrieval (Harman 1992),
where user feedback (either explicit or implicit) is
used to refine the search results of an IR system.
Relevance feedback has been successfully applied
to many IR applications including content-based
image retrieval (Zhouand Huang 2003) and web
search (Vishwa et al. 2005). Within NLP applica-
tions relevance feedback has also been used to
generate sense tagged examples for WSD tasks
(Stevenson et al. 2008), and Question Answering
(Negri 2004). Our methods use relevance feedback
in the form of negative examples to refine the re-
sults of a set expansion system.
</bodyText>
<page confidence="0.998302">
291
</page>
<sectionHeader confidence="0.997154" genericHeader="method">
3 Dynamic Similarity Modeling
</sectionHeader>
<bodyText confidence="0.999863588235294">
The set expansion algorithms discussed in Section
2 often produce high quality entity sets, however
inevitably errors are introduced. Applications re-
quiring high precision sets must invest significant-
ly in editorial efforts to clean up the sets. Although
companies like Yahoo! and Google can afford to
routinely support such manual labor, there is a
large opportunity to reduce the refinement cost
(i.e., number of required human judgments).
Recall the set expansion example of Roman
Gods from Section 1. Key to our approach is the
hypothesis that most expansion errors result from
some systematic cause. Manual inspection of ex-
pansions from GoogleSets and distributional set
expansion techniques revealed that most errors are
due to the inherent ambiguity of seed terms (such
as Neptune in our example) and data sparseness
(such as Sonne in our example, a very rare term).
The former kind of error is systematic and can be
leveraged by an automatic method by assuming
that any entity semantically similar to an identified
error will also be erroneous.
In this section, we propose two methods for le-
veraging this hypothesis. In the first method, de-
scribed in Section 3.1, we use a simple
distributional thesaurus and remove all entities
which are distributionally similar to manually iden-
tified errors. In the second method, described in
Section 3.2, we model the semantics of the seeds
using distributional features and then dynamically
change the feature space according to the manually
identified errors and rerank the entities in the set.
Both methods rely on the following two observa-
tions:
</bodyText>
<listItem confidence="0.803369785714286">
a) Many expansion errors are systematically
caused by ambiguous seed examples which
draw in several incorrect entities of its unin-
tended senses (such as seed Neptune in our
Roman Gods example which drew in celestial
bodies such as Earth and Sun);
b) Entities which are similar in one sense are
usually not similar in their other senses. For
example, Apple and Sun are similar in their
Company sense but their other senses (Fruit
and Celestial Body) are not similar. Our exam-
ple in Section 1 illustrates a rare counterexam-
ple where Neptune and Mercury are similar in
both their Planets and Roman Gods senses.
</listItem>
<bodyText confidence="0.9913133125">
Task Outline: Our task is to remove errors from
entity sets by using a minimal amount of manual
judgments. Incorporating feedback into this
process can be done in multiple ways. The most
flexible system would allow a judge to iteratively
remove as many errors as desired and then have
the system automatically remove other errors in
each iteration. Because it is intractable to test arbi-
trary numbers of manually identified errors in each
iteration, we constrain the judge to identify at most
one error in each iteration.
Although this paper focuses solely on removing
errors in an entity set, it is also possible to improve
expanded sets by using feedback to add new ele-
ments to the sets. We consider this task out of
scope for this paper.
</bodyText>
<subsectionHeader confidence="0.997954">
3.1 Similarity Method (SIM)
</subsectionHeader>
<bodyText confidence="0.9990995">
Our first method directly models observation a) in
the previous section. Following Lin (1998), we
model the similarity between entities using the dis-
tributional hypothesis, which states that similar
terms tend to occur in similar contexts (Harris
1985). A semantic model can be obtained by re-
cording the surrounding contexts for each term in a
large collection of unstructured text. Methods dif-
fer in their definition of a context (e.g., text win-
dow or syntactic relations), or a means to weigh
contexts (e.g., frequency, tf-idf, pointwise mutual
information), or ultimately in measuring the simi-
larity between two context vectors (e.g., using Euc-
lidean distance, Cosine, Dice). In this paper, we
use a text window of size 1, we weigh our contexts
using pointwise mutual information, and we use
the cosine score to compute the similarity between
context vectors (i.e., terms). Section 5.1 describes
our source corpus and extraction details. Compu-
ting the full similarity matrix for many terms over
a very large corpus is computationally intensive.
Our specific implementation follows the one pre-
sented in (Bayardo et al. 2007).
The similarity matrix computed above is then
directly used to refine entity sets. Given a manual-
ly identified error at each iteration, we automatical-
ly remove each entity in the set that is found to be
semantically similar to the error. The similarity
threshold was determined by manual inspection
and is reported in Section 5.1.
Due to observation b) in the previous section,
we expect that this method will perform poorly on
</bodyText>
<page confidence="0.981697">
292
</page>
<bodyText confidence="0.9999332">
entity sets such as the one presented in our exam-
ple of Section 1 where the manual removal of
Earth would likely remove correct entities such as
Mars, Venus and Jupiter. The method presented in
the next section attempts to alleviate this problem.
</bodyText>
<subsectionHeader confidence="0.997703">
3.2 Feature Modification Method (FMM)
</subsectionHeader>
<bodyText confidence="0.966760341463415">
Under the distributional hypothesis, the semantics
of a term are captured by the contexts in which it
occurs. The Feature Modification Method (FMM),
in short, attempts to automatically discover the
incorrect contexts of the unintended senses of seed
elements and then filters out expanded terms
whose contexts do not overlap with the other con-
texts of the seed elements.
Consider the set of seed terms S and an errone-
ous expanded instance e. In the SIM method of
Section 3.1 all set elements that have a feature vec-
tor (i.e., context vector) similar to e are removed.
The Feature Modification Method (FMM) instead
tries to identify the subset of features of the error e
which represent the unintended sense of the seed
terms S. For example, let S = {Minerva, Neptune,
Baccus, Juno, Apollo}. Looking at the contexts of
these words in a large corpus, we construct a cen-
troid context vector for S by taking a weighted av-
erage of the contexts of the seeds in S. In
Wikipedia articles we see contexts (i.e., features)
such as4:
attack, kill, *planet, destroy,
Goddess, *observe, statue, *launch,
Rome, *orbit, ...
Given an erroneous expansion such as e = Earth,
we postulate that removing the intersecting fea-
tures from Earth’s feature vector and the above
feature vector will remove the unintended Planet
sense of the seed set caused by the seed element
Neptune. The intersecting features that are re-
moved are bolded in the above feature vector for S.
The similarity between this modified feature vector
for S and all entities in the expansion set can be
recomputed as described in Section 3.1. Entities
with a low similarity score are removed from the
expanded set since they are assumed to be part of
the unintended semantic class (Planet in this ex-
ample).
Unlike the SIM method from Section 3.1, this
method is more stable with respect to observation
</bodyText>
<footnote confidence="0.8334715">
4 The full feature vector for these and all other terms in Wiki-
pedia can be found at http://demo.patrickpantel.com/..
</footnote>
<bodyText confidence="0.999062565217392">
b) in Section 3. We showed that SIM would incor-
rectly remove expansions such as Mars, Venus and
Jupiter given the erroneous expansion Earth. The
FMM method would instead remove the Planet
features from the seed feature vectors and the re-
maining features would still overlap with Mars,
Venus and Jupiter’s Roman God sense.
Efficiency: FMM requires online similarity com-
putations between centroid vectors and all ele-
ments of the expanded set. For large corpora such
as Wikipedia articles or the Web, feature vectors
are large and storing them in memory and perform-
ing similarity computations repeatedly for each
editorial judgment is computationally intensive.
For example, the size of the feature vector for a
single word extracted from Wikipedia can be in the
order of a few gigabytes. Storing the feature vec-
tors for all candidate expansions and the seed set is
inefficient and too slow for an interactive system.
The next section proposes a solution that makes
this computation very fast, requires little memory,
and produces near perfect approximations of the
similarity scores.
</bodyText>
<subsectionHeader confidence="0.999245">
3.3 Approximating Cosine Similarity
</subsectionHeader>
<bodyText confidence="0.99988988">
There are engineering optimizations that are avail-
able that allow us to perform a near perfect approx-
imation of the similarity computation from the
previous section. The proposed method requires us
to only store the shared features between the cen-
troid and the words rather than the complete fea-
ture vectors, thus reducing our space requirements
dramatically. Also, FMM requires us to repeatedly
calculate the cosine similarity between a modified
centroid feature vector and each candidate expan-
sion at each iteration. Without the full context vec-
tors of all candidate expansions, computing the
exact cosine similarity is impossible. Given, how-
ever, the original cosine scores between the seed
elements and the candidate expansions before the
first refinement iteration as well as the shared fea-
tures, we can approximate with very high accuracy
the updated cosine score between the modified
centroid and each candidate expansion. Our me-
thod relies on the fact that features (i.e., contexts)
are only ever removed from the original centroid –
no new features are ever added.
Let μ be the original centroid representing the
seed instances. Given an expansion error e, FMM
creates a modified centroid by removing all fea-
</bodyText>
<page confidence="0.992523">
293
</page>
<bodyText confidence="0.999810034482759">
tures intersecting between e and μ. Let μ&apos; be this
modified centroid. FMM requires us to compute
the similarity between μ&apos; and all candidate expan-
sions x as:
where i iterates over the feature space.
In our efficient setting, the only element that we
do not have for calculating the exact cosine simi-
larity is the norm of x, x . Given that we have the
original cosine similarity score, cos(x, μ) and that
we have the shared features between the original
centroid μ and the candidate expansion x we can
calculate x as:
In the above equation, the modified cosine score
can be considered as an update to the original co-
sine score, where the update depends only on the
shared features and the original centroid. The
above update equation can be used to recalculate
the similarity scores without resorting to an expen-
sive computation involving complete feature vec-
tors.
Storing the original centroid is expensive and
can be approximated instead from only the shared
features between the centroid and all instances in
the expanded set. We empirically tested this ap-
proximation by comparing the cosine scores be-
tween the candidate expansions and both the true
centroid and the approximated centroid. The aver-
age error in cosine score was 9.5E-04 ± 7.83E-05
(95% confidence interval).
</bodyText>
<sectionHeader confidence="0.99607" genericHeader="method">
4 Datasets and Baseline Algorithm
</sectionHeader>
<bodyText confidence="0.997689272727273">
We evaluate our algorithms against manually
scraped gold standard sets, which were extracted
from Wikipedia to represent a random collection of
concepts. Section 4.1 discusses the gold standard
sets and the criteria behind their selection. To
present a statistically significant view of our results
we generated a set of trials from gold standard sets
to use as seeds for our seed set expansion algo-
rithm. Also, in section 4.2 we discuss how we can
simulate editorial feedback using our gold standard
sets.
</bodyText>
<subsectionHeader confidence="0.990785">
4.1 Gold Standard Entity Sets
</subsectionHeader>
<bodyText confidence="0.9999378">
The gold standard sets form an essential part of our
evaluation. These sets were chosen to represent a
single concept such as Countries and Archbishops
of Canterbury. These sets were selected from the
List of pages from Wikipedia5. We randomly
sorted the list of every noun occurring in Wikipe-
dia. Then, for each noun we verified whether or
not it existed in a Wikipedia list, and if so we ex-
tracted this list – up to a maximum of 50 lists. If a
noun belonged to multiple lists, the authors chose
the list that seemed most appropriate. Although
this does not generate a perfect random sample,
diversity is ensured by the random selection of
nouns and relevancy is ensured by the author adju-
dication.
Lists were then scraped from the Wikipedia
website and they went through a manual cleanup
process which included merging variants. . The 50
sets contain on average 208 elements (with a min-
imum of 11 and a maximum of 1116 elements) for
a total of 10,377 elements. The final gold standard
lists contain 50 sets including classical pianists,
Spanish provinces, Texas counties, male tennis
players, first ladies, cocktails, bottled water
brands, and Archbishops of Canterbury6.
</bodyText>
<subsectionHeader confidence="0.996734">
4.2 Generation of Experimental Trials
</subsectionHeader>
<bodyText confidence="0.999929285714286">
To provide a statistically significant view of the
performance of our algorithm, we created more
than 1000 trials as follows. For each of the gold
standard seed sets, we created 30 random sortings.
These 30 random sortings were then used to gener-
ate trial seed sets with a maximum size of 20
seeds.
</bodyText>
<subsectionHeader confidence="0.9760855">
4.3 Simulating User Feedback and Baseline
Algorithm
</subsectionHeader>
<bodyText confidence="0.972239">
User feedback forms an integral part of our algo-
rithm. We used the gold standard sets to judge the
</bodyText>
<footnote confidence="0.8528006">
5 In this paper, extractions from Wikipedia are taken from a
snapshot of the resource in December 2007.
6 The gold standard is available for download at
http://www.patrickpantel.com/cgi-bin/Web/Tools/getfile.pl?
type=data&amp;id=sse-gold/wikipedia.20071218.goldsets.tgz
</footnote>
<equation confidence="0.939691785714286">
cos(x,μ ′ )=
∑ xiμ ′i
x ⋅ μ ′
∑ xiμi
μ ⋅ cos (x,μ)
Combining the two equations, have:
x =
∑ xiμ ′i
∑ xiμi
cos(x,μ ′ )= cos (x,μ)⋅
⋅
′
μ
μ
</equation>
<page confidence="0.993911">
294
</page>
<bodyText confidence="0.999928466666667">
candidate expansions. The judged expansions were
used to simulate user feedback by marking those
candidate expansions that were incorrect. The first
candidate expansion that was marked incorrect in
each editorial iteration was used as the editor’s
negative example and was given to the system as
an error.
In the next section, we report R-precision gains
at each iteration in the editorial process for our two
methods described in Section 3. Our baseline me-
thod simply measures the gains obtained by re-
moving the first incorrect entry in a candidate
expansion set at each iteration. This simulates the
process of manually cleaning a set by removing
one error at a time.
</bodyText>
<sectionHeader confidence="0.997867" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.841906">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999916032258065">
Wikipedia5 served as the source corpus for our al-
gorithms described in Sections 3.1 and 3.2. All
articles were POS-tagged using (Brill 1995) and
later chunked using a variant of (Abney 1991).
Corpus statistics from this processed text were col-
lected to build the similarity matrix for the SIM
method (Section 3.1) and to extract the features
required for the FMM method (Section 3.2). In
both cases corpus statistics were extracted over the
semi-syntactic contexts (chunks) to approximate
term meanings. The minimum similarity thresholds
were experimentally set to 0.15 and 0.11 for the
SIM and FMM algorithms respectively.
Each experimental trial described in Section
4.2, which consists of a set of seed instances of one
of our 50 random semantic classes, was expanded
using a variant of the distributional set expansion
algorithm from Sarmento et al. (2007). The expan-
sions were judged against the gold standard and
each candidate expansion was marked as either
correct or incorrect. This set of expanded and
judged candidate files were used as inputs to the
algorithms described in Sections 3.1 and 3.2.
Choosing the first candidate expansion that was
judged as incorrect simulated our user feedback.
This process was repeated for each iteration of the
algorithm and results are reported for 10 iterations.
The outputs of our algorithms were again
judged against the gold standard lists and the per-
formance was measured in terms of precision gains
over the baseline at various ranks. Precision gain
</bodyText>
<tableCaption confidence="0.9807955">
Table 1. R-precision of the three methods with 95% confi-
dence bounds.
</tableCaption>
<table confidence="0.999711454545455">
ITERATION BASELINE SIM FMM
1 0.219±0.012 0.234±0.013 0.220±0.015
2 0.223±0.013 0.242±0.014 0.227±0.017
3 0.227±0.013 0.251±0.015 0.235±0.019
4 0.232±0.013 0.26±0.016 0.252±0.021
5 0.235±0.014 0.266±0.017 0.267±0.022
6 0.236±0.014 0.269±0.017 0.282±0.023
7 0.238±0.014 0.273±0.018 0.294±0.023
8 0.24±0.014 0.28±0.018 0.303±0.024
9 0.242±0.014 0.285±0.018 0.315±0.025
10 0.243±0.014 0.286±0.018 0.322±0.025
</table>
<bodyText confidence="0.999049090909091">
for an algorithm over a baseline is the percentage
increase in precision for the same values of para-
meters of the algorithm over the baseline. Also, as
the size of our gold standard lists vary, we report
another commonly used statistic, R-precision. R-
precision for any set is the precision at the size of
the gold standard set. For example, if a gold stan-
dard set contains 20 elements, then R-precision for
any set expansion is measured as the precision at
rank 20. The average R-precision over each set is
then reported.
</bodyText>
<subsectionHeader confidence="0.999731">
5.2 Quantitative Analysis
</subsectionHeader>
<bodyText confidence="0.999884380952381">
Table 1 lists the performance of our baseline algo-
rithm (Section 4.3) and our proposed methods SIM
and FMM (Sections 3.1 and 3.2) in terms of their
R-precision with 95% confidence bounds over 10
iterations of each algorithm.
The FMM of Section 3.2 is the best performing
method in terms of R-precision reaching a maxi-
mum value of 0.322 after the 10th iteration. For
small numbers of iterations, however, the SIM me-
thod outperforms FMM since it is bolder in its re-
finements by removing all elements similar to the
tagged error. Inspection of FMM results showed
that bad instances get ranked lower in early itera-
tions but it is only after 4 or 5 iterations that they
get pushed passed the similarity threshold (ac-
counting for the low marginal increase in precision
gain for FMM in the first 4 to 5 iterations).
FMM outperforms the SIM method by an aver-
age of 4% increase in performance (13% im-
provement after 10 iterations). However both the
FMM and the SIM method are able to outperform
</bodyText>
<page confidence="0.998146">
295
</page>
<figureCaption confidence="0.997944">
Figure 1. Precision gain over baseline algorithm for SIM
method.
</figureCaption>
<bodyText confidence="0.999902375">
the baseline method. Using the FMM method one
would achieve an average of 17% improvement in
R-precision over manually cleaning up the set
(32.5% improvement after 10 iterations). Using the
SIM method one would achieve an average of 13%
improvement in R-precision over manually clean-
ing up the set (17.7% improvement after 10 itera-
tions).
</bodyText>
<subsectionHeader confidence="0.987107">
5.3 Intrinsic Analysis of the SIM Algorithm
</subsectionHeader>
<bodyText confidence="0.907878695652174">
Figure 1 shows the precision gain of the similarity
matrix based algorithm over the baseline algo-
rithm. The results are shown for precision at ranks
1, 2, 5, 10, 25, 50 and 100, as well as for R-
precision. The results are also shown for the first
10 iterations of the algorithm.
SIM outperforms the baseline algorithm for all
ranks and increases in gain throughout the 10 itera-
tions. As the number of iterations increases the
change in precision gain levels off. This behavior
can be attributed to the fact that we start removing
errors from top to bottom and in each iteration the
rank of the error candidate provided to the algo-
rithm is lower than in the previous iteration. This
results in errors which are not similar to any other
candidate expansions. These are random errors and
the discriminative capacity of this method reduces
severely.
Figure 1 also shows that the precision gain of
the similarity matrix algorithm over the baseline
algorithm is higher at ranks 1, 2 and 5. Also, the
performance increase drops at ranks 50 and 100.
This is because low ranks contain candidate expan-
</bodyText>
<figureCaption confidence="0.9978815">
Figure 2. Precision gain over baseline algorithm for FMM
method.
</figureCaption>
<bodyText confidence="0.998292333333333">
sions that are random errors introduced due to data
sparsity. Such unsystematic errors are not detecta-
ble by the SIM method.
</bodyText>
<subsectionHeader confidence="0.986756">
5.4 Intrinsic Analysis of the FMM Algorithm
</subsectionHeader>
<bodyText confidence="0.999990037037037">
The feature modification method of Section 3.2
shows similar behavior to the SIM method, how-
ever as Figure 2 shows, it outperforms SIM me-
thod in terms of precision gain for all values of
ranks tested. This is because the FMM method is
able to achieve fine-grained control over what it
removes and what it doesn’t, as described in Sec-
tion 5.2.
Another interesting aspect of FMM is illu-
strated in the R-precision curve. There is a sudden
jump in precision gain after the fifth iteration of
the algorithm. In the first iterations only few errors
are pushed beneath the similarity threshold as cen-
troid features intersecting with tagged errors are
slowly removed. As the feature vector for the cen-
troid gets smaller and smaller, remaining features
look more and more unambiguous to the target
entity type and erroneous example have less
chance of overlapping with the centroid causing
them to be pushed pass the conservative similarity
threshold. Different conservative thresholds
yielded similar curves. High thresholds yield bad
performance since all but the only very prototypi-
cal set instances are removed as errors.
The R-precision measure indirectly models re-
call as a function of the target coverage of each set.
We also directly measured recall at various ranks
</bodyText>
<page confidence="0.99294">
296
</page>
<bodyText confidence="0.9873695">
and FMM outperformed SIM at all ranks and itera-
tions.
</bodyText>
<subsectionHeader confidence="0.729298">
5.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999926318181818">
In this paper we proposed two techniques which
use user feedback to remove systematic errors in
set expansion systems caused by ambiguous seed
instances. Inspection of expansion errors yielded
other types of errors.
First, model errors are introduced in candidate
expansion sets by noise from various pre-
processing steps involved in generating the expan-
sions. Such errors cause incorrect contexts (or fea-
tures) to be extracted for seed instances and
ultimately can cause erroneous expansions to be
produced. These errors do not seem to be systemat-
ic and are hence not discoverable by our proposed
method.
Other errors are due to data sparsity. As the fea-
ture space can be very large, the difference in simi-
larity between a correct candidate expansion and
an incorrect expansion can be very small for sparse
entities. Previous approaches have suggested re-
moving candidate expansions for which too few
statistics can be extracted, however at the great
cost of recall (and lower R-precision).
</bodyText>
<sectionHeader confidence="0.999543" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999854928571429">
In this paper we presented two algorithms for im-
proving the precision of automatically expanded
entity sets by using minimal human negative
judgments. We showed that systematic errors
which arise from the semantic ambiguity inherent
in seed instances can be leveraged to automatically
refine entity sets. We proposed two techniques:
SIM which boldly removes instances that are dis-
tributionally similar to errors, and FMM which
more conservatively removes features from the
seed set representing its unintended (ambiguous)
concept in order to rank lower potential errors.
We showed empirical evidence that average R-
precision over random entity sets improves by 26%
to 51% when given from 5 to 10 manually tagged
errors. These results were reported by testing the
refinement algorithms on a set of 50 randomly
chosen entity sets expanded using a state of the art
expansion algorithm. Given very small amounts of
manual judgments, the SIM method outperformed
FMM (up to 4 manual judgments). FMM outper-
formed the SIM method given more than 6 manual
judgments. Both proposed refinement models have
linear time complexity in set size allowing for
practical online use in set expansion systems.
This paper only addresses techniques for re-
moving erroneous entities from expanded entity
sets. A complimentary way to improve perfor-
mance would be to investigate the addition of rele-
vant candidate expansions that are not already in
the initial expansion. We are currently investigat-
ing extensions to FMM that can efficiently add
new candidate expansions to the set by computing
the similarity between modified centroids and all
terms occurring in a large body of text.
We are also investigating ways to use the find-
ings of this work to a priori remove ambiguous
seed instances (or their ambiguous contexts) before
running the initial expansion algorithm. It is our
hope that most of the errors identified in this work
could be automatically discovered without any
manual judgments.
</bodyText>
<sectionHeader confidence="0.9981" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999173516129032">
Abney, S. Parsing by Chunks. 1991. In: Robert Ber-
wick, Steven Abney and Carol Tenny (eds.), Prin-
ciple-Based Parsing. Kluwer Academic Publishers,
Dordrecht.
Banko, M. and Brill, E. 2001. Scaling to very large cor-
pora for natural language disambiguation. In Pro-
ceedings of ACL-2001.pp 26-33. Morristown, NJ.
Banko, M.; Cafarella, M.; Soderland, S.; Broadhead,
M.; Etzioni, O. 2007. Open Information Extraction
from the Web. In Proceedings of IJCAI-07.
Bayardo, R. J; Yiming Ma,; Ramakrishnan Srikant.;
Scaling Up All-Pairs Similarity Search. In Proc. of
the 16th Int&apos;l Conf. on World Wide Web. pp 131-140
2007.
Brill, E. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing: A Case
Study in Part of Speech Tagging. Computational
Linguistics.
Bunescu, R. and Mooney, R. 2004 Collective Informa-
tion Extraction with Relational Markov Networks. In
Proceedings of ACL-04.pp. 438-445.
Cohn, D. A., Atlas, L., and Ladner, R. E. 1994. Improv-
ing Generalization with Active Learning. Machine
Learning, 15(2):201-221. Springer, Netherlands.
Dagan, I. and Engelson, S. P. 1995. Selective Sampling
in Natural Language Learning. In Proceedings of
IJCAI-95 Workshop on New Approaches to Learning
for Natural Language Processing. Montreal, Canada.
Downey, D.; Broadhead, M; Etzioni, O. 2007. Locating
Complex Named Entities in Web Text. In Proceed-
ings of IJCAI-07.
</reference>
<page confidence="0.961033">
297
</page>
<reference confidence="0.999675797468355">
Etzioni, O.; Cafarella, M.; Downey. D.; Popescu, A.;
Shaked, T; Soderland, S.; Weld, D.; Yates, A. 2005.
Unsupervised named-entity extraction from the Web:
An Experimental Study. In Artificial Intelligence,
165(1):91-134.
Harris, Z. 1985. Distributional Structure. In: Katz, J. J.
(ed.), The Philosophy of Linguistics. New York: Ox-
ford University Press. pp. 26-47.
Harman, D. 1992. Relevance feedback revisited. In
Proceeedings of SIGIR-92. Copenhagen, Denmark.
Hearst, M. A. 1992.Automatic acquisition of hyponyms
from large text corpora.In Proceedings of COLING-
92. Nantes, France.
Kozareva, Z., Riloff, E. and Hovy, E. 2008. Semantic
Class Learning from the Web with Hyponym Pattern
Linkage Graphs.In Proceedings of ACL-08.pp 1048-
1056. Columbus, OH
Lin, D. 1998.Automatic retrieval and clustering of simi-
lar words.In Proceedings of COLING/ACL-98.pp.
768–774. Montreal, Canada.
Nadeau, D., Turney, P. D. and Matwin., S. 2006. Unsu-
pervised Named-Entity Recognition: Generating Ga-
zetteers and Resolving Ambiguity. In Advances in
Artifical Intelligence.pp 266-277. Springer Berlin,
Heidelberg.
Negri, M. 2004. Sense-based blind relevance feedback
for question answering. In Proceedings of SIGIR-04
Workshop on Information Retrieval For Question
Answering (IR4QA). Sheffield, UK,
Pantel, P. and Lin, D. 2002. Discovering Word Senses
from Text. In Proceedings of KDD-02.pp. 613-619.
Edmonton, Canada.
Paşca, M. 2007a.Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of CIKM-07.pp. 683-690.
Pasca, M. 2007b. Organizing and Searching the World
Wide Web of Facts - Step Two: Harnessing the Wis-
dom of the Crowds. In Proceedings of WWW-07. pp.
101-110.
Paşca, M.; Lin, D.; Bigham, J.; Lifchits, A.; Jain, A.
2006. Names and Similarities on the Web: Fact Ex-
traction in the Fast Lane. In Proceedings of ACL-
2006.pp. 113-120.
Paşca, M. and Durme, B.J. 2008. Weakly-supervised
Acquisition of Open-Domain Classes and Class
Attributes from Web Documents and Query Logs. In
Proceedings of ACL-08.
Riloff, E. and Jones, R. 1999 Learning Dictionaries for
Information Extraction by Multi-Level Boostrap-
ping.In Proceedings of AAAI/IAAAI-99.
Riloff, E. and Shepherd, J. 1997. A corpus-based ap-
proach for building semantic lexicons.In Proceedings
of EMNLP-97.
Sarmento, L.; Jijkuon, V.; de Rijke, M.; and Oliveira, E.
2007. “More like these”: growing entity classes from
seeds. In Proceedings of CIKM-07. pp. 959-962. Lis-
bon, Portugal.
Stevenson, M., Guo, Y. and Gaizauskas, R. 2008. Ac-
quiring Sense Tagged Examples using Relevance
Feedback. In Proceedings ofCOLING-08. Manches-
ter UK.
Tang, M., Luo, X., and Roukos, S. 2001. Active learn-
ing for statistical natural language parsing.In Pro-
ceedings of ACL-2001.pp 120 -127. Philadelphia,
PA.
Vishwa. V, Wood, K., Milic-Frayling, N. and Cox, I. J.
2005. Comparing Relevance Feedback Algorithms
for Web Search. In Proceedings of WWW 2005. Chi-
ba, Japan.
Wang. R.C. and Cohen, W.W. 2007.Language-
Independent Set Expansion of Named Entities Using
the Web.In Proceedings of ICDM-07.
Zhou, X. S. and Huang, S. T. 2003. Relevance Feedback
in Image Retrieval: A Comprehensive Review -
Xiang Sean Zhou, Thomas S. Huang Multimedia
Systems. pp 8:536-544.
Zhou, G. and Su, J. 2001. Named entity recognition
using an HMM-based chunk tagger. In Proceedings
of ACL-2001.pp. 473-480. Morristown, NJ.
</reference>
<page confidence="0.997105">
298
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.668544">
<title confidence="0.997329">Semi-Automatic Entity Set Refinement</title>
<author confidence="0.79149">Vishnu Vyas</author>
<author confidence="0.79149">Patrick</author>
<affiliation confidence="0.698074">Yahoo! Labs</affiliation>
<address confidence="0.998854">Santa Clara, CA 95054</address>
<email confidence="0.999649">vishnu@yahoo-inc.com</email>
<email confidence="0.999649">ppantel@yahoo-inc.com</email>
<abstract confidence="0.999887">State of the art set expansion algorithms produce varying quality expansions for different entity types. Even for the highest quality expansions, errors still occur and manual refinements are necessary for most practical uses. In this paper, we propose algorithms to aide this refinement process, greatly reducing the amount of manual labor required. The methods rely on the fact that most expansion errors are systematic, often stemming from the fact that some seed elements are ambiguous. Using our methods, empirical evidence shows that average R-precision over random entity sets improves by 26% to 51% when given from 5 to 10 manually tagged errors. Both proposed refinement models have linear time complexity in set size allowing for practical online use in set expansion systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by Chunks.</title>
<date>1991</date>
<editor>In: Robert Berwick, Steven Abney and Carol Tenny (eds.), Principle-Based Parsing.</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="21816" citStr="Abney 1991" startWordPosition="3539" endWordPosition="3540"> an error. In the next section, we report R-precision gains at each iteration in the editorial process for our two methods described in Section 3. Our baseline method simply measures the gains obtained by removing the first incorrect entry in a candidate expansion set at each iteration. This simulates the process of manually cleaning a set by removing one error at a time. 5 Experimental Results 5.1 Experimental Setup Wikipedia5 served as the source corpus for our algorithms described in Sections 3.1 and 3.2. All articles were POS-tagged using (Brill 1995) and later chunked using a variant of (Abney 1991). Corpus statistics from this processed text were collected to build the similarity matrix for the SIM method (Section 3.1) and to extract the features required for the FMM method (Section 3.2). In both cases corpus statistics were extracted over the semi-syntactic contexts (chunks) to approximate term meanings. The minimum similarity thresholds were experimentally set to 0.15 and 0.11 for the SIM and FMM algorithms respectively. Each experimental trial described in Section 4.2, which consists of a set of seed instances of one of our 50 random semantic classes, was expanded using a variant of </context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Abney, S. Parsing by Chunks. 1991. In: Robert Berwick, Steven Abney and Carol Tenny (eds.), Principle-Based Parsing. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>E Brill</author>
</authors>
<title>Scaling to very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL-2001.pp</booktitle>
<pages>26--33</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="7159" citStr="Banko and Brill 2001" startWordPosition="1123" endWordPosition="1126">pansion errors. Using user feedback to improve a system’s performance is a common theme within many information retrieval and machine learning tasks. One form of user feedback is active learning (Cohn et al. 1994), where one or more classifiers are used to focus human annotation efforts on the most beneficial test cases. Active learning has been successfully applied to various natural language tasks such as parsing (Tang et al. 2001), POS tagging (Dagan and Engelson 1995) and providing large amounts of annotations for common natural language processing tasks such as word sense disambiguation (Banko and Brill 2001). Relevance feedback is another popular feedback paradigm commonly used in information retrieval (Harman 1992), where user feedback (either explicit or implicit) is used to refine the search results of an IR system. Relevance feedback has been successfully applied to many IR applications including content-based image retrieval (Zhouand Huang 2003) and web search (Vishwa et al. 2005). Within NLP applications relevance feedback has also been used to generate sense tagged examples for WSD tasks (Stevenson et al. 2008), and Question Answering (Negri 2004). Our methods use relevance feedback in the</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Banko, M. and Brill, E. 2001. Scaling to very large corpora for natural language disambiguation. In Proceedings of ACL-2001.pp 26-33. Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open Information Extraction from the Web. In</title>
<date>2007</date>
<booktitle>Proceedings of IJCAI-07.</booktitle>
<contexts>
<context position="6045" citStr="Banko et al. 2007" startWordPosition="941" endWordPosition="944"> in their other senses. data and rely on approaches such as clustering, targeted patterns and co-occurrences to extract sets of entities (Pantel and Lin 2002; Downey et al. 2007). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes. These methods rely on a small set of seed examples to extract sets of entities. They either are based on distributional approaches or employ lexico-syntactic patterns to expand a small set of seeds to a larger set of candidate expansions. Some methods such as (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007;Paşca 2007a) use lexico-syntactic patterns to expand a set of seeds from web text and query logs. Others such as (Paşca et al. 2006; Paşca 2007b; Paşca and Durme 2008) use distributional approaches. Wang and Cohen (2007) use structural cues in semistructured text to expand sets of seed elements. In all methods however, expansion errors inevitably occur. This paper focuses on the task of post processing any such system’s expansion output using minimal human judgments in order to remove expansion errors. Using user feedback to improve a system’s performance is a common theme within many informa</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Banko, M.; Cafarella, M.; Soderland, S.; Broadhead, M.; Etzioni, O. 2007. Open Information Extraction from the Web. In Proceedings of IJCAI-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Bayardo</author>
</authors>
<title>Yiming Ma,; Ramakrishnan Srikant.; Scaling Up All-Pairs Similarity Search.</title>
<date>2007</date>
<booktitle>In Proc. of the 16th Int&apos;l Conf. on World Wide Web.</booktitle>
<pages>131--140</pages>
<marker>Bayardo, 2007</marker>
<rawString>Bayardo, R. J; Yiming Ma,; Ramakrishnan Srikant.; Scaling Up All-Pairs Similarity Search. In Proc. of the 16th Int&apos;l Conf. on World Wide Web. pp 131-140 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging. Computational Linguistics.</title>
<date>1995</date>
<contexts>
<context position="21766" citStr="Brill 1995" startWordPosition="3530" endWordPosition="3531">’s negative example and was given to the system as an error. In the next section, we report R-precision gains at each iteration in the editorial process for our two methods described in Section 3. Our baseline method simply measures the gains obtained by removing the first incorrect entry in a candidate expansion set at each iteration. This simulates the process of manually cleaning a set by removing one error at a time. 5 Experimental Results 5.1 Experimental Setup Wikipedia5 served as the source corpus for our algorithms described in Sections 3.1 and 3.2. All articles were POS-tagged using (Brill 1995) and later chunked using a variant of (Abney 1991). Corpus statistics from this processed text were collected to build the similarity matrix for the SIM method (Section 3.1) and to extract the features required for the FMM method (Section 3.2). In both cases corpus statistics were extracted over the semi-syntactic contexts (chunks) to approximate term meanings. The minimum similarity thresholds were experimentally set to 0.15 and 0.11 for the SIM and FMM algorithms respectively. Each experimental trial described in Section 4.2, which consists of a set of seed instances of one of our 50 random </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Brill, E. 1995. Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>R Mooney</author>
</authors>
<title>Collective Information Extraction with Relational Markov Networks.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-04.pp.</booktitle>
<pages>438--445</pages>
<contexts>
<context position="5139" citStr="Bunescu and Mooney 2004" startWordPosition="792" endWordPosition="795">ithms for refining sets of named entities. The datasets and our evaluation methodology used to perform our experiments are presented in Section 4 and in Section 5 we describe experimental results. Finally, we conclude with some discussion and future work. 2 Related Work There is a large body of work for automatically building sets of named entities using various techniques including supervised, unsupervised and semi-supervised methods. Supervised techniques use large amounts of training data to detect and classify entities into coarse grained classes such as People, Organizations, and Places (Bunescu and Mooney 2004; Etzioni et al. 2005). On the other hand, unsupervised methods require no training 2 See http://demo.patrickpantel.com/ for a demonstration of the distributional thesaurus. 3 In practice, this problem is rare since most terms that are similar in one of their senses tend not to be similar in their other senses. data and rely on approaches such as clustering, targeted patterns and co-occurrences to extract sets of entities (Pantel and Lin 2002; Downey et al. 2007). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes. These methods rely on</context>
</contexts>
<marker>Bunescu, Mooney, 2004</marker>
<rawString>Bunescu, R. and Mooney, R. 2004 Collective Information Extraction with Relational Markov Networks. In Proceedings of ACL-04.pp. 438-445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cohn</author>
<author>L Atlas</author>
<author>R E Ladner</author>
</authors>
<date>1994</date>
<booktitle>Improving Generalization with Active Learning. Machine Learning,</booktitle>
<pages>15--2</pages>
<publisher>Springer,</publisher>
<location>Netherlands.</location>
<contexts>
<context position="6751" citStr="Cohn et al. 1994" startWordPosition="1057" endWordPosition="1060">ery logs. Others such as (Paşca et al. 2006; Paşca 2007b; Paşca and Durme 2008) use distributional approaches. Wang and Cohen (2007) use structural cues in semistructured text to expand sets of seed elements. In all methods however, expansion errors inevitably occur. This paper focuses on the task of post processing any such system’s expansion output using minimal human judgments in order to remove expansion errors. Using user feedback to improve a system’s performance is a common theme within many information retrieval and machine learning tasks. One form of user feedback is active learning (Cohn et al. 1994), where one or more classifiers are used to focus human annotation efforts on the most beneficial test cases. Active learning has been successfully applied to various natural language tasks such as parsing (Tang et al. 2001), POS tagging (Dagan and Engelson 1995) and providing large amounts of annotations for common natural language processing tasks such as word sense disambiguation (Banko and Brill 2001). Relevance feedback is another popular feedback paradigm commonly used in information retrieval (Harman 1992), where user feedback (either explicit or implicit) is used to refine the search r</context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>Cohn, D. A., Atlas, L., and Ladner, R. E. 1994. Improving Generalization with Active Learning. Machine Learning, 15(2):201-221. Springer, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>S P Engelson</author>
</authors>
<title>Selective Sampling in Natural Language Learning.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI-95 Workshop on New Approaches to Learning for Natural Language Processing.</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="7014" citStr="Dagan and Engelson 1995" startWordPosition="1101" endWordPosition="1104">ably occur. This paper focuses on the task of post processing any such system’s expansion output using minimal human judgments in order to remove expansion errors. Using user feedback to improve a system’s performance is a common theme within many information retrieval and machine learning tasks. One form of user feedback is active learning (Cohn et al. 1994), where one or more classifiers are used to focus human annotation efforts on the most beneficial test cases. Active learning has been successfully applied to various natural language tasks such as parsing (Tang et al. 2001), POS tagging (Dagan and Engelson 1995) and providing large amounts of annotations for common natural language processing tasks such as word sense disambiguation (Banko and Brill 2001). Relevance feedback is another popular feedback paradigm commonly used in information retrieval (Harman 1992), where user feedback (either explicit or implicit) is used to refine the search results of an IR system. Relevance feedback has been successfully applied to many IR applications including content-based image retrieval (Zhouand Huang 2003) and web search (Vishwa et al. 2005). Within NLP applications relevance feedback has also been used to gen</context>
</contexts>
<marker>Dagan, Engelson, 1995</marker>
<rawString>Dagan, I. and Engelson, S. P. 1995. Selective Sampling in Natural Language Learning. In Proceedings of IJCAI-95 Workshop on New Approaches to Learning for Natural Language Processing. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Locating Complex Named Entities in Web Text.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI-07.</booktitle>
<contexts>
<context position="5606" citStr="Downey et al. 2007" startWordPosition="867" endWordPosition="870"> amounts of training data to detect and classify entities into coarse grained classes such as People, Organizations, and Places (Bunescu and Mooney 2004; Etzioni et al. 2005). On the other hand, unsupervised methods require no training 2 See http://demo.patrickpantel.com/ for a demonstration of the distributional thesaurus. 3 In practice, this problem is rare since most terms that are similar in one of their senses tend not to be similar in their other senses. data and rely on approaches such as clustering, targeted patterns and co-occurrences to extract sets of entities (Pantel and Lin 2002; Downey et al. 2007). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes. These methods rely on a small set of seed examples to extract sets of entities. They either are based on distributional approaches or employ lexico-syntactic patterns to expand a small set of seeds to a larger set of candidate expansions. Some methods such as (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007;Paşca 2007a) use lexico-syntactic patterns to expand a set of seeds from web text and query logs. Others such as (Paşca et al. 2006; Paşca 2007b; Paşca and Durm</context>
</contexts>
<marker>Downey, Broadhead, Etzioni, 2007</marker>
<rawString>Downey, D.; Broadhead, M; Etzioni, O. 2007. Locating Complex Named Entities in Web Text. In Proceedings of IJCAI-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Popescu</author>
<author>A Shaked</author>
<author>T Soderland</author>
<author>S Weld</author>
<author>D Yates</author>
<author>A</author>
</authors>
<title>Unsupervised named-entity extraction from the Web: An Experimental Study.</title>
<date>2005</date>
<journal>In Artificial Intelligence,</journal>
<pages>165--1</pages>
<marker>Popescu, Shaked, Soderland, Weld, Yates, A, 2005</marker>
<rawString>Etzioni, O.; Cafarella, M.; Downey. D.; Popescu, A.; Shaked, T; Soderland, S.; Weld, D.; Yates, A. 2005. Unsupervised named-entity extraction from the Web: An Experimental Study. In Artificial Intelligence, 165(1):91-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Distributional Structure. In:</title>
<date>1985</date>
<booktitle>The Philosophy of Linguistics.</booktitle>
<pages>26--47</pages>
<editor>Katz, J. J. (ed.),</editor>
<publisher>University Press.</publisher>
<location>New York: Oxford</location>
<contexts>
<context position="11094" citStr="Harris 1985" startWordPosition="1763" endWordPosition="1764">lly identified errors in each iteration, we constrain the judge to identify at most one error in each iteration. Although this paper focuses solely on removing errors in an entity set, it is also possible to improve expanded sets by using feedback to add new elements to the sets. We consider this task out of scope for this paper. 3.1 Similarity Method (SIM) Our first method directly models observation a) in the previous section. Following Lin (1998), we model the similarity between entities using the distributional hypothesis, which states that similar terms tend to occur in similar contexts (Harris 1985). A semantic model can be obtained by recording the surrounding contexts for each term in a large collection of unstructured text. Methods differ in their definition of a context (e.g., text window or syntactic relations), or a means to weigh contexts (e.g., frequency, tf-idf, pointwise mutual information), or ultimately in measuring the similarity between two context vectors (e.g., using Euclidean distance, Cosine, Dice). In this paper, we use a text window of size 1, we weigh our contexts using pointwise mutual information, and we use the cosine score to compute the similarity between contex</context>
</contexts>
<marker>Harris, 1985</marker>
<rawString>Harris, Z. 1985. Distributional Structure. In: Katz, J. J. (ed.), The Philosophy of Linguistics. New York: Oxford University Press. pp. 26-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harman</author>
</authors>
<title>Relevance feedback revisited.</title>
<date>1992</date>
<booktitle>In Proceeedings of SIGIR-92.</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="7269" citStr="Harman 1992" startWordPosition="1139" endWordPosition="1140">al and machine learning tasks. One form of user feedback is active learning (Cohn et al. 1994), where one or more classifiers are used to focus human annotation efforts on the most beneficial test cases. Active learning has been successfully applied to various natural language tasks such as parsing (Tang et al. 2001), POS tagging (Dagan and Engelson 1995) and providing large amounts of annotations for common natural language processing tasks such as word sense disambiguation (Banko and Brill 2001). Relevance feedback is another popular feedback paradigm commonly used in information retrieval (Harman 1992), where user feedback (either explicit or implicit) is used to refine the search results of an IR system. Relevance feedback has been successfully applied to many IR applications including content-based image retrieval (Zhouand Huang 2003) and web search (Vishwa et al. 2005). Within NLP applications relevance feedback has also been used to generate sense tagged examples for WSD tasks (Stevenson et al. 2008), and Question Answering (Negri 2004). Our methods use relevance feedback in the form of negative examples to refine the results of a set expansion system. 291 3 Dynamic Similarity Modeling </context>
</contexts>
<marker>Harman, 1992</marker>
<rawString>Harman, D. 1992. Relevance feedback revisited. In Proceeedings of SIGIR-92. Copenhagen, Denmark.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M A Hearst</author>
</authors>
<title>1992.Automatic acquisition of hyponyms from large text corpora.In</title>
<booktitle>Proceedings of COLING92.</booktitle>
<location>Nantes, France.</location>
<marker>Hearst, </marker>
<rawString>Hearst, M. A. 1992.Automatic acquisition of hyponyms from large text corpora.In Proceedings of COLING92. Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Riloff</author>
<author>E Hovy</author>
</authors>
<date>2008</date>
<booktitle>Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs.In Proceedings of ACL-08.pp</booktitle>
<pages>1048--1056</pages>
<location>Columbus, OH</location>
<contexts>
<context position="1559" citStr="Kozareva et al. 2008" startWordPosition="239" endWordPosition="242">ction Sets of named entities are extremely useful in a variety of natural language and information retrieval tasks. For example, companies such as Yahoo! and Google maintain sets of named entities such as cities, products and celebrities to improve search engine relevance. Manually creating and maintaining large sets of named entities is expensive and laborious. In response, many automatic and semi-automatic methods of creating sets of named entities have been proposed, some are supervised (Zhou and Su, 2001), unsupervised (Pantel and Lin 2002, Nadeau et al. 2006), and others semi-supervised (Kozareva et al. 2008). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes such as European Cities and French Impressionist Painters. Methods differ in complexity from simple ones using lexicosyntactic patterns (Hearst 1992) to more complicated techniques based on distributional similarity (Paşca 2007a). Even for state of the art methods, expansion errors inevitably occur and manual refinements are necessary for most practical uses requiring high precision (such as for query interpretation at commercial search engines). Looking at expansions from state of th</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Kozareva, Z., Riloff, E. and Hovy, E. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs.In Proceedings of ACL-08.pp 1048-1056. Columbus, OH</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>retrieval and clustering of similar words.In</title>
<date>1998</date>
<booktitle>Proceedings of COLING/ACL-98.pp.</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="3197" citStr="Lin 1998" startWordPosition="483" endWordPosition="484">ne is both a Roman God and a Planet. Below is an excerpt of the GoogleSet expansion: Mars, Venus, *Moon, Mercury, *asteroid, Jupiter, *Earth, *comet, *Sonne, *Sun, ... The inherent semantic similarity between the errors can be leveraged to quickly clean up the expansion. For example, given a manually tagged error “asteroid”, a distributional similarity thesaurus 1 http://labs.google.com/sets 290 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 290–298, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics such as (Lin 1998)2 can identify comet as similar to asteroid and therefore potentially also as an error. This method has its limitations since a manually tagged error such as Earth would correctly remove Moon and Sun, but it would also incorrectly remove Mars, Venus and Jupiter since they are also similar to Earth3. In this paper, we propose two algorithms to improve the precision of automatically expanded entity sets by using minimal human negative judgments. The algorithms leverage the fact that set expansion errors are systematically caused by ambiguous seed instances which attract incorrect instances of an</context>
<context position="10935" citStr="Lin (1998)" startWordPosition="1739" endWordPosition="1740"> errors as desired and then have the system automatically remove other errors in each iteration. Because it is intractable to test arbitrary numbers of manually identified errors in each iteration, we constrain the judge to identify at most one error in each iteration. Although this paper focuses solely on removing errors in an entity set, it is also possible to improve expanded sets by using feedback to add new elements to the sets. We consider this task out of scope for this paper. 3.1 Similarity Method (SIM) Our first method directly models observation a) in the previous section. Following Lin (1998), we model the similarity between entities using the distributional hypothesis, which states that similar terms tend to occur in similar contexts (Harris 1985). A semantic model can be obtained by recording the surrounding contexts for each term in a large collection of unstructured text. Methods differ in their definition of a context (e.g., text window or syntactic relations), or a means to weigh contexts (e.g., frequency, tf-idf, pointwise mutual information), or ultimately in measuring the similarity between two context vectors (e.g., using Euclidean distance, Cosine, Dice). In this paper,</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998.Automatic retrieval and clustering of similar words.In Proceedings of COLING/ACL-98.pp. 768–774. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nadeau</author>
<author>P D Turney</author>
<author>S Matwin</author>
</authors>
<title>Unsupervised Named-Entity Recognition: Generating Gazetteers and Resolving Ambiguity.</title>
<date>2006</date>
<booktitle>In Advances in Artifical Intelligence.pp</booktitle>
<pages>266--277</pages>
<publisher>Springer</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="1508" citStr="Nadeau et al. 2006" startWordPosition="232" endWordPosition="235">al online use in set expansion systems. 1 Introduction Sets of named entities are extremely useful in a variety of natural language and information retrieval tasks. For example, companies such as Yahoo! and Google maintain sets of named entities such as cities, products and celebrities to improve search engine relevance. Manually creating and maintaining large sets of named entities is expensive and laborious. In response, many automatic and semi-automatic methods of creating sets of named entities have been proposed, some are supervised (Zhou and Su, 2001), unsupervised (Pantel and Lin 2002, Nadeau et al. 2006), and others semi-supervised (Kozareva et al. 2008). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes such as European Cities and French Impressionist Painters. Methods differ in complexity from simple ones using lexicosyntactic patterns (Hearst 1992) to more complicated techniques based on distributional similarity (Paşca 2007a). Even for state of the art methods, expansion errors inevitably occur and manual refinements are necessary for most practical uses requiring high precision (such as for query interpretation at commercial sear</context>
</contexts>
<marker>Nadeau, Turney, Matwin, 2006</marker>
<rawString>Nadeau, D., Turney, P. D. and Matwin., S. 2006. Unsupervised Named-Entity Recognition: Generating Gazetteers and Resolving Ambiguity. In Advances in Artifical Intelligence.pp 266-277. Springer Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Negri</author>
</authors>
<title>Sense-based blind relevance feedback for question answering.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGIR-04 Workshop on Information Retrieval For Question Answering (IR4QA).</booktitle>
<location>Sheffield, UK,</location>
<contexts>
<context position="7716" citStr="Negri 2004" startWordPosition="1208" endWordPosition="1209">ch as word sense disambiguation (Banko and Brill 2001). Relevance feedback is another popular feedback paradigm commonly used in information retrieval (Harman 1992), where user feedback (either explicit or implicit) is used to refine the search results of an IR system. Relevance feedback has been successfully applied to many IR applications including content-based image retrieval (Zhouand Huang 2003) and web search (Vishwa et al. 2005). Within NLP applications relevance feedback has also been used to generate sense tagged examples for WSD tasks (Stevenson et al. 2008), and Question Answering (Negri 2004). Our methods use relevance feedback in the form of negative examples to refine the results of a set expansion system. 291 3 Dynamic Similarity Modeling The set expansion algorithms discussed in Section 2 often produce high quality entity sets, however inevitably errors are introduced. Applications requiring high precision sets must invest significantly in editorial efforts to clean up the sets. Although companies like Yahoo! and Google can afford to routinely support such manual labor, there is a large opportunity to reduce the refinement cost (i.e., number of required human judgments). Recal</context>
</contexts>
<marker>Negri, 2004</marker>
<rawString>Negri, M. 2004. Sense-based blind relevance feedback for question answering. In Proceedings of SIGIR-04 Workshop on Information Retrieval For Question Answering (IR4QA). Sheffield, UK,</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering Word Senses from Text.</title>
<date>2002</date>
<booktitle>In Proceedings of KDD-02.pp.</booktitle>
<pages>613--619</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1487" citStr="Pantel and Lin 2002" startWordPosition="228" endWordPosition="231"> allowing for practical online use in set expansion systems. 1 Introduction Sets of named entities are extremely useful in a variety of natural language and information retrieval tasks. For example, companies such as Yahoo! and Google maintain sets of named entities such as cities, products and celebrities to improve search engine relevance. Manually creating and maintaining large sets of named entities is expensive and laborious. In response, many automatic and semi-automatic methods of creating sets of named entities have been proposed, some are supervised (Zhou and Su, 2001), unsupervised (Pantel and Lin 2002, Nadeau et al. 2006), and others semi-supervised (Kozareva et al. 2008). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes such as European Cities and French Impressionist Painters. Methods differ in complexity from simple ones using lexicosyntactic patterns (Hearst 1992) to more complicated techniques based on distributional similarity (Paşca 2007a). Even for state of the art methods, expansion errors inevitably occur and manual refinements are necessary for most practical uses requiring high precision (such as for query interpretati</context>
<context position="5585" citStr="Pantel and Lin 2002" startWordPosition="863" endWordPosition="866"> techniques use large amounts of training data to detect and classify entities into coarse grained classes such as People, Organizations, and Places (Bunescu and Mooney 2004; Etzioni et al. 2005). On the other hand, unsupervised methods require no training 2 See http://demo.patrickpantel.com/ for a demonstration of the distributional thesaurus. 3 In practice, this problem is rare since most terms that are similar in one of their senses tend not to be similar in their other senses. data and rely on approaches such as clustering, targeted patterns and co-occurrences to extract sets of entities (Pantel and Lin 2002; Downey et al. 2007). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes. These methods rely on a small set of seed examples to extract sets of entities. They either are based on distributional approaches or employ lexico-syntactic patterns to expand a small set of seeds to a larger set of candidate expansions. Some methods such as (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007;Paşca 2007a) use lexico-syntactic patterns to expand a set of seeds from web text and query logs. Others such as (Paşca et al. 2006; Paşca </context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In Proceedings of KDD-02.pp. 613-619. Edmonton, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Paşca</author>
</authors>
<title>2007a.Weakly-supervised discovery of named entities using web search queries.</title>
<booktitle>In Proceedings of CIKM-07.pp.</booktitle>
<pages>683--690</pages>
<marker>Paşca, </marker>
<rawString>Paşca, M. 2007a.Weakly-supervised discovery of named entities using web search queries. In Proceedings of CIKM-07.pp. 683-690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
</authors>
<title>Organizing and Searching the World Wide Web of Facts - Step Two: Harnessing the Wisdom of the Crowds.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW-07.</booktitle>
<pages>101--110</pages>
<marker>Pasca, 2007</marker>
<rawString>Pasca, M. 2007b. Organizing and Searching the World Wide Web of Facts - Step Two: Harnessing the Wisdom of the Crowds. In Proceedings of WWW-07. pp. 101-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paşca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Names and Similarities on the Web: Fact Extraction in the Fast Lane.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL2006.pp.</booktitle>
<pages>113--120</pages>
<contexts>
<context position="6177" citStr="Paşca et al. 2006" startWordPosition="964" endWordPosition="967">ties (Pantel and Lin 2002; Downey et al. 2007). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes. These methods rely on a small set of seed examples to extract sets of entities. They either are based on distributional approaches or employ lexico-syntactic patterns to expand a small set of seeds to a larger set of candidate expansions. Some methods such as (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007;Paşca 2007a) use lexico-syntactic patterns to expand a set of seeds from web text and query logs. Others such as (Paşca et al. 2006; Paşca 2007b; Paşca and Durme 2008) use distributional approaches. Wang and Cohen (2007) use structural cues in semistructured text to expand sets of seed elements. In all methods however, expansion errors inevitably occur. This paper focuses on the task of post processing any such system’s expansion output using minimal human judgments in order to remove expansion errors. Using user feedback to improve a system’s performance is a common theme within many information retrieval and machine learning tasks. One form of user feedback is active learning (Cohn et al. 1994), where one or more classi</context>
</contexts>
<marker>Paşca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>Paşca, M.; Lin, D.; Bigham, J.; Lifchits, A.; Jain, A. 2006. Names and Similarities on the Web: Fact Extraction in the Fast Lane. In Proceedings of ACL2006.pp. 113-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paşca</author>
<author>B J Durme</author>
</authors>
<title>Weakly-supervised Acquisition of Open-Domain Classes and Class Attributes from Web Documents and Query Logs.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08.</booktitle>
<contexts>
<context position="6213" citStr="Paşca and Durme 2008" startWordPosition="970" endWordPosition="973">y et al. 2007). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes. These methods rely on a small set of seed examples to extract sets of entities. They either are based on distributional approaches or employ lexico-syntactic patterns to expand a small set of seeds to a larger set of candidate expansions. Some methods such as (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007;Paşca 2007a) use lexico-syntactic patterns to expand a set of seeds from web text and query logs. Others such as (Paşca et al. 2006; Paşca 2007b; Paşca and Durme 2008) use distributional approaches. Wang and Cohen (2007) use structural cues in semistructured text to expand sets of seed elements. In all methods however, expansion errors inevitably occur. This paper focuses on the task of post processing any such system’s expansion output using minimal human judgments in order to remove expansion errors. Using user feedback to improve a system’s performance is a common theme within many information retrieval and machine learning tasks. One form of user feedback is active learning (Cohn et al. 1994), where one or more classifiers are used to focus human annota</context>
</contexts>
<marker>Paşca, Durme, 2008</marker>
<rawString>Paşca, M. and Durme, B.J. 2008. Weakly-supervised Acquisition of Open-Domain Classes and Class Attributes from Web Documents and Query Logs. In Proceedings of ACL-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Boostrapping.In</title>
<date>1999</date>
<booktitle>Proceedings of AAAI/IAAAI-99.</booktitle>
<contexts>
<context position="6026" citStr="Riloff and Jones 1999" startWordPosition="937" endWordPosition="940"> tend not to be similar in their other senses. data and rely on approaches such as clustering, targeted patterns and co-occurrences to extract sets of entities (Pantel and Lin 2002; Downey et al. 2007). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes. These methods rely on a small set of seed examples to extract sets of entities. They either are based on distributional approaches or employ lexico-syntactic patterns to expand a small set of seeds to a larger set of candidate expansions. Some methods such as (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007;Paşca 2007a) use lexico-syntactic patterns to expand a set of seeds from web text and query logs. Others such as (Paşca et al. 2006; Paşca 2007b; Paşca and Durme 2008) use distributional approaches. Wang and Cohen (2007) use structural cues in semistructured text to expand sets of seed elements. In all methods however, expansion errors inevitably occur. This paper focuses on the task of post processing any such system’s expansion output using minimal human judgments in order to remove expansion errors. Using user feedback to improve a system’s performance is a common theme </context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Riloff, E. and Jones, R. 1999 Learning Dictionaries for Information Extraction by Multi-Level Boostrapping.In Proceedings of AAAI/IAAAI-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Shepherd</author>
</authors>
<title>A corpus-based approach for building semantic lexicons.In</title>
<date>1997</date>
<booktitle>Proceedings of EMNLP-97.</booktitle>
<contexts>
<context position="6003" citStr="Riloff and Shepherd 1997" startWordPosition="933" endWordPosition="936">lar in one of their senses tend not to be similar in their other senses. data and rely on approaches such as clustering, targeted patterns and co-occurrences to extract sets of entities (Pantel and Lin 2002; Downey et al. 2007). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes. These methods rely on a small set of seed examples to extract sets of entities. They either are based on distributional approaches or employ lexico-syntactic patterns to expand a small set of seeds to a larger set of candidate expansions. Some methods such as (Riloff and Shepherd 1997; Riloff and Jones 1999; Banko et al. 2007;Paşca 2007a) use lexico-syntactic patterns to expand a set of seeds from web text and query logs. Others such as (Paşca et al. 2006; Paşca 2007b; Paşca and Durme 2008) use distributional approaches. Wang and Cohen (2007) use structural cues in semistructured text to expand sets of seed elements. In all methods however, expansion errors inevitably occur. This paper focuses on the task of post processing any such system’s expansion output using minimal human judgments in order to remove expansion errors. Using user feedback to improve a system’s perform</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>Riloff, E. and Shepherd, J. 1997. A corpus-based approach for building semantic lexicons.In Proceedings of EMNLP-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Sarmento</author>
<author>V Jijkuon</author>
<author>M de Rijke</author>
<author>E Oliveira</author>
</authors>
<title>More like these”: growing entity classes from seeds.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM-07.</booktitle>
<pages>959--962</pages>
<location>Lisbon,</location>
<marker>Sarmento, Jijkuon, de Rijke, Oliveira, 2007</marker>
<rawString>Sarmento, L.; Jijkuon, V.; de Rijke, M.; and Oliveira, E. 2007. “More like these”: growing entity classes from seeds. In Proceedings of CIKM-07. pp. 959-962. Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>Y Guo</author>
<author>R Gaizauskas</author>
</authors>
<title>Acquiring Sense Tagged Examples using Relevance Feedback.</title>
<date>2008</date>
<booktitle>In Proceedings ofCOLING-08.</booktitle>
<publisher>Manchester UK.</publisher>
<contexts>
<context position="7679" citStr="Stevenson et al. 2008" startWordPosition="1201" endWordPosition="1204"> for common natural language processing tasks such as word sense disambiguation (Banko and Brill 2001). Relevance feedback is another popular feedback paradigm commonly used in information retrieval (Harman 1992), where user feedback (either explicit or implicit) is used to refine the search results of an IR system. Relevance feedback has been successfully applied to many IR applications including content-based image retrieval (Zhouand Huang 2003) and web search (Vishwa et al. 2005). Within NLP applications relevance feedback has also been used to generate sense tagged examples for WSD tasks (Stevenson et al. 2008), and Question Answering (Negri 2004). Our methods use relevance feedback in the form of negative examples to refine the results of a set expansion system. 291 3 Dynamic Similarity Modeling The set expansion algorithms discussed in Section 2 often produce high quality entity sets, however inevitably errors are introduced. Applications requiring high precision sets must invest significantly in editorial efforts to clean up the sets. Although companies like Yahoo! and Google can afford to routinely support such manual labor, there is a large opportunity to reduce the refinement cost (i.e., numbe</context>
</contexts>
<marker>Stevenson, Guo, Gaizauskas, 2008</marker>
<rawString>Stevenson, M., Guo, Y. and Gaizauskas, R. 2008. Acquiring Sense Tagged Examples using Relevance Feedback. In Proceedings ofCOLING-08. Manchester UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tang</author>
<author>X Luo</author>
<author>S Roukos</author>
</authors>
<title>Active learning for statistical natural language parsing.In</title>
<date>2001</date>
<booktitle>Proceedings of ACL-2001.pp 120 -127.</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="6975" citStr="Tang et al. 2001" startWordPosition="1095" endWordPosition="1098">however, expansion errors inevitably occur. This paper focuses on the task of post processing any such system’s expansion output using minimal human judgments in order to remove expansion errors. Using user feedback to improve a system’s performance is a common theme within many information retrieval and machine learning tasks. One form of user feedback is active learning (Cohn et al. 1994), where one or more classifiers are used to focus human annotation efforts on the most beneficial test cases. Active learning has been successfully applied to various natural language tasks such as parsing (Tang et al. 2001), POS tagging (Dagan and Engelson 1995) and providing large amounts of annotations for common natural language processing tasks such as word sense disambiguation (Banko and Brill 2001). Relevance feedback is another popular feedback paradigm commonly used in information retrieval (Harman 1992), where user feedback (either explicit or implicit) is used to refine the search results of an IR system. Relevance feedback has been successfully applied to many IR applications including content-based image retrieval (Zhouand Huang 2003) and web search (Vishwa et al. 2005). Within NLP applications relev</context>
</contexts>
<marker>Tang, Luo, Roukos, 2001</marker>
<rawString>Tang, M., Luo, X., and Roukos, S. 2001. Active learning for statistical natural language parsing.In Proceedings of ACL-2001.pp 120 -127. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Wood</author>
<author>K Milic-Frayling</author>
<author>N</author>
<author>I J Cox</author>
</authors>
<title>Comparing Relevance Feedback Algorithms for Web Search.</title>
<date>2005</date>
<booktitle>In Proceedings of WWW</booktitle>
<location>Chiba, Japan.</location>
<marker>Wood, Milic-Frayling, N, Cox, 2005</marker>
<rawString>Vishwa. V, Wood, K., Milic-Frayling, N. and Cox, I. J. 2005. Comparing Relevance Feedback Algorithms for Web Search. In Proceedings of WWW 2005. Chiba, Japan.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R C</author>
<author>W W Cohen</author>
</authors>
<booktitle>2007.LanguageIndependent Set Expansion of Named Entities Using the Web.In Proceedings of ICDM-07.</booktitle>
<marker>C, Cohen, </marker>
<rawString>Wang. R.C. and Cohen, W.W. 2007.LanguageIndependent Set Expansion of Named Entities Using the Web.In Proceedings of ICDM-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X S Zhou</author>
<author>S T Huang</author>
</authors>
<title>Relevance Feedback in Image Retrieval: A Comprehensive Review -Xiang Sean Zhou, Thomas S. Huang Multimedia Systems.</title>
<date>2003</date>
<pages>8--536</pages>
<marker>Zhou, Huang, 2003</marker>
<rawString>Zhou, X. S. and Huang, S. T. 2003. Relevance Feedback in Image Retrieval: A Comprehensive Review -Xiang Sean Zhou, Thomas S. Huang Multimedia Systems. pp 8:536-544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Su</author>
</authors>
<title>Named entity recognition using an HMM-based chunk tagger.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL-2001.pp.</booktitle>
<pages>473--480</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="1452" citStr="Zhou and Su, 2001" startWordPosition="223" endWordPosition="226">linear time complexity in set size allowing for practical online use in set expansion systems. 1 Introduction Sets of named entities are extremely useful in a variety of natural language and information retrieval tasks. For example, companies such as Yahoo! and Google maintain sets of named entities such as cities, products and celebrities to improve search engine relevance. Manually creating and maintaining large sets of named entities is expensive and laborious. In response, many automatic and semi-automatic methods of creating sets of named entities have been proposed, some are supervised (Zhou and Su, 2001), unsupervised (Pantel and Lin 2002, Nadeau et al. 2006), and others semi-supervised (Kozareva et al. 2008). Semi-supervised approaches are often used in practice since they allow for targeting specific entity classes such as European Cities and French Impressionist Painters. Methods differ in complexity from simple ones using lexicosyntactic patterns (Hearst 1992) to more complicated techniques based on distributional similarity (Paşca 2007a). Even for state of the art methods, expansion errors inevitably occur and manual refinements are necessary for most practical uses requiring high precis</context>
</contexts>
<marker>Zhou, Su, 2001</marker>
<rawString>Zhou, G. and Su, J. 2001. Named entity recognition using an HMM-based chunk tagger. In Proceedings of ACL-2001.pp. 473-480. Morristown, NJ.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>