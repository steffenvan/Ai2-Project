<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.785308">
A Probabilistic Model for Text Categorization:
Based on a Single Random Variable with Multiple Values
Makoto IWAYA M A
</title>
<author confidence="0.3546085">
Advanced Research Laboratory
Hitachi Ltd.
</author>
<affiliation confidence="0.62555">
HATOYAMA
</affiliation>
<address confidence="0.911484">
SAITAMA 350-03, JAPAN
</address>
<email confidence="0.9292">
iwayamaeharl.hitachi.co.jp
</email>
<sectionHeader confidence="0.992986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993722">
Text categorization is the classification of
documents with respect to a set of prede-
fined categories. In this paper, we propose
a new probabilistic model for text catego-
rization, that is based on a Single random
Variable with Multiple Values (SVMV). Com-
pared to previous probabilistic models, our
model has the following advantages; 1) it
considers within-document term frequencies,
2) considers term weighting for target docu-
ments, and 3) is less affected by having insuf-
ficient training cases. We verify our model&apos;s
superiority over the others in the task of cat-
egorizing news articles from the &amp;quot;Wall Street
Journal&amp;quot;.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999567083333333">
Text categorization is the classification of documents
with respect to a set of predefined categories. As an
example, let us take a look at the following article from
the &amp;quot;Wall Street Journal&amp;quot; (1989/11/2).
McDermott International, Inc. said its Bab-
cock &amp; Wilcox unit completed the sale of its
Bailey Controls Operations to Finmeccanica
S.p.A for $295 million. Finmeccanica is an
Italian state-owned holding company with in-
terests in the mechanical engineering indus-
try. Bailey Controls, based in Wickliffe, Ohio,
makes computerized industrial controls sys-
tems. It employs 2,700 people and has annual
revenue of about $370 million.
Two categories (topics) are manually assigned to
this article; &amp;quot;TENDER OFFERS, MERGERS, ACQUISI-
TIONS (TNM)&amp;quot; and &amp;quot;ComPuTERs AND INFORMA-
TION TECHNOLOGY (CPR).&amp;quot; While there may be cer-
tain rules or standards for categorization, it is very
difficult for human experts to assign categories con-
sistently and efficiently to large numbers of daily in-
coming documents. The purpose of this paper is to
propose a new probabilistic model for automatic text
categorization.
</bodyText>
<author confidence="0.386059">
Takenobu TOKUNAGA
</author>
<affiliation confidence="0.9989905">
Department of Computer Science
Tokyo Institute of Technology
</affiliation>
<address confidence="0.864548">
2-12-1, OOKAYAMA, MEGURO-KU
TOKYO 152, JAPAN
</address>
<email confidence="0.77272">
takeecs.titech.ac.jp
</email>
<bodyText confidence="0.999090666666667">
While many text categorization models have been
proposed so far, in this paper, we concentrate on
the probabilistic models (Robertson and Sparck Jones,
1976; Kwok, 1990; Fuhr, 1989; Lewis, 1992; Croft,
1981; Wong and Yao, 1989; Yu et al., 1989) because
these models have solid formal grounding in probabil-
ity theory. Section 2 quickly reviews the probabilis-
tic models and lists their individual problems. In sec-
tion 3, we propose a new probabilistic model based on a
Single random Variable with Multiple Values (SVMV).
Our model is very simple, but solves some problems
of the previous models. In section 4, we verify our
model&apos;s superiority over the others through experi-
ments in which we categorize &amp;quot;Wall Street Journal&amp;quot;
articles.
</bodyText>
<sectionHeader confidence="0.995132" genericHeader="introduction">
2 A Brief Survey of Probabilistic Text
Categorization
</sectionHeader>
<bodyText confidence="0.9768042">
In this section, we will briefly review three major prob-
abilistic models for text categorization. Originally,
these models have been exploited for information re-
trieval, but the adaptation to text categorization is
straightforward.
In a model of probabilistic text categorization,
P(cid) = &amp;quot;the probability that a document d is
categorized into a category c&amp;quot; (1)
is calculated. Usually, a set of categories is defined be-
forehand. For every document di, probability P(cldi) is
calculated and all the documents are ranked in decreas-
ing order according to their probabilities. The larger
P(cldi) a document di has, the more probably it will
be categorized into category c. This is called the Prob-
abilistic Ranking Principle (PRP) (Robertson, 1977).
Several strategies can be used to assign categories to a
document based on PRP (Lewis, 1992).
There are several ways to calculate P(c1d). Three
representatives are (Robertson and Sparck Jones,
1976), (Kwok, 1990), and (Fuhr, 1989).
</bodyText>
<sectionHeader confidence="0.6617145" genericHeader="method">
2.1 Probabilistic Relevance Weighting
(PRW)
</sectionHeader>
<bodyText confidence="0.754293">
Robertson and Sparck Jones (1976) make use of the
well-known logistic (or log-odds) transformation of the
</bodyText>
<page confidence="0.944564">
162
</page>
<equation confidence="0.96207775">
probability P(c1d).
P(c1d)
g(c1d)= log (2)
PCeld)
</equation>
<bodyText confidence="0.9501976">
where E means &amp;quot;not c&amp;quot;, that is &amp;quot;a document is not
categorized into c.&amp;quot; Since this is a monotonic transfor-
mation of P(c1d), PRP is still satisfied after transfor-
mation.
Using Bayes&apos; theorem, Eq. (2) becomes
</bodyText>
<equation confidence="0.999281">
1c d) =logP(d1c) -1- log PP((TO). (3)
P(dI)
</equation>
<bodyText confidence="0.99304075">
Here, P(c) is the prior probability that a document is
categorized into c. This is estimated from given train-
ing data, i.e., the number of documents assigned to the
category c. P(dc) is calculated as follows. If we as-
sume that a document consists of a set of terms (usually
nouns are used for the first approximation) and each
term appears independently in a document, P(d1c) is
decomposed to
</bodyText>
<equation confidence="0.990925">
P(d1c) =fi P(T,: =11c) rf P(Ti = 01c) (4)
t, Ed
</equation>
<bodyText confidence="0.995289090909091">
where &amp;quot;c - d&amp;quot; is a set of terms that do not appear
in d but appear in the training cases assigned to c.
&amp;quot;ti&amp;quot; represents the name of a term and &amp;quot;Ti = 1,0&amp;quot;
represents whether or not the corresponding term &amp;quot;ti&amp;quot;
appears in a document. Therefore, P(Ti =1,01c) is the
probability that a document does or does not contain
the term ti, given that the document is categorized into
c. This probability is estimated from the training data;
the number of documents that are categorized into c
and have the term ti. Substituting Eq. (4) into Eq. (3)
yields
</bodyText>
<equation confidence="0.999822714285714">
g(c1d) = E log P((Ti 1
Ti= 11c) E log P(Ti = 01c)
P(T, 1!—) =
t,Ed tiEc—d
, P(c)
+ log (5)
P(c)
</equation>
<bodyText confidence="0.998544666666667">
We refer to Robertson and Sparck Jones&apos; formulation
as Probabilistic Relevance Weighting (PRW).
While PRW is the first attempt to formalize well-
known relevance weighting (Sparck Jones, 1972; Salton
and McGill, 1983) by probability theory, there are sev-
eral drawbacks in PRW.
</bodyText>
<subsectionHeader confidence="0.521693">
[Problem 1] no within-document term fre-
quencies
</subsectionHeader>
<bodyText confidence="0.981520333333333">
PRW does not make use of within-document term
frequencies. P(T =1, 01c) in Eq. (5) takes into account
only the existence/absence of the term tin a document.
In general, frequently appearing terms in a document
play an important role in information retrieval (Salton
and McGill, 1983). Salton and Yang experimentally
verified the importance of within-document term fre-
quencies in their vector model (Salton and Yang, 1973).
[Problem 2] no term weighting for target
documents
In the PRW formulation, there is no factor of term
weighting for target documents (i.e., P(•1d)). Accord-
ing to Eq. (5), even if a term exists in a target docu-
ment, only the importance of the term in a category
(i.e., P(T = 11c)) is considered for overall probability.
Term weighting for target documents would also be
necessary for sophisticated information retrieval (Fuhr,
1989; Kwok, 1990).
[Problem 3] affected by having insufficient
training cases
In practical situations, the estimation of P(T =
1, 01c) is not always straightforward. Let us consider
the following case. In the training data, we are given
R documents that are assigned to c. Among them,
r documents have the term t. In this example, the
straightforward estimate of P(T = 11c) is &amp;quot;r/R.&amp;quot; If
&amp;quot;r = 0&amp;quot; (i.e., none of the documents in c has 0 and
the target document d contains the term t, g(cd) be-
comes -oo, which means that d is never categorized
into c. Robertson and Sparck Jones mentioned other
special cases like the above example (Robertson and
Sparck Jones, 1976). A well-known remedy for this
problem is to use &amp;quot;(r + 0.5)/(R + 1)&amp;quot; as the estimate
of P(T = 11c) (Robertson and Sparck Jones, 1976).
While various smoothing methods (Church and Gale,
1991; Jelinek, 1990) are also applicable to these situa-
tions and would be expected to work better, we used
the simple &amp;quot;add one&amp;quot; remedy in the following experi-
ments.
</bodyText>
<subsectionHeader confidence="0.999523">
2.2 Component Theory (CT)
</subsectionHeader>
<bodyText confidence="0.997558666666667">
To solve problems 1 and 2 of PRW, Kwok (1990)
stresses the assumption that a document consists of
terms. This theory is called the Component Theory
(CT).
To introduce within-document term frequencies (i.e.,
to solve problem 1), CT assumes that a document
is completely decomposed into its constituting terms.
Therefore, rather than counting the number of docu-
ments, as in PRW, CT counts the number of terms in
a document for probability estimation. This leads to
within-document term frequencies. Moreover, to in-
corporate term weighting for target documents (i.e., to
solve problem 2), CT defines g(cd) as the geometric
mean probabilities over components of the target doc-
ument d;
</bodyText>
<equation confidence="0.985084625">
P(d1c)P(T1c)
P(d) = [II — ]T • I
tEd P(TIC)
Following Kwok&apos;s derivation, g(c1d) becomes
g(c1d)= E = tld)(log TT ;
tEd
P(T 4E) P(c)
log P(T = t1E)) + log PM.
</equation>
<page confidence="0.976433">
163
</page>
<bodyText confidence="0.999922">
For precise derivation, refer to (Kwok, 1990).
Here, note that P(T = tld) and P(T = tic) represent
the frequency of a term t within a target document
d and that within a category c respectively. There-
fore, CT is not subject to problems 1 and 2. However,
problem 3 still affects CT. Furthermore, Fuhr (1989)
pointed out that transformation, as in Eq. (6), is not
monotonic of P(c1d). It follows then, that CT does not
satisfy the probabilistic ranking principle (PRP) any
more.
</bodyText>
<subsectionHeader confidence="0.849095">
2.3 Retrieval with Probabilistic Indexing
(RPI)
</subsectionHeader>
<bodyText confidence="0.9998933">
Fuhr (1989) solves problem 2 by assuming that a doc-
ument is probabilistically indexed by its term vectors.
This model is called Retrieval with Probabilistic Index-
ing (RPI).
In RPI, a document d has a binary vector
x = , TO where each component corresponds to
a term. Ti = 1 means that the document d contains
the term ti. X is defined as the set of all possible in-
dexings, where 1X1 = r. Conditioning P(c1d) for each
possible indexing gives
</bodyText>
<equation confidence="0.9897275">
P(c1d) = E P(cld, x)P(veld). (8)
ZEX
</equation>
<bodyText confidence="0.9958195">
By assuming conditional independence between c and
d given xi , and using Bayes&apos; theorem, Eq. (8) becomes,
</bodyText>
<equation confidence="0.95784925">
P(cld)= P(c)
P(x1c)P(2id) (9)
E
zEx P(x)
</equation>
<bodyText confidence="0.99963025">
Assuming that each term appears independently in a
target document d and in a document assigned to c,
Eq. (9) is rewritten as
As far as other problems are concerned, RPI still
problematic. In particular, because of problem 3,
P(cid) would become an illegitimate value. In our
experiments, as well as in Lewis&apos; experiments (1992),
P(c1d) ranges from 0 to more than 1010.
</bodyText>
<sectionHeader confidence="0.783888" genericHeader="method">
3 A Probabilistic Model Based on a
Single Random Variable with
Multiple Values (SVMV)
</sectionHeader>
<bodyText confidence="0.99955476">
In this section, we propose a new probabilistic model
for text categorization, and compare it to the previous
three models from several viewpoints. Our model is
very simple, but yet solves problems 1, 2, and 3 in
PRW.
Document representation of our model is basically
the same as CT, that is a document is a set of its
constituting terms. The major difference between our
model and others is the way of document characteriza-
tion through probabilities. While almost all previous
models assume that an event space for a document is
whether the document is indexed or not by a term2,
our model characterizes a document as random sam-
pling of a term from the term set that represents the
document. For example, an event &amp;quot;T = ti&amp;quot; means that
a randomly selected term from a document is ti. If we
want to emphasis indexing process like other models, it
is possible to interpret &amp;quot;T = ti&amp;quot; as a randomly selected
element from a document being indexed by the term
ti.
Formally, our model can be seen as modifying Fuhr&apos;s
derivation of P(c1d) by replacing an index vector with
a single random variable whose value is one of possi-
ble terms. Conditioning P(c1d) for each possible event
gives
</bodyText>
<equation confidence="0.999849166666667">
P(c1d) = P(c)ll(&apos; = lic)P(Ti
P(Ti = 1)
P(Ti = Oic)P(Ti = Old))-
-1-
P(Ti = 0)
P(cid) = E P(cid,T = ti)P(T = tild). (11)
</equation>
<bodyText confidence="0.983792083333334">
If we assume conditional independence between c and
(10) d, given T = ti, that is P(cid,T = ti) = P(cIT = ti),
we obtain
Here, all the probabilities are estimated from the train-
ing data using the same method described in Sec-
tion 2.1.
Since Eq. (10) includes the factor P(T = 1,0Id) as
well as P(T = 1, 01c), RPI takes into account term
weighting for target documents. While this in prin-
ciple solves problem 2, if we use a simple estimation
method counting the number of documents which have
a term, P(T = 1, 01d) reduces to 1 or 0 (i.e, binary,
not weighted). For example, when a target docu-
ment d has a term t, P(t = 11d) = 1 and when not,
P(T = lid) = 0. In the following experiments we used
this binary estimation method, but non binary esti-
mates could be used as in (Fuhr, 1989).
&apos;More precisely, P(cld, x) = P(clz) which assumes that
if we know x, information for c is independent of that for
d. This assumption sounds valid because x is a kind of
representation of d.
t,
All the probabilities in Eq. (13) can be estimated from
given training data based on the following definitions.
</bodyText>
<listItem confidence="0.9912955">
• P(T = tiJc) is the probability that a randomly
selected term in a document is ti, given that the
document is assigned to c. We used14-1,- as the
estimator. NCi is the frequency of the term ti in
the category c, and NC is the total frequency of
terms in c.
</listItem>
<footnote confidence="0.589742333333333">
2In section 2 explaining previous models, we simplified
&amp;quot;a document is indexed by a term&amp;quot; as &amp;quot;a document contains
a term&amp;quot; for ease of explanation.
</footnote>
<equation confidence="0.993964333333333">
p(cid) = E p(cIT = ti)P(T = tild). (12)
t,
Using Bayes&apos; theorem, this becomes
P(T = tilc)P(T = tild)
P(cid) = P(c) . (13)
P(T = ti)
</equation>
<page confidence="0.992904">
164
</page>
<listItem confidence="0.999707083333333">
• P(T = 111d) is the probability that a randomly
selected term in a target document d is ti. We
used MI- as the estimator. NDi is the frequency
of the term ti in the document d, and ND is the
total frequency of terms in d.
• P(T =ti) is the prior probability that a randomly
selected term in a randomly selected document is
ti. We used.1i1,11-1 as the estimator. AT,: is the fre-
quency of the term ti in the given training doc-
uments, and N is the total frequency of terms in
the training documents.
• P(c) is the prior probability that a randomly se-
</listItem>
<bodyText confidence="0.967117361111111">
lected document is categorized into c. We used
--a as the estimator. D, is the frequency of docu-
ments that is categorized to c in the given training
documents, and D is the frequency of documents
in the training documents.
Here, let us recall the three problems of PRW. Since
SVMIT&apos;s primitive probabilities are based on within-
document term frequencies, SVMV does not have prob-
lem 1. Furthermore, SVMV does not have problem 2
either because Eq. (13) includes a factor P(T =
which accomplishes term weighting for a target docu-
ment d.
For problem 3, let us reconsider the previous exam-
ple; R documents in the training data are categorized
into a category c, none of the R documents has term
ti, but a target document d does. If the straightfor-
ward estimate of P(Ti =11c) = 0 or P(T = 410= 0 is
adopted, the document d would never be categorized
into c in the previous models (PRW, CT, and RPI).
In SVMV, the probability P(cld) is much less affected
by such estimates. This is because P(c1d) in Eq. (13)
takes the sum of each term&apos;s weight. In this example,
the weight for ti is estimated to be 0 as in the other
models, but this little affect the total value of P(cid).
A similar argument applies to all other problems in
(Robertson and Sparck Jones, 1976) that are caused
by having insufficient training cases. SVMV is formally
proven not to suffer from the serious effects (like never
being assigned to a category or always being assigned
to a category) by having insufficient training cases. In
other words, SVMV can directly use the straightfor-
ward estimates. In addition, we experimentally verified
that the value of P(d1c) in SVMV is always a legitimate
value (i.e., 0 to 1) unlike in RPI.
Table 1 summarizes the characteristics of the four
probabilistic models.
</bodyText>
<tableCaption confidence="0.958301">
Table 1 Summary of the four probabilistic models
</tableCaption>
<figureCaption confidence="0.8717668">
PRW CT RPI SVMV
Problem 1 considered no yes no yes
Problem 2 considered no yes (yes) yes
Problem 3 considered no no no yes
PRP satisfied yes no yes yes
</figureCaption>
<bodyText confidence="0.99736075">
As illustrated in the table, SVMV has better character-
istics for text categorization compared to the previous
models. In the next section, we will experimentally
verify SVMV &apos;s superiority.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999976">
This section describes experiments conducted to eval-
uate the performance of our model (SVMV) compared
to the other three (PRW, CT, and RPI).
</bodyText>
<subsectionHeader confidence="0.998073">
4.1 Data and Preprocessing
</subsectionHeader>
<bodyText confidence="0.999635310344828">
A collection of Wall Street Journal (WSJ) full-text
news stories (Liberman, 1991)3 was used in the experi-
ments. We extracted all 12,380 articles from 1989/7/25
to 1989/11/2.
The WSJ articles from 1989 are indexed with 78
categories (topics). Articles having no category were
excluded. 8,907 articles remained; each having 1.94
categories on the average. The largest category is
&amp;quot;TENDER OFFERS, MERGERS, ACQUISITIONS (TNM)&amp;quot;
which encompassed 2,475 articles; the smallest one is
&amp;quot;RUBBER (RUB)&amp;quot;, assigned to only 2 articles. On the
average, one category is assigned to 443 articles.
All 8,907 articles were tagged by the Xerox Part-of-
Speech Tagger (Cutting et al., 1992)4. From the tagged
articles, we extracted the root words of nouns using
the &amp;quot;ispell&amp;quot; program&apos;. As a result, each article has a
set of root words representing it, and each element in
the set (i.e. root word of a noun) corresponds to a
term. We did not reduce the number of terms by using
stop words list or feature selection method, etc. The
number of terms amounts to 32,975.
Before the experiments, we divided 8,907 articles
into two sets; one for training (i.e., for probability
estimation), and the other for testing. The division
was made according to chronology. All articles that
appeared from 1989/7/25 to 1989/9/29 went into a
training set of 5,820 documents, and all articles from
1989/10/2 to 1989/11/2 went into a test set of 3,087
documents.
</bodyText>
<subsectionHeader confidence="0.998214">
4.2 Category Assignment Strategies
</subsectionHeader>
<bodyText confidence="0.8258583">
In the experiments, the probabilities, P(c), P(Ti =
11c), P(T = tilc), and so forth, were estimated from
the 5,820 training documents, as described in the pre-
vious sections. Using these estimates, we calculated the
posterior probability (P(c1d)) for each document (d) of
the 3,087 test documents and each of the 78 categories
3We used &amp;quot;ACL/DCI (September 1991)&amp;quot; CD-ROM
which is distributed from the Linguistic Data Consortium
(LDC). For more details, please contact Mark Liberman
(mylaunagi.cis.upenn.edu).
</bodyText>
<footnote confidence="0.984314285714286">
4The xerox part-of-speech tagger version 1.0 is available
via anonymous FTP from the host parcf tp. xerox .com in
the directory pub/tagger.
5Ispell is a program for correcting English spelling. We
used the &amp;quot;ispell version 3.0&amp;quot; which is available via anony-
mous FTP from the host ftp.cs.ucla.edu in the directory
pub/ispell.
</footnote>
<page confidence="0.997976">
165
</page>
<bodyText confidence="0.999976736842105">
(c). The four probabilistic models are compared in this
calculation.
There are several strategies for assigning categories
to a document based on the probability P(c1d). The
simplest one is the k-per-doc strategy (Field, 1975) that
assigns the top k categories to each document. A more
sophisticated one is the probability threshold strategy,
in which all the categories above a user-defined thresh-
old are assigned to a document.
Lewis proposed the proportional assignment strat-
egy based on the probabilistic ranking principle (Lewis,
1992). Each category is assigned to its top scoring doc-
uments in proportion to the number of times the cat-
egory was assigned in the training data. For example,
a category assigned to 2% of the training documents
would be assigned to the top scoring 0.2% of the test
documents if the proportionality constant was 0.1, or
to 10% of the test documents if the proportionality
constant was 5.0.
</bodyText>
<subsectionHeader confidence="0.977352">
4.3 Results and Discussions
</subsectionHeader>
<bodyText confidence="0.979956384615385">
By using a category assignment strategy, several cat-
egories me assigned to each test document. The
best known measures for evaluating text categoriza-
tion models are recall and precision, calculated by the
following equations (Lewis, 1992):
the number of categories that are
R correctly assigned to documents
ecall =
the number of categories that should be&apos;
assigned to documents
the number of categories that are
correctly assigned to documents
Precision —
the number of categories that are •
assigned to documents
Note that recall and precision have somewhat mutually
exclusive characteristics. To raise the recall value, one
can simply assign many categories to each document.
However, this leads to a degradation in precision; i.e.,
almost all the assigned categories are false. A breakeven
point might be used to summarize the balance between
recall and precision, the point at which they are equal.
For each strategy, we calculated breakeven points by
using the four probabilistic models. Table 2 shows the
best breakeven points identified for the three strategies
along with the used models.
</bodyText>
<tableCaption confidence="0.7558095">
Table 2 Best breakeven points for three category
assignment strategies
</tableCaption>
<table confidence="0.78615475">
Breakeven Pts.
Prop. assignment 0.63 (by SVMV)
Prob. thresholding 0.47 (by SVMV)
k-per-doc 0.43 (by SVMV)
</table>
<bodyText confidence="0.999533866666667">
From Table 2, we find that SVMV with proportional
assignment gives the best result (0.63). The superior-
ity of proportional assignment over the other strategies
has already been reported by Lewis (1992). Our ex-
periment verified Lewis&apos; assumption. In addition, for
any of the three strategies, SVMV gives the highest
breakeven point among the four probabilistic models.
Figure 1 shows the recall/precision trade off for
the four probabilistic models with proportional assign-
ment strategy. As a reference, the recall/precision
curve of a well-known vector model (Salton and Yang,
1973) (&amp;quot;TF4DF&amp;quot;)6 is also presented. Table 3 lists the
breakeven point for each model. All the breakeven
points were obtained when proportionality constant
was about 1.0.
</bodyText>
<figureCaption confidence="0.8062165">
Fig. 1 Recall/precision with proportional
assignment strategy
</figureCaption>
<table confidence="0.3735095">
01 02 02 02 02 02 017 02 09
Recall
</table>
<tableCaption confidence="0.960698">
Table 3 Breakeven points with proportional
assignment strategy
</tableCaption>
<table confidence="0.923210142857143">
Breakeven Pts.
SVMV 0.63
CT 0.60
RPI 0.51
PRW 0.53
TF.IDF 0.48
From Figure 1 and Table 3, we can see that:
</table>
<listItem confidence="0.978515666666667">
• as far as this dataset is concerned, SVMV with
proportional assignment strategy gives the best re-
sult among the four probabilistic models,
• the models that consider within-document term
frequencies (SVMV, CT) are better than those
that do not (PRW, RPI),
</listItem>
<bodyText confidence="0.970893666666667">
61n the model we used, each element of document vector
is the &amp;quot;term frequency&amp;quot; multiplied by the &amp;quot;inverted docu-
ment frequency.&amp;quot; Similarity between every pair of vectors
is measured by cosine. Note that this is the simplest version
of TF.IDF model, and there has been many improvements
which we did not consider in the experiments.
</bodyText>
<page confidence="0.994019">
166
</page>
<listItem confidence="0.993596">
• the models that consider term weighting for tar-
get documents (SVMV, CT) are better than those
that do not (PRW, (RPI)), and
• the models that are less affected by having insuffi-
cient training cases (SVMV) are better than those
that are (CT, RPI, PRW).
</listItem>
<sectionHeader confidence="0.99301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.95774944">
We have proposed a new probabilistic model for text
categorization. Compared to previous models, our
model has the following advantages; 1) it considers
within document term frequencies, 2) considers term
weighting for target documents, and 3) is less affected
by having insufficient training cases. We have also pro-
vided empirical results verifying our model&apos;s superior-
ity over the others in the task of categorizing news
articles from the &amp;quot;Wall Street Journal.&amp;quot;
There are several directions along which this work
could be extended.
• We have to compare our probabilistic model
to other non probabilistic models like decision
tree/rule based models, one of which has recently
been reported to be promising (Apte et al., 1994).
• While we used simple document representation in
which a document is defined as a set of nouns,
there could be considered several improvements,
such as using phrasal information (Lewis, 1992),
clustering terms (Sparck Jones, 1973), reducing
the number of features by using local dictio-
nary (Apte et al., 1994), etc.
• We are incorporating our probabilistic model into
cluster-based text categorization that offers an ef-
ficient and effective search strategy.
</bodyText>
<sectionHeader confidence="0.9984" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999964333333333">
The authors are grateful to Hiroshi Motoda for bene-
ficial discussions, and would like to thank the anony-
mous reviewers for their useful comments.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999662888888889">
C. Apte, F. Damerau, and S. M. Weiss. 1994. Au-
tomated learning of decision rules for text catego-
rization. ACM Transactions on Office Information
Systems. (to appear).
K. W. Church and W. A. Gale. 1991. A comparison
of the enhanced Good-Turing and deleted estima-
tion methods for estimating probabilities of English
bigrams. Computer Speech and Language, 5:19-54.
W. B. Croft. 1981. Document representation in prob-
abilistic models of information retrieval. Journal
of the American Society for Information Science,
32(6):451-457.
D. Cutting, J. Kupiec, J. Pedersen, and P. Sibun. 1992.
A practical part-of-speech tagger. In In Proc. of the
Third Conference on Applied Natural Language Pro-
cessing.
B. Field. 1975. Towards automatic indexing: Au-
tomatic assignment of controlled language indexing
and classification from free indexing. Journal of
Documentation, 31(4):246-265.
N. Fuhr. 1989. Models for retrieval with probabilis-
tic indexing. Information Processing &amp; Retrieval,
25(455-72.
F. Jelinek. 1990. Self-organized language modeling
for speech recognition. In A. Waibel and K. Lee,
editors, Readings in Speech Recognition, pages 450-
506. Morgan Kaufmann.
K. L. Kwok. 1990. Experiments with a component
theory of probabilistic information retrieval based on
single terms as document components. ACM Thins-
actions on Information Systems, 8(4):363-386.
D. D. Lewis. 1992. An evaluation of phrasal and clus-
tered representation on a text categorization task. In
Proceedins of the Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 37-50.
M. Liberman, editor. 1991. A CL/DCI (CD-ROM).
Association for Computational Linguistics Data
Collection Initiative, University of Pennsylvania,
September.
S. E. Robertson and K. Spank Jones. 1976. Relevance
weighting of search terms. Journal of the American
Society for Information Science, 27:129-146.
S. E. Robertson. 1977. The probability ranking prin-
ciple in IR. Journal of Documentation, 33:294-304.
G. Salton and M. J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill Pub-
lishing Company.
G. Salton and C. S. Yang. 1973. On the specification
of term values in automatic indexing. Journal of
Documentation, 29(4):351-372.
K. Sparck Jones. 1972. A statistical interpretation
of term specificity and its application in retrieval.
Journal of Documentation, 28(411-21.
K. Sparck Jones. 1973. Collection properties influenc-
ing automatic term classification performance. In-
formation Storage and Retrieval, 9:499-513.
S. K. M. Wong and Y. Y. Yao. 1989. A probability
distribution model for information retrieval. Infor-
mation Processing &amp; Management, 25(1):39-53.
C. T. Yu, W. Meng, and S. Park. 1989. A frame-
work for effective retrieval. ACM Transactions on
Database Systems, 14(2):147-167.
</reference>
<page confidence="0.997752">
167
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.252573">
<title confidence="0.9996">A Probabilistic Model for Text Categorization: Based on a Single Random Variable with Multiple Values</title>
<author confidence="0.95451">Makoto IWAYA M A</author>
<affiliation confidence="0.9917665">Advanced Research Laboratory Hitachi Ltd.</affiliation>
<address confidence="0.871355">HATOYAMA SAITAMA 350-03, JAPAN</address>
<email confidence="0.976942">iwayamaeharl.hitachi.co.jp</email>
<abstract confidence="0.978973733333333">Text categorization is the classification of documents with respect to a set of predefined categories. In this paper, we propose a new probabilistic model for text categothat is based on a random with Multiple Values (SVMV). Compared to previous probabilistic models, our model has the following advantages; 1) it considers within-document term frequencies, 2) considers term weighting for target documents, and 3) is less affected by having insufficient training cases. We verify our model&apos;s superiority over the others in the task of categorizing news articles from the &amp;quot;Wall Street</abstract>
<intro confidence="0.421731">Journal&amp;quot;.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Apte</author>
<author>F Damerau</author>
<author>S M Weiss</author>
</authors>
<title>Automated learning of decision rules for text categorization.</title>
<date>1994</date>
<journal>ACM Transactions on Office Information Systems.</journal>
<note>(to appear).</note>
<contexts>
<context position="22992" citStr="Apte et al., 1994" startWordPosition="3863" endWordPosition="3866">as the following advantages; 1) it considers within document term frequencies, 2) considers term weighting for target documents, and 3) is less affected by having insufficient training cases. We have also provided empirical results verifying our model&apos;s superiority over the others in the task of categorizing news articles from the &amp;quot;Wall Street Journal.&amp;quot; There are several directions along which this work could be extended. • We have to compare our probabilistic model to other non probabilistic models like decision tree/rule based models, one of which has recently been reported to be promising (Apte et al., 1994). • While we used simple document representation in which a document is defined as a set of nouns, there could be considered several improvements, such as using phrasal information (Lewis, 1992), clustering terms (Sparck Jones, 1973), reducing the number of features by using local dictionary (Apte et al., 1994), etc. • We are incorporating our probabilistic model into cluster-based text categorization that offers an efficient and effective search strategy. Acknowledgments The authors are grateful to Hiroshi Motoda for beneficial discussions, and would like to thank the anonymous reviewers for </context>
</contexts>
<marker>Apte, Damerau, Weiss, 1994</marker>
<rawString>C. Apte, F. Damerau, and S. M. Weiss. 1994. Automated learning of decision rules for text categorization. ACM Transactions on Office Information Systems. (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>W A Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--19</pages>
<contexts>
<context position="7424" citStr="Church and Gale, 1991" startWordPosition="1206" endWordPosition="1209">given R documents that are assigned to c. Among them, r documents have the term t. In this example, the straightforward estimate of P(T = 11c) is &amp;quot;r/R.&amp;quot; If &amp;quot;r = 0&amp;quot; (i.e., none of the documents in c has 0 and the target document d contains the term t, g(cd) becomes -oo, which means that d is never categorized into c. Robertson and Sparck Jones mentioned other special cases like the above example (Robertson and Sparck Jones, 1976). A well-known remedy for this problem is to use &amp;quot;(r + 0.5)/(R + 1)&amp;quot; as the estimate of P(T = 11c) (Robertson and Sparck Jones, 1976). While various smoothing methods (Church and Gale, 1991; Jelinek, 1990) are also applicable to these situations and would be expected to work better, we used the simple &amp;quot;add one&amp;quot; remedy in the following experiments. 2.2 Component Theory (CT) To solve problems 1 and 2 of PRW, Kwok (1990) stresses the assumption that a document consists of terms. This theory is called the Component Theory (CT). To introduce within-document term frequencies (i.e., to solve problem 1), CT assumes that a document is completely decomposed into its constituting terms. Therefore, rather than counting the number of documents, as in PRW, CT counts the number of terms in a d</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>K. W. Church and W. A. Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5:19-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Croft</author>
</authors>
<title>Document representation in probabilistic models of information retrieval.</title>
<date>1981</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>32--6</pages>
<contexts>
<context position="2306" citStr="Croft, 1981" startWordPosition="346" endWordPosition="347">rds for categorization, it is very difficult for human experts to assign categories consistently and efficiently to large numbers of daily incoming documents. The purpose of this paper is to propose a new probabilistic model for automatic text categorization. Takenobu TOKUNAGA Department of Computer Science Tokyo Institute of Technology 2-12-1, OOKAYAMA, MEGURO-KU TOKYO 152, JAPAN takeecs.titech.ac.jp While many text categorization models have been proposed so far, in this paper, we concentrate on the probabilistic models (Robertson and Sparck Jones, 1976; Kwok, 1990; Fuhr, 1989; Lewis, 1992; Croft, 1981; Wong and Yao, 1989; Yu et al., 1989) because these models have solid formal grounding in probability theory. Section 2 quickly reviews the probabilistic models and lists their individual problems. In section 3, we propose a new probabilistic model based on a Single random Variable with Multiple Values (SVMV). Our model is very simple, but solves some problems of the previous models. In section 4, we verify our model&apos;s superiority over the others through experiments in which we categorize &amp;quot;Wall Street Journal&amp;quot; articles. 2 A Brief Survey of Probabilistic Text Categorization In this section, we</context>
</contexts>
<marker>Croft, 1981</marker>
<rawString>W. B. Croft. 1981. Document representation in probabilistic models of information retrieval. Journal of the American Society for Information Science, 32(6):451-457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cutting</author>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>P Sibun</author>
</authors>
<title>A practical part-of-speech tagger. In</title>
<date>1992</date>
<booktitle>In Proc. of the Third Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="16541" citStr="Cutting et al., 1992" startWordPosition="2829" endWordPosition="2832">-text news stories (Liberman, 1991)3 was used in the experiments. We extracted all 12,380 articles from 1989/7/25 to 1989/11/2. The WSJ articles from 1989 are indexed with 78 categories (topics). Articles having no category were excluded. 8,907 articles remained; each having 1.94 categories on the average. The largest category is &amp;quot;TENDER OFFERS, MERGERS, ACQUISITIONS (TNM)&amp;quot; which encompassed 2,475 articles; the smallest one is &amp;quot;RUBBER (RUB)&amp;quot;, assigned to only 2 articles. On the average, one category is assigned to 443 articles. All 8,907 articles were tagged by the Xerox Part-ofSpeech Tagger (Cutting et al., 1992)4. From the tagged articles, we extracted the root words of nouns using the &amp;quot;ispell&amp;quot; program&apos;. As a result, each article has a set of root words representing it, and each element in the set (i.e. root word of a noun) corresponds to a term. We did not reduce the number of terms by using stop words list or feature selection method, etc. The number of terms amounts to 32,975. Before the experiments, we divided 8,907 articles into two sets; one for training (i.e., for probability estimation), and the other for testing. The division was made according to chronology. All articles that appeared from </context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>D. Cutting, J. Kupiec, J. Pedersen, and P. Sibun. 1992. A practical part-of-speech tagger. In In Proc. of the Third Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Field</author>
</authors>
<title>Towards automatic indexing: Automatic assignment of controlled language indexing and classification from free indexing.</title>
<date>1975</date>
<journal>Journal of Documentation,</journal>
<pages>31--4</pages>
<contexts>
<context position="18390" citStr="Field, 1975" startWordPosition="3126" endWordPosition="3127">s, please contact Mark Liberman (mylaunagi.cis.upenn.edu). 4The xerox part-of-speech tagger version 1.0 is available via anonymous FTP from the host parcf tp. xerox .com in the directory pub/tagger. 5Ispell is a program for correcting English spelling. We used the &amp;quot;ispell version 3.0&amp;quot; which is available via anonymous FTP from the host ftp.cs.ucla.edu in the directory pub/ispell. 165 (c). The four probabilistic models are compared in this calculation. There are several strategies for assigning categories to a document based on the probability P(c1d). The simplest one is the k-per-doc strategy (Field, 1975) that assigns the top k categories to each document. A more sophisticated one is the probability threshold strategy, in which all the categories above a user-defined threshold are assigned to a document. Lewis proposed the proportional assignment strategy based on the probabilistic ranking principle (Lewis, 1992). Each category is assigned to its top scoring documents in proportion to the number of times the category was assigned in the training data. For example, a category assigned to 2% of the training documents would be assigned to the top scoring 0.2% of the test documents if the proporti</context>
</contexts>
<marker>Field, 1975</marker>
<rawString>B. Field. 1975. Towards automatic indexing: Automatic assignment of controlled language indexing and classification from free indexing. Journal of Documentation, 31(4):246-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fuhr</author>
</authors>
<title>Models for retrieval with probabilistic indexing.</title>
<date>1989</date>
<booktitle>Information Processing &amp; Retrieval,</booktitle>
<pages>25--455</pages>
<contexts>
<context position="2280" citStr="Fuhr, 1989" startWordPosition="342" endWordPosition="343">e certain rules or standards for categorization, it is very difficult for human experts to assign categories consistently and efficiently to large numbers of daily incoming documents. The purpose of this paper is to propose a new probabilistic model for automatic text categorization. Takenobu TOKUNAGA Department of Computer Science Tokyo Institute of Technology 2-12-1, OOKAYAMA, MEGURO-KU TOKYO 152, JAPAN takeecs.titech.ac.jp While many text categorization models have been proposed so far, in this paper, we concentrate on the probabilistic models (Robertson and Sparck Jones, 1976; Kwok, 1990; Fuhr, 1989; Lewis, 1992; Croft, 1981; Wong and Yao, 1989; Yu et al., 1989) because these models have solid formal grounding in probability theory. Section 2 quickly reviews the probabilistic models and lists their individual problems. In section 3, we propose a new probabilistic model based on a Single random Variable with Multiple Values (SVMV). Our model is very simple, but solves some problems of the previous models. In section 4, we verify our model&apos;s superiority over the others through experiments in which we categorize &amp;quot;Wall Street Journal&amp;quot; articles. 2 A Brief Survey of Probabilistic Text Categori</context>
<context position="3863" citStr="Fuhr, 1989" startWordPosition="593" endWordPosition="594"> calculated. Usually, a set of categories is defined beforehand. For every document di, probability P(cldi) is calculated and all the documents are ranked in decreasing order according to their probabilities. The larger P(cldi) a document di has, the more probably it will be categorized into category c. This is called the Probabilistic Ranking Principle (PRP) (Robertson, 1977). Several strategies can be used to assign categories to a document based on PRP (Lewis, 1992). There are several ways to calculate P(c1d). Three representatives are (Robertson and Sparck Jones, 1976), (Kwok, 1990), and (Fuhr, 1989). 2.1 Probabilistic Relevance Weighting (PRW) Robertson and Sparck Jones (1976) make use of the well-known logistic (or log-odds) transformation of the 162 probability P(c1d). P(c1d) g(c1d)= log (2) PCeld) where E means &amp;quot;not c&amp;quot;, that is &amp;quot;a document is not categorized into c.&amp;quot; Since this is a monotonic transformation of P(c1d), PRP is still satisfied after transformation. Using Bayes&apos; theorem, Eq. (2) becomes 1c d) =logP(d1c) -1- log PP((TO). (3) P(dI) Here, P(c) is the prior probability that a document is categorized into c. This is estimated from given training data, i.e., the number of docum</context>
<context position="6576" citStr="Fuhr, 1989" startWordPosition="1058" endWordPosition="1059">tion retrieval (Salton and McGill, 1983). Salton and Yang experimentally verified the importance of within-document term frequencies in their vector model (Salton and Yang, 1973). [Problem 2] no term weighting for target documents In the PRW formulation, there is no factor of term weighting for target documents (i.e., P(•1d)). According to Eq. (5), even if a term exists in a target document, only the importance of the term in a category (i.e., P(T = 11c)) is considered for overall probability. Term weighting for target documents would also be necessary for sophisticated information retrieval (Fuhr, 1989; Kwok, 1990). [Problem 3] affected by having insufficient training cases In practical situations, the estimation of P(T = 1, 01c) is not always straightforward. Let us consider the following case. In the training data, we are given R documents that are assigned to c. Among them, r documents have the term t. In this example, the straightforward estimate of P(T = 11c) is &amp;quot;r/R.&amp;quot; If &amp;quot;r = 0&amp;quot; (i.e., none of the documents in c has 0 and the target document d contains the term t, g(cd) becomes -oo, which means that d is never categorized into c. Robertson and Sparck Jones mentioned other special case</context>
<context position="8759" citStr="Fuhr (1989)" startWordPosition="1438" endWordPosition="1439">for target documents (i.e., to solve problem 2), CT defines g(cd) as the geometric mean probabilities over components of the target document d; P(d1c)P(T1c) P(d) = [II — ]T • I tEd P(TIC) Following Kwok&apos;s derivation, g(c1d) becomes g(c1d)= E = tld)(log TT ; tEd P(T 4E) P(c) log P(T = t1E)) + log PM. 163 For precise derivation, refer to (Kwok, 1990). Here, note that P(T = tld) and P(T = tic) represent the frequency of a term t within a target document d and that within a category c respectively. Therefore, CT is not subject to problems 1 and 2. However, problem 3 still affects CT. Furthermore, Fuhr (1989) pointed out that transformation, as in Eq. (6), is not monotonic of P(c1d). It follows then, that CT does not satisfy the probabilistic ranking principle (PRP) any more. 2.3 Retrieval with Probabilistic Indexing (RPI) Fuhr (1989) solves problem 2 by assuming that a document is probabilistically indexed by its term vectors. This model is called Retrieval with Probabilistic Indexing (RPI). In RPI, a document d has a binary vector x = , TO where each component corresponds to a term. Ti = 1 means that the document d contains the term ti. X is defined as the set of all possible indexings, where 1X</context>
<context position="12138" citStr="Fuhr, 1989" startWordPosition="2051" endWordPosition="2052"> training data using the same method described in Section 2.1. Since Eq. (10) includes the factor P(T = 1,0Id) as well as P(T = 1, 01c), RPI takes into account term weighting for target documents. While this in principle solves problem 2, if we use a simple estimation method counting the number of documents which have a term, P(T = 1, 01d) reduces to 1 or 0 (i.e, binary, not weighted). For example, when a target document d has a term t, P(t = 11d) = 1 and when not, P(T = lid) = 0. In the following experiments we used this binary estimation method, but non binary estimates could be used as in (Fuhr, 1989). &apos;More precisely, P(cld, x) = P(clz) which assumes that if we know x, information for c is independent of that for d. This assumption sounds valid because x is a kind of representation of d. t, All the probabilities in Eq. (13) can be estimated from given training data based on the following definitions. • P(T = tiJc) is the probability that a randomly selected term in a document is ti, given that the document is assigned to c. We used14-1,- as the estimator. NCi is the frequency of the term ti in the category c, and NC is the total frequency of terms in c. 2In section 2 explaining previous m</context>
</contexts>
<marker>Fuhr, 1989</marker>
<rawString>N. Fuhr. 1989. Models for retrieval with probabilistic indexing. Information Processing &amp; Retrieval, 25(455-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Self-organized language modeling for speech recognition.</title>
<date>1990</date>
<booktitle>Readings in Speech Recognition,</booktitle>
<pages>450--506</pages>
<editor>In A. Waibel and K. Lee, editors,</editor>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="7440" citStr="Jelinek, 1990" startWordPosition="1210" endWordPosition="1211">are assigned to c. Among them, r documents have the term t. In this example, the straightforward estimate of P(T = 11c) is &amp;quot;r/R.&amp;quot; If &amp;quot;r = 0&amp;quot; (i.e., none of the documents in c has 0 and the target document d contains the term t, g(cd) becomes -oo, which means that d is never categorized into c. Robertson and Sparck Jones mentioned other special cases like the above example (Robertson and Sparck Jones, 1976). A well-known remedy for this problem is to use &amp;quot;(r + 0.5)/(R + 1)&amp;quot; as the estimate of P(T = 11c) (Robertson and Sparck Jones, 1976). While various smoothing methods (Church and Gale, 1991; Jelinek, 1990) are also applicable to these situations and would be expected to work better, we used the simple &amp;quot;add one&amp;quot; remedy in the following experiments. 2.2 Component Theory (CT) To solve problems 1 and 2 of PRW, Kwok (1990) stresses the assumption that a document consists of terms. This theory is called the Component Theory (CT). To introduce within-document term frequencies (i.e., to solve problem 1), CT assumes that a document is completely decomposed into its constituting terms. Therefore, rather than counting the number of documents, as in PRW, CT counts the number of terms in a document for prob</context>
</contexts>
<marker>Jelinek, 1990</marker>
<rawString>F. Jelinek. 1990. Self-organized language modeling for speech recognition. In A. Waibel and K. Lee, editors, Readings in Speech Recognition, pages 450-506. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Kwok</author>
</authors>
<title>Experiments with a component theory of probabilistic information retrieval based on single terms as document components.</title>
<date>1990</date>
<journal>ACM Thinsactions on Information Systems,</journal>
<pages>8--4</pages>
<contexts>
<context position="2268" citStr="Kwok, 1990" startWordPosition="340" endWordPosition="341"> there may be certain rules or standards for categorization, it is very difficult for human experts to assign categories consistently and efficiently to large numbers of daily incoming documents. The purpose of this paper is to propose a new probabilistic model for automatic text categorization. Takenobu TOKUNAGA Department of Computer Science Tokyo Institute of Technology 2-12-1, OOKAYAMA, MEGURO-KU TOKYO 152, JAPAN takeecs.titech.ac.jp While many text categorization models have been proposed so far, in this paper, we concentrate on the probabilistic models (Robertson and Sparck Jones, 1976; Kwok, 1990; Fuhr, 1989; Lewis, 1992; Croft, 1981; Wong and Yao, 1989; Yu et al., 1989) because these models have solid formal grounding in probability theory. Section 2 quickly reviews the probabilistic models and lists their individual problems. In section 3, we propose a new probabilistic model based on a Single random Variable with Multiple Values (SVMV). Our model is very simple, but solves some problems of the previous models. In section 4, we verify our model&apos;s superiority over the others through experiments in which we categorize &amp;quot;Wall Street Journal&amp;quot; articles. 2 A Brief Survey of Probabilistic T</context>
<context position="3845" citStr="Kwok, 1990" startWordPosition="590" endWordPosition="591">category c&amp;quot; (1) is calculated. Usually, a set of categories is defined beforehand. For every document di, probability P(cldi) is calculated and all the documents are ranked in decreasing order according to their probabilities. The larger P(cldi) a document di has, the more probably it will be categorized into category c. This is called the Probabilistic Ranking Principle (PRP) (Robertson, 1977). Several strategies can be used to assign categories to a document based on PRP (Lewis, 1992). There are several ways to calculate P(c1d). Three representatives are (Robertson and Sparck Jones, 1976), (Kwok, 1990), and (Fuhr, 1989). 2.1 Probabilistic Relevance Weighting (PRW) Robertson and Sparck Jones (1976) make use of the well-known logistic (or log-odds) transformation of the 162 probability P(c1d). P(c1d) g(c1d)= log (2) PCeld) where E means &amp;quot;not c&amp;quot;, that is &amp;quot;a document is not categorized into c.&amp;quot; Since this is a monotonic transformation of P(c1d), PRP is still satisfied after transformation. Using Bayes&apos; theorem, Eq. (2) becomes 1c d) =logP(d1c) -1- log PP((TO). (3) P(dI) Here, P(c) is the prior probability that a document is categorized into c. This is estimated from given training data, i.e., t</context>
<context position="6589" citStr="Kwok, 1990" startWordPosition="1060" endWordPosition="1061">al (Salton and McGill, 1983). Salton and Yang experimentally verified the importance of within-document term frequencies in their vector model (Salton and Yang, 1973). [Problem 2] no term weighting for target documents In the PRW formulation, there is no factor of term weighting for target documents (i.e., P(•1d)). According to Eq. (5), even if a term exists in a target document, only the importance of the term in a category (i.e., P(T = 11c)) is considered for overall probability. Term weighting for target documents would also be necessary for sophisticated information retrieval (Fuhr, 1989; Kwok, 1990). [Problem 3] affected by having insufficient training cases In practical situations, the estimation of P(T = 1, 01c) is not always straightforward. Let us consider the following case. In the training data, we are given R documents that are assigned to c. Among them, r documents have the term t. In this example, the straightforward estimate of P(T = 11c) is &amp;quot;r/R.&amp;quot; If &amp;quot;r = 0&amp;quot; (i.e., none of the documents in c has 0 and the target document d contains the term t, g(cd) becomes -oo, which means that d is never categorized into c. Robertson and Sparck Jones mentioned other special cases like the ab</context>
<context position="8498" citStr="Kwok, 1990" startWordPosition="1389" endWordPosition="1390">sed into its constituting terms. Therefore, rather than counting the number of documents, as in PRW, CT counts the number of terms in a document for probability estimation. This leads to within-document term frequencies. Moreover, to incorporate term weighting for target documents (i.e., to solve problem 2), CT defines g(cd) as the geometric mean probabilities over components of the target document d; P(d1c)P(T1c) P(d) = [II — ]T • I tEd P(TIC) Following Kwok&apos;s derivation, g(c1d) becomes g(c1d)= E = tld)(log TT ; tEd P(T 4E) P(c) log P(T = t1E)) + log PM. 163 For precise derivation, refer to (Kwok, 1990). Here, note that P(T = tld) and P(T = tic) represent the frequency of a term t within a target document d and that within a category c respectively. Therefore, CT is not subject to problems 1 and 2. However, problem 3 still affects CT. Furthermore, Fuhr (1989) pointed out that transformation, as in Eq. (6), is not monotonic of P(c1d). It follows then, that CT does not satisfy the probabilistic ranking principle (PRP) any more. 2.3 Retrieval with Probabilistic Indexing (RPI) Fuhr (1989) solves problem 2 by assuming that a document is probabilistically indexed by its term vectors. This model is</context>
</contexts>
<marker>Kwok, 1990</marker>
<rawString>K. L. Kwok. 1990. Experiments with a component theory of probabilistic information retrieval based on single terms as document components. ACM Thinsactions on Information Systems, 8(4):363-386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
</authors>
<title>An evaluation of phrasal and clustered representation on a text categorization task.</title>
<date>1992</date>
<booktitle>In Proceedins of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>37--50</pages>
<contexts>
<context position="2293" citStr="Lewis, 1992" startWordPosition="344" endWordPosition="345">les or standards for categorization, it is very difficult for human experts to assign categories consistently and efficiently to large numbers of daily incoming documents. The purpose of this paper is to propose a new probabilistic model for automatic text categorization. Takenobu TOKUNAGA Department of Computer Science Tokyo Institute of Technology 2-12-1, OOKAYAMA, MEGURO-KU TOKYO 152, JAPAN takeecs.titech.ac.jp While many text categorization models have been proposed so far, in this paper, we concentrate on the probabilistic models (Robertson and Sparck Jones, 1976; Kwok, 1990; Fuhr, 1989; Lewis, 1992; Croft, 1981; Wong and Yao, 1989; Yu et al., 1989) because these models have solid formal grounding in probability theory. Section 2 quickly reviews the probabilistic models and lists their individual problems. In section 3, we propose a new probabilistic model based on a Single random Variable with Multiple Values (SVMV). Our model is very simple, but solves some problems of the previous models. In section 4, we verify our model&apos;s superiority over the others through experiments in which we categorize &amp;quot;Wall Street Journal&amp;quot; articles. 2 A Brief Survey of Probabilistic Text Categorization In thi</context>
<context position="3725" citStr="Lewis, 1992" startWordPosition="573" endWordPosition="574">ward. In a model of probabilistic text categorization, P(cid) = &amp;quot;the probability that a document d is categorized into a category c&amp;quot; (1) is calculated. Usually, a set of categories is defined beforehand. For every document di, probability P(cldi) is calculated and all the documents are ranked in decreasing order according to their probabilities. The larger P(cldi) a document di has, the more probably it will be categorized into category c. This is called the Probabilistic Ranking Principle (PRP) (Robertson, 1977). Several strategies can be used to assign categories to a document based on PRP (Lewis, 1992). There are several ways to calculate P(c1d). Three representatives are (Robertson and Sparck Jones, 1976), (Kwok, 1990), and (Fuhr, 1989). 2.1 Probabilistic Relevance Weighting (PRW) Robertson and Sparck Jones (1976) make use of the well-known logistic (or log-odds) transformation of the 162 probability P(c1d). P(c1d) g(c1d)= log (2) PCeld) where E means &amp;quot;not c&amp;quot;, that is &amp;quot;a document is not categorized into c.&amp;quot; Since this is a monotonic transformation of P(c1d), PRP is still satisfied after transformation. Using Bayes&apos; theorem, Eq. (2) becomes 1c d) =logP(d1c) -1- log PP((TO). (3) P(dI) Here, </context>
<context position="18704" citStr="Lewis, 1992" startWordPosition="3174" endWordPosition="3175">mous FTP from the host ftp.cs.ucla.edu in the directory pub/ispell. 165 (c). The four probabilistic models are compared in this calculation. There are several strategies for assigning categories to a document based on the probability P(c1d). The simplest one is the k-per-doc strategy (Field, 1975) that assigns the top k categories to each document. A more sophisticated one is the probability threshold strategy, in which all the categories above a user-defined threshold are assigned to a document. Lewis proposed the proportional assignment strategy based on the probabilistic ranking principle (Lewis, 1992). Each category is assigned to its top scoring documents in proportion to the number of times the category was assigned in the training data. For example, a category assigned to 2% of the training documents would be assigned to the top scoring 0.2% of the test documents if the proportionality constant was 0.1, or to 10% of the test documents if the proportionality constant was 5.0. 4.3 Results and Discussions By using a category assignment strategy, several categories me assigned to each test document. The best known measures for evaluating text categorization models are recall and precision, </context>
<context position="20593" citStr="Lewis (1992)" startWordPosition="3476" endWordPosition="3477">ion, the point at which they are equal. For each strategy, we calculated breakeven points by using the four probabilistic models. Table 2 shows the best breakeven points identified for the three strategies along with the used models. Table 2 Best breakeven points for three category assignment strategies Breakeven Pts. Prop. assignment 0.63 (by SVMV) Prob. thresholding 0.47 (by SVMV) k-per-doc 0.43 (by SVMV) From Table 2, we find that SVMV with proportional assignment gives the best result (0.63). The superiority of proportional assignment over the other strategies has already been reported by Lewis (1992). Our experiment verified Lewis&apos; assumption. In addition, for any of the three strategies, SVMV gives the highest breakeven point among the four probabilistic models. Figure 1 shows the recall/precision trade off for the four probabilistic models with proportional assignment strategy. As a reference, the recall/precision curve of a well-known vector model (Salton and Yang, 1973) (&amp;quot;TF4DF&amp;quot;)6 is also presented. Table 3 lists the breakeven point for each model. All the breakeven points were obtained when proportionality constant was about 1.0. Fig. 1 Recall/precision with proportional assignment s</context>
</contexts>
<marker>Lewis, 1992</marker>
<rawString>D. D. Lewis. 1992. An evaluation of phrasal and clustered representation on a text categorization task. In Proceedins of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 37-50.</rawString>
</citation>
<citation valid="false">
<date></date>
<editor>M. Liberman, editor. 1991. A CL/DCI (CD-ROM).</editor>
<publisher>Association</publisher>
<institution>for Computational Linguistics Data Collection Initiative, University of Pennsylvania,</institution>
<marker></marker>
<rawString>M. Liberman, editor. 1991. A CL/DCI (CD-ROM). Association for Computational Linguistics Data Collection Initiative, University of Pennsylvania, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>K Spank Jones</author>
</authors>
<title>Relevance weighting of search terms.</title>
<date>1976</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>27--129</pages>
<marker>Robertson, Jones, 1976</marker>
<rawString>S. E. Robertson and K. Spank Jones. 1976. Relevance weighting of search terms. Journal of the American Society for Information Science, 27:129-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
</authors>
<title>The probability ranking principle in IR.</title>
<date>1977</date>
<journal>Journal of Documentation,</journal>
<pages>33--294</pages>
<contexts>
<context position="3631" citStr="Robertson, 1977" startWordPosition="557" endWordPosition="558">been exploited for information retrieval, but the adaptation to text categorization is straightforward. In a model of probabilistic text categorization, P(cid) = &amp;quot;the probability that a document d is categorized into a category c&amp;quot; (1) is calculated. Usually, a set of categories is defined beforehand. For every document di, probability P(cldi) is calculated and all the documents are ranked in decreasing order according to their probabilities. The larger P(cldi) a document di has, the more probably it will be categorized into category c. This is called the Probabilistic Ranking Principle (PRP) (Robertson, 1977). Several strategies can be used to assign categories to a document based on PRP (Lewis, 1992). There are several ways to calculate P(c1d). Three representatives are (Robertson and Sparck Jones, 1976), (Kwok, 1990), and (Fuhr, 1989). 2.1 Probabilistic Relevance Weighting (PRW) Robertson and Sparck Jones (1976) make use of the well-known logistic (or log-odds) transformation of the 162 probability P(c1d). P(c1d) g(c1d)= log (2) PCeld) where E means &amp;quot;not c&amp;quot;, that is &amp;quot;a document is not categorized into c.&amp;quot; Since this is a monotonic transformation of P(c1d), PRP is still satisfied after transforma</context>
</contexts>
<marker>Robertson, 1977</marker>
<rawString>S. E. Robertson. 1977. The probability ranking principle in IR. Journal of Documentation, 33:294-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill Publishing Company.</publisher>
<contexts>
<context position="5615" citStr="Salton and McGill, 1983" startWordPosition="901" endWordPosition="904">Therefore, P(Ti =1,01c) is the probability that a document does or does not contain the term ti, given that the document is categorized into c. This probability is estimated from the training data; the number of documents that are categorized into c and have the term ti. Substituting Eq. (4) into Eq. (3) yields g(c1d) = E log P((Ti 1 Ti= 11c) E log P(Ti = 01c) P(T, 1!—) = t,Ed tiEc—d , P(c) + log (5) P(c) We refer to Robertson and Sparck Jones&apos; formulation as Probabilistic Relevance Weighting (PRW). While PRW is the first attempt to formalize wellknown relevance weighting (Sparck Jones, 1972; Salton and McGill, 1983) by probability theory, there are several drawbacks in PRW. [Problem 1] no within-document term frequencies PRW does not make use of within-document term frequencies. P(T =1, 01c) in Eq. (5) takes into account only the existence/absence of the term tin a document. In general, frequently appearing terms in a document play an important role in information retrieval (Salton and McGill, 1983). Salton and Yang experimentally verified the importance of within-document term frequencies in their vector model (Salton and Yang, 1973). [Problem 2] no term weighting for target documents In the PRW formula</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M. J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C S Yang</author>
</authors>
<title>On the specification of term values in automatic indexing.</title>
<date>1973</date>
<journal>Journal of Documentation,</journal>
<pages>29--4</pages>
<contexts>
<context position="6144" citStr="Salton and Yang, 1973" startWordPosition="984" endWordPosition="987">empt to formalize wellknown relevance weighting (Sparck Jones, 1972; Salton and McGill, 1983) by probability theory, there are several drawbacks in PRW. [Problem 1] no within-document term frequencies PRW does not make use of within-document term frequencies. P(T =1, 01c) in Eq. (5) takes into account only the existence/absence of the term tin a document. In general, frequently appearing terms in a document play an important role in information retrieval (Salton and McGill, 1983). Salton and Yang experimentally verified the importance of within-document term frequencies in their vector model (Salton and Yang, 1973). [Problem 2] no term weighting for target documents In the PRW formulation, there is no factor of term weighting for target documents (i.e., P(•1d)). According to Eq. (5), even if a term exists in a target document, only the importance of the term in a category (i.e., P(T = 11c)) is considered for overall probability. Term weighting for target documents would also be necessary for sophisticated information retrieval (Fuhr, 1989; Kwok, 1990). [Problem 3] affected by having insufficient training cases In practical situations, the estimation of P(T = 1, 01c) is not always straightforward. Let us</context>
<context position="20974" citStr="Salton and Yang, 1973" startWordPosition="3531" endWordPosition="3534">0.47 (by SVMV) k-per-doc 0.43 (by SVMV) From Table 2, we find that SVMV with proportional assignment gives the best result (0.63). The superiority of proportional assignment over the other strategies has already been reported by Lewis (1992). Our experiment verified Lewis&apos; assumption. In addition, for any of the three strategies, SVMV gives the highest breakeven point among the four probabilistic models. Figure 1 shows the recall/precision trade off for the four probabilistic models with proportional assignment strategy. As a reference, the recall/precision curve of a well-known vector model (Salton and Yang, 1973) (&amp;quot;TF4DF&amp;quot;)6 is also presented. Table 3 lists the breakeven point for each model. All the breakeven points were obtained when proportionality constant was about 1.0. Fig. 1 Recall/precision with proportional assignment strategy 01 02 02 02 02 02 017 02 09 Recall Table 3 Breakeven points with proportional assignment strategy Breakeven Pts. SVMV 0.63 CT 0.60 RPI 0.51 PRW 0.53 TF.IDF 0.48 From Figure 1 and Table 3, we can see that: • as far as this dataset is concerned, SVMV with proportional assignment strategy gives the best result among the four probabilistic models, • the models that consider </context>
</contexts>
<marker>Salton, Yang, 1973</marker>
<rawString>G. Salton and C. S. Yang. 1973. On the specification of term values in automatic indexing. Journal of Documentation, 29(4):351-372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal of Documentation,</journal>
<pages>28--411</pages>
<contexts>
<context position="5589" citStr="Jones, 1972" startWordPosition="899" endWordPosition="900"> a document. Therefore, P(Ti =1,01c) is the probability that a document does or does not contain the term ti, given that the document is categorized into c. This probability is estimated from the training data; the number of documents that are categorized into c and have the term ti. Substituting Eq. (4) into Eq. (3) yields g(c1d) = E log P((Ti 1 Ti= 11c) E log P(Ti = 01c) P(T, 1!—) = t,Ed tiEc—d , P(c) + log (5) P(c) We refer to Robertson and Sparck Jones&apos; formulation as Probabilistic Relevance Weighting (PRW). While PRW is the first attempt to formalize wellknown relevance weighting (Sparck Jones, 1972; Salton and McGill, 1983) by probability theory, there are several drawbacks in PRW. [Problem 1] no within-document term frequencies PRW does not make use of within-document term frequencies. P(T =1, 01c) in Eq. (5) takes into account only the existence/absence of the term tin a document. In general, frequently appearing terms in a document play an important role in information retrieval (Salton and McGill, 1983). Salton and Yang experimentally verified the importance of within-document term frequencies in their vector model (Salton and Yang, 1973). [Problem 2] no term weighting for target do</context>
</contexts>
<marker>Jones, 1972</marker>
<rawString>K. Sparck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(411-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck Jones</author>
</authors>
<title>Collection properties influencing automatic term classification performance.</title>
<date>1973</date>
<journal>Information Storage and Retrieval,</journal>
<pages>9--499</pages>
<marker>Jones, 1973</marker>
<rawString>K. Sparck Jones. 1973. Collection properties influencing automatic term classification performance. Information Storage and Retrieval, 9:499-513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K M Wong</author>
<author>Y Y Yao</author>
</authors>
<title>A probability distribution model for information retrieval.</title>
<date>1989</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<pages>25--1</pages>
<contexts>
<context position="2326" citStr="Wong and Yao, 1989" startWordPosition="348" endWordPosition="351">orization, it is very difficult for human experts to assign categories consistently and efficiently to large numbers of daily incoming documents. The purpose of this paper is to propose a new probabilistic model for automatic text categorization. Takenobu TOKUNAGA Department of Computer Science Tokyo Institute of Technology 2-12-1, OOKAYAMA, MEGURO-KU TOKYO 152, JAPAN takeecs.titech.ac.jp While many text categorization models have been proposed so far, in this paper, we concentrate on the probabilistic models (Robertson and Sparck Jones, 1976; Kwok, 1990; Fuhr, 1989; Lewis, 1992; Croft, 1981; Wong and Yao, 1989; Yu et al., 1989) because these models have solid formal grounding in probability theory. Section 2 quickly reviews the probabilistic models and lists their individual problems. In section 3, we propose a new probabilistic model based on a Single random Variable with Multiple Values (SVMV). Our model is very simple, but solves some problems of the previous models. In section 4, we verify our model&apos;s superiority over the others through experiments in which we categorize &amp;quot;Wall Street Journal&amp;quot; articles. 2 A Brief Survey of Probabilistic Text Categorization In this section, we will briefly review</context>
</contexts>
<marker>Wong, Yao, 1989</marker>
<rawString>S. K. M. Wong and Y. Y. Yao. 1989. A probability distribution model for information retrieval. Information Processing &amp; Management, 25(1):39-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Yu</author>
<author>W Meng</author>
<author>S Park</author>
</authors>
<title>A framework for effective retrieval.</title>
<date>1989</date>
<journal>ACM Transactions on Database Systems,</journal>
<pages>14--2</pages>
<contexts>
<context position="2344" citStr="Yu et al., 1989" startWordPosition="352" endWordPosition="355">y difficult for human experts to assign categories consistently and efficiently to large numbers of daily incoming documents. The purpose of this paper is to propose a new probabilistic model for automatic text categorization. Takenobu TOKUNAGA Department of Computer Science Tokyo Institute of Technology 2-12-1, OOKAYAMA, MEGURO-KU TOKYO 152, JAPAN takeecs.titech.ac.jp While many text categorization models have been proposed so far, in this paper, we concentrate on the probabilistic models (Robertson and Sparck Jones, 1976; Kwok, 1990; Fuhr, 1989; Lewis, 1992; Croft, 1981; Wong and Yao, 1989; Yu et al., 1989) because these models have solid formal grounding in probability theory. Section 2 quickly reviews the probabilistic models and lists their individual problems. In section 3, we propose a new probabilistic model based on a Single random Variable with Multiple Values (SVMV). Our model is very simple, but solves some problems of the previous models. In section 4, we verify our model&apos;s superiority over the others through experiments in which we categorize &amp;quot;Wall Street Journal&amp;quot; articles. 2 A Brief Survey of Probabilistic Text Categorization In this section, we will briefly review three major proba</context>
</contexts>
<marker>Yu, Meng, Park, 1989</marker>
<rawString>C. T. Yu, W. Meng, and S. Park. 1989. A framework for effective retrieval. ACM Transactions on Database Systems, 14(2):147-167.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>