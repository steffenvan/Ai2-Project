<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001219">
<title confidence="0.930626">
DualSum: a Topic-Model based approach for update summarization
</title>
<author confidence="0.836768">
Jean-Yves Delort
</author>
<affiliation confidence="0.795405">
Google Research
</affiliation>
<address confidence="0.9320845">
Brandschenkestrasse 110
8002 Zurich, Switzerland
</address>
<email confidence="0.995886">
jydelort@google.com
</email>
<author confidence="0.801207">
Enrique Alfonseca
</author>
<affiliation confidence="0.758722">
Google Research
</affiliation>
<address confidence="0.9073665">
Brandschenkestrasse 110
8002 Zurich, Switzerland
</address>
<email confidence="0.998766">
ealfonseca@google.com
</email>
<sectionHeader confidence="0.994783" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999817352941176">
Update summarization is a new challenge
in multi-document summarization focusing
on summarizing a set of recent documents
relatively to another set of earlier docu-
ments. We present an unsupervised proba-
bilistic approach to model novelty in a doc-
ument collection and apply it to the genera-
tion of update summaries. The new model,
called DUALSUM, results in the second or
third position in terms of the ROUGE met-
rics when tuned for previous TAC competi-
tions and tested on TAC-2011, being statis-
tically indistinguishable from the winning
system. A manual evaluation of the gen-
erated summaries shows state-of-the art re-
sults for DUALSUM with respect to focus,
coherence and overall responsiveness.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99947362962963">
Update summarization is the problem of extract-
ing and synthesizing novel information in a col-
lection of documents with respect to a set of doc-
uments assumed to be known by the reader. This
problem has received much attention in recent
years, as can be observed in the number of partic-
ipants to the special track on update summariza-
tion organized by DUC and TAC since 2007. The
problem is usually formalized as follows: Given
two collections A and B, where the documents in
A chronologically precede the documents in B,
generate a summary of B under the assumption
that the user of the summary has already read the
documents in A.
Extractive techniques are the most common
approaches in multi-document summarization.
Summaries generated by such techniques consist
of sentences extracted from the document collec-
tion. Extracts can have coherence and cohesion
problems, but they generally offer a good trade-
off between linguistic quality and informative-
ness.
While numerous extractive summarization
techniques have been proposed for multi-
document summarization (Erkan and Radev,
2004; Radev et al., 2004; Shen and Li, 2010; Li et
al., 2011), few techniques have been specifically
designed for update summarization. Most exist-
ing approaches handle it as a redundancy removal
problem, with the goal of producing a summary of
collection B that is as dissimilar as possible from
either collection A or from a summary of collec-
tion A. A problem with this approach is that it can
easily classify as redundant sentences in which
novel information is mixed with existing informa-
tion (from collection A). Furthermore, while this
approach can identify sentences that contain novel
information, it cannot model explicitly what the
novel information is.
Recently, Bayesian models have successfully
been applied to multi-document summarization
showing state-of-the-art results in summarization
competitions (Haghighi and Vanderwende, 2009;
Jin et al., 2010). These approaches offer clear and
rigorous probabilistic interpretations that many
other techniques lack. Furthermore, they have the
advantage of operating in unsupervised settings,
which can be used in real-world scenarios, across
domains and languages. To our best knowledge,
previous work has not used this approach for up-
date summarization.
In this article, we propose a novel nonpara-
metric Bayesian approach for update summariza-
tion. Our approach, which is a variation of Latent
</bodyText>
<page confidence="0.982221">
214
</page>
<note confidence="0.976819">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 214–223,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.997710111111111">
Dirichlet Allocation (LDA) (Blei et al., 2003),
aims to learn to distinguish between common in-
formation and novel information. We have eval-
uated this approach on the ROUGE scores and
demonstrate that it produces comparable results
to the top system in TAC-2011. Furthermore, our
approach improves over that system when evalu-
ated manually in terms of linguistic quality and
overall responsiveness.
</bodyText>
<sectionHeader confidence="0.999898" genericHeader="introduction">
2 Related work
</sectionHeader>
<subsectionHeader confidence="0.999376">
2.1 Bayesian approaches in Summarization
</subsectionHeader>
<bodyText confidence="0.989714571428571">
Most Bayesian approaches to summarization are
based on topic models. These generative mod-
els represent documents as mixtures of latent top-
ics, where a topic is a probability distribution over
words. In TOPICSUM (Haghighi and Vander-
wende, 2009), each word is generated by a sin-
gle topic which can be a corpus-wide background
distribution over common words, a distribution
of document-specific words or a distribution of
the core content of a given cluster. BAYESSUM
(Daum´e and Marcu, 2006) and the Special Words
and Background model (Chemudugunta et al.,
2006) are very similar to TOPICSUM.
A commonality of all these models is the use of
collection and document-specific distributions in
order to distinguish between the general and spe-
cific topics in documents. In the context of sum-
marization, this distinction helps to identify the
important pieces of information in a collection.
Models that use more structure in the repre-
sentation of documents have also been proposed
for generating more coherent and less redun-
dant summaries, such as HIERSUM (Haghighi
and Vanderwende, 2009) and TTM (Celikyilmaz
and Hakkani-Tur, 2011). For instance, HIERSUM
models the intuitions that first sentences in docu-
ments should contain more general information,
and that adjacent sentences are likely to share
specic content vocabulary. However, HIERSUM,
which builds upon TOPICSUM, does not show
a statistically signicant improvement in ROUGE
over TOPICSUM.
A number of techniques have been proposed to
rank sentences of a collection given a word distri-
bution (Carbonell and Goldstein, 1998; Goldstein
et al., 1999). The Kullback-Leibler divergence
(KL) is a widely used measure in summarization.
Given a target distribution T that we want a sum-
mary S to approximate, KL is commonly used as
the scoring function to select the subset of sen-
tences S* that minimizes the KL divergence with
T:
</bodyText>
<equation confidence="0.988524666666667">
�
S� = argminKL(T, S) =
S WEV
</equation>
<bodyText confidence="0.99998153125">
where w is a word from the vocabulary V. This
strategy is called KLSum. Usually, a smoothing
factor ,r is applied on the candidate distribution S
in order to avoid the divergence to be undefined1.
This objective function selects the most repre-
sentative sentences of the collection, and at the
same time it also diversifies the generated sum-
mary by penalizing redundancy. Since the prob-
lem of finding the subset of sentences from a
collection that minimizes the KL divergence is
NP-complete, a greedy algorithm is often used in
practice2. Some variations of this objective func-
tion can be considered, such as penalizing sen-
tences that contain document-specific topics (Ma-
son and Charniak, 2011) or rewarding sentences
appearing closer to the beginning of the docu-
ment.
Wang et al. (2009) propose a Bayesian ap-
proach for summarization that does not use KL
for reranking. In their model, Bayesian Sentence-
based Topic Models, every sentence in a docu-
ment is assumed to be associated to a unique la-
tent topic. Once the model parameters have been
calculated, a summary is generated by choosing
the sentence with the highest probability for each
topic.
While hierarchical topic modeling approaches
have shown remarkable effectiveness in learning
the latent topics of document collections, they are
not designed to capture the novel information in
a collection with respect to another one, which is
the primary focus of update summarization.
</bodyText>
<subsectionHeader confidence="0.998679">
2.2 Update Summarization
</subsectionHeader>
<bodyText confidence="0.99955725">
The goal of update summarization is to generate
an update summary of a collection 13 of recent
documents assuming that the users already read
earlier documents from a collection A. We refer
</bodyText>
<footnote confidence="0.9973985">
1In our experiments we set T = 0.01.
2In our experiments, we follow the same approach as in
(Haghighi and Vanderwende, 2009) by greedily adding sen-
tences to a summary so long as they decrease KL divergence.
</footnote>
<equation confidence="0.9860635">
pT (w) log pT (w)
pS(w)
</equation>
<page confidence="0.996445">
215
</page>
<bodyText confidence="0.998109222222222">
to collection A as the base collection and to col-
lection B as the update collection.
Update summarization is related to novelty de-
tection which can be defined as the problem of
determining whether a document contains new in-
formation given an existing collection (Soboroff
and Harman, 2005). Thus, while the goal of nov-
elty detection is to determine whether some infor-
mation is new, the goal of update summarization
is to extract and synthesize the novel information.
Update summarization is also related to con-
trastive summarization, i.e. the problem of jointly
generating summaries for two entities in order to
highlight their differences (Lerman and McDon-
ald, 2009). The primary difference here is that
update summarization aims to extract novel or up-
dated information in the update collection with re-
spect to the base collection.
The most common approach for update sum-
marization is to apply a normal multi-document
summarizer, with some added functionality to re-
move sentences that are redundant with respect
to collection A. This can be achieved using sim-
ple filtering rules (Fisher and Roark, 2008), Max-
imal Marginal Relevance (Boudin et al., 2008), or
more complex graph-based algorithms (Shen and
Li, 2010; Wenjie et al., 2008). The goal here is
to boost sentences in B that bring out completely
novel information. One problem with this ap-
proach is that it is likely to discard as redundant
sentences in B containing novel information if it
is mixed with known information from collection
Another approach is to introduce specific fea-
tures intended to capture the novelty in collection
B. For example, comparing collections A and B,
FastSum derives features for the collection B such
as number of named entities in the sentence that
already occurred in the old cluster or the number
of new content words in the sentence not already
mentioned in the old cluster that are subsequently
used to train a Support Vector Machine classifier
(Schilder et al., 2008). A limitation with this ap-
proach is there are no large training sets available
and, the more features it has, the more it is af-
fected by the sparsity of the training data.
</bodyText>
<sectionHeader confidence="0.999061" genericHeader="method">
3 DualSum
</sectionHeader>
<subsectionHeader confidence="0.999945">
3.1 Model Formulation
</subsectionHeader>
<bodyText confidence="0.999928606060606">
The input for DUALSUM is a set of pairs of collec-
tions of documents C = {(Ai,Bi)}i=1...m, where
Ai is a base document collection and Bi is an up-
date document collection. We use c to refer to a
collection pair (A,, B,).
In DUALSUM, documents are modeled as a bag
of words that are assumed to be sampled from a
mixture of latent topics. Each word is associated
with a latent variable that specifies which topic
distribution is used to generate it. Words in a doc-
ument are assumed to be conditionally indepen-
dent given the hidden topic.
As in previous Bayesian works for summariza-
tion (Daum´e and Marcu, 2006; Chemudugunta
et al., 2006; Haghighi and Vanderwende, 2009),
DUALSUM not only learns collection-specific dis-
tributions, but also a general background distri-
bution over common words, φG and a document-
specific distribution φ,d for each document d in
collection pair c, which is useful to separate the
specific aspects from the general aspects of c. The
main novelty is that DUALSUM introduces spe-
cific machinery for identifying novelty.
To capture the differences between the base and
the update collection for each pair c, DUALSUM
learns two topics for every collection pair. The
joint topic, φA- captures the common information
between the two collections in the pair, i.e. the
main event that both collections are discussing.
The update topic, φB- focuses on the specific as-
pects that are specific of the documents inside the
update collection.
In the generative model,
</bodyText>
<listItem confidence="0.9802498">
• For a document d in a collection A,, words
can be originated from one of three differ-
ent topics: φG, φ,d and φA-, the last one of
which captures the main topic described in
the collection pair.
• For a document d in a collection B,, words
can be originated from one of four different
topics: φG, φ,d, φA- and φB-. The last one
will capture the most important updates to
the main topic.
</listItem>
<bodyText confidence="0.998779333333333">
To make this representation easier, we can also
state that both collections are generated from the
four topics, but we constrain the topic probability
</bodyText>
<page confidence="0.989207">
216
</page>
<listItem confidence="0.977959">
1. Sample φG — Dir(λG)
2. For each collection pair c = (Ac, 13c):
• Sample φA° — Dir(λA)
• Sample φB° — Dir(λB)
• For each document d of type ucd E {A, 13}:
- Sample φcd — Dir(λD)
- If (ucd = A) sample ψcd — Dir(γA)
- If (ucd = 13) sample ψcd — Dir(γB)
- For each word w in document d:
(a) Sample a topic z — Mult(ψcd), z E
{G, cd, Ac,13c}
(b) Sample a word w — Mult(φz)
</listItem>
<figureCaption confidence="0.9665065">
Figure 2: Graphical model representation of DUAL-
SUM.
</figureCaption>
<bodyText confidence="0.999152842105263">
for φB- to be always zero when generating a base
document.
We denote u�d E {A, B} the type of a docu-
ment d in pair c. This is an observed, Boolean
variable stating whether the document d belongs
to the base or the update collection inside the pair
c.
The generation process of documents in DU-
ALSUM is described in Figure 1, and the plate
diagram corresponding to this generative story
is shown in Figure 2. DUALSUM is an LDA-
like model, where topic distributions are multi-
nomial distributions over words and topics that
are sampled from Dirichlet distributions. We use
λ = (λG, λD, λA, λB) as symmetric priors for the
Dirichlet distributions generating the word distri-
butions. In our experiments, we set λG = 0.1 and
λD = λA = λB = 0.001. A greater value is as-
signed to λG in order to reflect the intuition that
</bodyText>
<figure confidence="0.9596845">
ryB
ryA
AB
AA
</figure>
<figureCaption confidence="0.964905">
Figure 1: Generative model in DUALSUM.
</figureCaption>
<figure confidence="0.9956169">
U
�
AG
Z
AD
OD
W
OG
OA
OB
</figure>
<bodyText confidence="0.999363952380952">
there should be more words in the background
than in the other distributions, so the mass is ex-
pected to be shared on a larger number of words.
Unlike for the word distributions, mixing prob-
abilities are drawn from a Dirichlet distribution
with asymmetric priors. The prior knowledge
about the origin of words in the base and up-
date collections is again encoded at the level the
hyper-parameters. For example, if we set γA =
(5, 3, 2, 0), this would reflect the intuition that,
on average, in the base collections, 50% of the
words originate from the background distribution,
30% from the document-specific distribution, and
20% from the joint topic. Similarly, if we set
γB = (5, 2, 2, 1), the prior reflects the assumption
that, on average, in the update collections, 50% of
the words originate from the background distri-
bution, 20% from the document-specific distribu-
tion, 20% from the joint topic, and 10% from the
novel, update topic3. The priors we have actually
used are reported in Section 4.
</bodyText>
<subsectionHeader confidence="0.892956">
3.2 Learning and inference
</subsectionHeader>
<bodyText confidence="0.9999845">
In order to find the optimal model parameters, the
following equation needs to be computed:
</bodyText>
<equation confidence="0.942098">
p(z,ψ, φ|w, u) = p(w, u)
p(z, ψ,φ, w, u)
</equation>
<bodyText confidence="0.993024333333333">
Omitting hyper-parameters for notational sim-
plicity, the joint distribution over the observed
variables is:
</bodyText>
<equation confidence="0.964083666666667">
p(w, u) = p(φG) x
Hp(φA°)p(φB°) x
c
</equation>
<bodyText confidence="0.780356916666667">
where A denotes the 4-dimensional simplex4.
Since this equation is intractable, we need to per-
form approximate inference in order to estimate
the model parameters. A number of Bayesian sta-
tistical inference techniques can be used to ad-
dress this problem.
3To highlight the difference between asymmetric and
symmetric priors we put the indices in superscript and sub-
script respectively.
4Remember that, for base documents, words cannot
be generated by the update topic, so A denotes the 3-
dimensional simplex for base documents.
</bodyText>
<equation confidence="0.880333">
11
HdE
n cdn
p(ucd)p(φcd) IA p(ψcd|ucd)dψcd x
p(wcdn|zcdn)p(zcdn|ψcd)
</equation>
<page confidence="0.991186">
217
</page>
<bodyText confidence="0.999907192307692">
Variational approaches (Blei et al., 2003) and
collapsed Gibbs sampling (Griffiths and Steyvers,
2004) are common techniques for approximate in-
ference in Bayesian models. They offer different
advantages: the variational approach is arguably
faster computationally, but the Gibbs sampling
approach is in principal more accurate since it
asymptotically approaches the correct distribution
(Porteous et al., 2008). In this section, we pro-
vide details on a collapsed Gibbs sampling strat-
egy to infer the model parameters of DUALSUM
for a given dataset.
Collapsed Gibbs sampling is a particular case
of Markov Chain Monte Carlo (MCMC) that in-
volves repeatedly sampling a topic assignment for
each word in the corpus. A single iteration of the
Gibbs sampler is completed after sampling a new
topic for each word based on the previous assign-
ment. In a collapsed Gibbs sampler, the model
parameters are integrated out (or collapsed), al-
lowing to only sample z. Let us call wcdn the n-th
word in document d in collection c, and zcdn its
topic assignment. For Gibbs sampling, we need
to calculate p(zcdn|w, u, z−cdn) where z−cdn de-
notes the random vector of topic assignments ex-
cept the assignment zcdn.
</bodyText>
<equation confidence="0.991206">
p(zcdn = j|w, u, z−cdn, γ�, γ&amp;quot;, λ) a
n(wcdn) +
cdn,j λj
EkEK/ (cd) ucd
ln−cdn,k + γk )
</equation>
<bodyText confidence="0.986935454545455">
where K = {G, cd, Ac, BcJ, n(v)
−cdn,j denotes the
number of times word v is assigned to topic j
excluding current assignment of word wcdn and
n(cd) denotes the number of words in document
−cdn,k
d of collection c that are assigned to topic j ex-
cluding current assignment of word wcdn.
After each sampling iteration, the model pa-
rameters can be estimated using the following for-
mulas5.
</bodyText>
<equation confidence="0.9289914">
cd
ψcd
k= nk + λk
En(cd)
. + V λk
</equation>
<footnote confidence="0.614075333333333">
5The interested reader is invited to consult (Wang, 2011)
for more details on using Gibbs sampling for LDA-like mod-
els
</footnote>
<bodyText confidence="0.993828058823529">
where k E K, n(v)
k denotes the number of times
word v is assigned to topic k, and n(kcd) denotes
the number of words in document d of collection
c that are assigned to topic k.
By the strong law of large numbers, the average
of sample parameters should converge towards
the true expected value of the model parameter.
Therefore, good estimates of the model parame-
ters can be obtained averaging over the sampled
values. As suggested by Gamerman and Lopes
(2006), we have set a lag (20 iterations) between
samples in order to reduce auto-correlation be-
tween samples. Our sampler also discards the first
100 iterations as burn-in period in order to avoid
averaging from samples that are still strongly in-
fluenced by the initial assignment.
</bodyText>
<sectionHeader confidence="0.9944495" genericHeader="method">
4 Experiments in Update
Summarization
</sectionHeader>
<bodyText confidence="0.996299083333333">
The Bayesian graphical model described in the
previous section can be run over a set of news
collections to learn the background distribution,
a joint distribution for each collection, an update
distribution for each collection and the document-
specific distributions. Once this is done, one of
the learned collections can be used to generate the
summary that best approximates this collection,
using the greedy algorithm described by Haghighi
and Vanderwende (2009). Still, there are some pa-
rameters that can be defined and which affects the
results obtained:
</bodyText>
<listItem confidence="0.941969142857143">
• DUALSUM’s choice of hyper-parameters af-
fects how the topics are learned.
• The documents can be represented with n-
grams of different lengths.
• It is possible to generate a summary that ap-
proximates the joint distribution, the update-
only distribution, or a combination of both.
</listItem>
<bodyText confidence="0.9837875">
This section describes how these parameters
have been tuned.
</bodyText>
<subsectionHeader confidence="0.992665">
4.1 Parameter tuning
</subsectionHeader>
<bodyText confidence="0.999988">
We use the TAC 2008 and 2009 update task
datasets as training set for tuning the hyper-
parameters for the model, namely the pseudo-
counts for the two Dirichlet priors that affects the
topic mix assignment for each document. By per-
forming a grid search over a large set of pos-
sible hyper-parameters, these have been fixed to
</bodyText>
<equation confidence="0.9982504">
EVv�1 n(v)
−cdn,j + V λj
n(cd)
−cdn,j + γucd
j
(w)
φk = nk + λk
w
EVv=1 n(v)
k + V λk
</equation>
<page confidence="0.951096">
218
</page>
<bodyText confidence="0.997545039215687">
γA = (90,190, 50, 0) and γB = (90,170, 45, 25)
as the values that produced the best ROUGE-2
score on those two datasets.
Regarding the base collection, this can be inter-
preted as setting as prior knowledge that roughly
27% of the words in the original dataset originate
from the background distribution, 58% from the
document-specific distributions, and 15% from
the topic of the original collection. We remind
the reader that the last value in γA is set to zero
because, due to the problem definition, the origi-
nal collection must have no words generated from
the update topic, which reflects the most recent
developments that are still not present in the base
collections A.
Regarding the update set, 27% of the words are
assumed to originate again from the background
distribution, 51% from the document-specific dis-
tributions, 14% from an topic in common with
the original collection, and 8% from the update-
specific topic. One interesting fact to note from
these settings is that most of the words belong to
topics that are specific to single documents (58%
and 51% respectively for both sets A and B) and
to the background distribution, whereas the joint
and update topics generate a much smaller, lim-
ited set of words. This helps these two distribu-
tions to be more focused.
The other settings mentioned at the beginning
of this section have been tuned using the TAC-
2010 dataset, which we reserved as our develop-
ment set. Once the different document-specific
and collection-specific distributions have been ob-
tained, we have to choose the target distribu-
tion T to with which the possible summaries will
be compared using the KL metric. Usually, the
human-generated update summaries not only in-
clude the terms that are very specific about the last
developments, but they also include a little back-
ground regarding the developing event. There-
fore, we try, for KLSum, a simple mixture be-
tween the joint topic (φA) and the update topic
(φB).
Figure 3 shows the ROUGE-2 results obtained
as we vary the mixture weight between the joint
φA distribution and the update-specific φB distri-
bution. As can be seen at the left of the curve, us-
ing only the update-specific model, which disre-
gards the generic words about the topic described,
produces much lower results. The results improve
as the relative weight of the joined topic model
</bodyText>
<figureCaption confidence="0.998524">
Figure 3: Variation in ROUGE-2 score in the TAC-
2010 dataset as we change the mixture weight for the
joined topic model between 0 and 1.
Figure 4: Effect of the mixture weight in ROUGE-2
scores (TAC-2010 dataset). Results are reported us-
ing bigrams (above, blue), unigrams (middle, red) and
trigrams (below, yellow).
</figureCaption>
<bodyText confidence="0.999861">
increases until it plateaus at a maximum around
roughly the interval [0.6, 0.8], and from that point
performance slowly degrades as at the right part
of the curve the update model is given very little
importance in generating the summary. Based on
these results, from this point onwards, the mixture
weight has been set to 0.7. Note that using only
the joint distribution (setting the mixture weight
to 1.0) also produces reasonable results, hinting
that it successfully incorporates the most impor-
tant n-grams from across the base and the update
collections at the same time.
A second parameter is the size of the n-grams
for representing the documents. The original
implementations of SUMBASIC (Nenkova and
Vanderwende, 2005) and TOPICSUM (Haghighi
and Vanderwende, 2009) were defined over sin-
</bodyText>
<page confidence="0.997767">
219
</page>
<bodyText confidence="0.999967428571429">
gle words (unigrams). Still, Haghighi and Van-
derwende (2009) report some improvements in
the ROUGE-2 score when representing words as
a bag of bigrams, and Darling (2010) mention
similar improvements when running SUMBASIC
with bigrams. Figure 4 shows the effect on the
ROUGE-2 curve when we switch to using uni-
grams and trigrams. As stated in previous work,
using bigrams has better results than using uni-
grams. Using trigrams was worse than either of
them. This is probably because trigrams are too
specific and the document collections are small,
so the models are more likely to suffer from data
sparseness.
</bodyText>
<subsectionHeader confidence="0.986085">
4.2 Baselines
</subsectionHeader>
<bodyText confidence="0.998517444444444">
DUALSUM is a modification of TOPICSUM de-
signed specifically for the case of update sum-
marization, by modifying TOPICSUM’s graphical
model in a way that captures the dependency be-
tween the joint and the update collections. Still, it
is important to discover whether the new graphi-
cal model actually improves over simpler applica-
tions of TOPICSUM to this task. The three base-
lines that we have considered are:
</bodyText>
<listItem confidence="0.911767083333333">
• Running TOPICSUM on the set of collections
containing only the update documents. We
call this run TOPICSUMB.
• Running TOPICSUM on the set of collections
containing both the base and the update doc-
uments. Contrary to the previous run, the
topic model for each collection in this run
will contain information relevant to the base
events. We call this run TOPICSUMAuB.
• Running TOPICSUM twice, once on the set
of collections containing the update docu-
ments, and the second time on the set of
</listItem>
<bodyText confidence="0.997133285714286">
collections containing the base documents.
Then, for each collection, the obtained base
and update models are combined in a mix-
ture model using a mixture weight between
zero and one. The weight has been tuned us-
ing TAC-2010 as development set. We call
this run TOPICSUMA+TOPICSUMB.
</bodyText>
<subsectionHeader confidence="0.999744">
4.3 Automatic evaluation
</subsectionHeader>
<bodyText confidence="0.979009">
DUALSUM and the three baselines6 have been
</bodyText>
<footnote confidence="0.679213333333333">
6Using the settings obtained in the previous section, hav-
ing been optimized on the datasets from previous TAC com-
petitions.
</footnote>
<bodyText confidence="0.99979164">
automatically evaluated using the TAC-2011
dataset. Table 1 shows the ROUGE results ob-
tained. Because of the non-deterministic nature
of Gibbs sampling, the results reported here are
the average of five runs for all the baselines and
for DUALSUM. DUALSUM outperforms two of
the baselines in all three ROUGE metrics, and it
also outperforms TOPICSUMB on two of the three
metrics.
The top three systems in TAC-2011 have been
included for comparison. The results between
these three systems, and between them and DU-
ALSUM, are all indistinguishable at 95% confi-
dence. Note that the best baseline, TOPICSUMB,
is quite competitive, with results that are indis-
tinguishable to the top participants in this year’s
evaluation. Note as well that, because we have
five different runs for our algorithms, whereas
we just have one output for the TAC participants,
the confidence intervals in the second case were
slightly bigger when checking for statistical sig-
nificance, so it is slightly harder for these systems
to assert that they outperform the baselines with
95% confidence. These results would have made
DUALSUM the second best system for ROUGE-
1 and ROUGE-SU4, and the third best system in
terms of ROUGE-2.
The supplementary materials contain a detailed
example of the the topic model obtained for the
background in the TAC-2011 dataset, and the base
and update models for collection D1110. As
expected, the top unigrams and bigrams are all
closed-class words and auxiliary verbs. Because
trigrams are longer, background trigrams actu-
ally include some content words (e.g. university
or director). Regarding the models for OA and
0B, the base distribution contains words related
to the original event of an earthquake in Sichuan
province (China), and the update distribution fo-
cuses more on the official (updated) death toll
numbers. It can be noted here that the tokenizer
we used is very simple (splitting tokens separated
with white-spaces or punctuation) so that num-
bers such as 7.9 (the magnitude of the earthquake)
and 12,000 or 14,000 are divided into two tokens.
We thought this might be a for the bigram-based
system to produce better results, but we ran the
summarizers with a numbers-aware tokenizer and
the statistical differences between versions still
hold.
</bodyText>
<page confidence="0.989698">
220
</page>
<table confidence="0.999635">
Method R-1 R-2 R-SU4
TOPICSUM.6 0.3442 0.0868 0.1194
TOPICSUMAu.6 0.3385 0.0809 0.1159
TOPICSUMA+TOPICSUM.6 0.3328 0.0770 0.1125
DUALSUM 0.3575$†* 0.0924†* 0.1285$†*
TAC-2011 best system (Peer 43) 0.3559†* 0.0958†* 0.1308$†*
TAC-2011 2nd system (Peer 25) 0.3582†* 0.0926* 0.1276†*
TAC-2011 3rd system (Peer 17) 0.3558†* 0.0886 0.1279†*
</table>
<tableCaption confidence="0.9906715">
Table 1: Results on the TAC-2011 dataset. t, t and * indicate that a result is significantly better than TOPICSUMB,
TOPICSUMAUB and TOPICSUMA+TOPICSUMB, respectively (p &lt; 0.05).
</tableCaption>
<subsectionHeader confidence="0.99891">
4.4 Manual evaluation
</subsectionHeader>
<bodyText confidence="0.999950914285714">
While the ROUGE metrics provides an arguable
estimate of the informativeness of a generated
summary, it does not account for other important
aspects such as the readability or the overall re-
sponsiveness. To evaluate such aspects, a manual
evaluation is required. A fairly standard approach
for manual evaluation is through pairwise com-
parison (Haghighi and Vanderwende, 2009; Ce-
likyilmaz and Hakkani-Tur, 2011).
In this approach, raters are presented with pairs
of summaries generated by two systems and they
are asked to say which one is best with respect
to some aspects. We followed a similar approach
to compare DualSum with Peer 43 - the best sys-
tem with respect to ROUGE-2, on the TAC 2011
dataset. For each collection, raters were presented
with three summaries: a reference summary ran-
domly chosen from the model summaries, and the
summaries generated by Peer 43 and DualSum.
They were asked to read the summaries and say
which one of the two generated summaries is best
with respect to: 1) Overall responsiveness: which
summary is best overall (both in terms of content
and fluency), 2) Focus: which summary contains
less irrelevant details, 3) Coherence: which sum-
mary is more coherent and 4) Non-redundancy:
which summary repeats less the same informa-
tion. For each aspect, the rater could also reply
that both summary were of the same quality.
For each of the 44 collections in TAC-2011, 3
ratings were collected from raters7. Results are
reported in Table 2. DualSum outperforms Peer
43 in three aspects, including Overall Responsive-
ness, which aggregates all the other scores and
can be considered the most important one. Re-
</bodyText>
<footnote confidence="0.9122575">
7In total 132 raters participated to the task via our own
crowdsourcing platform, not mentioned yet for blind review.
</footnote>
<table confidence="0.999875">
Aspect Peer 43 Best system
Same DualSum
Overall Responsiveness 39 25 68
Focus 41 22 69
Coherence 39 30 63
Non-redundancy 40 53 39
</table>
<tableCaption confidence="0.999737">
Table 2: Results of the side-by-side manual evaluation.
</tableCaption>
<bodyText confidence="0.999412111111111">
garding Non-redundancy, DualSum and Peer 43
obtain similar results but the majority of raters
found no difference between the two systems.
Fleiss n has been used to measure the inter-rater
agreement. For each aspect, we observe n - 0.2
which corresponds to a slight agreement; but if we
focus on tasks where the 3 ratings reflect a prefer-
ence for either of the two systems, then n - 0.5,
which indicates moderate agreement.
</bodyText>
<subsectionHeader confidence="0.929189">
4.5 Efficiency and applicability
</subsectionHeader>
<bodyText confidence="0.9999834">
The running time for summarizing the TAC col-
lections with DualSum, averaged over a hundred
runs, is 4.97 minutes, using one core (2.3 GHz).
Memory consumption was 143 MB.
It is important to note as well that, while TOP-
ICSUM incorporates an additional layer to model
topic distributions at the sentence level, we noted
early in our experiments that this did not improve
the performance (as evaluated with ROUGE) and
consequently relaxed that assumption in Dual-
Sum. This resulted in a simplification of the
model and a reduction of the sampling time.
While five minutes is fast enough to be able
to experiment and tune parameters with the TAC
collections, it would be quite slow for a real-
time summarization system able to generate sum-
maries on request. As can be seen from the plate
diagram in Figure 2, all the collections are gen-
erated independently from each other. The only
exception, for which it is necessary to have all
</bodyText>
<page confidence="0.992172">
221
</page>
<bodyText confidence="0.99997644">
the collections available at the same time dur-
ing Gibbs sampling, is the background distribu-
tion, which is estimated from all the collections
simultaneously, roughly representing 27% of the
words, that should appear distributed across all
documents.
The good news is that this background distri-
bution will contain closed-class words in the lan-
guage, which are domain-independent (see sup-
plementary material for examples). Therefore,
we can generate this distribution from one of
the TAC datasets only once, and then it can be
reused. Fixing the background distribution to a
pre-computed value requires a very simple mod-
ification of the Gibbs sampling implementation,
which just needs to adjust at each iteration the
collection and document-specific models, and the
topic assignment for the words.
Using this modified implementation, it is now
possible to summarize a single collection inde-
pendently. The summarization of a single col-
lection of the size of the TAC collections is re-
duced on average to only three seconds on the
same hardware settings, allowing the use of this
summarizer in an on-line application.
</bodyText>
<sectionHeader confidence="0.999309" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999974696969697">
The main contribution of this paper is DUALSUM,
a new topic model that is specifically designed to
identify and extract novelty from pairs of collec-
tions.
It is inspired by TOPICSUM (Haghighi and
Vanderwende, 2009), with two main changes:
Firstly, while TOPICSUM can only learn the main
topic of a collection, DUALSUM focuses on the
differences between two collections. Secondly,
while TOPICSUM incorporates an additional layer
to model topic distributions at the sentence level,
we have found that relaxing this assumption and
modeling the topic distribution at document level
does not decrease the ROUGE scores and reduces
the sampling time.
The generated summaries, tested on the TAC-
2011 collection, would have resulted on the sec-
ond and third position in the last summarization
competition according to the different ROUGE
scores. This would make DUALSUM statistically
indistinguishable from the top system with 0.95
confidence.
We also propose and evaluate the applicability
of an alternative implementation of Gibbs sam-
pling to on-line settings. By fixing the back-
ground distribution we are able to summarize a
distribution in only three seconds, which seems
reasonable for some on-line applications.
As future work, we plan to explore the use of
DUALSUM to generate more general contrastive
summaries, by identifying differences between
collections whose differences are not of temporal
nature.
</bodyText>
<sectionHeader confidence="0.996879" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999979142857143">
The research leading to these results has received
funding from the European Union’s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement number 257790. We would also
like to thank Yasemin Altun and the anonymous
reviewers for their useful comments on the draft
of this paper.
</bodyText>
<sectionHeader confidence="0.99834" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992644606060606">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.
Florian Boudin, Marc El-B`eze, and Juan-Manuel
Torres-Moreno. 2008. A scalable MMR approach
to sentence scoring for multi-document update sum-
marization. In Coling 2008: Companion volume:
Posters, pages 23–26, Manchester, UK, August.
Coling 2008 Organizing Committee.
J. Carbonell and J. Goldstein. 1998. The use of mmr,
diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the
21st annual international ACM SIGIR conference
on Research and development in information re-
trieval, pages 335–336. ACM.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2011. Dis-
covery of topically coherent sentences for extrac-
tive summarization. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
491–499, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2006. Modeling general and specific as-
pects of documents with a probabilistic topic model.
In NIPS, pages 241–248.
W.M. Darling. 2010. Multi-document summarization
from first principles. In Proceedings of the third
Text Analysis Conference, TAC-2010. NIST.
Hal Daum´e, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of
the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting
</reference>
<page confidence="0.970487">
222
</page>
<reference confidence="0.999717417475728">
of the Association for Computational Linguistics,
ACL-2006, pages 305–312, Stroudsburg, PA, USA.
Association for Computational Linguistics.
G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text
summarization. J. Artif. Int. Res., 22:457–479, De-
cember.
S. Fisher and B. Roark. 2008. Query-focused super-
vised sentence ranking for update summaries. In
Proceedings of the first Text Analysis Conference,
TAC-2008.
Dani Gamerman and Hedibert F. Lopes. 2006.
Markov Chain Monte Carlo: Stochastic Simulation
for Bayesian Inference. Chapman and Hall/CRC.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text docu-
ments: sentence selection and evaluation metrics.
In Proceedings of the 22nd annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, SIGIR ’99, pages
121–128, New York, NY, USA. ACM.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy
of Sciences, 101(Suppl. 1):5228–5235, April.
A. Haghighi and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 362–370. Association for Com-
putational Linguistics.
Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. The
thu summarization systems at tac 2010. In Proceed-
ings of the third Text Analysis Conference, TAC-
2010.
Kevin Lerman and Ryan McDonald. 2009. Con-
trastive summarization: an experiment with con-
sumer reviews. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, NAACL-Short ’09, pages 113–116,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Xuan Li, Liang Du, and Yi-Dong Shen. 2011. Graph-
based marginal ranking for update summarization.
In Proceedings of the Eleventh SIAM International
Conference on Data Mining. SIAM / Omnipress.
Rebecca Mason and Eugene Charniak. 2011. Ex-
tractive multi-document summaries should explic-
itly not contain document-specific content. In Pro-
ceedings of the Workshop on Automatic Summariza-
tion for Different Genres, Media, and Languages,
WASDGML ’11, pages 49–54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
A. Nenkova and L. Vanderwende. 2005. The im-
pact of frequency on summarization. Microsoft Re-
search, Redmond, Washington, Tech. Rep. MSR-TR-
2005-101.
Ian Porteous, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling.
2008. Fast collapsed Gibbs sampling for latent
Dirichlet allocation. In KDD ’08: Proceeding of
the 14th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 569–
577, New York, NY, USA, August. ACM.
Dragomir R. Radev, Hongyan Jing, Malgorzata Sty´s,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Inf. Process. Manage.,
40:919–938, November.
Frank Schilder, Ravikumar Kondadadi, Jochen L. Lei-
dner, and Jack G. Conrad. 2008. Thomson reuters
at tac 2008: Aggressive filtering with fastsum for
update and opinion summarization. In Proceedings
of the first Text Analysis Conference, TAC-2008.
Chao Shen and Tao Li. 2010. Multi-document sum-
marization via the minimum dominating set. In
Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ’10, pages
984–992, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Ian Soboroff and Donna Harman. 2005. Novelty de-
tection: the trec experience. In Proceedings of the
conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, HLT ’05, pages 105–112, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-document summarization us-
ing sentence-based topic models. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ’09, pages 297–300, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Yi Wang. 2011. Distributed gibbs sampling of latent
dirichlet allocation: The gritty details.
Li Wenjie, Wei Furu, Lu Qin, and He Yanxiang. 2008.
Pnr2: ranking sentences with positive and nega-
tive reinforcement for query-oriented update sum-
marization. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics -
Volume 1, COLING ’08, pages 489–496, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.999167">
223
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.076962">
<title confidence="0.68623525">DualSum: a Topic-Model based approach for update summarization Jean-Yves Delort Google Brandschenkestrasse</title>
<address confidence="0.968649">8002 Zurich,</address>
<email confidence="0.999938">jydelort@google.com</email>
<author confidence="0.799299">Enrique</author>
<affiliation confidence="0.576767">Google Brandschenkestrasse</affiliation>
<address confidence="0.980581">8002 Zurich,</address>
<email confidence="0.999964">ealfonseca@google.com</email>
<abstract confidence="0.997569333333333">Update summarization is a new challenge in multi-document summarization focusing on summarizing a set of recent documents relatively to another set of earlier documents. We present an unsupervised probabilistic approach to model novelty in a document collection and apply it to the generation of update summaries. The new model, results in the second or third position in terms of the ROUGE metrics when tuned for previous TAC competitions and tested on TAC-2011, being statistically indistinguishable from the winning system. A manual evaluation of the generated summaries shows state-of-the art refor respect to focus, coherence and overall responsiveness.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="3674" citStr="Blei et al., 2003" startWordPosition="551" endWordPosition="554">ve the advantage of operating in unsupervised settings, which can be used in real-world scenarios, across domains and languages. To our best knowledge, previous work has not used this approach for update summarization. In this article, we propose a novel nonparametric Bayesian approach for update summarization. Our approach, which is a variation of Latent 214 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 214–223, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics Dirichlet Allocation (LDA) (Blei et al., 2003), aims to learn to distinguish between common information and novel information. We have evaluated this approach on the ROUGE scores and demonstrate that it produces comparable results to the top system in TAC-2011. Furthermore, our approach improves over that system when evaluated manually in terms of linguistic quality and overall responsiveness. 2 Related work 2.1 Bayesian approaches in Summarization Most Bayesian approaches to summarization are based on topic models. These generative models represent documents as mixtures of latent topics, where a topic is a probability distribution over w</context>
<context position="15324" citStr="Blei et al., 2003" startWordPosition="2525" endWordPosition="2528">sional simplex4. Since this equation is intractable, we need to perform approximate inference in order to estimate the model parameters. A number of Bayesian statistical inference techniques can be used to address this problem. 3To highlight the difference between asymmetric and symmetric priors we put the indices in superscript and subscript respectively. 4Remember that, for base documents, words cannot be generated by the update topic, so A denotes the 3- dimensional simplex for base documents. 11 HdE n cdn p(ucd)p(φcd) IA p(ψcd|ucd)dψcd x p(wcdn|zcdn)p(zcdn|ψcd) 217 Variational approaches (Blei et al., 2003) and collapsed Gibbs sampling (Griffiths and Steyvers, 2004) are common techniques for approximate inference in Bayesian models. They offer different advantages: the variational approach is arguably faster computationally, but the Gibbs sampling approach is in principal more accurate since it asymptotically approaches the correct distribution (Porteous et al., 2008). In this section, we provide details on a collapsed Gibbs sampling strategy to infer the model parameters of DUALSUM for a given dataset. Collapsed Gibbs sampling is a particular case of Markov Chain Monte Carlo (MCMC) that involve</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Boudin</author>
<author>Marc El-B`eze</author>
<author>Juan-Manuel Torres-Moreno</author>
</authors>
<title>A scalable MMR approach to sentence scoring for multi-document update summarization.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2008: Companion volume: Posters,</booktitle>
<pages>23--26</pages>
<location>Manchester, UK,</location>
<marker>Boudin, El-B`eze, Torres-Moreno, 2008</marker>
<rawString>Florian Boudin, Marc El-B`eze, and Juan-Manuel Torres-Moreno. 2008. A scalable MMR approach to sentence scoring for multi-document update summarization. In Coling 2008: Companion volume: Posters, pages 23–26, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>J Goldstein</author>
</authors>
<title>The use of mmr, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>335--336</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5657" citStr="Carbonell and Goldstein, 1998" startWordPosition="857" endWordPosition="860">nts have also been proposed for generating more coherent and less redundant summaries, such as HIERSUM (Haghighi and Vanderwende, 2009) and TTM (Celikyilmaz and Hakkani-Tur, 2011). For instance, HIERSUM models the intuitions that first sentences in documents should contain more general information, and that adjacent sentences are likely to share specic content vocabulary. However, HIERSUM, which builds upon TOPICSUM, does not show a statistically signicant improvement in ROUGE over TOPICSUM. A number of techniques have been proposed to rank sentences of a collection given a word distribution (Carbonell and Goldstein, 1998; Goldstein et al., 1999). The Kullback-Leibler divergence (KL) is a widely used measure in summarization. Given a target distribution T that we want a summary S to approximate, KL is commonly used as the scoring function to select the subset of sentences S* that minimizes the KL divergence with T: � S� = argminKL(T, S) = S WEV where w is a word from the vocabulary V. This strategy is called KLSum. Usually, a smoothing factor ,r is applied on the candidate distribution S in order to avoid the divergence to be undefined1. This objective function selects the most representative sentences of the </context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>J. Carbonell and J. Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 335–336. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>Discovery of topically coherent sentences for extractive summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>491--499</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="5207" citStr="Celikyilmaz and Hakkani-Tur, 2011" startWordPosition="790" endWordPosition="793">d the Special Words and Background model (Chemudugunta et al., 2006) are very similar to TOPICSUM. A commonality of all these models is the use of collection and document-specific distributions in order to distinguish between the general and specific topics in documents. In the context of summarization, this distinction helps to identify the important pieces of information in a collection. Models that use more structure in the representation of documents have also been proposed for generating more coherent and less redundant summaries, such as HIERSUM (Haghighi and Vanderwende, 2009) and TTM (Celikyilmaz and Hakkani-Tur, 2011). For instance, HIERSUM models the intuitions that first sentences in documents should contain more general information, and that adjacent sentences are likely to share specic content vocabulary. However, HIERSUM, which builds upon TOPICSUM, does not show a statistically signicant improvement in ROUGE over TOPICSUM. A number of techniques have been proposed to rank sentences of a collection given a word distribution (Carbonell and Goldstein, 1998; Goldstein et al., 1999). The Kullback-Leibler divergence (KL) is a widely used measure in summarization. Given a target distribution T that we want </context>
<context position="27874" citStr="Celikyilmaz and Hakkani-Tur, 2011" startWordPosition="4602" endWordPosition="4606">.3558†* 0.0886 0.1279†* Table 1: Results on the TAC-2011 dataset. t, t and * indicate that a result is significantly better than TOPICSUMB, TOPICSUMAUB and TOPICSUMA+TOPICSUMB, respectively (p &lt; 0.05). 4.4 Manual evaluation While the ROUGE metrics provides an arguable estimate of the informativeness of a generated summary, it does not account for other important aspects such as the readability or the overall responsiveness. To evaluate such aspects, a manual evaluation is required. A fairly standard approach for manual evaluation is through pairwise comparison (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2011). In this approach, raters are presented with pairs of summaries generated by two systems and they are asked to say which one is best with respect to some aspects. We followed a similar approach to compare DualSum with Peer 43 - the best system with respect to ROUGE-2, on the TAC 2011 dataset. For each collection, raters were presented with three summaries: a reference summary randomly chosen from the model summaries, and the summaries generated by Peer 43 and DualSum. They were asked to read the summaries and say which one of the two generated summaries is best with respect to: 1) Overall res</context>
</contexts>
<marker>Celikyilmaz, Hakkani-Tur, 2011</marker>
<rawString>Asli Celikyilmaz and Dilek Hakkani-Tur. 2011. Discovery of topically coherent sentences for extractive summarization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 491–499, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chaitanya Chemudugunta</author>
<author>Padhraic Smyth</author>
<author>Mark Steyvers</author>
</authors>
<title>Modeling general and specific aspects of documents with a probabilistic topic model.</title>
<date>2006</date>
<booktitle>In NIPS,</booktitle>
<pages>241--248</pages>
<contexts>
<context position="4641" citStr="Chemudugunta et al., 2006" startWordPosition="702" endWordPosition="705">siveness. 2 Related work 2.1 Bayesian approaches in Summarization Most Bayesian approaches to summarization are based on topic models. These generative models represent documents as mixtures of latent topics, where a topic is a probability distribution over words. In TOPICSUM (Haghighi and Vanderwende, 2009), each word is generated by a single topic which can be a corpus-wide background distribution over common words, a distribution of document-specific words or a distribution of the core content of a given cluster. BAYESSUM (Daum´e and Marcu, 2006) and the Special Words and Background model (Chemudugunta et al., 2006) are very similar to TOPICSUM. A commonality of all these models is the use of collection and document-specific distributions in order to distinguish between the general and specific topics in documents. In the context of summarization, this distinction helps to identify the important pieces of information in a collection. Models that use more structure in the representation of documents have also been proposed for generating more coherent and less redundant summaries, such as HIERSUM (Haghighi and Vanderwende, 2009) and TTM (Celikyilmaz and Hakkani-Tur, 2011). For instance, HIERSUM models the</context>
<context position="10672" citStr="Chemudugunta et al., 2006" startWordPosition="1705" endWordPosition="1708">ut for DUALSUM is a set of pairs of collections of documents C = {(Ai,Bi)}i=1...m, where Ai is a base document collection and Bi is an update document collection. We use c to refer to a collection pair (A,, B,). In DUALSUM, documents are modeled as a bag of words that are assumed to be sampled from a mixture of latent topics. Each word is associated with a latent variable that specifies which topic distribution is used to generate it. Words in a document are assumed to be conditionally independent given the hidden topic. As in previous Bayesian works for summarization (Daum´e and Marcu, 2006; Chemudugunta et al., 2006; Haghighi and Vanderwende, 2009), DUALSUM not only learns collection-specific distributions, but also a general background distribution over common words, φG and a documentspecific distribution φ,d for each document d in collection pair c, which is useful to separate the specific aspects from the general aspects of c. The main novelty is that DUALSUM introduces specific machinery for identifying novelty. To capture the differences between the base and the update collection for each pair c, DUALSUM learns two topics for every collection pair. The joint topic, φA- captures the common informatio</context>
</contexts>
<marker>Chemudugunta, Smyth, Steyvers, 2006</marker>
<rawString>Chaitanya Chemudugunta, Padhraic Smyth, and Mark Steyvers. 2006. Modeling general and specific aspects of documents with a probabilistic topic model. In NIPS, pages 241–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Darling</author>
</authors>
<title>Multi-document summarization from first principles.</title>
<date>2010</date>
<booktitle>In Proceedings of the third Text Analysis Conference, TAC-2010.</booktitle>
<publisher>NIST.</publisher>
<contexts>
<context position="22828" citStr="Darling (2010)" startWordPosition="3800" endWordPosition="3801">int distribution (setting the mixture weight to 1.0) also produces reasonable results, hinting that it successfully incorporates the most important n-grams from across the base and the update collections at the same time. A second parameter is the size of the n-grams for representing the documents. The original implementations of SUMBASIC (Nenkova and Vanderwende, 2005) and TOPICSUM (Haghighi and Vanderwende, 2009) were defined over sin219 gle words (unigrams). Still, Haghighi and Vanderwende (2009) report some improvements in the ROUGE-2 score when representing words as a bag of bigrams, and Darling (2010) mention similar improvements when running SUMBASIC with bigrams. Figure 4 shows the effect on the ROUGE-2 curve when we switch to using unigrams and trigrams. As stated in previous work, using bigrams has better results than using unigrams. Using trigrams was worse than either of them. This is probably because trigrams are too specific and the document collections are small, so the models are more likely to suffer from data sparseness. 4.2 Baselines DUALSUM is a modification of TOPICSUM designed specifically for the case of update summarization, by modifying TOPICSUM’s graphical model in a wa</context>
</contexts>
<marker>Darling, 2010</marker>
<rawString>W.M. Darling. 2010. Multi-document summarization from first principles. In Proceedings of the third Text Analysis Conference, TAC-2010. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian query-focused summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-2006,</booktitle>
<pages>305--312</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e, III and Daniel Marcu. 2006. Bayesian query-focused summarization. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-2006, pages 305–312, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Int. Res.,</journal>
<pages>22--457</pages>
<contexts>
<context position="2068" citStr="Erkan and Radev, 2004" startWordPosition="311" endWordPosition="314"> documents in A chronologically precede the documents in B, generate a summary of B under the assumption that the user of the summary has already read the documents in A. Extractive techniques are the most common approaches in multi-document summarization. Summaries generated by such techniques consist of sentences extracted from the document collection. Extracts can have coherence and cohesion problems, but they generally offer a good tradeoff between linguistic quality and informativeness. While numerous extractive summarization techniques have been proposed for multidocument summarization (Erkan and Radev, 2004; Radev et al., 2004; Shen and Li, 2010; Li et al., 2011), few techniques have been specifically designed for update summarization. Most existing approaches handle it as a redundancy removal problem, with the goal of producing a summary of collection B that is as dissimilar as possible from either collection A or from a summary of collection A. A problem with this approach is that it can easily classify as redundant sentences in which novel information is mixed with existing information (from collection A). Furthermore, while this approach can identify sentences that contain novel information,</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank: graph-based lexical centrality as salience in text summarization. J. Artif. Int. Res., 22:457–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fisher</author>
<author>B Roark</author>
</authors>
<title>Query-focused supervised sentence ranking for update summaries.</title>
<date>2008</date>
<booktitle>In Proceedings of the first Text Analysis Conference, TAC-2008.</booktitle>
<contexts>
<context position="8975" citStr="Fisher and Roark, 2008" startWordPosition="1408" endWordPosition="1411">zation is also related to contrastive summarization, i.e. the problem of jointly generating summaries for two entities in order to highlight their differences (Lerman and McDonald, 2009). The primary difference here is that update summarization aims to extract novel or updated information in the update collection with respect to the base collection. The most common approach for update summarization is to apply a normal multi-document summarizer, with some added functionality to remove sentences that are redundant with respect to collection A. This can be achieved using simple filtering rules (Fisher and Roark, 2008), Maximal Marginal Relevance (Boudin et al., 2008), or more complex graph-based algorithms (Shen and Li, 2010; Wenjie et al., 2008). The goal here is to boost sentences in B that bring out completely novel information. One problem with this approach is that it is likely to discard as redundant sentences in B containing novel information if it is mixed with known information from collection Another approach is to introduce specific features intended to capture the novelty in collection B. For example, comparing collections A and B, FastSum derives features for the collection B such as number of</context>
</contexts>
<marker>Fisher, Roark, 2008</marker>
<rawString>S. Fisher and B. Roark. 2008. Query-focused supervised sentence ranking for update summaries. In Proceedings of the first Text Analysis Conference, TAC-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani Gamerman</author>
<author>Hedibert F Lopes</author>
</authors>
<title>Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference. Chapman and Hall/CRC.</title>
<date>2006</date>
<contexts>
<context position="17563" citStr="Gamerman and Lopes (2006)" startWordPosition="2916" endWordPosition="2919">formulas5. cd ψcd k= nk + λk En(cd) . + V λk 5The interested reader is invited to consult (Wang, 2011) for more details on using Gibbs sampling for LDA-like models where k E K, n(v) k denotes the number of times word v is assigned to topic k, and n(kcd) denotes the number of words in document d of collection c that are assigned to topic k. By the strong law of large numbers, the average of sample parameters should converge towards the true expected value of the model parameter. Therefore, good estimates of the model parameters can be obtained averaging over the sampled values. As suggested by Gamerman and Lopes (2006), we have set a lag (20 iterations) between samples in order to reduce auto-correlation between samples. Our sampler also discards the first 100 iterations as burn-in period in order to avoid averaging from samples that are still strongly influenced by the initial assignment. 4 Experiments in Update Summarization The Bayesian graphical model described in the previous section can be run over a set of news collections to learn the background distribution, a joint distribution for each collection, an update distribution for each collection and the documentspecific distributions. Once this is done</context>
</contexts>
<marker>Gamerman, Lopes, 2006</marker>
<rawString>Dani Gamerman and Hedibert F. Lopes. 2006. Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference. Chapman and Hall/CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Mark Kantrowitz</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
</authors>
<title>Summarizing text documents: sentence selection and evaluation metrics.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’99,</booktitle>
<pages>121--128</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5682" citStr="Goldstein et al., 1999" startWordPosition="861" endWordPosition="864"> generating more coherent and less redundant summaries, such as HIERSUM (Haghighi and Vanderwende, 2009) and TTM (Celikyilmaz and Hakkani-Tur, 2011). For instance, HIERSUM models the intuitions that first sentences in documents should contain more general information, and that adjacent sentences are likely to share specic content vocabulary. However, HIERSUM, which builds upon TOPICSUM, does not show a statistically signicant improvement in ROUGE over TOPICSUM. A number of techniques have been proposed to rank sentences of a collection given a word distribution (Carbonell and Goldstein, 1998; Goldstein et al., 1999). The Kullback-Leibler divergence (KL) is a widely used measure in summarization. Given a target distribution T that we want a summary S to approximate, KL is commonly used as the scoring function to select the subset of sentences S* that minimizes the KL divergence with T: � S� = argminKL(T, S) = S WEV where w is a word from the vocabulary V. This strategy is called KLSum. Usually, a smoothing factor ,r is applied on the candidate distribution S in order to avoid the divergence to be undefined1. This objective function selects the most representative sentences of the collection, and at the sa</context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and Jaime Carbonell. 1999. Summarizing text documents: sentence selection and evaluation metrics. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’99, pages 121–128, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="15384" citStr="Griffiths and Steyvers, 2004" startWordPosition="2533" endWordPosition="2536">le, we need to perform approximate inference in order to estimate the model parameters. A number of Bayesian statistical inference techniques can be used to address this problem. 3To highlight the difference between asymmetric and symmetric priors we put the indices in superscript and subscript respectively. 4Remember that, for base documents, words cannot be generated by the update topic, so A denotes the 3- dimensional simplex for base documents. 11 HdE n cdn p(ucd)p(φcd) IA p(ψcd|ucd)dψcd x p(wcdn|zcdn)p(zcdn|ψcd) 217 Variational approaches (Blei et al., 2003) and collapsed Gibbs sampling (Griffiths and Steyvers, 2004) are common techniques for approximate inference in Bayesian models. They offer different advantages: the variational approach is arguably faster computationally, but the Gibbs sampling approach is in principal more accurate since it asymptotically approaches the correct distribution (Porteous et al., 2008). In this section, we provide details on a collapsed Gibbs sampling strategy to infer the model parameters of DUALSUM for a given dataset. Collapsed Gibbs sampling is a particular case of Markov Chain Monte Carlo (MCMC) that involves repeatedly sampling a topic assignment for each word in th</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl. 1):5228–5235, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>L Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2910" citStr="Haghighi and Vanderwende, 2009" startWordPosition="438" endWordPosition="441">l of producing a summary of collection B that is as dissimilar as possible from either collection A or from a summary of collection A. A problem with this approach is that it can easily classify as redundant sentences in which novel information is mixed with existing information (from collection A). Furthermore, while this approach can identify sentences that contain novel information, it cannot model explicitly what the novel information is. Recently, Bayesian models have successfully been applied to multi-document summarization showing state-of-the-art results in summarization competitions (Haghighi and Vanderwende, 2009; Jin et al., 2010). These approaches offer clear and rigorous probabilistic interpretations that many other techniques lack. Furthermore, they have the advantage of operating in unsupervised settings, which can be used in real-world scenarios, across domains and languages. To our best knowledge, previous work has not used this approach for update summarization. In this article, we propose a novel nonparametric Bayesian approach for update summarization. Our approach, which is a variation of Latent 214 Proceedings of the 13th Conference of the European Chapter of the Association for Computatio</context>
<context position="4324" citStr="Haghighi and Vanderwende, 2009" startWordPosition="650" endWordPosition="654">distinguish between common information and novel information. We have evaluated this approach on the ROUGE scores and demonstrate that it produces comparable results to the top system in TAC-2011. Furthermore, our approach improves over that system when evaluated manually in terms of linguistic quality and overall responsiveness. 2 Related work 2.1 Bayesian approaches in Summarization Most Bayesian approaches to summarization are based on topic models. These generative models represent documents as mixtures of latent topics, where a topic is a probability distribution over words. In TOPICSUM (Haghighi and Vanderwende, 2009), each word is generated by a single topic which can be a corpus-wide background distribution over common words, a distribution of document-specific words or a distribution of the core content of a given cluster. BAYESSUM (Daum´e and Marcu, 2006) and the Special Words and Background model (Chemudugunta et al., 2006) are very similar to TOPICSUM. A commonality of all these models is the use of collection and document-specific distributions in order to distinguish between the general and specific topics in documents. In the context of summarization, this distinction helps to identify the importa</context>
<context position="7760" citStr="Haghighi and Vanderwende, 2009" startWordPosition="1207" endWordPosition="1210">ic. While hierarchical topic modeling approaches have shown remarkable effectiveness in learning the latent topics of document collections, they are not designed to capture the novel information in a collection with respect to another one, which is the primary focus of update summarization. 2.2 Update Summarization The goal of update summarization is to generate an update summary of a collection 13 of recent documents assuming that the users already read earlier documents from a collection A. We refer 1In our experiments we set T = 0.01. 2In our experiments, we follow the same approach as in (Haghighi and Vanderwende, 2009) by greedily adding sentences to a summary so long as they decrease KL divergence. pT (w) log pT (w) pS(w) 215 to collection A as the base collection and to collection B as the update collection. Update summarization is related to novelty detection which can be defined as the problem of determining whether a document contains new information given an existing collection (Soboroff and Harman, 2005). Thus, while the goal of novelty detection is to determine whether some information is new, the goal of update summarization is to extract and synthesize the novel information. Update summarization i</context>
<context position="10705" citStr="Haghighi and Vanderwende, 2009" startWordPosition="1709" endWordPosition="1712">pairs of collections of documents C = {(Ai,Bi)}i=1...m, where Ai is a base document collection and Bi is an update document collection. We use c to refer to a collection pair (A,, B,). In DUALSUM, documents are modeled as a bag of words that are assumed to be sampled from a mixture of latent topics. Each word is associated with a latent variable that specifies which topic distribution is used to generate it. Words in a document are assumed to be conditionally independent given the hidden topic. As in previous Bayesian works for summarization (Daum´e and Marcu, 2006; Chemudugunta et al., 2006; Haghighi and Vanderwende, 2009), DUALSUM not only learns collection-specific distributions, but also a general background distribution over common words, φG and a documentspecific distribution φ,d for each document d in collection pair c, which is useful to separate the specific aspects from the general aspects of c. The main novelty is that DUALSUM introduces specific machinery for identifying novelty. To capture the differences between the base and the update collection for each pair c, DUALSUM learns two topics for every collection pair. The joint topic, φA- captures the common information between the two collections in </context>
<context position="18343" citStr="Haghighi and Vanderwende (2009)" startWordPosition="3037" endWordPosition="3040">ations as burn-in period in order to avoid averaging from samples that are still strongly influenced by the initial assignment. 4 Experiments in Update Summarization The Bayesian graphical model described in the previous section can be run over a set of news collections to learn the background distribution, a joint distribution for each collection, an update distribution for each collection and the documentspecific distributions. Once this is done, one of the learned collections can be used to generate the summary that best approximates this collection, using the greedy algorithm described by Haghighi and Vanderwende (2009). Still, there are some parameters that can be defined and which affects the results obtained: • DUALSUM’s choice of hyper-parameters affects how the topics are learned. • The documents can be represented with ngrams of different lengths. • It is possible to generate a summary that approximates the joint distribution, the updateonly distribution, or a combination of both. This section describes how these parameters have been tuned. 4.1 Parameter tuning We use the TAC 2008 and 2009 update task datasets as training set for tuning the hyperparameters for the model, namely the pseudocounts for the</context>
<context position="22632" citStr="Haghighi and Vanderwende, 2009" startWordPosition="3766" endWordPosition="3769">ht part of the curve the update model is given very little importance in generating the summary. Based on these results, from this point onwards, the mixture weight has been set to 0.7. Note that using only the joint distribution (setting the mixture weight to 1.0) also produces reasonable results, hinting that it successfully incorporates the most important n-grams from across the base and the update collections at the same time. A second parameter is the size of the n-grams for representing the documents. The original implementations of SUMBASIC (Nenkova and Vanderwende, 2005) and TOPICSUM (Haghighi and Vanderwende, 2009) were defined over sin219 gle words (unigrams). Still, Haghighi and Vanderwende (2009) report some improvements in the ROUGE-2 score when representing words as a bag of bigrams, and Darling (2010) mention similar improvements when running SUMBASIC with bigrams. Figure 4 shows the effect on the ROUGE-2 curve when we switch to using unigrams and trigrams. As stated in previous work, using bigrams has better results than using unigrams. Using trigrams was worse than either of them. This is probably because trigrams are too specific and the document collections are small, so the models are more li</context>
<context position="27838" citStr="Haghighi and Vanderwende, 2009" startWordPosition="4598" endWordPosition="4601"> TAC-2011 3rd system (Peer 17) 0.3558†* 0.0886 0.1279†* Table 1: Results on the TAC-2011 dataset. t, t and * indicate that a result is significantly better than TOPICSUMB, TOPICSUMAUB and TOPICSUMA+TOPICSUMB, respectively (p &lt; 0.05). 4.4 Manual evaluation While the ROUGE metrics provides an arguable estimate of the informativeness of a generated summary, it does not account for other important aspects such as the readability or the overall responsiveness. To evaluate such aspects, a manual evaluation is required. A fairly standard approach for manual evaluation is through pairwise comparison (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2011). In this approach, raters are presented with pairs of summaries generated by two systems and they are asked to say which one is best with respect to some aspects. We followed a similar approach to compare DualSum with Peer 43 - the best system with respect to ROUGE-2, on the TAC 2011 dataset. For each collection, raters were presented with three summaries: a reference summary randomly chosen from the model summaries, and the summaries generated by Peer 43 and DualSum. They were asked to read the summaries and say which one of the two generated summaries is </context>
<context position="32135" citStr="Haghighi and Vanderwende, 2009" startWordPosition="5311" endWordPosition="5314"> the collection and document-specific models, and the topic assignment for the words. Using this modified implementation, it is now possible to summarize a single collection independently. The summarization of a single collection of the size of the TAC collections is reduced on average to only three seconds on the same hardware settings, allowing the use of this summarizer in an on-line application. 5 Conclusions The main contribution of this paper is DUALSUM, a new topic model that is specifically designed to identify and extract novelty from pairs of collections. It is inspired by TOPICSUM (Haghighi and Vanderwende, 2009), with two main changes: Firstly, while TOPICSUM can only learn the main topic of a collection, DUALSUM focuses on the differences between two collections. Secondly, while TOPICSUM incorporates an additional layer to model topic distributions at the sentence level, we have found that relaxing this assumption and modeling the topic distribution at document level does not decrease the ROUGE scores and reduces the sampling time. The generated summaries, tested on the TAC2011 collection, would have resulted on the second and third position in the last summarization competition according to the dif</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>A. Haghighi and L. Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Jin</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>The thu summarization systems at tac</title>
<date>2010</date>
<booktitle>In Proceedings of the third Text Analysis Conference, TAC2010.</booktitle>
<contexts>
<context position="2929" citStr="Jin et al., 2010" startWordPosition="442" endWordPosition="445">ection B that is as dissimilar as possible from either collection A or from a summary of collection A. A problem with this approach is that it can easily classify as redundant sentences in which novel information is mixed with existing information (from collection A). Furthermore, while this approach can identify sentences that contain novel information, it cannot model explicitly what the novel information is. Recently, Bayesian models have successfully been applied to multi-document summarization showing state-of-the-art results in summarization competitions (Haghighi and Vanderwende, 2009; Jin et al., 2010). These approaches offer clear and rigorous probabilistic interpretations that many other techniques lack. Furthermore, they have the advantage of operating in unsupervised settings, which can be used in real-world scenarios, across domains and languages. To our best knowledge, previous work has not used this approach for update summarization. In this article, we propose a novel nonparametric Bayesian approach for update summarization. Our approach, which is a variation of Latent 214 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pa</context>
</contexts>
<marker>Jin, Huang, Zhu, 2010</marker>
<rawString>Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. The thu summarization systems at tac 2010. In Proceedings of the third Text Analysis Conference, TAC2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lerman</author>
<author>Ryan McDonald</author>
</authors>
<title>Contrastive summarization: an experiment with consumer reviews.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, NAACL-Short ’09,</booktitle>
<pages>113--116</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8538" citStr="Lerman and McDonald, 2009" startWordPosition="1336" endWordPosition="1340">d to collection B as the update collection. Update summarization is related to novelty detection which can be defined as the problem of determining whether a document contains new information given an existing collection (Soboroff and Harman, 2005). Thus, while the goal of novelty detection is to determine whether some information is new, the goal of update summarization is to extract and synthesize the novel information. Update summarization is also related to contrastive summarization, i.e. the problem of jointly generating summaries for two entities in order to highlight their differences (Lerman and McDonald, 2009). The primary difference here is that update summarization aims to extract novel or updated information in the update collection with respect to the base collection. The most common approach for update summarization is to apply a normal multi-document summarizer, with some added functionality to remove sentences that are redundant with respect to collection A. This can be achieved using simple filtering rules (Fisher and Roark, 2008), Maximal Marginal Relevance (Boudin et al., 2008), or more complex graph-based algorithms (Shen and Li, 2010; Wenjie et al., 2008). The goal here is to boost sent</context>
</contexts>
<marker>Lerman, McDonald, 2009</marker>
<rawString>Kevin Lerman and Ryan McDonald. 2009. Contrastive summarization: an experiment with consumer reviews. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, NAACL-Short ’09, pages 113–116, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan Li</author>
<author>Liang Du</author>
<author>Yi-Dong Shen</author>
</authors>
<title>Graphbased marginal ranking for update summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the Eleventh SIAM International Conference on Data Mining. SIAM / Omnipress.</booktitle>
<contexts>
<context position="2125" citStr="Li et al., 2011" startWordPosition="323" endWordPosition="326">nerate a summary of B under the assumption that the user of the summary has already read the documents in A. Extractive techniques are the most common approaches in multi-document summarization. Summaries generated by such techniques consist of sentences extracted from the document collection. Extracts can have coherence and cohesion problems, but they generally offer a good tradeoff between linguistic quality and informativeness. While numerous extractive summarization techniques have been proposed for multidocument summarization (Erkan and Radev, 2004; Radev et al., 2004; Shen and Li, 2010; Li et al., 2011), few techniques have been specifically designed for update summarization. Most existing approaches handle it as a redundancy removal problem, with the goal of producing a summary of collection B that is as dissimilar as possible from either collection A or from a summary of collection A. A problem with this approach is that it can easily classify as redundant sentences in which novel information is mixed with existing information (from collection A). Furthermore, while this approach can identify sentences that contain novel information, it cannot model explicitly what the novel information is</context>
</contexts>
<marker>Li, Du, Shen, 2011</marker>
<rawString>Xuan Li, Liang Du, and Yi-Dong Shen. 2011. Graphbased marginal ranking for update summarization. In Proceedings of the Eleventh SIAM International Conference on Data Mining. SIAM / Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Mason</author>
<author>Eugene Charniak</author>
</authors>
<title>Extractive multi-document summaries should explicitly not contain document-specific content.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, WASDGML ’11,</booktitle>
<pages>49--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6680" citStr="Mason and Charniak, 2011" startWordPosition="1030" endWordPosition="1034">um. Usually, a smoothing factor ,r is applied on the candidate distribution S in order to avoid the divergence to be undefined1. This objective function selects the most representative sentences of the collection, and at the same time it also diversifies the generated summary by penalizing redundancy. Since the problem of finding the subset of sentences from a collection that minimizes the KL divergence is NP-complete, a greedy algorithm is often used in practice2. Some variations of this objective function can be considered, such as penalizing sentences that contain document-specific topics (Mason and Charniak, 2011) or rewarding sentences appearing closer to the beginning of the document. Wang et al. (2009) propose a Bayesian approach for summarization that does not use KL for reranking. In their model, Bayesian Sentencebased Topic Models, every sentence in a document is assumed to be associated to a unique latent topic. Once the model parameters have been calculated, a summary is generated by choosing the sentence with the highest probability for each topic. While hierarchical topic modeling approaches have shown remarkable effectiveness in learning the latent topics of document collections, they are no</context>
</contexts>
<marker>Mason, Charniak, 2011</marker>
<rawString>Rebecca Mason and Eugene Charniak. 2011. Extractive multi-document summaries should explicitly not contain document-specific content. In Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, WASDGML ’11, pages 49–54, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>L Vanderwende</author>
</authors>
<title>The impact of frequency on summarization. Microsoft Research,</title>
<date>2005</date>
<pages>2005--101</pages>
<location>Redmond, Washington, Tech. Rep.</location>
<contexts>
<context position="22586" citStr="Nenkova and Vanderwende, 2005" startWordPosition="3760" endWordPosition="3763">int performance slowly degrades as at the right part of the curve the update model is given very little importance in generating the summary. Based on these results, from this point onwards, the mixture weight has been set to 0.7. Note that using only the joint distribution (setting the mixture weight to 1.0) also produces reasonable results, hinting that it successfully incorporates the most important n-grams from across the base and the update collections at the same time. A second parameter is the size of the n-grams for representing the documents. The original implementations of SUMBASIC (Nenkova and Vanderwende, 2005) and TOPICSUM (Haghighi and Vanderwende, 2009) were defined over sin219 gle words (unigrams). Still, Haghighi and Vanderwende (2009) report some improvements in the ROUGE-2 score when representing words as a bag of bigrams, and Darling (2010) mention similar improvements when running SUMBASIC with bigrams. Figure 4 shows the effect on the ROUGE-2 curve when we switch to using unigrams and trigrams. As stated in previous work, using bigrams has better results than using unigrams. Using trigrams was worse than either of them. This is probably because trigrams are too specific and the document co</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>A. Nenkova and L. Vanderwende. 2005. The impact of frequency on summarization. Microsoft Research, Redmond, Washington, Tech. Rep. MSR-TR2005-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Porteous</author>
<author>David Newman</author>
<author>Alexander Ihler</author>
<author>Arthur Asuncion</author>
<author>Padhraic Smyth</author>
<author>Max Welling</author>
</authors>
<title>Fast collapsed Gibbs sampling for latent Dirichlet allocation.</title>
<date>2008</date>
<booktitle>In KDD ’08: Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>569--577</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="15692" citStr="Porteous et al., 2008" startWordPosition="2575" endWordPosition="2578">mber that, for base documents, words cannot be generated by the update topic, so A denotes the 3- dimensional simplex for base documents. 11 HdE n cdn p(ucd)p(φcd) IA p(ψcd|ucd)dψcd x p(wcdn|zcdn)p(zcdn|ψcd) 217 Variational approaches (Blei et al., 2003) and collapsed Gibbs sampling (Griffiths and Steyvers, 2004) are common techniques for approximate inference in Bayesian models. They offer different advantages: the variational approach is arguably faster computationally, but the Gibbs sampling approach is in principal more accurate since it asymptotically approaches the correct distribution (Porteous et al., 2008). In this section, we provide details on a collapsed Gibbs sampling strategy to infer the model parameters of DUALSUM for a given dataset. Collapsed Gibbs sampling is a particular case of Markov Chain Monte Carlo (MCMC) that involves repeatedly sampling a topic assignment for each word in the corpus. A single iteration of the Gibbs sampler is completed after sampling a new topic for each word based on the previous assignment. In a collapsed Gibbs sampler, the model parameters are integrated out (or collapsed), allowing to only sample z. Let us call wcdn the n-th word in document d in collectio</context>
</contexts>
<marker>Porteous, Newman, Ihler, Asuncion, Smyth, Welling, 2008</marker>
<rawString>Ian Porteous, David Newman, Alexander Ihler, Arthur Asuncion, Padhraic Smyth, and Max Welling. 2008. Fast collapsed Gibbs sampling for latent Dirichlet allocation. In KDD ’08: Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 569– 577, New York, NY, USA, August. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Malgorzata Sty´s</author>
<author>Daniel Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<journal>Inf. Process. Manage.,</journal>
<pages>40--919</pages>
<marker>Radev, Jing, Sty´s, Tam, 2004</marker>
<rawString>Dragomir R. Radev, Hongyan Jing, Malgorzata Sty´s, and Daniel Tam. 2004. Centroid-based summarization of multiple documents. Inf. Process. Manage., 40:919–938, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Schilder</author>
<author>Ravikumar Kondadadi</author>
<author>Jochen L Leidner</author>
<author>Jack G Conrad</author>
</authors>
<title>Thomson reuters at tac 2008: Aggressive filtering with fastsum for update and opinion summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the first Text Analysis Conference, TAC-2008.</booktitle>
<contexts>
<context position="9835" citStr="Schilder et al., 2008" startWordPosition="1552" endWordPosition="1555">is approach is that it is likely to discard as redundant sentences in B containing novel information if it is mixed with known information from collection Another approach is to introduce specific features intended to capture the novelty in collection B. For example, comparing collections A and B, FastSum derives features for the collection B such as number of named entities in the sentence that already occurred in the old cluster or the number of new content words in the sentence not already mentioned in the old cluster that are subsequently used to train a Support Vector Machine classifier (Schilder et al., 2008). A limitation with this approach is there are no large training sets available and, the more features it has, the more it is affected by the sparsity of the training data. 3 DualSum 3.1 Model Formulation The input for DUALSUM is a set of pairs of collections of documents C = {(Ai,Bi)}i=1...m, where Ai is a base document collection and Bi is an update document collection. We use c to refer to a collection pair (A,, B,). In DUALSUM, documents are modeled as a bag of words that are assumed to be sampled from a mixture of latent topics. Each word is associated with a latent variable that specifie</context>
</contexts>
<marker>Schilder, Kondadadi, Leidner, Conrad, 2008</marker>
<rawString>Frank Schilder, Ravikumar Kondadadi, Jochen L. Leidner, and Jack G. Conrad. 2008. Thomson reuters at tac 2008: Aggressive filtering with fastsum for update and opinion summarization. In Proceedings of the first Text Analysis Conference, TAC-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Shen</author>
<author>Tao Li</author>
</authors>
<title>Multi-document summarization via the minimum dominating set.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>984--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2107" citStr="Shen and Li, 2010" startWordPosition="319" endWordPosition="322"> documents in B, generate a summary of B under the assumption that the user of the summary has already read the documents in A. Extractive techniques are the most common approaches in multi-document summarization. Summaries generated by such techniques consist of sentences extracted from the document collection. Extracts can have coherence and cohesion problems, but they generally offer a good tradeoff between linguistic quality and informativeness. While numerous extractive summarization techniques have been proposed for multidocument summarization (Erkan and Radev, 2004; Radev et al., 2004; Shen and Li, 2010; Li et al., 2011), few techniques have been specifically designed for update summarization. Most existing approaches handle it as a redundancy removal problem, with the goal of producing a summary of collection B that is as dissimilar as possible from either collection A or from a summary of collection A. A problem with this approach is that it can easily classify as redundant sentences in which novel information is mixed with existing information (from collection A). Furthermore, while this approach can identify sentences that contain novel information, it cannot model explicitly what the no</context>
<context position="9084" citStr="Shen and Li, 2010" startWordPosition="1425" endWordPosition="1428">ies in order to highlight their differences (Lerman and McDonald, 2009). The primary difference here is that update summarization aims to extract novel or updated information in the update collection with respect to the base collection. The most common approach for update summarization is to apply a normal multi-document summarizer, with some added functionality to remove sentences that are redundant with respect to collection A. This can be achieved using simple filtering rules (Fisher and Roark, 2008), Maximal Marginal Relevance (Boudin et al., 2008), or more complex graph-based algorithms (Shen and Li, 2010; Wenjie et al., 2008). The goal here is to boost sentences in B that bring out completely novel information. One problem with this approach is that it is likely to discard as redundant sentences in B containing novel information if it is mixed with known information from collection Another approach is to introduce specific features intended to capture the novelty in collection B. For example, comparing collections A and B, FastSum derives features for the collection B such as number of named entities in the sentence that already occurred in the old cluster or the number of new content words i</context>
</contexts>
<marker>Shen, Li, 2010</marker>
<rawString>Chao Shen and Tao Li. 2010. Multi-document summarization via the minimum dominating set. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 984–992, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Soboroff</author>
<author>Donna Harman</author>
</authors>
<title>Novelty detection: the trec experience.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>105--112</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8160" citStr="Soboroff and Harman, 2005" startWordPosition="1277" endWordPosition="1280">recent documents assuming that the users already read earlier documents from a collection A. We refer 1In our experiments we set T = 0.01. 2In our experiments, we follow the same approach as in (Haghighi and Vanderwende, 2009) by greedily adding sentences to a summary so long as they decrease KL divergence. pT (w) log pT (w) pS(w) 215 to collection A as the base collection and to collection B as the update collection. Update summarization is related to novelty detection which can be defined as the problem of determining whether a document contains new information given an existing collection (Soboroff and Harman, 2005). Thus, while the goal of novelty detection is to determine whether some information is new, the goal of update summarization is to extract and synthesize the novel information. Update summarization is also related to contrastive summarization, i.e. the problem of jointly generating summaries for two entities in order to highlight their differences (Lerman and McDonald, 2009). The primary difference here is that update summarization aims to extract novel or updated information in the update collection with respect to the base collection. The most common approach for update summarization is to </context>
</contexts>
<marker>Soboroff, Harman, 2005</marker>
<rawString>Ian Soboroff and Donna Harman. 2005. Novelty detection: the trec experience. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 105–112, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Shenghuo Zhu</author>
<author>Tao Li</author>
<author>Yihong Gong</author>
</authors>
<title>Multi-document summarization using sentence-based topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09,</booktitle>
<pages>297--300</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6773" citStr="Wang et al. (2009)" startWordPosition="1047" endWordPosition="1050">divergence to be undefined1. This objective function selects the most representative sentences of the collection, and at the same time it also diversifies the generated summary by penalizing redundancy. Since the problem of finding the subset of sentences from a collection that minimizes the KL divergence is NP-complete, a greedy algorithm is often used in practice2. Some variations of this objective function can be considered, such as penalizing sentences that contain document-specific topics (Mason and Charniak, 2011) or rewarding sentences appearing closer to the beginning of the document. Wang et al. (2009) propose a Bayesian approach for summarization that does not use KL for reranking. In their model, Bayesian Sentencebased Topic Models, every sentence in a document is assumed to be associated to a unique latent topic. Once the model parameters have been calculated, a summary is generated by choosing the sentence with the highest probability for each topic. While hierarchical topic modeling approaches have shown remarkable effectiveness in learning the latent topics of document collections, they are not designed to capture the novel information in a collection with respect to another one, whic</context>
</contexts>
<marker>Wang, Zhu, Li, Gong, 2009</marker>
<rawString>Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong Gong. 2009. Multi-document summarization using sentence-based topic models. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09, pages 297–300, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Wang</author>
</authors>
<title>Distributed gibbs sampling of latent dirichlet allocation: The gritty details.</title>
<date>2011</date>
<contexts>
<context position="17040" citStr="Wang, 2011" startWordPosition="2824" endWordPosition="2825"> topic assignments except the assignment zcdn. p(zcdn = j|w, u, z−cdn, γ�, γ&amp;quot;, λ) a n(wcdn) + cdn,j λj EkEK/ (cd) ucd ln−cdn,k + γk ) where K = {G, cd, Ac, BcJ, n(v) −cdn,j denotes the number of times word v is assigned to topic j excluding current assignment of word wcdn and n(cd) denotes the number of words in document −cdn,k d of collection c that are assigned to topic j excluding current assignment of word wcdn. After each sampling iteration, the model parameters can be estimated using the following formulas5. cd ψcd k= nk + λk En(cd) . + V λk 5The interested reader is invited to consult (Wang, 2011) for more details on using Gibbs sampling for LDA-like models where k E K, n(v) k denotes the number of times word v is assigned to topic k, and n(kcd) denotes the number of words in document d of collection c that are assigned to topic k. By the strong law of large numbers, the average of sample parameters should converge towards the true expected value of the model parameter. Therefore, good estimates of the model parameters can be obtained averaging over the sampled values. As suggested by Gamerman and Lopes (2006), we have set a lag (20 iterations) between samples in order to reduce auto-c</context>
</contexts>
<marker>Wang, 2011</marker>
<rawString>Yi Wang. 2011. Distributed gibbs sampling of latent dirichlet allocation: The gritty details.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Wenjie</author>
<author>Wei Furu</author>
<author>Lu Qin</author>
<author>He Yanxiang</author>
</authors>
<title>Pnr2: ranking sentences with positive and negative reinforcement for query-oriented update summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics -Volume 1, COLING ’08,</booktitle>
<pages>489--496</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9106" citStr="Wenjie et al., 2008" startWordPosition="1429" endWordPosition="1432">hlight their differences (Lerman and McDonald, 2009). The primary difference here is that update summarization aims to extract novel or updated information in the update collection with respect to the base collection. The most common approach for update summarization is to apply a normal multi-document summarizer, with some added functionality to remove sentences that are redundant with respect to collection A. This can be achieved using simple filtering rules (Fisher and Roark, 2008), Maximal Marginal Relevance (Boudin et al., 2008), or more complex graph-based algorithms (Shen and Li, 2010; Wenjie et al., 2008). The goal here is to boost sentences in B that bring out completely novel information. One problem with this approach is that it is likely to discard as redundant sentences in B containing novel information if it is mixed with known information from collection Another approach is to introduce specific features intended to capture the novelty in collection B. For example, comparing collections A and B, FastSum derives features for the collection B such as number of named entities in the sentence that already occurred in the old cluster or the number of new content words in the sentence not alr</context>
</contexts>
<marker>Wenjie, Furu, Qin, Yanxiang, 2008</marker>
<rawString>Li Wenjie, Wei Furu, Lu Qin, and He Yanxiang. 2008. Pnr2: ranking sentences with positive and negative reinforcement for query-oriented update summarization. In Proceedings of the 22nd International Conference on Computational Linguistics -Volume 1, COLING ’08, pages 489–496, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>