<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.999339">
Low-Rank Regularization for Sparse Conjunctive Feature Spaces:
An Application to Named Entity Classification
</title>
<author confidence="0.832848">
Audi Primadhanty Xavier Carreras Ariadna Quattoni
</author>
<affiliation confidence="0.600227">
Universitat Polit`ecnica de Catalunya Xerox Research Centre Europe
</affiliation>
<email confidence="0.9808105">
primadhanty@cs.upc.edu xavier.carreras@xrce.xerox.com
ariadna.quattoni@xrce.xerox.com
</email>
<sectionHeader confidence="0.997289" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999568555555556">
Entity classification, like many other
important problems in NLP, involves
learning classifiers over sparse high-
dimensional feature spaces that result
from the conjunction of elementary fea-
tures of the entity mention and its context.
In this paper we develop a low-rank reg-
ularization framework for training max-
entropy models in such sparse conjunctive
feature spaces. Our approach handles con-
junctive feature spaces using matrices and
induces an implicit low-dimensional rep-
resentation via low-rank constraints. We
show that when learning entity classifiers
under minimal supervision, using a seed
set, our approach is more effective in con-
trolling model capacity than standard tech-
niques for linear classifiers.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919518518519">
Many important problems in NLP involve learn-
ing classifiers over sparse high-dimensional fea-
ture spaces that result from the conjunction of el-
ementary features. For example, to classify an en-
tity in a document, it is standard to exploit features
of the left and right context in which the entity oc-
curs as well as spelling features of the entity men-
tion itself. These sets of features can be grouped
into vectors which we call elementary feature vec-
tors. In our example, there will be one elementary
feature vector for the left context, one for the right
context and one for the features of the mention.
Observe that, when the elementary vectors consist
of binary indicator features, the outer product of
any pair of vectors represents all conjunctions of
the corresponding elementary features.
Ideally, we would like to train a classifier that
can leverage all conjunctions of elementary fea-
tures, since among them there might be some
that are discriminative for the classification task at
hand. However, allowing for such expressive high
dimensional feature space comes at a cost: data
sparsity becomes a key challenge and controlling
the capacity of the model is crucial to avoid over-
fitting the training data.
The problem of data sparsity is even more se-
vere when the goal is to train classifiers with min-
imal supervision, i.e. small training sets. For ex-
ample, in the entity classification setting we might
be interested in training a classifier using only a
small set of examples of each entity class. This
is a typical scenario in an industrial setting, where
developers are interested in classifying entities ac-
cording to their own classification schema and can
only provide a handful of examples of each class.
A standard approach to control the capacity of a
linear classifier is to use E1 or E2 regularization on
the parameter vector. However, this type of regu-
larization does not seem to be effective when deal-
ing with sparse conjunctive feature spaces. The
main limitation is that E1 and E2 regularization can
not let the model give weight to conjunctions that
have not been observed at training. Without such
ability it is unlikely that the model will generalize
to novel examples, where most of the conjunctions
will be unseen in the training set.
Of course, one could impose a strong prior on
the weight vector so that it assigns weight to un-
seen conjunctions, but how can we build such a
prior? What kind of reasonable constraints can we
put on unseen conjunctions?
Another common approach to handle high di-
mensional conjunctive feature spaces is to manu-
ally design the feature function so that it includes
</bodyText>
<page confidence="0.976745">
126
</page>
<note confidence="0.977939">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 126–135,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999273066666667">
only a subset of “relevant” conjunctions. But de-
signing such a feature function can be time con-
suming and one might need to design a new fea-
ture function for each classification task. Ide-
ally, we would have a learning algorithm that does
not require such feature engineering and that it
can automatically leverage rich conjunctive fea-
ture spaces.
In this paper we present a solution to this prob-
lem by developing a regularization framework
specifically designed for sparse conjunctive fea-
ture spaces. Our approach results in a more effec-
tive way of controlling model capacity and it does
not require feature engineering.
Our strategy is based on:
</bodyText>
<listItem confidence="0.926634428571429">
• Employing tensors to define the scoring func-
tion of a max-entropy model as a multilinear
form that computes weighted inner products
between elementary vectors.
• Forcing the model to induce low-dimensional
embeddings of elementary vectors via low-
rank regularization on the tensor parameters.
</listItem>
<bodyText confidence="0.99963396">
The proposed regularization framework is based
on a simple conceptual trick. The standard ap-
proach to handle conjunctive feature spaces in
NLP is to regard the parameters of the linear
model as long vectors computing an inner prod-
uct with a high dimensional feature representation
that lists explicitly all possible conjunctions. In-
stead, the parameters of our the model will be ten-
sors and the compatibility score between an input
pattern and a class will be defined as the sum of
multilinear functions over elementary vectors.
We then show that the rank1 of the tensor has a
very natural interpretation. It can be seen as the
intrinsic dimensionality of a latent embedding of
the elementary feature vectors. Thus by impos-
ing a low-rank penalty on the tensor parameters
we are encouraging the model to induce a low-
dimensional projection of the elementary feature
vectors . Using the rank itself as a regularization
constraint in the learning algorithm would result
in a non-convex optimization. Instead, we follow
a standard approach which is to use the nuclear
norm as a convex relaxation of the rank.
In summary the main contributions of this paper
are:
</bodyText>
<footnote confidence="0.76572825">
1There are many ways of defining the rank of a tensor. In
this paper we matricize tensors into matrices and use the rank
of the resulting matrix. Matricization is also referred to as
unfolding.
</footnote>
<listItem confidence="0.9229065">
• We develop a new regularization frame-
work for training max-entropy models in
high-dimensional sparse conjunctive feature
spaces. Since the proposed regularization im-
plicitly induces a low dimensional embed-
ding of feature vectors, our algorithm can
also be seen as a way of implicitly learning
a latent variable model.
• We present a simple convex learning al-
gorithm for training the parameters of the
model.
• We conduct experiments on learning entity
classifiers with minimal supervision. Our re-
sults show that the proposed regularization
</listItem>
<bodyText confidence="0.994540428571429">
framework is better for sparse conjunctive
feature spaces than standard E2 and E1 reg-
ularization. These results make us conclude
that encouraging the max-entropy model to
operate on a low-dimensional space is an ef-
fective way of controlling the capacity of the
model an ensure good generalization.
</bodyText>
<sectionHeader confidence="0.957069" genericHeader="method">
2 Entity Classification with Log-linear
Models
</sectionHeader>
<bodyText confidence="0.999836555555556">
The formulation we develop in this paper applies
to any prediction task whose inputs are some form
of tuple. We focus on classification of entity men-
tions, or entities in the context of a sentence. For-
mally, our input objects are tuples x = (l, e, r)
consisting of an entity e, a left context l and a right
context r. The goal is to classify x into one entity
class in the set Y.
We will use log-linear models of the form:
</bodyText>
<equation confidence="0.983211666666667">
exp{se(x, y)}
Pr(y  |x; θ) = (1)
,y, exp{se(x, y&apos;)}
</equation>
<bodyText confidence="0.998972888888889">
where se : X x Y → R is a scoring function of
entity tuples with a candidate class, and θ are the
parameters of this function, to be specified below.
In the literature it is common to employ a
feature-based linear model. That is, one defines a
feature function O : X → {0,1}n that represents
entity tuples in an n-dimensional binary feature
space2, and the model has a weight vector for each
class, θ = {wy}y∈Y. Then se(x, y) = O(x) · wy.
</bodyText>
<footnote confidence="0.945829857142857">
2In general, all models in this paper accept real-valued
feature functions. But we focus on binary indicator features
because in practice these are the standard type of features in
NLP classifiers, and the ones we use here. In fact, in this pa-
per we develop feature spaces based on products of elemen-
tary feature functions, in which case the resulting representa-
tions correspond to conjunctions of the elementary features.
</footnote>
<page confidence="0.997226">
127
</page>
<sectionHeader confidence="0.988924" genericHeader="method">
3 Low-rank Entity Classification Models
</sectionHeader>
<bodyText confidence="0.998795">
In this section we propose a specific family of
models for classifying entity tuples.
</bodyText>
<subsectionHeader confidence="0.9740895">
3.1 A Low-rank Model of Left-Right
Contexts
</subsectionHeader>
<bodyText confidence="0.993730461538461">
We start from the observation that when repre-
senting tuple objects such as x = (l, e, r) with
features, we often depart from a feature represen-
tation of each element of the tuple. Hence, let
φl and φr be two feature functions representing
left and right contexts, with binary dimensions d1
and d2 respectively. For now, we will define a
model that ignores the entity mention e and makes
predictions using context features. It is natural
to define conjunctions of left and right features.
Hence, in its most general form, one can define
a matrix Wy E Rd,×d2 for each class, such that
θ = {Wy}y∈Y and the score is:
</bodyText>
<equation confidence="0.973743">
sθ((l, e, r), y) = φl(l)&gt;Wyφr(r) . (2)
</equation>
<bodyText confidence="0.906973935483871">
Note that this corresponds to a feature-based
linear model operating in the product space of φl
and φr, that is, the score has one term for each pair
of features: Ei,j φl(l)[i] φr(r)[j] Wy[i, j]. Note
also that it is trivial to include elementary features
of φl and φr, in addition to conjunctions, by having
a constant dimension in each of the two represen-
tations set to 1.
In all, the model in Eq. (2) is very expressive,
with the caveat that it can easily overfit the data,
specially when we work only with a handful of la-
beled examples. The standard way to control the
capacity of a linear model is via `1 or `2 regular-
ization.
Regarding our parameters as matrices allows us
to control the capacity of the model via regulariz-
ers that favor parameter matrices with low rank.
To see the effect of these regularizers, consider
that Wy has rank k, and let Wy = UyEyV&gt;y
be the singular value decomposition, where Uy E
Rd,×k and Vy E Rd2×k are orthonormal projec-
tions and Ey E Rk×k is a diagonal matrix of sin-
gular values. We can rewrite the score function as
sθ((l, e, r), y) = (φl(l)&gt;Uy) Ey (V&gt;y φr(r)) .
(3)
In words, the rank k is the intrinsic dimensionality
of the inner product behind the score function. A
low-rank regularizer will favor parameter matrices
that have low intrinsic dimensionality. Below we
describe a convex optimization for low-rank mod-
els using nuclear norm regularization.
</bodyText>
<subsectionHeader confidence="0.999947">
3.2 Adding Entity Features
</subsectionHeader>
<bodyText confidence="0.96097325">
The model above classifies entities based only on
the context. Here we propose an extension to make
use of features of the entity. Let T be a set of pos-
sible entity feature tags, i.e. tags that describe an
entity, such as ISCAPITALIZED, CONTAINSDIG-
ITS, SINGLETOKEN, ... Let φe be a feature func-
tion representing entities. For this case, to simplify
our expression, we will use a set notation and de-
note by φe(e) ⊆ T the set of feature tags that de-
scribe e. Our model will be defined with one pa-
rameter matrix per feature tag and class label, i.e.
θ = {Wt,y}t∈T ,y∈Y. The model form is:
</bodyText>
<equation confidence="0.991487666666667">
�sθ((l, e, r), y) = φl(l)&gt;Wt,y .φr(r).
t∈φe(e)
(4)
</equation>
<subsectionHeader confidence="0.999655">
3.3 Learning with Low-rank Constraints
</subsectionHeader>
<bodyText confidence="0.999996925925926">
In this section we describe a convex procedure to
learn models of the above form that have low rank.
We will define an objective that combines a loss
and a regularization term.
Our first observation is that our parameters are
a tensor with up to four axes, namely left and right
context representations, entity features, and entity
classes. While a matrix has a clear definition of
rank, it is not the case for general tensors, and
there exist various definitions in the literature. The
technique that we use is based on matricization of
the tensor, that is, turning the tensor into a matrix
that has the same parameters as the tensor but or-
ganized in two axes. This is done by partitioning
the tensor axes into two sets, one for matrix rows
and another for columns. Once the tensor has been
turned into a matrix, we can use the standard def-
inition of matrix rank. A main advantage of this
approach is that we can make use of standard rou-
tines like singular value decomposition (SVD) to
decompose the matricized tensor. This is the main
reason behind our choice.
In general, different ways of partitioning the
tensor axes will lead to different notions of intrin-
sic dimensions. In our case we choose the left con-
text axes as the row dimension, and the rest of axes
as the column dimension.3 In this section, we will
</bodyText>
<footnote confidence="0.8738655">
3In preliminary experiments we tried variations, such as
having right prefixes in the columns, and left prefixes, entity
tags and classes in the rows. We only observer minor, non-
significant variations in the results.
</footnote>
<page confidence="0.994011">
128
</page>
<bodyText confidence="0.999938466666667">
denote as W the matricized version of the param-
eters θ of our models.
The second observation is that minimizing the
rank of a matrix is a non-convex problem. We
make use of a convex relaxation based on the nu-
clear norm (Srebro and Shraibman, 2005). The
nuclear norm4 of a matrix W, denoted IIWII*, is
the sum of its singular values: IIWII* = EZ EZ,Z
where W = UEVT is the singular value decom-
position of W. This norm has been used in several
applications in machine learning as a convex sur-
rogate for imposing low rank, e.g. (Srebro et al.,
2004).
Thus, the nuclear norm is used as a regularizer.
With this, we define our objective as follows:
</bodyText>
<equation confidence="0.977264">
argmin L(W) + TR(W) , (5)
W
</equation>
<bodyText confidence="0.999890666666667">
where L(W) is a convex loss function, R(W) is a
regularizer, and T is a constant that trades off error
and capacity. In experiments we will compare nu-
clear norm regularization with E1 and E2 regulariz-
ers. In all cases we use the negative log-likelihood
as loss function, denoting the training data as D:
</bodyText>
<equation confidence="0.991507666666667">
L(W) = � −log Pr(y I (l, e, r); W) .
((l,e,r),y)ED
(6)
</equation>
<bodyText confidence="0.999957846153846">
To solve the objective in Eq. (5) we use a simple
optimization scheme known as forward-backward
splitting (FOBOS) (Duchi and Singer, 2009). In
a series of iterations, this algorithm performs a
gradient update followed by a proximal projec-
tion of the parameters. Such projection depends
on the regularizer used: for E1 it thresholds the pa-
rameters; for E2 it scales them; and for nuclear-
norm regularization it thresholds the singular val-
ues. This means that, for nuclear norm regulariza-
tion, each iteration requires to decompose W us-
ing SVD. See (Madhyastha et al., 2014) for details
about this optimization for a related application.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999884285714286">
The main aspect of our approach is the use of
a spectral penalty (i.e., the rank) to control the
capacity of multilinear functions parameterized
by matrices or tensors. Quattoni et al. (2014)
used nuclear-norm regularization to learn latent-
variable max-margin sequence taggers. Mad-
hyastha et al. (2014) defined bilexical distribu-
</bodyText>
<footnote confidence="0.704757">
4Also known as the trace norm.
</footnote>
<bodyText confidence="0.999864274509804">
tions parameterized by matrices which result lex-
ical embeddings tailored for a particular linguis-
tic relation. Like in our case, the low-dimensional
latent projections in these papers are learned im-
plicitly by imposing low-rank constraints on the
predictions of the model.
Lei et al. (2014) also use low-rank tensor learn-
ing in the context of dependency parsing, where
like in our case dependencies are represented by
conjunctive feature spaces. While the motivation
is similar, their technical solution is different. We
use the technique of matricization of a tensor com-
bined with a nuclear-norm relaxation to obtain a
convex learning procedure. In their case they ex-
plicitly look for a low-dimensional factorization of
the tensor using a greedy alternating optimization.
Also recently, Yao et al. (2013) have framed
entity classification as a low-rank matrix comple-
tion problem. The idea is based on the fact that if
two entities (in rows) have similar descriptions (in
columns) they should have similar classes. The
low-rank structure of the matrix defines intrin-
sic representations of entities and feature descrip-
tions. The same idea was applied to relation ex-
traction (Riedel et al., 2013), using a matrix of
entity pairs times descriptions that corresponds to
a matricization of an entity-entity-description ten-
sor. Very recently Singh et al. (2015) explored al-
ternative ways of applying low-rank constraints to
tensor-based relation extraction.
Another aspect of this paper is training entity
classification models using minimal supervision,
which has been addressed by multiple works in
the literature. A classical successful approach
for this problem is to use co-training (Blum and
Mitchell, 1998): learn two classifiers that use dif-
ferent views of the data by using each other’s pre-
dictions. In the same line, Collins and Singer
(1999) trained entity classifiers by bootstraping
from an initial set of seeds, using a boosting ver-
sion of co-training. Seed sets have also been ex-
ploited by graphical model approaches. Haghighi
and Klein (2006) define a graphical model that is
soft-constrained such that the prediction for an un-
labeled example agrees with the labels of seeds
that are distributionally similar. Li et al. (2010)
present a Bayesian approach to expand an initial
seed set, with the goal of creating a gazetteer.
Another approach to entity recognition that, like
in our case, learns projections of contextual fea-
tures is the method by Ando and Zhang (2005).
</bodyText>
<page confidence="0.991076">
129
</page>
<table confidence="0.9452710625">
Class Nb Mentions
10-30 Seed
10-30 40-120 640-1920 All
334 747 3,133 6,516
1,384 2,885 5,812 6,159
295 699 3,435 5,271
611 1326 3,085 3,205
5,326 11,595 31,071 36,673
PER clinton, dole, arafat, yeltsin, wasim akram, lebed, dutroux, waqar you-
nis, mushtaq ahmed, croft
LOC u.s., england, germany, britain, australia, france, spain, pakistan, italy,
china
ORG reuters, u.n., oakland, puk, osce, cincinnati, eu, nato, ajax, honda
MISC russian, german, british, french, dutch, english, israeli, european, iraqi,
australian
O year, percent, thursday, government, police, results, tuesday, soccer,
</table>
<tableCaption confidence="0.7802204">
president, monday, friday, people, minister, sunday, division, week,
time, state, market, years, officials, group, company, saturday, match,
at, world, home, august, standings
Table 1: For each entity class, the seed of entities for the 10-30 set, together with the number of mentions
in the training data that involve entities in the seed for various sizes of the seeds.
</tableCaption>
<bodyText confidence="0.999606615384615">
They define a set of auxiliary tasks, which can be
supervised using unlabeled data, and find a projec-
tion of the data that works well as input represen-
tation for the auxiliary tasks. This representation
is then used for the target task.
More recently Neelakantan and Collins (2014)
presented another approach to gazetteer expansion
using an initial seed. A novel aspect is the use
of Canonical Correlation Analysis (CCA) to com-
pute embeddings of entity contexts, that are used
by the named entity classifier. Like in our case,
their method learns a compressed representation
of contexts that helps prediction.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999990363636364">
In this section we evaluate our regulariza-
tion framework for training models in high-
dimensional sparse conjunctive feature spaces. We
run experiments on learning entity classifiers with
minimal supervision. We focus on classification of
unseen entities to highlight the ability of the reg-
ularizer to generalize over conjunctions that are
not observed at training. We simulate minimal
supervision using the CoNLL-2003 Shared Task
data (Tjong Kim Sang and De Meulder, 2003), and
compare the performance to E1 and E2 regularizers.
</bodyText>
<subsectionHeader confidence="0.980282">
5.1 Minimal Supervision Task
</subsectionHeader>
<bodyText confidence="0.999971">
We use a minimal supervision setting where we
provide the algorithm a seed of entities for each
class, that is, a list of entities that is representative
for that class. The assumption is that any men-
tion of an entity in the seed is a positive example
for the corresponding class. Given unlabeled data
and a seed of entities for each class, the goal is
to learn a model that correctly classifies mentions
of entities that are not in the seed. In addition to
standard entity classes, we also consider a special
non-entity class, which is part of the classification
but is excluded from evaluation.
Note that named entity classification for unseen
entities is a challenging problem. Even in the stan-
dard fully-supervised scenario, when we measure
the performance of state-of-the-art methods on un-
seen entities, the F1 values are in the range of 60%.
This represents a significant drop with respect to
the standard metrics for named entity recognition,
which consider all entity mentions of the test set
irrespective of whether they appear in the training
data or not, and where F1 values at 90% levels are
obtained (e.g. (Ratinov and Roth, 2009)). This
suggests that part of the success of state-of-the-art
models is in storing known entities together with
their type (in the form of gazetteers or directly in
lexicalized parameters of the model).
</bodyText>
<subsectionHeader confidence="0.999402">
5.2 Setting
</subsectionHeader>
<bodyText confidence="0.999968">
We use the CoNLL-2003 English data, which is
annotated with four types: person (PER), location
(LOC), organization (ORG), and miscellaneous
(MISC). In addition, the data is tagged with parts-
of-speech (PoS), and we compute word clusters
running the Brown clustering algorithm (Brown et
al., 1992) on the words in the training set.
We consider annotated entity phrases as candi-
date entities, and all single nouns that are not part
of an entity as candidate non-entities (O). Both
candidate entities and non-entities will be referred
to as candidates in the remaining of this section.
We lowercase all candidates and remove the am-
</bodyText>
<page confidence="0.995802">
130
</page>
<table confidence="0.999768272727273">
Features Window Bag-of-words N-grams
Lexical Cluster Lexical Cluster
1 13.63 14.59 13.63 14.59
Elementary features of left and right contexts 2 15.49 13.86 13.08 13.54
3 12.18 14.45 12.14 13.28
1 12.90 13.75 12.90 13.75
Only full conjunctions of left and right contexts 2 8.59 8.85 12.31 12.43
3 8.57 10.59 10.15 10.49
1 15.30 16.98 15.30 16.98
Elementary features and all conjunctions of left and right contexts 2 13.26 12.89 14.28 15.33
3 11.87 11.54 13.94 13.15
</table>
<tableCaption confidence="0.991905">
Table 2: Average-F1 of classification of unseen entity candidates on development data, using the 10-30
</tableCaption>
<bodyText confidence="0.995648782608696">
training seed and E2 regularization, for different conjunctive spaces (elementary only, full conjunctions,
all). Bag-of-words elementary features contain all clusters/PoS in separate windows to the left and to
the right of the candidate. N-grams elementary features contain all n-grams of clusters/PoS in separate
left and right windows (e.g. for size 3 it includes unigrams, bigrams and trigrams on each side).
biguous ones (i.e., those with more than one label
in different mentions).5
To simulate a minimal supervision, we create
supervision seeds by picking the n most frequent
training candidates for entity types, and the m
most frequent candidate non-entities. We create
seeds of various sizes n-m, namely 10-30, 40-120,
640-1920, as well as all of the candidates. For
each seed, the training set consists of all training
mentions that involve entities in the seed. Table 1
shows the smaller seed, as well as the number of
mentions for each seed size.
For evaluation we use the development and test
sections of the data, but we remove the instances
of candidates in the training data (i.e., that are in
the all seed). We do not remove instances that are
ambiguous in the tests. 6 As evaluation metric we
use the average F1 score computed over all entity
types, excluding the non-entity type.
</bodyText>
<footnote confidence="0.8343521">
5In the CoNLL-2003 English training set, only 235 can-
didates are ambiguous out of 13,441 candidates, i.e. less than
2%. This suggests that in this data the difficulty behind the
task is in recognizing and classifying unseen entities, and not
in disambiguating known entities in a certain context.
6After removing the ambiguous candidates from the train-
ing data, and removing candidates seen in the training from
the development and test sets, this is the number of mentions
(and number of unique candidates in parenthesis) in the data
used in our experiments:
</footnote>
<table confidence="0.991387666666667">
training dev. test
PER 6,516 (3,489) 1,040 (762) 1,342 (925)
LOC 6,159 ( 987) 176 (128) 246 (160)
ORG 5,271 (2,149) 400 (273) 638 (358)
MISC 3,205 ( 760) 177 (142) 213 (152)
O 36,673 (5,821) 951 (671) 995 (675)
</table>
<subsectionHeader confidence="0.995885">
5.3 Context Representations
</subsectionHeader>
<bodyText confidence="0.999897931034483">
We refer to context as the sequence of tokens be-
fore (left context) and after (right context) a can-
didate mention in a sentence. Different classifiers
can be built using different representations of the
contexts. For example we can change the window
size of the context sequence (i.e., for a window
size of 1 we only use the last token before the men-
tion and the first token after the mention). We can
treat the left and right contexts independently of
each other, we can treat them as a unique combi-
nation, or we can use both. We can also choose to
use the word form of a token, its PoS tag, a word
cluster, or a combination of these.
Table 2 compares different context represen-
tations and their performance in classifying un-
seen candidates using maximum-entropy classi-
fiers trained with Mallet (McCallum, 2002) with
E2 regularization, using the 10-30 seed. We use
the lexical representation (the word itself) and a
word cluster representation of the context tokens
and use a window size of one to three. We use
two types of features: bag-of-words features (1-
grams of tokens in the specified window) and n-
gram features (with n smaller or equal to the win-
dow size). The performance of using word clusters
is comparable, and sometimes better, to using lexi-
cal representations. Moreover, using a longer win-
dow, in this case, does not necessarily result in bet-
ter performance. 7 In the rest of the experiments
</bodyText>
<footnote confidence="0.933795">
7Our learner and feature configuration, using 22 regular-
ization, obtains state-of-the-art results on the standard evalu-
</footnote>
<page confidence="0.995328">
131
</page>
<figure confidence="0.998206129032259">
Seed set
(a) Only full conjunctions of left-right contexts (cluster),
window size = 1
Seed set
(c) Elementary features and all conjunctions of entity tags and
left-right contexts (cluster), window size = 1
Seed set
(e) Elementary features and all conjunctions of entity tags and
left-right contexts (cluster &amp; PoS), window size = 1
Seed set
(b) Only full conjunctions of entity tags and left-right contexts
(cluster), window size = 1
Seed set
(d) Elementary features and all conjunctions of entity tags and
left-right contexts (cluster), window size = 2
Seed set
(f) Elementary features and all conjunctions of entity tags and
left-right contexts (cluster &amp; PoS), window size = 2
60
40
28.25
25.11
17.72
14.12
17.58
14.23
42.81
38.3
27.18 28.54
27.41 28.88
L1
L2
NN
0
10-30 40-120 640-1920 All
60
41
36.87
28.48 25.91
28.57 25.3
60.94
56.16
44.2
40.45
45.04
39.52
L1
L2
NN
AVG-F1 (%)
0
10-30 40-120 640-1920 All
61.13
57.7
41.14
37.1
29.67 27.07
30.21 27.05
20
0
10-30 40-120 640-1920 All
AVG-F1 (%)
60
45.16
39.84
44.67
39.74
L1
L2
NN
53.65
38.95
42.72
33.67
32.76
28.9
38.01
20.72
17.4
20.05
17.39
0
10-30 40-120 640-1920 All
32.73
L1
L2
NN
60
40
60
35.58 37.45
28.92
28.33 24.41
59.46
56.74
44.09
40.54
39.95
L1
L2
NN
AVG-F1 (%)
24.62
44.03
0
10-30 40-120 640-1920 All
60
38.22 37.11
28.65 27.62
28.21 28.04
58.43
54.56
44.1
39.34
42.83
37.21
L1
L2
NN
AVG-F1 (%)
0
10-30 40-120 640-1920 All
AVG-F1 (%)
AVG-F1 (%)
</figure>
<figureCaption confidence="0.7552474">
Figure 1: Average F1 of classification of unseen entity candidates on development data, with respect to
the size of the seed. NN refers to models with nuclear norm regularization, L1 and L2 refer to E1 and
E2 regularization. Each plot corresponds to a different conjunctive feature space with respect to window
size (1 or 2), context representation (cluster with/out PoS), using entity features or not, and combining
or not full conjunctions with lower-order conjunctions and elementary features.
</figureCaption>
<listItem confidence="0.999962">
• cap=1, cap=0: whether the first letter of the entity candidate is uppercase, or not
• all-low=1, all-low=0: whether all letters of the candidate are lowercase letters, or not
• all-cap1=1, all-cap1=0: whether all letters of the candidate are uppercase letters, or not
• all-cap2=1, all-cap2=0: whether all letters of the candidate are uppercase letters and periods, or not
• num-tokens=1, num-tokens=2, num-tok&gt;2: whether the candidate consists of one token, two or more
• dummy: a tag that holds for any entity candidate, used to capture context features alone
</listItem>
<tableCaption confidence="0.9807805">
Table 3: The 12 entity tags used to represent entity candidates. The tags all-cap1 and all-cap2 are from
(Neelakantan and Collins, 2014).
</tableCaption>
<page confidence="0.910952">
132
</page>
<table confidence="0.998631266666667">
PER LOC ORG MISC AVG
F1
PREC REC F1 PREC REC F1 PREC REC F1 PREC REC F1
10-30 `1 65.69 65.40 65.55 15.38 23.58 18.62 59.33 19.44 29.28 23.36 30.05 26.28 34.93
`1 65.54 64.80 65.17 15.12 23.17 18.30 60.82 18.50 28.37 23.30 30.52 26.42 34.56
NN 72.41 74.52 73.45 14.89 21.55 17.61 49.09 21.16 17.61 31.40 38.03 34.40 38.76
40-120 `1 72.16 44.07 54.72 13.38 40.24 20.08 48.89 31.19 38.09 22.03 35.68 27.24 35.03
`2 71.75 44.89 55.23 13.61 41.87 20.54 49.39 31.50 38.47 21.64 30.99 25.48 34.93
NN 75.16 61.33 67.54 13.08 20.73 16.04 49.03 35.74 41.34 29.97 47.42 36.73 40.41
640-1920 `1 79.52 62.27 69.85 23.59 44.31 30.79 55.78 47.65 51.39 19.81 30.05 23.88 43.98
`2 78.62 65.55 71.49 26.55 43.50 32.97 60.19 49.06 54.06 21.73 31.92 25.86 46.10
NN 80.73 80.55 80.64 51.91 44.31 47.81 53.82 54.08 53.95 29.14 51.17 37.14 54.88
All `1 75.58 72.48 74.00 32.84 36.18 34.43 57.28 46.24 51.17 27.93 29.11 28.51 47.03
`2 76.59 70.77 73.57 34.21 36.99 35.55 57.79 50.00 53.61 28.93 32.86 30.77 48.37
NN 73.83 90.84 81.46 64.96 36.18 46.48 72.11 44.98 55.41 37.20 43.66 40.17 55.88
</table>
<tableCaption confidence="0.995864">
Table 4: Results on the test for models trained with different sizes of the seed, using the parameters
</tableCaption>
<bodyText confidence="0.883588833333333">
and features that obtain the best evaluation results the development set. NN refers to nuclear norm
regularization, L1 and L2 refer to E1 and E2 regularization. Only test entities unseen at training are
considered. Avg. F1 is over PER, LOC, ORG and MISC, excluding O.
we will use the elementary features that are more
predictive and compact: clusters and PoS tags in
windows of size at most 2.
</bodyText>
<subsectionHeader confidence="0.997714">
5.4 Comparing Regularizers
</subsectionHeader>
<bodyText confidence="0.998819035714286">
We compare the performance of models trained
using the nuclear norm regularizer with models
trained using E1 and E2 regularizers. To train each
model, we validate the regularization parameter
and the number of iterations on development data,
trying a wide range of values. The best performing
configuration is then used for the comparison.
Figure 1 shows results on the development set
for different feature sets. We started representing
context using cluster labels, as it is the most com-
pact representation obtaining good results in pre-
liminary experiments. We tried several conjunc-
tions: a conjunction of the left and right context,
as well as conjunctions of left and right contexts
and features of the candidate entity. We also tried
all different conjunction combinations of the con-
texts and the candidate entity features, as well as
adding PoS tags to represent contexts. To repre-
sent an entity candidate we use standard traits of
the spelling of the mention, such as capitalization,
ation. Using our richest feature set, the model obtains 76.76
of accuracy in the development, for the task of classifing enti-
ties with correct boundaries. If we add features capturing the
full entity and its tokens, then the accuracy is 87.63, which
is similar to state-of-the-art performance (the best results in
literature typically exploit additional gazetteers). Since our
evaluation focuses on unknown entities, our features do not
include information about the word tokens of entites.
</bodyText>
<figure confidence="0.952796857142857">
61.73 61.13
54.24
58.14
43.93
28.22
1 2 3 4 5 6 7 8 9 10203040506070
dimension
</figure>
<figureCaption confidence="0.999742666666667">
Figure 2: Avg. F1 on development for increasing
dimensions, using the low-rank model in Figure 1e
trained with all seeds.
</figureCaption>
<bodyText confidence="0.980831411764706">
the existence of symbols, as well as the number of
tokens in the candidate. See Table 3 for the defini-
tion of the features describing entity candidates.
We observe that for most conjunction settings
our regularizer performs better than the E1 and
E2 regularizers. Using the best model from each
regularizer, we evaluated on the test set. Table
4 shows the test results. For all seed sets, the
nuclear norm regularizer obtains the best aver-
age F1 performance. This shows that encourag-
ing the max-entropy model to operate on a low-
dimensional space is effective. Moreover, Figure
2 shows model performance as a function of the
number of dimensions of the intrinsic projection.
The model obtains a good performance even if
only a few intrinsic dimensions are used.
Figure 3 shows the parameter matrix of the low-
</bodyText>
<figure confidence="0.946314692307692">
AVG-F1 (%) 60
40
20
0
50.19
133
Cluster
PoS
O PER LOC ORG MISC
(a) Full parameter matrix of the low-rank model. The ticks in x-axis indicate the space for different entity types, while the ticks
in y-axis indicate the space for different prefix context representations.
(b) The subblock for PER entity type and PoS representation of the prefixes. The ticks in x-axis indicate the space of the entity
features used, while the tick in y-axis indicates an example of a frequently observed prefix for this entity type.
</figure>
<figureCaption confidence="0.981542">
Figure 3: Parameter matrix of the low-rank model in Figure 1f trained with the 10-30 seed, with respect to
</figureCaption>
<bodyText confidence="0.9762418125">
observations of the associated features in training and development. Non-white conjunctions correspond
to non-zero weights: black is for conjunctions seen in both the training and development sets; blue is for
those seen in training but not in the development; red indicates that the conjunctions were observed only
in the development; yellow is for those not observed in training nor development.
prefix=NNP
rank model in Figure 1f trained with the 10-30
seed, with respect to observed features in training
and development data. Many of the conjunctions
of the development set were never observed in the
training set. Our regularizer framework is able to
propagate weights from the conjunctive features
seen in training to unseen conjunctive features that
are close to each other in the projected space (these
are the yellow and red cells in the matrix). In con-
trast, E1 and E2 regularization techniques can not
put weight on unseen conjunctions.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999995545454546">
We have developed a low-rank regularization
framework for training max-entropy models in
sparse conjunctive feature spaces. Our formula-
tion is based on using tensors to parameterize clas-
sifiers. We control the capacity of the model using
the nuclear-norm of a matricization of the tensor.
Overall, our formulation results in a convex proce-
dure for training model parameters.
We have experimented with these techniques in
the context of learning entity classifiers. Com-
pared to E1 and E2 penalties, the low-rank model
obtains better performance, without the need to
manually specify feature conjunctions. In our
analysis, we have illustrated how the low-rank ap-
proach can assign non-zero weights to conjunc-
tions that were unobserved at training, but are sim-
ilar to observed conjunctions with respect to the
low-dimensional projection of their elements.
We have used matricization of a tensor to define
its rank, using a fixed transformation of the tensor
into a matrix. Future work should explore how to
combine efficiently different transformations.
</bodyText>
<sectionHeader confidence="0.998713" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.993717">
We thank Gabriele Musillo and the anonymous re-
viewers for their helpful comments and sugges-
tions. This work has been partially funded by the
Spanish Government through the SKATER project
(TIN2012-38584-C06-01) and an FPI predoctoral
grant for Audi Primadhanty.
</bodyText>
<page confidence="0.998269">
134
</page>
<sectionHeader confidence="0.998279" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999753490196078">
Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from multi-
ple tasks and unlabeled data. J. Mach. Learn. Res.,
6:1817–1853, December.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of the Eleventh Annual Conference on
Computational Learning Theory, COLT’ 98, pages
92–100, New York, NY, USA. ACM.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467–479.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10:2899–
2934.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Main Conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
HLT-NAACL ’06, pages 320–327, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1381–1391, Baltimore, Maryland, June. Association
for Computational Linguistics.
Xiao-Li Li, Lei Zhang, Bing Liu, and See-Kiong
Ng. 2010. Distributional similarity vs. pu learn-
ing for entity set expansion. In Proceedings of the
ACL 2010 Conference Short Papers, ACLShort ’10,
pages 359–364, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Pranava Swaroop Madhyastha, Xavier Carreras, and
Ariadna Quattoni. 2014. Learning Task-specific
Bilexical Embeddings. In Proceedings of COLING
2014, the 25th International Conference on Compu-
tational Linguistics: Technical Papers, pages 161–
171, Dublin, Ireland, August. Dublin City Univer-
sity and Association for Computational Linguistics.
Andrew K. McCallum. 2002. Mallet: A machine
learning for language toolkit.
Arvind Neelakantan and Michael Collins. 2014.
Learning dictionaries for named entity recognition
using minimal supervision. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 452–
461, Gothenburg, Sweden, April. Association for
Computational Linguistics.
Ariadna Quattoni, Borja Balle, Xavier Carreras, and
Amir Globerson. 2014. Spectral regularization for
max-margin sequence tagging. In Tony Jebara and
Eric P. Xing, editors, Proceedings of the 31st Inter-
national Conference on Machine Learning (ICML-
14), pages 1710–1718. JMLR Workshop and Con-
ference Proceedings.
Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147–155, Boulder, Colorado,
June. Association for Computational Linguistics.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 74–84, Atlanta, Georgia, June. Association
for Computational Linguistics.
Sameer Singh, Tim Rockt¨aschel, and Sebastian Riedel.
2015. Towards Combined Matrix and Tensor Fac-
torization for Universal Schema Relation Extraction.
In NAACL Workshop on Vector Space Modeling for
NLP (VSM).
Nathan Srebro and Adi Shraibman. 2005. Rank, trace-
norm and max-norm. In Learning Theory, pages
545–560. Springer Berlin Heidelberg.
Nathan Srebro, Jason Rennie, and Tommi S Jaakkola.
2004. Maximum-margin matrix factorization. In
Advances in neural information processing systems,
pages 1329–1336.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared
Task: Language-Independent Named Entity Recog-
nition. In Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003,
pages 142–147.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2013. Universal schema for entity type prediction.
In Proceedings of the 2013 Workshop on Automated
Knowledge Base Construction, AKBC ’13, pages
79–84, New York, NY, USA. ACM.
</reference>
<page confidence="0.998791">
135
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912750">
<title confidence="0.9997685">Low-Rank Regularization for Sparse Conjunctive Feature An Application to Named Entity Classification</title>
<author confidence="0.997808">Audi Primadhanty Xavier Carreras Ariadna Quattoni</author>
<affiliation confidence="0.993767">Universitat Polit`ecnica de Catalunya Xerox Research Centre Europe</affiliation>
<email confidence="0.9901285">primadhanty@cs.upc.eduxavier.carreras@xrce.xerox.comariadna.quattoni@xrce.xerox.com</email>
<abstract confidence="0.996709368421053">Entity classification, like many other important problems in NLP, involves learning classifiers over sparse highdimensional feature spaces that result from the conjunction of elementary features of the entity mention and its context. In this paper we develop a low-rank regularization framework for training maxentropy models in such sparse conjunctive feature spaces. Our approach handles conjunctive feature spaces using matrices and induces an implicit low-dimensional representation via low-rank constraints. We show that when learning entity classifiers under minimal supervision, using a seed set, our approach is more effective in controlling model capacity than standard techniques for linear classifiers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled</title>
<date>2005</date>
<pages>6--1817</pages>
<contexts>
<context position="17436" citStr="Ando and Zhang (2005)" startWordPosition="2915" endWordPosition="2918">tity classifiers by bootstraping from an initial set of seeds, using a boosting version of co-training. Seed sets have also been exploited by graphical model approaches. Haghighi and Klein (2006) define a graphical model that is soft-constrained such that the prediction for an unlabeled example agrees with the labels of seeds that are distributionally similar. Li et al. (2010) present a Bayesian approach to expand an initial seed set, with the goal of creating a gazetteer. Another approach to entity recognition that, like in our case, learns projections of contextual features is the method by Ando and Zhang (2005). 129 Class Nb Mentions 10-30 Seed 10-30 40-120 640-1920 All 334 747 3,133 6,516 1,384 2,885 5,812 6,159 295 699 3,435 5,271 611 1326 3,085 3,205 5,326 11,595 31,071 36,673 PER clinton, dole, arafat, yeltsin, wasim akram, lebed, dutroux, waqar younis, mushtaq ahmed, croft LOC u.s., england, germany, britain, australia, france, spain, pakistan, italy, china ORG reuters, u.n., oakland, puk, osce, cincinnati, eu, nato, ajax, honda MISC russian, german, british, french, dutch, english, israeli, european, iraqi, australian O year, percent, thursday, government, police, results, tuesday, soccer, pre</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. J. Mach. Learn. Res., 6:1817–1853, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT’ 98,</booktitle>
<pages>92--100</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="16665" citStr="Blum and Mitchell, 1998" startWordPosition="2786" endWordPosition="2789">ns of entities and feature descriptions. The same idea was applied to relation extraction (Riedel et al., 2013), using a matrix of entity pairs times descriptions that corresponds to a matricization of an entity-entity-description tensor. Very recently Singh et al. (2015) explored alternative ways of applying low-rank constraints to tensor-based relation extraction. Another aspect of this paper is training entity classification models using minimal supervision, which has been addressed by multiple works in the literature. A classical successful approach for this problem is to use co-training (Blum and Mitchell, 1998): learn two classifiers that use different views of the data by using each other’s predictions. In the same line, Collins and Singer (1999) trained entity classifiers by bootstraping from an initial set of seeds, using a boosting version of co-training. Seed sets have also been exploited by graphical model approaches. Haghighi and Klein (2006) define a graphical model that is soft-constrained such that the prediction for an unlabeled example agrees with the labels of seeds that are distributionally similar. Li et al. (2010) present a Bayesian approach to expand an initial seed set, with the go</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT’ 98, pages 92–100, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="21239" citStr="Brown et al., 1992" startWordPosition="3518" endWordPosition="3521">ar in the training data or not, and where F1 values at 90% levels are obtained (e.g. (Ratinov and Roth, 2009)). This suggests that part of the success of state-of-the-art models is in storing known entities together with their type (in the form of gazetteers or directly in lexicalized parameters of the model). 5.2 Setting We use the CoNLL-2003 English data, which is annotated with four types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC). In addition, the data is tagged with partsof-speech (PoS), and we compute word clusters running the Brown clustering algorithm (Brown et al., 1992) on the words in the training set. We consider annotated entity phrases as candidate entities, and all single nouns that are not part of an entity as candidate non-entities (O). Both candidate entities and non-entities will be referred to as candidates in the remaining of this section. We lowercase all candidates and remove the am130 Features Window Bag-of-words N-grams Lexical Cluster Lexical Cluster 1 13.63 14.59 13.63 14.59 Elementary features of left and right contexts 2 15.49 13.86 13.08 13.54 3 12.18 14.45 12.14 13.28 1 12.90 13.75 12.90 13.75 Only full conjunctions of left and right con</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="16804" citStr="Collins and Singer (1999)" startWordPosition="2811" endWordPosition="2814">pairs times descriptions that corresponds to a matricization of an entity-entity-description tensor. Very recently Singh et al. (2015) explored alternative ways of applying low-rank constraints to tensor-based relation extraction. Another aspect of this paper is training entity classification models using minimal supervision, which has been addressed by multiple works in the literature. A classical successful approach for this problem is to use co-training (Blum and Mitchell, 1998): learn two classifiers that use different views of the data by using each other’s predictions. In the same line, Collins and Singer (1999) trained entity classifiers by bootstraping from an initial set of seeds, using a boosting version of co-training. Seed sets have also been exploited by graphical model approaches. Haghighi and Klein (2006) define a graphical model that is soft-constrained such that the prediction for an unlabeled example agrees with the labels of seeds that are distributionally similar. Li et al. (2010) present a Bayesian approach to expand an initial seed set, with the goal of creating a gazetteer. Another approach to entity recognition that, like in our case, learns projections of contextual features is the</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Yoram Singer</author>
</authors>
<title>Efficient online and batch learning using forward backward splitting.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>10</volume>
<pages>2934</pages>
<contexts>
<context position="14087" citStr="Duchi and Singer, 2009" startWordPosition="2384" endWordPosition="2387">. Thus, the nuclear norm is used as a regularizer. With this, we define our objective as follows: argmin L(W) + TR(W) , (5) W where L(W) is a convex loss function, R(W) is a regularizer, and T is a constant that trades off error and capacity. In experiments we will compare nuclear norm regularization with E1 and E2 regularizers. In all cases we use the negative log-likelihood as loss function, denoting the training data as D: L(W) = � −log Pr(y I (l, e, r); W) . ((l,e,r),y)ED (6) To solve the objective in Eq. (5) we use a simple optimization scheme known as forward-backward splitting (FOBOS) (Duchi and Singer, 2009). In a series of iterations, this algorithm performs a gradient update followed by a proximal projection of the parameters. Such projection depends on the regularizer used: for E1 it thresholds the parameters; for E2 it scales them; and for nuclearnorm regularization it thresholds the singular values. This means that, for nuclear norm regularization, each iteration requires to decompose W using SVD. See (Madhyastha et al., 2014) for details about this optimization for a related application. 4 Related Work The main aspect of our approach is the use of a spectral penalty (i.e., the rank) to cont</context>
</contexts>
<marker>Duchi, Singer, 2009</marker>
<rawString>John Duchi and Yoram Singer. 2009. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10:2899– 2934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06,</booktitle>
<pages>320--327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17010" citStr="Haghighi and Klein (2006)" startWordPosition="2844" endWordPosition="2847">sed relation extraction. Another aspect of this paper is training entity classification models using minimal supervision, which has been addressed by multiple works in the literature. A classical successful approach for this problem is to use co-training (Blum and Mitchell, 1998): learn two classifiers that use different views of the data by using each other’s predictions. In the same line, Collins and Singer (1999) trained entity classifiers by bootstraping from an initial set of seeds, using a boosting version of co-training. Seed sets have also been exploited by graphical model approaches. Haghighi and Klein (2006) define a graphical model that is soft-constrained such that the prediction for an unlabeled example agrees with the labels of seeds that are distributionally similar. Li et al. (2010) present a Bayesian approach to expand an initial seed set, with the goal of creating a gazetteer. Another approach to entity recognition that, like in our case, learns projections of contextual features is the method by Ando and Zhang (2005). 129 Class Nb Mentions 10-30 Seed 10-30 40-120 640-1920 All 334 747 3,133 6,516 1,384 2,885 5,812 6,159 295 699 3,435 5,271 611 1326 3,085 3,205 5,326 11,595 31,071 36,673 P</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06, pages 320–327, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yu Xin</author>
<author>Yuan Zhang</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Low-rank tensors for scoring dependency structures.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1381--1391</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="15248" citStr="Lei et al. (2014)" startWordPosition="2569" endWordPosition="2572">s the use of a spectral penalty (i.e., the rank) to control the capacity of multilinear functions parameterized by matrices or tensors. Quattoni et al. (2014) used nuclear-norm regularization to learn latentvariable max-margin sequence taggers. Madhyastha et al. (2014) defined bilexical distribu4Also known as the trace norm. tions parameterized by matrices which result lexical embeddings tailored for a particular linguistic relation. Like in our case, the low-dimensional latent projections in these papers are learned implicitly by imposing low-rank constraints on the predictions of the model. Lei et al. (2014) also use low-rank tensor learning in the context of dependency parsing, where like in our case dependencies are represented by conjunctive feature spaces. While the motivation is similar, their technical solution is different. We use the technique of matricization of a tensor combined with a nuclear-norm relaxation to obtain a convex learning procedure. In their case they explicitly look for a low-dimensional factorization of the tensor using a greedy alternating optimization. Also recently, Yao et al. (2013) have framed entity classification as a low-rank matrix completion problem. The idea </context>
</contexts>
<marker>Lei, Xin, Zhang, Barzilay, Jaakkola, 2014</marker>
<rawString>Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank tensors for scoring dependency structures. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1381–1391, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao-Li Li</author>
<author>Lei Zhang</author>
<author>Bing Liu</author>
<author>See-Kiong Ng</author>
</authors>
<title>Distributional similarity vs. pu learning for entity set expansion.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10,</booktitle>
<pages>359--364</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17194" citStr="Li et al. (2010)" startWordPosition="2874" endWordPosition="2877">al successful approach for this problem is to use co-training (Blum and Mitchell, 1998): learn two classifiers that use different views of the data by using each other’s predictions. In the same line, Collins and Singer (1999) trained entity classifiers by bootstraping from an initial set of seeds, using a boosting version of co-training. Seed sets have also been exploited by graphical model approaches. Haghighi and Klein (2006) define a graphical model that is soft-constrained such that the prediction for an unlabeled example agrees with the labels of seeds that are distributionally similar. Li et al. (2010) present a Bayesian approach to expand an initial seed set, with the goal of creating a gazetteer. Another approach to entity recognition that, like in our case, learns projections of contextual features is the method by Ando and Zhang (2005). 129 Class Nb Mentions 10-30 Seed 10-30 40-120 640-1920 All 334 747 3,133 6,516 1,384 2,885 5,812 6,159 295 699 3,435 5,271 611 1326 3,085 3,205 5,326 11,595 31,071 36,673 PER clinton, dole, arafat, yeltsin, wasim akram, lebed, dutroux, waqar younis, mushtaq ahmed, croft LOC u.s., england, germany, britain, australia, france, spain, pakistan, italy, china</context>
</contexts>
<marker>Li, Zhang, Liu, Ng, 2010</marker>
<rawString>Xiao-Li Li, Lei Zhang, Bing Liu, and See-Kiong Ng. 2010. Distributional similarity vs. pu learning for entity set expansion. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 359–364, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pranava Swaroop Madhyastha</author>
<author>Xavier Carreras</author>
<author>Ariadna Quattoni</author>
</authors>
<title>Learning Task-specific Bilexical Embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>161--171</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="14519" citStr="Madhyastha et al., 2014" startWordPosition="2456" endWordPosition="2459">(W) = � −log Pr(y I (l, e, r); W) . ((l,e,r),y)ED (6) To solve the objective in Eq. (5) we use a simple optimization scheme known as forward-backward splitting (FOBOS) (Duchi and Singer, 2009). In a series of iterations, this algorithm performs a gradient update followed by a proximal projection of the parameters. Such projection depends on the regularizer used: for E1 it thresholds the parameters; for E2 it scales them; and for nuclearnorm regularization it thresholds the singular values. This means that, for nuclear norm regularization, each iteration requires to decompose W using SVD. See (Madhyastha et al., 2014) for details about this optimization for a related application. 4 Related Work The main aspect of our approach is the use of a spectral penalty (i.e., the rank) to control the capacity of multilinear functions parameterized by matrices or tensors. Quattoni et al. (2014) used nuclear-norm regularization to learn latentvariable max-margin sequence taggers. Madhyastha et al. (2014) defined bilexical distribu4Also known as the trace norm. tions parameterized by matrices which result lexical embeddings tailored for a particular linguistic relation. Like in our case, the low-dimensional latent proje</context>
</contexts>
<marker>Madhyastha, Carreras, Quattoni, 2014</marker>
<rawString>Pranava Swaroop Madhyastha, Xavier Carreras, and Ariadna Quattoni. 2014. Learning Task-specific Bilexical Embeddings. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 161– 171, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<contexts>
<context position="25053" citStr="McCallum, 2002" startWordPosition="4159" endWordPosition="4160">f the contexts. For example we can change the window size of the context sequence (i.e., for a window size of 1 we only use the last token before the mention and the first token after the mention). We can treat the left and right contexts independently of each other, we can treat them as a unique combination, or we can use both. We can also choose to use the word form of a token, its PoS tag, a word cluster, or a combination of these. Table 2 compares different context representations and their performance in classifying unseen candidates using maximum-entropy classifiers trained with Mallet (McCallum, 2002) with E2 regularization, using the 10-30 seed. We use the lexical representation (the word itself) and a word cluster representation of the context tokens and use a window size of one to three. We use two types of features: bag-of-words features (1- grams of tokens in the specified window) and ngram features (with n smaller or equal to the window size). The performance of using word clusters is comparable, and sometimes better, to using lexical representations. Moreover, using a longer window, in this case, does not necessarily result in better performance. 7 In the rest of the experiments 7Ou</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew K. McCallum. 2002. Mallet: A machine learning for language toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Neelakantan</author>
<author>Michael Collins</author>
</authors>
<title>Learning dictionaries for named entity recognition using minimal supervision.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>452--461</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="18686" citStr="Neelakantan and Collins (2014)" startWordPosition="3108" endWordPosition="3111">ay, people, minister, sunday, division, week, time, state, market, years, officials, group, company, saturday, match, at, world, home, august, standings Table 1: For each entity class, the seed of entities for the 10-30 set, together with the number of mentions in the training data that involve entities in the seed for various sizes of the seeds. They define a set of auxiliary tasks, which can be supervised using unlabeled data, and find a projection of the data that works well as input representation for the auxiliary tasks. This representation is then used for the target task. More recently Neelakantan and Collins (2014) presented another approach to gazetteer expansion using an initial seed. A novel aspect is the use of Canonical Correlation Analysis (CCA) to compute embeddings of entity contexts, that are used by the named entity classifier. Like in our case, their method learns a compressed representation of contexts that helps prediction. 5 Experiments In this section we evaluate our regularization framework for training models in highdimensional sparse conjunctive feature spaces. We run experiments on learning entity classifiers with minimal supervision. We focus on classification of unseen entities to h</context>
<context position="28386" citStr="Neelakantan and Collins, 2014" startWordPosition="4717" endWordPosition="4720">percase, or not • all-low=1, all-low=0: whether all letters of the candidate are lowercase letters, or not • all-cap1=1, all-cap1=0: whether all letters of the candidate are uppercase letters, or not • all-cap2=1, all-cap2=0: whether all letters of the candidate are uppercase letters and periods, or not • num-tokens=1, num-tokens=2, num-tok&gt;2: whether the candidate consists of one token, two or more • dummy: a tag that holds for any entity candidate, used to capture context features alone Table 3: The 12 entity tags used to represent entity candidates. The tags all-cap1 and all-cap2 are from (Neelakantan and Collins, 2014). 132 PER LOC ORG MISC AVG F1 PREC REC F1 PREC REC F1 PREC REC F1 PREC REC F1 10-30 `1 65.69 65.40 65.55 15.38 23.58 18.62 59.33 19.44 29.28 23.36 30.05 26.28 34.93 `1 65.54 64.80 65.17 15.12 23.17 18.30 60.82 18.50 28.37 23.30 30.52 26.42 34.56 NN 72.41 74.52 73.45 14.89 21.55 17.61 49.09 21.16 17.61 31.40 38.03 34.40 38.76 40-120 `1 72.16 44.07 54.72 13.38 40.24 20.08 48.89 31.19 38.09 22.03 35.68 27.24 35.03 `2 71.75 44.89 55.23 13.61 41.87 20.54 49.39 31.50 38.47 21.64 30.99 25.48 34.93 NN 75.16 61.33 67.54 13.08 20.73 16.04 49.03 35.74 41.34 29.97 47.42 36.73 40.41 640-1920 `1 79.52 62.27</context>
</contexts>
<marker>Neelakantan, Collins, 2014</marker>
<rawString>Arvind Neelakantan and Michael Collins. 2014. Learning dictionaries for named entity recognition using minimal supervision. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 452– 461, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadna Quattoni</author>
<author>Borja Balle</author>
<author>Xavier Carreras</author>
<author>Amir Globerson</author>
</authors>
<title>Spectral regularization for max-margin sequence tagging.</title>
<date>2014</date>
<booktitle>Proceedings of the 31st International Conference on Machine Learning (ICML14),</booktitle>
<pages>1710--1718</pages>
<editor>In Tony Jebara and Eric P. Xing, editors,</editor>
<contexts>
<context position="14789" citStr="Quattoni et al. (2014)" startWordPosition="2500" endWordPosition="2503">by a proximal projection of the parameters. Such projection depends on the regularizer used: for E1 it thresholds the parameters; for E2 it scales them; and for nuclearnorm regularization it thresholds the singular values. This means that, for nuclear norm regularization, each iteration requires to decompose W using SVD. See (Madhyastha et al., 2014) for details about this optimization for a related application. 4 Related Work The main aspect of our approach is the use of a spectral penalty (i.e., the rank) to control the capacity of multilinear functions parameterized by matrices or tensors. Quattoni et al. (2014) used nuclear-norm regularization to learn latentvariable max-margin sequence taggers. Madhyastha et al. (2014) defined bilexical distribu4Also known as the trace norm. tions parameterized by matrices which result lexical embeddings tailored for a particular linguistic relation. Like in our case, the low-dimensional latent projections in these papers are learned implicitly by imposing low-rank constraints on the predictions of the model. Lei et al. (2014) also use low-rank tensor learning in the context of dependency parsing, where like in our case dependencies are represented by conjunctive f</context>
</contexts>
<marker>Quattoni, Balle, Carreras, Globerson, 2014</marker>
<rawString>Ariadna Quattoni, Borja Balle, Xavier Carreras, and Amir Globerson. 2014. Spectral regularization for max-margin sequence tagging. In Tony Jebara and Eric P. Xing, editors, Proceedings of the 31st International Conference on Machine Learning (ICML14), pages 1710–1718. JMLR Workshop and Conference Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>147--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="20729" citStr="Ratinov and Roth, 2009" startWordPosition="3439" endWordPosition="3442"> class, which is part of the classification but is excluded from evaluation. Note that named entity classification for unseen entities is a challenging problem. Even in the standard fully-supervised scenario, when we measure the performance of state-of-the-art methods on unseen entities, the F1 values are in the range of 60%. This represents a significant drop with respect to the standard metrics for named entity recognition, which consider all entity mentions of the test set irrespective of whether they appear in the training data or not, and where F1 values at 90% levels are obtained (e.g. (Ratinov and Roth, 2009)). This suggests that part of the success of state-of-the-art models is in storing known entities together with their type (in the form of gazetteers or directly in lexicalized parameters of the model). 5.2 Setting We use the CoNLL-2003 English data, which is annotated with four types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC). In addition, the data is tagged with partsof-speech (PoS), and we compute word clusters running the Brown clustering algorithm (Brown et al., 1992) on the words in the training set. We consider annotated entity phrases as candidate entit</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009), pages 147–155, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
<author>Benjamin M Marlin</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>74--84</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="16152" citStr="Riedel et al., 2013" startWordPosition="2712" endWordPosition="2715">ed with a nuclear-norm relaxation to obtain a convex learning procedure. In their case they explicitly look for a low-dimensional factorization of the tensor using a greedy alternating optimization. Also recently, Yao et al. (2013) have framed entity classification as a low-rank matrix completion problem. The idea is based on the fact that if two entities (in rows) have similar descriptions (in columns) they should have similar classes. The low-rank structure of the matrix defines intrinsic representations of entities and feature descriptions. The same idea was applied to relation extraction (Riedel et al., 2013), using a matrix of entity pairs times descriptions that corresponds to a matricization of an entity-entity-description tensor. Very recently Singh et al. (2015) explored alternative ways of applying low-rank constraints to tensor-based relation extraction. Another aspect of this paper is training entity classification models using minimal supervision, which has been addressed by multiple works in the literature. A classical successful approach for this problem is to use co-training (Blum and Mitchell, 1998): learn two classifiers that use different views of the data by using each other’s pred</context>
</contexts>
<marker>Riedel, Yao, McCallum, Marlin, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 74–84, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Tim Rockt¨aschel</author>
<author>Sebastian Riedel</author>
</authors>
<title>Towards Combined Matrix and Tensor Factorization for Universal Schema Relation Extraction.</title>
<date>2015</date>
<booktitle>In NAACL Workshop on Vector Space Modeling for NLP (VSM).</booktitle>
<marker>Singh, Rockt¨aschel, Riedel, 2015</marker>
<rawString>Sameer Singh, Tim Rockt¨aschel, and Sebastian Riedel. 2015. Towards Combined Matrix and Tensor Factorization for Universal Schema Relation Extraction. In NAACL Workshop on Vector Space Modeling for NLP (VSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Srebro</author>
<author>Adi Shraibman</author>
</authors>
<title>Rank, tracenorm and max-norm.</title>
<date>2005</date>
<booktitle>In Learning Theory,</booktitle>
<pages>545--560</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="13165" citStr="Srebro and Shraibman, 2005" startWordPosition="2214" endWordPosition="2217">ntrinsic dimensions. In our case we choose the left context axes as the row dimension, and the rest of axes as the column dimension.3 In this section, we will 3In preliminary experiments we tried variations, such as having right prefixes in the columns, and left prefixes, entity tags and classes in the rows. We only observer minor, nonsignificant variations in the results. 128 denote as W the matricized version of the parameters θ of our models. The second observation is that minimizing the rank of a matrix is a non-convex problem. We make use of a convex relaxation based on the nuclear norm (Srebro and Shraibman, 2005). The nuclear norm4 of a matrix W, denoted IIWII*, is the sum of its singular values: IIWII* = EZ EZ,Z where W = UEVT is the singular value decomposition of W. This norm has been used in several applications in machine learning as a convex surrogate for imposing low rank, e.g. (Srebro et al., 2004). Thus, the nuclear norm is used as a regularizer. With this, we define our objective as follows: argmin L(W) + TR(W) , (5) W where L(W) is a convex loss function, R(W) is a regularizer, and T is a constant that trades off error and capacity. In experiments we will compare nuclear norm regularization</context>
</contexts>
<marker>Srebro, Shraibman, 2005</marker>
<rawString>Nathan Srebro and Adi Shraibman. 2005. Rank, tracenorm and max-norm. In Learning Theory, pages 545–560. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Srebro</author>
<author>Jason Rennie</author>
<author>Tommi S Jaakkola</author>
</authors>
<title>Maximum-margin matrix factorization.</title>
<date>2004</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>1329--1336</pages>
<contexts>
<context position="13464" citStr="Srebro et al., 2004" startWordPosition="2271" endWordPosition="2274">ws. We only observer minor, nonsignificant variations in the results. 128 denote as W the matricized version of the parameters θ of our models. The second observation is that minimizing the rank of a matrix is a non-convex problem. We make use of a convex relaxation based on the nuclear norm (Srebro and Shraibman, 2005). The nuclear norm4 of a matrix W, denoted IIWII*, is the sum of its singular values: IIWII* = EZ EZ,Z where W = UEVT is the singular value decomposition of W. This norm has been used in several applications in machine learning as a convex surrogate for imposing low rank, e.g. (Srebro et al., 2004). Thus, the nuclear norm is used as a regularizer. With this, we define our objective as follows: argmin L(W) + TR(W) , (5) W where L(W) is a convex loss function, R(W) is a regularizer, and T is a constant that trades off error and capacity. In experiments we will compare nuclear norm regularization with E1 and E2 regularizers. In all cases we use the negative log-likelihood as loss function, denoting the training data as D: L(W) = � −log Pr(y I (l, e, r); W) . ((l,e,r),y)ED (6) To solve the objective in Eq. (5) we use a simple optimization scheme known as forward-backward splitting (FOBOS) (</context>
</contexts>
<marker>Srebro, Rennie, Jaakkola, 2004</marker>
<rawString>Nathan Srebro, Jason Rennie, and Tommi S Jaakkola. 2004. Maximum-margin matrix factorization. In Advances in neural information processing systems, pages 1329–1336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL</booktitle>
<pages>142--147</pages>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Universal schema for entity type prediction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction, AKBC ’13,</booktitle>
<pages>79--84</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="15763" citStr="Yao et al. (2013)" startWordPosition="2649" endWordPosition="2652">learned implicitly by imposing low-rank constraints on the predictions of the model. Lei et al. (2014) also use low-rank tensor learning in the context of dependency parsing, where like in our case dependencies are represented by conjunctive feature spaces. While the motivation is similar, their technical solution is different. We use the technique of matricization of a tensor combined with a nuclear-norm relaxation to obtain a convex learning procedure. In their case they explicitly look for a low-dimensional factorization of the tensor using a greedy alternating optimization. Also recently, Yao et al. (2013) have framed entity classification as a low-rank matrix completion problem. The idea is based on the fact that if two entities (in rows) have similar descriptions (in columns) they should have similar classes. The low-rank structure of the matrix defines intrinsic representations of entities and feature descriptions. The same idea was applied to relation extraction (Riedel et al., 2013), using a matrix of entity pairs times descriptions that corresponds to a matricization of an entity-entity-description tensor. Very recently Singh et al. (2015) explored alternative ways of applying low-rank co</context>
</contexts>
<marker>Yao, Riedel, McCallum, 2013</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2013. Universal schema for entity type prediction. In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction, AKBC ’13, pages 79–84, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>