<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000610">
<title confidence="0.942227">
Correcting Misuse of Verb Forms
</title>
<author confidence="0.865955">
John Lee and Stephanie Seneff
</author>
<affiliation confidence="0.6144935">
Spoken Language Systems
MIT Computer Science and Artificial Intelligence Laboratory
</affiliation>
<address confidence="0.860648">
Cambridge, MA 02139, USA
</address>
<email confidence="0.999179">
{jsylee,seneff}@csail.mit.edu
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999722916666667">
This paper proposes a method to correct En-
glish verb form errors made by non-native
speakers. A basic approach is template match-
ing on parse trees. The proposed method im-
proves on this approach in two ways. To
improve recall, irregularities in parse trees
caused by verb form errors are taken into ac-
count; to improve precision, n-gram counts
are utilized to filter proposed corrections.
Evaluation on non-native corpora, represent-
ing two genres and mother tongues, shows
promising results.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97821148">
In order to describe the nuances of an action, a verb
may be associated with various concepts such as
tense, aspect, voice, mood, person and number. In
some languages, such as Chinese, the verb itself is
not inflected, and these concepts are expressed via
other words in the sentence. In highly inflected lan-
guages, such as Turkish, many of these concepts are
encoded in the inflection of the verb. In between
these extremes, English uses a combination of in-
flections (see Table 1) and “helping words”, or aux-
iliaries, to form complex verb phrases.
It should come as no surprise, then, that the mis-
use of verb forms is a common error category for
some non-native speakers of English. For example,
in the Japanese Learners of English corpus (Izumi et
al., 2003), errors related to verbs are among the most
frequent categories. Table 2 shows some sentences
with these errors.
Form Example
base (bare) speak
base (infinitive) to speak
third person singular speaks
past spoke
-ing participle speaking
-ed participle spoken
</bodyText>
<tableCaption confidence="0.661788">
Table 1: Five forms of inflections of English verbs (Quirk
</tableCaption>
<bodyText confidence="0.989790916666667">
et al., 1985), illustrated with the verb “speak”. The base
form is also used to construct the infinitive with “to”. An
exception is the verb “to be”, which has more forms.
A system that automatically detects and corrects
misused verb forms would be both an educational
and practical tool for students of English. It may
also potentially improve the performance of ma-
chine translation and natural language generation
systems, especially when the source and target lan-
guages employ very different verb systems.
Research on automatic grammar correction has
been conducted on a number of different parts-of-
speech, such as articles (Knight and Chander, 1994)
and prepositions (Chodorow et al., 2007). Errors in
verb forms have been covered as part of larger sys-
tems such as (Heidorn, 2000), but we believe that
their specific research challenges warrant more de-
tailed examination.
We build on the basic approach of template-
matching on parse trees in two ways. To improve re-
call, irregularities in parse trees caused by verb form
errors are considered; to improve precision, n-gram
counts are utilized to filter proposed corrections.
We start with a discussion on the scope of our
</bodyText>
<page confidence="0.979791">
174
</page>
<note confidence="0.714531">
Proceedings of ACL-08: HLT, pages 174–182,
</note>
<page confidence="0.538021">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.99935425">
task in the next section. We then analyze the spe-
cific research issues in §3 and survey previous work
in §4. A description of our data follows. Finally, we
present experimental results and conclude.
</bodyText>
<sectionHeader confidence="0.957328" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.997879">
An English verb can be inflected in five forms (see
Table 1). Our goal is to correct confusions among
these five forms, as well as the infinitive. These
confusions can be viewed as symptoms of one of
two main underlying categories of errors; roughly
speaking, one category is semantic in nature, and the
other, syntactic.
</bodyText>
<subsectionHeader confidence="0.990813">
2.1 Semantic Errors
</subsectionHeader>
<bodyText confidence="0.987418259259259">
The first type of error is concerned with inappropri-
ate choices of tense, aspect, voice, or mood. These
may be considered errors in semantics. In the sen-
tence below, the verb “live” is expressed in the sim-
ple present tense, rather than the perfect progressive:
He *lives there since June. (1)
Either “has been living” or “had been living” may
be the valid correction, depending on the context. If
there is no temporal expression, correction of tense
and aspect would be even more challenging.
Similarly, correcting voice and mood often re-
quires real-world knowledge. Suppose one wants
to say “I am prepared for the exam”, but writes “I
am preparing for the exam”. Semantic analysis of
the context would be required to correct this kind of
error, which will not be tackled in this paper1.
1If the input is “I am *prepare for the exam”, however, we
will attempt to choose between the two possibilities.
Example Usage
I take a bath and *reading books. FINITE
I can’t *skiing well, but ... BASEmd
Why did this *happened? BASEdo
But I haven’t *decide where to go. EDperf
I don’t want *have a baby. INFverb
I have to save my money for *ski. INGprep
My son was very *satisfy with ... EDpass
I am always *talk to my father. INGprog
</bodyText>
<tableCaption confidence="0.989972">
Table 2: Sentences with verb form errors. The intended
usages, shown on the right column, are defined in Table 3.
</tableCaption>
<subsectionHeader confidence="0.999001">
2.2 Syntactic Errors
</subsectionHeader>
<bodyText confidence="0.993411690476191">
The second type of error is the misuse of verb forms.
Even if the intended tense, aspect, voice and mood
are correct, the verb phrase may still be constructed
erroneously. This type of error may be further sub-
divided as follows:
Subject-Verb Agreement The verb is not correctly
inflected in number and person with respect to
the subject. A common error is the confusion
between the base form and the third person sin-
gular form, e.g.,
He *have been living there since June. (2)
Auxiliary Agreement In addition to the modal aux-
iliaries, other auxiliaries must be used when
specifying the perfective or progressive aspect,
or the passive voice. Their use results in a com-
plex verb phrase, i.e., one that consists of two
or more verb constituents. Mistakes arise when
the main verb does not “agree” with the aux-
iliary. In the sentence below, the present per-
fect progressive tense (“has been living”) is in-
tended, but the main verb “live” is mistakenly
left in the base form:
He has been *live there since June. (3)
In general, the auxiliaries can serve as a hint to
the intended verb form, even as the auxiliaries
“has been” in the above case suggest that the
progressive aspect was intended.
Complementation A nonfinite clause can serve as
complementation to a verb or to a preposition.
In the former case, the verb form in the clause
is typically an infinitive or an -ing participle; in
the latter, it is usually an -ing participle. Here
is an example of a wrong choice of verb form
in complementation to a verb:
He wants *live there. (4)
In this sentence, “live”, in its base form, should
be modified to its infinitive form as a comple-
mentation to the verb “wants”.
This paper focuses on correcting the above three
error types: subject-verb agreement, auxiliary agree-
ment, and complementation. Table 3 gives a com-
plete list of verb form usages which will be covered.
</bodyText>
<page confidence="0.997637">
175
</page>
<table confidence="0.999369666666667">
Form Usage Description Example
Base Form as BASEmd After modals He may call. May he call?
Bare Infinitive BASEdo “Do”-support/-periphrasis; He did not call. Did he call?
emphatic positive I did call.
Base or 3rd person FINITE Simple present or past tense He calls.
Base Form as INFverb Verb complementation He wants her to call.
to-Infinitive
-ing INGproy Progressive aspect He was calling. Was he calling?
participle INGverb Verb complementation He hated calling.
INGprep Prepositional complementation The device is designed for calling
-ed EDperf Perfect aspect He has called. Has he called?
participle EDpass Passive voice He was called. Was he called?
</table>
<tableCaption confidence="0.9772355">
Table 3: Usage of various verb forms. In the examples, the italized verbs are the “targets” for correction. In comple-
mentations, the main verbs or prepositions are bolded; in all other cases, the auxiliaries are bolded.
</tableCaption>
<sectionHeader confidence="0.994354" genericHeader="method">
3 Research Issues
</sectionHeader>
<bodyText confidence="0.999988933333333">
One strategy for correcting verb form errors is to
identify the intended syntactic relationships between
the verb in question and its neighbors. For subject-
verb agreement, the subject of the verb is obviously
crucial (e.g., “he” in (2)); the auxiliary is relevant
for resolving auxiliary agreement (e.g., “has been”
in (3)); determining the verb that receives the com-
plementation is necessary for detecting any comple-
mentation errors (e.g., “wants” in (4)). Once these
items are identified, most verb form errors may be
corrected in a rather straightforward manner.
The success of this strategy, then, hinges on accu-
rate identification of these items, for example, from
parse trees. Ambiguities will need to be resolved,
leading to two research issues (§3.2 and §3.3).
</bodyText>
<subsectionHeader confidence="0.996122">
3.1 Ambiguities
</subsectionHeader>
<bodyText confidence="0.983014666666667">
The three so-called primary verbs, “have”, “do” and
“be”, can serve as either main or auxiliary verbs.
The verb “be” can be utilized as a main verb, but also
as an auxiliary in the progressive aspect (INGproy in
Table 3) or the passive voice (EDpass). The three ex-
amples below illustrate these possibilities:
This is work not play. (main verb)
My father is working in the lab. (INGproy)
A solution is worked out. (EDpass)
These different roles clearly affect the forms re-
quired for the verbs (if any) that follow. Dis-
ambiguation among these roles is usually straight-
forward because of the different verb forms (e.g.,
“working” vs. “worked”). If the verb forms are in-
correct, disambiguation is made more difficult:
This is work not play.
My father is *work in the lab.
A solution is *work out.
Similar ambiguities are introduced by the other pri-
mary verbs2. The verb “have” can function as an
auxiliary in the perfect aspect (EDperf) as well as
a main verb. The versatile “do” can serve as “do”-
support or add emphasis (BASEdo), or simply act as
a main verb.
</bodyText>
<subsectionHeader confidence="0.999696">
3.2 Automatic Parsing
</subsectionHeader>
<bodyText confidence="0.999908142857143">
The ambiguities discussed above may be expected
to cause degradation in automatic parsing perfor-
mance. In other words, sentences containing verb
form errors are more likely to yield an “incorrect”
parse tree, sometimes with significant differences.
For example, the sentence “My father is *work in
the laboratory” is parsed (Collins, 1997) as:
</bodyText>
<equation confidence="0.636159">
(S (NP My father)
(VP is (NP work))
</equation>
<bodyText confidence="0.405206">
(PP in the laboratory))
</bodyText>
<footnote confidence="0.993679">
2The abbreviations ’s (is or has) and ’d (would or had) com-
pound the ambiguities.
</footnote>
<page confidence="0.997763">
176
</page>
<bodyText confidence="0.999035538461538">
The progressive form “working” is substituted with
its bare form, which happens to be also a noun.
The parser, not unreasonably, identifies “work” as
a noun. Correcting the verb form error in this sen-
tence, then, necessitates considering the noun that is
apparently a copular complementation.
Anecdotal observations like this suggest that one
cannot use parser output naively3. We will show that
some of the irregularities caused by verb form errors
are consistent and can be taken into account.
One goal of this paper is to recognize irregular-
ities in parse trees caused by verb form errors, in
order to increase recall.
</bodyText>
<subsectionHeader confidence="0.993687">
3.3 Overgeneralization
</subsectionHeader>
<bodyText confidence="0.999453363636364">
One potential consequence of allowing for irregu-
larities in parse tree patterns is overgeneralization.
For example, to allow for the “parse error” in §3.2
and to retrieve the word “work”, every determiner-
less noun would potentially be turned into an -ing
participle. This would clearly result in many invalid
corrections. We propose using n-gram counts as a
filter to counter this kind of overgeneralization.
A second goal is to show that n-gram counts can
effectively serve as a filter, in order to increase pre-
cision.
</bodyText>
<sectionHeader confidence="0.988174" genericHeader="method">
4 Previous Research
</sectionHeader>
<bodyText confidence="0.999954666666667">
This section discusses previous research on process-
ing verb form errors, and contrasts verb form errors
with those of the other parts-of-speech.
</bodyText>
<subsectionHeader confidence="0.990658">
4.1 Verb Forms
</subsectionHeader>
<bodyText confidence="0.9998649">
Detection and correction of grammatical errors, in-
cluding verb forms, have been explored in various
applications. Hand-crafted error production rules
(or “mal-rules”), augmenting a context-free gram-
mar, are designed for a writing tutor aimed at deaf
students (Michaud et al., 2000). Similar strategies
with parse trees are pursued in (Bender et al., 2004),
and error templates are utilized in (Heidorn, 2000)
for a word processor. Carefully hand-crafted rules,
when used alone, tend to yield high precision; they
</bodyText>
<footnote confidence="0.7798112">
3According to a study on parsing ungrammatical sen-
tences (Foster, 2007), subject-verb and determiner-noun agree-
ment errors can lower the F-score of a state-of-the-art prob-
abilistic parser by 1.4%, and context-sensitive spelling errors
(not verbs specifically), by 6%.
</footnote>
<bodyText confidence="0.999916357142857">
may, however, be less equipped to detect verb form
errors within a perfectly grammatical sentence, such
as the example given in §3.2.
An approach combining a hand-crafted context-
free grammar and stochastic probabilities is pursued
in (Lee and Seneff, 2006), but it is designed for a
restricted domain only. A maximum entropy model,
using lexical and POS features, is trained in (Izumi
et al., 2003) to recognize a variety of errors. It
achieves 55% precision and 23% recall overall, on
evaluation data that partially overlap with those of
the present paper. Unfortunately, results on verb
form errors are not reported separately, and compar-
ison with our approach is therefore impossible.
</bodyText>
<subsectionHeader confidence="0.805881">
4.2 Other Parts-of-speech
</subsectionHeader>
<bodyText confidence="0.99996475">
Automatic error detection has been performed on
other parts-of-speech, e.g., articles (Knight and
Chander, 1994) and prepositions (Chodorow et al.,
2007). The research issues with these parts-of-
speech, however, are quite distinct. Relative to verb
forms, errors in these categories do not “disturb” the
parse tree as much. The process of feature extraction
is thus relatively simple.
</bodyText>
<sectionHeader confidence="0.99924" genericHeader="method">
5 Data
</sectionHeader>
<subsectionHeader confidence="0.998393">
5.1 Development Data
</subsectionHeader>
<bodyText confidence="0.999975">
To investigate irregularities in parse tree patterns
(see §3.2), we utilized the AQUAINT Corpus of En-
glish News Text. After parsing the corpus (Collins,
1997), we artificially introduced verb form errors
into these sentences, and observed the resulting “dis-
turbances” to the parse trees.
For disambiguation with n-grams (see §3.3), we
made use of the WEB 1T 5-GRAM corpus. Prepared
by Google Inc., it contains English n-grams, up to
5-grams, with their observed frequency counts from
a large number of web pages.
</bodyText>
<subsectionHeader confidence="0.996489">
5.2 Evaluation Data
</subsectionHeader>
<bodyText confidence="0.99508">
Two corpora were used for evaluation. They were
selected to represent two different genres, and two
different mother tongues.
JLE (Japanese Learners of English corpus) This
corpus is based on interviews for the Stan-
dard Speaking Test, an English-language pro-
ficiency test conducted in Japan (Izumi et al.,
</bodyText>
<page confidence="0.975561">
177
</page>
<table confidence="0.99954525">
Input Hypothesized Correction
None Valid Invalid
w/ errors false neg true pos inv pos
w/o errors true neg false pos
</table>
<tableCaption confidence="0.999669">
Table 4: Possible outcomes of a hypothesized correction.
</tableCaption>
<bodyText confidence="0.99990335">
2003). For 167 of the transcribed interviews,
totalling 15,637 sentences4, grammatical errors
were annotated and their corrections provided.
By retaining the verb form errors5, but correct-
ing all other error types, we generated a test set
in which 477 sentences (3.1%) contain subject-
verb agreement errors, and 238 (1.5%) contain
auxiliary agreement and complementation er-
rors.
HKUST This corpus6 of short essays was col-
lected from students, all native Chinese speak-
ers, at the Hong Kong University of Science
and Technology. It contains a total of 2556 sen-
tences. They tend to be longer and have more
complex structures than their counterparts in
the JLE. Corrections are not provided; how-
ever, part-of-speech tags are given for the orig-
inal words, and for the intended (but unwrit-
ten) corrections. Implications on our evaluation
procedure are discussed in §5.4.
</bodyText>
<subsectionHeader confidence="0.989247">
5.3 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.9995842">
For each verb in the input sentence, a change in verb
form may be hypothesized. There are five possible
outcomes for this hypothesis, as enumerated in Ta-
ble 4. To penalize “false alarms”, a strict definition
is used for false positives — even when the hypoth-
esized correction yields a good sentence, it is still
considered a false positive so long as the original
sentence is acceptable.
It can sometimes be difficult to determine which
words should be considered verbs, as they are not
</bodyText>
<footnote confidence="0.775142625">
4Obtained by segmenting (Reynar and Ratnaparkhi, 1997)
the interviewee turns, and discarding sentences with only one
word. The HKUST corpus was processed likewise.
5Specifically, those tagged with the “v fml”, “v fin” (cov-
ering auxiliary agreement and complementation) and “v agr”
(subject-verb agreement) types; those with semantic errors (see
§2.1), i.e. “v tns” (tense), are excluded.
6Provided by Prof. John Milton, personal communication.
</footnote>
<bodyText confidence="0.999618689655172">
clearly demarcated in our evaluation corpora. We
will thus apply the outcomes in Table 4 at the sen-
tence level; that is, the output sentence is considered
a true positive only if the original sentence contains
errors, and only if valid corrections are offered for
all errors.
The following statistics are computed:
Accuracy The proportion of sentences which, after
being treated by the system, have correct verb
forms. That is, (true neg + true pos) divided
by the total number of sentences.
Recall Out of all sentences with verb form errors,
the percentage whose errors have been success-
fully corrected by the system. That is, true pos
divided by (true pos + false neg + inv pos).
Detection Precision This is the first of two types
of precision to be reported, and is defined as
follows: Out of all sentences for which the
system has hypothesized corrections, the per-
centage that actually contain errors, without re-
gard to the validity of the corrections. That is,
(true pos + inv pos) divided by (true pos +
inv pos + false pos).
Correction Precision This is the more stringent
type of precision. In addition to successfully
determining that a correction is needed, the sys-
tem must offer a valid correction. Formally, it is
true pos divided by (true pos + false pos +
inv pos).
</bodyText>
<subsectionHeader confidence="0.975302">
5.4 Evaluation Procedure
</subsectionHeader>
<bodyText confidence="0.999913166666667">
For the JLE corpus, all figures above will be re-
ported. The HKUST corpus, however, will not be
evaluated on subject-verb agreement, since a sizable
number of these errors are induced by other changes
in the sentence7.
Furthermore, the HKUST corpus will require
manual evaluation, since the corrections are not an-
notated. Two native speakers of English were given
the edited sentences, as well as the original input.
For each pair, they were asked to select one of four
statements: one of the two is better, or both are
equally correct, or both are equally incorrect. The
</bodyText>
<footnote confidence="0.6916955">
7e.g., the subject of the verb needs to be changed from sin-
gular to plural.
</footnote>
<page confidence="0.99023">
178
</page>
<table confidence="0.9994054375">
Expected Tree {(usage),...} Tree disturbed by substitution [(crr) → (err)]
{INGprog,EDpass} A dog is [sleeping→sleep]. I’m [living→live] in XXX city.
VP VP VP
be VP be NP be ADJP
crr/{VBG,VBN} err/NN err/JJ
{INGverb,INFverb} I like [skiing→ski] very much; She likes to [go→going] around
VP VP VP
*/V SG */V NP */V PP
VP err/NN to/TO SG
crr/{VBG,TO} ... VP
err/VBG
INGprep I lived in France for [studying→study] French language.
PP PP
*/IN SG */IN NP
VP err/NN
crr/VBG ...
</table>
<tableCaption confidence="0.996633">
Table 5: Effects of incorrect verb forms on parse trees. The left column shows trees normally expected for the indicated
usages (see Table 3). The right column shows the resulting trees when the correct verb form (crr) is replaced by (err).
Detailed comments are provided in §6.1.
</tableCaption>
<bodyText confidence="0.9998118">
correction precision is thus the proportion of pairs
where the edited sentence is deemed better. Accu-
racy and recall cannot be computed, since it was im-
possible to distinguish syntactic errors from seman-
tic ones (see §2).
</bodyText>
<subsectionHeader confidence="0.993564">
5.5 Baselines
</subsectionHeader>
<bodyText confidence="0.999942307692308">
Since the vast majority of verbs are in their cor-
rect forms, the majority baseline is to propose no
correction. Although trivial, it is a surprisingly
strong baseline, achieving more than 98% for aux-
iliary agreement and complementation in JLE, and
just shy of 97% for subject-verb agreement.
For auxiliary agreement and complementation,
the verb-only baseline is also reported. It attempts
corrections only when the word in question is actu-
ally tagged as a verb. That is, it ignores the spurious
noun- and adjectival phrases in the parse tree dis-
cussed in §3.2, and relies only on the output of the
part-of-speech tagger.
</bodyText>
<sectionHeader confidence="0.998806" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999418">
Corresponding to the issues discussed in §3.2 and
§3.3, our experiment consists of two main steps.
</bodyText>
<subsectionHeader confidence="0.999817">
6.1 Derivation of Tree Patterns
</subsectionHeader>
<bodyText confidence="0.9999535">
Based on (Quirk et al., 1985), we observed tree pat-
terns for a set of verb form usages, as summarized
in Table 3. Using these patterns, we introduced verb
form errors into AQUAINT, then re-parsed the cor-
pus (Collins, 1997), and compiled the changes in the
“disturbed” trees into a catalog.
</bodyText>
<page confidence="0.995234">
179
</page>
<table confidence="0.982542333333333">
N-gram Example
be {INGprog, The dog is sleeping.
EDpass} * The door is open.
verb {INGverb, I need to do this.
INFverb} * I need beef for the curry.
verba *ing enjoy reading and
and {INGverb, going to pachinko
INFverb} go shopping and have dinner
prep for studying French language
{INGprep} * a class for sign language
have I have rented a video
{EDperf} * I have lunch in Ginza
</table>
<tableCaption confidence="0.771923">
Table 6: The n-grams used for filtering, with examples
of sentences which they are intended to differentiate. The
hypothesized usages (shown in the curly brackets) as well
</tableCaption>
<bodyText confidence="0.998526875">
as the original verb form, are considered. For example,
the first sentence is originally “The dog is *sleep.” The
three trigrams “is sleeping .”, “is slept .” and “is sleep .”
are compared; the first trigram has the highest count, and
the correction “sleeping” is therefore applied.
A portion of this catalog8 is shown in Table 5.
Comments on {INGprog,EDpass} can be found in
§3.2. Two cases are shown for {INGverb,INFverb}.
In the first case, an -ing participle in verb comple-
mentation is reduced to its base form, resulting in
a noun phrase. In the second, an infinitive is con-
structed with the -ing participle rather than the base
form, causing “to” to be misconstrued as a preposi-
tion. Finally, in INGprep, an -ing participle in prepo-
sition complementation is reduced to its base form,
and is subsumed in a noun phrase.
</bodyText>
<subsectionHeader confidence="0.99998">
6.2 Disambiguation with N-grams
</subsectionHeader>
<bodyText confidence="0.999868181818182">
The tree patterns derived from the previous step
may be considered as the “necessary” conditions for
proposing a change in verb forms. They are not “suf-
ficient”, however, since they tend to be overly gen-
eral. Indiscriminate application of these patterns on
AQUAINT would result in false positives for 46.4%
of the sentences.
For those categories with a high rate of false posi-
tives (all except BASEmd, BASEdo and FINITE), we
utilized n-grams as filters, allowing a correction
only when its n-gram count in the WEB 1T 5-GRAM
</bodyText>
<footnote confidence="0.9752775">
8Due to space constraints, only those trees with significant
changes above the leaf level are shown.
</footnote>
<table confidence="0.9990595">
Hyp. False Hypothesized False
Usage Pos. Usage Pos.
BASEmd 16.2% {INGverb,INFverb} 33.9%
BASEdo 0.9% {INGprog,EDpass} 21.0%
FINITE 12.8% INGprep 13.7%
EDperf 1.4%
</table>
<tableCaption confidence="0.971199333333333">
Table 7: The distribution of false positives in AQUAINT.
The total number of false positives is 994, represents less
than 1% of the 100,000 sentences drawn from the corpus.
</tableCaption>
<bodyText confidence="0.9921062">
corpus is greater than that of the original. The filter-
ing step reduced false positives from 46.4% to less
than 1%. Table 6 shows the n-grams, and Table 7
provides a breakdown of false positives in AQUAINT
after n-gram filtering.
</bodyText>
<sectionHeader confidence="0.867318" genericHeader="method">
6.3 Results for Subject-Verb Agreement
</sectionHeader>
<bodyText confidence="0.999971285714286">
In JLE, the accuracy of subject-verb agreement er-
ror correction is 98.93%. Compared to the majority
baseline of 96.95%, the improvement is statistically
significant9. Recall is 80.92%; detection precision is
83.93%, and correction precision is 81.61%.
Most mistakes are caused by misidentified sub-
jects. Some wh-questions prove to be especially dif-
ficult, perhaps due to their relative infrequency in
newswire texts, on which the parser is trained. One
example is the question “How much extra time does
the local train *takes?”. The word “does” is not
recognized as a “do”-support, and so the verb “take”
was mistakenly turned into a third person form to
agree with “train”.
</bodyText>
<sectionHeader confidence="0.9963055" genericHeader="method">
6.4 Results for Auxiliary Agreement &amp;
Complementation
</sectionHeader>
<bodyText confidence="0.999897555555555">
Table 8 summarizes the results for auxiliary agree-
ment and complementation, and Table 2 shows some
examples of real sentences corrected by the system.
Our proposed method yields 98.94% accuracy. It
is a statistically significant improvement over the
majority baseline (98.47%), although not significant
over the verb-only baseline10 (98.85%), perhaps a
reflection of the small number of test sentences with
verb form errors. The Kappa statistic for the man-
</bodyText>
<footnote confidence="0.945893">
9p &lt; 0.005 according to McNemar’s test.
10Withp = 1∗10−10 and p = 0.038, respectively, according
to McNemar’s test
</footnote>
<page confidence="0.968686">
180
</page>
<table confidence="0.9984294">
Corpus Method Accuracy Precision Precision Recall
(correction) (detection)
JLE verb-only 98.85% 71.43% 84.75% 31.51%
all 98.94% 68.00% 80.67% 42.86%
HKUST all not available 71.71% not available
</table>
<tableCaption confidence="0.803744333333333">
Table 8: Results on the JLE and HKUST corpora for auxiliary agreement and complementation. The majority baseline
accuracy is 98.47% for JLE. The verb-only baseline accuracy is 98.85%, as indicated on the second row. “All” denotes
the complete proposed method. See §6.4 for detailed comments.
</tableCaption>
<table confidence="0.999036333333333">
Usage JLE HKUST
Count (Prec.) Count (Prec.)
BASEmd 13 (92.3%) 25 (80.0%)
BASEdo 5 (100%) 0
FINITE 9 (55.6%) 0
EDperf 11 (90.9%) 3 (66.7%)
{INGprog,EDpass} 54 (58.6%) 30 (70.0%)
{INGverb,INFverb} 45 (60.0%) 16 (59.4%)
INGprep 10 (60.0%) 2 (100%)
</table>
<tableCaption confidence="0.9936225">
Table 9: Correction precision of individual correction
patterns (see Table 5) on the JLE and HKUST corpus.
</tableCaption>
<bodyText confidence="0.999921804347826">
ual evaluation of HKUST is 0.76, corresponding
to “substantial agreement” between the two evalu-
ators (Landis and Koch, 1977). The correction pre-
cisions for the JLE and HKUST corpora are compa-
rable.
Our analysis will focus on {INGprog,EDpass} and
{INGverb,INFverb}, two categories with relatively
numerous correction attempts and low precisions,
as shown in Table 9. For {INGprog,EDpass}, many
invalid corrections are due to wrong predictions of
voice, which involve semantic choices (see §2.1).
For example, the sentence “... the main duty is study
well” is edited to “... the main duty is studied well”,
a grammatical sentence but semantically unlikely.
For {INGverb,INFverb}, a substantial portion of the
false positives are valid, but unnecessary, correc-
tions. For example, there is no need to turn “I like
cooking” into “I like to cook”, as the original is per-
fectly acceptable. Some kind of confidence measure
on the n-gram counts might be appropriate for re-
ducing such false alarms.
Characteristics of speech transcripts pose some
further problems. First, colloquial expressions, such
as the word “like”, can be tricky to process. In the
question “Can you like give me the money back”,
“like” is misconstrued to be the main verb, and
“give” is turned into an infinitive, resulting in “Can
you like *to give me the money back”. Second, there
are quite a few incomplete sentences that lack sub-
jects for the verbs. No correction is attempted on
them.
Also left uncorrected are misused forms in non-
finite clauses that describe a noun. These are typ-
ically base forms that should be replaced with -ing
participles, as in “The girl *wear a purple skiwear
is a student of this ski school”. Efforts to detect this
kind of error had resulted in a large number of false
alarms.
Recall is further affected by cases where a verb is
separated from its auxiliary or main verb by many
words, often with conjunctions and other verbs in
between. One example is the sentence “I used to
climb up the orange trees and *catching insects”.
The word “catching” should be an infinitive comple-
menting “used”, but is placed within a noun phrase
together with “trees” and “insects”.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999995714285714">
We have presented a method for correcting verb
form errors. We investigated the ways in which verb
form errors affect parse trees. When allowed for,
these unusual tree patterns can expand correction
coverage, but also tend to result in overgeneration
of hypothesized corrections. N-grams have been
shown to be an effective filter for this problem.
</bodyText>
<sectionHeader confidence="0.9992" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99670375">
We thank Prof. John Milton for the HKUST cor-
pus, Tom Lee and Ken Schutte for their assistance
with the evaluation, and the anonymous reviewers
for their helpful feedback.
</bodyText>
<page confidence="0.998038">
181
</page>
<sectionHeader confidence="0.995889" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999874357142857">
E. Bender, D. Flickinger, S. Oepen, A. Walsh, and T.
Baldwin. 2004. Arboretum: Using a Precision Gram-
mar for Grammar Checking in CALL. Proc. In-
STIL/ICALL Symposium on Computer Assisted Learn-
ing.
M. Chodorow, J. R. Tetreault, and N.-R. Han. 2007.
Detection of Grammatical Errors Involving Preposi-
tions. In Proc. ACL-SIGSEM Workshop on Preposi-
tions. Prague, Czech Republic.
M. Collins. 1997. Three Generative, Lexicalised Models
for Statistical Parsing. Proc. ACL.
J. Foster. 2007. Treebanks Gone Bad: Generating a Tree-
bank of Ungrammatical English. In Proc. IJCAI Work-
shop on Analytics for Noisy Unstructured Data. Hy-
derabad, India.
G. Heidorn. 2000. Intelligent Writing Assistance.
Handbook of Natural Language Processing. Robert
Dale, Hermann Moisi and Harold Somers (ed.). Mar-
cel Dekker, Inc.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H.
Isahara. 2003. Automatic Error Detection in the
Japanese Learner’s English Spoken Data. In Compan-
ion Volume to Proc. ACL. Sapporo, Japan.
K. Knight and I. Chander. 1994. Automated Postediting
of Documents. In Proc. AAAI. Seattle, WA.
J. R. Landis and G. G. Koch. 1977. The Measurement of
Observer Agreement for Categorical Data. Biometrics
33(1):159–174.
L. Michaud, K. McCoy and C. Pennington. 2000. An In-
telligent Tutoring System for Deaf Learners of Written
English. Proc. 4th International ACM Conference on
Assistive Technologies.
J. Lee and S. Seneff. 2006. Automatic Grammar Cor-
rection for Second-Language Learners. In Proc. Inter-
speech. Pittsburgh, PA.
J. C. Reynar and A. Ratnaparkhi. 1997. A Maximum En-
tropy Approach to Identifying Sentence Boundaries.
In Proc. 5th Conference on Applied Natural Language
Processing. Washington, D.C.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman, New York.
</reference>
<page confidence="0.997995">
182
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.711589">
<title confidence="0.99985">Correcting Misuse of Verb Forms</title>
<author confidence="0.917398">John Lee</author>
<author confidence="0.917398">Stephanie Seneff</author>
<affiliation confidence="0.8587755">Spoken Language Systems MIT Computer Science and Artificial Intelligence Laboratory</affiliation>
<address confidence="0.999958">Cambridge, MA 02139, USA</address>
<abstract confidence="0.999468307692308">This paper proposes a method to correct English verb form errors made by non-native speakers. A basic approach is template matching on parse trees. The proposed method improves on this approach in two ways. To improve recall, irregularities in parse trees caused by verb form errors are taken into acto improve precision, counts are utilized to filter proposed corrections. Evaluation on non-native corpora, representing two genres and mother tongues, shows promising results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Bender</author>
<author>D Flickinger</author>
<author>S Oepen</author>
<author>A Walsh</author>
<author>T Baldwin</author>
</authors>
<title>Arboretum: Using a Precision Grammar for Grammar Checking in CALL.</title>
<date>2004</date>
<booktitle>Proc. InSTIL/ICALL Symposium on Computer Assisted Learning.</booktitle>
<contexts>
<context position="11830" citStr="Bender et al., 2004" startWordPosition="1956" endWordPosition="1959">hat n-gram counts can effectively serve as a filter, in order to increase precision. 4 Previous Research This section discusses previous research on processing verb form errors, and contrasts verb form errors with those of the other parts-of-speech. 4.1 Verb Forms Detection and correction of grammatical errors, including verb forms, have been explored in various applications. Hand-crafted error production rules (or “mal-rules”), augmenting a context-free grammar, are designed for a writing tutor aimed at deaf students (Michaud et al., 2000). Similar strategies with parse trees are pursued in (Bender et al., 2004), and error templates are utilized in (Heidorn, 2000) for a word processor. Carefully hand-crafted rules, when used alone, tend to yield high precision; they 3According to a study on parsing ungrammatical sentences (Foster, 2007), subject-verb and determiner-noun agreement errors can lower the F-score of a state-of-the-art probabilistic parser by 1.4%, and context-sensitive spelling errors (not verbs specifically), by 6%. may, however, be less equipped to detect verb form errors within a perfectly grammatical sentence, such as the example given in §3.2. An approach combining a hand-crafted con</context>
</contexts>
<marker>Bender, Flickinger, Oepen, Walsh, Baldwin, 2004</marker>
<rawString>E. Bender, D. Flickinger, S. Oepen, A. Walsh, and T. Baldwin. 2004. Arboretum: Using a Precision Grammar for Grammar Checking in CALL. Proc. InSTIL/ICALL Symposium on Computer Assisted Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chodorow</author>
<author>J R Tetreault</author>
<author>N-R Han</author>
</authors>
<title>Detection of Grammatical Errors Involving Prepositions.</title>
<date>2007</date>
<booktitle>In Proc. ACL-SIGSEM Workshop on Prepositions.</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2488" citStr="Chodorow et al., 2007" startWordPosition="394" endWordPosition="397">used to construct the infinitive with “to”. An exception is the verb “to be”, which has more forms. A system that automatically detects and corrects misused verb forms would be both an educational and practical tool for students of English. It may also potentially improve the performance of machine translation and natural language generation systems, especially when the source and target languages employ very different verb systems. Research on automatic grammar correction has been conducted on a number of different parts-ofspeech, such as articles (Knight and Chander, 1994) and prepositions (Chodorow et al., 2007). Errors in verb forms have been covered as part of larger systems such as (Heidorn, 2000), but we believe that their specific research challenges warrant more detailed examination. We build on the basic approach of templatematching on parse trees in two ways. To improve recall, irregularities in parse trees caused by verb form errors are considered; to improve precision, n-gram counts are utilized to filter proposed corrections. We start with a discussion on the scope of our 174 Proceedings of ACL-08: HLT, pages 174–182, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Lin</context>
<context position="13123" citStr="Chodorow et al., 2007" startWordPosition="2152" endWordPosition="2155">neff, 2006), but it is designed for a restricted domain only. A maximum entropy model, using lexical and POS features, is trained in (Izumi et al., 2003) to recognize a variety of errors. It achieves 55% precision and 23% recall overall, on evaluation data that partially overlap with those of the present paper. Unfortunately, results on verb form errors are not reported separately, and comparison with our approach is therefore impossible. 4.2 Other Parts-of-speech Automatic error detection has been performed on other parts-of-speech, e.g., articles (Knight and Chander, 1994) and prepositions (Chodorow et al., 2007). The research issues with these parts-ofspeech, however, are quite distinct. Relative to verb forms, errors in these categories do not “disturb” the parse tree as much. The process of feature extraction is thus relatively simple. 5 Data 5.1 Development Data To investigate irregularities in parse tree patterns (see §3.2), we utilized the AQUAINT Corpus of English News Text. After parsing the corpus (Collins, 1997), we artificially introduced verb form errors into these sentences, and observed the resulting “disturbances” to the parse trees. For disambiguation with n-grams (see §3.3), we made u</context>
</contexts>
<marker>Chodorow, Tetreault, Han, 2007</marker>
<rawString>M. Chodorow, J. R. Tetreault, and N.-R. Han. 2007. Detection of Grammatical Errors Involving Prepositions. In Proc. ACL-SIGSEM Workshop on Prepositions. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three Generative, Lexicalised Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="9979" citStr="Collins, 1997" startWordPosition="1659" endWordPosition="1660">uities are introduced by the other primary verbs2. The verb “have” can function as an auxiliary in the perfect aspect (EDperf) as well as a main verb. The versatile “do” can serve as “do”- support or add emphasis (BASEdo), or simply act as a main verb. 3.2 Automatic Parsing The ambiguities discussed above may be expected to cause degradation in automatic parsing performance. In other words, sentences containing verb form errors are more likely to yield an “incorrect” parse tree, sometimes with significant differences. For example, the sentence “My father is *work in the laboratory” is parsed (Collins, 1997) as: (S (NP My father) (VP is (NP work)) (PP in the laboratory)) 2The abbreviations ’s (is or has) and ’d (would or had) compound the ambiguities. 176 The progressive form “working” is substituted with its bare form, which happens to be also a noun. The parser, not unreasonably, identifies “work” as a noun. Correcting the verb form error in this sentence, then, necessitates considering the noun that is apparently a copular complementation. Anecdotal observations like this suggest that one cannot use parser output naively3. We will show that some of the irregularities caused by verb form errors</context>
<context position="13540" citStr="Collins, 1997" startWordPosition="2220" endWordPosition="2221">efore impossible. 4.2 Other Parts-of-speech Automatic error detection has been performed on other parts-of-speech, e.g., articles (Knight and Chander, 1994) and prepositions (Chodorow et al., 2007). The research issues with these parts-ofspeech, however, are quite distinct. Relative to verb forms, errors in these categories do not “disturb” the parse tree as much. The process of feature extraction is thus relatively simple. 5 Data 5.1 Development Data To investigate irregularities in parse tree patterns (see §3.2), we utilized the AQUAINT Corpus of English News Text. After parsing the corpus (Collins, 1997), we artificially introduced verb form errors into these sentences, and observed the resulting “disturbances” to the parse trees. For disambiguation with n-grams (see §3.3), we made use of the WEB 1T 5-GRAM corpus. Prepared by Google Inc., it contains English n-grams, up to 5-grams, with their observed frequency counts from a large number of web pages. 5.2 Evaluation Data Two corpora were used for evaluation. They were selected to represent two different genres, and two different mother tongues. JLE (Japanese Learners of English corpus) This corpus is based on interviews for the Standard Speak</context>
<context position="20149" citStr="Collins, 1997" startWordPosition="3314" endWordPosition="3315">d. It attempts corrections only when the word in question is actually tagged as a verb. That is, it ignores the spurious noun- and adjectival phrases in the parse tree discussed in §3.2, and relies only on the output of the part-of-speech tagger. 6 Experiments Corresponding to the issues discussed in §3.2 and §3.3, our experiment consists of two main steps. 6.1 Derivation of Tree Patterns Based on (Quirk et al., 1985), we observed tree patterns for a set of verb form usages, as summarized in Table 3. Using these patterns, we introduced verb form errors into AQUAINT, then re-parsed the corpus (Collins, 1997), and compiled the changes in the “disturbed” trees into a catalog. 179 N-gram Example be {INGprog, The dog is sleeping. EDpass} * The door is open. verb {INGverb, I need to do this. INFverb} * I need beef for the curry. verba *ing enjoy reading and and {INGverb, going to pachinko INFverb} go shopping and have dinner prep for studying French language {INGprep} * a class for sign language have I have rented a video {EDperf} * I have lunch in Ginza Table 6: The n-grams used for filtering, with examples of sentences which they are intended to differentiate. The hypothesized usages (shown in the c</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three Generative, Lexicalised Models for Statistical Parsing. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Foster</author>
</authors>
<title>Treebanks Gone Bad: Generating a Treebank of Ungrammatical English.</title>
<date>2007</date>
<booktitle>In Proc. IJCAI Workshop on Analytics for Noisy Unstructured Data.</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="12059" citStr="Foster, 2007" startWordPosition="1993" endWordPosition="1994">ts-of-speech. 4.1 Verb Forms Detection and correction of grammatical errors, including verb forms, have been explored in various applications. Hand-crafted error production rules (or “mal-rules”), augmenting a context-free grammar, are designed for a writing tutor aimed at deaf students (Michaud et al., 2000). Similar strategies with parse trees are pursued in (Bender et al., 2004), and error templates are utilized in (Heidorn, 2000) for a word processor. Carefully hand-crafted rules, when used alone, tend to yield high precision; they 3According to a study on parsing ungrammatical sentences (Foster, 2007), subject-verb and determiner-noun agreement errors can lower the F-score of a state-of-the-art probabilistic parser by 1.4%, and context-sensitive spelling errors (not verbs specifically), by 6%. may, however, be less equipped to detect verb form errors within a perfectly grammatical sentence, such as the example given in §3.2. An approach combining a hand-crafted contextfree grammar and stochastic probabilities is pursued in (Lee and Seneff, 2006), but it is designed for a restricted domain only. A maximum entropy model, using lexical and POS features, is trained in (Izumi et al., 2003) to r</context>
</contexts>
<marker>Foster, 2007</marker>
<rawString>J. Foster. 2007. Treebanks Gone Bad: Generating a Treebank of Ungrammatical English. In Proc. IJCAI Workshop on Analytics for Noisy Unstructured Data. Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Heidorn</author>
</authors>
<title>Intelligent Writing Assistance. Handbook of Natural Language Processing.</title>
<date>2000</date>
<editor>Robert Dale, Hermann Moisi and Harold Somers (ed.).</editor>
<publisher>Marcel Dekker, Inc.</publisher>
<contexts>
<context position="2578" citStr="Heidorn, 2000" startWordPosition="413" endWordPosition="414">. A system that automatically detects and corrects misused verb forms would be both an educational and practical tool for students of English. It may also potentially improve the performance of machine translation and natural language generation systems, especially when the source and target languages employ very different verb systems. Research on automatic grammar correction has been conducted on a number of different parts-ofspeech, such as articles (Knight and Chander, 1994) and prepositions (Chodorow et al., 2007). Errors in verb forms have been covered as part of larger systems such as (Heidorn, 2000), but we believe that their specific research challenges warrant more detailed examination. We build on the basic approach of templatematching on parse trees in two ways. To improve recall, irregularities in parse trees caused by verb form errors are considered; to improve precision, n-gram counts are utilized to filter proposed corrections. We start with a discussion on the scope of our 174 Proceedings of ACL-08: HLT, pages 174–182, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics task in the next section. We then analyze the specific research issues in §3 and </context>
<context position="11883" citStr="Heidorn, 2000" startWordPosition="1966" endWordPosition="1967">er to increase precision. 4 Previous Research This section discusses previous research on processing verb form errors, and contrasts verb form errors with those of the other parts-of-speech. 4.1 Verb Forms Detection and correction of grammatical errors, including verb forms, have been explored in various applications. Hand-crafted error production rules (or “mal-rules”), augmenting a context-free grammar, are designed for a writing tutor aimed at deaf students (Michaud et al., 2000). Similar strategies with parse trees are pursued in (Bender et al., 2004), and error templates are utilized in (Heidorn, 2000) for a word processor. Carefully hand-crafted rules, when used alone, tend to yield high precision; they 3According to a study on parsing ungrammatical sentences (Foster, 2007), subject-verb and determiner-noun agreement errors can lower the F-score of a state-of-the-art probabilistic parser by 1.4%, and context-sensitive spelling errors (not verbs specifically), by 6%. may, however, be less equipped to detect verb form errors within a perfectly grammatical sentence, such as the example given in §3.2. An approach combining a hand-crafted contextfree grammar and stochastic probabilities is purs</context>
</contexts>
<marker>Heidorn, 2000</marker>
<rawString>G. Heidorn. 2000. Intelligent Writing Assistance. Handbook of Natural Language Processing. Robert Dale, Hermann Moisi and Harold Somers (ed.). Marcel Dekker, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>T Saiga</author>
<author>T Supnithi</author>
<author>H Isahara</author>
</authors>
<date>2003</date>
<booktitle>Automatic Error Detection in the Japanese Learner’s English Spoken Data. In Companion Volume to Proc. ACL.</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1476" citStr="Izumi et al., 2003" startWordPosition="235" endWordPosition="238">number. In some languages, such as Chinese, the verb itself is not inflected, and these concepts are expressed via other words in the sentence. In highly inflected languages, such as Turkish, many of these concepts are encoded in the inflection of the verb. In between these extremes, English uses a combination of inflections (see Table 1) and “helping words”, or auxiliaries, to form complex verb phrases. It should come as no surprise, then, that the misuse of verb forms is a common error category for some non-native speakers of English. For example, in the Japanese Learners of English corpus (Izumi et al., 2003), errors related to verbs are among the most frequent categories. Table 2 shows some sentences with these errors. Form Example base (bare) speak base (infinitive) to speak third person singular speaks past spoke -ing participle speaking -ed participle spoken Table 1: Five forms of inflections of English verbs (Quirk et al., 1985), illustrated with the verb “speak”. The base form is also used to construct the infinitive with “to”. An exception is the verb “to be”, which has more forms. A system that automatically detects and corrects misused verb forms would be both an educational and practical</context>
<context position="12654" citStr="Izumi et al., 2003" startWordPosition="2083" endWordPosition="2086">sentences (Foster, 2007), subject-verb and determiner-noun agreement errors can lower the F-score of a state-of-the-art probabilistic parser by 1.4%, and context-sensitive spelling errors (not verbs specifically), by 6%. may, however, be less equipped to detect verb form errors within a perfectly grammatical sentence, such as the example given in §3.2. An approach combining a hand-crafted contextfree grammar and stochastic probabilities is pursued in (Lee and Seneff, 2006), but it is designed for a restricted domain only. A maximum entropy model, using lexical and POS features, is trained in (Izumi et al., 2003) to recognize a variety of errors. It achieves 55% precision and 23% recall overall, on evaluation data that partially overlap with those of the present paper. Unfortunately, results on verb form errors are not reported separately, and comparison with our approach is therefore impossible. 4.2 Other Parts-of-speech Automatic error detection has been performed on other parts-of-speech, e.g., articles (Knight and Chander, 1994) and prepositions (Chodorow et al., 2007). The research issues with these parts-ofspeech, however, are quite distinct. Relative to verb forms, errors in these categories do</context>
</contexts>
<marker>Izumi, Uchimoto, Saiga, Supnithi, Isahara, 2003</marker>
<rawString>E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isahara. 2003. Automatic Error Detection in the Japanese Learner’s English Spoken Data. In Companion Volume to Proc. ACL. Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Chander</author>
</authors>
<title>Automated Postediting of Documents.</title>
<date>1994</date>
<booktitle>In Proc. AAAI.</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="2447" citStr="Knight and Chander, 1994" startWordPosition="388" endWordPosition="391">ith the verb “speak”. The base form is also used to construct the infinitive with “to”. An exception is the verb “to be”, which has more forms. A system that automatically detects and corrects misused verb forms would be both an educational and practical tool for students of English. It may also potentially improve the performance of machine translation and natural language generation systems, especially when the source and target languages employ very different verb systems. Research on automatic grammar correction has been conducted on a number of different parts-ofspeech, such as articles (Knight and Chander, 1994) and prepositions (Chodorow et al., 2007). Errors in verb forms have been covered as part of larger systems such as (Heidorn, 2000), but we believe that their specific research challenges warrant more detailed examination. We build on the basic approach of templatematching on parse trees in two ways. To improve recall, irregularities in parse trees caused by verb form errors are considered; to improve precision, n-gram counts are utilized to filter proposed corrections. We start with a discussion on the scope of our 174 Proceedings of ACL-08: HLT, pages 174–182, Columbus, Ohio, USA, June 2008.</context>
<context position="13082" citStr="Knight and Chander, 1994" startWordPosition="2146" endWordPosition="2149">stic probabilities is pursued in (Lee and Seneff, 2006), but it is designed for a restricted domain only. A maximum entropy model, using lexical and POS features, is trained in (Izumi et al., 2003) to recognize a variety of errors. It achieves 55% precision and 23% recall overall, on evaluation data that partially overlap with those of the present paper. Unfortunately, results on verb form errors are not reported separately, and comparison with our approach is therefore impossible. 4.2 Other Parts-of-speech Automatic error detection has been performed on other parts-of-speech, e.g., articles (Knight and Chander, 1994) and prepositions (Chodorow et al., 2007). The research issues with these parts-ofspeech, however, are quite distinct. Relative to verb forms, errors in these categories do not “disturb” the parse tree as much. The process of feature extraction is thus relatively simple. 5 Data 5.1 Development Data To investigate irregularities in parse tree patterns (see §3.2), we utilized the AQUAINT Corpus of English News Text. After parsing the corpus (Collins, 1997), we artificially introduced verb form errors into these sentences, and observed the resulting “disturbances” to the parse trees. For disambig</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>K. Knight and I. Chander. 1994. Automated Postediting of Documents. In Proc. AAAI. Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data.</title>
<date>1977</date>
<journal>Biometrics</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="25124" citStr="Landis and Koch, 1977" startWordPosition="4117" endWordPosition="4120">erb-only baseline accuracy is 98.85%, as indicated on the second row. “All” denotes the complete proposed method. See §6.4 for detailed comments. Usage JLE HKUST Count (Prec.) Count (Prec.) BASEmd 13 (92.3%) 25 (80.0%) BASEdo 5 (100%) 0 FINITE 9 (55.6%) 0 EDperf 11 (90.9%) 3 (66.7%) {INGprog,EDpass} 54 (58.6%) 30 (70.0%) {INGverb,INFverb} 45 (60.0%) 16 (59.4%) INGprep 10 (60.0%) 2 (100%) Table 9: Correction precision of individual correction patterns (see Table 5) on the JLE and HKUST corpus. ual evaluation of HKUST is 0.76, corresponding to “substantial agreement” between the two evaluators (Landis and Koch, 1977). The correction precisions for the JLE and HKUST corpora are comparable. Our analysis will focus on {INGprog,EDpass} and {INGverb,INFverb}, two categories with relatively numerous correction attempts and low precisions, as shown in Table 9. For {INGprog,EDpass}, many invalid corrections are due to wrong predictions of voice, which involve semantic choices (see §2.1). For example, the sentence “... the main duty is study well” is edited to “... the main duty is studied well”, a grammatical sentence but semantically unlikely. For {INGverb,INFverb}, a substantial portion of the false positives a</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. R. Landis and G. G. Koch. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics 33(1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Michaud</author>
<author>K McCoy</author>
<author>C Pennington</author>
</authors>
<title>An Intelligent Tutoring System for Deaf Learners of Written English.</title>
<date>2000</date>
<booktitle>Proc. 4th International ACM Conference on Assistive Technologies.</booktitle>
<contexts>
<context position="11756" citStr="Michaud et al., 2000" startWordPosition="1944" endWordPosition="1947">lter to counter this kind of overgeneralization. A second goal is to show that n-gram counts can effectively serve as a filter, in order to increase precision. 4 Previous Research This section discusses previous research on processing verb form errors, and contrasts verb form errors with those of the other parts-of-speech. 4.1 Verb Forms Detection and correction of grammatical errors, including verb forms, have been explored in various applications. Hand-crafted error production rules (or “mal-rules”), augmenting a context-free grammar, are designed for a writing tutor aimed at deaf students (Michaud et al., 2000). Similar strategies with parse trees are pursued in (Bender et al., 2004), and error templates are utilized in (Heidorn, 2000) for a word processor. Carefully hand-crafted rules, when used alone, tend to yield high precision; they 3According to a study on parsing ungrammatical sentences (Foster, 2007), subject-verb and determiner-noun agreement errors can lower the F-score of a state-of-the-art probabilistic parser by 1.4%, and context-sensitive spelling errors (not verbs specifically), by 6%. may, however, be less equipped to detect verb form errors within a perfectly grammatical sentence, s</context>
</contexts>
<marker>Michaud, McCoy, Pennington, 2000</marker>
<rawString>L. Michaud, K. McCoy and C. Pennington. 2000. An Intelligent Tutoring System for Deaf Learners of Written English. Proc. 4th International ACM Conference on Assistive Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>S Seneff</author>
</authors>
<title>Automatic Grammar Correction for Second-Language Learners.</title>
<date>2006</date>
<booktitle>In Proc. Interspeech.</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="12512" citStr="Lee and Seneff, 2006" startWordPosition="2058" endWordPosition="2061">ord processor. Carefully hand-crafted rules, when used alone, tend to yield high precision; they 3According to a study on parsing ungrammatical sentences (Foster, 2007), subject-verb and determiner-noun agreement errors can lower the F-score of a state-of-the-art probabilistic parser by 1.4%, and context-sensitive spelling errors (not verbs specifically), by 6%. may, however, be less equipped to detect verb form errors within a perfectly grammatical sentence, such as the example given in §3.2. An approach combining a hand-crafted contextfree grammar and stochastic probabilities is pursued in (Lee and Seneff, 2006), but it is designed for a restricted domain only. A maximum entropy model, using lexical and POS features, is trained in (Izumi et al., 2003) to recognize a variety of errors. It achieves 55% precision and 23% recall overall, on evaluation data that partially overlap with those of the present paper. Unfortunately, results on verb form errors are not reported separately, and comparison with our approach is therefore impossible. 4.2 Other Parts-of-speech Automatic error detection has been performed on other parts-of-speech, e.g., articles (Knight and Chander, 1994) and prepositions (Chodorow et</context>
</contexts>
<marker>Lee, Seneff, 2006</marker>
<rawString>J. Lee and S. Seneff. 2006. Automatic Grammar Correction for Second-Language Learners. In Proc. Interspeech. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Approach to Identifying Sentence Boundaries.</title>
<date>1997</date>
<booktitle>In Proc. 5th Conference on Applied Natural Language Processing.</booktitle>
<location>Washington, D.C.</location>
<contexts>
<context position="15825" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="2586" endWordPosition="2589">. Implications on our evaluation procedure are discussed in §5.4. 5.3 Evaluation Metric For each verb in the input sentence, a change in verb form may be hypothesized. There are five possible outcomes for this hypothesis, as enumerated in Table 4. To penalize “false alarms”, a strict definition is used for false positives — even when the hypothesized correction yields a good sentence, it is still considered a false positive so long as the original sentence is acceptable. It can sometimes be difficult to determine which words should be considered verbs, as they are not 4Obtained by segmenting (Reynar and Ratnaparkhi, 1997) the interviewee turns, and discarding sentences with only one word. The HKUST corpus was processed likewise. 5Specifically, those tagged with the “v fml”, “v fin” (covering auxiliary agreement and complementation) and “v agr” (subject-verb agreement) types; those with semantic errors (see §2.1), i.e. “v tns” (tense), are excluded. 6Provided by Prof. John Milton, personal communication. clearly demarcated in our evaluation corpora. We will thus apply the outcomes in Table 4 at the sentence level; that is, the output sentence is considered a true positive only if the original sentence contains </context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>J. C. Reynar and A. Ratnaparkhi. 1997. A Maximum Entropy Approach to Identifying Sentence Boundaries. In Proc. 5th Conference on Applied Natural Language Processing. Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quirk</author>
<author>S Greenbaum</author>
<author>G Leech</author>
<author>J Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<publisher>Longman,</publisher>
<location>New York.</location>
<contexts>
<context position="1807" citStr="Quirk et al., 1985" startWordPosition="287" endWordPosition="290"> Table 1) and “helping words”, or auxiliaries, to form complex verb phrases. It should come as no surprise, then, that the misuse of verb forms is a common error category for some non-native speakers of English. For example, in the Japanese Learners of English corpus (Izumi et al., 2003), errors related to verbs are among the most frequent categories. Table 2 shows some sentences with these errors. Form Example base (bare) speak base (infinitive) to speak third person singular speaks past spoke -ing participle speaking -ed participle spoken Table 1: Five forms of inflections of English verbs (Quirk et al., 1985), illustrated with the verb “speak”. The base form is also used to construct the infinitive with “to”. An exception is the verb “to be”, which has more forms. A system that automatically detects and corrects misused verb forms would be both an educational and practical tool for students of English. It may also potentially improve the performance of machine translation and natural language generation systems, especially when the source and target languages employ very different verb systems. Research on automatic grammar correction has been conducted on a number of different parts-ofspeech, suc</context>
<context position="19956" citStr="Quirk et al., 1985" startWordPosition="3278" endWordPosition="3281"> more than 98% for auxiliary agreement and complementation in JLE, and just shy of 97% for subject-verb agreement. For auxiliary agreement and complementation, the verb-only baseline is also reported. It attempts corrections only when the word in question is actually tagged as a verb. That is, it ignores the spurious noun- and adjectival phrases in the parse tree discussed in §3.2, and relies only on the output of the part-of-speech tagger. 6 Experiments Corresponding to the issues discussed in §3.2 and §3.3, our experiment consists of two main steps. 6.1 Derivation of Tree Patterns Based on (Quirk et al., 1985), we observed tree patterns for a set of verb form usages, as summarized in Table 3. Using these patterns, we introduced verb form errors into AQUAINT, then re-parsed the corpus (Collins, 1997), and compiled the changes in the “disturbed” trees into a catalog. 179 N-gram Example be {INGprog, The dog is sleeping. EDpass} * The door is open. verb {INGverb, I need to do this. INFverb} * I need beef for the curry. verba *ing enjoy reading and and {INGverb, going to pachinko INFverb} go shopping and have dinner prep for studying French language {INGprep} * a class for sign language have I have rent</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>