<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001146">
<title confidence="0.986949">
K-Best Suffix Arrays
</title>
<author confidence="0.98763">
Kenneth Church Bo Thiesson Robert Ragno
</author>
<affiliation confidence="0.947159">
Microsoft Microsoft Microsoft
</affiliation>
<address confidence="0.8806525">
One Microsoft Way One Microsoft Way One Microsoft Way
Redmond, WA 98052 Redmond, WA 98052 Redmond, WA 98052
</address>
<email confidence="0.963093">
church@microsoft.com thiesson@microsoft.com rragno@microsoft.com
</email>
<sectionHeader confidence="0.980408" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957764705882">
Suppose we have a large dictionary of
strings. Each entry starts with a figure of
merit (popularity). We wish to find the k-
best matches for a substring, s, in a dicti-
noary, dict. That is, grep s dict  |sort –n |
head –k, but we would like to do this in
sublinear time. Example applications: (1)
web queries with popularities, (2) prod-
ucts with prices and (3) ads with click
through rates. This paper proposes a
novel index, k-best suffix arrays, based on
ideas borrowed from suffix arrays and kd-
trees. A standard suffix array sorts the
suffixes by a single order (lexicographic)
whereas k-best suffix arrays are sorted by
two orders (lexicographic and popularity).
Lookup time is between log N and sqrt N.
</bodyText>
<sectionHeader confidence="0.555639" genericHeader="method">
1 Standard Suffix Arrays
</sectionHeader>
<bodyText confidence="0.999801736842105">
This paper will introduce k-best suffix arrays,
which are similar to standard suffix arrays (Manber
and Myers, 1990), an index that makes it conven-
ient to compute the frequency and location of a
substring, s, in a long sequence, corpus. A suffix
array, suf, is an array of all N suffixes, sorted al-
phabetically. A suffix, suf[i], also known as a
semi-infinite string, is a string that starts at position
j in the corpus and continues to the end of the cor-
pus. In practical implementations, a suffix is a 4-
byte integer, j. In this way, an int (constant space)
denotes a long string (N bytes).
The make_standard_suf program below creates
a standard suffix array. The program starts with a
corpus, a global variable containing a long string
of N characters. The program allocates the suffix
array suf and initializes it to a vector of N ints (suf-
fixes) ranging from 0 to N−1. The suffix array is
sorted by lexicographic order and returned.
</bodyText>
<equation confidence="0.99320925">
int* make_standard_suf () {
int N = strlen(corpus);
int* suf = (int*)malloc(N * sizeof(int));
for (int i=0; i&lt;N; i++) suf[i] = i;
qsort(suf, N, sizeof(int), lexcomp);
return suf;}
int lexcomp(int* a, int* b)
{ return strcmp(corpus + *a, corpus + *b);}
</equation>
<bodyText confidence="0.997154833333333">
This program is simple to describe (but inefficient,
at least in theory) because strcmp can take O(N)
time in the worst case (where the corpus contains
two copies of an arbitrarily long string). See
http://cm.bell-labs.com/cm/cs/who/doug/ssort.c for
an implementation of the O(N log N) Manber and
Myers algorithm. However, in practice, when the
corpus is a dictionary of relatively short entries
(such as web queries), the worst case is unlikely to
come up. In which case, the simple make_suf pro-
gram above is good enough, and maybe even better
than the O(N log N) solution.
</bodyText>
<subsectionHeader confidence="0.999531">
1.1 Standard Suffix Array Lookup
</subsectionHeader>
<bodyText confidence="0.999905111111111">
To compute the frequency and locations of a sub-
string s, use a pair of binary searches to find i and
j, the locations of the first and last suffix in the suf-
fix array that start with s. Each suffix between i
and j point to a location of s in the corpus. The
frequency is simply: j − i + 1.
Here is some simple code. We show how to
find the first suffix. The last suffix is left as an
exercise. As above, we ignore the unlikely worst
</bodyText>
<page confidence="0.688514">
17
</page>
<note confidence="0.819295">
Proceedings of NAACL HLT 2007, Companion Volume, pages 17–20,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.6632815">
case (two copies of a long string). See references
mentioned above for worst case solutions.
</bodyText>
<construct confidence="0.905920882352941">
void standard_lookup(char* s, int* suf, int N){
int* i = find_first_suf(s, suf, N);
int* j = find_last_suf(s, suf, N);
for (int* k=i; k&lt;=j; k++) output(*k);}
int* find_first_suf(char* s, int* suf, int N) {
int len = strlen(s);
int* high = suf + N;
while (suf + 2 &lt; high) {
int* mid = suf + (high−suf)/2;
int c = strncmp(s, corpus + *mid, len);
if (c == 0) high = mid+1;
else if (c &lt; 0) high = mid;
else suf = mid;}
for ( ; suf &lt; high; suf++)
if (strncmp(s, corpus + *suf, len) == 0)
return suf;
return NULL;} // not found
</construct>
<sectionHeader confidence="0.600972" genericHeader="method">
2 K-Best Suffix Arrays
</sectionHeader>
<bodyText confidence="0.999855857142857">
K-best suffix arrays are like standard suffix arrays,
except there are two orders instead of one. In addi-
tion to lexicographic order, we assume a figure of
merit, which we will refer to as popularity. For
example, the popularity of a string could be its fre-
quency in a search log. The code below assumes
that the corpus is a sequence of strings that comes
pre-sorted by popularity, and then the popularities
have been stripped off. These assumptions make
it very easy to compare two strings by popularity.
All popcomp has to do is to compare the two posi-
tions in the corpus.1
The make_kbest_suf program below is similar to
the make_standard_suf program above except we
now sort by the two orders at alternating depths in
the tree. First we sort lexicographically and then
we sort by popularity and so on, using a construc-
tion similar to KD-Trees (Bentley, 1975). The
code below is simple to describe (though there are
more efficient implementations that avoid unnec-
essary qsorts).
</bodyText>
<equation confidence="0.939105333333333">
int* make_kbest_suf () {
int N = strlen(corpus);
int* suf = (int*)malloc(N * sizeof(int));
</equation>
<bodyText confidence="0.96494475">
1 With a little extra book keeping, one can keep a table on the
side that makes it possible to map back and forth between
popularity rank and the actual popularity. This turns out to be
useful for some applications.
</bodyText>
<construct confidence="0.877957142857143">
for (int i=0; i&lt;N; i++) suf[i]=i;
process(suf, suf+N, 0);
return suf;}
void process(int* start, int* end, int depth) {
int* mid = start + (end − start)/2;
if (end &lt;= start+1) return;
qsort(start, end-start, sizeof(int),
(depth &amp; 1) ? popcomp : lexcomp);
process(start, mid, depth+1);
process(mid+1, end, depth+1);}
int popcomp(int* a, int* b) {
if (*a &gt; *b) return 1;
if (*a &lt; *b) return −1;
return 0;}
</construct>
<subsectionHeader confidence="0.975667">
2.1 K-Best Suffix Array Lookup
</subsectionHeader>
<bodyText confidence="0.99998175">
To find the k-best matches for a particular sub-
string s, we do what we would normally do for
standard suffix arrays on lexicographic splits.
However, on popularity splits, we search the more
popular half first and then we search the less popu-
lar half, if necessary.
An implementation of kbest-lookup is given be-
low. D denotes the depth of the search thus far.
Kbest-lookup is initially called with D of 0. Pro-
pose maintains a heap of the k-best matches found
thus far. Done returns true if its argument is less
popular than the kth best match found thus far.
</bodyText>
<construct confidence="0.920855523809524">
void kbest_lookup(char* s, int* suf, int N, int D){
int* mid = suf + N/2;
int len = strlen(s);
if (N==1 &amp;&amp; strncmp(s, corpus+*suf, len)==0)
propose(*suf);
if (N &lt;= 1) return;
if (D&amp;1) { // popularity split
kbest_lookup(s, suf, mid−suf, D+1);
if (done(*mid)) return;
if (strncmp(s, corpus + *mid, len) == 0)
propose(*mid);
kbest_lookup(s, mid+1, (suf+N)−mid−1,
D+1);}
else { // lexicographic split
int c = strncmp(s, corpus + *mid, len);
int n = (suf+N)−mid−1;
if (c &lt; 0) kbest_lookup(s, suf, mid-suf, D+1);
else if (c &gt; 0) kbest_lookup(s, mid+1, n, D+1);
else { kbest_lookup(s, suf, mid-suf, depth+1);
propose(*mid);
kbest_lookup(s, mid+1, n, D+1); }}}
</construct>
<page confidence="0.739884">
18
</page>
<subsectionHeader confidence="0.958762">
2.2 A Short Example: To be or not to be
</subsectionHeader>
<bodyText confidence="0.998214">
Suppose we were given the text, “to be or not to
be.” We could then generate the following dic-
tionary with frequencies (popularities).
</bodyText>
<figure confidence="0.863641333333333">
Popularity Word
2 to
2 be
1 or
1 not
The dictionary is sorted by popularity. We treat
the second column as an N=13 byte corpus (with
underscores at record boundaries): to_be_or_not_
Standard K-Best
suf corpus + sufji] suf corpus + sufji]
12 2 be or not
_
3 be_or_not_
4 e_or_not_
5 or not
3 be_or_not_
4 e_or_not_
9 not_
1 o_be_or_not_
6 or_not_
0 to_be_or_not_
7 r_not_
10 ot_
0 to_be_or_not_ 11 t_
</figure>
<bodyText confidence="0.999950136363636">
The standard suffix array is the 1st column of the
table above. For illustrative convenience, we show
the corresponding strings in the 2nd column. Note
that the 2nd column is sorted lexicographically.
The k-best suffix array is the 3rd column with the
corresponding strings in the 4th column. The first
split is a lexicographic split at 9 (“not_”). On both
sides of that split we have a popularity split at 5
(“_or_not_”) and 7 (“r_not_”). (Recall that relative
popularity depends on corpus position.) Following
there are 4 lexicographic splits, and so on.
If k-best lookup were given the query string s =
“o,” then it would find 1 (o_be_or_not_), 6
(or_not_) and 10 (ot_) as the best choices (in that
order). The first split is a lexicographic split. All
the matches are below 9 (not_). The next split is
on popularity. The matches above this split (1&amp;6)
are as popular as the matches below this split (10).
It is often desirable to output matching records
(rather than suffixes). Records are output in popu-
larity order. The actual popularity can be output,
using the side table mentioned in footnote 1:
</bodyText>
<subsectionHeader confidence="0.646716">
Popularity Record
</subsectionHeader>
<footnote confidence="0.755485333333333">
2 to
1 or
1 not
</footnote>
<subsectionHeader confidence="0.976149">
2.3 Time and Space Complexity
</subsectionHeader>
<bodyText confidence="0.99712975">
The space requirements are the same for both stan-
dard and k-best suffix arrays. Both indexes are
permutations of the same suffixes.
The time requirements are quite different. Stan-
dard suffix arrays were designed to find all
matches, not the k-best. Standard suffix arrays can
find all matches in O(log N) time. However, if we
attempt to use standard suffix arrays to find the k-
best, something they were not designed to do, then
it could take a long time to sort through the worst
case (an embarrassment of riches with lots of
matches). When the query matches every string in
the dictionary, standard suffix arrays do not help us
find the best matches. K-best suffix arrays were
designed to handle an embarrassment of riches,
which is quite common, especially when the sub-
string s is short. Each popularity split cuts the
search space in half when there are lots of lexico-
graphic matches.
The best case for k-best suffix arrays is when the
popularity splits always work in our favor and we
never have to search the less popular half. The
worst case is when the popularity splits always fail,
such as when the query string s is not in the corpus.
In this case, we must always check both the popu-
lar half and the unpopular half at each split, since
the failure to find a lexicographic match in the first
tells us nothing about the existence of matches in
the second.
Asymptotically, k-best lookup takes between log
N and sqrt N time. To see this complexity result,
let P(N) be the work to process N items starting
with a popularity splits and let L(N) be the work to
process N items starting with a lexicographic
splits.
Thus,
</bodyText>
<figure confidence="0.987152833333333">
9 not_
1 o_be_or_not_
6 or_not_
10 ot_
7 r_not_
11 t_
2 be or not
8 not
5 or not
8 not
12 _
19
</figure>
<equation confidence="0.963616">
P(N) = αL(N/2) + C1
L(N) = P(N/2) + C2
</equation>
<bodyText confidence="0.9957995">
where α = 2−p, when p is the probability that the
popular half contains sufficient matches. α lies
between 1 (best case) and 2 (worst case). C1 and
C2 are constants. Thus,
</bodyText>
<equation confidence="0.998744">
P(N) = α P(N/4) + C (1)
</equation>
<bodyText confidence="0.999980125">
where C = C1 + αC2. Using the master method
(Cormen et al, 2001), P(N) = O(log2N) in the best
case (α=1). In the worst case (α=2), P(N) = O(sqrt
N). In general, for α &gt; 1, P(N) = O(N(log2 α)/2).
In practical applications, we expect popularity
splits to work more often than not, and therefore
we expect the typical case to be closer to the best
case than the worst case.
</bodyText>
<sectionHeader confidence="0.992282" genericHeader="method">
3 Empirical Study
</sectionHeader>
<bodyText confidence="0.998827615384615">
The plot below shows the k-best lookup time as
a function of square root of corpus size. We ex-
tracted sub-corpora from a 150 MB collection of
8M queries, sorted by popularity, according to the
logs from Microsoft www.live.com. All experi-
ments were performed on a Pentium 4, 3.2GHz
dual processor machine with enough memory to
avoid paging.
The line of diamonds shows the worst case,
where we the query string is not in the index. Note
that the diamonds fit the regression line quite well,
confirming the theory in the previous section: The
worst case lookup is O(sqrt N).
</bodyText>
<figure confidence="0.980446714285714">
50
40
30
20
10
0
0 1000 2000 3000
</figure>
<subsectionHeader confidence="0.853874">
Sqrt(Corpus size)
</subsectionHeader>
<bodyText confidence="0.999972538461538">
To simulate a more typical scenario, we con-
structed random samples of queries by popularity,
represented by squares in the figure. Note that the
squares are well below the line, demonstrating that
these queries are considerably easier than the worst
case.
K-best suffix arrays have been used in auto-
complete applications (Church and Thiesson,
2005). The triangles with the fastest lookup times
demonstrate the effectiveness of the index for this
application. We started with the random sample
above, but replaced each query q in the sample
with a substring of q (of random size).
</bodyText>
<sectionHeader confidence="0.999077" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999990761904762">
A new data structure, k-best suffix arrays, was pro-
posed. K-best suffix arrays are sorted by two or-
ders, lexicographic and popularity, which make it
convenient to find the most popular matches, espe-
cially when there are lots of matches. In many ap-
plications, such as the web, there are often
embarrassments of riches (lots of matches).
Lookup time varies from log N to sqrt N, de-
pending on the effectiveness of the popularity
splits. In the best case (e.g., very short query
strings that match nearly everything), the popular-
ity splits work nearly every time and we rarely
have to search the less popular side of a popularity
split. In this case, the time is close to log N. On
the other hand, in the worst case (e.g., query
strings that match nothing), the popularity splits
never work, and we always have to search both
sides of a popularity split. In this case, lookup
time is sqrt N. In many cases, popularity splits
work more often than not, and therefore, perform-
ance is closer to log N than sqrt N.
</bodyText>
<sectionHeader confidence="0.996462" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997098583333333">
Jon Louis Bentley. 1975. Multidimensional Binary
Search Trees Used for Associative Searching, Com-
munications of the ACM, 18:9, pp. 509-517.
Kenneth Church and Bo Thiesson. 2005. The Wild
Thing, ACL, pp. 93-96.
Udi Manber and Gene Myers. 1990. Suffix Arrays: A
New Method for On-line String Searches, SODA, pp.
319-327.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to Al-
gorithms, Second Edition. MIT Press and McGraw-
Hill, pp.73–90.
</reference>
<figure confidence="0.9847545">
Time (sec) for 10k lookups
20
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.010137">
<title confidence="0.999301">K-Best Suffix Arrays</title>
<author confidence="0.999952">Kenneth Church Bo Thiesson Robert Ragno</author>
<affiliation confidence="0.99595">Microsoft Microsoft Microsoft</affiliation>
<address confidence="0.993095">One Microsoft Way One Microsoft Way One Microsoft Way Redmond, WA 98052 Redmond, WA 98052 Redmond, WA</address>
<email confidence="0.998778">church@microsoft.comthiesson@microsoft.comrragno@microsoft.com</email>
<abstract confidence="0.99118101">Suppose we have a large dictionary of strings. Each entry starts with a figure of (popularity). We wish to find the matches for a substring, a dicti- That is, grep dict sort –n | but we would like to do this in sublinear time. Example applications: (1) web queries with popularities, (2) products with prices and (3) ads with click through rates. This paper proposes a index, suffix based on ideas borrowed from suffix arrays and kdtrees. A standard suffix array sorts the suffixes by a single order (lexicographic) whereas k-best suffix arrays are sorted by two orders (lexicographic and popularity). time is between N 1 Standard Suffix Arrays paper will introduce suffix which are similar to standard suffix arrays (Manber and Myers, 1990), an index that makes it convenient to compute the frequency and location of a a long sequence, A suffix is an array of all sorted al- A suffix, also known as a semi-infinite string, is a string that starts at position the corpus and continues to the end of the corpus. In practical implementations, a suffix is a 4integer, In this way, an int (constant space) a long string below creates a standard suffix array. The program starts with a a global variable containing a long string The program allocates the suffix initializes it to a vector of (sufranging from 0 to The suffix array is sorted by lexicographic order and returned. { int N = strlen(corpus); int* suf = (int*)malloc(N * sizeof(int)); for (int i=0; i&lt;N; i++) suf[i] = i; qsort(suf, N, sizeof(int), lexcomp); return suf;} a, int* b) { return strcmp(corpus + *a, corpus + *b);} This program is simple to describe (but inefficient, least in theory) because take time in the worst case (where the corpus contains two copies of an arbitrarily long string). See http://cm.bell-labs.com/cm/cs/who/doug/ssort.cfor implementation of the Manber and Myers algorithm. However, in practice, when the corpus is a dictionary of relatively short entries (such as web queries), the worst case is unlikely to up. In which case, the simple program above is good enough, and maybe even better the solution. Suffix Array Lookup To compute the frequency and locations of a subuse a pair of binary searches to find the locations of the first and last suffix in the sufarray that start with Each suffix between to a location of the corpus. The is simply: 1. Here is some simple code. We show how to find the first suffix. The last suffix is left as an exercise. As above, we ignore the unlikely worst 17 of NAACL HLT 2007, Companion pages NY, April 2007. Association for Computational Linguistics case (two copies of a long string). See references mentioned above for worst case solutions. s, int* suf, int N){ int* i = find_first_suf(s, suf, N); int* j = find_last_suf(s, suf, N); for (int* k=i; k&lt;=j; k++) output(*k);} s, int* suf, int N) { int len = strlen(s); int* high = suf + N; while (suf + 2 &lt; high) { int* mid = suf + (high−suf)/2; int c = strncmp(s, corpus + *mid, len); if (c == 0) high = mid+1; else if (c &lt; 0) high = mid; else suf = mid;} for ( ; suf &lt; high; suf++) if (strncmp(s, corpus + *suf, len) == 0) return suf; return NULL;} // not found 2 K-Best Suffix Arrays K-best suffix arrays are like standard suffix arrays, except there are two orders instead of one. In addition to lexicographic order, we assume a figure of merit, which we will refer to as popularity. For example, the popularity of a string could be its frequency in a search log. The code below assumes that the corpus is a sequence of strings that comes pre-sorted by popularity, and then the popularities have been stripped off. These assumptions make it very easy to compare two strings by popularity. to do is to compare the two posiin the below is similar to above except we now sort by the two orders at alternating depths in the tree. First we sort lexicographically and then we sort by popularity and so on, using a construction similar to KD-Trees (Bentley, 1975). The code below is simple to describe (though there are more efficient implementations that avoid unnecessary qsorts). { int N = strlen(corpus); int* suf = (int*)malloc(N * sizeof(int)); a little extra book keeping, one can keep a table on the side that makes it possible to map back and forth between popularity rank and the actual popularity. This turns out to be useful for some applications. for (int i=0; i&lt;N; i++) suf[i]=i; process(suf, suf+N, 0); return suf;} start, int* end, int depth) { int* mid = start + (end − start)/2; if (end &lt;= start+1) return; qsort(start, end-start, sizeof(int), (depth &amp; 1) ? popcomp : lexcomp); process(start, mid, depth+1); process(mid+1, end, depth+1);} a, int* b) { if (*a &gt; *b) return 1; if (*a &lt; *b) return −1; return 0;} 2.1 K-Best Suffix Array Lookup To find the k-best matches for a particular subwe do what we would normally do for standard suffix arrays on lexicographic splits. However, on popularity splits, we search the more popular half first and then we search the less popular half, if necessary. implementation of given bethe depth of the search thus far. initially called with 0. Proa heap of the k-best matches found far. true if its argument is less than the best match found thus far. s, int* suf, int N, int D){ int* mid = suf + N/2; int len = strlen(s); if (N==1 &amp;&amp; strncmp(s, corpus+*suf, len)==0) propose(*suf); if (N &lt;= 1) return; if (D&amp;1) { // popularity split kbest_lookup(s, suf, mid−suf, D+1); if (done(*mid)) return; if (strncmp(s, corpus + *mid, len) == 0) propose(*mid); kbest_lookup(s, mid+1, (suf+N)−mid−1, D+1);} else { // lexicographic int c = strncmp(s, corpus + *mid, len); int n = (suf+N)−mid−1; if (c &lt; 0) kbest_lookup(s, suf, mid-suf, D+1); else if (c &gt; 0) kbest_lookup(s, mid+1, n, D+1); else { kbest_lookup(s, suf, mid-suf, depth+1); propose(*mid); kbest_lookup(s, mid+1, n, D+1); }}} 18 2.2 A Short Example: To be or not to be Suppose we were given the text, “to be or not to be.” We could then generate the following dictionary with frequencies (popularities). Popularity Word 2 to 2 be 1 or 1 not The dictionary is sorted by popularity. We treat second column as an byte corpus (with underscores at record boundaries): to_be_or_not_ Standard K-Best suf corpus + sufji] suf corpus + sufji] 12 2 be or not _ 3 be_or_not_ 4 e_or_not_ 5 or not 3 be_or_not_ 4 e_or_not_ 9 not_ 1 o_be_or_not_ 6 or_not_ 0 to_be_or_not_ 7 r_not_ 10 ot_ 0 to_be_or_not_ 11 t_ standard suffix array is the column of the table above. For illustrative convenience, we show corresponding strings in the column. Note the column is sorted lexicographically. k-best suffix array is the column with the strings in the column. The first split is a lexicographic split at 9 (“not_”). On both sides of that split we have a popularity split at 5 (“_or_not_”) and 7 (“r_not_”). (Recall that relative popularity depends on corpus position.) Following there are 4 lexicographic splits, and so on. k-best lookup were given the query string “o,” then it would find 1 (o_be_or_not_), 6 (or_not_) and 10 (ot_) as the best choices (in that order). The first split is a lexicographic split. All the matches are below 9 (not_). The next split is on popularity. The matches above this split (1&amp;6) are as popular as the matches below this split (10). It is often desirable to output matching records (rather than suffixes). Records are output in popularity order. The actual popularity can be output, using the side table mentioned in footnote 1: Popularity Record 2 to 1 or 1 not 2.3 Time and Space Complexity The space requirements are the same for both standard and k-best suffix arrays. Both indexes are permutations of the same suffixes. The time requirements are quite different. Standard suffix arrays were designed to find all matches, not the k-best. Standard suffix arrays can all matches in time. However, if we attempt to use standard suffix arrays to find the kbest, something they were not designed to do, then it could take a long time to sort through the worst case (an embarrassment of riches with lots of matches). When the query matches every string in the dictionary, standard suffix arrays do not help us find the best matches. K-best suffix arrays were designed to handle an embarrassment of riches, which is quite common, especially when the subshort. Each popularity split cuts the search space in half when there are lots of lexicographic matches. The best case for k-best suffix arrays is when the popularity splits always work in our favor and we never have to search the less popular half. The worst case is when the popularity splits always fail, as when the query string not in the corpus. In this case, we must always check both the popular half and the unpopular half at each split, since the failure to find a lexicographic match in the first tells us nothing about the existence of matches in the second. Asymptotically, k-best lookup takes between log N and sqrt N time. To see this complexity result, let P(N) be the work to process N items starting with a popularity splits and let L(N) be the work to process N items starting with a lexicographic splits. Thus, 9 not_ 1 o_be_or_not_ 6 or_not_ 10 ot_ 7 r_not_ 11 t_ 2 be or not 8 not 5 or not 8 not 12 _ 19 = + = when the probability that the half contains sufficient matches. 1 (best case) and 2 (worst case). constants. Thus, + C = Using the master method 2001), = in the best the worst case = In general, for 1, = In practical applications, we expect popularity splits to work more often than not, and therefore we expect the typical case to be closer to the best case than the worst case. 3 Empirical Study The plot below shows the k-best lookup time as a function of square root of corpus size. We extracted sub-corpora from a 150 MB collection of 8M queries, sorted by popularity, according to the from Microsoftwww.live.com.All experiments were performed on a Pentium 4, 3.2GHz dual processor machine with enough memory to avoid paging. The line of diamonds shows the worst case, where we the query string is not in the index. Note that the diamonds fit the regression line quite well, confirming the theory in the previous section: The case lookup is</abstract>
<note confidence="0.703149166666667">50 40 30 20 10 0</note>
<phone confidence="0.482753">0 1000 2000 3000</phone>
<abstract confidence="0.964047861111111">Sqrt(Corpus size) To simulate a more typical scenario, we constructed random samples of queries by popularity, represented by squares in the figure. Note that the squares are well below the line, demonstrating that these queries are considerably easier than the worst case. K-best suffix arrays have been used in autocomplete applications (Church and Thiesson, 2005). The triangles with the fastest lookup times demonstrate the effectiveness of the index for this application. We started with the random sample but replaced each query the sample a substring of random size). 4 Conclusion new data structure, suffix was proposed. K-best suffix arrays are sorted by two orders, lexicographic and popularity, which make it convenient to find the most popular matches, especially when there are lots of matches. In many applications, such as the web, there are often embarrassments of riches (lots of matches). time varies from N depending on the effectiveness of the popularity splits. In the best case (e.g., very short query strings that match nearly everything), the popularity splits work nearly every time and we rarely have to search the less popular side of a popularity In this case, the time is close to On the other hand, in the worst case (e.g., query strings that match nothing), the popularity splits never work, and we always have to search both sides of a popularity split. In this case, lookup is In many cases, popularity splits work more often than not, and therefore, performis closer to N</abstract>
<note confidence="0.8553163">References Jon Louis Bentley. 1975. Multidimensional Binary Trees Used for Associative Searching, Comof the 18:9, pp. 509-517. Kenneth Church and Bo Thiesson. 2005. The Wild pp. 93-96. Udi Manber and Gene Myers. 1990. Suffix Arrays: A Method for On-line String Searches, pp. 319-327. Thomas H. Cormen, Charles E. Leiserson, Ronald L.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jon Louis Bentley</author>
</authors>
<title>Multidimensional Binary Search Trees Used for Associative Searching,</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<pages>509--517</pages>
<contexts>
<context position="4875" citStr="Bentley, 1975" startWordPosition="850" endWordPosition="851">be its frequency in a search log. The code below assumes that the corpus is a sequence of strings that comes pre-sorted by popularity, and then the popularities have been stripped off. These assumptions make it very easy to compare two strings by popularity. All popcomp has to do is to compare the two positions in the corpus.1 The make_kbest_suf program below is similar to the make_standard_suf program above except we now sort by the two orders at alternating depths in the tree. First we sort lexicographically and then we sort by popularity and so on, using a construction similar to KD-Trees (Bentley, 1975). The code below is simple to describe (though there are more efficient implementations that avoid unnecessary qsorts). int* make_kbest_suf () { int N = strlen(corpus); int* suf = (int*)malloc(N * sizeof(int)); 1 With a little extra book keeping, one can keep a table on the side that makes it possible to map back and forth between popularity rank and the actual popularity. This turns out to be useful for some applications. for (int i=0; i&lt;N; i++) suf[i]=i; process(suf, suf+N, 0); return suf;} void process(int* start, int* end, int depth) { int* mid = start + (end − start)/2; if (end &lt;= start+1</context>
</contexts>
<marker>Bentley, 1975</marker>
<rawString>Jon Louis Bentley. 1975. Multidimensional Binary Search Trees Used for Associative Searching, Communications of the ACM, 18:9, pp. 509-517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Bo Thiesson</author>
</authors>
<date>2005</date>
<booktitle>The Wild Thing, ACL,</booktitle>
<pages>93--96</pages>
<contexts>
<context position="12010" citStr="Church and Thiesson, 2005" startWordPosition="2115" endWordPosition="2118">ne of diamonds shows the worst case, where we the query string is not in the index. Note that the diamonds fit the regression line quite well, confirming the theory in the previous section: The worst case lookup is O(sqrt N). 50 40 30 20 10 0 0 1000 2000 3000 Sqrt(Corpus size) To simulate a more typical scenario, we constructed random samples of queries by popularity, represented by squares in the figure. Note that the squares are well below the line, demonstrating that these queries are considerably easier than the worst case. K-best suffix arrays have been used in autocomplete applications (Church and Thiesson, 2005). The triangles with the fastest lookup times demonstrate the effectiveness of the index for this application. We started with the random sample above, but replaced each query q in the sample with a substring of q (of random size). 4 Conclusion A new data structure, k-best suffix arrays, was proposed. K-best suffix arrays are sorted by two orders, lexicographic and popularity, which make it convenient to find the most popular matches, especially when there are lots of matches. In many applications, such as the web, there are often embarrassments of riches (lots of matches). Lookup time varies </context>
</contexts>
<marker>Church, Thiesson, 2005</marker>
<rawString>Kenneth Church and Bo Thiesson. 2005. The Wild Thing, ACL, pp. 93-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udi Manber</author>
<author>Gene Myers</author>
</authors>
<title>Suffix Arrays: A New Method for On-line String Searches, SODA,</title>
<date>1990</date>
<pages>319--327</pages>
<contexts>
<context position="1119" citStr="Manber and Myers, 1990" startWordPosition="177" endWordPosition="180"> we would like to do this in sublinear time. Example applications: (1) web queries with popularities, (2) products with prices and (3) ads with click through rates. This paper proposes a novel index, k-best suffix arrays, based on ideas borrowed from suffix arrays and kdtrees. A standard suffix array sorts the suffixes by a single order (lexicographic) whereas k-best suffix arrays are sorted by two orders (lexicographic and popularity). Lookup time is between log N and sqrt N. 1 Standard Suffix Arrays This paper will introduce k-best suffix arrays, which are similar to standard suffix arrays (Manber and Myers, 1990), an index that makes it convenient to compute the frequency and location of a substring, s, in a long sequence, corpus. A suffix array, suf, is an array of all N suffixes, sorted alphabetically. A suffix, suf[i], also known as a semi-infinite string, is a string that starts at position j in the corpus and continues to the end of the corpus. In practical implementations, a suffix is a 4- byte integer, j. In this way, an int (constant space) denotes a long string (N bytes). The make_standard_suf program below creates a standard suffix array. The program starts with a corpus, a global variable c</context>
</contexts>
<marker>Manber, Myers, 1990</marker>
<rawString>Udi Manber and Gene Myers. 1990. Suffix Arrays: A New Method for On-line String Searches, SODA, pp. 319-327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
<author>Clifford Stein</author>
</authors>
<title>Introduction to Algorithms, Second Edition.</title>
<date>2001</date>
<pages>73--90</pages>
<publisher>MIT Press</publisher>
<contexts>
<context position="10713" citStr="Cormen et al, 2001" startWordPosition="1887" endWordPosition="1890"> between log N and sqrt N time. To see this complexity result, let P(N) be the work to process N items starting with a popularity splits and let L(N) be the work to process N items starting with a lexicographic splits. Thus, 9 not_ 1 o_be_or_not_ 6 or_not_ 10 ot_ 7 r_not_ 11 t_ 2 be or not 8 not 5 or not 8 not 12 _ 19 P(N) = αL(N/2) + C1 L(N) = P(N/2) + C2 where α = 2−p, when p is the probability that the popular half contains sufficient matches. α lies between 1 (best case) and 2 (worst case). C1 and C2 are constants. Thus, P(N) = α P(N/4) + C (1) where C = C1 + αC2. Using the master method (Cormen et al, 2001), P(N) = O(log2N) in the best case (α=1). In the worst case (α=2), P(N) = O(sqrt N). In general, for α &gt; 1, P(N) = O(N(log2 α)/2). In practical applications, we expect popularity splits to work more often than not, and therefore we expect the typical case to be closer to the best case than the worst case. 3 Empirical Study The plot below shows the k-best lookup time as a function of square root of corpus size. We extracted sub-corpora from a 150 MB collection of 8M queries, sorted by popularity, according to the logs from Microsoft www.live.com. All experiments were performed on a Pentium 4, 3</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2001</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2001. Introduction to Algorithms, Second Edition. MIT Press and McGrawHill, pp.73–90.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>