<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000211">
<title confidence="0.993023">
Emotion Analysis Using Latent Affective Folding and Embedding
</title>
<author confidence="0.709961">
Jerome R. Bellegarda
</author>
<affiliation confidence="0.644861333333333">
Speech &amp; Language Technologies
Apple Inc.
Cupertino, California 95014, USA
</affiliation>
<email confidence="0.543718">
jerome @ apple.com
</email>
<sectionHeader confidence="0.993883" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999834578947368">
Though data-driven in nature, emotion analy-
sis based on latent semantic analysis still relies
on some measure of expert knowledge in or-
der to isolate the emotional keywords or key-
sets necessary to the construction of affective
categories. This makes it vulnerable to any
discrepancy between the ensuing taxonomy of
affective states and the underlying domain of
discourse. This paper proposes a more gen-
eral strategy which leverages two distincts se-
mantic levels, one that encapsulates the foun-
dations of the domain considered, and one that
specifically accounts for the overall affective
fabric of the language. Exposing the emergent
relationship between these two levels advan-
tageously informs the emotion classification
process. Empirical evidence suggests that this
is a promising solution for automatic emotion
detection in text.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996783979166667">
The automatic detection of emotions in text is
a necessary pre-processing step in many differ-
ent fields touching on affective computing (Picard,
1997), such as natural language interfaces (Cosatto
et al., 2003), e-learning environments (Ryan et al.,
2000), educational or entertainment games (Pivec
and Kearney, 2007), opinion mining and sentiment
analysis (Pang and Lee, 2008), humor recognition
(Mihalcea and Strapparava, 2006), and security in-
formatics (Abbasi, 2007). In the latter case, for ex-
ample, it can be used for monitoring levels of hate-
ful or violent rhetoric (perhaps in multilingual set-
tings). More generally, emotion detection is of great
1
interest in human-computer interaction: if a system
determines that a user is upset or annoyed, for in-
stance, it could switch to a different mode of inter-
action (Liscombe et al., 2005). And of course, it
plays a critical role in the generation of expressive
synthetic speech (Schr¨oder, 2006).
Emphasis has traditionally been placed on the set
of six “universal” emotions (Ekman, 1993): ANGER,
DISGUST, FEAR, JOY, SADNESS, and SURPRISE
(Alm et al., 2005; Liu et al., 2003; Subasic and Huet-
tner, 2001). Emotion analysis is typically carried out
using a simplified description of emotional states in
a low-dimensional space, which normally comprises
dimensions such as valence (positive/negative eva-
lution), activation (stimulation of activity), and/or
control (dominant/submissive power) (Mehrabian,
1995; Russell, 1980; Strapparava and Mihalcea,
2008). Classification proceeds based on an underly-
ing emotional knowledge base, which strives to pro-
vide adequate distinctions between different emo-
tions. This affective information can either be built
entirely upon manually selected vocabulary as in
(Whissell, 1989), or derived automatically from data
based on expert knowledge of the most relevant fea-
tures that can be extracted from the input text (Alm
et al., 2005). In both cases, the resulting system
tends to rely, for the most part, on a few thousand
annotated “emotional keywords,” the presence of
which triggers the associated emotional label(s).
The drawback of such confined lexical affinity is
that the analysis tends to be hampered by the bias
inherent in the underlying taxonomy of emotional
states. Because this taxonomy only supports simpli-
fied relationships between affective words and emo-
</bodyText>
<note confidence="0.9845295">
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 1–9,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99989330952381">
tional categories, it often fails to meaningfully gen-
eralize beyond the relatively few core terms explic-
itly considered in its construction. This has sparked
interest in data-driven approaches based on latent
semantic analysis (LSA), a paradigm originally de-
veloped for information retrieval (Deerwester et al.,
1990). Upon suitable training using a large corpus
of texts, LSA allows a similarity score to be com-
puted between generic terms and affective categories
(Strapparava et al., 2006). This way, every word can
automatically be assigned some fractional affective
influence. Still, the affective categories themselves
are usually specified with the help of a reference lex-
ical database like WordNet (Fellbaum, 1998).
The purpose of this paper is to more broadly lever-
age the principle of latent semantics in emotion anal-
ysis. We cast the problem as a general application
of latent semantic mapping (LSM), an extrapolation
of LSA for modeling global relationships implicit
in large volumes of data (Bellegarda, 2005; Belle-
garda, 2008). More specifically, we use the LSM
framework to describe two distinct semantic levels:
one that encapsulates the foundations of the domain
considered (e.g., broadcast news, email messages,
SMS conversations, etc.), and one that specifically
accounts for the overall affective fabric of the lan-
guage. Then, we leverage these two descriptions
to appropriately relate domain and affective levels,
and thereby inform the emotion classification pro-
cess. This de facto bypasses the need for any explicit
external knowledge.
The paper is organized as follows. The next sec-
tion provides some motivation for, and gives an
overview of, the proposed latent affective frame-
work. In Sections 3 and 4, we describe the two main
alternatives considered, latent folding and latent em-
bedding. In Section 5, we discuss the mechanics
of emotion detection based on such latent affective
processing. Finally, Section 6 reports the outcome
of experimental evaluations conducted on the “Af-
fective Text” portion of the SemEval-2007 corpus
(Strapparava and Mihalcea, 2007).
</bodyText>
<sectionHeader confidence="0.935119" genericHeader="introduction">
2 Motivation and Overview
</sectionHeader>
<bodyText confidence="0.999946">
As alluded to above, lexical affinity alone fails
to provide sufficient distinction between different
emotions, in large part because only relatively few
</bodyText>
<subsectionHeader confidence="0.8387032">
WordNet Synset
Specific Word
Pseudo−document
Similarity
Input Text
</subsectionHeader>
<figureCaption confidence="0.997676">
Figure 1: Typical LSA-Based Emotion Analysis.
</figureCaption>
<bodyText confidence="0.999893973684211">
words have inherently clear, unambiguous emo-
tional meaning. For example, happy and sad encap-
sulate JOY and SADNESS, respectively, in all con-
ceivable scenarios. But is thrilling a marker of JOY
or SURPRISE? Does awful capture SADNESS or DIS-
GUST? It largely depends on contextual informa-
tion: thrilling as a synonym for uplifting conveys
JOY (as in a thrilling speech), while thrilling as a
synonym for amazing may well mark SURPRISE (as
in a thrilling waterfall ride); similarly, awful as a
synonym for grave reflects SADNESS (as in an aw-
ful car accident), while awful as a synonym for foul
is closer to DISGUST (as in an awful smell). The vast
majority of words likewise carry multiple potential
emotional connotations, with the degree of affective
polysemy tightly linked to the granularity selected
for the underlying taxonomy of emotions.
Data-driven approaches based on LSA purport
to “individuate” such indirect affective words via
inference mechanisms automatically derived in an
unsupervised way from a large corpus of texts,
such as the British National Corpus (Strapparava
et al., 2006). By looking at document-level co-
occurrences, contextual information is exploited to
encapsulate semantic information into a relatively
low dimensional vector space. Suitable affective cat-
egories are then constructed in that space by “folding
in” either the specific word denoting the emotion, or
its associated synset (say, from WordNet), or even
the entire set of words in all synsets that can be la-
belled with that emotion (Strapparava and Mihalcea,
2008). This is typically done by placing the rele-
vant word(s) into a “pseudo-document,” and map it
into the space as if it were a real one (Deerwester et
al., 1990). Finally, the global emotional affinity of a
given input text is determined by computing similar-
ities between all pseudo-documents. The resulting
framework is depicted in Fig. 1.
</bodyText>
<figure confidence="0.997988111111111">
All Synsets
LSA
Processing
Homogeneous
Representation
Large
Corpus
Detected
Emotion
</figure>
<page confidence="0.998408">
2
</page>
<bodyText confidence="0.999986895833334">
This solution is attractive, if for no other reason
than it allows every word to automatically be as-
signed some fractional affective influence. However,
it suffers from two limitations which may well prove
deleterious in practical situations. First, the inherent
lack of supervision routinely leads to a latent seman-
tic space which is not particularly representative of
the underlying domain of discourse. And second,
the construction of the affective categories still relies
heavily on pre-defined lexical affinity, potentially re-
sulting in an unwarranted bias in the taxonomy of
affective states.
The first limitation impinges on the effectiveness
of any LSA-based approach, which is known to vary
substantially based on the size and quality of the
training data (Bellegarda, 2008; Mohler and Mihal-
cea, 2009). In the present case, any discrepancy
between latent semantic space and domain of dis-
course may distort the position of certain words in
the space, which could in turn lead to subsequent
sub-optimal affective weight assignment. For in-
stance, in the examples above, the word smell is con-
siderably more critical to the resolution of awful as
a marker of DISGUST than the word car. But that
fact may never be uncovered if the only pertinent
documents in the training corpus happen to be about
expensive fragrances and automobiles. Thus, it is
highly desirable to derive the latent semantic space
using data representative of the application consid-
ered. This points to a modicum of supervision.
The second limitation is tied to the difficulty of
coming up with an a priori affective description that
will work universally. Stipulating the affective cat-
egories using only the specific word denoting the
emotion is likely to be less robust than using the set
of words in all synsets labelled with that emotion.
On the other hand, the latter may well expose some
inherent ambiguities resulting from affective poly-
semy. This is compounded by the relatively small
number of words for which an affective distribution
is even available. For example, the well-known Gen-
eral Inquirer content analysis system (Stone, 1997)
lists only about 2000 words with positive outlook
and 2000 words with negative outlook. There are ex-
actly 1281 words inventoried in the affective exten-
sion of WordNet (Strapparava and Mihalcea, 2008),
and the affective word list from (Johnson–Laird and
Oatley, 1989) comprises less than 1000 words. This
</bodyText>
<subsectionHeader confidence="0.725826">
Input Text
</subsectionHeader>
<figureCaption confidence="0.998577">
Figure 2: Proposed Latent Affective Framework.
</figureCaption>
<bodyText confidence="0.999916454545454">
considerably complicates the construction of reli-
able affective categories in the latent space.
To address the two limitations above, we pro-
pose to more broadly leverage the LSM paradigm
(Bellegarda, 2005; Bellegarda, 2008), following the
overall framework depicted in Fig. 2. Compared to
Fig. 1, we inject some supervision at two separate
levels: not only regarding the particular domain con-
sidered, but also how the affective categories them-
selves are defined. The first task is to exploit a suit-
able training collection to encapsulate into a (do-
main) latent semantic space the general foundations
of the domain at hand. Next, we leverage a sepa-
rate affective corpus, such as mood-annotated blog
entries from LiveJournal.com (Strapparava and Mi-
halcea, 2008), to serve as a descriptive blueprint for
the construction of affective categories.
This blueprint is then folded into the domain
space in one of two ways. The easiest approach,
called latent affective folding, is simply to super-
impose affective anchors inferred in the space for
every affective category. This is largely analogous
to what happens in Fig. 1, with a crucial difference
regarding the representation of affective categories:
in latent affective folding, it is derived from a cor-
pus of texts as opposed to a pre-specified keyword
or keyset. This is likely to help making the cat-
egories more robust, but may not satisfactorily re-
solve subtle distinctions between emotional conno-
tations. This technique is described in detail in the
next section.
The second approach, called latent affective em-
bedding, is to extract a distinct LSM representation
</bodyText>
<figure confidence="0.996481357142857">
LSM
Processing
Affective
Space
Affective Corpus
Latent Affective
Folding
Latent Affective
Embedding
Domain LSM Domain Pseudo−document Detected
Processing Similarity
Corpus Space Emotion
Affective
Anchors
</figure>
<page confidence="0.531055">
3
</page>
<figureCaption confidence="0.997361">
Figure 3: Emotion Analysis Using Latent Folding.
</figureCaption>
<bodyText confidence="0.999972066666667">
from the affective corpus, to encapsulate all prior
affective information into a separate (affective) la-
tent semantic space. In this space, affective anchors
can be computed directly, instead of inferred after
folding, presumably leading to a more accurate posi-
tioning. Domain and affective LSM spaces can then
be related to each other via a mapping derived from
words that are common to both. This way, the af-
fective anchors can be precisely embedded into the
domain space. This technique is described in detail
in Section 4.
In both cases, the input text is mapped into the
domain space as before. Emotion classification then
follows from assessing how closely it aligns with
each affective anchor.
</bodyText>
<sectionHeader confidence="0.969977" genericHeader="method">
3 Latent Affective Folding
</sectionHeader>
<bodyText confidence="0.965477448275862">
Expanding the basic framework of Fig. 2 to take into
account the two separate phases of training and anal-
ysis, latent affective folding proceeds as illustrated
in Fig. 3.
Let T, |7 |= N1, be a collection of training texts
(be they sentences, paragraphs, or documents) re-
flecting the domain of interest, and V1, |V1 |= M1,
the associated set of all words (possibly augmented
with some strategic word pairs, triplets, etc., as ap-
propriate) observed in this collection. Generally, M1
is on the order of several tens of thousands, while N1
may be as high as a million.
We first construct a (M1 x N1) matrix W1, whose
elements wij suitably reflect the extent to which
each word wi E V1 appeared in each text tj E Ti.
From (Bellegarda, 2008), a reasonable expression
for wij is:
wi,j = (1 − Ei) ci,j
nj
where ci,j is the number of times wi occurs in text
tj, nj is the total number of words present in this
text, and Ei is the normalized entropy of wi in V1.
The global weighting implied by 1 − Ei reflects the
fact that two words appearing with the same count in
a particular text do not necessarily convey the same
amount of information; this is subordinated to the
distribution of words in the entire set V1.
We then perform a singular value decomposition
(SVD) of W1 as (Bellegarda, 2008):
</bodyText>
<equation confidence="0.9977145">
W1 = U1 S1 V T
1 , (2)
</equation>
<bodyText confidence="0.971986090909091">
where U1 is the (M1 x R1) left singular matrix with
row vectors u1,i (1 &lt; i &lt; M1), S1 is the (R1 x R1)
diagonal matrix of singular values s1,1 &gt; s1,2 &gt;
... &gt; s1,Ri &gt; 0, V1 is the (N1 x R1) right sin-
gular matrix with row vectors v1,j (1 &lt; j &lt; N1),
R1 « M1, N1 is the order of the decomposition,
and T denotes matrix transposition.
As is well known, both left and right singular
matrices U1 and V1 are column-orthonormal, i.e.,
U1T U1 = V T
1 V1 = IR, (the identity matrix of order
R1). Thus, the column vectors of U1 and V1 each
define an orthornormal basis for the space of dimen-
sion R1 spanned by the u1,i’s and v1,j’s. We refer
to this space as the latent semantic space L1. The
(rank-R1) decomposition (2) encapsulates a map-
ping between the set of words wi and texts tj and
(after apropriate scaling by the singular values) the
set of R1-dimensional vectors y1,i = u1,iS1 and
z1,j = v1,jS1.
The basic idea behind (2) is that the rank-R1 de-
composition captures the major structural associa-
tions in W1 and ignores higher order effects. Hence,
the relative positions of the input words in the space
L1 reflect a parsimonious encoding of the semantic
concepts used in the domain considered. This means
that any new text mapped onto a vector “close” (in
some suitable metric) to a particular set of words can
be expected to be closely related to the concept en-
capsulated by this set. If each of these words is then
scored in terms of their affective affinity, this offers
a way to automatically predict the overall emotional
affinity of the text.
</bodyText>
<figure confidence="0.99817964">
ANALYSIS
TRAINING
Domain
Corpus
Input
Text
LSM
Map Creation
LSM
Mapping
Vector
Domain
Input
Space
Affective
Closeness Measure
Affective Corpus
Similarity
Computation
Latent
Folding
Anchors
Detected
Emotion
, (1)
</figure>
<page confidence="0.958668">
4
</page>
<bodyText confidence="0.978891545454545">
In order to do so, we need to isolate regions in
that space which are representative of the underly-
ing taxonomy of emotions considered. The centroid
of each such region is the affective anchor associ-
ated with that basic emotion. Affective anchors are
superimposed onto the space L1 on the basis of the
affective corpus available.
Let T2, |T2 |= N2, represent a separate collection
of mood-annotated texts (again they could be sen-
tences, paragraphs, or documents), representative of
the desired categories of emotions (such as JOY and
SADNESS), and V2, |V2 |= M2, the associated set of
words or expressions observed in this collection. As
such affective data may be more difficult to gather
than regular texts (especially in annotated form), in
practice N2 &lt; N1.
Further let V12, |V12 |= M12, represent the in-
tersection between V1 and V2. We will denote the
representations of these words in L1 by λ1,k (1 &lt;
k &lt; M12). —
Clearly, it is possible to form, for each 1 &lt; Q &lt; L,
where L is the number of distinct emotions consid-
ered, each subset V(~)
12 of all entries from V12 which
is aligned with a particular emotion.1 We can then
compute:
as the affective anchor of emotion E (1 &lt; Q &lt; L)
in the domain space. The notation ˆz1,P is chosen to
underscore the connection with z1,j: in essence, ˆz1,P
represents the (fictitious) text in the domain space
that would be perfectly aligned with emotion E, had
it been seen the training collection T1. Comparing
the representation of an input text to each of these
anchors therefore leads to a quantitative assessment
for the overall emotional affinity of the text.
A potential drawback of this approach is that (3) is
patently sensitive to the distribution of words within
T2, which may be quite different from the distribu-
tion of words within T1. In such a case, “folding in”
the affective anchors as described above may well
introduce a bias in the position of the anchors in the
domain space. This could in turn lead to an inability
to satisfactorily resolve subtle distinctions between
emotional connotations.
</bodyText>
<footnote confidence="0.9844885">
1Note that one entry could conceivably contribute to several
such subsets.
</footnote>
<figureCaption confidence="0.99585">
Figure 4: Emotion Analysis Using Latent Embedding.
</figureCaption>
<sectionHeader confidence="0.977167" genericHeader="method">
4 Latent Affective Embedding
</sectionHeader>
<bodyText confidence="0.999327">
To remedy this situation, a natural solution is to
build a separate LSM space from the affective train-
ing data. Referring back to the basic framework
of Fig. 2 and taking into account the two separate
phases of training and analysis as in Fig. 3, latent af-
fective embedding proceeds as illustrated in Fig. 4.
The first task is to group all N2 documents present
in T2 into L bins, one for each of the emotions con-
sidered. Then we can construct a (M2 x L) matrix
W2, whose elements wk,t suitably reflect the extent
to which each word or expression wk ∈ V2 appeared
in each affective category ct, 1 &lt; Q &lt; L. This leads
to:
</bodyText>
<equation confidence="0.916722">
(4)
n,
</equation>
<bodyText confidence="0.9993442">
withc&apos; k,P, nt&apos;, and εk following definitions analogous
to (1), albeit with domain texts replaced by affective
categories.
We then perform the SVD of W2 in a similar vein
as (2):
</bodyText>
<equation confidence="0.99369">
W2 = U2 S2 V T
2, (5)
</equation>
<bodyText confidence="0.999921666666667">
where all definitions are analogous. As before,
both left and right singular matrices U2 and V2 are
column-orthonormal, and their column vectors each
define an orthornormal basis for the space of dimen-
sion R2 spanned by the u2,k’s and v2,g’s. We refer
to this space as the latent affective space L2. The
</bodyText>
<figure confidence="0.991115428571429">
ˆz1,t =
�l,k , (3)
12 V12�
1
|V(P)
ANALYSIS
TRAINING
Domain
Corpus
Input
Text
LSM
Map Creation
LSM
Mapping
Affective
Domain
Vector
Corpus
Input
Space
Affective Space
Affective
Closeness Measure
LSM
Map Creation
Similarity
Computation
Latent
Embedding
Anchors
Detected
Emotion
i i c~k,t
wk ,t = (1 − εk)
</figure>
<page confidence="0.653771">
5
</page>
<bodyText confidence="0.900288326086957">
(rank-R2) decomposition (5) encapsulates a map- where P is the R1 to R2 projection matrix. Note
ping between the set of words wk and categories ct that K12 is typically full rank as long as M12 &gt; R22.
and (after apropriate scaling by the singular values) Performing the SVD of K12 yields the expression:
the set of R2-dimensional vectors y2,k = u2,kS2 and K12 = bQΨT , (10)
z2,t = v2,A. where as before Q is the diagonal matrix of singu-
Thus, each vector z2,t can be viewed as the cen- lar values, and b and Ψ are both unitary in the unit
troid of an emotion in L2, or, said another way, an sphere of dimension R2. This in turn leads to the
affective anchor in the affective space. Since their definition:
relative positions reflect a parsimonious encoding of r = bΨT , (11)
the affective annotations observed in the emotion which can be shown (cf. (Bellegarda et al., 1994))
corpus, these affective anchors now properly take to represent the least squares rotation that must be
into account any accidental skew in the distribution applied (in that unit sphere) to λ2,k to obtain an esti-
of words which contribute to them. All that remains mate of P ¯λ1,kP T.
to do is map them back to the domain space. Now what is needed is to apply this transforma-
This is done on the basis of words that are com- tion to the centroids z2,t (1 G Q G L) of the affective
mon to both the affective space and the domain categories in the affective space, so as to map them
space, i.e., the words in V12. Since these words were to the domain space. We first project each vector
denoted by λ1,k in L1, we similarly denote them by into the unit sphere, resulting in:
λ2,k (1 G k G M12) in L2. ¯z2,P = E−1/2
Now let μ1, μ2 and E1, E2 denote the mean vec- 2 (z2,t − μ2) , (12)
tor and covariance matrix for all observations λ1,k as prescribed in (7). We then synthesize from ¯z2,t
and λ2,k in the two spaces, respectively. We first a unit sphere vector corresponding to the estimate
transform each feature vector as: in the projected domain space. From the foregoing,
¯λ1,k = E−1/2 this estimate is given by:
1 (λ1,k − μ1) , (6) ˆ¯z1,t = r ¯z2,t . (13)
¯λ2,k = E−1/2 Finally, we restore the resulting contribution at the
2 (λ2,k − μ2) , (7) appropriate place in the domain space, by reversing
so that the resulting sets { λ1,kI and { λ2,kI each have the transformation (6):
zero mean and identity covariance matrix. 12ˆ¯
For this purpose, the inverse square root of each ˆz1 g = E1/z1 g + μ1 . (14)
covariance matrix can be obtained as: Combining the three steps (12)–(14) together, the
E−1/2 = QA−1/2QT ,(8) overall mapping can be written as:
where Q is the eigenvector matrix of the covariance ˆz1,P = (Ei/2rE21/2) z2,P + (μ1−E1/21rE21/2μ2) .
matrix E, and A is the diagonal matrix of corre- (15)
sponding eigenvalues. This applies to both domain This expression stipulates how to leverage the ob-
and affective data. served affective anchors z2,t in the affective space
We next relate each vector λ2,k in the affective to obtain an estimate of the unobserved affective an-
space to the corresponding vector ¯λ1,k in the do- chors ˆz1,P in the domain space, for 1 G Q G L. The
main space. For a relative measure of how the two overall procedure is illustrated in Fig. 5 (in the sim-
spaces are correlated with each other, as accumu- ple case of two dimensions).
lated on a common word basis, we first project λ1,k Once the affective anchors are suitably embedded
into the unit sphere of same dimension as λ2,k, i.e., into the domain space, we proceed as before to com-
R2 = min(R1, R2). We then compute the (normal- pare the representation of a given input text to each
ized) cross-covariance matrix between the two unit of these anchors, which leads to the desired quan-
sphere representations, specified as: titative assessment for the overall emotional affinity
M12 of the text.
</bodyText>
<figure confidence="0.793852666666667">
K12 = E P ¯λ1,kP T ¯λT 2,k , (9)
k=1
6
</figure>
<figureCaption confidence="0.999051">
Figure 5: Affective Anchor Embedding (2-D Case).
</figureCaption>
<sectionHeader confidence="0.987649" genericHeader="method">
5 Emotion Classification
</sectionHeader>
<bodyText confidence="0.999492733333333">
To summarize, using either latent affective folding
or latent affective embedding, we end up with an es-
timate ˆz1,e of the affective anchor for each emotion
Q in the domain space L1. What remains to be de-
scribed is how to perform emotion classification in
that space.
To proceed, we first need to specify how to repre-
sent in that space an input text not seen in the train-
ing corpus, say tp (where p &gt; N1). For each entry in
T1, we compute for the new text the weighted counts
(1) with j = p. The resulting feature vector, a col-
umn vector of dimension N1, can be thought of as
an additional column of the matrix W1. Assuming
the matrices U1 and 51 do not change appreciably,
the SVD expansion (2) therefore implies:
</bodyText>
<equation confidence="0.969107">
tp = U1 51 vT1,p , (16)
</equation>
<bodyText confidence="0.9937403">
where the R1-dimensional vector vT1,p acts as an ad-
ditional column of the matrix V T
1 . Thus, the repre-
sention of the new text in the domain space can be
obtained from z1,p = v1,p51.
All is needed now is a suitable closeness measure
to compare this representation to each affective an-
chor ˆz1,e (1 G Q G L). From (Bellegarda, 2008), a
natural metric to consider is the cosine of the angle
between them. This yields:
</bodyText>
<equation confidence="0.632886">
C(z1,p, ˆz1,t) =
</equation>
<bodyText confidence="0.9998708">
for any 1 G Q G L. Using (17), it is a simple matter
to directly compute the relevance of the input text to
each emotional category. It is important to note that
word weighting is now implicitly taken into account
by the LSM formalism.
</bodyText>
<sectionHeader confidence="0.992385" genericHeader="method">
6 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.99993665">
In order to evaluate the latent affective framework
described above, we used the data set that was devel-
oped for the SemEval 2007 task on “Affective Text”
(Strapparava and Mihalcea, 2007). This task was fo-
cused on the emotion classification of news head-
lines. Headlines typically consist of a few words
and are often written by creative people with the
intention to “provoke” emotions, and consequently
attract the readers’ attention. These characteris-
tics make this kind of data particularly suitable for
use in an automatic emotion recognition setting,
as the affective/emotional features (if present) are
guaranteed to appear in these short sentences. The
test data accordingly consisted of 1,250 short news
headlines2 extracted from news web sites (such as
Google news, CNN) and/or newspapers, and anno-
tated along L = 6 emotions (ANGER, DISGUST,
FEAR, JOY, SADNESS, and SURPRISE) by different
evaluators.
For baseline purposes, we considered the follow-
ing approaches: (i) a simple word accumulation sys-
tem, which annotates the emotions in a text based on
the presence of words from the WordNet-Affect lex-
icon; and (ii) three LSA-based systems implemented
as in Fig. 1, which only differ in the way each emo-
tion is represented in the LSA space: either based
on a specific word only (e.g., JOY), or the word
plus its WordNet synset, or the word plus all Word-
Net synsets labelled with that emotion in WordNet-
Affect (cf. (Strapparava and Mihalcea, 2007)). In all
three cases, the large corpus used for LSA process-
ing was the Wall Street Journal text collection (Graff
et al., 1995), comprising about 86,000 articles.
For the latent affective framework, we needed to
select two separate training corpora. For the “do-
main” corpus, we selected a collection of about
N1 = 8, 500 relatively short English sentences (with
a vocabulary of roughly M1 = 12, 000 words)
originally compiled for the purpose of a building
a concatenative text-to-speech voice. Though not
</bodyText>
<footnote confidence="0.979114">
2Development data was merged into the original SemEval
2007 test set to produce a larger test set.
</footnote>
<figure confidence="0.991097301886792">
x
x
x
x
x
x
x
x
x
x
λ1,k
x
x x
x
Domain
Space
1/2 ^
Σ1 z 1,l + μ1
+ +
+
Σ−1/2
2
+ +
+
z2,l
λ2,k
(λ 2,k− μ2)
+ +
+ +
Affective
+
Space
+
Σ−1/2
1
(λ 1,k − μ1 )
1/2
Σ2
( z2,l− μ2 )
x
Γ
+
z2,l
+
x
^
z1,l= Γ z2,l
Unit
Sphere
T
z1,p z1,P
Iz1,pI Iˆz1,d ,
(17)
</figure>
<page confidence="0.997895">
7
</page>
<tableCaption confidence="0.999078">
Table I: Results on SemEval-2007 Test Corpus.
</tableCaption>
<table confidence="0.999441142857143">
Approach Considered Precision Recall F-Measure
Baseline Word Accumulation 44.7 2.4 4.6
LSA (Specific Word Only) 11.5 65.8 19.6
LSA (With WordNet Synset) 12.2 77.5 21.1
LSA (With All WordNet Synsets) 11.4 89.6 20.3
Latent Affective Folding 18.8 90.1 31.1
Latent Affective Embedding 20.9 91.7 34.0
</table>
<bodyText confidence="0.985358361111111">
completely congruent with news headlines, we felt
that the type and range of topics covered was close
enough to serve as a good proxy for the domain.
For the “affective” corpus, we relied on about N2 =
5, 000 mood-annotated blog entries from LiveJour-
nal.com, with a filtered3 vocabulary of about M2 =
20, 000 words. The indication of mood being ex-
plicitly specified when posting on LiveJournal, with-
out particular coercion from the interface, mood-
annotated posts are likely to reflect the true mood of
the blog authors (Strapparava and Mihalcea, 2008).
The moods were then mapped to the L = 6 emotions
considered in the classification.
Next, we formed the domain and affective matri-
ces W1 and W2 and processed them as in (2) and (5).
We used R1 = 100 for the dimension of the domain
space L1 and R2 = L = 6 for the dimension of
the affective space L2. We then compared latent af-
fective folding and embedding to the above systems.
The results are summarized in Table I.
Consistent with the observations in (Strapparava
and Mihalcea, 2008), word accumulation secures the
highest precision at the cost of the lowest recall,
while LSA-based systems achieve high recall but
significantly lower precision. Encouragingly, the F-
measure obtained with both latent affective mapping
techniques is substantially higher than with all four
baseline approaches. Of the two techniques, latent
embedding performs better, presumably because the
embedded affective anchors are less sensitive than
the folded affective anchors to the distribution of
words within the affective corpus. Both techniques
seem to exhibit an improved ability to resolve dis-
tinctions between emotional connotations.
3Extensive text pre-processing is usually required on blog
entries, to address typos and assorted creative license.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999991628571429">
We have proposed a data-driven strategy for emotion
analysis which focuses on two coupled phases: (i)
separately encapsulate both the foundations of the
domain considered and the overall affective fabric
of the language, and (ii) exploit the emergent rela-
tionship between these two semantic levels of de-
scription in order to inform the emotion classifica-
tion process. We address (i) by leveraging the la-
tent topicality of two distinct corpora, as uncovered
by a global LSM analysis of domain-oriented and
emotion-oriented training documents. The two de-
scriptions are then superimposed to produce the de-
sired connection between all terms and emotional
categories. Because this connection automatically
takes into account the influence of the entire train-
ing corpora, it is more encompassing than that based
on the relatively few affective terms typically con-
sidered in conventional processing.
Empirical evidence gathered on the “Affective
Text” portion of the SemEval-2007 corpus (Strap-
parava and Mihalcea, 2007) shows the effective-
ness of the proposed strategy. Classification per-
formance with latent affective embedding is slightly
better than with latent affective folding, presumably
because of its ability to more richly describe the
affective space. Both techniques outperform stan-
dard LSA-based approaches, as well as affectively
weighted word accumulation. This bodes well for
the general deployability of latent affective process-
ing across a wide range of applications.
Future efforts will concentrate on characterizing
the influence of the parameters R1 and R2 on the
vector spaces L1 and L2, and the corresponding
trade-off between modeling power and generaliza-
tion properties. It is also of interest to investigate
</bodyText>
<page confidence="0.989845">
8
</page>
<bodyText confidence="0.999476666666667">
how incorporating higher level units (such as com-
mon lexical compounds) into the LSM procedure
might further increase performance.
</bodyText>
<sectionHeader confidence="0.998435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999946755102041">
A. Abbasi (2007), “Affect Intensity Analysis of Dark
Web Forums,” in Proc. IEEE Int. Conf. Intelligence
and Security Informatics (ISI), New Brunswick, NJ,
282–288.
C. Ovesdotter Alm, D. Roth, and R. Sproat (2005),
“Emotions from Text: Machine Learning for Text–
Based Emotion Prediction,” in Proc. Conf. Human
Language Technology and Empirical Methods in NLP,
Vancouver, BC, 579–586.
J.R. Bellegarda (2005), “Latent Semantic Mapping: A
Data–Driven Framework for Modeling Global Rela-
tionships Implicit in Large Volumes of Data,” IEEE
Signal Processing Magazine, 22(5):70–80.
J.R. Bellegarda (2008), Latent Semantic Mapping: Prin-
ciples &amp; Applications, Synthesis Lectures on Speech
and Audio Processing Series, Fort Collins, CO: Mor-
gan &amp; Claypool.
J.R. Bellegarda, P.V. de Souza, A. Nadas, D. Nahamoo,
M.A. Picheny and L.R. Bahl (1994), “The Metamor-
phic Algorithm: A Speaker Mapping Approach to
Data Augmentation,” IEEE Trans. Speech and Audio
Processing, 2(3):413–420.
E. Cosatto, J. Ostermann, H.P. Graf, and J. Schroeter
(2003), “Lifelike talking faces for interactive ser-
vices,” in Proc. IEEE, 91(9), 1406–1429.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman (1990), “Indexing by Latent Se-
mantic Analysis,” J. Amer. Soc. Information Science,
41:391–407.
P. Ekman (1993), “Facial Expression and Emotion”,
American Psychologist, 48(4), 384–392.
C. Fellbaum, Ed., (1998), WordNet: An Electronic Lexi-
cal Database, Cambridge, MA: MIT Press.
D. Graff, R. Rosenfeld, and D. Paul (1995), “CSR-III
Text,” Linguistic Data Consortium, #LDC95T6.
P. Johnson–Laird and K. Oatley (1989), “The Language
of Emotions: An Analysis of a Semantic Field,” Cog-
nition and Emotion, 3:81–123.
J. Liscombe, G. Riccardi, and D. Hakkani-T¨ur (2005),
“Using Context to Improve Emotion Detection in Spo-
ken Dialog Systems,” Proc. Interspeech, Lisbon, Por-
tugal, 1845–1848.
H. Liu, H. Lieberman, and T. Selker (2003), “A Model
of Textual Affect Sensing Using Real-World Knowl-
edge,” in Proc. Intelligent User Interfaces (IUI), Mi-
ami, FL, 125–132.
A. Mehrabian (1995), “Framework for a Comprehensive
Description and Measurement of Emotional States,”
Genetic, Social, and General Psychology Mono-
graphs, 121(3):339–361.
R. Mihalcea and C. Strapparava (2006), “Learning to
Laugh (Automatically): Computational Models for
Humor Recognition,” J. Computational Intelligence,
22(2):126–142.
M. Mohler and R. Mihalcea (2009), “Text-to-text Seman-
tic Similarity for Automatic Short Answer Grading,”
in Proc. 12th Conf. European Chap. ACL, Athens,
Greece, 567–575.
B. Pang and L. Lee (2008), “Opinion Mining and Sen-
timent Analysis,” in Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135.
R.W. Picard (1997), Affective Computing, Cambridge,
MA: MIT Press.
M. Pivec and P. Kearney (2007), “Games for Learning
and Learning from Games,” Informatica, 31:419–423.
J.A. Russell (1980), “A Circumplex Model of Affect,” J.
Personality and Social Psychology, 39:1161–1178.
S. Ryan, B. Scott, H. Freeman, and D. Patel (2000), The
Virtual University: The Internet and Resource-based
Learning, London, UK: Kogan Page.
M. Schr¨oder (2006), “Expressing Degree of Activation
in Synthetic Speech,” IEEE Trans. Audio, Speech, and
Language Processing, 14(4):1128–1136.
P.J. Stone (1997), “Thematic Text Analysis: New agen-
das for Analyzing Text Content,” in Text Analysis for
the Social Sciences: Methods for Drawing Statistical
Inferences from Texts and Transcripts, C.W. Roberts,
Ed., Mahwah, NJ: Lawrence Erlbaum Assoc. Publish-
ers, 35–54.
C. Strapparava and R. Mihalcea (2007), “SemEval-2007
Task 14: Affective Text,” in Proc. 4th Int. Workshop on
Semantic Evaluations (SemEval 2007), Prague, Czech
Republic.
C. Strapparava and R. Mihalcea (2008), “Learning to
Identify Emotions in Text,” in Proc. 2008 ACM Sym-
posium on Applied Computing, New York, NY, 1556–
1560.
C. Strapparava, A. Valitutti, and O. Stock (2006), “The
Affective Weight of Lexicon,” in Proc. 5th Int. Conf.
Language Resources and Evaluation (LREC), Lisbon,
Portugal.
P. Subasic and A. Huettner (2001), “Affect Analysis
of Text Using Fuzzy Semantic Typing,” IEEE Trans.
Fuzzy Systems, 9(4):483–496.
C.M. Whissell (1989), “The Dictionary of Affect in Lan-
guage,” in Emotion: Theory, Research, and Experi-
ence, R. Plutchik and H. Kellerman, Eds., New York,
NY: Academic Press, 13–131.
</reference>
<page confidence="0.997112">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.622468">
<title confidence="0.999729">Emotion Analysis Using Latent Affective Folding and Embedding</title>
<author confidence="0.999904">Jerome R Bellegarda</author>
<affiliation confidence="0.8345215">Speech &amp; Language Apple</affiliation>
<address confidence="0.998224">Cupertino, California 95014,</address>
<email confidence="0.963754">jerome@apple.com</email>
<abstract confidence="0.99719805">Though data-driven in nature, emotion analysis based on latent semantic analysis still relies on some measure of expert knowledge in order to isolate the emotional keywords or keysets necessary to the construction of affective categories. This makes it vulnerable to any discrepancy between the ensuing taxonomy of affective states and the underlying domain of discourse. This paper proposes a more general strategy which leverages two distincts semantic levels, one that encapsulates the foundations of the domain considered, and one that specifically accounts for the overall affective fabric of the language. Exposing the emergent relationship between these two levels advantageously informs the emotion classification process. Empirical evidence suggests that this is a promising solution for automatic emotion detection in text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abbasi</author>
</authors>
<title>Affect Intensity Analysis of Dark Web Forums,” in</title>
<date>2007</date>
<booktitle>Proc. IEEE Int. Conf. Intelligence and Security Informatics (ISI),</booktitle>
<pages>282--288</pages>
<location>New Brunswick, NJ,</location>
<contexts>
<context position="1505" citStr="Abbasi, 2007" startWordPosition="218" endWordPosition="219"> classification process. Empirical evidence suggests that this is a promising solution for automatic emotion detection in text. 1 Introduction The automatic detection of emotions in text is a necessary pre-processing step in many different fields touching on affective computing (Picard, 1997), such as natural language interfaces (Cosatto et al., 2003), e-learning environments (Ryan et al., 2000), educational or entertainment games (Pivec and Kearney, 2007), opinion mining and sentiment analysis (Pang and Lee, 2008), humor recognition (Mihalcea and Strapparava, 2006), and security informatics (Abbasi, 2007). In the latter case, for example, it can be used for monitoring levels of hateful or violent rhetoric (perhaps in multilingual settings). More generally, emotion detection is of great 1 interest in human-computer interaction: if a system determines that a user is upset or annoyed, for instance, it could switch to a different mode of interaction (Liscombe et al., 2005). And of course, it plays a critical role in the generation of expressive synthetic speech (Schr¨oder, 2006). Emphasis has traditionally been placed on the set of six “universal” emotions (Ekman, 1993): ANGER, DISGUST, FEAR, JOY,</context>
</contexts>
<marker>Abbasi, 2007</marker>
<rawString>A. Abbasi (2007), “Affect Intensity Analysis of Dark Web Forums,” in Proc. IEEE Int. Conf. Intelligence and Security Informatics (ISI), New Brunswick, NJ, 282–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Ovesdotter Alm</author>
<author>D Roth</author>
<author>R Sproat</author>
</authors>
<title>Emotions from Text: Machine Learning for Text– Based Emotion Prediction,” in</title>
<date>2005</date>
<booktitle>Proc. Conf. Human Language Technology and Empirical Methods in NLP, Vancouver, BC,</booktitle>
<pages>579--586</pages>
<contexts>
<context position="2145" citStr="Alm et al., 2005" startWordPosition="323" endWordPosition="326">r example, it can be used for monitoring levels of hateful or violent rhetoric (perhaps in multilingual settings). More generally, emotion detection is of great 1 interest in human-computer interaction: if a system determines that a user is upset or annoyed, for instance, it could switch to a different mode of interaction (Liscombe et al., 2005). And of course, it plays a critical role in the generation of expressive synthetic speech (Schr¨oder, 2006). Emphasis has traditionally been placed on the set of six “universal” emotions (Ekman, 1993): ANGER, DISGUST, FEAR, JOY, SADNESS, and SURPRISE (Alm et al., 2005; Liu et al., 2003; Subasic and Huettner, 2001). Emotion analysis is typically carried out using a simplified description of emotional states in a low-dimensional space, which normally comprises dimensions such as valence (positive/negative evalution), activation (stimulation of activity), and/or control (dominant/submissive power) (Mehrabian, 1995; Russell, 1980; Strapparava and Mihalcea, 2008). Classification proceeds based on an underlying emotional knowledge base, which strives to provide adequate distinctions between different emotions. This affective information can either be built entir</context>
</contexts>
<marker>Alm, Roth, Sproat, 2005</marker>
<rawString>C. Ovesdotter Alm, D. Roth, and R. Sproat (2005), “Emotions from Text: Machine Learning for Text– Based Emotion Prediction,” in Proc. Conf. Human Language Technology and Empirical Methods in NLP, Vancouver, BC, 579–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Bellegarda</author>
</authors>
<title>Latent Semantic Mapping: A Data–Driven Framework for Modeling Global Relationships Implicit in Large Volumes of Data,”</title>
<date>2005</date>
<journal>IEEE Signal Processing Magazine,</journal>
<volume>22</volume>
<issue>5</issue>
<contexts>
<context position="4630" citStr="Bellegarda, 2005" startWordPosition="695" endWordPosition="696">o be computed between generic terms and affective categories (Strapparava et al., 2006). This way, every word can automatically be assigned some fractional affective influence. Still, the affective categories themselves are usually specified with the help of a reference lexical database like WordNet (Fellbaum, 1998). The purpose of this paper is to more broadly leverage the principle of latent semantics in emotion analysis. We cast the problem as a general application of latent semantic mapping (LSM), an extrapolation of LSA for modeling global relationships implicit in large volumes of data (Bellegarda, 2005; Bellegarda, 2008). More specifically, we use the LSM framework to describe two distinct semantic levels: one that encapsulates the foundations of the domain considered (e.g., broadcast news, email messages, SMS conversations, etc.), and one that specifically accounts for the overall affective fabric of the language. Then, we leverage these two descriptions to appropriately relate domain and affective levels, and thereby inform the emotion classification process. This de facto bypasses the need for any explicit external knowledge. The paper is organized as follows. The next section provides s</context>
<context position="10655" citStr="Bellegarda, 2005" startWordPosition="1636" endWordPosition="1637">al Inquirer content analysis system (Stone, 1997) lists only about 2000 words with positive outlook and 2000 words with negative outlook. There are exactly 1281 words inventoried in the affective extension of WordNet (Strapparava and Mihalcea, 2008), and the affective word list from (Johnson–Laird and Oatley, 1989) comprises less than 1000 words. This Input Text Figure 2: Proposed Latent Affective Framework. considerably complicates the construction of reliable affective categories in the latent space. To address the two limitations above, we propose to more broadly leverage the LSM paradigm (Bellegarda, 2005; Bellegarda, 2008), following the overall framework depicted in Fig. 2. Compared to Fig. 1, we inject some supervision at two separate levels: not only regarding the particular domain considered, but also how the affective categories themselves are defined. The first task is to exploit a suitable training collection to encapsulate into a (domain) latent semantic space the general foundations of the domain at hand. Next, we leverage a separate affective corpus, such as mood-annotated blog entries from LiveJournal.com (Strapparava and Mihalcea, 2008), to serve as a descriptive blueprint for the</context>
</contexts>
<marker>Bellegarda, 2005</marker>
<rawString>J.R. Bellegarda (2005), “Latent Semantic Mapping: A Data–Driven Framework for Modeling Global Relationships Implicit in Large Volumes of Data,” IEEE Signal Processing Magazine, 22(5):70–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Bellegarda</author>
</authors>
<date>2008</date>
<booktitle>Latent Semantic Mapping: Principles &amp; Applications, Synthesis Lectures on Speech and Audio Processing Series,</booktitle>
<publisher>Morgan &amp; Claypool.</publisher>
<location>Fort Collins, CO:</location>
<contexts>
<context position="4649" citStr="Bellegarda, 2008" startWordPosition="697" endWordPosition="699">een generic terms and affective categories (Strapparava et al., 2006). This way, every word can automatically be assigned some fractional affective influence. Still, the affective categories themselves are usually specified with the help of a reference lexical database like WordNet (Fellbaum, 1998). The purpose of this paper is to more broadly leverage the principle of latent semantics in emotion analysis. We cast the problem as a general application of latent semantic mapping (LSM), an extrapolation of LSA for modeling global relationships implicit in large volumes of data (Bellegarda, 2005; Bellegarda, 2008). More specifically, we use the LSM framework to describe two distinct semantic levels: one that encapsulates the foundations of the domain considered (e.g., broadcast news, email messages, SMS conversations, etc.), and one that specifically accounts for the overall affective fabric of the language. Then, we leverage these two descriptions to appropriately relate domain and affective levels, and thereby inform the emotion classification process. This de facto bypasses the need for any explicit external knowledge. The paper is organized as follows. The next section provides some motivation for,</context>
<context position="8751" citStr="Bellegarda, 2008" startWordPosition="1329" endWordPosition="1330">om two limitations which may well prove deleterious in practical situations. First, the inherent lack of supervision routinely leads to a latent semantic space which is not particularly representative of the underlying domain of discourse. And second, the construction of the affective categories still relies heavily on pre-defined lexical affinity, potentially resulting in an unwarranted bias in the taxonomy of affective states. The first limitation impinges on the effectiveness of any LSA-based approach, which is known to vary substantially based on the size and quality of the training data (Bellegarda, 2008; Mohler and Mihalcea, 2009). In the present case, any discrepancy between latent semantic space and domain of discourse may distort the position of certain words in the space, which could in turn lead to subsequent sub-optimal affective weight assignment. For instance, in the examples above, the word smell is considerably more critical to the resolution of awful as a marker of DISGUST than the word car. But that fact may never be uncovered if the only pertinent documents in the training corpus happen to be about expensive fragrances and automobiles. Thus, it is highly desirable to derive the </context>
<context position="10674" citStr="Bellegarda, 2008" startWordPosition="1638" endWordPosition="1639">t analysis system (Stone, 1997) lists only about 2000 words with positive outlook and 2000 words with negative outlook. There are exactly 1281 words inventoried in the affective extension of WordNet (Strapparava and Mihalcea, 2008), and the affective word list from (Johnson–Laird and Oatley, 1989) comprises less than 1000 words. This Input Text Figure 2: Proposed Latent Affective Framework. considerably complicates the construction of reliable affective categories in the latent space. To address the two limitations above, we propose to more broadly leverage the LSM paradigm (Bellegarda, 2005; Bellegarda, 2008), following the overall framework depicted in Fig. 2. Compared to Fig. 1, we inject some supervision at two separate levels: not only regarding the particular domain considered, but also how the affective categories themselves are defined. The first task is to exploit a suitable training collection to encapsulate into a (domain) latent semantic space the general foundations of the domain at hand. Next, we leverage a separate affective corpus, such as mood-annotated blog entries from LiveJournal.com (Strapparava and Mihalcea, 2008), to serve as a descriptive blueprint for the construction of af</context>
<context position="13786" citStr="Bellegarda, 2008" startWordPosition="2143" endWordPosition="2144">affective folding proceeds as illustrated in Fig. 3. Let T, |7 |= N1, be a collection of training texts (be they sentences, paragraphs, or documents) reflecting the domain of interest, and V1, |V1 |= M1, the associated set of all words (possibly augmented with some strategic word pairs, triplets, etc., as appropriate) observed in this collection. Generally, M1 is on the order of several tens of thousands, while N1 may be as high as a million. We first construct a (M1 x N1) matrix W1, whose elements wij suitably reflect the extent to which each word wi E V1 appeared in each text tj E Ti. From (Bellegarda, 2008), a reasonable expression for wij is: wi,j = (1 − Ei) ci,j nj where ci,j is the number of times wi occurs in text tj, nj is the total number of words present in this text, and Ei is the normalized entropy of wi in V1. The global weighting implied by 1 − Ei reflects the fact that two words appearing with the same count in a particular text do not necessarily convey the same amount of information; this is subordinated to the distribution of words in the entire set V1. We then perform a singular value decomposition (SVD) of W1 as (Bellegarda, 2008): W1 = U1 S1 V T 1 , (2) where U1 is the (M1 x R1</context>
<context position="24799" citStr="Bellegarda, 2008" startWordPosition="4149" endWordPosition="4150">t the weighted counts (1) with j = p. The resulting feature vector, a column vector of dimension N1, can be thought of as an additional column of the matrix W1. Assuming the matrices U1 and 51 do not change appreciably, the SVD expansion (2) therefore implies: tp = U1 51 vT1,p , (16) where the R1-dimensional vector vT1,p acts as an additional column of the matrix V T 1 . Thus, the represention of the new text in the domain space can be obtained from z1,p = v1,p51. All is needed now is a suitable closeness measure to compare this representation to each affective anchor ˆz1,e (1 G Q G L). From (Bellegarda, 2008), a natural metric to consider is the cosine of the angle between them. This yields: C(z1,p, ˆz1,t) = for any 1 G Q G L. Using (17), it is a simple matter to directly compute the relevance of the input text to each emotional category. It is important to note that word weighting is now implicitly taken into account by the LSM formalism. 6 Experimental Evaluation In order to evaluate the latent affective framework described above, we used the data set that was developed for the SemEval 2007 task on “Affective Text” (Strapparava and Mihalcea, 2007). This task was focused on the emotion classifica</context>
</contexts>
<marker>Bellegarda, 2008</marker>
<rawString>J.R. Bellegarda (2008), Latent Semantic Mapping: Principles &amp; Applications, Synthesis Lectures on Speech and Audio Processing Series, Fort Collins, CO: Morgan &amp; Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Bellegarda</author>
<author>P V de Souza</author>
<author>A Nadas</author>
<author>D Nahamoo</author>
<author>M A Picheny</author>
<author>L R Bahl</author>
</authors>
<title>The Metamorphic Algorithm: A Speaker Mapping Approach to Data Augmentation,”</title>
<date>1994</date>
<journal>IEEE Trans. Speech and Audio Processing,</journal>
<volume>2</volume>
<issue>3</issue>
<marker>Bellegarda, de Souza, Nadas, Nahamoo, Picheny, Bahl, 1994</marker>
<rawString>J.R. Bellegarda, P.V. de Souza, A. Nadas, D. Nahamoo, M.A. Picheny and L.R. Bahl (1994), “The Metamorphic Algorithm: A Speaker Mapping Approach to Data Augmentation,” IEEE Trans. Speech and Audio Processing, 2(3):413–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Cosatto</author>
<author>J Ostermann</author>
<author>H P Graf</author>
<author>J Schroeter</author>
</authors>
<title>Lifelike talking faces for interactive services,” in</title>
<date>2003</date>
<booktitle>Proc. IEEE,</booktitle>
<volume>91</volume>
<issue>9</issue>
<pages>1406--1429</pages>
<contexts>
<context position="1245" citStr="Cosatto et al., 2003" startWordPosition="181" endWordPosition="184">stincts semantic levels, one that encapsulates the foundations of the domain considered, and one that specifically accounts for the overall affective fabric of the language. Exposing the emergent relationship between these two levels advantageously informs the emotion classification process. Empirical evidence suggests that this is a promising solution for automatic emotion detection in text. 1 Introduction The automatic detection of emotions in text is a necessary pre-processing step in many different fields touching on affective computing (Picard, 1997), such as natural language interfaces (Cosatto et al., 2003), e-learning environments (Ryan et al., 2000), educational or entertainment games (Pivec and Kearney, 2007), opinion mining and sentiment analysis (Pang and Lee, 2008), humor recognition (Mihalcea and Strapparava, 2006), and security informatics (Abbasi, 2007). In the latter case, for example, it can be used for monitoring levels of hateful or violent rhetoric (perhaps in multilingual settings). More generally, emotion detection is of great 1 interest in human-computer interaction: if a system determines that a user is upset or annoyed, for instance, it could switch to a different mode of inte</context>
</contexts>
<marker>Cosatto, Ostermann, Graf, Schroeter, 2003</marker>
<rawString>E. Cosatto, J. Ostermann, H.P. Graf, and J. Schroeter (2003), “Lifelike talking faces for interactive services,” in Proc. IEEE, 91(9), 1406–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<date>1990</date>
<journal>Indexing by Latent Semantic Analysis,” J. Amer. Soc. Information Science,</journal>
<pages>41--391</pages>
<contexts>
<context position="3927" citStr="Deerwester et al., 1990" startWordPosition="583" endWordPosition="586"> Because this taxonomy only supports simplified relationships between affective words and emoProceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics tional categories, it often fails to meaningfully generalize beyond the relatively few core terms explicitly considered in its construction. This has sparked interest in data-driven approaches based on latent semantic analysis (LSA), a paradigm originally developed for information retrieval (Deerwester et al., 1990). Upon suitable training using a large corpus of texts, LSA allows a similarity score to be computed between generic terms and affective categories (Strapparava et al., 2006). This way, every word can automatically be assigned some fractional affective influence. Still, the affective categories themselves are usually specified with the help of a reference lexical database like WordNet (Fellbaum, 1998). The purpose of this paper is to more broadly leverage the principle of latent semantics in emotion analysis. We cast the problem as a general application of latent semantic mapping (LSM), an ext</context>
<context position="7703" citStr="Deerwester et al., 1990" startWordPosition="1170" endWordPosition="1173">, 2006). By looking at document-level cooccurrences, contextual information is exploited to encapsulate semantic information into a relatively low dimensional vector space. Suitable affective categories are then constructed in that space by “folding in” either the specific word denoting the emotion, or its associated synset (say, from WordNet), or even the entire set of words in all synsets that can be labelled with that emotion (Strapparava and Mihalcea, 2008). This is typically done by placing the relevant word(s) into a “pseudo-document,” and map it into the space as if it were a real one (Deerwester et al., 1990). Finally, the global emotional affinity of a given input text is determined by computing similarities between all pseudo-documents. The resulting framework is depicted in Fig. 1. All Synsets LSA Processing Homogeneous Representation Large Corpus Detected Emotion 2 This solution is attractive, if for no other reason than it allows every word to automatically be assigned some fractional affective influence. However, it suffers from two limitations which may well prove deleterious in practical situations. First, the inherent lack of supervision routinely leads to a latent semantic space which is</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman (1990), “Indexing by Latent Semantic Analysis,” J. Amer. Soc. Information Science, 41:391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
</authors>
<date>1993</date>
<journal>Facial Expression and Emotion”, American Psychologist,</journal>
<volume>48</volume>
<issue>4</issue>
<pages>384--392</pages>
<contexts>
<context position="2077" citStr="Ekman, 1993" startWordPosition="314" endWordPosition="315"> and security informatics (Abbasi, 2007). In the latter case, for example, it can be used for monitoring levels of hateful or violent rhetoric (perhaps in multilingual settings). More generally, emotion detection is of great 1 interest in human-computer interaction: if a system determines that a user is upset or annoyed, for instance, it could switch to a different mode of interaction (Liscombe et al., 2005). And of course, it plays a critical role in the generation of expressive synthetic speech (Schr¨oder, 2006). Emphasis has traditionally been placed on the set of six “universal” emotions (Ekman, 1993): ANGER, DISGUST, FEAR, JOY, SADNESS, and SURPRISE (Alm et al., 2005; Liu et al., 2003; Subasic and Huettner, 2001). Emotion analysis is typically carried out using a simplified description of emotional states in a low-dimensional space, which normally comprises dimensions such as valence (positive/negative evalution), activation (stimulation of activity), and/or control (dominant/submissive power) (Mehrabian, 1995; Russell, 1980; Strapparava and Mihalcea, 2008). Classification proceeds based on an underlying emotional knowledge base, which strives to provide adequate distinctions between diff</context>
</contexts>
<marker>Ekman, 1993</marker>
<rawString>P. Ekman (1993), “Facial Expression and Emotion”, American Psychologist, 48(4), 384–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
<author>Ed</author>
</authors>
<title>WordNet: An Electronic Lexical Database,</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Fellbaum, Ed, 1998</marker>
<rawString>C. Fellbaum, Ed., (1998), WordNet: An Electronic Lexical Database, Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
<author>R Rosenfeld</author>
<author>D Paul</author>
</authors>
<date>1995</date>
<journal>CSR-III Text,” Linguistic Data Consortium,</journal>
<pages>95--6</pages>
<contexts>
<context position="26744" citStr="Graff et al., 1995" startWordPosition="4473" endWordPosition="4476">proaches: (i) a simple word accumulation system, which annotates the emotions in a text based on the presence of words from the WordNet-Affect lexicon; and (ii) three LSA-based systems implemented as in Fig. 1, which only differ in the way each emotion is represented in the LSA space: either based on a specific word only (e.g., JOY), or the word plus its WordNet synset, or the word plus all WordNet synsets labelled with that emotion in WordNetAffect (cf. (Strapparava and Mihalcea, 2007)). In all three cases, the large corpus used for LSA processing was the Wall Street Journal text collection (Graff et al., 1995), comprising about 86,000 articles. For the latent affective framework, we needed to select two separate training corpora. For the “domain” corpus, we selected a collection of about N1 = 8, 500 relatively short English sentences (with a vocabulary of roughly M1 = 12, 000 words) originally compiled for the purpose of a building a concatenative text-to-speech voice. Though not 2Development data was merged into the original SemEval 2007 test set to produce a larger test set. x x x x x x x x x x λ1,k x x x x Domain Space 1/2 ^ Σ1 z 1,l + μ1 + + + Σ−1/2 2 + + + z2,l λ2,k (λ 2,k− μ2) + + + + Affecti</context>
</contexts>
<marker>Graff, Rosenfeld, Paul, 1995</marker>
<rawString>D. Graff, R. Rosenfeld, and D. Paul (1995), “CSR-III Text,” Linguistic Data Consortium, #LDC95T6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Johnson–Laird</author>
<author>K Oatley</author>
</authors>
<title>The Language of Emotions: An Analysis of a Semantic Field,” Cognition and Emotion,</title>
<date>1989</date>
<pages>3--81</pages>
<marker>Johnson–Laird, Oatley, 1989</marker>
<rawString>P. Johnson–Laird and K. Oatley (1989), “The Language of Emotions: An Analysis of a Semantic Field,” Cognition and Emotion, 3:81–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Liscombe</author>
<author>G Riccardi</author>
<author>D Hakkani-T¨ur</author>
</authors>
<title>Using Context to Improve Emotion Detection in Spoken Dialog Systems,”</title>
<date>2005</date>
<booktitle>Proc. Interspeech,</booktitle>
<location>Lisbon, Portugal,</location>
<marker>Liscombe, Riccardi, Hakkani-T¨ur, 2005</marker>
<rawString>J. Liscombe, G. Riccardi, and D. Hakkani-T¨ur (2005), “Using Context to Improve Emotion Detection in Spoken Dialog Systems,” Proc. Interspeech, Lisbon, Portugal, 1845–1848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Liu</author>
<author>H Lieberman</author>
<author>T Selker</author>
</authors>
<title>A Model of Textual Affect Sensing Using Real-World Knowledge,” in</title>
<date>2003</date>
<booktitle>Proc. Intelligent User Interfaces (IUI),</booktitle>
<pages>125--132</pages>
<location>Miami, FL,</location>
<contexts>
<context position="2163" citStr="Liu et al., 2003" startWordPosition="327" endWordPosition="330">be used for monitoring levels of hateful or violent rhetoric (perhaps in multilingual settings). More generally, emotion detection is of great 1 interest in human-computer interaction: if a system determines that a user is upset or annoyed, for instance, it could switch to a different mode of interaction (Liscombe et al., 2005). And of course, it plays a critical role in the generation of expressive synthetic speech (Schr¨oder, 2006). Emphasis has traditionally been placed on the set of six “universal” emotions (Ekman, 1993): ANGER, DISGUST, FEAR, JOY, SADNESS, and SURPRISE (Alm et al., 2005; Liu et al., 2003; Subasic and Huettner, 2001). Emotion analysis is typically carried out using a simplified description of emotional states in a low-dimensional space, which normally comprises dimensions such as valence (positive/negative evalution), activation (stimulation of activity), and/or control (dominant/submissive power) (Mehrabian, 1995; Russell, 1980; Strapparava and Mihalcea, 2008). Classification proceeds based on an underlying emotional knowledge base, which strives to provide adequate distinctions between different emotions. This affective information can either be built entirely upon manually </context>
</contexts>
<marker>Liu, Lieberman, Selker, 2003</marker>
<rawString>H. Liu, H. Lieberman, and T. Selker (2003), “A Model of Textual Affect Sensing Using Real-World Knowledge,” in Proc. Intelligent User Interfaces (IUI), Miami, FL, 125–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mehrabian</author>
</authors>
<title>Framework for a Comprehensive Description and Measurement of</title>
<date>1995</date>
<journal>Emotional States,” Genetic, Social, and General Psychology Monographs,</journal>
<volume>121</volume>
<issue>3</issue>
<contexts>
<context position="2495" citStr="Mehrabian, 1995" startWordPosition="371" endWordPosition="372">d of course, it plays a critical role in the generation of expressive synthetic speech (Schr¨oder, 2006). Emphasis has traditionally been placed on the set of six “universal” emotions (Ekman, 1993): ANGER, DISGUST, FEAR, JOY, SADNESS, and SURPRISE (Alm et al., 2005; Liu et al., 2003; Subasic and Huettner, 2001). Emotion analysis is typically carried out using a simplified description of emotional states in a low-dimensional space, which normally comprises dimensions such as valence (positive/negative evalution), activation (stimulation of activity), and/or control (dominant/submissive power) (Mehrabian, 1995; Russell, 1980; Strapparava and Mihalcea, 2008). Classification proceeds based on an underlying emotional knowledge base, which strives to provide adequate distinctions between different emotions. This affective information can either be built entirely upon manually selected vocabulary as in (Whissell, 1989), or derived automatically from data based on expert knowledge of the most relevant features that can be extracted from the input text (Alm et al., 2005). In both cases, the resulting system tends to rely, for the most part, on a few thousand annotated “emotional keywords,” the presence of</context>
</contexts>
<marker>Mehrabian, 1995</marker>
<rawString>A. Mehrabian (1995), “Framework for a Comprehensive Description and Measurement of Emotional States,” Genetic, Social, and General Psychology Monographs, 121(3):339–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Strapparava</author>
</authors>
<title>Learning to Laugh (Automatically): Computational Models for Humor Recognition,”</title>
<date>2006</date>
<journal>J. Computational Intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="1464" citStr="Mihalcea and Strapparava, 2006" startWordPosition="210" endWordPosition="213">between these two levels advantageously informs the emotion classification process. Empirical evidence suggests that this is a promising solution for automatic emotion detection in text. 1 Introduction The automatic detection of emotions in text is a necessary pre-processing step in many different fields touching on affective computing (Picard, 1997), such as natural language interfaces (Cosatto et al., 2003), e-learning environments (Ryan et al., 2000), educational or entertainment games (Pivec and Kearney, 2007), opinion mining and sentiment analysis (Pang and Lee, 2008), humor recognition (Mihalcea and Strapparava, 2006), and security informatics (Abbasi, 2007). In the latter case, for example, it can be used for monitoring levels of hateful or violent rhetoric (perhaps in multilingual settings). More generally, emotion detection is of great 1 interest in human-computer interaction: if a system determines that a user is upset or annoyed, for instance, it could switch to a different mode of interaction (Liscombe et al., 2005). And of course, it plays a critical role in the generation of expressive synthetic speech (Schr¨oder, 2006). Emphasis has traditionally been placed on the set of six “universal” emotions </context>
</contexts>
<marker>Mihalcea, Strapparava, 2006</marker>
<rawString>R. Mihalcea and C. Strapparava (2006), “Learning to Laugh (Automatically): Computational Models for Humor Recognition,” J. Computational Intelligence, 22(2):126–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohler</author>
<author>R Mihalcea</author>
</authors>
<title>Text-to-text Semantic Similarity for Automatic Short Answer Grading,” in</title>
<date>2009</date>
<booktitle>Proc. 12th Conf. European Chap. ACL,</booktitle>
<pages>567--575</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="8779" citStr="Mohler and Mihalcea, 2009" startWordPosition="1331" endWordPosition="1335"> which may well prove deleterious in practical situations. First, the inherent lack of supervision routinely leads to a latent semantic space which is not particularly representative of the underlying domain of discourse. And second, the construction of the affective categories still relies heavily on pre-defined lexical affinity, potentially resulting in an unwarranted bias in the taxonomy of affective states. The first limitation impinges on the effectiveness of any LSA-based approach, which is known to vary substantially based on the size and quality of the training data (Bellegarda, 2008; Mohler and Mihalcea, 2009). In the present case, any discrepancy between latent semantic space and domain of discourse may distort the position of certain words in the space, which could in turn lead to subsequent sub-optimal affective weight assignment. For instance, in the examples above, the word smell is considerably more critical to the resolution of awful as a marker of DISGUST than the word car. But that fact may never be uncovered if the only pertinent documents in the training corpus happen to be about expensive fragrances and automobiles. Thus, it is highly desirable to derive the latent semantic space using </context>
</contexts>
<marker>Mohler, Mihalcea, 2009</marker>
<rawString>M. Mohler and R. Mihalcea (2009), “Text-to-text Semantic Similarity for Automatic Short Answer Grading,” in Proc. 12th Conf. European Chap. ACL, Athens, Greece, 567–575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<date>2008</date>
<booktitle>Opinion Mining and Sentiment Analysis,” in Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="1412" citStr="Pang and Lee, 2008" startWordPosition="204" endWordPosition="207">age. Exposing the emergent relationship between these two levels advantageously informs the emotion classification process. Empirical evidence suggests that this is a promising solution for automatic emotion detection in text. 1 Introduction The automatic detection of emotions in text is a necessary pre-processing step in many different fields touching on affective computing (Picard, 1997), such as natural language interfaces (Cosatto et al., 2003), e-learning environments (Ryan et al., 2000), educational or entertainment games (Pivec and Kearney, 2007), opinion mining and sentiment analysis (Pang and Lee, 2008), humor recognition (Mihalcea and Strapparava, 2006), and security informatics (Abbasi, 2007). In the latter case, for example, it can be used for monitoring levels of hateful or violent rhetoric (perhaps in multilingual settings). More generally, emotion detection is of great 1 interest in human-computer interaction: if a system determines that a user is upset or annoyed, for instance, it could switch to a different mode of interaction (Liscombe et al., 2005). And of course, it plays a critical role in the generation of expressive synthetic speech (Schr¨oder, 2006). Emphasis has traditionally</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>B. Pang and L. Lee (2008), “Opinion Mining and Sentiment Analysis,” in Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Picard</author>
</authors>
<title>Affective Computing,</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="1185" citStr="Picard, 1997" startWordPosition="174" endWordPosition="175">poses a more general strategy which leverages two distincts semantic levels, one that encapsulates the foundations of the domain considered, and one that specifically accounts for the overall affective fabric of the language. Exposing the emergent relationship between these two levels advantageously informs the emotion classification process. Empirical evidence suggests that this is a promising solution for automatic emotion detection in text. 1 Introduction The automatic detection of emotions in text is a necessary pre-processing step in many different fields touching on affective computing (Picard, 1997), such as natural language interfaces (Cosatto et al., 2003), e-learning environments (Ryan et al., 2000), educational or entertainment games (Pivec and Kearney, 2007), opinion mining and sentiment analysis (Pang and Lee, 2008), humor recognition (Mihalcea and Strapparava, 2006), and security informatics (Abbasi, 2007). In the latter case, for example, it can be used for monitoring levels of hateful or violent rhetoric (perhaps in multilingual settings). More generally, emotion detection is of great 1 interest in human-computer interaction: if a system determines that a user is upset or annoye</context>
</contexts>
<marker>Picard, 1997</marker>
<rawString>R.W. Picard (1997), Affective Computing, Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pivec</author>
<author>P Kearney</author>
</authors>
<date>2007</date>
<journal>A Circumplex Model of Affect,” J. Personality and Social Psychology,</journal>
<booktitle>Games for Learning and Learning from Games,” Informatica, 31:419–423. J.A. Russell</booktitle>
<pages>39--1161</pages>
<contexts>
<context position="1352" citStr="Pivec and Kearney, 2007" startWordPosition="195" endWordPosition="198">ecifically accounts for the overall affective fabric of the language. Exposing the emergent relationship between these two levels advantageously informs the emotion classification process. Empirical evidence suggests that this is a promising solution for automatic emotion detection in text. 1 Introduction The automatic detection of emotions in text is a necessary pre-processing step in many different fields touching on affective computing (Picard, 1997), such as natural language interfaces (Cosatto et al., 2003), e-learning environments (Ryan et al., 2000), educational or entertainment games (Pivec and Kearney, 2007), opinion mining and sentiment analysis (Pang and Lee, 2008), humor recognition (Mihalcea and Strapparava, 2006), and security informatics (Abbasi, 2007). In the latter case, for example, it can be used for monitoring levels of hateful or violent rhetoric (perhaps in multilingual settings). More generally, emotion detection is of great 1 interest in human-computer interaction: if a system determines that a user is upset or annoyed, for instance, it could switch to a different mode of interaction (Liscombe et al., 2005). And of course, it plays a critical role in the generation of expressive sy</context>
</contexts>
<marker>Pivec, Kearney, 2007</marker>
<rawString>M. Pivec and P. Kearney (2007), “Games for Learning and Learning from Games,” Informatica, 31:419–423. J.A. Russell (1980), “A Circumplex Model of Affect,” J. Personality and Social Psychology, 39:1161–1178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ryan</author>
<author>B Scott</author>
<author>H Freeman</author>
<author>D Patel</author>
</authors>
<title>The Virtual University: The Internet and Resource-based Learning,</title>
<date>2000</date>
<location>London, UK: Kogan Page.</location>
<contexts>
<context position="1290" citStr="Ryan et al., 2000" startWordPosition="187" endWordPosition="190">he foundations of the domain considered, and one that specifically accounts for the overall affective fabric of the language. Exposing the emergent relationship between these two levels advantageously informs the emotion classification process. Empirical evidence suggests that this is a promising solution for automatic emotion detection in text. 1 Introduction The automatic detection of emotions in text is a necessary pre-processing step in many different fields touching on affective computing (Picard, 1997), such as natural language interfaces (Cosatto et al., 2003), e-learning environments (Ryan et al., 2000), educational or entertainment games (Pivec and Kearney, 2007), opinion mining and sentiment analysis (Pang and Lee, 2008), humor recognition (Mihalcea and Strapparava, 2006), and security informatics (Abbasi, 2007). In the latter case, for example, it can be used for monitoring levels of hateful or violent rhetoric (perhaps in multilingual settings). More generally, emotion detection is of great 1 interest in human-computer interaction: if a system determines that a user is upset or annoyed, for instance, it could switch to a different mode of interaction (Liscombe et al., 2005). And of cours</context>
</contexts>
<marker>Ryan, Scott, Freeman, Patel, 2000</marker>
<rawString>S. Ryan, B. Scott, H. Freeman, and D. Patel (2000), The Virtual University: The Internet and Resource-based Learning, London, UK: Kogan Page.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schr¨oder</author>
</authors>
<title>Expressing Degree of Activation in Synthetic Speech,”</title>
<date>2006</date>
<journal>IEEE Trans. Audio, Speech, and Language Processing,</journal>
<volume>14</volume>
<issue>4</issue>
<marker>Schr¨oder, 2006</marker>
<rawString>M. Schr¨oder (2006), “Expressing Degree of Activation in Synthetic Speech,” IEEE Trans. Audio, Speech, and Language Processing, 14(4):1128–1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Stone</author>
</authors>
<title>Thematic Text Analysis: New agendas for Analyzing Text Content,” in Text Analysis for the Social Sciences: Methods for Drawing Statistical Inferences from Texts and Transcripts,</title>
<date>1997</date>
<pages>35--54</pages>
<publisher>Publishers,</publisher>
<location>C.W. Roberts, Ed., Mahwah, NJ:</location>
<contexts>
<context position="10088" citStr="Stone, 1997" startWordPosition="1549" endWordPosition="1550">econd limitation is tied to the difficulty of coming up with an a priori affective description that will work universally. Stipulating the affective categories using only the specific word denoting the emotion is likely to be less robust than using the set of words in all synsets labelled with that emotion. On the other hand, the latter may well expose some inherent ambiguities resulting from affective polysemy. This is compounded by the relatively small number of words for which an affective distribution is even available. For example, the well-known General Inquirer content analysis system (Stone, 1997) lists only about 2000 words with positive outlook and 2000 words with negative outlook. There are exactly 1281 words inventoried in the affective extension of WordNet (Strapparava and Mihalcea, 2008), and the affective word list from (Johnson–Laird and Oatley, 1989) comprises less than 1000 words. This Input Text Figure 2: Proposed Latent Affective Framework. considerably complicates the construction of reliable affective categories in the latent space. To address the two limitations above, we propose to more broadly leverage the LSM paradigm (Bellegarda, 2005; Bellegarda, 2008), following th</context>
</contexts>
<marker>Stone, 1997</marker>
<rawString>P.J. Stone (1997), “Thematic Text Analysis: New agendas for Analyzing Text Content,” in Text Analysis for the Social Sciences: Methods for Drawing Statistical Inferences from Texts and Transcripts, C.W. Roberts, Ed., Mahwah, NJ: Lawrence Erlbaum Assoc. Publishers, 35–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>R Mihalcea</author>
</authors>
<date>2007</date>
<booktitle>SemEval-2007 Task 14: Affective Text,” in Proc. 4th Int. Workshop on Semantic Evaluations (SemEval</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5696" citStr="Strapparava and Mihalcea, 2007" startWordPosition="855" endWordPosition="858">e emotion classification process. This de facto bypasses the need for any explicit external knowledge. The paper is organized as follows. The next section provides some motivation for, and gives an overview of, the proposed latent affective framework. In Sections 3 and 4, we describe the two main alternatives considered, latent folding and latent embedding. In Section 5, we discuss the mechanics of emotion detection based on such latent affective processing. Finally, Section 6 reports the outcome of experimental evaluations conducted on the “Affective Text” portion of the SemEval-2007 corpus (Strapparava and Mihalcea, 2007). 2 Motivation and Overview As alluded to above, lexical affinity alone fails to provide sufficient distinction between different emotions, in large part because only relatively few WordNet Synset Specific Word Pseudo−document Similarity Input Text Figure 1: Typical LSA-Based Emotion Analysis. words have inherently clear, unambiguous emotional meaning. For example, happy and sad encapsulate JOY and SADNESS, respectively, in all conceivable scenarios. But is thrilling a marker of JOY or SURPRISE? Does awful capture SADNESS or DISGUST? It largely depends on contextual information: thrilling as a</context>
<context position="25350" citStr="Strapparava and Mihalcea, 2007" startWordPosition="4244" endWordPosition="4247">presentation to each affective anchor ˆz1,e (1 G Q G L). From (Bellegarda, 2008), a natural metric to consider is the cosine of the angle between them. This yields: C(z1,p, ˆz1,t) = for any 1 G Q G L. Using (17), it is a simple matter to directly compute the relevance of the input text to each emotional category. It is important to note that word weighting is now implicitly taken into account by the LSM formalism. 6 Experimental Evaluation In order to evaluate the latent affective framework described above, we used the data set that was developed for the SemEval 2007 task on “Affective Text” (Strapparava and Mihalcea, 2007). This task was focused on the emotion classification of news headlines. Headlines typically consist of a few words and are often written by creative people with the intention to “provoke” emotions, and consequently attract the readers’ attention. These characteristics make this kind of data particularly suitable for use in an automatic emotion recognition setting, as the affective/emotional features (if present) are guaranteed to appear in these short sentences. The test data accordingly consisted of 1,250 short news headlines2 extracted from news web sites (such as Google news, CNN) and/or n</context>
<context position="26616" citStr="Strapparava and Mihalcea, 2007" startWordPosition="4450" endWordPosition="4453">6 emotions (ANGER, DISGUST, FEAR, JOY, SADNESS, and SURPRISE) by different evaluators. For baseline purposes, we considered the following approaches: (i) a simple word accumulation system, which annotates the emotions in a text based on the presence of words from the WordNet-Affect lexicon; and (ii) three LSA-based systems implemented as in Fig. 1, which only differ in the way each emotion is represented in the LSA space: either based on a specific word only (e.g., JOY), or the word plus its WordNet synset, or the word plus all WordNet synsets labelled with that emotion in WordNetAffect (cf. (Strapparava and Mihalcea, 2007)). In all three cases, the large corpus used for LSA processing was the Wall Street Journal text collection (Graff et al., 1995), comprising about 86,000 articles. For the latent affective framework, we needed to select two separate training corpora. For the “domain” corpus, we selected a collection of about N1 = 8, 500 relatively short English sentences (with a vocabulary of roughly M1 = 12, 000 words) originally compiled for the purpose of a building a concatenative text-to-speech voice. Though not 2Development data was merged into the original SemEval 2007 test set to produce a larger test </context>
<context position="30632" citStr="Strapparava and Mihalcea, 2007" startWordPosition="5123" endWordPosition="5127">) by leveraging the latent topicality of two distinct corpora, as uncovered by a global LSM analysis of domain-oriented and emotion-oriented training documents. The two descriptions are then superimposed to produce the desired connection between all terms and emotional categories. Because this connection automatically takes into account the influence of the entire training corpora, it is more encompassing than that based on the relatively few affective terms typically considered in conventional processing. Empirical evidence gathered on the “Affective Text” portion of the SemEval-2007 corpus (Strapparava and Mihalcea, 2007) shows the effectiveness of the proposed strategy. Classification performance with latent affective embedding is slightly better than with latent affective folding, presumably because of its ability to more richly describe the affective space. Both techniques outperform standard LSA-based approaches, as well as affectively weighted word accumulation. This bodes well for the general deployability of latent affective processing across a wide range of applications. Future efforts will concentrate on characterizing the influence of the parameters R1 and R2 on the vector spaces L1 and L2, and the c</context>
</contexts>
<marker>Strapparava, Mihalcea, 2007</marker>
<rawString>C. Strapparava and R. Mihalcea (2007), “SemEval-2007 Task 14: Affective Text,” in Proc. 4th Int. Workshop on Semantic Evaluations (SemEval 2007), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>R Mihalcea</author>
</authors>
<title>Learning to Identify Emotions in Text,” in</title>
<date>2008</date>
<booktitle>Proc. 2008 ACM Symposium on Applied Computing,</booktitle>
<pages>1556--1560</pages>
<location>New York, NY,</location>
<contexts>
<context position="2543" citStr="Strapparava and Mihalcea, 2008" startWordPosition="375" endWordPosition="378"> role in the generation of expressive synthetic speech (Schr¨oder, 2006). Emphasis has traditionally been placed on the set of six “universal” emotions (Ekman, 1993): ANGER, DISGUST, FEAR, JOY, SADNESS, and SURPRISE (Alm et al., 2005; Liu et al., 2003; Subasic and Huettner, 2001). Emotion analysis is typically carried out using a simplified description of emotional states in a low-dimensional space, which normally comprises dimensions such as valence (positive/negative evalution), activation (stimulation of activity), and/or control (dominant/submissive power) (Mehrabian, 1995; Russell, 1980; Strapparava and Mihalcea, 2008). Classification proceeds based on an underlying emotional knowledge base, which strives to provide adequate distinctions between different emotions. This affective information can either be built entirely upon manually selected vocabulary as in (Whissell, 1989), or derived automatically from data based on expert knowledge of the most relevant features that can be extracted from the input text (Alm et al., 2005). In both cases, the resulting system tends to rely, for the most part, on a few thousand annotated “emotional keywords,” the presence of which triggers the associated emotional label(s</context>
<context position="7544" citStr="Strapparava and Mihalcea, 2008" startWordPosition="1140" endWordPosition="1143">tive words via inference mechanisms automatically derived in an unsupervised way from a large corpus of texts, such as the British National Corpus (Strapparava et al., 2006). By looking at document-level cooccurrences, contextual information is exploited to encapsulate semantic information into a relatively low dimensional vector space. Suitable affective categories are then constructed in that space by “folding in” either the specific word denoting the emotion, or its associated synset (say, from WordNet), or even the entire set of words in all synsets that can be labelled with that emotion (Strapparava and Mihalcea, 2008). This is typically done by placing the relevant word(s) into a “pseudo-document,” and map it into the space as if it were a real one (Deerwester et al., 1990). Finally, the global emotional affinity of a given input text is determined by computing similarities between all pseudo-documents. The resulting framework is depicted in Fig. 1. All Synsets LSA Processing Homogeneous Representation Large Corpus Detected Emotion 2 This solution is attractive, if for no other reason than it allows every word to automatically be assigned some fractional affective influence. However, it suffers from two li</context>
<context position="10288" citStr="Strapparava and Mihalcea, 2008" startWordPosition="1579" endWordPosition="1582">ic word denoting the emotion is likely to be less robust than using the set of words in all synsets labelled with that emotion. On the other hand, the latter may well expose some inherent ambiguities resulting from affective polysemy. This is compounded by the relatively small number of words for which an affective distribution is even available. For example, the well-known General Inquirer content analysis system (Stone, 1997) lists only about 2000 words with positive outlook and 2000 words with negative outlook. There are exactly 1281 words inventoried in the affective extension of WordNet (Strapparava and Mihalcea, 2008), and the affective word list from (Johnson–Laird and Oatley, 1989) comprises less than 1000 words. This Input Text Figure 2: Proposed Latent Affective Framework. considerably complicates the construction of reliable affective categories in the latent space. To address the two limitations above, we propose to more broadly leverage the LSM paradigm (Bellegarda, 2005; Bellegarda, 2008), following the overall framework depicted in Fig. 2. Compared to Fig. 1, we inject some supervision at two separate levels: not only regarding the particular domain considered, but also how the affective categorie</context>
<context position="28369" citStr="Strapparava and Mihalcea, 2008" startWordPosition="4773" endWordPosition="4776">tent Affective Folding 18.8 90.1 31.1 Latent Affective Embedding 20.9 91.7 34.0 completely congruent with news headlines, we felt that the type and range of topics covered was close enough to serve as a good proxy for the domain. For the “affective” corpus, we relied on about N2 = 5, 000 mood-annotated blog entries from LiveJournal.com, with a filtered3 vocabulary of about M2 = 20, 000 words. The indication of mood being explicitly specified when posting on LiveJournal, without particular coercion from the interface, moodannotated posts are likely to reflect the true mood of the blog authors (Strapparava and Mihalcea, 2008). The moods were then mapped to the L = 6 emotions considered in the classification. Next, we formed the domain and affective matrices W1 and W2 and processed them as in (2) and (5). We used R1 = 100 for the dimension of the domain space L1 and R2 = L = 6 for the dimension of the affective space L2. We then compared latent affective folding and embedding to the above systems. The results are summarized in Table I. Consistent with the observations in (Strapparava and Mihalcea, 2008), word accumulation secures the highest precision at the cost of the lowest recall, while LSA-based systems achiev</context>
</contexts>
<marker>Strapparava, Mihalcea, 2008</marker>
<rawString>C. Strapparava and R. Mihalcea (2008), “Learning to Identify Emotions in Text,” in Proc. 2008 ACM Symposium on Applied Computing, New York, NY, 1556– 1560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Valitutti</author>
<author>O Stock</author>
</authors>
<title>The Affective Weight of Lexicon,” in</title>
<date>2006</date>
<booktitle>Proc. 5th Int. Conf. Language Resources and Evaluation (LREC),</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="4101" citStr="Strapparava et al., 2006" startWordPosition="611" endWordPosition="614">is and Generation of Emotion in Text, pages 1–9, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics tional categories, it often fails to meaningfully generalize beyond the relatively few core terms explicitly considered in its construction. This has sparked interest in data-driven approaches based on latent semantic analysis (LSA), a paradigm originally developed for information retrieval (Deerwester et al., 1990). Upon suitable training using a large corpus of texts, LSA allows a similarity score to be computed between generic terms and affective categories (Strapparava et al., 2006). This way, every word can automatically be assigned some fractional affective influence. Still, the affective categories themselves are usually specified with the help of a reference lexical database like WordNet (Fellbaum, 1998). The purpose of this paper is to more broadly leverage the principle of latent semantics in emotion analysis. We cast the problem as a general application of latent semantic mapping (LSM), an extrapolation of LSA for modeling global relationships implicit in large volumes of data (Bellegarda, 2005; Bellegarda, 2008). More specifically, we use the LSM framework to des</context>
<context position="7086" citStr="Strapparava et al., 2006" startWordPosition="1070" endWordPosition="1073">milarly, awful as a synonym for grave reflects SADNESS (as in an awful car accident), while awful as a synonym for foul is closer to DISGUST (as in an awful smell). The vast majority of words likewise carry multiple potential emotional connotations, with the degree of affective polysemy tightly linked to the granularity selected for the underlying taxonomy of emotions. Data-driven approaches based on LSA purport to “individuate” such indirect affective words via inference mechanisms automatically derived in an unsupervised way from a large corpus of texts, such as the British National Corpus (Strapparava et al., 2006). By looking at document-level cooccurrences, contextual information is exploited to encapsulate semantic information into a relatively low dimensional vector space. Suitable affective categories are then constructed in that space by “folding in” either the specific word denoting the emotion, or its associated synset (say, from WordNet), or even the entire set of words in all synsets that can be labelled with that emotion (Strapparava and Mihalcea, 2008). This is typically done by placing the relevant word(s) into a “pseudo-document,” and map it into the space as if it were a real one (Deerwes</context>
</contexts>
<marker>Strapparava, Valitutti, Stock, 2006</marker>
<rawString>C. Strapparava, A. Valitutti, and O. Stock (2006), “The Affective Weight of Lexicon,” in Proc. 5th Int. Conf. Language Resources and Evaluation (LREC), Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Subasic</author>
<author>A Huettner</author>
</authors>
<title>Affect Analysis of Text Using Fuzzy Semantic Typing,”</title>
<date>2001</date>
<journal>IEEE Trans. Fuzzy Systems,</journal>
<volume>9</volume>
<issue>4</issue>
<contexts>
<context position="2192" citStr="Subasic and Huettner, 2001" startWordPosition="331" endWordPosition="335">ring levels of hateful or violent rhetoric (perhaps in multilingual settings). More generally, emotion detection is of great 1 interest in human-computer interaction: if a system determines that a user is upset or annoyed, for instance, it could switch to a different mode of interaction (Liscombe et al., 2005). And of course, it plays a critical role in the generation of expressive synthetic speech (Schr¨oder, 2006). Emphasis has traditionally been placed on the set of six “universal” emotions (Ekman, 1993): ANGER, DISGUST, FEAR, JOY, SADNESS, and SURPRISE (Alm et al., 2005; Liu et al., 2003; Subasic and Huettner, 2001). Emotion analysis is typically carried out using a simplified description of emotional states in a low-dimensional space, which normally comprises dimensions such as valence (positive/negative evalution), activation (stimulation of activity), and/or control (dominant/submissive power) (Mehrabian, 1995; Russell, 1980; Strapparava and Mihalcea, 2008). Classification proceeds based on an underlying emotional knowledge base, which strives to provide adequate distinctions between different emotions. This affective information can either be built entirely upon manually selected vocabulary as in (Wh</context>
</contexts>
<marker>Subasic, Huettner, 2001</marker>
<rawString>P. Subasic and A. Huettner (2001), “Affect Analysis of Text Using Fuzzy Semantic Typing,” IEEE Trans. Fuzzy Systems, 9(4):483–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Whissell</author>
</authors>
<title>The Dictionary of Affect</title>
<date>1989</date>
<booktitle>in Language,” in Emotion: Theory, Research, and</booktitle>
<pages>13--131</pages>
<publisher>Academic Press,</publisher>
<location>Eds., New York, NY:</location>
<contexts>
<context position="2805" citStr="Whissell, 1989" startWordPosition="414" endWordPosition="415">1). Emotion analysis is typically carried out using a simplified description of emotional states in a low-dimensional space, which normally comprises dimensions such as valence (positive/negative evalution), activation (stimulation of activity), and/or control (dominant/submissive power) (Mehrabian, 1995; Russell, 1980; Strapparava and Mihalcea, 2008). Classification proceeds based on an underlying emotional knowledge base, which strives to provide adequate distinctions between different emotions. This affective information can either be built entirely upon manually selected vocabulary as in (Whissell, 1989), or derived automatically from data based on expert knowledge of the most relevant features that can be extracted from the input text (Alm et al., 2005). In both cases, the resulting system tends to rely, for the most part, on a few thousand annotated “emotional keywords,” the presence of which triggers the associated emotional label(s). The drawback of such confined lexical affinity is that the analysis tends to be hampered by the bias inherent in the underlying taxonomy of emotional states. Because this taxonomy only supports simplified relationships between affective words and emoProceedin</context>
</contexts>
<marker>Whissell, 1989</marker>
<rawString>C.M. Whissell (1989), “The Dictionary of Affect in Language,” in Emotion: Theory, Research, and Experience, R. Plutchik and H. Kellerman, Eds., New York, NY: Academic Press, 13–131.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>