<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996066666666667">
Disambiguating Nouns, Verbs, and
Adjectives Using Automatically Acquired
Selectional Preferences
</title>
<author confidence="0.999833">
Diana McCarthy∗ John Carroll*
</author>
<affiliation confidence="0.999081">
University of Sussex University of Sussex
</affiliation>
<bodyText confidence="0.996619230769231">
Selectional preferences have been used by word sense disambiguation (WSD) systems as one
source of disambiguating information. We evaluate WSD using selectional preferences acquired
for English adjective–noun, subject, and direct object grammatical relationships with respect to
a standard test corpus. The selectional preferences are specific to verb or adjective classes, rather
than individual word forms, so they can be used to disambiguate the co-occurring adjectives and
verbs, rather than just the nominal argument heads. We also investigate use of the one-sense-
per-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word
within the current document in order to increase coverage. Although the preferences perform
well in comparison with other unsupervised WSD systems on the same corpus, the results show
that for many applications, further knowledge sources would be required to achieve an adequate
level of accuracy and coverage. In addition to quantifying performance, we analyze the results to
investigate the situations in which the selectional preferences achieve the best precision and in
which the one-sense-per-discourse heuristic increases performance.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999887263157895">
Although selectional preferences are a possible knowledge source in an automatic
word sense disambiguation (WDS) system, they are not a panacea. One problem is
coverage: Most previous work has focused on acquiring selectional preferences for
verbs and applying them to disambiguate nouns occurring at subject and direct ob-
ject slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson
2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion
of word tokens do not fall at these slots. There has been some work looking at other
slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Fed-
erici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of
coverage remains. Selectional preferences can be used for WSD in combination with
other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascer-
tain when they work well so that they can be utilized to their full advantage. This
article is aimed at quantifying the disambiguation performance of automatically ac-
quired selectional preferences in regard to nouns, verbs, and adjectives with respect to
a standard test corpus and evaluation setup (SENSEVAL-2) and to identify strengths
and weaknesses. Although there is clearly a limit to coverage using preferences alone,
because preferences are acquired only with respect to specific grammatical roles, we
show that when dealing with running text, rather than isolated examples, coverage can
be increased at little cost in accuracy by using the one-sense-per-discourse heuristic.
</bodyText>
<footnote confidence="0.408458">
* Department of Informatics, University of Sussex, Brighton BN1 9QH, UK. E-mail: {dianam,
johnca}@sussex.ac.uk
© 2003 Association for Computational Linguistics
</footnote>
<note confidence="0.823334">
Computational Linguistics Volume 29, Number 4
</note>
<bodyText confidence="0.999983125">
We acquire selectional preferences as probability distributions over the Word-
Net (Fellbaum 1998) noun hyponym hierarchy. The probability distributions are con-
ditioned on a verb or adjective class and a grammatical relationship. A noun is disam-
biguated by using the preferences to give probability estimates for each of its senses
in WordNet, that is, for WordNet synsets. Verbs and adjectives are disambiguated by
using the probability distributions and Bayes’ rule to obtain an estimate of the proba-
bility of the adjective or verb class, given the noun and the grammatical relationship.
Previously, we evaluated noun and verb disambiguation on the English all-words task
in the SENSEVAL-2 exercise (Cotton et al. 2001). We now present results also using
preferences for adjectives, again evaluated on the SENSEVAL-2 test corpus (but carried
out after the formal evaluation deadline). The results are encouraging, given that this
method does not rely for training on any hand-tagged data or frequency distributions
derived from such data. Although a modest amount of English sense-tagged data is
available, we nevertheless believe it is important to investigate methods that do not
require such data, because there will be languages or texts for which sense-tagged
data for a given word is not available or relevant.
</bodyText>
<sectionHeader confidence="0.997658" genericHeader="keywords">
2. Motivation
</sectionHeader>
<bodyText confidence="0.999680533333333">
The goal of this article is to assess the WSD performance of selectional preference
models for adjectives, verbs, and nouns on the SENSEVAL-2 test corpus. There are two
applications for WSD that we have in mind and are directing our research. The first
application is text simplification, as outlined by Carroll, Minnen, Pearce et al. (1999).
One subtask in this application involves substituting words with thier more frequent
synonyms, for example, substituting letter for missive. Our motivation for using WSD
is to filter out inappropriate senses of a word token, so that the substituting synonym
is appropriate given the context. For example, in the following sentence we would
like to use strategy, rather than dodge, as a substitute for scheme:
A recent government study singled out the scheme as an example to others.
We are also investigating the disambiguation of verb senses in running text before
subcategorization information for the verbs is acquired, in order to produce a sub-
categorization lexicon specific to sense (Preiss and Korhonen 2002). For example, if
subcategorization were acquired specific to sense, rather than verb form, then distinct
senses of fire could have different subcategorization entries:
</bodyText>
<equation confidence="0.984881">
fire(1) - sack: NP V NP
fire(2) - shoot: NP V NP, NP V
</equation>
<bodyText confidence="0.999489">
Selectional preferences could also then be acquired automatically from sense-tagged
data in an iterative approach (McCarthy 2001).
</bodyText>
<sectionHeader confidence="0.998519" genericHeader="introduction">
3. Methodology
</sectionHeader>
<bodyText confidence="0.9998234">
We acquire selectional preferences from automatically preprocessed and parsed text
during a training phase. The parser is applied to the test data as well in the run-
time phase to identify grammatical relations among nouns, verbs, and adjectives. The
acquired selectional preferences are then applied to the noun-verb and noun-adjective
pairs in these grammatical constructions for disambiguation.
</bodyText>
<page confidence="0.996204">
640
</page>
<note confidence="0.979512">
McCarthy and Carroll Disambiguating Using Selectional Preferences
</note>
<figureCaption confidence="0.773504666666667">
Figure 1
System overview. Solid lines indicate flow of data during training, and broken lines show that
at run time.
</figureCaption>
<bodyText confidence="0.990821">
The overall structure of the system is illustrated in Figure 1. We describe the
individual components in sections 3.1–3.3 and 4.
</bodyText>
<subsectionHeader confidence="0.996595">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999991083333334">
The preprocessor consists of three modules applied in sequence: a tokenizer, a part-
of-speech (POS) tagger, and a lemmatizer.
The tokenizer comprises a small set of manually developed finite-state rules for
identifying word and sentence boundaries. The tagger (Elworthy 1994) uses a bigram
hidden Markov model augmented with a statistical unknown word guesser. When
applied to the training data for selectional preference acquisition, it produces the sin-
gle highest-ranked POS tag for each word. In the run-time phase, it returns multiple
tag hypotheses, each with an associated forward-backward probability to reduce the
impact of tagging errors. The lemmatizer (Minnen, Carroll, and Pearce 2001) reduces
inflected verbs and nouns to their base forms. It uses a set of finite-state rules express-
ing morphological regularities and subregularities, together with a list of exceptions
for specific (irregular) word forms.
</bodyText>
<subsectionHeader confidence="0.998825">
3.2 Parsing
</subsectionHeader>
<bodyText confidence="0.999977416666667">
The parser uses a wide-coverage unification-based shallow grammar of English POS
tags and punctuation (Briscoe and Carroll 1995) and performs disambiguation using
a context-sensitive probabilistic model (Briscoe and Carroll 1993), recovering from
extra-grammaticality by returning partial parses. The output of the parser is a set of
grammatical relations (Carroll, Briscoe, and Sanfilippo 1998) specifying the syntactic
dependency between each head and its dependent(s), taken from the phrase structure
tree that is returned from the disambiguation phase.
For selectional preference acquisition we applied the analysis system to the 90
million words of the written portion of the British National Corpus (BNC); the parser
produced complete analyses for around 60% of the sentences and partial analyses
for over 95% of the remainder. Both in the acquisition phase and at run time, we
extract from the analyser output subject–verb, verb–direct object, and noun–adjective
</bodyText>
<figure confidence="0.999233137931034">
ncmod glass_NN1 single_JJ
WordNet
Selectional Preferences
Sense Tagged output
Selectional
Preference
Acquisition
instance ID WordNet sense tag
model for verbclass
drink suck sip
d06.s13.t03 man%1:18:00::
Disambiguator
d04.s12.t09 drink%2:34:00::
&lt;entity&gt; &lt;measure&gt;
&lt;time&gt;
&lt;attribute&gt;
0.7
0.08
0.0005 0.09
Training data
where you can drink
a single glass with...
Test data
The men drink here ...
Parser
ncsubj drink_VV0 you_PPY
Grammatical Relations
dobj drink_VV0 glass_NN1
Preprocessor
</figure>
<page confidence="0.716667">
641
</page>
<note confidence="0.316669">
Computational Linguistics Volume 29, Number 4
</note>
<bodyText confidence="0.9333415">
modifier dependencies.1 We did not use the SENSEVAL-2 Penn Treebank–style brack-
etings supplied for the test data.
</bodyText>
<subsectionHeader confidence="0.999735">
3.3 Selectional Preference Acquisition
</subsectionHeader>
<bodyText confidence="0.998752764705883">
The preferences are acquired for grammatical relations (subject, direct object, and
adjective–noun) involving nouns and grammatically related adjectives or verbs. We
use WordNet synsets to define our sense inventory. Our method exploits the hyponym
links given for nouns (e.g., cheese is a hyponym of food), the troponym links for verbs 2
(e.g., limp is a troponym of walk), and the “similar-to” relationship given for adjectives
(e.g., one sense of cheap is similar to flimsy).
The preference models are modifications of the tree cut models (TCMs) originally
proposed by Li and Abe (1995, 1998). The main differences between that work and
ours are that we acquire adjective as well as verb models, and also that our models
are with respect to verb and adjective classes, rather than forms. We acquire models
for classes because we are using the models for WSD, whereas Li and Abe used them
for structural disambiguation.
We define a TCM as follows. Let NC be the set of noun synsets (noun classes)
in WordNet: NC = {nc E WordNet}, and NS be the set of noun senses 3 in Wordnet:
NS = {ns E WordNet}. A TCM is a set of noun classes that partition NS disjointly.
We use P to refer to such a set of classes in a TCM. A TCM is defined by P and a
probability distribution:
</bodyText>
<equation confidence="0.994405">
E p(nc) = 1 (8)
nc∈Γ
</equation>
<bodyText confidence="0.999836">
The probability distribution is conditioned by the grammatical context. In this
work, the probability distribution associated with a TCM is conditioned on a verb
class (vc) and either the subject or direct-object relation, or an adjective class (ac)
and the adjective–noun relation. Let VC be the set of verb synsets (verb classes) in
WordNet: VC = {vc E WordNet}. Let AC be the set of adjective classes (which subsume
WordNet synsets; we elaborate further on this subsequently). Thus, the TCMs define
a probability distribution over NS that is conditioned on a verb class (vc) or adjective
class (ac) and a particular grammatical relation (gr):
</bodyText>
<equation confidence="0.9934995">
E p(nc|vc,gr) = 1 (9)
nc∈Γ
</equation>
<bodyText confidence="0.999755333333333">
Acquisition of a TCM for a given vc and gr proceeds as follows. The data for
acquiring the preference are obtained from a subset of the tuples involving verbs
in the synset or troponym (subordinate) synsets. Not all verbs that are troponyms
or direct members of the synset are used in training. We take the noun argument
heads occurring with verbs that have no more than 10 senses in WordNet and a
frequency of 20 or more occurrences in the BNC data in the specified grammatical
relationship. The threshold of 10 senses removes some highly polysemous verbs having
many sense distinctions that are rather subtle. Verbs that have more than 10 senses
include very frequent verbs such as be and do that do not select strongly for their
</bodyText>
<footnote confidence="0.999006166666667">
1 In a previous evaluation of grammatical-relation accuracy with in-coverage text, the analyzer returned
subject–verb and verb–direct object dependencies with 84–88% recall and precision (Carroll, Minnen,
and Briscoe 1999).
2 In WordNet, verbs are organized by the troponymy relation, but this is represented with the same
hyponym pointer as is used in the noun hierarchy.
3 We refer to nouns attached to synsets as noun senses.
</footnote>
<page confidence="0.994833">
642
</page>
<figure confidence="0.9810953">
McCarthy and Carroll Disambiguating Using Selectional Preferences
% not in WordNet
100
80
60
40
20
0
0 50 100 150 200
frequency rank
</figure>
<figureCaption confidence="0.976535">
Figure 2
</figureCaption>
<subsectionHeader confidence="0.368752">
Verbs not in WordNet by BNC frequency.
</subsectionHeader>
<bodyText confidence="0.988562217391304">
arguments. The frequency threshold of 20 is intended to remove noisy data. We set
the threshold by examining a plot of BNC frequency and the percentage of verbs at
particular frequencies that are not listed in WordNet (Figure 2). Using 20 as a threshold
for the subject slot results in only 5% verbs that are not found in WordNet, whereas
73% of verbs with fewer than 20 BNC occurrences are not present in WordNet.4
The selectional-preference models for adjective–noun relations are conditioned on
an ac. Each ac comprises a group of adjective WordNet synsets linked by the “similar-
to” relation. These groups are formed such that they partition all adjective synsets.
Thus AC = {ac E WordNet adjective synsets linked by similar-to}. For example, Figure 3
shows the adjective classes that include the adjective fundamental and that are formed
in this way.5 For selectional-preference models conditioned on adjective classes, we
use only those adjectives that have 10 synsets or less in WordNet and have 20 or more
occurrences in the BNC.
The set of ncs in P are selected from all the possibilities in the hyponym hierarchy
according to the minimum description length (MDL) principle (Rissanen 1978) as used
by Li and Abe (1995, 1998). MDL finds the best TCM by considering the cost (in bits)
of describing both the model and the argument head data encoded in the model. The
cost (or description length) for a TCM is calculated according to equation (10). The
number of parameters of the model is given by k, which is the number of ncs in P
minus one. N is the sample of the argument head data. The cost of describing each
noun argument head (n) is calculated by the log of the probability estimate for that
noun:
description length = model description length + data description length
</bodyText>
<equation confidence="0.654560666666667">
k � logp(n) (10)
= 2 x log |N |+ −
n∈N
</equation>
<footnote confidence="0.964964666666667">
4 These threshold values are somewhat arbitrary, but it turns out that our results are not sensitive to the
exact values.
5 For the sake of brevity, not all adjectives are included in this diagram.
</footnote>
<page confidence="0.995436">
643
</page>
<figure confidence="0.999436222222222">
Computational Linguistics Volume 29, Number 4
basic
root radical
grassroots
underlying fundamental
rudimentary
primal elementary
basal base
of import important
primal key
central fundamental cardinal
crucial essential
valuable useful
historic
chief principal primary main
measurable
weighty grevious grave
significant important
monumental
profound fundamental
epochal
earthshaking
head
portentous prodigious
evidentiary evidential
noteworthy remarkable
large
</figure>
<figureCaption confidence="0.778545">
Figure 3
</figureCaption>
<bodyText confidence="0.952719">
Adjective classes that include fundamental.
The probability estimate for each n is obtained using the estimates for all the nss
that n has. Let Cn be the set of ncs that include n as a direct member: Cn = {nc ∈ NC|n ∈
nc}. Let nc&apos; be a hypernym of nc on P (i.e. nc&apos; ∈ {P|nc ∈ nc&apos;}) and let nsnc&apos; = {ns ∈ nc&apos;}
(i.e., the set of nouns senses at and beneath nc&apos; in the hyponym hierarchy). Then the
estimate p(n) is obtained using the estimates for the hypernym classes on P for all the
Cn that n belongs to:
</bodyText>
<equation confidence="0.947592333333333">
E p(nc&apos;) (11)
p(n) = |nsnc&apos;|
ncECn
</equation>
<bodyText confidence="0.999636571428571">
The probability at any particular nc&apos; is divided by nsnc&apos; to give the estimate for each
p(ns) under that nc&apos;.
The probability estimates for the {nc ∈ P} ( p(nc|vc,gr) or p(nc|ac,gr)) are obtained
from the tuples from the data of nouns co-occurring with verbs (or adjectives) belong-
ing to the conditioning vc (or ac) in the specified grammatical relationship (&lt; n, v,gr &gt;).
The frequency credit for a tuple is divided by |Cn |for any n, and by the number of
synsets of v, Cv (or Ca if the gr is adjective-noun):
</bodyText>
<equation confidence="0.994458285714286">
Efreq(nc|vc,gr) =
vEvc
(12)
|Cn||Cv|
E
nEnc
freq(n|v,gr)
</equation>
<bodyText confidence="0.7918535">
A hypernym nc&apos; includes the frequency credit attributed to all its hyponyms ({nc ∈
nc&apos;}).
</bodyText>
<equation confidence="0.898985">
Efreq(nc&apos;|vc, gr) = freq(nc|vc,gr) (13)
ncEncl
</equation>
<bodyText confidence="0.9998955">
This ensures that the total frequency credit at any P across the hyponym hierarchy
equals the credit for the conditioning vc. This will be the sum of the frequency credit
for all verbs that are direct members or troponyms of the vc, divided by the number
of other senses of each of these verbs:
</bodyText>
<equation confidence="0.899051666666667">
E freq(verb|gr) (14)
freq(vc|gr) = |Cv|
verbEvc
</equation>
<page confidence="0.967917">
644
</page>
<figure confidence="0.9883424">
McCarthy and Carroll Disambiguating Using Selectional Preferences
Root
seize assume usurp
seize clutch
human_action
control money monarchy straw party
throne handle
0.07 0.02 0.05 0.53
possession
0.14
0.02
0.06
group
0.26
entity
0.01
0.01
event
TCM for
TCM for
</figure>
<figureCaption confidence="0.994587">
Figure 4
</figureCaption>
<bodyText confidence="0.985568">
TCMs for the direct-object slot of two verb classes that include the verb seize.
To ensure that the TCM covers all NS in WordNet, we modify Li and Abe’s original
scheme by creating hyponym leaf classes below all WordNet’s internal classes in the
hyponym hierarchy. Each leaf holds the ns previously held at the internal class.
Figure 4 shows portions of two TCMs. The TCMs are similar, as they both contain
the verb seize, but the TCM for the class that includes clutch has a higher probability
for the entity noun class compared to the class that also includes assume and usurp.
This example includes only top-level WordNet classes, although the TCM may use
more specific noun classes.
</bodyText>
<sectionHeader confidence="0.996381" genericHeader="method">
4. Disambiguation
</sectionHeader>
<bodyText confidence="0.99992">
Nouns, adjectives and verbs are disambiguated by finding the sense (nc, vc, or ac) with
the maximum probability estimate in the given context. The method disambiguates
nouns and verbs to the WordNet synset level and adjectives to a coarse-grained level
of WordNet synsets linked by the similar-to relation, as described previously.
</bodyText>
<subsectionHeader confidence="0.993785">
4.1 Disambiguating Nouns
</subsectionHeader>
<bodyText confidence="0.999146111111111">
Nouns are disambiguated when they occur as subjects or direct objects and when
modified by adjectives. We obtain a probability estimate for each nc to which the target
noun belongs, using the distribution of the TCM associated with the co-occurring verb
or adjective and the grammatical relationship.
Li and Abe used TCMs for the task of structural disambiguation. To obtain proba-
bility estimates for noun senses occurring at classes beneath hypernyms on the cut, Li
and Abe used the probability estimate at the nc&apos; on the cut divided by the number of
ns descendants, as we do when finding r during training, so the probability estimate
is shared equally among all nouns in the nc&apos;, as in equation (15).
</bodyText>
<equation confidence="0.970937">
p(ns E nsnc,) = p(nc&apos;) (15)
|nsncI|
</equation>
<bodyText confidence="0.999772875">
One problem with doing this is that in cases in which the TCM is quite high in the
hierarchy, for example, at the entity class, the probability of any ns’s occurring under
this nc&apos; on the TCM will be the same and does not allow us to discriminate among
senses beneath this level.
For the WSD task, we compare the probability estimates at each nc E Cn, so if
a noun belongs to several synsets, we compare the probability estimates, given the
context, of these synsets. We obtain estimates for each nc by using the probability of
the hypernym nc&apos; on r. Rather than assume that all synsets under a given nc&apos; on r
</bodyText>
<page confidence="0.992842">
645
</page>
<note confidence="0.631155">
Computational Linguistics Volume 29, Number 4
</note>
<bodyText confidence="0.997924">
have the same likelihood of occurrence, we multiply the probability estimate for the
hypernym nc&apos; by the ratio of the prior frequency of the nc, that is, p(nc|gr), for which
we seek the estimate divided by the prior frequency of the hypernym nc&apos; (p(nc&apos;|gr)):
</bodyText>
<equation confidence="0.915538">
p(nc E hyponyms of nc&apos;|vc,gr) = p(nc&apos;|vc,gr) x p(nc|gr) (16)
p(nc�|gr)
</equation>
<bodyText confidence="0.999713222222222">
These prior estimates are taken from populating the noun hyponym hierarchy with the
prior frequency data for the gr irrespective of the co-occurring verbs. The probability
at the hypernym nc&apos; will necessarily total the probability at all hyponyms, since the
frequency credit of hyponyms is propagated to hypernyms.
Thus, to disambiguate a noun occurring in a given relationship with a given verb,
the nc E Cn that gives the largest estimate for p(nc|vc,gr) is taken, where the verb class
(vc) is that which maximizes this estimate from Cv. The TCM acquired for each vc of
the verb in the given gr provides an estimate for p(nc&apos;|vc,gr), and the estimate for nc
is obtained as in equation (16).
For example, one target noun was letter, which occurred as the direct object of
sign in our parses of the SENSEVAL-2 data. The TCM that maximized the probability
estimate for p(nc|vc,direct object) is shown in Figure 5. The noun letter is disambiguated
by comparing the probability estimates on the TCM above the five senses of letter mul-
tiplied by the proportion of that probability mass attributed to that synset. Although
entity has a higher probability on the TCM, compared to matter, which is above the
correct sense of letter,6 the ratio of prior probabilities for the synset containing letter7
under entity is 0.001, whereas that for the synset under matter is 0.226. This gives a
probability of 0.009 x 0.226 = 0.002 for the noun class probability given the verb class
</bodyText>
<figure confidence="0.897044411764706">
root
Figure 5
TCM for the direct-object slot of the verb class including sign and ratify.
6 The gloss is “a written message addressed to a person or organization; e.g. wrote an indignant letter to
the editor.”
7 The gloss is “owner who lets another person use something (housing usually) for hire.”
letter
matter
writing
0.009
draft
explanation
abstraction
0.006
relation
communication
approval
0.012
message
letter
0.0329
alphabetic_letter
interpretation
symbol
0.16
human_act
sign
letter
letter
0.114
capitalist
human
entity
letter
</figure>
<page confidence="0.982234">
646
</page>
<note confidence="0.835351">
McCarthy and Carroll Disambiguating Using Selectional Preferences
</note>
<bodyText confidence="0.9976185">
(with maximum probability) and grammatical context. This is the highest probability
for any of the synsets of letter, and so in this case the correct sense is selected.
</bodyText>
<subsectionHeader confidence="0.99947">
4.2 Disambiguating Verbs and Adjectives
</subsectionHeader>
<bodyText confidence="0.997897666666667">
Verbs and adjectives are disambiguated using TCMs to give estimates for p(nc|vc,gr)
and p(nc|ac,gr), respectively. These are combined with prior estimates for p(nc|gr) and
p(vc|gr) (or p(ac|gr)) using Bayes’ rule to give:
</bodyText>
<equation confidence="0.996232">
p(vc|nc,gr) = p(nc|vc,gr) × p(vc|gr)(17)
p(nc|gr)
</equation>
<bodyText confidence="0.898689">
and for adjective–noun relations:
</bodyText>
<equation confidence="0.998731">
p(ac|nc,adjnoun) = p(nc|ac,adjnoun) × p(ac|adjnoun) (18)
p(nc|adjnoun)
</equation>
<bodyText confidence="0.9999486">
The prior distributions for p(nc|gr), p(vc|gr) and p(ac|adjnoun) are obtained dur-
ing the training phase. For the prior distribution over NC, the frequency credit of
each noun in the specified gr in the training data is divided by |Cn|. The frequency
credit attached to a hyponym is propagated to the superordinate hypernyms, and the
frequency of a hypernym (nc&apos;) totals the frequency at its hyponyms:
</bodyText>
<equation confidence="0.9604855">
freq(nc&apos;|gr) = � freq(nc|gr) (19)
nc∈ncl
</equation>
<bodyText confidence="0.999647636363636">
The distribution over VC is obtained similarly using the troponym relation. For
the distribution over AC, the frequency credit for each adjective is divided by the
number of synsets to which the adjective belongs, and the credit for an ac is the sum
over all the synsets that are members by virtue of the similar-to WordNet link.
To disambiguate a verb occurring with a given noun, the vc from Cv that gives
the largest estimate for p(vc|nc,gr) is taken. The nc for the co-occurring noun is the
nc from Cn that maximizes this estimate. The estimate for p(nc|vc,gr) is taken as in
equation (16) but selecting the vc to maximize the estimate for p(vc|nc,gr) rather than
p(nc|vc,gr). An adjective is likewise disambiguated to the ac from all those to which the
adjective belongs, using the estimate for p(nc|ac,gr) and selecting the nc that maximizes
the p(ac|nc,gr) estimate.
</bodyText>
<subsectionHeader confidence="0.999002">
4.3 Increasing Coverage: One Sense per Discourse
</subsectionHeader>
<bodyText confidence="0.999965533333333">
There is a significant limitation to the word tokens that can be disambiguated using
selectional preferences, in that they are restricted to those that occur in the specified
grammatical relations and in argument head position. Moreover, we have TCMs only
for adjective and verb classes in which there was at least one adjective or verb mem-
ber that met our criteria for training (having no more than a threshold of 10 senses in
WordNet and a frequency of 20 or more occurrences in the BNC data in the specified
grammatical relationship). We chose not to apply TCMs for disambiguation where we
did not have TCMs for one or more classes for the verb or adjective. To increase cov-
erage, we experimented with applying the one-sense-per-discourse (OSPD) heuristic
(Gale, Church, and Yarowsky 1992). With this heuristic, a sense tag for a given word
is propagated to other occurrences of the same word within the current document in
order to increase coverage. When applying the OSPD heuristic, we simply applied a
tag for a noun, verb, or adjective to all the other instances of the same word type with
the same part of speech in the discourse, provided that only one possible tag for that
word was supplied by the selectional preferences for that discourse.
</bodyText>
<page confidence="0.977654">
647
</page>
<figure confidence="0.999533722222222">
Computational Linguistics Volume 29, Number 4
sel-ospd-ana sel-ospd
sel
100
80
60
Recall
40
20
0
0 20 40 60 80 100
Precision
supervised
other unsupervised
first sense heuristic
sel
sel-ospd
sel-ospd-ana
</figure>
<figureCaption confidence="0.84679">
Figure 6
</figureCaption>
<bodyText confidence="0.190106">
SENSEVAL-2 English all-words task results.
</bodyText>
<sectionHeader confidence="0.989394" genericHeader="method">
5. Evaluation
</sectionHeader>
<bodyText confidence="0.999830588235294">
We evaluated our system using the SENSEVAL-2 test corpus on the English all-
words task (Cotton et al., 2001). We entered a previous version of this system for
the SENSEVAL-2 exercise, in three variants, under the names “sussex-sel” (selectional
preferences), “sussex-sel-ospd” (with the OSPD heuristic), and “sussex-sel-ospd-ana”
(with anaphora resolution). 8 For SENSEVAL-2 we used only the direct object and
subject slots, since we had not yet dealt with adjectives. In Figure 6 we show how our
system fared at the time of SENSEVAL-2 compared to other unsupervised systems.9
We have also plotted the results of the supervised systems and the precision and recall
achieved by using the most frequent sense (as listed in WordNet).10
In the work reported here, we attempted disambiguation for head nouns and verbs
in subject and direct object relationships, and for adjectives and nouns in adjective-
noun relationships. For each test instance, we applied subject preferences before direct
object preferences, and direct object preferences before adjective–noun preferences. We
also propagated sense tags to test instances not in these relationships by applying the
one-sense-per-discourse heuristic.
We did not use the SENSEVAL-2 coarse-grained classification, as this was not
available at the time when we were acquiring the selectional preferences. We therefore
</bodyText>
<footnote confidence="0.935829833333333">
8 The motivation for using anaphora resolution was increased coverage, but anaphora resolution turned
out not actually to improve performance.
9 We use unsupervised to refer to systems that do not use manually sense-tagged training data, such as
SemCor. Our systems, marked in the figure as sel, sel-ospd, and sel-ospd-ana are unsupervised.
10 We are indebted to Judita Preiss for the most-frequent-sense result. This was obtained using the
frequency data supplied with the WordNet 1.7 version prereleased for SENSEVAL-2.
</footnote>
<page confidence="0.994309">
648
</page>
<note confidence="0.958174">
McCarthy and Carroll Disambiguating Using Selectional Preferences
</note>
<tableCaption confidence="0.999373">
Table 1
</tableCaption>
<table confidence="0.962639">
Overall results.
With OSPD Without OSPD
Precision 51.1% Precision 52.3%
Recall 23.2% Recall 20.0%
Attempted 45.5% Attempted 38.3%
</table>
<tableCaption confidence="0.992278">
Table 2
</tableCaption>
<table confidence="0.9849182">
Precision results by part of speech.
Precision (%) Baseline precision (%)
Nouns 58.5 51.7
Polysemous nouns 36.8 25.8
Verbs 40.9 29.7
Polysemous verbs 38.1 25.3
Adjectives 49.8 48.6
Polysemous adjectives 35.5 24.0
Nouns, verbs, and adjectives 51.1 44.9
Polysemous nouns, verbs, and adjectives 36.8 27.3
</table>
<bodyText confidence="0.997955958333333">
do not include in the following the coarse-grained results; they are just slightly better
than the fine-grained results, which seems to be typical of other systems.
Our latest overall results are shown in Table 1. In this table we show the results
both with and without the OSPD heuristic. The results for the English SENSEVAL-2
tasks were generally much lower than those for the original SENSEVAL competition.
At the time of the SENSEVAL-2 workshop, this was assumed to be due largely to the
use of WordNet as the inventory, as opposed to HECTOR (Atkins 1993), but Palmer,
Trang Dang, and Fellbaum (forthcoming) have subsequently shown that, at least for the
lexical sample tasks, this was due to a harder selection of words, with a higher average
level of polysemy. For three of the most polysemous verbs that overlapped between
the English lexical sample for SENSEVAL and SENSEVAL-2, the performance was
comparable. Table 2 shows our precision results including use of the OSPD heuristic,
broken down by part of speech. Although the precision for nouns is greater than that
for verbs, the difference is much less when we remove the trivial monosemous cases.
Nouns, verbs, and adjectives all outperform their random baseline for precision, and
the difference is more marked when monosemous instances are dropped.
Table 3 shows the precision results for polysemous words given the slot and the
disambiguation source. Overall, once at least one word token has been disambiguated
by the preferences, the OSPD heuristic seems to perform better than the selectional
preferences. We can see, however, that although this is certainly true for the nouns,
the difference for the adjectives (1.3%) is less marked, and the preferences outperform
OSPD for the verbs. It seems that verbs obey the OSPD principle much less than nouns.
Also, verbs are best disambiguated by their direct objects, whereas nouns appear to
be better disambiguated as subjects and when modified by adjectives.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="method">
6. Discussion
</sectionHeader>
<subsectionHeader confidence="0.994603">
6.1 Selectional Preferences
</subsectionHeader>
<bodyText confidence="0.997531">
The precision of our system compares well with that of other unsupervised systems on
the SENSEVAL-2 English all-words task, despite the fact that these other systems use a
</bodyText>
<page confidence="0.997828">
649
</page>
<note confidence="0.599166">
Computational Linguistics Volume 29, Number 4
</note>
<tableCaption confidence="0.995347">
Table 3
</tableCaption>
<table confidence="0.9905835">
Precision results for polysemous words by part of speech and slot or disambiguation source.
Subject (%) Dobj (%) Adjm (%) OSPD (%)
Polysemous nouns 33.7 26.8 31.0 49.0
Polysemous verbs 33.8 47.3 — 29.8
Polysemous adjectives — — 35.1 36.4
Polysemous nouns, verbs, and adjectives 33.4 36.0 31.6 44.8
</table>
<bodyText confidence="0.999851365853659">
number of different sources of information for disambiguation, rather than selectional
preferences in isolation. Light and Greiff (2002) summarize some earlier WSD results
for automatically acquired selectional preferences. These results were obtained for
three systems (Resnik 1997; Abney and Light 1999; Ciaramita and Johnson 2000) on a
training and test data set constructed by Resnik containing nouns occurring as direct
objects of 100 verbs that select strongly for their objects.
Both the test and training sets were extracted from the section of the Brown cor-
pus within the Penn Treebank and used the treebank parses. The test set comprised
the portion of this data within SemCor containing these 100 verbs, and the training
set comprised 800,000 words from the Penn Treebank parses of the Brown corpus not
within SemCor. All three systems obtained higher precision than the results we report
here, with Ciaramita and Johnson’s Bayesian belief networks achieving the best accu-
racy at 51.4%. These results are not comparable with ours, however, for three reasons.
First, our results for the direct-object slot are for all verbs in the English all-words task,
as opposed to just those selecting strongly for their direct objects. We would expect
that WSD results using selectional preferences would be better for the latter class of
verbs. Second, we do not use manually produced parses, but the output from our fully
automatic shallow parser. Third and finally, the baselines reported for Resnik’s test set
were higher than those for the all-words task. For Resnik’s test data, the random base-
line was 28.5%, whereas for the polysemous nouns in the direct-object relation on the
all-words task, it was 23.9%. The distribution of senses was also perhaps more skewed
for Resnik’s test set, since the first sense heuristic was 82.8% (Abney and Light 1999),
whereas it was 53.6% for the polysemous direct objects in the all-words task. Although
our results do show that the precision for the TCMs compares favorably with that of
other unsupervised systems on the English all-words task, it would be worthwhile to
compare other selectional preference models on the same data.
Although the accuracy of our system is encouraging given that it does not use
hand-tagged data, the results are below the level of state-of-the-art supervised systems.
Indeed, a system just assigning to each word its most frequent sense as listed in
WordNet (the “first-sense heuristic”) would do better than our preference models
(and in fact better than the majority of the SENSEVAL-2 English all-words supervised
systems). The first-sense heuristic, however, assumes the existence of sense-tagged data
that are able to give a definitive first sense. We do not use any first-sense information.
Although a modest amount of sense-tagged data is available for English (Miller et al.
1993, Ng and Lee 1996), for other languages with minimal sense-tagged resources, the
heuristic is not applicable. Moreover, for some words the predominant sense varies
depending on the domain and text type.
To quantify this, we carried out an analysis of the polysemous nouns, verbs,
and adjectives in SemCor occurring in more than one SemCor file and found that
a large proportion of words have a different first sense in different files and also
in different genres (see Table 4). For adjectives there seems to be a lot less ambi-
</bodyText>
<page confidence="0.997797">
650
</page>
<note confidence="0.969884">
McCarthy and Carroll Disambiguating Using Selectional Preferences
</note>
<tableCaption confidence="0.999291">
Table 4
</tableCaption>
<table confidence="0.9790814">
Percentages of words with a different predominant sense in SemCor, across files and genres.
File Genre
Nouns 70 66
Verbs 79 74
Adjectives 25 21
</table>
<bodyText confidence="0.999045454545454">
guity (this has also been noted by Krovetz [1998]; the data in SENSEVAL-2 bear
this out, with many adjectives occurring only in their first sense. For nouns and
verbs, for which the predominant sense is more likely to vary among texts, it would
be worthwhile to try to detect words for which using the predominant sense is
not a reliable strategy, for example, because the word shows “bursty” topic-related
behavior.
We therefore examined our disambiguation results to see if there was any pattern
in the predicates or arguments that were easily disambiguated themselves or were
good disambiguators of the co-occurring word. No particular patterns were evident
in this respect, perhaps because of the small size of the test data. There were nouns
such as team (precision= 22) and cancer (8 ) that did better than average, but whether
</bodyText>
<page confidence="0.714728">
10
</page>
<bodyText confidence="0.999925903225806">
or not they did better than the first-sense heuristic depends of course on the sense
in which they are used. For example, all 10 occurrences of cancer are in the first
sense, so the first sense heuristic is impossible to beat in this case. For the test items
that are not in their first sense, we beat the first-sense heuristic, but on the other
hand, we failed to beat the random baseline. (The random baseline is 21.8% and
we obtained 21.4% for these items overall.) Our performance on these items is low
probably because they are lower-frequency senses for which there is less evidence
in the untagged training corpus (the BNC). We believe that selectional preferences
would perform best if they were acquired from similar training data to that for which
disambiguation is required. In the future, we plan to investigate our models for WSD
in specific domains, such as sport and finance. The senses and frequency distribution
of senses for a given domain will in general be quite different from those in a balanced
corpus.
There are individual words that are not used in the first sense on which our TCM
preferences do well, for example sound (precision = 22), but there are not enough data
to isolate predicates or arguments that are good disambiguators from those that are
not. We intend to investigate this issue further with the SENSEVAL-2 lexical sample
data, which contains more instances of a smaller number of words.
Performance of selectional preferences depends not just on the actual word being
disambiguated, but the cohesiveness of the tuple &lt;pred, arg, gr&gt;. We have therefore
investigated applying a threshold on the probability of the class (nc, vc, or ac) before
disambiguation. Figure 7 presents a graph of precision against threshold applied to the
probability estimate for the highest-scoring class. We show alongside this the random
baseline and the first-sense heuristic for these items. Selectional preferences appear to
do better on items for which the probability predicted by our model is higher, but the
first-sense heuristic does even better on these. The first sense heuristic, with respect to
SemCor, outperforms the selectional preferences when it is averaged over a given text.
That seems to be the case overall, but there will be some words and texts for which
the first sense from SemCor is not relevant, and use of a threshold on probability, and
perhaps a differential between probability of the top-ranked senses suggested by the
model, should increase precision.
</bodyText>
<page confidence="0.993639">
651
</page>
<figure confidence="0.997206230769231">
Computational Linguistics Volume 29, Number 4
precision
1
0.8
0.6
0.4
0.2
0
&amp;quot;first sense&amp;quot;
&amp;quot;TCMs&amp;quot;
&amp;quot;random&amp;quot;
0 0.002 0.004 0.006 0.008 0.01
threshold
</figure>
<figureCaption confidence="0.99599">
Figure 7
</figureCaption>
<bodyText confidence="0.342901">
Thresholding the probability estimate for the highest-scoring class.
</bodyText>
<tableCaption confidence="0.979986">
Table 5
</tableCaption>
<table confidence="0.71188775">
Lemma/file combinations in SemCor with more than one sense evident.
Nouns 23%
Verbs 19%
Adjectives 1.6%
</table>
<subsectionHeader confidence="0.992997">
6.2 The OSPD Heuristic
</subsectionHeader>
<bodyText confidence="0.9999923">
In these experiments we applied the OSPD heuristic to increase coverage. One problem
in doing this when using a fine-grained classification like WordNet is that although
the OSPD heuristic works well for homonyms, it is less accurate for related senses
(Krovetz 1998), and this distinction is not made in WordNet. We did, however, find
that in SemCor, for the majority of polysemous11 lemma and file combinations, there
was only one sense exhibited (see Table 5). We refrained from using the OSPD in
situations in which there was conflicting evidence regarding the appropriate sense for
a word type occurring more than once in an individual file. In our experiments the
OSPD heuristic increased coverage by 7% and recall by 3%, at a cost of only a 1%
decrease in precision.
</bodyText>
<sectionHeader confidence="0.990145" genericHeader="conclusions">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.999939166666667">
We quantified coverage and accuracy of sense disambiguation of verbs, adjectives,
and nouns in the SENSEVAL-2 English all-words test corpus, using automatically
acquired selectional preferences. We improved coverage and recall by applying the
one-sense-per-discourse heuristic. The results show that disambiguation models using
only selectional preferences can perform with accuracy well above the random base-
line, although accuracy would not be high enough for applications in the absence of
</bodyText>
<footnote confidence="0.995767666666667">
11 Krovetz just looked at “actual ambiguity,” that is, words with more than one sense in SemCor. We
define polysemy as those words having more than one sense in WordNet, since we are using
SENSEVAL-2 data, and not SemCor.
</footnote>
<page confidence="0.986346">
652
</page>
<note confidence="0.868365">
McCarthy and Carroll Disambiguating Using Selectional Preferences
</note>
<bodyText confidence="0.999840857142857">
other knowledge sources (Stevenson and Wilks 2001). The results compare well with
those for other systems that do not use sense-tagged training data.
Selectional preferences work well for some word combinations and grammatical
relationships, but not well for others. We hope in future work to identify the situations
in which selectional preferences have high precision and to focus on these at the
expense of coverage, on the assumption that other knowledge sources can be used
where there is not strong evidence from the preferences. The first-sense heuristic, based
on sense-tagged data such as that available in SemCor, seems to beat unsupervised
models such as ours. For many words, however, the predominant sense varies across
domains, and so we contend that it is worth concentrating on detecting when the
first sense is not relevant, and where the selectional-preference models provide a high
probability for a secondary sense. In these cases evidence for a sense can be taken from
multiple occurrences of the word in the document, using the one-sense-per-discourse
heuristic.
</bodyText>
<sectionHeader confidence="0.992517" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.950363333333333">
This work was supported by UK EPSRC
project GR/N36493 “Robust Accurate
Statistical Parsing (RASP)” and EU FW5
project IST-2001-34460 “MEANING.” We are
grateful to Rob Koeling and three
anonymous reviewers for their helpful
comments on earlier drafts. We would also
like to thank David Weir and Mark
Mclauchlan for useful discussions.
</bodyText>
<sectionHeader confidence="0.988702" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997958">
Abney, Steven and Marc Light. 1999. Hiding
a semantic class hierarchy in a Markov
model. In Proceedings of the ACL Workshop
on Unsupervised Learning in Natural
Language Processing, pages 1–8.
Agirre, Eneko and David Martinez. 2001.
Learning class-to-class selectional
preferences. In Proceedings of the Fifth
Workshop on Computational Language
Learning (CoNLL-2001), pages 15–22.
Atkins, Sue. 1993. Tools for computer-aided
lexicography: The Hector project. In
Papers in Computational Lexicography:
COMPLEX 93, Budapest.
Briscoe, Ted and John Carroll. 1993.
Generalised probabilistic LR parsing of
natural language (corpora) with
unification-based grammars.
Computational Linguistics, 19(1):25–59.
Briscoe, Ted and John Carroll. 1995.
Developing and evaluating a probabilistic
LR parser of part-of-speech and
punctuation labels. In fourth
ACL/SIGPARSE International Workshop on
Parsing Technologies, pages 48–58, Prague,
Czech Republic.
Carroll, John, Ted Briscoe, and Antonio
Sanfilippo. 1998. Parser evaluation: A
survey and a new proposal. In Proceedings
of the International Conference on Language
Resources and Evaluation, pages 447–454.
Carroll, John, Guido Minnen, and Ted
Briscoe. 1999. Corpus annotation for
parser evaluation. In EACL-99
Post-conference Workshop on Linguistically
Interpreted Corpora, pages 35–41, Bergen,
Norway.
Carroll, John, Guido Minnen, Darren Pearce,
Yvonne Canning, Siobhan Devlin, and
John Tait. 1999. Simplifying English text
for language impaired readers. In
Proceedings of the Ninth Conference of the
European Chapter of the Association for
Computational Linguistics, pages 269–270,
Bergen, Norway.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional preference with
Bayesian networks. In Proceedings of the
18th International Conference of
Computational Linguistics (COLING-00),
pages 187–193.
Cotton, Scott, Phil Edmonds, Adam
Kilgarriff, and Martha Palmer. 2001.
SENSEVAL-2. Available at http://www.
sle.sharp.co.uk/senseval2/.
Elworthy, David. 1994. Does Baum-Welch
re-estimation help taggers? In Proceedings
of the fourth ACL Conference on Applied
Natural Language Processing, pages 53–58,
Stuttgart, Germany.
Federici, Stefano, Simonetta Montemagni,
and Vito Pirrelli. 1999. SENSE: An
analogy-based word sense
disambiguation system. Natural Language
Engineering, 5(2):207–218.
Fellbaum, Christiane, editor. 1998. WordNet,
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Gale, William, Kenneth Church, and David
Yarowsky. 1992. A method for
disambiguating word senses in a large
corpus. Computers and the Humanities,
</reference>
<page confidence="0.980362">
653
</page>
<note confidence="0.356405">
Computational Linguistics Volume 29, Number 4
</note>
<reference confidence="0.999545448717949">
26:415–439.
Krovetz, Robert. 1998. More than one sense
per discourse. In Proceedings of the
ACL-SIGLEX SENSEVAL Workshop.
Available at http://www.itri.bton.ac.
uk/events/senseval/
ARCHIVE/PROCEEDINGS/.
Li, Hang and Naoki Abe. 1995. Generalizing
case frames using a thesaurus and the
MDL principle. In Proceedings of the
International Conference on Recent Advances
in Natural Language Processing, pages
239–248, Bulgaria.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217–244.
Light, Marc and Warren Greiff. 2002.
Statistical models for the induction and
use of selectional preferences. Cognitive
Science, 26(3):269–281.
McCarthy, Diana. 1997. Word sense
disambiguation for acquisition of
selectional preferences. In Proceedings of
the ACL/EACL 97 Workshop Automatic
Information Extraction and Building of Lexical
Semantic Resources for NLP Applications,
pages 52–61.
McCarthy, Diana. 2001. Lexical Acquisition at
the Syntax-Semantics Interface: Diathesis
Alternations, Subcategorization Frames and
Selectional Preferences. Ph.D. thesis,
University of Sussex.
Miller, George, A., Claudia Leacock, Randee
Tengi, and Ross T. Bunker. 1993. A
semantic concordance. In Proceedings of the
ARPA Workshop on Human Language
Technology, pages 303–308. Morgan
Kaufmann.
Minnen, Guido, John Carroll, and Darren
Pearce. 2001. Applied morphological
processing of English. Natural Language
Engineering, 7(3):207–223.
Ng, Hwee Tou and Hian Beng Lee. 1996.
Integrating multiple knowledge sources
to disambiguate word sense: An
exemplar-based approach. In Proceedings
of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 40–47.
Palmer, Martha, Hoa Trang Dang, and
Christiane Fellbaum. 2003. Making
fine-grained and coarse-grained sense
distinctions, both manually and
automatically. Forthcoming, Natural
Language Engineering.
Preiss, Judita and Anna Korhonen. 2002.
Improving subcategorization acquisition
with WSD. In Proceedings of the ACL
Workshop on Word Sense Disambiguation:
Recent Successes and Future Directions,
Philadelphia, PA.
Resnik, Philip. 1997. Selectional preference
and sense disambiguation. In Proceedings
of the SIGLEX Workshop on Tagging Text with
Lexical Semantics: Why, What, and How?
pages 52–57, Washington, DC.
Ribas, Francesc. 1995. On learning more
appropriate selectional restrictions. In
Proceedings of the Seventh Conference of the
European Chapter of the Association for
Computational Linguistics, pages 112–118.
Rissanen, Jorma. 1978. Modelling by
shortest data description. Automatica,
14:465–471.
Stevenson, Mark and Yorick Wilks. 2001.
The interaction of knowledge sources in
word sense disambiguation. Computational
Linguistics, 17(3):321–349.
</reference>
<page confidence="0.999023">
654
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867365">
<title confidence="0.968751">Disambiguating Nouns, Verbs, and Adjectives Using Automatically Acquired Selectional Preferences</title>
<author confidence="0.940403">John</author>
<affiliation confidence="0.986885">University of Sussex University of Sussex</affiliation>
<abstract confidence="0.997744076923077">Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information. We evaluate WSD using selectional preferences acquired for English adjective–noun, subject, and direct object grammatical relationships with respect to a standard test corpus. The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads. We also investigate use of the one-senseper-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word within the current document in order to increase coverage. Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage. In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
<author>Marc Light</author>
</authors>
<title>Hiding a semantic class hierarchy in a Markov model.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL Workshop on Unsupervised Learning in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1775" citStr="Abney and Light 1999" startWordPosition="251" endWordPosition="254">ion to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. 1. Introduction Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized to their full advantag</context>
<context position="30528" citStr="Abney and Light 1999" startWordPosition="4865" endWordPosition="4868"> Number 4 Table 3 Precision results for polysemous words by part of speech and slot or disambiguation source. Subject (%) Dobj (%) Adjm (%) OSPD (%) Polysemous nouns 33.7 26.8 31.0 49.0 Polysemous verbs 33.8 47.3 — 29.8 Polysemous adjectives — — 35.1 36.4 Polysemous nouns, verbs, and adjectives 33.4 36.0 31.6 44.8 number of different sources of information for disambiguation, rather than selectional preferences in isolation. Light and Greiff (2002) summarize some earlier WSD results for automatically acquired selectional preferences. These results were obtained for three systems (Resnik 1997; Abney and Light 1999; Ciaramita and Johnson 2000) on a training and test data set constructed by Resnik containing nouns occurring as direct objects of 100 verbs that select strongly for their objects. Both the test and training sets were extracted from the section of the Brown corpus within the Penn Treebank and used the treebank parses. The test set comprised the portion of this data within SemCor containing these 100 verbs, and the training set comprised 800,000 words from the Penn Treebank parses of the Brown corpus not within SemCor. All three systems obtained higher precision than the results we report here</context>
<context position="32082" citStr="Abney and Light 1999" startWordPosition="5120" endWordPosition="5123"> would expect that WSD results using selectional preferences would be better for the latter class of verbs. Second, we do not use manually produced parses, but the output from our fully automatic shallow parser. Third and finally, the baselines reported for Resnik’s test set were higher than those for the all-words task. For Resnik’s test data, the random baseline was 28.5%, whereas for the polysemous nouns in the direct-object relation on the all-words task, it was 23.9%. The distribution of senses was also perhaps more skewed for Resnik’s test set, since the first sense heuristic was 82.8% (Abney and Light 1999), whereas it was 53.6% for the polysemous direct objects in the all-words task. Although our results do show that the precision for the TCMs compares favorably with that of other unsupervised systems on the English all-words task, it would be worthwhile to compare other selectional preference models on the same data. Although the accuracy of our system is encouraging given that it does not use hand-tagged data, the results are below the level of state-of-the-art supervised systems. Indeed, a system just assigning to each word its most frequent sense as listed in WordNet (the “first-sense heuri</context>
</contexts>
<marker>Abney, Light, 1999</marker>
<rawString>Abney, Steven and Marc Light. 1999. Hiding a semantic class hierarchy in a Markov model. In Proceedings of the ACL Workshop on Unsupervised Learning in Natural Language Processing, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
</authors>
<title>Learning class-to-class selectional preferences.</title>
<date>2001</date>
<booktitle>In Proceedings of the Fifth Workshop on Computational Language Learning (CoNLL-2001),</booktitle>
<pages>15--22</pages>
<contexts>
<context position="2116" citStr="Agirre and Martinez 2001" startWordPosition="306" endWordPosition="309">biguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized to their full advantage. This article is aimed at quantifying the disambiguation performance of automatically acquired selectional preferences in regard to nouns, verbs, and adjectives with respect to a standard test corpus and evaluation setup (SENSEVAL-2) and to identify strengths and weaknesses. Although there is clearly a limit to coverage using preferences</context>
</contexts>
<marker>Agirre, Martinez, 2001</marker>
<rawString>Agirre, Eneko and David Martinez. 2001. Learning class-to-class selectional preferences. In Proceedings of the Fifth Workshop on Computational Language Learning (CoNLL-2001), pages 15–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sue Atkins</author>
</authors>
<title>Tools for computer-aided lexicography: The Hector project.</title>
<date>1993</date>
<booktitle>In Papers in Computational Lexicography: COMPLEX 93,</booktitle>
<location>Budapest.</location>
<contexts>
<context position="28238" citStr="Atkins 1993" startWordPosition="4511" endWordPosition="4512">9 Polysemous nouns, verbs, and adjectives 36.8 27.3 do not include in the following the coarse-grained results; they are just slightly better than the fine-grained results, which seems to be typical of other systems. Our latest overall results are shown in Table 1. In this table we show the results both with and without the OSPD heuristic. The results for the English SENSEVAL-2 tasks were generally much lower than those for the original SENSEVAL competition. At the time of the SENSEVAL-2 workshop, this was assumed to be due largely to the use of WordNet as the inventory, as opposed to HECTOR (Atkins 1993), but Palmer, Trang Dang, and Fellbaum (forthcoming) have subsequently shown that, at least for the lexical sample tasks, this was due to a harder selection of words, with a higher average level of polysemy. For three of the most polysemous verbs that overlapped between the English lexical sample for SENSEVAL and SENSEVAL-2, the performance was comparable. Table 2 shows our precision results including use of the OSPD heuristic, broken down by part of speech. Although the precision for nouns is greater than that for verbs, the difference is much less when we remove the trivial monosemous cases.</context>
</contexts>
<marker>Atkins, 1993</marker>
<rawString>Atkins, Sue. 1993. Tools for computer-aided lexicography: The Hector project. In Papers in Computational Lexicography: COMPLEX 93, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalised probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="7839" citStr="Briscoe and Carroll 1993" startWordPosition="1170" endWordPosition="1173">le tag hypotheses, each with an associated forward-backward probability to reduce the impact of tagging errors. The lemmatizer (Minnen, Carroll, and Pearce 2001) reduces inflected verbs and nouns to their base forms. It uses a set of finite-state rules expressing morphological regularities and subregularities, together with a list of exceptions for specific (irregular) word forms. 3.2 Parsing The parser uses a wide-coverage unification-based shallow grammar of English POS tags and punctuation (Briscoe and Carroll 1995) and performs disambiguation using a context-sensitive probabilistic model (Briscoe and Carroll 1993), recovering from extra-grammaticality by returning partial parses. The output of the parser is a set of grammatical relations (Carroll, Briscoe, and Sanfilippo 1998) specifying the syntactic dependency between each head and its dependent(s), taken from the phrase structure tree that is returned from the disambiguation phase. For selectional preference acquisition we applied the analysis system to the 90 million words of the written portion of the British National Corpus (BNC); the parser produced complete analyses for around 60% of the sentences and partial analyses for over 95% of the remain</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Briscoe, Ted and John Carroll. 1993. Generalised probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):25–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Developing and evaluating a probabilistic LR parser of part-of-speech and punctuation labels.</title>
<date>1995</date>
<booktitle>In fourth ACL/SIGPARSE International Workshop on Parsing Technologies,</booktitle>
<pages>48--58</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7738" citStr="Briscoe and Carroll 1995" startWordPosition="1158" endWordPosition="1161">it produces the single highest-ranked POS tag for each word. In the run-time phase, it returns multiple tag hypotheses, each with an associated forward-backward probability to reduce the impact of tagging errors. The lemmatizer (Minnen, Carroll, and Pearce 2001) reduces inflected verbs and nouns to their base forms. It uses a set of finite-state rules expressing morphological regularities and subregularities, together with a list of exceptions for specific (irregular) word forms. 3.2 Parsing The parser uses a wide-coverage unification-based shallow grammar of English POS tags and punctuation (Briscoe and Carroll 1995) and performs disambiguation using a context-sensitive probabilistic model (Briscoe and Carroll 1993), recovering from extra-grammaticality by returning partial parses. The output of the parser is a set of grammatical relations (Carroll, Briscoe, and Sanfilippo 1998) specifying the syntactic dependency between each head and its dependent(s), taken from the phrase structure tree that is returned from the disambiguation phase. For selectional preference acquisition we applied the analysis system to the 90 million words of the written portion of the British National Corpus (BNC); the parser produ</context>
</contexts>
<marker>Briscoe, Carroll, 1995</marker>
<rawString>Briscoe, Ted and John Carroll. 1995. Developing and evaluating a probabilistic LR parser of part-of-speech and punctuation labels. In fourth ACL/SIGPARSE International Workshop on Parsing Technologies, pages 48–58, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ted Briscoe</author>
<author>Antonio Sanfilippo</author>
</authors>
<title>Parser evaluation: A survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation,</booktitle>
<pages>447--454</pages>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>Carroll, John, Ted Briscoe, and Antonio Sanfilippo. 1998. Parser evaluation: A survey and a new proposal. In Proceedings of the International Conference on Language Resources and Evaluation, pages 447–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Ted Briscoe</author>
</authors>
<title>Corpus annotation for parser evaluation.</title>
<date>1999</date>
<booktitle>In EACL-99 Post-conference Workshop on Linguistically Interpreted Corpora,</booktitle>
<pages>35--41</pages>
<location>Bergen,</location>
<marker>Carroll, Minnen, Briscoe, 1999</marker>
<rawString>Carroll, John, Guido Minnen, and Ted Briscoe. 1999. Corpus annotation for parser evaluation. In EACL-99 Post-conference Workshop on Linguistically Interpreted Corpora, pages 35–41, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Darren Pearce</author>
<author>Yvonne Canning</author>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>Simplifying English text for language impaired readers.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>269--270</pages>
<location>Bergen,</location>
<marker>Carroll, Minnen, Pearce, Canning, Devlin, Tait, 1999</marker>
<rawString>Carroll, John, Guido Minnen, Darren Pearce, Yvonne Canning, Siobhan Devlin, and John Tait. 1999. Simplifying English text for language impaired readers. In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics, pages 269–270, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Mark Johnson</author>
</authors>
<title>Explaining away ambiguity: Learning verb selectional preference with Bayesian networks.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference of Computational Linguistics (COLING-00),</booktitle>
<pages>187--193</pages>
<contexts>
<context position="1803" citStr="Ciaramita and Johnson 2000" startWordPosition="255" endWordPosition="258">formance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. 1. Introduction Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized to their full advantage. This article is aimed at </context>
<context position="30557" citStr="Ciaramita and Johnson 2000" startWordPosition="4869" endWordPosition="4872">ision results for polysemous words by part of speech and slot or disambiguation source. Subject (%) Dobj (%) Adjm (%) OSPD (%) Polysemous nouns 33.7 26.8 31.0 49.0 Polysemous verbs 33.8 47.3 — 29.8 Polysemous adjectives — — 35.1 36.4 Polysemous nouns, verbs, and adjectives 33.4 36.0 31.6 44.8 number of different sources of information for disambiguation, rather than selectional preferences in isolation. Light and Greiff (2002) summarize some earlier WSD results for automatically acquired selectional preferences. These results were obtained for three systems (Resnik 1997; Abney and Light 1999; Ciaramita and Johnson 2000) on a training and test data set constructed by Resnik containing nouns occurring as direct objects of 100 verbs that select strongly for their objects. Both the test and training sets were extracted from the section of the Brown corpus within the Penn Treebank and used the treebank parses. The test set comprised the portion of this data within SemCor containing these 100 verbs, and the training set comprised 800,000 words from the Penn Treebank parses of the Brown corpus not within SemCor. All three systems obtained higher precision than the results we report here, with Ciaramita and Johnson’</context>
</contexts>
<marker>Ciaramita, Johnson, 2000</marker>
<rawString>Ciaramita, Massimiliano and Mark Johnson. 2000. Explaining away ambiguity: Learning verb selectional preference with Bayesian networks. In Proceedings of the 18th International Conference of Computational Linguistics (COLING-00), pages 187–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Cotton</author>
<author>Phil Edmonds</author>
<author>Adam Kilgarriff</author>
<author>Martha Palmer</author>
</authors>
<date>2001</date>
<note>SENSEVAL-2. Available at http://www. sle.sharp.co.uk/senseval2/.</note>
<contexts>
<context position="3908" citStr="Cotton et al. 2001" startWordPosition="575" endWordPosition="578">n hyponym hierarchy. The probability distributions are conditioned on a verb or adjective class and a grammatical relationship. A noun is disambiguated by using the preferences to give probability estimates for each of its senses in WordNet, that is, for WordNet synsets. Verbs and adjectives are disambiguated by using the probability distributions and Bayes’ rule to obtain an estimate of the probability of the adjective or verb class, given the noun and the grammatical relationship. Previously, we evaluated noun and verb disambiguation on the English all-words task in the SENSEVAL-2 exercise (Cotton et al. 2001). We now present results also using preferences for adjectives, again evaluated on the SENSEVAL-2 test corpus (but carried out after the formal evaluation deadline). The results are encouraging, given that this method does not rely for training on any hand-tagged data or frequency distributions derived from such data. Although a modest amount of English sense-tagged data is available, we nevertheless believe it is important to investigate methods that do not require such data, because there will be languages or texts for which sense-tagged data for a given word is not available or relevant. 2.</context>
<context position="25384" citStr="Cotton et al., 2001" startWordPosition="4075" endWordPosition="4078">or a noun, verb, or adjective to all the other instances of the same word type with the same part of speech in the discourse, provided that only one possible tag for that word was supplied by the selectional preferences for that discourse. 647 Computational Linguistics Volume 29, Number 4 sel-ospd-ana sel-ospd sel 100 80 60 Recall 40 20 0 0 20 40 60 80 100 Precision supervised other unsupervised first sense heuristic sel sel-ospd sel-ospd-ana Figure 6 SENSEVAL-2 English all-words task results. 5. Evaluation We evaluated our system using the SENSEVAL-2 test corpus on the English allwords task (Cotton et al., 2001). We entered a previous version of this system for the SENSEVAL-2 exercise, in three variants, under the names “sussex-sel” (selectional preferences), “sussex-sel-ospd” (with the OSPD heuristic), and “sussex-sel-ospd-ana” (with anaphora resolution). 8 For SENSEVAL-2 we used only the direct object and subject slots, since we had not yet dealt with adjectives. In Figure 6 we show how our system fared at the time of SENSEVAL-2 compared to other unsupervised systems.9 We have also plotted the results of the supervised systems and the precision and recall achieved by using the most frequent sense (</context>
</contexts>
<marker>Cotton, Edmonds, Kilgarriff, Palmer, 2001</marker>
<rawString>Cotton, Scott, Phil Edmonds, Adam Kilgarriff, and Martha Palmer. 2001. SENSEVAL-2. Available at http://www. sle.sharp.co.uk/senseval2/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Elworthy</author>
</authors>
<title>Does Baum-Welch re-estimation help taggers?</title>
<date>1994</date>
<booktitle>In Proceedings of the fourth ACL Conference on Applied Natural Language Processing,</booktitle>
<pages>53--58</pages>
<location>Stuttgart, Germany.</location>
<contexts>
<context position="6953" citStr="Elworthy 1994" startWordPosition="1044" endWordPosition="1045">tion. 640 McCarthy and Carroll Disambiguating Using Selectional Preferences Figure 1 System overview. Solid lines indicate flow of data during training, and broken lines show that at run time. The overall structure of the system is illustrated in Figure 1. We describe the individual components in sections 3.1–3.3 and 4. 3.1 Preprocessing The preprocessor consists of three modules applied in sequence: a tokenizer, a partof-speech (POS) tagger, and a lemmatizer. The tokenizer comprises a small set of manually developed finite-state rules for identifying word and sentence boundaries. The tagger (Elworthy 1994) uses a bigram hidden Markov model augmented with a statistical unknown word guesser. When applied to the training data for selectional preference acquisition, it produces the single highest-ranked POS tag for each word. In the run-time phase, it returns multiple tag hypotheses, each with an associated forward-backward probability to reduce the impact of tagging errors. The lemmatizer (Minnen, Carroll, and Pearce 2001) reduces inflected verbs and nouns to their base forms. It uses a set of finite-state rules expressing morphological regularities and subregularities, together with a list of exc</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>Elworthy, David. 1994. Does Baum-Welch re-estimation help taggers? In Proceedings of the fourth ACL Conference on Applied Natural Language Processing, pages 53–58, Stuttgart, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Federici</author>
<author>Simonetta Montemagni</author>
<author>Vito Pirrelli</author>
</authors>
<title>SENSE: An analogy-based word sense disambiguation system.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<issue>2</issue>
<marker>Federici, Montemagni, Pirrelli, 1999</marker>
<rawString>Federici, Stefano, Simonetta Montemagni, and Vito Pirrelli. 1999. SENSE: An analogy-based word sense disambiguation system. Natural Language Engineering, 5(2):207–218.</rawString>
</citation>
<citation valid="true">
<title>WordNet, An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Fellbaum, Christiane, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="33885" citStr="[1998]" startWordPosition="5414" endWordPosition="5414">text type. To quantify this, we carried out an analysis of the polysemous nouns, verbs, and adjectives in SemCor occurring in more than one SemCor file and found that a large proportion of words have a different first sense in different files and also in different genres (see Table 4). For adjectives there seems to be a lot less ambi650 McCarthy and Carroll Disambiguating Using Selectional Preferences Table 4 Percentages of words with a different predominant sense in SemCor, across files and genres. File Genre Nouns 70 66 Verbs 79 74 Adjectives 25 21 guity (this has also been noted by Krovetz [1998]; the data in SENSEVAL-2 bear this out, with many adjectives occurring only in their first sense. For nouns and verbs, for which the predominant sense is more likely to vary among texts, it would be worthwhile to try to detect words for which using the predominant sense is not a reliable strategy, for example, because the word shows “bursty” topic-related behavior. We therefore examined our disambiguation results to see if there was any pattern in the predicates or arguments that were easily disambiguated themselves or were good disambiguators of the co-occurring word. No particular patterns w</context>
</contexts>
<marker>1998</marker>
<rawString>Fellbaum, Christiane, editor. 1998. WordNet, An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26--415</pages>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, William, Kenneth Church, and David Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26:415–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Krovetz</author>
</authors>
<title>More than one sense per discourse.</title>
<date>1998</date>
<booktitle>In Proceedings of the ACL-SIGLEX SENSEVAL Workshop. Available at http://www.itri.bton.ac. uk/events/senseval/ ARCHIVE/PROCEEDINGS/.</booktitle>
<contexts>
<context position="37807" citStr="Krovetz 1998" startWordPosition="6055" endWordPosition="6056">Linguistics Volume 29, Number 4 precision 1 0.8 0.6 0.4 0.2 0 &amp;quot;first sense&amp;quot; &amp;quot;TCMs&amp;quot; &amp;quot;random&amp;quot; 0 0.002 0.004 0.006 0.008 0.01 threshold Figure 7 Thresholding the probability estimate for the highest-scoring class. Table 5 Lemma/file combinations in SemCor with more than one sense evident. Nouns 23% Verbs 19% Adjectives 1.6% 6.2 The OSPD Heuristic In these experiments we applied the OSPD heuristic to increase coverage. One problem in doing this when using a fine-grained classification like WordNet is that although the OSPD heuristic works well for homonyms, it is less accurate for related senses (Krovetz 1998), and this distinction is not made in WordNet. We did, however, find that in SemCor, for the majority of polysemous11 lemma and file combinations, there was only one sense exhibited (see Table 5). We refrained from using the OSPD in situations in which there was conflicting evidence regarding the appropriate sense for a word type occurring more than once in an individual file. In our experiments the OSPD heuristic increased coverage by 7% and recall by 3%, at a cost of only a 1% decrease in precision. 7. Conclusion We quantified coverage and accuracy of sense disambiguation of verbs, adjective</context>
</contexts>
<marker>Krovetz, 1998</marker>
<rawString>Krovetz, Robert. 1998. More than one sense per discourse. In Proceedings of the ACL-SIGLEX SENSEVAL Workshop. Available at http://www.itri.bton.ac. uk/events/senseval/ ARCHIVE/PROCEEDINGS/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>239--248</pages>
<contexts>
<context position="9858" citStr="Li and Abe (1995" startWordPosition="1463" endWordPosition="1466">onal Preference Acquisition The preferences are acquired for grammatical relations (subject, direct object, and adjective–noun) involving nouns and grammatically related adjectives or verbs. We use WordNet synsets to define our sense inventory. Our method exploits the hyponym links given for nouns (e.g., cheese is a hyponym of food), the troponym links for verbs 2 (e.g., limp is a troponym of walk), and the “similar-to” relationship given for adjectives (e.g., one sense of cheap is similar to flimsy). The preference models are modifications of the tree cut models (TCMs) originally proposed by Li and Abe (1995, 1998). The main differences between that work and ours are that we acquire adjective as well as verb models, and also that our models are with respect to verb and adjective classes, rather than forms. We acquire models for classes because we are using the models for WSD, whereas Li and Abe used them for structural disambiguation. We define a TCM as follows. Let NC be the set of noun synsets (noun classes) in WordNet: NC = {nc E WordNet}, and NS be the set of noun senses 3 in Wordnet: NS = {ns E WordNet}. A TCM is a set of noun classes that partition NS disjointly. We use P to refer to such a</context>
<context position="13803" citStr="Li and Abe (1995" startWordPosition="2143" endWordPosition="2146">are formed such that they partition all adjective synsets. Thus AC = {ac E WordNet adjective synsets linked by similar-to}. For example, Figure 3 shows the adjective classes that include the adjective fundamental and that are formed in this way.5 For selectional-preference models conditioned on adjective classes, we use only those adjectives that have 10 synsets or less in WordNet and have 20 or more occurrences in the BNC. The set of ncs in P are selected from all the possibilities in the hyponym hierarchy according to the minimum description length (MDL) principle (Rissanen 1978) as used by Li and Abe (1995, 1998). MDL finds the best TCM by considering the cost (in bits) of describing both the model and the argument head data encoded in the model. The cost (or description length) for a TCM is calculated according to equation (10). The number of parameters of the model is given by k, which is the number of ncs in P minus one. N is the sample of the argument head data. The cost of describing each noun argument head (n) is calculated by the log of the probability estimate for that noun: description length = model description length + data description length k � logp(n) (10) = 2 x log |N |+ − n∈N 4 </context>
</contexts>
<marker>Li, Abe, 1995</marker>
<rawString>Li, Hang and Naoki Abe. 1995. Generalizing case frames using a thesaurus and the MDL principle. In Proceedings of the International Conference on Recent Advances in Natural Language Processing, pages 239–248, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Li, Abe, 1998</marker>
<rawString>Li, Hang and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics, 24(2):217–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
<author>Warren Greiff</author>
</authors>
<title>Statistical models for the induction and use of selectional preferences.</title>
<date>2002</date>
<journal>Cognitive Science,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="30360" citStr="Light and Greiff (2002)" startWordPosition="4842" endWordPosition="4845"> with that of other unsupervised systems on the SENSEVAL-2 English all-words task, despite the fact that these other systems use a 649 Computational Linguistics Volume 29, Number 4 Table 3 Precision results for polysemous words by part of speech and slot or disambiguation source. Subject (%) Dobj (%) Adjm (%) OSPD (%) Polysemous nouns 33.7 26.8 31.0 49.0 Polysemous verbs 33.8 47.3 — 29.8 Polysemous adjectives — — 35.1 36.4 Polysemous nouns, verbs, and adjectives 33.4 36.0 31.6 44.8 number of different sources of information for disambiguation, rather than selectional preferences in isolation. Light and Greiff (2002) summarize some earlier WSD results for automatically acquired selectional preferences. These results were obtained for three systems (Resnik 1997; Abney and Light 1999; Ciaramita and Johnson 2000) on a training and test data set constructed by Resnik containing nouns occurring as direct objects of 100 verbs that select strongly for their objects. Both the test and training sets were extracted from the section of the Brown corpus within the Penn Treebank and used the treebank parses. The test set comprised the portion of this data within SemCor containing these 100 verbs, and the training set </context>
</contexts>
<marker>Light, Greiff, 2002</marker>
<rawString>Light, Marc and Warren Greiff. 2002. Statistical models for the induction and use of selectional preferences. Cognitive Science, 26(3):269–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Word sense disambiguation for acquisition of selectional preferences.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL 97 Workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,</booktitle>
<pages>52--61</pages>
<contexts>
<context position="1753" citStr="McCarthy 1997" startWordPosition="249" endWordPosition="250">erage. In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. 1. Introduction Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized </context>
</contexts>
<marker>McCarthy, 1997</marker>
<rawString>McCarthy, Diana. 1997. Word sense disambiguation for acquisition of selectional preferences. In Proceedings of the ACL/EACL 97 Workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications, pages 52–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Lexical Acquisition at the Syntax-Semantics Interface: Diathesis Alternations, Subcategorization Frames and Selectional Preferences.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sussex.</institution>
<contexts>
<context position="5931" citStr="McCarthy 2001" startWordPosition="894" endWordPosition="895">eme as an example to others. We are also investigating the disambiguation of verb senses in running text before subcategorization information for the verbs is acquired, in order to produce a subcategorization lexicon specific to sense (Preiss and Korhonen 2002). For example, if subcategorization were acquired specific to sense, rather than verb form, then distinct senses of fire could have different subcategorization entries: fire(1) - sack: NP V NP fire(2) - shoot: NP V NP, NP V Selectional preferences could also then be acquired automatically from sense-tagged data in an iterative approach (McCarthy 2001). 3. Methodology We acquire selectional preferences from automatically preprocessed and parsed text during a training phase. The parser is applied to the test data as well in the runtime phase to identify grammatical relations among nouns, verbs, and adjectives. The acquired selectional preferences are then applied to the noun-verb and noun-adjective pairs in these grammatical constructions for disambiguation. 640 McCarthy and Carroll Disambiguating Using Selectional Preferences Figure 1 System overview. Solid lines indicate flow of data during training, and broken lines show that at run time.</context>
</contexts>
<marker>McCarthy, 2001</marker>
<rawString>McCarthy, Diana. 2001. Lexical Acquisition at the Syntax-Semantics Interface: Diathesis Alternations, Subcategorization Frames and Selectional Preferences. Ph.D. thesis, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Claudia Leacock A</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="33087" citStr="Miller et al. 1993" startWordPosition="5278" endWordPosition="5281">s not use hand-tagged data, the results are below the level of state-of-the-art supervised systems. Indeed, a system just assigning to each word its most frequent sense as listed in WordNet (the “first-sense heuristic”) would do better than our preference models (and in fact better than the majority of the SENSEVAL-2 English all-words supervised systems). The first-sense heuristic, however, assumes the existence of sense-tagged data that are able to give a definitive first sense. We do not use any first-sense information. Although a modest amount of sense-tagged data is available for English (Miller et al. 1993, Ng and Lee 1996), for other languages with minimal sense-tagged resources, the heuristic is not applicable. Moreover, for some words the predominant sense varies depending on the domain and text type. To quantify this, we carried out an analysis of the polysemous nouns, verbs, and adjectives in SemCor occurring in more than one SemCor file and found that a large proportion of words have a different first sense in different files and also in different genres (see Table 4). For adjectives there seems to be a lot less ambi650 McCarthy and Carroll Disambiguating Using Selectional Preferences Tab</context>
</contexts>
<marker>Miller, A, Tengi, Bunker, 1993</marker>
<rawString>Miller, George, A., Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In Proceedings of the ARPA Workshop on Human Language Technology, pages 303–308. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Minnen, Guido, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="33105" citStr="Ng and Lee 1996" startWordPosition="5282" endWordPosition="5285">d data, the results are below the level of state-of-the-art supervised systems. Indeed, a system just assigning to each word its most frequent sense as listed in WordNet (the “first-sense heuristic”) would do better than our preference models (and in fact better than the majority of the SENSEVAL-2 English all-words supervised systems). The first-sense heuristic, however, assumes the existence of sense-tagged data that are able to give a definitive first sense. We do not use any first-sense information. Although a modest amount of sense-tagged data is available for English (Miller et al. 1993, Ng and Lee 1996), for other languages with minimal sense-tagged resources, the heuristic is not applicable. Moreover, for some words the predominant sense varies depending on the domain and text type. To quantify this, we carried out an analysis of the polysemous nouns, verbs, and adjectives in SemCor occurring in more than one SemCor file and found that a large proportion of words have a different first sense in different files and also in different genres (see Table 4). For adjectives there seems to be a lot less ambi650 McCarthy and Carroll Disambiguating Using Selectional Preferences Table 4 Percentages o</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Ng, Hwee Tou and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Hoa Trang Dang</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Forthcoming, Natural Language Engineering.</title>
<date>2003</date>
<marker>Palmer, Dang, Fellbaum, 2003</marker>
<rawString>Palmer, Martha, Hoa Trang Dang, and Christiane Fellbaum. 2003. Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Forthcoming, Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judita Preiss</author>
<author>Anna Korhonen</author>
</authors>
<title>Improving subcategorization acquisition with WSD.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5578" citStr="Preiss and Korhonen 2002" startWordPosition="838" endWordPosition="841">yms, for example, substituting letter for missive. Our motivation for using WSD is to filter out inappropriate senses of a word token, so that the substituting synonym is appropriate given the context. For example, in the following sentence we would like to use strategy, rather than dodge, as a substitute for scheme: A recent government study singled out the scheme as an example to others. We are also investigating the disambiguation of verb senses in running text before subcategorization information for the verbs is acquired, in order to produce a subcategorization lexicon specific to sense (Preiss and Korhonen 2002). For example, if subcategorization were acquired specific to sense, rather than verb form, then distinct senses of fire could have different subcategorization entries: fire(1) - sack: NP V NP fire(2) - shoot: NP V NP, NP V Selectional preferences could also then be acquired automatically from sense-tagged data in an iterative approach (McCarthy 2001). 3. Methodology We acquire selectional preferences from automatically preprocessed and parsed text during a training phase. The parser is applied to the test data as well in the runtime phase to identify grammatical relations among nouns, verbs, </context>
</contexts>
<marker>Preiss, Korhonen, 2002</marker>
<rawString>Preiss, Judita and Anna Korhonen. 2002. Improving subcategorization acquisition with WSD. In Proceedings of the ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?</booktitle>
<pages>52--57</pages>
<location>Washington, DC.</location>
<contexts>
<context position="1988" citStr="Resnik 1997" startWordPosition="289" endWordPosition="290">. 1. Introduction Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized to their full advantage. This article is aimed at quantifying the disambiguation performance of automatically acquired selectional preferences in regard to nouns, verbs, and adjectives with respect to a standard test corpus and evaluat</context>
<context position="30506" citStr="Resnik 1997" startWordPosition="4863" endWordPosition="4864">cs Volume 29, Number 4 Table 3 Precision results for polysemous words by part of speech and slot or disambiguation source. Subject (%) Dobj (%) Adjm (%) OSPD (%) Polysemous nouns 33.7 26.8 31.0 49.0 Polysemous verbs 33.8 47.3 — 29.8 Polysemous adjectives — — 35.1 36.4 Polysemous nouns, verbs, and adjectives 33.4 36.0 31.6 44.8 number of different sources of information for disambiguation, rather than selectional preferences in isolation. Light and Greiff (2002) summarize some earlier WSD results for automatically acquired selectional preferences. These results were obtained for three systems (Resnik 1997; Abney and Light 1999; Ciaramita and Johnson 2000) on a training and test data set constructed by Resnik containing nouns occurring as direct objects of 100 verbs that select strongly for their objects. Both the test and training sets were extracted from the section of the Brown corpus within the Penn Treebank and used the treebank parses. The test set comprised the portion of this data within SemCor containing these 100 verbs, and the training set comprised 800,000 words from the Penn Treebank parses of the Brown corpus not within SemCor. All three systems obtained higher precision than the </context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Resnik, Philip. 1997. Selectional preference and sense disambiguation. In Proceedings of the SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How? pages 52–57, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesc Ribas</author>
</authors>
<title>On learning more appropriate selectional restrictions.</title>
<date>1995</date>
<booktitle>In Proceedings of the Seventh Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>112--118</pages>
<contexts>
<context position="1738" citStr="Ribas 1995" startWordPosition="247" endWordPosition="248">racy and coverage. In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. 1. Introduction Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they c</context>
</contexts>
<marker>Ribas, 1995</marker>
<rawString>Ribas, Francesc. 1995. On learning more appropriate selectional restrictions. In Proceedings of the Seventh Conference of the European Chapter of the Association for Computational Linguistics, pages 112–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Modelling by shortest data description.</title>
<date>1978</date>
<journal>Automatica,</journal>
<pages>14--465</pages>
<contexts>
<context position="13775" citStr="Rissanen 1978" startWordPosition="2138" endWordPosition="2139">o” relation. These groups are formed such that they partition all adjective synsets. Thus AC = {ac E WordNet adjective synsets linked by similar-to}. For example, Figure 3 shows the adjective classes that include the adjective fundamental and that are formed in this way.5 For selectional-preference models conditioned on adjective classes, we use only those adjectives that have 10 synsets or less in WordNet and have 20 or more occurrences in the BNC. The set of ncs in P are selected from all the possibilities in the hyponym hierarchy according to the minimum description length (MDL) principle (Rissanen 1978) as used by Li and Abe (1995, 1998). MDL finds the best TCM by considering the cost (in bits) of describing both the model and the argument head data encoded in the model. The cost (or description length) for a TCM is calculated according to equation (10). The number of parameters of the model is given by k, which is the number of ncs in P minus one. N is the sample of the argument head data. The cost of describing each noun argument head (n) is calculated by the log of the probability estimate for that noun: description length = model description length + data description length k � logp(n) (</context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>Rissanen, Jorma. 1978. Modelling by shortest data description. Automatica, 14:465–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
<author>Yorick Wilks</author>
</authors>
<title>The interaction of knowledge sources in word sense disambiguation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="1830" citStr="Stevenson and Wilks 2001" startWordPosition="259" endWordPosition="262">ults to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. 1. Introduction Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea. One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion of word tokens do not fall at these slots. There has been some work looking at other slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Federici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of coverage remains. Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized to their full advantage. This article is aimed at quantifying the disambiguat</context>
<context position="39163" citStr="Stevenson and Wilks 2001" startWordPosition="6264" endWordPosition="6267">erage and recall by applying the one-sense-per-discourse heuristic. The results show that disambiguation models using only selectional preferences can perform with accuracy well above the random baseline, although accuracy would not be high enough for applications in the absence of 11 Krovetz just looked at “actual ambiguity,” that is, words with more than one sense in SemCor. We define polysemy as those words having more than one sense in WordNet, since we are using SENSEVAL-2 data, and not SemCor. 652 McCarthy and Carroll Disambiguating Using Selectional Preferences other knowledge sources (Stevenson and Wilks 2001). The results compare well with those for other systems that do not use sense-tagged training data. Selectional preferences work well for some word combinations and grammatical relationships, but not well for others. We hope in future work to identify the situations in which selectional preferences have high precision and to focus on these at the expense of coverage, on the assumption that other knowledge sources can be used where there is not strong evidence from the preferences. The first-sense heuristic, based on sense-tagged data such as that available in SemCor, seems to beat unsupervised</context>
</contexts>
<marker>Stevenson, Wilks, 2001</marker>
<rawString>Stevenson, Mark and Yorick Wilks. 2001. The interaction of knowledge sources in word sense disambiguation. Computational Linguistics, 17(3):321–349.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>