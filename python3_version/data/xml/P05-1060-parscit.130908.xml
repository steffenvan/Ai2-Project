<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.976211">
Multi-Field Information Extraction and Cross-Document Fusion
</title>
<author confidence="0.967966">
Gideon S. Mann and David Yarowsky
</author>
<affiliation confidence="0.9730165">
Department of Computer Science
The Johns Hopkins University
</affiliation>
<address confidence="0.887753">
Baltimore, MD 21218 USA
</address>
<email confidence="0.999781">
{gsm,yarowsky}@cs.jhu.edu
</email>
<sectionHeader confidence="0.997402" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997575">
In this paper, we examine the task of extracting a
set of biographic facts about target individuals from
a collection of Web pages. We automatically anno-
tate training text with positive and negative exam-
ples of fact extractions and train Rote, Naive Bayes,
and Conditional Random Field extraction models
for fact extraction from individual Web pages. We
then propose and evaluate methods for fusing the
extracted information across documents to return a
consensus answer. A novel cross-field bootstrapping
method leverages data interdependencies to yield
improved performance.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999025581395349">
Much recent statistical information extraction re-
search has applied graphical models to extract in-
formation from one particular document after train-
ing on a large corpus of annotated data (Leek, 1997;
Freitag and McCallum, 1999).1 Such systems are
widely applicable, yet there remain many informa-
tion extraction tasks that are not readily amenable to
these methods. Annotated data required for training
statistical extraction systems is sometimes unavail-
able, while there are examples of the desired infor-
mation. Further, the goal may be to find a few inter-
related pieces of information that are stated multiple
times in a set of documents.
Here, we investigate one task that meets the above
criteria. Given the name of a celebrity such as
1Alternatively, Riloff(1996) trains on in-domain and
out-of-domain texts and then has a human filtering step.
Huffman (1995) proposes a method to train a different type of
extraction system by example.
“Frank Zappa”, our goal is to extract a set of bio-
graphic facts (e.g., birthdate, birth place and occupa-
tion) about that person from documents on the Web.
First, we describe a general method of automatic
annotation for training from positive and negative
examples and use the method to train Rote, Naive
Bayes, and Conditional Random Field models (Sec-
tion 2). We then examine how multiple extractions
can be combined to form one consensus answer
(Section 3). We compare fusion methods and show
that frequency voting outperforms the single high-
est confidence answer by an average of 11% across
the various extractors. Increasing the number of re-
trieved documents boosts the overall system accu-
racy as additional documents which mention the in-
dividual in question lead to higher recall. This im-
proved recall more than compensates for a loss in
per-extraction precision from these additional doc-
uments. Next, we present a method for cross-field
bootstrapping (Section 4) which improves per-field
accuracy by 7%. We demonstrate that a small train-
ing set with only the most relevant documents can be
as effective as a larger training set with additional,
less relevant documents (Section 5).
</bodyText>
<sectionHeader confidence="0.869039" genericHeader="method">
2 Training by Automatic Annotation
</sectionHeader>
<bodyText confidence="0.999191777777778">
Typically, statistical extraction systems (such as
HMMs and CRFs) are trained using hand-annotated
data. Annotating the necessary data by hand is time-
consuming and brittle, since it may require large-
scale re-annotation when the annotation scheme
changes. For the special case of Rote extrac-
tors, a more attractive alternative has been proposed
by Brin (1998), Agichtein and Gravano (2000), and
Ravichandran and Hovy (2002).
</bodyText>
<page confidence="0.990746">
483
</page>
<note confidence="0.991647">
Proceedings of the 43rd Annual Meeting of the ACL, pages 483–490,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.99809525">
Essentially, for any text snippet of the form
A1pA2qA3, these systems estimate the probability
that a relationship r(p, q) holds between entities p
and q, given the interstitial context, as2
</bodyText>
<equation confidence="0.998855">
P(r(p, q)  |pA2q) = P(r(p, q)  |pA2q)
E
x,yET c(xA2y)
= E
x c(xA2)
</equation>
<bodyText confidence="0.999967">
That is, the probability of a relationship r(p, q) is
the number of times that pattern xA2y predicts any
relationship r(x, y) in the training set T. c(.) is the
count. We will refer to x as the hook3 and y as the
target. In this paper, the hook is always an indi-
vidual. Training a Rote extractor is straightforward
given a set T of example relationships r(x, y). For
each hook, download a separate set of relevant doc-
uments (a hook corpus, Dx) from the Web.4 Then
for any particular pattern A2 and an element x, count
how often the pattern xA2 predicts y and how often
it retrieves a spurious 9.5
This annotation method extends to training other
statistical models with positive examples, for exam-
ple a Naive Bayes (NB) unigram model. In this
model, instead of looking for an exact A2 pattern
as above, each individual word in the pattern A2 is
used to predict the presence of a relationship.
</bodyText>
<equation confidence="0.9983504">
P(r(p, q)  |pA2q)
ocP(pA2q  |r(p, q))P(r(p, q))
=P(A2  |r(p, q))
11 = P(a  |r(p, q))
aEA2
</equation>
<bodyText confidence="0.997477">
We perform add-lambda smoothing for out-of-
vocabulary words and thus assign a positive prob-
ability to any sequence. As before, a set of relevant
</bodyText>
<footnote confidence="0.980677">
2The above Rote models also condition on the preceding and
trailing words, for simplicity we only model interstitial words
A2.
3Following (Ravichandran and Hovy, 2002).
4In the following experiments we assume that there is one
main object of interest p, for whom we want to find certain
pieces of information r(p, q), where r denotes the type of re-
lationship (e.g., birthday) and q is a value (e.g., May 20th). We
require one hook corpus for each hook, not a separate one for
each relationship.
5Having a functional constraint `dq =� q, r(p, q) makes this
estimate much more reliable, but it is possible to use this method
of estimation even when this constraint does not hold.
</footnote>
<bodyText confidence="0.9997893">
documents is downloaded for each particular hook.
Then every hook and target is annotated. From that
markup, we can pick out the interstitial A2 patterns
and calculate the necessary probabilities.
Since the NB model assigns a positive probability
to every sequence, we need to pick out likely tar-
gets from those proposed by the NB extractor. We
construct a background model which is a basic un-
igram language model, P(A2) = MEA2 P(a). We
then pick targets chosen by the confidence estimate
</bodyText>
<equation confidence="0.999903">
CNB(q) = logP(A2  |r(p, q))
P (A2)
</equation>
<bodyText confidence="0.997206181818182">
However, this confidence estimate does not work-
well in our dataset.
We propose to use negative examples to estimate
P(A2  |r(p, q))6 as well as P(A2  |r(p, q)). For
each relationship, we define the target set Er to be
all potential targets and model it using regular ex-
pressions.7 In training, for each relationship r(p, q),
we markup the hook p, the target q, and all spuri-
ous targets (q E {Er − q}) which provide negative
examples. Targets can then be chosen with the fol-
lowing confidence estimate
</bodyText>
<equation confidence="0.9999545">
P(A2  |r(p, q))
CNB+E(q) = logP(A2  |r(p,q))
</equation>
<bodyText confidence="0.999889545454545">
We call this NB+E in the following experiments.
The above process describes a general method for
automatically annotating a corpus with positive and
negative examples, and this corpus can be used to
train statistical models that rely on annotated data.8
In this paper, we test automatic annotation using
Conditional Random Fields (CRFs) (Lafferty et al.,
2001) which have achieved high performance for in-
formation extraction. CRFs are undirected graphical
models that estimate the conditional probability of a
state sequence given an output sequence
</bodyText>
<equation confidence="0.988087">
T
P(s  |o) = Z exp �J: J: Akfk (st−1, st, o, t) I
t=1 k /
</equation>
<footnote confidence="0.928231888888889">
6T stands in for all other possible relationships (including no
relationship) between p and q. P(A2  |r(p, q)) is estimated as
P(A2  |r(p, q)) is, except with spurious targets.
7e.g., E6irthyear = {\d\d\d\d}. This is the only source of
human knowledge put into the system and required only around
4 hours of effort, less effort than annotating an entire corpus or
writing information extraction rules.
8This corpus markup gives automatic annotation that yields
noisier training data than manual annotation would.
</footnote>
<page confidence="0.99725">
484
</page>
<figureCaption confidence="0.99767225">
Figure 1: CRF state-transition graphs for extracting a relation-
ship r(p, q) from a sentence pA2q. Left: CRF Extraction with
a background model (B). Right: CRF+E As before but with
spurious target prediction (pA2q).
</figureCaption>
<bodyText confidence="0.999973083333333">
We use the Mallet system (McCallum, 2002) for
training and evaluation of the CRFs. In order to ex-
amine the improvement by using negative examples,
we train CRFs with two topologies (Figure 1). The
first, CRF, models the target relationship and back-
ground sequences and is trained on a corpus where
targets (positive examples) are annotated. The sec-
ond, CRF+E, models the target relationship, spu-
rious targets and background sequences, and it is
trained on a corpus where targets (positive exam-
ples) as well as spurious targets (negative examples)
are annotated.
</bodyText>
<subsectionHeader confidence="0.837677">
Experimental Results
</subsectionHeader>
<bodyText confidence="0.999933">
To test the performance of the different ex-
tractors, we collected a set of 152 semi-
structured mini-biographies from an online site
(www.infoplease.com), and used simple rules to
extract a biographic fact database of birthday and
month (henceforth birthday), birth year, occupation,
birth place, and year of death (when applicable).
An example of the data can be found in Table
1. In our system, we normalized birthdays, and
performed capitalization normalization for the
remaining fields. We did no further normalization,
such as normalizing state names to their two letter
acronyms (e.g., California —* CA). Fifteen names
were set aside as training data, and the rest were
used for testing. For each name, 150 documents
were downloaded from Google to serve as the hook
corpus for either training or testing.9
In training, we automatically annotated docu-
ments using people in the training set as hooks, and
in testing, tried to get targets that exactly matched
what was present in the database. This is a very strict
method of evaluation for three reasons. First, since
the facts were automatically collected, they contain
</bodyText>
<footnote confidence="0.955329">
9Name polyreference, along with ranking errors, result in
the retrieval of undesired documents.
</footnote>
<table confidence="0.998305">
Aaron Neville Frank Zappa
Birthday January 24 December 21
Birth year 1941 1940
Occupation Singer Musician
Birthplace New Orleans Baltimore,Maryland
Year of Death - 1993
</table>
<tableCaption confidence="0.98929825">
Table 1: Two of 152 entries in the Biographic Database. Each
entry contains incomplete information about various celebrities.
Here, Aaron Neville’s birth state is missing, and Frank Zappa
could be equally well described as a guitarist or rock-star.
</tableCaption>
<bodyText confidence="0.999663611111111">
errors and thus the system is tested against wrong
answers.10 Second, the extractors might have re-
trieved information that was simply not present in
the database but nevertheless correct (e.g., some-
one’s occupation might be listed as writer and the
retrieved occupation might be novelist). Third, since
the retrieved targets were not normalized, there sys-
tem may have retrieved targets that were correct but
were not recognized (e.g., the database birthplace is
New York, and the system retrieves NY).
In testing, we rejected candidate targets that were
not present in our target set models Er. In some
cases, this resulted in the system being unable to find
the correct target for a particular relationship, since
it was not in the target set.
Before fusion (Section 3), we gathered all the
facts extracted by the system and graded them in iso-
lation. We present the per-extraction precision
</bodyText>
<subsubsectionHeader confidence="0.506027">
Pre-Fusion Precision =
# Correct Extracted Targets
# Total Extracted Targets
</subsubsectionHeader>
<bodyText confidence="0.990319833333333">
We also present the pseudo-recall, which is the av-
erage number of times per person a correct target
was extracted. It is difficult to calculate true re-
call without manual annotation of the entire corpus,
since it cannot be known for certain how many times
the document set contains the desired information.11
</bodyText>
<subsubsectionHeader confidence="0.441591">
# Correct Extracted Targets
Pre-Fusion Pseudo-Recall =
#People
</subsubsectionHeader>
<bodyText confidence="0.998638692307692">
The precision of each of the various extraction
methods is listed in Table 2. The data show that
on average the Rote method has the best precision,
10These deficiencies in testing also have implications for
training, since the models will be trained on annotated data that
has errors. The phenomenon of missing and inaccurate data
was most prevalent for occupation and birthplace relationships,
though it was observed for other relationships as well.
11It is insufficient to count all text matches as instances that
the system should extract. To obtain the true recall, it is nec-
essary to decide whether each sentence contains the desired re-
lationship, even in cases where the information is not what the
biographies have listed.
</bodyText>
<figure confidence="0.972713">
p A_2 9
H
p
A_2
A_2
H
9
9
</figure>
<page confidence="0.99535">
485
</page>
<table confidence="0.9996186">
Birthday Birth year Occupation Birthplace Year of Death Avg.
Rote .789 .355 .305 .510 .527 .497
NB+E .423 .361 .255 .217 .088 .269
CRF .509 .342 .219 .139 .267 .295
CRF+E .680 .654 .246 .357 .314 .450
</table>
<tableCaption confidence="0.9939905">
Table 2: Pre-Fusion Precision of extracted facts for various extraction systems, trained on 15 people each with 150 documents, and
tested on 137 people each with 150 documents.
</tableCaption>
<table confidence="0.99977">
Birthday Birth year Occupation Birthplace Year of Death Avg.
Rote 4.8 1.9 1.5 1.0 0.1 1.9
NB+E 9.6 11.5 20.3 11.3 0.7 10.9
CRF 3.0 16.3 31.1 10.7 3.2 12.9
CRF+E 6.8 9.9 3.2 3.6 1.4 5.0
</table>
<tableCaption confidence="0.999892">
Table 3: Pre-Fusion Pseudo-Recall of extract facts with the identical training/testing set-up as above.
</tableCaption>
<bodyText confidence="0.99980945">
while the NB+E extractor has the worst. Train-
ing the CRF with negative examples (CRF+E) gave
better precision in extracted information then train-
ing it without negative examples. Table 3 lists the
pseudo-recall or average number of correctly ex-
tracted targets per person. The results illustrate that
the Rote has the worst pseudo-recall, and the plain
CRF, trained without negative examples, has the best
pseudo-recall.
To test how the extraction precision changes as
more documents are retrieved from the ranked re-
sults from Google, we created retrieval sets of 1, 5,
15, 30, 75, and 150 documents per person and re-
peated the above experiments with the CRF+E ex-
tractor. The data in Figure 2 suggest that there is a
gradual drop in extraction precision throughout the
corpus, which may be caused by the fact that doc-
uments further down the retrieved list are less rele-
vant, and therefore less likely to contain the relevant
biographic data.
</bodyText>
<figure confidence="0.431895">
# Retrieved Documents per Person
</figure>
<figureCaption confidence="0.989579">
Figure 2: As more documents are retrieved per person, pre-
fusion precision drops.
</figureCaption>
<bodyText confidence="0.999582">
However, even though the extractor’s precision
drops, the data in Figure 3 indicate that there con-
tinue to be instances of the relevant biographic data.
</bodyText>
<figure confidence="0.441396">
# Retrieved Documents Per Person
</figure>
<figureCaption confidence="0.992716">
Figure 3: Pre-fusion pseudo-recall increases as more documents
are added.
</figureCaption>
<sectionHeader confidence="0.980537" genericHeader="method">
3 Cross-Document Information Fusion
</sectionHeader>
<bodyText confidence="0.999965421052632">
The per-extraction performance was presented in
Section 2, but the final task is to find the single cor-
rect target for each person.12 In this section, we ex-
amine two basic methodologies for combining can-
didate targets. Masterson and Kushmerick (2003)
propose Best which gives each candidate a
score equal to its highest confidence extraction:
C(x).13 We further consider
Voting, which counts the number of times each can-
didate x was extracted: Vote(x) = IC(x) &gt; 01.
Each of these methods ranks the candidate targets
by score and chooses the top-ranked one.
The experimental setup used in the fusion exper-
iments was the same as before: training on 15 peo-
ple, and testing on 137 people. However, the post-
fusion evaluation differs from the pre-fusion evalua-
tion. After fusion, the system returns one consensus
target for each person and thus the evaluation is on
the accuracy of those targets. That is, missing tar-
</bodyText>
<footnote confidence="0.9606032">
12This is a simplifying assumption, since there are many
cases where there might exist multiple possible values, e.g., a
person may be both a writer and a musician.
13C(x) is either the confidence estimate (NB+E) or the prob-
ability score (Rote,CRF,CRF+E).
</footnote>
<figure confidence="0.9870365">
Pre−Fusion Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
1
0 20 40 60 100 120
80 140 160
Occupation
Deathyear
Birthday
Birthyear
Birthplace
10
9
8
7
6
5
4
3
2
1
00 20 40 60 80 100 120 140 160
Pre−Fusion Pseudo−Recall
Birthyear
Birthday
Birthplace
Occupation
Deathyear
Best(x) = argmax
X
</figure>
<page confidence="0.989157">
486
</page>
<table confidence="0.9998026">
Best Vote
Rote .364 .450
NB+E .385 .588
CRF .513 .624
CRF+E .650 .678
</table>
<tableCaption confidence="0.928575">
Table 4: Average Accuracy of the Highest Confidence (Best)
and Most Frequent (Vote) across five extraction fields.
</tableCaption>
<table confidence="0.87528975">
gets are graded as wrong.14
# People with Correct Target
Post-Fusion Accuracy =
# People
</table>
<bodyText confidence="0.998264875">
Additionally, since the targets are ranked, we also
calculated the mean reciprocal rank (MRR).15 The
data in Table 4 show the average system perfor-
mance with the different fusion methods. Frequency
voting gave anywhere from a 2% to a 20% improve-
ment over picking the highest confidence candidate.
CRF+E (the CRF trained with negative examples)
was the highest performing system overall.
</bodyText>
<table confidence="0.999933538461538">
Birth Day
Fusion Accuracy Fusion MRR
Rote Vote .854 .877
NB+E Vote .854 .889
CRF Vote .650 .703
CRF+E Vote .883 .911
Birth year
Rote Vote .387 .497
NB+E Vote .778 .838
CRF Vote .796 .860
CRF+E Vote .869 .876
Occupation
Rote Vote .299 .405
NB+E Vote .642 .751
CRF Vote .606 .740
CRF+E Vote .423 .553
Birthplace
Rote Vote .321 .338
NB+E Vote .474 .586
CRF Vote .321 .476
CRF+E Vote .467 .560
Year of Death
Rote Vote .389 .389
NB+E Vote .194 .383
CRF .750 .840
CRF+E Vote .750 .827
</table>
<tableCaption confidence="0.7285914">
Table 5: Voting for information fusion, evaluated per person.
CRF+E has best average performance (67.8%).
Table 5 shows the results of using each of these
extractors to extract correct relationships from the
top 150 ranked documents downloaded from the
</tableCaption>
<footnote confidence="0.809546">
14For year of death, we only graded cases where the person
had died.
15The reciprocal rank = 1 / the rank of the correct target.
</footnote>
<bodyText confidence="0.9994804">
Web. CRF+E was a top performer in 3/5 of the
cases. In the other 2 cases, the NB+E was the most
successful, perhaps because NB+E’s increased re-
call was more useful than CRF+E’s improved pre-
cision.
</bodyText>
<subsectionHeader confidence="0.976247">
Retrieval Set Size and Performance
</subsectionHeader>
<bodyText confidence="0.9999665">
As with pre-fusion, we performed a set of exper-
iments with different retrieval set sizes and used
the CRF+E extraction system trained on 150 docu-
ments per person. The data in Figure 4 show that
performance improves as the retrieval set size in-
creases. Most of the gains come in the first 30 doc-
uments, where average performance increased from
14% (1 document) to 63% (30 documents). Increas-
ing the retrieval set size to 150 documents per person
yielded an additional 5% absolute improvement.
</bodyText>
<figure confidence="0.52119">
# Retrieved Documents Per Person
</figure>
<figureCaption confidence="0.9951685">
Figure 4: Fusion accuracy increases with more documents per
person
</figureCaption>
<bodyText confidence="0.99972025">
Post-fusion errors come from two major sources.
The first source is the misranking of correct relation-
ships. The second is the case where relevant infor-
mation is not retrieved at all, which we measure as
</bodyText>
<listItem confidence="0.480759">
# Missing Targets
# People
</listItem>
<bodyText confidence="0.999798666666667">
The data in Figure 5 suggest that the decrease in
missing targets is a significant contributing factor
to the improvement in performance with increased
document size. Missing targets were a major prob-
lem for Birthplace, constituting more than half the
errors (32% at 150 documents).
</bodyText>
<sectionHeader confidence="0.997567" genericHeader="method">
4 Cross-Field Bootstrapping
</sectionHeader>
<bodyText confidence="0.99987925">
Sections 2 and 3 presented methods for training sep-
arate extractors for particular relationships and for
doing fusion across multiple documents. In this sec-
tion, we leverage data interdependencies to improve
performance.
The method we propose is to bootstrap across
fields and use knowledge of one relationship to im-
prove performance on the extraction of another. For
</bodyText>
<figure confidence="0.96294515">
Post−Fusion Accuracy
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 20 40 60 80 100 120 140 160
Birthday
Deathyear
Birthplace
Birthyear
Occupation
Post-Fusion Missing =
487
# Retrieved Documents Per Person
</figure>
<figureCaption confidence="0.972563333333333">
Figure 5: Additional documents decrease the number of post-
fusion missing targets, targets which are never extracted in any
document.
</figureCaption>
<table confidence="0.999860153846154">
Birth year
Extraction Precision Fusion Accuracy
CRF .342 .797
+ birthday .472 .861
CRF+E .654 .869
+ birthday .809 .891
Occupation
Extraction Precision Fusion Accuracy
CRF .219 .606
+ birthday .217 .569
+ birth year(f) 21.9 .599
+ all .214 .591
CRF+E .246 .423
+ birthday .325 .577
+ birth year(f) .387 .672
+ all .382 .642
Birthplace
Extraction Precision Fusion Accuracy
CRF .139 .321
+ birthday .158 .372
+ birth year(f) .156 .350
CRF+E .357 .467
+ birthday .350 .474
+ birth year(f) .294 .350
+ occupation(f) .314 .354
+ all .362 .532
</table>
<tableCaption confidence="0.9320345">
Table 6: Performance of Cross-Field Bootstrapping Models.
(f) indicates that the best fused result was taken. birth year(f)
means birth years were annotated using the system that discov-
ered the most accurate birth years.
</tableCaption>
<bodyText confidence="0.999860545454546">
example, to extract birth year given knowledge of
the birthday, in training we mark up each hook cor-
pus D,, with the known birthday b : birthday(x, b)
and the target birth year y : birthyear(x, y) and
add an additional feature to the CRF that indicates
whether the birthday has been seen in the sentence.16
In testing, for each hook, we first find the birthday
using the methods presented in the previous sec-
tions, annotate the corpus with the extracted birth-
day, and then apply the birth year CRF (see Figure 6
next page).
</bodyText>
<footnote confidence="0.914072333333333">
16The CRF state model doesn’t change. When bootstrapping
from multiple fields, we add the conjunctions of the fields as
features.
</footnote>
<bodyText confidence="0.99984852173913">
Table 6 shows the effect of using this bootstrapped
data to estimate other fields. Based on the relative
performance of each of the individual extraction sys-
tems, we chose the following schedule for perform-
ing the bootstrapping: 1) Birthday, 2) Birth year, 3)
Occupation, 4) Birthplace. We tried adding in all
knowledge available to the system at each point in
the schedule.17 There are gains in accuracy for birth
year, occupation and birthplace by using cross-field
bootstrapping. The performance of the plain CRF+E
averaged across all five fields is 67.4%, while for the
best bootstrapped system it is 74.6%, a gain of 7%.
Doing bootstrapping in this way improves for
people whose information is already partially cor-
rect. As a result, the percentage of people who
have completely correct information improves to
37% from 13.8%, a gain of 24% over the non-
bootstrapped CRF+E system. Additionally, erro-
neous extractions do not hurt accuracy on extraction
of other fields. Performance in the bootstrapped sys-
tem for birthyear, occupation and birth place when
the birthday is wrong is almost the same as perfor-
mance in the non-bootstrapped system.
</bodyText>
<sectionHeader confidence="0.900184" genericHeader="method">
5 Training Set Size Reduction
</sectionHeader>
<bodyText confidence="0.999940315789474">
One of the results from Section 2 is that lower
ranked documents are less likely to contain the rel-
evant biographic information. While this does not
have an dramatic effect on the post-fusion accuracy
(which improves with more documents), it suggests
that training on a smaller corpus, with more relevant
documents and more sentences with the desired in-
formation, might lead to equivalent or improved per-
formance. In a final set of experiments we looked at
system performance when the extractor is trained on
fewer than 150 documents per person.
The data in Figure 7 show that training on 30 doc-
uments per person yields around the same perfor-
mance as training on 150 documents per person. Av-
erage performance when the system was trained on
30 documents per person is 70%, while average per-
formance when trained on 150 documents per per-
son is 68%. Most of this loss in performance comes
from losses in occupation, but the other relationships
</bodyText>
<footnote confidence="0.970043">
17This system has the extra knowledge of which fused
method is the best for each relationship. This was assessed by
inspection.
</footnote>
<figure confidence="0.993101833333333">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0 20 40 60 80 100 120 140 160
Birthplace
Occupation
Deathyear
Birthday
Birthyear
Post−Fusion Missing Targets
</figure>
<page confidence="0.912992">
488
</page>
<figureCaption confidence="0.987005666666667">
Figure 6: Cross-Field Bootstrapping: In step (1) The birthday,
December 21, is extracted and the text marked. In step 2, cooc-
currences with the discovered birthday make 1940 a better can-
didate for birthyear. In step (3), the discovered birthyear ap-
pears in contexts where the discovered birthday does not and
improves extraction of birth place.
</figureCaption>
<figure confidence="0.614524">
# Training Documents Per Person
</figure>
<figureCaption confidence="0.996115">
Figure 7: Fusion accuracy doesn’t improve with more than 30
training documents per person.
</figureCaption>
<bodyText confidence="0.991834692307692">
have either little or no gain from training on addi-
tional documents. There are two possible reasons
why more training data may not help, and even may
hurt performance.
One possibility is that higher ranked retrieved
documents are more likely to contain biographical
facts, while in later documents it is more likely that
automatically annotated training instances are in fact
false positives. That is, higher ranked documents are
cleaner training data. Pre-Fusion precision results
(Figure 8) support this hypothesis since it appears
that later instances are often contaminating earlier
models.
</bodyText>
<figure confidence="0.314725">
# Training Documents Per Person
</figure>
<figureCaption confidence="0.9966995">
Figure 8: Pre-Fusion precision shows slight drops with in-
creased training documents.
</figureCaption>
<bodyText confidence="0.99908125">
The data in Figure 9 suggest an alternate possibil-
ity that later documents also shift the prior toward
a model where it is less likely that a relationship is
observed as fewer targets are extracted.
</bodyText>
<note confidence="0.369509">
# Training Documents Per Person
</note>
<figureCaption confidence="0.9973695">
Figure 9: Pre-Fusion Pseudo-Recall also drops with increased
training documents.
</figureCaption>
<sectionHeader confidence="0.99994" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999825342105263">
The closest related work to the task of biographic
fact extraction was done by Cowie et al. (2000) and
Schiffman et al. (2001), who explore the problem of
biographic summarization.
There has been rather limited published
work in multi-document information extrac-
tion. The closest work to what we present here is
Masterson and Kushmerick (2003), who perform
multi-document information extraction trained on
manually annotated training data and use Best
Confidence to resolve each particular template slot.
In summarizarion, many systems have examined
the multi-document case. Notable systems are
SUMMONS (Radev and McKeown, 1998) and
RIPTIDE (White et al., 2001), which assume per-
fect extracted information and then perform closed
domain summarization. Barzilay et al. (1999) does
not explicitly extract facts, but instead picks out
relevant repeated elements and combines them to
obtain a summary which retains the semantics of
the original.
In recent question answering research, informa-
tion fusion has been used to combine multiple
candidate answers to form a consensus answer.
Clarke et al. (2001) use frequency of n-gram occur-
rence to pick answers for particular questions. An-
other example of answer fusion comes in (Brill et
al., 2001) which combines the output of multiple
question answering systems in order to rank an-
swers. Dalmas and Webber (2004) use a WordNet
cover heuristic to choose an appropriate location
from a large candidate set of answers.
There has been a considerable amount of work in
training information extraction systems from anno-
tated data since the mid-90s. The initial work in the
field used lexico-syntactic template patterns learned
using a variety of different empirical approaches
(Riloff and Schmelzenbach, 1998; Huffman, 1995;
</bodyText>
<figure confidence="0.998775836363636">
Frank Zappa was born on December 21.
1. Birthday
Zappa : December 21, 1940.
1. Birthday 2. Birthyear
Zappa was born in 1940 in Baltimore.
2. Birthyear 3. Birthplace
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.20 20 40 60 80 100 120 140 160
Post−Fusion Accuracy
Birthday
Birthyear
Deathyear
Occupation
Birthplace
Pre−Fusion Precision
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 20 40 60 80 100 120 140 160
Birthday
Birthyear
Birthplace
Deathyear
Occupation
Pre−Fusion Pseudo−Recall
11
10
9
7
6
5
4
2
0
8
3
1
0 20 40 60 80 100 120 140 160
Birthyear
Birthday
Occupation
Birthplace
Deathyear
</figure>
<page confidence="0.997798">
489
</page>
<bodyText confidence="0.999668285714286">
Soderland et al., 1995). Seymore et al. (1999) use
HMMs for information extraction and explore ways
to improve the learning process.
Nahm and Mooney (2002) suggest a method to
learn word-to-word relationships across fields by do-
ing data mining on information extraction results.
Prager et al. (2004) uses knowledge of birth year to
weed out candidate years of death that are impos-
sible. Using the CRF extractors in our data set,
this heuristic did not yield any improvement. More
distantly related work for multi-field extraction sug-
gests methods for combining information in graphi-
cal models across multiple extraction instances (Sut-
ton et al., 2004; Bunescu and Mooney, 2004) .
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999991928571429">
This paper has presented new experimental method-
ologies and results for cross-document information
fusion, focusing on the task of biographic fact ex-
traction and has proposed a new method for cross-
field bootstrapping. In particular, we have shown
that automatic annotation can be used effectively
to train statistical information extractors such Naive
Bayes and CRFs, and that CRF extraction accuracy
can be improved by 5% with a negative example
model. We looked at cross-document fusion and
demonstrated that voting outperforms choosing the
highest confidence extracted information by 2% to
20%. Finally, we introduced a cross-field bootstrap-
ping method that improved average accuracy by 7%.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999964457831325">
E. Agichtein and L. Gravano. 2000. Snowball: Extracting re-
lations from large plain-text collections. In Proceedings of
ICDL, pages 85–94.
R. Barzilay, K. R. McKeown, and M. Elhadad. 1999. Informa-
tion fusion in the context of multi-document summarization.
In Proceedings ofACL, pages 550–557.
E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2001. Data-
intensive question answering. In Proceedings of TREC,
pages 183–189.
S. Brin. 1998. Extracting patterns and relations from the world
wide web. In WebDB Workshop at 6th International Confer-
ence on Extending Database Technology, EDBT’98, pages
172–183.
R. Bunescu and R. Mooney. 2004. Collective information ex-
traction with relational markov networks. In Proceedings of
ACL, pages 438–445.
C. L. A. Clarke, G. V. Cormack, and T. R. Lynam. 2001. Ex-
ploiting redundancy in question answering. In Proceedings
of SIGIR, pages 358–365.
J. Cowie, S. Nirenburg, and H. Molina-Salgado. 2000. Gener-
ating personal profiles. In The International Conference On
MTAnd Multilingual NLP.
T. Dalmas and B. Webber. 2004. Information fusion
for answering factoid questions. In Proceedings of 2nd
CoLogNET-ElsNET Symposium. Questions and Answers:
Theoretical Perspectives.
D. Freitag and A. McCallum. 1999. Information extraction
with hmms and shrinkage. In Proceedings of the AAAI-99
Workshop on Machine Learning for Information Extraction,
pages 31–36.
S. B. Huffman. 1995. Learning information extraction patterns
from examples. In Working Notes of the IJCAI-95 Workshop
on New Approaches to Learning for Natural Language Pro-
cessing, pages 127–134.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings ofICML, pages 282–
289.
T. R. Leek. 1997. Information extraction using hidden markov
models. Master’s Thesis, UC San Diego.
D. Masterson and N. Kushmerick. 2003. Information ex-
traction from multi-document threads. In Proceedings of
ECML-2003: Workshop on Adaptive Text Extraction and
Mining, pages 34–41.
A. McCallum. 2002. Mallet: A machine learning for language
toolkit.
U. Nahm and R. Mooney. 2002. Text mining with information
extraction. In Proceedings of the AAAI 2220 Spring Sympo-
sium on Mining Answers from Texts and Knowledge Bases,
pages 60–67.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Question an-
swering by constraint satisfaction: Qa-by-dossier with con-
straints. In Proceedings ofACL, pages 574–581.
D. R. Radev and K. R. McKeown. 1998. Generating natural
language summaries from multiple on-line sources. Compu-
tational Linguistics, 24(3):469–500.
D. Ravichandran and E. Hovy. 2002. Learning surface text
patterns for a question answering system. In Proceedings of
ACL, pages 41–47.
E. Riloff and M. Schmelzenbach. 1998. An empirical ap-
proach to conceptual case frame acquisition. In Proceedings
of WVLC, pages 49–56.
E. Riloff. 1996. Automatically Generating Extraction Patterns
from Untagged Text. In Proceedings of AAAI, pages 1044–
1049.
B. Schiffman, I. Mani, and K. J. Concepcion. 2001. Produc-
ing biographical summaries: Combining linguistic knowl-
edge with corpus statistics. In Proceedings of ACL, pages
450–457.
K. Seymore, A. McCallum, and R. Rosenfeld. 1999. Learning
hidden markov model structure for information extraction.
In AAAI’99 Workshop on Machine Learning for Information
Extraction, pages 37–42.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995.
CRYSTAL: Inducing a conceptual dictionary. In Proceed-
ings ofIJCAI, pages 1314–1319.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004. Dy-
namic conditional random fields: factorize probabilistic
models for labeling and segmenting sequence data. In Pro-
ceedings ofICML.
M. White, T. Korelsky, C. Cardie, V. Ng, D. Pierce, and
K. Wagstaff. 2001. Multi-document summarization via in-
formation extraction. In Proceedings of HLT.
</reference>
<page confidence="0.998271">
490
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950482">
<title confidence="0.999937">Multi-Field Information Extraction and Cross-Document Fusion</title>
<author confidence="0.999838">S Mann Yarowsky</author>
<affiliation confidence="0.999847">Department of Computer Science The Johns Hopkins University</affiliation>
<address confidence="0.999901">Baltimore, MD 21218 USA</address>
<abstract confidence="0.996191">In this paper, we examine the task of extracting a set of biographic facts about target individuals from a collection of Web pages. We automatically annotate training text with positive and negative examples of fact extractions and train Rote, Naive Bayes, and Conditional Random Field extraction models for fact extraction from individual Web pages. We then propose and evaluate methods for fusing the extracted information across documents to return a consensus answer. A novel cross-field bootstrapping method leverages data interdependencies to yield improved performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>L Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of ICDL,</booktitle>
<pages>85--94</pages>
<contexts>
<context position="3358" citStr="Agichtein and Gravano (2000)" startWordPosition="516" endWordPosition="519"> per-field accuracy by 7%. We demonstrate that a small training set with only the most relevant documents can be as effective as a larger training set with additional, less relevant documents (Section 5). 2 Training by Automatic Annotation Typically, statistical extraction systems (such as HMMs and CRFs) are trained using hand-annotated data. Annotating the necessary data by hand is timeconsuming and brittle, since it may require largescale re-annotation when the annotation scheme changes. For the special case of Rote extractors, a more attractive alternative has been proposed by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). 483 Proceedings of the 43rd Annual Meeting of the ACL, pages 483–490, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics Essentially, for any text snippet of the form A1pA2qA3, these systems estimate the probability that a relationship r(p, q) holds between entities p and q, given the interstitial context, as2 P(r(p, q) |pA2q) = P(r(p, q) |pA2q) E x,yET c(xA2y) = E x c(xA2) That is, the probability of a relationship r(p, q) is the number of times that pattern xA2y predicts any relationship r(x, y) in the training set T. c(.) is the count.</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>E. Agichtein and L. Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of ICDL, pages 85–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K R McKeown</author>
<author>M Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>550--557</pages>
<contexts>
<context position="25467" citStr="Barzilay et al. (1999)" startWordPosition="4185" endWordPosition="4188">of biographic summarization. There has been rather limited published work in multi-document information extraction. The closest work to what we present here is Masterson and Kushmerick (2003), who perform multi-document information extraction trained on manually annotated training data and use Best Confidence to resolve each particular template slot. In summarizarion, many systems have examined the multi-document case. Notable systems are SUMMONS (Radev and McKeown, 1998) and RIPTIDE (White et al., 2001), which assume perfect extracted information and then perform closed domain summarization. Barzilay et al. (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. In recent question answering research, information fusion has been used to combine multiple candidate answers to form a consensus answer. Clarke et al. (2001) use frequency of n-gram occurrence to pick answers for particular questions. Another example of answer fusion comes in (Brill et al., 2001) which combines the output of multiple question answering systems in order to rank answers. Dalmas and Webber (2004) use a WordNet cov</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>R. Barzilay, K. R. McKeown, and M. Elhadad. 1999. Information fusion in the context of multi-document summarization. In Proceedings ofACL, pages 550–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Lin</author>
<author>M Banko</author>
<author>S Dumais</author>
<author>A Ng</author>
</authors>
<title>Dataintensive question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of TREC,</booktitle>
<pages>183--189</pages>
<contexts>
<context position="25933" citStr="Brill et al., 2001" startWordPosition="4260" endWordPosition="4263">, 1998) and RIPTIDE (White et al., 2001), which assume perfect extracted information and then perform closed domain summarization. Barzilay et al. (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. In recent question answering research, information fusion has been used to combine multiple candidate answers to form a consensus answer. Clarke et al. (2001) use frequency of n-gram occurrence to pick answers for particular questions. Another example of answer fusion comes in (Brill et al., 2001) which combines the output of multiple question answering systems in order to rank answers. Dalmas and Webber (2004) use a WordNet cover heuristic to choose an appropriate location from a large candidate set of answers. There has been a considerable amount of work in training information extraction systems from annotated data since the mid-90s. The initial work in the field used lexico-syntactic template patterns learned using a variety of different empirical approaches (Riloff and Schmelzenbach, 1998; Huffman, 1995; Frank Zappa was born on December 21. 1. Birthday Zappa : December 21, 1940. 1</context>
</contexts>
<marker>Brill, Lin, Banko, Dumais, Ng, 2001</marker>
<rawString>E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2001. Dataintensive question answering. In Proceedings of TREC, pages 183–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
</authors>
<title>Extracting patterns and relations from the world wide web.</title>
<date>1998</date>
<booktitle>In WebDB Workshop at 6th International Conference on Extending Database Technology, EDBT’98,</booktitle>
<pages>172--183</pages>
<contexts>
<context position="3328" citStr="Brin (1998)" startWordPosition="514" endWordPosition="515">hich improves per-field accuracy by 7%. We demonstrate that a small training set with only the most relevant documents can be as effective as a larger training set with additional, less relevant documents (Section 5). 2 Training by Automatic Annotation Typically, statistical extraction systems (such as HMMs and CRFs) are trained using hand-annotated data. Annotating the necessary data by hand is timeconsuming and brittle, since it may require largescale re-annotation when the annotation scheme changes. For the special case of Rote extractors, a more attractive alternative has been proposed by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). 483 Proceedings of the 43rd Annual Meeting of the ACL, pages 483–490, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics Essentially, for any text snippet of the form A1pA2qA3, these systems estimate the probability that a relationship r(p, q) holds between entities p and q, given the interstitial context, as2 P(r(p, q) |pA2q) = P(r(p, q) |pA2q) E x,yET c(xA2y) = E x c(xA2) That is, the probability of a relationship r(p, q) is the number of times that pattern xA2y predicts any relationship r(x, y) in the trai</context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>S. Brin. 1998. Extracting patterns and relations from the world wide web. In WebDB Workshop at 6th International Conference on Extending Database Technology, EDBT’98, pages 172–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>R Mooney</author>
</authors>
<title>Collective information extraction with relational markov networks.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>438--445</pages>
<contexts>
<context position="27703" citStr="Bunescu and Mooney, 2004" startWordPosition="4554" endWordPosition="4557">MMs for information extraction and explore ways to improve the learning process. Nahm and Mooney (2002) suggest a method to learn word-to-word relationships across fields by doing data mining on information extraction results. Prager et al. (2004) uses knowledge of birth year to weed out candidate years of death that are impossible. Using the CRF extractors in our data set, this heuristic did not yield any improvement. More distantly related work for multi-field extraction suggests methods for combining information in graphical models across multiple extraction instances (Sutton et al., 2004; Bunescu and Mooney, 2004) . 7 Conclusion This paper has presented new experimental methodologies and results for cross-document information fusion, focusing on the task of biographic fact extraction and has proposed a new method for crossfield bootstrapping. In particular, we have shown that automatic annotation can be used effectively to train statistical information extractors such Naive Bayes and CRFs, and that CRF extraction accuracy can be improved by 5% with a negative example model. We looked at cross-document fusion and demonstrated that voting outperforms choosing the highest confidence extracted information </context>
</contexts>
<marker>Bunescu, Mooney, 2004</marker>
<rawString>R. Bunescu and R. Mooney. 2004. Collective information extraction with relational markov networks. In Proceedings of ACL, pages 438–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L A Clarke</author>
<author>G V Cormack</author>
<author>T R Lynam</author>
</authors>
<title>Exploiting redundancy in question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>358--365</pages>
<contexts>
<context position="25793" citStr="Clarke et al. (2001)" startWordPosition="4236" endWordPosition="4239">articular template slot. In summarizarion, many systems have examined the multi-document case. Notable systems are SUMMONS (Radev and McKeown, 1998) and RIPTIDE (White et al., 2001), which assume perfect extracted information and then perform closed domain summarization. Barzilay et al. (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. In recent question answering research, information fusion has been used to combine multiple candidate answers to form a consensus answer. Clarke et al. (2001) use frequency of n-gram occurrence to pick answers for particular questions. Another example of answer fusion comes in (Brill et al., 2001) which combines the output of multiple question answering systems in order to rank answers. Dalmas and Webber (2004) use a WordNet cover heuristic to choose an appropriate location from a large candidate set of answers. There has been a considerable amount of work in training information extraction systems from annotated data since the mid-90s. The initial work in the field used lexico-syntactic template patterns learned using a variety of different empiri</context>
</contexts>
<marker>Clarke, Cormack, Lynam, 2001</marker>
<rawString>C. L. A. Clarke, G. V. Cormack, and T. R. Lynam. 2001. Exploiting redundancy in question answering. In Proceedings of SIGIR, pages 358–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cowie</author>
<author>S Nirenburg</author>
<author>H Molina-Salgado</author>
</authors>
<title>Generating personal profiles.</title>
<date>2000</date>
<booktitle>In The International Conference On MTAnd Multilingual NLP.</booktitle>
<contexts>
<context position="24791" citStr="Cowie et al. (2000)" startWordPosition="4088" endWordPosition="4091">ince it appears that later instances are often contaminating earlier models. # Training Documents Per Person Figure 8: Pre-Fusion precision shows slight drops with increased training documents. The data in Figure 9 suggest an alternate possibility that later documents also shift the prior toward a model where it is less likely that a relationship is observed as fewer targets are extracted. # Training Documents Per Person Figure 9: Pre-Fusion Pseudo-Recall also drops with increased training documents. 6 Related Work The closest related work to the task of biographic fact extraction was done by Cowie et al. (2000) and Schiffman et al. (2001), who explore the problem of biographic summarization. There has been rather limited published work in multi-document information extraction. The closest work to what we present here is Masterson and Kushmerick (2003), who perform multi-document information extraction trained on manually annotated training data and use Best Confidence to resolve each particular template slot. In summarizarion, many systems have examined the multi-document case. Notable systems are SUMMONS (Radev and McKeown, 1998) and RIPTIDE (White et al., 2001), which assume perfect extracted info</context>
</contexts>
<marker>Cowie, Nirenburg, Molina-Salgado, 2000</marker>
<rawString>J. Cowie, S. Nirenburg, and H. Molina-Salgado. 2000. Generating personal profiles. In The International Conference On MTAnd Multilingual NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dalmas</author>
<author>B Webber</author>
</authors>
<title>Information fusion for answering factoid questions.</title>
<date>2004</date>
<booktitle>In Proceedings of 2nd CoLogNET-ElsNET Symposium. Questions and Answers: Theoretical Perspectives.</booktitle>
<contexts>
<context position="26049" citStr="Dalmas and Webber (2004)" startWordPosition="4279" endWordPosition="4282">main summarization. Barzilay et al. (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. In recent question answering research, information fusion has been used to combine multiple candidate answers to form a consensus answer. Clarke et al. (2001) use frequency of n-gram occurrence to pick answers for particular questions. Another example of answer fusion comes in (Brill et al., 2001) which combines the output of multiple question answering systems in order to rank answers. Dalmas and Webber (2004) use a WordNet cover heuristic to choose an appropriate location from a large candidate set of answers. There has been a considerable amount of work in training information extraction systems from annotated data since the mid-90s. The initial work in the field used lexico-syntactic template patterns learned using a variety of different empirical approaches (Riloff and Schmelzenbach, 1998; Huffman, 1995; Frank Zappa was born on December 21. 1. Birthday Zappa : December 21, 1940. 1. Birthday 2. Birthyear Zappa was born in 1940 in Baltimore. 2. Birthyear 3. Birthplace 0.9 0.8 0.7 0.6 0.5 0.4 0.3 </context>
</contexts>
<marker>Dalmas, Webber, 2004</marker>
<rawString>T. Dalmas and B. Webber. 2004. Information fusion for answering factoid questions. In Proceedings of 2nd CoLogNET-ElsNET Symposium. Questions and Answers: Theoretical Perspectives.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
<author>A McCallum</author>
</authors>
<title>Information extraction with hmms and shrinkage.</title>
<date>1999</date>
<booktitle>In Proceedings of the AAAI-99 Workshop on Machine Learning for Information Extraction,</booktitle>
<pages>31--36</pages>
<contexts>
<context position="1034" citStr="Freitag and McCallum, 1999" startWordPosition="145" endWordPosition="148">ative examples of fact extractions and train Rote, Naive Bayes, and Conditional Random Field extraction models for fact extraction from individual Web pages. We then propose and evaluate methods for fusing the extracted information across documents to return a consensus answer. A novel cross-field bootstrapping method leverages data interdependencies to yield improved performance. 1 Introduction Much recent statistical information extraction research has applied graphical models to extract information from one particular document after training on a large corpus of annotated data (Leek, 1997; Freitag and McCallum, 1999).1 Such systems are widely applicable, yet there remain many information extraction tasks that are not readily amenable to these methods. Annotated data required for training statistical extraction systems is sometimes unavailable, while there are examples of the desired information. Further, the goal may be to find a few interrelated pieces of information that are stated multiple times in a set of documents. Here, we investigate one task that meets the above criteria. Given the name of a celebrity such as 1Alternatively, Riloff(1996) trains on in-domain and out-of-domain texts and then has a </context>
</contexts>
<marker>Freitag, McCallum, 1999</marker>
<rawString>D. Freitag and A. McCallum. 1999. Information extraction with hmms and shrinkage. In Proceedings of the AAAI-99 Workshop on Machine Learning for Information Extraction, pages 31–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Huffman</author>
</authors>
<title>Learning information extraction patterns from examples.</title>
<date>1995</date>
<booktitle>In Working Notes of the IJCAI-95 Workshop on New Approaches to Learning for Natural Language Processing,</booktitle>
<pages>127--134</pages>
<contexts>
<context position="1670" citStr="Huffman (1995)" startWordPosition="248" endWordPosition="249">dely applicable, yet there remain many information extraction tasks that are not readily amenable to these methods. Annotated data required for training statistical extraction systems is sometimes unavailable, while there are examples of the desired information. Further, the goal may be to find a few interrelated pieces of information that are stated multiple times in a set of documents. Here, we investigate one task that meets the above criteria. Given the name of a celebrity such as 1Alternatively, Riloff(1996) trains on in-domain and out-of-domain texts and then has a human filtering step. Huffman (1995) proposes a method to train a different type of extraction system by example. “Frank Zappa”, our goal is to extract a set of biographic facts (e.g., birthdate, birth place and occupation) about that person from documents on the Web. First, we describe a general method of automatic annotation for training from positive and negative examples and use the method to train Rote, Naive Bayes, and Conditional Random Field models (Section 2). We then examine how multiple extractions can be combined to form one consensus answer (Section 3). We compare fusion methods and show that frequency voting outper</context>
<context position="26454" citStr="Huffman, 1995" startWordPosition="4342" endWordPosition="4343">ers for particular questions. Another example of answer fusion comes in (Brill et al., 2001) which combines the output of multiple question answering systems in order to rank answers. Dalmas and Webber (2004) use a WordNet cover heuristic to choose an appropriate location from a large candidate set of answers. There has been a considerable amount of work in training information extraction systems from annotated data since the mid-90s. The initial work in the field used lexico-syntactic template patterns learned using a variety of different empirical approaches (Riloff and Schmelzenbach, 1998; Huffman, 1995; Frank Zappa was born on December 21. 1. Birthday Zappa : December 21, 1940. 1. Birthday 2. Birthyear Zappa was born in 1940 in Baltimore. 2. Birthyear 3. Birthplace 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.20 20 40 60 80 100 120 140 160 Post−Fusion Accuracy Birthday Birthyear Deathyear Occupation Birthplace Pre−Fusion Precision 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 20 40 60 80 100 120 140 160 Birthday Birthyear Birthplace Deathyear Occupation Pre−Fusion Pseudo−Recall 11 10 9 7 6 5 4 2 0 8 3 1 0 20 40 60 80 100 120 140 160 Birthyear Birthday Occupation Birthplace Deathyear 489 Soderland et al., 1995). Sey</context>
</contexts>
<marker>Huffman, 1995</marker>
<rawString>S. B. Huffman. 1995. Learning information extraction patterns from examples. In Working Notes of the IJCAI-95 Workshop on New Approaches to Learning for Natural Language Processing, pages 127–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="7016" citStr="Lafferty et al., 2001" startWordPosition="1147" endWordPosition="1150">In training, for each relationship r(p, q), we markup the hook p, the target q, and all spurious targets (q E {Er − q}) which provide negative examples. Targets can then be chosen with the following confidence estimate P(A2 |r(p, q)) CNB+E(q) = logP(A2 |r(p,q)) We call this NB+E in the following experiments. The above process describes a general method for automatically annotating a corpus with positive and negative examples, and this corpus can be used to train statistical models that rely on annotated data.8 In this paper, we test automatic annotation using Conditional Random Fields (CRFs) (Lafferty et al., 2001) which have achieved high performance for information extraction. CRFs are undirected graphical models that estimate the conditional probability of a state sequence given an output sequence T P(s |o) = Z exp �J: J: Akfk (st−1, st, o, t) I t=1 k / 6T stands in for all other possible relationships (including no relationship) between p and q. P(A2 |r(p, q)) is estimated as P(A2 |r(p, q)) is, except with spurious targets. 7e.g., E6irthyear = {\d\d\d\d}. This is the only source of human knowledge put into the system and required only around 4 hours of effort, less effort than annotating an entire c</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings ofICML, pages 282– 289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T R Leek</author>
</authors>
<title>Information extraction using hidden markov models. Master’s Thesis,</title>
<date>1997</date>
<location>UC San Diego.</location>
<contexts>
<context position="1005" citStr="Leek, 1997" startWordPosition="143" endWordPosition="144">tive and negative examples of fact extractions and train Rote, Naive Bayes, and Conditional Random Field extraction models for fact extraction from individual Web pages. We then propose and evaluate methods for fusing the extracted information across documents to return a consensus answer. A novel cross-field bootstrapping method leverages data interdependencies to yield improved performance. 1 Introduction Much recent statistical information extraction research has applied graphical models to extract information from one particular document after training on a large corpus of annotated data (Leek, 1997; Freitag and McCallum, 1999).1 Such systems are widely applicable, yet there remain many information extraction tasks that are not readily amenable to these methods. Annotated data required for training statistical extraction systems is sometimes unavailable, while there are examples of the desired information. Further, the goal may be to find a few interrelated pieces of information that are stated multiple times in a set of documents. Here, we investigate one task that meets the above criteria. Given the name of a celebrity such as 1Alternatively, Riloff(1996) trains on in-domain and out-of</context>
</contexts>
<marker>Leek, 1997</marker>
<rawString>T. R. Leek. 1997. Information extraction using hidden markov models. Master’s Thesis, UC San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Masterson</author>
<author>N Kushmerick</author>
</authors>
<title>Information extraction from multi-document threads.</title>
<date>2003</date>
<booktitle>In Proceedings of ECML-2003: Workshop on Adaptive Text Extraction and Mining,</booktitle>
<pages>34--41</pages>
<contexts>
<context position="14582" citStr="Masterson and Kushmerick (2003)" startWordPosition="2378" endWordPosition="2381">son Figure 2: As more documents are retrieved per person, prefusion precision drops. However, even though the extractor’s precision drops, the data in Figure 3 indicate that there continue to be instances of the relevant biographic data. # Retrieved Documents Per Person Figure 3: Pre-fusion pseudo-recall increases as more documents are added. 3 Cross-Document Information Fusion The per-extraction performance was presented in Section 2, but the final task is to find the single correct target for each person.12 In this section, we examine two basic methodologies for combining candidate targets. Masterson and Kushmerick (2003) propose Best which gives each candidate a score equal to its highest confidence extraction: C(x).13 We further consider Voting, which counts the number of times each candidate x was extracted: Vote(x) = IC(x) &gt; 01. Each of these methods ranks the candidate targets by score and chooses the top-ranked one. The experimental setup used in the fusion experiments was the same as before: training on 15 people, and testing on 137 people. However, the postfusion evaluation differs from the pre-fusion evaluation. After fusion, the system returns one consensus target for each person and thus the evaluat</context>
<context position="25036" citStr="Masterson and Kushmerick (2003)" startWordPosition="4125" endWordPosition="4128">te possibility that later documents also shift the prior toward a model where it is less likely that a relationship is observed as fewer targets are extracted. # Training Documents Per Person Figure 9: Pre-Fusion Pseudo-Recall also drops with increased training documents. 6 Related Work The closest related work to the task of biographic fact extraction was done by Cowie et al. (2000) and Schiffman et al. (2001), who explore the problem of biographic summarization. There has been rather limited published work in multi-document information extraction. The closest work to what we present here is Masterson and Kushmerick (2003), who perform multi-document information extraction trained on manually annotated training data and use Best Confidence to resolve each particular template slot. In summarizarion, many systems have examined the multi-document case. Notable systems are SUMMONS (Radev and McKeown, 1998) and RIPTIDE (White et al., 2001), which assume perfect extracted information and then perform closed domain summarization. Barzilay et al. (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. I</context>
</contexts>
<marker>Masterson, Kushmerick, 2003</marker>
<rawString>D. Masterson and N. Kushmerick. 2003. Information extraction from multi-document threads. In Proceedings of ECML-2003: Workshop on Adaptive Text Extraction and Mining, pages 34–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<contexts>
<context position="8034" citStr="McCallum, 2002" startWordPosition="1315" endWordPosition="1316"> spurious targets. 7e.g., E6irthyear = {\d\d\d\d}. This is the only source of human knowledge put into the system and required only around 4 hours of effort, less effort than annotating an entire corpus or writing information extraction rules. 8This corpus markup gives automatic annotation that yields noisier training data than manual annotation would. 484 Figure 1: CRF state-transition graphs for extracting a relationship r(p, q) from a sentence pA2q. Left: CRF Extraction with a background model (B). Right: CRF+E As before but with spurious target prediction (pA2q). We use the Mallet system (McCallum, 2002) for training and evaluation of the CRFs. In order to examine the improvement by using negative examples, we train CRFs with two topologies (Figure 1). The first, CRF, models the target relationship and background sequences and is trained on a corpus where targets (positive examples) are annotated. The second, CRF+E, models the target relationship, spurious targets and background sequences, and it is trained on a corpus where targets (positive examples) as well as spurious targets (negative examples) are annotated. Experimental Results To test the performance of the different extractors, we co</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>A. McCallum. 2002. Mallet: A machine learning for language toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Nahm</author>
<author>R Mooney</author>
</authors>
<title>Text mining with information extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the AAAI 2220 Spring Symposium on Mining Answers from Texts and Knowledge Bases,</booktitle>
<pages>60--67</pages>
<contexts>
<context position="27181" citStr="Nahm and Mooney (2002)" startWordPosition="4471" endWordPosition="4474">ppa was born in 1940 in Baltimore. 2. Birthyear 3. Birthplace 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.20 20 40 60 80 100 120 140 160 Post−Fusion Accuracy Birthday Birthyear Deathyear Occupation Birthplace Pre−Fusion Precision 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 20 40 60 80 100 120 140 160 Birthday Birthyear Birthplace Deathyear Occupation Pre−Fusion Pseudo−Recall 11 10 9 7 6 5 4 2 0 8 3 1 0 20 40 60 80 100 120 140 160 Birthyear Birthday Occupation Birthplace Deathyear 489 Soderland et al., 1995). Seymore et al. (1999) use HMMs for information extraction and explore ways to improve the learning process. Nahm and Mooney (2002) suggest a method to learn word-to-word relationships across fields by doing data mining on information extraction results. Prager et al. (2004) uses knowledge of birth year to weed out candidate years of death that are impossible. Using the CRF extractors in our data set, this heuristic did not yield any improvement. More distantly related work for multi-field extraction suggests methods for combining information in graphical models across multiple extraction instances (Sutton et al., 2004; Bunescu and Mooney, 2004) . 7 Conclusion This paper has presented new experimental methodologies and re</context>
</contexts>
<marker>Nahm, Mooney, 2002</marker>
<rawString>U. Nahm and R. Mooney. 2002. Text mining with information extraction. In Proceedings of the AAAI 2220 Spring Symposium on Mining Answers from Texts and Knowledge Bases, pages 60–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
</authors>
<title>Question answering by constraint satisfaction: Qa-by-dossier with constraints.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>574--581</pages>
<contexts>
<context position="27325" citStr="Prager et al. (2004)" startWordPosition="4493" endWordPosition="4496">rthday Birthyear Deathyear Occupation Birthplace Pre−Fusion Precision 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 20 40 60 80 100 120 140 160 Birthday Birthyear Birthplace Deathyear Occupation Pre−Fusion Pseudo−Recall 11 10 9 7 6 5 4 2 0 8 3 1 0 20 40 60 80 100 120 140 160 Birthyear Birthday Occupation Birthplace Deathyear 489 Soderland et al., 1995). Seymore et al. (1999) use HMMs for information extraction and explore ways to improve the learning process. Nahm and Mooney (2002) suggest a method to learn word-to-word relationships across fields by doing data mining on information extraction results. Prager et al. (2004) uses knowledge of birth year to weed out candidate years of death that are impossible. Using the CRF extractors in our data set, this heuristic did not yield any improvement. More distantly related work for multi-field extraction suggests methods for combining information in graphical models across multiple extraction instances (Sutton et al., 2004; Bunescu and Mooney, 2004) . 7 Conclusion This paper has presented new experimental methodologies and results for cross-document information fusion, focusing on the task of biographic fact extraction and has proposed a new method for crossfield boo</context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, 2004</marker>
<rawString>J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Question answering by constraint satisfaction: Qa-by-dossier with constraints. In Proceedings ofACL, pages 574–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>K R McKeown</author>
</authors>
<title>Generating natural language summaries from multiple on-line sources.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="25321" citStr="Radev and McKeown, 1998" startWordPosition="4163" endWordPosition="4166">closest related work to the task of biographic fact extraction was done by Cowie et al. (2000) and Schiffman et al. (2001), who explore the problem of biographic summarization. There has been rather limited published work in multi-document information extraction. The closest work to what we present here is Masterson and Kushmerick (2003), who perform multi-document information extraction trained on manually annotated training data and use Best Confidence to resolve each particular template slot. In summarizarion, many systems have examined the multi-document case. Notable systems are SUMMONS (Radev and McKeown, 1998) and RIPTIDE (White et al., 2001), which assume perfect extracted information and then perform closed domain summarization. Barzilay et al. (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. In recent question answering research, information fusion has been used to combine multiple candidate answers to form a consensus answer. Clarke et al. (2001) use frequency of n-gram occurrence to pick answers for particular questions. Another example of answer fusion comes in (Brill e</context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>D. R. Radev and K. R. McKeown. 1998. Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24(3):469–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>41--47</pages>
<contexts>
<context position="3392" citStr="Ravichandran and Hovy (2002)" startWordPosition="521" endWordPosition="524">nstrate that a small training set with only the most relevant documents can be as effective as a larger training set with additional, less relevant documents (Section 5). 2 Training by Automatic Annotation Typically, statistical extraction systems (such as HMMs and CRFs) are trained using hand-annotated data. Annotating the necessary data by hand is timeconsuming and brittle, since it may require largescale re-annotation when the annotation scheme changes. For the special case of Rote extractors, a more attractive alternative has been proposed by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). 483 Proceedings of the 43rd Annual Meeting of the ACL, pages 483–490, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics Essentially, for any text snippet of the form A1pA2qA3, these systems estimate the probability that a relationship r(p, q) holds between entities p and q, given the interstitial context, as2 P(r(p, q) |pA2q) = P(r(p, q) |pA2q) E x,yET c(xA2y) = E x c(xA2) That is, the probability of a relationship r(p, q) is the number of times that pattern xA2y predicts any relationship r(x, y) in the training set T. c(.) is the count. We will refer to x as the hook3 a</context>
<context position="5081" citStr="Ravichandran and Hovy, 2002" startWordPosition="814" endWordPosition="817">ls with positive examples, for example a Naive Bayes (NB) unigram model. In this model, instead of looking for an exact A2 pattern as above, each individual word in the pattern A2 is used to predict the presence of a relationship. P(r(p, q) |pA2q) ocP(pA2q |r(p, q))P(r(p, q)) =P(A2 |r(p, q)) 11 = P(a |r(p, q)) aEA2 We perform add-lambda smoothing for out-ofvocabulary words and thus assign a positive probability to any sequence. As before, a set of relevant 2The above Rote models also condition on the preceding and trailing words, for simplicity we only model interstitial words A2. 3Following (Ravichandran and Hovy, 2002). 4In the following experiments we assume that there is one main object of interest p, for whom we want to find certain pieces of information r(p, q), where r denotes the type of relationship (e.g., birthday) and q is a value (e.g., May 20th). We require one hook corpus for each hook, not a separate one for each relationship. 5Having a functional constraint `dq =� q, r(p, q) makes this estimate much more reliable, but it is possible to use this method of estimation even when this constraint does not hold. documents is downloaded for each particular hook. Then every hook and target is annotated</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>D. Ravichandran and E. Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of ACL, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>M Schmelzenbach</author>
</authors>
<title>An empirical approach to conceptual case frame acquisition.</title>
<date>1998</date>
<booktitle>In Proceedings of WVLC,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="26439" citStr="Riloff and Schmelzenbach, 1998" startWordPosition="4338" endWordPosition="4341">f n-gram occurrence to pick answers for particular questions. Another example of answer fusion comes in (Brill et al., 2001) which combines the output of multiple question answering systems in order to rank answers. Dalmas and Webber (2004) use a WordNet cover heuristic to choose an appropriate location from a large candidate set of answers. There has been a considerable amount of work in training information extraction systems from annotated data since the mid-90s. The initial work in the field used lexico-syntactic template patterns learned using a variety of different empirical approaches (Riloff and Schmelzenbach, 1998; Huffman, 1995; Frank Zappa was born on December 21. 1. Birthday Zappa : December 21, 1940. 1. Birthday 2. Birthyear Zappa was born in 1940 in Baltimore. 2. Birthyear 3. Birthplace 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.20 20 40 60 80 100 120 140 160 Post−Fusion Accuracy Birthday Birthyear Deathyear Occupation Birthplace Pre−Fusion Precision 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 20 40 60 80 100 120 140 160 Birthday Birthyear Birthplace Deathyear Occupation Pre−Fusion Pseudo−Recall 11 10 9 7 6 5 4 2 0 8 3 1 0 20 40 60 80 100 120 140 160 Birthyear Birthday Occupation Birthplace Deathyear 489 Soderland et </context>
</contexts>
<marker>Riloff, Schmelzenbach, 1998</marker>
<rawString>E. Riloff and M. Schmelzenbach. 1998. An empirical approach to conceptual case frame acquisition. In Proceedings of WVLC, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically Generating Extraction Patterns from Untagged Text.</title>
<date>1996</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>1044--1049</pages>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proceedings of AAAI, pages 1044– 1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Schiffman</author>
<author>I Mani</author>
<author>K J Concepcion</author>
</authors>
<title>Producing biographical summaries: Combining linguistic knowledge with corpus statistics.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>450--457</pages>
<contexts>
<context position="24819" citStr="Schiffman et al. (2001)" startWordPosition="4093" endWordPosition="4096">er instances are often contaminating earlier models. # Training Documents Per Person Figure 8: Pre-Fusion precision shows slight drops with increased training documents. The data in Figure 9 suggest an alternate possibility that later documents also shift the prior toward a model where it is less likely that a relationship is observed as fewer targets are extracted. # Training Documents Per Person Figure 9: Pre-Fusion Pseudo-Recall also drops with increased training documents. 6 Related Work The closest related work to the task of biographic fact extraction was done by Cowie et al. (2000) and Schiffman et al. (2001), who explore the problem of biographic summarization. There has been rather limited published work in multi-document information extraction. The closest work to what we present here is Masterson and Kushmerick (2003), who perform multi-document information extraction trained on manually annotated training data and use Best Confidence to resolve each particular template slot. In summarizarion, many systems have examined the multi-document case. Notable systems are SUMMONS (Radev and McKeown, 1998) and RIPTIDE (White et al., 2001), which assume perfect extracted information and then perform clo</context>
</contexts>
<marker>Schiffman, Mani, Concepcion, 2001</marker>
<rawString>B. Schiffman, I. Mani, and K. J. Concepcion. 2001. Producing biographical summaries: Combining linguistic knowledge with corpus statistics. In Proceedings of ACL, pages 450–457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Seymore</author>
<author>A McCallum</author>
<author>R Rosenfeld</author>
</authors>
<title>Learning hidden markov model structure for information extraction.</title>
<date>1999</date>
<booktitle>In AAAI’99 Workshop on Machine Learning for Information Extraction,</booktitle>
<pages>37--42</pages>
<contexts>
<context position="27072" citStr="Seymore et al. (1999)" startWordPosition="4454" endWordPosition="4457">995; Frank Zappa was born on December 21. 1. Birthday Zappa : December 21, 1940. 1. Birthday 2. Birthyear Zappa was born in 1940 in Baltimore. 2. Birthyear 3. Birthplace 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.20 20 40 60 80 100 120 140 160 Post−Fusion Accuracy Birthday Birthyear Deathyear Occupation Birthplace Pre−Fusion Precision 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 20 40 60 80 100 120 140 160 Birthday Birthyear Birthplace Deathyear Occupation Pre−Fusion Pseudo−Recall 11 10 9 7 6 5 4 2 0 8 3 1 0 20 40 60 80 100 120 140 160 Birthyear Birthday Occupation Birthplace Deathyear 489 Soderland et al., 1995). Seymore et al. (1999) use HMMs for information extraction and explore ways to improve the learning process. Nahm and Mooney (2002) suggest a method to learn word-to-word relationships across fields by doing data mining on information extraction results. Prager et al. (2004) uses knowledge of birth year to weed out candidate years of death that are impossible. Using the CRF extractors in our data set, this heuristic did not yield any improvement. More distantly related work for multi-field extraction suggests methods for combining information in graphical models across multiple extraction instances (Sutton et al., </context>
</contexts>
<marker>Seymore, McCallum, Rosenfeld, 1999</marker>
<rawString>K. Seymore, A. McCallum, and R. Rosenfeld. 1999. Learning hidden markov model structure for information extraction. In AAAI’99 Workshop on Machine Learning for Information Extraction, pages 37–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soderland</author>
<author>D Fisher</author>
<author>J Aseltine</author>
<author>W Lehnert</author>
</authors>
<title>CRYSTAL: Inducing a conceptual dictionary.</title>
<date>1995</date>
<booktitle>In Proceedings ofIJCAI,</booktitle>
<pages>1314--1319</pages>
<contexts>
<context position="27049" citStr="Soderland et al., 1995" startWordPosition="4450" endWordPosition="4453">zenbach, 1998; Huffman, 1995; Frank Zappa was born on December 21. 1. Birthday Zappa : December 21, 1940. 1. Birthday 2. Birthyear Zappa was born in 1940 in Baltimore. 2. Birthyear 3. Birthplace 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.20 20 40 60 80 100 120 140 160 Post−Fusion Accuracy Birthday Birthyear Deathyear Occupation Birthplace Pre−Fusion Precision 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 20 40 60 80 100 120 140 160 Birthday Birthyear Birthplace Deathyear Occupation Pre−Fusion Pseudo−Recall 11 10 9 7 6 5 4 2 0 8 3 1 0 20 40 60 80 100 120 140 160 Birthyear Birthday Occupation Birthplace Deathyear 489 Soderland et al., 1995). Seymore et al. (1999) use HMMs for information extraction and explore ways to improve the learning process. Nahm and Mooney (2002) suggest a method to learn word-to-word relationships across fields by doing data mining on information extraction results. Prager et al. (2004) uses knowledge of birth year to weed out candidate years of death that are impossible. Using the CRF extractors in our data set, this heuristic did not yield any improvement. More distantly related work for multi-field extraction suggests methods for combining information in graphical models across multiple extraction ins</context>
</contexts>
<marker>Soderland, Fisher, Aseltine, Lehnert, 1995</marker>
<rawString>S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995. CRYSTAL: Inducing a conceptual dictionary. In Proceedings ofIJCAI, pages 1314–1319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>K Rohanimanesh</author>
<author>A McCallum</author>
</authors>
<title>Dynamic conditional random fields: factorize probabilistic models for labeling and segmenting sequence data.</title>
<date>2004</date>
<booktitle>In Proceedings ofICML.</booktitle>
<contexts>
<context position="27676" citStr="Sutton et al., 2004" startWordPosition="4549" endWordPosition="4553">e et al. (1999) use HMMs for information extraction and explore ways to improve the learning process. Nahm and Mooney (2002) suggest a method to learn word-to-word relationships across fields by doing data mining on information extraction results. Prager et al. (2004) uses knowledge of birth year to weed out candidate years of death that are impossible. Using the CRF extractors in our data set, this heuristic did not yield any improvement. More distantly related work for multi-field extraction suggests methods for combining information in graphical models across multiple extraction instances (Sutton et al., 2004; Bunescu and Mooney, 2004) . 7 Conclusion This paper has presented new experimental methodologies and results for cross-document information fusion, focusing on the task of biographic fact extraction and has proposed a new method for crossfield bootstrapping. In particular, we have shown that automatic annotation can be used effectively to train statistical information extractors such Naive Bayes and CRFs, and that CRF extraction accuracy can be improved by 5% with a negative example model. We looked at cross-document fusion and demonstrated that voting outperforms choosing the highest confid</context>
</contexts>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>C. Sutton, K. Rohanimanesh, and A. McCallum. 2004. Dynamic conditional random fields: factorize probabilistic models for labeling and segmenting sequence data. In Proceedings ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
<author>T Korelsky</author>
<author>C Cardie</author>
<author>V Ng</author>
<author>D Pierce</author>
<author>K Wagstaff</author>
</authors>
<title>Multi-document summarization via information extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of HLT.</booktitle>
<contexts>
<context position="25354" citStr="White et al., 2001" startWordPosition="4169" endWordPosition="4172">ographic fact extraction was done by Cowie et al. (2000) and Schiffman et al. (2001), who explore the problem of biographic summarization. There has been rather limited published work in multi-document information extraction. The closest work to what we present here is Masterson and Kushmerick (2003), who perform multi-document information extraction trained on manually annotated training data and use Best Confidence to resolve each particular template slot. In summarizarion, many systems have examined the multi-document case. Notable systems are SUMMONS (Radev and McKeown, 1998) and RIPTIDE (White et al., 2001), which assume perfect extracted information and then perform closed domain summarization. Barzilay et al. (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. In recent question answering research, information fusion has been used to combine multiple candidate answers to form a consensus answer. Clarke et al. (2001) use frequency of n-gram occurrence to pick answers for particular questions. Another example of answer fusion comes in (Brill et al., 2001) which combines the o</context>
</contexts>
<marker>White, Korelsky, Cardie, Ng, Pierce, Wagstaff, 2001</marker>
<rawString>M. White, T. Korelsky, C. Cardie, V. Ng, D. Pierce, and K. Wagstaff. 2001. Multi-document summarization via information extraction. In Proceedings of HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>