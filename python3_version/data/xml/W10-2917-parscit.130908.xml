<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001607">
<title confidence="0.984978">
Improving Word Alignment by Semi-supervised Ensemble
</title>
<author confidence="0.992944">
Shujian Huang1, Kangxi Li2, Xinyu Dai1, Jiajun Chen1
</author>
<affiliation confidence="0.964334">
1State Key Laboratory for Novel Software Technology at Nanjing University
</affiliation>
<address confidence="0.61284">
Nanjing 210093, P.R.China
</address>
<email confidence="0.955251">
{huangsj,daixy,chenjj}@nlp.nju.edu.cn
</email>
<affiliation confidence="0.783689">
2School of Foreign Studies, Nanjing University
</affiliation>
<address confidence="0.452815">
Nanjing 210093, P.R.China
</address>
<email confidence="0.989658">
richardlkx@126.com
</email>
<sectionHeader confidence="0.994598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992132">
Supervised learning has been recently
used to improve the performance of word
alignment. However, due to the limited
amount of labeled data, the performance
of ”pure” supervised learning, which only
used labeled data, is limited. As a re-
sult, many existing methods employ fea-
tures learnt from a large amount of unla-
beled data to assist the task. In this pa-
per, we propose a semi-supervised ensem-
ble method to better incorporate both la-
beled and unlabeled data during learning.
Firstly, we employ an ensemble learning
framework, which effectively uses align-
ment results from different unsupervised
alignment models. We then propose to
use a semi-supervised learning method,
namely Tri-training, to train classifiers us-
ing both labeled and unlabeled data col-
laboratively and further improve the result.
Experimental results show that our meth-
ods can substantially improve the quality
of word alignment. The final translation
quality of a phrase-based translation sys-
tem is slightly improved, as well.
</bodyText>
<sectionHeader confidence="0.998878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986564814814815">
Word alignment is the process of learning bilin-
gual word correspondences. Conventional word
alignment process is treated as an unsupervised
learning task, which automatically learns the cor-
respondences between bilingual words using an
EM style algorithm (Brown et al., 1993; Vogel
et al., 1996; Och and Ney, 2003). Recently, su-
pervised learning methods have been used to im-
prove the performance. They firstly re-formalize
word alignment as some kind of classification
task. Then the labeled data is used to train the
classification model, which is finally used to clas-
sify unseen test data (Liu et al., 2005; Taskar et
al., 2005; Moore, 2005; Cherry and Lin, 2006;
Haghighi et al., 2009).
It is well understood that the performance of
supervised learning relies heavily on the fea-
ture set. As more and more features are added
into the model, more data is needed for train-
ing. However, due to the expensive cost of la-
beling, we usually cannot get as much labeled
word alignment data as we want. This may limit
the performance of supervised methods (Wu et
al., 2006). One possible alternative is to use
features learnt in some unsupervised manner to
help the task. For example, Moore (2005) uses
statistics like log-likelihood-ratio and conditional-
likelihood-probability to measure word associa-
tions; Liu et al. (2005) and Taskar et al. (2005)
use results from IBM Model 3 and Model 4, re-
spectively.
Ayan and Dorr (2006) propose another way of
incorporating unlabeled data. They first train some
existing alignment models, e.g. IBM Model4 and
Hidden Markov Model, using unlabeled data. The
results of these models are then combined using a
maximum entropy classifier, which is trained us-
ing labeled data. This method is highly efficient
in training because it only makes decisions on
alignment links from existing models and avoids
searching the entire alignment space.
In this paper, we follow Ayan and Dorr (2006)’s
idea of combining multiple alignment results. And
we use more features, such as bi-lexical features,
which help capture more information from unla-
beled data. To further improve the decision mak-
ing during combination, we propose to use a semi-
supervised strategy, namely Tri-training (Zhou
and Li, 2005), which ensembles three classifiers
using both labeled and unlabeled data. More
specifically, Tri-training iteratively trains three
classifiers and labels all the unlabeled instances.
It then uses some instances among the unlabeled
ones to expand the labeled training set of each in-
</bodyText>
<page confidence="0.983393">
135
</page>
<note confidence="0.956198">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 135–143,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.997115363636364">
dividual classifier. As word alignment task usually
faces a huge parallel corpus, which contains mil-
lions of unlabeled instances, we develop specific
algorithms to adapt Tri-training for this large scale
task.
The next section introduces the supervised
alignment combination framework; Section 3
presents our semi-supervised learning algorithm.
We show the experiments and results in Section
4; briefly overview related work in Section 5 and
conclude in the last section.
</bodyText>
<sectionHeader confidence="0.9476215" genericHeader="method">
2 Word Alignment as a Classification
Task
</sectionHeader>
<subsectionHeader confidence="0.998014">
2.1 Modeling
</subsectionHeader>
<bodyText confidence="0.999575192307692">
Given a sentence pair (e, f), where e =
e1, e2, ... , eI and f = f1, f2, ... , fJ, an align-
ment link ai,j indicates the translation correspon-
dence between words ei and fj. Word alignment
is to learn the correct alignment A between e and
f, which is a set of such alignment links.
As the number of possible alignment links
grows exponentially with the length of e and f, we
restrict the candidate set using results from several
existing alignment models. Note that, all the mod-
els we employ are unsupervised models. We will
refer to them as sub-models in the rest of this pa-
per.
Let A = {A1, A2, ... , An} be a set of align-
ment results from sub-models; AI and AU be the
intersection and union of these results, respec-
tively. We define our learning task as: for each
alignment link ai,j in the candidate set AC =
AU−AI, deciding whether ai,j should be included
in the alignment result. We use a random variable
yi,j (or simply y) to indicate whether an alignment
link ai,j E AC is correct. A Maximum Entropy
model is employed to directly model the distribu-
tion of y. The probability of y is defined in For-
mula 1, where hm(y, e, f, A, i, j) is the mth fea-
ture function, and λm is the corresponding weight.
</bodyText>
<construct confidence="0.801823">
p(y|e, f, A, i, j)
exp∑Mm�1 λmhm(y, e, f, A, i, j)
∑OE{0,1} exp∑Mm--1 λmhm(9, e, f, A, i, j)
</construct>
<equation confidence="0.925222">
(1)
</equation>
<bodyText confidence="0.99979075">
While Ayan and Dorr (Ayan and Dorr, 2006)
make decisions on each alignment link in AU, we
take a different strategy by assuming that all the
alignment links in AI are correct, which means
alignment links in AI are always included in the
combination result. One reason for using this
strategy is that it makes no sense to exclude an
alignment link, which all the sub-models vote for
including. Also, links in AI usually have a good
quality (In our experiment, AI can always achieve
an accuracy higher than 96%). On the other hand,
because AI is decided before the supervised learn-
ing starts, it will be able to provide evidence for
making decisions on candidate links.
Also note that, Formula 1 is based on the as-
sumption that given AI, the decision on each y is
independent of each other. This is the crucial point
that saves us from searching the whole alignment
space. We take this assumption so that the Tri-
training strategy can be easily applied.
</bodyText>
<subsectionHeader confidence="0.941294">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.935169448275862">
For ensemble, the most important features are the
decisions of sub-models. We also use some other
features, such as POS tags, neighborhood infor-
mation, etc. Details of the features for a given link
ai,j are listed below.
Decision of sub-models: Whether ai,j exists in
the result of kth sub-model Ak. Besides in-
dividual features for each model, we also in-
clude features describing the combination of
sub-models’ decisions. For example, if we
have 3 sub-models, there will be 8 features
indicating the decisions of all the sub-models
as 000, 001, 010, ... , 111.
Part of speech tags: POS tags of previous, cur-
rent and next words in both languages. We
also include features describing the POS tag
pairs of previous, current and next word pairs
in the two languages.
Neighborhood: Whether each neighbor link ex-
ists in the intersection AI. Neighbor links re-
fer to links in a 3*3 window with (i, j) in the
center.
Fertilities: The number of words that ei (or fj) is
aligned to in AI.
Relative distance: The relative distance between
ei and fj, which is calculated as abs(i/I −
j/J).
Conditional Link Probability (CLP) : The con-
ditional link probability (Moore, 2005) of ei
</bodyText>
<page confidence="0.998601">
136
</page>
<bodyText confidence="0.9984365">
and fj. CLP of word e and f is estimated on
an aligned parallel corpus using Formula 2,
</bodyText>
<equation confidence="0.995960333333333">
link(e, f) − d
CLPd(e, f) = (2)
cooc(e, f )
</equation>
<bodyText confidence="0.990720666666666">
where link(e, f) is the number of times e and
f are linked in the aligned corpus; cooc(e, f)
is the number of times e and f appear in
the same sentence pair; d is a discount-
ing constant which is set to 0.4 following
Moore (2005). We estimate these counts on
our set of unlabeled data, with the union of
all sub-model results AU as the alignment.
Union is used in order to get a better link cov-
erage. Probabilities are computed only for
those words that occur at least twice in the
parallel corpus.
bi-lexical features: The lexical word pair ez-fj.
Lexical features have been proved to be useful in
tasks such as parsing and name entity recognition.
Taskar et al. (2005) also employ similar bi-lexical
features of the top 5 non-punctuation words for
word alignment. Using bi-lexicons for arbitrary
word pairs will capture more evidence from the
data; although it results in a huge feature set which
may suffer from data sparseness. In the next sec-
tion, we introduce a semi-supervised strategy will
may alleviate this problem and further improve the
learning procedure.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="method">
3 Semi-supervised methods
</sectionHeader>
<bodyText confidence="0.999953740740741">
Semi-supervised methods aim at using unlabeled
instances to assist the supervised learning. One
of the prominent achievements in this area is
the Co-training paradigm proposed by Blum and
Mitchell (1998). Co-training applies when the fea-
tures of an instance can be naturally divided into
two sufficient and redundant subsets. Two weak
classifiers can be trained using each subset of fea-
tures and strengthened using unlabeled data. Blum
and Mitchell (1998) prove the effectiveness of this
algorithm, under the assumption that features in
one set is conditionally independent of features in
the other set. Intuitively speaking, if this condi-
tional independence assumption holds, the most
confident instance of one classifier will act as a
random instance for the other classifier. Thus it
can be safely used to expand the training set of the
other classifier.
The standard Co-training algorithm requires a
naturally splitting in the feature set, which is hard
to meet in most scenarios, including the task of
word alignment. Variations include using random
split feature sets or two different classification al-
gorithms. In this paper, we use the other Co-
training style algorithm called Tri-training, which
requires neither sufficient and redundant views nor
different classification algorithms.
</bodyText>
<subsectionHeader confidence="0.999926">
3.1 Tri-training
</subsectionHeader>
<bodyText confidence="0.999976952380952">
Similar with Co-training, the basic idea of Tri-
training (Zhou and Li, 2005) is to iteratively ex-
pand the labeled training set for the next-round
training based on the decisions of the current clas-
sifiers. However, Tri-training employs three clas-
sifiers instead of two. To get diverse initial classi-
fiers, the training set of each classifier is initially
generated via bootstrap sampling from the origi-
nal labeled training set and updated separately. In
each round, these three classifiers are used to clas-
sify all the unlabeled instances. An unlabeled in-
stance is added to the training set of any classifier
if the other two classifiers agree on the labeling
of this example. So there is no need to explicitly
measure the confidence of any individual classi-
fier, which might be a problem for some learning
algorithms. Zhou and Li (2005) also give a termi-
nate criterion derived from PAC analysis. As the
algorithm goes, the number of labeled instances
increases, which may bring in more bi-lexical fea-
tures and alleviate the problem of data sparseness.
</bodyText>
<subsectionHeader confidence="0.996889">
3.2 Tri-training for Word Alignment
</subsectionHeader>
<bodyText confidence="0.9999765">
One crucial problem for word alignment is the
huge amount of unlabeled instances. Typical par-
allel corpus for word alignment contains at least
hundreds of thousands of sentence pairs, with each
sentence pair containing tens of instances. That
makes a large set of millions of instances. There-
fore, we develop a modified version of Tri-training
algorithm using sampling techniques, which can
work well with such large scale data. A sketch of
our algorithm is shown in Figure 1.
The algorithm takes original labeled instance
set L, unlabeled sentence set 5U, sub-model re-
sults As for each s in 5U and a sampling ratio r as
input. Fk represents the kth classifier. Variables
with superscript i represent their values during the
ith iteration.
Line 2 initializes candidate instance set AC�s of
each sentence s to be the difference set between
</bodyText>
<page confidence="0.986692">
137
</page>
<bodyText confidence="0.64564">
Input: L, SU, As for each s and sampling ratio r.
</bodyText>
<listItem confidence="0.988598384615385">
1: for all sentence s in SU do
2: A0C,s +- AU,s − AI,s //initializing candidate set
3: end for
4: for all l E {1, 2, 31 do
5: L0l +- Subsample(L, 0.33)
6: Fl0 +- Train(L0l)
7: end for
8: repeat
9: for all l E {1, 2, 31 do
10: Let m, n E {1, 2, 31 and m =� n =� l; Lil = O
11: for all sentence s in SU do
12: for all instance a in Ai−1
C,s do
</listItem>
<figure confidence="0.835459">
13: if F i−1
m (a) = F i−1
n (a) then
Ai−1
14: C,s +- Ai−1
C,s − {(a, F i−1
m (a))1
15: Lil +- Lil U {(a, F i−1
m (a))1
16: end if
17: end for
18: end for
19: end for
20: for all l E {1, 2, 31 do
21: Lil +- Subsampling(Lil, r) U Li−1
l
22: Fli +- Train(Lil)
23: AiC,s +- Ai−1
C,s
24: end for
25: until all Ai
C,s are unchanged or empty
∑
Output: F (x) +- arg maxyE{0,1l l:Fl(x)=y 1
</figure>
<figureCaption confidence="0.999998">
Figure 1: Modified Tri-training Algorithm
</figureCaption>
<bodyText confidence="0.999979">
AU,, and A,,,. In line 5-6, sub-samplings are per-
formed on the original labeled set L and the ini-
tial classifier F0 is trained using the sampling re-
sults. In each iteration, the algorithm labels each
instance in the candidate set Ac,s for each clas-
sifier with the other two classifiers trained in last
iteration. Instances are removed from the candi-
date set and added to the labeled training set (Li)
of classifier l, if they are given the same label by
the other two classifiers (line 13-16).
A sub-sampling is performed before the labeled
training set is used for training (line 21), which
means all the instances in Li are accepted as cor-
rect, but only part of them are added into the train-
ing set. The sampling rate is controlled by a pa-
rameter r, which we empirically set to 0.01 in all
our experiments. The classifier is then re-trained
using the augmented training set Li (line 22). The
algorithm iterates until all instances in the candi-
date sets get labeled or the candidate sets do not
change since the last iteration (line 25). The result-
ing classifiers can be used to label new instances
via majority voting.
Our algorithm differs from Zhou and Li (2005)
in the following three aspects. First of all, com-
paring to the original bootstrap sampling initial-
ization, we use a more aggressive strategy, which
</bodyText>
<table confidence="0.9992965">
Source Usage Sent. Pairs Cand. Links
LDC Train 288111 8.8M
NIST’02 Train 200 5,849
NIST’02 Eval 291 7,797
</table>
<tableCaption confidence="0.999899">
Table 1: Data used in the experiment
</tableCaption>
<bodyText confidence="0.999933090909091">
actually divides the original labeled set into three
parts. This strategy ensures that initial classifiers
are trained using different sets of instances and
maximizes the diversity between classifiers. We
will compare these two initializations in the ex-
periments section. Secondly, we introduce sam-
pling techniques for the huge number of unlabeled
instances. Sampling is essential for maintain-
ing a reasonable growing speed of training data
and keeping the computation physically feasible.
Thirdly, because the original terminate criterion
requires an error estimation process in each iter-
ation, we adapt the much simpler terminate cri-
terion of standard Co-training into our algorithm,
which iterates until all the unlabeled data are fi-
nally labeled or the candidate sets do not change
since the last iteration. In other words, our algo-
rithm inherits both the benefits of using three clas-
sifiers and the simplicity of using Co-training style
termination criterion. Parallel computing tech-
niques are also used during the processing of un-
labeled data to speed up the computation.
</bodyText>
<sectionHeader confidence="0.99942" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.99771">
4.1 Data and Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.999936631578948">
All our experiments are conducted on the lan-
guage pair of Chinese and English. For training
alignment systems, a parallel corpus coming from
LDC2005T10 and LDC2005T14 is used as un-
labeled training data. Labeled data comes from
NIST Open MT Eval’02, which has 491 labeled
sentence pairs. The first 200 labeled sentence pairs
are used as labeled training data and the rest are
used for evaluation (Table 1). The number of can-
didate alignment links in each data set is also listed
in Table 1. These candidate alignment links are
generated using the three sub-models described in
Section 4.2.
The quality of word alignment is evaluated in
terms of alignment error rate (AER) (Och and Ney,
2003), classifier’s accuracy and recall of correct
decisions. Formula 3 shows the definition of AER,
where P and S refer to the set of possible and sure
alignment links, respectively. In our experiments,
</bodyText>
<page confidence="0.990159">
138
</page>
<table confidence="0.999473357142857">
ModelName AER Dev AER Test Accuracy Recall F1
Model4C2E 0.4269 0.4196 0.4898 0.3114 0.3808
Model4E2C 0.3715 0.3592 0.5642 0.5368 0.5502
BerkeleyAl. 0.3075 0.2939 0.7064 0.6377 0.6703
Model4GDF 0.3328 0.3336 0.6059 0.6184 0.6121
Supervised 0.2291 0.2430 0.8124 0.7027 0.7536
Table 2: Experiments of Sub-models
ModelName AER Dev AER Test Accuracy Recall F1
Supervised 0.2291 0.2430 0.8124 0.7027 0.7536
BerkeleyAl. 0.3075 0.2939 0.7064 0.6377 0.6703
Tri-Bootstrap0 0.2301 0.2488 0.8030 0.6858 0.7398
Tri-Divide0 0.2458 0.2525 0.8002 0.6630 0.7251
Tri-Bootstrap 0.2264 0.2468 0.7934 0.7449 0.7684
Tri-Divide 0.2416 0.2494 0.7832 0.7605 0.7717
</table>
<tableCaption confidence="0.999226">
Table 3: Experiments of Semi-supervised Models
</tableCaption>
<equation confidence="0.904132333333333">
we treat all alignment links as sure links.
AER = 1 _ JA n PJ + JA n SJ (3)
JAJ + JSJ
</equation>
<bodyText confidence="0.999594333333333">
We also define a F1 score to be the harmonic mean
of classifier’s accuracy and recall of correct deci-
sions (Formula 4).
</bodyText>
<equation confidence="0.984871666666667">
_ 2 * accuracy * recall
F1 (4)
accuracy + recall
</equation>
<bodyText confidence="0.9999805">
We also evaluate the machine translation quality
using unlabeled data (in Table 1) and these align-
ment results as aligned training data. We use
multi-references data sets from NIST Open MT
Evaluation as development and test data. The En-
glish side of the parallel corpus is trained into
a language model using SRILM (Stolcke, 2002).
Moses (Koehn et al., 2003) is used for decoding.
Translation quality is measured by BLEU4 score
ignoring the case.
</bodyText>
<subsectionHeader confidence="0.999522">
4.2 Experiments of Sub-models
</subsectionHeader>
<bodyText confidence="0.999985130434782">
We use the following three sub-models: bidi-
rectional results of Giza++ (Och and Ney,
2003) Model4, namely Model4C2E and
Model4E2C, and the joint training result of
BerkeleyAligner (Liang et al., 2006) (Berke-
leyAl.). To evaluate AER, all three data sets
listed in Table 1 are combined and used for the
unsupervised training of each sub-model.
Table 2 presents the alignment quality of those
sub-models, as well as a supervised ensemble of
them, as described in Section 2.1. We use the sym-
metrized IBM Model4 results by the grow-diag-
final-and heuristic as our baseline (Model4GDF).
Scores in Table 2 show the great improvement
of supervised learning, which reduce the align-
ment error rate significantly (more than 5% AER
points from the best sub-model, i.e. Berke-
leyAligner). This result is consistent with Ayan
and Dorr (2006)’s experiments. It is quite reason-
able that supervised model achieves a much higher
classification accuracy of 0.8124 than any unsu-
pervised sub-model. Besides, it also achieves the
highest recall of correct alignment links (0.7027).
</bodyText>
<subsectionHeader confidence="0.999394">
4.3 Experiments of Semi-supervised Models
</subsectionHeader>
<bodyText confidence="0.999835166666667">
We present our experiment results on semi-
supervised models in Table 3. The two strategies
of generating initial classifiers are compared. Tri-
Bootstrap is the model using the original boot-
strap sampling initialization; and Tri-Divide is
the model using the dividing initialization as de-
scribed in Section 3.2. Items with superscripts 0
indicate models before the first iteration, i.e. ini-
tial models. The scores of BerkeleyAligner and
the supervised model are also included for com-
parison.
In general, all supervised and semi-supervised
models achieve better results than the best sub-
model, which proves the effectiveness of labeled
training data. It is also reasonable that initial mod-
els are not as good as the supervised model, be-
cause they only use part of the labeled data for
training. After the iterative training, both the two
</bodyText>
<page confidence="0.99196">
139
</page>
<figure confidence="0.999629478260869">
0.9
0.8
0.7
0.6
0.5
0.40 1000 2000 3000 4000 5000 6000
Scores(F−1, Accuracy, Recall)
F−1
Recall
Accuracy
F−1 scores
0.79
0.78
0.77
0.76
0.75
0.74
0.730 0.5 1 1.5 2 2.5 3
Tri−Divide
Supervised
Tri−Bootstrap
Training Instances Number Number of sentences x 105
(a) (b)
</figure>
<figureCaption confidence="0.98175">
Figure 2: (a) Experiments on the Size of Labeled Training Data in Supervised Training; (b) Experiments
on the Size of Unlabeled Data in Tri-training
</figureCaption>
<bodyText confidence="0.999963619047619">
Tri-training models get a significant increase in
recall. We attribute this to the use of bi-lexical
features described in Section 2.2. Analysis of
the resulting model shows that the number of
bi-lexical features increases from around 300 to
nearly 7,800 after Tri-training. It demonstrates
that semi-supervised algorithms are able to learn
more bi-lexical features automatically from the
unlabeled data, which may help recognize more
translation equivalences. However, we also notice
that the accuracy drops a little after Tri-training.
This might also be caused by the large set of bi-
lexical features, which may contain some noises.
In the comparison of initialization strategies,
the dividing strategy achieves a much higher re-
call of 0.7605, which is also the highest among
all models. It also achieves the best F1 score of
0.7717, higher than the bootstrap sampling strat-
egy (0.7684). This result confirms that diversity of
initial classifiers is important for Co-training style
algorithms.
</bodyText>
<subsectionHeader confidence="0.9992855">
4.4 Experiments on the Size of Data
4.4.1 Size of Labeled Data
</subsectionHeader>
<bodyText confidence="0.999960375">
We design this experiment to see how the size of
labeled data affects the supervised training proce-
dure. Our labeled training set contains 5,800 train-
ing instances. We randomly sample different sets
of instances from the whole set and perform the
supervised training.
The alignment results are plotted in Figure 2a.
Basically, both accuracy and recall increase with
the size of labeled data. However, we also find that
the increase of all the scores gets slower when the
number of training instances exceeds 3,000. One
possible explanation for this is that the training
set itself is too small and contains redundant in-
stances, which may prevent further improvement.
We can see in the Section 4.4.2 that the scores can
be largely improved when more data is added.
</bodyText>
<subsectionHeader confidence="0.988303">
4.4.2 Size of Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.999945814814815">
For better understanding the effect of unlabeled
data, we run the Tri-training algorithm on unla-
beled corpus of different sizes. The original un-
labeled corpus contains about 288 thousand sen-
tence pairs. We create 12 sub-corpus of it with
different sizes by selecting certain amounts of sen-
tences from the beginning. Our smallest sub-
corpus consists of the first 5,000 sentence pairs of
the original corpus; while the largest sub-corpus
contains the first 275 thousand sentence pairs. The
alignment results on these different sub-corpus are
evaluated (See Figure 2b).
The result shows that as the size of unlabeled
data grows, the F1 score of Tri-Divide increases
from around 0.74 to 0.772. The F1 score of Tri-
Bootstrap also gets a similar increase. This proves
that adding unlabeled data does help the learning
process. The result also suggests that when the
size of unlabeled data is small, both Tri-Bootstrap
and Tri-Divide get lower scores than the super-
vised model. This is because the Tri-training mod-
els only use part of the labeled data for the training
of each individual classifier, while the supervised
model use the whole set. We can see that when
there are more than 50 thousand unlabeled sen-
tence pairs, both Tri-training models outperform
the supervised model significantly.
</bodyText>
<page confidence="0.994182">
140
</page>
<table confidence="0.9999045">
ModelName Dev04 Test05 Test06 Test08
Model4C2E 24.54 17.10 17.52 14.59
Model4E2C 26.54 19.00 20.18 16.56
BerkeleyAl. 26.19 20.08 19.65 16.70
Model4GDF 26.75 20.67 20.58 17.05
Supervised 27.07 20.00 19.47 16.13
Tri-Bootstrap 26.88 20.49 20.76 17.31
Tri-Divide 27.04 20.96 20.79 17.18
</table>
<tableCaption confidence="0.999898">
Table 4: Experiments on machine translation (BLEU4 scores in percentage)
</tableCaption>
<bodyText confidence="0.999936">
Note that, both experiments on data size show
some unsteadiness during the learning process.
We attribute this mainly to the random sampling
we use in the algorithm. As there are, in all, about
8.8 million instances , it is highly possible that
some of these instances are redundant or noisy.
And because our random sampling does not dis-
tinguish different instances, the quality of result-
ing model may get affected if these redundant or
noisy instances are selected and added to the train-
ing set.
</bodyText>
<subsectionHeader confidence="0.998704">
4.5 Experiments on Machine Translation
</subsectionHeader>
<bodyText confidence="0.99996925">
We compare the machine translation results of
each sub-models, supervised models and semi-
supervised models in Table 4. Among sub-models,
BerkeleyAligner gets better BLEU4 scores in al-
most all the data sets except TEST06, which
agrees with its highest F1 score among all sub-
models. The supervised method gets the highest
BLEU score of 27.07 on the dev set. However, its
performance on the test sets is a bit lower than that
of BerkeleyAligner.
As we expect, our two semi-supervised mod-
els achieve highest scores on almost all the data
sets, which are also higher than the commonly
used grow-diag-final-and symmetrization of IBM
Model 4. More specifically, Tri-Divide is the
best of all systems. It gets a dev score of 27.04,
which is comparable with the highest one (27.07).
Tri-Divide also gets the highest BLEU scores
on Test05 and Test06 (20.96 and 20.79, respec-
tively), which are nearly 1 point higher than all
sub-models. The other Tri-training model, Tri-
Bootstrap, gets the highest score on Test08, which
is also significantly better than those sub-models.
Despite the large improvement in F1 score, our
two Tri-training models only get slightly better
score than the well-known Model4GDF. This kind
of inconsistence between AER or F1 scores and
BLEU scores is a known issue in machine trans-
lation community (Fraser and Marcu, 2007). One
possible explanation is that both AER or F1 are
0-1 loss functions, which means missing one link
and adding one redundant link will get the same
penalty. And more importantly, every wrong link
receives the same penalty under these metrics.
However, these different errors may have different
effects on the machine translation quality. Thus,
improving alignment quality according to AER or
F1 may not directly lead to an increase of BLEU
scores. The relationship among these metrics are
still under investigation.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.99971356">
Previous work mainly focuses on supervised
learning of word alignment. Liu et al. (2005)
propose a log-linear model for the alignment be-
tween two sentences, in which different features
can be used to describe the alignment quality.
Moore (2005) proposes a similar framework, but
with more features and a different search method.
Other models such as SVM and CRF are also
used (Taskar et al., 2005; Cherry and Lin, 2006;
Haghighi et al., 2009). For alignment ensemble,
Wu and Wang (2005) introduce a boosting ap-
proach, in which the labeled data is used to cal-
culate the weight of each sub-model.
These researches all focus on the modeling of
alignment structure and employ some strategy to
search for the optimal alignment. Our main con-
tribution here is the use Co-training style semi-
supervised methods to assist the ensemble learn-
ing framework of Ayan and Dorr (2006). Although
we use a maximum entropy model in our experi-
ment, other models like SVM and CRF can also
be incorporated into our learning framework.
In the area of semi-supervised learning of word
alignment, Callison-Burch et al. (2004) compare
the results of interpolating statistical machine
</bodyText>
<page confidence="0.995157">
141
</page>
<bodyText confidence="0.9999236875">
translation models learnt from labeled and unla-
beled data, respectively. Wu et al. (2006) propose
a modified boosting algorithm, where two differ-
ent models are also trained using labeled and un-
labeled data respectively and interpolated. Fraser
and Marcu (2006) propose an EMD algorithm,
where labeled data is used for discriminative re-
ranking. It should be pointed out that these pieces
of work all use two separate processes for learn-
ing with labeled and unlabeled data. They either
train and interpolate two separate models or re-
rank previously learnt models with labeled data
only. Our proposed semi-supervised strategy is
able to incorporate both labeled and unlabeled data
in the same process, which is in a different line of
thinking.
</bodyText>
<sectionHeader confidence="0.998685" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999991681818182">
Semi-supervised techniques are useful when there
is a large amount of unlabeled data. In this
paper, we introduce a semi-supervised learning
method, called Tri-training, to improve the word
alignment combination task. Although experi-
ments have proved the effectiveness of our meth-
ods, there is one defect that should be mentioned.
As we previously assume that all the decisions
on alignment links are independent of each other
(in Section 2.1), our model are only able to cap-
ture link level evidence like bi-lexical features.
Some global features, such as final word fertil-
ity, cannot be integrated into the current frame-
work. In the future, we plan to apply our semi-
supervised strategy in more complicated learning
frameworks, which are able to capture those global
features.
Currently we use a random sampling to handle
the 8.8 million instances. We will also explore
better and more aggressive sampling techniques,
which may lead to more stable training results and
also enable us to process larger corpus.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999762571428572">
The authors would like to thank Dr. Ming Li,
Mr. Junming Xu and the anonymous reviewers for
their valuable comments. This work is supported
by the National Fundamental Research Program
of China(2010CB327903) and the Scientific Re-
search Foundation of Graduate School of Nanjing
University(2008CL08).
</bodyText>
<sectionHeader confidence="0.993808" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999216685185185">
Necip Fazil Ayan and Bonnie J. Dorr. 2006. A max-
imum entropy approach to combining word align-
ments. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 96–103, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the 11th Annual Conference on Com-
putational Learning Theory, pages 92–100. Morgan
Kaufmann Publishers.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matic of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263–
311.
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with
word- and sentence-aligned parallel corpora. In ACL
’04: Proceedings of the 42nd Annual Meeting on As-
sociation for Computational Linguistics, page 175,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In Proceedings of the COLING/ACL
on Main conference poster sessions, pages 105–112,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment.
In ACL-44: Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Com-
putational Linguistics, pages 769–776, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Comput. Linguist., 33(3):293–303.
Aria Haghighi, John Blitzer, and Dan Klein. 2009.
Better word alignments with supervised itg models.
In Association for Computational Linguistics, Sin-
gapore.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Percy Liang, Benjamin Taskar, and Dan Klein. 2006.
Alignment by agreement. In Robert C. Moore,
Jeff A. Bilmes, Jennifer Chu-Carroll, and Mark
Sanderson, editors, HLT-NAACL. The Association
for Computational Linguistics.
</reference>
<page confidence="0.978266">
142
</page>
<reference confidence="0.9991045">
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In ACL ’05:
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 459–
466, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In HLT ’05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 81–88, Morristown, NJ, USA.
Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19–51.
A. Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, page
901 904.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In HLT ’05: Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
73–80, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. 1996. Hmm-based word alignment in sta-
tistical translation. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics,
pages 836–841.
Hua Wu and Haifeng Wang. 2005. Boosting statistical
word alignment. In Proceedings of MT SUMMIT X,
pages 364–371, Phuket Island, Thailand, September.
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2006. Boost-
ing statistical word alignment using labeled and un-
labeled data. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 913–
920, Sydney, Australia, July. Association for Com-
putational Linguistics.
Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Ex-
ploiting unlabeled data using three classifiers. vol-
ume 17, pages 1529–1541, Piscataway, NJ, USA.
IEEE Educational Activities Department.
</reference>
<page confidence="0.999128">
143
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.806413">
<title confidence="0.998733">Improving Word Alignment by Semi-supervised Ensemble</title>
<author confidence="0.938217">Kangxi Xinyu Jiajun</author>
<affiliation confidence="0.893206">Key Laboratory for Novel Software Technology at Nanjing</affiliation>
<address confidence="0.982069666666667">Nanjing 210093, of Foreign Studies, Nanjing Nanjing 210093,</address>
<email confidence="0.99906">richardlkx@126.com</email>
<abstract confidence="0.999149346153846">Supervised learning has been recently used to improve the performance of word alignment. However, due to the limited amount of labeled data, the performance of ”pure” supervised learning, which only used labeled data, is limited. As a result, many existing methods employ features learnt from a large amount of unlabeled data to assist the task. In this paper, we propose a semi-supervised ensemble method to better incorporate both labeled and unlabeled data during learning. Firstly, we employ an ensemble learning framework, which effectively uses alignment results from different unsupervised alignment models. We then propose to use a semi-supervised learning method, namely Tri-training, to train classifiers using both labeled and unlabeled data collaboratively and further improve the result. Experimental results show that our methods can substantially improve the quality of word alignment. The final translation quality of a phrase-based translation system is slightly improved, as well.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>A maximum entropy approach to combining word alignments.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>96--103</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2777" citStr="Ayan and Dorr (2006)" startWordPosition="428" endWordPosition="431">and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word associations; Liu et al. (2005) and Taskar et al. (2005) use results from IBM Model 3 and Model 4, respectively. Ayan and Dorr (2006) propose another way of incorporating unlabeled data. They first train some existing alignment models, e.g. IBM Model4 and Hidden Markov Model, using unlabeled data. The results of these models are then combined using a maximum entropy classifier, which is trained using labeled data. This method is highly efficient in training because it only makes decisions on alignment links from existing models and avoids searching the entire alignment space. In this paper, we follow Ayan and Dorr (2006)’s idea of combining multiple alignment results. And we use more features, such as bi-lexical features, w</context>
<context position="5913" citStr="Ayan and Dorr, 2006" startWordPosition="956" endWordPosition="959">y. We define our learning task as: for each alignment link ai,j in the candidate set AC = AU−AI, deciding whether ai,j should be included in the alignment result. We use a random variable yi,j (or simply y) to indicate whether an alignment link ai,j E AC is correct. A Maximum Entropy model is employed to directly model the distribution of y. The probability of y is defined in Formula 1, where hm(y, e, f, A, i, j) is the mth feature function, and λm is the corresponding weight. p(y|e, f, A, i, j) exp∑Mm�1 λmhm(y, e, f, A, i, j) ∑OE{0,1} exp∑Mm--1 λmhm(9, e, f, A, i, j) (1) While Ayan and Dorr (Ayan and Dorr, 2006) make decisions on each alignment link in AU, we take a different strategy by assuming that all the alignment links in AI are correct, which means alignment links in AI are always included in the combination result. One reason for using this strategy is that it makes no sense to exclude an alignment link, which all the sub-models vote for including. Also, links in AI usually have a good quality (In our experiment, AI can always achieve an accuracy higher than 96%). On the other hand, because AI is decided before the supervised learning starts, it will be able to provide evidence for making dec</context>
<context position="19025" citStr="Ayan and Dorr (2006)" startWordPosition="3195" endWordPosition="3198">keleyAl.). To evaluate AER, all three data sets listed in Table 1 are combined and used for the unsupervised training of each sub-model. Table 2 presents the alignment quality of those sub-models, as well as a supervised ensemble of them, as described in Section 2.1. We use the symmetrized IBM Model4 results by the grow-diagfinal-and heuristic as our baseline (Model4GDF). Scores in Table 2 show the great improvement of supervised learning, which reduce the alignment error rate significantly (more than 5% AER points from the best sub-model, i.e. BerkeleyAligner). This result is consistent with Ayan and Dorr (2006)’s experiments. It is quite reasonable that supervised model achieves a much higher classification accuracy of 0.8124 than any unsupervised sub-model. Besides, it also achieves the highest recall of correct alignment links (0.7027). 4.3 Experiments of Semi-supervised Models We present our experiment results on semisupervised models in Table 3. The two strategies of generating initial classifiers are compared. TriBootstrap is the model using the original bootstrap sampling initialization; and Tri-Divide is the model using the dividing initialization as described in Section 3.2. Items with super</context>
<context position="27327" citStr="Ayan and Dorr (2006)" startWordPosition="4536" endWordPosition="4539">oposes a similar framework, but with more features and a different search method. Other models such as SVM and CRF are also used (Taskar et al., 2005; Cherry and Lin, 2006; Haghighi et al., 2009). For alignment ensemble, Wu and Wang (2005) introduce a boosting approach, in which the labeled data is used to calculate the weight of each sub-model. These researches all focus on the modeling of alignment structure and employ some strategy to search for the optimal alignment. Our main contribution here is the use Co-training style semisupervised methods to assist the ensemble learning framework of Ayan and Dorr (2006). Although we use a maximum entropy model in our experiment, other models like SVM and CRF can also be incorporated into our learning framework. In the area of semi-supervised learning of word alignment, Callison-Burch et al. (2004) compare the results of interpolating statistical machine 141 translation models learnt from labeled and unlabeled data, respectively. Wu et al. (2006) propose a modified boosting algorithm, where two different models are also trained using labeled and unlabeled data respectively and interpolated. Fraser and Marcu (2006) propose an EMD algorithm, where labeled data </context>
</contexts>
<marker>Ayan, Dorr, 2006</marker>
<rawString>Necip Fazil Ayan and Bonnie J. Dorr. 2006. A maximum entropy approach to combining word alignments. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 96–103, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="9436" citStr="Blum and Mitchell (1998)" startWordPosition="1569" endWordPosition="1572">o employ similar bi-lexical features of the top 5 non-punctuation words for word alignment. Using bi-lexicons for arbitrary word pairs will capture more evidence from the data; although it results in a huge feature set which may suffer from data sparseness. In the next section, we introduce a semi-supervised strategy will may alleviate this problem and further improve the learning procedure. 3 Semi-supervised methods Semi-supervised methods aim at using unlabeled instances to assist the supervised learning. One of the prominent achievements in this area is the Co-training paradigm proposed by Blum and Mitchell (1998). Co-training applies when the features of an instance can be naturally divided into two sufficient and redundant subsets. Two weak classifiers can be trained using each subset of features and strengthened using unlabeled data. Blum and Mitchell (1998) prove the effectiveness of this algorithm, under the assumption that features in one set is conditionally independent of features in the other set. Intuitively speaking, if this conditional independence assumption holds, the most confident instance of one classifier will act as a random instance for the other classifier. Thus it can be safely us</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 92–100. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematic of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="1632" citStr="Brown et al., 1993" startWordPosition="233" endWordPosition="236">od, namely Tri-training, to train classifiers using both labeled and unlabeled data collaboratively and further improve the result. Experimental results show that our methods can substantially improve the quality of word alignment. The final translation quality of a phrase-based translation system is slightly improved, as well. 1 Introduction Word alignment is the process of learning bilingual word correspondences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et al., 2005; Moore, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). It is well understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for trainin</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematic of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Statistical machine translation with word- and sentence-aligned parallel corpora.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>175</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="27559" citStr="Callison-Burch et al. (2004)" startWordPosition="4574" endWordPosition="4577">and Wang (2005) introduce a boosting approach, in which the labeled data is used to calculate the weight of each sub-model. These researches all focus on the modeling of alignment structure and employ some strategy to search for the optimal alignment. Our main contribution here is the use Co-training style semisupervised methods to assist the ensemble learning framework of Ayan and Dorr (2006). Although we use a maximum entropy model in our experiment, other models like SVM and CRF can also be incorporated into our learning framework. In the area of semi-supervised learning of word alignment, Callison-Burch et al. (2004) compare the results of interpolating statistical machine 141 translation models learnt from labeled and unlabeled data, respectively. Wu et al. (2006) propose a modified boosting algorithm, where two different models are also trained using labeled and unlabeled data respectively and interpolated. Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. It should be pointed out that these pieces of work all use two separate processes for learning with labeled and unlabeled data. They either train and interpolate two separate models or rerank pr</context>
</contexts>
<marker>Callison-Burch, Talbot, Osborne, 2004</marker>
<rawString>Chris Callison-Burch, David Talbot, and Miles Osborne. 2004. Statistical machine translation with word- and sentence-aligned parallel corpora. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 175, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Soft syntactic constraints for word alignment through discriminative training.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>105--112</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2022" citStr="Cherry and Lin, 2006" startWordPosition="300" endWordPosition="303">ilingual word correspondences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et al., 2005; Moore, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). It is well understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probab</context>
<context position="26878" citStr="Cherry and Lin, 2006" startWordPosition="4460" endWordPosition="4463">quality. Thus, improving alignment quality according to AER or F1 may not directly lead to an increase of BLEU scores. The relationship among these metrics are still under investigation. 5 Related work Previous work mainly focuses on supervised learning of word alignment. Liu et al. (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. Moore (2005) proposes a similar framework, but with more features and a different search method. Other models such as SVM and CRF are also used (Taskar et al., 2005; Cherry and Lin, 2006; Haghighi et al., 2009). For alignment ensemble, Wu and Wang (2005) introduce a boosting approach, in which the labeled data is used to calculate the weight of each sub-model. These researches all focus on the modeling of alignment structure and employ some strategy to search for the optimal alignment. Our main contribution here is the use Co-training style semisupervised methods to assist the ensemble learning framework of Ayan and Dorr (2006). Although we use a maximum entropy model in our experiment, other models like SVM and CRF can also be incorporated into our learning framework. In the</context>
</contexts>
<marker>Cherry, Lin, 2006</marker>
<rawString>Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints for word alignment through discriminative training. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 105–112, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Semisupervised training for statistical word alignment.</title>
<date>2006</date>
<booktitle>In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>769--776</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="27881" citStr="Fraser and Marcu (2006)" startWordPosition="4622" endWordPosition="4625">ods to assist the ensemble learning framework of Ayan and Dorr (2006). Although we use a maximum entropy model in our experiment, other models like SVM and CRF can also be incorporated into our learning framework. In the area of semi-supervised learning of word alignment, Callison-Burch et al. (2004) compare the results of interpolating statistical machine 141 translation models learnt from labeled and unlabeled data, respectively. Wu et al. (2006) propose a modified boosting algorithm, where two different models are also trained using labeled and unlabeled data respectively and interpolated. Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. It should be pointed out that these pieces of work all use two separate processes for learning with labeled and unlabeled data. They either train and interpolate two separate models or rerank previously learnt models with labeled data only. Our proposed semi-supervised strategy is able to incorporate both labeled and unlabeled data in the same process, which is in a different line of thinking. 6 Conclusions and Future Work Semi-supervised techniques are useful when there is a large amount of unlabeled data. In </context>
</contexts>
<marker>Fraser, Marcu, 2006</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2006. Semisupervised training for statistical word alignment. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 769–776, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring word alignment quality for statistical machine translation.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="25926" citStr="Fraser and Marcu, 2007" startWordPosition="4305" endWordPosition="4308">of 27.04, which is comparable with the highest one (27.07). Tri-Divide also gets the highest BLEU scores on Test05 and Test06 (20.96 and 20.79, respectively), which are nearly 1 point higher than all sub-models. The other Tri-training model, TriBootstrap, gets the highest score on Test08, which is also significantly better than those sub-models. Despite the large improvement in F1 score, our two Tri-training models only get slightly better score than the well-known Model4GDF. This kind of inconsistence between AER or F1 scores and BLEU scores is a known issue in machine translation community (Fraser and Marcu, 2007). One possible explanation is that both AER or F1 are 0-1 loss functions, which means missing one link and adding one redundant link will get the same penalty. And more importantly, every wrong link receives the same penalty under these metrics. However, these different errors may have different effects on the machine translation quality. Thus, improving alignment quality according to AER or F1 may not directly lead to an increase of BLEU scores. The relationship among these metrics are still under investigation. 5 Related work Previous work mainly focuses on supervised learning of word alignm</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Measuring word alignment quality for statistical machine translation. Comput. Linguist., 33(3):293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised itg models.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics,</booktitle>
<contexts>
<context position="2046" citStr="Haghighi et al., 2009" startWordPosition="304" endWordPosition="307">ndences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et al., 2005; Moore, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). It is well understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word as</context>
<context position="26902" citStr="Haghighi et al., 2009" startWordPosition="4464" endWordPosition="4467">ng alignment quality according to AER or F1 may not directly lead to an increase of BLEU scores. The relationship among these metrics are still under investigation. 5 Related work Previous work mainly focuses on supervised learning of word alignment. Liu et al. (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. Moore (2005) proposes a similar framework, but with more features and a different search method. Other models such as SVM and CRF are also used (Taskar et al., 2005; Cherry and Lin, 2006; Haghighi et al., 2009). For alignment ensemble, Wu and Wang (2005) introduce a boosting approach, in which the labeled data is used to calculate the weight of each sub-model. These researches all focus on the modeling of alignment structure and employ some strategy to search for the optimal alignment. Our main contribution here is the use Co-training style semisupervised methods to assist the ensemble learning framework of Ayan and Dorr (2006). Although we use a maximum entropy model in our experiment, other models like SVM and CRF can also be incorporated into our learning framework. In the area of semi-supervised</context>
</contexts>
<marker>Haghighi, Blitzer, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, and Dan Klein. 2009. Better word alignments with supervised itg models. In Association for Computational Linguistics, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="18081" citStr="Koehn et al., 2003" startWordPosition="3043" endWordPosition="3046">odels we treat all alignment links as sure links. AER = 1 _ JA n PJ + JA n SJ (3) JAJ + JSJ We also define a F1 score to be the harmonic mean of classifier’s accuracy and recall of correct decisions (Formula 4). _ 2 * accuracy * recall F1 (4) accuracy + recall We also evaluate the machine translation quality using unlabeled data (in Table 1) and these alignment results as aligned training data. We use multi-references data sets from NIST Open MT Evaluation as development and test data. The English side of the parallel corpus is trained into a language model using SRILM (Stolcke, 2002). Moses (Koehn et al., 2003) is used for decoding. Translation quality is measured by BLEU4 score ignoring the case. 4.2 Experiments of Sub-models We use the following three sub-models: bidirectional results of Giza++ (Och and Ney, 2003) Model4, namely Model4C2E and Model4E2C, and the joint training result of BerkeleyAligner (Liang et al., 2006) (BerkeleyAl.). To evaluate AER, all three data sets listed in Table 1 are combined and used for the unsupervised training of each sub-model. Table 2 presents the alignment quality of those sub-models, as well as a supervised ensemble of them, as described in Section 2.1. We use t</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Benjamin Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<editor>In Robert C. Moore, Jeff A. Bilmes, Jennifer Chu-Carroll, and Mark Sanderson, editors, HLT-NAACL.</editor>
<publisher>The Association for Computational Linguistics.</publisher>
<contexts>
<context position="18400" citStr="Liang et al., 2006" startWordPosition="3092" endWordPosition="3095">abeled data (in Table 1) and these alignment results as aligned training data. We use multi-references data sets from NIST Open MT Evaluation as development and test data. The English side of the parallel corpus is trained into a language model using SRILM (Stolcke, 2002). Moses (Koehn et al., 2003) is used for decoding. Translation quality is measured by BLEU4 score ignoring the case. 4.2 Experiments of Sub-models We use the following three sub-models: bidirectional results of Giza++ (Och and Ney, 2003) Model4, namely Model4C2E and Model4E2C, and the joint training result of BerkeleyAligner (Liang et al., 2006) (BerkeleyAl.). To evaluate AER, all three data sets listed in Table 1 are combined and used for the unsupervised training of each sub-model. Table 2 presents the alignment quality of those sub-models, as well as a supervised ensemble of them, as described in Section 2.1. We use the symmetrized IBM Model4 results by the grow-diagfinal-and heuristic as our baseline (Model4GDF). Scores in Table 2 show the great improvement of supervised learning, which reduce the alignment error rate significantly (more than 5% AER points from the best sub-model, i.e. BerkeleyAligner). This result is consistent </context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Benjamin Taskar, and Dan Klein. 2006. Alignment by agreement. In Robert C. Moore, Jeff A. Bilmes, Jennifer Chu-Carroll, and Mark Sanderson, editors, HLT-NAACL. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Loglinear models for word alignment.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>459--466</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1966" citStr="Liu et al., 2005" startWordPosition="290" endWordPosition="293">oduction Word alignment is the process of learning bilingual word correspondences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et al., 2005; Moore, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). It is well understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics li</context>
<context position="26548" citStr="Liu et al. (2005)" startWordPosition="4404" endWordPosition="4407"> possible explanation is that both AER or F1 are 0-1 loss functions, which means missing one link and adding one redundant link will get the same penalty. And more importantly, every wrong link receives the same penalty under these metrics. However, these different errors may have different effects on the machine translation quality. Thus, improving alignment quality according to AER or F1 may not directly lead to an increase of BLEU scores. The relationship among these metrics are still under investigation. 5 Related work Previous work mainly focuses on supervised learning of word alignment. Liu et al. (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. Moore (2005) proposes a similar framework, but with more features and a different search method. Other models such as SVM and CRF are also used (Taskar et al., 2005; Cherry and Lin, 2006; Haghighi et al., 2009). For alignment ensemble, Wu and Wang (2005) introduce a boosting approach, in which the labeled data is used to calculate the weight of each sub-model. These researches all focus on the modeling of alignment structure and employ some strategy to</context>
</contexts>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2005. Loglinear models for word alignment. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 459– 466, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>81--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2000" citStr="Moore, 2005" startWordPosition="298" endWordPosition="299">of learning bilingual word correspondences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et al., 2005; Moore, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). It is well understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and condit</context>
<context position="7996" citStr="Moore, 2005" startWordPosition="1320" endWordPosition="1321">, ... , 111. Part of speech tags: POS tags of previous, current and next words in both languages. We also include features describing the POS tag pairs of previous, current and next word pairs in the two languages. Neighborhood: Whether each neighbor link exists in the intersection AI. Neighbor links refer to links in a 3*3 window with (i, j) in the center. Fertilities: The number of words that ei (or fj) is aligned to in AI. Relative distance: The relative distance between ei and fj, which is calculated as abs(i/I − j/J). Conditional Link Probability (CLP) : The conditional link probability (Moore, 2005) of ei 136 and fj. CLP of word e and f is estimated on an aligned parallel corpus using Formula 2, link(e, f) − d CLPd(e, f) = (2) cooc(e, f ) where link(e, f) is the number of times e and f are linked in the aligned corpus; cooc(e, f) is the number of times e and f appear in the same sentence pair; d is a discounting constant which is set to 0.4 following Moore (2005). We estimate these counts on our set of unlabeled data, with the union of all sub-model results AU as the alignment. Union is used in order to get a better link coverage. Probabilities are computed only for those words that occu</context>
<context position="26704" citStr="Moore (2005)" startWordPosition="4431" endWordPosition="4432">re importantly, every wrong link receives the same penalty under these metrics. However, these different errors may have different effects on the machine translation quality. Thus, improving alignment quality according to AER or F1 may not directly lead to an increase of BLEU scores. The relationship among these metrics are still under investigation. 5 Related work Previous work mainly focuses on supervised learning of word alignment. Liu et al. (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. Moore (2005) proposes a similar framework, but with more features and a different search method. Other models such as SVM and CRF are also used (Taskar et al., 2005; Cherry and Lin, 2006; Haghighi et al., 2009). For alignment ensemble, Wu and Wang (2005) introduce a boosting approach, in which the labeled data is used to calculate the weight of each sub-model. These researches all focus on the modeling of alignment structure and employ some strategy to search for the optimal alignment. Our main contribution here is the use Co-training style semisupervised methods to assist the ensemble learning framework </context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert C. Moore. 2005. A discriminative framework for bilingual word alignment. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 81–88, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1672" citStr="Och and Ney, 2003" startWordPosition="241" endWordPosition="244">fiers using both labeled and unlabeled data collaboratively and further improve the result. Experimental results show that our methods can substantially improve the quality of word alignment. The final translation quality of a phrase-based translation system is slightly improved, as well. 1 Introduction Word alignment is the process of learning bilingual word correspondences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et al., 2005; Moore, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). It is well understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of</context>
<context position="16578" citStr="Och and Ney, 2003" startWordPosition="2801" endWordPosition="2804">glish. For training alignment systems, a parallel corpus coming from LDC2005T10 and LDC2005T14 is used as unlabeled training data. Labeled data comes from NIST Open MT Eval’02, which has 491 labeled sentence pairs. The first 200 labeled sentence pairs are used as labeled training data and the rest are used for evaluation (Table 1). The number of candidate alignment links in each data set is also listed in Table 1. These candidate alignment links are generated using the three sub-models described in Section 4.2. The quality of word alignment is evaluated in terms of alignment error rate (AER) (Och and Ney, 2003), classifier’s accuracy and recall of correct decisions. Formula 3 shows the definition of AER, where P and S refer to the set of possible and sure alignment links, respectively. In our experiments, 138 ModelName AER Dev AER Test Accuracy Recall F1 Model4C2E 0.4269 0.4196 0.4898 0.3114 0.3808 Model4E2C 0.3715 0.3592 0.5642 0.5368 0.5502 BerkeleyAl. 0.3075 0.2939 0.7064 0.6377 0.6703 Model4GDF 0.3328 0.3336 0.6059 0.6184 0.6121 Supervised 0.2291 0.2430 0.8124 0.7027 0.7536 Table 2: Experiments of Sub-models ModelName AER Dev AER Test Accuracy Recall F1 Supervised 0.2291 0.2430 0.8124 0.7027 0.7</context>
<context position="18290" citStr="Och and Ney, 2003" startWordPosition="3076" endWordPosition="3079">. _ 2 * accuracy * recall F1 (4) accuracy + recall We also evaluate the machine translation quality using unlabeled data (in Table 1) and these alignment results as aligned training data. We use multi-references data sets from NIST Open MT Evaluation as development and test data. The English side of the parallel corpus is trained into a language model using SRILM (Stolcke, 2002). Moses (Koehn et al., 2003) is used for decoding. Translation quality is measured by BLEU4 score ignoring the case. 4.2 Experiments of Sub-models We use the following three sub-models: bidirectional results of Giza++ (Och and Ney, 2003) Model4, namely Model4C2E and Model4E2C, and the joint training result of BerkeleyAligner (Liang et al., 2006) (BerkeleyAl.). To evaluate AER, all three data sets listed in Table 1 are combined and used for the unsupervised training of each sub-model. Table 2 presents the alignment quality of those sub-models, as well as a supervised ensemble of them, as described in Section 2.1. We use the symmetrized IBM Model4 results by the grow-diagfinal-and heuristic as our baseline (Model4GDF). Scores in Table 2 show the great improvement of supervised learning, which reduce the alignment error rate sig</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="18053" citStr="Stolcke, 2002" startWordPosition="3040" endWordPosition="3041">ts of Semi-supervised Models we treat all alignment links as sure links. AER = 1 _ JA n PJ + JA n SJ (3) JAJ + JSJ We also define a F1 score to be the harmonic mean of classifier’s accuracy and recall of correct decisions (Formula 4). _ 2 * accuracy * recall F1 (4) accuracy + recall We also evaluate the machine translation quality using unlabeled data (in Table 1) and these alignment results as aligned training data. We use multi-references data sets from NIST Open MT Evaluation as development and test data. The English side of the parallel corpus is trained into a language model using SRILM (Stolcke, 2002). Moses (Koehn et al., 2003) is used for decoding. Translation quality is measured by BLEU4 score ignoring the case. 4.2 Experiments of Sub-models We use the following three sub-models: bidirectional results of Giza++ (Och and Ney, 2003) Model4, namely Model4C2E and Model4E2C, and the joint training result of BerkeleyAligner (Liang et al., 2006) (BerkeleyAl.). To evaluate AER, all three data sets listed in Table 1 are combined and used for the unsupervised training of each sub-model. Table 2 presents the alignment quality of those sub-models, as well as a supervised ensemble of them, as descri</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing, page 901 904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>73--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1987" citStr="Taskar et al., 2005" startWordPosition="294" endWordPosition="297">nment is the process of learning bilingual word correspondences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et al., 2005; Moore, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). It is well understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-rat</context>
<context position="8808" citStr="Taskar et al. (2005)" startWordPosition="1474" endWordPosition="1477">re linked in the aligned corpus; cooc(e, f) is the number of times e and f appear in the same sentence pair; d is a discounting constant which is set to 0.4 following Moore (2005). We estimate these counts on our set of unlabeled data, with the union of all sub-model results AU as the alignment. Union is used in order to get a better link coverage. Probabilities are computed only for those words that occur at least twice in the parallel corpus. bi-lexical features: The lexical word pair ez-fj. Lexical features have been proved to be useful in tasks such as parsing and name entity recognition. Taskar et al. (2005) also employ similar bi-lexical features of the top 5 non-punctuation words for word alignment. Using bi-lexicons for arbitrary word pairs will capture more evidence from the data; although it results in a huge feature set which may suffer from data sparseness. In the next section, we introduce a semi-supervised strategy will may alleviate this problem and further improve the learning procedure. 3 Semi-supervised methods Semi-supervised methods aim at using unlabeled instances to assist the supervised learning. One of the prominent achievements in this area is the Co-training paradigm proposed</context>
<context position="26856" citStr="Taskar et al., 2005" startWordPosition="4456" endWordPosition="4459"> machine translation quality. Thus, improving alignment quality according to AER or F1 may not directly lead to an increase of BLEU scores. The relationship among these metrics are still under investigation. 5 Related work Previous work mainly focuses on supervised learning of word alignment. Liu et al. (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. Moore (2005) proposes a similar framework, but with more features and a different search method. Other models such as SVM and CRF are also used (Taskar et al., 2005; Cherry and Lin, 2006; Haghighi et al., 2009). For alignment ensemble, Wu and Wang (2005) introduce a boosting approach, in which the labeled data is used to calculate the weight of each sub-model. These researches all focus on the modeling of alignment structure and employ some strategy to search for the optimal alignment. Our main contribution here is the use Co-training style semisupervised methods to assist the ensemble learning framework of Ayan and Dorr (2006). Although we use a maximum entropy model in our experiment, other models like SVM and CRF can also be incorporated into our lear</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A discriminative matching approach to word alignment. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 73–80, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="1652" citStr="Vogel et al., 1996" startWordPosition="237" endWordPosition="240">ing, to train classifiers using both labeled and unlabeled data collaboratively and further improve the result. Experimental results show that our methods can substantially improve the quality of word alignment. The final translation quality of a phrase-based translation system is slightly improved, as well. 1 Introduction Word alignment is the process of learning bilingual word correspondences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et al., 2005; Moore, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). It is well understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to t</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment in statistical translation. In Proceedings of the 16th International Conference on Computational Linguistics, pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Boosting statistical word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of MT SUMMIT X,</booktitle>
<pages>364--371</pages>
<location>Phuket Island, Thailand,</location>
<contexts>
<context position="26946" citStr="Wu and Wang (2005)" startWordPosition="4471" endWordPosition="4474">not directly lead to an increase of BLEU scores. The relationship among these metrics are still under investigation. 5 Related work Previous work mainly focuses on supervised learning of word alignment. Liu et al. (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. Moore (2005) proposes a similar framework, but with more features and a different search method. Other models such as SVM and CRF are also used (Taskar et al., 2005; Cherry and Lin, 2006; Haghighi et al., 2009). For alignment ensemble, Wu and Wang (2005) introduce a boosting approach, in which the labeled data is used to calculate the weight of each sub-model. These researches all focus on the modeling of alignment structure and employ some strategy to search for the optimal alignment. Our main contribution here is the use Co-training style semisupervised methods to assist the ensemble learning framework of Ayan and Dorr (2006). Although we use a maximum entropy model in our experiment, other models like SVM and CRF can also be incorporated into our learning framework. In the area of semi-supervised learning of word alignment, Callison-Burch </context>
</contexts>
<marker>Wu, Wang, 2005</marker>
<rawString>Hua Wu and Haifeng Wang. 2005. Boosting statistical word alignment. In Proceedings of MT SUMMIT X, pages 364–371, Phuket Island, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Zhanyi Liu</author>
</authors>
<title>Boosting statistical word alignment using labeled and unlabeled data.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>913--920</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="2423" citStr="Wu et al., 2006" startWordPosition="372" endWordPosition="375">d of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et al., 2005; Moore, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). It is well understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word associations; Liu et al. (2005) and Taskar et al. (2005) use results from IBM Model 3 and Model 4, respectively. Ayan and Dorr (2006) propose another way of incorporating unlabeled data. They first train some existing alignment models, e.g. IBM Model4 and Hidden Markov Model, using unlabeled data. The results of these models are then combined using a maximum entropy classifier</context>
<context position="27710" citStr="Wu et al. (2006)" startWordPosition="4596" endWordPosition="4599">ing of alignment structure and employ some strategy to search for the optimal alignment. Our main contribution here is the use Co-training style semisupervised methods to assist the ensemble learning framework of Ayan and Dorr (2006). Although we use a maximum entropy model in our experiment, other models like SVM and CRF can also be incorporated into our learning framework. In the area of semi-supervised learning of word alignment, Callison-Burch et al. (2004) compare the results of interpolating statistical machine 141 translation models learnt from labeled and unlabeled data, respectively. Wu et al. (2006) propose a modified boosting algorithm, where two different models are also trained using labeled and unlabeled data respectively and interpolated. Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. It should be pointed out that these pieces of work all use two separate processes for learning with labeled and unlabeled data. They either train and interpolate two separate models or rerank previously learnt models with labeled data only. Our proposed semi-supervised strategy is able to incorporate both labeled and unlabeled data in the same</context>
</contexts>
<marker>Wu, Wang, Liu, 2006</marker>
<rawString>Hua Wu, Haifeng Wang, and Zhanyi Liu. 2006. Boosting statistical word alignment using labeled and unlabeled data. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 913– 920, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Hua Zhou</author>
<author>Ming Li</author>
</authors>
<title>Tri-training: Exploiting unlabeled data using three classifiers.</title>
<date>2005</date>
<journal>IEEE Educational Activities Department.</journal>
<volume>17</volume>
<pages>1529--1541</pages>
<location>Piscataway, NJ, USA.</location>
<contexts>
<context position="3576" citStr="Zhou and Li, 2005" startWordPosition="553" endWordPosition="556">these models are then combined using a maximum entropy classifier, which is trained using labeled data. This method is highly efficient in training because it only makes decisions on alignment links from existing models and avoids searching the entire alignment space. In this paper, we follow Ayan and Dorr (2006)’s idea of combining multiple alignment results. And we use more features, such as bi-lexical features, which help capture more information from unlabeled data. To further improve the decision making during combination, we propose to use a semisupervised strategy, namely Tri-training (Zhou and Li, 2005), which ensembles three classifiers using both labeled and unlabeled data. More specifically, Tri-training iteratively trains three classifiers and labels all the unlabeled instances. It then uses some instances among the unlabeled ones to expand the labeled training set of each in135 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 135–143, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics dividual classifier. As word alignment task usually faces a huge parallel corpus, which contains millions of unlabeled instances, w</context>
<context position="10618" citStr="Zhou and Li, 2005" startWordPosition="1751" endWordPosition="1754">classifier. Thus it can be safely used to expand the training set of the other classifier. The standard Co-training algorithm requires a naturally splitting in the feature set, which is hard to meet in most scenarios, including the task of word alignment. Variations include using random split feature sets or two different classification algorithms. In this paper, we use the other Cotraining style algorithm called Tri-training, which requires neither sufficient and redundant views nor different classification algorithms. 3.1 Tri-training Similar with Co-training, the basic idea of Tritraining (Zhou and Li, 2005) is to iteratively expand the labeled training set for the next-round training based on the decisions of the current classifiers. However, Tri-training employs three classifiers instead of two. To get diverse initial classifiers, the training set of each classifier is initially generated via bootstrap sampling from the original labeled training set and updated separately. In each round, these three classifiers are used to classify all the unlabeled instances. An unlabeled instance is added to the training set of any classifier if the other two classifiers agree on the labeling of this example.</context>
<context position="14456" citStr="Zhou and Li (2005)" startWordPosition="2461" endWordPosition="2464">g set is used for training (line 21), which means all the instances in Li are accepted as correct, but only part of them are added into the training set. The sampling rate is controlled by a parameter r, which we empirically set to 0.01 in all our experiments. The classifier is then re-trained using the augmented training set Li (line 22). The algorithm iterates until all instances in the candidate sets get labeled or the candidate sets do not change since the last iteration (line 25). The resulting classifiers can be used to label new instances via majority voting. Our algorithm differs from Zhou and Li (2005) in the following three aspects. First of all, comparing to the original bootstrap sampling initialization, we use a more aggressive strategy, which Source Usage Sent. Pairs Cand. Links LDC Train 288111 8.8M NIST’02 Train 200 5,849 NIST’02 Eval 291 7,797 Table 1: Data used in the experiment actually divides the original labeled set into three parts. This strategy ensures that initial classifiers are trained using different sets of instances and maximizes the diversity between classifiers. We will compare these two initializations in the experiments section. Secondly, we introduce sampling tech</context>
</contexts>
<marker>Zhou, Li, 2005</marker>
<rawString>Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Exploiting unlabeled data using three classifiers. volume 17, pages 1529–1541, Piscataway, NJ, USA. IEEE Educational Activities Department.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>