<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.998035">
A Latent Dirichlet Allocation method for Selectional Preferences
</title>
<author confidence="0.993425">
Alan Ritter, Mausam and Oren Etzioni
</author>
<affiliation confidence="0.999565">
Department of Computer Science and Engineering
</affiliation>
<address confidence="0.495625">
Box 352350, University of Washington, Seattle, WA 98195, USA
</address>
<email confidence="0.998232">
{aritter,mausam,etzioni}@cs.washington.edu
</email>
<sectionHeader confidence="0.993888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999830666666667">
The computation of selectional prefer-
ences, the admissible argument values for
a relation, is a well-known NLP task with
broad applicability. We present LDA-SP,
which utilizes LinkLDA (Erosheva et al.,
2004) to model selectional preferences.
By simultaneously inferring latent top-
ics and topic distributions over relations,
LDA-SP combines the benefits of pre-
vious approaches: like traditional class-
based approaches, it produces human-
interpretable classes describing each re-
lation’s preferences, but it is competitive
with non-class-based methods in predic-
tive power.
We compare LDA-SP to several state-of-
the-art methods achieving an 85% increase
in recall at 0.9 precision over mutual in-
formation (Erk, 2007). We also eval-
uate LDA-SP’s effectiveness at filtering
improper applications of inference rules,
where we show substantial improvement
over Pantel et al.’s system (Pantel et al.,
2007).
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926351851852">
Selectional Preferences encode the set of admissi-
ble argument values for a relation. For example,
locations are likely to appear in the second argu-
ment of the relation X is headquartered in Y and
companies or organizations in the first. A large,
high-quality database of preferences has the po-
tential to improve the performance of a wide range
of NLP tasks including semantic role labeling
(Gildea and Jurafsky, 2002), pronoun resolution
(Bergsma et al., 2008), textual inference (Pantel
et al., 2007), word-sense disambiguation (Resnik,
1997), and many more. Therefore, much atten-
tion has been focused on automatically computing
them based on a corpus of relation instances.
Resnik (1996) presented the earliest work in
this area, describing an information-theoretic ap-
proach that inferred selectional preferences based
on the WordNet hypernym hierarchy. Recent work
(Erk, 2007; Bergsma et al., 2008) has moved away
from generalization to known classes, instead
utilizing distributional similarity between nouns
to generalize beyond observed relation-argument
pairs. This avoids problems like WordNet’s poor
coverage of proper nouns and is shown to improve
performance. These methods, however, no longer
produce the generalized class for an argument.
In this paper we describe a novel approach to
computing selectional preferences by making use
of unsupervised topic models. Our approach is
able to combine benefits of both kinds of meth-
ods: it retains the generalization and human-
interpretability of class-based approaches and is
also competitive with the direct methods on pre-
dictive tasks.
Unsupervised topic models, such as latent
Dirichlet allocation (LDA) (Blei et al., 2003) and
its variants are characterized by a set of hidden
topics, which represent the underlying semantic
structure of a document collection. For our prob-
lem these topics offer an intuitive interpretation –
they represent the (latent) set of classes that store
the preferences for the different relations. Thus,
topic models are a natural fit for modeling our re-
lation data.
In particular, our system, called LDA-SP, uses
LinkLDA (Erosheva et al., 2004), an extension of
LDA that simultaneously models two sets of dis-
tributions for each topic. These two sets represent
the two arguments for the relations. Thus, LDA-SP
is able to capture information about the pairs of
topics that commonly co-occur. This information
is very helpful in guiding inference.
We run LDA-SP to compute preferences on a
massive dataset of binary relations r(a1, a2) ex-
</bodyText>
<page confidence="0.98457">
424
</page>
<note confidence="0.9446755">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424–434,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998317333333333">
tracted from the Web by TEXTRUNNER (Banko
and Etzioni, 2008). Our experiments demon-
strate that LDA-SP significantly outperforms state
of the art approaches obtaining an 85% increase
in recall at precision 0.9 on the standard pseudo-
disambiguation task.
Additionally, because LDA-SP is based on a for-
mal probabilistic model, it has the advantage that
it can naturally be applied in many scenarios. For
example, we can obtain a better understanding of
similar relations (Table 1), filter out incorrect in-
ferences based on querying our model (Section
4.3), as well as produce a repository of class-based
preferences with a little manual effort as demon-
strated in Section 4.4. In all these cases we obtain
high quality results, for example, massively out-
performing Pantel et al.’s approach in the textual
inference task.1
</bodyText>
<sectionHeader confidence="0.997187" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999742230769231">
Previous work on selectional preferences can
be broken into four categories: class-based ap-
proaches (Resnik, 1996; Li and Abe, 1998; Clark
and Weir, 2002; Pantel et al., 2007), similarity
based approaches (Dagan et al., 1999; Erk, 2007),
discriminative (Bergsma et al., 2008), and genera-
tive probabilistic models (Rooth et al., 1999).
Class-based approaches, first proposed by
Resnik (1996), are the most studied of the four.
They make use of a pre-defined set of classes, ei-
ther manually produced (e.g. WordNet), or auto-
matically generated (Pantel, 2003). For each re-
lation, some measure of the overlap between the
classes and observed arguments is used to iden-
tify those that best describe the arguments. These
techniques produce a human-interpretable output,
but often suffer in quality due to an incoherent tax-
onomy, inability to map arguments to a class (poor
lexical coverage), and word sense ambiguity.
Because of these limitations researchers have
investigated non-class based approaches, which
attempt to directly classify a given noun-phrase
as plausible/implausible for a relation. Of these,
the similarity based approaches make use of a dis-
tributional similarity measure between arguments
and evaluate a heuristic scoring function:
</bodyText>
<equation confidence="0.9904805">
�5rel(arg)� sim(arg, arg�) · wtrel(arg)
arg&apos;ESeen(rel)
</equation>
<footnote confidence="0.991259">
1Our repository of selectional preferences is available
at http://www.cs.washington.edu/research/
ldasp.
</footnote>
<bodyText confidence="0.999485882352941">
Erk (2007) showed the advantages of this ap-
proach over Resnik’s information-theoretic class-
based method on a pseudo-disambiguation evalu-
ation. These methods obtain better lexical cover-
age, but are unable to obtain any abstract represen-
tation of selectional preferences.
Our solution fits into the general category
of generative probabilistic models, which model
each relation/argument combination as being gen-
erated by a latent class variable. These classes
are automatically learned from the data. This re-
tains the class-based flavor of the problem, with-
out the knowledge limitations of the explicit class-
based approaches. Probably the closest to our
work is a model proposed by Rooth et al. (1999),
in which each class corresponds to a multinomial
over relations and arguments and EM is used to
learn the parameters of the model. In contrast,
we use a LinkLDA framework in which each re-
lation is associated with a corresponding multi-
nomial distribution over classes, and each argu-
ment is drawn from a class-specific distribution
over words; LinkLDA captures co-occurrence of
classes in the two arguments. Additionally we
perform full Bayesian inference using collapsed
Gibbs sampling, in which parameters are inte-
grated out (Griffiths and Steyvers, 2004).
Recently, Bergsma et. al. (2008) proposed the
first discriminative approach to selectional prefer-
ences. Their insight that pseudo-negative exam-
ples could be used as training data allows the ap-
plication of an SVM classifier, which makes use of
many features in addition to the relation-argument
co-occurrence frequencies used by other meth-
ods. They automatically generated positive and
negative examples by selecting arguments having
high and low mutual information with the rela-
tion. Since it is a discriminative approach it is
amenable to feature engineering, but needs to be
retrained and tuned for each task. On the other
hand, generative models produce complete prob-
ability distributions of the data, and hence can be
integrated with other systems and tasks in a more
principled manner (see Sections 4.2.2 and 4.3.1).
Additionally, unlike LDA-SP Bergsma et al.’s sys-
tem doesn’t produce human-interpretable topics.
Finally, we note that LDA-SP and Bergsma’s sys-
tem are potentially complimentary – the output of
LDA-SP could be used to generate higher-quality
training data for Bergsma, potentially improving
their results.
</bodyText>
<page confidence="0.998646">
425
</page>
<bodyText confidence="0.999983212121212">
Topic models such as LDA (Blei et al., 2003)
and its variants have recently begun to see use
in many NLP applications such as summarization
(Daum´e III and Marcu, 2006), document align-
ment and segmentation (Chen et al., 2009), and
inferring class-attribute hierarchies (Reisinger and
Pasca, 2009). Our particular model, LinkLDA, has
been applied to a few NLP tasks such as simul-
taneously modeling the words appearing in blog
posts and users who will likely respond to them
(Yano et al., 2009), modeling topic-aligned arti-
cles in different languages (Mimno et al., 2009),
and word sense induction (Brody and Lapata,
2009).
Finally, we highlight two systems, developed
independently of our own, which apply LDA-style
models to similar tasks. O´ S´eaghdha (2010) pro-
poses a series of LDA-style models for the task
of computing selectional preferences. This work
learns selectional preferences between the fol-
lowing grammatical relations: verb-object, noun-
noun, and adjective-noun. It also focuses on
jointly modeling the generation of both predicate
and argument, and evaluation is performed on a
set of human-plausibility judgments obtaining im-
pressive results against Keller and Lapata’s (2003)
Web hit-count based system. Van Durme and
Gildea (2009) proposed applying LDA to general
knowledge templates extracted using the KNEXT
system (Schubert and Tong, 2003). In contrast,
our work uses LinkLDA and focuses on modeling
multiple arguments of a relation (e.g., the subject
and direct object of a verb).
</bodyText>
<sectionHeader confidence="0.992888" genericHeader="method">
3 Topic Models for Selectional Prefs.
</sectionHeader>
<bodyText confidence="0.999943772727273">
We present a series of topic models for the task of
computing selectional preferences. These models
vary in the amount of independence they assume
between a1 and a2. At one extreme is Indepen-
dentLDA, a model which assumes that both a1 and
a2 are generated completely independently. On
the other hand, JointLDA, the model at the other
extreme (Figure 1) assumes both arguments of a
specific extraction are generated based on a single
hidden variable z. LinkLDA (Figure 2) lies be-
tween these two extremes, and as demonstrated in
Section 4, it is the best model for our relation data.
We are given a set R of binary relations and a
corpus D = {r(a1, a2)} of extracted instances for
these relations. 2 Our task is to compute, for each
argument ai of each relation r, a set of usual ar-
gument values (noun phrases) that it takes. For
example, for the relation is headquartered in the
first argument set will include companies like Mi-
crosoft, Intel, General Motors and second argu-
ment will favor locations like New York, Califor-
nia, Seattle.
</bodyText>
<subsectionHeader confidence="0.963123">
3.1 IndependentLDA
</subsectionHeader>
<bodyText confidence="0.98938445">
We first describe the straightforward application
of LDA to modeling our corpus of extracted rela-
tions. In this case two separate LDA models are
used to model a1 and a2 independently.
In the generative model for our data, each rela-
tion r has a corresponding multinomial over topics
0r, drawn from a Dirichlet. For each extraction, a
hidden topic z is first picked according to 0r, and
then the observed argument a is chosen according
to the multinomial 0,
Readers familiar with topic modeling terminol-
ogy can understand our approach as follows: we
treat each relation as a document whose contents
consist of a bags of words corresponding to all the
noun phrases observed as arguments of the rela-
tion in our corpus. Formally, LDA generates each
argument in the corpus of relations as follows:
for each topic t = 1... T do
Generate Ot according to symmetric Dirich-
let distribution Dir(,q).
</bodyText>
<subsectionHeader confidence="0.479213">
end for
</subsectionHeader>
<bodyText confidence="0.9450555">
for each relation r = 1... |R |do
Generate 0r according to Dirichlet distribu-
tion Dir(α).
for each tuple i = 1... Nr do
Generate zr,i from Multinomial(Or).
Generate the argument ar,i from multi-
</bodyText>
<listItem confidence="0.468239">
nomial �����.
end for
end for
</listItem>
<bodyText confidence="0.999724777777778">
One weakness of IndependentLDA is that it
doesn’t jointly model a1 and a2 together. Clearly
this is undesirable, as information about which
topics one of the arguments favors can help inform
the topics chosen for the other. For example, class
pairs such as (team, game), (politician, political is-
sue) form much more plausible selectional prefer-
ences than, say, (team, political issue), (politician,
game).
</bodyText>
<footnote confidence="0.834502">
2We focus on binary relations, though the techniques pre-
sented in the paper are easily extensible to n-ary relations.
</footnote>
<page confidence="0.996524">
426
</page>
<subsectionHeader confidence="0.983456">
3.2 JointLDA
</subsectionHeader>
<bodyText confidence="0.999802214285715">
As a more tightly coupled alternative, we first
propose JointLDA, whose graphical model is de-
picted in Figure 1. The key difference in JointLDA
(versus LDA) is that instead of one, it maintains
two sets of topics (latent distributions over words)
denoted by Q and -y, one for classes of each ar-
gument. A topic id k represents a pair of topics,
Qk and -yk, that co-occur in the arguments of ex-
tracted relations. Common examples include (Per-
son, Location), (Politician, Political issue), etc.
The hidden variable z = k indicates that the noun
phrase for the first argument was drawn from the
multinomial Qk, and that the second argument was
drawn from -yk. The per-relation distribution 0r is
a multinomial over the topic ids and represents the
selectional preferences, both for arg1s and arg2s
of a relation r.
Although JointLDA has many desirable proper-
ties, it has some drawbacks as well. Most notably,
in JointLDA topics correspond to pairs of multi-
nomials (Qk, -yk); this leads to a situation in which
multiple redundant distributions are needed to rep-
resent the same underlying semantic class. For
example consider the case where we we need to
represent the following selectional preferences for
our corpus of relations: (person, location), (per-
son, organization), and (person, crime). Because
JointLDA requires a separate pair of multinomials
for each topic, it is forced to use 3 separate multi-
nomials to represent the class person, rather than
learning a single distribution representing person
and choosing 3 different topics for a2. This results
in poor generalization because the data for a single
class is divided into multiple topics.
In order to address this problem while maintain-
ing the sharing of influence between a1 and a2, we
next present LinkLDA, which represents a com-
promise between IndependentLDA and JointLDA.
LinkLDA is more flexible than JointLDA, allow-
ing different topics to be chosen for a1, and a2,
however still models the generation of topics from
the same distribution for a given relation.
</bodyText>
<subsectionHeader confidence="0.990749">
3.3 LinkLDA
</subsectionHeader>
<bodyText confidence="0.999962314285714">
Figure 2 illustrates the LinkLDA model in the
plate notation, which is analogous to the model
in (Erosheva et al., 2004). In particular note that
each ai is drawn from a different hidden topic zi,
however the zi’s are drawn from the same distri-
bution 0r for a given relation r. To facilitate learn-
ing related topic pairs between arguments we em-
ploy a sparse prior over the per-relation topic dis-
tributions. Because a few topics are likely to be
assigned most of the probability mass for a given
relation it is more likely (although not necessary)
that the same topic number k will be drawn for
both arguments.
When comparing LinkLDA with JointLDA the
better model may not seem immediately clear. On
the one hand, JointLDA jointly models the gen-
eration of both arguments in an extracted tuple.
This allows one argument to help disambiguate
the other in the case of ambiguous relation strings.
LinkLDA, however, is more flexible; rather than
requiring both arguments to be generated from one
of IZI possible pairs of multinomials (Qz, -yz), Lin-
kLDA allows the arguments of a given extraction
to be generated from IZ12 possible pairs. Thus,
instead of imposing a hard constraint that z1 =
z2 (as in JointLDA), LinkLDA simply assigns a
higher probability to states in which z1 = z2, be-
cause both hidden variables are drawn from the
same (sparse) distribution 0r. LinkLDA can thus
re-use argument classes, choosing different com-
binations of topics for the arguments if it fits the
data better. In Section 4 we show experimentally
that LinkLDA outperforms JointLDA (and Inde-
pendentLDA) by wide margins. We use LDA-SP
to refer to LinkLDA in all the experiments below.
</bodyText>
<subsectionHeader confidence="0.508538">
3.4 Inference
</subsectionHeader>
<bodyText confidence="0.99984475">
For all the models we use collapsed Gibbs sam-
pling for inference in which each of the hid-
den variables (e.g., zr,i,1 and zr,i,2 in LinkLDA)
are sampled sequentially conditioned on a full-
assignment to all others, integrating out the param-
eters (Griffiths and Steyvers, 2004). This produces
robust parameter estimates, as it allows computa-
tion of expectations over the posterior distribution
</bodyText>
<figureCaption confidence="0.998835">
Figure 1: JointLDA Figure 2: LinkLDA
</figureCaption>
<figure confidence="0.996226869565217">
a
771
a
1 a2
a
z
B
712
7
T
N
�R1
z1 z2
a1 a2
771
a
a
B
772
7
T
N
�R1
</figure>
<page confidence="0.991825">
427
</page>
<bodyText confidence="0.99995575">
as opposed to estimating maximum likelihood pa-
rameters. In addition, the integration allows the
use of sparse priors, which are typically more ap-
propriate for natural language data. In all exper-
iments we use hyperparameters α = 771 = 772 =
0.1. We generated initial code for our samplers us-
ing the Hierarchical Bayes Compiler (Daume III,
2007).
</bodyText>
<subsectionHeader confidence="0.996071">
3.5 Advantages of Topic Models
</subsectionHeader>
<bodyText confidence="0.99998975">
There are several advantages to using topic mod-
els for our task. First, they naturally model the
class-based nature of selectional preferences, but
don’t take a pre-defined set of classes as input.
Instead, they compute the classes automatically.
This leads to better lexical coverage since the is-
sue of matching a new argument to a known class
is side-stepped. Second, the models naturally han-
dle ambiguous arguments, as they are able to as-
sign different topics to the same phrase in different
contexts. Inference in these models is also scalable
– linear in both the size of the corpus as well as
the number of topics. In addition, there are several
scalability enhancements such as SparseLDA (Yao
et al., 2009), and an approximation of the Gibbs
Sampling procedure can be efficiently parallelized
(Newman et al., 2009). Finally we note that, once
a topic distribution has been learned over a set of
training relations, one can efficiently apply infer-
ence to unseen relations (Yao et al., 2009).
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999911375">
We perform three main experiments to assess the
quality of the preferences obtained using topic
models. The first is a task-independent evaluation
using a pseudo-disambiguation experiment (Sec-
tion 4.2), which is a standard way to evaluate the
quality of selectional preferences (Rooth et al.,
1999; Erk, 2007; Bergsma et al., 2008). We use
this experiment to compare the various topic mod-
els as well as the best model with the known state
of the art approaches to selectional preferences.
Secondly, we show significant improvements to
performance at an end-task of textual inference in
Section 4.3. Finally, we report on the quality of
a large database of Wordnet-based preferences ob-
tained after manually associating our topics with
Wordnet classes (Section 4.4).
</bodyText>
<subsectionHeader confidence="0.938142">
4.1 Generalization Corpus
</subsectionHeader>
<bodyText confidence="0.999566115384615">
For all experiments we make use of a corpus
of r(a1, a2) tuples, which was automatically ex-
tracted by TEXTRUNNER (Banko and Etzioni,
2008) from 500 million Web pages.
To create a generalization corpus from this
large dataset. We first selected 3,000 relations
from the middle of the tail (we used the 2,000-
5,000 most frequent ones)3 and collected all in-
stances. To reduce sparsity, we discarded all tu-
ples containing an NP that occurred fewer than 50
times in the data. This resulted in a vocabulary of
about 32,000 noun phrases, and a set of about 2.4
million tuples in our generalization corpus.
We inferred topic-argument and relation-topic
multinomials (Q, &apos;y, and 0) on the generalization
corpus by taking 5 samples at a lag of 50 after
a burn in of 750 iterations. Using multiple sam-
ples introduces the risk of topic drift due to lack
of identifiability, however we found this to not be
a problem in practice. During development we
found that the topics tend to remain stable across
multiple samples after sufficient burn in, and mul-
tiple samples improved performance. Table 1 lists
sample topics and high ranked words for each (for
both arguments) as well as relations favoring those
topics.
</bodyText>
<subsectionHeader confidence="0.96839">
4.2 Task Independent Evaluation
</subsectionHeader>
<bodyText confidence="0.999995071428571">
We first compare the three LDA-based approaches
to each other and two state of the art similarity
based systems (Erk, 2007) (using mutual informa-
tion and Jaccard similarity respectively). These
similarity measures were shown to outperform the
generative model of Rooth et al. (1999), as well
as class-based methods such as Resnik’s. In this
pseudo-disambiguation experiment an observed
tuple is paired with a pseudo-negative, which
has both arguments randomly generated from the
whole vocabulary (according to the corpus-wide
distribution over arguments). The task is, for each
relation-argument pair, to determine whether it is
observed, or a random distractor.
</bodyText>
<subsectionHeader confidence="0.925117">
4.2.1 Test Set
</subsectionHeader>
<bodyText confidence="0.999954333333333">
For this experiment we gathered a primary corpus
by first randomly selecting 100 high-frequency re-
lations not in the generalization corpus. For each
relation we collected all tuples containing argu-
ments in the vocabulary. We held out 500 ran-
domly selected tuples as the test set. For each tu-
</bodyText>
<footnote confidence="0.63628975">
3Many of the most frequent relations have very weak se-
lectional preferences, and thus provide little signal for infer-
ring meaningful topics. For example, the relations has and is
can take just about any arguments.
</footnote>
<page confidence="0.98138">
428
</page>
<table confidence="0.986261444444445">
Topic t Arg1 Relations which assign Arg2
highest probability to t
18 The residue - The mixture - The reaction was treated with, is EtOAc - CH2Cl2 - H2O - CH.sub.2Cl.sub.2
mixture - The solution - the mixture - the re- treated with, was - H.sub.2O - water - MeOH - NaHCO3 -
action mixture - the residue - The reaction - poured into, was Et2O - NHCl - CHCl.sub.3 - NHCl - drop-
the solution - The filtrate - the reaction - The extracted with, was wise - CH2Cl.sub.2 - Celite - Et.sub.2O -
product - The crude product - The pellet - purified by, was di- Cl.sub.2 - NaOH - AcOEt - CH2C12 - the
The organic layer - Thereto - This solution luted with, was filtered mixture - saturated NaHCO3 - SiO2 - H2O
- The resulting solution - Next - The organic through, is disolved in, - N hydrochloric acid - NHCl - preparative
phase - The resulting mixture - C. ) is washed with HPLC - to0 C
151 the Court - The Court - the Supreme Court will hear, ruled in, de- the case - the appeal - arguments - a case -
- The Supreme Court - this Court - Court cides, upholds, struck evidence - this case - the decision - the law
- The US Supreme Court - the court - This down, overturned, - testimony - the State - an interview - an
Court - the US Supreme Court - The court sided with, affirms appeal - cases - the Court - that decision -
- Supreme Court - Judge - the Court of Ap- Congress - a decision - the complaint - oral
peals - A federal judge arguments - a law - the statute
211 President Bush - Bush - The President - hailed, vetoed, pro- the bill - a bill - the decision - the war - the
Clinton - the President - President Clinton moted, will deliver, idea - the plan - the move - the legislation -
- President George W. Bush - Mr. Bush - favors, denounced, legislation - the measure - the proposal - the
The Governor - the Governor - Romney - defended deal - this bill - a measure - the program -
McCain - The White House - President - the law - the resolution - efforts - the agree-
Schwarzenegger - Obama ment - gay marriage - the report - abortion
224 Google - Software - the CPU - Clicking - will display, to store, to data - files - the data - the file - the URL -
Excel - the user - Firefox - System - The load, processes, cannot information - the files - images - a URL - the
CPU - Internet Explorer - the ability - Pro- find, invokes, to search information - the IP address - the user - text
gram - users - Option - SQL Server - Code for, to delete - the code - a file - the page - IP addresses -
- the OS - the BIOS PDF files - messages - pages - an IP address
</table>
<tableCaption confidence="0.999487">
Table 1: Example argument lists from the inferred topics. For each topic number t we list the most
</tableCaption>
<bodyText confidence="0.939096857142857">
probable values according to the multinomial distributions for each argument (Qt and -yt). The middle
column reports a few relations whose inferred topic distributions Br assign highest probability to t.
ple r(a1, a2) in the held-out set, we removed all
tuples in the training set containing either of the
rel-arg pairs, i.e., any tuple matching r(a1, *) or
r(*, a2). Next we used collapsed Gibbs sampling
to infer a distribution over topics, Br, for each of
the relations in the primary corpus (based solely
on tuples in the training set) using the topics from
the generalization corpus.
For each of the 500 observed tuples in the test-
set we generated a pseudo-negative tuple by ran-
domly sampling two noun phrases from the distri-
bution of NPs in both corpora.
</bodyText>
<subsectionHeader confidence="0.682903">
4.2.2 Prediction
</subsectionHeader>
<bodyText confidence="0.999987">
Our prediction system needs to determine whether
a specific relation-argument pair is admissible ac-
cording to the selectional preferences or is a ran-
dom distractor (D). Following previous work, we
perform this experiment independently for the two
relation-argument pairs (r, a1) and (r, a2).
We first compute the probability of observing
a1 for first argument of relation r given that it is
not a distractor, P(a1|r, -,D), which we approx-
imate by its probability given an estimate of the
parameters inferred by our model, marginalizing
over hidden topics t. The analysis for the second
</bodyText>
<equation confidence="0.935102428571429">
argument is similar.
T
P(a1|r, ¬D) ≈ PLDA(a1|r) = E P(a1|t)P(t|r)
t=0
T
= E ,3t(a1)0,(t)
t=0
</equation>
<bodyText confidence="0.99893">
A simple application of Bayes Rule gives the
probability that a particular argument is not a
distractor. Here the distractor-related proba-
bilities are independent of r, i.e., P(D|r) =
P(D), P(a1|D, r) = P(a1|D), etc. We estimate
P(a1|D) according to their frequency in the gen-
eralization corpus.
</bodyText>
<equation confidence="0.869358">
P(¬D|r)P(a1|r, ¬D)
</equation>
<sectionHeader confidence="0.809754" genericHeader="evaluation">
4.2.3 Results
</sectionHeader>
<bodyText confidence="0.999623125">
Figure 3 plots the precision-recall curve for the
pseudo-disambiguation experiment comparing the
three different topic models. LDA-SP, which uses
LinkLDA, substantially outperforms both Inde-
pendentLDA and JointLDA.
Next, in figure 4, we compare LDA-SP with
mutual information and Jaccard similarities us-
ing both the generalization and primary corpus for
</bodyText>
<equation confidence="0.9997026">
P(¬D|r, a1) =
P(a1|r)
P(¬D)PLDA(a1|r)
≈
P(D)P(a1|D) + P(¬D)PLDA(a1|r)
</equation>
<page confidence="0.995812">
429
</page>
<figure confidence="0.9838">
0.0 0.2 0.4 0.6 0.8 1.0
recall
</figure>
<figureCaption confidence="0.9848345">
Figure 3: Comparison of LDA-based approaches
on the pseudo-disambiguation task. LDA-SP (Lin-
kLDA) substantially outperforms the other mod-
els.
</figureCaption>
<figure confidence="0.986271">
0.0 0.2 0.4 0.6 0.8 1.0
recall
</figure>
<figureCaption confidence="0.896422666666667">
Figure 4: Comparison to similarity-based selec-
tional preference systems. LDA-SP obtains 85%
higher recall at precision 0.9.
</figureCaption>
<bodyText confidence="0.999306294117647">
computation of similarities. We find LDA-SP sig-
nificantly outperforms these methods. Its edge is
most noticed at high precisions; it obtains 85%
more recall at 0.9 precision compared to mutual
information. Overall LDA-SP obtains an 15% in-
crease in the area under precision-recall curve over
mutual information. All three systems’ AUCs are
shown in Table 2; LDA-SP’s improvements over
both Jaccard and mutual information are highly
significant with a significance level less than 0.01
using a paired t-test.
In addition to a superior performance in se-
lectional preference evaluation LDA-SP also pro-
duces a set of coherent topics, which can be use-
ful in their own right. For instance, one could use
them for tasks such as set-expansion (Carlson et
al., 2010) or automatic thesaurus induction (Et-
</bodyText>
<footnote confidence="0.712253">
MI-Sim Jaccard-Sim
0.727 0.711
Table 2: Area under the precision recall curve.
LDA-SP’s AUC is significantly higher than both
similarity-based methods according to a paired t-
test with a significance level below 0.01.
</footnote>
<note confidence="0.868539">
zioni et al., 2005; Kozareva et al., 2008).
</note>
<subsectionHeader confidence="0.978065">
4.3 End Task Evaluation
</subsectionHeader>
<bodyText confidence="0.999893071428571">
We now evaluate LDA-SP’s ability to improve per-
formance at an end-task. We choose the task of
improving textual entailment by learning selec-
tional preferences for inference rules and filtering
inferences that do not respect these. This applica-
tion of selectional preferences was introduced by
Pantel et. al. (2007). For now we stick to infer-
ence rules of the form r1(a1, a2) ==&gt;&apos; r2(a1, a2),
though our ideas are more generally applicable to
more complex rules. As an example, the rule (X
defeats Y) ==&gt;- (X plays Y) holds when X and Y
are both sports teams, however fails to produce a
reasonable inference if X and Y are Britain and
Nazi Germany respectively.
</bodyText>
<subsectionHeader confidence="0.804324">
4.3.1 Filtering Inferences
</subsectionHeader>
<bodyText confidence="0.9999955">
In order for an inference to be plausible, both re-
lations must have similar selectional preferences,
and further, the arguments must obey the selec-
tional preferences of both the antecedent r1 and
the consequent r2.4 Pantel et al. (2007) made
use of these intuitions by producing a set of class-
based selectional preferences for each relation,
then filtering out any inferences where the argu-
ments were incompatible with the intersection of
these preferences. In contrast, we take a proba-
bilistic approach, evaluating the quality of a spe-
cific inference by measuring the probability that
the arguments in both the antecedent and the con-
sequent were drawn from the same hidden topic
in our model. Note that this probability captures
both the requirement that the antecedent and con-
sequent have similar selectional preferences, and
that the arguments from a particular instance of the
rule’s application match their overlap.
We use zrz,j to denote the topic that generates
the jth argument of relation ri. The probability
that the two arguments a1, a2 were drawn from
the same hidden topic factorizes as follows due to
the conditional independences in our model:5
</bodyText>
<equation confidence="0.9999455">
P(zr1,1 = zr2,1, zr1,2 = zr2,2|a1, a2) =
P(zr1,1 = zr2,1|a1)P(zr1,2 = zr2,2|a2)
</equation>
<bodyText confidence="0.472668428571429">
4Similarity-based and discriminative methods are not ap-
plicable to this task as they offer no straightforward way
to compare the similarity between selectional preferences of
two relations.
5Note that all probabilities are conditioned on an estimate
of the parameters 0, ,Q, 7 from our model, which are omitted
for compactness.
</bodyText>
<figure confidence="0.9053181875">
LDA−SP
IndependentLDA
JointLDA
precision
0.4 0.6 0.8 1.0
LDA−SP
Jaccard
Mutual Information
0.4 0.6 0.8 1.0
precision
AUC
LDA-SP
0.833
430
0.4 0.6 0.8 1.0
precision
</figure>
<bodyText confidence="0.998488">
To compute each of these factors we simply
marginalize over the hidden topics:
</bodyText>
<equation confidence="0.997323">
T
P(zr1,j = zr2,j|aj) = E P(zr1,j = t|aj)P(zr2,j = t|aj)
t=1
</equation>
<bodyText confidence="0.98504">
where P(z = t|a) can be computed using
Bayes rule. For example,
</bodyText>
<equation confidence="0.966772">
P(a1|zr1,1 = t)P(zr1,1 = t)
P(zr1,1 = t|a1) =
P(a1)
</equation>
<figure confidence="0.890523909090909">
0.0 0.2 0.4 0.6 0.8 1.0
recall
LDA−SP
ISP.JIM
ISP.IIM−OR
X
O
X
O
= ,Qt(a1)Or1(t) Figure 5: Precision and recall on the inference fil-
P(a1) tering task.
</figure>
<subsectionHeader confidence="0.871149">
4.3.2 Experimental Conditions
</subsectionHeader>
<bodyText confidence="0.999780357142857">
In order to evaluate LDA-SP’s ability to filter in-
ferences based on selectional preferences we need
a set of inference rules between the relations in
our corpus. We therefore mapped the DIRT In-
ference rules (Lin and Pantel, 2001), (which con-
sist of pairs of dependency paths) to TEXTRUN-
NER relations as follows. We first gathered all in-
stances in the generalization corpus, and for each
r(a1, a2) created a corresponding simple sentence
by concatenating the arguments with the relation
string between them. Each such simple sentence
was parsed using Minipar (Lin, 1998). From
the parses we extracted all dependency paths be-
tween nouns that contain only words present in
the TEXTRUNNER relation string. These depen-
dency paths were then matched against each pair
in the DIRT database, and all pairs of associated
relations were collected producing about 26,000
inference rules.
Following Pantel et al. (2007) we randomly
sampled 100 inference rules. We then automati-
cally filtered out any rules which contained a nega-
tion, or for which the antecedent and consequent
contained a pair of antonyms found in WordNet
(this left us with 85 rules). For each rule we col-
lected 10 random instances of the antecedent, and
generated the consequent. We randomly sampled
300 of these inferences to hand-label.
</bodyText>
<sectionHeader confidence="0.770776" genericHeader="evaluation">
4.3.3 Results
</sectionHeader>
<bodyText confidence="0.815466955555556">
In figure 5 we compare the precision and recall of
LDA-SP against the top two performing systems
described by Pantel et al. (ISP.IIM-V and ISP.JIM,
both using the CBC clusters (Pantel, 2003)). We
find that LDA-SP achieves both higher precision
and recall than ISP.IIM-V. It is also able to achieve
the high-precision point of ISP.JIM and can trade
precision to get a much larger recall.
Top 10 Inference Rules Ranked by LDA-SP
antecedent consequent KL-div
will begin at will start at 0.014999
shall review shall determine 0.129434
may increase may reduce 0.214841
walk from walk to 0.219471
consume absorb 0.240730
shall keep shall maintain 0.264299
shall pay to will notify 0.290555
may apply for may obtain 0.313916
copy download 0.316502
should pay must pay 0.371544
Bottom 10 Inference Rules Ranked by LDA-SP
antecedent consequent KL-div
lose to shall take 10.011848
should play could do 10.028904
could play get in 10.048857
will start at move to 10.060994
shall keep will spend 10.105493
should play get in 10.131299
shall pay to leave for 10.131364
shall keep return to 10.149797
shall keep could do 10.178032
shall maintain have spent 10.221618
Table 3: Top 10 and Bottom 10 ranked inference
rules ranked by LDA-SPafter automatically filter-
ing out negations and antonyms (using WordNet).
In addition we demonstrate LDA-SP’s abil-
ity to rank inference rules by measuring the
Kullback Leibler Divergence6 between the topic-
distributions of the antecedent and consequent, Br1
and Bre respectively. Table 3 shows the top 10 and
bottom 10 rules out of the 26,000 ranked by KL
Divergence after automatically filtering antonyms
(using WordNet) and negations. For slight varia-
tions in rules (e.g., symmetric pairs) we mention
only one example to show more variety.
</bodyText>
<footnote confidence="0.908098333333333">
6KL-Divergence is an information-theoretic measure of
the similarity between two probability distributions, and de-
fined as follows: KL(P||Q) = Ex P(x) log PW Q�x�.
</footnote>
<page confidence="0.995214">
431
</page>
<subsectionHeader confidence="0.989867">
4.4 A Repository of Class-Based Preferences
</subsectionHeader>
<bodyText confidence="0.999966209302326">
Finally we explore LDA-SP’s ability to produce a
repository of human interpretable class-based se-
lectional preferences. As an example, for the re-
lation was born in, we would like to infer that
the plausible arguments include (person, location)
and (person, date).
Since we already have a set of topics, our
task reduces to mapping the inferred topics to an
equivalent class in a taxonomy (e.g., WordNet).
We experimented with automatic methods such
as Resnik’s, but found them to have all the same
problems as directly applying these approaches to
the SP task.7 Guided by the fact that we have a
relatively small number of topics (600 total, 300
for each argument) we simply chose to label them
manually. By labeling this small number of topics
we can infer class-based preferences for an arbi-
trary number of relations.
In particular, we applied a semi-automatic
scheme to map topics to WordNet. We first applied
Resnik’s approach to automatically shortlist a few
candidate WordNet classes for each topic. We then
manually picked the best class from the shortlist
that best represented the 20 top arguments for a
topic (similar to Table 1). We marked all incoher-
ent topics with a special symbol 0. This process
took one of the authors about 4 hours to complete.
To evaluate how well our topic-class associa-
tions carry over to unseen relations we used the
same random sample of 100 relations from the
pseudo-disambiguation experiment.8 For each ar-
gument of each relation we picked the top two top-
ics according to frequency in the 5 Gibbs samples.
We then discarded any topics which were labeled
with 0; this resulted in a set of 236 predictions. A
few examples are displayed in table 4.
We evaluated these classes and found the accu-
racy to be around 0.88. We contrast this with Pan-
tel’s repository,9 the only other released database
of selectional preferences to our knowledge. We
evaluated the same 100 relations from his website
and tagged the top 2 classes for each argument and
evaluated the accuracy to be roughly 0.55.
</bodyText>
<footnote confidence="0.95094575">
7Perhaps recent work on automatic coherence ranking
(Newman et al., 2010) and labeling (Mei et al., 2007) could
produce better results.
8Recall that these 100 were not part of the original 3,000
in the generalization corpus, and are, therefore, representative
of new “unseen” relations.
9http://demo.patrickpantel.com/
Content/LexSem/paraphrase.htm
</footnote>
<table confidence="0.890319">
arg1 class relation arg2 class
politician#1 was running for leader#1
people#1 will love show#3
organization#1 has responded to accusation#2
administrative unit#1 has appointed administrator#3
</table>
<tableCaption confidence="0.998272">
Table 4: Class-based Selectional Preferences.
</tableCaption>
<bodyText confidence="0.9997626">
We emphasize that tagging a pair of class-based
preferences is a highly subjective task, so these re-
sults should be treated as preliminary. Still, these
early results are promising. We wish to undertake
a larger scale study soon.
</bodyText>
<sectionHeader confidence="0.998102" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.998866782608696">
We have presented an application of topic mod-
eling to the problem of automatically computing
selectional preferences. Our method, LDA-SP,
learns a distribution over topics for each rela-
tion while simultaneously grouping related words
into these topics. This approach is capable of
producing human interpretable classes, however,
avoids the drawbacks of traditional class-based ap-
proaches (poor lexical coverage and ambiguity).
LDA-SP achieves state-of-the-art performance on
predictive tasks such as pseudo-disambiguation,
and filtering incorrect inferences.
Because LDA-SP generates a complete proba-
bilistic model for our relation data, its results are
easily applicable to many other tasks such as iden-
tifying similar relations, ranking inference rules,
etc. In the future, we wish to apply our model
to automatically discover new inference rules and
paraphrases.
Finally, our repository of selectional pref-
erences for 10,000 relations is available at
http://www.cs.washington.edu/
research/ldasp.
</bodyText>
<sectionHeader confidence="0.99749" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999574363636364">
We would like to thank Tim Baldwin, Colin
Cherry, Jesse Davis, Elena Erosheva, Stephen
Soderland, Dan Weld, in addition to the anony-
mous reviewers for helpful comments on a previ-
ous draft. This research was supported in part by
NSF grant IIS-0803481, ONR grant N00014-08-
1-0431, DARPA contract FA8750-09-C-0179, a
National Defense Science and Engineering Grad-
uate (NDSEG) Fellowship 32 CFR 168a, and car-
ried out at the University of Washington’s Turing
Center.
</bodyText>
<page confidence="0.998123">
432
</page>
<sectionHeader confidence="0.995884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99992085">
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
ACL-08: HLT.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional pref-
erence from unlabeled text. In EMNLP.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In EACL, pages 103–111,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Andrew Carlson, Justin Betteridge, Richard C. Wang,
Estevam R. Hruschka Jr., and Tom M. Mitchell.
2010. Coupled semi-supervised learning for infor-
mation extraction. In WSDM 2010.
Harr Chen, S. R. K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Global models of document
structure using latent permutations. In NAACL.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Comput. Linguist.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. In Machine Learning.
Hal Daum´e III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics.
Hal Daume III. 2007. hbc: Hierarchical bayes com-
piler. http://hal3.name/hbc.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics.
Elena Erosheva, Stephen Fienberg, and John Lafferty.
2004. Mixed-membership models of scientific pub-
lications. Proceedings of the National Academy of
Sciences of the United States of America.
Oren Etzioni, Michael Cafarella, Doug Downey,
Ana maria Popescu, Tal Shaked, Stephen Soderl,
Daniel S. Weld, and Alex Yates. 2005. Unsuper-
vised named-entity extraction from the web: An ex-
perimental study. Artificial Intelligence.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Comput. Linguist.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proc Natl Acad Sci U S A.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Comput.
Linguist.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In ACL-08: HLT.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the mdl principle.
Comput. Linguist.
Dekang Lin and Patrick Pantel. 2001. Dirt-discovery
of inference rules from text. In KDD.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proc. Workshop on the Evaluation of
Parsing Systems.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In KDD.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms for
topic models. JMLR.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of topic
coherence. In NAACL-HLT.
Diarmuid O´ S´eaghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard H. Hovy. 2007.
Isp: Learning inferential selectional preferences. In
HLT-NAACL.
Patrick Andre Pantel. 2003. Clustering by commit-
tee. Ph.D. thesis, University of Alberta, Edmonton,
Alta., Canada.
Joseph Reisinger and Marius Pasca. 2009. Latent vari-
able models of concept-attribute attachment. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP.
P. Resnik. 1996. Selectional constraints: an
information-theoretic model and its computational
realization. Cognition.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proc. of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why,
What, and How?
</reference>
<page confidence="0.992151">
433
</page>
<reference confidence="0.99937895">
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
Proceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics on Computa-
tional Linguistics.
Lenhart Schubert and Matthew Tong. 2003. Extract-
ing and evaluating general world knowledge from
the brown corpus. In In Proc. of the HLT-NAACL
Workshop on Text Meaning, pages 7–13.
Benjamin Van Durme and Daniel Gildea. 2009. Topic
models for corpus-centric knowledge generalization.
In Technical Report TR-946, Department of Com-
puter Science, University of Rochester, Rochester.
Tae Yano, William W. Cohen, and Noah A. Smith.
2009. Predicting response to political blog posts
with topic models. In NAACL.
L. Yao, D. Mimno, and A. Mccallum. 2009. Effi-
cient methods for topic model inference on stream-
ing document collections. In KDD.
</reference>
<page confidence="0.999111">
434
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.379349">
<title confidence="0.999874">A Latent Dirichlet Allocation method for Selectional Preferences</title>
<author confidence="0.999958">Alan Ritter</author>
<author confidence="0.999958">Mausam</author>
<author confidence="0.999958">Oren Etzioni</author>
<affiliation confidence="0.999876">Department of Computer Science and Engineering</affiliation>
<address confidence="0.999576">Box 352350, University of Washington, Seattle, WA 98195, USA</address>
<abstract confidence="0.997849916666667">computation of preferthe admissible argument values for a relation, is a well-known NLP task with applicability. We present which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. compare several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaleffectiveness at filtering improper applications of inference rules, where we show substantial improvement Pantel system (Pantel et al.,</abstract>
<intro confidence="0.388194">2007).</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>The tradeoffs between open and traditional relation extraction.</title>
<date>2008</date>
<booktitle>In ACL-08: HLT.</booktitle>
<contexts>
<context position="3947" citStr="Banko and Etzioni, 2008" startWordPosition="583" endWordPosition="586">DA that simultaneously models two sets of distributions for each topic. These two sets represent the two arguments for the relations. Thus, LDA-SP is able to capture information about the pairs of topics that commonly co-occur. This information is very helpful in guiding inference. We run LDA-SP to compute preferences on a massive dataset of binary relations r(a1, a2) ex424 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424–434, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics tracted from the Web by TEXTRUNNER (Banko and Etzioni, 2008). Our experiments demonstrate that LDA-SP significantly outperforms state of the art approaches obtaining an 85% increase in recall at precision 0.9 on the standard pseudodisambiguation task. Additionally, because LDA-SP is based on a formal probabilistic model, it has the advantage that it can naturally be applied in many scenarios. For example, we can obtain a better understanding of similar relations (Table 1), filter out incorrect inferences based on querying our model (Section 4.3), as well as produce a repository of class-based preferences with a little manual effort as demonstrated in S</context>
<context position="19296" citStr="Banko and Etzioni, 2008" startWordPosition="3069" endWordPosition="3072">007; Bergsma et al., 2008). We use this experiment to compare the various topic models as well as the best model with the known state of the art approaches to selectional preferences. Secondly, we show significant improvements to performance at an end-task of textual inference in Section 4.3. Finally, we report on the quality of a large database of Wordnet-based preferences obtained after manually associating our topics with Wordnet classes (Section 4.4). 4.1 Generalization Corpus For all experiments we make use of a corpus of r(a1, a2) tuples, which was automatically extracted by TEXTRUNNER (Banko and Etzioni, 2008) from 500 million Web pages. To create a generalization corpus from this large dataset. We first selected 3,000 relations from the middle of the tail (we used the 2,000- 5,000 most frequent ones)3 and collected all instances. To reduce sparsity, we discarded all tuples containing an NP that occurred fewer than 50 times in the data. This resulted in a vocabulary of about 32,000 noun phrases, and a set of about 2.4 million tuples in our generalization corpus. We inferred topic-argument and relation-topic multinomials (Q, &apos;y, and 0) on the generalization corpus by taking 5 samples at a lag of 50 </context>
</contexts>
<marker>Banko, Etzioni, 2008</marker>
<rawString>Michele Banko and Oren Etzioni. 2008. The tradeoffs between open and traditional relation extraction. In ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Discriminative learning of selectional preference from unlabeled text.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1632" citStr="Bergsma et al., 2008" startWordPosition="234" endWordPosition="237">s at filtering improper applications of inference rules, where we show substantial improvement over Pantel et al.’s system (Pantel et al., 2007). 1 Introduction Selectional Preferences encode the set of admissible argument values for a relation. For example, locations are likely to appear in the second argument of the relation X is headquartered in Y and companies or organizations in the first. A large, high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997), and many more. Therefore, much attention has been focused on automatically computing them based on a corpus of relation instances. Resnik (1996) presented the earliest work in this area, describing an information-theoretic approach that inferred selectional preferences based on the WordNet hypernym hierarchy. Recent work (Erk, 2007; Bergsma et al., 2008) has moved away from generalization to known classes, instead utilizing distributional similarity between nouns to generalize beyond observed relation-argument</context>
<context position="4995" citStr="Bergsma et al., 2008" startWordPosition="748" endWordPosition="751">t incorrect inferences based on querying our model (Section 4.3), as well as produce a repository of class-based preferences with a little manual effort as demonstrated in Section 4.4. In all these cases we obtain high quality results, for example, massively outperforming Pantel et al.’s approach in the textual inference task.1 2 Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable output, but often suffer in quality due to an incoherent taxonomy, inability to map arguments to a class (poor lexical co</context>
<context position="18698" citStr="Bergsma et al., 2008" startWordPosition="2973" endWordPosition="2976">approximation of the Gibbs Sampling procedure can be efficiently parallelized (Newman et al., 2009). Finally we note that, once a topic distribution has been learned over a set of training relations, one can efficiently apply inference to unseen relations (Yao et al., 2009). 4 Experiments We perform three main experiments to assess the quality of the preferences obtained using topic models. The first is a task-independent evaluation using a pseudo-disambiguation experiment (Section 4.2), which is a standard way to evaluate the quality of selectional preferences (Rooth et al., 1999; Erk, 2007; Bergsma et al., 2008). We use this experiment to compare the various topic models as well as the best model with the known state of the art approaches to selectional preferences. Secondly, we show significant improvements to performance at an end-task of textual inference in Section 4.3. Finally, we report on the quality of a large database of Wordnet-based preferences obtained after manually associating our topics with Wordnet classes (Section 4.4). 4.1 Generalization Corpus For all experiments we make use of a corpus of r(a1, a2) tuples, which was automatically extracted by TEXTRUNNER (Banko and Etzioni, 2008) f</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>Shane Bergsma, Dekang Lin, and Randy Goebel. 2008. Discriminative learning of selectional preference from unlabeled text. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.</journal>
<contexts>
<context position="2854" citStr="Blei et al., 2003" startWordPosition="412" endWordPosition="415">rs. This avoids problems like WordNet’s poor coverage of proper nouns and is shown to improve performance. These methods, however, no longer produce the generalized class for an argument. In this paper we describe a novel approach to computing selectional preferences by making use of unsupervised topic models. Our approach is able to combine benefits of both kinds of methods: it retains the generalization and humaninterpretability of class-based approaches and is also competitive with the direct methods on predictive tasks. Unsupervised topic models, such as latent Dirichlet allocation (LDA) (Blei et al., 2003) and its variants are characterized by a set of hidden topics, which represent the underlying semantic structure of a document collection. For our problem these topics offer an intuitive interpretation – they represent the (latent) set of classes that store the preferences for the different relations. Thus, topic models are a natural fit for modeling our relation data. In particular, our system, called LDA-SP, uses LinkLDA (Erosheva et al., 2004), an extension of LDA that simultaneously models two sets of distributions for each topic. These two sets represent the two arguments for the relation</context>
<context position="8556" citStr="Blei et al., 2003" startWordPosition="1285" endWordPosition="1288">ing, but needs to be retrained and tuned for each task. On the other hand, generative models produce complete probability distributions of the data, and hence can be integrated with other systems and tasks in a more principled manner (see Sections 4.2.2 and 4.3.1). Additionally, unlike LDA-SP Bergsma et al.’s system doesn’t produce human-interpretable topics. Finally, we note that LDA-SP and Bergsma’s system are potentially complimentary – the output of LDA-SP could be used to generate higher-quality training data for Bergsma, potentially improving their results. 425 Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). Finally, we highlight </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In EACL,</booktitle>
<pages>103--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="9132" citStr="Brody and Lapata, 2009" startWordPosition="1377" endWordPosition="1380">25 Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). Finally, we highlight two systems, developed independently of our own, which apply LDA-style models to similar tasks. O´ S´eaghdha (2010) proposes a series of LDA-style models for the task of computing selectional preferences. This work learns selectional preferences between the following grammatical relations: verb-object, nounnoun, and adjective-noun. It also focuses on jointly modeling the generation of both predicate and argument, and evaluation is performed on a set of human-plausibility judgments obtaining impressive results against Keller and Lapata’s (2003) Web hit-count based system</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In EACL, pages 103–111, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Richard C Wang</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Coupled semi-supervised learning for information extraction.</title>
<date>2010</date>
<booktitle>In WSDM</booktitle>
<contexts>
<context position="27526" citStr="Carlson et al., 2010" startWordPosition="4511" endWordPosition="4514">tains 85% more recall at 0.9 precision compared to mutual information. Overall LDA-SP obtains an 15% increase in the area under precision-recall curve over mutual information. All three systems’ AUCs are shown in Table 2; LDA-SP’s improvements over both Jaccard and mutual information are highly significant with a significance level less than 0.01 using a paired t-test. In addition to a superior performance in selectional preference evaluation LDA-SP also produces a set of coherent topics, which can be useful in their own right. For instance, one could use them for tasks such as set-expansion (Carlson et al., 2010) or automatic thesaurus induction (EtMI-Sim Jaccard-Sim 0.727 0.711 Table 2: Area under the precision recall curve. LDA-SP’s AUC is significantly higher than both similarity-based methods according to a paired ttest with a significance level below 0.01. zioni et al., 2005; Kozareva et al., 2008). 4.3 End Task Evaluation We now evaluate LDA-SP’s ability to improve performance at an end-task. We choose the task of improving textual entailment by learning selectional preferences for inference rules and filtering inferences that do not respect these. This application of selectional preferences was</context>
</contexts>
<marker>Carlson, Betteridge, Wang, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Richard C. Wang, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Coupled semi-supervised learning for information extraction. In WSDM 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harr Chen</author>
<author>S R K Branavan</author>
<author>Regina Barzilay</author>
<author>David R Karger</author>
</authors>
<title>Global models of document structure using latent permutations.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="8737" citStr="Chen et al., 2009" startWordPosition="1315" endWordPosition="1318"> other systems and tasks in a more principled manner (see Sections 4.2.2 and 4.3.1). Additionally, unlike LDA-SP Bergsma et al.’s system doesn’t produce human-interpretable topics. Finally, we note that LDA-SP and Bergsma’s system are potentially complimentary – the output of LDA-SP could be used to generate higher-quality training data for Bergsma, potentially improving their results. 425 Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). Finally, we highlight two systems, developed independently of our own, which apply LDA-style models to similar tasks. O´ S´eaghdha (2010) proposes a series of LDA-style models for the task of computing s</context>
</contexts>
<marker>Chen, Branavan, Barzilay, Karger, 2009</marker>
<rawString>Harr Chen, S. R. K. Branavan, Regina Barzilay, and David R. Karger. 2009. Global models of document structure using latent permutations. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Comput. Linguist.</journal>
<contexts>
<context position="4873" citStr="Clark and Weir, 2002" startWordPosition="730" endWordPosition="733">be applied in many scenarios. For example, we can obtain a better understanding of similar relations (Table 1), filter out incorrect inferences based on querying our model (Section 4.3), as well as produce a repository of class-based preferences with a little manual effort as demonstrated in Section 4.4. In all these cases we obtain high quality results, for example, massively outperforming Pantel et al.’s approach in the textual inference task.1 2 Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Stephen Clark and David Weir. 2002. Class-based probability estimation using a semantic hierarchy. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Similarity-based models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>In Machine Learning.</booktitle>
<contexts>
<context position="4944" citStr="Dagan et al., 1999" startWordPosition="741" endWordPosition="744">anding of similar relations (Table 1), filter out incorrect inferences based on querying our model (Section 4.3), as well as produce a repository of class-based preferences with a little manual effort as demonstrated in Section 4.4. In all these cases we obtain high quality results, for example, massively outperforming Pantel et al.’s approach in the textual inference task.1 2 Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable output, but often suffer in quality due to an incoherent taxonomy, ina</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando C. N. Pereira. 1999. Similarity-based models of word cooccurrence probabilities. In Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian query-focused summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Bayesian query-focused summarization. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<date>2007</date>
<note>hbc: Hierarchical bayes compiler. http://hal3.name/hbc.</note>
<marker>Daume, 2007</marker>
<rawString>Hal Daume III. 2007. hbc: Hierarchical bayes compiler. http://hal3.name/hbc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="971" citStr="Erk, 2007" startWordPosition="132" endWordPosition="133">l-known NLP task with broad applicability. We present LDA-SP, which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, LDA-SP combines the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. We compare LDA-SP to several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaluate LDA-SP’s effectiveness at filtering improper applications of inference rules, where we show substantial improvement over Pantel et al.’s system (Pantel et al., 2007). 1 Introduction Selectional Preferences encode the set of admissible argument values for a relation. For example, locations are likely to appear in the second argument of the relation X is headquartered in Y and companies or organizations in the first. A large, high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including semantic role labeling (Gildea a</context>
<context position="4956" citStr="Erk, 2007" startWordPosition="745" endWordPosition="746">lations (Table 1), filter out incorrect inferences based on querying our model (Section 4.3), as well as produce a repository of class-based preferences with a little manual effort as demonstrated in Section 4.4. In all these cases we obtain high quality results, for example, massively outperforming Pantel et al.’s approach in the textual inference task.1 2 Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable output, but often suffer in quality due to an incoherent taxonomy, inability to ma</context>
<context position="18675" citStr="Erk, 2007" startWordPosition="2971" endWordPosition="2972">9), and an approximation of the Gibbs Sampling procedure can be efficiently parallelized (Newman et al., 2009). Finally we note that, once a topic distribution has been learned over a set of training relations, one can efficiently apply inference to unseen relations (Yao et al., 2009). 4 Experiments We perform three main experiments to assess the quality of the preferences obtained using topic models. The first is a task-independent evaluation using a pseudo-disambiguation experiment (Section 4.2), which is a standard way to evaluate the quality of selectional preferences (Rooth et al., 1999; Erk, 2007; Bergsma et al., 2008). We use this experiment to compare the various topic models as well as the best model with the known state of the art approaches to selectional preferences. Secondly, we show significant improvements to performance at an end-task of textual inference in Section 4.3. Finally, we report on the quality of a large database of Wordnet-based preferences obtained after manually associating our topics with Wordnet classes (Section 4.4). 4.1 Generalization Corpus For all experiments we make use of a corpus of r(a1, a2) tuples, which was automatically extracted by TEXTRUNNER (Ban</context>
<context position="20514" citStr="Erk, 2007" startWordPosition="3277" endWordPosition="3278">burn in of 750 iterations. Using multiple samples introduces the risk of topic drift due to lack of identifiability, however we found this to not be a problem in practice. During development we found that the topics tend to remain stable across multiple samples after sufficient burn in, and multiple samples improved performance. Table 1 lists sample topics and high ranked words for each (for both arguments) as well as relations favoring those topics. 4.2 Task Independent Evaluation We first compare the three LDA-based approaches to each other and two state of the art similarity based systems (Erk, 2007) (using mutual information and Jaccard similarity respectively). These similarity measures were shown to outperform the generative model of Rooth et al. (1999), as well as class-based methods such as Resnik’s. In this pseudo-disambiguation experiment an observed tuple is paired with a pseudo-negative, which has both arguments randomly generated from the whole vocabulary (according to the corpus-wide distribution over arguments). The task is, for each relation-argument pair, to determine whether it is observed, or a random distractor. 4.2.1 Test Set For this experiment we gathered a primary cor</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Katrin Erk. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Erosheva</author>
<author>Stephen Fienberg</author>
<author>John Lafferty</author>
</authors>
<title>Mixed-membership models of scientific publications.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America.</booktitle>
<contexts>
<context position="3304" citStr="Erosheva et al., 2004" startWordPosition="484" endWordPosition="487">sed approaches and is also competitive with the direct methods on predictive tasks. Unsupervised topic models, such as latent Dirichlet allocation (LDA) (Blei et al., 2003) and its variants are characterized by a set of hidden topics, which represent the underlying semantic structure of a document collection. For our problem these topics offer an intuitive interpretation – they represent the (latent) set of classes that store the preferences for the different relations. Thus, topic models are a natural fit for modeling our relation data. In particular, our system, called LDA-SP, uses LinkLDA (Erosheva et al., 2004), an extension of LDA that simultaneously models two sets of distributions for each topic. These two sets represent the two arguments for the relations. Thus, LDA-SP is able to capture information about the pairs of topics that commonly co-occur. This information is very helpful in guiding inference. We run LDA-SP to compute preferences on a massive dataset of binary relations r(a1, a2) ex424 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424–434, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics tracted from the </context>
<context position="14924" citStr="Erosheva et al., 2004" startWordPosition="2332" endWordPosition="2335">for a2. This results in poor generalization because the data for a single class is divided into multiple topics. In order to address this problem while maintaining the sharing of influence between a1 and a2, we next present LinkLDA, which represents a compromise between IndependentLDA and JointLDA. LinkLDA is more flexible than JointLDA, allowing different topics to be chosen for a1, and a2, however still models the generation of topics from the same distribution for a given relation. 3.3 LinkLDA Figure 2 illustrates the LinkLDA model in the plate notation, which is analogous to the model in (Erosheva et al., 2004). In particular note that each ai is drawn from a different hidden topic zi, however the zi’s are drawn from the same distribution 0r for a given relation r. To facilitate learning related topic pairs between arguments we employ a sparse prior over the per-relation topic distributions. Because a few topics are likely to be assigned most of the probability mass for a given relation it is more likely (although not necessary) that the same topic number k will be drawn for both arguments. When comparing LinkLDA with JointLDA the better model may not seem immediately clear. On the one hand, JointLD</context>
</contexts>
<marker>Erosheva, Fienberg, Lafferty, 2004</marker>
<rawString>Elena Erosheva, Stephen Fienberg, and John Lafferty. 2004. Mixed-membership models of scientific publications. Proceedings of the National Academy of Sciences of the United States of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>Ana maria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderl</author>
<author>Daniel S Weld</author>
<author>Alex Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence.</journal>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderl, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, Ana maria Popescu, Tal Shaked, Stephen Soderl, Daniel S. Weld, and Alex Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Comput. Linguist.</journal>
<contexts>
<context position="1589" citStr="Gildea and Jurafsky, 2002" startWordPosition="228" endWordPosition="231">k, 2007). We also evaluate LDA-SP’s effectiveness at filtering improper applications of inference rules, where we show substantial improvement over Pantel et al.’s system (Pantel et al., 2007). 1 Introduction Selectional Preferences encode the set of admissible argument values for a relation. For example, locations are likely to appear in the second argument of the relation X is headquartered in Y and companies or organizations in the first. A large, high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997), and many more. Therefore, much attention has been focused on automatically computing them based on a corpus of relation instances. Resnik (1996) presented the earliest work in this area, describing an information-theoretic approach that inferred selectional preferences based on the WordNet hypernym hierarchy. Recent work (Erk, 2007; Bergsma et al., 2008) has moved away from generalization to known classes, instead utilizing distributional similarity between nouns to g</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<journal>Proc Natl Acad Sci U S A.</journal>
<contexts>
<context position="7380" citStr="Griffiths and Steyvers, 2004" startWordPosition="1104" endWordPosition="1107">bably the closest to our work is a model proposed by Rooth et al. (1999), in which each class corresponds to a multinomial over relations and arguments and EM is used to learn the parameters of the model. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Recently, Bergsma et. al. (2008) proposed the first discriminative approach to selectional preferences. Their insight that pseudo-negative examples could be used as training data allows the application of an SVM classifier, which makes use of many features in addition to the relation-argument co-occurrence frequencies used by other methods. They automatically generated positive and negative examples by selecting arguments having high and low mutual information with the relation. Since it is a discriminative approach it is amenable to feature engineering, but needs to be retrained and tuned f</context>
<context position="16752" citStr="Griffiths and Steyvers, 2004" startWordPosition="2642" endWordPosition="2645">from the same (sparse) distribution 0r. LinkLDA can thus re-use argument classes, choosing different combinations of topics for the arguments if it fits the data better. In Section 4 we show experimentally that LinkLDA outperforms JointLDA (and IndependentLDA) by wide margins. We use LDA-SP to refer to LinkLDA in all the experiments below. 3.4 Inference For all the models we use collapsed Gibbs sampling for inference in which each of the hidden variables (e.g., zr,i,1 and zr,i,2 in LinkLDA) are sampled sequentially conditioned on a fullassignment to all others, integrating out the parameters (Griffiths and Steyvers, 2004). This produces robust parameter estimates, as it allows computation of expectations over the posterior distribution Figure 1: JointLDA Figure 2: LinkLDA a 771 a 1 a2 a z B 712 7 T N �R1 z1 z2 a1 a2 771 a a B 772 7 T N �R1 427 as opposed to estimating maximum likelihood parameters. In addition, the integration allows the use of sparse priors, which are typically more appropriate for natural language data. In all experiments we use hyperparameters α = 771 = 772 = 0.1. We generated initial code for our samplers using the Hierarchical Bayes Compiler (Daume III, 2007). 3.5 Advantages of Topic Mode</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proc Natl Acad Sci U S A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Mirella Lapata</author>
</authors>
<title>Using the web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Comput. Linguist.</journal>
<marker>Keller, Lapata, 2003</marker>
<rawString>Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In ACL-08: HLT.</booktitle>
<contexts>
<context position="27822" citStr="Kozareva et al., 2008" startWordPosition="4557" endWordPosition="4560"> significant with a significance level less than 0.01 using a paired t-test. In addition to a superior performance in selectional preference evaluation LDA-SP also produces a set of coherent topics, which can be useful in their own right. For instance, one could use them for tasks such as set-expansion (Carlson et al., 2010) or automatic thesaurus induction (EtMI-Sim Jaccard-Sim 0.727 0.711 Table 2: Area under the precision recall curve. LDA-SP’s AUC is significantly higher than both similarity-based methods according to a paired ttest with a significance level below 0.01. zioni et al., 2005; Kozareva et al., 2008). 4.3 End Task Evaluation We now evaluate LDA-SP’s ability to improve performance at an end-task. We choose the task of improving textual entailment by learning selectional preferences for inference rules and filtering inferences that do not respect these. This application of selectional preferences was introduced by Pantel et. al. (2007). For now we stick to inference rules of the form r1(a1, a2) ==&gt;&apos; r2(a1, a2), though our ideas are more generally applicable to more complex rules. As an example, the rule (X defeats Y) ==&gt;- (X plays Y) holds when X and Y are both sports teams, however fails t</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the mdl principle.</title>
<date>1998</date>
<journal>Comput. Linguist.</journal>
<contexts>
<context position="4851" citStr="Li and Abe, 1998" startWordPosition="726" endWordPosition="729"> it can naturally be applied in many scenarios. For example, we can obtain a better understanding of similar relations (Table 1), filter out incorrect inferences based on querying our model (Section 4.3), as well as produce a repository of class-based preferences with a little manual effort as demonstrated in Section 4.4. In all these cases we obtain high quality results, for example, massively outperforming Pantel et al.’s approach in the textual inference task.1 2 Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the mdl principle. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt-discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="30934" citStr="Lin and Pantel, 2001" startWordPosition="5072" endWordPosition="5075">rginalize over the hidden topics: T P(zr1,j = zr2,j|aj) = E P(zr1,j = t|aj)P(zr2,j = t|aj) t=1 where P(z = t|a) can be computed using Bayes rule. For example, P(a1|zr1,1 = t)P(zr1,1 = t) P(zr1,1 = t|a1) = P(a1) 0.0 0.2 0.4 0.6 0.8 1.0 recall LDA−SP ISP.JIM ISP.IIM−OR X O X O = ,Qt(a1)Or1(t) Figure 5: Precision and recall on the inference filP(a1) tering task. 4.3.2 Experimental Conditions In order to evaluate LDA-SP’s ability to filter inferences based on selectional preferences we need a set of inference rules between the relations in our corpus. We therefore mapped the DIRT Inference rules (Lin and Pantel, 2001), (which consist of pairs of dependency paths) to TEXTRUNNER relations as follows. We first gathered all instances in the generalization corpus, and for each r(a1, a2) created a corresponding simple sentence by concatenating the arguments with the relation string between them. Each such simple sentence was parsed using Minipar (Lin, 1998). From the parses we extracted all dependency paths between nouns that contain only words present in the TEXTRUNNER relation string. These dependency paths were then matched against each pair in the DIRT database, and all pairs of associated relations were col</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt-discovery of inference rules from text. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of minipar.</title>
<date>1998</date>
<booktitle>In Proc. Workshop on the Evaluation of Parsing Systems.</booktitle>
<contexts>
<context position="31274" citStr="Lin, 1998" startWordPosition="5128" endWordPosition="5129">ring task. 4.3.2 Experimental Conditions In order to evaluate LDA-SP’s ability to filter inferences based on selectional preferences we need a set of inference rules between the relations in our corpus. We therefore mapped the DIRT Inference rules (Lin and Pantel, 2001), (which consist of pairs of dependency paths) to TEXTRUNNER relations as follows. We first gathered all instances in the generalization corpus, and for each r(a1, a2) created a corresponding simple sentence by concatenating the arguments with the relation string between them. Each such simple sentence was parsed using Minipar (Lin, 1998). From the parses we extracted all dependency paths between nouns that contain only words present in the TEXTRUNNER relation string. These dependency paths were then matched against each pair in the DIRT database, and all pairs of associated relations were collected producing about 26,000 inference rules. Following Pantel et al. (2007) we randomly sampled 100 inference rules. We then automatically filtered out any rules which contained a negation, or for which the antecedent and consequent contained a pair of antonyms found in WordNet (this left us with 85 rules). For each rule we collected 10</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of minipar. In Proc. Workshop on the Evaluation of Parsing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xuehua Shen</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Automatic labeling of multinomial topic models.</title>
<date>2007</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="36118" citStr="Mei et al., 2007" startWordPosition="5920" endWordPosition="5923">frequency in the 5 Gibbs samples. We then discarded any topics which were labeled with 0; this resulted in a set of 236 predictions. A few examples are displayed in table 4. We evaluated these classes and found the accuracy to be around 0.88. We contrast this with Pantel’s repository,9 the only other released database of selectional preferences to our knowledge. We evaluated the same 100 relations from his website and tagged the top 2 classes for each argument and evaluated the accuracy to be roughly 0.55. 7Perhaps recent work on automatic coherence ranking (Newman et al., 2010) and labeling (Mei et al., 2007) could produce better results. 8Recall that these 100 were not part of the original 3,000 in the generalization corpus, and are, therefore, representative of new “unseen” relations. 9http://demo.patrickpantel.com/ Content/LexSem/paraphrase.htm arg1 class relation arg2 class politician#1 was running for leader#1 people#1 will love show#3 organization#1 has responded to accusation#2 administrative unit#1 has appointed administrator#3 Table 4: Class-based Selectional Preferences. We emphasize that tagging a pair of class-based preferences is a highly subjective task, so these results should be tr</context>
</contexts>
<marker>Mei, Shen, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. 2007. Automatic labeling of multinomial topic models. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="9081" citStr="Mimno et al., 2009" startWordPosition="1369" endWordPosition="1372">Bergsma, potentially improving their results. 425 Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). Finally, we highlight two systems, developed independently of our own, which apply LDA-style models to similar tasks. O´ S´eaghdha (2010) proposes a series of LDA-style models for the task of computing selectional preferences. This work learns selectional preferences between the following grammatical relations: verb-object, nounnoun, and adjective-noun. It also focuses on jointly modeling the generation of both predicate and argument, and evaluation is performed on a set of human-plausibility judgments obtaining impressive results against Ke</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Arthur Asuncion</author>
<author>Padhraic Smyth</author>
<author>Max Welling</author>
</authors>
<title>Distributed algorithms for topic models.</title>
<date>2009</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="18176" citStr="Newman et al., 2009" startWordPosition="2890" endWordPosition="2893">tead, they compute the classes automatically. This leads to better lexical coverage since the issue of matching a new argument to a known class is side-stepped. Second, the models naturally handle ambiguous arguments, as they are able to assign different topics to the same phrase in different contexts. Inference in these models is also scalable – linear in both the size of the corpus as well as the number of topics. In addition, there are several scalability enhancements such as SparseLDA (Yao et al., 2009), and an approximation of the Gibbs Sampling procedure can be efficiently parallelized (Newman et al., 2009). Finally we note that, once a topic distribution has been learned over a set of training relations, one can efficiently apply inference to unseen relations (Yao et al., 2009). 4 Experiments We perform three main experiments to assess the quality of the preferences obtained using topic models. The first is a task-independent evaluation using a pseudo-disambiguation experiment (Section 4.2), which is a standard way to evaluate the quality of selectional preferences (Rooth et al., 1999; Erk, 2007; Bergsma et al., 2008). We use this experiment to compare the various topic models as well as the be</context>
</contexts>
<marker>Newman, Asuncion, Smyth, Welling, 2009</marker>
<rawString>David Newman, Arthur Asuncion, Padhraic Smyth, and Max Welling. 2009. Distributed algorithms for topic models. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="36086" citStr="Newman et al., 2010" startWordPosition="5914" endWordPosition="5917">ed the top two topics according to frequency in the 5 Gibbs samples. We then discarded any topics which were labeled with 0; this resulted in a set of 236 predictions. A few examples are displayed in table 4. We evaluated these classes and found the accuracy to be around 0.88. We contrast this with Pantel’s repository,9 the only other released database of selectional preferences to our knowledge. We evaluated the same 100 relations from his website and tagged the top 2 classes for each argument and evaluated the accuracy to be roughly 0.55. 7Perhaps recent work on automatic coherence ranking (Newman et al., 2010) and labeling (Mei et al., 2007) could produce better results. 8Recall that these 100 were not part of the original 3,000 in the generalization corpus, and are, therefore, representative of new “unseen” relations. 9http://demo.patrickpantel.com/ Content/LexSem/paraphrase.htm arg1 class relation arg2 class politician#1 was running for leader#1 people#1 will love show#3 organization#1 has responded to accusation#2 administrative unit#1 has appointed administrator#3 Table 4: Class-based Selectional Preferences. We emphasize that tagging a pair of class-based preferences is a highly subjective tas</context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O´ S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Rahul Bhagat</author>
<author>Bonaventura Coppola</author>
<author>Timothy Chklovski</author>
<author>Eduard H Hovy</author>
</authors>
<title>Isp: Learning inferential selectional preferences.</title>
<date>2007</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="1155" citStr="Pantel et al., 2007" startWordPosition="157" endWordPosition="160">ent topics and topic distributions over relations, LDA-SP combines the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. We compare LDA-SP to several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaluate LDA-SP’s effectiveness at filtering improper applications of inference rules, where we show substantial improvement over Pantel et al.’s system (Pantel et al., 2007). 1 Introduction Selectional Preferences encode the set of admissible argument values for a relation. For example, locations are likely to appear in the second argument of the relation X is headquartered in Y and companies or organizations in the first. A large, high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997), and many more. Therefore, much attenti</context>
<context position="4895" citStr="Pantel et al., 2007" startWordPosition="734" endWordPosition="737">narios. For example, we can obtain a better understanding of similar relations (Table 1), filter out incorrect inferences based on querying our model (Section 4.3), as well as produce a repository of class-based preferences with a little manual effort as demonstrated in Section 4.4. In all these cases we obtain high quality results, for example, massively outperforming Pantel et al.’s approach in the textual inference task.1 2 Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable output, but often suf</context>
<context position="28772" citStr="Pantel et al. (2007)" startWordPosition="4717" endWordPosition="4720">al. (2007). For now we stick to inference rules of the form r1(a1, a2) ==&gt;&apos; r2(a1, a2), though our ideas are more generally applicable to more complex rules. As an example, the rule (X defeats Y) ==&gt;- (X plays Y) holds when X and Y are both sports teams, however fails to produce a reasonable inference if X and Y are Britain and Nazi Germany respectively. 4.3.1 Filtering Inferences In order for an inference to be plausible, both relations must have similar selectional preferences, and further, the arguments must obey the selectional preferences of both the antecedent r1 and the consequent r2.4 Pantel et al. (2007) made use of these intuitions by producing a set of classbased selectional preferences for each relation, then filtering out any inferences where the arguments were incompatible with the intersection of these preferences. In contrast, we take a probabilistic approach, evaluating the quality of a specific inference by measuring the probability that the arguments in both the antecedent and the consequent were drawn from the same hidden topic in our model. Note that this probability captures both the requirement that the antecedent and consequent have similar selectional preferences, and that the</context>
<context position="31611" citStr="Pantel et al. (2007)" startWordPosition="5179" endWordPosition="5182">NER relations as follows. We first gathered all instances in the generalization corpus, and for each r(a1, a2) created a corresponding simple sentence by concatenating the arguments with the relation string between them. Each such simple sentence was parsed using Minipar (Lin, 1998). From the parses we extracted all dependency paths between nouns that contain only words present in the TEXTRUNNER relation string. These dependency paths were then matched against each pair in the DIRT database, and all pairs of associated relations were collected producing about 26,000 inference rules. Following Pantel et al. (2007) we randomly sampled 100 inference rules. We then automatically filtered out any rules which contained a negation, or for which the antecedent and consequent contained a pair of antonyms found in WordNet (this left us with 85 rules). For each rule we collected 10 random instances of the antecedent, and generated the consequent. We randomly sampled 300 of these inferences to hand-label. 4.3.3 Results In figure 5 we compare the precision and recall of LDA-SP against the top two performing systems described by Pantel et al. (ISP.IIM-V and ISP.JIM, both using the CBC clusters (Pantel, 2003)). We f</context>
</contexts>
<marker>Pantel, Bhagat, Coppola, Chklovski, Hovy, 2007</marker>
<rawString>Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard H. Hovy. 2007. Isp: Learning inferential selectional preferences. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Andre Pantel</author>
</authors>
<title>Clustering by committee.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Alberta,</institution>
<location>Edmonton, Alta., Canada.</location>
<contexts>
<context position="5275" citStr="Pantel, 2003" startWordPosition="794" endWordPosition="795"> approach in the textual inference task.1 2 Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable output, but often suffer in quality due to an incoherent taxonomy, inability to map arguments to a class (poor lexical coverage), and word sense ambiguity. Because of these limitations researchers have investigated non-class based approaches, which attempt to directly classify a given noun-phrase as plausible/implausible for a relation. Of these, the similarity based approaches make use of a distri</context>
<context position="32204" citStr="Pantel, 2003" startWordPosition="5280" endWordPosition="5281">Pantel et al. (2007) we randomly sampled 100 inference rules. We then automatically filtered out any rules which contained a negation, or for which the antecedent and consequent contained a pair of antonyms found in WordNet (this left us with 85 rules). For each rule we collected 10 random instances of the antecedent, and generated the consequent. We randomly sampled 300 of these inferences to hand-label. 4.3.3 Results In figure 5 we compare the precision and recall of LDA-SP against the top two performing systems described by Pantel et al. (ISP.IIM-V and ISP.JIM, both using the CBC clusters (Pantel, 2003)). We find that LDA-SP achieves both higher precision and recall than ISP.IIM-V. It is also able to achieve the high-precision point of ISP.JIM and can trade precision to get a much larger recall. Top 10 Inference Rules Ranked by LDA-SP antecedent consequent KL-div will begin at will start at 0.014999 shall review shall determine 0.129434 may increase may reduce 0.214841 walk from walk to 0.219471 consume absorb 0.240730 shall keep shall maintain 0.264299 shall pay to will notify 0.290555 may apply for may obtain 0.313916 copy download 0.316502 should pay must pay 0.371544 Bottom 10 Inference </context>
</contexts>
<marker>Pantel, 2003</marker>
<rawString>Patrick Andre Pantel. 2003. Clustering by committee. Ph.D. thesis, University of Alberta, Edmonton, Alta., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Marius Pasca</author>
</authors>
<title>Latent variable models of concept-attribute attachment.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</booktitle>
<contexts>
<context position="8808" citStr="Reisinger and Pasca, 2009" startWordPosition="1323" endWordPosition="1326">ons 4.2.2 and 4.3.1). Additionally, unlike LDA-SP Bergsma et al.’s system doesn’t produce human-interpretable topics. Finally, we note that LDA-SP and Bergsma’s system are potentially complimentary – the output of LDA-SP could be used to generate higher-quality training data for Bergsma, potentially improving their results. 425 Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). Finally, we highlight two systems, developed independently of our own, which apply LDA-style models to similar tasks. O´ S´eaghdha (2010) proposes a series of LDA-style models for the task of computing selectional preferences. This work learns selectional preferences betwee</context>
</contexts>
<marker>Reisinger, Pasca, 2009</marker>
<rawString>Joseph Reisinger and Marius Pasca. 2009. Latent variable models of concept-attribute attachment. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selectional constraints: an information-theoretic model and its computational realization.</title>
<date>1996</date>
<journal>Cognition.</journal>
<contexts>
<context position="1861" citStr="Resnik (1996)" startWordPosition="269" endWordPosition="270">elation. For example, locations are likely to appear in the second argument of the relation X is headquartered in Y and companies or organizations in the first. A large, high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997), and many more. Therefore, much attention has been focused on automatically computing them based on a corpus of relation instances. Resnik (1996) presented the earliest work in this area, describing an information-theoretic approach that inferred selectional preferences based on the WordNet hypernym hierarchy. Recent work (Erk, 2007; Bergsma et al., 2008) has moved away from generalization to known classes, instead utilizing distributional similarity between nouns to generalize beyond observed relation-argument pairs. This avoids problems like WordNet’s poor coverage of proper nouns and is shown to improve performance. These methods, however, no longer produce the generalized class for an argument. In this paper we describe a novel app</context>
<context position="4833" citStr="Resnik, 1996" startWordPosition="724" endWordPosition="725">advantage that it can naturally be applied in many scenarios. For example, we can obtain a better understanding of similar relations (Table 1), filter out incorrect inferences based on querying our model (Section 4.3), as well as produce a repository of class-based preferences with a little manual effort as demonstrated in Section 4.4. In all these cases we obtain high quality results, for example, massively outperforming Pantel et al.’s approach in the textual inference task.1 2 Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These </context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>P. Resnik. 1996. Selectional constraints: an information-theoretic model and its computational realization. Cognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proc. of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why,</booktitle>
<location>What, and How?</location>
<contexts>
<context position="1715" citStr="Resnik, 1997" startWordPosition="246" endWordPosition="247">ent over Pantel et al.’s system (Pantel et al., 2007). 1 Introduction Selectional Preferences encode the set of admissible argument values for a relation. For example, locations are likely to appear in the second argument of the relation X is headquartered in Y and companies or organizations in the first. A large, high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997), and many more. Therefore, much attention has been focused on automatically computing them based on a corpus of relation instances. Resnik (1996) presented the earliest work in this area, describing an information-theoretic approach that inferred selectional preferences based on the WordNet hypernym hierarchy. Recent work (Erk, 2007; Bergsma et al., 2008) has moved away from generalization to known classes, instead utilizing distributional similarity between nouns to generalize beyond observed relation-argument pairs. This avoids problems like WordNet’s poor coverage of proper nouns and is sh</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In Proc. of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Glenn Carroll</author>
<author>Franz Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via em-based clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics.</booktitle>
<contexts>
<context position="5053" citStr="Rooth et al., 1999" startWordPosition="757" endWordPosition="760">4.3), as well as produce a repository of class-based preferences with a little manual effort as demonstrated in Section 4.4. In all these cases we obtain high quality results, for example, massively outperforming Pantel et al.’s approach in the textual inference task.1 2 Previous Work Previous work on selectional preferences can be broken into four categories: class-based approaches (Resnik, 1996; Li and Abe, 1998; Clark and Weir, 2002; Pantel et al., 2007), similarity based approaches (Dagan et al., 1999; Erk, 2007), discriminative (Bergsma et al., 2008), and generative probabilistic models (Rooth et al., 1999). Class-based approaches, first proposed by Resnik (1996), are the most studied of the four. They make use of a pre-defined set of classes, either manually produced (e.g. WordNet), or automatically generated (Pantel, 2003). For each relation, some measure of the overlap between the classes and observed arguments is used to identify those that best describe the arguments. These techniques produce a human-interpretable output, but often suffer in quality due to an incoherent taxonomy, inability to map arguments to a class (poor lexical coverage), and word sense ambiguity. Because of these limita</context>
<context position="6823" citStr="Rooth et al. (1999)" startWordPosition="1017" endWordPosition="1020">retic classbased method on a pseudo-disambiguation evaluation. These methods obtain better lexical coverage, but are unable to obtain any abstract representation of selectional preferences. Our solution fits into the general category of generative probabilistic models, which model each relation/argument combination as being generated by a latent class variable. These classes are automatically learned from the data. This retains the class-based flavor of the problem, without the knowledge limitations of the explicit classbased approaches. Probably the closest to our work is a model proposed by Rooth et al. (1999), in which each class corresponds to a multinomial over relations and arguments and EM is used to learn the parameters of the model. In contrast, we use a LinkLDA framework in which each relation is associated with a corresponding multinomial distribution over classes, and each argument is drawn from a class-specific distribution over words; LinkLDA captures co-occurrence of classes in the two arguments. Additionally we perform full Bayesian inference using collapsed Gibbs sampling, in which parameters are integrated out (Griffiths and Steyvers, 2004). Recently, Bergsma et. al. (2008) proposed</context>
<context position="18664" citStr="Rooth et al., 1999" startWordPosition="2967" endWordPosition="2970">LDA (Yao et al., 2009), and an approximation of the Gibbs Sampling procedure can be efficiently parallelized (Newman et al., 2009). Finally we note that, once a topic distribution has been learned over a set of training relations, one can efficiently apply inference to unseen relations (Yao et al., 2009). 4 Experiments We perform three main experiments to assess the quality of the preferences obtained using topic models. The first is a task-independent evaluation using a pseudo-disambiguation experiment (Section 4.2), which is a standard way to evaluate the quality of selectional preferences (Rooth et al., 1999; Erk, 2007; Bergsma et al., 2008). We use this experiment to compare the various topic models as well as the best model with the known state of the art approaches to selectional preferences. Secondly, we show significant improvements to performance at an end-task of textual inference in Section 4.3. Finally, we report on the quality of a large database of Wordnet-based preferences obtained after manually associating our topics with Wordnet classes (Section 4.4). 4.1 Generalization Corpus For all experiments we make use of a corpus of r(a1, a2) tuples, which was automatically extracted by TEXT</context>
<context position="20673" citStr="Rooth et al. (1999)" startWordPosition="3298" endWordPosition="3301"> problem in practice. During development we found that the topics tend to remain stable across multiple samples after sufficient burn in, and multiple samples improved performance. Table 1 lists sample topics and high ranked words for each (for both arguments) as well as relations favoring those topics. 4.2 Task Independent Evaluation We first compare the three LDA-based approaches to each other and two state of the art similarity based systems (Erk, 2007) (using mutual information and Jaccard similarity respectively). These similarity measures were shown to outperform the generative model of Rooth et al. (1999), as well as class-based methods such as Resnik’s. In this pseudo-disambiguation experiment an observed tuple is paired with a pseudo-negative, which has both arguments randomly generated from the whole vocabulary (according to the corpus-wide distribution over arguments). The task is, for each relation-argument pair, to determine whether it is observed, or a random distractor. 4.2.1 Test Set For this experiment we gathered a primary corpus by first randomly selecting 100 high-frequency relations not in the generalization corpus. For each relation we collected all tuples containing arguments i</context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically annotated lexicon via em-based clustering. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lenhart Schubert</author>
<author>Matthew Tong</author>
</authors>
<title>Extracting and evaluating general world knowledge from the brown corpus. In</title>
<date>2003</date>
<booktitle>In Proc. of the HLT-NAACL Workshop on Text Meaning,</booktitle>
<pages>7--13</pages>
<contexts>
<context position="9873" citStr="Schubert and Tong, 2003" startWordPosition="1484" endWordPosition="1487">s. O´ S´eaghdha (2010) proposes a series of LDA-style models for the task of computing selectional preferences. This work learns selectional preferences between the following grammatical relations: verb-object, nounnoun, and adjective-noun. It also focuses on jointly modeling the generation of both predicate and argument, and evaluation is performed on a set of human-plausibility judgments obtaining impressive results against Keller and Lapata’s (2003) Web hit-count based system. Van Durme and Gildea (2009) proposed applying LDA to general knowledge templates extracted using the KNEXT system (Schubert and Tong, 2003). In contrast, our work uses LinkLDA and focuses on modeling multiple arguments of a relation (e.g., the subject and direct object of a verb). 3 Topic Models for Selectional Prefs. We present a series of topic models for the task of computing selectional preferences. These models vary in the amount of independence they assume between a1 and a2. At one extreme is IndependentLDA, a model which assumes that both a1 and a2 are generated completely independently. On the other hand, JointLDA, the model at the other extreme (Figure 1) assumes both arguments of a specific extraction are generated base</context>
</contexts>
<marker>Schubert, Tong, 2003</marker>
<rawString>Lenhart Schubert and Matthew Tong. 2003. Extracting and evaluating general world knowledge from the brown corpus. In In Proc. of the HLT-NAACL Workshop on Text Meaning, pages 7–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Daniel Gildea</author>
</authors>
<title>Topic models for corpus-centric knowledge generalization. In</title>
<date>2009</date>
<tech>Technical Report TR-946,</tech>
<institution>Department of Computer Science, University of Rochester,</institution>
<location>Rochester.</location>
<marker>Van Durme, Gildea, 2009</marker>
<rawString>Benjamin Van Durme and Daniel Gildea. 2009. Topic models for corpus-centric knowledge generalization. In Technical Report TR-946, Department of Computer Science, University of Rochester, Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tae Yano</author>
<author>William W Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting response to political blog posts with topic models.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="9004" citStr="Yano et al., 2009" startWordPosition="1358" endWordPosition="1361">output of LDA-SP could be used to generate higher-quality training data for Bergsma, potentially improving their results. 425 Topic models such as LDA (Blei et al., 2003) and its variants have recently begun to see use in many NLP applications such as summarization (Daum´e III and Marcu, 2006), document alignment and segmentation (Chen et al., 2009), and inferring class-attribute hierarchies (Reisinger and Pasca, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al., 2009), modeling topic-aligned articles in different languages (Mimno et al., 2009), and word sense induction (Brody and Lapata, 2009). Finally, we highlight two systems, developed independently of our own, which apply LDA-style models to similar tasks. O´ S´eaghdha (2010) proposes a series of LDA-style models for the task of computing selectional preferences. This work learns selectional preferences between the following grammatical relations: verb-object, nounnoun, and adjective-noun. It also focuses on jointly modeling the generation of both predicate and argument, and evaluation is performed on </context>
</contexts>
<marker>Yano, Cohen, Smith, 2009</marker>
<rawString>Tae Yano, William W. Cohen, and Noah A. Smith. 2009. Predicting response to political blog posts with topic models. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Yao</author>
<author>D Mimno</author>
<author>A Mccallum</author>
</authors>
<title>Efficient methods for topic model inference on streaming document collections.</title>
<date>2009</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="18068" citStr="Yao et al., 2009" startWordPosition="2874" endWordPosition="2877"> class-based nature of selectional preferences, but don’t take a pre-defined set of classes as input. Instead, they compute the classes automatically. This leads to better lexical coverage since the issue of matching a new argument to a known class is side-stepped. Second, the models naturally handle ambiguous arguments, as they are able to assign different topics to the same phrase in different contexts. Inference in these models is also scalable – linear in both the size of the corpus as well as the number of topics. In addition, there are several scalability enhancements such as SparseLDA (Yao et al., 2009), and an approximation of the Gibbs Sampling procedure can be efficiently parallelized (Newman et al., 2009). Finally we note that, once a topic distribution has been learned over a set of training relations, one can efficiently apply inference to unseen relations (Yao et al., 2009). 4 Experiments We perform three main experiments to assess the quality of the preferences obtained using topic models. The first is a task-independent evaluation using a pseudo-disambiguation experiment (Section 4.2), which is a standard way to evaluate the quality of selectional preferences (Rooth et al., 1999; Er</context>
</contexts>
<marker>Yao, Mimno, Mccallum, 2009</marker>
<rawString>L. Yao, D. Mimno, and A. Mccallum. 2009. Efficient methods for topic model inference on streaming document collections. In KDD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>