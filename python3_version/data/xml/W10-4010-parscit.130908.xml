<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9950885">
Towards multi-lingual summarization: A comparative analysis of
sentence extraction methods on English and Hebrew corpora
</title>
<author confidence="0.98674">
Marina Litvak and Hagay Lipman and Assaf Ben Gur and Mark Last
</author>
<affiliation confidence="0.985116">
Ben Gurion University of the Negev
</affiliation>
<email confidence="0.951086">
{litvakm, lipmanh, bengura, mlast}@bgu.ac.il
</email>
<author confidence="0.996197">
Slava Kisilevich and Daniel Keim
</author>
<affiliation confidence="0.998938">
University of Konstanz
</affiliation>
<email confidence="0.7385545">
slaks@dbvis.inf.uni-konstanz.de
Daniel.Keim@uni-konstanz.de
</email>
<sectionHeader confidence="0.987597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948083333334">
The trend toward the growing multi-
linguality of the Internet requires text
summarization techniques that work
equally well in multiple languages. Only
some of the automated summarization
methods proposed in the literature, how-
ever, can be defined as “language-
independent”, as they are not based on
any morphological analysis of the sum-
marized text. In this paper, we per-
form an in-depth comparative analysis of
language-independent sentence scoring
methods for extractive single-document
summarization. We evaluate 15 pub-
lished summarization methods proposed
in the literature and 16 methods intro-
duced in (Litvak et al., 2010). The eval-
uation is performed on English and He-
brew corpora. The results suggest that
the performance ranking of the com-
pared methods is quite similar in both
languages. The top ten bilingual scoring
methods include six methods introduced
in (Litvak et al., 2010).
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965634146342">
Automatically generated summaries can signif-
icantly reduce the information overload on pro-
fessionals in a variety of fields, could prove ben-
eficial for the automated classification and fil-
tering of documents, the search for information
over the Internet and applications that utilize
large textual databases.
Document summarization methodologies in-
clude statistic-based, using either the classic vec-
tor space model or a graph representation, and
semantic-based, using ontologies and language-
specific knowledge (Mani &amp; Maybury, 1999).
Although the use of language-specific knowl-
edge can potentially improve the quality of auto-
mated summaries generated in a particular lan-
guage, its language specificity ultimately re-
stricts the use of such a summarizer to a sin-
gle language. Only systems that perform equally
well on different languages in the absence of any
language-specific knowledge can be considered
language-independent summarizers.
As the number of languages used on the In-
ternet increases continiously (there are at least
75 different languages according to a estimate
performed by A. Gulli and A. Signorini1 in the
end of January 2005), there is a growing need
for language-independent statistical summariza-
tion techniques that can be readily applied to text
in any language without using language-specific
morphological tools.
In this work, we perform an in-depth com-
parative analysis of 16 methods for language-
independent extractive summarization intro-
duced in (Litvak et al., 2010) that utilize ei-
ther vector or graph-based representations of text
documents computed from word segmentation
and 15 state-of-the art language-independent
scoring methods. The main goal of the eval-
uation experiments, which focused on English
and Hebrew corpora, is to find the most efficient
language-independent sentence scoring methods
</bodyText>
<footnote confidence="0.996821">
1http://www.cs.uiowa.edu/ asignori/web-size/
</footnote>
<page confidence="0.979041">
61
</page>
<note confidence="0.686734">
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 61–69,
Beijing, August 2010
</note>
<bodyText confidence="0.9997599">
in terms of summarization accuracy and com-
putational complexity across two different lan-
guages.
This paper is organized as follows. The
next section describes related work in extrac-
tive summarization. Section 3 reviews the evalu-
ated language-independent sentence scoring ap-
proaches. Section 4 contains our experimental
results on English and Hebrew corpora. The last
section comprises conclusions and future work.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999937296296296">
Extractive summarization is aimed at the selec-
tion of a subset of the most relevant fragments,
which can be paragraphs, sentences, keyphrases,
or keywords from a given source text. The ex-
tractive summarization process usually involves
ranking, such that each fragment of a summa-
rized text gets a relevance score, and extraction,
during which the top-ranked fragments are ex-
tracted and arranged in a summary in the same
order they appeared in the original text. Statisti-
cal methods for calculating the relevance score
of each fragment can rely on such informa-
tion as: fragment position inside the document,
its length, whether it contains keywords or title
words.
Research by Luhn (1958), in which the sig-
nificance factor of a sentence is based on the
frequency and the relative position of significant
words within that sentence, is considered the first
on automated text summarization. Luhn’s work
was followed shortly thereafter by that of Ed-
mundson (1969) and some time later by stud-
ies from Radev et al. (2001) and Saggion et al.
(2003), all of who applied linear combinations
of multiple statistical methods to rank sentences
using the vector space model as a text representa-
tion. In (Litvak et al., 2010) we improve the sum-
marization quality by identifying the best linear
combination of the metrics evaluated in this pa-
per.
Several information retrieval and machine
learning techniques have been proposed for de-
termining sentence importance (Kupiec et al.,
1995; Wong et al., 2008). Gong and Liu (2001)
and Steinberger and Jezek (2004) showed that
singular value decomposition (SVD) can be ap-
plied to generate extracts.
Among text representation models, graph-
based text representations have gained popular-
ity in automated summarization, as they enable
the model to be enriched with syntactic and se-
mantic relations. Salton et al. (1997) were
among the first to attempt graph-based ranking
methods for single document extractive summa-
rization by generating similarity links between
document paragraphs. The important paragraphs
of a text were extracted using degree scores.
Erkan and Radev (2004) and Mihalcea (2005) in-
troduced approaches for unsupervised extractive
summarization that rely on the application of it-
erative graph based ranking algorithms. In their
approaches, each document is represented as a
graph of sentences interconnected by similarity
relations.
</bodyText>
<sectionHeader confidence="0.992663" genericHeader="method">
3 Language-Independent Scoring
Methods for Sentence Extraction
</sectionHeader>
<bodyText confidence="0.999917608695652">
Various language dependent and independent
sentence scoring methods have been introduced
in the literature. We selected the 15 most promi-
nent language independent methods for evalua-
tion. Most of them can be categorized as fre-
quency, position, length, or title-based, and they
utilize vector representation. TextRank (ML TR)
is the only method that is based on graph repre-
sentation, but there are also position and length-
based methods that calculate scores using the
overall structure of a document. We have also
considered 16 methods proposed in (Litvak et al.,
2010), including 13 based on the graph-theoretic
representation (Section 3.1).
Figure 1 (Litvak et al., 2010) shows the taxon-
omy of the 31 methods considered in our work.
All methods introduced in (Litvak et al., 2010)
are denoted by an asterisk (*). Methods requir-
ing a threshold value t ∈ [0, 1] that specifies the
portion of the top rated terms considered signifi-
cant are marked by a cross in Figure 1 and listed
in Table 1 along with the optimal average thresh-
old values obtained after evaluating the methods
</bodyText>
<page confidence="0.999781">
62
</page>
<tableCaption confidence="0.997889">
Table 1: Selected thresholds for threshold-based
scoring methods
</tableCaption>
<table confidence="0.9920616">
Method Threshold
LUHN 0.9
LUHN DEG 0.9
LUHN PR 0.0
KEY [0.8, 1.0]
KEY DEG [0.8, 1.0]
KEY PR [0.1, 1.0]
COV 0.9
COV DEG [0.7, 0.9]
COV PR 0.1
</table>
<bodyText confidence="0.998914583333333">
on English and Hebrew documents (Litvak et al.,
2010).
The methods are divided into three main cat-
egories: structure-, vector-, and graph-based
methods, and each category also contains an
internal taxonomy. Sections 3.2, 3.3, and
3.4 present structure-, vector-, and graph-based
methods, respectively. With each description, a
reference to the original work where the method
was proposed for extractive summarization is in-
cluded. We denote sentence by 5 and text docu-
ment by D.
</bodyText>
<subsectionHeader confidence="0.998891">
3.1 Text Representation Models
</subsectionHeader>
<bodyText confidence="0.999910416666667">
The vector-based scoring methods listed below
use tf or tf-idf term weights to evaluate sen-
tence importance while that used by the graph-
based methods (except for TextRank) is based
on the word-based graph representation model
presented in Schenker et al. (2004). We repre-
sent each document by a directed, labeled, un-
weighted graph in which nodes represent unique
terms (distinct normalized words) and edges rep-
resent order-relationships between two terms.
Each edge is labeled with the IDs of sentences
that contain both words in the specified order.
</bodyText>
<subsectionHeader confidence="0.999912">
3.2 Structure-based Scoring Methods
</subsectionHeader>
<bodyText confidence="0.99872825">
In this section, we describe the existing
structure-based methods for multilingual sen-
tence scoring. These methods do not require any
text representation and are based on its structure.
</bodyText>
<listItem confidence="0.504893833333333">
– Position (Baxendale, 1958):
POS L Closeness to the end of the document:
score(5i) = i, where i is a sequential number of
a sentence in a document;
POS F Closeness to the beginning of the docu-
ment: score(5i) = 1i ;
</listItem>
<bodyText confidence="0.732799714285714">
POS B Closeness to the borders of the docu-
ment: score(5i) = max(1i , 1
n−i+1), where n is
the total number of sentences in D.
– Length (Satoshi et al., 2001):
LEN W Number of words in a sentence;
LEN CH Number of characters in a sentence.
</bodyText>
<subsectionHeader confidence="0.994051">
3.3 Vector-based Scoring Methods
</subsectionHeader>
<bodyText confidence="0.997397">
In this section, we describe the vector-based
methods for multilingual sentence scoring, that
are based on the vector space model for text rep-
resentation.
</bodyText>
<equation confidence="0.451664666666667">
– Frequency-based:
LUHN (Luhn, 1958)
score(5) = maxciE{clusters(S)}{csi}, where
</equation>
<bodyText confidence="0.752685166666667">
clusters are portions of a sentence brack-
eted by keywords2 and csi = |keywords(ci)|2 |ci|
KEY (Edmundson, 1969) Sum of the keyword
frequencies: score(5) = EiE{keywords(S)} tfi,
where tfi is term in-document frequency of
keyword i.
</bodyText>
<equation confidence="0.761021428571429">
COV (Kallel et al., 2004) Ratio of keyword
numbers (Coverage): score(5) = |keywords(S)|
|keywords(D)|
TF (Vanderwende et al., 2007) Average term
frequency for all sentence words:
/�i∈{words(S)} tfi
score(5) = r
</equation>
<bodyText confidence="0.772393">
2Luhn’s experiments suggest an optimal limit of 4 or 5
non-significant words between keywords.
3Due to multilingual focus of our work, exact word
matching was used in all similarity-based methods.
</bodyText>
<figure confidence="0.857835">
S
TFISF (Neto et
</figure>
<figureCaption confidence="0.8800775">
al., 2000) Average term
frequency inverted sentence frequency
for all sentence words: score(5) =
&amp;E{words(S)} tfi X isfi,
</figureCaption>
<bodyText confidence="0.8895866">
where isfi = 1 − log(ni), where n is the number
log(n)
of sentences in a document and ni is the number
of sentences containing word i.
SVD (Steinberger &amp; Jezek, 2004) score(5)
is equal to the length of a sentence vector
in E2VT after computing the Singular Value
Decomposition of a term by sentence matrix
A = UEVT
– Title (Edmundson, 1969) similarity3 to the
title, score(5) = sim(5,T):
TITLE O using overlap similarity: |S∩T|
min{|S|,|T|}
TITLE J using Jaccard similarity: S∩T
�S∪Tj
</bodyText>
<page confidence="0.815913">
63
</page>
<figure confidence="0.9996364">
Multilingual sentence
scoringmethods
D_COV_O*
D_COV_J*
D_COV_C*
TITLE_O
TITLE_J
TITLE_C
Structure-
based
Vector-
based
Similarity
Position
Length
Frequency
Title
Document
POS_F
POS_L
POS_B
LEN_W
LEN_CH
LUHN†
KEY †
COV †
TF
TFIISF
SVD
Graph-
based
Pagerank
Similarity
Degree
LUHN_DEG † *
KEY_DEG † *
COV_DEG † *
DEG*
GRASE*
LUHN_PR † *
KEY_PR † *
COV PR † *
FR*
ML_TR
Title
Document
TITLE_E_O*
TITLE_E_J*
D_COV_E_O*
D_COV_E_J*
</figure>
<figureCaption confidence="0.999831">
Figure 1: Taxonomy of statistical language-independent sentence scoring methods (Litvak et al.,
</figureCaption>
<equation confidence="0.68967425">
2010)
TITLE C using cosine similarity:
sim(~S, T~) = cos(~S, T~ ) = ~S×~T
|~S|×|~T|
</equation>
<bodyText confidence="0.996611444444444">
– Document Coverage (Litvak et al., 2010).
These methods score a sentence according to
its similarity to the rest of the sentences in
the document (D − S) based on the following
intuition: the more document content is covered
by a sentence, the more important the sentence is
to a summary. Redundant sentences containing
repetitive information are removed using a
similarity filter. score(S) = sim(S, D − S):
</bodyText>
<equation confidence="0.739440875">
D COV O using Overlap similarity:
|S∩T |
min{|S|,|D−S|}
D COV J using Jaccard similarity: |S∩T |
|S∪D−S|
D COV C using Cosine similarity:
cos(~S, D ~− S) = ~S× ~D−S
|~S|× |~D−S|
</equation>
<subsectionHeader confidence="0.980745">
3.4 Graph-based Scoring Methods
</subsectionHeader>
<bodyText confidence="0.9019073">
In this section, we describe the methods for mul-
tilingual sentence scoring using the graph text
representation based on sentence (ML TR) or
word (all except ML TR) segmentation.
ML TR Multilingual version of Tex-
tRank (Mihalcea, 2005) without morphological
analysis. Each document is represented as a
directed graph of nodes that stand for sen-
tences interconnected by similarity (overlap)
relationship. To each edge connecting two
vertices the weight is assigned and equal to
the similarity value between the corresponding
sentences. We used backward links, as it was
the most successful according to the reported
results in (Mihalcea, 2005). score(S) is equal
to PageRank (Brin &amp; Page, 1998) of its node,
according to the formula adapted to the weights
assigned to edges.
– Degree-based (Litvak et al., 2010):4
LUHN DEG A graph-based extension of the
LUHN measure, in which a node degree is
used instead of a word frequency: words are
considered significant if they are represented
by nodes of a higher degree than a predefined
threshold (see Table 1).
KEY DEG Graph-based extension of KEY
measure.
COV DEG Graph-based extension of COV
measure.
DEG Average degree for all sentence nodes:
</bodyText>
<equation confidence="0.8823415">
score(S) = K∈{wor ds(S)} Degi .
|S|
</equation>
<bodyText confidence="0.593927222222222">
GRASE(GRaph-based Automated Sentence
Extractor) Modification of Salton’s algo-
rithm (Salton et al., 1997) using the graph
4All proposed here degree-based methods, except for
GRASE, use undirected graphs and degree of nodes as a
predictive feature. The methods based on the directed word
graphs and distinguishing between in- and out-links were
outperformed in our preliminary experiments by the undi-
rected approach.
</bodyText>
<page confidence="0.997738">
64
</page>
<bodyText confidence="0.999386636363636">
representation defined in Section 3.1 above.
In our graph representation, all sentences are
represented by paths, completely or partially.
To identify the relevant sentences, we search
for the bushy paths and extract from them the
sentences that appear the most frequently. Each
sentence in the bushy path gets a domination
score that is the number of edges with its label
in the path normalized by the sentence length.
The relevance score for a sentence is calculated
as a sum of its domination scores over all paths.
</bodyText>
<table confidence="0.951899833333333">
– PageRank-based:5
LUHN PR A graph-based extension of the
LUHN measure in which the node PageRank
value is used instead of the word frequency:
keywords are those words represented by nodes
with a PageRank score higher than a predefined
threshold (see Table 1).
KEY PR Graph-based extension of KEY mea-
sure.
COV PR Graph-based extension of COV mea-
sure.
PR Average PageRank for all sentence nodes:
</table>
<equation confidence="0.7535175">
L-�i∈{wor
score(S) =ds(S)} PRi|S|
</equation>
<bodyText confidence="0.816764615384616">
– Similarity-based. Edge matching techniques
similar to those of Nastase and Szpakowicz
(2006) are used. Edge matching is an alternative
approach to measure the similarity between
graphs based on the number of common edges:
TITLE E O Graph-based extension of TI-
TLE O – Overlap-based edge matching between
title and sentence graphs.
TITLE E J Graph-based extension of TITLE J
– Jaccard-based edge matching between title
and sentence graphs.
D COV E O Graph-based extension of
D COV O – Overlap-based edge matching
between sentence and document complement
(the rest of a document sentences) graphs.
D COV E J Graph-based extension of
D COV J – Jaccard-based edge matching
5Using undirected word graphs with PageRank does not
make sense, since for an undirected graph a node pagerank
score is known to be proportional to its degree. Revers-
ing links will result in hub scores instead authority. The
methods distinguishing between authority and hub scores
were outperformed in our preliminary experiments by the
degree-based approach.
between sentence and document complement
graphs.
</bodyText>
<sectionHeader confidence="0.999321" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.926969">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.97990284">
The quality of the above-mentioned sentence
ranking methods was evaluated through a com-
parative experiment on corpora of English and
Hebrew texts. These two languages, which
belong to different language families (Indo-
European and Semitic languages, respectively),
were intentionally chosen for this experiment to
increase the generality of our evaluation. The
main difference between these languages, is that
Hebrew morphology allows morphemes to be
combined systematically into complex word-
forms. In different contexts, the same morpheme
can appear as a separate word-form, while in oth-
ers it appears agglutinated as a suffix or prefix to
another word-form (Adler, 2009).
The goals of the experiment were as follows:
- To evaluate the performance of different ap-
proaches for extractive single-document summa-
rization using graph and vector representations.
- To compare the quality of the multilingual sum-
marization methods proposed in our previous
work (Litvak et al., 2010) to the state-of-the-art
approaches.
- To identify sentence ranking methods that work
equally well on both languages.
</bodyText>
<subsectionHeader confidence="0.98782">
4.2 Text Preprocessing
</subsectionHeader>
<bodyText confidence="0.999993285714286">
Extractive summarization relies critically on
proper sentence segmentation to insure the qual-
ity of the summarization results. We used a sen-
tence splitter provided with the MEAD summa-
rizer (Radev et al., 2001) for English and a sim-
ple splitter for Hebrew splitting the text at every
period, exclamation point, or question mark.6
</bodyText>
<subsectionHeader confidence="0.995469">
4.3 Experimental Data
</subsectionHeader>
<bodyText confidence="0.775036">
For English texts, we used the corpus of sum-
marized documents provided for the single doc-
</bodyText>
<footnote confidence="0.998585333333333">
6Although the same set of splitting rules may be used
for both languages, separate splitters were used since the
MEAD splitter is restricted to European languages.
</footnote>
<page confidence="0.999694">
65
</page>
<bodyText confidence="0.999988785714286">
ument summarization task at the Document
Understanding Conference 2002 (DUC, 2002).
This benchmark dataset contains 533 news arti-
cles, each of which is at least ten sentences long
and has two to three human-generated abstracts
of approximately 100 words apiece.
However, to the best of our knowledge, no
summarization benchmarks exist for the Hebrew
language texts. To collect summarized texts in
Hebrew, we set up an experiment7 in which 50
news articles of 250 to 830 words each from the
Haaretz8 newspaper internet site were summa-
rized by human assessors by extracting the most
salient sentences. In total, 70 undergraduate stu-
dents from the Department of Information Sys-
tems Engineering, Ben Gurion University of the
Negev participated in the experiment. Ten doc-
uments were randomly assigned to each of the
70 study participants who were instructed (1)
To dedicate at least five minutes to each doc-
ument, (2) To ignore dialogs and citations, (3)
To read the whole document before starting sen-
tence extraction, (4) To ignore redundant, repet-
itive, or overly detailed information, (5) To obey
the minimal and maximal summary constraints
of 95 and 100 words, respectively. Summaries
were assessed for quality by procedure described
in (Litvak et al., 2010).
</bodyText>
<subsectionHeader confidence="0.992042">
4.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.99983275">
We evaluated English and Hebrew summaries
using the ROUGE-1, 2, 3, 4, L, SU and W met-
rics9, described in Lin (2004). Our results were
not statistically distinguishable and matched the
conclusion of Lin (2004). However, because
ROUGE-1 showed the largest variation across
the methods, all results in the following com-
parisons are presented in terms of ROUGE-1
metric. Similar to the approach described
in Dang (2006), we performed multiple com-
parisons between the sentence scoring methods.
The Friedman test was used to reject the null hy-
</bodyText>
<footnote confidence="0.87668">
7The software enabling easy selection and storage of
sentences to be included in the document extract, can be
provided upon request.
8http://www.haaretz.co.il
9ROUGE toolkit was adapted to Hebrew by specifying
“token” using Hebrew alphabet
</footnote>
<tableCaption confidence="0.929313333333333">
Table 2: English: Multiple comparisons of sen-
tence ranking approaches using the Bonferroni-
Dunn test of ROUGE-1 Recall
</tableCaption>
<table confidence="0.99994559375">
Approach ROUGE-1
COV DEG* 0.436 A
KEY DEG* 0.433 A B
KEY 0.429 A B C
COV PR* 0.428 A B C D
COV 0.428 A B C D
D COV C* 0.428 A B C D
D COV J* 0.425 B C D E
KEY PR* 0.424 B C D E
LUHN DEG* 0.422 C D E F
POS F 0.419 E F G
LEN CH 0.418 C D E F G
LUHN 0.418 D E F G
LUHN PR* 0.418 E F G H
LEN W 0.416 D E F G H
ML TR 0.414 E F G H
TITLE E J* 0.413 F G H I
TITLE E O* 0.413 F G H I
D COV E J* 0.410 F G H I
D COV O* 0.405 G H I J
TFISF 0.405 G H I J
DEG* 0.403 G H I J
D COV E O* 0.401 H I J K
PR* 0.400 G H I J K
TITLE J 0.399 I J K
TF 0.397 I J K
TITLE O 0.396 J K
SVD 0.395 I J K
TITLE C 0.395 J K
POS B 0.392 K L
GRASE* 0.372 L
POS L 0.339 M
</table>
<bodyText confidence="0.94729537037037">
pothesis (all methods perform the same) at the
0.0001 significance level, after which we ran the
Bonferroni-Dunn test (Demsar, 2006) for pair-
wise comparisons. Tables 2 and 3 show the re-
sults of multiple comparisons and are arranged
in descending order with the best approaches
on top. Methods not sharing any common let-
ter were significantly different at the 95% confi-
dence level.
The Pearson correlation between methods
ranking in English and Hebrew was 0.775, which
was larger than zero at a significance level of
0.0001. In other words, most of the methods
were ranked in nearly the same relative positions
in both corpora, and the top ranked methods per-
formed equally well in both languages. The dif-
ferences in ranking were caused by morphologi-
cal differences between two languages.
To determine which approaches performed
best in both languages, we analyzed the cluster-
ing results of the methods in both corpora and
found the intersection of the top clusters from
the two clustering results. For each language,
a document-method matrix of ROUGE scores
was created with methods represented by vec-
tors of their ROUGE scores for each document
in a corpora. Since most scores are not normally
</bodyText>
<page confidence="0.995157">
66
</page>
<tableCaption confidence="0.9255635">
Table 3: Hebrew: Multiple comparisons of sen-
tence ranking approaches using the Bonferroni-
Dunn test of ROUGE-1 Recall
Table 5: Hebrew: Correlation between sentence
</tableCaption>
<table confidence="0.990155294117647">
ranking approaches using Pearson
Approach Correlated With
Approach ROUGE-1
D COV J* 0.574 A
KEY 0.570 A B
COV DEG* 0.568 A B
POS F 0.567 A B
COV 0.567 A B
TITLE J 0.567 A B
POS B 0.565 A B
LUHN PR* 0.560 A B C
LUHN DEG* 0.560 A B C
D COV E J* 0.559 A B C
LUHN 0.559 A B C
TITLE E J* 0.556 A B C
TITLE E O* 0.556 A B C
KEY DEG* 0.555 A B C
LEN W 0.555 A B C
LEN CH 0.553 A B C
KEY PR* 0.546 A B C
COV PR* 0.546 A B C
TITLE O 0.545 A B C
D COV C* 0.543 A B C
TITLE C 0.541 A B C
ML TR 0.519 A B C D
TFISF 0.514 A B C D
D COV E O* 0.498 A B C D
SVD 0.498 A B C D
D COV O* 0.466 B C D
TF 0.427 C D E
DEG* 0.399 D E F
PR* 0.331 E F
GRASE* 0.243 F
POS L 0.237 F
</table>
<tableCaption confidence="0.8678835">
Table 4: English: Correlation between sentence
ranking approaches using Pearson
</tableCaption>
<table confidence="0.956330666666667">
Approach Correlated With
POS F (LUHN PR, 0.973), (TITLE E J, 0.902), (TITLE E O, 0.902)
TITLE O (TITLE J, 0.950)
LEN W (LEN CH, 0.909)
KEY PR (COV PR, 0.944)
TITLE E O (TITLE E J, 0.997)
</table>
<bodyText confidence="0.999664238095238">
distributed, we chose the K-means algorithm,
which does not assume normal distribution of
data, for clustering. We ran the algorithm with
different numbers of clusters (2 G K G 10),
and for each K, we measured two parameters:
the minimal distance between neighboring clus-
ters in the clustered data for each language and
the level of similarity between the clustering re-
sults for the two languages. For both param-
eters, we used the regular Euclidean distance.
For K ≥ 6, the clusters were highly similar
for each language, and the distance between En-
glish and Hebrew clustering data was maximal.
Based on the obtained results, we left results
only for 2 G K G 5 for each corpus. Then,
we ordered the clusters by the average ROUGE
score of each cluster’s instances (methods) and
identified the methods appearing in the top clus-
ters for all K values in both corpora. Table 6
shows the resulting top ten scoring methods with
their rank in each corpus. Six methods intro-
</bodyText>
<table confidence="0.9982313">
KEY (KEY DEG, 0.930)
COV (D COV J, 0.911)
POS F (POS B, 0.945), (LUHN DEG, 0.959), (LUHN PR, 0.958)
POS B (LUHN DEG, 0.927), (LUHN PR, 0.925)
TITLE O (TITLE E J, 0.920), (TITLE E O, 0.920)
TITLE J (TITLE E J, 0.942), (TITLE E O, 0.942)
LEN W (LEN CH, 0.954), (KEY PR, 0.912)
LEN CH (KEY PR, 0.936), (KEY DEG, 0.915), (COV DEG, 0.901)
LUHN DEG (LUHN PR, 0.998)
KEY DEG (COV DEG, 0.904)
</table>
<tableCaption confidence="0.982004">
Table 6: Ranking of the best bilingual scores
</tableCaption>
<table confidence="0.9997535">
Scoring Rank in Rank in Text
method English corpus Hebrew corpus Representation
KEY 3 2 vector
COV 4 4 vector
KEY DEG 2 10 graph
COV DEG 1 3 graph
KEY PR 6 12 graph
COV PR 4 12 graph
D COV C 4 14 vector
D COV J 5 1 vector
LEN W 10 10 structure
LEN CH 9 11 structure
</table>
<bodyText confidence="0.999650266666667">
duced in this paper, such as Document Cover-
age (D COV C/J) and graph adaptations of Cov-
erage (COV DEG/PR) and Key (KEY DEG/PR),
are among these top ten bilingual methods.
Neither vector- nor graph-based text represen-
tation models, however, can claim ultimate supe-
riority, as methods based on both models promi-
nently in the top-evaluated cluster. Moreover,
highly-correlated methods (see Tables 4 and 5
for highly-correlated pairs of methods in English
and Hebrew corpora, respectively) appear in the
same cluster in most cases. As a result, some
pairs from among the top ten methods are highly-
correlated in at least one language, and only one
from each pair can be considered. For example,
LEN W and LEN CH have high correlation coef-
ficients (0.909 and 0.954 in English and Hebrew,
respectively). Since LEN CH is more appropri-
ate for multilingual processing due to variations
in the rules of tokenization between languages
(e.g., English vs. German), it may be considered
a preferable multilingual metric.
In terms of summarization quality and com-
putational complexity, all scoring functions pre-
sented in Table 6 can be considered to perform
equally well for bilingual extractive summariza-
tion. Assuming their efficient implementation,
all methods have a linear computational com-
plexity, O(n), relative to the total number of
words in a document. KEY PR and COV PR re-
</bodyText>
<page confidence="0.998205">
67
</page>
<bodyText confidence="0.999866944444444">
quire additional O(c(|E|+|V |)) time for running
PageRank, where c is the number of iterations it
needs to converge, |E |is the number of edges,
and |V  |is the number of nodes (distinct words)
in a document graph. Since neither |E |nor |V  |in
our graph representation can be as large as n, the
total computation time for KEY PR and COV PR
metrics is also linear relative to the document
size.
In terms of implementation complexity,
LEN W and LEN CH are simpliest, since they
even do not require any preprocessing and repre-
sentation building; KEY and COV require key-
words identification; D COV C, and D COV J
require vector space model building; KEY DEG
and COV DEG need graphs building (order of
words); whereas KEY PR and COV PR, in ad-
dition, require PageRank implementation.
</bodyText>
<sectionHeader confidence="0.998298" genericHeader="conclusions">
5 Conclusion and Future Research
</sectionHeader>
<bodyText confidence="0.979587848484849">
In this paper, we conducted in-depth, compar-
ative evaluations of 31 existing (16 of which
are mostly graph-based modifications of exist-
ing state-of-the-art methods, introduced in (Lit-
vak et al., 2010)) scoring methods10 using En-
glish and Hebrew language texts.
The experimental results suggest that the rel-
ative ranking of methods performance is quite
similar in both languages. We identified meth-
ods that performed significantly better in only
one of the languages and those that performed
equally well in both languages. Moreover, al-
though vector and graph-based approaches were
among the top ranked methods for bilingual ap-
plication, no text representation model presented
itself as markedly superior to the other.
Our future research will extend the evaluations
of language-independent sentence ranking met-
rics to a range of other languages such as Ger-
man, Arabic, Greek, and Russian. We will adapt
similarity-based metrics to multilingual applica-
tion by implementing them via n-gram matching
instead of exact word matching. We will fur-
ther improve the summarization quality by ap-
10We will provide the code for our summarizer upon re-
quest.
plying machine learning on described features.
We will use additional techniques for summary
evaluation and study the impact of morpholog-
ical analysis on the top ranked bilingual scores
using part-of-speech (POS) tagging11, anaphora
resolution, named entity recognition, and taking
word sense into account.
</bodyText>
<sectionHeader confidence="0.998091" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999695333333333">
We are grateful to Michael Elhadad and Galina
Volk for providing the ROUGE toolkit adapted
to Hebrew alphabet.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99831844">
Adler, M. (2009). Hebrew morphological
disambiguation: An unsupervised stochas-
tic word-based approach. Dissertation.
http://www.cs.bgu.ac.il/ adlerm/dat/thesis.pdf.
Baxendale, P. (1958). Machine-made index for
technical literature-an experiment. IBM Jour-
nal of Research and Development, 2, 354–361.
Brin, S., &amp; Page, L. (1998). The anatomy of
a large-scale hypertextual web search engine.
Computer networks and ISDN systems, 30,
107–117.
Dang, H. T. (2006). Overview of DUC 2006.
Proceedings of the Document Understanding
Conference.
Demsar, J. (2006). Statistical comparisons of
classifiers over multiple data sets. Journal of
Machine Learning Research, 7, 1–30.
DUC (2002). Document understanding confer-
ence. http://duc.nist.gov.
Edmundson, H. P. (1969). New methods in auto-
matic extracting. J. ACM, 16.
Erkan, G., &amp; Radev, D. R. (2004). LexRank:
Graph-based lexical centrality as salience in
text summarization. Journal of Artificial In-
telligence Research, 22, 457–479.
</reference>
<footnote confidence="0.595514">
11Our experiments have shown that syntactic filters,
which select only lexical units of a certain part of speech,
do not significantly improve the performance of the evalu-
ated bilingual scoring methods.
</footnote>
<page confidence="0.99495">
68
</page>
<reference confidence="0.986845947368421">
Gong, Y., &amp; Liu, X. (2001). Generic text summa-
rization using relevance measure and latent se-
mantic analysis. Proceedings of the 24th ACM
SIGIR conference on Research and develop-
ment in information retrieval (pp. 19–25).
Kallel, F. J., Jaoua, M., Hadrich, L. B., &amp;
Hamadou, A. B. (2004). Summarization at
LARIS laboratory. Proceedings of the Doc-
ument Understanding Conference.
Kupiec, J., Pedersen, J., &amp; Chen, F. (1995). A
trainable document summarizer. Proceedings
of the 18th annual international ACM SIGIR
conference (pp. 68–73).
Lin, C.-Y. (2004). ROUGE: A package for au-
tomatic evaluation of summaries. Proceedings
of the ACL’04 Workshop: Text Summarization
Branches Out (pp. 74–81).
Litvak, M., Last, M., &amp; Friedman, M. (2010). A
new approach to improving multilingual sum-
marization using a genetic algorithm. Pro-
ceedings of the Association for Computational
Linguistics (ACL) 2010. Uppsala, Sweden.
Luhn, H. P. (1958). The automatic creation of
literature abstracts. IBM Journal of Research
and Development, 2, 159–165.
Mani, I., &amp; Maybury, M. (1999). Advances in
automatic text summarization.
Mihalcea, R. (2005). Language independent ex-
tractive summarization. AAAI’05: Proceed-
ings of the 20th national conference on Artifi-
cial intelligence (pp. 1688–1689).
Nastase, V., &amp; Szpakowicz, S. (2006). A study
of two graph algorithms in topic-driven sum-
marization. Proceedings of the Workshop
on Graph-based Algorithms for Natural Lan-
guage.
Neto, J., Santos, A., Kaestner, C., &amp; Freitas, A.
(2000). Generating text summaries through
the relative importance of topics. Lecture
Notes in Computer Science, 300–309.
Radev, D., Blair-Goldensohn, S., &amp; Zhang, Z.
(2001). Experiments in single and multidocu-
ment summarization using MEAD. First Doc-
ument Understanding Conference.
Saggion, H., Bontcheva, K., &amp; Cunningham, H.
(2003). Robust generic and query-based sum-
marisation. EACL ’03: Proceedings of the
tenth conference on European chapter of the
Association for Computational Linguistics.
Salton, G., Singhal, A., Mitra, M., &amp; Buckley, C.
(1997). Automatic text structuring and sum-
marization. Information Processing and Man-
agement, 33, 193–207.
Satoshi, C. N., Satoshi, S., Murata, M., Uchi-
moto, K., Utiyama, M., &amp; Isahara, H. (2001).
Sentence extraction system assembling mul-
tiple evidence. Proceedings of 2nd NTCIR
Workshop (pp. 319–324).
Schenker, A., Bunke, H., Last, M., &amp; Kandel,
A. (2004). Classification of web documents
using graph matching. International Journal
of Pattern Recognition and Artificial Intelli-
gence, 18, 475–496.
Steinberger, J., &amp; Jezek, K. (2004). Text sum-
marization and singular value decomposition.
Lecture Notes in Computer Science, 245–254.
Vanderwende, L., Suzuki, H., Brockett, C., &amp;
Nenkova, A. (2007). Beyond SumBasic: Task-
focused summarization with sentence simplifi-
cation and lexical expansion. Information pro-
cessing and management, 43, 1606–1618.
Wong, K., Wu, M., &amp; Li, W. (2008). Ex-
tractive summarization using supervised and
semi-supervised learning. Proceedings of the
22nd International Conference on Computa-
tional Linguistics (pp. 985–992).
</reference>
<page confidence="0.999316">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.229680">
<title confidence="0.977048">Towards multi-lingual summarization: A comparative analysis sentence extraction methods on English and Hebrew corpora</title>
<author confidence="0.9763485">Litvak Lipman Ben Gur Ben Gurion University of the Negev</author>
<email confidence="0.93273">lipmanh,bengura,</email>
<affiliation confidence="0.7592395">Kisilevich University of</affiliation>
<email confidence="0.655884">Daniel.Keim@uni-konstanz.de</email>
<abstract confidence="0.999851333333333">The trend toward the growing multilinguality of the Internet requires text summarization techniques that work equally well in multiple languages. Only some of the automated summarization methods proposed in the literature, however, can be defined as “languageindependent”, as they are not based on any morphological analysis of the summarized text. In this paper, we perform an in-depth comparative analysis of language-independent sentence scoring methods for extractive single-document We evaluate published summarization methods proposed the literature and introduced in (Litvak et al., 2010). The evaluation is performed on English and Hebrew corpora. The results suggest that the performance ranking of the compared methods is quite similar in both languages. The top ten bilingual scoring methods include six methods introduced</abstract>
<note confidence="0.734621">in (Litvak et al., 2010).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Adler</author>
</authors>
<title>Hebrew morphological disambiguation: An unsupervised stochastic word-based approach.</title>
<date>2009</date>
<note>Dissertation. http://www.cs.bgu.ac.il/ adlerm/dat/thesis.pdf.</note>
<contexts>
<context position="16313" citStr="Adler, 2009" startWordPosition="2553" endWordPosition="2554"> was evaluated through a comparative experiment on corpora of English and Hebrew texts. These two languages, which belong to different language families (IndoEuropean and Semitic languages, respectively), were intentionally chosen for this experiment to increase the generality of our evaluation. The main difference between these languages, is that Hebrew morphology allows morphemes to be combined systematically into complex wordforms. In different contexts, the same morpheme can appear as a separate word-form, while in others it appears agglutinated as a suffix or prefix to another word-form (Adler, 2009). The goals of the experiment were as follows: - To evaluate the performance of different approaches for extractive single-document summarization using graph and vector representations. - To compare the quality of the multilingual summarization methods proposed in our previous work (Litvak et al., 2010) to the state-of-the-art approaches. - To identify sentence ranking methods that work equally well on both languages. 4.2 Text Preprocessing Extractive summarization relies critically on proper sentence segmentation to insure the quality of the summarization results. We used a sentence splitter </context>
</contexts>
<marker>Adler, 2009</marker>
<rawString>Adler, M. (2009). Hebrew morphological disambiguation: An unsupervised stochastic word-based approach. Dissertation. http://www.cs.bgu.ac.il/ adlerm/dat/thesis.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Baxendale</author>
</authors>
<title>Machine-made index for technical literature-an experiment.</title>
<date>1958</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>2</volume>
<pages>354--361</pages>
<contexts>
<context position="8766" citStr="Baxendale, 1958" startWordPosition="1340" endWordPosition="1341">he word-based graph representation model presented in Schenker et al. (2004). We represent each document by a directed, labeled, unweighted graph in which nodes represent unique terms (distinct normalized words) and edges represent order-relationships between two terms. Each edge is labeled with the IDs of sentences that contain both words in the specified order. 3.2 Structure-based Scoring Methods In this section, we describe the existing structure-based methods for multilingual sentence scoring. These methods do not require any text representation and are based on its structure. – Position (Baxendale, 1958): POS L Closeness to the end of the document: score(5i) = i, where i is a sequential number of a sentence in a document; POS F Closeness to the beginning of the document: score(5i) = 1i ; POS B Closeness to the borders of the document: score(5i) = max(1i , 1 n−i+1), where n is the total number of sentences in D. – Length (Satoshi et al., 2001): LEN W Number of words in a sentence; LEN CH Number of characters in a sentence. 3.3 Vector-based Scoring Methods In this section, we describe the vector-based methods for multilingual sentence scoring, that are based on the vector space model for text r</context>
</contexts>
<marker>Baxendale, 1958</marker>
<rawString>Baxendale, P. (1958). Machine-made index for technical literature-an experiment. IBM Journal of Research and Development, 2, 354–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<journal>Computer networks and ISDN systems,</journal>
<volume>30</volume>
<pages>107--117</pages>
<contexts>
<context position="12637" citStr="Brin &amp; Page, 1998" startWordPosition="1971" endWordPosition="1974">g using the graph text representation based on sentence (ML TR) or word (all except ML TR) segmentation. ML TR Multilingual version of TextRank (Mihalcea, 2005) without morphological analysis. Each document is represented as a directed graph of nodes that stand for sentences interconnected by similarity (overlap) relationship. To each edge connecting two vertices the weight is assigned and equal to the similarity value between the corresponding sentences. We used backward links, as it was the most successful according to the reported results in (Mihalcea, 2005). score(S) is equal to PageRank (Brin &amp; Page, 1998) of its node, according to the formula adapted to the weights assigned to edges. – Degree-based (Litvak et al., 2010):4 LUHN DEG A graph-based extension of the LUHN measure, in which a node degree is used instead of a word frequency: words are considered significant if they are represented by nodes of a higher degree than a predefined threshold (see Table 1). KEY DEG Graph-based extension of KEY measure. COV DEG Graph-based extension of COV measure. DEG Average degree for all sentence nodes: score(S) = K∈{wor ds(S)} Degi . |S| GRASE(GRaph-based Automated Sentence Extractor) Modification of Sal</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Brin, S., &amp; Page, L. (1998). The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30, 107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2006</date>
<booktitle>Proceedings of the Document Understanding Conference.</booktitle>
<contexts>
<context position="19064" citStr="Dang (2006)" startWordPosition="2991" endWordPosition="2992">o obey the minimal and maximal summary constraints of 95 and 100 words, respectively. Summaries were assessed for quality by procedure described in (Litvak et al., 2010). 4.4 Experimental Results We evaluated English and Hebrew summaries using the ROUGE-1, 2, 3, 4, L, SU and W metrics9, described in Lin (2004). Our results were not statistically distinguishable and matched the conclusion of Lin (2004). However, because ROUGE-1 showed the largest variation across the methods, all results in the following comparisons are presented in terms of ROUGE-1 metric. Similar to the approach described in Dang (2006), we performed multiple comparisons between the sentence scoring methods. The Friedman test was used to reject the null hy7The software enabling easy selection and storage of sentences to be included in the document extract, can be provided upon request. 8http://www.haaretz.co.il 9ROUGE toolkit was adapted to Hebrew by specifying “token” using Hebrew alphabet Table 2: English: Multiple comparisons of sentence ranking approaches using the BonferroniDunn test of ROUGE-1 Recall Approach ROUGE-1 COV DEG* 0.436 A KEY DEG* 0.433 A B KEY 0.429 A B C COV PR* 0.428 A B C D COV 0.428 A B C D D COV C* 0.</context>
</contexts>
<marker>Dang, 2006</marker>
<rawString>Dang, H. T. (2006). Overview of DUC 2006. Proceedings of the Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Demsar</author>
</authors>
<title>Statistical comparisons of classifiers over multiple data sets.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>7</volume>
<pages>1--30</pages>
<contexts>
<context position="20316" citStr="Demsar, 2006" startWordPosition="3283" endWordPosition="3284">PR* 0.424 B C D E LUHN DEG* 0.422 C D E F POS F 0.419 E F G LEN CH 0.418 C D E F G LUHN 0.418 D E F G LUHN PR* 0.418 E F G H LEN W 0.416 D E F G H ML TR 0.414 E F G H TITLE E J* 0.413 F G H I TITLE E O* 0.413 F G H I D COV E J* 0.410 F G H I D COV O* 0.405 G H I J TFISF 0.405 G H I J DEG* 0.403 G H I J D COV E O* 0.401 H I J K PR* 0.400 G H I J K TITLE J 0.399 I J K TF 0.397 I J K TITLE O 0.396 J K SVD 0.395 I J K TITLE C 0.395 J K POS B 0.392 K L GRASE* 0.372 L POS L 0.339 M pothesis (all methods perform the same) at the 0.0001 significance level, after which we ran the Bonferroni-Dunn test (Demsar, 2006) for pairwise comparisons. Tables 2 and 3 show the results of multiple comparisons and are arranged in descending order with the best approaches on top. Methods not sharing any common letter were significantly different at the 95% confidence level. The Pearson correlation between methods ranking in English and Hebrew was 0.775, which was larger than zero at a significance level of 0.0001. In other words, most of the methods were ranked in nearly the same relative positions in both corpora, and the top ranked methods performed equally well in both languages. The differences in ranking were caus</context>
</contexts>
<marker>Demsar, 2006</marker>
<rawString>Demsar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7, 1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DUC</author>
</authors>
<date>2002</date>
<note>Document understanding conference. http://duc.nist.gov.</note>
<contexts>
<context position="17447" citStr="DUC, 2002" startWordPosition="2730" endWordPosition="2731">nsure the quality of the summarization results. We used a sentence splitter provided with the MEAD summarizer (Radev et al., 2001) for English and a simple splitter for Hebrew splitting the text at every period, exclamation point, or question mark.6 4.3 Experimental Data For English texts, we used the corpus of summarized documents provided for the single doc6Although the same set of splitting rules may be used for both languages, separate splitters were used since the MEAD splitter is restricted to European languages. 65 ument summarization task at the Document Understanding Conference 2002 (DUC, 2002). This benchmark dataset contains 533 news articles, each of which is at least ten sentences long and has two to three human-generated abstracts of approximately 100 words apiece. However, to the best of our knowledge, no summarization benchmarks exist for the Hebrew language texts. To collect summarized texts in Hebrew, we set up an experiment7 in which 50 news articles of 250 to 830 words each from the Haaretz8 newspaper internet site were summarized by human assessors by extracting the most salient sentences. In total, 70 undergraduate students from the Department of Information Systems Eng</context>
</contexts>
<marker>DUC, 2002</marker>
<rawString>DUC (2002). Document understanding conference. http://duc.nist.gov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>J. ACM,</journal>
<volume>16</volume>
<contexts>
<context position="4692" citStr="Edmundson (1969)" startWordPosition="694" endWordPosition="696">the top-ranked fragments are extracted and arranged in a summary in the same order they appeared in the original text. Statistical methods for calculating the relevance score of each fragment can rely on such information as: fragment position inside the document, its length, whether it contains keywords or title words. Research by Luhn (1958), in which the significance factor of a sentence is based on the frequency and the relative position of significant words within that sentence, is considered the first on automated text summarization. Luhn’s work was followed shortly thereafter by that of Edmundson (1969) and some time later by studies from Radev et al. (2001) and Saggion et al. (2003), all of who applied linear combinations of multiple statistical methods to rank sentences using the vector space model as a text representation. In (Litvak et al., 2010) we improve the summarization quality by identifying the best linear combination of the metrics evaluated in this paper. Several information retrieval and machine learning techniques have been proposed for determining sentence importance (Kupiec et al., 1995; Wong et al., 2008). Gong and Liu (2001) and Steinberger and Jezek (2004) showed that sin</context>
<context position="9572" citStr="Edmundson, 1969" startWordPosition="1481" endWordPosition="1482">S B Closeness to the borders of the document: score(5i) = max(1i , 1 n−i+1), where n is the total number of sentences in D. – Length (Satoshi et al., 2001): LEN W Number of words in a sentence; LEN CH Number of characters in a sentence. 3.3 Vector-based Scoring Methods In this section, we describe the vector-based methods for multilingual sentence scoring, that are based on the vector space model for text representation. – Frequency-based: LUHN (Luhn, 1958) score(5) = maxciE{clusters(S)}{csi}, where clusters are portions of a sentence bracketed by keywords2 and csi = |keywords(ci)|2 |ci| KEY (Edmundson, 1969) Sum of the keyword frequencies: score(5) = EiE{keywords(S)} tfi, where tfi is term in-document frequency of keyword i. COV (Kallel et al., 2004) Ratio of keyword numbers (Coverage): score(5) = |keywords(S)| |keywords(D)| TF (Vanderwende et al., 2007) Average term frequency for all sentence words: /�i∈{words(S)} tfi score(5) = r 2Luhn’s experiments suggest an optimal limit of 4 or 5 non-significant words between keywords. 3Due to multilingual focus of our work, exact word matching was used in all similarity-based methods. S TFISF (Neto et al., 2000) Average term frequency inverted sentence fre</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>Edmundson, H. P. (1969). New methods in automatic extracting. J. ACM, 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>D R Radev</author>
</authors>
<title>LexRank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>22</volume>
<pages>457--479</pages>
<contexts>
<context position="5836" citStr="Erkan and Radev (2004)" startWordPosition="873" endWordPosition="876">., 2008). Gong and Liu (2001) and Steinberger and Jezek (2004) showed that singular value decomposition (SVD) can be applied to generate extracts. Among text representation models, graphbased text representations have gained popularity in automated summarization, as they enable the model to be enriched with syntactic and semantic relations. Salton et al. (1997) were among the first to attempt graph-based ranking methods for single document extractive summarization by generating similarity links between document paragraphs. The important paragraphs of a text were extracted using degree scores. Erkan and Radev (2004) and Mihalcea (2005) introduced approaches for unsupervised extractive summarization that rely on the application of iterative graph based ranking algorithms. In their approaches, each document is represented as a graph of sentences interconnected by similarity relations. 3 Language-Independent Scoring Methods for Sentence Extraction Various language dependent and independent sentence scoring methods have been introduced in the literature. We selected the 15 most prominent language independent methods for evaluation. Most of them can be categorized as frequency, position, length, or title-base</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Erkan, G., &amp; Radev, D. R. (2004). LexRank: Graph-based lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research, 22, 457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gong</author>
<author>X Liu</author>
</authors>
<title>Generic text summarization using relevance measure and latent semantic analysis.</title>
<date>2001</date>
<booktitle>Proceedings of the 24th ACM SIGIR conference on Research and development in information retrieval</booktitle>
<pages>(pp.</pages>
<contexts>
<context position="5243" citStr="Gong and Liu (2001)" startWordPosition="785" endWordPosition="788">n’s work was followed shortly thereafter by that of Edmundson (1969) and some time later by studies from Radev et al. (2001) and Saggion et al. (2003), all of who applied linear combinations of multiple statistical methods to rank sentences using the vector space model as a text representation. In (Litvak et al., 2010) we improve the summarization quality by identifying the best linear combination of the metrics evaluated in this paper. Several information retrieval and machine learning techniques have been proposed for determining sentence importance (Kupiec et al., 1995; Wong et al., 2008). Gong and Liu (2001) and Steinberger and Jezek (2004) showed that singular value decomposition (SVD) can be applied to generate extracts. Among text representation models, graphbased text representations have gained popularity in automated summarization, as they enable the model to be enriched with syntactic and semantic relations. Salton et al. (1997) were among the first to attempt graph-based ranking methods for single document extractive summarization by generating similarity links between document paragraphs. The important paragraphs of a text were extracted using degree scores. Erkan and Radev (2004) and Mi</context>
</contexts>
<marker>Gong, Liu, 2001</marker>
<rawString>Gong, Y., &amp; Liu, X. (2001). Generic text summarization using relevance measure and latent semantic analysis. Proceedings of the 24th ACM SIGIR conference on Research and development in information retrieval (pp. 19–25).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Kallel</author>
<author>M Jaoua</author>
<author>L B Hadrich</author>
<author>A B Hamadou</author>
</authors>
<date>2004</date>
<booktitle>Summarization at LARIS laboratory. Proceedings of the Document Understanding Conference.</booktitle>
<contexts>
<context position="9717" citStr="Kallel et al., 2004" startWordPosition="1502" endWordPosition="1505"> et al., 2001): LEN W Number of words in a sentence; LEN CH Number of characters in a sentence. 3.3 Vector-based Scoring Methods In this section, we describe the vector-based methods for multilingual sentence scoring, that are based on the vector space model for text representation. – Frequency-based: LUHN (Luhn, 1958) score(5) = maxciE{clusters(S)}{csi}, where clusters are portions of a sentence bracketed by keywords2 and csi = |keywords(ci)|2 |ci| KEY (Edmundson, 1969) Sum of the keyword frequencies: score(5) = EiE{keywords(S)} tfi, where tfi is term in-document frequency of keyword i. COV (Kallel et al., 2004) Ratio of keyword numbers (Coverage): score(5) = |keywords(S)| |keywords(D)| TF (Vanderwende et al., 2007) Average term frequency for all sentence words: /�i∈{words(S)} tfi score(5) = r 2Luhn’s experiments suggest an optimal limit of 4 or 5 non-significant words between keywords. 3Due to multilingual focus of our work, exact word matching was used in all similarity-based methods. S TFISF (Neto et al., 2000) Average term frequency inverted sentence frequency for all sentence words: score(5) = &amp;E{words(S)} tfi X isfi, where isfi = 1 − log(ni), where n is the number log(n) of sentences in a docum</context>
</contexts>
<marker>Kallel, Jaoua, Hadrich, Hamadou, 2004</marker>
<rawString>Kallel, F. J., Jaoua, M., Hadrich, L. B., &amp; Hamadou, A. B. (2004). Summarization at LARIS laboratory. Proceedings of the Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>F Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>Proceedings of the 18th annual international ACM SIGIR conference</booktitle>
<pages>68--73</pages>
<contexts>
<context position="5202" citStr="Kupiec et al., 1995" startWordPosition="777" endWordPosition="780">irst on automated text summarization. Luhn’s work was followed shortly thereafter by that of Edmundson (1969) and some time later by studies from Radev et al. (2001) and Saggion et al. (2003), all of who applied linear combinations of multiple statistical methods to rank sentences using the vector space model as a text representation. In (Litvak et al., 2010) we improve the summarization quality by identifying the best linear combination of the metrics evaluated in this paper. Several information retrieval and machine learning techniques have been proposed for determining sentence importance (Kupiec et al., 1995; Wong et al., 2008). Gong and Liu (2001) and Steinberger and Jezek (2004) showed that singular value decomposition (SVD) can be applied to generate extracts. Among text representation models, graphbased text representations have gained popularity in automated summarization, as they enable the model to be enriched with syntactic and semantic relations. Salton et al. (1997) were among the first to attempt graph-based ranking methods for single document extractive summarization by generating similarity links between document paragraphs. The important paragraphs of a text were extracted using deg</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Kupiec, J., Pedersen, J., &amp; Chen, F. (1995). A trainable document summarizer. Proceedings of the 18th annual international ACM SIGIR conference (pp. 68–73).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
</authors>
<title>ROUGE: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL’04 Workshop: Text Summarization Branches Out</booktitle>
<pages>74--81</pages>
<contexts>
<context position="18764" citStr="Lin (2004)" startWordPosition="2946" endWordPosition="2947">signed to each of the 70 study participants who were instructed (1) To dedicate at least five minutes to each document, (2) To ignore dialogs and citations, (3) To read the whole document before starting sentence extraction, (4) To ignore redundant, repetitive, or overly detailed information, (5) To obey the minimal and maximal summary constraints of 95 and 100 words, respectively. Summaries were assessed for quality by procedure described in (Litvak et al., 2010). 4.4 Experimental Results We evaluated English and Hebrew summaries using the ROUGE-1, 2, 3, 4, L, SU and W metrics9, described in Lin (2004). Our results were not statistically distinguishable and matched the conclusion of Lin (2004). However, because ROUGE-1 showed the largest variation across the methods, all results in the following comparisons are presented in terms of ROUGE-1 metric. Similar to the approach described in Dang (2006), we performed multiple comparisons between the sentence scoring methods. The Friedman test was used to reject the null hy7The software enabling easy selection and storage of sentences to be included in the document extract, can be provided upon request. 8http://www.haaretz.co.il 9ROUGE toolkit was </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Lin, C.-Y. (2004). ROUGE: A package for automatic evaluation of summaries. Proceedings of the ACL’04 Workshop: Text Summarization Branches Out (pp. 74–81).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Litvak</author>
<author>M Last</author>
<author>M Friedman</author>
</authors>
<title>A new approach to improving multilingual summarization using a genetic algorithm.</title>
<date>2010</date>
<booktitle>Proceedings of the Association for Computational Linguistics (ACL)</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="1016" citStr="Litvak et al., 2010" startWordPosition="138" endWordPosition="141">nd toward the growing multilinguality of the Internet requires text summarization techniques that work equally well in multiple languages. Only some of the automated summarization methods proposed in the literature, however, can be defined as “languageindependent”, as they are not based on any morphological analysis of the summarized text. In this paper, we perform an in-depth comparative analysis of language-independent sentence scoring methods for extractive single-document summarization. We evaluate 15 published summarization methods proposed in the literature and 16 methods introduced in (Litvak et al., 2010). The evaluation is performed on English and Hebrew corpora. The results suggest that the performance ranking of the compared methods is quite similar in both languages. The top ten bilingual scoring methods include six methods introduced in (Litvak et al., 2010). 1 Introduction Automatically generated summaries can significantly reduce the information overload on professionals in a variety of fields, could prove beneficial for the automated classification and filtering of documents, the search for information over the Internet and applications that utilize large textual databases. Document su</context>
<context position="2788" citStr="Litvak et al., 2010" startWordPosition="406" endWordPosition="409">fic knowledge can be considered language-independent summarizers. As the number of languages used on the Internet increases continiously (there are at least 75 different languages according to a estimate performed by A. Gulli and A. Signorini1 in the end of January 2005), there is a growing need for language-independent statistical summarization techniques that can be readily applied to text in any language without using language-specific morphological tools. In this work, we perform an in-depth comparative analysis of 16 methods for languageindependent extractive summarization introduced in (Litvak et al., 2010) that utilize either vector or graph-based representations of text documents computed from word segmentation and 15 state-of-the art language-independent scoring methods. The main goal of the evaluation experiments, which focused on English and Hebrew corpora, is to find the most efficient language-independent sentence scoring methods 1http://www.cs.uiowa.edu/ asignori/web-size/ 61 Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 61–69, Beijing, August 2010 in terms of summarization accuracy and computational complexity across two differen</context>
<context position="4944" citStr="Litvak et al., 2010" startWordPosition="738" endWordPosition="741">e the document, its length, whether it contains keywords or title words. Research by Luhn (1958), in which the significance factor of a sentence is based on the frequency and the relative position of significant words within that sentence, is considered the first on automated text summarization. Luhn’s work was followed shortly thereafter by that of Edmundson (1969) and some time later by studies from Radev et al. (2001) and Saggion et al. (2003), all of who applied linear combinations of multiple statistical methods to rank sentences using the vector space model as a text representation. In (Litvak et al., 2010) we improve the summarization quality by identifying the best linear combination of the metrics evaluated in this paper. Several information retrieval and machine learning techniques have been proposed for determining sentence importance (Kupiec et al., 1995; Wong et al., 2008). Gong and Liu (2001) and Steinberger and Jezek (2004) showed that singular value decomposition (SVD) can be applied to generate extracts. Among text representation models, graphbased text representations have gained popularity in automated summarization, as they enable the model to be enriched with syntactic and semanti</context>
<context position="6739" citStr="Litvak et al., 2010" startWordPosition="1008" endWordPosition="1011">-Independent Scoring Methods for Sentence Extraction Various language dependent and independent sentence scoring methods have been introduced in the literature. We selected the 15 most prominent language independent methods for evaluation. Most of them can be categorized as frequency, position, length, or title-based, and they utilize vector representation. TextRank (ML TR) is the only method that is based on graph representation, but there are also position and lengthbased methods that calculate scores using the overall structure of a document. We have also considered 16 methods proposed in (Litvak et al., 2010), including 13 based on the graph-theoretic representation (Section 3.1). Figure 1 (Litvak et al., 2010) shows the taxonomy of the 31 methods considered in our work. All methods introduced in (Litvak et al., 2010) are denoted by an asterisk (*). Methods requiring a threshold value t ∈ [0, 1] that specifies the portion of the top rated terms considered significant are marked by a cross in Figure 1 and listed in Table 1 along with the optimal average threshold values obtained after evaluating the methods 62 Table 1: Selected thresholds for threshold-based scoring methods Method Threshold LUHN 0.</context>
<context position="11248" citStr="Litvak et al., 2010" startWordPosition="1744" endWordPosition="1747">ITLE O using overlap similarity: |S∩T| min{|S|,|T|} TITLE J using Jaccard similarity: S∩T �S∪Tj 63 Multilingual sentence scoringmethods D_COV_O* D_COV_J* D_COV_C* TITLE_O TITLE_J TITLE_C Structurebased Vectorbased Similarity Position Length Frequency Title Document POS_F POS_L POS_B LEN_W LEN_CH LUHN† KEY † COV † TF TFIISF SVD Graphbased Pagerank Similarity Degree LUHN_DEG † * KEY_DEG † * COV_DEG † * DEG* GRASE* LUHN_PR † * KEY_PR † * COV PR † * FR* ML_TR Title Document TITLE_E_O* TITLE_E_J* D_COV_E_O* D_COV_E_J* Figure 1: Taxonomy of statistical language-independent sentence scoring methods (Litvak et al., 2010) TITLE C using cosine similarity: sim(~S, T~) = cos(~S, T~ ) = ~S×~T |~S|×|~T| – Document Coverage (Litvak et al., 2010). These methods score a sentence according to its similarity to the rest of the sentences in the document (D − S) based on the following intuition: the more document content is covered by a sentence, the more important the sentence is to a summary. Redundant sentences containing repetitive information are removed using a similarity filter. score(S) = sim(S, D − S): D COV O using Overlap similarity: |S∩T | min{|S|,|D−S|} D COV J using Jaccard similarity: |S∩T | |S∪D−S| D COV C</context>
<context position="12754" citStr="Litvak et al., 2010" startWordPosition="1991" endWordPosition="1994">ilingual version of TextRank (Mihalcea, 2005) without morphological analysis. Each document is represented as a directed graph of nodes that stand for sentences interconnected by similarity (overlap) relationship. To each edge connecting two vertices the weight is assigned and equal to the similarity value between the corresponding sentences. We used backward links, as it was the most successful according to the reported results in (Mihalcea, 2005). score(S) is equal to PageRank (Brin &amp; Page, 1998) of its node, according to the formula adapted to the weights assigned to edges. – Degree-based (Litvak et al., 2010):4 LUHN DEG A graph-based extension of the LUHN measure, in which a node degree is used instead of a word frequency: words are considered significant if they are represented by nodes of a higher degree than a predefined threshold (see Table 1). KEY DEG Graph-based extension of KEY measure. COV DEG Graph-based extension of COV measure. DEG Average degree for all sentence nodes: score(S) = K∈{wor ds(S)} Degi . |S| GRASE(GRaph-based Automated Sentence Extractor) Modification of Salton’s algorithm (Salton et al., 1997) using the graph 4All proposed here degree-based methods, except for GRASE, use </context>
<context position="16617" citStr="Litvak et al., 2010" startWordPosition="2598" endWordPosition="2601"> The main difference between these languages, is that Hebrew morphology allows morphemes to be combined systematically into complex wordforms. In different contexts, the same morpheme can appear as a separate word-form, while in others it appears agglutinated as a suffix or prefix to another word-form (Adler, 2009). The goals of the experiment were as follows: - To evaluate the performance of different approaches for extractive single-document summarization using graph and vector representations. - To compare the quality of the multilingual summarization methods proposed in our previous work (Litvak et al., 2010) to the state-of-the-art approaches. - To identify sentence ranking methods that work equally well on both languages. 4.2 Text Preprocessing Extractive summarization relies critically on proper sentence segmentation to insure the quality of the summarization results. We used a sentence splitter provided with the MEAD summarizer (Radev et al., 2001) for English and a simple splitter for Hebrew splitting the text at every period, exclamation point, or question mark.6 4.3 Experimental Data For English texts, we used the corpus of summarized documents provided for the single doc6Although the same </context>
<context position="18622" citStr="Litvak et al., 2010" startWordPosition="2919" endWordPosition="2922">rom the Department of Information Systems Engineering, Ben Gurion University of the Negev participated in the experiment. Ten documents were randomly assigned to each of the 70 study participants who were instructed (1) To dedicate at least five minutes to each document, (2) To ignore dialogs and citations, (3) To read the whole document before starting sentence extraction, (4) To ignore redundant, repetitive, or overly detailed information, (5) To obey the minimal and maximal summary constraints of 95 and 100 words, respectively. Summaries were assessed for quality by procedure described in (Litvak et al., 2010). 4.4 Experimental Results We evaluated English and Hebrew summaries using the ROUGE-1, 2, 3, 4, L, SU and W metrics9, described in Lin (2004). Our results were not statistically distinguishable and matched the conclusion of Lin (2004). However, because ROUGE-1 showed the largest variation across the methods, all results in the following comparisons are presented in terms of ROUGE-1 metric. Similar to the approach described in Dang (2006), we performed multiple comparisons between the sentence scoring methods. The Friedman test was used to reject the null hy7The software enabling easy selectio</context>
<context position="26509" citStr="Litvak et al., 2010" startWordPosition="4428" endWordPosition="4432">e. In terms of implementation complexity, LEN W and LEN CH are simpliest, since they even do not require any preprocessing and representation building; KEY and COV require keywords identification; D COV C, and D COV J require vector space model building; KEY DEG and COV DEG need graphs building (order of words); whereas KEY PR and COV PR, in addition, require PageRank implementation. 5 Conclusion and Future Research In this paper, we conducted in-depth, comparative evaluations of 31 existing (16 of which are mostly graph-based modifications of existing state-of-the-art methods, introduced in (Litvak et al., 2010)) scoring methods10 using English and Hebrew language texts. The experimental results suggest that the relative ranking of methods performance is quite similar in both languages. We identified methods that performed significantly better in only one of the languages and those that performed equally well in both languages. Moreover, although vector and graph-based approaches were among the top ranked methods for bilingual application, no text representation model presented itself as markedly superior to the other. Our future research will extend the evaluations of language-independent sentence r</context>
</contexts>
<marker>Litvak, Last, Friedman, 2010</marker>
<rawString>Litvak, M., Last, M., &amp; Friedman, M. (2010). A new approach to improving multilingual summarization using a genetic algorithm. Proceedings of the Association for Computational Linguistics (ACL) 2010. Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>2</volume>
<pages>159--165</pages>
<contexts>
<context position="4420" citStr="Luhn (1958)" startWordPosition="651" endWordPosition="652">relevant fragments, which can be paragraphs, sentences, keyphrases, or keywords from a given source text. The extractive summarization process usually involves ranking, such that each fragment of a summarized text gets a relevance score, and extraction, during which the top-ranked fragments are extracted and arranged in a summary in the same order they appeared in the original text. Statistical methods for calculating the relevance score of each fragment can rely on such information as: fragment position inside the document, its length, whether it contains keywords or title words. Research by Luhn (1958), in which the significance factor of a sentence is based on the frequency and the relative position of significant words within that sentence, is considered the first on automated text summarization. Luhn’s work was followed shortly thereafter by that of Edmundson (1969) and some time later by studies from Radev et al. (2001) and Saggion et al. (2003), all of who applied linear combinations of multiple statistical methods to rank sentences using the vector space model as a text representation. In (Litvak et al., 2010) we improve the summarization quality by identifying the best linear combina</context>
<context position="9417" citStr="Luhn, 1958" startWordPosition="1459" endWordPosition="1460">ment: score(5i) = i, where i is a sequential number of a sentence in a document; POS F Closeness to the beginning of the document: score(5i) = 1i ; POS B Closeness to the borders of the document: score(5i) = max(1i , 1 n−i+1), where n is the total number of sentences in D. – Length (Satoshi et al., 2001): LEN W Number of words in a sentence; LEN CH Number of characters in a sentence. 3.3 Vector-based Scoring Methods In this section, we describe the vector-based methods for multilingual sentence scoring, that are based on the vector space model for text representation. – Frequency-based: LUHN (Luhn, 1958) score(5) = maxciE{clusters(S)}{csi}, where clusters are portions of a sentence bracketed by keywords2 and csi = |keywords(ci)|2 |ci| KEY (Edmundson, 1969) Sum of the keyword frequencies: score(5) = EiE{keywords(S)} tfi, where tfi is term in-document frequency of keyword i. COV (Kallel et al., 2004) Ratio of keyword numbers (Coverage): score(5) = |keywords(S)| |keywords(D)| TF (Vanderwende et al., 2007) Average term frequency for all sentence words: /�i∈{words(S)} tfi score(5) = r 2Luhn’s experiments suggest an optimal limit of 4 or 5 non-significant words between keywords. 3Due to multilingua</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>Luhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal of Research and Development, 2, 159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>M Maybury</author>
</authors>
<title>Advances in automatic text summarization.</title>
<date>1999</date>
<contexts>
<context position="1828" citStr="Mani &amp; Maybury, 1999" startWordPosition="259" endWordPosition="262"> scoring methods include six methods introduced in (Litvak et al., 2010). 1 Introduction Automatically generated summaries can significantly reduce the information overload on professionals in a variety of fields, could prove beneficial for the automated classification and filtering of documents, the search for information over the Internet and applications that utilize large textual databases. Document summarization methodologies include statistic-based, using either the classic vector space model or a graph representation, and semantic-based, using ontologies and languagespecific knowledge (Mani &amp; Maybury, 1999). Although the use of language-specific knowledge can potentially improve the quality of automated summaries generated in a particular language, its language specificity ultimately restricts the use of such a summarizer to a single language. Only systems that perform equally well on different languages in the absence of any language-specific knowledge can be considered language-independent summarizers. As the number of languages used on the Internet increases continiously (there are at least 75 different languages according to a estimate performed by A. Gulli and A. Signorini1 in the end of Ja</context>
</contexts>
<marker>Mani, Maybury, 1999</marker>
<rawString>Mani, I., &amp; Maybury, M. (1999). Advances in automatic text summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Language independent extractive summarization.</title>
<date>2005</date>
<booktitle>AAAI’05: Proceedings of the 20th national conference on Artificial intelligence</booktitle>
<pages>1688--1689</pages>
<contexts>
<context position="5856" citStr="Mihalcea (2005)" startWordPosition="878" endWordPosition="879">1) and Steinberger and Jezek (2004) showed that singular value decomposition (SVD) can be applied to generate extracts. Among text representation models, graphbased text representations have gained popularity in automated summarization, as they enable the model to be enriched with syntactic and semantic relations. Salton et al. (1997) were among the first to attempt graph-based ranking methods for single document extractive summarization by generating similarity links between document paragraphs. The important paragraphs of a text were extracted using degree scores. Erkan and Radev (2004) and Mihalcea (2005) introduced approaches for unsupervised extractive summarization that rely on the application of iterative graph based ranking algorithms. In their approaches, each document is represented as a graph of sentences interconnected by similarity relations. 3 Language-Independent Scoring Methods for Sentence Extraction Various language dependent and independent sentence scoring methods have been introduced in the literature. We selected the 15 most prominent language independent methods for evaluation. Most of them can be categorized as frequency, position, length, or title-based, and they utilize </context>
<context position="12179" citStr="Mihalcea, 2005" startWordPosition="1903" endWordPosition="1904">ce, the more important the sentence is to a summary. Redundant sentences containing repetitive information are removed using a similarity filter. score(S) = sim(S, D − S): D COV O using Overlap similarity: |S∩T | min{|S|,|D−S|} D COV J using Jaccard similarity: |S∩T | |S∪D−S| D COV C using Cosine similarity: cos(~S, D ~− S) = ~S× ~D−S |~S|× |~D−S| 3.4 Graph-based Scoring Methods In this section, we describe the methods for multilingual sentence scoring using the graph text representation based on sentence (ML TR) or word (all except ML TR) segmentation. ML TR Multilingual version of TextRank (Mihalcea, 2005) without morphological analysis. Each document is represented as a directed graph of nodes that stand for sentences interconnected by similarity (overlap) relationship. To each edge connecting two vertices the weight is assigned and equal to the similarity value between the corresponding sentences. We used backward links, as it was the most successful according to the reported results in (Mihalcea, 2005). score(S) is equal to PageRank (Brin &amp; Page, 1998) of its node, according to the formula adapted to the weights assigned to edges. – Degree-based (Litvak et al., 2010):4 LUHN DEG A graph-based</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>Mihalcea, R. (2005). Language independent extractive summarization. AAAI’05: Proceedings of the 20th national conference on Artificial intelligence (pp. 1688–1689).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Nastase</author>
<author>S Szpakowicz</author>
</authors>
<title>A study of two graph algorithms in topic-driven summarization.</title>
<date>2006</date>
<booktitle>Proceedings of the Workshop on Graph-based Algorithms for Natural Language.</booktitle>
<contexts>
<context position="14630" citStr="Nastase and Szpakowicz (2006)" startWordPosition="2291" endWordPosition="2294">ength. The relevance score for a sentence is calculated as a sum of its domination scores over all paths. – PageRank-based:5 LUHN PR A graph-based extension of the LUHN measure in which the node PageRank value is used instead of the word frequency: keywords are those words represented by nodes with a PageRank score higher than a predefined threshold (see Table 1). KEY PR Graph-based extension of KEY measure. COV PR Graph-based extension of COV measure. PR Average PageRank for all sentence nodes: L-�i∈{wor score(S) =ds(S)} PRi|S| – Similarity-based. Edge matching techniques similar to those of Nastase and Szpakowicz (2006) are used. Edge matching is an alternative approach to measure the similarity between graphs based on the number of common edges: TITLE E O Graph-based extension of TITLE O – Overlap-based edge matching between title and sentence graphs. TITLE E J Graph-based extension of TITLE J – Jaccard-based edge matching between title and sentence graphs. D COV E O Graph-based extension of D COV O – Overlap-based edge matching between sentence and document complement (the rest of a document sentences) graphs. D COV E J Graph-based extension of D COV J – Jaccard-based edge matching 5Using undirected word g</context>
</contexts>
<marker>Nastase, Szpakowicz, 2006</marker>
<rawString>Nastase, V., &amp; Szpakowicz, S. (2006). A study of two graph algorithms in topic-driven summarization. Proceedings of the Workshop on Graph-based Algorithms for Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Neto</author>
<author>A Santos</author>
<author>C Kaestner</author>
<author>A Freitas</author>
</authors>
<title>Generating text summaries through the relative importance of topics.</title>
<date>2000</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>300--309</pages>
<contexts>
<context position="10127" citStr="Neto et al., 2000" startWordPosition="1564" endWordPosition="1567"> keywords2 and csi = |keywords(ci)|2 |ci| KEY (Edmundson, 1969) Sum of the keyword frequencies: score(5) = EiE{keywords(S)} tfi, where tfi is term in-document frequency of keyword i. COV (Kallel et al., 2004) Ratio of keyword numbers (Coverage): score(5) = |keywords(S)| |keywords(D)| TF (Vanderwende et al., 2007) Average term frequency for all sentence words: /�i∈{words(S)} tfi score(5) = r 2Luhn’s experiments suggest an optimal limit of 4 or 5 non-significant words between keywords. 3Due to multilingual focus of our work, exact word matching was used in all similarity-based methods. S TFISF (Neto et al., 2000) Average term frequency inverted sentence frequency for all sentence words: score(5) = &amp;E{words(S)} tfi X isfi, where isfi = 1 − log(ni), where n is the number log(n) of sentences in a document and ni is the number of sentences containing word i. SVD (Steinberger &amp; Jezek, 2004) score(5) is equal to the length of a sentence vector in E2VT after computing the Singular Value Decomposition of a term by sentence matrix A = UEVT – Title (Edmundson, 1969) similarity3 to the title, score(5) = sim(5,T): TITLE O using overlap similarity: |S∩T| min{|S|,|T|} TITLE J using Jaccard similarity: S∩T �S∪Tj 63 </context>
</contexts>
<marker>Neto, Santos, Kaestner, Freitas, 2000</marker>
<rawString>Neto, J., Santos, A., Kaestner, C., &amp; Freitas, A. (2000). Generating text summaries through the relative importance of topics. Lecture Notes in Computer Science, 300–309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>S Blair-Goldensohn</author>
<author>Z Zhang</author>
</authors>
<title>Experiments in single and multidocument summarization using MEAD. First Document Understanding Conference.</title>
<date>2001</date>
<contexts>
<context position="4748" citStr="Radev et al. (2001)" startWordPosition="705" endWordPosition="708">n a summary in the same order they appeared in the original text. Statistical methods for calculating the relevance score of each fragment can rely on such information as: fragment position inside the document, its length, whether it contains keywords or title words. Research by Luhn (1958), in which the significance factor of a sentence is based on the frequency and the relative position of significant words within that sentence, is considered the first on automated text summarization. Luhn’s work was followed shortly thereafter by that of Edmundson (1969) and some time later by studies from Radev et al. (2001) and Saggion et al. (2003), all of who applied linear combinations of multiple statistical methods to rank sentences using the vector space model as a text representation. In (Litvak et al., 2010) we improve the summarization quality by identifying the best linear combination of the metrics evaluated in this paper. Several information retrieval and machine learning techniques have been proposed for determining sentence importance (Kupiec et al., 1995; Wong et al., 2008). Gong and Liu (2001) and Steinberger and Jezek (2004) showed that singular value decomposition (SVD) can be applied to genera</context>
<context position="16967" citStr="Radev et al., 2001" startWordPosition="2651" endWordPosition="2654">as follows: - To evaluate the performance of different approaches for extractive single-document summarization using graph and vector representations. - To compare the quality of the multilingual summarization methods proposed in our previous work (Litvak et al., 2010) to the state-of-the-art approaches. - To identify sentence ranking methods that work equally well on both languages. 4.2 Text Preprocessing Extractive summarization relies critically on proper sentence segmentation to insure the quality of the summarization results. We used a sentence splitter provided with the MEAD summarizer (Radev et al., 2001) for English and a simple splitter for Hebrew splitting the text at every period, exclamation point, or question mark.6 4.3 Experimental Data For English texts, we used the corpus of summarized documents provided for the single doc6Although the same set of splitting rules may be used for both languages, separate splitters were used since the MEAD splitter is restricted to European languages. 65 ument summarization task at the Document Understanding Conference 2002 (DUC, 2002). This benchmark dataset contains 533 news articles, each of which is at least ten sentences long and has two to three h</context>
</contexts>
<marker>Radev, Blair-Goldensohn, Zhang, 2001</marker>
<rawString>Radev, D., Blair-Goldensohn, S., &amp; Zhang, Z. (2001). Experiments in single and multidocument summarization using MEAD. First Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>K Bontcheva</author>
<author>H Cunningham</author>
</authors>
<title>Robust generic and query-based summarisation.</title>
<date>2003</date>
<booktitle>EACL ’03: Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4774" citStr="Saggion et al. (2003)" startWordPosition="710" endWordPosition="713">order they appeared in the original text. Statistical methods for calculating the relevance score of each fragment can rely on such information as: fragment position inside the document, its length, whether it contains keywords or title words. Research by Luhn (1958), in which the significance factor of a sentence is based on the frequency and the relative position of significant words within that sentence, is considered the first on automated text summarization. Luhn’s work was followed shortly thereafter by that of Edmundson (1969) and some time later by studies from Radev et al. (2001) and Saggion et al. (2003), all of who applied linear combinations of multiple statistical methods to rank sentences using the vector space model as a text representation. In (Litvak et al., 2010) we improve the summarization quality by identifying the best linear combination of the metrics evaluated in this paper. Several information retrieval and machine learning techniques have been proposed for determining sentence importance (Kupiec et al., 1995; Wong et al., 2008). Gong and Liu (2001) and Steinberger and Jezek (2004) showed that singular value decomposition (SVD) can be applied to generate extracts. Among text re</context>
</contexts>
<marker>Saggion, Bontcheva, Cunningham, 2003</marker>
<rawString>Saggion, H., Bontcheva, K., &amp; Cunningham, H. (2003). Robust generic and query-based summarisation. EACL ’03: Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Singhal</author>
<author>M Mitra</author>
<author>C Buckley</author>
</authors>
<title>Automatic text structuring and summarization.</title>
<date>1997</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>33</volume>
<pages>193--207</pages>
<contexts>
<context position="5577" citStr="Salton et al. (1997)" startWordPosition="836" endWordPosition="839">he summarization quality by identifying the best linear combination of the metrics evaluated in this paper. Several information retrieval and machine learning techniques have been proposed for determining sentence importance (Kupiec et al., 1995; Wong et al., 2008). Gong and Liu (2001) and Steinberger and Jezek (2004) showed that singular value decomposition (SVD) can be applied to generate extracts. Among text representation models, graphbased text representations have gained popularity in automated summarization, as they enable the model to be enriched with syntactic and semantic relations. Salton et al. (1997) were among the first to attempt graph-based ranking methods for single document extractive summarization by generating similarity links between document paragraphs. The important paragraphs of a text were extracted using degree scores. Erkan and Radev (2004) and Mihalcea (2005) introduced approaches for unsupervised extractive summarization that rely on the application of iterative graph based ranking algorithms. In their approaches, each document is represented as a graph of sentences interconnected by similarity relations. 3 Language-Independent Scoring Methods for Sentence Extraction Vario</context>
<context position="13274" citStr="Salton et al., 1997" startWordPosition="2074" endWordPosition="2077">ording to the formula adapted to the weights assigned to edges. – Degree-based (Litvak et al., 2010):4 LUHN DEG A graph-based extension of the LUHN measure, in which a node degree is used instead of a word frequency: words are considered significant if they are represented by nodes of a higher degree than a predefined threshold (see Table 1). KEY DEG Graph-based extension of KEY measure. COV DEG Graph-based extension of COV measure. DEG Average degree for all sentence nodes: score(S) = K∈{wor ds(S)} Degi . |S| GRASE(GRaph-based Automated Sentence Extractor) Modification of Salton’s algorithm (Salton et al., 1997) using the graph 4All proposed here degree-based methods, except for GRASE, use undirected graphs and degree of nodes as a predictive feature. The methods based on the directed word graphs and distinguishing between in- and out-links were outperformed in our preliminary experiments by the undirected approach. 64 representation defined in Section 3.1 above. In our graph representation, all sentences are represented by paths, completely or partially. To identify the relevant sentences, we search for the bushy paths and extract from them the sentences that appear the most frequently. Each sentenc</context>
</contexts>
<marker>Salton, Singhal, Mitra, Buckley, 1997</marker>
<rawString>Salton, G., Singhal, A., Mitra, M., &amp; Buckley, C. (1997). Automatic text structuring and summarization. Information Processing and Management, 33, 193–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C N Satoshi</author>
<author>S Satoshi</author>
<author>M Murata</author>
<author>K Uchimoto</author>
<author>M Utiyama</author>
<author>H Isahara</author>
</authors>
<title>Sentence extraction system assembling multiple evidence.</title>
<date>2001</date>
<booktitle>Proceedings of 2nd NTCIR Workshop</booktitle>
<pages>319--324</pages>
<contexts>
<context position="9111" citStr="Satoshi et al., 2001" startWordPosition="1408" endWordPosition="1411">s in the specified order. 3.2 Structure-based Scoring Methods In this section, we describe the existing structure-based methods for multilingual sentence scoring. These methods do not require any text representation and are based on its structure. – Position (Baxendale, 1958): POS L Closeness to the end of the document: score(5i) = i, where i is a sequential number of a sentence in a document; POS F Closeness to the beginning of the document: score(5i) = 1i ; POS B Closeness to the borders of the document: score(5i) = max(1i , 1 n−i+1), where n is the total number of sentences in D. – Length (Satoshi et al., 2001): LEN W Number of words in a sentence; LEN CH Number of characters in a sentence. 3.3 Vector-based Scoring Methods In this section, we describe the vector-based methods for multilingual sentence scoring, that are based on the vector space model for text representation. – Frequency-based: LUHN (Luhn, 1958) score(5) = maxciE{clusters(S)}{csi}, where clusters are portions of a sentence bracketed by keywords2 and csi = |keywords(ci)|2 |ci| KEY (Edmundson, 1969) Sum of the keyword frequencies: score(5) = EiE{keywords(S)} tfi, where tfi is term in-document frequency of keyword i. COV (Kallel et al.,</context>
</contexts>
<marker>Satoshi, Satoshi, Murata, Uchimoto, Utiyama, Isahara, 2001</marker>
<rawString>Satoshi, C. N., Satoshi, S., Murata, M., Uchimoto, K., Utiyama, M., &amp; Isahara, H. (2001). Sentence extraction system assembling multiple evidence. Proceedings of 2nd NTCIR Workshop (pp. 319–324).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Schenker</author>
<author>H Bunke</author>
<author>M Last</author>
<author>A Kandel</author>
</authors>
<title>Classification of web documents using graph matching.</title>
<date>2004</date>
<journal>International Journal of Pattern Recognition and Artificial Intelligence,</journal>
<volume>18</volume>
<pages>475--496</pages>
<contexts>
<context position="8226" citStr="Schenker et al. (2004)" startWordPosition="1256" endWordPosition="1259">ods, and each category also contains an internal taxonomy. Sections 3.2, 3.3, and 3.4 present structure-, vector-, and graph-based methods, respectively. With each description, a reference to the original work where the method was proposed for extractive summarization is included. We denote sentence by 5 and text document by D. 3.1 Text Representation Models The vector-based scoring methods listed below use tf or tf-idf term weights to evaluate sentence importance while that used by the graphbased methods (except for TextRank) is based on the word-based graph representation model presented in Schenker et al. (2004). We represent each document by a directed, labeled, unweighted graph in which nodes represent unique terms (distinct normalized words) and edges represent order-relationships between two terms. Each edge is labeled with the IDs of sentences that contain both words in the specified order. 3.2 Structure-based Scoring Methods In this section, we describe the existing structure-based methods for multilingual sentence scoring. These methods do not require any text representation and are based on its structure. – Position (Baxendale, 1958): POS L Closeness to the end of the document: score(5i) = i,</context>
</contexts>
<marker>Schenker, Bunke, Last, Kandel, 2004</marker>
<rawString>Schenker, A., Bunke, H., Last, M., &amp; Kandel, A. (2004). Classification of web documents using graph matching. International Journal of Pattern Recognition and Artificial Intelligence, 18, 475–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Steinberger</author>
<author>K Jezek</author>
</authors>
<title>Text summarization and singular value decomposition.</title>
<date>2004</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>245--254</pages>
<contexts>
<context position="5276" citStr="Steinberger and Jezek (2004)" startWordPosition="790" endWordPosition="793">ortly thereafter by that of Edmundson (1969) and some time later by studies from Radev et al. (2001) and Saggion et al. (2003), all of who applied linear combinations of multiple statistical methods to rank sentences using the vector space model as a text representation. In (Litvak et al., 2010) we improve the summarization quality by identifying the best linear combination of the metrics evaluated in this paper. Several information retrieval and machine learning techniques have been proposed for determining sentence importance (Kupiec et al., 1995; Wong et al., 2008). Gong and Liu (2001) and Steinberger and Jezek (2004) showed that singular value decomposition (SVD) can be applied to generate extracts. Among text representation models, graphbased text representations have gained popularity in automated summarization, as they enable the model to be enriched with syntactic and semantic relations. Salton et al. (1997) were among the first to attempt graph-based ranking methods for single document extractive summarization by generating similarity links between document paragraphs. The important paragraphs of a text were extracted using degree scores. Erkan and Radev (2004) and Mihalcea (2005) introduced approach</context>
<context position="10405" citStr="Steinberger &amp; Jezek, 2004" startWordPosition="1612" endWordPosition="1615"> |keywords(D)| TF (Vanderwende et al., 2007) Average term frequency for all sentence words: /�i∈{words(S)} tfi score(5) = r 2Luhn’s experiments suggest an optimal limit of 4 or 5 non-significant words between keywords. 3Due to multilingual focus of our work, exact word matching was used in all similarity-based methods. S TFISF (Neto et al., 2000) Average term frequency inverted sentence frequency for all sentence words: score(5) = &amp;E{words(S)} tfi X isfi, where isfi = 1 − log(ni), where n is the number log(n) of sentences in a document and ni is the number of sentences containing word i. SVD (Steinberger &amp; Jezek, 2004) score(5) is equal to the length of a sentence vector in E2VT after computing the Singular Value Decomposition of a term by sentence matrix A = UEVT – Title (Edmundson, 1969) similarity3 to the title, score(5) = sim(5,T): TITLE O using overlap similarity: |S∩T| min{|S|,|T|} TITLE J using Jaccard similarity: S∩T �S∪Tj 63 Multilingual sentence scoringmethods D_COV_O* D_COV_J* D_COV_C* TITLE_O TITLE_J TITLE_C Structurebased Vectorbased Similarity Position Length Frequency Title Document POS_F POS_L POS_B LEN_W LEN_CH LUHN† KEY † COV † TF TFIISF SVD Graphbased Pagerank Similarity Degree LUHN_DEG †</context>
</contexts>
<marker>Steinberger, Jezek, 2004</marker>
<rawString>Steinberger, J., &amp; Jezek, K. (2004). Text summarization and singular value decomposition. Lecture Notes in Computer Science, 245–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vanderwende</author>
<author>H Suzuki</author>
<author>C Brockett</author>
<author>A Nenkova</author>
</authors>
<title>Beyond SumBasic: Taskfocused summarization with sentence simplification and lexical expansion. Information processing and management,</title>
<date>2007</date>
<volume>43</volume>
<pages>1606--1618</pages>
<contexts>
<context position="9823" citStr="Vanderwende et al., 2007" startWordPosition="1516" endWordPosition="1519">ector-based Scoring Methods In this section, we describe the vector-based methods for multilingual sentence scoring, that are based on the vector space model for text representation. – Frequency-based: LUHN (Luhn, 1958) score(5) = maxciE{clusters(S)}{csi}, where clusters are portions of a sentence bracketed by keywords2 and csi = |keywords(ci)|2 |ci| KEY (Edmundson, 1969) Sum of the keyword frequencies: score(5) = EiE{keywords(S)} tfi, where tfi is term in-document frequency of keyword i. COV (Kallel et al., 2004) Ratio of keyword numbers (Coverage): score(5) = |keywords(S)| |keywords(D)| TF (Vanderwende et al., 2007) Average term frequency for all sentence words: /�i∈{words(S)} tfi score(5) = r 2Luhn’s experiments suggest an optimal limit of 4 or 5 non-significant words between keywords. 3Due to multilingual focus of our work, exact word matching was used in all similarity-based methods. S TFISF (Neto et al., 2000) Average term frequency inverted sentence frequency for all sentence words: score(5) = &amp;E{words(S)} tfi X isfi, where isfi = 1 − log(ni), where n is the number log(n) of sentences in a document and ni is the number of sentences containing word i. SVD (Steinberger &amp; Jezek, 2004) score(5) is equal</context>
</contexts>
<marker>Vanderwende, Suzuki, Brockett, Nenkova, 2007</marker>
<rawString>Vanderwende, L., Suzuki, H., Brockett, C., &amp; Nenkova, A. (2007). Beyond SumBasic: Taskfocused summarization with sentence simplification and lexical expansion. Information processing and management, 43, 1606–1618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wong</author>
<author>M Wu</author>
<author>W Li</author>
</authors>
<title>Extractive summarization using supervised and semi-supervised learning.</title>
<date>2008</date>
<booktitle>Proceedings of the 22nd International Conference on Computational Linguistics</booktitle>
<pages>985--992</pages>
<contexts>
<context position="5222" citStr="Wong et al., 2008" startWordPosition="781" endWordPosition="784">t summarization. Luhn’s work was followed shortly thereafter by that of Edmundson (1969) and some time later by studies from Radev et al. (2001) and Saggion et al. (2003), all of who applied linear combinations of multiple statistical methods to rank sentences using the vector space model as a text representation. In (Litvak et al., 2010) we improve the summarization quality by identifying the best linear combination of the metrics evaluated in this paper. Several information retrieval and machine learning techniques have been proposed for determining sentence importance (Kupiec et al., 1995; Wong et al., 2008). Gong and Liu (2001) and Steinberger and Jezek (2004) showed that singular value decomposition (SVD) can be applied to generate extracts. Among text representation models, graphbased text representations have gained popularity in automated summarization, as they enable the model to be enriched with syntactic and semantic relations. Salton et al. (1997) were among the first to attempt graph-based ranking methods for single document extractive summarization by generating similarity links between document paragraphs. The important paragraphs of a text were extracted using degree scores. Erkan an</context>
</contexts>
<marker>Wong, Wu, Li, 2008</marker>
<rawString>Wong, K., Wu, M., &amp; Li, W. (2008). Extractive summarization using supervised and semi-supervised learning. Proceedings of the 22nd International Conference on Computational Linguistics (pp. 985–992).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>