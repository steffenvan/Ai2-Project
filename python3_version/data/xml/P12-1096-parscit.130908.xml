<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000112">
<title confidence="0.999691">
A Ranking-based Approach to Word Reordering
for Statistical Machine Translation*
</title>
<author confidence="0.999013">
Nan Yangt, Mu Lit, Dongdong Zhangt, and Nenghai Yut
</author>
<affiliation confidence="0.996633">
tMOE-MS Key Lab of MCC
University of Science and Technology of China
</affiliation>
<email confidence="0.703333">
v-nayang@microsoft.com, ynh@ustc.edu.cn
</email>
<affiliation confidence="0.820306">
tMicrosoft Research Asia
</affiliation>
<email confidence="0.99832">
{muli,dozhang}@microsoft.com
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99952675">
Long distance word reordering is a major
challenge in statistical machine translation re-
search. Previous work has shown using source
syntactic trees is an effective way to tackle
this problem between two languages with sub-
stantial word order difference. In this work,
we further extend this line of exploration and
propose a novel but simple approach, which
utilizes a ranking model based on word or-
der precedence in the target language to repo-
sition nodes in the syntactic parse tree of a
source sentence. The ranking model is auto-
matically derived from word aligned parallel
data with a syntactic parser for source lan-
guage based on both lexical and syntactical
features. We evaluated our approach on large-
scale Japanese-English and English-Japanese
machine translation tasks, and show that it can
significantly outperform the baseline phrase-
based SMT system.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9992237">
Modeling word reordering between source and tar-
get sentences has been a research focus since the
emerging of statistical machine translation. In
phrase-based models (Och, 2002; Koehn et al.,
2003), phrase is introduced to serve as the funda-
mental translation element and deal with local re-
ordering, while a distance based distortion model is
used to coarsely depict the exponentially decayed
word movement probabilities in language transla-
tion. Further work in this direction employed lexi-
</bodyText>
<footnote confidence="0.9726175">
*This work has been done while the first author was visiting
Microsoft Research Asia.
</footnote>
<bodyText confidence="0.99915994117647">
calized distortion models, including both generative
(Koehn et al., 2005) and discriminative (Zens and
Ney, 2006; Xiong et al., 2006) variants, to achieve
finer-grained estimations, while other work took into
account the hierarchical language structures in trans-
lation (Chiang, 2005; Galley and Manning, 2008).
Long-distance word reordering between language
pairs with substantial word order difference, such as
Japanese with Subject-Object-Verb (SOV) structure
and English with Subject-Verb-Object (SVO) struc-
ture, is generally viewed beyond the scope of the
phrase-based systems discussed above, because of
either distortion limits or lack of discriminative fea-
tures for modeling. The most notable solution to this
problem is adopting syntax-based SMT models, es-
pecially methods making use of source side syntac-
tic parse trees. There are two major categories in this
line of research. One is tree-to-string model (Quirk
et al., 2005; Liu et al., 2006) which directly uses
source parse trees to derive a large set of translation
rules and associated model parameters. The other
is called syntax pre-reordering – an approach that
re-positions source words to approximate target lan-
guage word order as much as possible based on the
features from source syntactic parse trees. This is
usually done in a preprocessing step, and then fol-
lowed by a standard phrase-based SMT system that
takes the re-ordered source sentence as input to fin-
ish the translation.
In this paper, we continue this line of work and
address the problem of word reordering based on
source syntactic parse trees for SMT. Similar to most
previous work, our approach tries to rearrange the
source tree nodes sharing a common parent to mimic
</bodyText>
<page confidence="0.937229">
912
</page>
<note confidence="0.9859465">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 912–920,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99957953125">
the word order in target language. To this end, we
propose a simple but effective ranking-based ap-
proach to word reordering. The ranking model is
automatically derived from the word aligned parallel
data, viewing the source tree nodes to be reordered
as list items to be ranked. The ranks of tree nodes are
determined by their relative positions in the target
language – the node in the most front gets the high-
est rank, while the ending word in the target sentence
gets the lowest rank. The ranking model is trained
to directly minimize the mis-ordering of tree nodes,
which differs from the prior work based on maxi-
mum likelihood estimations of reordering patterns
(Li et al., 2007; Genzel, 2010), and does not require
any special tweaking in model training. The ranking
model can not only be used in a pre-reordering based
SMT system, but also be integrated into a phrase-
based decoder serving as additional distortion fea-
tures.
We evaluated our approach on large-scale
Japanese-English and English-Japanese machine
translation tasks, and experimental results show that
our approach can bring significant improvements to
the baseline phrase-based SMT system in both pre-
ordering and integrated decoding settings.
In the rest of the paper, we will first formally
present our ranking-based word reordering model,
then followed by detailed steps of modeling train-
ing and integration into a phrase-based SMT system.
Experimental results are shown in Section 5. Section
6 consists of more discussions on related work, and
Section 7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.854751" genericHeader="introduction">
2 Word Reordering as Syntax Tree Node
Ranking
</sectionHeader>
<bodyText confidence="0.999952833333333">
Given a source side parse tree Te, the task of word
reordering is to transform Te to T&apos;e, so that e&apos; can
match the word order in target language as much as
possible. In this work, we only focus on reordering
that can be obtained by permuting children of every
tree nodes in Te. We use children to denote direct de-
scendants of tree nodes for constituent trees; while
for dependency trees, children of a node include not
only all direct dependents, but also the head word
itself. Figure 1 gives a simple example showing the
word reordering between English and Japanese. By
rearranging the position of tree nodes in the English
</bodyText>
<equation confidence="0.993372125">
PRP VBP VBG TO VB NN
e0 e1 e2 e3 e4 e5
I am trying to play music
私は 音楽を 再生 しようと している
j0 j1 j2 j3 j4
PRP NN VB TO VBG VBP
私は 音楽を 再生 しようと している
j0 j1 j2 j3 j4
</equation>
<figureCaption confidence="0.733111">
Figure 1: An English-to-Japanese sentence pair. By
permuting tree nodes in the parse tree, the source
sentence is reordered into the target language or-
</figureCaption>
<bodyText confidence="0.978022105263158">
der. Constituent tree is shown above the source
sentence; arrows below the source sentences show
head-dependent arcs for dependency tree; word
alignment links are lines without arrow between the
source and target sentences.
parse tree, we can obtain the same word order of
Japanese translation. It is true that tree-based re-
ordering cannot cover all word movement operations
in language translation, previous work showed that
this method is still very effective in practice (Xu et
al., 2009, Visweswariah et al., 2010).
Following this principle, the word reordering task
can be broken into sub-tasks, in which we only
need to determine the order of children nodes for
all non-leaf nodes in the source parse tree. For a
tree node t with children {c1, c2,. . . , cnI, we re-
arrange the children to target-language-like order
{cπ(i1), cπ(i2), ... , cπ(in)I. If we treat the reordered
position 7r(i) of child ci as its “rank”, the reorder-
</bodyText>
<figure confidence="0.99093080952381">
S
VP
Original
Tree
VP
NP
NP
S
VP
VP
S
VP
Reordered Tree
VP
S
VP
NP
VP
NP
e0 e1 e2 e3 e4 e5
I music play to trying am
</figure>
<page confidence="0.998261">
913
</page>
<bodyText confidence="0.999970083333333">
ing problem is naturally translated into a ranking
problem: to reorder, we determine a “rank” for each
child, then the children are sorted according to their
“ranks”. As it is often impractical to directly assign
a score for each permutation due to huge number of
possible permutations, a widely used method is to
use a real valued function f to assign a value to each
node, which is called a ranking function (Herbrich
et al., 2000). If we can guarantee (f(i) − f(j)) and
(7r(i) − 7r(j)) always has the same sign, we can get
the same permutation as 7r because values of f are
only used to sort the children. For example, con-
sider the node rooted at trying in the dependency
tree in Figure 1. Four children form a list {I, am, try-
ing, play} to be ranked. Assuming ranking function
f can assign values {0.94, −1.83, −1.50, −1.20}
for {I, am, trying, play} respectively, we can get a
sorted list {I, play, trying, am}, which is the desired
permutation according to the target.
More formally, for a tree node t with children
{c1, c2, ... , cn}, our ranking model assigns a rank
f(ci, t) for each child ci, then the children are sorted
according to the rank in a descending order. The
ranking function f has the following form:
</bodyText>
<equation confidence="0.888699">
f(ci, t) = � Bj(ci, t) · wj (1)
j
</equation>
<bodyText confidence="0.999769333333333">
where the Bj is a feature representing the tree node t
and its child ci, and wj is the corresponding feature
weight.
</bodyText>
<sectionHeader confidence="0.998952" genericHeader="method">
3 Ranking Model Training
</sectionHeader>
<bodyText confidence="0.999989857142857">
To learn ranking function in Equation (1), we need to
determine the feature set 0 and learn weight vector
w from reorder examples. In this section, we first
describe how to extract reordering examples from
parallel corpus; then we show our features for rank-
ing function; finally, we discuss how to train the
model from the extracted examples.
</bodyText>
<subsectionHeader confidence="0.997957">
3.1 Reorder Example Acquisition
</subsectionHeader>
<bodyText confidence="0.999889833333333">
For a sentence pair (e, f, a) with syntax tree Te on
the source side, we need to determine which re-
ordered tree T0e, best represents the word order in
target sentence f. For a tree node t in Te, if its chil-
dren align to disjoint target spans, we can simply ar-
range them in the order of their corresponding target
</bodyText>
<figure confidence="0.913758">
Problem with latter procedure
後者 の 手順 で 問題 は ... に ある
(a) gold alignment
後者 の 手順 で 問題 は ... に ある
(b) auto alignment
</figure>
<figureCaption confidence="0.920051">
Figure 2: Fragment of a sentence pair. (a) shows
gold alignment; (b) shows automatically generated
alignment which contains errors.
</figureCaption>
<bodyText confidence="0.999906576923077">
spans. Figure 2 shows a fragment of one sentence
pair in our training data. Consider the subtree rooted
at word “Problem”. With the gold alignment, “Prob-
lem” is aligned to the 5th target word, and “with
latter procedure” are aligned to target span [1, 3],
thus we can simply put “Problem” after “with latter
procedure”. Recursively applying this process down
the subtree, we get “latter procedure with Problem”
which perfectly matches the target language.
As pointed out by (Li et al., 2007), in practice,
nodes often have overlapping target spans due to er-
roneous word alignment or different syntactic struc-
tures between source and target sentences. (b) in
Figure 2 shows the automatically generated align-
ment for the sentence pair fragment. The word
“with” is incorrectly aligned to the 6th Japanese
word “ha”; as a result, “with latter procedure” now
has target span [1, 6], while “Problem” aligns to
[5, 5]. Due to this overlapping, it becomes unclear
which permutation of “Problem” and “with latter
procedure” is a better match of the target phrase; we
need a better metric to measure word order similar-
ity between reordered source and target sentences.
We choose to find the tree T0e, with minimal align-
ment crossing-link number (CLN) (Genzel, 2010)
to f as our golden reordered tree.1 Each crossing-
</bodyText>
<footnote confidence="0.508804">
1A simple solution is to exclude all trees with overlapping
target spans from training. But in our experiment, this method
</footnote>
<figure confidence="0.976918">
Problem with latter procedure
lies
in ...
lies in ...
</figure>
<page confidence="0.992605">
914
</page>
<bodyText confidence="0.999885166666667">
link (i1j1, i2j2) is a pair of alignment links crossing
each other. CLN reaches zero if f is monotonically
aligned to e0, and increases as there are more word
reordering between e0 and f. For example, in Fig-
ure 1, there are 6 crossing-links in the original tree:
(e1j4, e2j3), (e1j4, e4j2), (e1j4, e5j1), (e2j3, e4j2),
(e2j3, e5j1) and (e4j2, e5j1); thus CLN for the origi-
nal tree is 6. CLN for the reordered tree is 0 as there
are no crossing-links. This metric is easy to com-
pute, and is not affected by unaligned words (Gen-
zel, 2010).
We need to find the reordered tree with minimal
CLN among all reorder candidates. As the number
of candidates is in the magnitude exponential with
respect to the degree of tree Te 2, it is not always
computationally feasible to enumerate through all
candidates. Our solution is as follows.
First, we give two definitions.
</bodyText>
<listItem confidence="0.982046714285714">
• CLN(t): the number of crossing-links
(i1j1, i2j2) whose source words e0i1 and e0i2
both fall under sub span of the tree node t.
• CCLN(t): the number of crossing-links
(i1j1, i2j2) whose source words e0i1 and e0i2 fall
under sub span of t’s two different children
nodes c1 and c2 respectively.
</listItem>
<bodyText confidence="0.857147">
Apparently CLN of a tree T0 equals to
CLN(root of T0), and CLN(t) can be recur-
sively expressed as:
</bodyText>
<equation confidence="0.991425">
CLN(t) = CCLN(t) + � CLN(c)
</equation>
<bodyText confidence="0.966794166666667">
child c of t
Take the original tree in Figure 1 for example. At the
root node trying, CLN(trying) is 6 because there are
six crossing-links under its sub-span: (e1j4, e2j3),
(e1j4, e4j2), (e1j4,e5j1), (e2j3,e4j2), (e2j3, e5j1)
and (e4j2, e5j1). On the other hand, CCLN(trying)
is 5 because (e4j2, e5j1) falls under its child node
play, thus does not count towards CCLN of trying.
From the definition, we can easily see that
CCLN(t) can be determined solely by the order of
t’s direct children, and CLN(t) is only affected by
discarded too many training instances and led to degraded re-
ordering performance.
2In our experiments, there are nodes with more than 10 chil-
dren for English dependency trees.
the reorder in the subtree of t. This observation en-
ables us to divide the task of finding the reordered
tree T0e, with minimal CLN into independently find-
ing the children permutation of each node with min-
imal CCLN. Unfortunately, the time cost for the sub-
task is still O(n!) for a node with n children. Instead
of enumerating through all permutations, we only
search the Inversion Transduction Grammar neigh-
borhood of the initial sequence (Tromble, 2009). As
pointed out by (Tromble, 2009), the ITG neighbor-
hood is large enough for reordering task, and can be
searched through efficiently using a CKY decoder.
After finding the best reordered tree T0e,, we can
extract one reorder example from every node with
more than one child.
</bodyText>
<subsectionHeader confidence="0.908692">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999965">
Features for the ranking model are extracted from
source syntax trees. For English-to-Japanese task,
we extract features from Stanford English Depen-
dency Tree (Marneffe et al., 2006), including lexi-
cons, Part-of-Speech tags, dependency labels, punc-
tuations and tree distance between head and depen-
dent. For Japanese-to-English task, we use a chunk-
based Japanese dependency tree (Kudo and Mat-
sumoto, 2002). Different from features for English,
we do not use dependency labels because they are
not available from the Japanese parser. Additionally,
Japanese function words are also included as fea-
tures because they are important grammatical clues.
The detailed feature templates are shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.999301">
3.3 Learning Method
</subsectionHeader>
<bodyText confidence="0.999939142857143">
There are many well studied methods available to
learn the ranking function from extracted examples.,
ListNet (?) etc. We choose to use RankingSVM
(Herbrich et al., 2000), a pair-wised ranking method,
for its simplicity and good performance.
For every reorder example t with children
{c1, c2, ... , cn} and their desired permutation
{cπ(i1), cπ(i2), ... , cπ(in)}, we decompose it into a
set of pair-wised training instances. For any two
children nodes ci and cj with i &lt; j , we extract a
positive instance if 7r(i) &lt; 7r(j), otherwise we ex-
tract a negative instance. The feature vector for both
positive instance and negative instance is (Bci −Bcj),
where B,, and Bpi are feature vectors for ci and cj
</bodyText>
<page confidence="0.986796">
915
</page>
<table confidence="0.999839266666667">
E-J
cl cl · dst cl · pct
cl · dst · pct cl · lcl cl · rcl
cl · lcl · dst cl · rcl · dst cl · clex
cl · clex cl · clex · dst cl · clex · dst
cl · hlex cl · hlex cl · hlex · dst
cl · hlex · dst cl · clex · pct cl · clex · pct
cl · hlex · pct cl · hlex · pct
J-E
ctf ctf · dst ctf · lct
ctf · rct ctf · lct · dst cl · rct · dst
ctf · clex ctf · clex ctf · clex · dst
ctf · clex · dst ctf · hf ctf · hf
ctf · hf · dst ctf · hf · dst ctf · hlex
ctf · hlex ctf · hlex · dst ctf · hlex · dst
</table>
<tableCaption confidence="0.9446735">
Table 1: Feature templates for ranking function. All
templates are implicitly conjuncted with the pos tag
</tableCaption>
<bodyText confidence="0.844994083333333">
of head node.
c: child to be ranked; h: head node
lc: left sibling of c; rc: right sibling of c
l: dependency label; t: pos tag
lex: top frequency lexicons
f: Japanese function word
dst: tree distance between c and h
pct: punctuation node between c and h
respectively. In this way, ranking function learning
is turned into a simple binary classification problem,
which can be easily solved by a two-class linear sup-
port vector machine.
</bodyText>
<sectionHeader confidence="0.998716" genericHeader="method">
4 Integration into SMT system
</sectionHeader>
<bodyText confidence="0.99986448">
There are two ways to integrate the ranking reorder-
ing model into a phrase-based SMT system: the pre-
reorder method, and the decoding time constraint
method.
For pre-reorder method, ranking reorder model
is applied to reorder source sentences during both
training and decoding. Reordered sentences can go
through the normal pipeline of a phrase-based de-
coder.
The ranking reorder model can also be integrated
into a phrase based decoder. Integrated method takes
the original source sentence e as input, and ranking
model generates a reordered e&apos; as a word order ref-
erence for the decoder. A simple penalty scheme
is utilized to penalize decoder reordering violating
ranking reorder model’s prediction e&apos;. In this paper,
our underlying decoder is a CKY decoder follow-
ing Bracketing Transduction Grammar (Wu, 1997;
Xiong et al., 2006), thus we show how the penalty
is implemented in the BTG decoder as an example.
Similar penalty can be designed for other decoders
without much effort.
Under BTG, three rules are used to derive transla-
tions: one unary terminal rule, one straight rule and
one inverse rule:
</bodyText>
<equation confidence="0.93343">
A e/f
A [A1, A2]
A (A1, A2)
</equation>
<bodyText confidence="0.998391">
We have three penalty triggers when any rules are
applied during decoding:
</bodyText>
<listItem confidence="0.979756636363636">
• Discontinuous penalty fde: it fires for all rules
when source span of either A, A1 or A2 is
mapped to discontinuous span in e&apos;.
• Wrong straight rule penalty ft: it fires for
straight rule when source spans of A1 and A2
are not mapped to two adjacent spans in e&apos; in
straight order.
• Wrong inverse rule penalty fi,,: it fires for in-
verse rule when source spans of A1 and A2 are
not mapped to two adjacent spans in e&apos; in in-
verse order.
</listItem>
<bodyText confidence="0.999956666666667">
The above three penalties are added as additional
features into the log-linear model of the phrase-
based system. Essentially they are soft constraints
to encourage the decoder to choose translations with
word order similar to the prediction of ranking re-
order model.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999623">
To test our ranking reorder model, we carry out ex-
periments on large scale English-To-Japanese, and
Japanese-To-English translation tasks.
</bodyText>
<subsectionHeader confidence="0.84054">
5.1 Data
</subsectionHeader>
<subsubsectionHeader confidence="0.558463">
5.1.1 Evaluation Data
</subsubsectionHeader>
<bodyText confidence="0.999722">
We collect 3,500 Japanese sentences and 3,500
English sentences from the web. They come from
</bodyText>
<page confidence="0.99706">
916
</page>
<bodyText confidence="0.999915">
a wide range of domains, such as technical docu-
ments, web forum data, travel logs etc. They are
manually translated into the other language to pro-
duce 7,000 sentence pairs, which are split into two
parts: 2,000 pairs as development set (dev) and the
other 5,000 pairs as test set (web test).
Beside that, we collect another 999 English sen-
tences from newswire domain which are translated
into Japanese to form an out-of-domain test data set
(news test).
</bodyText>
<subsubsectionHeader confidence="0.853372">
5.1.2 Parallel Corpus
</subsubsectionHeader>
<bodyText confidence="0.99924625">
Our parallel corpus is crawled from the web,
containing news articles, technical documents, blog
entries etc. After removing duplicates, we have
about 18 million sentence pairs, which contain about
270 millions of English tokens and 320 millions of
Japanese tokens. We use Giza++ (Och and Ney,
2003) to generate the word alignment for the parallel
corpus.
</bodyText>
<subsubsectionHeader confidence="0.581728">
5.1.3 Monolingual Corpus
</subsubsectionHeader>
<bodyText confidence="0.999945666666667">
Our monolingual Corpus is also crawled from the
web. After removing duplicate sentences, we have a
corpus of over 10 billion tokens for both English and
Japanese. This monolingual corpus is used to train
a 4-gram language model for English and Japanese
respectively.
</bodyText>
<subsectionHeader confidence="0.994858">
5.2 Parsers
</subsectionHeader>
<bodyText confidence="0.999934083333333">
For English, we train a dependency parser as (Nivre
and Scholz, 2004) on WSJ portion of Penn Tree-
bank, which are converted to dependency trees us-
ing Stanford Parser (Marneffe et al., 2006). We con-
vert the tokens in training data to lower case, and
re-tokenize the sentences using the same tokenizer
from our MT system.
For Japanese parser, we use CABOCHA, a
chunk-based dependency parser (Kudo and Mat-
sumoto, 2002). Some heuristics are used to adapt
CABOCHA generated trees to our word segmenta-
tion.
</bodyText>
<subsectionHeader confidence="0.9364835">
5.3 Settings
5.3.1 Baseline System
</subsectionHeader>
<bodyText confidence="0.9998985">
We use a BTG phrase-based system with a Max-
Ent based lexicalized reordering model (Wu, 1997;
Xiong et al., 2006) as our baseline system for
both English-to-Japanese and Japanese-to-English
Experiment. The distortion model is trained on the
same parallel corpus as the phrase table using a
home implemented maximum entropy trainer.
In addition, a pre-reorder system using manual
rules as (Xu et al., 2009) is included for the English-
to-Japanese experiment (ManR-PR). Manual rules
are tuned by a bilingual speaker on the development
set.
</bodyText>
<subsectionHeader confidence="0.974149">
5.3.2 Ranking Reordering System
</subsectionHeader>
<bodyText confidence="0.999935285714286">
Ranking reordering model is learned from the
same parallel corpus as phrase table. For efficiency
reason, we only use 25% of the corpus to train our
reordering model. LIBLINEAR (Fan et al., 2008) is
used to do the SVM optimization for RankingSVM.
We test it on both pre-reorder setting (Rank-PR)
and integrated setting (Rank-IT).
</bodyText>
<sectionHeader confidence="0.371378" genericHeader="method">
5.4 End-to-End Result
</sectionHeader>
<table confidence="0.985679375">
system dev web test news test
Baseline 21.45 21.12 14.18
E-J ManR-PR 23.00 22.42 15.61
Rank-PR 22.92 22.51 15.90
Rank-IT 23.14 22.85 15.72
Baseline 25.39 24.20 14.26
J-E Rank-PR 26.57 25.56 15.42
Rank-IT 26.72 25.87 15.27
</table>
<tableCaption confidence="0.987774">
Table 2: BLEU(%) score on dev and test data for
</tableCaption>
<bodyText confidence="0.995993333333333">
both E-J and J-E experiment. All settings signifi-
cantly improve over the baseline at 95% confidence
level. Baseline is the BTG phrase system system;
ManR-PR is pre-reorder with manual rule; Rank-PR
is pre-reorder with ranking reorder model; Rank-IT
is system with integrated ranking reorder model.
From Table 2, we can see our ranking reordering
model significantly improves the performance for
both English-to-Japanese and Japanese-to-English
experiments over the BTG baseline system. It also
out-performs the manual rule set on English-to-
Japanese result, but the difference is not significant.
</bodyText>
<subsectionHeader confidence="0.998285">
5.5 Reordering Performance
</subsectionHeader>
<bodyText confidence="0.999904">
In order to show whether the improved performance
is really due to improved reordering, we would like
to measure the reorder performance directly.
</bodyText>
<page confidence="0.992586">
917
</page>
<bodyText confidence="0.9998847">
As we do not have access to a golden re-
ordered sentence set, we decide to use the align-
ment crossing-link numbers between aligned sen-
tence pairs as the measure for reorder performance.
We train the ranking model on 25% of our par-
allel corpus, and use the rest 75% as test data
(auto). We sample a small corpus (575 sentence
pairs) and do manual alignment (man-small). We
denote the automatic alignment for these 575 sen-
tences as (auto-small). From Table 3, we can see
</bodyText>
<table confidence="0.998040571428571">
setting auto auto-small man-small
None 36.3 35.9 40.1
Oracle 4.3 4.1 7.4
E-J ManR 13.4 13.6 16.7
Rank 12.1 12.8 17.2
J-E Oracle 6.9 7.0 9.4
Rank 15.7 15.3 20.5
</table>
<tableCaption confidence="0.999433">
Table 3: Reorder performance measured by
</tableCaption>
<bodyText confidence="0.989077684210527">
crossing-link number per sentence. None means the
original sentences without reordering; Oracle means
the best permutation allowed by the source parse
tree; ManR refers to manual reorder rules; Rank
means ranking reordering model.
our ranking reordering model indeed significantly
reduces the crossing-link numbers over the original
sentence pairs. On the other hand, the performance
of the ranking reorder model still fall far short of or-
acle, which is the lowest crossing-link number of all
possible permutations allowed by the parse tree. By
manual analysis, we find that the gap is due to both
errors of the ranking reorder model and errors from
word alignment and parser.
Another thing to note is that the crossing-link
number of manual alignment is higher than auto-
matic alignment. The reason is that our annotators
tend to align function words which might be left un-
aligned by automatic word aligner.
</bodyText>
<subsectionHeader confidence="0.999323">
5.6 Effect of Ranking Features
</subsectionHeader>
<bodyText confidence="0.960672166666667">
Here we examine the effect of features for ranking
reorder model. We compare their influence on Rank-
ingSVM accuracy, alignment crossing-link number,
end-to-end BLEU score, and the model size. As
Table 4 shows, a major part of reduction of CLN
comes from features such as Part-of-Speech tags,
</bodyText>
<table confidence="0.999552333333333">
Features Acc. CLN BLEU Feat.#
E-J tag+label 88.6 16.4 22.24 26k
+dst 91.5 13.5 22.66 55k
+pct 92.2 13.1 22.73 79k
+lexl00 92.9 12.1 22.85 347k
+lexl000 94.0 11.5 22.79 2,410k
+lexl000 95.2 10.7 22.81 3,794k
J-E tag+fw 85.0 18.6 25.43 31k
+dst 90.3 16.9 25.62 65k
+lex�00 91.6 15.7 25.87 293k
+lexl000 92.4 14.8 25.91 2,156k
+lexl000 93.0 14.3 25.84 3,297k
</table>
<tableCaption confidence="0.992668">
Table 4: Effect of ranking features. Acc. is Rank-
</tableCaption>
<bodyText confidence="0.983742631578947">
ingSVM accuracy in percentage on the training data;
CLN is the crossing-link number per sentence on
parallel corpus with automatically generated word
alignment; BLEU is the BLEU score in percentage
on web test set on Rank-IT setting (system with in-
tegrated rank reordering model); lex,, means n most
frequent lexicons in the training corpus.
dependency labels (for English), function words (for
Japanese), and the distance and punctuations be-
tween child and head. These features also corre-
spond to BLEU score improvement for End-to-End
evaluations. Lexicon features generally continue to
improve the RankingSVM accuracy and reduce CLN
on training data, but they do not bring further im-
provement for SMT systems beyond the top 100
most frequent words. Our explanation is that less
frequent lexicons tend to help local reordering only,
which is already handled by the underlying phrase-
based system.
</bodyText>
<subsectionHeader confidence="0.978264">
5.7 Performance on different domains
</subsectionHeader>
<bodyText confidence="0.999973818181818">
From Table 2 we can see that pre-reorder method has
higher BLEU score on news test, while integrated
model performs better on web test set which con-
tains informal texts. By error analysis, we find that
the parser commits more errors on informal texts,
and informal texts usually have more flexible trans-
lations. Pre-reorder method makes “hard” decision
before decoding, thus is more sensitive to parser er-
rors; on the other hand, integrated model is forced
to use a longer distortion limit which leads to more
search errors during decoding time. It is possible to
</bodyText>
<page confidence="0.994106">
918
</page>
<bodyText confidence="0.981688">
use system combination method to get the best of
both systems, but we leave this to future work.
</bodyText>
<sectionHeader confidence="0.993484" genericHeader="method">
6 Discussion on Related Work
</sectionHeader>
<bodyText confidence="0.999991107142857">
There have been several studies focusing on compil-
ing hand-crafted syntactic reorder rules. Collins et
al. (2005), Wang et al. (2007), Ramanathan et al.
(2008), Lee et al. (2010) have developed rules for
German-English, Chinese-English, English-Hindi
and English-Japanese respectively. Xu et al. (2009)
designed a clever precedence reordering rule set for
translation from English to several SOV languages.
The drawback for hand-crafted rules is that they de-
pend upon expert knowledge to produce and are lim-
ited to their targeted language pairs.
Automatically learning syntactic reordering rules
have also been explored in several work. Li et
al. (2007) and Visweswariah et al. (2010) learned
probability of reordering patterns from constituent
trees using either Maximum Entropy or maximum
likelihood estimation. Since reordering patterns
are matched against a tree node together with all
its direct children, data sparseness problem will
arise when tree nodes have many children (Li et
al., 2007); Visweswariah et al. (2010) also men-
tioned their method yielded no improvement when
applied to dependency trees in their initial experi-
ments. Genzel (2010) dealt with the data sparseness
problem by using window heuristic, and learned re-
ordering pattern sequence from dependency trees.
Even with the window heuristic, they were unable
to evaluate all candidates due to the huge num-
ber of possible patterns. Different from the pre-
vious approaches, we treat syntax-based reordering
as a ranking problem between different source tree
nodes. Our method does not require the source
nodes to match some specific patterns, but encodes
reordering knowledge in the form of a ranking func-
tion, which naturally handles reordering between
any number of tree nodes; the ranking function is
trained by well-established rank learning method to
minimize the number of mis-ordered tree nodes in
the training data.
Tree-to-string systems (Quirk et al., 2005; Liu et
al., 2006) model syntactic reordering using minimal
or composed translation rules, which may contain
reordering involving tree nodes from multiple tree
levels. Our method can be naturally extended to deal
with such multiple level reordering. For a tree-to-
string rule with multiple tree levels, instead of rank-
ing the direct children of the root node, we rank all
leaf nodes (Most are frontier nodes (Galley et al.,
2006)) in the translation rule. We need to redesign
our ranking feature templates to encode the reorder-
ing information in the source part of the translation
rules. We need to remember the source side con-
text of the rules, the model size would still be much
smaller than a full-fledged tree-to-string system be-
cause we do not need to explicitly store the target
variants for each rule.
</bodyText>
<sectionHeader confidence="0.997379" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999716090909091">
In this paper we present a ranking based reorder-
ing method to reorder source language to match the
word order of target language given the source side
parse tree. Reordering is formulated as a task to rank
different nodes in the source side syntax tree accord-
ing to their relative position in the target language.
The ranking model is automatically trained to min-
imize the mis-ordering of tree nodes in the training
data. Large scale experiment shows improvement on
both reordering metric and SMT performance, with
up to 1.73 point BLEU gain in our evaluation test.
In future work, we plan to extend the ranking
model to handle reordering between multiple lev-
els of source trees. We also expect to explore bet-
ter way to integrate ranking reorder model into SMT
system instead of a simple penalty scheme. Along
the research direction of preprocessing the source
language to facilitate translation, we consider to not
only change the order of the source language, but
also inject syntactic structure of the target language
into source language by adding pseudo words into
source sentences.
</bodyText>
<sectionHeader confidence="0.999732" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.91802">
Nan Yang and Nenghai Yu were partially supported
by Fundamental Research Funds for the Central
Universities (No. WK2100230002), National Nat-
ural Science Foundation of China (No. 60933013),
and National Science and Technology Major Project
(No. 2010ZX03004-003).
</reference>
<page confidence="0.999393">
919
</page>
<sectionHeader confidence="0.998338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999776074468085">
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
ACL, pages 263-270.
Michael Collins, Philipp Koehn and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large lin-
ear classification. In Journal of Machine Learning Re-
search.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training of
Context-Rich Syntactic Translation Models. In Proc.
ACL-Coling, pages 961-968.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model. In Proc. EMNLP, pages 263-270.
Dmitriy Genzel. 2010. Automatically Learning Source-
side Reordering Rules for Large Scale Machine Trans-
lation. In Proc. Coling, pages 376-384.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer
2000. Large Margin Rank Boundaries for Ordinal Re-
gression. In Advances in Large Margin Classifiers,
pages 115-132.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinborgh System Description
for the 2005IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Transla-
tion.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. HLT-
NAACL, pages 127-133.
Taku Kudo, Yuji Matsumoto. 2002. Japanese Depen-
dency Analysis using Cascaded Chunking. In Proc.
CoNLL, pages 63-69.
Young-Suk Lee, Bing Zhao and Xiaoqiang Luo. 2010.
Constituent reordering and syntax models for English-
to-Japanese statistical machine translation. In Proc.
Coling.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li and
Ming Zhou and Yi Guan 2007. A Probabilistic Ap-
proach to Syntax-based Reordering for Statistical Ma-
chine Translation. In Proc. ACL, pages 720-727.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-String Alignment Template for Statistical Machine
Translation. In Proc. ACL-Coling, pages 609-616.
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
LREC 2006
Joakim Nivre and Mario Scholz 2004. Deterministic De-
pendency Parsing for English Text. In Proc. Coling.
Franz J. Och. 2002. Statistical Machine Translation:
From Single Word Models to Alignment Template.
Ph.D.Thesis, RWTH Aachen, Germany
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1): pages 19-51.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proc. ACL, pages 271-279.
A. Ramanathan, Pushpak Bhattacharyya, Jayprasad
Hegde, Ritesh M. Shah and Sasikumar M. 2008.
Simple syntactic and morphological processing can
help English-Hindi Statistical Machine Translation.
In Proc. IJCNLP.
Roy Tromble. 2009. Search and Learning for the Lin-
ear Ordering Problem with an Application to Machine
Translation. Ph.D. Thesis.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan and Nandakishore Kamb-
hatla. 2010. Syntax Based Reordering with Automat-
ically Derived Rules for Improved Statistical Machine
Translation. In Proc. Coling, pages 1119-1127.
Chao Wang, Michael Collins, Philipp Koehn. 2007. Chi-
nese syntactic reordering for statistical machine trans-
lation. In Proc. EMNLP-CoNLL.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3): pages 377-403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for Sta-
tistical Machine Translation. In Proc. ACL-Coling,
pages 521-528.
Peng Xu, Jaeho Kang, Michael Ringgaard, Franz Och.
2009. Using a Dependency Parser to Improve SMT
for Subject-Object-Verb Languages. In Proc. HLT-
NAACL, pages 376-384.
Richard Zens and Hermann Ney. 2006. Discriminative
Reordering Models for Statistical Machine Transla-
tion. In Proc. Workshop on Statistical Machine Trans-
lation, HLT-NAACL, pages 127-133.
</reference>
<page confidence="0.99693">
920
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.070262">
<title confidence="0.97414">A Ranking-based Approach to Word Statistical Machine</title>
<author confidence="0.7198115">Key Lab of</author>
<affiliation confidence="0.959496">University of Science and Technology of</affiliation>
<email confidence="0.513216">v-nayang@microsoft.com,</email>
<note confidence="0.272148">Research</note>
<abstract confidence="0.995468333333333">Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrasebased SMT system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Nan Yang</author>
</authors>
<title>and Nenghai Yu were partially supported by Fundamental Research Funds for the Central Universities (No.</title>
<booktitle>WK2100230002), National Natural Science Foundation of China (No. 60933013), and National Science and Technology Major Project (No.</booktitle>
<pages>2010--03004</pages>
<marker>Yang, </marker>
<rawString>Nan Yang and Nenghai Yu were partially supported by Fundamental Research Funds for the Central Universities (No. WK2100230002), National Natural Science Foundation of China (No. 60933013), and National Science and Technology Major Project (No. 2010ZX03004-003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation. In</title>
<date>2005</date>
<booktitle>Proc. ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="2039" citStr="Chiang, 2005" startWordPosition="304" endWordPosition="305">l translation element and deal with local reordering, while a distance based distortion model is used to coarsely depict the exponentially decayed word movement probabilities in language translation. Further work in this direction employed lexi*This work has been done while the first author was visiting Microsoft Research Asia. calized distortion models, including both generative (Koehn et al., 2005) and discriminative (Zens and Ney, 2006; Xiong et al., 2006) variants, to achieve finer-grained estimations, while other work took into account the hierarchical language structures in translation (Chiang, 2005; Galley and Manning, 2008). Long-distance word reordering between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally viewed beyond the scope of the phrase-based systems discussed above, because of either distortion limits or lack of discriminative features for modeling. The most notable solution to this problem is adopting syntax-based SMT models, especially methods making use of source side syntactic parse trees. There are two major categories in this line of resea</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proc. ACL, pages 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="26149" citStr="Collins et al. (2005)" startWordPosition="4430" endWordPosition="4433">s, we find that the parser commits more errors on informal texts, and informal texts usually have more flexible translations. Pre-reorder method makes “hard” decision before decoding, thus is more sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to 918 use system combination method to get the best of both systems, but we leave this to future work. 6 Discussion on Related Work There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reord</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R-E Fan</author>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>X-R Wang</author>
<author>C-J Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>In Journal of Machine Learning Research.</journal>
<contexts>
<context position="20902" citStr="Fan et al., 2008" startWordPosition="3576" endWordPosition="3579">nglish-to-Japanese and Japanese-to-English Experiment. The distortion model is trained on the same parallel corpus as the phrase table using a home implemented maximum entropy trainer. In addition, a pre-reorder system using manual rules as (Xu et al., 2009) is included for the Englishto-Japanese experiment (ManR-PR). Manual rules are tuned by a bilingual speaker on the development set. 5.3.2 Ranking Reordering System Ranking reordering model is learned from the same parallel corpus as phrase table. For efficiency reason, we only use 25% of the corpus to train our reordering model. LIBLINEAR (Fan et al., 2008) is used to do the SVM optimization for RankingSVM. We test it on both pre-reorder setting (Rank-PR) and integrated setting (Rank-IT). 5.4 End-to-End Result system dev web test news test Baseline 21.45 21.12 14.18 E-J ManR-PR 23.00 22.42 15.61 Rank-PR 22.92 22.51 15.90 Rank-IT 23.14 22.85 15.72 Baseline 25.39 24.20 14.26 J-E Rank-PR 26.57 25.56 15.42 Rank-IT 26.72 25.87 15.27 Table 2: BLEU(%) score on dev and test data for both E-J and J-E experiment. All settings significantly improve over the baseline at 95% confidence level. Baseline is the BTG phrase system system; ManR-PR is pre-reorder w</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. 2008. LIBLINEAR: A library for large linear classification. In Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Translation Models. In</title>
<date>2006</date>
<booktitle>Proc. ACL-Coling,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="28402" citStr="Galley et al., 2006" startWordPosition="4778" endWordPosition="4781">des; the ranking function is trained by well-established rank learning method to minimize the number of mis-ordered tree nodes in the training data. Tree-to-string systems (Quirk et al., 2005; Liu et al., 2006) model syntactic reordering using minimal or composed translation rules, which may contain reordering involving tree nodes from multiple tree levels. Our method can be naturally extended to deal with such multiple level reordering. For a tree-tostring rule with multiple tree levels, instead of ranking the direct children of the root node, we rank all leaf nodes (Most are frontier nodes (Galley et al., 2006)) in the translation rule. We need to redesign our ranking feature templates to encode the reordering information in the source part of the translation rules. We need to remember the source side context of the rules, the model size would still be much smaller than a full-fledged tree-to-string system because we do not need to explicitly store the target variants for each rule. 7 Conclusion and Future Work In this paper we present a ranking based reordering method to reorder source language to match the word order of target language given the source side parse tree. Reordering is formulated as </context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proc. ACL-Coling, pages 961-968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A Simple and Effective Hierarchical Phrase Reordering Model.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="2066" citStr="Galley and Manning, 2008" startWordPosition="306" endWordPosition="309">element and deal with local reordering, while a distance based distortion model is used to coarsely depict the exponentially decayed word movement probabilities in language translation. Further work in this direction employed lexi*This work has been done while the first author was visiting Microsoft Research Asia. calized distortion models, including both generative (Koehn et al., 2005) and discriminative (Zens and Ney, 2006; Xiong et al., 2006) variants, to achieve finer-grained estimations, while other work took into account the hierarchical language structures in translation (Chiang, 2005; Galley and Manning, 2008). Long-distance word reordering between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally viewed beyond the scope of the phrase-based systems discussed above, because of either distortion limits or lack of discriminative features for modeling. The most notable solution to this problem is adopting syntax-based SMT models, especially methods making use of source side syntactic parse trees. There are two major categories in this line of research. One is tree-to-string </context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Model. In Proc. EMNLP, pages 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Automatically Learning Sourceside Reordering Rules for Large Scale Machine Translation. In</title>
<date>2010</date>
<booktitle>Proc. Coling,</booktitle>
<pages>376--384</pages>
<contexts>
<context position="4361" citStr="Genzel, 2010" startWordPosition="677" endWordPosition="678">king-based approach to word reordering. The ranking model is automatically derived from the word aligned parallel data, viewing the source tree nodes to be reordered as list items to be ranked. The ranks of tree nodes are determined by their relative positions in the target language – the node in the most front gets the highest rank, while the ending word in the target sentence gets the lowest rank. The ranking model is trained to directly minimize the mis-ordering of tree nodes, which differs from the prior work based on maximum likelihood estimations of reordering patterns (Li et al., 2007; Genzel, 2010), and does not require any special tweaking in model training. The ranking model can not only be used in a pre-reordering based SMT system, but also be integrated into a phrasebased decoder serving as additional distortion features. We evaluated our approach on large-scale Japanese-English and English-Japanese machine translation tasks, and experimental results show that our approach can bring significant improvements to the baseline phrase-based SMT system in both preordering and integrated decoding settings. In the rest of the paper, we will first formally present our ranking-based word reor</context>
<context position="10835" citStr="Genzel, 2010" startWordPosition="1820" endWordPosition="1821">ntences. (b) in Figure 2 shows the automatically generated alignment for the sentence pair fragment. The word “with” is incorrectly aligned to the 6th Japanese word “ha”; as a result, “with latter procedure” now has target span [1, 6], while “Problem” aligns to [5, 5]. Due to this overlapping, it becomes unclear which permutation of “Problem” and “with latter procedure” is a better match of the target phrase; we need a better metric to measure word order similarity between reordered source and target sentences. We choose to find the tree T0e, with minimal alignment crossing-link number (CLN) (Genzel, 2010) to f as our golden reordered tree.1 Each crossing1A simple solution is to exclude all trees with overlapping target spans from training. But in our experiment, this method Problem with latter procedure lies in ... lies in ... 914 link (i1j1, i2j2) is a pair of alignment links crossing each other. CLN reaches zero if f is monotonically aligned to e0, and increases as there are more word reordering between e0 and f. For example, in Figure 1, there are 6 crossing-links in the original tree: (e1j4, e2j3), (e1j4, e4j2), (e1j4, e5j1), (e2j3, e4j2), (e2j3, e5j1) and (e4j2, e5j1); thus CLN for the or</context>
<context position="27190" citStr="Genzel (2010)" startWordPosition="4588" endWordPosition="4589">irs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from constituent trees using either Maximum Entropy or maximum likelihood estimation. Since reordering patterns are matched against a tree node together with all its direct children, data sparseness problem will arise when tree nodes have many children (Li et al., 2007); Visweswariah et al. (2010) also mentioned their method yielded no improvement when applied to dependency trees in their initial experiments. Genzel (2010) dealt with the data sparseness problem by using window heuristic, and learned reordering pattern sequence from dependency trees. Even with the window heuristic, they were unable to evaluate all candidates due to the huge number of possible patterns. Different from the previous approaches, we treat syntax-based reordering as a ranking problem between different source tree nodes. Our method does not require the source nodes to match some specific patterns, but encodes reordering knowledge in the form of a ranking function, which naturally handles reordering between any number of tree nodes; the</context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>Dmitriy Genzel. 2010. Automatically Learning Sourceside Reordering Rules for Large Scale Machine Translation. In Proc. Coling, pages 376-384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Herbrich</author>
</authors>
<title>Thore Graepel, and Klaus Obermayer</title>
<date>2000</date>
<booktitle>In Advances in Large Margin Classifiers,</booktitle>
<pages>115--132</pages>
<marker>Herbrich, 2000</marker>
<rawString>Ralf Herbrich, Thore Graepel, and Klaus Obermayer 2000. Large Margin Rank Boundaries for Ordinal Regression. In Advances in Large Margin Classifiers, pages 115-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinborgh System Description for the 2005IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>In International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="1830" citStr="Koehn et al., 2005" startWordPosition="272" endWordPosition="275">een source and target sentences has been a research focus since the emerging of statistical machine translation. In phrase-based models (Och, 2002; Koehn et al., 2003), phrase is introduced to serve as the fundamental translation element and deal with local reordering, while a distance based distortion model is used to coarsely depict the exponentially decayed word movement probabilities in language translation. Further work in this direction employed lexi*This work has been done while the first author was visiting Microsoft Research Asia. calized distortion models, including both generative (Koehn et al., 2005) and discriminative (Zens and Ney, 2006; Xiong et al., 2006) variants, to achieve finer-grained estimations, while other work took into account the hierarchical language structures in translation (Chiang, 2005; Galley and Manning, 2008). Long-distance word reordering between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally viewed beyond the scope of the phrase-based systems discussed above, because of either distortion limits or lack of discriminative features for </context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne and David Talbot. 2005. Edinborgh System Description for the 2005IWSLT Speech Translation Evaluation. In International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proc. HLTNAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1378" citStr="Koehn et al., 2003" startWordPosition="203" endWordPosition="206">es in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrasebased SMT system. 1 Introduction Modeling word reordering between source and target sentences has been a research focus since the emerging of statistical machine translation. In phrase-based models (Och, 2002; Koehn et al., 2003), phrase is introduced to serve as the fundamental translation element and deal with local reordering, while a distance based distortion model is used to coarsely depict the exponentially decayed word movement probabilities in language translation. Further work in this direction employed lexi*This work has been done while the first author was visiting Microsoft Research Asia. calized distortion models, including both generative (Koehn et al., 2005) and discriminative (Zens and Ney, 2006; Xiong et al., 2006) variants, to achieve finer-grained estimations, while other work took into account the </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. HLTNAACL, pages 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese Dependency Analysis using Cascaded Chunking. In</title>
<date>2002</date>
<booktitle>Proc. CoNLL,</booktitle>
<pages>63--69</pages>
<contexts>
<context position="14199" citStr="Kudo and Matsumoto, 2002" startWordPosition="2391" endWordPosition="2395">ough for reordering task, and can be searched through efficiently using a CKY decoder. After finding the best reordered tree T0e,, we can extract one reorder example from every node with more than one child. 3.2 Features Features for the ranking model are extracted from source syntax trees. For English-to-Japanese task, we extract features from Stanford English Dependency Tree (Marneffe et al., 2006), including lexicons, Part-of-Speech tags, dependency labels, punctuations and tree distance between head and dependent. For Japanese-to-English task, we use a chunkbased Japanese dependency tree (Kudo and Matsumoto, 2002). Different from features for English, we do not use dependency labels because they are not available from the Japanese parser. Additionally, Japanese function words are also included as features because they are important grammatical clues. The detailed feature templates are shown in Table 1. 3.3 Learning Method There are many well studied methods available to learn the ranking function from extracted examples., ListNet (?) etc. We choose to use RankingSVM (Herbrich et al., 2000), a pair-wised ranking method, for its simplicity and good performance. For every reorder example t with children {</context>
<context position="20018" citStr="Kudo and Matsumoto, 2002" startWordPosition="3435" endWordPosition="3439">duplicate sentences, we have a corpus of over 10 billion tokens for both English and Japanese. This monolingual corpus is used to train a 4-gram language model for English and Japanese respectively. 5.2 Parsers For English, we train a dependency parser as (Nivre and Scholz, 2004) on WSJ portion of Penn Treebank, which are converted to dependency trees using Stanford Parser (Marneffe et al., 2006). We convert the tokens in training data to lower case, and re-tokenize the sentences using the same tokenizer from our MT system. For Japanese parser, we use CABOCHA, a chunk-based dependency parser (Kudo and Matsumoto, 2002). Some heuristics are used to adapt CABOCHA generated trees to our word segmentation. 5.3 Settings 5.3.1 Baseline System We use a BTG phrase-based system with a MaxEnt based lexicalized reordering model (Wu, 1997; Xiong et al., 2006) as our baseline system for both English-to-Japanese and Japanese-to-English Experiment. The distortion model is trained on the same parallel corpus as the phrase table using a home implemented maximum entropy trainer. In addition, a pre-reorder system using manual rules as (Xu et al., 2009) is included for the Englishto-Japanese experiment (ManR-PR). Manual rules </context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo, Yuji Matsumoto. 2002. Japanese Dependency Analysis using Cascaded Chunking. In Proc. CoNLL, pages 63-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
<author>Bing Zhao</author>
<author>Xiaoqiang Luo</author>
</authors>
<title>Constituent reordering and syntax models for Englishto-Japanese statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proc. Coling.</booktitle>
<contexts>
<context position="26214" citStr="Lee et al. (2010)" startWordPosition="4442" endWordPosition="4445"> informal texts usually have more flexible translations. Pre-reorder method makes “hard” decision before decoding, thus is more sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to 918 use system combination method to get the best of both systems, but we leave this to future work. 6 Discussion on Related Work There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from constituent trees using either Maximum Entrop</context>
</contexts>
<marker>Lee, Zhao, Luo, 2010</marker>
<rawString>Young-Suk Lee, Bing Zhao and Xiaoqiang Luo. 2010. Constituent reordering and syntax models for Englishto-Japanese statistical machine translation. In Proc. Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
<author>Minghui Li</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Yi Guan</author>
</authors>
<title>A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation. In</title>
<date>2007</date>
<booktitle>Proc. ACL,</booktitle>
<pages>720--727</pages>
<contexts>
<context position="4346" citStr="Li et al., 2007" startWordPosition="673" endWordPosition="676">but effective ranking-based approach to word reordering. The ranking model is automatically derived from the word aligned parallel data, viewing the source tree nodes to be reordered as list items to be ranked. The ranks of tree nodes are determined by their relative positions in the target language – the node in the most front gets the highest rank, while the ending word in the target sentence gets the lowest rank. The ranking model is trained to directly minimize the mis-ordering of tree nodes, which differs from the prior work based on maximum likelihood estimations of reordering patterns (Li et al., 2007; Genzel, 2010), and does not require any special tweaking in model training. The ranking model can not only be used in a pre-reordering based SMT system, but also be integrated into a phrasebased decoder serving as additional distortion features. We evaluated our approach on large-scale Japanese-English and English-Japanese machine translation tasks, and experimental results show that our approach can bring significant improvements to the baseline phrase-based SMT system in both preordering and integrated decoding settings. In the rest of the paper, we will first formally present our ranking-</context>
<context position="10071" citStr="Li et al., 2007" startWordPosition="1693" endWordPosition="1696">2: Fragment of a sentence pair. (a) shows gold alignment; (b) shows automatically generated alignment which contains errors. spans. Figure 2 shows a fragment of one sentence pair in our training data. Consider the subtree rooted at word “Problem”. With the gold alignment, “Problem” is aligned to the 5th target word, and “with latter procedure” are aligned to target span [1, 3], thus we can simply put “Problem” after “with latter procedure”. Recursively applying this process down the subtree, we get “latter procedure with Problem” which perfectly matches the target language. As pointed out by (Li et al., 2007), in practice, nodes often have overlapping target spans due to erroneous word alignment or different syntactic structures between source and target sentences. (b) in Figure 2 shows the automatically generated alignment for the sentence pair fragment. The word “with” is incorrectly aligned to the 6th Japanese word “ha”; as a result, “with latter procedure” now has target span [1, 6], while “Problem” aligns to [5, 5]. Due to this overlapping, it becomes unclear which permutation of “Problem” and “with latter procedure” is a better match of the target phrase; we need a better metric to measure w</context>
<context position="26689" citStr="Li et al. (2007)" startWordPosition="4511" endWordPosition="4514">ing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from constituent trees using either Maximum Entropy or maximum likelihood estimation. Since reordering patterns are matched against a tree node together with all its direct children, data sparseness problem will arise when tree nodes have many children (Li et al., 2007); Visweswariah et al. (2010) also mentioned their method yielded no improvement when applied to dependency trees in their initial experiments. Genzel (2010) dealt with the data sparseness problem by using window heuristic, and learned reordering pattern s</context>
</contexts>
<marker>Li, Li, Zhang, Li, Zhou, Guan, 2007</marker>
<rawString>Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li and Ming Zhou and Yi Guan 2007. A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation. In Proc. ACL, pages 720-727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-String Alignment Template for Statistical Machine Translation. In</title>
<date>2006</date>
<booktitle>Proc. ACL-Coling,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="2710" citStr="Liu et al., 2006" startWordPosition="402" endWordPosition="405">ring between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally viewed beyond the scope of the phrase-based systems discussed above, because of either distortion limits or lack of discriminative features for modeling. The most notable solution to this problem is adopting syntax-based SMT models, especially methods making use of source side syntactic parse trees. There are two major categories in this line of research. One is tree-to-string model (Quirk et al., 2005; Liu et al., 2006) which directly uses source parse trees to derive a large set of translation rules and associated model parameters. The other is called syntax pre-reordering – an approach that re-positions source words to approximate target language word order as much as possible based on the features from source syntactic parse trees. This is usually done in a preprocessing step, and then followed by a standard phrase-based SMT system that takes the re-ordered source sentence as input to finish the translation. In this paper, we continue this line of work and address the problem of word reordering based on s</context>
<context position="27992" citStr="Liu et al., 2006" startWordPosition="4712" endWordPosition="4715">luate all candidates due to the huge number of possible patterns. Different from the previous approaches, we treat syntax-based reordering as a ranking problem between different source tree nodes. Our method does not require the source nodes to match some specific patterns, but encodes reordering knowledge in the form of a ranking function, which naturally handles reordering between any number of tree nodes; the ranking function is trained by well-established rank learning method to minimize the number of mis-ordered tree nodes in the training data. Tree-to-string systems (Quirk et al., 2005; Liu et al., 2006) model syntactic reordering using minimal or composed translation rules, which may contain reordering involving tree nodes from multiple tree levels. Our method can be naturally extended to deal with such multiple level reordering. For a tree-tostring rule with multiple tree levels, instead of ranking the direct children of the root node, we rank all leaf nodes (Most are frontier nodes (Galley et al., 2006)) in the translation rule. We need to redesign our ranking feature templates to encode the reordering information in the source part of the translation rules. We need to remember the source </context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-String Alignment Template for Statistical Machine Translation. In Proc. ACL-Coling, pages 609-616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses. In</title>
<date>2006</date>
<booktitle>LREC</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In LREC 2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic Dependency Parsing for English Text. In</title>
<date>2004</date>
<booktitle>Proc. Coling.</booktitle>
<contexts>
<context position="19673" citStr="Nivre and Scholz, 2004" startWordPosition="3377" endWordPosition="3380">tc. After removing duplicates, we have about 18 million sentence pairs, which contain about 270 millions of English tokens and 320 millions of Japanese tokens. We use Giza++ (Och and Ney, 2003) to generate the word alignment for the parallel corpus. 5.1.3 Monolingual Corpus Our monolingual Corpus is also crawled from the web. After removing duplicate sentences, we have a corpus of over 10 billion tokens for both English and Japanese. This monolingual corpus is used to train a 4-gram language model for English and Japanese respectively. 5.2 Parsers For English, we train a dependency parser as (Nivre and Scholz, 2004) on WSJ portion of Penn Treebank, which are converted to dependency trees using Stanford Parser (Marneffe et al., 2006). We convert the tokens in training data to lower case, and re-tokenize the sentences using the same tokenizer from our MT system. For Japanese parser, we use CABOCHA, a chunk-based dependency parser (Kudo and Matsumoto, 2002). Some heuristics are used to adapt CABOCHA generated trees to our word segmentation. 5.3 Settings 5.3.1 Baseline System We use a BTG phrase-based system with a MaxEnt based lexicalized reordering model (Wu, 1997; Xiong et al., 2006) as our baseline syste</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz 2004. Deterministic Dependency Parsing for English Text. In Proc. Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Statistical Machine Translation: From Single Word Models to Alignment Template. Ph.D.Thesis,</title>
<date>2002</date>
<location>RWTH Aachen, Germany</location>
<contexts>
<context position="1357" citStr="Och, 2002" startWordPosition="201" endWordPosition="202">osition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrasebased SMT system. 1 Introduction Modeling word reordering between source and target sentences has been a research focus since the emerging of statistical machine translation. In phrase-based models (Och, 2002; Koehn et al., 2003), phrase is introduced to serve as the fundamental translation element and deal with local reordering, while a distance based distortion model is used to coarsely depict the exponentially decayed word movement probabilities in language translation. Further work in this direction employed lexi*This work has been done while the first author was visiting Microsoft Research Asia. calized distortion models, including both generative (Koehn et al., 2005) and discriminative (Zens and Ney, 2006; Xiong et al., 2006) variants, to achieve finer-grained estimations, while other work t</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>Franz J. Och. 2002. Statistical Machine Translation: From Single Word Models to Alignment Template. Ph.D.Thesis, RWTH Aachen, Germany</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>pages</pages>
<contexts>
<context position="19243" citStr="Och and Ney, 2003" startWordPosition="3308" endWordPosition="3311">sentence pairs, which are split into two parts: 2,000 pairs as development set (dev) and the other 5,000 pairs as test set (web test). Beside that, we collect another 999 English sentences from newswire domain which are translated into Japanese to form an out-of-domain test data set (news test). 5.1.2 Parallel Corpus Our parallel corpus is crawled from the web, containing news articles, technical documents, blog entries etc. After removing duplicates, we have about 18 million sentence pairs, which contain about 270 millions of English tokens and 320 millions of Japanese tokens. We use Giza++ (Och and Ney, 2003) to generate the word alignment for the parallel corpus. 5.1.3 Monolingual Corpus Our monolingual Corpus is also crawled from the web. After removing duplicate sentences, we have a corpus of over 10 billion tokens for both English and Japanese. This monolingual corpus is used to train a 4-gram language model for English and Japanese respectively. 5.2 Parsers For English, we train a dependency parser as (Nivre and Scholz, 2004) on WSJ portion of Penn Treebank, which are converted to dependency trees using Stanford Parser (Marneffe et al., 2006). We convert the tokens in training data to lower c</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1): pages 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency Treelet Translation: Syntactically Informed Phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="2691" citStr="Quirk et al., 2005" startWordPosition="398" endWordPosition="401">distance word reordering between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally viewed beyond the scope of the phrase-based systems discussed above, because of either distortion limits or lack of discriminative features for modeling. The most notable solution to this problem is adopting syntax-based SMT models, especially methods making use of source side syntactic parse trees. There are two major categories in this line of research. One is tree-to-string model (Quirk et al., 2005; Liu et al., 2006) which directly uses source parse trees to derive a large set of translation rules and associated model parameters. The other is called syntax pre-reordering – an approach that re-positions source words to approximate target language word order as much as possible based on the features from source syntactic parse trees. This is usually done in a preprocessing step, and then followed by a standard phrase-based SMT system that takes the re-ordered source sentence as input to finish the translation. In this paper, we continue this line of work and address the problem of word re</context>
<context position="27973" citStr="Quirk et al., 2005" startWordPosition="4708" endWordPosition="4711">y were unable to evaluate all candidates due to the huge number of possible patterns. Different from the previous approaches, we treat syntax-based reordering as a ranking problem between different source tree nodes. Our method does not require the source nodes to match some specific patterns, but encodes reordering knowledge in the form of a ranking function, which naturally handles reordering between any number of tree nodes; the ranking function is trained by well-established rank learning method to minimize the number of mis-ordered tree nodes in the training data. Tree-to-string systems (Quirk et al., 2005; Liu et al., 2006) model syntactic reordering using minimal or composed translation rules, which may contain reordering involving tree nodes from multiple tree levels. Our method can be naturally extended to deal with such multiple level reordering. For a tree-tostring rule with multiple tree levels, instead of ranking the direct children of the root node, we rank all leaf nodes (Most are frontier nodes (Galley et al., 2006)) in the translation rule. We need to redesign our ranking feature templates to encode the reordering information in the source part of the translation rules. We need to r</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically Informed Phrasal SMT. In Proc. ACL, pages 271-279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ramanathan</author>
<author>Pushpak Bhattacharyya</author>
<author>Jayprasad Hegde</author>
<author>Ritesh M Shah</author>
<author>M Sasikumar</author>
</authors>
<title>Simple syntactic and morphological processing can help English-Hindi Statistical Machine Translation. In</title>
<date>2008</date>
<booktitle>Proc. IJCNLP.</booktitle>
<contexts>
<context position="26195" citStr="Ramanathan et al. (2008)" startWordPosition="4438" endWordPosition="4441">ors on informal texts, and informal texts usually have more flexible translations. Pre-reorder method makes “hard” decision before decoding, thus is more sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to 918 use system combination method to get the best of both systems, but we leave this to future work. 6 Discussion on Related Work There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from constituent trees using ei</context>
</contexts>
<marker>Ramanathan, Bhattacharyya, Hegde, Shah, Sasikumar, 2008</marker>
<rawString>A. Ramanathan, Pushpak Bhattacharyya, Jayprasad Hegde, Ritesh M. Shah and Sasikumar M. 2008. Simple syntactic and morphological processing can help English-Hindi Statistical Machine Translation. In Proc. IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
</authors>
<title>Search and Learning for the Linear Ordering Problem with an Application to Machine Translation.</title>
<date>2009</date>
<tech>Ph.D. Thesis.</tech>
<contexts>
<context position="13505" citStr="Tromble, 2009" startWordPosition="2285" endWordPosition="2286">y training instances and led to degraded reordering performance. 2In our experiments, there are nodes with more than 10 children for English dependency trees. the reorder in the subtree of t. This observation enables us to divide the task of finding the reordered tree T0e, with minimal CLN into independently finding the children permutation of each node with minimal CCLN. Unfortunately, the time cost for the subtask is still O(n!) for a node with n children. Instead of enumerating through all permutations, we only search the Inversion Transduction Grammar neighborhood of the initial sequence (Tromble, 2009). As pointed out by (Tromble, 2009), the ITG neighborhood is large enough for reordering task, and can be searched through efficiently using a CKY decoder. After finding the best reordered tree T0e,, we can extract one reorder example from every node with more than one child. 3.2 Features Features for the ranking model are extracted from source syntax trees. For English-to-Japanese task, we extract features from Stanford English Dependency Tree (Marneffe et al., 2006), including lexicons, Part-of-Speech tags, dependency labels, punctuations and tree distance between head and dependent. For Jap</context>
</contexts>
<marker>Tromble, 2009</marker>
<rawString>Roy Tromble. 2009. Search and Learning for the Linear Ordering Problem with an Application to Machine Translation. Ph.D. Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Jiri Navratil</author>
<author>Jeffrey Sorensen</author>
<author>Vijil Chenthamarakshan</author>
<author>Nandakishore Kambhatla</author>
</authors>
<title>Syntax Based Reordering with Automatically Derived Rules for Improved Statistical Machine Translation. In</title>
<date>2010</date>
<booktitle>Proc. Coling,</booktitle>
<pages>1119--1127</pages>
<contexts>
<context position="6708" citStr="Visweswariah et al., 2010" startWordPosition="1074" endWordPosition="1077">air. By permuting tree nodes in the parse tree, the source sentence is reordered into the target language order. Constituent tree is shown above the source sentence; arrows below the source sentences show head-dependent arcs for dependency tree; word alignment links are lines without arrow between the source and target sentences. parse tree, we can obtain the same word order of Japanese translation. It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al., 2010). Following this principle, the word reordering task can be broken into sub-tasks, in which we only need to determine the order of children nodes for all non-leaf nodes in the source parse tree. For a tree node t with children {c1, c2,. . . , cnI, we rearrange the children to target-language-like order {cπ(i1), cπ(i2), ... , cπ(in)I. If we treat the reordered position 7r(i) of child ci as its “rank”, the reorderS VP Original Tree VP NP NP S VP VP S VP Reordered Tree VP S VP NP VP NP e0 e1 e2 e3 e4 e5 I music play to trying am 913 ing problem is naturally translated into a ranking problem: to r</context>
<context position="26720" citStr="Visweswariah et al. (2010)" startWordPosition="4516" endWordPosition="4519">-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from constituent trees using either Maximum Entropy or maximum likelihood estimation. Since reordering patterns are matched against a tree node together with all its direct children, data sparseness problem will arise when tree nodes have many children (Li et al., 2007); Visweswariah et al. (2010) also mentioned their method yielded no improvement when applied to dependency trees in their initial experiments. Genzel (2010) dealt with the data sparseness problem by using window heuristic, and learned reordering pattern sequence from dependency trees. </context>
</contexts>
<marker>Visweswariah, Navratil, Sorensen, Chenthamarakshan, Kambhatla, 2010</marker>
<rawString>Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan and Nandakishore Kambhatla. 2010. Syntax Based Reordering with Automatically Derived Rules for Improved Statistical Machine Translation. In Proc. Coling, pages 1119-1127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL.</booktitle>
<contexts>
<context position="26169" citStr="Wang et al. (2007)" startWordPosition="4434" endWordPosition="4437">ser commits more errors on informal texts, and informal texts usually have more flexible translations. Pre-reorder method makes “hard” decision before decoding, thus is more sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to 918 use system combination method to get the best of both systems, but we leave this to future work. 6 Discussion on Related Work There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from </context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proc. EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<pages>377--403</pages>
<contexts>
<context position="17087" citStr="Wu, 1997" startWordPosition="2940" endWordPosition="2941"> is applied to reorder source sentences during both training and decoding. Reordered sentences can go through the normal pipeline of a phrase-based decoder. The ranking reorder model can also be integrated into a phrase based decoder. Integrated method takes the original source sentence e as input, and ranking model generates a reordered e&apos; as a word order reference for the decoder. A simple penalty scheme is utilized to penalize decoder reordering violating ranking reorder model’s prediction e&apos;. In this paper, our underlying decoder is a CKY decoder following Bracketing Transduction Grammar (Wu, 1997; Xiong et al., 2006), thus we show how the penalty is implemented in the BTG decoder as an example. Similar penalty can be designed for other decoders without much effort. Under BTG, three rules are used to derive translations: one unary terminal rule, one straight rule and one inverse rule: A e/f A [A1, A2] A (A1, A2) We have three penalty triggers when any rules are applied during decoding: • Discontinuous penalty fde: it fires for all rules when source span of either A, A1 or A2 is mapped to discontinuous span in e&apos;. • Wrong straight rule penalty ft: it fires for straight rule when source </context>
<context position="20230" citStr="Wu, 1997" startWordPosition="3473" endWordPosition="3474">ain a dependency parser as (Nivre and Scholz, 2004) on WSJ portion of Penn Treebank, which are converted to dependency trees using Stanford Parser (Marneffe et al., 2006). We convert the tokens in training data to lower case, and re-tokenize the sentences using the same tokenizer from our MT system. For Japanese parser, we use CABOCHA, a chunk-based dependency parser (Kudo and Matsumoto, 2002). Some heuristics are used to adapt CABOCHA generated trees to our word segmentation. 5.3 Settings 5.3.1 Baseline System We use a BTG phrase-based system with a MaxEnt based lexicalized reordering model (Wu, 1997; Xiong et al., 2006) as our baseline system for both English-to-Japanese and Japanese-to-English Experiment. The distortion model is trained on the same parallel corpus as the phrase table using a home implemented maximum entropy trainer. In addition, a pre-reorder system using manual rules as (Xu et al., 2009) is included for the Englishto-Japanese experiment (ManR-PR). Manual rules are tuned by a bilingual speaker on the development set. 5.3.2 Ranking Reordering System Ranking reordering model is learned from the same parallel corpus as phrase table. For efficiency reason, we only use 25% o</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3): pages 377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In</title>
<date>2006</date>
<booktitle>Proc. ACL-Coling,</booktitle>
<pages>521--528</pages>
<contexts>
<context position="1890" citStr="Xiong et al., 2006" startWordPosition="282" endWordPosition="285">nce the emerging of statistical machine translation. In phrase-based models (Och, 2002; Koehn et al., 2003), phrase is introduced to serve as the fundamental translation element and deal with local reordering, while a distance based distortion model is used to coarsely depict the exponentially decayed word movement probabilities in language translation. Further work in this direction employed lexi*This work has been done while the first author was visiting Microsoft Research Asia. calized distortion models, including both generative (Koehn et al., 2005) and discriminative (Zens and Ney, 2006; Xiong et al., 2006) variants, to achieve finer-grained estimations, while other work took into account the hierarchical language structures in translation (Chiang, 2005; Galley and Manning, 2008). Long-distance word reordering between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally viewed beyond the scope of the phrase-based systems discussed above, because of either distortion limits or lack of discriminative features for modeling. The most notable solution to this problem is adopt</context>
<context position="17108" citStr="Xiong et al., 2006" startWordPosition="2942" endWordPosition="2945">d to reorder source sentences during both training and decoding. Reordered sentences can go through the normal pipeline of a phrase-based decoder. The ranking reorder model can also be integrated into a phrase based decoder. Integrated method takes the original source sentence e as input, and ranking model generates a reordered e&apos; as a word order reference for the decoder. A simple penalty scheme is utilized to penalize decoder reordering violating ranking reorder model’s prediction e&apos;. In this paper, our underlying decoder is a CKY decoder following Bracketing Transduction Grammar (Wu, 1997; Xiong et al., 2006), thus we show how the penalty is implemented in the BTG decoder as an example. Similar penalty can be designed for other decoders without much effort. Under BTG, three rules are used to derive translations: one unary terminal rule, one straight rule and one inverse rule: A e/f A [A1, A2] A (A1, A2) We have three penalty triggers when any rules are applied during decoding: • Discontinuous penalty fde: it fires for all rules when source span of either A, A1 or A2 is mapped to discontinuous span in e&apos;. • Wrong straight rule penalty ft: it fires for straight rule when source spans of A1 and A2 ar</context>
<context position="20251" citStr="Xiong et al., 2006" startWordPosition="3475" endWordPosition="3478">ndency parser as (Nivre and Scholz, 2004) on WSJ portion of Penn Treebank, which are converted to dependency trees using Stanford Parser (Marneffe et al., 2006). We convert the tokens in training data to lower case, and re-tokenize the sentences using the same tokenizer from our MT system. For Japanese parser, we use CABOCHA, a chunk-based dependency parser (Kudo and Matsumoto, 2002). Some heuristics are used to adapt CABOCHA generated trees to our word segmentation. 5.3 Settings 5.3.1 Baseline System We use a BTG phrase-based system with a MaxEnt based lexicalized reordering model (Wu, 1997; Xiong et al., 2006) as our baseline system for both English-to-Japanese and Japanese-to-English Experiment. The distortion model is trained on the same parallel corpus as the phrase table using a home implemented maximum entropy trainer. In addition, a pre-reorder system using manual rules as (Xu et al., 2009) is included for the Englishto-Japanese experiment (ManR-PR). Manual rules are tuned by a bilingual speaker on the development set. 5.3.2 Ranking Reordering System Ranking reordering model is learned from the same parallel corpus as phrase table. For efficiency reason, we only use 25% of the corpus to train</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In Proc. ACL-Coling, pages 521-528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz Och</author>
</authors>
<title>Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages. In</title>
<date>2009</date>
<booktitle>Proc. HLTNAACL,</booktitle>
<pages>376--384</pages>
<contexts>
<context position="6680" citStr="Xu et al., 2009" startWordPosition="1070" endWordPosition="1073">panese sentence pair. By permuting tree nodes in the parse tree, the source sentence is reordered into the target language order. Constituent tree is shown above the source sentence; arrows below the source sentences show head-dependent arcs for dependency tree; word alignment links are lines without arrow between the source and target sentences. parse tree, we can obtain the same word order of Japanese translation. It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al., 2010). Following this principle, the word reordering task can be broken into sub-tasks, in which we only need to determine the order of children nodes for all non-leaf nodes in the source parse tree. For a tree node t with children {c1, c2,. . . , cnI, we rearrange the children to target-language-like order {cπ(i1), cπ(i2), ... , cπ(in)I. If we treat the reordered position 7r(i) of child ci as its “rank”, the reorderS VP Original Tree VP NP NP S VP VP S VP Reordered Tree VP S VP NP VP NP e0 e1 e2 e3 e4 e5 I music play to trying am 913 ing problem is naturally translated </context>
<context position="20543" citStr="Xu et al., 2009" startWordPosition="3519" endWordPosition="3522">Japanese parser, we use CABOCHA, a chunk-based dependency parser (Kudo and Matsumoto, 2002). Some heuristics are used to adapt CABOCHA generated trees to our word segmentation. 5.3 Settings 5.3.1 Baseline System We use a BTG phrase-based system with a MaxEnt based lexicalized reordering model (Wu, 1997; Xiong et al., 2006) as our baseline system for both English-to-Japanese and Japanese-to-English Experiment. The distortion model is trained on the same parallel corpus as the phrase table using a home implemented maximum entropy trainer. In addition, a pre-reorder system using manual rules as (Xu et al., 2009) is included for the Englishto-Japanese experiment (ManR-PR). Manual rules are tuned by a bilingual speaker on the development set. 5.3.2 Ranking Reordering System Ranking reordering model is learned from the same parallel corpus as phrase table. For efficiency reason, we only use 25% of the corpus to train our reordering model. LIBLINEAR (Fan et al., 2008) is used to do the SVM optimization for RankingSVM. We test it on both pre-reorder setting (Rank-PR) and integrated setting (Rank-IT). 5.4 End-to-End Result system dev web test news test Baseline 21.45 21.12 14.18 E-J ManR-PR 23.00 22.42 15.</context>
<context position="26338" citStr="Xu et al. (2009)" startWordPosition="4456" endWordPosition="4459">re sensitive to parser errors; on the other hand, integrated model is forced to use a longer distortion limit which leads to more search errors during decoding time. It is possible to 918 use system combination method to get the best of both systems, but we leave this to future work. 6 Discussion on Related Work There have been several studies focusing on compiling hand-crafted syntactic reorder rules. Collins et al. (2005), Wang et al. (2007), Ramanathan et al. (2008), Lee et al. (2010) have developed rules for German-English, Chinese-English, English-Hindi and English-Japanese respectively. Xu et al. (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. The drawback for hand-crafted rules is that they depend upon expert knowledge to produce and are limited to their targeted language pairs. Automatically learning syntactic reordering rules have also been explored in several work. Li et al. (2007) and Visweswariah et al. (2010) learned probability of reordering patterns from constituent trees using either Maximum Entropy or maximum likelihood estimation. Since reordering patterns are matched against a tree node together with all its direct c</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, Franz Och. 2009. Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages. In Proc. HLTNAACL, pages 376-384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Reordering Models for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. Workshop on Statistical Machine Translation, HLT-NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1869" citStr="Zens and Ney, 2006" startWordPosition="278" endWordPosition="281"> a research focus since the emerging of statistical machine translation. In phrase-based models (Och, 2002; Koehn et al., 2003), phrase is introduced to serve as the fundamental translation element and deal with local reordering, while a distance based distortion model is used to coarsely depict the exponentially decayed word movement probabilities in language translation. Further work in this direction employed lexi*This work has been done while the first author was visiting Microsoft Research Asia. calized distortion models, including both generative (Koehn et al., 2005) and discriminative (Zens and Ney, 2006; Xiong et al., 2006) variants, to achieve finer-grained estimations, while other work took into account the hierarchical language structures in translation (Chiang, 2005; Galley and Manning, 2008). Long-distance word reordering between language pairs with substantial word order difference, such as Japanese with Subject-Object-Verb (SOV) structure and English with Subject-Verb-Object (SVO) structure, is generally viewed beyond the scope of the phrase-based systems discussed above, because of either distortion limits or lack of discriminative features for modeling. The most notable solution to </context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. Discriminative Reordering Models for Statistical Machine Translation. In Proc. Workshop on Statistical Machine Translation, HLT-NAACL, pages 127-133.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>