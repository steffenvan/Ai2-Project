<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.939746">
Compositional Matrix-Space Models for Sentiment Analysis
</title>
<author confidence="0.985509">
Ainur Yessenalina
</author>
<affiliation confidence="0.996455">
Dept. of Computer Science
Cornell University
</affiliation>
<address confidence="0.910708">
Ithaca, NY, 14853
</address>
<email confidence="0.999303">
ainur@cs.cornell.edu
</email>
<author confidence="0.983268">
Claire Cardie
</author>
<affiliation confidence="0.9963335">
Dept. of Computer Science
Cornell University
</affiliation>
<address confidence="0.910745">
Ithaca, NY, 14853
</address>
<email confidence="0.999484">
cardie@cs.cornell.edu
</email>
<sectionHeader confidence="0.99667" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999235068965517">
We present a general learning-based approach
for phrase-level sentiment analysis that adopts
an ordinal sentiment scale and is explicitly
compositional in nature. Thus, we can model
the compositional effects required for accu-
rate assignment of phrase-level sentiment. For
example, combining an adverb (e.g., “very”)
with a positive polar adjective (e.g., “good”)
produces a phrase (“very good”) with in-
creased polarity over the adjective alone. In-
spired by recent work on distributional ap-
proaches to compositionality, we model each
word as a matrix and combine words us-
ing iterated matrix multiplication, which al-
lows for the modeling of both additive and
multiplicative semantic effects. Although the
multiplication-based matrix-space framework
has been shown to be a theoretically ele-
gant way to model composition (Rudolph and
Giesbrecht, 2010), training such models has
to be done carefully: the optimization is non-
convex and requires a good initial starting
point. This paper presents the first such al-
gorithm for learning a matrix-space model for
semantic composition. In the context of the
phrase-level sentiment analysis task, our ex-
perimental results show statistically signifi-
cant improvements in performance over a bag-
of-words model.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950857142857">
Sentiment analysis has been an active research area
in recent years. Work in the area ranges from iden-
tifying the sentiment of individual words to deter-
mining the sentiment of phrases, sentences and doc-
uments (see Pang and Lee (2008) for a survey). The
bulk of previous research, however, models just pos-
itive vs. negative sentiment, collapsing positive (or
negative) words, phrases and documents of differ-
ing intensities into just one positive (or negative)
class. For word-level sentiment, therefore, these
methods would not recognize a difference in senti-
ment between words like “good” and “great”, which
have the same direction of polarity (i.e., positive)
but different intensities. At the phrase level, the
methods will fail to register compositional effects in
sentiment brought about by intensifiers like “very”,
“absolutely”, “extremely”, etc. “Happy” and “very
happy”, for example, will both be considered sim-
ply “positive” in sentiment. In real-world settings,
on the other hand, sentiment values extend across a
polarity spectrum — from very negative, to neutral,
to very positive. Recent research has shown, in par-
ticular, that modeling intensity at the phrase level is
important for real-world natural language process-
ing tasks including question answering and textual
entailment (de Marneffe et al., 2010).
This paper describes a general approach for
phrase-level sentiment analysis that takes these real-
world requirements into account: we adopt a five-
level ordinal sentiment scale and present a learning-
based method that assigns ordinal sentiment scores
to phrases.
Importantly, our approach will also be explicitly
compositional1 in nature so that it can accurately ac-
count for critical interactions among the words in
</bodyText>
<footnote confidence="0.933686">
1The Principle of Compositionality asserts that the meaning
of a complex expression is a function of the meanings of its
constituent expressions and the rules used to combine them.
</footnote>
<page confidence="0.896585">
172
</page>
<note confidence="0.9581865">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 172–182,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999345">
each sentiment-bearing phrase. Consider, for exam-
ple, combining an adverb like “very” with a polar
adjective like “good”. “Good” has an a priori posi-
tive sentiment, so “very good” should be considered
more positive even though “very”, on its own, does
not bear sentiment. Combining “very” with a nega-
tive adjective, like “bad”, produces a phrase (“very
bad”) that should be characterized as more negative
than the original adjective. Thus, it is convenient
to think of the effect of combining an intensifying
adverb with a polar adjective as being multiplica-
tive in nature, if we assume the adjectives (“good”
and “bad”) to have positive and a negative sentiment
scores, respectively.
Next, let us consider adverbial negators like “not”
combined with polar adjectives. When model-
ing only positive and negative labels for sentiment,
negators are generally treated as flipping the polar-
ity of the adjective it modifies (Choi and Cardie,
2008; Nakagawa et al., 2010). However, recent work
(Taboada et al., 2011; Liu and Seneff, 2009) sug-
gests that the effect of the negator when ordinal sen-
timent scores are employed is more akin to damp-
ening the adjective’s polarity rather than flipping it.
For example, if “perfect” has a strong positive sen-
timent, then the phrase “not perfect” is still positive,
though to a lesser degree. And while “not terrible” is
still negative, it is less negative than “terrible”. For
these cases, it is convenient to view “not” as shift-
ing polarity to the opposite side of polarity scale by
some value.
There are, of course, more interesting examples of
compositional semantic effects on sentiment: e.g.,
prevent cancer, ease the burden. Here, the verbs
prevent and ease act as content-word negators (Choi
and Cardie, 2008) in that they modify the negative
sentiment of their direct object arguments so that the
phrase as a whole is perceived as somewhat positive.
Nonetheless, the vast majority of methods for
phrase- and sentence-level sentiment analysis do not
tackle the task compositionally: they, instead, em-
ploy a bag-of-words representation and, at best, in-
corporate additional features to account for nega-
tors, intensifiers, and for contextual valence shifters,
which can change the sentiment over neighboring
words (e.g., Polanyi and Zaenen (2004), Wilson et
al. (2005) , Kennedy and Inkpen (2006), Shaikh et
al. (2007)).
One notable exception is Moilanen and Pulman
(2007), who propose a compositional semantic ap-
proach to assign a positive or negative sentiment to
newspaper article titles. However, their knowledge-
based approach presupposes the existence of a sen-
timent lexicon and a set of symbolic compositional
rules.
But learning-based compositional approaches
for sentiment analyis also exist. Choi and
Cardie (2008), for example, propose an algo-
rithm for phrase-based sentiment analysis that learns
proper assignments of intermediate sentiment anal-
ysis decision variables given the a priori (i.e., out
of context) polarity of the words in the phrase and
the (correct) phrase-level polarity. As in Moilianen
and Pulman (2007), semantic inference is based on
(a small set of) hand-written compositional rules. In
contrast, Nakagawa et. al (2010) use a dependency
parse tree to guide the learning of compositional ef-
fects. Each of the above, however, uses a binary
rather than an ordinal sentiment scale.
In contrast, our proposed method for phrase-
level sentiment analysis is inspired by recent work
on distributional approaches to compositionality.
In particular, Baroni and Zamparelli (2010) tackle
adjective-noun compositions using a vector repre-
sentation for nouns and learning a matrix represen-
tation for each adjective. The adjective matrices are
then applied as functions over the meanings of nouns
— via matrix-vector multiplication — to derive the
meaning of adjective-noun combinations. Rudolph
and Giesbrecht (2010) show theoretically, that mul-
tiplicative matrix-space models are a general case
of vector-space models and furthermore exhibit de-
sirable properties for semantic analysis: they take
into account word order and are algebraically, neuro-
logically and psychologically plausible. This work,
however, does not present an algorithm for learning
such models; nor does it provide empirical evidence
in favor of matrix-space models over vector-space
models.
In the sections below, we propose a learning-
based approach to assign ordinal sentiment scores to
sentiment-bearing phrases using a general composi-
tional matrix-space model of language. In contrast
to previous work, all words are modeled as matri-
ces, independent of their part-of-speech, and com-
positional inference is uniformly modeled as ma-
</bodyText>
<page confidence="0.998284">
173
</page>
<bodyText confidence="0.999814733333333">
trix multiplication. To predict an ordinal scale sen-
timent value, we employ Ordered Logistic Regres-
sion, introducing a novel training algorithm to ac-
commodate our compositional matrix-space repre-
sentations (Section 2). To our knowledge, this is the
first such algorithm for learning matrix-space mod-
els for semantic composition. We evaluate the ap-
proach on a standard sentiment corpus (Wiebe et al.,
2005) (Section 3), making use of its manually anno-
tated phrase-level annotations for polarity and inten-
sity, and compare our approach to the more com-
monly employed bag-of-words model. We show
(Section 4) that our matrix-space model significantly
outperforms a bag-of-words model for the ordinal
scale sentiment prediction task.
</bodyText>
<sectionHeader confidence="0.9467915" genericHeader="method">
2 The Model for Ordinal Scale Sentiment
Prediction
</sectionHeader>
<bodyText confidence="0.999965647058824">
As described above, our task is to predict an ordi-
nal scale sentiment value for a phrase. To this end,
we employ a sentiment scale with five ordinal val-
ues: VERY NEGATIVE, NEGATIVE, NEUTRAL, POS-
ITIVE and VERY POSITIVE. Given a set of phrase-
level training examples with their gold-standard or-
dinal sentiment value, we then use an Ordered Lo-
gistic Regression (OLogReg) model for prediction.
Unfortunately, our matrix-space representation pre-
cludes doing this directly.
We have chosen OLogReg, as opposed to say
PRanking (Crammer and Singer, 2001), because op-
timization of the former is more attractive: the ob-
jective (likelihood) is smooth and the gradients are
continuous. As will become clear shortly, learn-
ing our models is not trivial and it is important to
use sophisticated off-the-shelf optimizers such as L-
BFGS.
For a bag-of-words model, OLogReg learns one
weight for each word and a set of thresholds by max-
imizing the likelihood of the training data. Typically,
this is accomplished by using an optimizer like L-
BFGS whose interface needs the value and gradient
of the likelihood with respect to the parameters at
their current values. In the next subsections, we in-
stantiate OLogReg for our sentiment prediction task
using a matrix-space word model (2.1 and 2.2) and
a bag-of-words model (2.3). The learning formula-
tion of bag-of-words OLogReg is convex therefore
we will get the global optimum; in contrast, the op-
timization problem for matrix-space model is non-
convex, it is important to initialize the model well.
Initialization of the matrix-space model is discussed
in Section 2.4.
</bodyText>
<subsectionHeader confidence="0.971885">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.982116066666667">
In the subsequent subsections we will use the
following notation. Let n be the number of phrases
in the training set and let d be the number of words
in the dictionary. Let xi be the i-th phrase and yi
would be the label of xi, where yi takes r different
values yi ∈ {0, ... , r − 1}. Then |xi |will denote
the length of the phrase xi, and the words in i-th
phrase are: xi = xi1, xi2, ... , xi|xi|; xij, 1 ≤ j ≤ |xi|
is the j-th word of i-th phrase; where xij is from the
dictionary: 1 ≤ xij ≤ d.
In the case of the bag-of-words model, Φ(xi) ∈
Rd is the representation of the i-th phrase. Φj(xi)
counts the number of times the j-th word from the
dictionary appears in the i-th phrase. Given a w ∈
Rd it assigns a score ξi to a phrase xi by
</bodyText>
<equation confidence="0.989614">
ξi = wT Φ(xi) =
</equation>
<bodyText confidence="0.995172333333333">
In the case of the matrix-space model the Φ(xi) ∈
R|xi|×d is the representation of the i-th phrase.
Φjk(xi) is 1, if xij is the k-th word in the dictionary,
and zero otherwise. Given u,v ∈ Rm and a set of
matrices {Wp ∈ Rm×m}dp=1, one for each word, it
assigns a score ξi to a phrase xi by
</bodyText>
<equation confidence="0.944181708333333">
⎛ ⎞
�ξi = T d
WkΦjk(xi) v
|xi|
u
j=1k=1
⎞
Wxi ⎠ v (2)
j
=
in exactly
i
must be taken in choosing way to map matrix to a re
2Care
al
wxi (1)
j
� |xi|
j=1
=
⎛
uT ⎝
� |xi|
j=1
</equation>
<bodyText confidence="0.927265833333333">
where H�x
i
j=1
xi
xi
xi···
xi
|xi|
this order. We choose to map matrices to the real
numbers by using vectors u and v from Rm×1; so
that ξ = uTMv, where M ∈ Rm×m, which is sen-
sitive to the order of matrices2 , i.e. uTM1M2v =6
</bodyText>
<page confidence="0.918018">
174
</page>
<bodyText confidence="0.9933148">
uT M2M1v.
Modeling composition. A m×m matrix, represent-
ing a word, can be considered as a linear function,
mapping from Rm to Rm. Composition of words is
modeled by function composition, in our case com-
position of linear functions, i.e. matrix multipli-
cation. Note, that unlike bag-of-words model, the
matrix-space model takes word order into account,
since matrix multiplication is not commutative op-
eration.
</bodyText>
<subsectionHeader confidence="0.998892">
2.2 Ordered Logistic Regression
</subsectionHeader>
<bodyText confidence="0.999942375">
Now we will describe our objective function for
OLogReg and its derivatives. OLogReg has r −
1 thresholds (K0, ... Kr−2), so introducing K−1 =
−∞ and Kr−1 = ∞ leads to the unified expression
for posterior probabilities for all values of k:
(The constraints are similar to the ones in PRank al-
gorithm). For ease of optimization we parametrize
our model via K0, and τj, 1 ≤ j ≤ r − 2:
</bodyText>
<equation confidence="0.999380555555556">
K−1 = −∞,
K0,
K1 = K0 + τ1,
2
K2 = K0 + Pj=1 τj,
. . . ,
r−2
Kr−2 = K0 + Pj=1 τj
Kr−1 = ∞,
</equation>
<bodyText confidence="0.9381875">
where τ1, . . ., τr−2 are non-negative values, that rep-
resent how far the corresponding thresholds are from
each other. Then the constraints (3) would be:
τj ≥ 0, 1 ≤ j ≤ r − 2 (4)
To simplify the equations we can rewrite the nega-
tive loglikelihood as follows:
</bodyText>
<equation confidence="0.994122875">
P(yi = k|x) = P(Kk−1 &lt; Si ≤ Kk) L = − Xn r−1X ln(Aik − Bik)I(yi = k) (5)
= F(Kk − Si) − F(Kk−1 − Si) i=1 k=0
F(x) is an inverse-logit function
ex
F(x) =
1 + ex
this is its derivative:
= F(x)(1 − F(x))
</equation>
<bodyText confidence="0.999688333333333">
Therefore the negative loglikelihood of the training
data will look like the following (Hardin and Hilbe,
2007):
</bodyText>
<equation confidence="0.737437">
ln(F(Kk − Si) − F(Kk−1 − Si))I(yi = k)
</equation>
<bodyText confidence="0.792126">
where
</bodyText>
<equation confidence="0.999135714285714">
F(K0 + Pkj=1 τj − Si), if k = 0,...,r − 2
_
Aik 11, if k = r − 1
(
0, if k = 0
Bik = F(K0 + Pk−1
j=1 τj − Si), if k = 1, ... , r − 1
</equation>
<bodyText confidence="0.998893666666667">
Let’s introduce Lik = − ln(Aik − Bik)I(yi = k)
and then the derivative of Lik with respect to K0 will
be:
</bodyText>
<equation confidence="0.991037692307692">
−[Aik(1 − Aik) − Bik(1 − Bik)]
= I(yi = k)
aK0 Aik − Bik
= (Aik + Bik − 1)I(yi = k)
For j = yi:
dF(x)
dx
Xn
i=1
L = −
r−1X
k=0
aLik
</equation>
<bodyText confidence="0.9999126">
where r is the number of ordinal classes, Si is the
score of i-th phrase, I is the indicator function that
is equal to 1 – when yi = k, and zero otherwise. We
need to minimize the objective L with respect to the
following constraints:
</bodyText>
<equation confidence="0.9880367">
aLik
aτj
For all j &lt; yi:
aLik
aτj
−Aik(1 − Aik)
= I(yi = k)
= (Aik + Bik − 1)I(yi = k)
Aik − Bik
Kk−1 ≤ Kk, 1 ≤ k ≤ r − 2 (3)
</equation>
<bodyText confidence="0.9993102">
number. For example, one other way to map matrices to the
real numbers is to use the determinant of a matrix; however, the
determinant is not sensitive to the word order: det(M1M2) =
det(M1)det(M2) = det(M2M1); which is not desirable for a
model that needs to account for word order.
</bodyText>
<figure confidence="0.745777166666667">
For all j &gt; yi: ∂Lik = 0.
∂τj
The derivative with respect to the score Si is:
aLik
aSi
= (−Aik − Bik + 1)I(yi = k) (6)
</figure>
<page confidence="0.968814">
175
</page>
<subsectionHeader confidence="0.766901">
2.2.1 Matrix-Space Word Model
</subsectionHeader>
<bodyText confidence="0.999388666666667">
Here we show the derivatives with respect to a
word. For the OLogReg model with matrix-space
word representations, we have:
</bodyText>
<equation confidence="0.896257333333333">
∂ξi
∂Wxi
j
</equation>
<bodyText confidence="0.6377965">
The expression for ∂L
∂ξi is given in (6); we will derive
</bodyText>
<equation confidence="0.99311975">
∂ξi
∂Wxi
j
model each word is represented as an m x m affine
matrix W:
� A b �
W = (7)
0 1
</equation>
<bodyText confidence="0.998008105263158">
We choose the class of affine matrices since for
affine matrices matrix multiplication represents both
operations: linear transformation and translation.
Linear transformation is important for modeling
changes in sentiment - translation is also useful (we
make use of a translation vector during initialization,
see Section 2.4). In this work we consider m &gt; 3
since we want the matrix A from (7) to represent
rotation and scaling. Applying the affine transfor-
mation W to vector [x, 1]T is equivalent to applying
linear transformation A and translation b to x. 3
Though vectors u and v can be learned together
with word matrices Wj, we choose to fix u and v.
The main intuition behind fixing u and v is to re-
duce the degrees of freedom of the model: differ-
ent assignments of u, v and Wj-s can lead to the
same score ξ, i.e. there exist uˆ vˆ and ˆWj-s dif-
ferent from u, v and Wj-s respectively, such that
Wˆ). 4
</bodyText>
<equation confidence="0.98250275">
3
„ Ab « „ x « „ Ax + b «
=
0 1 1 1
</equation>
<bodyText confidence="0.580641166666667">
where A is a linear transformation, b is a translation vector.
Also the product of affine matrices is an affine matrix.
4The specific choice of u and v leads to an equivalent model
for all uˆ and vˆ such that uˆ = MTu, vˆ = M−1v, where M is
any invertible transformation (i.e. ˆu, vˆ are derived from u,v by
applying linear transformations MT, M−1 respectively):
</bodyText>
<equation confidence="0.901351">
uTW1W2v = (uTM)(M−1W1M)(M−1W2M)(M−1v)
</equation>
<bodyText confidence="0.999896">
The derivative of the phrase ξi with respect to j-th
word Wj would be (for brevity we drop the phrase
index and Wj refers to Wxij and p refers to |xi|):
</bodyText>
<equation confidence="0.949374">
�∂uT W1W2 ... Wpv�
∂Wj
= [(uT W1 ... Wj−1)T (Wj+1 ... Wpv
= [(WTj−1 ... W1 T )(uvT)(Wp T ... WTj+1)]
</equation>
<bodyText confidence="0.995841833333333">
(see Peterson and Pederson(2008)).
In case if a certain word appears multiple times in
the phrase, the derivative with respect to that word
would be a sum of derivatives with respect to each
appearance of a word, while all other appearances
are fixed. For example,
</bodyText>
<equation confidence="0.983094">
∂uTWW1Wv
( �
∂W
</equation>
<bodyText confidence="0.980605875">
where W is a representation of a word that is re-
peated.
So given the expression (6) for ∂L
∂ξi , the derivative
with respect to each word can be computed. Notice
that the update for the j-th word in a sentence de-
pends on the order words, which is in line with our
desire to account for word order.
</bodyText>
<subsectionHeader confidence="0.413902">
2.2.2 Optimization
</subsectionHeader>
<bodyText confidence="0.998868833333333">
The goal of training procedure is for the i-th
phrase with p words x1x2 ... xp to learn word ma-
trices W1, W2, ... , Wp such that resulting ξi-s will
lead to the lowest negative loglikelihood. The goal
of training procedure is to find word matrices W1,
W2, ... Wp and thresholds κ0, τ1, ... τr−2 such
that the negative loglikelihood is minimized. So,
given the negative loglikelihood and the derivatives
with respect κ0 and τj-s and word matrices W, we
optimize objective (5) subject to τj &gt; 0. We use L-
BFGS-B (Large-scale Bound-constrained Optimiza-
tion) by Byrd et al. (1995) as an optimizer.
</bodyText>
<subsectionHeader confidence="0.531701">
2.2.3 Regularization in Matrix-Space Model
</subsectionHeader>
<bodyText confidence="0.99866325">
In order to make sure that the L-BFGS-B updates
do not cause numerical issues we perform the fol-
lowing regularization to the resulting matrices. An
m by m matrix Wj that can be represented as:
</bodyText>
<equation confidence="0.987170846153846">
∂L
∂L �
∂ξi
∂Wxi
j
from (2). In the case of the Matrix-Space word
ξ(u, v, W) would be equal to ξ(ˆu, ˆv,
∂ξi
∂Wj
)T]
= u(W1Wv)T + (uTWW1)TvT
= ˆu T ˆW1 ˆW2ˆv C A11 a12 )
Wj = aT21 a22
</equation>
<page confidence="0.985737">
176
</page>
<bodyText confidence="0.998369333333333">
where A11 E Rm−1xm−1, a12,a21 E Rm−1x1,
a22 E R. First make the matrix affine by updating
the last row, then the updated matrix will look like:
</bodyText>
<equation confidence="0.9978685">
( =
Wj
A11 a12
0 1
</equation>
<bodyText confidence="0.98007905">
It can be proven that such a projection returns the
closest affine matrix in Frobenius norm.
However, we also want to regularize the model to
avoid ill-conditioned matrices. Ill-conditioned ma-
trices represent transformations whose output is very
sensitive to small changes in the input and therefore
they have a similar effect to having large weights
in a bag-of-words model. To perform such a reg-
ularization we ”shrink” the singular values of A11
towards one. More specifically, we first use the
Singular Value Decomposition (SVD) of the A11:
UEVT = A11, where U and V are orthogonal ma-
trices, E is a matrix with singular values on the diag-
onal. Then we update singular values in the follow-
ing way to get ˜E: ˜Eii = Ehii, where h is a parameter
between 0 and 1. If h = 1 then Eii remains the
same. In the extreme case h = 0 then Ehii = 1. For
intermediate values of h the singular values of A11
would be brought closer to one. Finally, we recom-
˜A11: ˜A11 = U˜EVT . So, Wj would be :
</bodyText>
<equation confidence="0.9493725">
( ˜A11 a12)
˜Wj = 0 1
</equation>
<subsubsectionHeader confidence="0.783469">
2.2.4 Learning in the Matrix-Space Model
</subsubsectionHeader>
<bodyText confidence="0.999971555555556">
We use Algorithm 1 to learn the matrix-space
model. What essentially happens is that we iter-
ate two steps: optimizing the W matrices using L-
BFGS-B and the projection step. L-BFGS-B returns
a solution that is not necessarily an affine matrix.
After projecting to the space of affine matrices we
start L-BFGS-B from a better initial point. In prac-
tice, the first few iterations lead to large decrease in
negative loglikelihood.
</bodyText>
<subsectionHeader confidence="0.947421">
2.3 Bag-Of-Words Model
</subsectionHeader>
<bodyText confidence="0.987023666666667">
In the bag-of-words model the score of the i-th
phrase is given in (1). Therefore, the partial deriva-
tive with respect to j-th word in i-th phrase ∂ξi
</bodyText>
<equation confidence="0.6324005">
∂wxi
j
</equation>
<bodyText confidence="0.793792">
equal to the number cj of times xji appears in xi, so:
</bodyText>
<equation confidence="0.6871105">
∂L · cj
∂ξi
</equation>
<bodyText confidence="0.4293945">
Algorithm 1 Training Algorithm for Matrix-Space
OLogReg
</bodyText>
<listItem confidence="0.961472454545455">
1: Input: {(x1, y1), ... , (xn, yn)} //training data
2: Input: h //projection parameter
3: Input: T //number of iterations
4: Input: W, κ0 and τj //initial values
5: for t = 1,..., T do
6: (W, κ0, τj)=minimize L using L-BFGS-B
7: for i = 1,...,d do
8: Wi=Project(Wi, h)
9: end for
10: end for
11: Return W, κ0, τj
</listItem>
<bodyText confidence="0.992967625">
Optimization. We minimize negative loglikelihood
using L-BFGS-B subject to τj &gt; 0.
Regularization. To prevent overfitting for bag-of-
words model we regularize w. The L2-regularized
negative loglikelihood will consist of the expression
in (5) and an additional term λ2 ||w||22, where  ||· ||2
is the L2-norm of a vector. The derivative of the
additional term with respect to w will be:
</bodyText>
<equation confidence="0.8513715">
∂λ2||w||22
∂w
</equation>
<bodyText confidence="0.853108666666667">
Hence the partial derivative with respect to wxi j will
have an additional term λwxi .
j
</bodyText>
<subsectionHeader confidence="0.971371">
2.4 Initialization
</subsectionHeader>
<bodyText confidence="0.999365333333333">
Initialization of bag-of-words OLogReg. We ini-
tialize the weight for each word with zero and κ0
with a random number and τj-s with non-negative
random numbers. Since the learning problem for
bag-of-words OLogReg is convex, we will get the
global optimum.
</bodyText>
<subsectionHeader confidence="0.657262">
Better Initialization of Matrix-Space Model. Pre-
</subsectionHeader>
<bodyText confidence="0.999864363636364">
liminary experiments showed that the Matrix-Space
model needs a good initialization. Initializing with
different random matrices reaches different local
minima and the quality of local minima depends on
initialization. Therefore, it is important to initialize
the model with a good initial point. One way to ini-
tialize the Matrix-Space model is to use the weights
learned by the bag-of-words model. We use the
following intuition for initializing the Matrix-Space
model. As noted in Section 2.2.1 applying trans-
formation A of affine matrix W can model a linear
</bodyText>
<equation confidence="0.657498">
pute
is
∂L
=
∂wxi
j
= λw
</equation>
<page confidence="0.98815">
177
</page>
<bodyText confidence="0.999994153846154">
transformation, while vector b represents a transla-
tion. Since matrix-space model can encode a vector-
space model (Rudolph and Giesbrecht, 2010), we
can initialize the matrices to exactly mimic the bag-
of-words model. In order to do that we place the
weight, learned by the bag-of-words model in the
first component of b. Let’s assume that wx1 and wx2
are the weights learned for two distinct words x1 and
x2 respectively. To compute the polarity score of
a phrase x1, x2 the bag-of-words model sums the
weights of these two words: wx1 and wx2. Now we
want to have the same effect in matrix-space model.
Here we assume m = 3.
</bodyText>
<equation confidence="0.990178714285714">
⎞ ⎛ ⎞
1 0 wx2
⎠⎝0 1 0 ⎠
0 0 1
⎛1 0 wx1 + wx2
= ⎝0 1 0
0 0 1
</equation>
<bodyText confidence="0.985872666666667">
Finally, there is a step of mapping matrix Z to a
number using u and v, such that ξ(Z) = wx1 +
wx2.We also want vector u and v to be such that:
</bodyText>
<equation confidence="0.9701955">
⎞
⎠v = wx1 + wx2 (8)
</equation>
<bodyText confidence="0.9990476">
The last equation can help us construct u and v.
We also set u and v to be orthogonal: uT v = 0.
So, we arbitrarily choose two orthogonal vectors for
which equation (8) holds: u = [1, .\/2,1]T and v =
[1, −.\/2, 1]T.5
</bodyText>
<sectionHeader confidence="0.997972" genericHeader="method">
3 Experimental Methodology
</sectionHeader>
<bodyText confidence="0.999885222222222">
For experimental evaluation of the proposed method
we use the publicly available Multi-Perspective
Question Answering (MPQA)6 corpus (Wiebe et al.,
2005) version 1.2, which contains 535 newswire
documents that are manually annotated with phrase-
level subjectivity and intensity. We use the
expression-level boundary markings in MPQA to
extract phrases. We evaluate on positive, negative
and neutral opinion expressions that have intensities
</bodyText>
<footnote confidence="0.9995255">
5If m &gt; 3, u and v can be set using the same intuition.
6http://www.cs.pitt.edu/mpqa/
</footnote>
<table confidence="0.999543428571429">
Polarity Intensity Ordinal
label
negative high, extreme 0
negative medium 1
neutral high, extreme, medium 2
positive medium 3
positive high, extreme 4
</table>
<tableCaption confidence="0.996416">
Table 1: Mapping of combination of polarities and inten-
sities from MPQA dataset to our ordinal sentiment scale.
</tableCaption>
<bodyText confidence="0.998414">
“medium”, “high” or “extreme”.7 The schematic
mapping of phrase polarity and intensity values on
ordinal sentimental scale is shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.997749">
3.1 Training Details
</subsectionHeader>
<bodyText confidence="0.999991384615385">
We perform 10-fold cross-validation on phrases ex-
tracted from the MPQA corpus: eight folds for train-
ing; one as a validation set; and one as test set. In
total there were 8022 phrases. Before training, we
extract lemmas for each word. For evaluation we
use Ranking Loss: n �i �ˆyi − yi�, where yi is the
prediction.
Choice of dimensionality m. The reported ex-
periments are done by setting m = 3. Preliminary
experiments with higher values of m (5, 20, 50), did
not lead to a better performance and increased the
training time; therefore we did not use those values
in our final experiments.
</bodyText>
<subsectionHeader confidence="0.992838">
3.2 Methods
</subsectionHeader>
<bodyText confidence="0.999944">
PRank. For each of the folds, we run 500 iterations
of PRank and choose an early stopping iteration us-
ing a model that led to the lowest ranking loss on the
validation set; afterwards report the average perfor-
mance of on a test set.
Bag-of-words OLogReg. To prevent overfitting we
search for the best regularization parameter among
the following values of λ: 10i, from 10−4 to 104.
The lowest negative log-likelihood value on the val-
idation set is attained for8 λ = 0.1. With this value
of λ fixed, the final model is the one with the lowest
negative loglikelihood on the training set.
</bodyText>
<footnote confidence="0.8603062">
7We ignored low-intensity phrases similar to (Choi and
Cardie, 2008; Nakagawa et al., 2010).
8We pick single a that gives best average validation set per-
formance, and then use it to compute the average test set perfor-
mance.
</footnote>
<equation confidence="0.8478398">
⎛1 0 wx1
Z = ⎝0 1 0
0 0 1
⎞
⎠
0 0 1
⎛uT 0 1 0
x1
1 0 w
+ wx2
</equation>
<page confidence="0.983972">
178
</page>
<table confidence="0.9987186">
Method Ranking loss
PRank 0.7808
Bag-of-words OLogReg 0.6665
Matrix-space OLogReg+RandInit 0.7417
Matrix-space OLogReg+BowInit 0.6375†
</table>
<tableCaption confidence="0.9840456">
Table 2: Ranking loss for vector-space Ordered Logistic
Regression and Matrix-Space Logistic Regression.
† Stands for a significant difference w.r.t. the Bag-Of-
Words OLogReg model with p-value less than 0.001
(p &lt; 0.001)
</tableCaption>
<bodyText confidence="0.998231833333333">
Matrix-space OLogReg+RandInit. First, we ini-
tialized matrices with with random numbers from
normal distribution N(0, 0.1) and set u and v as in
section 2.4, T is set to 25. We run with two different
random seeds and three different values for the pa-
rameter h: [0.1, 0.5, 0.9] and report the performance
of the model that had the lowest likelihood on the
validation set. The setting of h that lead to the best
model was 0.9.
Matrix-space OLogReg+BowInit. For the matrix-
space models we initialize the model with the out-
put of the regularized Bag-of-words OLogReg as de-
scribed in Section 2.4, T is set to 25. Then we use
the training procedure of Algorithm 1. We consider
three different values for the parameter h [0.1, 0.5,
0.9] and choose as the model with the lowest valida-
tion set negative log-likelihood. The best setting of
h was 0.1.
</bodyText>
<sectionHeader confidence="0.998971" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999958933333333">
We report Ranking Loss for the four models in Ta-
ble 2. The worst performance (denoted by the high-
est ranking loss value) is obtained by PRank, fol-
lowed by matrix-space OLogReg with random ini-
tialization. Bag-of-words OLogReg obtains quite
good performance, and matrix-space OLogReg, ini-
tialized using the bag-of-words model performs the
best, showing statistically significant improvements
over the bag-of-words OLogReg model according to
a paired t-test. .
To see what the bag-of-word and matrix-space
models are learning we performed inference on a
few examples. In Table 3 we show the sentiment
scores of the best performing bag-of-words OLo-
gReg model and the best performing model based
</bodyText>
<table confidence="0.999565166666667">
Phrase Matrix-space Bag-of-words
OLogReg+BowInit OLogReg
not -0.83 -0.42
very 0.23 0.04
good 2.81 1.51
very good 3.53 1.55
not good -0.16 1.09
not very good 0.66 1.13
bad -1.67 -1.42
very bad -2.01 -1.38
not bad -0.54 -1.85
not very bad -1.36 -1.80
</table>
<tableCaption confidence="0.9769665">
Table 3: Phrase and the sentiment scores of the phrase for
2 models Matrix-space OLogReg+BowInit and Bag-of-
words OLogReg respectively. Notice that relative rank-
ing order what matters
</tableCaption>
<bodyText confidence="0.997132333333333">
on matrices Matrix-space OLogReg+BowInit. By
sentiment score, we mean equation (1) of Bag-of-
words OLogReg and equation (2) of Matrix-space
OLogReg+BowInit.
Here we choose two popular adjectives like
‘good’ and ‘bad’ that appeared in the training data,
and examine the effect of applying the intensifier
‘very’ on the sentiment score. As we can see,
the matrix-space model learns a matrix that inten-
sifies both ‘bad’ and ‘good’ in the correct sentiment
scale, i.e., (good) &lt; (very good) and (bad) &lt;
(very bad), while the bag-of-words model gets the
sentiment of ‘very bad’ wrong: it is more positive
than ‘bad’. We also looked at the effect of combin-
ing ‘not’ with these adjectives. The matrix-space
model correctly encodes the effect of the negator
for both positive and negative adjectives, such that
(not good) &lt; (good) and (bad) &lt; (not bad).
For the interesting case of applying a negator to a
phrase with an intensifier, (not good) should be
less than (not very good) and (not very bad)
should be less than (not bad).9 As shown in Ta-
ble 3, these are predicted correctly by the matrix-
space model, which the matrix-space model gets
right, but the bag-of-words model misses in the case
of “bad”.
Also notice that since in the matrix-space model
</bodyText>
<footnote confidence="0.997876">
9See the detailed discussion in Taboada et al. (2011) and Liu
and Seneff (2009).
</footnote>
<page confidence="0.997988">
179
</page>
<bodyText confidence="0.9999502">
each word is represented as a function, more specif-
ically a linear operator, and the function composi-
tion defined as matrix multiplication, we can think
of ”not very” being an operator itself, that is a com-
position of operator ”not” and operator ”very”.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.993915048780488">
Sentiment Analysis. There has been a lot of
research in determining the sentiment of words
and constructing polarity dictionaries (Hatzivas-
siloglou and McKeown, 1997; Wiebe, 2000; Rao
and Ravichandran, 2009; Mohammad et al., 2009;
Velikovich et al., 2010). Some recent work is try-
ing to identify the degree of sentiment of adjectives
and adverbs from text using co-occurrence statistics.
Work by Taboada et. al (2011) and Liu and Sen-
eff (2009), suggest ways of computing the sentiment
of adjectives from data, and computing the effect
of combining adjective with adverb as multiplica-
tive effect and combining adjective with negation as
additive effect. However these models require the
knowledge of a part of speech of given words and
the list of negators (since the negator is an adjective
as well). In our work we propose a single unified
model for handling all words of any part of speech.
On the other hand, there has been some research
in trying to model compositional effects for senti-
ment at the phrase- and sentence-level. Choi and
Cardie (2008) hand-code compositional rules in or-
der to model compositional effects of combining dif-
ferent words in the phrase. The hand-coded rules
are based on domain knowledge and used to learn
the effects of combining words in the phrase. An-
other recent work that tries to model the compo-
sitional semantics of combining different words is
Nakagawa et. al. (2010), which proposes a model
that learns the effects of combining different words
using phrase/sentence dependency parse trees and an
initial polarity dictionary. They present a learning
method that employs hidden variables for sentiment
classification: given the polarity of a sentence and
the a priori polarities of its words, they learn how
to model the interactions between words with head-
modifier relations in the dependency tree.
Some of the previous work looked at MPQA
phrase-level classification. Wilson et al. (2004) tack-
les the problem of classifying clauses according to
their subjective strength but not polarity; Wilson et
al. (2005) classifies phrases according to their po-
larity/sentiment but not strength. Our task is differ-
ent: we classify phrases according to a single ordinal
scale that combines both polarity and strength.
Task of predicting document-level star ratings was
considered in (Pang and Lee, 2005; Goldberg and
Zhu, 2006). In the current work we look at fine-
grained sentiment analysis, more specifically we
study word representations for use in true compo-
sitional semantic settings.
Distributional Semantics and Compositional-
ity. Research in the area of distributional seman-
tics in NLP and Cognitive Science has looked at
different word representations and different ways of
combining words. Mitchell and Lapata (2010) pro-
pose a framework for vector-based semantic com-
position. They define composition as an additive
or multiplicative function of two vectors and show
that compositional approaches generally outperform
non-compositional approaches that treat the phrase
as the union of single lexical items.
Work by Baroni and Zamparelli (2010) models
nouns as vectors in some semantic space and ad-
jectives as matrices. It shows that modeling adjec-
tives as linear transformations and applying those
linear transformations to nouns results in final vec-
tors for adjective-noun compositions that are close
in semantic space to other similar phrases. The
authors argue that modeling adjectives as a linear
transformation is a better idea than using additive
vector-space models. In this work, a separate ma-
trix for each adjective is learned using the Par-
tial Least Squares method in a completely unsuper-
vised way. The recent paper by Rudolph and Gies-
brecht (2010), described in the introduction, argues
for multiplicative matrix-space models. In contrast
to other work in this area, our work is concerned
with a specific dimension of word meaning — sen-
timent. Our techniques, however, are quite general
and should be applicable to other problems in lexical
semantics.
</bodyText>
<sectionHeader confidence="0.998594" genericHeader="conclusions">
6 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.998668666666667">
In the current work we present a novel matrix-space
model for ordinal scale sentiment prediction and an
algorithm for learning such a model. The proposed
</bodyText>
<page confidence="0.99004">
180
</page>
<bodyText confidence="0.999918510638298">
model learns a matrix for each word; the composi-
tion of words is modeled as iterated matrix multi-
plication. The matrix-space framework with iterated
matrix multiplication defines an elegant framework
for modeling composition; it is also quite general.
We use the matrix-space framework in the context
of sentiment prediction, a domain where interesting
compositional effects can be observed. The main fo-
cus of this work was to study word representations
(represent as a single weight vs. as a matrix) for use
in true compositional semantic settings. One of the
benefits of the proposed approach is that by learn-
ing matrices for words, the model can handle unseen
word compositions (e.g. unseen bigrams) when the
unigrams involved have been seen.
However, it is not trivial to learn a matrix-space
model. Since the final optimization problem is non-
convex, the initialization has to be done carefully.
Here the weights learned in bag-of-words model
come to rescue and provide good initial point for op-
timization procedure. The final model outperforms
the bag-of-words based model, which suggests that
this research direction is very promising.
Though in our model the order of composition is
the same as the word order, we believe that a linguis-
tically informed order of composition can give us
further performance gains. For example, one can use
the output of a dependency parser to guide the order
of composition, similar to Nakagawa et al. (2010).
Another possibility for improvement is to use the in-
formation about the scope of negation. In the current
work we assume the scope of negation to be the ex-
pression following the negation; in reality, however,
determining the scope of negation is a complex lin-
guistic phenomenon (Moilanen and Pulman, 2007).
So the proposed model can benefit from identify-
ing the scope of negation, similar to (Councill et al.,
2010).
Also we plan to consider other ways to initialize
the matrix-space model. One interesting direction to
explore might be to use non-negative matrix factor-
ization (Lee and Seung, 2001), co-clustering tech-
niques (Dhillon, 2001) to better initialize words that
share similar contexts. The other possible direction
is to use existing sentiment lexicons and employ-
ing a “curriculum learning” strategy (Bengio et al.,
2009; Kumar et al., 2010) for our learning problem.
</bodyText>
<sectionHeader confidence="0.994563" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999789">
This work was supported in part by National Science
Foundation Grants BCS-0904822, BCS-0624277,
IIS-0968450; and by a gift from Google. We thank
the anonymous reviewers, and David Bindel, Nikos
Karampatziakis, Lillian Lee and Cornell NLP group
for useful suggestions and insightful discussions.
</bodyText>
<sectionHeader confidence="0.997829" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998464023809524">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, pages 1183–1193, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and
Jason Weston. 2009. Curriculum learning. In Pro-
ceedings of the 26th Annual International Conference
on Machine Learning, ICML ’09. ACM.
R. H. Byrd, P. Lu, and J. Nocedal. 1995. A limited
memory algorithm for bound constrained optimiza-
tion. SIAM Journal on Scientific and Statistical Com-
puting, pages 1190–1208.
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Empirical Methods in
Natural Language Processing (EMNLP).
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What’s great and what’s not: learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing, NeSp-NLP ’10, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Koby Crammer and Yoram Singer. 2001. Pranking with
ranking. In Advances in Neural Information Process-
ing Systems 14, pages 641–647. MIT Press.
Marie-Catherine de Marneffe, Christopher D. Manning,
and Christopher Potts. 2010. Was it good? It was
provocative. learning the meaning of scalar adjectives.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, Uppsala, Swe-
den, July 11–16. ACL.
I. S. Dhillon. 2001. Co-clustering documents and words
using bipartite spectral graph partitioning. In KDD.
Andrew B. Goldberg and Jerry Zhu. 2006. Seeing
stars when there aren’t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL Workshop on Textgraphs: Graph-based
Algorithms for Natural Language Processing.
</reference>
<page confidence="0.981376">
181
</page>
<reference confidence="0.998698923913043">
James W. Hardin and Joseph Hilbe. 2007. Generalized
Linear Models and Extensions. Stata Press.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL, pages 174–181.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie reviews using contextual va-
lence shifters. Computational Intelligence, 22(2, Spe-
cial Issue on Sentiment Analysis)):110–125.
M. Pawan Kumar, Benjamin Packer, and Daphne Koller.
2010. Self-paced learning for latent variable models.
In Advances in Neural Information Processing Sys-
tems 23. NIPS.
D. Lee and H. Seung. 2001. Algorithms for non-negative
matrix factorization. In NIPS.
Jingjing Liu and Stephanie Seneff. 2009. Review sen-
timent scoring via a parse-and-paraphrase paradigm.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
161–169, Singapore, August. Association for Compu-
tational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388–1429.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.
Generating high-coverage semantic orientation lexi-
cons from overtly marked words and a thesaurus.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
599–608, Singapore, August. Association for Compu-
tational Linguistics.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. In Proceedings of Recent Advances in
Natural Language Processing (RANLP 2007), pages
378–382, September 27-29.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using crfs with hidden variables. In Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL).
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL,
pages 115–124.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135.
K. B. Petersen and M. S. Pedersen. ”2008”. The Matrix
Cookbook. ”Technical University of Denmark”, ”oct”.
”Version 20081110”.
Livia Polanyi and Annie Zaenen. 2004. Contextual
lexical valence shifters. In Proceedings of the AAAI
Spring Symposium on Exploring Attitude and Affect in
Text: Theories and Applications.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 675–682, Athens, Greece,
March. Association for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010. Com-
positional matrix-space models of language. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ’10, pages 907–
916, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Mostafa Shaikh, Helmut Prendinger, and Ishizuka Mit-
suru. 2007. Assessing sentiment of text by semantic
dependency and contextual valence analysis.
Maite Taboada, Julian Brooke, Milan Tofiloskiy, and
Kimberly Vollz. 2011). Lexicon-based methods for
sentiment analysis. In Computational Linguistics.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 777–785, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164–
210.
Janyce M. Wiebe. 2000. Learning subjective adjectives
from corpora. In In AAAI, pages 735–740.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? In AAAI. AAAI.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Empirical Methods in Natural
Language Processing (EMNLP).
</reference>
<page confidence="0.997965">
182
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.286393">
<title confidence="0.984031">Compositional Matrix-Space Models for Sentiment Analysis</title>
<author confidence="0.799474">Ainur</author>
<affiliation confidence="0.9628835">Dept. of Computer Cornell</affiliation>
<address confidence="0.706239">Ithaca, NY,</address>
<email confidence="0.999125">ainur@cs.cornell.edu</email>
<author confidence="0.912602">Claire</author>
<affiliation confidence="0.9675335">Dept. of Computer Cornell</affiliation>
<address confidence="0.690204">Ithaca, NY,</address>
<email confidence="0.999689">cardie@cs.cornell.edu</email>
<abstract confidence="0.998392733333333">We present a general learning-based approach for phrase-level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature. Thus, we can model the compositional effects required for accurate assignment of phrase-level sentiment. For example, combining an adverb (e.g., “very”) with a positive polar adjective (e.g., “good”) produces a phrase (“very good”) with increased polarity over the adjective alone. Inspired by recent work on distributional approaches to compositionality, we model each word as a matrix and combine words using iterated matrix multiplication, which allows for the modeling of both additive and multiplicative semantic effects. Although the multiplication-based matrix-space framework has been shown to be a theoretically elegant way to model composition (Rudolph and Giesbrecht, 2010), training such models has to be done carefully: the optimization is nonconvex and requires a good initial starting point. This paper presents the first such alfor matrix-space model for semantic composition. In the context of the phrase-level sentiment analysis task, our experimental results show statistically significant improvements in performance over a bagof-words model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1183--1193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7178" citStr="Baroni and Zamparelli (2010)" startWordPosition="1090" endWordPosition="1093">variables given the a priori (i.e., out of context) polarity of the words in the phrase and the (correct) phrase-level polarity. As in Moilianen and Pulman (2007), semantic inference is based on (a small set of) hand-written compositional rules. In contrast, Nakagawa et. al (2010) use a dependency parse tree to guide the learning of compositional effects. Each of the above, however, uses a binary rather than an ordinal sentiment scale. In contrast, our proposed method for phraselevel sentiment analysis is inspired by recent work on distributional approaches to compositionality. In particular, Baroni and Zamparelli (2010) tackle adjective-noun compositions using a vector representation for nouns and learning a matrix representation for each adjective. The adjective matrices are then applied as functions over the meanings of nouns — via matrix-vector multiplication — to derive the meaning of adjective-noun combinations. Rudolph and Giesbrecht (2010) show theoretically, that multiplicative matrix-space models are a general case of vector-space models and furthermore exhibit desirable properties for semantic analysis: they take into account word order and are algebraically, neurologically and psychologically plau</context>
<context position="32546" citStr="Baroni and Zamparelli (2010)" startWordPosition="5610" endWordPosition="5613">d representations for use in true compositional semantic settings. Distributional Semantics and Compositionality. Research in the area of distributional semantics in NLP and Cognitive Science has looked at different word representations and different ways of combining words. Mitchell and Lapata (2010) propose a framework for vector-based semantic composition. They define composition as an additive or multiplicative function of two vectors and show that compositional approaches generally outperform non-compositional approaches that treat the phrase as the union of single lexical items. Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. It shows that modeling adjectives as linear transformations and applying those linear transformations to nouns results in final vectors for adjective-noun compositions that are close in semantic space to other similar phrases. The authors argue that modeling adjectives as a linear transformation is a better idea than using additive vector-space models. In this work, a separate matrix for each adjective is learned using the Partial Least Squares method in a completely unsupervised way. The recent paper by Rudolph and Gi</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1183–1193, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>J´erˆome Louradour</author>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>Curriculum learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09.</booktitle>
<publisher>ACM.</publisher>
<marker>Bengio, Louradour, Collobert, Weston, 2009</marker>
<rawString>Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Byrd</author>
<author>P Lu</author>
<author>J Nocedal</author>
</authors>
<title>A limited memory algorithm for bound constrained optimization.</title>
<date>1995</date>
<journal>SIAM Journal on Scientific and Statistical Computing,</journal>
<pages>1190--1208</pages>
<contexts>
<context position="17905" citStr="Byrd et al. (1995)" startWordPosition="3100" endWordPosition="3103">account for word order. 2.2.2 Optimization The goal of training procedure is for the i-th phrase with p words x1x2 ... xp to learn word matrices W1, W2, ... , Wp such that resulting ξi-s will lead to the lowest negative loglikelihood. The goal of training procedure is to find word matrices W1, W2, ... Wp and thresholds κ0, τ1, ... τr−2 such that the negative loglikelihood is minimized. So, given the negative loglikelihood and the derivatives with respect κ0 and τj-s and word matrices W, we optimize objective (5) subject to τj &gt; 0. We use LBFGS-B (Large-scale Bound-constrained Optimization) by Byrd et al. (1995) as an optimizer. 2.2.3 Regularization in Matrix-Space Model In order to make sure that the L-BFGS-B updates do not cause numerical issues we perform the following regularization to the resulting matrices. An m by m matrix Wj that can be represented as: ∂L ∂L � ∂ξi ∂Wxi j from (2). In the case of the Matrix-Space word ξ(u, v, W) would be equal to ξ(ˆu, ˆv, ∂ξi ∂Wj )T] = u(W1Wv)T + (uTWW1)TvT = ˆu T ˆW1 ˆW2ˆv C A11 a12 ) Wj = aT21 a22 176 where A11 E Rm−1xm−1, a12,a21 E Rm−1x1, a22 E R. First make the matrix affine by updating the last row, then the updated matrix will look like: ( = Wj A11 a12</context>
</contexts>
<marker>Byrd, Lu, Nocedal, 1995</marker>
<rawString>R. H. Byrd, P. Lu, and J. Nocedal. 1995. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific and Statistical Computing, pages 1190–1208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Learning with compositional semantics as structural inference for subsentential sentiment analysis.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="4582" citStr="Choi and Cardie, 2008" startWordPosition="685" endWordPosition="688">ke “bad”, produces a phrase (“very bad”) that should be characterized as more negative than the original adjective. Thus, it is convenient to think of the effect of combining an intensifying adverb with a polar adjective as being multiplicative in nature, if we assume the adjectives (“good” and “bad”) to have positive and a negative sentiment scores, respectively. Next, let us consider adverbial negators like “not” combined with polar adjectives. When modeling only positive and negative labels for sentiment, negators are generally treated as flipping the polarity of the adjective it modifies (Choi and Cardie, 2008; Nakagawa et al., 2010). However, recent work (Taboada et al., 2011; Liu and Seneff, 2009) suggests that the effect of the negator when ordinal sentiment scores are employed is more akin to dampening the adjective’s polarity rather than flipping it. For example, if “perfect” has a strong positive sentiment, then the phrase “not perfect” is still positive, though to a lesser degree. And while “not terrible” is still negative, it is less negative than “terrible”. For these cases, it is convenient to view “not” as shifting polarity to the opposite side of polarity scale by some value. There are,</context>
<context position="6403" citStr="Choi and Cardie (2008)" startWordPosition="972" endWordPosition="975">gators, intensifiers, and for contextual valence shifters, which can change the sentiment over neighboring words (e.g., Polanyi and Zaenen (2004), Wilson et al. (2005) , Kennedy and Inkpen (2006), Shaikh et al. (2007)). One notable exception is Moilanen and Pulman (2007), who propose a compositional semantic approach to assign a positive or negative sentiment to newspaper article titles. However, their knowledgebased approach presupposes the existence of a sentiment lexicon and a set of symbolic compositional rules. But learning-based compositional approaches for sentiment analyis also exist. Choi and Cardie (2008), for example, propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis decision variables given the a priori (i.e., out of context) polarity of the words in the phrase and the (correct) phrase-level polarity. As in Moilianen and Pulman (2007), semantic inference is based on (a small set of) hand-written compositional rules. In contrast, Nakagawa et. al (2010) use a dependency parse tree to guide the learning of compositional effects. Each of the above, however, uses a binary rather than an ordinal sentiment scale. In contrast, </context>
<context position="25303" citStr="Choi and Cardie, 2008" startWordPosition="4420" endWordPosition="4423"> run 500 iterations of PRank and choose an early stopping iteration using a model that led to the lowest ranking loss on the validation set; afterwards report the average performance of on a test set. Bag-of-words OLogReg. To prevent overfitting we search for the best regularization parameter among the following values of λ: 10i, from 10−4 to 104. The lowest negative log-likelihood value on the validation set is attained for8 λ = 0.1. With this value of λ fixed, the final model is the one with the lowest negative loglikelihood on the training set. 7We ignored low-intensity phrases similar to (Choi and Cardie, 2008; Nakagawa et al., 2010). 8We pick single a that gives best average validation set performance, and then use it to compute the average test set performance. ⎛1 0 wx1 Z = ⎝0 1 0 0 0 1 ⎞ ⎠ 0 0 1 ⎛uT 0 1 0 x1 1 0 w + wx2 178 Method Ranking loss PRank 0.7808 Bag-of-words OLogReg 0.6665 Matrix-space OLogReg+RandInit 0.7417 Matrix-space OLogReg+BowInit 0.6375† Table 2: Ranking loss for vector-space Ordered Logistic Regression and Matrix-Space Logistic Regression. † Stands for a significant difference w.r.t. the Bag-OfWords OLogReg model with p-value less than 0.001 (p &lt; 0.001) Matrix-space OLogReg+R</context>
<context position="30521" citStr="Choi and Cardie (2008)" startWordPosition="5300" endWordPosition="5303">eneff (2009), suggest ways of computing the sentiment of adjectives from data, and computing the effect of combining adjective with adverb as multiplicative effect and combining adjective with negation as additive effect. However these models require the knowledge of a part of speech of given words and the list of negators (since the negator is an adjective as well). In our work we propose a single unified model for handling all words of any part of speech. On the other hand, there has been some research in trying to model compositional effects for sentiment at the phrase- and sentence-level. Choi and Cardie (2008) hand-code compositional rules in order to model compositional effects of combining different words in the phrase. The hand-coded rules are based on domain knowledge and used to learn the effects of combining words in the phrase. Another recent work that tries to model the compositional semantics of combining different words is Nakagawa et. al. (2010), which proposes a model that learns the effects of combining different words using phrase/sentence dependency parse trees and an initial polarity dictionary. They present a learning method that employs hidden variables for sentiment classificatio</context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Yejin Choi and Claire Cardie. 2008. Learning with compositional semantics as structural inference for subsentential sentiment analysis. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac G Councill</author>
<author>Ryan McDonald</author>
<author>Leonid Velikovich</author>
</authors>
<title>What’s great and what’s not: learning to classify the scope of negation for improved sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, NeSp-NLP ’10,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="35518" citStr="Councill et al., 2010" startWordPosition="6092" endWordPosition="6095">stically informed order of composition can give us further performance gains. For example, one can use the output of a dependency parser to guide the order of composition, similar to Nakagawa et al. (2010). Another possibility for improvement is to use the information about the scope of negation. In the current work we assume the scope of negation to be the expression following the negation; in reality, however, determining the scope of negation is a complex linguistic phenomenon (Moilanen and Pulman, 2007). So the proposed model can benefit from identifying the scope of negation, similar to (Councill et al., 2010). Also we plan to consider other ways to initialize the matrix-space model. One interesting direction to explore might be to use non-negative matrix factorization (Lee and Seung, 2001), co-clustering techniques (Dhillon, 2001) to better initialize words that share similar contexts. The other possible direction is to use existing sentiment lexicons and employing a “curriculum learning” strategy (Bengio et al., 2009; Kumar et al., 2010) for our learning problem. Acknowledgments This work was supported in part by National Science Foundation Grants BCS-0904822, BCS-0624277, IIS-0968450; and by a g</context>
</contexts>
<marker>Councill, McDonald, Velikovich, 2010</marker>
<rawString>Isaac G. Councill, Ryan McDonald, and Leonid Velikovich. 2010. What’s great and what’s not: learning to classify the scope of negation for improved sentiment analysis. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, NeSp-NLP ’10, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Pranking with ranking.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems 14,</booktitle>
<pages>641--647</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9626" citStr="Crammer and Singer, 2001" startWordPosition="1460" endWordPosition="1463">le sentiment prediction task. 2 The Model for Ordinal Scale Sentiment Prediction As described above, our task is to predict an ordinal scale sentiment value for a phrase. To this end, we employ a sentiment scale with five ordinal values: VERY NEGATIVE, NEGATIVE, NEUTRAL, POSITIVE and VERY POSITIVE. Given a set of phraselevel training examples with their gold-standard ordinal sentiment value, we then use an Ordered Logistic Regression (OLogReg) model for prediction. Unfortunately, our matrix-space representation precludes doing this directly. We have chosen OLogReg, as opposed to say PRanking (Crammer and Singer, 2001), because optimization of the former is more attractive: the objective (likelihood) is smooth and the gradients are continuous. As will become clear shortly, learning our models is not trivial and it is important to use sophisticated off-the-shelf optimizers such as LBFGS. For a bag-of-words model, OLogReg learns one weight for each word and a set of thresholds by maximizing the likelihood of the training data. Typically, this is accomplished by using an optimizer like LBFGS whose interface needs the value and gradient of the likelihood with respect to the parameters at their current values. I</context>
</contexts>
<marker>Crammer, Singer, 2001</marker>
<rawString>Koby Crammer and Yoram Singer. 2001. Pranking with ranking. In Advances in Neural Information Processing Systems 14, pages 641–647. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
<author>Christopher Potts</author>
</authors>
<title>Was it good? It was provocative. learning the meaning of scalar adjectives.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>ACL.</publisher>
<location>Uppsala, Sweden,</location>
<marker>de Marneffe, Manning, Potts, 2010</marker>
<rawString>Marie-Catherine de Marneffe, Christopher D. Manning, and Christopher Potts. 2010. Was it good? It was provocative. learning the meaning of scalar adjectives. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, July 11–16. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I S Dhillon</author>
</authors>
<title>Co-clustering documents and words using bipartite spectral graph partitioning.</title>
<date>2001</date>
<booktitle>In KDD.</booktitle>
<marker>Dhillon, 2001</marker>
<rawString>I. S. Dhillon. 2001. Co-clustering documents and words using bipartite spectral graph partitioning. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew B Goldberg</author>
<author>Jerry Zhu</author>
</authors>
<title>Seeing stars when there aren’t many stars: Graph-based semisupervised learning for sentiment categorization.</title>
<date>2006</date>
<booktitle>In HLT-NAACL Workshop on Textgraphs: Graph-based Algorithms for Natural Language Processing.</booktitle>
<contexts>
<context position="31823" citStr="Goldberg and Zhu, 2006" startWordPosition="5504" endWordPosition="5507">hey learn how to model the interactions between words with headmodifier relations in the dependency tree. Some of the previous work looked at MPQA phrase-level classification. Wilson et al. (2004) tackles the problem of classifying clauses according to their subjective strength but not polarity; Wilson et al. (2005) classifies phrases according to their polarity/sentiment but not strength. Our task is different: we classify phrases according to a single ordinal scale that combines both polarity and strength. Task of predicting document-level star ratings was considered in (Pang and Lee, 2005; Goldberg and Zhu, 2006). In the current work we look at finegrained sentiment analysis, more specifically we study word representations for use in true compositional semantic settings. Distributional Semantics and Compositionality. Research in the area of distributional semantics in NLP and Cognitive Science has looked at different word representations and different ways of combining words. Mitchell and Lapata (2010) propose a framework for vector-based semantic composition. They define composition as an additive or multiplicative function of two vectors and show that compositional approaches generally outperform no</context>
</contexts>
<marker>Goldberg, Zhu, 2006</marker>
<rawString>Andrew B. Goldberg and Jerry Zhu. 2006. Seeing stars when there aren’t many stars: Graph-based semisupervised learning for sentiment categorization. In HLT-NAACL Workshop on Textgraphs: Graph-based Algorithms for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Hardin</author>
<author>Joseph Hilbe</author>
</authors>
<title>Generalized Linear Models and Extensions.</title>
<date>2007</date>
<publisher>Stata Press.</publisher>
<contexts>
<context position="13629" citStr="Hardin and Hilbe, 2007" startWordPosition="2242" endWordPosition="2245">j=1 τj, . . . , r−2 Kr−2 = K0 + Pj=1 τj Kr−1 = ∞, where τ1, . . ., τr−2 are non-negative values, that represent how far the corresponding thresholds are from each other. Then the constraints (3) would be: τj ≥ 0, 1 ≤ j ≤ r − 2 (4) To simplify the equations we can rewrite the negative loglikelihood as follows: P(yi = k|x) = P(Kk−1 &lt; Si ≤ Kk) L = − Xn r−1X ln(Aik − Bik)I(yi = k) (5) = F(Kk − Si) − F(Kk−1 − Si) i=1 k=0 F(x) is an inverse-logit function ex F(x) = 1 + ex this is its derivative: = F(x)(1 − F(x)) Therefore the negative loglikelihood of the training data will look like the following (Hardin and Hilbe, 2007): ln(F(Kk − Si) − F(Kk−1 − Si))I(yi = k) where F(K0 + Pkj=1 τj − Si), if k = 0,...,r − 2 _ Aik 11, if k = r − 1 ( 0, if k = 0 Bik = F(K0 + Pk−1 j=1 τj − Si), if k = 1, ... , r − 1 Let’s introduce Lik = − ln(Aik − Bik)I(yi = k) and then the derivative of Lik with respect to K0 will be: −[Aik(1 − Aik) − Bik(1 − Bik)] = I(yi = k) aK0 Aik − Bik = (Aik + Bik − 1)I(yi = k) For j = yi: dF(x) dx Xn i=1 L = − r−1X k=0 aLik where r is the number of ordinal classes, Si is the score of i-th phrase, I is the indicator function that is equal to 1 – when yi = k, and zero otherwise. We need to minimize the ob</context>
</contexts>
<marker>Hardin, Hilbe, 2007</marker>
<rawString>James W. Hardin and Joseph Hilbe. 2007. Generalized Linear Models and Extensions. Stata Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In EACL,</booktitle>
<pages>174--181</pages>
<contexts>
<context position="29633" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="5149" endWordPosition="5153">ts right, but the bag-of-words model misses in the case of “bad”. Also notice that since in the matrix-space model 9See the detailed discussion in Taboada et al. (2011) and Liu and Seneff (2009). 179 each word is represented as a function, more specifically a linear operator, and the function composition defined as matrix multiplication, we can think of ”not very” being an operator itself, that is a composition of operator ”not” and operator ”very”. 5 Related Work Sentiment Analysis. There has been a lot of research in determining the sentiment of words and constructing polarity dictionaries (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Rao and Ravichandran, 2009; Mohammad et al., 2009; Velikovich et al., 2010). Some recent work is trying to identify the degree of sentiment of adjectives and adverbs from text using co-occurrence statistics. Work by Taboada et. al (2011) and Liu and Seneff (2009), suggest ways of computing the sentiment of adjectives from data, and computing the effect of combining adjective with adverb as multiplicative effect and combining adjective with negation as additive effect. However these models require the knowledge of a part of speech of given words and the list of negators (since th</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In EACL, pages 174–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Kennedy</author>
<author>Diana Inkpen</author>
</authors>
<title>Sentiment classification of movie reviews using contextual valence shifters.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<note>Special Issue on Sentiment Analysis)):110–125.</note>
<contexts>
<context position="5976" citStr="Kennedy and Inkpen (2006)" startWordPosition="909" endWordPosition="912">ent-word negators (Choi and Cardie, 2008) in that they modify the negative sentiment of their direct object arguments so that the phrase as a whole is perceived as somewhat positive. Nonetheless, the vast majority of methods for phrase- and sentence-level sentiment analysis do not tackle the task compositionally: they, instead, employ a bag-of-words representation and, at best, incorporate additional features to account for negators, intensifiers, and for contextual valence shifters, which can change the sentiment over neighboring words (e.g., Polanyi and Zaenen (2004), Wilson et al. (2005) , Kennedy and Inkpen (2006), Shaikh et al. (2007)). One notable exception is Moilanen and Pulman (2007), who propose a compositional semantic approach to assign a positive or negative sentiment to newspaper article titles. However, their knowledgebased approach presupposes the existence of a sentiment lexicon and a set of symbolic compositional rules. But learning-based compositional approaches for sentiment analyis also exist. Choi and Cardie (2008), for example, propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis decision variables given the a prio</context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>Alistair Kennedy and Diana Inkpen. 2006. Sentiment classification of movie reviews using contextual valence shifters. Computational Intelligence, 22(2, Special Issue on Sentiment Analysis)):110–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pawan Kumar</author>
<author>Benjamin Packer</author>
<author>Daphne Koller</author>
</authors>
<title>Self-paced learning for latent variable models.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems 23.</booktitle>
<publisher>NIPS.</publisher>
<marker>Kumar, Packer, Koller, 2010</marker>
<rawString>M. Pawan Kumar, Benjamin Packer, and Daphne Koller. 2010. Self-paced learning for latent variable models. In Advances in Neural Information Processing Systems 23. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lee</author>
<author>H Seung</author>
</authors>
<title>Algorithms for non-negative matrix factorization.</title>
<date>2001</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="35702" citStr="Lee and Seung, 2001" startWordPosition="6121" endWordPosition="6124">agawa et al. (2010). Another possibility for improvement is to use the information about the scope of negation. In the current work we assume the scope of negation to be the expression following the negation; in reality, however, determining the scope of negation is a complex linguistic phenomenon (Moilanen and Pulman, 2007). So the proposed model can benefit from identifying the scope of negation, similar to (Councill et al., 2010). Also we plan to consider other ways to initialize the matrix-space model. One interesting direction to explore might be to use non-negative matrix factorization (Lee and Seung, 2001), co-clustering techniques (Dhillon, 2001) to better initialize words that share similar contexts. The other possible direction is to use existing sentiment lexicons and employing a “curriculum learning” strategy (Bengio et al., 2009; Kumar et al., 2010) for our learning problem. Acknowledgments This work was supported in part by National Science Foundation Grants BCS-0904822, BCS-0624277, IIS-0968450; and by a gift from Google. We thank the anonymous reviewers, and David Bindel, Nikos Karampatziakis, Lillian Lee and Cornell NLP group for useful suggestions and insightful discussions. Referenc</context>
</contexts>
<marker>Lee, Seung, 2001</marker>
<rawString>D. Lee and H. Seung. 2001. Algorithms for non-negative matrix factorization. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingjing Liu</author>
<author>Stephanie Seneff</author>
</authors>
<title>Review sentiment scoring via a parse-and-paraphrase paradigm.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>161--169</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4673" citStr="Liu and Seneff, 2009" startWordPosition="700" endWordPosition="703"> the original adjective. Thus, it is convenient to think of the effect of combining an intensifying adverb with a polar adjective as being multiplicative in nature, if we assume the adjectives (“good” and “bad”) to have positive and a negative sentiment scores, respectively. Next, let us consider adverbial negators like “not” combined with polar adjectives. When modeling only positive and negative labels for sentiment, negators are generally treated as flipping the polarity of the adjective it modifies (Choi and Cardie, 2008; Nakagawa et al., 2010). However, recent work (Taboada et al., 2011; Liu and Seneff, 2009) suggests that the effect of the negator when ordinal sentiment scores are employed is more akin to dampening the adjective’s polarity rather than flipping it. For example, if “perfect” has a strong positive sentiment, then the phrase “not perfect” is still positive, though to a lesser degree. And while “not terrible” is still negative, it is less negative than “terrible”. For these cases, it is convenient to view “not” as shifting polarity to the opposite side of polarity scale by some value. There are, of course, more interesting examples of compositional semantic effects on sentiment: e.g.,</context>
<context position="29193" citStr="Liu and Seneff (2009)" startWordPosition="5079" endWordPosition="5082"> correctly encodes the effect of the negator for both positive and negative adjectives, such that (not good) &lt; (good) and (bad) &lt; (not bad). For the interesting case of applying a negator to a phrase with an intensifier, (not good) should be less than (not very good) and (not very bad) should be less than (not bad).9 As shown in Table 3, these are predicted correctly by the matrixspace model, which the matrix-space model gets right, but the bag-of-words model misses in the case of “bad”. Also notice that since in the matrix-space model 9See the detailed discussion in Taboada et al. (2011) and Liu and Seneff (2009). 179 each word is represented as a function, more specifically a linear operator, and the function composition defined as matrix multiplication, we can think of ”not very” being an operator itself, that is a composition of operator ”not” and operator ”very”. 5 Related Work Sentiment Analysis. There has been a lot of research in determining the sentiment of words and constructing polarity dictionaries (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Rao and Ravichandran, 2009; Mohammad et al., 2009; Velikovich et al., 2010). Some recent work is trying to identify the degree of sentiment of ad</context>
</contexts>
<marker>Liu, Seneff, 2009</marker>
<rawString>Jingjing Liu and Stephanie Seneff. 2009. Review sentiment scoring via a parse-and-paraphrase paradigm. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 161–169, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="32220" citStr="Mitchell and Lapata (2010)" startWordPosition="5563" endWordPosition="5566">ur task is different: we classify phrases according to a single ordinal scale that combines both polarity and strength. Task of predicting document-level star ratings was considered in (Pang and Lee, 2005; Goldberg and Zhu, 2006). In the current work we look at finegrained sentiment analysis, more specifically we study word representations for use in true compositional semantic settings. Distributional Semantics and Compositionality. Research in the area of distributional semantics in NLP and Cognitive Science has looked at different word representations and different ways of combining words. Mitchell and Lapata (2010) propose a framework for vector-based semantic composition. They define composition as an additive or multiplicative function of two vectors and show that compositional approaches generally outperform non-compositional approaches that treat the phrase as the union of single lexical items. Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. It shows that modeling adjectives as linear transformations and applying those linear transformations to nouns results in final vectors for adjective-noun compositions that are close in semantic spa</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Cody Dunne</author>
<author>Bonnie Dorr</author>
</authors>
<title>Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>599--608</pages>
<institution>Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="29697" citStr="Mohammad et al., 2009" startWordPosition="5160" endWordPosition="5163"> that since in the matrix-space model 9See the detailed discussion in Taboada et al. (2011) and Liu and Seneff (2009). 179 each word is represented as a function, more specifically a linear operator, and the function composition defined as matrix multiplication, we can think of ”not very” being an operator itself, that is a composition of operator ”not” and operator ”very”. 5 Related Work Sentiment Analysis. There has been a lot of research in determining the sentiment of words and constructing polarity dictionaries (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Rao and Ravichandran, 2009; Mohammad et al., 2009; Velikovich et al., 2010). Some recent work is trying to identify the degree of sentiment of adjectives and adverbs from text using co-occurrence statistics. Work by Taboada et. al (2011) and Liu and Seneff (2009), suggest ways of computing the sentiment of adjectives from data, and computing the effect of combining adjective with adverb as multiplicative effect and combining adjective with negation as additive effect. However these models require the knowledge of a part of speech of given words and the list of negators (since the negator is an adjective as well). In our work we propose a sin</context>
</contexts>
<marker>Mohammad, Dunne, Dorr, 2009</marker>
<rawString>Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009. Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 599–608, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karo Moilanen</author>
<author>Stephen Pulman</author>
</authors>
<title>Sentiment composition.</title>
<date>2007</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing (RANLP</booktitle>
<pages>378--382</pages>
<contexts>
<context position="6052" citStr="Moilanen and Pulman (2007)" startWordPosition="921" endWordPosition="924">sentiment of their direct object arguments so that the phrase as a whole is perceived as somewhat positive. Nonetheless, the vast majority of methods for phrase- and sentence-level sentiment analysis do not tackle the task compositionally: they, instead, employ a bag-of-words representation and, at best, incorporate additional features to account for negators, intensifiers, and for contextual valence shifters, which can change the sentiment over neighboring words (e.g., Polanyi and Zaenen (2004), Wilson et al. (2005) , Kennedy and Inkpen (2006), Shaikh et al. (2007)). One notable exception is Moilanen and Pulman (2007), who propose a compositional semantic approach to assign a positive or negative sentiment to newspaper article titles. However, their knowledgebased approach presupposes the existence of a sentiment lexicon and a set of symbolic compositional rules. But learning-based compositional approaches for sentiment analyis also exist. Choi and Cardie (2008), for example, propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis decision variables given the a priori (i.e., out of context) polarity of the words in the phrase and the (corre</context>
<context position="35408" citStr="Moilanen and Pulman, 2007" startWordPosition="6073" endWordPosition="6076">ry promising. Though in our model the order of composition is the same as the word order, we believe that a linguistically informed order of composition can give us further performance gains. For example, one can use the output of a dependency parser to guide the order of composition, similar to Nakagawa et al. (2010). Another possibility for improvement is to use the information about the scope of negation. In the current work we assume the scope of negation to be the expression following the negation; in reality, however, determining the scope of negation is a complex linguistic phenomenon (Moilanen and Pulman, 2007). So the proposed model can benefit from identifying the scope of negation, similar to (Councill et al., 2010). Also we plan to consider other ways to initialize the matrix-space model. One interesting direction to explore might be to use non-negative matrix factorization (Lee and Seung, 2001), co-clustering techniques (Dhillon, 2001) to better initialize words that share similar contexts. The other possible direction is to use existing sentiment lexicons and employing a “curriculum learning” strategy (Bengio et al., 2009; Kumar et al., 2010) for our learning problem. Acknowledgments This work</context>
</contexts>
<marker>Moilanen, Pulman, 2007</marker>
<rawString>Karo Moilanen and Stephen Pulman. 2007. Sentiment composition. In Proceedings of Recent Advances in Natural Language Processing (RANLP 2007), pages 378–382, September 27-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using crfs with hidden variables.</title>
<date>2010</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="4606" citStr="Nakagawa et al., 2010" startWordPosition="689" endWordPosition="692">rase (“very bad”) that should be characterized as more negative than the original adjective. Thus, it is convenient to think of the effect of combining an intensifying adverb with a polar adjective as being multiplicative in nature, if we assume the adjectives (“good” and “bad”) to have positive and a negative sentiment scores, respectively. Next, let us consider adverbial negators like “not” combined with polar adjectives. When modeling only positive and negative labels for sentiment, negators are generally treated as flipping the polarity of the adjective it modifies (Choi and Cardie, 2008; Nakagawa et al., 2010). However, recent work (Taboada et al., 2011; Liu and Seneff, 2009) suggests that the effect of the negator when ordinal sentiment scores are employed is more akin to dampening the adjective’s polarity rather than flipping it. For example, if “perfect” has a strong positive sentiment, then the phrase “not perfect” is still positive, though to a lesser degree. And while “not terrible” is still negative, it is less negative than “terrible”. For these cases, it is convenient to view “not” as shifting polarity to the opposite side of polarity scale by some value. There are, of course, more interes</context>
<context position="25327" citStr="Nakagawa et al., 2010" startWordPosition="4424" endWordPosition="4427">PRank and choose an early stopping iteration using a model that led to the lowest ranking loss on the validation set; afterwards report the average performance of on a test set. Bag-of-words OLogReg. To prevent overfitting we search for the best regularization parameter among the following values of λ: 10i, from 10−4 to 104. The lowest negative log-likelihood value on the validation set is attained for8 λ = 0.1. With this value of λ fixed, the final model is the one with the lowest negative loglikelihood on the training set. 7We ignored low-intensity phrases similar to (Choi and Cardie, 2008; Nakagawa et al., 2010). 8We pick single a that gives best average validation set performance, and then use it to compute the average test set performance. ⎛1 0 wx1 Z = ⎝0 1 0 0 0 1 ⎞ ⎠ 0 0 1 ⎛uT 0 1 0 x1 1 0 w + wx2 178 Method Ranking loss PRank 0.7808 Bag-of-words OLogReg 0.6665 Matrix-space OLogReg+RandInit 0.7417 Matrix-space OLogReg+BowInit 0.6375† Table 2: Ranking loss for vector-space Ordered Logistic Regression and Matrix-Space Logistic Regression. † Stands for a significant difference w.r.t. the Bag-OfWords OLogReg model with p-value less than 0.001 (p &lt; 0.001) Matrix-space OLogReg+RandInit. First, we initi</context>
<context position="35101" citStr="Nakagawa et al. (2010)" startWordPosition="6022" endWordPosition="6025">ization problem is nonconvex, the initialization has to be done carefully. Here the weights learned in bag-of-words model come to rescue and provide good initial point for optimization procedure. The final model outperforms the bag-of-words based model, which suggests that this research direction is very promising. Though in our model the order of composition is the same as the word order, we believe that a linguistically informed order of composition can give us further performance gains. For example, one can use the output of a dependency parser to guide the order of composition, similar to Nakagawa et al. (2010). Another possibility for improvement is to use the information about the scope of negation. In the current work we assume the scope of negation to be the expression following the negation; in reality, however, determining the scope of negation is a complex linguistic phenomenon (Moilanen and Pulman, 2007). So the proposed model can benefit from identifying the scope of negation, similar to (Councill et al., 2010). Also we plan to consider other ways to initialize the matrix-space model. One interesting direction to explore might be to use non-negative matrix factorization (Lee and Seung, 2001</context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency tree-based sentiment classification using crfs with hidden variables. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="31798" citStr="Pang and Lee, 2005" startWordPosition="5500" endWordPosition="5503">ties of its words, they learn how to model the interactions between words with headmodifier relations in the dependency tree. Some of the previous work looked at MPQA phrase-level classification. Wilson et al. (2004) tackles the problem of classifying clauses according to their subjective strength but not polarity; Wilson et al. (2005) classifies phrases according to their polarity/sentiment but not strength. Our task is different: we classify phrases according to a single ordinal scale that combines both polarity and strength. Task of predicting document-level star ratings was considered in (Pang and Lee, 2005; Goldberg and Zhu, 2006). In the current work we look at finegrained sentiment analysis, more specifically we study word representations for use in true compositional semantic settings. Distributional Semantics and Compositionality. Research in the area of distributional semantics in NLP and Cognitive Science has looked at different word representations and different ways of combining words. Mitchell and Lapata (2010) propose a framework for vector-based semantic composition. They define composition as an additive or multiplicative function of two vectors and show that compositional approache</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="1760" citStr="Pang and Lee (2008)" startWordPosition="255" endWordPosition="258">to be done carefully: the optimization is nonconvex and requires a good initial starting point. This paper presents the first such algorithm for learning a matrix-space model for semantic composition. In the context of the phrase-level sentiment analysis task, our experimental results show statistically significant improvements in performance over a bagof-words model. 1 Introduction Sentiment analysis has been an active research area in recent years. Work in the area ranges from identifying the sentiment of individual words to determining the sentiment of phrases, sentences and documents (see Pang and Lee (2008) for a survey). The bulk of previous research, however, models just positive vs. negative sentiment, collapsing positive (or negative) words, phrases and documents of differing intensities into just one positive (or negative) class. For word-level sentiment, therefore, these methods would not recognize a difference in sentiment between words like “good” and “great”, which have the same direction of polarity (i.e., positive) but different intensities. At the phrase level, the methods will fail to register compositional effects in sentiment brought about by intensifiers like “very”, “absolutely”</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K B Petersen</author>
<author>M S Pedersen</author>
</authors>
<date>2008</date>
<journal>Version</journal>
<pages>20081110</pages>
<institution>The Matrix Cookbook. ”Technical University of Denmark”,</institution>
<marker>Petersen, Pedersen, 2008</marker>
<rawString>K. B. Petersen and M. S. Pedersen. ”2008”. The Matrix Cookbook. ”Technical University of Denmark”, ”oct”. ”Version 20081110”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Annie Zaenen</author>
</authors>
<title>Contextual lexical valence shifters.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.</booktitle>
<contexts>
<context position="5926" citStr="Polanyi and Zaenen (2004)" startWordPosition="900" endWordPosition="903">rden. Here, the verbs prevent and ease act as content-word negators (Choi and Cardie, 2008) in that they modify the negative sentiment of their direct object arguments so that the phrase as a whole is perceived as somewhat positive. Nonetheless, the vast majority of methods for phrase- and sentence-level sentiment analysis do not tackle the task compositionally: they, instead, employ a bag-of-words representation and, at best, incorporate additional features to account for negators, intensifiers, and for contextual valence shifters, which can change the sentiment over neighboring words (e.g., Polanyi and Zaenen (2004), Wilson et al. (2005) , Kennedy and Inkpen (2006), Shaikh et al. (2007)). One notable exception is Moilanen and Pulman (2007), who propose a compositional semantic approach to assign a positive or negative sentiment to newspaper article titles. However, their knowledgebased approach presupposes the existence of a sentiment lexicon and a set of symbolic compositional rules. But learning-based compositional approaches for sentiment analyis also exist. Choi and Cardie (2008), for example, propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sent</context>
</contexts>
<marker>Polanyi, Zaenen, 2004</marker>
<rawString>Livia Polanyi and Annie Zaenen. 2004. Contextual lexical valence shifters. In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Semisupervised polarity lexicon induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>675--682</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="29674" citStr="Rao and Ravichandran, 2009" startWordPosition="5156" endWordPosition="5159">e case of “bad”. Also notice that since in the matrix-space model 9See the detailed discussion in Taboada et al. (2011) and Liu and Seneff (2009). 179 each word is represented as a function, more specifically a linear operator, and the function composition defined as matrix multiplication, we can think of ”not very” being an operator itself, that is a composition of operator ”not” and operator ”very”. 5 Related Work Sentiment Analysis. There has been a lot of research in determining the sentiment of words and constructing polarity dictionaries (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Rao and Ravichandran, 2009; Mohammad et al., 2009; Velikovich et al., 2010). Some recent work is trying to identify the degree of sentiment of adjectives and adverbs from text using co-occurrence statistics. Work by Taboada et. al (2011) and Liu and Seneff (2009), suggest ways of computing the sentiment of adjectives from data, and computing the effect of combining adjective with adverb as multiplicative effect and combining adjective with negation as additive effect. However these models require the knowledge of a part of speech of given words and the list of negators (since the negator is an adjective as well). In ou</context>
</contexts>
<marker>Rao, Ravichandran, 2009</marker>
<rawString>Delip Rao and Deepak Ravichandran. 2009. Semisupervised polarity lexicon induction. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 675–682, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Rudolph</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Compositional matrix-space models of language.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>907--916</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1114" citStr="Rudolph and Giesbrecht, 2010" startWordPosition="152" endWordPosition="155">or accurate assignment of phrase-level sentiment. For example, combining an adverb (e.g., “very”) with a positive polar adjective (e.g., “good”) produces a phrase (“very good”) with increased polarity over the adjective alone. Inspired by recent work on distributional approaches to compositionality, we model each word as a matrix and combine words using iterated matrix multiplication, which allows for the modeling of both additive and multiplicative semantic effects. Although the multiplication-based matrix-space framework has been shown to be a theoretically elegant way to model composition (Rudolph and Giesbrecht, 2010), training such models has to be done carefully: the optimization is nonconvex and requires a good initial starting point. This paper presents the first such algorithm for learning a matrix-space model for semantic composition. In the context of the phrase-level sentiment analysis task, our experimental results show statistically significant improvements in performance over a bagof-words model. 1 Introduction Sentiment analysis has been an active research area in recent years. Work in the area ranges from identifying the sentiment of individual words to determining the sentiment of phrases, se</context>
<context position="7511" citStr="Rudolph and Giesbrecht (2010)" startWordPosition="1138" endWordPosition="1141">of compositional effects. Each of the above, however, uses a binary rather than an ordinal sentiment scale. In contrast, our proposed method for phraselevel sentiment analysis is inspired by recent work on distributional approaches to compositionality. In particular, Baroni and Zamparelli (2010) tackle adjective-noun compositions using a vector representation for nouns and learning a matrix representation for each adjective. The adjective matrices are then applied as functions over the meanings of nouns — via matrix-vector multiplication — to derive the meaning of adjective-noun combinations. Rudolph and Giesbrecht (2010) show theoretically, that multiplicative matrix-space models are a general case of vector-space models and furthermore exhibit desirable properties for semantic analysis: they take into account word order and are algebraically, neurologically and psychologically plausible. This work, however, does not present an algorithm for learning such models; nor does it provide empirical evidence in favor of matrix-space models over vector-space models. In the sections below, we propose a learningbased approach to assign ordinal sentiment scores to sentiment-bearing phrases using a general compositional </context>
<context position="22142" citStr="Rudolph and Giesbrecht, 2010" startWordPosition="3854" endWordPosition="3857">ifferent random matrices reaches different local minima and the quality of local minima depends on initialization. Therefore, it is important to initialize the model with a good initial point. One way to initialize the Matrix-Space model is to use the weights learned by the bag-of-words model. We use the following intuition for initializing the Matrix-Space model. As noted in Section 2.2.1 applying transformation A of affine matrix W can model a linear pute is ∂L = ∂wxi j = λw 177 transformation, while vector b represents a translation. Since matrix-space model can encode a vectorspace model (Rudolph and Giesbrecht, 2010), we can initialize the matrices to exactly mimic the bagof-words model. In order to do that we place the weight, learned by the bag-of-words model in the first component of b. Let’s assume that wx1 and wx2 are the weights learned for two distinct words x1 and x2 respectively. To compute the polarity score of a phrase x1, x2 the bag-of-words model sums the weights of these two words: wx1 and wx2. Now we want to have the same effect in matrix-space model. Here we assume m = 3. ⎞ ⎛ ⎞ 1 0 wx2 ⎠⎝0 1 0 ⎠ 0 0 1 ⎛1 0 wx1 + wx2 = ⎝0 1 0 0 0 1 Finally, there is a step of mapping matrix Z to a number us</context>
<context position="33161" citStr="Rudolph and Giesbrecht (2010)" startWordPosition="5709" endWordPosition="5713">parelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. It shows that modeling adjectives as linear transformations and applying those linear transformations to nouns results in final vectors for adjective-noun compositions that are close in semantic space to other similar phrases. The authors argue that modeling adjectives as a linear transformation is a better idea than using additive vector-space models. In this work, a separate matrix for each adjective is learned using the Partial Least Squares method in a completely unsupervised way. The recent paper by Rudolph and Giesbrecht (2010), described in the introduction, argues for multiplicative matrix-space models. In contrast to other work in this area, our work is concerned with a specific dimension of word meaning — sentiment. Our techniques, however, are quite general and should be applicable to other problems in lexical semantics. 6 Conclusions and Future work In the current work we present a novel matrix-space model for ordinal scale sentiment prediction and an algorithm for learning such a model. The proposed 180 model learns a matrix for each word; the composition of words is modeled as iterated matrix multiplication.</context>
</contexts>
<marker>Rudolph, Giesbrecht, 2010</marker>
<rawString>Sebastian Rudolph and Eugenie Giesbrecht. 2010. Compositional matrix-space models of language. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 907– 916, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mostafa Shaikh</author>
<author>Helmut Prendinger</author>
<author>Ishizuka Mitsuru</author>
</authors>
<title>Assessing sentiment of text by semantic dependency and contextual valence analysis.</title>
<date>2007</date>
<contexts>
<context position="5998" citStr="Shaikh et al. (2007)" startWordPosition="913" endWordPosition="916"> Cardie, 2008) in that they modify the negative sentiment of their direct object arguments so that the phrase as a whole is perceived as somewhat positive. Nonetheless, the vast majority of methods for phrase- and sentence-level sentiment analysis do not tackle the task compositionally: they, instead, employ a bag-of-words representation and, at best, incorporate additional features to account for negators, intensifiers, and for contextual valence shifters, which can change the sentiment over neighboring words (e.g., Polanyi and Zaenen (2004), Wilson et al. (2005) , Kennedy and Inkpen (2006), Shaikh et al. (2007)). One notable exception is Moilanen and Pulman (2007), who propose a compositional semantic approach to assign a positive or negative sentiment to newspaper article titles. However, their knowledgebased approach presupposes the existence of a sentiment lexicon and a set of symbolic compositional rules. But learning-based compositional approaches for sentiment analyis also exist. Choi and Cardie (2008), for example, propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis decision variables given the a priori (i.e., out of conte</context>
</contexts>
<marker>Shaikh, Prendinger, Mitsuru, 2007</marker>
<rawString>Mostafa Shaikh, Helmut Prendinger, and Ishizuka Mitsuru. 2007. Assessing sentiment of text by semantic dependency and contextual valence analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloskiy</author>
<author>Kimberly Vollz</author>
</authors>
<title>Lexicon-based methods for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="4650" citStr="Taboada et al., 2011" startWordPosition="696" endWordPosition="699"> as more negative than the original adjective. Thus, it is convenient to think of the effect of combining an intensifying adverb with a polar adjective as being multiplicative in nature, if we assume the adjectives (“good” and “bad”) to have positive and a negative sentiment scores, respectively. Next, let us consider adverbial negators like “not” combined with polar adjectives. When modeling only positive and negative labels for sentiment, negators are generally treated as flipping the polarity of the adjective it modifies (Choi and Cardie, 2008; Nakagawa et al., 2010). However, recent work (Taboada et al., 2011; Liu and Seneff, 2009) suggests that the effect of the negator when ordinal sentiment scores are employed is more akin to dampening the adjective’s polarity rather than flipping it. For example, if “perfect” has a strong positive sentiment, then the phrase “not perfect” is still positive, though to a lesser degree. And while “not terrible” is still negative, it is less negative than “terrible”. For these cases, it is convenient to view “not” as shifting polarity to the opposite side of polarity scale by some value. There are, of course, more interesting examples of compositional semantic effe</context>
<context position="29167" citStr="Taboada et al. (2011)" startWordPosition="5074" endWordPosition="5077">es. The matrix-space model correctly encodes the effect of the negator for both positive and negative adjectives, such that (not good) &lt; (good) and (bad) &lt; (not bad). For the interesting case of applying a negator to a phrase with an intensifier, (not good) should be less than (not very good) and (not very bad) should be less than (not bad).9 As shown in Table 3, these are predicted correctly by the matrixspace model, which the matrix-space model gets right, but the bag-of-words model misses in the case of “bad”. Also notice that since in the matrix-space model 9See the detailed discussion in Taboada et al. (2011) and Liu and Seneff (2009). 179 each word is represented as a function, more specifically a linear operator, and the function composition defined as matrix multiplication, we can think of ”not very” being an operator itself, that is a composition of operator ”not” and operator ”very”. 5 Related Work Sentiment Analysis. There has been a lot of research in determining the sentiment of words and constructing polarity dictionaries (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Rao and Ravichandran, 2009; Mohammad et al., 2009; Velikovich et al., 2010). Some recent work is trying to identify the</context>
</contexts>
<marker>Taboada, Brooke, Tofiloskiy, Vollz, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloskiy, and Kimberly Vollz. 2011). Lexicon-based methods for sentiment analysis. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Velikovich</author>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
</authors>
<title>The viability of webderived polarity lexicons.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>777--785</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="29723" citStr="Velikovich et al., 2010" startWordPosition="5164" endWordPosition="5167">ix-space model 9See the detailed discussion in Taboada et al. (2011) and Liu and Seneff (2009). 179 each word is represented as a function, more specifically a linear operator, and the function composition defined as matrix multiplication, we can think of ”not very” being an operator itself, that is a composition of operator ”not” and operator ”very”. 5 Related Work Sentiment Analysis. There has been a lot of research in determining the sentiment of words and constructing polarity dictionaries (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Rao and Ravichandran, 2009; Mohammad et al., 2009; Velikovich et al., 2010). Some recent work is trying to identify the degree of sentiment of adjectives and adverbs from text using co-occurrence statistics. Work by Taboada et. al (2011) and Liu and Seneff (2009), suggest ways of computing the sentiment of adjectives from data, and computing the effect of combining adjective with adverb as multiplicative effect and combining adjective with negation as additive effect. However these models require the knowledge of a part of speech of given words and the list of negators (since the negator is an adjective as well). In our work we propose a single unified model for hand</context>
</contexts>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, 2010</marker>
<rawString>Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of webderived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 777–785, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<booktitle>Language Resources and Evaluation (formerly Computers and the Humanities),</booktitle>
<pages>39--2</pages>
<contexts>
<context position="8708" citStr="Wiebe et al., 2005" startWordPosition="1316" endWordPosition="1319">general compositional matrix-space model of language. In contrast to previous work, all words are modeled as matrices, independent of their part-of-speech, and compositional inference is uniformly modeled as ma173 trix multiplication. To predict an ordinal scale sentiment value, we employ Ordered Logistic Regression, introducing a novel training algorithm to accommodate our compositional matrix-space representations (Section 2). To our knowledge, this is the first such algorithm for learning matrix-space models for semantic composition. We evaluate the approach on a standard sentiment corpus (Wiebe et al., 2005) (Section 3), making use of its manually annotated phrase-level annotations for polarity and intensity, and compare our approach to the more commonly employed bag-of-words model. We show (Section 4) that our matrix-space model significantly outperforms a bag-of-words model for the ordinal scale sentiment prediction task. 2 The Model for Ordinal Scale Sentiment Prediction As described above, our task is to predict an ordinal scale sentiment value for a phrase. To this end, we employ a sentiment scale with five ordinal values: VERY NEGATIVE, NEGATIVE, NEUTRAL, POSITIVE and VERY POSITIVE. Given a</context>
<context position="23246" citStr="Wiebe et al., 2005" startWordPosition="4078" endWordPosition="4081">⎞ 1 0 wx2 ⎠⎝0 1 0 ⎠ 0 0 1 ⎛1 0 wx1 + wx2 = ⎝0 1 0 0 0 1 Finally, there is a step of mapping matrix Z to a number using u and v, such that ξ(Z) = wx1 + wx2.We also want vector u and v to be such that: ⎞ ⎠v = wx1 + wx2 (8) The last equation can help us construct u and v. We also set u and v to be orthogonal: uT v = 0. So, we arbitrarily choose two orthogonal vectors for which equation (8) holds: u = [1, .\/2,1]T and v = [1, −.\/2, 1]T.5 3 Experimental Methodology For experimental evaluation of the proposed method we use the publicly available Multi-Perspective Question Answering (MPQA)6 corpus (Wiebe et al., 2005) version 1.2, which contains 535 newswire documents that are manually annotated with phraselevel subjectivity and intensity. We use the expression-level boundary markings in MPQA to extract phrases. We evaluate on positive, negative and neutral opinion expressions that have intensities 5If m &gt; 3, u and v can be set using the same intuition. 6http://www.cs.pitt.edu/mpqa/ Polarity Intensity Ordinal label negative high, extreme 0 negative medium 1 neutral high, extreme, medium 2 positive medium 3 positive high, extreme 4 Table 1: Mapping of combination of polarities and intensities from MPQA data</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation (formerly Computers and the Humanities), 39(2/3):164– 210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In In AAAI,</booktitle>
<pages>735--740</pages>
<contexts>
<context position="29646" citStr="Wiebe, 2000" startWordPosition="5154" endWordPosition="5155"> misses in the case of “bad”. Also notice that since in the matrix-space model 9See the detailed discussion in Taboada et al. (2011) and Liu and Seneff (2009). 179 each word is represented as a function, more specifically a linear operator, and the function composition defined as matrix multiplication, we can think of ”not very” being an operator itself, that is a composition of operator ”not” and operator ”very”. 5 Related Work Sentiment Analysis. There has been a lot of research in determining the sentiment of words and constructing polarity dictionaries (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Rao and Ravichandran, 2009; Mohammad et al., 2009; Velikovich et al., 2010). Some recent work is trying to identify the degree of sentiment of adjectives and adverbs from text using co-occurrence statistics. Work by Taboada et. al (2011) and Liu and Seneff (2009), suggest ways of computing the sentiment of adjectives from data, and computing the effect of combining adjective with adverb as multiplicative effect and combining adjective with negation as additive effect. However these models require the knowledge of a part of speech of given words and the list of negators (since the negator is </context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>Janyce M. Wiebe. 2000. Learning subjective adjectives from corpora. In In AAAI, pages 735–740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Rebecca Hwa</author>
</authors>
<title>Just how mad are you?</title>
<date>2004</date>
<booktitle>In AAAI.</booktitle>
<publisher>AAAI.</publisher>
<contexts>
<context position="31396" citStr="Wilson et al. (2004)" startWordPosition="5438" endWordPosition="5441">tries to model the compositional semantics of combining different words is Nakagawa et. al. (2010), which proposes a model that learns the effects of combining different words using phrase/sentence dependency parse trees and an initial polarity dictionary. They present a learning method that employs hidden variables for sentiment classification: given the polarity of a sentence and the a priori polarities of its words, they learn how to model the interactions between words with headmodifier relations in the dependency tree. Some of the previous work looked at MPQA phrase-level classification. Wilson et al. (2004) tackles the problem of classifying clauses according to their subjective strength but not polarity; Wilson et al. (2005) classifies phrases according to their polarity/sentiment but not strength. Our task is different: we classify phrases according to a single ordinal scale that combines both polarity and strength. Task of predicting document-level star ratings was considered in (Pang and Lee, 2005; Goldberg and Zhu, 2006). In the current work we look at finegrained sentiment analysis, more specifically we study word representations for use in true compositional semantic settings. Distributio</context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004. Just how mad are you? In AAAI. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5948" citStr="Wilson et al. (2005)" startWordPosition="904" endWordPosition="907">nt and ease act as content-word negators (Choi and Cardie, 2008) in that they modify the negative sentiment of their direct object arguments so that the phrase as a whole is perceived as somewhat positive. Nonetheless, the vast majority of methods for phrase- and sentence-level sentiment analysis do not tackle the task compositionally: they, instead, employ a bag-of-words representation and, at best, incorporate additional features to account for negators, intensifiers, and for contextual valence shifters, which can change the sentiment over neighboring words (e.g., Polanyi and Zaenen (2004), Wilson et al. (2005) , Kennedy and Inkpen (2006), Shaikh et al. (2007)). One notable exception is Moilanen and Pulman (2007), who propose a compositional semantic approach to assign a positive or negative sentiment to newspaper article titles. However, their knowledgebased approach presupposes the existence of a sentiment lexicon and a set of symbolic compositional rules. But learning-based compositional approaches for sentiment analyis also exist. Choi and Cardie (2008), for example, propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis decisio</context>
<context position="31517" citStr="Wilson et al. (2005)" startWordPosition="5457" endWordPosition="5460">l that learns the effects of combining different words using phrase/sentence dependency parse trees and an initial polarity dictionary. They present a learning method that employs hidden variables for sentiment classification: given the polarity of a sentence and the a priori polarities of its words, they learn how to model the interactions between words with headmodifier relations in the dependency tree. Some of the previous work looked at MPQA phrase-level classification. Wilson et al. (2004) tackles the problem of classifying clauses according to their subjective strength but not polarity; Wilson et al. (2005) classifies phrases according to their polarity/sentiment but not strength. Our task is different: we classify phrases according to a single ordinal scale that combines both polarity and strength. Task of predicting document-level star ratings was considered in (Pang and Lee, 2005; Goldberg and Zhu, 2006). In the current work we look at finegrained sentiment analysis, more specifically we study word representations for use in true compositional semantic settings. Distributional Semantics and Compositionality. Research in the area of distributional semantics in NLP and Cognitive Science has loo</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>