<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056472">
<title confidence="0.998185">
Turn-Taking Cues in a Human Tutoring Corpus
</title>
<author confidence="0.997284">
Heather Friedberg
</author>
<affiliation confidence="0.998865">
Department of Computer Science
University of Pittsburgh
</affiliation>
<address confidence="0.546455">
Pittsburgh, PA, 15260, USA
</address>
<email confidence="0.999436">
friedberg@cs.pitt.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999824642857143">
Most spoken dialogue systems are still
lacking in their ability to accurately model
the complex process that is human turn-
taking. This research analyzes a human-
human tutoring corpus in order to identify
prosodic turn-taking cues, with the hopes
that they can be used by intelligent tutoring
systems to predict student turn boundaries.
Results show that while there was variation
between subjects, three features were sig-
nificant turn-yielding cues overall. In addi-
tion, a positive relationship between the
number of cues present and the probability
of a turn yield was demonstrated.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988975">
Human conversation is a seemingly simple, every-
day phenomenon that requires a complex mental
process of turn-taking, in which participants man-
age to yield and hold the floor with little pause in-
between speaking turns. Most linguists subscribe
to the idea that this process is governed by a sub-
conscious internal mechanism, that is, a set of cues
or rules that steers humans toward proper turn-
taking (Duncan, 1972). These cues may include
lexical features such as the words used to end the
turn, or prosodic features such as speaking rate,
pitch, and intensity (Cutler and Pearson, 1986).
While successful turn-taking is fairly easy for
humans to accomplish, it is still difficult for mod-
els to be implemented in spoken dialogue sys-
tems. Many systems use a set time-out to decide
</bodyText>
<page confidence="0.990669">
94
</page>
<bodyText confidence="0.999632428571428">
when a user is finished speaking, often resulting in
unnaturally long pauses or awkward overlaps
(Ward, et. al., 2005). Others detect when a user
interrupts the system, known as “barge-in”, though
this is characteristic of failed turn-taking rather
than successful conversation (Glass, 1999).
Improper turn-taking can often be a source of us-
er discomfort and dissatisfaction with a spoken
dialogue system. Little work has been done to
study turn-taking in tutoring, so we hope to inves-
tigate it further while using a human-human (HH)
tutoring corpus and language technologies to ex-
tract useful information about turn-taking cues.
This analysis is particularly interesting in a tutor-
ing domain because of the speculated unequal sta-
tuses of participants. The goal is to eventually
develop a model for turn-taking based on this anal-
ysis which can be implemented in an existent tutor-
ing system, ITSPOKE, an intelligent tutor for
college-level Newtonian physics (Litman and Sil-
liman, 2004). ITSPOKE currently uses a time-out
to determine the end of a student turn and does not
recognize student barge-in. We hypothesize that
improving upon the turn-taking model this system
uses will help engage students and hopefully lead
to increased student learning, a standard perfor-
mance measure of intelligent tutoring systems
(Litman et. al., 2006).
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999575">
Turn-taking has been a recent focus in spoken di-
alogue system work, with research producing
many different models and approaches. Raux and
Eskenazi (2009) proposed a finite-state turn-taking
</bodyText>
<subsectionHeader confidence="0.377071">
Proceedings of the ACL-HLT 2011 Student Session, pages 94–98,
</subsectionHeader>
<bodyText confidence="0.993579138888889">
Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics
model, which is used to predict end-of-turn and
performed significantly better than a fixed-
threshold baseline in reducing endpointing latency
in a spoken dialogue system. Selfridge and Hee-
man (2010) took a different approach and pre-
sented a bidding model for turn-taking, in which
dialogue participants compete for the turn based on
the importance of what they will say next.
Of considerable inspiration to the research in this
paper was Gravano and Hirschberg’s (2009) analy-
sis of their games corpus, which showed that it was
possible for turn-yielding cues to be identified in
an HH corpus. A similar method was used in this
analysis, though it was adapted based on the tools
and data that were readily available for our corpus.
Since these differences may prevent direct compar-
ison between corpora, future work will focus on
making our method more analogous.
Since our work is similar to that done by Grava-
no and Hirschberg (2009), we hypothesize that
turn-yielding cues can also be identified in our HH
tutoring corpus. However, it is possible that the
cues identified will be very different, due to factors
specific to a tutoring environment. These include,
but are not limited to, status differences between
the student and tutor, engagement of the student,
and the different goals of the student and tutor.
Our hypothesis is that for certain prosodic fea-
tures, there will be a significant difference between
places where students yield their turn (allow the
tutor to speak) and places where they hold it (con-
tinue talking). This would designate these features
as turn-taking cues, and would allow them to be
used as features in a turn-taking model for a spo-
ken dialogue system in the future.
</bodyText>
<sectionHeader confidence="0.978925" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999918814814815">
The data for this analysis is from an HH tutoring
corpus recorded during the 2002-2003 school
year. This is an audio corpus of 17 university stu-
dents, all native Standard English speakers, work-
ing with a tutor (the same for all subjects) on
physics problems (Litman et. al., 2006). Both the
student and the tutor were sitting in front of sepa-
rate work stations, so they could communicate only
through microphones or, in the case of a student-
written essay, through the shared computer envi-
ronment. Any potential turn-taking cues that the
tutor received from the student were very compa-
rable to what a spoken dialogue system would have
to analyze during a user interaction.
For each participant, student speech was iso-
lated and segmented into breath groups. A breath
group is defined as any segment of speech by one
dialogue participant bounded by 200 ms of silence
or more based on a certain threshold of intensity
(Liscombe et. al., 2005). This break-down allowed
for feature measurement and comparison at places
that were and were not turn boundaries. Although
Gravano and Hirschberg (2009) segmented their
corpus by 50 ms of silence, we used 200 ms to di-
vide the breath groups, as this data had already
been calculated for another experiment done with
the HH corpus (Liscombe et. al., 2005). 1
</bodyText>
<figureCaption confidence="0.998675">
Figure 1. Conversation Segmented into Breath Groups
</figureCaption>
<bodyText confidence="0.99995656">
Each breath group was automatically labeled as
one of the following: HOLD, when a breath group
was immediately followed by a second breath
group from the same person, YIELD, when a
breath group was immediately followed by speech
from the other participant, or OVERLAP, when
speech from another participant started before the
current one ended. Figure 1 is a diagram of a hy-
pothetical conversation between two participants,
with examples of HOLD’s and YIELD’s labeled.
These groups were determined strictly by time and
not by the actually speech being spoken. Speech
acts such as backchannels, then, would be included
in the YIELD group if they were spoken during
clear gaps in the tutor’s speech, but would be
placed in the OVERLAP group if they occurred
during or overlapping with tutor speech. There
were 9,169 total HOLD&apos;s in the corpus and 4,773
YIELD’s; these were used for comparison, while
the OVERLAP’s were set aside for future work.
Four prosodic features were calculated for each
breath group: duration, pitch, RMS, and percent
silence. Duration is the length of the breath group
in seconds. Pitch is the mean fundamental fre-
quency (f0) of the speech. RMS (the root mean
</bodyText>
<footnote confidence="0.9906445">
1 Many thanks to the researchers at Columbia University for
providing the breath group data for this corpus.
</footnote>
<page confidence="0.987035">
95
</page>
<table confidence="0.996794">
N duration percent silence pitch RMS
HOLD Group Mean 993 1.07 0.34 102.24 165.27
YIELD Group Mean 480 0.78 0.39 114.87 138.89
Significance * p = 0.018 * p &lt; 0.001 * p &lt; 0.001 * p &lt; 0.001
</table>
<tableCaption confidence="0.919025">
Table 1. Individual Results for Subject 111
* denotes a significant p value
</tableCaption>
<table confidence="0.998035">
N duration percent silence pitch RMS
HOLD Group Mean 17 1.49 0.300 140.44 418.00
YIELD Group Mean 17 0.82 0.310 147.58 354.65
Significance * p = 0.022 p = 0.590 * p = 0.009 * p &lt; 0.001
</table>
<tableCaption confidence="0.999894">
Table 2. Results from Paired T-Test
</tableCaption>
<bodyText confidence="0.9998999">
squared amplitude) is the energy or loudness. Per-
cent silence was the amount of internal silence
within the breath group. For pitch and RMS, the
mean was taken over the length of the breath
group. These features were used because they are
similar to those used by Gravano and Hirschberg
(2009), and are already used in the spoken dialo-
gue system we will be using (Forbes-Riley and
Litman, 2011). While only a small set of features
is examined here, future work will include expand-
ing the feature set.
Mean values for each feature for HOLD’s and
YIELD’s were calculated and compared using the
student T-test in SPSS Statistics software. Two
separate tests were done, one to compare the
means for each student individually, and one to
compare the means across all students. p ≤ .05 is
considered significant for all statistical tests. The
p-values given are the probability of obtaining the
difference between groups by chance.
</bodyText>
<sectionHeader confidence="0.99999" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.991408">
4.1 Individual Cues
</subsectionHeader>
<bodyText confidence="0.999324045454545">
First, means for each feature for HOLD’s and
YIELD’s were compared for each subject indivi-
dually. These individual results indicated that
while turn-taking cues could be identified, there
was much variation between students. Table 1
displays the results of the analysis for one subject,
student 111. For this student, all four prosodic fea-
tures are turn-taking cues, as there is a significant
different between the HOLD and YIELD groups
for all of them. However, for all other students,
this was not the case. As shown in Table 3, mul-
tiple significant cues could be identified for most
students, and there was only one which appeared to
have no significant turn-yielding cues.
Because there was so much individual variation,
a paired T-test was used to compare the means
across subjects. In this analysis, duration, pitch,
and RMS were all found to be significant cues.
Percent silence, however, was not. The results of
this test are summarized in Table 2. A more de-
tailed look at each of the three significant cues is
done below.
</bodyText>
<figure confidence="0.955161">
Number of Number of
Significant Cues Students
0 1
1 0
2 6
3 9
4 1
</figure>
<tableCaption confidence="0.829654">
Table 3. Number of Students with
Significant Cues
</tableCaption>
<bodyText confidence="0.999622692307692">
Duration: The mean duration for HOLD’s is
longer than the mean duration for YIELD’s. This
suggests that students speak for a longer uninter-
rupted time when they are trying to hold their turn,
and yield their turns with shorter utterances. This is
the opposite of Gravano and Hirschberg’s (2009)
results, which found that YIELD’s were longer.
Pitch: The mean pitch for YIELD’s is higher
than the mean pitch for HOLD’s. Gravano and
Hirschberg (2009), on the other hand, found that
YIELD’s were lower pitched than HOLD’s. This
difference may be accounted for by the difference
in tasks. During tutoring, students are possibly
</bodyText>
<page confidence="0.980607">
96
</page>
<bodyText confidence="0.999829777777778">
more uncertain, which may raise the mean pitch of
the YIELD breath groups.
RMS: The mean RMS, or energy, for HOLD’s is
higher than the mean energy for YIELD’s. This is
consistent with student’s speaking more softly, i.e.,
trailing off, at the end of their turn, a usual pheno-
menon in human speech. This is consistent with
the results from the Columbia games corpus (Gra-
vano and Hirshberg, 2009).
</bodyText>
<subsectionHeader confidence="0.99658">
4.2 Combining Cues
</subsectionHeader>
<bodyText confidence="0.99973308">
Gravano and Hirschberg (2009) were able to show
using their cues and corpus that there is a positive
relationship between the number of turn-yielding
cues present and the probability of a turn actually
being taken. This suggests that in order to make
sure that the other participant is aware whether the
turn is going to continue or end, the speaker may
subconsciously give them more information
through multiple cues.
To see whether this relationship existed in our
data, each breath group was marked with a binary
value for each significant cue, representing wheth-
er the cue was present or not present within that
breath group. A cue was considered present if the
value for that breath group was strictly closer to
the student’s mean for YIELD’s than
HOLD’s. The number of cues present for each
breath group was totaled. Only the three cues
found to be significant cues were used for these
calculations. For each number of cues possible x
(0 to 3, inclusively), the probability of the turn be-
ing taken was calculated by p(x) = Y / T, where Y is
the number of YIELD’s with x cues present, and T
is the total number of breath groups with x cues
present.
</bodyText>
<table confidence="0.754414">
100.00%
90.00%
80.00%
70.00%
60.00%
0 1 2 3
</table>
<figureCaption confidence="0.999089">
Figure 2. Cues Present v. Probability of YIELD
</figureCaption>
<bodyText confidence="0.999961571428571">
According to these results, a positive relationship
seems to exist for these cues and this corpus. Fig-
ure 2 shows the results plotted with a fitted regres-
sion. The number of cues present and probability
of a turn yield is strongly correlated (r = .923,
p=.038). A regression analysis done using SPSS
showed that the adjusted r2 = .779 (p = .077).
When no turn-yielding cues are present, there is
still a majority chance that the student will yield
their turn; however, this is understandable due to
the small number of cues being analyzed. Regard-
less, this gives a very preliminary support for the
idea that it is possible to predict when a turn will
be taken based on the number of cues present.
</bodyText>
<sectionHeader confidence="0.995959" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999891">
This paper presented preliminary work in using an
HH tutoring corpus to construct a turn-taking mod-
el that can later be implemented in a spoken dialo-
gue system. A small set of prosodic features was
used to try and identify turn-taking cues by com-
paring their values at places where students yielded
their turn to the tutor and places where they held
it. Results show that turn-taking cues such as those
investigated can be identified for the corpus, and
may hold predictive ability for turn boundaries.
</bodyText>
<subsectionHeader confidence="0.951492">
5.1 Future Work
</subsectionHeader>
<bodyText confidence="0.99996444">
When building on this work, there are two differ-
ent directions in which we can go. While this
work uncovers some interesting results in the tutor-
ing domain, there are some shortcomings in the
method that may make it difficult to effectively
evaluate the results. As the breath group is differ-
ent from the segment used in Gravano and Hir-
schberg’s (2009) experiment, and the set of
prosodic features is smaller, direct comparison be-
comes quite difficult. The differences between the
two methods provide enough doubt for the results
to truly be interpreted as contradictory. Thus the
first line of future inquiry is to redo this method
using a smaller silence boundary (50 ms) and dif-
ferent set of prosodic features so that it is truly
comparable to Gravano and Hirschberg’s (2009)
work with the game corpus. This could yield in-
teresting discoveries in the differences between the
two corpora, shedding light on phenomena that are
particular to tutoring scenarios.
On the other hand, other researchers have used
different segments; for example, Clemens and Di-
ekhaus (2009) divide their corpus by “topic units”
that are grammatically and semantically complete.
In addition, Litman et. al. (2009) were able to use
</bodyText>
<page confidence="0.998568">
97
</page>
<bodyText confidence="0.999980409090909">
word-level units to calculate prosody and classify
turn-level uncertainty. Perhaps direct comparison
is not entirely necessary, and instead this work
should be considered an isolated look at an HH
corpus that provides insight into turn-taking, spe-
cifically in tutoring and other domains with un-
equal power levels. Future work in this direction
would include growing the set of features by add-
ing more prosodic ones and introducing lexical
ones such as bi-grams and uni-grams. Already,
work has been done to investigate the features used
in the INTERSPEECH 2009 Emotion Challenge
using openSMILE (Eyben et. al., 2009). When a
large feature bank has been developed, significant
cues will be used in conjunction with machine
learning techniques to build a model for turn-
taking which can be implemented in a spoken di-
alogue tutoring system. The goal would be to learn
more about human turn-taking while seeing if bet-
ter turn-taking by a computer tutor ultimately leads
to increased student learning in an intelligent tutor-
ing system.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999849">
This work was supported by the NSF (#0631930).
I would like to thank Diane Litman, my advisor,
Scott Silliman, for software assistance, Joanna
Drummond, for many helpful comments on this
paper, and the ITSPOKE research group for their
feedback on my work.
</bodyText>
<sectionHeader confidence="0.998515" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999313947368421">
Caroline Clemens and Christoph Diekhaus. 2009. Pro-
sodic turn-yielding Cues with and without optical
Feedback. In Proceedings of the SIGDIAL 2009 Con-
ference: The 10th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue.
Association for Computational Linguistics.
Anne Cutler and Mark Pearson. 1986. On the analysis
of prosodic turn-taking cues. In C. Johns-Lewis, Ed.,
Intonation in Discourse, pp. 139-156. College-Hill.
Starkey Duncan. 1972. Some signals and rules for tak-
ing speaking turns in conversations. Journal of Per-
sonality and Social Psychology, 24(2):283-292.
Florian Eyben, Martin Wöllmer, Björn Schuller. 2010.
openSMILE - The Munich Versatile and Fast Open-
Source Audio Feature Extractor. Proc. ACM Multi-
media (MM), ACM, Florence, Italy. pp. 1459-1462.
James R. Glass. 1999. Challenges for spoken dialogue
systems. In Proceedings of the 1999 IEEE ASRU
Workshop.
Agustín Gravano and Julia Hirschberg. 2009. Turn-
yielding cues in task-oriented dialogue. In Proceed-
ings of the SIGDIAL 2009 Conference: The 10th An-
nual Meeting of the Special Interest Group on
Discourse and Dialogue, 253--261. Association for
Computational Linguistics.
Jackson Liscombe, Julia Hirschberg, and Jennifer J.
Venditti. 2005. Detecting certainness in spoken tu-
torial dialogues. In Interspeech.
Diane J. Litman, Carolyn P. Rose, Kate Forbes-Riley,
Kurt VanLehn, Dumisizwe Bhembe, and Scott Silli-
man. 2006. Spoken Versus Typed Human and Com-
puter Dialogue Tutoring. In International Journal of
Artificial Intelligence in Education, 26: 145-170.
Diane Litman, Mihai Rotaru, and Greg Nicholas.
2009. Classifying Turn-Level Uncertainty Using
Word-Level Prosody. Proceedings Interspeech,
Brighton, UK, September.
Kate Forbes-Riley and Diane Litman. 2011. Benefits
and Challenges of Real-Time Uncertainty Detection
and Adaptation in a Spoken Dialogue Computer Tu-
tor. Speech Communication, in press.
Diane J. Litman and Scott Silliman. 2004. Itspoke: An
intelligent tutoring spoken dialogue system. In
HLT/NAACL.
Antoine Raux and Maxine Eskenazi. 2009. A finite-state
turn-taking model for spoken dialog systems. In
Proc. NAACL/HLT 2009, Boulder, CO, USA.
Ethan O. Selfridge and Peter A. Heeman. 2010. Impor-
tance-Driven Turn-Bidding for spoken dialogue sys-
tems. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL
&apos;10). Association for Computational Linguistics,
Stroudsburg, PA, USA, 177-185.
Nigel Ward, Anais Rivera, Karen Ward, and David G.
Novick. 2005. Root causes of lost time and user
stress in a simple dialog system. In Proceedings of
Interspeech.
</reference>
<page confidence="0.996204">
98
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.972373">
<title confidence="0.999811">Turn-Taking Cues in a Human Tutoring Corpus</title>
<author confidence="0.995856">Heather</author>
<affiliation confidence="0.999922">Department of Computer University of</affiliation>
<address confidence="0.989021">Pittsburgh, PA, 15260, USA</address>
<email confidence="0.999925">friedberg@cs.pitt.edu</email>
<abstract confidence="0.9991648">Most spoken dialogue systems are still lacking in their ability to accurately model the complex process that is human turntaking. This research analyzes a humanhuman tutoring corpus in order to identify prosodic turn-taking cues, with the hopes that they can be used by intelligent tutoring systems to predict student turn boundaries. Results show that while there was variation between subjects, three features were significant turn-yielding cues overall. In addition, a positive relationship between the number of cues present and the probability of a turn yield was demonstrated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Caroline Clemens</author>
<author>Christoph Diekhaus</author>
</authors>
<title>Prosodic turn-yielding Cues with and without optical Feedback.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14767" citStr="Clemens and Diekhaus (2009)" startWordPosition="2471" endWordPosition="2475">difficult. The differences between the two methods provide enough doubt for the results to truly be interpreted as contradictory. Thus the first line of future inquiry is to redo this method using a smaller silence boundary (50 ms) and different set of prosodic features so that it is truly comparable to Gravano and Hirschberg’s (2009) work with the game corpus. This could yield interesting discoveries in the differences between the two corpora, shedding light on phenomena that are particular to tutoring scenarios. On the other hand, other researchers have used different segments; for example, Clemens and Diekhaus (2009) divide their corpus by “topic units” that are grammatically and semantically complete. In addition, Litman et. al. (2009) were able to use 97 word-level units to calculate prosody and classify turn-level uncertainty. Perhaps direct comparison is not entirely necessary, and instead this work should be considered an isolated look at an HH corpus that provides insight into turn-taking, specifically in tutoring and other domains with unequal power levels. Future work in this direction would include growing the set of features by adding more prosodic ones and introducing lexical ones such as bi-gr</context>
</contexts>
<marker>Clemens, Diekhaus, 2009</marker>
<rawString>Caroline Clemens and Christoph Diekhaus. 2009. Prosodic turn-yielding Cues with and without optical Feedback. In Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Cutler</author>
<author>Mark Pearson</author>
</authors>
<title>On the analysis of prosodic turn-taking cues.</title>
<date>1986</date>
<booktitle>In C. Johns-Lewis, Ed., Intonation in Discourse,</booktitle>
<pages>139--156</pages>
<publisher>College-Hill.</publisher>
<contexts>
<context position="1360" citStr="Cutler and Pearson, 1986" startWordPosition="209" endWordPosition="212">yield was demonstrated. 1 Introduction Human conversation is a seemingly simple, everyday phenomenon that requires a complex mental process of turn-taking, in which participants manage to yield and hold the floor with little pause inbetween speaking turns. Most linguists subscribe to the idea that this process is governed by a subconscious internal mechanism, that is, a set of cues or rules that steers humans toward proper turntaking (Duncan, 1972). These cues may include lexical features such as the words used to end the turn, or prosodic features such as speaking rate, pitch, and intensity (Cutler and Pearson, 1986). While successful turn-taking is fairly easy for humans to accomplish, it is still difficult for models to be implemented in spoken dialogue systems. Many systems use a set time-out to decide 94 when a user is finished speaking, often resulting in unnaturally long pauses or awkward overlaps (Ward, et. al., 2005). Others detect when a user interrupts the system, known as “barge-in”, though this is characteristic of failed turn-taking rather than successful conversation (Glass, 1999). Improper turn-taking can often be a source of user discomfort and dissatisfaction with a spoken dialogue system</context>
</contexts>
<marker>Cutler, Pearson, 1986</marker>
<rawString>Anne Cutler and Mark Pearson. 1986. On the analysis of prosodic turn-taking cues. In C. Johns-Lewis, Ed., Intonation in Discourse, pp. 139-156. College-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Starkey Duncan</author>
</authors>
<title>Some signals and rules for taking speaking turns in conversations.</title>
<date>1972</date>
<journal>Journal of Personality and Social Psychology,</journal>
<pages>24--2</pages>
<contexts>
<context position="1187" citStr="Duncan, 1972" startWordPosition="182" endWordPosition="183">hree features were significant turn-yielding cues overall. In addition, a positive relationship between the number of cues present and the probability of a turn yield was demonstrated. 1 Introduction Human conversation is a seemingly simple, everyday phenomenon that requires a complex mental process of turn-taking, in which participants manage to yield and hold the floor with little pause inbetween speaking turns. Most linguists subscribe to the idea that this process is governed by a subconscious internal mechanism, that is, a set of cues or rules that steers humans toward proper turntaking (Duncan, 1972). These cues may include lexical features such as the words used to end the turn, or prosodic features such as speaking rate, pitch, and intensity (Cutler and Pearson, 1986). While successful turn-taking is fairly easy for humans to accomplish, it is still difficult for models to be implemented in spoken dialogue systems. Many systems use a set time-out to decide 94 when a user is finished speaking, often resulting in unnaturally long pauses or awkward overlaps (Ward, et. al., 2005). Others detect when a user interrupts the system, known as “barge-in”, though this is characteristic of failed t</context>
</contexts>
<marker>Duncan, 1972</marker>
<rawString>Starkey Duncan. 1972. Some signals and rules for taking speaking turns in conversations. Journal of Personality and Social Psychology, 24(2):283-292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Eyben</author>
<author>Martin Wöllmer</author>
<author>Björn Schuller</author>
</authors>
<title>openSMILE - The Munich Versatile and Fast OpenSource Audio Feature Extractor.</title>
<date>2010</date>
<booktitle>Proc. ACM Multimedia (MM), ACM,</booktitle>
<pages>1459--1462</pages>
<location>Florence,</location>
<marker>Eyben, Wöllmer, Schuller, 2010</marker>
<rawString>Florian Eyben, Martin Wöllmer, Björn Schuller. 2010. openSMILE - The Munich Versatile and Fast OpenSource Audio Feature Extractor. Proc. ACM Multimedia (MM), ACM, Florence, Italy. pp. 1459-1462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Glass</author>
</authors>
<title>Challenges for spoken dialogue systems.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 IEEE ASRU Workshop.</booktitle>
<contexts>
<context position="1847" citStr="Glass, 1999" startWordPosition="288" endWordPosition="289">s the words used to end the turn, or prosodic features such as speaking rate, pitch, and intensity (Cutler and Pearson, 1986). While successful turn-taking is fairly easy for humans to accomplish, it is still difficult for models to be implemented in spoken dialogue systems. Many systems use a set time-out to decide 94 when a user is finished speaking, often resulting in unnaturally long pauses or awkward overlaps (Ward, et. al., 2005). Others detect when a user interrupts the system, known as “barge-in”, though this is characteristic of failed turn-taking rather than successful conversation (Glass, 1999). Improper turn-taking can often be a source of user discomfort and dissatisfaction with a spoken dialogue system. Little work has been done to study turn-taking in tutoring, so we hope to investigate it further while using a human-human (HH) tutoring corpus and language technologies to extract useful information about turn-taking cues. This analysis is particularly interesting in a tutoring domain because of the speculated unequal statuses of participants. The goal is to eventually develop a model for turn-taking based on this analysis which can be implemented in an existent tutoring system, </context>
</contexts>
<marker>Glass, 1999</marker>
<rawString>James R. Glass. 1999. Challenges for spoken dialogue systems. In Proceedings of the 1999 IEEE ASRU Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Agustín Gravano</author>
<author>Julia Hirschberg</author>
</authors>
<title>Turnyielding cues in task-oriented dialogue.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>253--261</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="4178" citStr="Gravano and Hirschberg (2009)" startWordPosition="656" endWordPosition="660">ompete for the turn based on the importance of what they will say next. Of considerable inspiration to the research in this paper was Gravano and Hirschberg’s (2009) analysis of their games corpus, which showed that it was possible for turn-yielding cues to be identified in an HH corpus. A similar method was used in this analysis, though it was adapted based on the tools and data that were readily available for our corpus. Since these differences may prevent direct comparison between corpora, future work will focus on making our method more analogous. Since our work is similar to that done by Gravano and Hirschberg (2009), we hypothesize that turn-yielding cues can also be identified in our HH tutoring corpus. However, it is possible that the cues identified will be very different, due to factors specific to a tutoring environment. These include, but are not limited to, status differences between the student and tutor, engagement of the student, and the different goals of the student and tutor. Our hypothesis is that for certain prosodic features, there will be a significant difference between places where students yield their turn (allow the tutor to speak) and places where they hold it (continue talking). Th</context>
<context position="6044" citStr="Gravano and Hirschberg (2009)" startWordPosition="969" endWordPosition="972">h the shared computer environment. Any potential turn-taking cues that the tutor received from the student were very comparable to what a spoken dialogue system would have to analyze during a user interaction. For each participant, student speech was isolated and segmented into breath groups. A breath group is defined as any segment of speech by one dialogue participant bounded by 200 ms of silence or more based on a certain threshold of intensity (Liscombe et. al., 2005). This break-down allowed for feature measurement and comparison at places that were and were not turn boundaries. Although Gravano and Hirschberg (2009) segmented their corpus by 50 ms of silence, we used 200 ms to divide the breath groups, as this data had already been calculated for another experiment done with the HH corpus (Liscombe et. al., 2005). 1 Figure 1. Conversation Segmented into Breath Groups Each breath group was automatically labeled as one of the following: HOLD, when a breath group was immediately followed by a second breath group from the same person, YIELD, when a breath group was immediately followed by speech from the other participant, or OVERLAP, when speech from another participant started before the current one ended.</context>
<context position="8372" citStr="Gravano and Hirschberg (2009)" startWordPosition="1376" endWordPosition="1379">18 * p &lt; 0.001 * p &lt; 0.001 * p &lt; 0.001 Table 1. Individual Results for Subject 111 * denotes a significant p value N duration percent silence pitch RMS HOLD Group Mean 17 1.49 0.300 140.44 418.00 YIELD Group Mean 17 0.82 0.310 147.58 354.65 Significance * p = 0.022 p = 0.590 * p = 0.009 * p &lt; 0.001 Table 2. Results from Paired T-Test squared amplitude) is the energy or loudness. Percent silence was the amount of internal silence within the breath group. For pitch and RMS, the mean was taken over the length of the breath group. These features were used because they are similar to those used by Gravano and Hirschberg (2009), and are already used in the spoken dialogue system we will be using (Forbes-Riley and Litman, 2011). While only a small set of features is examined here, future work will include expanding the feature set. Mean values for each feature for HOLD’s and YIELD’s were calculated and compared using the student T-test in SPSS Statistics software. Two separate tests were done, one to compare the means for each student individually, and one to compare the means across all students. p ≤ .05 is considered significant for all statistical tests. The p-values given are the probability of obtaining the diff</context>
<context position="10629" citStr="Gravano and Hirschberg (2009)" startWordPosition="1758" endWordPosition="1761">etailed look at each of the three significant cues is done below. Number of Number of Significant Cues Students 0 1 1 0 2 6 3 9 4 1 Table 3. Number of Students with Significant Cues Duration: The mean duration for HOLD’s is longer than the mean duration for YIELD’s. This suggests that students speak for a longer uninterrupted time when they are trying to hold their turn, and yield their turns with shorter utterances. This is the opposite of Gravano and Hirschberg’s (2009) results, which found that YIELD’s were longer. Pitch: The mean pitch for YIELD’s is higher than the mean pitch for HOLD’s. Gravano and Hirschberg (2009), on the other hand, found that YIELD’s were lower pitched than HOLD’s. This difference may be accounted for by the difference in tasks. During tutoring, students are possibly 96 more uncertain, which may raise the mean pitch of the YIELD breath groups. RMS: The mean RMS, or energy, for HOLD’s is higher than the mean energy for YIELD’s. This is consistent with student’s speaking more softly, i.e., trailing off, at the end of their turn, a usual phenomenon in human speech. This is consistent with the results from the Columbia games corpus (Gravano and Hirshberg, 2009). 4.2 Combining Cues Gravan</context>
</contexts>
<marker>Gravano, Hirschberg, 2009</marker>
<rawString>Agustín Gravano and Julia Hirschberg. 2009. Turnyielding cues in task-oriented dialogue. In Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 253--261. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackson Liscombe</author>
<author>Julia Hirschberg</author>
<author>Jennifer J Venditti</author>
</authors>
<title>Detecting certainness in spoken tutorial dialogues.</title>
<date>2005</date>
<booktitle>In Interspeech.</booktitle>
<marker>Liscombe, Hirschberg, Venditti, 2005</marker>
<rawString>Jackson Liscombe, Julia Hirschberg, and Jennifer J. Venditti. 2005. Detecting certainness in spoken tutorial dialogues. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Carolyn P Rose</author>
<author>Kate Forbes-Riley</author>
<author>Kurt VanLehn</author>
<author>Dumisizwe Bhembe</author>
<author>Scott Silliman</author>
</authors>
<title>Spoken Versus Typed Human and Computer Dialogue Tutoring.</title>
<date>2006</date>
<booktitle>In International Journal of Artificial Intelligence in Education,</booktitle>
<volume>26</volume>
<pages>145--170</pages>
<marker>Litman, Rose, Forbes-Riley, VanLehn, Bhembe, Silliman, 2006</marker>
<rawString>Diane J. Litman, Carolyn P. Rose, Kate Forbes-Riley, Kurt VanLehn, Dumisizwe Bhembe, and Scott Silliman. 2006. Spoken Versus Typed Human and Computer Dialogue Tutoring. In International Journal of Artificial Intelligence in Education, 26: 145-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Mihai Rotaru</author>
<author>Greg Nicholas</author>
</authors>
<title>Classifying Turn-Level Uncertainty Using Word-Level Prosody.</title>
<date>2009</date>
<booktitle>Proceedings Interspeech,</booktitle>
<location>Brighton, UK,</location>
<marker>Litman, Rotaru, Nicholas, 2009</marker>
<rawString>Diane Litman, Mihai Rotaru, and Greg Nicholas. 2009. Classifying Turn-Level Uncertainty Using Word-Level Prosody. Proceedings Interspeech, Brighton, UK, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Forbes-Riley</author>
<author>Diane Litman</author>
</authors>
<title>Benefits and Challenges of Real-Time Uncertainty Detection and Adaptation in a Spoken Dialogue Computer Tutor. Speech Communication,</title>
<date>2011</date>
<note>in press.</note>
<contexts>
<context position="8473" citStr="Forbes-Riley and Litman, 2011" startWordPosition="1394" endWordPosition="1397">ficant p value N duration percent silence pitch RMS HOLD Group Mean 17 1.49 0.300 140.44 418.00 YIELD Group Mean 17 0.82 0.310 147.58 354.65 Significance * p = 0.022 p = 0.590 * p = 0.009 * p &lt; 0.001 Table 2. Results from Paired T-Test squared amplitude) is the energy or loudness. Percent silence was the amount of internal silence within the breath group. For pitch and RMS, the mean was taken over the length of the breath group. These features were used because they are similar to those used by Gravano and Hirschberg (2009), and are already used in the spoken dialogue system we will be using (Forbes-Riley and Litman, 2011). While only a small set of features is examined here, future work will include expanding the feature set. Mean values for each feature for HOLD’s and YIELD’s were calculated and compared using the student T-test in SPSS Statistics software. Two separate tests were done, one to compare the means for each student individually, and one to compare the means across all students. p ≤ .05 is considered significant for all statistical tests. The p-values given are the probability of obtaining the difference between groups by chance. 4 Results 4.1 Individual Cues First, means for each feature for HOLD</context>
</contexts>
<marker>Forbes-Riley, Litman, 2011</marker>
<rawString>Kate Forbes-Riley and Diane Litman. 2011. Benefits and Challenges of Real-Time Uncertainty Detection and Adaptation in a Spoken Dialogue Computer Tutor. Speech Communication, in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Scott Silliman</author>
</authors>
<title>Itspoke: An intelligent tutoring spoken dialogue system.</title>
<date>2004</date>
<booktitle>In HLT/NAACL.</booktitle>
<contexts>
<context position="2540" citStr="Litman and Silliman, 2004" startWordPosition="396" endWordPosition="400"> dissatisfaction with a spoken dialogue system. Little work has been done to study turn-taking in tutoring, so we hope to investigate it further while using a human-human (HH) tutoring corpus and language technologies to extract useful information about turn-taking cues. This analysis is particularly interesting in a tutoring domain because of the speculated unequal statuses of participants. The goal is to eventually develop a model for turn-taking based on this analysis which can be implemented in an existent tutoring system, ITSPOKE, an intelligent tutor for college-level Newtonian physics (Litman and Silliman, 2004). ITSPOKE currently uses a time-out to determine the end of a student turn and does not recognize student barge-in. We hypothesize that improving upon the turn-taking model this system uses will help engage students and hopefully lead to increased student learning, a standard performance measure of intelligent tutoring systems (Litman et. al., 2006). 2 Related Work Turn-taking has been a recent focus in spoken dialogue system work, with research producing many different models and approaches. Raux and Eskenazi (2009) proposed a finite-state turn-taking Proceedings of the ACL-HLT 2011 Student S</context>
</contexts>
<marker>Litman, Silliman, 2004</marker>
<rawString>Diane J. Litman and Scott Silliman. 2004. Itspoke: An intelligent tutoring spoken dialogue system. In HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Raux</author>
<author>Maxine Eskenazi</author>
</authors>
<title>A finite-state turn-taking model for spoken dialog systems.</title>
<date>2009</date>
<booktitle>In Proc. NAACL/HLT 2009,</booktitle>
<location>Boulder, CO, USA.</location>
<contexts>
<context position="3062" citStr="Raux and Eskenazi (2009)" startWordPosition="478" endWordPosition="481"> system, ITSPOKE, an intelligent tutor for college-level Newtonian physics (Litman and Silliman, 2004). ITSPOKE currently uses a time-out to determine the end of a student turn and does not recognize student barge-in. We hypothesize that improving upon the turn-taking model this system uses will help engage students and hopefully lead to increased student learning, a standard performance measure of intelligent tutoring systems (Litman et. al., 2006). 2 Related Work Turn-taking has been a recent focus in spoken dialogue system work, with research producing many different models and approaches. Raux and Eskenazi (2009) proposed a finite-state turn-taking Proceedings of the ACL-HLT 2011 Student Session, pages 94–98, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics model, which is used to predict end-of-turn and performed significantly better than a fixedthreshold baseline in reducing endpointing latency in a spoken dialogue system. Selfridge and Heeman (2010) took a different approach and presented a bidding model for turn-taking, in which dialogue participants compete for the turn based on the importance of what they will say next. Of considerable inspiration to the resear</context>
</contexts>
<marker>Raux, Eskenazi, 2009</marker>
<rawString>Antoine Raux and Maxine Eskenazi. 2009. A finite-state turn-taking model for spoken dialog systems. In Proc. NAACL/HLT 2009, Boulder, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ethan O Selfridge</author>
<author>Peter A Heeman</author>
</authors>
<title>Importance-Driven Turn-Bidding for spoken dialogue systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL &apos;10). Association for Computational Linguistics,</booktitle>
<pages>177--185</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="3443" citStr="Selfridge and Heeman (2010)" startWordPosition="531" endWordPosition="535"> performance measure of intelligent tutoring systems (Litman et. al., 2006). 2 Related Work Turn-taking has been a recent focus in spoken dialogue system work, with research producing many different models and approaches. Raux and Eskenazi (2009) proposed a finite-state turn-taking Proceedings of the ACL-HLT 2011 Student Session, pages 94–98, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics model, which is used to predict end-of-turn and performed significantly better than a fixedthreshold baseline in reducing endpointing latency in a spoken dialogue system. Selfridge and Heeman (2010) took a different approach and presented a bidding model for turn-taking, in which dialogue participants compete for the turn based on the importance of what they will say next. Of considerable inspiration to the research in this paper was Gravano and Hirschberg’s (2009) analysis of their games corpus, which showed that it was possible for turn-yielding cues to be identified in an HH corpus. A similar method was used in this analysis, though it was adapted based on the tools and data that were readily available for our corpus. Since these differences may prevent direct comparison between corpo</context>
</contexts>
<marker>Selfridge, Heeman, 2010</marker>
<rawString>Ethan O. Selfridge and Peter A. Heeman. 2010. Importance-Driven Turn-Bidding for spoken dialogue systems. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL &apos;10). Association for Computational Linguistics, Stroudsburg, PA, USA, 177-185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel Ward</author>
<author>Anais Rivera</author>
<author>Karen Ward</author>
<author>David G Novick</author>
</authors>
<title>Root causes of lost time and user stress in a simple dialog system.</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<marker>Ward, Rivera, Ward, Novick, 2005</marker>
<rawString>Nigel Ward, Anais Rivera, Karen Ward, and David G. Novick. 2005. Root causes of lost time and user stress in a simple dialog system. In Proceedings of Interspeech.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>