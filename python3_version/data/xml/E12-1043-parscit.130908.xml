<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005623">
<title confidence="0.996913">
Combining Tree Structures, Flat Features and Patterns
for Biomedical Relation Extraction
</title>
<author confidence="0.873">
Md. Faisal Mahbub Chowdhury t t and Alberto Lavelli t
</author>
<affiliation confidence="0.9252855">
t Fondazione Bruno Kessler (FBK-irst), Italy
t University of Trento, Italy
</affiliation>
<email confidence="0.998683">
{chowdhury,lavelli}@fbk.eu
</email>
<sectionHeader confidence="0.997392" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992014">
Kernel based methods dominate the current
trend for various relation extraction tasks
including protein-protein interaction (PPI)
extraction. PPI information is critical in un-
derstanding biological processes. Despite
considerable efforts, previously reported
PPI extraction results show that none of the
approaches already known in the literature
is consistently better than other approaches
when evaluated on different benchmark PPI
corpora. In this paper, we propose a
novel hybrid kernel that combines (auto-
matically collected) dependency patterns,
trigger words, negative cues, walk fea-
tures and regular expression patterns along
with tree kernel and shallow linguistic ker-
nel. The proposed kernel outperforms the
exiting state-of-the-art approaches on the
BioInfer corpus, the largest PPI benchmark
corpus available. On the other four smaller
benchmark corpora, it performs either bet-
ter or almost as good as the existing ap-
proaches. Moreover, empirical results show
that the proposed hybrid kernel attains con-
siderably higher precision than the existing
approaches, which indicates its capability
of learning more accurate models. This also
demonstrates that the different types of in-
formation that we use are able to comple-
ment each other for relation extraction.
</bodyText>
<sectionHeader confidence="0.999631" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992585">
Kernel methods are considered the most effective
techniques for various relation extraction (RE)
tasks on both general (e.g. newspaper text) and
specialized (e.g. biomedical text) domains. In
particular, as the importance of syntactic struc-
tures for deriving the relationships between en-
tities in text has been growing, several graph
and tree kernels have been designed and experi-
mented.
Early RE approaches more or less fall in one of
the following categories: (i) exploitation of statis-
tics about co-occurrences of entities, (ii) usage of
patterns and rules, and (iii) usage of flat features
to train machine learning (ML) classifiers. These
approaches have been studied for a long period
and have their own pros and cons. Exploitation
of co-occurrence statistics results in high recall
but low precision, while rule or pattern based ap-
proaches can increase precision but suffer from
low recall. Flat feature based ML approaches em-
ploy various kinds of linguistic, syntactic or con-
textual information and integrate them into the
feature space. They obtain relatively good results
but are hindered by drawbacks of limited feature
space and excessive feature engineering. Kernel
based approaches have become an attractive alter-
native solution, as they can exploit huge amount
of features without an explicit representation.
In this paper, we propose a new hybrid kernel
for RE. We apply the kernel to Protein–protein
interaction (PPI) extraction, the most widely re-
searched topic in biomedical relation extraction.
PPI1 information is very critical in understanding
biological processes. Considerable progress has
been made for this task. Nevertheless, empirical
results of previous studies show that none of the
approaches already known in the literature is con-
sistently better than other approaches when evalu-
ated on different benchmark PPI corpora (see Ta-
ble 4). This demands further study and innovation
</bodyText>
<footnote confidence="0.95684275">
1PPIs occur when two or more proteins bind together,
and are integral to virtually all cellular processes, such as
metabolism, signalling, regulation, and proliferation (Tikk
et al., 2010).
</footnote>
<page confidence="0.931572">
420
</page>
<note confidence="0.977778">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 420–429,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9991295">
of new approaches that are sensitive to the varia-
tions of complex linguistic constructions.
The proposed hybrid kernel is the composition
of one tree kernel and two feature based kernels
(one of them is already known in the literature
and the other is proposed in this paper for the first
time). The novelty of the newly proposed feature
based kernel is that it envisages to accommodate
the advantages of pattern based approaches. More
precisely:
</bodyText>
<listItem confidence="0.991983130434782">
1. We propose a new feature based kernel (de-
tails in Section 4.1) by using syntactic de-
pendency patterns, trigger words, negative
cues, regular expression (henceforth, regex)
patterns and walk features (i.e. e-walks and
v-walks)2.
2. The syntactic dependency patterns are au-
tomatically collected from a type of depen-
dency subgraph (we call it reduced graph,
more details in Section 4.1.1) during run-
time.
3. We only use the regex patterns, trigger words
and negative cues mentioned in the literature
(Ono et al., 2001; Fundel et al., 2007; Bui et
al., 2010). The objective is to verify whether
we can exploit knowledge which is already
known and used.
4. We propose a hybrid kernel by combin-
ing the proposed feature based kernel (out-
lined above) with the Shallow Linguistic
(SL) kernel (Giuliano et al., 2006) and the
Path-enclosed Tree (PET) kernel (Moschitti,
2004).
</listItem>
<bodyText confidence="0.99992375">
The aim of our work is to take advantage of
different types of information (i.e., dependency
patterns, regex patterns, trigger words, negative
cues, syntactic dependencies among words and
constituent parse trees) and their different repre-
sentations (i.e. flat features, tree structures and
graphs) which can complement each other to learn
more accurate models.
</bodyText>
<footnote confidence="0.874239285714286">
2The syntactic dependencies of the words of a sentence
create a dependency graph. A v-walk feature consists of
(wordi − dependency typei,i+1 − wordi+1), and an e-
walk feature is composed of (dependency typei_1,i −
wordi − dependency typei,i+1). Note that, in a depen-
dency graph, the words are nodes while the dependency
types are edges.
</footnote>
<bodyText confidence="0.9997892">
The remainder of the paper is organized as fol-
lows. In Section 2, we briefly review previous
work. Section 3 lists the datasets. Then, in Sec-
tion 4, we define our proposed hybrid kernel and
describe its individual component kernels. Sec-
tion 5 outlines the experimental settings. Follow-
ing that, empirical results are discussed in Section
6. Finally, we conclude with a summary of our
study as well as suggestions for further improve-
ment of our approach.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999771512820513">
In this section, we briefly discuss some of the
recent work on PPI extraction. Several RE ap-
proaches have been reported to date for the PPI
task, most of which are kernel based methods.
Tikk et al. (2010) reported a benchmark evalu-
ation of various kernels on PPI extraction. An
interesting finding is that the Shallow Linguis-
tic (SL) kernel (Giuliano et al., 2006) (to be dis-
cussed in Section 4.2), despite its simplicity, is on
par with the best kernels in most of the evaluation
settings.
Kim et al. (2010) proposed walk-weighted sub-
sequence kernel using e-walks, partial matches,
non-contiguous paths, and different weights for
different sub-structures (which are used to capture
structural similarities during kernel computation).
Miwa et al. (2009a) proposed a hybrid kernel,
which combines the all-paths graph (APG) kernel
(Airola et al., 2008), the bag-of-words kernel, and
the subset tree kernel (Moschitti, 2006) (applied
on the shortest dependency paths between target
protein pairs). They used multiple parser inputs.
The system is regarded as the current state-of-the-
art PPI extraction system because of its high re-
sults on different PPI corpora (see the results in
Table 4).
As an extension of their work, they boosted sys-
tem performance by training on multiple PPI cor-
pora instead of on a single corpus and adopting
a corpus weighting concept with support vector
machine (SVM) which they call SVM-CW (Miwa
et al., 2009b). Since most of their results are re-
ported by training on the combination of multi-
ple corpora, it is not possible to compare them
directly with the results published in the other re-
lated works (that usually adopt 10-fold cross vali-
dation on a single PPI corpus). To be comparable
with the vast majority of the existing work, we
also report results using 10-fold cross validation
</bodyText>
<page confidence="0.998127">
421
</page>
<table confidence="0.9993035">
Corpus Sentences Positive pairs Negative pairs
BioInfer 1,100 2,534 7,132
AIMed 1,955 1,000 4,834
IEPA 486 335 482
HPRD50 145 163 270
LLL 77 164 166
</table>
<tableCaption confidence="0.9875585">
Table 1: Basic statistics of the 5 benchmark PPI cor-
pora.
</tableCaption>
<bodyText confidence="0.997233769230769">
on single corpora.
Apart from the approaches described above,
there also exist other studies that used kernels for
PPI extraction (e.g. subsequence kernel (Bunescu
and Mooney, 2006)).
A notable exception is the work published by
Bui et al. (2010). They proposed an approach that
consists of two phases. In the first phase, their
system categorizes the data into different groups
(i.e. subsets) based on various properties and pat-
terns. Later they classify candidate PPI pairs in-
side each of the groups using SVM trained with
features specific for the corresponding group.
</bodyText>
<sectionHeader confidence="0.996929" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.998026769230769">
There are 5 benchmark corpora for the PPI task
that are frequently used: HPRD50 (Fundel et al.,
2007), IEPA (Ding et al., 2002), LLL (N´edellec,
2005), BioInfer (Pyysalo et al., 2007) and AIMed
(Bunescu et al., 2005). These corpora adopt dif-
ferent PPI annotation formats. For a comparative
evaluation Pyysalo et al. (2008) put all of them
in a common format which has become the stan-
dard evaluation format for the PPI task. In our
experiments, we use the versions of the corpora
converted to such format.
Table 1 shows various statistics regarding the 5
(converted) corpora.
</bodyText>
<sectionHeader confidence="0.988657" genericHeader="method">
4 Proposed Hybrid Kernel
</sectionHeader>
<bodyText confidence="0.984275">
The hybrid kernel that we propose is as follows:
</bodyText>
<equation confidence="0.530412">
KHybrid (R1, R2) = KTPWF (R1, R2)
+ KSL (R1, R2) + w * KPET (R1, R2)
</equation>
<bodyText confidence="0.999675277777778">
where KTPWF stands for the new feature
based kernel (henceforth, TPWF kernel) com-
puted using flat features collected by exploiting
patterns, trigger words, negative cues and walk
features. KSL and KPET stand for the Shallow
Linguistic (SL) kernel and the Path-enclosed Tree
(PET) kernel respectively. w is a multiplicative
constant used for the PET kernel. It allows the
hybrid kernel to assign more (or less) weight to
the information obtained using tree structures de-
pending on the corpus. The proposed hybrid ker-
nel is valid according to the closure properties of
kernels.
Both the TPWF and SL kernels are linear ker-
nels, while PET kernel is computed using Unlex-
icalized Partial Tree (uPT) kernel (Severyn and
Moschitti, 2010). The following subsections ex-
plain each of the individual kernels in more detail.
</bodyText>
<subsectionHeader confidence="0.970956">
4.1 Proposed TPWF Kernel
</subsectionHeader>
<bodyText confidence="0.966412272727273">
4.1.1 Reduced graph, trigger words,
negative cues and dependency patterns
For each of the candidate entity pairs, we
construct a type of subgraph from the depen-
dency graph formed by the syntactic dependen-
cies among the words of a sentence. We call it
“reduced graph” and define it in the follow-
ing way:
A reduced graph is a subgraph
of the dependency graph of a sentence
which includes:
</bodyText>
<listItem confidence="0.975045142857143">
• the two candidate entities and their
governor nodes up to their least
common governor (if exists).
• dependent nodes (if exist) of all the
nodes added in the previous step.
• the immediate governor(s) (if ex-
ists) of the least common governor.
</listItem>
<bodyText confidence="0.999761764705882">
Figure 1 shows an example of a reduced graph.
A reduced graph is an extension of the smallest
common subgraph of the dependency graph that
aims at overcoming its limitations. It is a known
issue that the smallest common subgraph (or sub-
tree) sometimes does not contain cue words. Pre-
viously, Chowdhury et al. (2011a) proposed a lin-
guistically motivated extension of the minimal
(i.e. smallest) common subtree (which includes
the candidate entity pairs), known as Mildly Ex-
tended Dependency Tree (MEDT). However, the
rules used for MEDT are too constrained. Our ob-
jective in constructing the reduced graph is to in-
clude any potential modifier(s) or cue word(s) that
describes the relation between the given pair of
entities. Sometimes such modifiers or cue words
are not directly dependent (syntactically) on any
</bodyText>
<page confidence="0.995047">
422
</page>
<table confidence="0.998884625">
BioInfer AIMed IEPA HPRD50 LLL
P R F P R F P R F P R F P R F
Only walk features 51.8 71.2 60.0 48.7 63.2 55.0 61.0 75.2 67.4 60.2 65.0 62.5 64.6 87.8 74.4
Features: dep. patterns, 53.8 68.8 60.4 50.6 63.9 56.5 63.9 74.6 68.9 65.0 71.8 68.2 66.5 89.6 76.4
trigger, neg. cues, walks
Features: dep. patterns, 53.5 68.6 60.1 52.5 62.9 57.2 63.8 74.6 68.8 65.1 69.9 67.5 67.4 88.4 76.5
trigger, neg. cues, walks,
regex patterns
</table>
<tableCaption confidence="0.973497">
Table 2: Results of the proposed TPWF feature based kernel on 5 benchmark PPI corpora before and after adding
features collected using dependency patterns, regex patterns, trigger words and negative cues to the walk features.
The TPWF kernel is a component of the new hybrid kernel.
</tableCaption>
<figureCaption confidence="0.77532275">
Figure 1: Dependency graph for the sentence “A pVHL mutant containing a P154L substitution does not promote
degradation of HIF1-Alpha” generated by the Stanford parser. The edges with blue dots form the smallest
common subgraph for the candidate entity pair pVHL and HIF1-Alpha, while the edges with red dots form the
reduced graph for the pair.
</figureCaption>
<bodyText confidence="0.999457">
of the entities (of the candidate pair). Rather they
are dependent on some other word(s) which is de-
pendent on one (or both) of the entities. The word
“not” in Figure 1 is one such example. The re-
duced graph aims to preserve these cue words.
The following types of features are collected
from the reduced graph of a candidate pair:
</bodyText>
<listItem confidence="0.993397">
1. HasTriggerWord: whether the least common
governor(s) of the target entity pairs inside
the reduced graph matches any trigger word.
2. Trigger-X: whether the least common gov-
ernor(s) of the target entity pairs inside the
reduced graph matches the trigger word ‘X’.
3. HasNegWord: whether the reduced graph
contains any negative word.
4. DepPattern-i: whether the reduced graph
contains all the syntactic dependencies of the
i-th pattern of dependency pattern list.
</listItem>
<bodyText confidence="0.999834913043478">
The dependency pattern list is automatically
constructed from the training data during the
learning phase. Each pattern is a set of syntactic
dependencies of the corresponding reduced graph
of a (positive or negative) entity pair in the train-
ing data. For example, the dependency pattern for
the reduced graph in Figure 1 is {det, amod, part-
mod, nsubj, aux, neg, dobj, prep of }. The same
dependency pattern might be constructed for mul-
tiple (positive or negative) entity pairs. However,
if it is constructed for both positive and negative
pairs, it has to be discarded from the pattern list.
The dependency patterns allow some kind of
underspecification as they do not contain the lex-
ical items (i.e. words) but contain the likely com-
bination of syntactic dependencies that a given re-
lated pair of entities would pose inside their re-
duced graph.
The list of trigger words contains 144 words
previously used by Bui et al. (2010) and Fundel
et al. (2007). The list of negative cues contain 18
words, most of which are mentioned in Fundel et
al. (2007).
</bodyText>
<subsubsectionHeader confidence="0.645312">
4.1.2 Walk features
</subsubsectionHeader>
<bodyText confidence="0.98863675">
We extract e-walk and v-walk features from
the Mildly Extended Dependency Tree (MEDT)
(Chowdhury et al., 2011a) of each candidate pair.
Reduced graphs sometimes include some unin-
</bodyText>
<page confidence="0.993958">
423
</page>
<table confidence="0.999954533333334">
BioInfer AIMed IEPA HPRD50 LLL
Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166
P R F P R F P R F P R F P R F
Proposed TPWF kernel 53.8 68.8 60.4 50.6 63.9 56.5 63.9 74.6 68.9 65.0 71.8 68.2 66.5 89.6 76.4
(without regex)
Proposed TPWF kernel 53.5 68.6 60.1 52.5 62.9 57.2 63.8 74.6 68.8 65.1 69.9 67.5 67.4 88.4 76.5
(with regex)
SL kernel 60.8 65.8 63.2 56.2 64.4 60.0 73.3 71.9 72.6 62.0 65.0 63.5 74.9 85.4 79.8
PET kernel 72.8 74.9 73.9 44.8 72.8 55.5 70.7 77.9 74.2 65.0 73.0 68.8 72.1 89.6 79.9
Proposed hybrid kernel 80.0 71.4 75.5 64.2 58.2 61.1 81.1 69.3 74.7 72.9 59.5 65.5 70.4 95.7 81.1
(PET + SL + TPWF
(without regex))
Proposed hybrid kernel 80.1 72.0 75.9 64.4 58.3 61.2 79.3 69.6 74.1 71.9 61.4 66.2 70.6 95.1 81.0
(PET + SL + TPWF
(with regex))
</table>
<tableCaption confidence="0.964672">
Table 3: Results of the proposed hybrid kernel and its individual components. Pos. and Neg. refer to number
positive and negative relations respectively. PET refers to the path-enclosed tree kernel, SL refers to the shallow
linguistic kernel, and TPWF refers to the kernel computed using trigger, pattern, negative cue and walk features.
</tableCaption>
<bodyText confidence="0.9993938">
formative words which produce uninformative
walk features. Hence, they are not suitable for
walk feature generation. MEDT suits better for
this purpose. The walk features extracted from
MEDTs have the following properties:
</bodyText>
<listItem confidence="0.990451444444444">
• The directionality of the edges (or nodes) in
an e-walk (or v-walk) is not considered. In
other words, e.g., pos(stimulatory)−amod−
pos(effects) and pos(effects) − amod −
pos(stimulatory) are treated as the same fea-
ture.
• The v-walk features are of the form (posi −
dependency typei,i+1−posi+1). Here, posi is
the POS tag of wordi, i is the governor node
and i + 1 is the dependent node.
• The e-walk features are of the form
(dep. typei_1,i − posi − dep. typei,i+1) and
(dep. typei_1,i − lemmai − dep. typei,i+1).
Here, lemmai is the lemmatized form of
wordi.
• Usually, the e-walk features are con-
structed using dependency types be-
tween {governor of X, node X} and
</listItem>
<bodyText confidence="0.982910416666667">
{node X, dependent of X}. However,
we also extract e-walk features from
the dependency types between any two
dependents and their common governor
(i.e. {node X, dependent 1 of X} and
{node X, dependent 2 of X}).
Apart from the above types of features, we also
add features for lemmas of the immediate preced-
ing and following words of the candidate entities.
These feature names are augmented with -1 or +1
depending on whether the corresponding words
are preceded or followed by a candidate entity.
</bodyText>
<subsubsectionHeader confidence="0.732413">
4.1.3 Regular expression patterns
</subsubsectionHeader>
<bodyText confidence="0.999891375">
We use a set of 22 regex patterns as binary
features. These patterns were previously used
by Ono et al. (2001) and Bui et al. (2010).
If there is a match for a pattern (e.g. “En-
tity 1.*activates.*Entity 2” where Entity 1 and
Entity 2 form the candidate entity pair) in a given
sentence, value 1 is added for the feature (i.e., pat-
tern) inside the feature vector.
</bodyText>
<subsectionHeader confidence="0.995197">
4.2 Shallow Linguistic (SL) Kernel
</subsectionHeader>
<bodyText confidence="0.995871375">
The Shallow Linguistic (SL) kernel was proposed
by Giuliano et al. (2006). It is one of the best
performing kernels applied on different biomedi-
cal RE tasks such as PPI and DDI (drug-drug in-
teraction) extraction (Tikk et al., 2010; Segura-
Bedmar et al., 2011; Chowdhury and Lavelli,
2011b; Chowdhury et al., 2011c). It is defined
as follows:
</bodyText>
<equation confidence="0.8728725">
KSL (R1, R2) = KLC (R1, R2) + KGC
(R1, R2)
</equation>
<page confidence="0.996884">
424
</page>
<table confidence="0.999869041666667">
BioInfer AIMed IEPA HPRD50 LLL
Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166
P R F P R F P R F P R F P R F
SL kernel – – – 60.9 57.2 59.0 – – – – – – – – –
(Giuliano et al., 2006)
APG kernel 56.7 67.2 61.3 52.9 61.8 56.4 69.6 82.7 75.1 64.3 65.8 63.4 72.5 87.2 76.8
(Airola et al., 2008)
Hybrid kernel and 65.7 71.1 68.1 55.0 68.8 60.8 67.5 78.6 71.7 68.5 76.1 70.9 77.6 86.0 80.1
multiple parser input
(Miwa et al., 2009a)
SVM-CW, multiple – – 67.6 – – 64.2 – – 74.4 – – 69.7 – – 80.5
parser input and graph,
walk and BOW features
(Miwa et al., 2009b)
kBSPS kernel 49.9 61.8 55.1 50.1 41.4 44.6 58.8 89.7 70.5 62.2 87.1 71.0 69.3 93.2 78.1
(Tikk et al., 2010)
Walk weighted 61.8 54.2 57.6 61.4 53.3 56.6 73.8 71.8 72.9 66.7 69.2 67.8 76.9 91.2 82.4
subsequence kernel
(Kim et al., 2010)
2 phase extraction 61.7 57.5 60.0 55.3 68.5 61.2 – – – – – – – – –
(Bui et al., 2010)
Our proposed hybrid 80.0 71.4 75.5 64.2 58.2 61.1 81.1 69.3 74.7 72.9 59.5 65.5 70.4 95.7 81.1
kernel (PET + SL +
TPWF without regex)
</table>
<tableCaption confidence="0.959791">
Table 4: Comparison of the results on the 5 benchmark PPI corpora. Pos. and Neg. refer to number positive and
</tableCaption>
<figureCaption confidence="0.828970857142857">
negative relations respectively. The underlined numbers indicate the best results for the corresponding corpus
reported by any of the existing state-of-the-art approaches. The results of Bui et al. (2010) on LLL, HPRD50,
and IEPA are not reported since thy did not use all the positive and negative examples during cross validation.
Miwa et al. (2009b) showed that better results can be obtained using multiple corpora for training. However,
we consider only those results of their experiments where they used single training corpus as it is the standard
evaluation approach adopted by all the other studies on PPI extraction for comparing results. All the results of
the previous approaches reported in this table are directly quoted from their respective original papers.
</figureCaption>
<bodyText confidence="0.999942625">
where KSL, KGC and KLC correspond to SL,
global context (GC) and local context (LC) ker-
nels respectively. The GC kernel exploits contex-
tual information of the words occurring before,
between and after the pair of entities (to be in-
vestigated for RE) in the corresponding sentence;
while the LC kernel exploits contextual informa-
tion surrounding individual entities.
</bodyText>
<subsectionHeader confidence="0.995011">
4.3 Path-enclosed tree (PET) Kernel
</subsectionHeader>
<bodyText confidence="0.9999318">
The path-enclosed tree (PET) kernel3 was first
proposed by Moschitti (2004) for semantic role
labeling. It was later successfully adapted by
Zhang et al. (2005) and other works for relation
extraction on general texts (such as newspaper do-
</bodyText>
<footnote confidence="0.547099">
3Also known as shortest path-enclosed tree (SPT) kernel.
</footnote>
<bodyText confidence="0.999717571428572">
main). A PET is the smallest common subtree of a
phrase structure tree that includes the two entities
involved in a relation.
A tree kernel calculates the similarity between
two input trees by counting the number of com-
mon sub-structures. Different techniques have
been proposed to measure such similarity. We use
the Unlexicalized Partial Tree (uPT) kernel (Sev-
eryn and Moschitti, 2010) for the computation of
the PET kernel since a comparative evaluation by
Chowdhury et al. (2011a) reported that uPT ker-
nels achieve better results for PPI extraction than
the other techniques used for tree kernel compu-
tation.
</bodyText>
<page confidence="0.999308">
425
</page>
<sectionHeader confidence="0.998775" genericHeader="method">
5 Experimental Settings
</sectionHeader>
<bodyText confidence="0.984433190476191">
We have followed the same criteria commonly
used for the PPI extraction tasks, i.e. abstract-
wise 10-fold cross validation on individual corpus
and one-answer-per-occurrence criterion. In fact,
we have used exactly the same (abstract-wise)
fold splitting of the 5 benchmark (converted) cor-
pora used by Tikk et al. (2010) for benchmarking
various kernel methods4.
The Charniak-Johnson reranking parser (Char-
niak and Johnson, 2005), along with a self-trained
biomedical parsing model (McClosky, 2010), has
been used for tokenization, POS-tagging and
parsing of the sentences. Before parsing the sen-
tences, all the entities are blinded by assigning
names as EntityX where X is the entity index.
In each example, the POS tags of the two can-
didate entities are changed to EntityX. The
parse trees produced by the Charniak-Johnson
reranking parser are then processed by the Stan-
ford parser5 (Klein and Manning, 2003) to obtain
syntactic dependencies according to the Stanford
Typed Dependency format.
The Stanford parser often skips some syntactic
dependencies in output. We use the following two
rules to add some of such dependencies:
• If there is a “conj and” or “conj or” depen-
dency between two words X and Y, then X
should be dependent on any word Z on which
Y is dependent and vice versa.
• If there are two verbs X and Y such that in-
side the corresponding sentence they have
only the word “and” or “or” between them,
then any word Z dependent on X should be
also dependent on Y and vice versa.
Our system exploits SVM-LIGHT-TK6 (Mos-
chitti, 2006; Joachims, 1999). We made minor
changes in the toolkit to compute the proposed
hybrid kernel. The ratio of negative and positive
examples has been used as the value of the cost-
ratio-factor parameter. We have done parameter
tuning following the approach described by Hsu
et al. (2003).
</bodyText>
<footnote confidence="0.9998925">
4Downloaded from http://informatik.hu-
berlin.de/forschung /gebiete/wbi/ppi-benchmark .
5http://nlp.stanford.edu/software/lex-parser.shtml
6http://disi.unitn.it/moschitti/Tree-Kernel.htm
</footnote>
<sectionHeader confidence="0.992885" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999993857142857">
To measure the contribution of the features col-
lected from the reduced graphs (using dependency
patterns, trigger words and negative cues) and
regex patterns, we have applied the new TPWF
kernel on the 5 PPI corpora before and after using
these features. Results shown in Table 2 clearly
indicate that usage of these features improve the
performance. The improvement of performance
is primarily due to the usage of dependency pat-
terns which resulted in higher precision for all the
corpora.
We have tried to measure the contribution of
the regex patterns. However, from the empirical
results a clear trend does not emerge (see Table
2).
Table 3 shows a comparison among the re-
sults of the proposed hybrid kernel and its indi-
vidual components. As we can see, the overall
results of the hybrid kernel (with and without us-
ing regex pattern features) are better than those
by any of its individual component kernels. Inter-
estingly, precision achieved on the 4 benchmark
corpora (other than the smallest corpus LLL) is
much higher for the hybrid kernel than for the in-
dividual components. This strongly indicates that
these different types of information (i.e. depen-
dency patterns, regex patterns, triggers, negative
cues, syntactic dependencies among words and
constituent parse trees) and their different repre-
sentations (i.e. flat features, tree structures and
graphs) can complement each other to learn more
accurate models.
Table 4 shows a comparison of the PPI extrac-
tion results of our proposed hybrid kernel with
those of other state-of-the-art approaches. Since
the contribution of regex patterns in the perfor-
mance of the hybrid kernel was not relevant (as
Tables 2 and 3 show), we used the results of pro-
posed hybrid kernel without regex for the compar-
ison. As we can see, the proposed kernel achieves
significantly higher results on the BioInfer corpus,
the largest benchmark PPI corpus (2,534 positive
PPI pair annotations) available, than any of the
existing approaches. Moreover, the results of the
proposed hybrid kernel are on par with the state-
of-the-art results on the other smaller corpora.
Furthermore, empirical results show that the
proposed hybrid kernel attains considerably
higher precision than the existing approaches.
</bodyText>
<page confidence="0.998052">
426
</page>
<bodyText confidence="0.999983433333333">
Since a dependency pattern, by construction,
contains all the syntactic dependencies inside the
corresponding reduced graph, it may happen that
some of the dependencies (e.g. det or determiner)
are not informative for classifying the label of the
corresponding class label (i.e., positive or nega-
tive relation) of the pattern. Their presence in-
side a pattern might make it unnecessarily rigid
and less general. So, we tried to identify and dis-
card such non informative dependencies by mea-
suring probabilities of the dependencies with re-
spect to the class label and then removing any of
them which has probability lower than a threshold
(we tried with different threshold values). But do-
ing so decreased the performance. This suggests
that the syntactic dependencies of a dependency
pattern are not independent of each other even if
some of them might have low probability (with
respect to the class label) individually. We plan to
further investigate whether there could be differ-
ent criteria for identifying non informative depen-
dencies. For the work reported in this paper, we
used the dependency patterns as they are initially
constructed.
We also did experiments to see whether collect-
ing features for trigger words from the whole re-
duced graph would help. But that also decreased
performance. This suggests that trigger words are
more likely to appear in the least common gover-
nors.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999942541666667">
In this paper, we have proposed a new hybrid
kernel for RE that combines two vector based
kernels and a tree kernel. The proposed kernel
outperforms any of the exiting approaches by a
wide margin on the BioInfer corpus, the largest
PPI benchmark corpus available. On the other
four smaller benchmark corpora, it performs ei-
ther better or almost as good as the existing state-
of-the art approaches.
We have also proposed a novel feature based
kernel, called TPWF kernel, using (automatically
collected) dependency patterns, trigger words,
negative cues, walk features and regular expres-
sion patterns. The TPWF kernel is used as a com-
ponent of the new hybrid kernel.
Empirical results show that the proposed hy-
brid kernel achieves considerably higher precision
than the existing approaches, which indicates its
capability of learning more accurate models. This
also demonstrates that the different types of infor-
mation that we use are able to complement each
other for relation extraction.
We believe there are at least three ways to
further improve the proposed approach. First
of all, the 22 regular expression patterns (col-
lected from Ono et al. (2001) and Bui et al.
(2010)) are applied at the level of the sen-
tences and this sometimes produces unwanted
matches. For example, consider the sentence
“X activates Y and inhibits Z” where X, Y,
and Z are entities. The pattern “Entity1. *
activates. * Entity2” matches both the X–Y and
X–Z pairs in the sentence. But only the X–Y pair
should be considered. So, the patterns should
be constrained to reduce the number of unwanted
matches. For example, they could be applied on
smaller linguistic units than full sentences. Sec-
ondly, different techniques could be used to iden-
tify less-informative syntactic dependencies in-
side dependency patterns to make them more ac-
curate and effective. Thirdly, usage of automati-
cally collected paraphrases of regular expression
patterns instead of the patterns directly could be
also helpful. Weakly supervised collection of
paraphrases for RE has been already investigated
(e.g. Romano et al. (2006)) and, hence, can be
tried for improving the TPWF kernel (which is a
component of the proposed hybrid kernel).
</bodyText>
<sectionHeader confidence="0.998631" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999471166666667">
This work was carried out in the context of the project
“eOnco - Pervasive knowledge and data management
in cancer care”. The authors are grateful to Alessan-
dro Moschitti for his help in the use of SVM-LIGHT-
TK. We also thank the anonymous reviewers for help-
ful suggestions.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982063272727273">
Antti Airola, Sampo Pyysalo, Jari Bjorne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
All-paths graph kernel for protein-protein inter-
action extraction with evaluation of cross-corpus
learning. BMC Bioinformatics, 9(Suppl 11):S2.
Quoc-Chinh Bui, Sophia Katrenko, and Peter M.A.
Sloot. 2010. A hybrid approach to extract protein-
protein interactions. Bioinformatics.
Razvan Bunescu and Raymond J. Mooney. 2006.
Subsequence kernels for relation extraction. In Pro-
ceedings of NIPS 2006, pages 171–178.
</reference>
<page confidence="0.991814">
427
</page>
<reference confidence="0.999696877192982">
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun Ku-
mar Ramani, and Yuk Wah Wong. 2005. Compara-
tive experiments on learning information extractors
for proteins and their interactions. Artificial Intelli-
gence in Medicine, 33(2):139–155.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL 2005.
Md. Faisal Mahbub Chowdhury and Alberto Lavelli.
2011b. Drug-drug interaction extraction using com-
posite kernels. In Proceedings of DDIExtrac-
tion2011: First Challenge Task: Drug-Drug In-
teraction Extraction, pages 27–33, Huelva, Spain,
September.
Md. Faisal Mahbub Chowdhury, Alberto Lavelli, and
Alessandro Moschitti. 2011a. A study on de-
pendency tree kernels for automatic extraction of
protein-protein interaction. In Proceedings of
BioNLP 2011 Workshop, pages 124–133, Portland,
Oregon, USA, June.
Md. Faisal Mahbub Chowdhury, Asma Ben Abacha,
Alberto Lavelli, and Pierre Zweigenbaum. 2011c.
Two dierent machine learning techniques for drug-
drug interaction extraction. In Proceedings of
DDIExtraction2011: First Challenge Task: Drug-
Drug Interaction Extraction, pages 19–26, Huelva,
Spain, September.
J. Ding, D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining MEDLINE: abstracts, sentences, or
phrases? Pacific Symposium on Biocomputing,
pages 326–337.
Katrin Fundel, Robert K¨uffner, and Ralf Zimmer.
2007. Relex–relation extraction using dependency
parse trees. Bioinformatics, 23(3):365–371.
Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2006. Exploiting shallow linguistic infor-
mation for relation extraction from biomedical lit-
erature. In Proceedings of EACL 2006, pages 401–
408.
CW Hsu, CC Chang, and CJ Lin, 2003. A practical
guide to support vector classification. Department
of Computer Science and Information Engineering,
National Taiwan University, Taipei, Taiwan.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Advances
in kernel methods: support vector learning, pages
169–184. MIT Press, Cambridge, MA, USA.
Seonho Kim, Juntae Yoon, Jihoon Yang, and Seog
Park. 2010. Walk-weighted subsequence kernels
for protein-protein interaction extraction. BMC
Bioinformatics, 11(1).
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL
2003, pages 423–430, Sapporo, Japan.
David McClosky. 2010. Any Domain Parsing: Au-
tomatic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Department of Computer
Science, Brown University.
Makoto Miwa, Rune Sætre, Yusuke Miyao, and
Jun’ichi Tsujii. 2009a. Protein-protein interac-
tion extraction by leveraging multiple kernels and
parsers. International Journal of Medical Informat-
ics, 78.
Makoto Miwa, Rune Sætre, Yusuke Miyao, and
Jun’ichi Tsujii. 2009b. A rich feature vector for
protein-protein interaction extraction from multiple
corpora. In Proceedings of EMNLP 2009, pages
121–130, Singapore.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceed-
ings of ACL 2004, Barcelona, Spain.
Alessandro Moschitti. 2006. Making Tree Kernels
Practical for Natural Language Learning. In Pro-
ceedings of EACL 2006, Trento, Italy.
Claire N´edellec. 2005. Learning language in logic -
genic interaction extraction challenge. Proceedings
of the ICML 2005 workshop: Learning Language in
Logic (LLL05), pages 31–37.
Toshihide Ono, Haretsugu Hishigaki, Akira Tanigami,
and Toshihisa Takagi. 2001. Automated ex-
traction of information on protein–protein interac-
tions from the biological literature. Bioinformatics,
17(2):155–161.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bj¨orne, Jorma Boberg, Jouni Jarvinen, and Tapio
Salakoski. 2007. Bioinfer: a corpus for information
extraction in the biomedical domain. BMC Bioin-
formatics, 8(1):50.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bj¨orne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein in-
teraction corpora. BMC Bioinformatics, 9(Suppl
3):S6.
Lorenza Romano, Milen Kouylekov, Idan Szpektor,
Ido Dagan, and Alberto Lavelli. 2006. Investi-
gating a generic paraphrase–based approach for re-
lation extraction. In Proceedings of EACL 2006,
pages 409–416.
Isabel Segura-Bedmar, Paloma Mart´ınez, and Cesar de
Pablo-S´anchez. 2011. Using a shallow linguistic
kernel for drug-drug interaction extraction. Jour-
nal of Biomedical Informatics, In Press, Corrected
Proof, Available online, 24 April.
Aliaksei Severyn and Alessandro Moschitti. 2010.
Fast cutting plane training for structural kernels. In
Proceedings of ECML-PKDD 2010.
Domonkos Tikk, Philippe Thomas, Peter Palaga,
J¨org Hakenberg, and Ulf Leser. 2010. A Compre-
hensive Benchmark of Kernel Methods to Extract
Protein-Protein Interactions from Literature. PLoS
Computational Biology, 6(7), July.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,
and Chew Lim Tan. 2005. Discovering relations
</reference>
<page confidence="0.98581">
428
</page>
<reference confidence="0.9060582">
between named entities from a large raw corpus us-
ing tree similarity-based clustering. In Natural Lan-
guage Processing – IJCNLP 2005, volume 3651 of
Lecture Notes in Computer Science, pages 378–389.
Springer Berlin / Heidelberg.
</reference>
<page confidence="0.999208">
429
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.709186">
<title confidence="0.999532">Combining Tree Structures, Flat Features and for Biomedical Relation Extraction</title>
<author confidence="0.90945">Faisal Mahbub Chowdhury t Alberto Lavelli Bruno Kessler</author>
<affiliation confidence="0.884921">of Trento,</affiliation>
<abstract confidence="0.999293129032258">Kernel based methods dominate the current trend for various relation extraction tasks including protein-protein interaction (PPI) extraction. PPI information is critical in understanding biological processes. Despite considerable efforts, previously reported PPI extraction results show that none of the approaches already known in the literature is consistently better than other approaches when evaluated on different benchmark PPI corpora. In this paper, we propose a novel hybrid kernel that combines (automatically collected) dependency patterns, trigger words, negative cues, walk features and regular expression patterns along with tree kernel and shallow linguistic kernel. The proposed kernel outperforms the exiting state-of-the-art approaches on the BioInfer corpus, the largest PPI benchmark corpus available. On the other four smaller benchmark corpora, it performs either better or almost as good as the existing approaches. Moreover, empirical results show that the proposed hybrid kernel attains considerably higher precision than the existing approaches, which indicates its capability of learning more accurate models. This also demonstrates that the different types of information that we use are able to complement each other for relation extraction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Antti Airola</author>
<author>Sampo Pyysalo</author>
<author>Jari Bjorne</author>
<author>Tapio Pahikkala</author>
<author>Filip Ginter</author>
<author>Tapio Salakoski</author>
</authors>
<title>All-paths graph kernel for protein-protein interaction extraction with evaluation of cross-corpus learning.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<pages>11--2</pages>
<contexts>
<context position="7184" citStr="Airola et al., 2008" startWordPosition="1108" endWordPosition="1111">uation of various kernels on PPI extraction. An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. Kim et al. (2010) proposed walk-weighted subsequence kernel using e-walks, partial matches, non-contiguous paths, and different weights for different sub-structures (which are used to capture structural similarities during kernel computation). Miwa et al. (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al., 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). They used multiple parser inputs. The system is regarded as the current state-of-theart PPI extraction system because of its high results on different PPI corpora (see the results in Table 4). As an extension of their work, they boosted system performance by training on multiple PPI corpora instead of on a single corpus and adopting a corpus weighting concept with support vector machine (SVM) which they call SVM-CW (Miwa et al., 2009b). Since most of </context>
<context position="18690" citStr="Airola et al., 2008" startWordPosition="3112" endWordPosition="3115">s one of the best performing kernels applied on different biomedical RE tasks such as PPI and DDI (drug-drug interaction) extraction (Tikk et al., 2010; SeguraBedmar et al., 2011; Chowdhury and Lavelli, 2011b; Chowdhury et al., 2011c). It is defined as follows: KSL (R1, R2) = KLC (R1, R2) + KGC (R1, R2) 424 BioInfer AIMed IEPA HPRD50 LLL Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166 P R F P R F P R F P R F P R F SL kernel – – – 60.9 57.2 59.0 – – – – – – – – – (Giuliano et al., 2006) APG kernel 56.7 67.2 61.3 52.9 61.8 56.4 69.6 82.7 75.1 64.3 65.8 63.4 72.5 87.2 76.8 (Airola et al., 2008) Hybrid kernel and 65.7 71.1 68.1 55.0 68.8 60.8 67.5 78.6 71.7 68.5 76.1 70.9 77.6 86.0 80.1 multiple parser input (Miwa et al., 2009a) SVM-CW, multiple – – 67.6 – – 64.2 – – 74.4 – – 69.7 – – 80.5 parser input and graph, walk and BOW features (Miwa et al., 2009b) kBSPS kernel 49.9 61.8 55.1 50.1 41.4 44.6 58.8 89.7 70.5 62.2 87.1 71.0 69.3 93.2 78.1 (Tikk et al., 2010) Walk weighted 61.8 54.2 57.6 61.4 53.3 56.6 73.8 71.8 72.9 66.7 69.2 67.8 76.9 91.2 82.4 subsequence kernel (Kim et al., 2010) 2 phase extraction 61.7 57.5 60.0 55.3 68.5 61.2 – – – – – – – – – (Bui et al., 2010) Our proposed </context>
</contexts>
<marker>Airola, Pyysalo, Bjorne, Pahikkala, Ginter, Salakoski, 2008</marker>
<rawString>Antti Airola, Sampo Pyysalo, Jari Bjorne, Tapio Pahikkala, Filip Ginter, and Tapio Salakoski. 2008. All-paths graph kernel for protein-protein interaction extraction with evaluation of cross-corpus learning. BMC Bioinformatics, 9(Suppl 11):S2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc-Chinh Bui</author>
<author>Sophia Katrenko</author>
<author>Peter M A Sloot</author>
</authors>
<title>A hybrid approach to extract proteinprotein interactions.</title>
<date>2010</date>
<journal>Bioinformatics.</journal>
<contexts>
<context position="4856" citStr="Bui et al., 2010" startWordPosition="731" endWordPosition="734">the advantages of pattern based approaches. More precisely: 1. We propose a new feature based kernel (details in Section 4.1) by using syntactic dependency patterns, trigger words, negative cues, regular expression (henceforth, regex) patterns and walk features (i.e. e-walks and v-walks)2. 2. The syntactic dependency patterns are automatically collected from a type of dependency subgraph (we call it reduced graph, more details in Section 4.1.1) during runtime. 3. We only use the regex patterns, trigger words and negative cues mentioned in the literature (Ono et al., 2001; Fundel et al., 2007; Bui et al., 2010). The objective is to verify whether we can exploit knowledge which is already known and used. 4. We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004). The aim of our work is to take advantage of different types of information (i.e., dependency patterns, regex patterns, trigger words, negative cues, syntactic dependencies among words and constituent parse trees) and their different representations (i.e. flat features, tree structures an</context>
<context position="8604" citStr="Bui et al. (2010)" startWordPosition="1347" endWordPosition="1350">d cross validation on a single PPI corpus). To be comparable with the vast majority of the existing work, we also report results using 10-fold cross validation 421 Corpus Sentences Positive pairs Negative pairs BioInfer 1,100 2,534 7,132 AIMed 1,955 1,000 4,834 IEPA 486 335 482 HPRD50 145 163 270 LLL 77 164 166 Table 1: Basic statistics of the 5 benchmark PPI corpora. on single corpora. Apart from the approaches described above, there also exist other studies that used kernels for PPI extraction (e.g. subsequence kernel (Bunescu and Mooney, 2006)). A notable exception is the work published by Bui et al. (2010). They proposed an approach that consists of two phases. In the first phase, their system categorizes the data into different groups (i.e. subsets) based on various properties and patterns. Later they classify candidate PPI pairs inside each of the groups using SVM trained with features specific for the corresponding group. 3 Data There are 5 benchmark corpora for the PPI task that are frequently used: HPRD50 (Fundel et al., 2007), IEPA (Ding et al., 2002), LLL (N´edellec, 2005), BioInfer (Pyysalo et al., 2007) and AIMed (Bunescu et al., 2005). These corpora adopt different PPI annotation form</context>
<context position="14719" citStr="Bui et al. (2010)" startWordPosition="2382" endWordPosition="2385">n Figure 1 is {det, amod, partmod, nsubj, aux, neg, dobj, prep of }. The same dependency pattern might be constructed for multiple (positive or negative) entity pairs. However, if it is constructed for both positive and negative pairs, it has to be discarded from the pattern list. The dependency patterns allow some kind of underspecification as they do not contain the lexical items (i.e. words) but contain the likely combination of syntactic dependencies that a given related pair of entities would pose inside their reduced graph. The list of trigger words contains 144 words previously used by Bui et al. (2010) and Fundel et al. (2007). The list of negative cues contain 18 words, most of which are mentioned in Fundel et al. (2007). 4.1.2 Walk features We extract e-walk and v-walk features from the Mildly Extended Dependency Tree (MEDT) (Chowdhury et al., 2011a) of each candidate pair. Reduced graphs sometimes include some unin423 BioInfer AIMed IEPA HPRD50 LLL Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166 P R F P R F P R F P R F P R F Proposed TPWF kernel 53.8 68.8 60.4 50.6 63.9 56.5 63.9 74.6 68.9 65.0 71.8 68.2 66.5 89.6 76.4 (without regex) Proposed TPWF kernel 53.5 68.6 </context>
<context position="17725" citStr="Bui et al. (2010)" startWordPosition="2915" endWordPosition="2918">ct e-walk features from the dependency types between any two dependents and their common governor (i.e. {node X, dependent 1 of X} and {node X, dependent 2 of X}). Apart from the above types of features, we also add features for lemmas of the immediate preceding and following words of the candidate entities. These feature names are augmented with -1 or +1 depending on whether the corresponding words are preceded or followed by a candidate entity. 4.1.3 Regular expression patterns We use a set of 22 regex patterns as binary features. These patterns were previously used by Ono et al. (2001) and Bui et al. (2010). If there is a match for a pattern (e.g. “Entity 1.*activates.*Entity 2” where Entity 1 and Entity 2 form the candidate entity pair) in a given sentence, value 1 is added for the feature (i.e., pattern) inside the feature vector. 4.2 Shallow Linguistic (SL) Kernel The Shallow Linguistic (SL) kernel was proposed by Giuliano et al. (2006). It is one of the best performing kernels applied on different biomedical RE tasks such as PPI and DDI (drug-drug interaction) extraction (Tikk et al., 2010; SeguraBedmar et al., 2011; Chowdhury and Lavelli, 2011b; Chowdhury et al., 2011c). It is defined as fo</context>
<context position="19276" citStr="Bui et al., 2010" startWordPosition="3232" endWordPosition="3235">87.2 76.8 (Airola et al., 2008) Hybrid kernel and 65.7 71.1 68.1 55.0 68.8 60.8 67.5 78.6 71.7 68.5 76.1 70.9 77.6 86.0 80.1 multiple parser input (Miwa et al., 2009a) SVM-CW, multiple – – 67.6 – – 64.2 – – 74.4 – – 69.7 – – 80.5 parser input and graph, walk and BOW features (Miwa et al., 2009b) kBSPS kernel 49.9 61.8 55.1 50.1 41.4 44.6 58.8 89.7 70.5 62.2 87.1 71.0 69.3 93.2 78.1 (Tikk et al., 2010) Walk weighted 61.8 54.2 57.6 61.4 53.3 56.6 73.8 71.8 72.9 66.7 69.2 67.8 76.9 91.2 82.4 subsequence kernel (Kim et al., 2010) 2 phase extraction 61.7 57.5 60.0 55.3 68.5 61.2 – – – – – – – – – (Bui et al., 2010) Our proposed hybrid 80.0 71.4 75.5 64.2 58.2 61.1 81.1 69.3 74.7 72.9 59.5 65.5 70.4 95.7 81.1 kernel (PET + SL + TPWF without regex) Table 4: Comparison of the results on the 5 benchmark PPI corpora. Pos. and Neg. refer to number positive and negative relations respectively. The underlined numbers indicate the best results for the corresponding corpus reported by any of the existing state-of-the-art approaches. The results of Bui et al. (2010) on LLL, HPRD50, and IEPA are not reported since thy did not use all the positive and negative examples during cross validation. Miwa et al. (2009b) sh</context>
<context position="28496" citStr="Bui et al. (2010)" startWordPosition="4705" endWordPosition="4708">features and regular expression patterns. The TPWF kernel is used as a component of the new hybrid kernel. Empirical results show that the proposed hybrid kernel achieves considerably higher precision than the existing approaches, which indicates its capability of learning more accurate models. This also demonstrates that the different types of information that we use are able to complement each other for relation extraction. We believe there are at least three ways to further improve the proposed approach. First of all, the 22 regular expression patterns (collected from Ono et al. (2001) and Bui et al. (2010)) are applied at the level of the sentences and this sometimes produces unwanted matches. For example, consider the sentence “X activates Y and inhibits Z” where X, Y, and Z are entities. The pattern “Entity1. * activates. * Entity2” matches both the X–Y and X–Z pairs in the sentence. But only the X–Y pair should be considered. So, the patterns should be constrained to reduce the number of unwanted matches. For example, they could be applied on smaller linguistic units than full sentences. Secondly, different techniques could be used to identify less-informative syntactic dependencies inside d</context>
</contexts>
<marker>Bui, Katrenko, Sloot, 2010</marker>
<rawString>Quoc-Chinh Bui, Sophia Katrenko, and Peter M.A. Sloot. 2010. A hybrid approach to extract proteinprotein interactions. Bioinformatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Subsequence kernels for relation extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of NIPS</booktitle>
<pages>171--178</pages>
<contexts>
<context position="8539" citStr="Bunescu and Mooney, 2006" startWordPosition="1335" endWordPosition="1338">e results published in the other related works (that usually adopt 10-fold cross validation on a single PPI corpus). To be comparable with the vast majority of the existing work, we also report results using 10-fold cross validation 421 Corpus Sentences Positive pairs Negative pairs BioInfer 1,100 2,534 7,132 AIMed 1,955 1,000 4,834 IEPA 486 335 482 HPRD50 145 163 270 LLL 77 164 166 Table 1: Basic statistics of the 5 benchmark PPI corpora. on single corpora. Apart from the approaches described above, there also exist other studies that used kernels for PPI extraction (e.g. subsequence kernel (Bunescu and Mooney, 2006)). A notable exception is the work published by Bui et al. (2010). They proposed an approach that consists of two phases. In the first phase, their system categorizes the data into different groups (i.e. subsets) based on various properties and patterns. Later they classify candidate PPI pairs inside each of the groups using SVM trained with features specific for the corresponding group. 3 Data There are 5 benchmark corpora for the PPI task that are frequently used: HPRD50 (Fundel et al., 2007), IEPA (Ding et al., 2002), LLL (N´edellec, 2005), BioInfer (Pyysalo et al., 2007) and AIMed (Bunescu</context>
</contexts>
<marker>Bunescu, Mooney, 2006</marker>
<rawString>Razvan Bunescu and Raymond J. Mooney. 2006. Subsequence kernels for relation extraction. In Proceedings of NIPS 2006, pages 171–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Ruifang Ge</author>
<author>Rohit J Kate</author>
<author>Edward M Marcotte</author>
<author>Raymond J Mooney</author>
<author>Arun Kumar Ramani</author>
<author>Yuk Wah Wong</author>
</authors>
<title>Comparative experiments on learning information extractors for proteins and their interactions.</title>
<date>2005</date>
<journal>Artificial Intelligence in Medicine,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="9153" citStr="Bunescu et al., 2005" startWordPosition="1438" endWordPosition="1441">, 2006)). A notable exception is the work published by Bui et al. (2010). They proposed an approach that consists of two phases. In the first phase, their system categorizes the data into different groups (i.e. subsets) based on various properties and patterns. Later they classify candidate PPI pairs inside each of the groups using SVM trained with features specific for the corresponding group. 3 Data There are 5 benchmark corpora for the PPI task that are frequently used: HPRD50 (Fundel et al., 2007), IEPA (Ding et al., 2002), LLL (N´edellec, 2005), BioInfer (Pyysalo et al., 2007) and AIMed (Bunescu et al., 2005). These corpora adopt different PPI annotation formats. For a comparative evaluation Pyysalo et al. (2008) put all of them in a common format which has become the standard evaluation format for the PPI task. In our experiments, we use the versions of the corpora converted to such format. Table 1 shows various statistics regarding the 5 (converted) corpora. 4 Proposed Hybrid Kernel The hybrid kernel that we propose is as follows: KHybrid (R1, R2) = KTPWF (R1, R2) + KSL (R1, R2) + w * KPET (R1, R2) where KTPWF stands for the new feature based kernel (henceforth, TPWF kernel) computed using flat </context>
</contexts>
<marker>Bunescu, Ge, Kate, Marcotte, Mooney, Ramani, Wong, 2005</marker>
<rawString>Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M. Marcotte, Raymond J. Mooney, Arun Kumar Ramani, and Yuk Wah Wong. 2005. Comparative experiments on learning information extractors for proteins and their interactions. Artificial Intelligence in Medicine, 33(2):139–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="22062" citStr="Charniak and Johnson, 2005" startWordPosition="3672" endWordPosition="3676">ive evaluation by Chowdhury et al. (2011a) reported that uPT kernels achieve better results for PPI extraction than the other techniques used for tree kernel computation. 425 5 Experimental Settings We have followed the same criteria commonly used for the PPI extraction tasks, i.e. abstractwise 10-fold cross validation on individual corpus and one-answer-per-occurrence criterion. In fact, we have used exactly the same (abstract-wise) fold splitting of the 5 benchmark (converted) corpora used by Tikk et al. (2010) for benchmarking various kernel methods4. The Charniak-Johnson reranking parser (Charniak and Johnson, 2005), along with a self-trained biomedical parsing model (McClosky, 2010), has been used for tokenization, POS-tagging and parsing of the sentences. Before parsing the sentences, all the entities are blinded by assigning names as EntityX where X is the entity index. In each example, the POS tags of the two candidate entities are changed to EntityX. The parse trees produced by the Charniak-Johnson reranking parser are then processed by the Stanford parser5 (Klein and Manning, 2003) to obtain syntactic dependencies according to the Stanford Typed Dependency format. The Stanford parser often skips so</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Faisal Mahbub Chowdhury</author>
<author>Alberto Lavelli</author>
</authors>
<title>Drug-drug interaction extraction using composite kernels.</title>
<date>2011</date>
<booktitle>In Proceedings of DDIExtraction2011: First Challenge Task: Drug-Drug Interaction Extraction,</booktitle>
<pages>27--33</pages>
<location>Huelva, Spain,</location>
<contexts>
<context position="18277" citStr="Chowdhury and Lavelli, 2011" startWordPosition="3011" endWordPosition="3014">se patterns were previously used by Ono et al. (2001) and Bui et al. (2010). If there is a match for a pattern (e.g. “Entity 1.*activates.*Entity 2” where Entity 1 and Entity 2 form the candidate entity pair) in a given sentence, value 1 is added for the feature (i.e., pattern) inside the feature vector. 4.2 Shallow Linguistic (SL) Kernel The Shallow Linguistic (SL) kernel was proposed by Giuliano et al. (2006). It is one of the best performing kernels applied on different biomedical RE tasks such as PPI and DDI (drug-drug interaction) extraction (Tikk et al., 2010; SeguraBedmar et al., 2011; Chowdhury and Lavelli, 2011b; Chowdhury et al., 2011c). It is defined as follows: KSL (R1, R2) = KLC (R1, R2) + KGC (R1, R2) 424 BioInfer AIMed IEPA HPRD50 LLL Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166 P R F P R F P R F P R F P R F SL kernel – – – 60.9 57.2 59.0 – – – – – – – – – (Giuliano et al., 2006) APG kernel 56.7 67.2 61.3 52.9 61.8 56.4 69.6 82.7 75.1 64.3 65.8 63.4 72.5 87.2 76.8 (Airola et al., 2008) Hybrid kernel and 65.7 71.1 68.1 55.0 68.8 60.8 67.5 78.6 71.7 68.5 76.1 70.9 77.6 86.0 80.1 multiple parser input (Miwa et al., 2009a) SVM-CW, multiple – – 67.6 – – 64.2 – – 74.4 – – 69</context>
</contexts>
<marker>Chowdhury, Lavelli, 2011</marker>
<rawString>Md. Faisal Mahbub Chowdhury and Alberto Lavelli. 2011b. Drug-drug interaction extraction using composite kernels. In Proceedings of DDIExtraction2011: First Challenge Task: Drug-Drug Interaction Extraction, pages 27–33, Huelva, Spain, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Faisal Mahbub Chowdhury</author>
<author>Alberto Lavelli</author>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on dependency tree kernels for automatic extraction of protein-protein interaction.</title>
<date>2011</date>
<booktitle>In Proceedings of BioNLP 2011 Workshop,</booktitle>
<pages>124--133</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="11438" citStr="Chowdhury et al. (2011" startWordPosition="1829" endWordPosition="1832">subgraph of the dependency graph of a sentence which includes: • the two candidate entities and their governor nodes up to their least common governor (if exists). • dependent nodes (if exist) of all the nodes added in the previous step. • the immediate governor(s) (if exists) of the least common governor. Figure 1 shows an example of a reduced graph. A reduced graph is an extension of the smallest common subgraph of the dependency graph that aims at overcoming its limitations. It is a known issue that the smallest common subgraph (or subtree) sometimes does not contain cue words. Previously, Chowdhury et al. (2011a) proposed a linguistically motivated extension of the minimal (i.e. smallest) common subtree (which includes the candidate entity pairs), known as Mildly Extended Dependency Tree (MEDT). However, the rules used for MEDT are too constrained. Our objective in constructing the reduced graph is to include any potential modifier(s) or cue word(s) that describes the relation between the given pair of entities. Sometimes such modifiers or cue words are not directly dependent (syntactically) on any 422 BioInfer AIMed IEPA HPRD50 LLL P R F P R F P R F P R F P R F Only walk features 51.8 71.2 60.0 48.</context>
<context position="14972" citStr="Chowdhury et al., 2011" startWordPosition="2425" endWordPosition="2428"> discarded from the pattern list. The dependency patterns allow some kind of underspecification as they do not contain the lexical items (i.e. words) but contain the likely combination of syntactic dependencies that a given related pair of entities would pose inside their reduced graph. The list of trigger words contains 144 words previously used by Bui et al. (2010) and Fundel et al. (2007). The list of negative cues contain 18 words, most of which are mentioned in Fundel et al. (2007). 4.1.2 Walk features We extract e-walk and v-walk features from the Mildly Extended Dependency Tree (MEDT) (Chowdhury et al., 2011a) of each candidate pair. Reduced graphs sometimes include some unin423 BioInfer AIMed IEPA HPRD50 LLL Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166 P R F P R F P R F P R F P R F Proposed TPWF kernel 53.8 68.8 60.4 50.6 63.9 56.5 63.9 74.6 68.9 65.0 71.8 68.2 66.5 89.6 76.4 (without regex) Proposed TPWF kernel 53.5 68.6 60.1 52.5 62.9 57.2 63.8 74.6 68.8 65.1 69.9 67.5 67.4 88.4 76.5 (with regex) SL kernel 60.8 65.8 63.2 56.2 64.4 60.0 73.3 71.9 72.6 62.0 65.0 63.5 74.9 85.4 79.8 PET kernel 72.8 74.9 73.9 44.8 72.8 55.5 70.7 77.9 74.2 65.0 73.0 68.8 72.1 89.6 79.9 Prop</context>
<context position="18302" citStr="Chowdhury et al., 2011" startWordPosition="3015" endWordPosition="3018">ed by Ono et al. (2001) and Bui et al. (2010). If there is a match for a pattern (e.g. “Entity 1.*activates.*Entity 2” where Entity 1 and Entity 2 form the candidate entity pair) in a given sentence, value 1 is added for the feature (i.e., pattern) inside the feature vector. 4.2 Shallow Linguistic (SL) Kernel The Shallow Linguistic (SL) kernel was proposed by Giuliano et al. (2006). It is one of the best performing kernels applied on different biomedical RE tasks such as PPI and DDI (drug-drug interaction) extraction (Tikk et al., 2010; SeguraBedmar et al., 2011; Chowdhury and Lavelli, 2011b; Chowdhury et al., 2011c). It is defined as follows: KSL (R1, R2) = KLC (R1, R2) + KGC (R1, R2) 424 BioInfer AIMed IEPA HPRD50 LLL Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166 P R F P R F P R F P R F P R F SL kernel – – – 60.9 57.2 59.0 – – – – – – – – – (Giuliano et al., 2006) APG kernel 56.7 67.2 61.3 52.9 61.8 56.4 69.6 82.7 75.1 64.3 65.8 63.4 72.5 87.2 76.8 (Airola et al., 2008) Hybrid kernel and 65.7 71.1 68.1 55.0 68.8 60.8 67.5 78.6 71.7 68.5 76.1 70.9 77.6 86.0 80.1 multiple parser input (Miwa et al., 2009a) SVM-CW, multiple – – 67.6 – – 64.2 – – 74.4 – – 69.7 – – 80.5 parser input </context>
<context position="21475" citStr="Chowdhury et al. (2011" startWordPosition="3585" endWordPosition="3588">005) and other works for relation extraction on general texts (such as newspaper do3Also known as shortest path-enclosed tree (SPT) kernel. main). A PET is the smallest common subtree of a phrase structure tree that includes the two entities involved in a relation. A tree kernel calculates the similarity between two input trees by counting the number of common sub-structures. Different techniques have been proposed to measure such similarity. We use the Unlexicalized Partial Tree (uPT) kernel (Severyn and Moschitti, 2010) for the computation of the PET kernel since a comparative evaluation by Chowdhury et al. (2011a) reported that uPT kernels achieve better results for PPI extraction than the other techniques used for tree kernel computation. 425 5 Experimental Settings We have followed the same criteria commonly used for the PPI extraction tasks, i.e. abstractwise 10-fold cross validation on individual corpus and one-answer-per-occurrence criterion. In fact, we have used exactly the same (abstract-wise) fold splitting of the 5 benchmark (converted) corpora used by Tikk et al. (2010) for benchmarking various kernel methods4. The Charniak-Johnson reranking parser (Charniak and Johnson, 2005), along with </context>
</contexts>
<marker>Chowdhury, Lavelli, Moschitti, 2011</marker>
<rawString>Md. Faisal Mahbub Chowdhury, Alberto Lavelli, and Alessandro Moschitti. 2011a. A study on dependency tree kernels for automatic extraction of protein-protein interaction. In Proceedings of BioNLP 2011 Workshop, pages 124–133, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Faisal Mahbub Chowdhury</author>
<author>Asma Ben Abacha</author>
<author>Alberto Lavelli</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Two dierent machine learning techniques for drugdrug interaction extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of DDIExtraction2011: First Challenge Task: DrugDrug Interaction Extraction,</booktitle>
<pages>pages</pages>
<location>Huelva, Spain,</location>
<contexts>
<context position="11438" citStr="Chowdhury et al. (2011" startWordPosition="1829" endWordPosition="1832">subgraph of the dependency graph of a sentence which includes: • the two candidate entities and their governor nodes up to their least common governor (if exists). • dependent nodes (if exist) of all the nodes added in the previous step. • the immediate governor(s) (if exists) of the least common governor. Figure 1 shows an example of a reduced graph. A reduced graph is an extension of the smallest common subgraph of the dependency graph that aims at overcoming its limitations. It is a known issue that the smallest common subgraph (or subtree) sometimes does not contain cue words. Previously, Chowdhury et al. (2011a) proposed a linguistically motivated extension of the minimal (i.e. smallest) common subtree (which includes the candidate entity pairs), known as Mildly Extended Dependency Tree (MEDT). However, the rules used for MEDT are too constrained. Our objective in constructing the reduced graph is to include any potential modifier(s) or cue word(s) that describes the relation between the given pair of entities. Sometimes such modifiers or cue words are not directly dependent (syntactically) on any 422 BioInfer AIMed IEPA HPRD50 LLL P R F P R F P R F P R F P R F Only walk features 51.8 71.2 60.0 48.</context>
<context position="14972" citStr="Chowdhury et al., 2011" startWordPosition="2425" endWordPosition="2428"> discarded from the pattern list. The dependency patterns allow some kind of underspecification as they do not contain the lexical items (i.e. words) but contain the likely combination of syntactic dependencies that a given related pair of entities would pose inside their reduced graph. The list of trigger words contains 144 words previously used by Bui et al. (2010) and Fundel et al. (2007). The list of negative cues contain 18 words, most of which are mentioned in Fundel et al. (2007). 4.1.2 Walk features We extract e-walk and v-walk features from the Mildly Extended Dependency Tree (MEDT) (Chowdhury et al., 2011a) of each candidate pair. Reduced graphs sometimes include some unin423 BioInfer AIMed IEPA HPRD50 LLL Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166 P R F P R F P R F P R F P R F Proposed TPWF kernel 53.8 68.8 60.4 50.6 63.9 56.5 63.9 74.6 68.9 65.0 71.8 68.2 66.5 89.6 76.4 (without regex) Proposed TPWF kernel 53.5 68.6 60.1 52.5 62.9 57.2 63.8 74.6 68.8 65.1 69.9 67.5 67.4 88.4 76.5 (with regex) SL kernel 60.8 65.8 63.2 56.2 64.4 60.0 73.3 71.9 72.6 62.0 65.0 63.5 74.9 85.4 79.8 PET kernel 72.8 74.9 73.9 44.8 72.8 55.5 70.7 77.9 74.2 65.0 73.0 68.8 72.1 89.6 79.9 Prop</context>
<context position="18302" citStr="Chowdhury et al., 2011" startWordPosition="3015" endWordPosition="3018">ed by Ono et al. (2001) and Bui et al. (2010). If there is a match for a pattern (e.g. “Entity 1.*activates.*Entity 2” where Entity 1 and Entity 2 form the candidate entity pair) in a given sentence, value 1 is added for the feature (i.e., pattern) inside the feature vector. 4.2 Shallow Linguistic (SL) Kernel The Shallow Linguistic (SL) kernel was proposed by Giuliano et al. (2006). It is one of the best performing kernels applied on different biomedical RE tasks such as PPI and DDI (drug-drug interaction) extraction (Tikk et al., 2010; SeguraBedmar et al., 2011; Chowdhury and Lavelli, 2011b; Chowdhury et al., 2011c). It is defined as follows: KSL (R1, R2) = KLC (R1, R2) + KGC (R1, R2) 424 BioInfer AIMed IEPA HPRD50 LLL Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166 P R F P R F P R F P R F P R F SL kernel – – – 60.9 57.2 59.0 – – – – – – – – – (Giuliano et al., 2006) APG kernel 56.7 67.2 61.3 52.9 61.8 56.4 69.6 82.7 75.1 64.3 65.8 63.4 72.5 87.2 76.8 (Airola et al., 2008) Hybrid kernel and 65.7 71.1 68.1 55.0 68.8 60.8 67.5 78.6 71.7 68.5 76.1 70.9 77.6 86.0 80.1 multiple parser input (Miwa et al., 2009a) SVM-CW, multiple – – 67.6 – – 64.2 – – 74.4 – – 69.7 – – 80.5 parser input </context>
<context position="21475" citStr="Chowdhury et al. (2011" startWordPosition="3585" endWordPosition="3588">005) and other works for relation extraction on general texts (such as newspaper do3Also known as shortest path-enclosed tree (SPT) kernel. main). A PET is the smallest common subtree of a phrase structure tree that includes the two entities involved in a relation. A tree kernel calculates the similarity between two input trees by counting the number of common sub-structures. Different techniques have been proposed to measure such similarity. We use the Unlexicalized Partial Tree (uPT) kernel (Severyn and Moschitti, 2010) for the computation of the PET kernel since a comparative evaluation by Chowdhury et al. (2011a) reported that uPT kernels achieve better results for PPI extraction than the other techniques used for tree kernel computation. 425 5 Experimental Settings We have followed the same criteria commonly used for the PPI extraction tasks, i.e. abstractwise 10-fold cross validation on individual corpus and one-answer-per-occurrence criterion. In fact, we have used exactly the same (abstract-wise) fold splitting of the 5 benchmark (converted) corpora used by Tikk et al. (2010) for benchmarking various kernel methods4. The Charniak-Johnson reranking parser (Charniak and Johnson, 2005), along with </context>
</contexts>
<marker>Chowdhury, Abacha, Lavelli, Zweigenbaum, 2011</marker>
<rawString>Md. Faisal Mahbub Chowdhury, Asma Ben Abacha, Alberto Lavelli, and Pierre Zweigenbaum. 2011c. Two dierent machine learning techniques for drugdrug interaction extraction. In Proceedings of DDIExtraction2011: First Challenge Task: DrugDrug Interaction Extraction, pages 19–26, Huelva, Spain, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ding</author>
<author>D Berleant</author>
<author>D Nettleton</author>
<author>E Wurtele</author>
</authors>
<title>Mining MEDLINE: abstracts, sentences, or phrases? Pacific Symposium on Biocomputing,</title>
<date>2002</date>
<pages>326--337</pages>
<contexts>
<context position="9064" citStr="Ding et al., 2002" startWordPosition="1424" endWordPosition="1427">dies that used kernels for PPI extraction (e.g. subsequence kernel (Bunescu and Mooney, 2006)). A notable exception is the work published by Bui et al. (2010). They proposed an approach that consists of two phases. In the first phase, their system categorizes the data into different groups (i.e. subsets) based on various properties and patterns. Later they classify candidate PPI pairs inside each of the groups using SVM trained with features specific for the corresponding group. 3 Data There are 5 benchmark corpora for the PPI task that are frequently used: HPRD50 (Fundel et al., 2007), IEPA (Ding et al., 2002), LLL (N´edellec, 2005), BioInfer (Pyysalo et al., 2007) and AIMed (Bunescu et al., 2005). These corpora adopt different PPI annotation formats. For a comparative evaluation Pyysalo et al. (2008) put all of them in a common format which has become the standard evaluation format for the PPI task. In our experiments, we use the versions of the corpora converted to such format. Table 1 shows various statistics regarding the 5 (converted) corpora. 4 Proposed Hybrid Kernel The hybrid kernel that we propose is as follows: KHybrid (R1, R2) = KTPWF (R1, R2) + KSL (R1, R2) + w * KPET (R1, R2) where KTP</context>
</contexts>
<marker>Ding, Berleant, Nettleton, Wurtele, 2002</marker>
<rawString>J. Ding, D. Berleant, D. Nettleton, and E. Wurtele. 2002. Mining MEDLINE: abstracts, sentences, or phrases? Pacific Symposium on Biocomputing, pages 326–337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Fundel</author>
<author>Robert K¨uffner</author>
<author>Ralf Zimmer</author>
</authors>
<title>Relex–relation extraction using dependency parse trees.</title>
<date>2007</date>
<journal>Bioinformatics,</journal>
<volume>23</volume>
<issue>3</issue>
<marker>Fundel, K¨uffner, Zimmer, 2007</marker>
<rawString>Katrin Fundel, Robert K¨uffner, and Ralf Zimmer. 2007. Relex–relation extraction using dependency parse trees. Bioinformatics, 23(3):365–371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alberto Lavelli</author>
<author>Lorenza Romano</author>
</authors>
<title>Exploiting shallow linguistic information for relation extraction from biomedical literature.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<pages>401--408</pages>
<contexts>
<context position="5108" citStr="Giuliano et al., 2006" startWordPosition="774" endWordPosition="777"> walk features (i.e. e-walks and v-walks)2. 2. The syntactic dependency patterns are automatically collected from a type of dependency subgraph (we call it reduced graph, more details in Section 4.1.1) during runtime. 3. We only use the regex patterns, trigger words and negative cues mentioned in the literature (Ono et al., 2001; Fundel et al., 2007; Bui et al., 2010). The objective is to verify whether we can exploit knowledge which is already known and used. 4. We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004). The aim of our work is to take advantage of different types of information (i.e., dependency patterns, regex patterns, trigger words, negative cues, syntactic dependencies among words and constituent parse trees) and their different representations (i.e. flat features, tree structures and graphs) which can complement each other to learn more accurate models. 2The syntactic dependencies of the words of a sentence create a dependency graph. A v-walk feature consists of (wordi − dependency typei,i+1 − wordi+1), and an ewalk feature is co</context>
<context position="6698" citStr="Giuliano et al., 2006" startWordPosition="1035" endWordPosition="1038">al component kernels. Section 5 outlines the experimental settings. Following that, empirical results are discussed in Section 6. Finally, we conclude with a summary of our study as well as suggestions for further improvement of our approach. 2 Related Work In this section, we briefly discuss some of the recent work on PPI extraction. Several RE approaches have been reported to date for the PPI task, most of which are kernel based methods. Tikk et al. (2010) reported a benchmark evaluation of various kernels on PPI extraction. An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. Kim et al. (2010) proposed walk-weighted subsequence kernel using e-walks, partial matches, non-contiguous paths, and different weights for different sub-structures (which are used to capture structural similarities during kernel computation). Miwa et al. (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al., 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths </context>
<context position="18064" citStr="Giuliano et al. (2006)" startWordPosition="2974" endWordPosition="2977">augmented with -1 or +1 depending on whether the corresponding words are preceded or followed by a candidate entity. 4.1.3 Regular expression patterns We use a set of 22 regex patterns as binary features. These patterns were previously used by Ono et al. (2001) and Bui et al. (2010). If there is a match for a pattern (e.g. “Entity 1.*activates.*Entity 2” where Entity 1 and Entity 2 form the candidate entity pair) in a given sentence, value 1 is added for the feature (i.e., pattern) inside the feature vector. 4.2 Shallow Linguistic (SL) Kernel The Shallow Linguistic (SL) kernel was proposed by Giuliano et al. (2006). It is one of the best performing kernels applied on different biomedical RE tasks such as PPI and DDI (drug-drug interaction) extraction (Tikk et al., 2010; SeguraBedmar et al., 2011; Chowdhury and Lavelli, 2011b; Chowdhury et al., 2011c). It is defined as follows: KSL (R1, R2) = KLC (R1, R2) + KGC (R1, R2) 424 BioInfer AIMed IEPA HPRD50 LLL Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166 P R F P R F P R F P R F P R F SL kernel – – – 60.9 57.2 59.0 – – – – – – – – – (Giuliano et al., 2006) APG kernel 56.7 67.2 61.3 52.9 61.8 56.4 69.6 82.7 75.1 64.3 65.8 63.4 72.5 87.2 </context>
</contexts>
<marker>Giuliano, Lavelli, Romano, 2006</marker>
<rawString>Claudio Giuliano, Alberto Lavelli, and Lorenza Romano. 2006. Exploiting shallow linguistic information for relation extraction from biomedical literature. In Proceedings of EACL 2006, pages 401– 408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CW Hsu</author>
<author>CC Chang</author>
<author>CJ Lin</author>
</authors>
<title>A practical guide to support vector classification.</title>
<date>2003</date>
<institution>Department of Computer Science and Information Engineering, National Taiwan University,</institution>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="23463" citStr="Hsu et al. (2003)" startWordPosition="3915" endWordPosition="3918"> X should be dependent on any word Z on which Y is dependent and vice versa. • If there are two verbs X and Y such that inside the corresponding sentence they have only the word “and” or “or” between them, then any word Z dependent on X should be also dependent on Y and vice versa. Our system exploits SVM-LIGHT-TK6 (Moschitti, 2006; Joachims, 1999). We made minor changes in the toolkit to compute the proposed hybrid kernel. The ratio of negative and positive examples has been used as the value of the costratio-factor parameter. We have done parameter tuning following the approach described by Hsu et al. (2003). 4Downloaded from http://informatik.huberlin.de/forschung /gebiete/wbi/ppi-benchmark . 5http://nlp.stanford.edu/software/lex-parser.shtml 6http://disi.unitn.it/moschitti/Tree-Kernel.htm 6 Results and Discussion To measure the contribution of the features collected from the reduced graphs (using dependency patterns, trigger words and negative cues) and regex patterns, we have applied the new TPWF kernel on the 5 PPI corpora before and after using these features. Results shown in Table 2 clearly indicate that usage of these features improve the performance. The improvement of performance is pri</context>
</contexts>
<marker>Hsu, Chang, Lin, 2003</marker>
<rawString>CW Hsu, CC Chang, and CJ Lin, 2003. A practical guide to support vector classification. Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1999</date>
<booktitle>In Advances in kernel methods: support vector learning,</booktitle>
<pages>169--184</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="23196" citStr="Joachims, 1999" startWordPosition="3872" endWordPosition="3873">rding to the Stanford Typed Dependency format. The Stanford parser often skips some syntactic dependencies in output. We use the following two rules to add some of such dependencies: • If there is a “conj and” or “conj or” dependency between two words X and Y, then X should be dependent on any word Z on which Y is dependent and vice versa. • If there are two verbs X and Y such that inside the corresponding sentence they have only the word “and” or “or” between them, then any word Z dependent on X should be also dependent on Y and vice versa. Our system exploits SVM-LIGHT-TK6 (Moschitti, 2006; Joachims, 1999). We made minor changes in the toolkit to compute the proposed hybrid kernel. The ratio of negative and positive examples has been used as the value of the costratio-factor parameter. We have done parameter tuning following the approach described by Hsu et al. (2003). 4Downloaded from http://informatik.huberlin.de/forschung /gebiete/wbi/ppi-benchmark . 5http://nlp.stanford.edu/software/lex-parser.shtml 6http://disi.unitn.it/moschitti/Tree-Kernel.htm 6 Results and Discussion To measure the contribution of the features collected from the reduced graphs (using dependency patterns, trigger words a</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale support vector machine learning practical. In Advances in kernel methods: support vector learning, pages 169–184. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seonho Kim</author>
<author>Juntae Yoon</author>
<author>Jihoon Yang</author>
<author>Seog Park</author>
</authors>
<title>Walk-weighted subsequence kernels for protein-protein interaction extraction.</title>
<date>2010</date>
<journal>BMC Bioinformatics,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="6842" citStr="Kim et al. (2010)" startWordPosition="1062" endWordPosition="1065">e with a summary of our study as well as suggestions for further improvement of our approach. 2 Related Work In this section, we briefly discuss some of the recent work on PPI extraction. Several RE approaches have been reported to date for the PPI task, most of which are kernel based methods. Tikk et al. (2010) reported a benchmark evaluation of various kernels on PPI extraction. An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. Kim et al. (2010) proposed walk-weighted subsequence kernel using e-walks, partial matches, non-contiguous paths, and different weights for different sub-structures (which are used to capture structural similarities during kernel computation). Miwa et al. (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al., 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). They used multiple parser inputs. The system is regarded as the current state-of-theart PPI extraction system bec</context>
<context position="19190" citStr="Kim et al., 2010" startWordPosition="3210" endWordPosition="3213">l., 2006) APG kernel 56.7 67.2 61.3 52.9 61.8 56.4 69.6 82.7 75.1 64.3 65.8 63.4 72.5 87.2 76.8 (Airola et al., 2008) Hybrid kernel and 65.7 71.1 68.1 55.0 68.8 60.8 67.5 78.6 71.7 68.5 76.1 70.9 77.6 86.0 80.1 multiple parser input (Miwa et al., 2009a) SVM-CW, multiple – – 67.6 – – 64.2 – – 74.4 – – 69.7 – – 80.5 parser input and graph, walk and BOW features (Miwa et al., 2009b) kBSPS kernel 49.9 61.8 55.1 50.1 41.4 44.6 58.8 89.7 70.5 62.2 87.1 71.0 69.3 93.2 78.1 (Tikk et al., 2010) Walk weighted 61.8 54.2 57.6 61.4 53.3 56.6 73.8 71.8 72.9 66.7 69.2 67.8 76.9 91.2 82.4 subsequence kernel (Kim et al., 2010) 2 phase extraction 61.7 57.5 60.0 55.3 68.5 61.2 – – – – – – – – – (Bui et al., 2010) Our proposed hybrid 80.0 71.4 75.5 64.2 58.2 61.1 81.1 69.3 74.7 72.9 59.5 65.5 70.4 95.7 81.1 kernel (PET + SL + TPWF without regex) Table 4: Comparison of the results on the 5 benchmark PPI corpora. Pos. and Neg. refer to number positive and negative relations respectively. The underlined numbers indicate the best results for the corresponding corpus reported by any of the existing state-of-the-art approaches. The results of Bui et al. (2010) on LLL, HPRD50, and IEPA are not reported since thy did not use </context>
</contexts>
<marker>Kim, Yoon, Yang, Park, 2010</marker>
<rawString>Seonho Kim, Juntae Yoon, Jihoon Yang, and Seog Park. 2010. Walk-weighted subsequence kernels for protein-protein interaction extraction. BMC Bioinformatics, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL 2003,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="22543" citStr="Klein and Manning, 2003" startWordPosition="3751" endWordPosition="3754">corpora used by Tikk et al. (2010) for benchmarking various kernel methods4. The Charniak-Johnson reranking parser (Charniak and Johnson, 2005), along with a self-trained biomedical parsing model (McClosky, 2010), has been used for tokenization, POS-tagging and parsing of the sentences. Before parsing the sentences, all the entities are blinded by assigning names as EntityX where X is the entity index. In each example, the POS tags of the two candidate entities are changed to EntityX. The parse trees produced by the Charniak-Johnson reranking parser are then processed by the Stanford parser5 (Klein and Manning, 2003) to obtain syntactic dependencies according to the Stanford Typed Dependency format. The Stanford parser often skips some syntactic dependencies in output. We use the following two rules to add some of such dependencies: • If there is a “conj and” or “conj or” dependency between two words X and Y, then X should be dependent on any word Z on which Y is dependent and vice versa. • If there are two verbs X and Y such that inside the corresponding sentence they have only the word “and” or “or” between them, then any word Z dependent on X should be also dependent on Y and vice versa. Our system exp</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL 2003, pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
</authors>
<title>Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Brown University.</institution>
<contexts>
<context position="22131" citStr="McClosky, 2010" startWordPosition="3684" endWordPosition="3685">er results for PPI extraction than the other techniques used for tree kernel computation. 425 5 Experimental Settings We have followed the same criteria commonly used for the PPI extraction tasks, i.e. abstractwise 10-fold cross validation on individual corpus and one-answer-per-occurrence criterion. In fact, we have used exactly the same (abstract-wise) fold splitting of the 5 benchmark (converted) corpora used by Tikk et al. (2010) for benchmarking various kernel methods4. The Charniak-Johnson reranking parser (Charniak and Johnson, 2005), along with a self-trained biomedical parsing model (McClosky, 2010), has been used for tokenization, POS-tagging and parsing of the sentences. Before parsing the sentences, all the entities are blinded by assigning names as EntityX where X is the entity index. In each example, the POS tags of the two candidate entities are changed to EntityX. The parse trees produced by the Charniak-Johnson reranking parser are then processed by the Stanford parser5 (Klein and Manning, 2003) to obtain syntactic dependencies according to the Stanford Typed Dependency format. The Stanford parser often skips some syntactic dependencies in output. We use the following two rules t</context>
</contexts>
<marker>McClosky, 2010</marker>
<rawString>David McClosky. 2010. Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing. Ph.D. thesis, Department of Computer Science, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Rune Sætre</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Protein-protein interaction extraction by leveraging multiple kernels and parsers.</title>
<date>2009</date>
<journal>International Journal of Medical Informatics,</journal>
<volume>78</volume>
<contexts>
<context position="7086" citStr="Miwa et al. (2009" startWordPosition="1093" endWordPosition="1096"> PPI task, most of which are kernel based methods. Tikk et al. (2010) reported a benchmark evaluation of various kernels on PPI extraction. An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. Kim et al. (2010) proposed walk-weighted subsequence kernel using e-walks, partial matches, non-contiguous paths, and different weights for different sub-structures (which are used to capture structural similarities during kernel computation). Miwa et al. (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al., 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). They used multiple parser inputs. The system is regarded as the current state-of-theart PPI extraction system because of its high results on different PPI corpora (see the results in Table 4). As an extension of their work, they boosted system performance by training on multiple PPI corpora instead of on a single corpus and adopting a corpus weighting con</context>
<context position="18824" citStr="Miwa et al., 2009" startWordPosition="3137" endWordPosition="3140">k et al., 2010; SeguraBedmar et al., 2011; Chowdhury and Lavelli, 2011b; Chowdhury et al., 2011c). It is defined as follows: KSL (R1, R2) = KLC (R1, R2) + KGC (R1, R2) 424 BioInfer AIMed IEPA HPRD50 LLL Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166 P R F P R F P R F P R F P R F SL kernel – – – 60.9 57.2 59.0 – – – – – – – – – (Giuliano et al., 2006) APG kernel 56.7 67.2 61.3 52.9 61.8 56.4 69.6 82.7 75.1 64.3 65.8 63.4 72.5 87.2 76.8 (Airola et al., 2008) Hybrid kernel and 65.7 71.1 68.1 55.0 68.8 60.8 67.5 78.6 71.7 68.5 76.1 70.9 77.6 86.0 80.1 multiple parser input (Miwa et al., 2009a) SVM-CW, multiple – – 67.6 – – 64.2 – – 74.4 – – 69.7 – – 80.5 parser input and graph, walk and BOW features (Miwa et al., 2009b) kBSPS kernel 49.9 61.8 55.1 50.1 41.4 44.6 58.8 89.7 70.5 62.2 87.1 71.0 69.3 93.2 78.1 (Tikk et al., 2010) Walk weighted 61.8 54.2 57.6 61.4 53.3 56.6 73.8 71.8 72.9 66.7 69.2 67.8 76.9 91.2 82.4 subsequence kernel (Kim et al., 2010) 2 phase extraction 61.7 57.5 60.0 55.3 68.5 61.2 – – – – – – – – – (Bui et al., 2010) Our proposed hybrid 80.0 71.4 75.5 64.2 58.2 61.1 81.1 69.3 74.7 72.9 59.5 65.5 70.4 95.7 81.1 kernel (PET + SL + TPWF without regex) Table 4: Comp</context>
</contexts>
<marker>Miwa, Sætre, Miyao, Tsujii, 2009</marker>
<rawString>Makoto Miwa, Rune Sætre, Yusuke Miyao, and Jun’ichi Tsujii. 2009a. Protein-protein interaction extraction by leveraging multiple kernels and parsers. International Journal of Medical Informatics, 78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Rune Sætre</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A rich feature vector for protein-protein interaction extraction from multiple corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP 2009,</booktitle>
<pages>121--130</pages>
<contexts>
<context position="7086" citStr="Miwa et al. (2009" startWordPosition="1093" endWordPosition="1096"> PPI task, most of which are kernel based methods. Tikk et al. (2010) reported a benchmark evaluation of various kernels on PPI extraction. An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. Kim et al. (2010) proposed walk-weighted subsequence kernel using e-walks, partial matches, non-contiguous paths, and different weights for different sub-structures (which are used to capture structural similarities during kernel computation). Miwa et al. (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al., 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). They used multiple parser inputs. The system is regarded as the current state-of-theart PPI extraction system because of its high results on different PPI corpora (see the results in Table 4). As an extension of their work, they boosted system performance by training on multiple PPI corpora instead of on a single corpus and adopting a corpus weighting con</context>
<context position="18824" citStr="Miwa et al., 2009" startWordPosition="3137" endWordPosition="3140">k et al., 2010; SeguraBedmar et al., 2011; Chowdhury and Lavelli, 2011b; Chowdhury et al., 2011c). It is defined as follows: KSL (R1, R2) = KLC (R1, R2) + KGC (R1, R2) 424 BioInfer AIMed IEPA HPRD50 LLL Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166 P R F P R F P R F P R F P R F SL kernel – – – 60.9 57.2 59.0 – – – – – – – – – (Giuliano et al., 2006) APG kernel 56.7 67.2 61.3 52.9 61.8 56.4 69.6 82.7 75.1 64.3 65.8 63.4 72.5 87.2 76.8 (Airola et al., 2008) Hybrid kernel and 65.7 71.1 68.1 55.0 68.8 60.8 67.5 78.6 71.7 68.5 76.1 70.9 77.6 86.0 80.1 multiple parser input (Miwa et al., 2009a) SVM-CW, multiple – – 67.6 – – 64.2 – – 74.4 – – 69.7 – – 80.5 parser input and graph, walk and BOW features (Miwa et al., 2009b) kBSPS kernel 49.9 61.8 55.1 50.1 41.4 44.6 58.8 89.7 70.5 62.2 87.1 71.0 69.3 93.2 78.1 (Tikk et al., 2010) Walk weighted 61.8 54.2 57.6 61.4 53.3 56.6 73.8 71.8 72.9 66.7 69.2 67.8 76.9 91.2 82.4 subsequence kernel (Kim et al., 2010) 2 phase extraction 61.7 57.5 60.0 55.3 68.5 61.2 – – – – – – – – – (Bui et al., 2010) Our proposed hybrid 80.0 71.4 75.5 64.2 58.2 61.1 81.1 69.3 74.7 72.9 59.5 65.5 70.4 95.7 81.1 kernel (PET + SL + TPWF without regex) Table 4: Comp</context>
</contexts>
<marker>Miwa, Sætre, Miyao, Tsujii, 2009</marker>
<rawString>Makoto Miwa, Rune Sætre, Yusuke Miyao, and Jun’ichi Tsujii. 2009b. A rich feature vector for protein-protein interaction extraction from multiple corpora. In Proceedings of EMNLP 2009, pages 121–130, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL 2004,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="5166" citStr="Moschitti, 2004" startWordPosition="784" endWordPosition="785">endency patterns are automatically collected from a type of dependency subgraph (we call it reduced graph, more details in Section 4.1.1) during runtime. 3. We only use the regex patterns, trigger words and negative cues mentioned in the literature (Ono et al., 2001; Fundel et al., 2007; Bui et al., 2010). The objective is to verify whether we can exploit knowledge which is already known and used. 4. We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004). The aim of our work is to take advantage of different types of information (i.e., dependency patterns, regex patterns, trigger words, negative cues, syntactic dependencies among words and constituent parse trees) and their different representations (i.e. flat features, tree structures and graphs) which can complement each other to learn more accurate models. 2The syntactic dependencies of the words of a sentence create a dependency graph. A v-walk feature consists of (wordi − dependency typei,i+1 − wordi+1), and an ewalk feature is composed of (dependency typei_1,i − wordi − dependency typei</context>
<context position="20772" citStr="Moschitti (2004)" startWordPosition="3474" endWordPosition="3475">ring results. All the results of the previous approaches reported in this table are directly quoted from their respective original papers. where KSL, KGC and KLC correspond to SL, global context (GC) and local context (LC) kernels respectively. The GC kernel exploits contextual information of the words occurring before, between and after the pair of entities (to be investigated for RE) in the corresponding sentence; while the LC kernel exploits contextual information surrounding individual entities. 4.3 Path-enclosed tree (PET) Kernel The path-enclosed tree (PET) kernel3 was first proposed by Moschitti (2004) for semantic role labeling. It was later successfully adapted by Zhang et al. (2005) and other works for relation extraction on general texts (such as newspaper do3Also known as shortest path-enclosed tree (SPT) kernel. main). A PET is the smallest common subtree of a phrase structure tree that includes the two entities involved in a relation. A tree kernel calculates the similarity between two input trees by counting the number of common sub-structures. Different techniques have been proposed to measure such similarity. We use the Unlexicalized Partial Tree (uPT) kernel (Severyn and Moschitt</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow semantic parsing. In Proceedings of ACL 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making Tree Kernels Practical for Natural Language Learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL 2006,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="7255" citStr="Moschitti, 2006" startWordPosition="1120" endWordPosition="1121"> the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. Kim et al. (2010) proposed walk-weighted subsequence kernel using e-walks, partial matches, non-contiguous paths, and different weights for different sub-structures (which are used to capture structural similarities during kernel computation). Miwa et al. (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al., 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). They used multiple parser inputs. The system is regarded as the current state-of-theart PPI extraction system because of its high results on different PPI corpora (see the results in Table 4). As an extension of their work, they boosted system performance by training on multiple PPI corpora instead of on a single corpus and adopting a corpus weighting concept with support vector machine (SVM) which they call SVM-CW (Miwa et al., 2009b). Since most of their results are reported by training on the combination of multiple c</context>
<context position="23179" citStr="Moschitti, 2006" startWordPosition="3869" endWordPosition="3871">dependencies according to the Stanford Typed Dependency format. The Stanford parser often skips some syntactic dependencies in output. We use the following two rules to add some of such dependencies: • If there is a “conj and” or “conj or” dependency between two words X and Y, then X should be dependent on any word Z on which Y is dependent and vice versa. • If there are two verbs X and Y such that inside the corresponding sentence they have only the word “and” or “or” between them, then any word Z dependent on X should be also dependent on Y and vice versa. Our system exploits SVM-LIGHT-TK6 (Moschitti, 2006; Joachims, 1999). We made minor changes in the toolkit to compute the proposed hybrid kernel. The ratio of negative and positive examples has been used as the value of the costratio-factor parameter. We have done parameter tuning following the approach described by Hsu et al. (2003). 4Downloaded from http://informatik.huberlin.de/forschung /gebiete/wbi/ppi-benchmark . 5http://nlp.stanford.edu/software/lex-parser.shtml 6http://disi.unitn.it/moschitti/Tree-Kernel.htm 6 Results and Discussion To measure the contribution of the features collected from the reduced graphs (using dependency patterns</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Making Tree Kernels Practical for Natural Language Learning. In Proceedings of EACL 2006, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire N´edellec</author>
</authors>
<title>Learning language in logic -genic interaction extraction challenge.</title>
<date>2005</date>
<booktitle>Proceedings of the ICML 2005 workshop: Learning Language in Logic (LLL05),</booktitle>
<pages>31--37</pages>
<marker>N´edellec, 2005</marker>
<rawString>Claire N´edellec. 2005. Learning language in logic -genic interaction extraction challenge. Proceedings of the ICML 2005 workshop: Learning Language in Logic (LLL05), pages 31–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshihide Ono</author>
</authors>
<title>Haretsugu Hishigaki, Akira Tanigami, and Toshihisa Takagi.</title>
<date>2001</date>
<journal>Bioinformatics,</journal>
<volume>17</volume>
<issue>2</issue>
<marker>Ono, 2001</marker>
<rawString>Toshihide Ono, Haretsugu Hishigaki, Akira Tanigami, and Toshihisa Takagi. 2001. Automated extraction of information on protein–protein interactions from the biological literature. Bioinformatics, 17(2):155–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Filip Ginter</author>
<author>Juho Heimonen</author>
<author>Jari Bj¨orne</author>
<author>Jorma Boberg</author>
<author>Jouni Jarvinen</author>
<author>Tapio Salakoski</author>
</authors>
<title>Bioinfer: a corpus for information extraction in the biomedical domain.</title>
<date>2007</date>
<journal>BMC Bioinformatics,</journal>
<volume>8</volume>
<issue>1</issue>
<marker>Pyysalo, Ginter, Heimonen, Bj¨orne, Boberg, Jarvinen, Salakoski, 2007</marker>
<rawString>Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari Bj¨orne, Jorma Boberg, Jouni Jarvinen, and Tapio Salakoski. 2007. Bioinfer: a corpus for information extraction in the biomedical domain. BMC Bioinformatics, 8(1):50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Antti Airola</author>
<author>Juho Heimonen</author>
<author>Jari Bj¨orne</author>
<author>Filip Ginter</author>
<author>Tapio Salakoski</author>
</authors>
<title>Comparative analysis of five protein-protein interaction corpora.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<pages>3--6</pages>
<marker>Pyysalo, Airola, Heimonen, Bj¨orne, Ginter, Salakoski, 2008</marker>
<rawString>Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari Bj¨orne, Filip Ginter, and Tapio Salakoski. 2008. Comparative analysis of five protein-protein interaction corpora. BMC Bioinformatics, 9(Suppl 3):S6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorenza Romano</author>
<author>Milen Kouylekov</author>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
<author>Alberto Lavelli</author>
</authors>
<title>Investigating a generic paraphrase–based approach for relation extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<pages>409--416</pages>
<marker>Romano, Kouylekov, Szpektor, Dagan, Lavelli, 2006</marker>
<rawString>Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido Dagan, and Alberto Lavelli. 2006. Investigating a generic paraphrase–based approach for relation extraction. In Proceedings of EACL 2006, pages 409–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabel Segura-Bedmar</author>
<author>Paloma Mart´ınez</author>
<author>Cesar de Pablo-S´anchez</author>
</authors>
<title>Using a shallow linguistic kernel for drug-drug interaction extraction.</title>
<date>2011</date>
<journal>Journal of Biomedical Informatics, In Press, Corrected Proof, Available online,</journal>
<volume>24</volume>
<marker>Segura-Bedmar, Mart´ınez, de Pablo-S´anchez, 2011</marker>
<rawString>Isabel Segura-Bedmar, Paloma Mart´ınez, and Cesar de Pablo-S´anchez. 2011. Using a shallow linguistic kernel for drug-drug interaction extraction. Journal of Biomedical Informatics, In Press, Corrected Proof, Available online, 24 April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Fast cutting plane training for structural kernels.</title>
<date>2010</date>
<booktitle>In Proceedings of ECML-PKDD</booktitle>
<contexts>
<context position="10384" citStr="Severyn and Moschitti, 2010" startWordPosition="1646" endWordPosition="1649">tures collected by exploiting patterns, trigger words, negative cues and walk features. KSL and KPET stand for the Shallow Linguistic (SL) kernel and the Path-enclosed Tree (PET) kernel respectively. w is a multiplicative constant used for the PET kernel. It allows the hybrid kernel to assign more (or less) weight to the information obtained using tree structures depending on the corpus. The proposed hybrid kernel is valid according to the closure properties of kernels. Both the TPWF and SL kernels are linear kernels, while PET kernel is computed using Unlexicalized Partial Tree (uPT) kernel (Severyn and Moschitti, 2010). The following subsections explain each of the individual kernels in more detail. 4.1 Proposed TPWF Kernel 4.1.1 Reduced graph, trigger words, negative cues and dependency patterns For each of the candidate entity pairs, we construct a type of subgraph from the dependency graph formed by the syntactic dependencies among the words of a sentence. We call it “reduced graph” and define it in the following way: A reduced graph is a subgraph of the dependency graph of a sentence which includes: • the two candidate entities and their governor nodes up to their least common governor (if exists). • de</context>
<context position="21380" citStr="Severyn and Moschitti, 2010" startWordPosition="3568" endWordPosition="3572"> by Moschitti (2004) for semantic role labeling. It was later successfully adapted by Zhang et al. (2005) and other works for relation extraction on general texts (such as newspaper do3Also known as shortest path-enclosed tree (SPT) kernel. main). A PET is the smallest common subtree of a phrase structure tree that includes the two entities involved in a relation. A tree kernel calculates the similarity between two input trees by counting the number of common sub-structures. Different techniques have been proposed to measure such similarity. We use the Unlexicalized Partial Tree (uPT) kernel (Severyn and Moschitti, 2010) for the computation of the PET kernel since a comparative evaluation by Chowdhury et al. (2011a) reported that uPT kernels achieve better results for PPI extraction than the other techniques used for tree kernel computation. 425 5 Experimental Settings We have followed the same criteria commonly used for the PPI extraction tasks, i.e. abstractwise 10-fold cross validation on individual corpus and one-answer-per-occurrence criterion. In fact, we have used exactly the same (abstract-wise) fold splitting of the 5 benchmark (converted) corpora used by Tikk et al. (2010) for benchmarking various k</context>
</contexts>
<marker>Severyn, Moschitti, 2010</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2010. Fast cutting plane training for structural kernels. In Proceedings of ECML-PKDD 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Domonkos Tikk</author>
<author>Philippe Thomas</author>
<author>Peter Palaga</author>
<author>J¨org Hakenberg</author>
<author>Ulf Leser</author>
</authors>
<date>2010</date>
<booktitle>A Comprehensive Benchmark of Kernel Methods to Extract Protein-Protein Interactions from Literature. PLoS Computational Biology,</booktitle>
<volume>6</volume>
<issue>7</issue>
<contexts>
<context position="3636" citStr="Tikk et al., 2010" startWordPosition="535" endWordPosition="538">c in biomedical relation extraction. PPI1 information is very critical in understanding biological processes. Considerable progress has been made for this task. Nevertheless, empirical results of previous studies show that none of the approaches already known in the literature is consistently better than other approaches when evaluated on different benchmark PPI corpora (see Table 4). This demands further study and innovation 1PPIs occur when two or more proteins bind together, and are integral to virtually all cellular processes, such as metabolism, signalling, regulation, and proliferation (Tikk et al., 2010). 420 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 420–429, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics of new approaches that are sensitive to the variations of complex linguistic constructions. The proposed hybrid kernel is the composition of one tree kernel and two feature based kernels (one of them is already known in the literature and the other is proposed in this paper for the first time). The novelty of the newly proposed feature based kernel is that it envisages to accommoda</context>
<context position="6538" citStr="Tikk et al. (2010)" startWordPosition="1009" endWordPosition="1012">Section 2, we briefly review previous work. Section 3 lists the datasets. Then, in Section 4, we define our proposed hybrid kernel and describe its individual component kernels. Section 5 outlines the experimental settings. Following that, empirical results are discussed in Section 6. Finally, we conclude with a summary of our study as well as suggestions for further improvement of our approach. 2 Related Work In this section, we briefly discuss some of the recent work on PPI extraction. Several RE approaches have been reported to date for the PPI task, most of which are kernel based methods. Tikk et al. (2010) reported a benchmark evaluation of various kernels on PPI extraction. An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. Kim et al. (2010) proposed walk-weighted subsequence kernel using e-walks, partial matches, non-contiguous paths, and different weights for different sub-structures (which are used to capture structural similarities during kernel computation). Miwa et al. (2009a) proposed a hybrid kernel, which combines the all-</context>
<context position="18221" citStr="Tikk et al., 2010" startWordPosition="3002" endWordPosition="3005">t of 22 regex patterns as binary features. These patterns were previously used by Ono et al. (2001) and Bui et al. (2010). If there is a match for a pattern (e.g. “Entity 1.*activates.*Entity 2” where Entity 1 and Entity 2 form the candidate entity pair) in a given sentence, value 1 is added for the feature (i.e., pattern) inside the feature vector. 4.2 Shallow Linguistic (SL) Kernel The Shallow Linguistic (SL) kernel was proposed by Giuliano et al. (2006). It is one of the best performing kernels applied on different biomedical RE tasks such as PPI and DDI (drug-drug interaction) extraction (Tikk et al., 2010; SeguraBedmar et al., 2011; Chowdhury and Lavelli, 2011b; Chowdhury et al., 2011c). It is defined as follows: KSL (R1, R2) = KLC (R1, R2) + KGC (R1, R2) 424 BioInfer AIMed IEPA HPRD50 LLL Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166 P R F P R F P R F P R F P R F SL kernel – – – 60.9 57.2 59.0 – – – – – – – – – (Giuliano et al., 2006) APG kernel 56.7 67.2 61.3 52.9 61.8 56.4 69.6 82.7 75.1 64.3 65.8 63.4 72.5 87.2 76.8 (Airola et al., 2008) Hybrid kernel and 65.7 71.1 68.1 55.0 68.8 60.8 67.5 78.6 71.7 68.5 76.1 70.9 77.6 86.0 80.1 multiple parser input (Miwa et al., 2</context>
<context position="21953" citStr="Tikk et al. (2010)" startWordPosition="3659" endWordPosition="3662">ee (uPT) kernel (Severyn and Moschitti, 2010) for the computation of the PET kernel since a comparative evaluation by Chowdhury et al. (2011a) reported that uPT kernels achieve better results for PPI extraction than the other techniques used for tree kernel computation. 425 5 Experimental Settings We have followed the same criteria commonly used for the PPI extraction tasks, i.e. abstractwise 10-fold cross validation on individual corpus and one-answer-per-occurrence criterion. In fact, we have used exactly the same (abstract-wise) fold splitting of the 5 benchmark (converted) corpora used by Tikk et al. (2010) for benchmarking various kernel methods4. The Charniak-Johnson reranking parser (Charniak and Johnson, 2005), along with a self-trained biomedical parsing model (McClosky, 2010), has been used for tokenization, POS-tagging and parsing of the sentences. Before parsing the sentences, all the entities are blinded by assigning names as EntityX where X is the entity index. In each example, the POS tags of the two candidate entities are changed to EntityX. The parse trees produced by the Charniak-Johnson reranking parser are then processed by the Stanford parser5 (Klein and Manning, 2003) to obtain</context>
</contexts>
<marker>Tikk, Thomas, Palaga, Hakenberg, Leser, 2010</marker>
<rawString>Domonkos Tikk, Philippe Thomas, Peter Palaga, J¨org Hakenberg, and Ulf Leser. 2010. A Comprehensive Benchmark of Kernel Methods to Extract Protein-Protein Interactions from Literature. PLoS Computational Biology, 6(7), July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jian Su</author>
<author>Danmei Wang</author>
<author>Guodong Zhou</author>
<author>Chew Lim Tan</author>
</authors>
<title>Discovering relations between named entities from a large raw corpus using tree similarity-based clustering.</title>
<date>2005</date>
<booktitle>In Natural Language Processing – IJCNLP</booktitle>
<volume>3651</volume>
<pages>378--389</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="20857" citStr="Zhang et al. (2005)" startWordPosition="3486" endWordPosition="3489">e directly quoted from their respective original papers. where KSL, KGC and KLC correspond to SL, global context (GC) and local context (LC) kernels respectively. The GC kernel exploits contextual information of the words occurring before, between and after the pair of entities (to be investigated for RE) in the corresponding sentence; while the LC kernel exploits contextual information surrounding individual entities. 4.3 Path-enclosed tree (PET) Kernel The path-enclosed tree (PET) kernel3 was first proposed by Moschitti (2004) for semantic role labeling. It was later successfully adapted by Zhang et al. (2005) and other works for relation extraction on general texts (such as newspaper do3Also known as shortest path-enclosed tree (SPT) kernel. main). A PET is the smallest common subtree of a phrase structure tree that includes the two entities involved in a relation. A tree kernel calculates the similarity between two input trees by counting the number of common sub-structures. Different techniques have been proposed to measure such similarity. We use the Unlexicalized Partial Tree (uPT) kernel (Severyn and Moschitti, 2010) for the computation of the PET kernel since a comparative evaluation by Chow</context>
</contexts>
<marker>Zhang, Su, Wang, Zhou, Tan, 2005</marker>
<rawString>Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and Chew Lim Tan. 2005. Discovering relations between named entities from a large raw corpus using tree similarity-based clustering. In Natural Language Processing – IJCNLP 2005, volume 3651 of Lecture Notes in Computer Science, pages 378–389. Springer Berlin / Heidelberg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>