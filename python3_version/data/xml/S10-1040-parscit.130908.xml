<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002125">
<title confidence="0.933217">
SZTERGAK : Feature Engineering for Keyphrase Extraction
</title>
<author confidence="0.994134">
G´abor Berend
</author>
<affiliation confidence="0.847460666666667">
Department of Informatics
University of Szeged
2. ´Arp´ad t´er Szeged, H-6720, Hungary
</affiliation>
<email confidence="0.995244">
berendg@inf.u-szeged.hu
</email>
<note confidence="0.41626">
Rich´ard Farkas
Hungarian Academy of Sciences
103. Tisza Lajos k¨or´ut
Szeged, H-6720, Hungary
</note>
<email confidence="0.997757">
rfarkas@inf.u-szeged.hu
</email>
<sectionHeader confidence="0.993875" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998807466666667">
Automatically assigning keyphrases to
documents has a great variety of applica-
tions. Here we focus on the keyphrase
extraction of scientific publications and
present a novel set of features for the su-
pervised learning of keyphraseness. Al-
though these features are intended for ex-
tracting keyphrases from scientific papers,
because of their generality and robust-
ness, they should have uses in other do-
mains as well. With the help of these fea-
tures SZTERGAK achieved top results on
the SemEval-2 shared task on Automatic
Keyphrase Extraction from Scientific Arti-
cles and exceeded its baseline by 10%.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999165">
Keyphrases summarize the content of documents
with the most important phrases. They can be
valuable in many application areas, ranging from
information retrieval to topic detection. However,
since manually assigned keyphrases are rarely pro-
vided and creating them by hand would be costly
and time-consuming, their automatic generation
is of great interest nowadays. Recent state-of-
the-art systems treat this kind of task as a super-
vised learning task, in which phrases of a docu-
ment should be classified with respect to their key
phrase characteristics based on manually labeled
corpora and various feature values.
This paper focuses on the task of keyphrase ex-
traction from scientific papers and we shall intro-
duce new features that can significantly improve
the overall performance. Although the experimen-
tal results presented here are solely based on sci-
entific articles, due to the robustness and univer-
sality of the features, our approach is expected to
achieve good results when applied on other do-
mains as well.
</bodyText>
<sectionHeader confidence="0.99955" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999938944444444">
In keyphrase extraction tasks, phrases are ex-
tracted from one document that are the most char-
acteristic of its content (Liu et al., 2009; Wit-
ten et al., 1999). In these approaches keyphrase
extraction is treated as a classification task, in
which certain n-grams of a specific document act
as keyphrase candidates, and the task is to classify
them as proper keyphrases or not.
While Frank et al. (1999) exploited domain spe-
cific knowledge to improve the quality of auto-
matic tagging, others like Liu et al. (2009) analyze
term co-occurence graphs. It was Nguyen and Kan
(2007) who dealt with the special characteristics of
scientific papers and introduced the state-of-the-
art feature set to keyphrase extraction tasks. Here
we will follow a similar approach and make sig-
nificant improvements by the introduction of novel
features.
</bodyText>
<sectionHeader confidence="0.99153" genericHeader="method">
3 The SZTERGAK system
</sectionHeader>
<bodyText confidence="0.999913">
The SZTERGAK framework treats the reproduc-
tion of reader-assigned keyphrases as a supervised
learning task. In our setting a restricted set of to-
ken sequences extracted from the documents was
used as classification instances. These instances
were ranked regarding to their posteriori proba-
bilities of the keyphrase class, estimated by a
Naive Bayes classifier. Finally, we chose the top-
15 candidates as keyphrases.
Our features can be grouped into four main cat-
egories: those that were calculated solely from
the surface characteristics of phrases, those that
took into account the document that contained a
keyphrase, those that were obtained from the given
document set and those that were based on exter-
nal sources of information.
</bodyText>
<page confidence="0.984011">
186
</page>
<bodyText confidence="0.451067">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 186–189,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.998606">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999918108108108">
Since there are parts of a document (e.g. tables
or author affiliations) that can not really contribute
to the keyphrase extractor, several preprocessing
steps were carried out. Preprocessing included the
elimination of author affiliations and messy lines.
The determination of the full title of an article
would be useful, however, it is not straightforward
because of multi-line titles. To solve this prob-
lem, a web query was sent with the first line of
a document and its most likely title was chosen
by simply selecting the most frequently occurring
one among the top 10 responses provided by the
Google API. This title was added to the document,
and all the lines before the first occurrence of the
line Abstract were omitted.
Lines unlikely to contain valuable information
were also excluded from the documents. These
lines were identified according to statistical data
of their surface forms (e.g. the average and
the deviation of line lengths) and regular expres-
sions. Lastly, section and sentence boundaries
were found in a rule-based way, and the POS and
syntactic tagging (using the Stanford parser (Klein
and Manning, 2003)) of each sentence were car-
ried out.
When syntactically parsed sentences were ob-
tained, keyphrase aspirants were extracted. The 1
to 4-long token sequences that did not start or end
with a stopword and consisted only of POS-codes
of an adjective, a noun or a verb were de-
fined to be possible keyphrases (resulting in classi-
fication instances). Tokens of key phrase aspirants
were stemmed to store them in a uniform way, but
they were also appended by the POS-code of the
derived form, so that the same root forms were dis-
tinguished if they came from tokens having differ-
ent POS-codes, like there shown in Table 1.
</bodyText>
<table confidence="0.9824148">
Textual Appearance Canonical form
regulations regul nns
Regulation regul nn
regulates regul vbz
regulated regul vbn
</table>
<tableCaption confidence="0.999869">
Table 1: Standardization of document terms.
</tableCaption>
<subsectionHeader confidence="0.9984">
3.2 The extended feature set
</subsectionHeader>
<bodyText confidence="0.999995375">
The features characterizing the extracted
keyphrase aspirants can be grouped into four
main types, namely phrase-, document-, corpus-
level and external knowledge-based features.
Below we will describe the different types of
features as well as those of KEA (Witten et al.,
1999) which are cited as default features by most
of the literature dealing with keyphrase extraction.
</bodyText>
<subsectionHeader confidence="0.743502">
3.2.1 Standard features
</subsectionHeader>
<bodyText confidence="0.999938666666667">
Features belonging to this set contain those of
KEA, namely Tf-idf and the first occurrence.
The Tf-idf feature assigns the tf-idf metric to
each keyphrase aspirant.
The first occurrence feature contains the rela-
tive first position for each keyphrase aspirant. The
feature value was obtained by dividing the abso-
lute first token position of a phrase by the number
of tokens of the document in question.
</bodyText>
<subsubsectionHeader confidence="0.784139">
3.2.2 Phrase-level features
</subsubsectionHeader>
<bodyText confidence="0.9948600625">
Features belonging to this group were calcu-
lated solely based on the keyphrase aspirants
themselves. Such features are able to get the
general characteristics of phrases functioning as
keyphrases.
Phrase length feature contains the number of
tokens a keyphrase aspirant consists of.
POS feature is a nominal one that stores
the POS-code sequence of each keyphrase aspi-
rant. (For example, for the phrase full JJ
space NN its value was JJ NN.)
Suffix feature is a binary feature that stores
information about whether the original form of
a keyphrase aspirant finished with some specific
ending according to a subset of the Michigan Suf-
ficiency Exams’ Suffix List. 1
</bodyText>
<subsubsectionHeader confidence="0.508869">
3.2.3 Document-level features
</subsubsectionHeader>
<bodyText confidence="0.999151">
Since keyphrases should summarize the particular
document they represent, and phrase-level features
introduced above were independent of their con-
text, document-level features were also invented.
Acronymity feature functions as a binary fea-
ture that is assigned a true value iff a phrase is
likely to be an extended form of an acronym in the
same document. A phrase is treated as an extended
form of an acronym if it starts with the same letter
as the acronym present in its document and it also
contains all the letters of the acronym in the very
same order as they occur in the acronym.
PMI feature provides a measure of the mul-
tiword expression nature of multi-token phrases,
</bodyText>
<footnote confidence="0.9261155">
1http://www.michigan-proficiency-exams.com/suffix-
list.html
</footnote>
<page confidence="0.994089">
187
</page>
<bodyText confidence="0.9997692">
and it is defined in Eq. (1), where p(ti) is the
document-level probability of the occurrence of
ith token in the phrase. This feature value is a gen-
eralized form of pointwise mutual information for
phrases with an arbitrary number of tokens.
</bodyText>
<equation confidence="0.9998395">
log(p(t1,t2,...,t.)
p(t1)·p(t2)·...·p(t.))
log(p(t1, t2, ..., tn))n−1
(1)
</equation>
<bodyText confidence="0.9999655">
Syntactic feature values refer to the average
minimal normalized depth of the NP-rooted parse
subtrees that contain a given keyphrase aspirant at
the leaf nodes in a given document.
</bodyText>
<subsectionHeader confidence="0.425324">
3.2.4 Corpus-level features
</subsectionHeader>
<bodyText confidence="0.999188">
Corpus-level features are used to determine the
relative importance of keyphrase aspirants based
on a comparison of corpus-level and document-
level frequencies.
The sf-isf feature was created to deal with logi-
cal positions of keyphrases and the formula shown
in Eq. (2) resembles that of tf-idf scores (hence
its name, i.e. Section Frequency-Inverted Section
Frequency). This feature value favors keyphrase
aspirants k that are included in several sections of
document d (sf), but are present in a relatively
small number of sections in the overall corpus
(isf). Phrases with higher sf-isf scores for a given
document are those that are more relevant with re-
spect to that document.
</bodyText>
<equation confidence="0.850387">
sfisf(k, d) = sf(k, d) ∗ isf(k) (2)
</equation>
<bodyText confidence="0.990572">
Keyphraseness feature is a binary one which
has a true value iff a phrase is one of the 785 dif-
ferent author-assigned keyphrases provided in the
training and test corpora.
</bodyText>
<subsectionHeader confidence="0.454627">
3.2.5 External knowledge-based features
</subsectionHeader>
<bodyText confidence="0.9999165">
Apart from relying on the given corpus, further en-
hancements in performance can be obtained by re-
lying on external knowledge sources.
Wikipedia-feature is assigned a true value
for keyphrase aspirants for which there exists a
Wikipedia article with the same title. Preliminary
experiments showed that this feature is noisy, thus
we also investigated a relaxed version of it, where
occurrences of Wikipedia article titles were looked
for only in the title and abstract of a paper.
Besides using Wikipedia for feature calculation,
it was also utilized to retrieve semantic orienta-
tions of phrases. Making use of redirect links of
Wikipedia, the semantic relation of synonymity
</bodyText>
<table confidence="0.9995734375">
Feature combinations F-score
Standard features (SF) 14.57
SF + phrase length feature 20.93
SF + POS feature 19.60
SF + suffix feature 16.35
SF + acronymity feature 16.87
SF + PMI feature 15.68
SF + syntactic feature 14.20
SF + sf-isf feature 14.79
SF + keyphraseness feature 15.17
SF + Wikipedia feature - full paper 14.37
SF + Wikipedia feature - abstract 16.50
SF + Wikipedia redirect 14.50
Shared Task best baseline 12.87
All features 23.82
All features - keyphraseness excluded 22.11
</table>
<tableCaption confidence="0.999835">
Table 2: Results obtained with different features.
</tableCaption>
<bodyText confidence="0.999941571428572">
can be exploited. For example, as there exists a
redirection between Wikipedia articles XML and
Extensible Markup Language, it may be
assumed that these phrases mean the same. For
this reason during the training phase we treated
a phrase equivalent to its redirected version, i.e.
if there is a keyphrase aspirant that is not as-
signed in the gold-standard reader annotation but
the Wikipedia article with the same title has a redi-
rection to such a phrase that is present among pos-
itive keyphrase instances of a particular document,
the original phrase can be treated as a positive in-
stance as well. In this way the ratio of positive ex-
amples could be increased from 0.99% to 1.14%.
</bodyText>
<sectionHeader confidence="0.998898" genericHeader="evaluation">
4 Results and discussion
</sectionHeader>
<bodyText confidence="0.9999436">
The training and test sets of the shared task (Kim
et al., 2010) consisted of 144 and 100 scien-
tific publications from the ACL repository, respec-
tively. Since the primary evaluation of the shared
task was based on the top-15 ranked automatic
keyphrases compared to the keyphrases assigned
by the readers of the articles, these results are re-
ported here. The evaluation results can be seen in
Table 2 where the individual effect of each feature
is given in combination with the standard features.
It is interesting to note the improvement ob-
tained by extending standard features with the
simple feature of phrase length. This indicates
that though the basic features were quite good,
they did not take into account the point that reader
</bodyText>
<equation confidence="0.976026">
pmi(t1, t2, ..., tn) =
</equation>
<page confidence="0.992772">
188
</page>
<bodyText confidence="0.999871">
keyphrases are likely to consist of several words.
Morphological features, such as POS or suffix
features were also among the top-performing ones,
which seems to show that most of the keyphrases
tend to have some common structure. In contrast,
the syntactic feature made some decrease in the
performance when it was combined just with the
standard ones. This can be due to the fact that the
input data were quite noisy, i.e. some inconsisten-
cies arose in the data during the pdf to text con-
version of articles, which made it difficult to parse
some sentences correctly.
It was also interesting to see that Wikipedia fea-
ture did not improve the result when it was applied
to the whole document. However, our previous ex-
periences on keyphrase extraction from scientific
abstracts showed that this feature can be very use-
ful. Hence, we relaxed the feature to handle occur-
rences just from the abstract. This modification of
the feature yielded a 14.8% improvement in the F-
measure. A possible explanation for this is that
Wikipedia has articles of very common phrases
(such as Calculation or Result) and the dis-
tribution of such non-keyphrase terms is higher in
the body of the articles than in abstracts.
The last row of Table 2 contains the result
achieved by the complete feature set excluding
keyphraseness. As keyphraseness exploits author-
assigned keyphrases and – to the best of our
knowledge – other participants of the shared task
did not utilize author-assigned keyphrases, this re-
sult is present in the final ranking of the shared
task systems. However, we believe that if the task
is to extract keyphrases from an article to gain se-
mantic meta-data for an NLP application (e.g. for
information retrieval or summarization), author-
assigned keyphrases are often present and can be
very useful. This latter statement was proved by
one of our experiments where we used the au-
thor keyphrases assigned to the document itself as
a binary feature (instead of using the pool of all
keyphrases). This feature set could achieve an F-
score of 27.44 on the evaluation set and we believe
that this should be the complete feature set in a
real-world semantic indexing application.
</bodyText>
<sectionHeader confidence="0.999637" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999536956521739">
In this paper we introduced a wide set of new fea-
tures that are able to enhance the overall perfor-
mance of supervised keyphrase extraction applica-
tions. Our features include those calculated simply
on surface forms of keyphrase aspirants, those that
make use of the document- and corpus-level envi-
ronment of phrases and those that rely on exter-
nal knowledge. Although features were designed
to the specific task of extracting keyphrases from
scientific papers, due to their generality it is highly
assumable that they can be successfully utilized on
different domains as well.
The features we selected in SZTERGAK per-
formed well enough to actually achieve the
third place on the shared task by excluding the
keyphraseness feature and would be the first by
using any author-assigned keyphrase-based fea-
ture. It is also worth emphasizing that we think
that there are many possibilities to further extend
the feature set (e.g. with features that take the
semantic relatedness among keyphrase aspirants
into account) and significant improvement could
be achievable.
</bodyText>
<sectionHeader confidence="0.987619" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9997005">
The authors would like to thank the annotators of
the shared task for the datasets used in the shared
task. This work was supported in part by the
NKTH grant (project codename TEXTREND).
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99984704">
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ing of 16th IJCAI, pages 668–673.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. Semeval-2010 task 5 : Au-
tomatic keyphrase extraction from scientific articles.
In Proc. of the 5th SIGLEX Workshop on Semantic
Evaluation.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pages 423–430.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong
Sun. 2009. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on EMNLP.
Thuy Dung Nguyen and Minyen Kan. 2007.
Keyphrase extraction in scientific publications. In
Proc. of International Conference on Asian Digital
Libraries (ICADL 07), pages 317–326.
Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. Kea:
Practical automatic keyphrase extraction. In ACM
DL, pages 254–255.
</reference>
<page confidence="0.998928">
189
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.443372">
<title confidence="0.999696">SZTERGAK : Feature Engineering for Keyphrase Extraction</title>
<author confidence="0.996459">G´abor Berend</author>
<affiliation confidence="0.999698">Department of Informatics University of Szeged</affiliation>
<address confidence="0.915658">2. ´Arp´ad t´er Szeged, H-6720, Hungary</address>
<email confidence="0.996306">berendg@inf.u-szeged.hu</email>
<author confidence="0.998574">Rich´ard Farkas</author>
<affiliation confidence="0.987243">Hungarian Academy of Sciences</affiliation>
<address confidence="0.98498">103. Tisza Lajos k¨or´ut Szeged, H-6720, Hungary</address>
<email confidence="0.997247">rfarkas@inf.u-szeged.hu</email>
<abstract confidence="0.9449393125">Automatically assigning keyphrases to documents has a great variety of applications. Here we focus on the keyphrase extraction of scientific publications and present a novel set of features for the supervised learning of keyphraseness. Although these features are intended for extracting keyphrases from scientific papers, because of their generality and robustness, they should have uses in other domains as well. With the help of these features SZTERGAK achieved top results on shared task on Automatic Keyphrase Extraction from Scientific Artiexceeded its baseline by 10%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Gordon W Paynter</author>
<author>Ian H Witten</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceeding of 16th IJCAI,</booktitle>
<pages>668--673</pages>
<contexts>
<context position="2359" citStr="Frank et al. (1999)" startWordPosition="364" endWordPosition="367">s presented here are solely based on scientific articles, due to the robustness and universality of the features, our approach is expected to achieve good results when applied on other domains as well. 2 Related work In keyphrase extraction tasks, phrases are extracted from one document that are the most characteristic of its content (Liu et al., 2009; Witten et al., 1999). In these approaches keyphrase extraction is treated as a classification task, in which certain n-grams of a specific document act as keyphrase candidates, and the task is to classify them as proper keyphrases or not. While Frank et al. (1999) exploited domain specific knowledge to improve the quality of automatic tagging, others like Liu et al. (2009) analyze term co-occurence graphs. It was Nguyen and Kan (2007) who dealt with the special characteristics of scientific papers and introduced the state-of-theart feature set to keyphrase extraction tasks. Here we will follow a similar approach and make significant improvements by the introduction of novel features. 3 The SZTERGAK system The SZTERGAK framework treats the reproduction of reader-assigned keyphrases as a supervised learning task. In our setting a restricted set of token </context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl Gutwin, and Craig G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. In Proceeding of 16th IJCAI, pages 668–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Olena Medelyan</author>
<author>Min-Yen Kan</author>
<author>Timothy Baldwin</author>
</authors>
<title>Semeval-2010 task 5 : Automatic keyphrase extraction from scientific articles.</title>
<date>2010</date>
<booktitle>In Proc. of the 5th SIGLEX Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="11389" citStr="Kim et al., 2010" startWordPosition="1814" endWordPosition="1817">hrases mean the same. For this reason during the training phase we treated a phrase equivalent to its redirected version, i.e. if there is a keyphrase aspirant that is not assigned in the gold-standard reader annotation but the Wikipedia article with the same title has a redirection to such a phrase that is present among positive keyphrase instances of a particular document, the original phrase can be treated as a positive instance as well. In this way the ratio of positive examples could be increased from 0.99% to 1.14%. 4 Results and discussion The training and test sets of the shared task (Kim et al., 2010) consisted of 144 and 100 scientific publications from the ACL repository, respectively. Since the primary evaluation of the shared task was based on the top-15 ranked automatic keyphrases compared to the keyphrases assigned by the readers of the articles, these results are reported here. The evaluation results can be seen in Table 2 where the individual effect of each feature is given in combination with the standard features. It is interesting to note the improvement obtained by extending standard features with the simple feature of phrase length. This indicates that though the basic feature</context>
</contexts>
<marker>Kim, Medelyan, Kan, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timothy Baldwin. 2010. Semeval-2010 task 5 : Automatic keyphrase extraction from scientific articles. In Proc. of the 5th SIGLEX Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="4879" citStr="Klein and Manning, 2003" startWordPosition="759" endWordPosition="762">ly selecting the most frequently occurring one among the top 10 responses provided by the Google API. This title was added to the document, and all the lines before the first occurrence of the line Abstract were omitted. Lines unlikely to contain valuable information were also excluded from the documents. These lines were identified according to statistical data of their surface forms (e.g. the average and the deviation of line lengths) and regular expressions. Lastly, section and sentence boundaries were found in a rule-based way, and the POS and syntactic tagging (using the Stanford parser (Klein and Manning, 2003)) of each sentence were carried out. When syntactically parsed sentences were obtained, keyphrase aspirants were extracted. The 1 to 4-long token sequences that did not start or end with a stopword and consisted only of POS-codes of an adjective, a noun or a verb were defined to be possible keyphrases (resulting in classification instances). Tokens of key phrase aspirants were stemmed to store them in a uniform way, but they were also appended by the POS-code of the derived form, so that the same root forms were distinguished if they came from tokens having different POS-codes, like there show</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Peng Li</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Clustering to find exemplar terms for keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on EMNLP.</booktitle>
<contexts>
<context position="2093" citStr="Liu et al., 2009" startWordPosition="319" endWordPosition="322">ased on manually labeled corpora and various feature values. This paper focuses on the task of keyphrase extraction from scientific papers and we shall introduce new features that can significantly improve the overall performance. Although the experimental results presented here are solely based on scientific articles, due to the robustness and universality of the features, our approach is expected to achieve good results when applied on other domains as well. 2 Related work In keyphrase extraction tasks, phrases are extracted from one document that are the most characteristic of its content (Liu et al., 2009; Witten et al., 1999). In these approaches keyphrase extraction is treated as a classification task, in which certain n-grams of a specific document act as keyphrase candidates, and the task is to classify them as proper keyphrases or not. While Frank et al. (1999) exploited domain specific knowledge to improve the quality of automatic tagging, others like Liu et al. (2009) analyze term co-occurence graphs. It was Nguyen and Kan (2007) who dealt with the special characteristics of scientific papers and introduced the state-of-theart feature set to keyphrase extraction tasks. Here we will foll</context>
</contexts>
<marker>Liu, Li, Zheng, Sun, 2009</marker>
<rawString>Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun. 2009. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thuy Dung Nguyen</author>
<author>Minyen Kan</author>
</authors>
<title>Keyphrase extraction in scientific publications.</title>
<date>2007</date>
<booktitle>In Proc. of International Conference on Asian Digital Libraries (ICADL 07),</booktitle>
<pages>317--326</pages>
<contexts>
<context position="2533" citStr="Nguyen and Kan (2007)" startWordPosition="393" endWordPosition="396">ied on other domains as well. 2 Related work In keyphrase extraction tasks, phrases are extracted from one document that are the most characteristic of its content (Liu et al., 2009; Witten et al., 1999). In these approaches keyphrase extraction is treated as a classification task, in which certain n-grams of a specific document act as keyphrase candidates, and the task is to classify them as proper keyphrases or not. While Frank et al. (1999) exploited domain specific knowledge to improve the quality of automatic tagging, others like Liu et al. (2009) analyze term co-occurence graphs. It was Nguyen and Kan (2007) who dealt with the special characteristics of scientific papers and introduced the state-of-theart feature set to keyphrase extraction tasks. Here we will follow a similar approach and make significant improvements by the introduction of novel features. 3 The SZTERGAK system The SZTERGAK framework treats the reproduction of reader-assigned keyphrases as a supervised learning task. In our setting a restricted set of token sequences extracted from the documents was used as classification instances. These instances were ranked regarding to their posteriori probabilities of the keyphrase class, e</context>
</contexts>
<marker>Nguyen, Kan, 2007</marker>
<rawString>Thuy Dung Nguyen and Minyen Kan. 2007. Keyphrase extraction in scientific publications. In Proc. of International Conference on Asian Digital Libraries (ICADL 07), pages 317–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Gordon W Paynter</author>
<author>Eibe Frank</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>Kea: Practical automatic keyphrase extraction.</title>
<date>1999</date>
<booktitle>In ACM DL,</booktitle>
<pages>254--255</pages>
<contexts>
<context position="2115" citStr="Witten et al., 1999" startWordPosition="323" endWordPosition="327">abeled corpora and various feature values. This paper focuses on the task of keyphrase extraction from scientific papers and we shall introduce new features that can significantly improve the overall performance. Although the experimental results presented here are solely based on scientific articles, due to the robustness and universality of the features, our approach is expected to achieve good results when applied on other domains as well. 2 Related work In keyphrase extraction tasks, phrases are extracted from one document that are the most characteristic of its content (Liu et al., 2009; Witten et al., 1999). In these approaches keyphrase extraction is treated as a classification task, in which certain n-grams of a specific document act as keyphrase candidates, and the task is to classify them as proper keyphrases or not. While Frank et al. (1999) exploited domain specific knowledge to improve the quality of automatic tagging, others like Liu et al. (2009) analyze term co-occurence graphs. It was Nguyen and Kan (2007) who dealt with the special characteristics of scientific papers and introduced the state-of-theart feature set to keyphrase extraction tasks. Here we will follow a similar approach </context>
<context position="5959" citStr="Witten et al., 1999" startWordPosition="936" endWordPosition="939">ode of the derived form, so that the same root forms were distinguished if they came from tokens having different POS-codes, like there shown in Table 1. Textual Appearance Canonical form regulations regul nns Regulation regul nn regulates regul vbz regulated regul vbn Table 1: Standardization of document terms. 3.2 The extended feature set The features characterizing the extracted keyphrase aspirants can be grouped into four main types, namely phrase-, document-, corpuslevel and external knowledge-based features. Below we will describe the different types of features as well as those of KEA (Witten et al., 1999) which are cited as default features by most of the literature dealing with keyphrase extraction. 3.2.1 Standard features Features belonging to this set contain those of KEA, namely Tf-idf and the first occurrence. The Tf-idf feature assigns the tf-idf metric to each keyphrase aspirant. The first occurrence feature contains the relative first position for each keyphrase aspirant. The feature value was obtained by dividing the absolute first token position of a phrase by the number of tokens of the document in question. 3.2.2 Phrase-level features Features belonging to this group were calculate</context>
</contexts>
<marker>Witten, Paynter, Frank, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl Gutwin, and Craig G. Nevill-Manning. 1999. Kea: Practical automatic keyphrase extraction. In ACM DL, pages 254–255.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>