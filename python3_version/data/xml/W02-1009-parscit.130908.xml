<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000026">
<note confidence="0.442078666666667">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 63-70.
Association for Computational Linguistics.
</note>
<title confidence="0.981137">
Transformational Priors Over Grammars
</title>
<author confidence="0.968905">
Jason Eisner &lt;jason@cs.jhu.edu&gt;
</author>
<affiliation confidence="0.880388">
Johns Hopkins University, 3400 N. Charles St., NEB 224, Baltimore, MD
</affiliation>
<sectionHeader confidence="0.976998" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9989287">
This paper proposes a novel class of PCFG parameterizations
that support linguistically reasonable priors over PCFGs. To
estimate the parameters is to discover a notion of relatedness
among context-free rules such that related rules tend to have
related probabilities. The prior favors grammars in which the
relationships are simple to describe and have few major excep-
tions. A basic version that bases relatedness on weighted edit
distance yields superior smoothing of grammars learned from
the Penn Treebank (20% reduction of rule perplexity over the
best previous method).
</bodyText>
<sectionHeader confidence="0.959406" genericHeader="keywords">
1 A Sketch of the Concrete Problem
</sectionHeader>
<bodyText confidence="0.9873075">
This paper uses a new kind of statistical model to smooth
the probabilities of PCFG rules. It focuses on “flat” or
“dependency-style” rules. These resemble subcategoriza-
tion frames, but include adjuncts as well as arguments.
The verb put typically generates 3 dependents—a
subject NP at left, and an object NP and goal PP at right:
</bodyText>
<listItem confidence="0.99982525">
• S → NP put NP PP: Jim put [the pizza] [in the oven]
But put may also take other dependents, in other rules:
• S → NP Adv put NP PP: Jim often put [a pizza] [in the oven]
• S → NP put NP PP PP: Jim put soup [in an oven] [at home]
• S → NP put NP: Jim put [some shares of IBM stock]
• S → NP put Prt NP: Jim put away [the sauce]
• S → TO put NP PP: to put [the pizza] [in the oven]
• S → NP put NP PP SBAR: Jim put it [to me] [that ... ]
</listItem>
<bodyText confidence="0.999785">
These other rules arise if put can add, drop, reorder,
or retype its dependents. These edit operations on rules
are semantically motivated and quite common (Table 1).
We wish to learn contextual probabilities for the edit
operations, based on an observed sample of flat rules. In
English we should discover, for example, that it is quite
common to add or delete PP at the right edge of a rule.
These contextual edit probabilities will help us guess the
true probabilities of novel or little-observed rules.
However, rules are often idiosyncratic. Our smooth-
ing method should not keep us from noticing (given
enough evidence) that put takes a PP more often than
most verbs. Hence this paper’s proposal is a Bayesian
smoothing method that allows idiosyncrasy in the gram-
mar while presuming regularity to be more likely a priori.
The model will assign a positive probability to each
of the infinitely many formally possible rules. The fol-
lowing bizarre rule is not observed in training, and seems
very unlikely. But there is no formal reason to rule it out,
and it might help us parse an unlikely test sentence. So
the model will allow it some tiny probability:
</bodyText>
<listItem confidence="0.955162">
• S → NP Adv PP put PP PP PP NP AdjP S
</listItem>
<sectionHeader confidence="0.645499" genericHeader="introduction">
2 Background and Other Approaches
</sectionHeader>
<bodyText confidence="0.999933977272727">
A PCFG is a conditional probability function p(RHS |
LHS).1 For example, p(V NP PP  |VP) gives the proba-
bility of the rule VP → V NP PP. With lexicalized non-
terminals, it has form p(Vput NPpizza PPin  |VPput).
Usually one makes an independence assumption and
defines this as p(Vput NP PP  |VPput) times factors that
choose dependent headwords pizza and in according
to the selectional preferences of put. This paper is about
estimating the first factor, p(Vput NP PP  |VPput).
In supervised learning, it is simplest to use a max-
imum likelihood estimate (perhaps with backoff from
put). Charniak (1997) calls this a “Treebank grammar”
and gambles that assigning 0 probability to rules unseen
in training data will not hurt parsing accuracy too much.
However, there are four reasons not to use a Treebank
grammar. First, ignoring unseen rules necessarily sacri-
fices some accuracy. Second, we will show that it im-
proves accuracy to flatten the parse trees and use flat,
dependency-style rules like p(NP put NP PP  |Sput);
this avoids overly strong independence assumptions, but
it increases the number of unseen rules and so makes
Treebank grammars less tenable. Third, backing off from
the word is a crude technique that does not distinguish
among words.2 Fourth, one would eventually like to re-
duce or eliminate supervision, and then generalization is
important to constrain the search to reasonable grammars.
To smooth the distribution p(RHS  |LHS), one can de-
fine it in terms of a set of parameters and then estimate
those parameters. Most researchers have used an n-gram
model (Eisner, 1996; Charniak, 2000) or more general
Markov model (Alshawi, 1996) to model the sequence
of nonterminals in the RHS. The sequence Vput NP PP
in our example is then assumed to be emitted by some
Markov model of VPput rules (again with backoff from
put). Collins (1997, model 2) uses a more sophisticated
model in which all arguments in this sequence are gener-
ated jointly, as in a Treebank grammar, and then a Markov
process is used to insert adjuncts among the arguments.
While Treebank models overfit the training data,
Markov models underfit. A simple compromise (novel to
this paper) is a hybrid Treebank/Markov model, which
backs off from a Treebank model to a Markov. Like
this paper’s main proposal, it can learn well-observed id-
iosyncratic rules but generalizes when data are sparse.3
</bodyText>
<footnote confidence="0.9856645">
1Nonstandardly, this allows infinitely many rules with p&gt;0.
2One might do better by backing off to word clusters, which
Charniak (1997) did find provided a small benefit.
3Carroll and Rooth (1998) used a similar hybrid technique
</footnote>
<bodyText confidence="0.997812555555555">
These models are beaten by our rather different model,
transformational smoothing, which learns common
rules and common edits to them. The comparison is a
direct one, based on the perplexity or cross-entropy of
the trained models on a test set of S → · · · rules.4
A subtlety is that two annotation styles are possible. In
the Penn Treebank, put is the head of three constituents
(V, VP, and S, where underlining denotes a head child)
and joins with different dependents at different levels:
</bodyText>
<listItem confidence="0.9018325">
• [S [NP Jim] [VP [V put] [NP pizza] [PP in the oven]]]
In the flattened or dependency version that we prefer,
each word joins with all of its dependents at once:
• [S [NP Jim] put [NP pizza] [PP in the oven]]
</listItem>
<bodyText confidence="0.999297615384615">
A PCFG generating the flat structure must estimate
p(NP put NP PP  |Sput). A non-flat PCFG adds
the dependents of put in 3 independent steps, so in ef-
fect it factors the flat rule’s probability into 3 suppos-
edly independent “subrule probabilities,” p(NP VPput |
Sput) · p(Vput NP PP  |VPput) · p(put  |Vput).
Our evaluation judges the estimates of flat-rule prob-
abilities. Is it better to estimate these directly, or as a
product of estimated subrule probabilities?5 Transforma-
tional smoothing is best applied to the former, so that the
edit operations can freely rearrange all of a word’s depen-
dents. We will see that the Markov and Treebank/Markov
models also work much better this way—a useful finding.
</bodyText>
<sectionHeader confidence="0.986101" genericHeader="method">
3 The Abstract Problem: Designing Priors
</sectionHeader>
<bodyText confidence="0.999979714285715">
This section outlines the Bayesian approach to learning
probabilistic grammars (for us, estimating a distribution
over flat CFG rules). By choosing among the many
grammars that could have generated the training data, the
learner is choosing how to generalize to novel sentences.
To guide the learner’s choice, one can explicitly spec-
ify a prior probability distribution p(θ) over possible
grammars θ, which themselves specify probability dis-
tributions over strings, rules, or trees. A learner should
seek θ that maximizes p(θ) · p(D  |θ), where D is the
set of strings, rules, or trees observed by the learner. The
first factor favors regularity (“pick an a priori plausible
grammar”), while the second favors fitting the idiosyn-
crasies of the data, especially the commonest data.6
</bodyText>
<footnote confidence="0.9952428">
to evaluate rule distributions that they acquired from an
automatically-parsed treebank.
4All the methods evaluated here apply also to full PCFGs,
but verb-headed rules S --+ · · · present the most varied, inter-
esting cases. Many researchers have tried to learn verb subcat-
egorization, though usually not probabilistic subcategorization.
5In testing the latter case, we sum over all possible internal
bracketings of the rule. We do train this case on the true internal
bracketing, but it loses even with this unfair advantage.
6This approach is called semi-Bayesian or Maximum A Pos-
</footnote>
<bodyText confidence="0.98898755">
Priors can help both unsupervised and supervised
learning. (In the semi-supervised experiments here, train-
ing data is not raw text but a sparse sample of flat rules.)
Indeed a good deal of syntax induction work has been
carried out in just this framework (Stolcke and Omohun-
dro, 1994; Chen, 1996; De Marcken, 1996; Gr¨unwald,
1996; Osborne and Briscoe, 1997). However, all such
work to date has adopted rather simple prior distributions.
Typically, it has defined p(θ) to favor PCFGs whose rules
are few, short, nearly equiprobable, and defined over a
small set of nonterminals. Such definitions are conve-
nient, especially when specifying an encoding for MDL,
but since they treat all rules alike, they may not be good
descriptions of linguistic plausibility. For example, they
will never penalize the absence of a predictable rule.
A prior distribution can, however, be used to encode
various kinds of linguistic notions. After all, a prior is
really a soft form of Universal Grammar: it gives the
learner enough prior knowledge of grammar to overcome
Chomsky’s “poverty of the stimulus” (i.e., sparse data).
</bodyText>
<listItem confidence="0.987592">
• A preference for small or simple grammars, as above.
• Substantive preferences, such as a preference for verbs
to take 2 nominal arguments, or to allow PP adjuncts.
• Preferences for systematicity, such as a preference for
the rules to be consistently head-initial or head-final.
</listItem>
<bodyText confidence="0.999980111111111">
This paper shows how to design a prior that favors a
certain kind of systematicity. Lexicalized grammars for
natural languages are very large—each word specifies a
distribution over all possible dependency rules it could
head—but they tend to have internal structure. The new
prior prefers grammars in which a rule’s probability can
be well-predictedfrom the probabilities ofother rules, us-
ing linguistic transformations such as edit operations.
For example, p(NP Adv w put NP PP  |Sw) cor-
relates with p(NP w NP PP  |Sw). Both numbers are
high for w = put, medium for w = fund, and low for
w = sleep. The slope of the regression line has to do
with the rate of preverbal Adv-insertion in English.
The correlation is not perfect (some verbs are espe-
cially prone to adverbial modification), which is why we
will only model it with a prior. To just the extent that evi-
dence about w is sparse, the prior will cause the learner to
smooth the two probabilities toward the regression line.
</bodyText>
<sectionHeader confidence="0.993445" genericHeader="method">
4 Patterns Worth Modeling
</sectionHeader>
<bodyText confidence="0.9915145">
Before spelling out our approach, let us do a sanity check.
A frame is a flat rule whose headword is replaced with
</bodyText>
<footnote confidence="0.977272333333333">
teriori learning, since it is equivalent to maximizing p(9  |D).
It is also equivalent to Minimum Description Length (MDL)
learning, which minimizes the total number ofbits f(9)+f(D |
9) needed to encode grammar and data, because one can choose
an encoding scheme where f(x) = − log, p(x), or conversely,
define probability distributions by p(x) = 2−e(x).
</footnote>
<table confidence="0.999839388888889">
MI α 0 MI α 0 MI α 0
9.01 [NP ADJP-PRD] [NP RB ADJP-PRD] 4.76 [TO S] [ S] 5.54 [TO NP PP] [NP TO NP]
8.65 [NP ADJP-PRD] [NP PP-LOC-PRD] 4.17 [TO S] NP PP] 5.25 [TO NP PP] [NP MD NP .]
[TO
[TO
8.01 [NP ADJP-PRD] [NP NP-PRD] 2.77 [TO S] NP] 4.67 [TO NP PP] [NP MD NP]
7.69 [NP ADJP-PRD] [NP ADJP-PRD .] 6.13 [TO NP] [TO NP SBAR-TMP] 4.62 [TO NP PP] [TO ]
8.49 [NP NP-PRD] [NP NP-PRD .] 5.72 [TO NP] [TO NP PP PP] 3.19 [TO NP PP] [TO NP]
7.91 [NP NP-PRD] [NP ADJP-PRD .] 5.36 [TO NP] [NP MD RB NP] 2.05 [TO NP PP] [ NP]
7.01 [NP NP-PRD] [NP ADJP-PRD] 5.16 [TO NP] [TO NP PP PP-TMP] 5.08 [ NP] [ADVP-TMP NP]
8.45 [NP ADJP-PRD .] [NP PP-LOC-PRD] 5.11 [TO NP] [TO NP ADVP] 4.86 [ NP] [ADVP NP]
8.30 [NP ADJP-PRD .] [NP NP-PRD .] 4.85 [TO NP] [TO NP PP-LOC] 4.53 [ NP] [ NP PP-LOC]
8.04 [NP ADJP-PRD .] [NP NP-PRD] 4.84 [TO NP] [MD NP] 3.50 [ NP] [ NP PP]
7.01 [NP ADJP-PRD .] [NP ADJP-PRD] 4.49 [TO NP] [NP TO NP] 3.17 [ NP] [ S]
7.01 [NP SBAR] [NP SBAR . ”] 4.36 [TO NP] [NP MD S] 2.28 [ NP] [NP NP]
4.75 [NP SBAR] [NP SBAR .] 4.36 [TO NP] [NP TO NP PP] 1.89 [ NP] [NP NP .]
6.94 [NP SBAR .] [“ NP SBAR .] 4.26 [TO NP] [NP MD NP PP] 2.56 [NP NP] [NP NP .]
5.94 [NP SBAR .] [NP SBAR . ”] 4.26 [TO NP] [TO NP PP-TMP] 2.20 [NP NP] [ NP]
5.90 [NP SBAR .] [S , NP .] 4.21 [TO NP] [TO PRT NP] 4.89 [NP NP .] [NP ADVP-TMP NP .]
5.82 [NP SBAR .] [NP ADVP 4.20 [TO NP] [NP MD NP] 4.57 [NP NP .] [NP ADVP NP .]
SBAR .]
4.68 [NP SBAR .] [ SBAR] 3.99 [TO NP] [TO NP PP] 4.51 [NP NP .] [NP NP PP-TMP]
4.50 [NP SBAR .] [NP SBAR] 3.69 [TO NP] [NP MD NP .] 3.35 [NP NP .] [NP S .]
3.23 [NP SBAR .] [NP S .] 3.60 [TO NP] [TO ] 2.99 [NP NP .] [NP NP]
2.07 [NP SBAR .] [NP ] 3.56 [TO NP] [TO PP] 2.96 [NP NP .] [NP NP PP .]
1.91 [NP SBAR .] [NP NP .] 2.56 [TO NP] [NP NP PP] 2.25 [NP NP .] [ NP PP]
1.63 [NP SBAR .] [NP NP] 2.04 [TO NP] [NP S] 2.20 [NP NP .] [ NP]
4.52 [NP S] [NP S .] 1.99 [TO NP] [NP NP] 4.82 [NP S .] [ S]
4.27 [NP S] [ S] 1.69 [TO NP] [NP NP .] 4.58 [NP S .]
[NP S]
[NP ]
3.36 [NP S] ] 1.68 [TO NP] [NP NP PP .] 3.30 [NP S .]
[NP
[NP
2.66 [NP S] NP .] 1.03 [TO NP] [ NP] 2.93 [NP S .] [NP NP .]
2.37 [NP S] [NP NP] 4.75 [S , NP .] [NP SBAR .] 2.28 [NP S .] [NP NP]
</table>
<tableCaption confidence="0.999921">
Table 1: The most predictive pairs of sentential frames. If S → α occurs in training data at least 5 times with a given headword in
</tableCaption>
<bodyText confidence="0.986317818181818">
the position, then S → Q also tends to appear at least once with that headword. MI measures the mutual information of these
two events, computed over all words. When MI is large, as here, the edit distance between α and Q tends to be strikingly small (1
or 2), and certain linguistically plausible edits are extremely common.
the variable “ ” (corresponding to w above). Table 1 il-
lustrates that in the Penn Treebank, if frequent rules with
frame α imply matching rules with frame 0, there are
usually edit operations (section 1) to easily turn α into 0.
How about rare rules, whose probabilities are most in
need of smoothing? Are the same edit transformations
that we can learn from frequent cases (Table 1) appropri-
ate for predicting the rare cases? The very rarity of these
rules makes it impossible to create a table like Table 1.
However, rare rules can be measured in the aggregate,
and the result suggests that the same kinds of transforma-
tions are indeed useful—perhaps even more useful—in
predicting them. Let us consider the set R of 2,809,545
possible flat rules that stand at edit distance 1 from the set
of S → · · · rules observed in our English training data.
That is, a rule such as SPDC → NP put NP is in R if it
did not appear in training data itself, but could be derived
by a single edit from some rule that did appear.
A bigram Markov model (section 2) was used to iden-
tify 2,714,763 rare rules in R—those that were predicted
to occur with probability &lt; 0.0001 given their head-
words. 79 of these rare rules actually appeared in a
development-data set of 1423 rules. The bigram model
would have expected only 26.2 appearances, given the
lexical headwords in the test data set. The difference is
statistically significant (p &lt; 0.001, bootstrap test).
In other words, the bigram model underpredicts the
edit-distance “neighbors” of observed rules by a factor
of 3.7 One can therefore hope to use the edit transforma-
tions to improve on the bigram model. For example, the
</bodyText>
<footnote confidence="0.593061">
7Similar results are obtained when we examine just one par-
ticular kind of edit operation, or rules of one particular length.
</footnote>
<bodyText confidence="0.999129736842105">
Delete Y transformation recognizes that if · · · X Y Z · · ·
has been observed, then · · · X Z · · · is plausible even if
the bigram X Z has not previously been observed.
Presumably, edit operations are common because they
modify a rule in semantically useful ways, allowing the
filler of a semantic role to be expressed (Insert), sup-
pressed (Delete), retyped (Substitute), or heavy-shifted
(Swap). Such “valency-affecting operations” have re-
peatedly been invoked by linguists; they are not confined
to English.8 So a learner of an unknown language can
reasonably expect a priori that flat rules related by edit
operations may have related probabilities.
However, which edit operations varies by language.
Each language defines its own weighted, contextual,
asymmetric edit distance. So the learner will have to dis-
cover how likely particular edits are in particular con-
texts. For example, it must learn the rates of prever-
bal Adv-insertion and right-edge PP-insertion. Evidence
about these rates comes mainly from the frequent rules.
</bodyText>
<sectionHeader confidence="0.997292" genericHeader="method">
5 A Transformation Model
</sectionHeader>
<bodyText confidence="0.891399636363636">
The form of our new model is shown in Figure 1. The
vertices are flat context-free rules, and the arcs between
them represent edit transformations. The set of arcs leav-
8Carpenter (1991) writes that whenever linguists run into the
problem of systematic redundancy in the syntactic lexicon, they
design a scheme in which lexical entries can be derived from
one another by just these operations. We are doing the same
thing. The only twist that the lexical entries (in our case, flat
PCFG rules) have probabilities that must also be derived, so we
will assume that the speaker applies these operations (randomly
from the hearer’s viewpoint) at various rates to be learned.
</bodyText>
<figure confidence="0.99623496">
0.0002
0.0011
START
HALT START(fund) HALT START(merge)
exp θ0
Z4
HALT
exp θ0
Z2
To fund NP
exp θ1
Z1 exp θ2+θ8
Z1
exp θ3+θ5+θ8
Z2
To fund PP NP
exp θ0
Z3
To fund NP PP
exp θ3+θ4
Z2
exp θ6
Z3
exp θ7+θ8
Z3
exp θ6
Z7
To merge PP NP
exp θ0
Z7
To merge NP
exp θ3+θ4
Z6
exp θ0
Z6
HALT
exp θ7+θ9
Z7
exp θ1
Z5
exp θ3+θ5+θ9
Z6
exp θ0
Z8
To merge NP PP
exp θ2 +θ9
Z5
00 halts 03 inserts PP 06 deletes PP 08 yields To fund NP PP
01 chooses To NP 04 inserts PP before NP 07 moves NP right past PP 09 yields To merge NP PP
02 chooses To NP PP 05 inserts PP before right edge
</figure>
<figureCaption confidence="0.99718875">
Figure 1: A fragment of a transformation model. Vertices are possible context-free rules (their left-hand sides, Sfund → and
Smerge → , are omitted to avoid visual clutter). Arc probabilities are determined log-linearly, as shown, from a real-valued vector θ
of feature weights. The Z values are chosen so that the arcs leaving each vertex have total probability 1. Dashed arrows represent
arcs not shown here (there are hundreds from each vertex, mainly insertions). Also, not all features are shown (see Table 2).
</figureCaption>
<bodyText confidence="0.99931928125">
ing any given vertex has total probability 1. The learner’s
job is to discover the probabilities.
Fortunately, the learner does not have to learn a sep-
arate probability for each of the (infinitely) many arcs,
since many of the arcs represent identical or similar edits.
As shown in Figure 1, an arc’s probability is determined
from meaningful features of the arc, using a conditional
log-linear model of p(arc  |source vertex). The learner
only has to learn the finite vector B of feature weights.
Arcs that represent similar transformations have similar
features, so they tend to have similar probabilities.
This transformation model is really a PCFG with un-
usual parameterization. That is, for any value of B, it
defines a language-specific probability distribution over
all possible context-free rules (graph vertices). To sam-
ple from this distribution, take a random walk from the
special vertex START to the special vertex HALT. The
rule at the last vertex reached before HALT is the sample.
This sampling procedure models a process where the
speaker chooses an initial rule and edits it repeatedly.
The random walk might reach Sfund → To fund NP
in two steps and simply halt there. This happens
with probability 0.0011 · exp θ1
Z1 · exp θ0 Z2. Or, having
arrived at Sfund → To fund NP, it might transform
it into Sfund → To fund PP NP and then further to
Sfund → To fund NP PP before halting.
Thus, pθ(Sfund → To fund NP PP) denotes the
probability that the random walk somehow reaches
Sfund → To fund NP PP and halts there. Condi-
tionalizing this probability gives pθ(To NP PP |
Sfund), as needed for the PCFG.9
</bodyText>
<footnote confidence="0.788344">
9The experiments of this paper do not allow transformations
</footnote>
<bodyText confidence="0.999702166666667">
Given B, it is nontrivial to solve for the probability dis-
tribution over grammar rules e. Let Iθ(e) denote the flow
to vertex e. This is defined to be the total probability of
all paths from START to e. Equivalently, it is the expected
number of times e would be visited by a random walk
from START. The following recurrence defines pθ(e):10
</bodyText>
<equation confidence="0.999994">
Iθ(e) = δe,START + Ee,Iθ(e�) · p(e� → e) (1)
pθ(e) = Iθ(e) · p(e → HALT) (2)
</equation>
<bodyText confidence="0.989265083333334">
Since solving the large linear system (1) would be pro-
hibitively expensive, in practice we use an approximate
relaxation algorithm (Eisner, 2001) that propagates flow
through the graph until near-convergence. In general this
may underestimate the true probabilities somewhat.
Now consider how the parameter vector B affects the
distribution over rules, pθ(e), in Figure 1:
• By raising the initial weight θ1, one can
increase the flow to Sfund → To fund NP,
Smerge → To merge NP, and the like. By equa-
tion (2), this also increases the probability of these rules.
But the effect also feeds through the graph to increase
the flow and probability at those rules’ descendants in
the graph, such as Smerge → To merge NP PP.
So a single parameter θ1 controls a whole complex of
rule probabilities (roughly speaking, the infinitival transi-
tives). The model thereby captures the fact that, although
that change the LHS or headword of a rule, so it is trivial to find
the divisor pθ(Sfund): in Figure 1 it is 0.0011. But in general,
LHS-changing transformations can be useful (Eisner, 2001).
10Where 5x,� = 1 if x = y, else 5x,, = 0.
rules are mutually exclusive events whose probabilities
sum to 1, transformationally related rules have positively
correlated probabilities that rise and fall together.
</bodyText>
<listItem confidence="0.911571352941176">
• The exception weight θ9 appears on all and only the
arcs to Smerge → To merge NP PP. That rule has
even higher probability than predicted by PP-insertion as
above (since merge, unlike fund, actually tends to sub-
categorize for PPwith). To model its idiosyncratic prob-
ability, one can raise θ9. This “lists” the rule specially
in the grammar. Rules derived from it also increase in
probability (e.g., Smerge → To Adv merge NP PP),
since again the effect feeds through the graph.
• The generalization weight θ3 models the strength of
the PP-insertion relationship. Equations (1) and (2) im-
ply that pθ(Sfund → To fund NP PP) is modeled as
a linear combination of the probabilities of that rule’s
parents in the graph. θ3 controls the coefficient of
pθ(Sfund → To fund NP) in this linear combination,
with the coefficient approaching zero as θ3 → —oo.
• Narrower generalization weights such as θ4 and θ5
</listItem>
<bodyText confidence="0.984409222222222">
control where PP is likely to be inserted. To learn the
feature weights is to learn which features of a transfor-
mation make it probable or improbable in the language.
Note that the vertex labels, graph topology, and arc
parameters are language independent. That is, Figure 1
is supposed to represent Universal Grammar: it tells a
learner what kinds of generalizations to look for. The
language-specific part is B, which specifies which gener-
alizations and exceptions help to model the data.
</bodyText>
<sectionHeader confidence="0.994729" genericHeader="method">
6 The Prior
</sectionHeader>
<bodyText confidence="0.99130696">
The model has more parameters than data. Why? Beyond
the initial weights and generalization weights, in practice
we allow one exception weight (e.g., θ8, θ9) for each rule
that appeared in training data. (This makes it possible to
learn arbitrary exceptions, as in a Treebank grammar.)
Parameter estimation is nonetheless possible, using a
prior to help choose among the many values of B that do
a reasonable job of explaining the training data. The prior
constrains the degrees of freedom: while many parame-
ters are available in principle, the prior will ensure that
the data are described using as few of them as possible.
The point of reparameterizing a PCFG in terms of B,
as in Figure 1, is precisely that only one parameter is
needed per linguistically salient property of the PCFG.
Making θ3 &gt; 0 creates a broadly targeted transforma-
tion. Making θ9 =� 0 or θ1 =� 0 lists an idiosyncratic rule,
or class of rules, together with other rules derived from
them. But it takes more parameters to encode less sys-
tematic properties, such as narrowly targeted edit trans-
formations (θ4, θ5) or families of unrelated exceptions.
A natural prior for the parameter vector B E Rk is
therefore specified in terms of a variance σ2. We simply
say that the weights θ1, θ2,... θk are independent sam-
ples from the normal distribution with mean 0 and vari-
ance σ2 &gt; 0 (Chen and Rosenfeld, 1999):
</bodyText>
<equation confidence="0.928912">
O — N(0, σ2) x N(0, σ2) x • • • x N(0, σ2) (3)
or equivalently, that B is drawn from a multivariate Gaus-
sian with mean 0~ and diagonal covariance matrix σ2I,
i.e., O — N(~0,σ2I).
</equation>
<bodyText confidence="0.99982325">
This says that a priori, the learner expects most fea-
tures in Figure 1 to have weights close to zero, i.e., to be
irrelevant. Maximizing p(B) • p(D B) means finding
a relatively small set of features that adequately describe
the rules and exceptions of the grammar. Reducing the
variance σ2 strengthens this bias toward simplicity.
For example, if Sfund → To fund NP PP and
Smerge → To fund NP PP are both observed more
often than the current pθ distribution predicts, then the
learner can follow either (or both) of two strategies: raise
θ8 and θ9, or raise θ3. The former strategy fits the training
data only; the latter affects many disparate arcs and leads
to generalization. The latter strategy may harm p(D B)
but is preferred by the prior p(B) because it uses one pa-
rameter instead of two. If more than two words act like
merge and fund, the pressure to generalize is stronger.
</bodyText>
<sectionHeader confidence="0.983353" genericHeader="method">
7 Perturbation Parameters
</sectionHeader>
<bodyText confidence="0.999992166666667">
In experiments, we have found that a slight variation on
this model gets slightly better results. Let θe denote the
exception weight (if any) that allows one to tune the prob-
ability of rule e. We eliminate θe and introduce a different
parameter πe, called a perturbation, which is used in the
following replacements for equations (1) and (2):
</bodyText>
<equation confidence="0.999886">
�Iθ(e) = δe,START + Iθ(e&apos;) • exp πe •p(e&apos; → e)(4)
e,
pθ(e) = Iθ(e) • exp πe • p(e → HALT)/Z (5)
</equation>
<bodyText confidence="0.960888470588235">
where Z is a global normalizing factor chosen so that
Ee pθ(e) = 1. The new prior on πe is the same as the
old prior on θe.
Increasing either θe or πe will raise pθ(e); the learner
may do this to account for observations of e in training
data. The probabilities of other rules consequently de-
crease so that Ee pθ(e) = 1. When πe is raised, all
rules’ probabilities are scaled down slightly and equally
(because Z increases). When θe is raised, e steals proba-
bility from its siblings,11 but these are similar to e so tend
to appear in test data if e is in training data. Raising θe
without disproportionately harming e’s siblings requires
manipulation of many other parameters, which is discour-
aged by the prior and may also suffer from search error.
We speculate that this is why πe works better.
&amp;quot;Raising the probability of an arc from e&apos; to e decreases the
probabilities of arcs from e&apos; to siblings of e, as they sum to 1.
</bodyText>
<table confidence="0.99743875">
basic Treebank/Markov
flat non-flats Katz one-count°
flat flat non-flat
Treebank 00 00
1-gram 1774.9 86435.1 340.9 160.0 193.2
2-gram 135.2 199.3 127.2 116.2 174.7
3-gram 136.5 177.4 132.7 123.3 174.8
Collins` 363.0 494.5 197.9
transformation 108.6
averaged` 102.3
1-gram 1991.2 96318.8 455.1 194.3 233.1
2-gram 162.2 236.6 153.2 138.8 205.6
3-gram 161.9 211.0 156.8 145.7 208.1
Collins 414.5 589.4 242.0
transformation 124.8
averaged 118.0
(Insert) (Insert, target)
(Insert, left) (Insert, target, left)
(Insert, right) (Insert, target, right)
(Insert, left, right)
(Insert, side) (Insert, side, target)
(Insert, side, left) (Insert, side, target, left)
(Insert, side, right) (Insert, side, target, right)
(Insert, side, left, right)
</table>
<tableCaption confidence="0.989633">
Table 2: Each Insert arc has 14 features. The features of any
given arc are found by instantiating the tuples above, as shown.
Each instantiated tuple has a weight specified in θ.
</tableCaption>
<table confidence="0.7951000625">
S - · · · rules only train dev test
Treebank sections 0–15 16 17
sentences 15554 1343 866
rule tokens 18836 1588 973
rule types 11565 1317 795
frame types 2722 564 365
headword types 3607 756 504
novel rule tokens 51.6% 47.8%
novel frame tokens 8.9% 6.3%
novel headword tokens 10.4% 10.2%
novel rule types 61.4% 57.5%
novel frame types 24.6% 16.4%
novel headword types 20.9% 18.8%
nonterminaltypes 78
# transformations applicable to 158n-1 158n-1 158n-1
rule with RHS length = n
</table>
<tableCaption confidence="0.9847995">
Table 3: Properties of the experimental data. “Novel” means
not observed in training. “Frame” was defined in section 4.
</tableCaption>
<sectionHeader confidence="0.271122" genericHeader="evaluation">
8 Evaluation12
</sectionHeader>
<bodyText confidence="0.999941666666667">
To evaluate the quality of generalization, we used pre-
parsed training data D and testing data E (Table 3).
Each dataset consisted of a collection of flat rules such as
Sput → NP put NP PP extracted from the Penn Tree-
bank (Marcus et al., 1993). Thus, p(D  |θ, π) and
p(E  |θ, π) were each defined as a product of rule prob-
abilities of the form pθ,π(NP put NP PP  |Sput).
The learner attempted to maximize p(θ, π) · p(D |
θ, π) by gradient ascent. This amounts to learning the
generalizations and exceptions that related the training
rules D. The evaluation measure was then the perplex-
ity on test data, − log2 p(E  |θ, π)/|E |. To get a good
(low) perplexity score, the model had to assign reason-
able probabilities to the many novel rules in E (Table 3).
For many of these rules, even the frame was novel.
Note that although the training data was preparsed into
rules, it was not annotated with the paths in Figure 1 that
generated those rules, so estimating θ and π was still an
unsupervised learning problem.
The transformation graph had about 14 features per arc
(Table 2). In the finite part of the transformation graph
that was actually explored (including bad arcs that com-
pete with good ones), about 70000 distinct features were
encountered, though after training, only a few hundred
</bodyText>
<footnote confidence="0.8881125">
12See (Eisner, 2001) for full details of data preparation,
model structure, parameter initialization, backoff levels for the
comparison models, efficient techniques for computing the ob-
jective and its gradient, and more analysis of the results.
</footnote>
<figure confidence="0.567511">
(b)
</figure>
<tableCaption confidence="0.676082666666667">
aBack off from Treebank grammar with Katz vs. one-count
backoff (Chen and Goodman, 1996) (Note: One-count was al-
ways used for backoff within the n-gram and Collins models.)
</tableCaption>
<table confidence="0.721311666666667">
bSee section 2 for discussion
cCollins (1997, model 2)
dAverage of transformation model with best other model
</table>
<tableCaption confidence="0.994078">
Table 4: Perplexity of the test set under various models. (a) Full
training set. (b) Half training set (sections 0–7 only).
</tableCaption>
<bodyText confidence="0.999557857142857">
feature weights were substantial, and only a few thousand
were even far enough from zero to affect performance.
There was also a parameter gyre for each observed rule e.
Results are given in Table 4a, which compares the
transformation model to various competing models dis-
cussed in section 2. The best (smallest) perplexities ap-
pear in boldface. The key results:
</bodyText>
<listItem confidence="0.988045923076923">
• The transformation model was the winner, reducing
perplexity by 20% over the best model replicated from
previous literature (a bigram model).
• Much of this improvement could be explained by
the transformation model’s ability to model exceptions.
Adding this ability more directly to the bigram model,
using the new Treebank/Markov approach of section 2,
also reduced perplexity from the bigram model, by 6%
or 14% depending on whether Katz or one-count backoff
was used, versus the transformation model’s 20%.
• Averaging the transformation model with the best com-
peting model (Treebank/bigram) improved it by an addi-
tional 6%. So using transformations yields a total per-
plexity reduction of 12% over Treebank/bigram, and 24%
over the best previous model from the literature (bigram).
• What would be the cost of achieving such a perplexity
improvement by additional annotation? Training the av-
eraged model on only the first half of the training set, with
no further tuning of any options (Table 4b), yielded a test
set perplexity of 118.0. So by using transformations, we
can achieve about the same perplexity as the best model
without transformations (Treebank/bigram, 116.2), using
only half as much training data.
• Furthermore, comparing Tables 4a and 4b shows that
the transformation model had the most graceful perfor-
mance degradation when the dataset was reduced in size.
</listItem>
<figure confidence="0.994443785714286">
If the arc inserts
Adv after TO
in TO fund PP,
then (a)
target=Adv
left=TO
right=——
side=left of head
p(rule  |headword): averaged transf.
1e−10 1e−07 1e−04 1e−01
5e−04 5e−03 5e−02 5e−01
0.001 0.010 0.100 1.000
1e−10 1e−07 1e−04 1e−01 5e−04 5e−03 5e−02 5e−01 0.001 0.010 0.100 1.000
p(rule  |headword): Treebank/bigram
</figure>
<figureCaption confidence="0.996092333333333">
Figure 2: Probabilities of test set flat rules under the averaged model, plotted against the corresponding probabilities under the
best transformation-free model. Improvements fall above the main diagonal; dashed diagonals indicate a factor of two. The three
log-log plots (at different scales!) partition the rules by the number of training observations: 0 (left graph), 1 (middle), ≥ 2 (right).
</figureCaption>
<bodyText confidence="0.995521666666667">
This is an encouraging result for the use of the method
in less supervised contexts (although results on a noisy
dataset would be more convincing in this regard).
• The competing models from the literature are best used
to predict flat rules directly, rather than by summing over
their possible non-flat internal structures, as has been
done in the past. This result is significant in itself. Ex-
tending Johnson (1998), it shows the inappropriateness of
the traditional independence assumptions that build up a
frame by several rule expansions (section 2).
Figure 2 shows that averaging the transformation
model with the Treebank/bigram model improves the lat-
ter not merely on balance, but across the board. In other
words, there is no evident class of phenomena for which
incorporating transformations would be a bad idea.
</bodyText>
<listItem confidence="0.97880575">
• Transformations particularly helped raise the estimates
of the low-probability novel rules in test data, as hoped.
• Transformations also helped on test rules that had
been observed once in training with relatively infrequent
words. (In other words, the transformation model does
not discount singletons too much.)
• Transformations hurt slightly on balance for rules ob-
served more than once in training, but the effect was tiny.
</listItem>
<bodyText confidence="0.9987825">
All these differences are slightly exaggerated if one com-
pares the transformation model directly with the Tree-
bank/bigram model, without averaging.
The transformation model was designed to use edit
operations in order to generalize appropriately from a
word’s observed frames to new frames that are likely to
appear with that word in test data. To directly test the
model’s success at such generalization, we compared it
to the bigram model on a pseudo-disambiguation task.
Each instance of the task consisted of a pair of rules
from test data, expressed as (word, frame) pairs (w1, f1)
and (w2, f2), such that f1 and f2 are “novel” frames that
did not appear in training data (with any headword).
Each model was then asked: Does f1 go with w1 and
f2 with w2, or vice-versa? In other words, which is big-
ger, p(f1  |w1) · p(f2  |w2) or p(f2  |w1) · p(f1  |w2)?
Since the frames were novel, the model had to make
the choice according to whether f1 or f2 looked more
like the frames that had actually been observed with w1
in the past, and likewise w2. What this means depends
on the model. The bigram model takes two frames to
look alike if they contain many bigrams in common. The
transformation model takes two frames to look alike if
they are connected by a path ofprobable transformations.
The test data contained 62 distinct rules (w, f) in
whichf was a novel frame. This yielded 62·61
2 = 1891
pairs of rules, leading to 1811 task instances after obvi-
ous ties were discarded.13
Baseline performance on this difficult task is 50% (ran-
dom guess). The bigram model chose correctly in 1595
of the 1811 instances (88.1%). Parameters for “memo-
rizing” specific frames do not help on this task, which in-
volves only novel frames, so the Treebank/bigram model
had the same performance. By contrast, the transforma-
tion model got 1669 of 1811 correct (92.2%), for a more-
than-34% reduction in error rate. (The development set
showed similar results.) However, since the 1811 task
instances were derived non-independently from just 62
novel rules, this result is based on a rather small sample.
</bodyText>
<sectionHeader confidence="0.998791" genericHeader="conclusions">
9 Discussion
</sectionHeader>
<bodyText confidence="0.999578479166667">
This paper has presented a nontrivial way to reparameter-
ize a PCFG in terms of “deep” parameters representing
transformations and exceptions. A linguistically sensible
prior was natural to define over these deep parameters.
Famous examples of “deep reparameterization” are the
Fourier transform in speech recognition and the SVD
transform for Latent Semantic Analysis in IR. Like our
technique, they are intended to reveal significant structure
through the leading parameters while relegating noise and
exceptions to minor parameters. Such representations
13An obvious tie is an instance where f1 = f2, or where
both w1 and w2 were novel headwords. (The 62 rules included
11 with novel headwords.) In such cases, neither the bigram nor
the transformation model has any basis for making its decision:
the probabilities being compared will necessarily be equal.
make it easier to model the similarity or probability of the
objects at hand (waveforms, documents, or grammars).
Beyond the fact that it shows at least a good perplex-
ity improvement (it has not yet been applied to a real
task), an exciting “big idea” aspect of this work is its
flexibility in defining linguistically sensible priors over
grammars. Our reparameterization is made with refer-
ence to a user-designed transformation graph (Figure 1).
The graph need not be confined to edit distance transfor-
mations, or to the simple features of Table 2 (used here
for comparability with the Markov models), which con-
dition a transformation’s probability on local context.
In principle, the approach could be used to capture
a great many linguistic phenomena. Figure 1 could be
extended with more ambitious transformations, such as
gapping, gap-threading, and passivization. The flat rules
could be annotated with internal structure (as in TAG) and
thematic roles. Finally, the arcs could bear further fea-
tures. For example, the probability of unaccusative move-
ment (someone sank the boat → the boat sank) should de-
pend on whether the headword is a change-of-state verb.
Indeed, Figure 1 can be converted to any lexicalized
theory of grammar, such as categorial grammar, TAG,
LFG, HPSG, or Minimalism. The vertices represent lex-
ical entries and the arcs represent probabilistic lexical re-
dundancy rules or metarules (see footnote 8). The trans-
formation model approach is therefore a full stochas-
tic treatment of lexicalized syntax— apparently the first
to treat lexical redundancy rules, although (Briscoe and
Copestake, 1999) give an ad hoc approach. See (Eisner,
2001; Eisner, 2002a) for more discussion.
It is worthwhile to compare the statistical approach
here with some other approaches:
</bodyText>
<listItem confidence="0.992782142857143">
• Transformation models are similar to graphical mod-
els: they allow similar patterns of deductive and abduc-
tive inference from observations. However, the vertices
of a transformation graph do not represent different ran-
dom variables, but rather mutually exclusive values of the
same random variable, whose probabilities sum to 1.
• Transformation models incorporate conditional log-
</listItem>
<bodyText confidence="0.942446222222222">
linear (maximum entropy) models. As an alternative,
one could directly build a conditional log-linear model
of p(RHS  |LHS). However, such a model would learn
probabilities, not relationships. A feature weight would
not really model the strength of the relationship between
two frames e, e&apos; that share that feature. It would only in-
fluence both frames’ probabilities. If the probability of e
were altered by some unrelated factor (e.g., an exception
weight), then the probability of e&apos; would not respond.
</bodyText>
<listItem confidence="0.706227">
• A transformation model can be regarded as a proba-
</listItem>
<bodyText confidence="0.981535">
bilistic FSA that consists mostly of e-transitions. (Rules
are only emitted on the arcs to HALT.) This perspective
allows use of generic methods for finite-state parameter
estimation (Eisner, 2002b). We are strongly interested in
improving the speed of such methods and their ability to
avoid local maxima, which are currently the major diffi-
culty with our system, as they are for many unsupervised
learning techniques. We expect to further pursue trans-
formation models (and simpler variants that are easier to
estimate) within this flexible finite-state framework.
The interested reader is encouraged to look at (Eisner,
2001) for a much more careful and wide-ranging discus-
sion of transformation models, their algorithms, and their
relation to linguistic theory, statistics, and parsing. Chap-
ter 1 provides a good overview. For a brief article high-
lighting the connection to linguistics, see (Eisner, 2002a).
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999113914893617">
Hiyan Alshawi. 1996. Head automata for speech translation.
In Proceedings ofICSLP, Philadelphia, PA.
T. Briscoe and A. Copestake. 1999. Lexical rules in constraint-
based grammar. Computational Linguistics, 25(4):487–526.
Bob Carpenter. 1991. The generative power of categorial gram-
mars and head-driven phrase structure grammars with lexical
rules. Computational Linguistics, 17(3):301–313.
Glenn Carroll and Mats Rooth. 1998. Valence induction with a
head-lexicalized PCFG. In Proceedings of EMNLP.
Eugene Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In Proc. ofAAAI, 598–603.
Eugene Charniak. 2000. A maximum-entropy inspired parser.
In Proceedings ofNAACL.
Stanley Chen and Joshua Goodman. 1996. An empirical study
of smoothing techniques. In Proceedings ofACL.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaussian prior
for smoothing maximum entropy models. Technical Report
CMU-CS-99-108, Carnegie Mellon University, February.
Stanley Chen. 1996. Building Probabilistic Models for Natural
Language. Ph.D. thesis, Harvard University.
Michael J. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings ofACL/EACL, 16–23.
Carl De Marcken. 1996. Unsupervised Language Acquisition.
Ph.D. thesis, MIT.
Jason Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. Proc. of COLING, 340–345.
Jason Eisner. 2001. Smoothing a Probabilistic Lexicon via Syn-
tactic Transformations. Ph.D. thesis, Univ. of Pennsylvania.
Jason Eisner. 2002a. Discovering syntactic deep structure via
Bayesian statistics. Cognitive Science, 26(3), May.
Jason Eisner. 2002b. Parameter estimation for probabilistic
finite-state transducers. In Proceedings of the 40th ACL.
P. Gr¨unwald. 1996. A minimum description length approach
to grammar inference. In S. Wermter et al., eds., Symbolic,
Connectionist and Statistical Approaches to Learning for
NLP, no. 1040 in Lecture Notes in AI, pages 203–216.
Mark Johnson. 1998. PCFG models of linguistic tree represen-
tations. Computational Linguistics, 24(4):613–632.
Beth Levin. 1993. English Verb Classes and Alternations: A
Preliminary Investigation. University of Chicago Press.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn Tree-
bank. Computational Linguistics, 19(2):313–330.
Miles Osborne and Ted Briscoe. 1997. Learning stochastic cat-
egorial grammars. In Proceedings of CoNLL, 80–87. ACL.
A. Stolcke and S.M. Omohundro. 1994. Inducing probabilistic
grammars by Bayesian model merging. In Proc. ofICGI.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.814308833333333">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 63-70. Association for Computational Linguistics. Transformational Priors Over Grammars Eisner Johns Hopkins University, 3400 N. Charles St., NEB 224, Baltimore, MD</note>
<abstract confidence="0.998415144278607">This paper proposes a novel class of PCFG parameterizations that support linguistically reasonable priors over PCFGs. To estimate the parameters is to discover a notion of relatedness among context-free rules such that related rules tend to have related probabilities. The prior favors grammars in which the relationships are simple to describe and have few major exceptions. A basic version that bases relatedness on weighted edit distance yields superior smoothing of grammars learned from the Penn Treebank (20% reduction of rule perplexity over the best previous method). 1 A Sketch of the Concrete Problem This paper uses a new kind of statistical model to smooth the probabilities of PCFG rules. It focuses on “flat” or “dependency-style” rules. These resemble subcategorization frames, but include adjuncts as well as arguments. verb generates 3 dependents—a left, and an object goal right: S put NP PP: Jim put [the pizza] [in the oven] also take other dependents, in other rules: S Adv put NP PP: Jim often put [a pizza] [in the oven] S put NP PP PP: Jim put soup [in an oven] [at home] S put NP: Jim put [some shares of IBM stock] S put Prt NP: Jim put away [the sauce] S put NP PP: to put [the pizza] [in the oven] S put NP PP SBAR: Jim put it [to me] [that other rules arise if add, drop, reorder, retype its dependents. These operations rules are semantically motivated and quite common (Table 1). We wish to learn contextual probabilities for the edit operations, based on an observed sample of flat rules. In English we should discover, for example, that it is quite to add or delete the right edge of a rule. These contextual edit probabilities will help us guess the true probabilities of novel or little-observed rules. However, rules are often idiosyncratic. Our smoothing method should not keep us from noticing (given evidence) that a often than most verbs. Hence this paper’s proposal is a Bayesian smoothing method that allows idiosyncrasy in the gramwhile presuming regularity to be more likely The model will assign a positive probability to each of the infinitely many formally possible rules. The following bizarre rule is not observed in training, and seems very unlikely. But there is no formal reason to rule it out, it us parse an unlikely test sentence. So the model will allow it some tiny probability: S Adv PP put PP PP PP NP AdjP S 2 Background and Other Approaches PCFG is a conditional probability function For example, NP PP the probaof the rule NP With lexicalized nonit has form Usually one makes an independence assumption and this as PP factors that dependent headwords the selectional preferences of This paper is about the first factor, PP In supervised learning, it is simplest to use a maximum likelihood estimate (perhaps with backoff from Charniak (1997) calls this a “Treebank grammar” and gambles that assigning 0 probability to rules unseen in training data will not hurt parsing accuracy too much. However, there are four reasons not to use a Treebank grammar. First, ignoring unseen rules necessarily sacrifices some accuracy. Second, we will show that it improves accuracy to flatten the parse trees and use flat, rules like put NP PP this avoids overly strong independence assumptions, but it increases the number of unseen rules and so makes Treebank grammars less tenable. Third, backing off from the word is a crude technique that does not distinguish Fourth, one would eventually like to reduce or eliminate supervision, and then generalization is important to constrain the search to reasonable grammars. smooth the distribution one can define it in terms of a set of parameters and then estimate parameters. Most researchers have used an model (Eisner, 1996; Charniak, 2000) or more general Markov model (Alshawi, 1996) to model the sequence nonterminals in the RHS. The sequence NP PP in our example is then assumed to be emitted by some model of rules (again with backoff from Collins (1997, model 2) uses a more sophisticated in which all this sequence are generated jointly, as in a Treebank grammar, and then a Markov process is used to insert adjuncts among the arguments. While Treebank models overfit the training data, Markov models underfit. A simple compromise (novel to paper) is a hybrid which backs off from a Treebank model to a Markov. Like this paper’s main proposal, it can learn well-observed idrules but generalizes when data are this allows infinitely many rules with might do better by backing off to word clusters, which Charniak (1997) did find provided a small benefit. and Rooth (1998) used a similar hybrid technique These models are beaten by our rather different model, which learns common rules and common edits to them. The comparison is a direct one, based on the perplexity or cross-entropy of trained models on a test set of · · · A subtlety is that two annotation styles are possible. In Penn Treebank, the head of three constituents where underlining denotes a head child) and joins with different dependents at different levels: • the oven]]] the dependency version that we prefer, each word joins with all of its dependents at once: • put the oven]] A PCFG generating the flat structure must estimate put NP PP A non-flat PCFG adds dependents of 3 independent steps, so in effect it factors the flat rule’s probability into 3 supposindependent “subrule probabilities,” PP Our evaluation judges the estimates of flat-rule probabilities. Is it better to estimate these directly, or as a of estimated subrule Transformational smoothing is best applied to the former, so that the edit operations can freely rearrange all of a word’s dependents. We will see that the Markov and Treebank/Markov models also work much better this way—a useful finding. 3 The Abstract Problem: Designing Priors This section outlines the Bayesian approach to learning probabilistic grammars (for us, estimating a distribution over flat CFG rules). By choosing among the many that generated the training data, the learner is choosing how to generalize to novel sentences. To guide the learner’s choice, one can explicitly speca prior probability distribution possible which themselves specify probability distributions over strings, rules, or trees. A learner should maximizes where the set of strings, rules, or trees observed by the learner. The factor favors regularity (“pick an priori grammar”), while the second favors fitting the idiosynof the data, especially the commonest distributions that they acquired from an automatically-parsed treebank. the methods evaluated here apply also to full PCFGs, verb-headed rules · · · the most varied, interesting cases. Many researchers have tried to learn verb subcategorization, though usually not probabilistic subcategorization. testing the latter case, we sum over internal bracketings of the rule. We do train this case on the true internal bracketing, but it loses even with this unfair advantage. approach is called semi-Bayesian or Maximum A Pos- Priors can help both unsupervised and supervised learning. (In the semi-supervised experiments here, training data is not raw text but a sparse sample of flat rules.) Indeed a good deal of syntax induction work has been carried out in just this framework (Stolcke and Omohundro, 1994; Chen, 1996; De Marcken, 1996; Gr¨unwald, 1996; Osborne and Briscoe, 1997). However, all such work to date has adopted rather simple prior distributions. it has defined favor PCFGs whose rules are few, short, nearly equiprobable, and defined over a small set of nonterminals. Such definitions are convenient, especially when specifying an encoding for MDL, but since they treat all rules alike, they may not be good descriptions of linguistic plausibility. For example, they never penalize the a predictable rule. A prior distribution can, however, be used to encode kinds of After all, a prior is really a soft form of Universal Grammar: it gives the learner enough prior knowledge of grammar to overcome Chomsky’s “poverty of the stimulus” (i.e., sparse data). • A preference for small or simple grammars, as above. • Substantive preferences, such as a preference for verbs take 2 nominal arguments, or to allow • Preferences for systematicity, such as a preference for the rules to be consistently head-initial or head-final. This paper shows how to design a prior that favors a certain kind of systematicity. Lexicalized grammars for natural languages are very large—each word specifies a distribution over all possible dependency rules it could head—but they tend to have internal structure. The new prefers in which a rule’s probability can well-predictedfrom the probabilities ofother rules, using linguistic transformations such as edit operations. example, Adv NP PP corwith PP Both numbers are for medium for and low for The slope of the regression line has to do the rate of preverbal in English. The correlation is not perfect (some verbs are especially prone to adverbial modification), which is why we will only model it with a prior. To just the extent that eviabout sparse, the prior will cause the learner to smooth the two probabilities toward the regression line. 4 Patterns Worth Modeling Before spelling out our approach, let us do a sanity check. a flat rule whose headword is replaced with learning, since it is equivalent to maximizing It is also equivalent to Minimum Description Length (MDL) which minimizes the total number ofbits to encode grammar and data, because one can choose encoding scheme where = or conversely, probability distributions by = MI α 0 MI α 0 MI α 0</abstract>
<address confidence="0.8834045">9.01 4.76 5.54 NP TO 8.65 4.17 5.25 NP MD NP 8.01 2.77 4.67 NP MD 7.69 6.13 4.62 NP 8.49 5.72 PP 3.19 NP 7.91 5.36 MD RB 2.05 NP 7.01 5.16 PP 5.08 8.45 5.11 4.86 8.30 4.85 4.53 8.04 4.84 3.50 7.01 4.49 TO 3.17 7.01 ”] 4.36 MD 2.28 4.75 4.36 TO 1.89 NP 6.94 4.26 MD 2.56 NP 5.94 ”] 4.26 2.20 5.90 .] 4.21 4.89 NP NP 5.82 ADVP 4.20 MD 4.57 NP ADVP NP 4.68 3.99 4.51 NP NP 4.50 3.69 MD 3.35 NP S 3.23 3.60 ] 2.99 NP 2.07 ] 3.56 2.96 NP NP PP 1.91 2.56 2.25 NP 1.63 2.04 2.20 NP 4.52 1.99 4.82 S 4.27 1.69 4.58 S 3.36 ] 1.68 PP 3.30 S 2.66 1.03 2.93 S NP 2.37 4.75 .] 2.28 S</address>
<abstract confidence="0.99824276119403">1: The most predictive pairs of sentential frames. If in training data at least 5 times with a given headword in position, then tends to appear at least once with that headword. MI measures the mutual information of events, computed over all words. When MI is large, as here, the edit distance between to be strikingly small (1 or 2), and certain linguistically plausible edits are extremely common. variable “ ” (corresponding to Table 1 illustrates that in the Penn Treebank, if frequent rules with matching rules with frame there are edit operations (section 1) to easily turn How about rare rules, whose probabilities are most in need of smoothing? Are the same edit transformations that we can learn from frequent cases (Table 1) appropriate for predicting the rare cases? The very rarity of these rules makes it impossible to create a table like Table 1. However, rare rules can be measured in the aggregate, and the result suggests that the same kinds of transformations are indeed useful—perhaps even more useful—in them. Let us consider the set 2,809,545 possible flat rules that stand at edit distance 1 from the set · · · observed in our English training data. is, a rule such as put NP in it did not appear in training data itself, but could be derived by a single edit from some rule that did appear. A bigram Markov model (section 2) was used to iden- 2,714,763 rare rules in that were predicted occur with probability their headwords. 79 of these rare rules actually appeared in a development-data set of 1423 rules. The bigram model would have expected only 26.2 appearances, given the lexical headwords in the test data set. The difference is significant &lt; bootstrap test). In other words, the bigram model underpredicts the edit-distance “neighbors” of observed rules by a factor can therefore hope to use the edit transformations to improve on the bigram model. For example, the results are obtained when we examine just one particular kind of edit operation, or rules of one particular length. recognizes that if · · Y Z · · been observed, then · · Z · · plausible even if bigram Z not previously been observed. Presumably, edit operations are common because they modify a rule in semantically useful ways, allowing the filler of a semantic role to be expressed (Insert), suppressed (Delete), retyped (Substitute), or heavy-shifted (Swap). Such “valency-affecting operations” have repeatedly been invoked by linguists; they are not confined a learner of an unknown language can expect priori flat rules related by edit operations may have related probabilities. operations varies by language. Each language defines its own weighted, contextual, asymmetric edit distance. So the learner will have to dishow likely are in particular contexts. For example, it must learn the rates of preverand right-edge Evidence about these rates comes mainly from the frequent rules. 5 A Transformation Model The form of our new model is shown in Figure 1. The vertices are flat context-free rules, and the arcs between represent edit transformations. The set of arcs leav- (1991) writes that whenever linguists run into the problem of systematic redundancy in the syntactic lexicon, they design a scheme in which lexical entries can be derived from one another by just these operations. We are doing the same thing. The only twist that the lexical entries (in our case, flat PCFG rules) have probabilities that must also be derived, so we will assume that the speaker applies these operations (randomly from the hearer’s viewpoint) at various rates to be learned.</abstract>
<date confidence="0.547489">0.0002 0.0011</date>
<note confidence="0.992590833333333">To fund NP To fund PP NP To fund NP PP To merge PP NP To merge NP To merge NP PP</note>
<abstract confidence="0.978799598901099">fund NP PP NP past merge NP PP NP PP right edge 1: A fragment of a transformation model. Vertices are possible context-free rules (their left-hand sides, are omitted to avoid visual clutter). Arc probabilities are determined log-linearly, as shown, from a real-valued vector feature weights. The are chosen so that the arcs leaving each vertex have total probability 1. Dashed arrows represent arcs not shown here (there are hundreds from each vertex, mainly insertions). Also, not all features are shown (see Table 2). ing any given vertex has total probability 1. The learner’s job is to discover the probabilities. Fortunately, the learner does not have to learn a separate probability for each of the (infinitely) many arcs, since many of the arcs represent identical or similar edits. As shown in Figure 1, an arc’s probability is determined from meaningful features of the arc, using a conditional model of The learner has to learn the finite vector feature weights. Arcs that represent similar transformations have similar features, so they tend to have similar probabilities. model really a PCFG with unparameterization. That is, for any value of it defines a language-specific probability distribution over all possible context-free rules (graph vertices). To sample from this distribution, take a random walk from the vertex the special vertex The at the last vertex reached before the sample. This sampling procedure models a process where the speaker chooses an initial rule and edits it repeatedly. random walk might reach fund NP in two steps and simply halt there. This happens probability expOr, having at fund it might transform into fund PP NP then further to fund NP PP halting. fund NP the probability that the random walk somehow reaches fund NP PP halts there. Condithis probability gives NP PP as needed for the experiments of this paper do not allow transformations it is nontrivial to solve for the probability disover grammar rules Let the vertex This is defined to be the total probability of paths from Equivalently, it is the expected of times be visited by a random walk The following recurrence defines = = Since solving the large linear system (1) would be prohibitively expensive, in practice we use an approximate relaxation algorithm (Eisner, 2001) that propagates flow through the graph until near-convergence. In general this may underestimate the true probabilities somewhat. consider how the parameter vector the over rules, in Figure 1: By raising the weight one can the flow to fund merge and the like. By equation (2), this also increases the probability of these rules. the effect through the graph to increase flow and probability at those rules’ graph, such as merge NP a single parameter a whole complex of rule probabilities (roughly speaking, the infinitival transitives). The model thereby captures the fact that, although that change the LHS or headword of a rule, so it is trivial to find divisor in Figure 1 it is 0.0011. But in general, LHS-changing transformations can be useful (Eisner, 2001). 1 else rules are mutually exclusive events whose probabilities sum to 1, transformationally related rules have positively correlated probabilities that rise and fall together. The weight on all and only the to merge NP That rule has higher probability than predicted by as (since unlike actually tends to subfor model its idiosyncratic probone can raise This “lists” the rule specially in the grammar. Rules derived from it also increase in (e.g., Adv merge NP since again the effect feeds through the graph. The weight the strength of relationship. Equations (1) and (2) imthat fund NP modeled as a linear combination of the probabilities of that rule’s in the graph. the coefficient of fund this linear combination, the coefficient approaching zero as Narrower generalization weights such as likely to be inserted. To learn the feature weights is to learn which features of a transformation make it probable or improbable in the language. Note that the vertex labels, graph topology, and arc parameters are language independent. That is, Figure 1 is supposed to represent Universal Grammar: it tells a learner what kinds of generalizations to look for. The part is which specifies which generalizations and exceptions help to model the data. 6 The Prior The model has more parameters than data. Why? Beyond the initial weights and generalization weights, in practice allow one exception weight (e.g., for each rule that appeared in training data. (This makes it possible to learn arbitrary exceptions, as in a Treebank grammar.) Parameter estimation is nonetheless possible, using a to help choose among the many values of do a reasonable job of explaining the training data. The prior constrains the degrees of freedom: while many parameters are available in principle, the prior will ensure that the data are described using as few of them as possible. point of reparameterizing a PCFG in terms of as in Figure 1, is precisely that only one parameter is needed per linguistically salient property of the PCFG. a broadly targeted transforma- Making an idiosyncratic rule, or class of rules, together with other rules derived from them. But it takes more parameters to encode less systematic properties, such as narrowly targeted edit transor families of unrelated exceptions. natural prior for the parameter vector specified in terms of a variance We simply that the weights independent samples from the normal distribution with mean 0 and variand Rosenfeld, 1999): • • • x equivalently, that drawn from a multivariate Gauswith mean diagonal covariance matrix says that the learner expects most features in Figure 1 to have weights close to zero, i.e., to be Maximizing finding a relatively small set of features that adequately describe the rules and exceptions of the grammar. Reducing the this bias toward simplicity. example, if fund NP PP fund NP PP both observed more than the current predicts, then the learner can follow either (or both) of two strategies: raise or raise The former strategy fits the training data only; the latter affects many disparate arcs and leads generalization. The latter strategy may harm is preferred by the prior it uses one painstead of two. If two words act like the pressure to generalize is stronger. 7 Perturbation Parameters In experiments, we have found that a slight variation on model gets slightly better results. Let the exception weight (if any) that allows one to tune the probof rule We eliminate introduce a different called a which is used in the following replacements for equations (1) and (2): = = a global normalizing factor chosen so that = The new prior on the same as the prior on either raise the learner do this to account for observations of training data. The probabilities of other rules consequently deso that = When raised, rules’ probabilities are scaled down slightly and equally When raised, probafrom its these are similar to tend appear in test data if in training data. Raising disproportionately harming siblings requires manipulation of many other parameters, which is discouraged by the prior and may also suffer from search error. speculate that this is why better. the probability of an arc from the of arcs from siblings of as they sum to 1. basic Treebank/Markov Katz flat flat non-flat Treebank 00 00 1-gram 1774.9 86435.1 340.9 160.0 193.2 2-gram 135.2 199.3 127.2 116.2 174.7 3-gram 136.5 177.4 132.7 123.3 174.8 363.0 494.5 197.9 transformation 108.6 102.3 1-gram 1991.2 96318.8 455.1 194.3 233.1 236.6 153.2 138.8 205.6 3-gram 161.9 211.0 156.8 145.7 208.1</abstract>
<note confidence="0.943161333333333">Collins 414.5 589.4 242.0 (Insert) (Insert, target) (Insert, left) (Insert, target, left) (Insert, right) (Insert, target, right) (Insert, left, right) (Insert, side) (Insert, side, target) (Insert, side, left) (Insert, side, target, left) (Insert, side, right) (Insert, side, target, right) (Insert, side, left, right)</note>
<abstract confidence="0.96494802661597">Table 2: Each Insert arc has 14 features. The features of any given arc are found by instantiating the tuples above, as shown. instantiated tuple has a weight specified in · · · only train dev test Treebank sections 0–15 16 17 sentences 15554 1343 866 rule tokens 18836 1588 973 rule types 11565 1317 795 frame types 2722 564 365 headword types 3607 756 504 novel rule tokens 51.6% 47.8% novel frame tokens 8.9% 6.3% novel headword tokens 10.4% 10.2% novel rule types 61.4% 57.5% novel frame types 24.6% 16.4% novel headword types 20.9% 18.8% nonterminaltypes 78 with RHS length = Table 3: Properties of the experimental data. “Novel” means not observed in training. “Frame” was defined in section 4. To evaluate the quality of generalization, we used pretraining data testing data 3). Each dataset consisted of a collection of flat rules such as put NP PP from the Penn Tree- (Marcus et al., 1993). Thus, each defined as a product of rule probof the form put NP PP learner attempted to maximize gradient ascent. This amounts to learning the generalizations and exceptions that related the training The evaluation measure was then the perplexon test data, To get a good (low) perplexity score, the model had to assign reasonprobabilities to the many novel rules in 3). For many of these rules, even the frame was novel. Note that although the training data was preparsed into rules, it was not annotated with the paths in Figure 1 that those rules, so estimating still an unsupervised learning problem. The transformation graph had about 14 features per arc (Table 2). In the finite part of the transformation graph that was actually explored (including bad arcs that compete with good ones), about 70000 distinct features were encountered, though after training, only a few hundred (Eisner, 2001) for full details of data preparation, model structure, parameter initialization, backoff levels for the comparison models, efficient techniques for computing the objective and its gradient, and more analysis of the results. (b) off from Treebank grammar with Katz vs. one-count backoff (Chen and Goodman, 1996) (Note: One-count was alused for backoff and Collins models.) section 2 for discussion (1997, model 2) of transformation model with best other model Table 4: Perplexity of the test set under various models. (a) Full training set. (b) Half training set (sections 0–7 only). feature weights were substantial, and only a few thousand were even far enough from zero to affect performance. was also a parameter each observed rule Results are given in Table 4a, which compares the transformation model to various competing models discussed in section 2. The best (smallest) perplexities appear in boldface. The key results: • The transformation model was the winner, reducing perplexity by 20% over the best model replicated from previous literature (a bigram model). • Much of this improvement could be explained by the transformation model’s ability to model exceptions. Adding this ability more directly to the bigram model, using the new Treebank/Markov approach of section 2, also reduced perplexity from the bigram model, by 6% or 14% depending on whether Katz or one-count backoff was used, versus the transformation model’s 20%. • Averaging the transformation model with the best competing model (Treebank/bigram) improved it by an additional 6%. So using transformations yields a total perplexity reduction of 12% over Treebank/bigram, and 24% over the best previous model from the literature (bigram). • What would be the cost of achieving such a perplexity improvement by additional annotation? Training the averaged model on only the first half of the training set, with no further tuning of any options (Table 4b), yielded a test set perplexity of 118.0. So by using transformations, we can achieve about the same perplexity as the best model transformations (Treebank/bigram, 116.2), half as much training • Furthermore, comparing Tables 4a and 4b shows that the transformation model had the most graceful performance degradation when the dataset was reduced in size. If the arc inserts fund then (a) right=—— side=left of head p(rule  |headword): averaged transf. 1e−10 1e−07 1e−04 1e−01 5e−04 5e−03 5e−02 5e−01 0.001 0.010 0.100 1.000 1e−10 1e−07 1e−04 1e−01 5e−04 5e−03 5e−02 5e−01 0.001 0.010 0.100 1.000 p(rule  |headword): Treebank/bigram Figure 2: Probabilities of test set flat rules under the averaged model, plotted against the corresponding probabilities under the best transformation-free model. Improvements fall above the main diagonal; dashed diagonals indicate a factor of two. The three plots (at different scales!) partition the rules by the number of training observations: 0 (left graph), 1 (middle), This is an encouraging result for the use of the method less supervised contexts (although results on a dataset would be more convincing in this regard). • The competing models from the literature are best used to predict flat rules directly, rather than by summing over their possible non-flat internal structures, as has been done in the past. This result is significant in itself. Extending Johnson (1998), it shows the inappropriateness of the traditional independence assumptions that build up a frame by several rule expansions (section 2). Figure 2 shows that averaging the transformation model with the Treebank/bigram model improves the latter not merely on balance, but across the board. In other words, there is no evident class of phenomena for which incorporating transformations would be a bad idea. • Transformations particularly helped raise the estimates of the low-probability novel rules in test data, as hoped. • Transformations also helped on test rules that had been observed once in training with relatively infrequent words. (In other words, the transformation model does not discount singletons too much.) • Transformations hurt slightly on balance for rules observed more than once in training, but the effect was tiny. All these differences are slightly exaggerated if one compares the transformation model directly with the Treebank/bigram model, without averaging. The transformation model was designed to use edit operations in order to generalize appropriately from a word’s observed frames to new frames that are likely to appear with that word in test data. To directly test the model’s success at such generalization, we compared it to the bigram model on a pseudo-disambiguation task. Each instance of the task consisted of a pair of rules test data, expressed as (word, frame) pairs such that “novel” frames that in training data (with any headword). model was then asked: Does with or vice-versa? In other words, which is big- Since the frames were novel, the model had to make choice according to whether more the frames that had actually been observed with the past, and likewise What this means depends on the model. The bigram model takes two frames to alike if they many bigrams in common. transformation model takes two frames to look alike if are by a path ofprobable transformations. test data contained 62 distinct rules a novel frame. This yielded 1891 pairs of rules, leading to 1811 task instances after obvities were Baseline performance on this difficult task is 50% (random guess). The bigram model chose correctly in 1595 of the 1811 instances (88.1%). Parameters for “memorizing” specific frames do not help on this task, which involves only novel frames, so the Treebank/bigram model had the same performance. By contrast, the transformation model got 1669 of 1811 correct (92.2%), for a morethan-34% reduction in error rate. (The development set showed similar results.) However, since the 1811 task instances were derived non-independently from just 62 novel rules, this result is based on a rather small sample. 9 Discussion This paper has presented a nontrivial way to reparameterize a PCFG in terms of “deep” parameters representing transformations and exceptions. A linguistically sensible prior was natural to define over these deep parameters. Famous examples of “deep reparameterization” are the Fourier transform in speech recognition and the SVD transform for Latent Semantic Analysis in IR. Like our technique, they are intended to reveal significant structure through the leading parameters while relegating noise and exceptions to minor parameters. Such representations obvious tie is an instance where or where novel headwords. (The 62 rules included 11 with novel headwords.) In such cases, neither the bigram nor the transformation model has any basis for making its decision: the probabilities being compared will necessarily be equal. make it easier to model the similarity or probability of the objects at hand (waveforms, documents, or grammars). Beyond the fact that it shows at least a good perplexity improvement (it has not yet been applied to a real task), an exciting “big idea” aspect of this work is its flexibility in defining linguistically sensible priors over grammars. Our reparameterization is made with reference to a user-designed transformation graph (Figure 1). The graph need not be confined to edit distance transformations, or to the simple features of Table 2 (used here for comparability with the Markov models), which cona transformation’s probability on In principle, the approach could be used to capture a great many linguistic phenomena. Figure 1 could be extended with more ambitious transformations, such as gapping, gap-threading, and passivization. The flat rules could be annotated with internal structure (as in TAG) and thematic roles. Finally, the arcs could bear further features. For example, the probability of unaccusative movesank the boat boat should depend on whether the headword is a change-of-state verb. Indeed, Figure 1 can be converted to any lexicalized theory of grammar, such as categorial grammar, TAG, LFG, HPSG, or Minimalism. The vertices represent lexical entries and the arcs represent probabilistic lexical redundancy rules or metarules (see footnote 8). The transformation model approach is therefore a full stochastic treatment of lexicalized syntax— apparently the first to treat lexical redundancy rules, although (Briscoe and 1999) give an hoc See (Eisner, 2001; Eisner, 2002a) for more discussion. It is worthwhile to compare the statistical approach here with some other approaches: • Transformation models are similar to graphical models: they allow similar patterns of deductive and abductive inference from observations. However, the vertices of a transformation graph do not represent different random variables, but rather mutually exclusive values of the same random variable, whose probabilities sum to 1. • Transformation models incorporate conditional loglinear (maximum entropy) models. As an alternative, one could directly build a conditional log-linear model However, such a model would learn probabilities, not relationships. A feature weight would not really model the strength of the relationship between frames share that feature. It would only inboth frames’ probabilities. If the probability of altered by some (e.g., an exception then the probability of not respond. • A transformation model can be regarded as a proba- FSA that consists mostly of (Rules only emitted on the arcs to This perspective allows use of generic methods for finite-state parameter estimation (Eisner, 2002b). We are strongly interested in improving the speed of such methods and their ability to avoid local maxima, which are currently the major difficulty with our system, as they are for many unsupervised learning techniques. We expect to further pursue transformation models (and simpler variants that are easier to estimate) within this flexible finite-state framework. The interested reader is encouraged to look at (Eisner, 2001) for a much more careful and wide-ranging discussion of transformation models, their algorithms, and their relation to linguistic theory, statistics, and parsing. Chapter 1 provides a good overview. For a brief article highlighting the connection to linguistics, see (Eisner, 2002a). References Hiyan Alshawi. 1996. Head automata for speech translation. Philadelphia, PA. T. Briscoe and A. Copestake. 1999. Lexical rules in constraintgrammar. 25(4):487–526. Bob Carpenter. 1991. The generative power of categorial grammars and head-driven phrase structure grammars with lexical 17(3):301–313. Glenn Carroll and Mats Rooth. 1998. Valence induction with a PCFG. In of Eugene Charniak. 1997. Statistical parsing with a context-free and word statistics. In 598–603. Eugene Charniak. 2000. A maximum-entropy inspired parser. Stanley Chen and Joshua Goodman. 1996. An empirical study smoothing techniques. In</abstract>
<author confidence="0.348798">A Gaussian prior</author>
<note confidence="0.820570933333333">for smoothing maximum entropy models. Technical Report CMU-CS-99-108, Carnegie Mellon University, February. Chen. 1996. Probabilistic Models for Natural Ph.D. thesis, Harvard University. Michael J. Collins. 1997. Three generative, lexicalised models statistical parsing. In 16–23. De Marcken. 1996. Language Ph.D. thesis, MIT. Eisner. 1996. Three new probabilistic models for depenparsing: An exploration. of 340–345. Eisner. 2001. a Probabilistic Lexicon via Syn- Ph.D. thesis, Univ. of Pennsylvania. Jason Eisner. 2002a. Discovering syntactic deep structure via statistics. 26(3), May. Jason Eisner. 2002b. Parameter estimation for probabilistic transducers. In of the 40th P. Gr¨unwald. 1996. A minimum description length approach grammar inference. In S. Wermter et al., eds., Connectionist and Statistical Approaches to Learning for no. 1040 in Lecture Notes in AI, pages 203–216. Mark Johnson. 1998. PCFG models of linguistic tree represen- 24(4):613–632. Levin. 1993. Verb Classes and Alternations: A University of Chicago Press. M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Tree- 19(2):313–330. Osborne and Ted Briscoe. 1997. Learning stochastic catgrammars. In of 80–87. ACL. A. Stolcke and S.M. Omohundro. 1994. Inducing probabilistic</note>
<author confidence="0.646072">In</author>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
</authors>
<title>Head automata for speech translation.</title>
<date>1996</date>
<booktitle>In Proceedings ofICSLP,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4580" citStr="Alshawi, 1996" startWordPosition="784" endWordPosition="785">trong independence assumptions, but it increases the number of unseen rules and so makes Treebank grammars less tenable. Third, backing off from the word is a crude technique that does not distinguish among words.2 Fourth, one would eventually like to reduce or eliminate supervision, and then generalization is important to constrain the search to reasonable grammars. To smooth the distribution p(RHS |LHS), one can define it in terms of a set of parameters and then estimate those parameters. Most researchers have used an n-gram model (Eisner, 1996; Charniak, 2000) or more general Markov model (Alshawi, 1996) to model the sequence of nonterminals in the RHS. The sequence Vput NP PP in our example is then assumed to be emitted by some Markov model of VPput rules (again with backoff from put). Collins (1997, model 2) uses a more sophisticated model in which all arguments in this sequence are generated jointly, as in a Treebank grammar, and then a Markov process is used to insert adjuncts among the arguments. While Treebank models overfit the training data, Markov models underfit. A simple compromise (novel to this paper) is a hybrid Treebank/Markov model, which backs off from a Treebank model to a M</context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>Hiyan Alshawi. 1996. Head automata for speech translation. In Proceedings ofICSLP, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>A Copestake</author>
</authors>
<title>Lexical rules in constraintbased grammar.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="38819" citStr="Briscoe and Copestake, 1999" startWordPosition="6750" endWordPosition="6753">her features. For example, the probability of unaccusative movement (someone sank the boat → the boat sank) should depend on whether the headword is a change-of-state verb. Indeed, Figure 1 can be converted to any lexicalized theory of grammar, such as categorial grammar, TAG, LFG, HPSG, or Minimalism. The vertices represent lexical entries and the arcs represent probabilistic lexical redundancy rules or metarules (see footnote 8). The transformation model approach is therefore a full stochastic treatment of lexicalized syntax— apparently the first to treat lexical redundancy rules, although (Briscoe and Copestake, 1999) give an ad hoc approach. See (Eisner, 2001; Eisner, 2002a) for more discussion. It is worthwhile to compare the statistical approach here with some other approaches: • Transformation models are similar to graphical models: they allow similar patterns of deductive and abductive inference from observations. However, the vertices of a transformation graph do not represent different random variables, but rather mutually exclusive values of the same random variable, whose probabilities sum to 1. • Transformation models incorporate conditional loglinear (maximum entropy) models. As an alternative, </context>
</contexts>
<marker>Briscoe, Copestake, 1999</marker>
<rawString>T. Briscoe and A. Copestake. 1999. Lexical rules in constraintbased grammar. Computational Linguistics, 25(4):487–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>The generative power of categorial grammars and head-driven phrase structure grammars with lexical rules.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="16866" citStr="Carpenter (1991)" startWordPosition="3024" endWordPosition="3026">ve related probabilities. However, which edit operations varies by language. Each language defines its own weighted, contextual, asymmetric edit distance. So the learner will have to discover how likely particular edits are in particular contexts. For example, it must learn the rates of preverbal Adv-insertion and right-edge PP-insertion. Evidence about these rates comes mainly from the frequent rules. 5 A Transformation Model The form of our new model is shown in Figure 1. The vertices are flat context-free rules, and the arcs between them represent edit transformations. The set of arcs leav8Carpenter (1991) writes that whenever linguists run into the problem of systematic redundancy in the syntactic lexicon, they design a scheme in which lexical entries can be derived from one another by just these operations. We are doing the same thing. The only twist that the lexical entries (in our case, flat PCFG rules) have probabilities that must also be derived, so we will assume that the speaker applies these operations (randomly from the hearer’s viewpoint) at various rates to be learned. 0.0002 0.0011 START HALT START(fund) HALT START(merge) exp θ0 Z4 HALT exp θ0 Z2 To fund NP exp θ1 Z1 exp θ2+θ8 Z1 e</context>
</contexts>
<marker>Carpenter, 1991</marker>
<rawString>Bob Carpenter. 1991. The generative power of categorial grammars and head-driven phrase structure grammars with lexical rules. Computational Linguistics, 17(3):301–313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Mats Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized PCFG.</title>
<date>1998</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5502" citStr="Carroll and Rooth (1998)" startWordPosition="936" endWordPosition="939">ointly, as in a Treebank grammar, and then a Markov process is used to insert adjuncts among the arguments. While Treebank models overfit the training data, Markov models underfit. A simple compromise (novel to this paper) is a hybrid Treebank/Markov model, which backs off from a Treebank model to a Markov. Like this paper’s main proposal, it can learn well-observed idiosyncratic rules but generalizes when data are sparse.3 1Nonstandardly, this allows infinitely many rules with p&gt;0. 2One might do better by backing off to word clusters, which Charniak (1997) did find provided a small benefit. 3Carroll and Rooth (1998) used a similar hybrid technique These models are beaten by our rather different model, transformational smoothing, which learns common rules and common edits to them. The comparison is a direct one, based on the perplexity or cross-entropy of the trained models on a test set of S → · · · rules.4 A subtlety is that two annotation styles are possible. In the Penn Treebank, put is the head of three constituents (V, VP, and S, where underlining denotes a head child) and joins with different dependents at different levels: • [S [NP Jim] [VP [V put] [NP pizza] [PP in the oven]]] In the flattened or</context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Glenn Carroll and Mats Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proc. ofAAAI,</booktitle>
<pages>598--603</pages>
<contexts>
<context position="3529" citStr="Charniak (1997)" startWordPosition="614" endWordPosition="615"> Other Approaches A PCFG is a conditional probability function p(RHS | LHS).1 For example, p(V NP PP |VP) gives the probability of the rule VP → V NP PP. With lexicalized nonterminals, it has form p(Vput NPpizza PPin |VPput). Usually one makes an independence assumption and defines this as p(Vput NP PP |VPput) times factors that choose dependent headwords pizza and in according to the selectional preferences of put. This paper is about estimating the first factor, p(Vput NP PP |VPput). In supervised learning, it is simplest to use a maximum likelihood estimate (perhaps with backoff from put). Charniak (1997) calls this a “Treebank grammar” and gambles that assigning 0 probability to rules unseen in training data will not hurt parsing accuracy too much. However, there are four reasons not to use a Treebank grammar. First, ignoring unseen rules necessarily sacrifices some accuracy. Second, we will show that it improves accuracy to flatten the parse trees and use flat, dependency-style rules like p(NP put NP PP |Sput); this avoids overly strong independence assumptions, but it increases the number of unseen rules and so makes Treebank grammars less tenable. Third, backing off from the word is a crud</context>
<context position="5441" citStr="Charniak (1997)" startWordPosition="928" endWordPosition="929">which all arguments in this sequence are generated jointly, as in a Treebank grammar, and then a Markov process is used to insert adjuncts among the arguments. While Treebank models overfit the training data, Markov models underfit. A simple compromise (novel to this paper) is a hybrid Treebank/Markov model, which backs off from a Treebank model to a Markov. Like this paper’s main proposal, it can learn well-observed idiosyncratic rules but generalizes when data are sparse.3 1Nonstandardly, this allows infinitely many rules with p&gt;0. 2One might do better by backing off to word clusters, which Charniak (1997) did find provided a small benefit. 3Carroll and Rooth (1998) used a similar hybrid technique These models are beaten by our rather different model, transformational smoothing, which learns common rules and common edits to them. The comparison is a direct one, based on the perplexity or cross-entropy of the trained models on a test set of S → · · · rules.4 A subtlety is that two annotation styles are possible. In the Penn Treebank, put is the head of three constituents (V, VP, and S, where underlining denotes a head child) and joins with different dependents at different levels: • [S [NP Jim] </context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proc. ofAAAI, 598–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="4535" citStr="Charniak, 2000" startWordPosition="777" endWordPosition="778">ke p(NP put NP PP |Sput); this avoids overly strong independence assumptions, but it increases the number of unseen rules and so makes Treebank grammars less tenable. Third, backing off from the word is a crude technique that does not distinguish among words.2 Fourth, one would eventually like to reduce or eliminate supervision, and then generalization is important to constrain the search to reasonable grammars. To smooth the distribution p(RHS |LHS), one can define it in terms of a set of parameters and then estimate those parameters. Most researchers have used an n-gram model (Eisner, 1996; Charniak, 2000) or more general Markov model (Alshawi, 1996) to model the sequence of nonterminals in the RHS. The sequence Vput NP PP in our example is then assumed to be emitted by some Markov model of VPput rules (again with backoff from put). Collins (1997, model 2) uses a more sophisticated model in which all arguments in this sequence are generated jointly, as in a Treebank grammar, and then a Markov process is used to insert adjuncts among the arguments. While Treebank models overfit the training data, Markov models underfit. A simple compromise (novel to this paper) is a hybrid Treebank/Markov model,</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy inspired parser. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques.</title>
<date>1996</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="30210" citStr="Chen and Goodman, 1996" startWordPosition="5359" endWordPosition="5362">blem. The transformation graph had about 14 features per arc (Table 2). In the finite part of the transformation graph that was actually explored (including bad arcs that compete with good ones), about 70000 distinct features were encountered, though after training, only a few hundred 12See (Eisner, 2001) for full details of data preparation, model structure, parameter initialization, backoff levels for the comparison models, efficient techniques for computing the objective and its gradient, and more analysis of the results. (b) aBack off from Treebank grammar with Katz vs. one-count backoff (Chen and Goodman, 1996) (Note: One-count was always used for backoff within the n-gram and Collins models.) bSee section 2 for discussion cCollins (1997, model 2) dAverage of transformation model with best other model Table 4: Perplexity of the test set under various models. (a) Full training set. (b) Half training set (sections 0–7 only). feature weights were substantial, and only a few thousand were even far enough from zero to affect performance. There was also a parameter gyre for each observed rule e. Results are given in Table 4a, which compares the transformation model to various competing models discussed in</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMU-CS-99-108,</tech>
<institution>Carnegie Mellon University,</institution>
<contexts>
<context position="24602" citStr="Chen and Rosenfeld, 1999" startWordPosition="4384" endWordPosition="4387">ally salient property of the PCFG. Making θ3 &gt; 0 creates a broadly targeted transformation. Making θ9 =� 0 or θ1 =� 0 lists an idiosyncratic rule, or class of rules, together with other rules derived from them. But it takes more parameters to encode less systematic properties, such as narrowly targeted edit transformations (θ4, θ5) or families of unrelated exceptions. A natural prior for the parameter vector B E Rk is therefore specified in terms of a variance σ2. We simply say that the weights θ1, θ2,... θk are independent samples from the normal distribution with mean 0 and variance σ2 &gt; 0 (Chen and Rosenfeld, 1999): O — N(0, σ2) x N(0, σ2) x • • • x N(0, σ2) (3) or equivalently, that B is drawn from a multivariate Gaussian with mean 0~ and diagonal covariance matrix σ2I, i.e., O — N(~0,σ2I). This says that a priori, the learner expects most features in Figure 1 to have weights close to zero, i.e., to be irrelevant. Maximizing p(B) • p(D B) means finding a relatively small set of features that adequately describe the rules and exceptions of the grammar. Reducing the variance σ2 strengthens this bias toward simplicity. For example, if Sfund → To fund NP PP and Smerge → To fund NP PP are both observed more</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy models. Technical Report CMU-CS-99-108, Carnegie Mellon University, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
</authors>
<title>Building Probabilistic Models for Natural Language.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="8639" citStr="Chen, 1996" startWordPosition="1460" endWordPosition="1461">subcategorization, though usually not probabilistic subcategorization. 5In testing the latter case, we sum over all possible internal bracketings of the rule. We do train this case on the true internal bracketing, but it loses even with this unfair advantage. 6This approach is called semi-Bayesian or Maximum A PosPriors can help both unsupervised and supervised learning. (In the semi-supervised experiments here, training data is not raw text but a sparse sample of flat rules.) Indeed a good deal of syntax induction work has been carried out in just this framework (Stolcke and Omohundro, 1994; Chen, 1996; De Marcken, 1996; Gr¨unwald, 1996; Osborne and Briscoe, 1997). However, all such work to date has adopted rather simple prior distributions. Typically, it has defined p(θ) to favor PCFGs whose rules are few, short, nearly equiprobable, and defined over a small set of nonterminals. Such definitions are convenient, especially when specifying an encoding for MDL, but since they treat all rules alike, they may not be good descriptions of linguistic plausibility. For example, they will never penalize the absence of a predictable rule. A prior distribution can, however, be used to encode various k</context>
</contexts>
<marker>Chen, 1996</marker>
<rawString>Stanley Chen. 1996. Building Probabilistic Models for Natural Language. Ph.D. thesis, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings ofACL/EACL, 16–23. Carl De Marcken.</booktitle>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="4780" citStr="Collins (1997" startWordPosition="821" endWordPosition="822">among words.2 Fourth, one would eventually like to reduce or eliminate supervision, and then generalization is important to constrain the search to reasonable grammars. To smooth the distribution p(RHS |LHS), one can define it in terms of a set of parameters and then estimate those parameters. Most researchers have used an n-gram model (Eisner, 1996; Charniak, 2000) or more general Markov model (Alshawi, 1996) to model the sequence of nonterminals in the RHS. The sequence Vput NP PP in our example is then assumed to be emitted by some Markov model of VPput rules (again with backoff from put). Collins (1997, model 2) uses a more sophisticated model in which all arguments in this sequence are generated jointly, as in a Treebank grammar, and then a Markov process is used to insert adjuncts among the arguments. While Treebank models overfit the training data, Markov models underfit. A simple compromise (novel to this paper) is a hybrid Treebank/Markov model, which backs off from a Treebank model to a Markov. Like this paper’s main proposal, it can learn well-observed idiosyncratic rules but generalizes when data are sparse.3 1Nonstandardly, this allows infinitely many rules with p&gt;0. 2One might do </context>
<context position="30339" citStr="Collins (1997" startWordPosition="5382" endWordPosition="5383">plored (including bad arcs that compete with good ones), about 70000 distinct features were encountered, though after training, only a few hundred 12See (Eisner, 2001) for full details of data preparation, model structure, parameter initialization, backoff levels for the comparison models, efficient techniques for computing the objective and its gradient, and more analysis of the results. (b) aBack off from Treebank grammar with Katz vs. one-count backoff (Chen and Goodman, 1996) (Note: One-count was always used for backoff within the n-gram and Collins models.) bSee section 2 for discussion cCollins (1997, model 2) dAverage of transformation model with best other model Table 4: Perplexity of the test set under various models. (a) Full training set. (b) Half training set (sections 0–7 only). feature weights were substantial, and only a few thousand were even far enough from zero to affect performance. There was also a parameter gyre for each observed rule e. Results are given in Table 4a, which compares the transformation model to various competing models discussed in section 2. The best (smallest) perplexities appear in boldface. The key results: • The transformation model was the winner, redu</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael J. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings ofACL/EACL, 16–23. Carl De Marcken. 1996. Unsupervised Language Acquisition. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>Proc. of COLING,</booktitle>
<pages>340--345</pages>
<contexts>
<context position="4518" citStr="Eisner, 1996" startWordPosition="775" endWordPosition="776">style rules like p(NP put NP PP |Sput); this avoids overly strong independence assumptions, but it increases the number of unseen rules and so makes Treebank grammars less tenable. Third, backing off from the word is a crude technique that does not distinguish among words.2 Fourth, one would eventually like to reduce or eliminate supervision, and then generalization is important to constrain the search to reasonable grammars. To smooth the distribution p(RHS |LHS), one can define it in terms of a set of parameters and then estimate those parameters. Most researchers have used an n-gram model (Eisner, 1996; Charniak, 2000) or more general Markov model (Alshawi, 1996) to model the sequence of nonterminals in the RHS. The sequence Vput NP PP in our example is then assumed to be emitted by some Markov model of VPput rules (again with backoff from put). Collins (1997, model 2) uses a more sophisticated model in which all arguments in this sequence are generated jointly, as in a Treebank grammar, and then a Markov process is used to insert adjuncts among the arguments. While Treebank models overfit the training data, Markov models underfit. A simple compromise (novel to this paper) is a hybrid Treeb</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. Proc. of COLING, 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Smoothing a Probabilistic Lexicon via Syntactic Transformations.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. of Pennsylvania.</institution>
<contexts>
<context position="20670" citStr="Eisner, 2001" startWordPosition="3711" endWordPosition="3712">ments of this paper do not allow transformations Given B, it is nontrivial to solve for the probability distribution over grammar rules e. Let Iθ(e) denote the flow to vertex e. This is defined to be the total probability of all paths from START to e. Equivalently, it is the expected number of times e would be visited by a random walk from START. The following recurrence defines pθ(e):10 Iθ(e) = δe,START + Ee,Iθ(e�) · p(e� → e) (1) pθ(e) = Iθ(e) · p(e → HALT) (2) Since solving the large linear system (1) would be prohibitively expensive, in practice we use an approximate relaxation algorithm (Eisner, 2001) that propagates flow through the graph until near-convergence. In general this may underestimate the true probabilities somewhat. Now consider how the parameter vector B affects the distribution over rules, pθ(e), in Figure 1: • By raising the initial weight θ1, one can increase the flow to Sfund → To fund NP, Smerge → To merge NP, and the like. By equation (2), this also increases the probability of these rules. But the effect also feeds through the graph to increase the flow and probability at those rules’ descendants in the graph, such as Smerge → To merge NP PP. So a single parameter θ1 c</context>
<context position="29893" citStr="Eisner, 2001" startWordPosition="5314" endWordPosition="5315">le probabilities to the many novel rules in E (Table 3). For many of these rules, even the frame was novel. Note that although the training data was preparsed into rules, it was not annotated with the paths in Figure 1 that generated those rules, so estimating θ and π was still an unsupervised learning problem. The transformation graph had about 14 features per arc (Table 2). In the finite part of the transformation graph that was actually explored (including bad arcs that compete with good ones), about 70000 distinct features were encountered, though after training, only a few hundred 12See (Eisner, 2001) for full details of data preparation, model structure, parameter initialization, backoff levels for the comparison models, efficient techniques for computing the objective and its gradient, and more analysis of the results. (b) aBack off from Treebank grammar with Katz vs. one-count backoff (Chen and Goodman, 1996) (Note: One-count was always used for backoff within the n-gram and Collins models.) bSee section 2 for discussion cCollins (1997, model 2) dAverage of transformation model with best other model Table 4: Perplexity of the test set under various models. (a) Full training set. (b) Hal</context>
<context position="38862" citStr="Eisner, 2001" startWordPosition="6760" endWordPosition="6761">movement (someone sank the boat → the boat sank) should depend on whether the headword is a change-of-state verb. Indeed, Figure 1 can be converted to any lexicalized theory of grammar, such as categorial grammar, TAG, LFG, HPSG, or Minimalism. The vertices represent lexical entries and the arcs represent probabilistic lexical redundancy rules or metarules (see footnote 8). The transformation model approach is therefore a full stochastic treatment of lexicalized syntax— apparently the first to treat lexical redundancy rules, although (Briscoe and Copestake, 1999) give an ad hoc approach. See (Eisner, 2001; Eisner, 2002a) for more discussion. It is worthwhile to compare the statistical approach here with some other approaches: • Transformation models are similar to graphical models: they allow similar patterns of deductive and abductive inference from observations. However, the vertices of a transformation graph do not represent different random variables, but rather mutually exclusive values of the same random variable, whose probabilities sum to 1. • Transformation models incorporate conditional loglinear (maximum entropy) models. As an alternative, one could directly build a conditional log-</context>
</contexts>
<marker>Eisner, 2001</marker>
<rawString>Jason Eisner. 2001. Smoothing a Probabilistic Lexicon via Syntactic Transformations. Ph.D. thesis, Univ. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Discovering syntactic deep structure via Bayesian statistics.</title>
<date>2002</date>
<journal>Cognitive Science,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="38876" citStr="Eisner, 2002" startWordPosition="6762" endWordPosition="6763">one sank the boat → the boat sank) should depend on whether the headword is a change-of-state verb. Indeed, Figure 1 can be converted to any lexicalized theory of grammar, such as categorial grammar, TAG, LFG, HPSG, or Minimalism. The vertices represent lexical entries and the arcs represent probabilistic lexical redundancy rules or metarules (see footnote 8). The transformation model approach is therefore a full stochastic treatment of lexicalized syntax— apparently the first to treat lexical redundancy rules, although (Briscoe and Copestake, 1999) give an ad hoc approach. See (Eisner, 2001; Eisner, 2002a) for more discussion. It is worthwhile to compare the statistical approach here with some other approaches: • Transformation models are similar to graphical models: they allow similar patterns of deductive and abductive inference from observations. However, the vertices of a transformation graph do not represent different random variables, but rather mutually exclusive values of the same random variable, whose probabilities sum to 1. • Transformation models incorporate conditional loglinear (maximum entropy) models. As an alternative, one could directly build a conditional log-linear model o</context>
<context position="40118" citStr="Eisner, 2002" startWordPosition="6952" endWordPosition="6953"> model would learn probabilities, not relationships. A feature weight would not really model the strength of the relationship between two frames e, e&apos; that share that feature. It would only influence both frames’ probabilities. If the probability of e were altered by some unrelated factor (e.g., an exception weight), then the probability of e&apos; would not respond. • A transformation model can be regarded as a probabilistic FSA that consists mostly of e-transitions. (Rules are only emitted on the arcs to HALT.) This perspective allows use of generic methods for finite-state parameter estimation (Eisner, 2002b). We are strongly interested in improving the speed of such methods and their ability to avoid local maxima, which are currently the major difficulty with our system, as they are for many unsupervised learning techniques. We expect to further pursue transformation models (and simpler variants that are easier to estimate) within this flexible finite-state framework. The interested reader is encouraged to look at (Eisner, 2001) for a much more careful and wide-ranging discussion of transformation models, their algorithms, and their relation to linguistic theory, statistics, and parsing. Chapte</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002a. Discovering syntactic deep structure via Bayesian statistics. Cognitive Science, 26(3), May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Parameter estimation for probabilistic finite-state transducers.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th ACL.</booktitle>
<contexts>
<context position="38876" citStr="Eisner, 2002" startWordPosition="6762" endWordPosition="6763">one sank the boat → the boat sank) should depend on whether the headword is a change-of-state verb. Indeed, Figure 1 can be converted to any lexicalized theory of grammar, such as categorial grammar, TAG, LFG, HPSG, or Minimalism. The vertices represent lexical entries and the arcs represent probabilistic lexical redundancy rules or metarules (see footnote 8). The transformation model approach is therefore a full stochastic treatment of lexicalized syntax— apparently the first to treat lexical redundancy rules, although (Briscoe and Copestake, 1999) give an ad hoc approach. See (Eisner, 2001; Eisner, 2002a) for more discussion. It is worthwhile to compare the statistical approach here with some other approaches: • Transformation models are similar to graphical models: they allow similar patterns of deductive and abductive inference from observations. However, the vertices of a transformation graph do not represent different random variables, but rather mutually exclusive values of the same random variable, whose probabilities sum to 1. • Transformation models incorporate conditional loglinear (maximum entropy) models. As an alternative, one could directly build a conditional log-linear model o</context>
<context position="40118" citStr="Eisner, 2002" startWordPosition="6952" endWordPosition="6953"> model would learn probabilities, not relationships. A feature weight would not really model the strength of the relationship between two frames e, e&apos; that share that feature. It would only influence both frames’ probabilities. If the probability of e were altered by some unrelated factor (e.g., an exception weight), then the probability of e&apos; would not respond. • A transformation model can be regarded as a probabilistic FSA that consists mostly of e-transitions. (Rules are only emitted on the arcs to HALT.) This perspective allows use of generic methods for finite-state parameter estimation (Eisner, 2002b). We are strongly interested in improving the speed of such methods and their ability to avoid local maxima, which are currently the major difficulty with our system, as they are for many unsupervised learning techniques. We expect to further pursue transformation models (and simpler variants that are easier to estimate) within this flexible finite-state framework. The interested reader is encouraged to look at (Eisner, 2001) for a much more careful and wide-ranging discussion of transformation models, their algorithms, and their relation to linguistic theory, statistics, and parsing. Chapte</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002b. Parameter estimation for probabilistic finite-state transducers. In Proceedings of the 40th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Gr¨unwald</author>
</authors>
<title>A minimum description length approach to grammar inference.</title>
<date>1996</date>
<booktitle>Symbolic, Connectionist and Statistical Approaches to Learning for NLP, no. 1040 in Lecture Notes in AI,</booktitle>
<pages>203--216</pages>
<editor>In S. Wermter et al., eds.,</editor>
<marker>Gr¨unwald, 1996</marker>
<rawString>P. Gr¨unwald. 1996. A minimum description length approach to grammar inference. In S. Wermter et al., eds., Symbolic, Connectionist and Statistical Approaches to Learning for NLP, no. 1040 in Lecture Notes in AI, pages 203–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="33406" citStr="Johnson (1998)" startWordPosition="5872" endWordPosition="5873">onal; dashed diagonals indicate a factor of two. The three log-log plots (at different scales!) partition the rules by the number of training observations: 0 (left graph), 1 (middle), ≥ 2 (right). This is an encouraging result for the use of the method in less supervised contexts (although results on a noisy dataset would be more convincing in this regard). • The competing models from the literature are best used to predict flat rules directly, rather than by summing over their possible non-flat internal structures, as has been done in the past. This result is significant in itself. Extending Johnson (1998), it shows the inappropriateness of the traditional independence assumptions that build up a frame by several rule expansions (section 2). Figure 2 shows that averaging the transformation model with the Treebank/bigram model improves the latter not merely on balance, but across the board. In other words, there is no evident class of phenomena for which incorporating transformations would be a bad idea. • Transformations particularly helped raise the estimates of the low-probability novel rules in test data, as hoped. • Transformations also helped on test rules that had been observed once in tr</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="28829" citStr="Marcus et al., 1993" startWordPosition="5123" endWordPosition="5126">e tokens 8.9% 6.3% novel headword tokens 10.4% 10.2% novel rule types 61.4% 57.5% novel frame types 24.6% 16.4% novel headword types 20.9% 18.8% nonterminaltypes 78 # transformations applicable to 158n-1 158n-1 158n-1 rule with RHS length = n Table 3: Properties of the experimental data. “Novel” means not observed in training. “Frame” was defined in section 4. 8 Evaluation12 To evaluate the quality of generalization, we used preparsed training data D and testing data E (Table 3). Each dataset consisted of a collection of flat rules such as Sput → NP put NP PP extracted from the Penn Treebank (Marcus et al., 1993). Thus, p(D |θ, π) and p(E |θ, π) were each defined as a product of rule probabilities of the form pθ,π(NP put NP PP |Sput). The learner attempted to maximize p(θ, π) · p(D | θ, π) by gradient ascent. This amounts to learning the generalizations and exceptions that related the training rules D. The evaluation measure was then the perplexity on test data, − log2 p(E |θ, π)/|E |. To get a good (low) perplexity score, the model had to assign reasonable probabilities to the many novel rules in E (Table 3). For many of these rules, even the frame was novel. Note that although the training data was </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
<author>Ted Briscoe</author>
</authors>
<title>Learning stochastic categorial grammars.</title>
<date>1997</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>80--87</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="8702" citStr="Osborne and Briscoe, 1997" startWordPosition="1467" endWordPosition="1470">stic subcategorization. 5In testing the latter case, we sum over all possible internal bracketings of the rule. We do train this case on the true internal bracketing, but it loses even with this unfair advantage. 6This approach is called semi-Bayesian or Maximum A PosPriors can help both unsupervised and supervised learning. (In the semi-supervised experiments here, training data is not raw text but a sparse sample of flat rules.) Indeed a good deal of syntax induction work has been carried out in just this framework (Stolcke and Omohundro, 1994; Chen, 1996; De Marcken, 1996; Gr¨unwald, 1996; Osborne and Briscoe, 1997). However, all such work to date has adopted rather simple prior distributions. Typically, it has defined p(θ) to favor PCFGs whose rules are few, short, nearly equiprobable, and defined over a small set of nonterminals. Such definitions are convenient, especially when specifying an encoding for MDL, but since they treat all rules alike, they may not be good descriptions of linguistic plausibility. For example, they will never penalize the absence of a predictable rule. A prior distribution can, however, be used to encode various kinds of linguistic notions. After all, a prior is really a soft</context>
</contexts>
<marker>Osborne, Briscoe, 1997</marker>
<rawString>Miles Osborne and Ted Briscoe. 1997. Learning stochastic categorial grammars. In Proceedings of CoNLL, 80–87. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>S M Omohundro</author>
</authors>
<title>Inducing probabilistic grammars by Bayesian model merging. In</title>
<date>1994</date>
<booktitle>Proc. ofICGI.</booktitle>
<contexts>
<context position="8627" citStr="Stolcke and Omohundro, 1994" startWordPosition="1455" endWordPosition="1459">ers have tried to learn verb subcategorization, though usually not probabilistic subcategorization. 5In testing the latter case, we sum over all possible internal bracketings of the rule. We do train this case on the true internal bracketing, but it loses even with this unfair advantage. 6This approach is called semi-Bayesian or Maximum A PosPriors can help both unsupervised and supervised learning. (In the semi-supervised experiments here, training data is not raw text but a sparse sample of flat rules.) Indeed a good deal of syntax induction work has been carried out in just this framework (Stolcke and Omohundro, 1994; Chen, 1996; De Marcken, 1996; Gr¨unwald, 1996; Osborne and Briscoe, 1997). However, all such work to date has adopted rather simple prior distributions. Typically, it has defined p(θ) to favor PCFGs whose rules are few, short, nearly equiprobable, and defined over a small set of nonterminals. Such definitions are convenient, especially when specifying an encoding for MDL, but since they treat all rules alike, they may not be good descriptions of linguistic plausibility. For example, they will never penalize the absence of a predictable rule. A prior distribution can, however, be used to enco</context>
</contexts>
<marker>Stolcke, Omohundro, 1994</marker>
<rawString>A. Stolcke and S.M. Omohundro. 1994. Inducing probabilistic grammars by Bayesian model merging. In Proc. ofICGI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>