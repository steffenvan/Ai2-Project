<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021274">
<note confidence="0.812733">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 142-144, Lisbon, Portugal, 2000.
</note>
<title confidence="0.987531">
Use of Support Vector Learning
for Chunk Identification
</title>
<author confidence="0.820824">
Taku Kudoh and Yuji Matsumoto
</author>
<affiliation confidence="0.801926">
Graduate School of Information Science, Nara Institute of Science and Technology
</affiliation>
<email confidence="0.873923">
ftaku-ku, matsuleas.aist-nara.ac.jp
</email>
<sectionHeader confidence="0.998951" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999891875">
In this paper, we explore the use of Support Vec-
tor Machines (SVMs) for CoNLL-2000 shared
task, chunk identification. SVMs are so-called
large margin classifiers and are well-known as
their good generalization performance. We in-
vestigate how SVMs with a very large number
of features perform with the classification task
of chunk labelling.
</bodyText>
<sectionHeader confidence="0.949567" genericHeader="method">
2 Support Vector Machines
</sectionHeader>
<bodyText confidence="0.9918734375">
Support Vector Machines (SVMs), first intro-
duced by Vapnik (Cortes and Vapnik, 1995;
Vapnik, 1995), are relatively new learning ap-
proaches for solving two-class pattern recog-
nition problems. SVMs are well-known for
their good generalization performance, and have
been applied to many pattern recognition prob-
lems. In the field of natural language process-
ing, SVMs are applied to text categorization,
and are reported to have achieved high accu-
racy without falling into over-fitting even with
a large number of words taken as the features
(Joachims, 1998; Taira and Haruno, 1999)
First of all, let us define the training data
which belongs to either positive or negative class
as follows:
</bodyText>
<equation confidence="0.475943">
(xi, yi), • • • , (x, y) x E Rn, yi E {+1, —1}
</equation>
<bodyText confidence="0.980117666666667">
xi is a feature vector of the i-th sample repre-
sented by an n dimensional vector. yi is the
class (positive(+1) or negative(-1) class) label
of the i-th data. In basic SVMs framework, we
try to separate the positive and negative exam-
ples by hyperplane written as:
(w-x)+b= 0 wElln,bE R.
SVMs find the &amp;quot;optimal&amp;quot; hyperplane (optimal
parameter w, b) which separates the training
</bodyText>
<subsectionHeader confidence="0.532633">
Small Margin Large Margin
</subsectionHeader>
<figureCaption confidence="0.995785">
Figure 1: Two possible separating hyperplanes
</figureCaption>
<bodyText confidence="0.999685466666667">
data into two classes precisely. What &amp;quot;opti-
mal&amp;quot; means? In order to define it, we need
to consider the margin between two classes.
Figures 1 illustrates this idea. The solid lines
show two possible hyperplanes, each of which
correctly separates the training data into two
classes. The two dashed lines parallel to the
separating hyperplane show the boundaries in
which one can move the separating hyperplane
without misclassification. We call the distance
between each parallel dashed lines as margin.
SVMs take a simple strategy that finds the sep-
arating hyperplane which maximizes its margin.
Precisely, two dashed lines and margin (d) can
be written as:
</bodyText>
<equation confidence="0.987778">
(w • x) + b = ±1, d = iiwii•
</equation>
<bodyText confidence="0.997766384615385">
SVMs can be regarded as an optimization prob-
lem; finding w and b which minimize liwil under
the constraints: yiRw xi) ± &gt; 1.
Furthermore, SVMs have potential to cope
with the linearly unseparable training data. We
leave the details to (Vapnik, 1995), the opti-
mization problems can be rewritten into a dual
form, where all feature vectors appear in their
dot product. By simply substituting every dot
product of xi and xi in dual form with any Ker-
nel function K(xi, xi), SVMs can handle non-
linear hypotheses. Among the many kinds of
Kernel functions available, we will focus on the
</bodyText>
<figure confidence="0.901441333333333">
0
o
o
</figure>
<page confidence="0.906008">
142
</page>
<bodyText confidence="0.9438638">
d-th polynomial kernel:
K(xi, xi) = (xi • xi + 1)d
Use of d-th polynomial kernel function allows
us to build an optimal separating hyperplane
which takes into account all combination of fea-
tures up to d.
We believe SVMs have advantage over con-
ventional statistical learning algorithms, such as
Decision Tree, and Maximum Entropy Models,
from the following two aspects:
</bodyText>
<listItem confidence="0.998975125">
• SVMs have high generalization perfor-
mance independent of dimension of fea-
ture vectors. Conventional algorithms re-
quire careful feature selection, which is usu-
ally optimized heuristically, to avoid over-
fitting.
• SVMs can carry out their learning with
all combinations of given features with-
out increasing computational complexity
by introducing the Kernel function. Con-
ventional algorithms cannot handle these
combinations efficiently, thus, we usually
select &amp;quot;important&amp;quot; combinations heuristi-
cally with taking the trade-off between ac-
curacy and computational complexity into
account.
</listItem>
<sectionHeader confidence="0.985548" genericHeader="method">
3 Approach for Chunk Identification
</sectionHeader>
<bodyText confidence="0.981000028571429">
The chunks in the CoNLL-2000 shared task are
represented with JOB based model, in which ev-
ery word is to be tagged with a chunk label ex-
tended with I (inside a chunk), 0 (outside a
chunk) and B (inside a chunk, but the preced-
ing word is in another chunk). Each chunk type
belongs to I or B tags. For example, NP could
be considered as two types of chunk, I-NP or
B-NP. In training data of CoNLL-2000 shared
task, we could find 22 types of chunk 1 consid-
ering all combinations of JOB-tags and chunk
types. We simply formulate the chunking task
as a classification problem of these 22 types of
chunk.
Basically, SVMs are binary classifiers, thus we
must extend SVMs to multi-class classifiers in
order to classify these 22 types of chunks. It is
&apos;Precisely, the number of combination becomes 23.
However, we do not consider I-LST tag since it dose not
appear in training data.
known that there are mainly two approaches to
extend from a binary classification task to those
with K classes. First approach is often used
and typical one &amp;quot;one class vs. all others&amp;quot;. The
idea is to build K classifiers that separate one
class among from all others. Second approach
is pairwise classification. The idea is to build
K x (K — 1)/2 classifiers considering all pairs of
classes, and final class decision is given by their
majority voting. We decided to construct pair-
wise classifiers for all the pairs of chunk labels,
so that the total number of classifiers becomes
2221
2 = 231. The reasons that we use pairwise
classifiers are as follows:
</bodyText>
<listItem confidence="0.9756665">
• Some experiments report that combination
of pairwise classifier perform better than K
classifier (Krael, 1999).
• The amount of training data for a pair is
less than the amount of training data for
separating one class with all others.
</listItem>
<bodyText confidence="0.983361678571429">
For the features, we decided to use all the in-
formation available in the surrounding contexts,
such as the words, their POS tags as well as the
chunk labels. More precisely, we give the fol-
lowing for the features to identify chunk label
ci at i-th word:
(j = i-2, i-1, i, i+1, i+ 2)
cj (j = i-2, i-1)
where wi is the word appearing at i-th word, ti
is the POS tag of wi, and ci is the (extended)
chunk label at i-th word. Since the chunk labels
are not given in the test data, they are decided
dynamically during the tagging of chunk labels.
This technique can be regarded as a sort of Dy-
namic Programming (DP) matching, in which
the best answer is searched by maximizing the
total certainty score for the combination of tags.
In using DP matching, we decided to keep not
all ambiguities but a limited number of them.
This means that a beam search is employed,
and only the top N candidates are kept for the
search for the best chunk tags. The algorithm
scans the test data from left to right and calls
the SVM classifiers for all pairs of chunk tags
for obtaining the certainty score. We defined
the certainty score as the number of votes for
the class (tag) obtained through the pairwise
voting.
</bodyText>
<page confidence="0.998014">
143
</page>
<bodyText confidence="0.999918347826087">
Since SVMs are vector based classifier, they
accept only numerical values for their features.
To cope with this constraints, we simply expand
all features as a binary-value taking either 0 or
1. By taking all words and POS tags appearing
in the training data as features, the total dimen-
sion of feature vector becomes as large as 92837.
Generally, we need vast computational complex-
ity and memories to handle such a huge dimen-
sion of vectors. In fact, we can reduce these
complexity considerably by holding only indices
and values of non-zero elements, since the fea-
ture vectors are usually sparse, and SVMs only
require the evaluation of dot products of each
feature vectors for their training.
In addition, although we could apply some
cut-off threshold for the number of occurrence
in the training set, we decided to use everything,
not only POS tags but also words themselves.
The reasons are that we simply do not want
to employ a kind of &amp;quot;heuristics&amp;quot;, and SVMs
are known to have a good generalization per-
formance even with very large features.
</bodyText>
<sectionHeader confidence="0.999049" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999958941176471">
We have applied our proposed method to the
test data of CoNLL-2000 shared task, while
training with the complete training data. For
the kernel function, we use the 2-nd polynomial
function. We set the beam width N to 5 ten-
tatively. SVMs training is carried out with the
Slight package, which is designed and opti-
mized to handle large sparse feature vector and
large numbers of training examples (Joachims,
2000; Joachims, 1999a). It took about 1 day
to train 231 classifiers with PC-Linux (Celeron
500Mhz, 512MB).
Figure 1 shows the results of our experiments.
The all the values of the chunking F-measure are
almost 93.5. Especially, our method performs
well for the chunk types of high frequency, such
as NP, VP and PP.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.997833571428571">
In this paper, we propose Chunk identification
analysis based on Support Vector Machines.
Although we select features for learning in
very straight way — using all available features
such as the words their POS tags without any
cut-off threshold for the number of occurrence,
we archive high performance for test data.
</bodyText>
<table confidence="0.999918916666667">
test data precision recall Fo=1
ADJP 79.22% 69.63% 74.12
ADVP 80.86% 80.48% 80.67
CONJP 62.50% 55.56% 58.82
INTJ 100.00% 50.00% 66.67
LST 0.00% 0.00% 0.00
NP 93.72% 94.02% 93.87
PP 96.60% 97.94% 97.26
PRT 80.58% 78.30% 79.43
SBAR 89.29% 84.11% 86.62
VP 93.76% 93.84% 93.80
all 93.45% 93.51% 93.48
</table>
<tableCaption confidence="0.993096">
Table 1: The results per chunk type with our
proposed SVMs based method
</tableCaption>
<bodyText confidence="0.999903071428571">
When we use other learning methods such as
Decision Tree, we have to select feature set man-
ually to avoid over-fitting. Usually, these fea-
ture selection depends on heuristics, so that it
is difficult to apply them to other classification
problems in other domains.
Memory based learning method can also han-
dle all available features. However, the function
to compute the distance between the test pat-
tern and the nearest cases in memory is usually
optimized in an ad-hoc way
Through our experiments, we have shown the
high generalization performance and high fea-
ture selection abilities of SVMs.
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99983885">
C. Cortes and Vladimir N. Vapnik. 1995. Support
Vector Networks. Machine Learning, 20:273-297.
Thorsten Joachims. 1998. Text Categorization with
Support Vector Machines: Learning with Many
Relevant Features. In European Conference on
Machine Learning (ECML).
Thorsten Joachims. 1999a. Making Large-Scale
Support Vector Machine Learning Practical. In
Advances in Kernel Methods. MIT Press.
Thorsten Joachims. 2000. SVMlight version
3.02. http://www-ai.cs.uni-dortmund.de/ SOFT-
WARE/SVM_LIGHT/svmlight.eng.html.
Ulrich H.-G Krefiel. 1999. Pairwise Classification
and Support Vector Machines. In Advances in
Kernel Methods. MIT Press.
Hirotoshi Taira and Masahiko Haruno. 1999. Fea-
ture Selection in SVM Text Categorization. In
AAAI-99.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.
</reference>
<page confidence="0.998596">
144
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.859869">
<note confidence="0.98552">of CoNLL-2000 and LLL-2000, 142-144, Lisbon, Portugal, 2000.</note>
<title confidence="0.9834745">Use of Support Vector for Chunk Identification</title>
<author confidence="0.942747">Kudoh Matsumoto</author>
<affiliation confidence="0.93263">Graduate School of Information Science, Nara Institute of Science and</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>Vladimir N Vapnik</author>
</authors>
<title>Support Vector Networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--273</pages>
<contexts>
<context position="751" citStr="Cortes and Vapnik, 1995" startWordPosition="105" endWordPosition="108">tification Taku Kudoh and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology ftaku-ku, matsuleas.aist-nara.ac.jp 1 Introduction In this paper, we explore the use of Support Vector Machines (SVMs) for CoNLL-2000 shared task, chunk identification. SVMs are so-called large margin classifiers and are well-known as their good generalization performance. We investigate how SVMs with a very large number of features perform with the classification task of chunk labelling. 2 Support Vector Machines Support Vector Machines (SVMs), first introduced by Vapnik (Cortes and Vapnik, 1995; Vapnik, 1995), are relatively new learning approaches for solving two-class pattern recognition problems. SVMs are well-known for their good generalization performance, and have been applied to many pattern recognition problems. In the field of natural language processing, SVMs are applied to text categorization, and are reported to have achieved high accuracy without falling into over-fitting even with a large number of words taken as the features (Joachims, 1998; Taira and Haruno, 1999) First of all, let us define the training data which belongs to either positive or negative class as foll</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and Vladimir N. Vapnik. 1995. Support Vector Networks. Machine Learning, 20:273-297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>In European Conference on Machine Learning (ECML).</booktitle>
<contexts>
<context position="1221" citStr="Joachims, 1998" startWordPosition="180" endWordPosition="181">ification task of chunk labelling. 2 Support Vector Machines Support Vector Machines (SVMs), first introduced by Vapnik (Cortes and Vapnik, 1995; Vapnik, 1995), are relatively new learning approaches for solving two-class pattern recognition problems. SVMs are well-known for their good generalization performance, and have been applied to many pattern recognition problems. In the field of natural language processing, SVMs are applied to text categorization, and are reported to have achieved high accuracy without falling into over-fitting even with a large number of words taken as the features (Joachims, 1998; Taira and Haruno, 1999) First of all, let us define the training data which belongs to either positive or negative class as follows: (xi, yi), • • • , (x, y) x E Rn, yi E {+1, —1} xi is a feature vector of the i-th sample represented by an n dimensional vector. yi is the class (positive(+1) or negative(-1) class) label of the i-th data. In basic SVMs framework, we try to separate the positive and negative examples by hyperplane written as: (w-x)+b= 0 wElln,bE R. SVMs find the &amp;quot;optimal&amp;quot; hyperplane (optimal parameter w, b) which separates the training Small Margin Large Margin Figure 1: Two po</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. In European Conference on Machine Learning (ECML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making Large-Scale Support Vector Machine Learning Practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8575" citStr="Joachims, 1999" startWordPosition="1454" endWordPosition="1455">mselves. The reasons are that we simply do not want to employ a kind of &amp;quot;heuristics&amp;quot;, and SVMs are known to have a good generalization performance even with very large features. 4 Results We have applied our proposed method to the test data of CoNLL-2000 shared task, while training with the complete training data. For the kernel function, we use the 2-nd polynomial function. We set the beam width N to 5 tentatively. SVMs training is carried out with the Slight package, which is designed and optimized to handle large sparse feature vector and large numbers of training examples (Joachims, 2000; Joachims, 1999a). It took about 1 day to train 231 classifiers with PC-Linux (Celeron 500Mhz, 512MB). Figure 1 shows the results of our experiments. The all the values of the chunking F-measure are almost 93.5. Especially, our method performs well for the chunk types of high frequency, such as NP, VP and PP. 5 Discussion In this paper, we propose Chunk identification analysis based on Support Vector Machines. Although we select features for learning in very straight way — using all available features such as the words their POS tags without any cut-off threshold for the number of occurrence, we archive high</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999a. Making Large-Scale Support Vector Machine Learning Practical. In Advances in Kernel Methods. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>SVMlight version 3.02.</title>
<date>2000</date>
<note>http://www-ai.cs.uni-dortmund.de/ SOFTWARE/SVM_LIGHT/svmlight.eng.html.</note>
<contexts>
<context position="8559" citStr="Joachims, 2000" startWordPosition="1452" endWordPosition="1453">t also words themselves. The reasons are that we simply do not want to employ a kind of &amp;quot;heuristics&amp;quot;, and SVMs are known to have a good generalization performance even with very large features. 4 Results We have applied our proposed method to the test data of CoNLL-2000 shared task, while training with the complete training data. For the kernel function, we use the 2-nd polynomial function. We set the beam width N to 5 tentatively. SVMs training is carried out with the Slight package, which is designed and optimized to handle large sparse feature vector and large numbers of training examples (Joachims, 2000; Joachims, 1999a). It took about 1 day to train 231 classifiers with PC-Linux (Celeron 500Mhz, 512MB). Figure 1 shows the results of our experiments. The all the values of the chunking F-measure are almost 93.5. Especially, our method performs well for the chunk types of high frequency, such as NP, VP and PP. 5 Discussion In this paper, we propose Chunk identification analysis based on Support Vector Machines. Although we select features for learning in very straight way — using all available features such as the words their POS tags without any cut-off threshold for the number of occurrence,</context>
</contexts>
<marker>Joachims, 2000</marker>
<rawString>Thorsten Joachims. 2000. SVMlight version 3.02. http://www-ai.cs.uni-dortmund.de/ SOFTWARE/SVM_LIGHT/svmlight.eng.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich H-G Krefiel</author>
</authors>
<title>Pairwise Classification and Support Vector Machines.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods.</booktitle>
<publisher>MIT Press.</publisher>
<marker>Krefiel, 1999</marker>
<rawString>Ulrich H.-G Krefiel. 1999. Pairwise Classification and Support Vector Machines. In Advances in Kernel Methods. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirotoshi Taira</author>
<author>Masahiko Haruno</author>
</authors>
<title>Feature Selection in SVM Text Categorization.</title>
<date>1999</date>
<booktitle>In AAAI-99.</booktitle>
<contexts>
<context position="1246" citStr="Taira and Haruno, 1999" startWordPosition="182" endWordPosition="185">f chunk labelling. 2 Support Vector Machines Support Vector Machines (SVMs), first introduced by Vapnik (Cortes and Vapnik, 1995; Vapnik, 1995), are relatively new learning approaches for solving two-class pattern recognition problems. SVMs are well-known for their good generalization performance, and have been applied to many pattern recognition problems. In the field of natural language processing, SVMs are applied to text categorization, and are reported to have achieved high accuracy without falling into over-fitting even with a large number of words taken as the features (Joachims, 1998; Taira and Haruno, 1999) First of all, let us define the training data which belongs to either positive or negative class as follows: (xi, yi), • • • , (x, y) x E Rn, yi E {+1, —1} xi is a feature vector of the i-th sample represented by an n dimensional vector. yi is the class (positive(+1) or negative(-1) class) label of the i-th data. In basic SVMs framework, we try to separate the positive and negative examples by hyperplane written as: (w-x)+b= 0 wElln,bE R. SVMs find the &amp;quot;optimal&amp;quot; hyperplane (optimal parameter w, b) which separates the training Small Margin Large Margin Figure 1: Two possible separating hyperpl</context>
</contexts>
<marker>Taira, Haruno, 1999</marker>
<rawString>Hirotoshi Taira and Masahiko Haruno. 1999. Feature Selection in SVM Text Categorization. In AAAI-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="751" citStr="Vapnik, 1995" startWordPosition="107" endWordPosition="108">Taku Kudoh and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology ftaku-ku, matsuleas.aist-nara.ac.jp 1 Introduction In this paper, we explore the use of Support Vector Machines (SVMs) for CoNLL-2000 shared task, chunk identification. SVMs are so-called large margin classifiers and are well-known as their good generalization performance. We investigate how SVMs with a very large number of features perform with the classification task of chunk labelling. 2 Support Vector Machines Support Vector Machines (SVMs), first introduced by Vapnik (Cortes and Vapnik, 1995; Vapnik, 1995), are relatively new learning approaches for solving two-class pattern recognition problems. SVMs are well-known for their good generalization performance, and have been applied to many pattern recognition problems. In the field of natural language processing, SVMs are applied to text categorization, and are reported to have achieved high accuracy without falling into over-fitting even with a large number of words taken as the features (Joachims, 1998; Taira and Haruno, 1999) First of all, let us define the training data which belongs to either positive or negative class as foll</context>
<context position="2785" citStr="Vapnik, 1995" startWordPosition="450" endWordPosition="451">rating hyperplane show the boundaries in which one can move the separating hyperplane without misclassification. We call the distance between each parallel dashed lines as margin. SVMs take a simple strategy that finds the separating hyperplane which maximizes its margin. Precisely, two dashed lines and margin (d) can be written as: (w • x) + b = ±1, d = iiwii• SVMs can be regarded as an optimization problem; finding w and b which minimize liwil under the constraints: yiRw xi) ± &gt; 1. Furthermore, SVMs have potential to cope with the linearly unseparable training data. We leave the details to (Vapnik, 1995), the optimization problems can be rewritten into a dual form, where all feature vectors appear in their dot product. By simply substituting every dot product of xi and xi in dual form with any Kernel function K(xi, xi), SVMs can handle nonlinear hypotheses. Among the many kinds of Kernel functions available, we will focus on the 0 o o 142 d-th polynomial kernel: K(xi, xi) = (xi • xi + 1)d Use of d-th polynomial kernel function allows us to build an optimal separating hyperplane which takes into account all combination of features up to d. We believe SVMs have advantage over conventional stati</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>