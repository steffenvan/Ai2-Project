<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.930858">
Grammars for Local and Long Dependencies.
</title>
<author confidence="0.979657">
Alexander Dikovsky
</author>
<affiliation confidence="0.953679">
Universit´e de Nantes, IRIN, 2, rue de la Houssini`ere
</affiliation>
<address confidence="0.907727">
BP 92208 F 44322 Nantes cedex 3 France
</address>
<email confidence="0.995526">
Alexandre.Dikovsky@irin.univ-nantes.fr
</email>
<sectionHeader confidence="0.994614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99507805882353">
Polarized dependency (PD-) grammars
are proposed as a means of efficient
treatment of discontinuous construc-
tions. PD-grammars describe two kinds
of dependencies : local, explicitly de-
rived by the rules, and long, implicitly
specified by negative and positive va-
lencies of words. If in a PD-grammar
the number of non-saturated valencies
in derived structures is bounded by a
constant, then it is weakly equivalent
to a cf-grammar and has a -
time parsing algorithm. It happens that
such bounded PD-grammars are strong
enough to express such phenomena as
unbounded raising, extraction and ex-
traposition.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999749479166666">
Syntactic theories based on the concept of depen-
dency have a long tradition. Tesni`ere (Tesni`ere,
1959) was the first who systematically described
the sentence structure in terms of binary relations
between words (dependencies), which form a de-
pendency tree (D-tree for short). D-tree itself
does not presume a linear order on words. How-
ever, any its surface realization projects some lin-
ear order relation (called also precedence). Some
properties of surface syntactic structure can be ex-
pressed only in terms of both dependency (or its
transitive closure called dominance) and prece-
dence. One of such properties, projectivity, re-
quires that any word occurring between a word
and a word dependent on be dominated by
In first dependency grammars (Gaifman, 1961)
and in some more recent proposals: link gram-
mars (Sleator and Temperly, 1993), projective
dependency grammars (Lombardo and Lesmo,
1996) the projectivity is implied by definition. In
some other theories, e.g. in word grammar (Hud-
son, 1984), it is used as one of the axioms defin-
ing acceptable surface structures. In presence
of this property, D-trees are in a sense equiva-
lent to phrase structures with head selection 1.
It is for this reason that D-trees determined by
grammars of Robinson (Robinson, 1970), cate-
gorial grammars (Bar-Hillel et al., 1960), classi-
cal Lambek calculus (Lambek, 1958), and some
other formalisms are projective. Projectivity af-
fects the complexity of parsing : as a rule, it al-
lows dynamic programming technics which lead
to polynomial time algorithms (cf. -time
algorithm for link grammars in (Sleator and Tem-
perly, 1993)). Meanwhile, the projectivity is not
the norm in natural languages. For example, in
most European languages there are such regu-
lar non-projective constructions as WH- or rel-
ative clause extraction, topicalization, compara-
tive constructions, and some constructions spe-
cific to a language, e.g. French pronominal cli-
tics or left dislocation. In terms of phrase struc-
ture, non-projectivity corresponds to discontinu-
ity. In this form it is in the center of dis-
cussions till 70-ies. There are various depen-
dency based approaches to this problem. In
the framework of Meaning-Text Theory (Mel’ˇcuk
and Pertsov, 1987), dependencies between (some-
</bodyText>
<footnote confidence="0.800733">
1See (Dikovsky and Modina, 2000) for more details.
</footnote>
<bodyText confidence="0.999678785714286">
times non adjacent) words are determined in
terms of their local neighborhood, which leads
to non-tractable parsing (the NP-hardness argu-
ment of (Neuhaus and Br¨oker, 1997) applies to
them). More recent versions of dependency gram-
mars (see e.g.(Kahane et al., 1998; Lombardo
and Lesmo, 1998; Br¨oker, 1998)) impose on non-
projective D-trees some constraints weaker than
projectivity (cf. meta-projectivity (Nasr, 1995) or
pseudo-projectivity (Kahane et al., 1998)), suffi-
cient for existence of a polynomial time parsing
algorithm. Still another approach is developed
in the context of intuitionistic resource-dependent
logics, where D-trees are constructed from deriva-
tions (cf. e.g. a method in (Lecomte, 1992) for
Lambek calculus). In this context, non-projective
D-trees are determined with the use of hypotheti-
cal reasoning and of structural rules such as com-
mutativity and associativity (see e.g. (Moortgat,
1990)).
In this paper, we put forward a novel ap-
proach to handling discontinuity in terms of de-
pendency structures. We propose a notion of a
polarized dependency (PD-) grammar combining
several ideas from cf-tree grammars, dependency
grammars and resource-dependent logics. As
most dependency grammars, the PD-grammars
are analyzing. They reduce continuous groups
to their types using local (context-free) reduction
rules and simultaneously assign partial depen-
dency structures to reduced groups. The valencies
(positive for governors and negative for subordi-
nates) are used to specify discontinuous (long) de-
pendencies lacking in partial dependency struc-
tures. The mechanism of establishing long de-
pendencies is orthogonal to reduction and is im-
plemented by a universal and simple rule of va-
lencies saturation. A simplified version of PD-
grammars adapted for the theoretical analysis is
introduced and explored in (Dikovsky, 2001). In
this paper, we describe a notion of PD-grammar
more adapted for practical tasks.
</bodyText>
<sectionHeader confidence="0.974261" genericHeader="method">
2 Dependency structures
</sectionHeader>
<bodyText confidence="0.898236538461539">
We fix finite alphabets of terminals (words),
of nonterminals (syntactic types or classes), and
of dependency names.
Definition 1. Let be a string. A
set of trees (called components
of ) which cover exactly have no nodes in
common, and whose arcs are labeled by names
in is a dependency (D-) structure on if one
component of is selected as its head 2. We
use the notation is a terminal D-
structure if is a string of terminals. When has
only one component, it is a dependency (D-) tree
on
</bodyText>
<figureCaption confidence="0.912925">
Fig. 1.
</figureCaption>
<bodyText confidence="0.969460863636364">
In distinction to (Dikovsky, 2001), the non-
terminals (and even dependency names) can be
structured. We follow (Mel’ˇcuk and Pertsov,
1987) and distinguish syntactical and mor-
phological features of a nonterminal
The alphabets being finite, the features unification
is a means of compacting a grammar.
The D-structures we will use will be polarized
in the sense that some words will have valencies
specifying long dependencies which must enter
or go from them. A valency is an expression of
one of the forms ,(a positive va-
lency), or ,(a negative valency),
being a dependency name. For example, the
intuitive sense of a positive valency of
a node is that a long dependency might go
from somewhere on the right. All nonterminals
will be signed: we presume that is decomposed
into two classes : of positive ( ) and negative
( ) nonterminals respectively. D-structures
with valencies, DV-structures, are defined so that
valencies saturation would imply connectivity.
</bodyText>
<construct confidence="0.5305875">
Definition 2. A terminal is polarized if a finite
list of pairwise different valencies 3 (its
</construct>
<footnote confidence="0.963883666666666">
valency list) is assigned to it. is positive, if
does not contain negative valencies, A
D-tree with polarized nodes is positive if its root
2We visualize underlining it or its root, when there are
some other components.
3In the original definition of (Dikovsky, 2001), valencies
</footnote>
<bodyText confidence="0.7781053125">
may repeat in but this seems to be a natural constraint.
For example, the D-structure in Fig. 1 has
two components. is the root of the non
projective head component, the other component
is a unit tree.
is positive, otherwise it is negative.
A D-structure on a string of polarized
symbols is a DV-structure on if the following
conditions are satisfied:
(v1) if a terminal node of is negative, then
contains exactly one negative valency,
(v2) if a dependency of enters a node then
is positive,
(v3) the non-head components of (if any)
are all negative.
The polarity of a DV-structure is that of its head.
</bodyText>
<figureCaption confidence="0.972469">
Fig. 2.
</figureCaption>
<bodyText confidence="0.9916561">
In Fig. 2 4, both words in have no valencies,
all nonterminals in and are positive (we
label only negative nonterminals), is positive
because its head component is a positive unit D-
tree, and are negative because their roots
are negative.
Valencies are saturated by long dependencies.
Definition 3. Let be a terminal DV-structure.
A triplet where are
nodes of and is a long dependency
</bodyText>
<footnote confidence="0.769473">
4For the reasons of space, in our examples we
</footnote>
<bodyText confidence="0.882977333333333">
are not accurate with morphological features. E.g.,
in the place of GrV(gov:upon) we should rather have
GrV(gov:upon) inf .
with the name directed from to (nota-
tion: ), if there are valencies
such that:
</bodyText>
<equation confidence="0.6949872">
( precedes ➤),
(v4) either
, and , or
(v5) , and
in
</equation>
<bodyText confidence="0.991194588235294">
Let be the structure resulting from by
adding the long dependency and replacing
by and
by We will say that
is a saturation of by and denote it by
Among all possible saturations of we will select
the following particular one:
Let be the first non saturated
positive valency in and be
the closest corresponding 5 non saturated neg-
ative valency in Then the long dependency
saturating by is first
available (FA) in The resulting saturation of
by is first available or FA-saturation (notation:
).
We transform the relations into partial
orders closing them by transitivity.
</bodyText>
<figureCaption confidence="0.875231">
Fig. 3.
</figureCaption>
<bodyText confidence="0.9943012">
Suppose that in Fig. 3, both occurrences of
in and the first occurrence of in have
and both occurrences of
in and the second occurrence of in have
Then
</bodyText>
<listItem confidence="0.787261">
5Corresponding means:
(c1) and if and
(c2) and if
</listItem>
<bodyText confidence="0.899090452830189">
.
We will say that saturates by long depen-
dency.
The set of valencies in is totally ordered by
the order of nodes and the orders in their valency
lists:
if
(o1) either and
(o2) or and
In (Dikovsky, 2001), we prove that
If is a terminal DV-structure and
then either has a cycle, or it is a DV-structure
(Lemma 1).
As it follows from Definition 3, each satura-
tion of a terminal DV-structure has the same set
of nodes and a strictly narrower set of valencies.
Therefore, any terminal DV-structure has maxi-
mal saturations with respect to the order relations
Very importantly, there is a single max-
imal FA-saturation of denoted E.g.,
in Fig. 3, is a D-tree.
In order to keep track of those valencies which
are not yet saturated we use the following notion
of integral valency.
Definition 4. Let be a terminal DV-structure.
The integral valency of is the list
sitions of the DV-structures in Fig. 2:
and
DV-structure ( )
ordered by the order of va- D-tree
lencies in If is a d-tree, we say that
this D-tree saturates and call saturable.
By this definition,
Saturability is easily expressed in terms of
integral valency (Lemma 2 in (Dikovsky, 2001)) :
Let be a terminal DV-structure. Then:
is a D-tree iff it is cycle-free and
has at most one saturating D-tree.
The semantics of PD-grammars will be defined
in terms of composition of DV-structures which
generalizes strings substitution.
Definition 5. Let be a DV-
structure, be a nonterminal node of one of its
components, and be a
DV-structure of the same polarity as and with
the head component Then the result of the
composition of into in is the DV-structure
in which is substituted for the
root of inherits all dependencies of in
and the head component is that of (changed
respectively if touched on by composition)6.
It is easy to see that DV-structure in Fig. 4
can be derived by the following series of compo-
</bodyText>
<footnote confidence="0.967073">
6This composition generalizes the substitution used in
TAGs (Joshi et al., 1975) ( needs not be a leaf) and is not
like the adjunction.
</footnote>
<figureCaption confidence="0.978937">
Fig. 4.
</figureCaption>
<bodyText confidence="0.983038875">
The DV-structures composition has natural
properties:
The result of a composition into a DV-
structure is a DV-structure of the same polarity
as (Lemma 3 in (Dikovsky, 2001)).
If then
for any terminal
(Lemma 4 in (Dikovsky, 2001)).
</bodyText>
<sectionHeader confidence="0.969762" genericHeader="method">
3 Polarized dependency grammars
</sectionHeader>
<bodyText confidence="0.9378995625">
Polarized dependency grammars determine DV-
structures in the bottom-up manner in the course
of reduction of phrases to their types, just as the
categorial grammars do. Each reduction step is
accompanied by DV-structures composition and
by subsequent FA-saturation. The yield of a suc-
cessful reduction is a D-tree. In this paper, we
describe a superclass of grammars in (Dikovsky,
2001) which are more realistic from the point of
view of real applications and have the same pars-
ing complexity.
Definition 6. A PD-grammar is a system
where are as de-
scribed above, is a set of axioms (which
are positive nonterminals), is a
ternary relation of lexical interpretation, being
</bodyText>
<figure confidence="0.997595818181818">
-R:prepos-obj
+L:prepos-obj
prepos-obj
dir-inf-obj
Nn
GrNn
GrV(gov:upon)
(Adj,wh)
GrNn
Cl/obj-upon
ClWh
</figure>
<figureCaption confidence="0.8558945">
Fig. 5.
the set of lists ofpairwise different valencies, and
</figureCaption>
<bodyText confidence="0.971234">
is a set of reduction rules. For simplicity,
we start with the strict reduction rules (the only
rules in (Dikovsky, 2001)) of the form
where and is a DV-structure over of
the same polarity as A (below we will extend the
strict rules by side effects). In the special case,
where the DV-structures in the rules are D-trees,
the PD-grammar is local7.
Intuitively, we can think of as of the com-
bined information available after the phase of
morphological analysis (i.e. dictionary informa-
tion and tags). So means that a
type and a valency list can be a priori as-
signed to the word
Semantics. 1. Let and
be the unit DV-structure with Then
is a reduction of the structure to its type
(notation ) and is the integral valency
of this reduction denoted by
2. Let be a reduction rule
with nonterminals’ occurrences in
and be some
reductions. Then is a reduction of
the structure
to its type (notation
as well as itself are subreductions of The
integral valency of via is
</bodyText>
<figure confidence="0.2926885">
A D-tree is
determined by if there is a reduction
</figure>
<figureCaption confidence="0.712145333333333">
7Local PD-grammars are strongly equivalent to depen-
dency tree grammars of (Dikovsky and Modina, 2000) which
are generating and not analyzing as here.
</figureCaption>
<bodyText confidence="0.922207590909091">
The DT-language determined by
is the set of all D-trees it determines.
is the language
determined by denotes the class of
languages determined by PD-grammars.
By way of illustration, let us consider the
PD-grammar with the lexical interpretation
containing triplets:
and the following reduction
rules whose left parts are shown in Fig. 2:
Then the D-tree in Fig. 4 is reducible in to
and its reduction is depicted in Fig. 5.
As we show in (Dikovsky, 2001), the weak
generative capacity of PD-grammars is stronger
than that of cf-grammars. For example, the PD-
grammar :
generates a non-cf language
D-tree in Fig. 3 is deter-
mined by on Its reduction combined
with the diagram of local and long dependencies
is presented in Fig. 6.
).
</bodyText>
<figureCaption confidence="0.787836">
Fig. 6.
</figureCaption>
<bodyText confidence="0.98287025">
The local PD-grammars are weakly equivalent
to cf-grammars, so they are weaker than general
PD-grammars. Meanwhile, what is really im-
portant concerning the dependency grammars, is
their strong generative capacity, i.e. the D-trees
they derive. From this point of view, the gram-
mars like above are too strong. Let us remark
that in the reduction in Fig. 6, the first saturation
becomes possible only after all positive valencies
emerge. This means that the integral valency of
subreductions increases with This seems to be
never the case in natural languages, where next
valencies arise only after the preceding ones are
saturated. This is why we restrict ourself to the
class of PD-grammars which have such a prop-
erty.
Definition 7. Let be a PD-grammar. For a
reduction of a terminal structure, its defect is
defined as is a subre-
duction of has bounded (unbounded) de-
fect if there is some (there is no) constant which
bounds the defect of all its reductions. The mini-
mal constant having this property (if any) is the
defect of (denoted ).
There is a certain technical problem concerning
PD-grammars. Even if in a reduction to an axiom
all valencies are saturated, this does not guaran-
tee that a D-tree is derived: the graph may have
cycles. In (Dikovsky, 2001) we give a sufficient
condition for a PD-grammar of never producing
cycles while FA-saturation. We call the grammars
satisfying this condition lc- (locally cycle-) free.
For the space reasons, we don’t cite its defini-
tion, the more so that the linguistic PD-grammars
should certainly be lc-free. In (Dikovsky, 2001)
we prove the following theorem.
</bodyText>
<construct confidence="0.5846495">
Theorem 1. For any lc-free PD-grammar of
bounded defect there is an equivalent cf-grammar.
</construct>
<bodyText confidence="0.9996022">
Together with this we show an example of a
DT-language which cannot be determined by lo-
cal PD-grammars. This means that not all struc-
tures determined in terms of long dependencies
can be determined without them.
</bodyText>
<sectionHeader confidence="0.86067" genericHeader="method">
4 Side effect rules and parsing
</sectionHeader>
<bodyText confidence="0.9996831875">
An important consequence of Theorem 1 is
that lc-free bounded defect PD-grammars have a
parsing algorithm. In fact, it is the clas-
sical Earley algorithm in charter form (the char-
ters being DV-structures). To apply this algo-
rithm in practice, we should analyze the asymp-
totic factor which depends on the size of the
grammar. The idea of theorem 1 is that the in-
tegral valency being bounded, it can be com-
piled into types. This means that a reduction
rule should be substituted by rules
with
types keeping all possible integral valencies not
causing cycles. Theoretically, this might blow
up times the size of a grammar with de-
fect valencies and the maximal length
of left parts of rules. So theoretically, the con-
stant factor in the time bound is great. In
practice, it shouldn’t be as awful, because in lin-
guistic grammars will certainly equal one
rule will mostly treat one valency (i.e. )
and the majority of rules will be local. Practi-
cally, the effect may be that some local rules will
have variants propagating upwards a certain va-
lency: The actual prob-
lem lies elsewhere. Let us analyze the illustration
grammar and the reduction in Fig. 5. This
reduction is successful due to the fact that the
negative valency is assigned to
the preposition and the corresponding pos-
itive valency is assigned to the
verb What might serve the formal basis for
these assignments? Let us start with This
verb has the strong government over prepositions
In the clause in Fig. 4, the group of
the preposition is moved, which is of course a
sufficient condition for assigning the positive va-
lency to the verb. But this condition is not avail-
able in the dictionary, nor even through morpho-
logical analysis ( may occur at a certain dis-
tance from the end of the clause). So it can only
be derived in the course of reduction, but strict
PD-grammars have no rules assigning valencies.
Theoretically, there is no problem: we should
just introduce into the dictionary both variants of
the verb description – with the local dependency
to the right and with the positive va-
lency to the left. Practically,
this “solution” is inacceptable because such a lex-
ical ambiguity will lead to a brute force search.
The same argument shows that we shouldn’t as-
sign the negative valency to
in the dictionary, but rather “calculate” it in
the reduction. If we compare the clause in Fig. 4
with the clauses what theories we may rely upon;
what kind of theories we may rely upon; the de-
pendency theories of what kind we may rely upon
etc., we see that we can assign a valency to
wh-words in the dictionary and then raise nega-
tive valencies till the FA-saturation. The problem
is that in the strict PD-grammars there are no rules
of valency raising. For these reasons we extend
the reduction rules by side effects sufficient for
the calculations of both kinds.
</bodyText>
<subsubsectionHeader confidence="0.398865">
Definition 8. We introduce two kinds of side
</subsubsectionHeader>
<bodyText confidence="0.918747833333333">
effects: valency raising and
valency assignment being
valency names and an integer. A rule of the
form
with nonterminals in and
is valency raising i f:
</bodyText>
<listItem confidence="0.9716162">
(r1) are of the same polarity,
(r2) a local dependency enters in ,
(r3) for positive is a strict
reduction rule,
(r4) if are negative, then
</listItem>
<bodyText confidence="0.737757">
and replacing by any positive nonterminal we
obtain a DV-structure 8. A rule of the form
with nonterminals in and
is valency assigning i f:
(a1) for a positive is a strict
</bodyText>
<footnote confidence="0.709432">
8So this occurrence of in contradicts to the point
(v2) of definition 2.
</footnote>
<bodyText confidence="0.994060555555556">
is a non
head component of 9 and replacing by any
negative nonterminal we obtain a DV-structure.
Semantics. We change the reduction semantics
as follows.
For a raising rule
the result of the reduction is the DV-structure
where is the DV-structure
resulting from by deleting from
and is the DV-structure resulting from
by adding to
For a valency assignment rule
the result of the reduc-
A PD-grammar with side effect rules is a
PDSE-grammar.
This definition is correct in the sense that the
result of a reduction with side effects is always a
DV-structure. We can prove
</bodyText>
<construct confidence="0.5543245">
Theorem 2. For any lc-free PDSE-grammar of
bounded defect there is an equivalent cf-grammar.
</construct>
<bodyText confidence="0.998248181818182">
Moreover, the bounded defect PDSE-
grammars are also parsed in time
In fact, we can drop negative in raising
rules (it is unique) and indicate the type of
in both side effect rules, because the
composition we use makes this information
local. Now, we can revise the grammar
above, e.g. excluding the dictionary assignment
and using
in its place several valency raising rules such as:
where -R:prepos-obj
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9955655">
The main ideas underlying our approach to dis-
continuity are the following:
</bodyText>
<footnote confidence="0.723716">
9So this occurrence of in contradicts to the point
(v3) of definition 2.
</footnote>
<figure confidence="0.7881485">
tion is the DV-structure
reduction rule,
(a2) if is negative and is the root of
then and
(a3) if is negative and is not the root
of then
</figure>
<bodyText confidence="0.998380130434782">
Continuous (local, even if non projective)
dependencies are treated in terms of trees com-
position (which reminds TAGs). E.g., the French
pronominal clitics can be treated in this way.
Discontinuous (long) dependencies are cap-
tured in terms of FA-saturation of valencies in
the course of bottom-up reduction of dependency
groups to their types. As compared with the
SLASH of GPSG or the regular expression lifting
control in non projective dependency grammars,
these means turn out to be more efficient under
the conjecture of bounded defect. This conjec-
ture seems to be true for natural languages (the
contrary would mean the possibility of unlimited
extraction from extracted groups).
The valency raising and assignment rules of-
fer a way of deriving a proper valency saturation
without unwarranted increase of lexical ambigu-
ity.
A theoretical analysis and experiments in En-
glish syntax description show that the proposed
grammars may serve for practical tasks and can
be implemented by an efficient parser.
</bodyText>
<sectionHeader confidence="0.999569" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998316">
I would like to express my heartfelt gratitude to
N. Pertsov for fruitful discussions of this paper.
The idea of valency raising has emerged from our
joint work over a project of a PD-grammar for a
fragment of English.
</bodyText>
<sectionHeader confidence="0.999269" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999918888888889">
Y. Bar-Hillel, H. Gaifman, and E. Shamir. 1960. On
categorial and phrase structure grammars. Bull.
Res. Council Israel, 9F:1–16.
N. Br¨oker. 1998. Separating surface order and syn-
tactic relations in a dependency grammar. In Proc.
COLING-ACL, pages 174–180, Montreal.
A.Ja. Dikovsky and L.S. Modina. 2000. Dependen-
cies on the other side of the Curtain. Traitement
Automatique des Langues (TAL), 41(1):79–111.
A. Dikovsky. 2001. Polarized non-projective depen-
dency grammars. In Ph. De Groote and G. Morrill,
editors, Logical Aspects of Computational Linguis-
tics, number 2099 in LNAI. Springer Verlag. To be
published.
H. Gaifman. 1961. Dependency systems and phrase
structure systems. Report p-2315, RAND Corp.
Santa Monica (CA). Published in: Information and
Control, 1965, v. 8, n 3, pp. 304-337.
R.A. Hudson. 1984. Word Grammar. Basil Black-
well, Oxford-New York.
A.K. Joshi, L.S. Levy, and M. Takahashi. 1975. Tree
adjunct grammars. Journ. of Comput. and Syst.
Sci., 10( 1):136–163.
S. Kahane, A. Nasr, and O. Rambow. 1998.
Pseudo-projectivity : A polynomially parsable
non-projective dependency grammar. In Proc.
COLING-ACL, pages 646–652, Montreal.
J. Lambek. 1958. The mathematics of sentence struc-
ture. American Mathematical Monthly, pages 154–
170.
A. Lecomte. 1992. Proof nets and dependencies. In
Proc. of COLING-92, pages 394–401, Nantes.
V. Lombardo and L. Lesmo. 1996. An earley-type
recognizer for dependency grammar. In Proc. 16th
COLING, pages 723–728.
V. Lombardo and L. Lesmo. 1998. Formal aspects
and parsing issues of dependency theory. In Proc.
COLING-ACL, pages 787–793, Montreal.
I. Mel’ˇcuk and N.V. Pertsov. 1987. Surface Syntax of
English. A Formal Model Within the Meaning-Text
Framework. John Benjamins Publishing Company,
Amsterdam/Philadelphia.
M. Moortgat. 1990. La grammaire cat´egorielle
g´en´eralis´ee : le calcul de lambek-gentzen. In Ph.
Miller and Th. Torris, editors, Structure of lan-
guages and its mathematical aspects, pages 127–
182. Hermes, Paris.
A. Nasr. 1995. A formalism and a parser for lexical-
ized dependency grammars. In Proc. Int. Workshop
on Parsing Technology, pages 186–195, Prague.
P. Neuhaus and N. Br¨oker. 1997. The Complexity
of Recognition of Linguistically Adequate Depen-
dency Grammars. In Proc. of 35th ACL Annual
Meeting and 8th Conf. of the ECACL, pages 337–
343.
Jane J. Robinson. 1970. Dependency structures and
transformational rules. Language, 46( 2):259–
285.
D. D. Sleator and D. Temperly. 1993. Parsing English
with a Link Grammar. In Proc. IWPT’93, pages
277–291.
L. Tesni`ere. 1959. ´El´ements de syntaxe structurale.
Librairie C. Klincksieck, Paris.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.222266">
<title confidence="0.868986">Grammars for Local and Long Dependencies.</title>
<author confidence="0.829902">Alexander Dikovsky</author>
<affiliation confidence="0.367823">Universit´e de Nantes, IRIN, 2, rue de la Houssini`ere</affiliation>
<address confidence="0.415189">BP 92208 F 44322 Nantes cedex 3 France</address>
<email confidence="0.626348">Alexandre.Dikovsky@irin.univ-nantes.fr</email>
<abstract confidence="0.988449166666667">Polarized dependency (PD-) grammars are proposed as a means of efficient treatment of discontinuous constructions. PD-grammars describe two kinds dependencies : explicitly deby the rules, and implicitly specified by negative and positive valencies of words. If in a PD-grammar the number of non-saturated valencies in derived structures is bounded by a constant, then it is weakly equivalent to a cf-grammar and has a time parsing algorithm. It happens that such bounded PD-grammars are strong enough to express such phenomena as unbounded raising, extraction and extraposition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bar-Hillel</author>
<author>H Gaifman</author>
<author>E Shamir</author>
</authors>
<title>On categorial and phrase structure grammars.</title>
<date>1960</date>
<journal>Bull. Res. Council</journal>
<contexts>
<context position="2138" citStr="Bar-Hillel et al., 1960" startWordPosition="329" endWordPosition="332">ndent on be dominated by In first dependency grammars (Gaifman, 1961) and in some more recent proposals: link grammars (Sleator and Temperly, 1993), projective dependency grammars (Lombardo and Lesmo, 1996) the projectivity is implied by definition. In some other theories, e.g. in word grammar (Hudson, 1984), it is used as one of the axioms defining acceptable surface structures. In presence of this property, D-trees are in a sense equivalent to phrase structures with head selection 1. It is for this reason that D-trees determined by grammars of Robinson (Robinson, 1970), categorial grammars (Bar-Hillel et al., 1960), classical Lambek calculus (Lambek, 1958), and some other formalisms are projective. Projectivity affects the complexity of parsing : as a rule, it allows dynamic programming technics which lead to polynomial time algorithms (cf. -time algorithm for link grammars in (Sleator and Temperly, 1993)). Meanwhile, the projectivity is not the norm in natural languages. For example, in most European languages there are such regular non-projective constructions as WH- or relative clause extraction, topicalization, comparative constructions, and some constructions specific to a language, e.g. French pro</context>
</contexts>
<marker>Bar-Hillel, Gaifman, Shamir, 1960</marker>
<rawString>Y. Bar-Hillel, H. Gaifman, and E. Shamir. 1960. On categorial and phrase structure grammars. Bull. Res. Council Israel, 9F:1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Br¨oker</author>
</authors>
<title>Separating surface order and syntactic relations in a dependency grammar.</title>
<date>1998</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<pages>174--180</pages>
<location>Montreal.</location>
<marker>Br¨oker, 1998</marker>
<rawString>N. Br¨oker. 1998. Separating surface order and syntactic relations in a dependency grammar. In Proc. COLING-ACL, pages 174–180, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dikovsky</author>
<author>L S Modina</author>
</authors>
<date>2000</date>
<booktitle>Dependencies on the other side of the Curtain. Traitement Automatique des Langues (TAL),</booktitle>
<pages>41--1</pages>
<contexts>
<context position="3104" citStr="Dikovsky and Modina, 2000" startWordPosition="478" endWordPosition="481">rm in natural languages. For example, in most European languages there are such regular non-projective constructions as WH- or relative clause extraction, topicalization, comparative constructions, and some constructions specific to a language, e.g. French pronominal clitics or left dislocation. In terms of phrase structure, non-projectivity corresponds to discontinuity. In this form it is in the center of discussions till 70-ies. There are various dependency based approaches to this problem. In the framework of Meaning-Text Theory (Mel’ˇcuk and Pertsov, 1987), dependencies between (some1See (Dikovsky and Modina, 2000) for more details. times non adjacent) words are determined in terms of their local neighborhood, which leads to non-tractable parsing (the NP-hardness argument of (Neuhaus and Br¨oker, 1997) applies to them). More recent versions of dependency grammars (see e.g.(Kahane et al., 1998; Lombardo and Lesmo, 1998; Br¨oker, 1998)) impose on nonprojective D-trees some constraints weaker than projectivity (cf. meta-projectivity (Nasr, 1995) or pseudo-projectivity (Kahane et al., 1998)), sufficient for existence of a polynomial time parsing algorithm. Still another approach is developed in the context </context>
<context position="13191" citStr="Dikovsky and Modina, 2000" startWordPosition="2168" endWordPosition="2171">on and tags). So means that a type and a valency list can be a priori assigned to the word Semantics. 1. Let and be the unit DV-structure with Then is a reduction of the structure to its type (notation ) and is the integral valency of this reduction denoted by 2. Let be a reduction rule with nonterminals’ occurrences in and be some reductions. Then is a reduction of the structure to its type (notation as well as itself are subreductions of The integral valency of via is A D-tree is determined by if there is a reduction 7Local PD-grammars are strongly equivalent to dependency tree grammars of (Dikovsky and Modina, 2000) which are generating and not analyzing as here. The DT-language determined by is the set of all D-trees it determines. is the language determined by denotes the class of languages determined by PD-grammars. By way of illustration, let us consider the PD-grammar with the lexical interpretation containing triplets: and the following reduction rules whose left parts are shown in Fig. 2: Then the D-tree in Fig. 4 is reducible in to and its reduction is depicted in Fig. 5. As we show in (Dikovsky, 2001), the weak generative capacity of PD-grammars is stronger than that of cf-grammars. For example,</context>
</contexts>
<marker>Dikovsky, Modina, 2000</marker>
<rawString>A.Ja. Dikovsky and L.S. Modina. 2000. Dependencies on the other side of the Curtain. Traitement Automatique des Langues (TAL), 41(1):79–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dikovsky</author>
</authors>
<title>Polarized non-projective dependency grammars.</title>
<date>2001</date>
<booktitle>Logical Aspects of Computational Linguistics, number 2099 in LNAI.</booktitle>
<editor>In Ph. De Groote and G. Morrill, editors,</editor>
<publisher>Springer Verlag.</publisher>
<note>To be published.</note>
<contexts>
<context position="4965" citStr="Dikovsky, 2001" startWordPosition="754" endWordPosition="755">ars are analyzing. They reduce continuous groups to their types using local (context-free) reduction rules and simultaneously assign partial dependency structures to reduced groups. The valencies (positive for governors and negative for subordinates) are used to specify discontinuous (long) dependencies lacking in partial dependency structures. The mechanism of establishing long dependencies is orthogonal to reduction and is implemented by a universal and simple rule of valencies saturation. A simplified version of PDgrammars adapted for the theoretical analysis is introduced and explored in (Dikovsky, 2001). In this paper, we describe a notion of PD-grammar more adapted for practical tasks. 2 Dependency structures We fix finite alphabets of terminals (words), of nonterminals (syntactic types or classes), and of dependency names. Definition 1. Let be a string. A set of trees (called components of ) which cover exactly have no nodes in common, and whose arcs are labeled by names in is a dependency (D-) structure on if one component of is selected as its head 2. We use the notation is a terminal Dstructure if is a string of terminals. When has only one component, it is a dependency (D-) tree on Fig</context>
<context position="6893" citStr="Dikovsky, 2001" startWordPosition="1078" endWordPosition="1079"> All nonterminals will be signed: we presume that is decomposed into two classes : of positive ( ) and negative ( ) nonterminals respectively. D-structures with valencies, DV-structures, are defined so that valencies saturation would imply connectivity. Definition 2. A terminal is polarized if a finite list of pairwise different valencies 3 (its valency list) is assigned to it. is positive, if does not contain negative valencies, A D-tree with polarized nodes is positive if its root 2We visualize underlining it or its root, when there are some other components. 3In the original definition of (Dikovsky, 2001), valencies may repeat in but this seems to be a natural constraint. For example, the D-structure in Fig. 1 has two components. is the root of the non projective head component, the other component is a unit tree. is positive, otherwise it is negative. A D-structure on a string of polarized symbols is a DV-structure on if the following conditions are satisfied: (v1) if a terminal node of is negative, then contains exactly one negative valency, (v2) if a dependency of enters a node then is positive, (v3) the non-head components of (if any) are all negative. The polarity of a DV-structure is tha</context>
<context position="9183" citStr="Dikovsky, 2001" startWordPosition="1484" endWordPosition="1485">cy saturating by is first available (FA) in The resulting saturation of by is first available or FA-saturation (notation: ). We transform the relations into partial orders closing them by transitivity. Fig. 3. Suppose that in Fig. 3, both occurrences of in and the first occurrence of in have and both occurrences of in and the second occurrence of in have Then 5Corresponding means: (c1) and if and (c2) and if . We will say that saturates by long dependency. The set of valencies in is totally ordered by the order of nodes and the orders in their valency lists: if (o1) either and (o2) or and In (Dikovsky, 2001), we prove that If is a terminal DV-structure and then either has a cycle, or it is a DV-structure (Lemma 1). As it follows from Definition 3, each saturation of a terminal DV-structure has the same set of nodes and a strictly narrower set of valencies. Therefore, any terminal DV-structure has maximal saturations with respect to the order relations Very importantly, there is a single maximal FA-saturation of denoted E.g., in Fig. 3, is a D-tree. In order to keep track of those valencies which are not yet saturated we use the following notion of integral valency. Definition 4. Let be a terminal</context>
<context position="11153" citStr="Dikovsky, 2001" startWordPosition="1828" endWordPosition="1829">the composition of into in is the DV-structure in which is substituted for the root of inherits all dependencies of in and the head component is that of (changed respectively if touched on by composition)6. It is easy to see that DV-structure in Fig. 4 can be derived by the following series of compo6This composition generalizes the substitution used in TAGs (Joshi et al., 1975) ( needs not be a leaf) and is not like the adjunction. Fig. 4. The DV-structures composition has natural properties: The result of a composition into a DVstructure is a DV-structure of the same polarity as (Lemma 3 in (Dikovsky, 2001)). If then for any terminal (Lemma 4 in (Dikovsky, 2001)). 3 Polarized dependency grammars Polarized dependency grammars determine DVstructures in the bottom-up manner in the course of reduction of phrases to their types, just as the categorial grammars do. Each reduction step is accompanied by DV-structures composition and by subsequent FA-saturation. The yield of a successful reduction is a D-tree. In this paper, we describe a superclass of grammars in (Dikovsky, 2001) which are more realistic from the point of view of real applications and have the same parsing complexity. Definition 6. A P</context>
<context position="13695" citStr="Dikovsky, 2001" startWordPosition="2255" endWordPosition="2256">reduction 7Local PD-grammars are strongly equivalent to dependency tree grammars of (Dikovsky and Modina, 2000) which are generating and not analyzing as here. The DT-language determined by is the set of all D-trees it determines. is the language determined by denotes the class of languages determined by PD-grammars. By way of illustration, let us consider the PD-grammar with the lexical interpretation containing triplets: and the following reduction rules whose left parts are shown in Fig. 2: Then the D-tree in Fig. 4 is reducible in to and its reduction is depicted in Fig. 5. As we show in (Dikovsky, 2001), the weak generative capacity of PD-grammars is stronger than that of cf-grammars. For example, the PDgrammar : generates a non-cf language D-tree in Fig. 3 is determined by on Its reduction combined with the diagram of local and long dependencies is presented in Fig. 6. ). Fig. 6. The local PD-grammars are weakly equivalent to cf-grammars, so they are weaker than general PD-grammars. Meanwhile, what is really important concerning the dependency grammars, is their strong generative capacity, i.e. the D-trees they derive. From this point of view, the grammars like above are too strong. Let us </context>
<context position="15239" citStr="Dikovsky, 2001" startWordPosition="2519" endWordPosition="2520"> restrict ourself to the class of PD-grammars which have such a property. Definition 7. Let be a PD-grammar. For a reduction of a terminal structure, its defect is defined as is a subreduction of has bounded (unbounded) defect if there is some (there is no) constant which bounds the defect of all its reductions. The minimal constant having this property (if any) is the defect of (denoted ). There is a certain technical problem concerning PD-grammars. Even if in a reduction to an axiom all valencies are saturated, this does not guarantee that a D-tree is derived: the graph may have cycles. In (Dikovsky, 2001) we give a sufficient condition for a PD-grammar of never producing cycles while FA-saturation. We call the grammars satisfying this condition lc- (locally cycle-) free. For the space reasons, we don’t cite its definition, the more so that the linguistic PD-grammars should certainly be lc-free. In (Dikovsky, 2001) we prove the following theorem. Theorem 1. For any lc-free PD-grammar of bounded defect there is an equivalent cf-grammar. Together with this we show an example of a DT-language which cannot be determined by local PD-grammars. This means that not all structures determined in terms of</context>
</contexts>
<marker>Dikovsky, 2001</marker>
<rawString>A. Dikovsky. 2001. Polarized non-projective dependency grammars. In Ph. De Groote and G. Morrill, editors, Logical Aspects of Computational Linguistics, number 2099 in LNAI. Springer Verlag. To be published.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gaifman</author>
</authors>
<title>Dependency systems and phrase structure systems. Report p-2315, RAND Corp. Santa Monica (CA). Published in: Information and Control,</title>
<date>1961</date>
<volume>8</volume>
<pages>304--337</pages>
<contexts>
<context position="1583" citStr="Gaifman, 1961" startWordPosition="241" endWordPosition="242">the sentence structure in terms of binary relations between words (dependencies), which form a dependency tree (D-tree for short). D-tree itself does not presume a linear order on words. However, any its surface realization projects some linear order relation (called also precedence). Some properties of surface syntactic structure can be expressed only in terms of both dependency (or its transitive closure called dominance) and precedence. One of such properties, projectivity, requires that any word occurring between a word and a word dependent on be dominated by In first dependency grammars (Gaifman, 1961) and in some more recent proposals: link grammars (Sleator and Temperly, 1993), projective dependency grammars (Lombardo and Lesmo, 1996) the projectivity is implied by definition. In some other theories, e.g. in word grammar (Hudson, 1984), it is used as one of the axioms defining acceptable surface structures. In presence of this property, D-trees are in a sense equivalent to phrase structures with head selection 1. It is for this reason that D-trees determined by grammars of Robinson (Robinson, 1970), categorial grammars (Bar-Hillel et al., 1960), classical Lambek calculus (Lambek, 1958), a</context>
</contexts>
<marker>Gaifman, 1961</marker>
<rawString>H. Gaifman. 1961. Dependency systems and phrase structure systems. Report p-2315, RAND Corp. Santa Monica (CA). Published in: Information and Control, 1965, v. 8, n 3, pp. 304-337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Hudson</author>
</authors>
<title>Word Grammar. Basil Blackwell,</title>
<date>1984</date>
<location>Oxford-New York.</location>
<contexts>
<context position="1823" citStr="Hudson, 1984" startWordPosition="277" endWordPosition="279">ear order relation (called also precedence). Some properties of surface syntactic structure can be expressed only in terms of both dependency (or its transitive closure called dominance) and precedence. One of such properties, projectivity, requires that any word occurring between a word and a word dependent on be dominated by In first dependency grammars (Gaifman, 1961) and in some more recent proposals: link grammars (Sleator and Temperly, 1993), projective dependency grammars (Lombardo and Lesmo, 1996) the projectivity is implied by definition. In some other theories, e.g. in word grammar (Hudson, 1984), it is used as one of the axioms defining acceptable surface structures. In presence of this property, D-trees are in a sense equivalent to phrase structures with head selection 1. It is for this reason that D-trees determined by grammars of Robinson (Robinson, 1970), categorial grammars (Bar-Hillel et al., 1960), classical Lambek calculus (Lambek, 1958), and some other formalisms are projective. Projectivity affects the complexity of parsing : as a rule, it allows dynamic programming technics which lead to polynomial time algorithms (cf. -time algorithm for link grammars in (Sleator and Temp</context>
</contexts>
<marker>Hudson, 1984</marker>
<rawString>R.A. Hudson. 1984. Word Grammar. Basil Blackwell, Oxford-New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journ. of Comput. and Syst. Sci.,</journal>
<volume>10</volume>
<pages>1--136</pages>
<contexts>
<context position="10918" citStr="Joshi et al., 1975" startWordPosition="1784" endWordPosition="1787">tion of DV-structures which generalizes strings substitution. Definition 5. Let be a DVstructure, be a nonterminal node of one of its components, and be a DV-structure of the same polarity as and with the head component Then the result of the composition of into in is the DV-structure in which is substituted for the root of inherits all dependencies of in and the head component is that of (changed respectively if touched on by composition)6. It is easy to see that DV-structure in Fig. 4 can be derived by the following series of compo6This composition generalizes the substitution used in TAGs (Joshi et al., 1975) ( needs not be a leaf) and is not like the adjunction. Fig. 4. The DV-structures composition has natural properties: The result of a composition into a DVstructure is a DV-structure of the same polarity as (Lemma 3 in (Dikovsky, 2001)). If then for any terminal (Lemma 4 in (Dikovsky, 2001)). 3 Polarized dependency grammars Polarized dependency grammars determine DVstructures in the bottom-up manner in the course of reduction of phrases to their types, just as the categorial grammars do. Each reduction step is accompanied by DV-structures composition and by subsequent FA-saturation. The yield </context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>A.K. Joshi, L.S. Levy, and M. Takahashi. 1975. Tree adjunct grammars. Journ. of Comput. and Syst. Sci., 10( 1):136–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kahane</author>
<author>A Nasr</author>
<author>O Rambow</author>
</authors>
<title>Pseudo-projectivity : A polynomially parsable non-projective dependency grammar.</title>
<date>1998</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<pages>646--652</pages>
<location>Montreal.</location>
<contexts>
<context position="3387" citStr="Kahane et al., 1998" startWordPosition="522" endWordPosition="525">ation. In terms of phrase structure, non-projectivity corresponds to discontinuity. In this form it is in the center of discussions till 70-ies. There are various dependency based approaches to this problem. In the framework of Meaning-Text Theory (Mel’ˇcuk and Pertsov, 1987), dependencies between (some1See (Dikovsky and Modina, 2000) for more details. times non adjacent) words are determined in terms of their local neighborhood, which leads to non-tractable parsing (the NP-hardness argument of (Neuhaus and Br¨oker, 1997) applies to them). More recent versions of dependency grammars (see e.g.(Kahane et al., 1998; Lombardo and Lesmo, 1998; Br¨oker, 1998)) impose on nonprojective D-trees some constraints weaker than projectivity (cf. meta-projectivity (Nasr, 1995) or pseudo-projectivity (Kahane et al., 1998)), sufficient for existence of a polynomial time parsing algorithm. Still another approach is developed in the context of intuitionistic resource-dependent logics, where D-trees are constructed from derivations (cf. e.g. a method in (Lecomte, 1992) for Lambek calculus). In this context, non-projective D-trees are determined with the use of hypothetical reasoning and of structural rules such as commu</context>
</contexts>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-projectivity : A polynomially parsable non-projective dependency grammar. In Proc. COLING-ACL, pages 646–652, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lambek</author>
</authors>
<title>The mathematics of sentence structure.</title>
<date>1958</date>
<journal>American Mathematical Monthly,</journal>
<pages>154--170</pages>
<contexts>
<context position="2180" citStr="Lambek, 1958" startWordPosition="337" endWordPosition="338"> (Gaifman, 1961) and in some more recent proposals: link grammars (Sleator and Temperly, 1993), projective dependency grammars (Lombardo and Lesmo, 1996) the projectivity is implied by definition. In some other theories, e.g. in word grammar (Hudson, 1984), it is used as one of the axioms defining acceptable surface structures. In presence of this property, D-trees are in a sense equivalent to phrase structures with head selection 1. It is for this reason that D-trees determined by grammars of Robinson (Robinson, 1970), categorial grammars (Bar-Hillel et al., 1960), classical Lambek calculus (Lambek, 1958), and some other formalisms are projective. Projectivity affects the complexity of parsing : as a rule, it allows dynamic programming technics which lead to polynomial time algorithms (cf. -time algorithm for link grammars in (Sleator and Temperly, 1993)). Meanwhile, the projectivity is not the norm in natural languages. For example, in most European languages there are such regular non-projective constructions as WH- or relative clause extraction, topicalization, comparative constructions, and some constructions specific to a language, e.g. French pronominal clitics or left dislocation. In te</context>
</contexts>
<marker>Lambek, 1958</marker>
<rawString>J. Lambek. 1958. The mathematics of sentence structure. American Mathematical Monthly, pages 154– 170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lecomte</author>
</authors>
<title>Proof nets and dependencies.</title>
<date>1992</date>
<booktitle>In Proc. of COLING-92,</booktitle>
<pages>394--401</pages>
<location>Nantes.</location>
<contexts>
<context position="3833" citStr="Lecomte, 1992" startWordPosition="586" endWordPosition="587">n-tractable parsing (the NP-hardness argument of (Neuhaus and Br¨oker, 1997) applies to them). More recent versions of dependency grammars (see e.g.(Kahane et al., 1998; Lombardo and Lesmo, 1998; Br¨oker, 1998)) impose on nonprojective D-trees some constraints weaker than projectivity (cf. meta-projectivity (Nasr, 1995) or pseudo-projectivity (Kahane et al., 1998)), sufficient for existence of a polynomial time parsing algorithm. Still another approach is developed in the context of intuitionistic resource-dependent logics, where D-trees are constructed from derivations (cf. e.g. a method in (Lecomte, 1992) for Lambek calculus). In this context, non-projective D-trees are determined with the use of hypothetical reasoning and of structural rules such as commutativity and associativity (see e.g. (Moortgat, 1990)). In this paper, we put forward a novel approach to handling discontinuity in terms of dependency structures. We propose a notion of a polarized dependency (PD-) grammar combining several ideas from cf-tree grammars, dependency grammars and resource-dependent logics. As most dependency grammars, the PD-grammars are analyzing. They reduce continuous groups to their types using local (contex</context>
</contexts>
<marker>Lecomte, 1992</marker>
<rawString>A. Lecomte. 1992. Proof nets and dependencies. In Proc. of COLING-92, pages 394–401, Nantes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lombardo</author>
<author>L Lesmo</author>
</authors>
<title>An earley-type recognizer for dependency grammar.</title>
<date>1996</date>
<booktitle>In Proc. 16th COLING,</booktitle>
<pages>723--728</pages>
<contexts>
<context position="1720" citStr="Lombardo and Lesmo, 1996" startWordPosition="259" endWordPosition="262">rt). D-tree itself does not presume a linear order on words. However, any its surface realization projects some linear order relation (called also precedence). Some properties of surface syntactic structure can be expressed only in terms of both dependency (or its transitive closure called dominance) and precedence. One of such properties, projectivity, requires that any word occurring between a word and a word dependent on be dominated by In first dependency grammars (Gaifman, 1961) and in some more recent proposals: link grammars (Sleator and Temperly, 1993), projective dependency grammars (Lombardo and Lesmo, 1996) the projectivity is implied by definition. In some other theories, e.g. in word grammar (Hudson, 1984), it is used as one of the axioms defining acceptable surface structures. In presence of this property, D-trees are in a sense equivalent to phrase structures with head selection 1. It is for this reason that D-trees determined by grammars of Robinson (Robinson, 1970), categorial grammars (Bar-Hillel et al., 1960), classical Lambek calculus (Lambek, 1958), and some other formalisms are projective. Projectivity affects the complexity of parsing : as a rule, it allows dynamic programming techni</context>
</contexts>
<marker>Lombardo, Lesmo, 1996</marker>
<rawString>V. Lombardo and L. Lesmo. 1996. An earley-type recognizer for dependency grammar. In Proc. 16th COLING, pages 723–728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lombardo</author>
<author>L Lesmo</author>
</authors>
<title>Formal aspects and parsing issues of dependency theory.</title>
<date>1998</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<pages>787--793</pages>
<location>Montreal.</location>
<contexts>
<context position="3413" citStr="Lombardo and Lesmo, 1998" startWordPosition="526" endWordPosition="529">rase structure, non-projectivity corresponds to discontinuity. In this form it is in the center of discussions till 70-ies. There are various dependency based approaches to this problem. In the framework of Meaning-Text Theory (Mel’ˇcuk and Pertsov, 1987), dependencies between (some1See (Dikovsky and Modina, 2000) for more details. times non adjacent) words are determined in terms of their local neighborhood, which leads to non-tractable parsing (the NP-hardness argument of (Neuhaus and Br¨oker, 1997) applies to them). More recent versions of dependency grammars (see e.g.(Kahane et al., 1998; Lombardo and Lesmo, 1998; Br¨oker, 1998)) impose on nonprojective D-trees some constraints weaker than projectivity (cf. meta-projectivity (Nasr, 1995) or pseudo-projectivity (Kahane et al., 1998)), sufficient for existence of a polynomial time parsing algorithm. Still another approach is developed in the context of intuitionistic resource-dependent logics, where D-trees are constructed from derivations (cf. e.g. a method in (Lecomte, 1992) for Lambek calculus). In this context, non-projective D-trees are determined with the use of hypothetical reasoning and of structural rules such as commutativity and associativity</context>
</contexts>
<marker>Lombardo, Lesmo, 1998</marker>
<rawString>V. Lombardo and L. Lesmo. 1998. Formal aspects and parsing issues of dependency theory. In Proc. COLING-ACL, pages 787–793, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mel’ˇcuk</author>
<author>N V Pertsov</author>
</authors>
<title>Surface Syntax of English. A Formal Model Within the Meaning-Text Framework.</title>
<date>1987</date>
<publisher>John Benjamins Publishing Company, Amsterdam/Philadelphia.</publisher>
<marker>Mel’ˇcuk, Pertsov, 1987</marker>
<rawString>I. Mel’ˇcuk and N.V. Pertsov. 1987. Surface Syntax of English. A Formal Model Within the Meaning-Text Framework. John Benjamins Publishing Company, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moortgat</author>
</authors>
<title>La grammaire cat´egorielle g´en´eralis´ee : le calcul de lambek-gentzen.</title>
<date>1990</date>
<booktitle>Structure of languages and its mathematical aspects,</booktitle>
<pages>127--182</pages>
<editor>In Ph. Miller and Th. Torris, editors,</editor>
<publisher>Hermes,</publisher>
<location>Paris.</location>
<contexts>
<context position="4040" citStr="Moortgat, 1990" startWordPosition="617" endWordPosition="618">998)) impose on nonprojective D-trees some constraints weaker than projectivity (cf. meta-projectivity (Nasr, 1995) or pseudo-projectivity (Kahane et al., 1998)), sufficient for existence of a polynomial time parsing algorithm. Still another approach is developed in the context of intuitionistic resource-dependent logics, where D-trees are constructed from derivations (cf. e.g. a method in (Lecomte, 1992) for Lambek calculus). In this context, non-projective D-trees are determined with the use of hypothetical reasoning and of structural rules such as commutativity and associativity (see e.g. (Moortgat, 1990)). In this paper, we put forward a novel approach to handling discontinuity in terms of dependency structures. We propose a notion of a polarized dependency (PD-) grammar combining several ideas from cf-tree grammars, dependency grammars and resource-dependent logics. As most dependency grammars, the PD-grammars are analyzing. They reduce continuous groups to their types using local (context-free) reduction rules and simultaneously assign partial dependency structures to reduced groups. The valencies (positive for governors and negative for subordinates) are used to specify discontinuous (long</context>
</contexts>
<marker>Moortgat, 1990</marker>
<rawString>M. Moortgat. 1990. La grammaire cat´egorielle g´en´eralis´ee : le calcul de lambek-gentzen. In Ph. Miller and Th. Torris, editors, Structure of languages and its mathematical aspects, pages 127– 182. Hermes, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nasr</author>
</authors>
<title>A formalism and a parser for lexicalized dependency grammars.</title>
<date>1995</date>
<booktitle>In Proc. Int. Workshop on Parsing Technology,</booktitle>
<pages>186--195</pages>
<location>Prague.</location>
<contexts>
<context position="3540" citStr="Nasr, 1995" startWordPosition="544" endWordPosition="545">s dependency based approaches to this problem. In the framework of Meaning-Text Theory (Mel’ˇcuk and Pertsov, 1987), dependencies between (some1See (Dikovsky and Modina, 2000) for more details. times non adjacent) words are determined in terms of their local neighborhood, which leads to non-tractable parsing (the NP-hardness argument of (Neuhaus and Br¨oker, 1997) applies to them). More recent versions of dependency grammars (see e.g.(Kahane et al., 1998; Lombardo and Lesmo, 1998; Br¨oker, 1998)) impose on nonprojective D-trees some constraints weaker than projectivity (cf. meta-projectivity (Nasr, 1995) or pseudo-projectivity (Kahane et al., 1998)), sufficient for existence of a polynomial time parsing algorithm. Still another approach is developed in the context of intuitionistic resource-dependent logics, where D-trees are constructed from derivations (cf. e.g. a method in (Lecomte, 1992) for Lambek calculus). In this context, non-projective D-trees are determined with the use of hypothetical reasoning and of structural rules such as commutativity and associativity (see e.g. (Moortgat, 1990)). In this paper, we put forward a novel approach to handling discontinuity in terms of dependency s</context>
</contexts>
<marker>Nasr, 1995</marker>
<rawString>A. Nasr. 1995. A formalism and a parser for lexicalized dependency grammars. In Proc. Int. Workshop on Parsing Technology, pages 186–195, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Neuhaus</author>
<author>N Br¨oker</author>
</authors>
<title>The Complexity of Recognition of Linguistically Adequate Dependency Grammars.</title>
<date>1997</date>
<booktitle>In Proc. of 35th ACL Annual Meeting and 8th Conf. of the ECACL,</booktitle>
<pages>337--343</pages>
<marker>Neuhaus, Br¨oker, 1997</marker>
<rawString>P. Neuhaus and N. Br¨oker. 1997. The Complexity of Recognition of Linguistically Adequate Dependency Grammars. In Proc. of 35th ACL Annual Meeting and 8th Conf. of the ECACL, pages 337– 343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane J Robinson</author>
</authors>
<title>Dependency structures and transformational rules.</title>
<date>1970</date>
<journal>Language,</journal>
<volume>46</volume>
<pages>2--259</pages>
<contexts>
<context position="2091" citStr="Robinson, 1970" startWordPosition="324" endWordPosition="325">curring between a word and a word dependent on be dominated by In first dependency grammars (Gaifman, 1961) and in some more recent proposals: link grammars (Sleator and Temperly, 1993), projective dependency grammars (Lombardo and Lesmo, 1996) the projectivity is implied by definition. In some other theories, e.g. in word grammar (Hudson, 1984), it is used as one of the axioms defining acceptable surface structures. In presence of this property, D-trees are in a sense equivalent to phrase structures with head selection 1. It is for this reason that D-trees determined by grammars of Robinson (Robinson, 1970), categorial grammars (Bar-Hillel et al., 1960), classical Lambek calculus (Lambek, 1958), and some other formalisms are projective. Projectivity affects the complexity of parsing : as a rule, it allows dynamic programming technics which lead to polynomial time algorithms (cf. -time algorithm for link grammars in (Sleator and Temperly, 1993)). Meanwhile, the projectivity is not the norm in natural languages. For example, in most European languages there are such regular non-projective constructions as WH- or relative clause extraction, topicalization, comparative constructions, and some constr</context>
</contexts>
<marker>Robinson, 1970</marker>
<rawString>Jane J. Robinson. 1970. Dependency structures and transformational rules. Language, 46( 2):259– 285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Sleator</author>
<author>D Temperly</author>
</authors>
<title>Parsing English with a Link Grammar. In</title>
<date>1993</date>
<booktitle>Proc. IWPT’93,</booktitle>
<pages>277--291</pages>
<contexts>
<context position="1661" citStr="Sleator and Temperly, 1993" startWordPosition="252" endWordPosition="255"> (dependencies), which form a dependency tree (D-tree for short). D-tree itself does not presume a linear order on words. However, any its surface realization projects some linear order relation (called also precedence). Some properties of surface syntactic structure can be expressed only in terms of both dependency (or its transitive closure called dominance) and precedence. One of such properties, projectivity, requires that any word occurring between a word and a word dependent on be dominated by In first dependency grammars (Gaifman, 1961) and in some more recent proposals: link grammars (Sleator and Temperly, 1993), projective dependency grammars (Lombardo and Lesmo, 1996) the projectivity is implied by definition. In some other theories, e.g. in word grammar (Hudson, 1984), it is used as one of the axioms defining acceptable surface structures. In presence of this property, D-trees are in a sense equivalent to phrase structures with head selection 1. It is for this reason that D-trees determined by grammars of Robinson (Robinson, 1970), categorial grammars (Bar-Hillel et al., 1960), classical Lambek calculus (Lambek, 1958), and some other formalisms are projective. Projectivity affects the complexity o</context>
</contexts>
<marker>Sleator, Temperly, 1993</marker>
<rawString>D. D. Sleator and D. Temperly. 1993. Parsing English with a Link Grammar. In Proc. IWPT’93, pages 277–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tesni`ere</author>
</authors>
<title>El´ements de syntaxe structurale. Librairie C. Klincksieck,</title>
<date>1959</date>
<location>Paris.</location>
<marker>Tesni`ere, 1959</marker>
<rawString>L. Tesni`ere. 1959. ´El´ements de syntaxe structurale. Librairie C. Klincksieck, Paris.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>