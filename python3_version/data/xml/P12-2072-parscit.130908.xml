<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.232796">
<title confidence="0.990849">
Lemmatisation as a Tagging Task
</title>
<author confidence="0.997701">
Andrea Gesmundo
</author>
<affiliation confidence="0.999471">
Department of Computer Science
University of Geneva
</affiliation>
<email confidence="0.988439">
andrea.gesmundo@unige.ch
</email>
<author confidence="0.986858">
Tanja Samardˇzi´c
</author>
<affiliation confidence="0.998847">
Department of Linguistics
University of Geneva
</affiliation>
<email confidence="0.994373">
tanja.samardzic@unige.ch
</email>
<sectionHeader confidence="0.995598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997365">
We present a novel approach to the task of
word lemmatisation. We formalise lemmati-
sation as a category tagging task, by describ-
ing how a word-to-lemma transformation rule
can be encoded in a single label and how a
set of such labels can be inferred for a specific
language. In this way, a lemmatisation sys-
tem can be trained and tested using any super-
vised tagging model. In contrast to previous
approaches, the proposed technique allows us
to easily integrate relevant contextual informa-
tion. We test our approach on eight languages
reaching a new state-of-the-art level for the
lemmatisation task.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9975315">
Lemmatisation and part-of-speech (POS) tagging
are necessary steps in automatic processing of lan-
guage corpora. This annotation is a prerequisite
for developing systems for more sophisticated au-
tomatic processing such as information retrieval, as
well as for using language corpora in linguistic re-
search and in the humanities. Lemmatisation is es-
pecially important for processing morphologically
rich languages, where the number of different word
forms is too large to be included in the part-of-
speech tag set. The work on morphologically rich
languages suggests that using comprehensive mor-
phological dictionaries is necessary for achieving
good results (Hajiˇc, 2000; Erjavec and Dˇzeroski,
2004). However, such dictionaries are constructed
manually and they cannot be expected to be devel-
oped quickly for many languages.
In this paper, we present a new general approach
to the task of lemmatisation which can be used to
overcome the shortage of comprehensive dictionar-
ies for languages for which they have not been devel-
oped. Our approach is based on redefining the task
of lemmatisation as a category tagging task. Formu-
lating lemmatisation as a tagging task allows the use
of advanced tagging techniques, and the efficient in-
tegration of contextual information. We show that
this approach gives the highest accuracy known on
eight European languages having different morpho-
logical complexity, including agglutinative (Hungar-
ian, Estonian) and fusional (Slavic) languages.
2 Lemmatisation as a Tagging Task
Lemmatisation is the task of grouping together word
forms that belong to the same inflectional morpho-
logical paradigm and assigning to each paradigm its
corresponding canonical form called lemma. For ex-
ample, English word forms go, goes, going, went,
gone constitute a single morphological paradigm
which is assigned the lemma go. Automatic lemma-
tisation requires defining a model that can determine
the lemma for a given word form. Approaching it
directly as a tagging task by considering the lemma
itself as the tag to be assigned is clearly unfeasible:
1) the size of the tag set would be proportional to the
vocabulary size, and 2) such a model would overfit
the training corpus missing important morphologi-
cal generalisations required to predict the lemma of
unseen words (e.g. the fact that the transformation
from going to go is governed by a general rule that
applies to most English verbs).
Our method assigns to each word a label encod-
</bodyText>
<page confidence="0.977744">
368
</page>
<note confidence="0.7041875">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 368–372,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.993711428571429">
label set size
ing the transformation required to obtain the lemma
string from the given word string. The generic trans-
formation from a word to a lemma is done in four
steps: 1) remove a suffix of length Ns; 2) add a
new lemma suffix, Ls; 3) remove a prefix of length
Np; 4) add a new lemma prefix, Lp. The tuple
z - (Ns, Ls, Np, Lp) defines the word-to-lemma
transformation. Each tuple is represented with a
label that lists the 4 parameters. For example, the
transformation of the word going into its lemma is
encoded by the label (3, 0, 0, 0). This label can be
observed on a specific lemma-word pair in the train-
ing set but it generalizes well to the unseen words
that are formed regularly by adding the suffix -ing.
The same label applies to any other transformation
which requires only removing the last 3 characters
of the word string.
Suffix transformations are more frequent than pre-
fix transformations (Jongejan and Dalianis, 2009).
In some languages, such as English, it is sufficient
to define only suffix transformations. In this case, all
the labels will have Np set to 0 and Lp set to 0. How-
ever, languages richer in morphology often require
encoding prefix transformations too. For example,
in assigning the lemma to the negated verb forms in
Czech the negation prefix needs to be removed. In
this case, the label (1, t, 2, 0) maps the word nevedel
to the lemma vedet. The same label generalises to
other (word, lemma) pairs: (nedok´azal, dok´azat),
(neexistoval, existovat), (nepamatoval, pamatovat).1
The set of labels for a specific language is induced
from a training set of pairs (word, lemma). For each
pair, we first find the Longest Common Substring
(LCS) (Gusfield, 1997). Then we set the value of
Np to the number of characters in the word that pre-
cede the start of LCS and Ns to the number of char-
acters in the word that follow the end of LCS. The
value of Lp is the substring preceding LCS in the
lemma and the value of Ls is the substring follow-
ing LCS in the lemma. In the case of the example
pair (nevedel, vedet), the LCS is vede, 2 characters
precede the LCS in the word and 1 follows it. There
are no characters preceding the start of the LCS in
1The transformation rules described in this section are well
adapted for a wide range of languages which encode morpho-
logical information by means of affixes. Other encodings can be
designed to handle other morphological types (such as Semitic
languages).
</bodyText>
<figure confidence="0.963193">
0 10000 20000 30000 40000 50000 60000 70000 80000 90000
word-lemma samples
</figure>
<figureCaption confidence="0.99954">
Figure 1: Growth of the label set with the number of train-
ing instances.
</figureCaption>
<bodyText confidence="0.986418">
the lemma and ‘t’ follows it. The generated label is
added to the set of labels.
</bodyText>
<sectionHeader confidence="0.938932" genericHeader="method">
3 Label set induction
</sectionHeader>
<bodyText confidence="0.999976185185185">
We apply the presented technique to induce the la-
bel set from annotated running text. This approach
results in a set of labels whose size convergences
quickly with the increase of training pairs.
Figure 1 shows the growth of the label set size
with the number of tokens seen in the training set for
three representative languages. This behavior is ex-
pected on the basis of the known interaction between
the frequency and the regularity of word forms that
is shared by all languages: infrequent words tend to
be formed according to a regular pattern, while ir-
regular word forms tend to occur in frequent words.
The described procedure leverages this fact to in-
duce a label set that covers most of the word occur-
rences in a text: a specialized label is learnt for fre-
quent irregular words, while a generic label is learnt
to handle words that follow a regular pattern.
We observe that the non-complete convergence of
the label set size is, to a large extent, due to the pres-
ence of noise in the corpus (annotation errors, ty-
pos or inconsistency). We test the robustness of our
method by deciding not to filter out the noise gener-
ated labels in the experimental evaluation. We also
observe that encoding the prefix transformation in
the label is fundamental for handling the size of the
label sets in the languages that frequently use lemma
prefixes. For example, the label set generated for
</bodyText>
<figure confidence="0.994819818181818">
350
300
250
200
150
100
50
0
English
Slovene
Serbian
</figure>
<page confidence="0.777773">
369
</page>
<figureCaption confidence="0.966217333333333">
Czech doubles in size if only the suffix transforma-
tion is encoded in the label. Finally, we observe that
the size of the set of induced labels depends on the
morphological complexity of languages, as shown in
Figure 1. The English set is smaller than the Slovene
and Serbian sets.
</figureCaption>
<table confidence="0.999798833333333">
Base Line [w0], flagChars(w0),
(BL) prefixes(w0), suffixes(w0)
+ context BL + [w1], [w−1], [lem1], [lem−1]
+ POS BL + [pos0]
+cont.&amp;POS BL + [w1], [w−1], [lem1], [lem−1],
[pos0], [pos−1], [pos1]
</table>
<tableCaption confidence="0.999294">
Table 1: Feature sets.
</tableCaption>
<sectionHeader confidence="0.995938" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9976603">
The advantage of structuring the lemmatisation task
as a tagging task is that it allows us to apply success-
ful tagging techniques and use the context informa-
tion in assigning transformation labels to the words
in a text. For the experimental evaluations we use
the Bidirectional Tagger with Guided Learning pre-
sented in Shen et al. (2007). We chose this model
since it has been shown to be easily adaptable for
solving a wide set of tagging and chunking tasks ob-
taining state-of-the-art performances with short ex-
ecution time (Gesmundo, 2011). Furthermore, this
model has consistently shown good generalisation
behaviour reaching significantly higher accuracy in
tagging unknown words than other systems.
We train and test the tagger on manually anno-
tated G. Orwell’s “1984” and its translations to seven
European languages (see Table 2, column 1), in-
cluded in the Multext-East corpora (Erjavec, 2010).
The words in the corpus are annotated with both
lemmas and detailed morphosyntactic descriptions
including the POS labels. The corpus contains 6737
sentences (approximatively 110k tokens) for each
language. We use 90% of the sentences for training
and 10% for testing.
We compare lemmatisation performance in differ-
ent settings. Each setting is defined by the set of fea-
tures that are used for training and prediction. Table
1 reports the four feature sets used. Table 2 reports
the accuracy scores achieved in each setting. We es-
tablish the Base Line (BL) setting and performance
in the first experiment. This setting involves only
features of the current word, [w0], such as the word
form, suffixes and prefixes and features that flag the
presence of special characters (digits, hyphen, caps).
The BL accuracy is reported in the second column of
Table 2).
In the second experiment, the BL feature set is
expanded with features of the surrounding words
([w_1], [w1]) and surrounding predicted lemmas
([lem_1], [lem1]). The accuracy scores obtained in
</bodyText>
<table confidence="0.999593545454545">
Language Base + + +cont.&amp;POS
Line cont. POS
Acc. UWA
Czech 96.6 96.8 96.8 97.7 86.3
English 98.8 99.1 99.2 99.6 94.7
Estonian 95.8 96.2 96.5 97.4 78.5
Hungarian 96.5 96.9 97.0 97.5 85.8
Polish 95.3 95.6 96.0 96.8 85.8
Romanian 96.2 97.4 97.5 98.3 86.9
Serbian 95.0 95.3 96.2 97.2 84.9
Slovene 96.1 96.6 97.0 98.1 87.7
</table>
<tableCaption confidence="0.999696">
Table 2: Accuracy of the lemmatizer in the four settings.
</tableCaption>
<bodyText confidence="0.999948833333333">
the second experiment are reported in the third col-
umn of Table 2. The consistent improvements over
the BL scores for all the languages, varying from
the lowest relative error reduction (RER) for Czech
(5.8%) to the highest for Romanian (31.6%), con-
firm the significance of the context information. In
the third experiment, we use a feature set in which
the BL set is expanded with the predicted POS tag of
the current word, [pos0].2 The accuracy measured
in the third experiment (Table 2, column 4) shows
consistent improvement over the BL (the best RER
is 34.2% for Romanian). Furthermore, we observe
that the accuracy scores in the third experiment are
close to those in the second experiment. This allows
us to state that it is possible to design high quality
lemmatisation systems which are independent of the
POS tagging. Instead of using the POS information,
which is currently standard practice for lemmatisa-
tion, the task can be performed in a context-wise set-
ting using only the information about surrounding
words and lemmas.
In the fourth experiment we use a feature set con-
sisting of contextual features of words, predicted
lemmas and predicted POS tags. This setting com-
</bodyText>
<footnote confidence="0.996811666666667">
2The POS tags that we use are extracted from the mor-
phosyntactic descriptions provided in the corpus and learned
using the same system that we use for lemmatisation.
</footnote>
<page confidence="0.997121">
370
</page>
<bodyText confidence="0.999979757575758">
bines the use of the context with the use of the pre-
dicted POS tags. The scores obtained in the fourth
experiment are considerably higher than those in the
previous experiments (Table 2, column 5). The RER
computed against the BL varies between 28.1% for
Hungarian and 66.7% for English. For this set-
ting, we also report accuracies on unseen words only
(UWA, column 6 in Table 2) to show the generalisa-
tion capacities of the lemmatizer. The UWA scores
85% or higher for all the languages except Estonian
(78.5%).
The results of the fourth experiment show that in-
teresting improvements in the performance are ob-
tained by combining the POS and context informa-
tion. This option has not been explored before.
Current systems typically use only the information
on the POS of the target word together with lem-
matisation rules acquired separately from a dictio-
nary, which roughly corresponds to the setting of
our third experiment. The improvement in the fourth
experiment compared to the third experiment (RER
varying between 12.5% for Czech and 50% for En-
glish) shows the advantage of our context-sensitive
approach over the currently used techniques.
All the scores reported in Table 2 represent per-
formance with raw text as input. It is important to
stress that the results are achieved using a general
tagging system trained only a small manually an-
notated corpus, with no language specific external
sources of data such as independent morphological
dictionaries, which have been considered necessary
for efficient processing of morphologically rich lan-
guages.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999984714285715">
Jurˇsiˇc et al. (2010) propose a general multilingual
lemmatisation tool, LemGen, which is tested on
the same corpora that we used in our evaluation.
LemGen learns word transformations in the form of
ripple-down rules. Disambiguition between multi-
ple possible lemmas for a word form is based on the
gold-standard morphosyntactic label of the word.
Our system outperforms LemGen on all the lan-
guages. We measure a Relative Error Reduction
varying between 81% for Serbian and 86% for En-
glish. It is worth noting that we do not use manually
constructed dictionaries for training, while Jurˇsiˇc et
al. (2010) use additional dictionaries for languages
for which they are available.
Chrupała (2006) proposes a system which, like
our system, learns the lemmatisation rules from a
corpus, without external dictionaries. The mappings
between word forms and lemmas are encoded by
means of the shortest edit script. The sets of edit
instructions are considered as class labels. They are
learnt using a SVM classifier and the word context
features. The most important limitation of this ap-
proach is that it cannot deal with both suffixes and
prefixes at the same time, which is crucial for effi-
cient processing of morphologically rich languages.
Our approach enables encoding transformations on
both sides of words. Furthermore, we propose a
more straightforward and a more compact way of
encoding the lemmatisation rules.
The majority of other methods are concentrated
on lemmatising out-of-lexicon words. Toutanova
and Cherry (2009) propose a joint model for as-
signing the set of possible lemmas and POS tags
to out-of-lexicon words which is language indepen-
dent. The lemmatizer component is a discrimina-
tive character transducer that uses a set of within-
word features to learn the transformations from in-
put data consisting of a lexicon with full morpho-
logical paradigms and unlabelled texts. They show
that the joint model outperforms the pipeline model
where the POS tag is used as input to the lemmati-
sation component.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999975">
We have shown that redefining the task of lemma-
tisation as a category tagging task and using an ef-
ficient tagger to perform it results in a performance
that is at the state-of-the-art level. The adaptive gen-
eral classification model used in our approach makes
use of different sources of information that can be
found in a small annotated corpus, with no need for
comprehensive, manually constructed morphologi-
cal dictionaries. For this reason, it can be expected
to be easily portable across languages enabling good
quality processing of languages with complex mor-
phology and scarce resources.
</bodyText>
<sectionHeader confidence="0.998849" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999748333333333">
The work described in this paper was partially
funded by the Swiss National Science Foundation
grants CRSI22 127510 (COMTIS) and 122643.
</bodyText>
<page confidence="0.997909">
371
</page>
<sectionHeader confidence="0.98994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999626038461538">
Grzegorz Chrupała. 2006. Simple data-driven context-
sensitive lemmatization. In Proceedings of the So-
ciedad Espa˜nola para el Procesamiento del Lenguaje
Natural, volume 37, page 121131, Zaragoza, Spain.
Tomaˇz Erjavec and Saˇso Dˇzeroski. 2004. Machine learn-
ing of morphosyntactic structure: lemmatizing un-
known Slovene words. Applied Artificial Intelligence,
18:17–41.
Tomaˇz Erjavec. 2010. Multext-east version 4: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC’10), pages 2544–2547, Valletta, Malta. Euro-
pean Language Resources Association (ELRA).
Andrea Gesmundo. 2011. Bidirectional sequence clas-
sification for tagging tasks with guided learning. In
Proceedings of TALN 2011, Montpellier, France.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences - Computer Science and Computational Bi-
ology. Cambridge University Press.
Jan Hajiˇc. 2000. Morphological tagging: data vs. dic-
tionaries. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 94–101, Seattle, Washington.
Association for Computational Linguistics.
Bart Jongejan and Hercules Dalianis. 2009. Automatic
training of lemmatization rules that handle morpholog-
ical changes in pre-, in- and suffixes alike. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP,
pages 145–153, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Matjaˇz Jurˇsiˇc, Igor Mozetiˇc, Tomaˇz Erjavec, and Nada
Lavraˇc. 2010. LemmaGen: Multilingual lemmatisa-
tion with induced ripple-down rules. Journal of Uni-
versal Computer Science, 16(9):1190–1214.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 760–
767, Prague, Czech Republic. Association for Compu-
tational Linguistics.
Kristina Toutanova and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech pre-
diction. In Proceedings of the 47th Annual Meeting
of the ACL and the 4th IJCNLP of the AFNLP, page
486494, Suntec, Singapore. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.998354">
372
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.231932">
<title confidence="0.999803">Lemmatisation as a Tagging Task</title>
<author confidence="0.999395">Andrea</author>
<affiliation confidence="0.999641">Department of Computer University of</affiliation>
<title confidence="0.298319">andrea.gesmundo@unige.ch</title>
<author confidence="0.668442">Tanja</author>
<affiliation confidence="0.9977675">Department of University of</affiliation>
<email confidence="0.814805">tanja.samardzic@unige.ch</email>
<abstract confidence="0.999542733333333">We present a novel approach to the task of word lemmatisation. We formalise lemmatisation as a category tagging task, by describing how a word-to-lemma transformation rule can be encoded in a single label and how a set of such labels can be inferred for a specific language. In this way, a lemmatisation system can be trained and tested using any supervised tagging model. In contrast to previous approaches, the proposed technique allows us to easily integrate relevant contextual information. We test our approach on eight languages reaching a new state-of-the-art level for the lemmatisation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
</authors>
<title>Simple data-driven contextsensitive lemmatization.</title>
<date>2006</date>
<booktitle>In Proceedings of the Sociedad Espa˜nola para el Procesamiento del Lenguaje Natural,</booktitle>
<volume>37</volume>
<pages>121131</pages>
<location>Zaragoza,</location>
<contexts>
<context position="14106" citStr="Chrupała (2006)" startWordPosition="2336" endWordPosition="2337">en, which is tested on the same corpora that we used in our evaluation. LemGen learns word transformations in the form of ripple-down rules. Disambiguition between multiple possible lemmas for a word form is based on the gold-standard morphosyntactic label of the word. Our system outperforms LemGen on all the languages. We measure a Relative Error Reduction varying between 81% for Serbian and 86% for English. It is worth noting that we do not use manually constructed dictionaries for training, while Jurˇsiˇc et al. (2010) use additional dictionaries for languages for which they are available. Chrupała (2006) proposes a system which, like our system, learns the lemmatisation rules from a corpus, without external dictionaries. The mappings between word forms and lemmas are encoded by means of the shortest edit script. The sets of edit instructions are considered as class labels. They are learnt using a SVM classifier and the word context features. The most important limitation of this approach is that it cannot deal with both suffixes and prefixes at the same time, which is crucial for efficient processing of morphologically rich languages. Our approach enables encoding transformations on both side</context>
</contexts>
<marker>Chrupała, 2006</marker>
<rawString>Grzegorz Chrupała. 2006. Simple data-driven contextsensitive lemmatization. In Proceedings of the Sociedad Espa˜nola para el Procesamiento del Lenguaje Natural, volume 37, page 121131, Zaragoza, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomaˇz Erjavec</author>
<author>Saˇso Dˇzeroski</author>
</authors>
<title>Machine learning of morphosyntactic structure: lemmatizing unknown Slovene words.</title>
<date>2004</date>
<journal>Applied Artificial Intelligence,</journal>
<pages>18--17</pages>
<marker>Erjavec, Dˇzeroski, 2004</marker>
<rawString>Tomaˇz Erjavec and Saˇso Dˇzeroski. 2004. Machine learning of morphosyntactic structure: lemmatizing unknown Slovene words. Applied Artificial Intelligence, 18:17–41.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomaˇz Erjavec</author>
</authors>
<title>Multext-east version 4: Multilingual morphosyntactic specifications, lexicons and corpora.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<pages>2544--2547</pages>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta,</location>
<contexts>
<context position="9055" citStr="Erjavec, 2010" startWordPosition="1504" endWordPosition="1505">ith Guided Learning presented in Shen et al. (2007). We chose this model since it has been shown to be easily adaptable for solving a wide set of tagging and chunking tasks obtaining state-of-the-art performances with short execution time (Gesmundo, 2011). Furthermore, this model has consistently shown good generalisation behaviour reaching significantly higher accuracy in tagging unknown words than other systems. We train and test the tagger on manually annotated G. Orwell’s “1984” and its translations to seven European languages (see Table 2, column 1), included in the Multext-East corpora (Erjavec, 2010). The words in the corpus are annotated with both lemmas and detailed morphosyntactic descriptions including the POS labels. The corpus contains 6737 sentences (approximatively 110k tokens) for each language. We use 90% of the sentences for training and 10% for testing. We compare lemmatisation performance in different settings. Each setting is defined by the set of features that are used for training and prediction. Table 1 reports the four feature sets used. Table 2 reports the accuracy scores achieved in each setting. We establish the Base Line (BL) setting and performance in the first expe</context>
</contexts>
<marker>Erjavec, 2010</marker>
<rawString>Tomaˇz Erjavec. 2010. Multext-east version 4: Multilingual morphosyntactic specifications, lexicons and corpora. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), pages 2544–2547, Valletta, Malta. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Gesmundo</author>
</authors>
<title>Bidirectional sequence classification for tagging tasks with guided learning.</title>
<date>2011</date>
<booktitle>In Proceedings of TALN 2011,</booktitle>
<location>Montpellier, France.</location>
<contexts>
<context position="8696" citStr="Gesmundo, 2011" startWordPosition="1451" endWordPosition="1452">[pos0], [pos−1], [pos1] Table 1: Feature sets. 4 Experimental Evaluation The advantage of structuring the lemmatisation task as a tagging task is that it allows us to apply successful tagging techniques and use the context information in assigning transformation labels to the words in a text. For the experimental evaluations we use the Bidirectional Tagger with Guided Learning presented in Shen et al. (2007). We chose this model since it has been shown to be easily adaptable for solving a wide set of tagging and chunking tasks obtaining state-of-the-art performances with short execution time (Gesmundo, 2011). Furthermore, this model has consistently shown good generalisation behaviour reaching significantly higher accuracy in tagging unknown words than other systems. We train and test the tagger on manually annotated G. Orwell’s “1984” and its translations to seven European languages (see Table 2, column 1), included in the Multext-East corpora (Erjavec, 2010). The words in the corpus are annotated with both lemmas and detailed morphosyntactic descriptions including the POS labels. The corpus contains 6737 sentences (approximatively 110k tokens) for each language. We use 90% of the sentences for </context>
</contexts>
<marker>Gesmundo, 2011</marker>
<rawString>Andrea Gesmundo. 2011. Bidirectional sequence classification for tagging tasks with guided learning. In Proceedings of TALN 2011, Montpellier, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<date>1997</date>
<booktitle>Algorithms on Strings, Trees, and Sequences - Computer Science and Computational Biology.</booktitle>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5191" citStr="Gusfield, 1997" startWordPosition="832" endWordPosition="833">to 0 and Lp set to 0. However, languages richer in morphology often require encoding prefix transformations too. For example, in assigning the lemma to the negated verb forms in Czech the negation prefix needs to be removed. In this case, the label (1, t, 2, 0) maps the word nevedel to the lemma vedet. The same label generalises to other (word, lemma) pairs: (nedok´azal, dok´azat), (neexistoval, existovat), (nepamatoval, pamatovat).1 The set of labels for a specific language is induced from a training set of pairs (word, lemma). For each pair, we first find the Longest Common Substring (LCS) (Gusfield, 1997). Then we set the value of Np to the number of characters in the word that precede the start of LCS and Ns to the number of characters in the word that follow the end of LCS. The value of Lp is the substring preceding LCS in the lemma and the value of Ls is the substring following LCS in the lemma. In the case of the example pair (nevedel, vedet), the LCS is vede, 2 characters precede the LCS in the word and 1 follows it. There are no characters preceding the start of the LCS in 1The transformation rules described in this section are well adapted for a wide range of languages which encode morp</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield. 1997. Algorithms on Strings, Trees, and Sequences - Computer Science and Computational Biology. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Morphological tagging: data vs. dictionaries.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>94--101</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington.</location>
<marker>Hajiˇc, 2000</marker>
<rawString>Jan Hajiˇc. 2000. Morphological tagging: data vs. dictionaries. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 94–101, Seattle, Washington. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bart Jongejan</author>
<author>Hercules Dalianis</author>
</authors>
<title>Automatic training of lemmatization rules that handle morphological changes in pre-, in- and suffixes alike.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>145--153</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="4436" citStr="Jongejan and Dalianis, 2009" startWordPosition="704" endWordPosition="707">z - (Ns, Ls, Np, Lp) defines the word-to-lemma transformation. Each tuple is represented with a label that lists the 4 parameters. For example, the transformation of the word going into its lemma is encoded by the label (3, 0, 0, 0). This label can be observed on a specific lemma-word pair in the training set but it generalizes well to the unseen words that are formed regularly by adding the suffix -ing. The same label applies to any other transformation which requires only removing the last 3 characters of the word string. Suffix transformations are more frequent than prefix transformations (Jongejan and Dalianis, 2009). In some languages, such as English, it is sufficient to define only suffix transformations. In this case, all the labels will have Np set to 0 and Lp set to 0. However, languages richer in morphology often require encoding prefix transformations too. For example, in assigning the lemma to the negated verb forms in Czech the negation prefix needs to be removed. In this case, the label (1, t, 2, 0) maps the word nevedel to the lemma vedet. The same label generalises to other (word, lemma) pairs: (nedok´azal, dok´azat), (neexistoval, existovat), (nepamatoval, pamatovat).1 The set of labels for </context>
</contexts>
<marker>Jongejan, Dalianis, 2009</marker>
<rawString>Bart Jongejan and Hercules Dalianis. 2009. Automatic training of lemmatization rules that handle morphological changes in pre-, in- and suffixes alike. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 145–153, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matjaˇz Jurˇsiˇc</author>
<author>Igor Mozetiˇc</author>
<author>Tomaˇz Erjavec</author>
<author>Nada Lavraˇc</author>
</authors>
<title>LemmaGen: Multilingual lemmatisation with induced ripple-down rules.</title>
<date>2010</date>
<journal>Journal of Universal Computer Science,</journal>
<volume>16</volume>
<issue>9</issue>
<marker>Jurˇsiˇc, Mozetiˇc, Erjavec, Lavraˇc, 2010</marker>
<rawString>Matjaˇz Jurˇsiˇc, Igor Mozetiˇc, Tomaˇz Erjavec, and Nada Lavraˇc. 2010. LemmaGen: Multilingual lemmatisation with induced ripple-down rules. Journal of Universal Computer Science, 16(9):1190–1214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Giorgio Satta</author>
<author>Aravind Joshi</author>
</authors>
<title>Guided learning for bidirectional sequence classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>760--767</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="8492" citStr="Shen et al. (2007)" startWordPosition="1415" endWordPosition="1418"> than the Slovene and Serbian sets. Base Line [w0], flagChars(w0), (BL) prefixes(w0), suffixes(w0) + context BL + [w1], [w−1], [lem1], [lem−1] + POS BL + [pos0] +cont.&amp;POS BL + [w1], [w−1], [lem1], [lem−1], [pos0], [pos−1], [pos1] Table 1: Feature sets. 4 Experimental Evaluation The advantage of structuring the lemmatisation task as a tagging task is that it allows us to apply successful tagging techniques and use the context information in assigning transformation labels to the words in a text. For the experimental evaluations we use the Bidirectional Tagger with Guided Learning presented in Shen et al. (2007). We chose this model since it has been shown to be easily adaptable for solving a wide set of tagging and chunking tasks obtaining state-of-the-art performances with short execution time (Gesmundo, 2011). Furthermore, this model has consistently shown good generalisation behaviour reaching significantly higher accuracy in tagging unknown words than other systems. We train and test the tagger on manually annotated G. Orwell’s “1984” and its translations to seven European languages (see Table 2, column 1), included in the Multext-East corpora (Erjavec, 2010). The words in the corpus are annotat</context>
</contexts>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>Libin Shen, Giorgio Satta, and Aravind Joshi. 2007. Guided learning for bidirectional sequence classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760– 767, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Colin Cherry</author>
</authors>
<title>A global model for joint lemmatization and part-of-speech prediction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>486494</pages>
<institution>Suntec, Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="14936" citStr="Toutanova and Cherry (2009)" startWordPosition="2463" endWordPosition="2466">t script. The sets of edit instructions are considered as class labels. They are learnt using a SVM classifier and the word context features. The most important limitation of this approach is that it cannot deal with both suffixes and prefixes at the same time, which is crucial for efficient processing of morphologically rich languages. Our approach enables encoding transformations on both sides of words. Furthermore, we propose a more straightforward and a more compact way of encoding the lemmatisation rules. The majority of other methods are concentrated on lemmatising out-of-lexicon words. Toutanova and Cherry (2009) propose a joint model for assigning the set of possible lemmas and POS tags to out-of-lexicon words which is language independent. The lemmatizer component is a discriminative character transducer that uses a set of withinword features to learn the transformations from input data consisting of a lexicon with full morphological paradigms and unlabelled texts. They show that the joint model outperforms the pipeline model where the POS tag is used as input to the lemmatisation component. 6 Conclusion We have shown that redefining the task of lemmatisation as a category tagging task and using an </context>
</contexts>
<marker>Toutanova, Cherry, 2009</marker>
<rawString>Kristina Toutanova and Colin Cherry. 2009. A global model for joint lemmatization and part-of-speech prediction. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, page 486494, Suntec, Singapore. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>