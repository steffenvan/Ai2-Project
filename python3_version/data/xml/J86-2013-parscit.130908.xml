<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.86342" genericHeader="abstract">
ABSTRACTS OF CURRENT LITERATURE
</sectionHeader>
<bodyText confidence="0.8932075">
The following Technical Reports cover work done under the DARPA research described on page 136. Reports are
available from
</bodyText>
<affiliation confidence="0.8689065">
Department of Computer and Information Service
University of Pennsylvania
</affiliation>
<figure confidence="0.916331333333333">
Philadelphia, PA 19104
Interactive Classification: A Technique for
the Acquisition and Maintenance of Know-
ledge Bases
Tim Fink:, David Silverman
MS-CIS-84-17
Correcting Object-Related Mis-
conceptions: How Should The System
Respond?
Kathleen F. McCoy
MS-CIS-84-18
Default Reasoning in Interaction
Aravind Joshi, Bonnie Webber, Ralph Weischedel
MS-CIS-84-58
Preventing False Inferences
</figure>
<page confidence="0.613787333333333">
Aravind Joshi, Bonnie Webber,
Ralph M. Weischedel
MS-CIS-84-59
</page>
<bodyText confidence="0.999980695652174">
The practical application of frame-based knowledge-based systems, such
as in expert systems, requires the maintenance of potentially very large
amounts of declarative knowledge stored in their knowledge bases (ICBs).
As a KB grows in size and complexity, it becomes more difficult to main-
tain and extend. Even someone who is familiar with the representation and
the contents of the existing KB may introduce inconsistencies and errors
whenever an addition or modification is made.
This paper describes an approach to this problem based on a tool called
an interactive classifier. An interactive classifier uses the contents of the
existing KB and knowledge about its representation to assist the person
who is maintaining the KB in describing new KB objects. The interactive
classifier will identify the appropriate taxonomic location for the newly
described object and add it to the KB. The new object is allowed to be a
generalization of existing KB objects, enabling the system to learn more
about existing objects. The ideas have been tested in a system call KuBIC,
for Knowledge Base Interactive Classifier, and are being extended to a
more complete knowledge representation language.
This paper describes a computational method for correcting users&apos; miscon-
ceptions concerning the objects modeled by a computer system. The
method involves classifying object-related misconceptions according to the
knowledge-base feature involved in the incorrect information. For each
resulting class, sub-types are identified, according to the structure of the
knowledge base, which indicate what information may be supporting the
misconception and, therefore, what information to include in the response.
Such a characterization, along with a model of what the user knows,
enables the system to reason in a domain-independent way about how best
to correct the user.
Nonmonotonic reasoning is usually studied in the context of a logical
system in its own right or as reasoning done by an agent, in which the
agent reasons about the world from partial information and hence may
draw conclusions unsupported by traditional logic. The main point of
departure here is looking at nonmonotonic reasoning in the context of
interacting with another agent. This information is partial, in that the other
agent neither will nor can make everything explicit. Knowing this, the
agent may attempt to derive more from the interaction than what has been
made explicit, by reasoning by default about what has been made explicit
(often by contrast with what he assumes would have been made explicit,
were something else the case). Thus there can be rules for default reason-
ing that are operative in the interactive situation (&amp;quot;interactional defaults&amp;quot;)
that are not operative with only a single agent.
In cooperative man-machine interaction, it is taken as necessary that a
system truthfully and informatively respond to a user&apos;s question. It is not,
however, sufficient. In particular, if the system has reason to believe that
its planned response might lead the user to draw an inference that it knows
to be false, then it must block it by modifying or adding to its response.
The problem is that a system neither can nor should explore all conclusions
</bodyText>
<page confidence="0.93002">
148 Computational Linguistics, Volume 12, Number 2, April-June 1986
</page>
<note confidence="0.780319">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.452419">
a user might possibly draw: its reasoning must be constrained in some
systematic and well motivated way.
</bodyText>
<figure confidence="0.990904615384615">
Living Up To Expectations: Computing
Expert Responses
Aravind Joshi, Bonnie Webber, Ralph Weischedel
MS-CIS-84-60
A Modal Temporal Logic for Reasoning
about Changing Databases with Applica-
tions to Natural Language Question
Answering
Eric Mays, Aravind Joshi, Bonnie Webber
MS-CIS-85-01
Explaining Concepts in Expert Systems:
The CLEAR System
Robert Rubinoff
MS-CIS-85-06, LINC LAB 02
The Linguistic Relevance of Tree Ad-
joining Grammars
Anthony S. Kroch
Department of Linguistics
Aravind K. Joshi
Department of Computer and Information
Science
MS-CIS-85-16, LINC LAB 03
A Computational Logic Approach to
Syntax and Semantics
Dale A. Miller, Gopalan Nadathur
MS-CIS-85 - 17
</figure>
<bodyText confidence="0.999884509803922">
In cooperative man-machine interaction, it is necessary but not sufficient
for a system to respond truthfully and informatively to a user&apos;s question. In
particular, if the system has reason to believe that its planned response
might mislead the user, then it must block that conclusion by modifying its
response. This paper focuses on identifying and avoiding potentially
misleading responses by acknowledging types of &amp;quot;informing behavior&amp;quot;
usually expected of an expert. We attempt to give a formal account of
several types of assertions that should be included in response to questions
concerning the achievement of some goal (in addition to the simple
answer), lest the questioner otherwise be misled.
A database that models a changing world must evolve in correspondence to
the world. Previous work on natural language question answering systems
for databases has largely ignored the issues which arise when the database
is viewed as a dynamic (rather than a static) object. We investigate the
question answering behaviors that become possible with the ability to
represent and reason about the possible evolution of a database. These
behaviors include offering to monitor for a possible future state of the
database as an indirect response to a query, and directly answering ques-
tions about prior and future possibility. We apply a propositional modal
temporal logic that captures possibility and temporality to represent and
reason about dynamic databases, and present a sound axiomatization and
proof, and proof procedure.
Existing expert systems provide limited explanatory ability. They can
explain the specific reasoning the system uses, but if the user is confused
about the concepts and terms the system is using, no help is available. The
CLEAR system allows users to ask for explanations of specific concepts.
The system generates the explanations by examining the rule base, select-
ing rules that are relevant to the concept asked about. These rules are then
turned into English by various simple translation schemes and presented to
the user, providing an explanation of how the concept is used by the
system.
In this paper the linguistic significance of the Tree Adjoining Grammar
(TAG) has been investigated. An important property of TAG is that it
defines a constrained theory of syntactic embedding, one requiring that
embedded structures be composed out of elementary structures in a fixed
way, and one that forces co-occurrence relations between elements that
are separated in surface constituent structures to be stated broadly as
constraints on elementary trees in which those elements are co-present.
The extra generative power of TAG beyond context-free grammar emerges
as a corollary of factoring recursion and co-occurrence relations. The
linguistic details specifically discussed are raising constructions, passive,
and WH-movements.
It is well known that higher-order logics are very expensive, and for this
reason have been used to represent many problems in mathematics and
theoretical computer science. In the latter domain, higher-order logics
are often used to describe the semantics of first-order logics, natural
languages, or programs, since the formalization of such semantics needs a
recourse to quantification over the domain of functions and sets. In these
settings, higher-order logic has generally been limited to a descriptive role.
Once the formalization is made, little has been made of it computationally,
largely because there is abundant evidence that theorem proving in high-
</bodyText>
<note confidence="0.612944">
Computational Linguistics, Volume 12, Number 2, April-June 1986 149
The FINITE STRING Newsletter Abstracts of Current Literature
The Role of Perspective in Responding to
</note>
<figure confidence="0.8127766875">
Property Misconceptions
Kathleen F. McCoy
MS-CIS-85-31, May 1985
Some Computational Properties of Tree
Adjoining Grammars
K. Vijay-Shankar, A. Joshi
MS-CIS-85-07
Grammar, Phrase Structure
Aravind K. Joshi
MS-CIS-85-45
Question, Answer and Responses: Inter-
acting with Knowledge Base Systems
Bonnie Lynn Webber
MS-CIS-85-50, LINC LAB 04
A Theory of Scalar Implicature
Julia Bell Hirschberg
</figure>
<page confidence="0.707611">
MS-CIS-85-56
</page>
<bodyText confidence="0.99333888">
er-order logics is very difficult. In this paper we look at a sublogic of a
particular higher-order logic that is derived from Church&apos;s Theory of
Types, and examine its representational power and its computational tract-
ability. This sublogic can also be described as Horn clauses logic extended
with quantifications over function variables and A-contraction. We shall
present a sound and complete theorem prover for this logic, which uses
higher-order unification and may be described as an extension of a unifica-
tion procedure for the typed A-calculus. There are at least three ways in
which this logic is different from the first-order logic it generalizes. First it
possesses function variables which can be instantiated with A-terms and
evaluated through A-contractions. This provides the logic with a new
source of computation. Second, since A-terms do not have most general
unifiers, the process of finding appropriate unifiers must branch, and hence
involves real search. This facet provides a new source of nondeterminism
in specifying computations. Finally, this logic can directly encode first-
order logic in its term structure and can manipulate such terms in logically
meaningful ways. We illustrate this with examples taken from knowledge
representation and natural language parsing.
In order to adequately respond to misconceptions involving an object&apos;s
properties, we must have a context-sensitive method for determining object
similarity. Such a method is introduced here. Some of the necessary
contextual information is captured by a new notion of object perspective.
It is shown how object perspective can be used to account for different
responses to a given misconception in different contexts.
Tree Adjoining Grammar (TAG) is a formalism for natural language gram-
mars. Some of the basic notions of TAGs were introduced in Joshi, Levy,
and Takahashi (1975) and by Joshi (1983). A detailed investigation of the
linguistic relevance of TAGs has been carried out in Kroch and Joshi
(1985). In this paper, we describe some new results for TAGS, especially in
the following areas: (1) parsing complexity of TAGs, (2) some closure
results for TAGs, and (3) the relationship to Head grammars.
Phrase-structure trees (phrase-markers) provide structural descriptions for
sentences. Phrase-structure trees can be generated by phrase-structure
grammars. Phrase-structure trees can be shown to be appropriate to char-
acterize structural descriptions for sentences, including those aspects which
are usually characterized by transformational grammars, by making certain
amendations to CFGs, without increasing their power, or by generating
them from elementary trees (phrase-markers) by a suitable rule of compo-
sition, increasing the power only mildly beyond that of CFGs. Structural
descriptions provided by phrase-structure trees are used explicitly or
implicitly in natural language processing systems.
The purpose of this chapter is to examine the character of information-
seeking interactions between a user and a knowledge base system (KBS).
In doing so, I advocate that a clear distinction be made between an answer
to a question and a response. The chapter characterizes questions,
answers, and responses, the role they play in effective information inter-
changes, and what is involved in facilitating such interactions between user
and KBS.
(No abstract.)
The Relationship Between Tree Adjoining Tree Adjoining Grammars (TAG) and Head Grammars (HG) were intro-
</bodyText>
<page confidence="0.941908">
150 Computational Linguistics, Volume 12, Number 2, April-June 1986
</page>
<note confidence="0.99191575">
The FINITE STRING Newsletter Abstracts of Current Literature
Grammars and Head Grammars
K. Vijay-Shanker, David J. Weir,
Aravind K. Joshi
</note>
<figure confidence="0.790956923076923">
MS-CIS-86-01, LINC LAB 06
Natural Language Interactions with Arti-
ficial Experts
Tim Finin, Aravind K. Jashi,
Bonnie Lynn Webber
MS-CIS-86-16, LINC LAB 08
Higher-Order Logic Programming
Dale A. Miller, Gopalan Nadathur
MS-CIS-86-1 7
Some Uses of Higher-Order Logic in
Computational Linguistics
Dale A. Miller, Gopalan Nadathur
MS-CIS-86-31, LINC LAB 08
</figure>
<footnote confidence="0.217206">
Some Aspects Of Default Reasoning in
Interactive Discourse
</footnote>
<note confidence="0.404659">
Aravind K. Jashi, Bonnie L. Webber,
Ralph M. Weischedel
</note>
<footnote confidence="0.43372725">
In cooperative interaction, it is taken
and informatively respond to a user&apos;s
cient. In particular, if the system has
response might lead the user to draw
</footnote>
<bodyText confidence="0.997915444444444">
as necessary that a system truthfully
question. It is not, however, suffi-
reason to believe that its planned
an inference that it knows to be false,
duced to capture certain structural properties of natural languages. These
formalisms, which were developed independently, appear to be quite
different notationally. In this paper we discuss the formal relationship
between the class of languages generated by TAGs (TAL) and the class of
languages generated by HGs (HL). In particular, we show that HLs are
included in TALs and that TAGs are equivalent to a modification of HGs
called Modified Head Grammars (MHG). The inclusion of MHL in HL,
and thus the equivalence of HGs and TAGs, in the most general case
remains to be established. We show that this relationship is very close
both linguistically and formally, the difference hinging on the status of
heads of empty strings and whether one deals with heads directly or with
the left and right wrapping positions around the head.
The aim of this paper is to justify why Natural Language NLP interaction,
of a very rich functionality, is critical to the effective use of Expert Systems
and to describe what is needed and what has been done to support such
interaction. Interactive functions discussed here include defining terms,
paraphrasing, correcting misconceptions, avoiding misconceptions and
modifying questions.
In this paper we consider the problem of extending Prolog to include predi-
cate and function variables and typed A-terms. For this purpose, we use a
higher-order logic to describe a generalization to first-order Horn clauses.
We show that this extension possesses certain desirable computational
properties. Specifically, we show that the familiar operational and least
fixpoint semantics can be given to these clauses. A language, XProlog, that
is based on this generalization is then presented, and several examples of
its use are provided. We also discuss an interpreter for this language in
which new sources of branching and backtracking must be accommodated.
An experimental interpreter has been constructed for the language, and all
the examples in this paper have been tested using it.
Consideration of the question of meaning in the framework of linguistics
often requires an allusion to sets and other higher-order notions. The
traditional approach to representing and reasoning about meaning in a
computational setting has been to use knowledge representation systems
that are either based on first-order logic or that use mechanisms whose
formal justifications are to be provided after the fact. In this paper we
shall consider the use of a higher-order logic for this task. We first present
a version of definite clauses (positive Horn clauses) that is based on this
logic. Predicate and function variables may occur in such clauses; the
terms in the language are the typed A-terms. Such term structures have a
richness that may be exploited in representing meanings. We also describe
a higher-order logic programming language, called XProlog, which repres-
ents programs as higher-order definite clauses and interprets them using a
depth-first interpreter. A virtue of this language is that it is possible to
write programs in it that integrate syntactic and semantic analyses into one
computational paradigm. This is to be contrasted with the more common
practice of using two entirely different computation paradigms, such as
DCGs or ATNs for parsing and frames or semantic nets for semantic proc-
essing. We illustrate such an integration in this language by considering a
simple example, and we claim that its use makes the task of providing
formal justifications for the computations specified much more direct.
</bodyText>
<figure confidence="0.876995333333333">
Computational Linguistics, Volume 12, Number 2, April-June 1986 151
The FINITE STRING Newsletter Abstracts of Current Literature
GUMS,: A General User Modeling
System
Tim Fin in, David Drager
MS-CIS-86-35
Breaking the Primitive Concept Barrier
Robert Kass, Ron Katriel, Tim Finin
MS-CIS-86-36
</figure>
<bodyText confidence="0.995920918918919">
then it must block it by modifying or adding to its response. In this paper
we investigate several aspects of such reasoning in interactive discourse.
This paper describes the construction of a MUMBLE-based (McDonald
1983) tactical component for the TEXT text generation system (McKeown
1985). This new component, which produces fluent English sentences
from the sequence of structured message units output from TEXT&apos;s strate-
gic component, has produced a 60-fold speed-up in sentence production.
Adapting MUMBLE required work on each of the three parts of the
MUMBLE framework: the interpreter, the grammar, and the dictionary. It
also provided some insight into the generation process and the conse-
quences of MUMBLE&apos;s commitment to a deterministic model.
This paper describes a general architecture of a domain independent
system for building and maintaining long term models of individual users.
The user modeling system is intended to provide a well-defined set of
services for an application system which is interacting with various users
and has a need to build and maintain models of them. As the application
system interacts with a user, it can acquire knowledge of him and pass that
knowledge on to the user model maintenance system for incorporation.
We describe a prototype general user modeling system we have imple-
mented in Prolog. This system satisfies some of the desirable character-
istics we discuss.
Building and maintaining a large knowledge base of general information
requires a knowledge representation system with precise semahtics and an
easy knowledge acquisition procedure. Systems such as KL-ONE meet
these criteria by using a classifier to install new concepts into a taxonomic
structure. These systems use a formal notion of a definition for concepts.
Unfortunately, many concepts do not seem to have such precise defi-
nitions, and end up represented as primitive concepts. Primitive concepts
form a barrier to classification, forcing the user to manually classify a new
concept with respect to all primitive concepts in the knowledge base.
We propose an extension to KL-ONE that retains its soundness and
greatly reduces the burden on the user during knowledge acquisition. This
extension consists of adding an explicit definitional component to concepts
and relaxing the strictness of concept definitions themselves. The relaxed
definition reduces the number primitive concepts in a knowledge base,
enables the classifier to handle concepts that do not have complete defi-
nitions, and enhances the usefulness of an interactive classifier.
</bodyText>
<figure confidence="0.876413833333333">
MS-CIS-86-27
(revised version of MS-CIS-84-58)
Adapting MUMBLE: Experience with
Natural Language Generation
Robert Rubinoff
MS-CIS-86-32, LINC LAB 09
</figure>
<bodyText confidence="0.860863533333333">
The following Technical Reports are available from
Computer Science Department
Boston University
Boston, MA 02215
Please enclose a check, made payable to Boston University, for the total amount due ($3.00 per report to cover cost of
copying and postage).
Constrained Semantic Transference: In this paper we propose a formal theory of metaphors called Constrained
A Formal Theory of Metaphors Semantic Transference (CST). We start from the assumptions that meta-
Bipin Indurkhya phors are characterized by the description of one domain, called the target
BUCS Tech Report 85-008 domain, in terms of another domain, called the source domain; and that a
metaphor works by transferring a set of structural relationships from the
source domain to the target domain coherently.
Starting from these assumptions, we formally define the concept of
T-MAPs which are partial coherent mappings from the source domain to
the target domain. We also define two operators, called Augmentation and
</bodyText>
<page confidence="0.907199">
152 Computational Linguistics, Volume 12, Number 2, April-June 1986
</page>
<note confidence="0.433934">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.989626">
Positing Structure, that define a given T-MAP by adding new structure to
the target domain.
We show how T-MAPs can be used to characterize metaphorical inter-
pretations of a given set of sentences. This characterization allows several
cognitive features of metaphors to be described in CST. In particular, our
characterization used the same criterion for deciding metaphorical truth as
for literal truths.
</bodyText>
<figure confidence="0.873773666666667">
Approximate Semantic Transference: A
Computational Theory of Metaphors and
Analogies
Bipin Indurkhya
BUCS Tech Report 85-012
Computational Linguistics Technical Notes
Weiguo Wang
BUCS Tech Report 85-013,
November 1985
Machine Translation in the Semantic
Definite Clause Grammars Formalism
Xiuming Huang
</figure>
<figureCaption confidence="0.30825">
MCCS-85- 7
</figureCaption>
<bodyText confidence="0.951465298245614">
In this paper we start from the assumption that in a metaphor, or an analo-
gy, some terms belonging to one domain (source domain) are used to refer
to objects other than their conventional referents belonging to a possibly
different domain (target domain). We describe a formalism, which is
based on the First Order Predicate Calculus, for representing knowledge
structure associated with a domain and then develop a theory of Con-
strained Semantic Transference (CST), which allows the terms and the
structural relationships of the source domain to be transferred coherently
across to the target domain. We show how metaphors and analogies can
be characterized in CST in such a way that many of their cognitive proper-
ties can be explained.
We then propose a theory of approximate Semantic Transference (AST),
which is a computational version of CST and is derived from it by replacing
the coherence requirement with approximate coherency. We show how
AST can be used as a basis for designing models of cognitive processes
involved in comprehending metaphors and analogies.
This technical report contains two technical notes on the analysis of the
Kilbury algorithm for parsing ID/LP Grammars which is part of the GPSG
framework. In the first one, two implementations of this algorithm are
given, one in C-prolog, one in Interlisp-D. In the second one, the algo-
rithm is compared against the Earley-Shieber algorithm on efficiency.
The paper claims that the right attachment rules for phrases originally
suggested by Frazier and Fodor are wrong, and that none of the subse-
quent patchings of the rules by syntactic methods have improved the situ-
ation. For each rule there are perfectly straightforward and indefinitely
large classes of simple counter-examples. We then examine suggestions by
Ford et al., Schubert and Hirst which are quasi-semantic in nature and
which we consider ingenious but unsatisfactory. We offer a straightforward
solution within the framework of preference semantics, and argue that the
principal issue is not the type and nature of information required to get
appropriate phrase attachments, but the issue of where to store the infor-
mation and with what process to apply it. We present a Prolog implemen-
tation of a best first algorithm covering the data and contrast it with closely
related ones, all of which are based on the preferences of nouns and prep-
ositions, as well as verbs.
The paper describes the SDCG (Semantic Definite Clause Grammars), a
formalism for Natural Language Processing (NLP), and the XTRA (English
Chinese Sentence TRAnslator) machine translation (MT) system based on
it. The system translates general domain English sentences into grammat-
ical Chinese sentences in a fully automatic manner. It is written in Prolog
and implemented on the DEC-10, the GEC, and the SUN workstation,
The following reports are available from
Computing Research Laboratory
Box 3CRL
New Mexico State University
Las Cruces, NM 88003
Syntax, Preference and Right Attachment
Yorick Wilks, Xiuming Huang, Dan Fass
MCCS-85-5
Computational Linguistics, Volume 12, Number 2, April-June 1986 153
The FINITE STRING Newsletter Abstracts of Current Literature
respectively. SDCG is an augmentation of the DCG (Definite Clause Gram-
mar) of Pereira et al. (1980), which in turn is based on CFG (Context Free
Grammar). Implemented in Prolog, the SDCG is highly suitable for NLP in
general, and MT in particular. A wide range of linguistic phenomena is
covered by the XTRA system, including multiple word senses, coordinate
constructions, and prepositional phrase attachment, among others.
</bodyText>
<figure confidence="0.957517647058823">
Bad Metaphors: Chomsky and Artificial
Intelligence
Yorick Wilks
MCCS-85-8
To appear in S. Mogdil (ed.): Noam
Chomsky: Consensus and Controversy
(with replies by Chomsky)
Collative Semantics
Dan Fats
MCCS-85-23
Relevance, Points of View and Speech
Acts: An Artificial Intelligence View
Yorick Wilks
MCCS-85-25
Impatient Users and Foreign-Speak
Brian M. Slator
MCCS-85-26on the
</figure>
<bodyText confidence="0.999756085106383">
The paper argues that the historical divisions between, on the one hand,
the cluster of approaches to language understanding by computer known
as Artificial Intelligence and, on the other, the Transformational Grammar
system of Chomsky were caused not so much by matters of principle (as
between scientific linguistics and computational applications) or by meth-
odology, as by Chomsky&apos;s attachment over the years to a succession of
unfortunate metaphors to explain his position. As recent developments in
linguistics have shown, once these are removed, there are no issues of prin-
ciple (though there may continue to be differences of fact and evidence)
between linguistics, on the one hand, and Artificial Intelligence and
Computational Linguistics on the other.
Collative Semantics (CS) is a new domain independent semantics for
natural language understanding. CS can resolve instances of word-sense
and case ambiguity and offers an approach to discriminating between a
subset of semantic relations that can exist between word-senses: conven-
tional meaningful semantic relations, semantic redundancy, contrariness
and contradiction (it cannot differentiate between the two), metonymy,
metaphor, and meaninglessness. Collative Semantics is described. The
word-sense is the basic unit of representation. Sense-frames, the sole form
of knowledge structure, perform a double function: they are dictionary
entries for individual word-senses and, in certain situations, behave like
semantic primitives. Semantic vectors are scores that represent the degree
of match between word-senses (and their associated sense-frames). Brief
examples are given of how metonymic relations are computed, and how
metaphor is recognised.
The paper compares two approaches to modelling human discourse and,
more particularly, dialogue: one is the relevance logic of Sperber and
Wilson, the other the environments and points-of-view approach of Wilks
and Bien. Although both descend from general insights of Grice that
understanding is a matter of inference from what is said and what is
assumed, the paper criticizes the Sperber and Wilson work on the ground
that it is weak exactly where Grice is weak and that, contrary to its
authors&apos; claims, it is not an information processing model, and does not
model the beliefs of individual speakers and hearers. The latter system, like
others in AT, attempts to do that, and it is argued that any system that
models discourse must do so.
There is a class of Natural Language interface that translates English
input into mnemonic formal language commands. These interfaces appear
&amp;quot;front-end&amp;quot; of data bases and other applications software. Trans-
lation time, system response time, and user keyboard time, can all be
improved if users acquire and use the mnemonics of the applications soft-
ware. This paper argues that users, when shown the mnemonic output of
their English input, will learn to communicate on a mnemonic level much
the same way native speakers learn to speak on the level of intelligent but
inarticulate foreigners. Foreign-Speak (so called) will act as a CAI teach-
ing tool to help users more effectively use systems and might usefully be
included as part of the design of these Natural Language interfaces.
</bodyText>
<page confidence="0.993529">
154 Computational Linguistics, Volume 12, Number 2, April-June 1986
</page>
<figure confidence="0.92518375">
The FINITE STRING Newsletter Abstracts of Current Literature
The Effects of Restricted Syntax on
Human-Computer Interaction
Gregg D. Bailey
MCCS-85-36
Parsing in Parallel
Xiuming Huang, Louise Guthrie
MCCS-85-40
Natural Language Processing and Expert
Systems
Jerry T. Ball
MCCS-85-41
</figure>
<bodyText confidence="0.990827029411764">
The study compared the performance of 48 computer novices on two
different problem solving tasks that involve computer interactions. One
third of the subjects interacted with very limited natural language re-
strictions. Another group (16 subjects) interacted with command-oriented
restriction. The final group (16 subjects) had object-oriented restrictions.
The results of the study indicate that command restriction may facilitate
performance over both unrestricted and object-oriented conditions.
This paper is a description of a parallel model for natural language parsing,
and a design for its implementation on the Hypercube multiprocessor. The
parallel model is based on the Semantic Definite Clause Grammar formal-
ism and integrates syntax and semantics through the communication of
processes. The main processes, of which there are six, contain either purely
syntactic or purely semantic information, giving the advantage of simple,
transparent algorithms dedicated to only one aspect of parsing. Communi-
cation between processes is used to impose semantic constraints on the
syntactic processes.
Natural Language Processing (NLP) systems fall within the domain of
problems suitable for solution using techniques developed for use in Expert
Systems. Given an NLP system design involving the integration of syntax,
semantics and pragmatics, a production system formalism augmented with
techniques for managing a large solution space with interacting subprob-
lems and reliable data is needed. The combined techniques of constraint
propagation — to constrain the solution space, predictive selection of alter-
natives — to avoid least-commitment deadlock, and dependency-directed
backtracking — to recover from selection of failed alternatives, are recom-
mended.
The following reports are available from the Department of Information Science, University of Constance:
Universitaet Konstanz
Informationswissenschaft
Projekt TOPIC II
Postfach 5560
D-7750 Konstanz 1, F.R.G.
Orders from non-profit organizations (universities, governments, etc.) are handled free of charge. Requests from other
organizations or from individuals should be accompanied by payment for postage and handling costs ($5.00 per report).
</bodyText>
<construct confidence="0.777236666666667">
TOPIC II / TOPOGRAPHIC II. Auto-
matische Textkondensierimg und text-
orientiertes Informationsmanagement
(Projektziele - State of the Art)
U. Hahn, R. Hammwoehner, R. Kithlen,
U. Reimer, U. Thiel
Bericht TOPIC-12/844
&amp;TOPOGRAPHIC-3/84,
in German, Dec. 1984, 72 pp.
</construct>
<subsectionHeader confidence="0.475192">
On Formal Semantic Properties of a
Frame Data Model
Ulrich Reimer, Udo Hahn
</subsectionHeader>
<bodyText confidence="0.999861888888889">
A concise overview is given of the aims of the second phase of the text
condensation (automatic abstracting) and information retrieval projects of
the Information Science Department at the University of Constance (FRG)
After providing a state-of-the-art report of current string-processing,
statistical, linguistical, knowledge-based, and cognitive approaches to full
text analysis and retrieval in information systems, the conception of a
knowledge-based full text information system is outlined. Combining
both, sophisticated text analysis as well as information retrieval proce-
dures, text analysis is dealt with applying a word expert system to text
cohesion and text coherence phenomena for text parsing, a frame repre-
sentation model for text representation, and appropriate text condensation,
mechanisms for text summarization (project TOPIC II). Text information
management on the other hand is based on various forms of graphical
man-machine interaction and alternative ways of graphics-based text infor-
mation retrieval (project TOPOGRAPHIC).
Standard knowledge representation languages (e.g., KRL, FRL) are almost
exclusively characterized by syntactic specifications but are seriously lack-
ing of an explicit formal semantics. The formal description of the frame
</bodyText>
<figure confidence="0.661459444444444">
Computational Linguistics, Volume 12, Number 2, April-June 1986 155
The FINITE STRING Newsletter Abstracts of Current Literature
Bericht TOPIC-13/84,
in English, Dec. 1984, 24 pp.
The TOPIC Project: Text-Oriented Pro-
cedures for Information Management
and Condensation of Expository Texts
(Final Report)
Udo Hahn, Ulrich Reimer
Bericht TOPIC-17/85,
in English, May 1985, 114 pp.
Enhancing Knowledge Base Reliability: A
Claim for Specifying the Empirical Seman-
tics of Knowledge Representation Models
Ulrich Reimer
Bericht TOPIC-18/86,
in English, May 1986
TOPIC Essentials
</figure>
<bodyText confidence="0.973085491803279">
Udo Hahn, Ulrich Reimer
Berticht TOPIC-19/86,
in English, April 1986, 35 pp.
On Lexically Distributed Text Parsing:
Computational Model for the Analysis of
Textuality on the Level of Text Cohesion
and Text Coherence
data model outlined in this paper is given with emphasis on explicit seman-
tic specifications applying a combination of a denotational and an axiomat-
ic approach. After introducing the formal syntax and the basic conceptual
relations (e.g., is-a, instance-of, part-of) of the frame data model, the set
of semantic integrity constraints is outlined, finally leading to the specifica-
tion of a set of basic operations in the frame data model. Based on an
abstract data type view of knowledge representation languages, these oper-
ations completely specify the frame data model in terms of its behavioral
properties. As these operations are the only means to access data in a
frame data base, semantic integrity is always guaranteed.
After considering requirements for advanced full text information systems,
the major methodological concepts underlying the automatic text conden-
sation (automatic abstracting) system TOPIC is outlined: partial parsing as
a text-specific parsing strategy, validity and reliability of the knowledge
base under the scope of a formally specified frame data model, recognition
of text cohesion and coherence phenomena of expository texts by an
appropriately tuned text parser, and variable degrees of thematic aggre-
gation of an input text, basically on a level of text summarization compara-
ble to that of structured abstracts (indicative abstracts plus facts acquired
from the original text). Subsequently, a coherent formal description of the
whole TOPIC system is given that distinguishes three major processes: text
parsing based on the close interaction between a frame knowledge base
and a word expert parser, text condensation that utilizes various activation,
property assignment, and connectivity patterns of the text representation
structures, and finally, the construction of the text graph that represents
the knowledge structures of text condensates on different levels of thema-
tic abstraction accessible to appropriate information retrieval procedures.
Starting from a request for higher functionality of knowledge-based
systems, this paper argues for a more elaborate semantic specification of
knowledge representation models. Semantics in the understanding of this
paper requires the representation constructs to reflect the empirical regu-
larities holding in some domain of discourse. This way the possibility of
representing illegal knowledge is reduced and semantic integrity of a know-
ledge base is increased with an accordingly positive effect on a correspond-
ing application system. The paper further calls for high-level represen-
tation constructs to be made available to a knowledge representation
model and argues that they are needed to cope with increasing complexity
and size of knowledge bases. Finally, integrity preserving operations are
demanded as the only means to access a knowledge base so that an
abstract data type view on knowledge bases is achieved which guarantees
knowledge based always to be in valid states
An overview of TOPIC, a knowledge-based text information system for the
analysis of German-language texts, is provided. TOPIC supplies text
condensates (summaries) on variable degrees of generality and makes
available facts acquired from the texts. The presentation focuses on the
major methodological principles underlying the design of TOPIC: a frame
representation model that incorporates text cohesion and text coherence
properties of expository texts, a lexically distributed semantic text grammar
in the format of word experts, a model of partial text parsing, and text
graphs as appropriate representation structures for text condensates.
Adequate linguistic models for text analysis must account for textuality on
the level of text cohesion and text coherence, i.e., for local texture and
global well-formedness of text propositions by text grammars on a seman-
tic basis. Considering the requirements of corresponding text parsing
</bodyText>
<page confidence="0.888956">
156 Computational Linguistics, Volume 12, Number 2, April-June 1986
</page>
<figure confidence="0.711377111111111">
The FINITE STRING Newsletter Abstracts of Current Literature
Udo Hahn
Bericht TOPIC-20/85,
in English, Dec. 1985, 46 pp.
A Generalized Word Expert Model of
Lexically Distributed Text Parsing
Udo Hahn
Bericht TOPIC-21/86,
in English, April 1986, 16 pp.
</figure>
<bodyText confidence="0.981685041666667">
devices, a distributed lexical text grammar is provided in the format of
word experts. The current specification level aims at the recognition of
semantic and thematic relations holding among nominal expressions of a
text. An empirical fragment of the semantic text grammar supplied thus is
confined to the description of text cohesion phenomena due to nominal
anaphora and lexical cohesion. Repeated application of corresponding
word experts yields text representation structures in a frame knowledge
base which reveal highly standardized topic/comment patterns, namely
thematic progression based on the regular modes of constant theme, linear
thematization of themes, and derives themes.
Adequate linguistic models for text analysis must account for textuality on
the level of text cohesion and text coherence, i.e., for local texture and
global well-formedness of text propositions by text grammars on a seman-
tic basis. Considering the requirements of corresponding text parsing
devices, a distributed lexical text grammar is provided in the format of
word experts. The generalizations proposed concern linguistic generality
and methodological requirements of the formal specification of word
expert parsers. The approach is illustrated by an informal account of word
experts applying to nominal anaphora and lexical cohesion phenomena.
Repeated application of corresponding cohesion devices yields regular text
coherence structures in a frame knowledge base in terms of basic patterns
of thematic progression.
Single copies of the following reports and memos can be ordered free of charge from
Mrs. D. Borchers
</bodyText>
<figure confidence="0.875333875">
Universitaet des Saarlandes
FR 10.2 Informatik IV
IM Stadtwald 15
D - 6600 Saarbrucken 11
Federal Republic of Germany
Dialog-Based User Models
W. Wahlster, A. Kobsa
Report No. 3
</figure>
<bodyText confidence="0.773642666666667">
to appear in Ferrari, G., Ed., Proceedings
of the IEEE, Special Issue on Natural
Language Processing, July 1986
</bodyText>
<construct confidence="0.414728444444444">
SYCON — EM Ralunensystem zur Con-
straint-Propagierung auf Netzwerken von
beliebigen symbolischen Constraints
M. Fendler, R. Wichlacz
Report No. 4
In Stoyan, H., Ed.GWAI-85. 9th German
Workshop on Artificial Intelligence
Dassel/Solling, September 1985
Springer, Heidelberg, 1986
</construct>
<subsectionHeader confidence="0.93726">
Entwurf eines aktiven, wissenbasierten
Hilfesystems für SINIX
</subsectionHeader>
<bodyText confidence="0.999781043478261">
The paper investigates several approaches to user modeling in natural-lan-
guage dialog systems. First, reasons are pointed out why user modeling
has become so important in the last few years, and definitions are proposed
for the notions of user model and user modeling. Then techniques for
constructing user models in the course of a dialog are presented and recent
proposals for representing a wide range of assumptions about a user&apos;s be-
liefs and goals in a system&apos;s knowledge base are surveyed. Examples for
the application of user models in systems developed to date are presented,
and some social implications are discussed. Finally, unsolved problems,
like coping with collective beliefs or resource-limited processes, are investi-
gated, and prospects for application-oriented research are outlined.
Constraint-propagation on networks has been successfully tested as a
processing mechanism in several fields of Artificial Intelligence. This paper
presents SYCON, with which symbolic constraints of any kind can be
defined and processed. After a short introduction, the propagation of
restricted variables on constraint networks is explained. Algorithms are
presented for the handling of cycles, for checking the consistency of the
network, and for backtracking during the propagation process. To demon-
strate the essential differences between SYCON and other systems, it is
contrasted to the systems of Steele, Gosling, and Freuder. SYCON is
implemented in Franz-Lisp on a VAX 11/780.
The SC system is a natural language help facility for the SINIX operating
system. SC should answer natural language questions about concepts and
</bodyText>
<table confidence="0.995969958333333">
Computational Linguistics, Voltune 12, Number 2, April-June 1986 157
The FINITE STRING Newsletter Abstracts of Current Literature
Ch. Kemke
Report No. 5
Shortened version in LDV-Forum Nr. 2,
December 1985: 43-60
The Role of Natural language in Advanced
Knowledge-Based Systems
W. Wahlster
Report No. 6
to appear in Winter, H., Ed., Artificial
Intelligence and Man-Machine Systems.
Springer, Heidelberg, 1986
Combining Deictic Gestures and Natural
Language for Referent Identification
A. Kobsa, J. Al!gayer, C. Reddig-Siekmann,
N. Reithinger, D. Schmauks, K. Harbusch,
W. Wahlster
Report No. 7
to appear in Proceedings of COLING-86
Wissenbasierte Fertigungsplanung in Stan-
zereien mit FERPLAN: Ein Systemiiberlick
R. Grasmiick, A. Guldner
Memo No. 2
</table>
<bodyText confidence="0.999788277777778">
commands of the SINIX system. Furthermore, as an active help system it
should give unsolicited advice to a user during his work with SINIX
In this paper, first a short introduction to intelligent help systems is
given, followed by an overview of the help systems UC, UCC, and
WIZARD, whose domains are the operating systems UNIX and VMS,
respectively. In the next sections the basic structure and function of the
SINIX Consultant is explained, with special regard to the aspects of
planning/problem solving and knowledge representation. The functions
and tasks of the system&apos;s components (natural language interface, problem
solving/planning module, plan recognition module) and knowledge bases
(knowledge about language, planning, the SINIX domain, the general/
individual user) are described according to the present state of develop-
ment. Finally, important characteristics of the implementation and the
actual status of the project are given.
Natural language processing is a prerequisite for advanced knowledge-
based systems since the ability to acquire, retrieve, exploit, and present
knowledge critically depends on natural language comprehension and
production. Natural language concepts guide the interpretation of what we
see, hear, read, or experience with other senses. In the first part of the
paper, we illustrate the needed capabilities of cooperative dialog systems
with a detailed example: the interaction between a customer and a clerk at
an information desk in a train station. It is shown that natural language
systems cannot just rely on knowledge about syntactic and semantic
aspects of language but also have to exploit conceptual and inferential
knowledge, and a user model. In the remainder, we analyze and evaluate
three natural language systems introduced to the commercial market in
1985: Language Craft (TM of Carnegie Group, Inc.), NLMenu (TM of
Texas Instruments, Inc.), and Q&amp;A (TM of Symantec, Inc.). The detailed
examination of these systems shows their capabilities and limitations.
We conclude that the technology for limited natural-language access
systems is available now, but in the foreseeable future the capabilities of
such systems will in no way match human performance in face-to-face
communication.
In virtually all current NL dialog systems, users can refer to objects by
linguistic descriptions only. In human face-to-face conversation, however,
participants also frequently use various sorts of deictic gestures. In this
paper, we present the referent identification component of XTRA, a nat-
ural-language access system for expert systems. XTRA allows the user to
combine NL input with pointing gestures on the terminal screen for refer-
ring to objects on the display. Information about the location and the type
of this deictic gesture, as well as about the linguistic description of the
referred object, the case frame, and the dialog memory, are utilized for
identifying the object. The system is tolerant in respect to impreciseness of
both the deictic and the natural language input. The user can thereby refer
to objects more easily, avoid referential failures, and employ vague every-
day terms instead of precise technical notions.
Production planning is the process of designing plans for manufacturing
parts. FERPLAN is a knowledge-based system designed for production
planning in punching plants. Based on a qualitative part description, a
raw-material specification, and the planned number of pieces, the system
works out a handling plan with the corresponding advance times.
After a survey of the system&apos;s architecture, the plan generation is exem-
plified, starting with a qualitative part description. Finally, FERPLAN is
contrasted in tabular form to other systems designed for similar purposes.
</bodyText>
<page confidence="0.730424">
158 Computational Linguistics, Volume 12, Number 2, April-June 1986
</page>
<table confidence="0.998273533333333">
The FINITE STRING Newsletter Abstracts of Current Literature
GAB1 — Ein wissensbasiertes Geldan-
lageberatimgsprogramm
H. Bakes
Memo No. 3
Formulardeixis mid ihre Simulation auf
dem Bildschirm. Ein Uberblick aux
linguistischer Sicht
D. Schmauks
Memo No. 4
Characterizing Trajectories of Moving
Objects using Natural Language Path
Descriptions
E. Andre, G. Bosch, G. Herzog, T. Rist
Memo No. 5
</table>
<bodyText confidence="0.998377159090909">
The system is implemented on a VAX 11/780 in Franz-Lisp, using the
production-system interpreter OPS5.
This paper presents the interactive expert system GABI, a consulting and
information system for investment that deduces investment proposals from
customer-specific statements. After a short description of the system&apos;s
capabilities, its design criteria, and range of applications, the system archi-
tecture is explained. In addition to a dialog component, a simple explana-
tion component, and an inference component, it comprises a highly parti-
tioned knowledge base. The representation of knowledge about the
various forms of investment, the situation of the financial market, and the
structure of the inference rules are exemplified. The system is imple-
mented in Franz-Lisp, version 38.79, and the embedded PLANNER-like Al
programming language PEARL on a VAX 11/780 under UNIX 4.2 BSD. A
distinguishing feature of the system is that the propositional knowledge as
well as the inference rules are represented as frame-like structures.
Up to the end of the seventies, local deixis was dealt with mostly from a
rather general point of view. As far as pointing gestures are concerned, a
detailed treatment is missing in most linguistic investigations even up to the
present. The present paper, unlike the previous ones, only deals with a
very special kind of deixis, namely pointing gestures onto a form, which
puts up a very restricted deictic space. In this universe of discourse,
objects are often (and efficiently) referred to by pointing gestures. Deixis
with regard to forms only offers a definite advantage from a linguistic point
of view: some problems of local deixis are reduced in complexity without
the setting having to become unnatural. The first chapter of the paper
defines some technical terms and presents the various linguistic and non-
linguistic deictic means of reference. Chapters 2-3 list the essential char-
acteristics of deixis with regard to forms. Emphasis is put on investigating
in how far means of local deixis become more salient and thus easier to
describe in such a setting. Chapter 5 deals with the problem from an artifi-
cial intelligence point of view, since also some non-linguistic aspects of
local deixis can now be simulated on a terminal screen. For example, the
NL dialog system XTRA currently under development at the University of
Saarbriicken, is designed to allow the user to refer to subareas of a form
not only by linguistic means but also by pointing gestures.
The topic of this paper is the analysis of the semantics of the particular
spatial relations along and past, which are used to characterize the
path of moving objects. The German dialog system CITYTOUR, which
answers questions about the spatial relations of objects in a scene, is
presented by means of a simple example dialog. The representational
prerequisites of the static and dynamic objects, required for computational
analysis are presented. Two concrete predicate functions testing whether a
dynamic object moves along or moves past a static object are described in
detail.
</bodyText>
<subsectionHeader confidence="0.7739">
Selected Dissertation Abstracts
</subsectionHeader>
<bodyText confidence="0.938051375">
Compiled by:
Susanne M. Humphrey, National Library of Medicine, Bethesda, MD 20894
Bob Krovetz, University of Massachusetts, Amherst, MA 01002
The following are citations selected by title and abstract as being related to computational linguistics or knowledge
representation, resulting from a computer search, using the BRS Information Technologies retrieval service, of the
Dissertation Abstracts International (DAT) data base produced by University Microfilms International.
Included are the title; author; university, degree, and, if available, number of pages; DAT subject category chosen by
the author of the dissertation; UM order number and year-month of entry into the data base; and abstract. References
</bodyText>
<table confidence="0.935328972972973">
Computational Linguistics, Volume 12, Number 2, April-June 1986 159
The FINITE STRING Newsletter Abstracts of Current Literature
are sorted first by DAT subject category and second by author. Citations denoted by an MAI reference do not yet have
abstracts in the data base and refer to abstracts in the published Masters Abstracts International.
Unless otherwise specified, paper or microform copies of dissertations may be ordered from
University Microfilms International
Dissertation Copies
Post Office Box 1764
Ann Arbor, MI 48106
telephone for U.S. (except Michigan, Hawaii, Alaska): 1-800-521-3042
for Canada: 1-800-268-6090.
Price lists and other ordering and shipping information are in the introduction to the published DAT. An alternate
source for copies is sometimes provided at the end of the abstract.
The dissertation titles and abstracts contained here are published with permission of University Microfilms Interna-
tional, publishers of Dissertation Abstracts International (copyright 1985) by University Microfilms International), and
may not be reproduced without their prior permission.
Plans and Situated Actions: an
Inquiry into the Idea of Human-
Machine Conmumication
Lucille Alice Suchman
University of California, Berkeley Ph.D.
1984, 183 pages
Anthropology, Cultural
SO DAT V46(04), SecA, pp1018
University Microfilms Order Number
ADG85-13007. 8510
An Investigation of Information
Requirements Determination and
Analogical Problem Solving
Lance Brian Eliot
University of Southern California
Ph.D. 1985
Business Administration, General
DAT V46(04), SecA, pp1021
This item is not available from
University Microfilms International.
ADG05-56025. 8510
</table>
<bodyText confidence="0.999922285714286">
This dissertation examines two alternative views of purposeful action
and shared understanding. The first, adopted by researchers in
Cognitive Science, views the organization and significance of action
as derived from plans, which are prerequisite to and prescribe action
at whatever level of detail one might imagine. Mutual intelligibility
on this view is a matter of the recognizability of plans, due to common
conventions for the expression of intent, and common knowledge
about typical situations and appropriate actions. The second view,
drawn from recent work in social science, treats plans as derivative
from situated action. Situated action as such comprises necessarily
ad hoc responses to the actions of others and to the contingencies of
particular situations. Rather than depend upon the reliable recognition of
intent, successful interaction consists in the collaborative production of
intelligibility through mutual access to situation resources, and through the
detection, repair, or exploitation of differences in understanding.
Researchers interested in machine intelligence attempt to remedy the
inherent vagueness of plans, to make them the basis for computational arti-
facts intended to embody intelligent behavior, including the ability to inter-
act with their human users. I examine that project through a case study of
people using a machine designed on the planning model, and intended to
be intelligent and interactive. A conversation analysis of &amp;quot;interactions&amp;quot;
between users and the machine reveals that the machine&apos;s insensitivity to
particular circumstances is a central design resource, and a fundamental
limitation. I conclude that problems in Cognitive Science&apos;s theorizing
about purposeful action as a basis for machine intelligence are due to the
project of substituting plans for actions, and representations of the situ-
ation of action for action&apos;s actual circumstances.
Recent studies on the design of information systems have indicated
that one important factor in the development of a computer-based
information system is the problem solving behavior of systems
analysts. This study was an investigation of the problem solving
behavior of expert and novice systems analysts, and examined the
implications that these skills have for their selection, training, and use
of system methodologies.
Two major aspects of analyst behavior were examined: (1) infor-
mation cues of analyst knowledge used during the requirements de-
termination task, and (2) influence of past experiences as used during
an analogical problem solving process. Focus of the study was on
the cognitive behavior of analysts and differences in performance between
analyst levels (expert and novice). Empirical work consisted of an exper-
iment measuring specific forms of similarity judgments by analysts in order
to examine the two major aspects of analyst behavior.
</bodyText>
<page confidence="0.883265">
160 Computational Linguistics, Volume 12, Number 2, April-June 1986
</page>
<note confidence="0.605186">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.9989693125">
Results of the investigation indicated that: (1) analysts appeared to
divide requirements into two dimensions, organizational requirements and
technical requirements; (2) expert analysts divided requirements into two
dimensions with greater consistency and clarity than did novice analysts;
(3) analogical problem solving can have an influence during requirements
determination; and (4) influence of analogies was more pronounced with
novice analysts than expert analysts.
Interpretation of the results suggested that attention to analyst behavior
may improve the development of information systems by matching of
certain underlying skills with project requirements and systems methodol-
ogies. Future research recommendations included the need for further
assumption surfacing of analyst behavior, and inspection of reward struc-
tures for assumption establishment. Implications for research in human
problem solving, including analogical thinking, were also considered.
(Copies available exclusively from Micrographics Department, Doheny
Library, USC, Los Angeles, CA 90089.).
</bodyText>
<figure confidence="0.8710944">
A Framework for Expert Control of
Interactive Software Systems
Chidanand V. Apte
Rutgers University The State U. of New
Jersey (New Brunswick) Ph.D. 1984,
146 pages
Computer Science
DAI V46(02), SecB, pp574
University Microfilms Order Number
ADG85-07087. 8508
</figure>
<subsectionHeader confidence="0.567225333333333">
Understanding the Bugs of Novice
Programmers
Jeffrey Guy Bonar
</subsectionHeader>
<footnote confidence="0.926794666666667">
University of Massachusetts Ph.D.
1985, 288 pages
Computer Science
SO DAI V46(03), SecB, pp896
University Microfilms Order Number
ADG85-09525. 8509
</footnote>
<bodyText confidence="0.993039298507463">
Expert problem-solving strategies in many domains require the use of
detailed mathematical techniques coupled with experiential know-
ledge about how and when to use the appropriate techniques. In
many of these domains, such techniques are made available to
experts in large software packages. In attempting to build expert
systems for these domains, we wish to make use of these existing
packages, and are therefore faced with an important problem: how to
integrate the existing software, and knowledge about its use, into a
practical expert system. The expert knowledge is used, in dynamic
selection of appropriate programs and parameters, to reach a successful
goal in the problem-solving. This kind of expert problem-solving is
achieved through two interacting bodies of knowledge; problem domain
knowledge, and knowledge about the programs that comprise the
software package.
This thesis describes the framework of a hybrid expert system
for representing problem-solving knowledge in these domains. This
hybrid system may be characterized as consisting of a surface model
and a deep model. The surface model is a production-rule based
expert subsystem that cOnsists of heuristics used by an expert. The
deep model is a collection of methods, each parameterized by a set
of controlling and observed parameters. The methods and their
results are reasoned about using their parameter sets. The existing
software is reorganized as necessary to map it into the deep model
structure of a hybrid system. This framework has evolved out of an
effort to build an expert system for performing well-log analysis
(ELAS — Expert Log Analysis System). A generalized expert-system
building methodology based upon principles drawn from ELAS is
introduced. The use of method-abstractions in assembling a hybrid
system is discussed. The notion of worksheet-reasoning is defined,
and discussed.
Why do people have trouble learning to program? This dissertation
presents a theory of novice programming bugs motivated by inter-
views where novice programmers solved simple programming prob-
lems. Novice programming knowledge is represented in two
components. Fragments of pragmatic programming knowledge (PKP)
describe the expert programming knowledge that a novice has
acquired. Step-by-step Natural Language procedural knowledge
(SSK) describes the experience a novice brings to programming from
Natural Language. These two knowledge bases are tied together by
Computational Linguistics, Volume 12, Number 2, April-June 1986 161
The FINITE STRING Newsletter Abstracts of Current Literature
functional and surface parallels. PK and SSK are represented by Plans:
frame-like bundles of knowledge about the pragmatic features of program-
ming.
When a novice is programming and encounters a gap or inconsistency in
the PK, he or she has reached an impasse. The theory proposes bug gener-
ators as strategies for patching the impasse and continue developing a
solution. Usually this patch introduces a bug. Three types of bug genera-
tors are discussed. SSK/PK bug generators use SSK knowledge to patch an
impasse, exploiting functional and surface parallels to convert a PK impasse
into a problem in SSK, the better known domain. With Intra-PK bug
generators, novices rely on interconnections between fragments of PK to
patch an impasse. Other/PK bug generators use other knowledge, from
algebra, for example, to patch an impasse.
The theory is evaluated based on interviews of novice programmers
working on Pascal programming problems. A key set of predictions from
the theory are formulated and a two part method to analyze the protocol
data is described. The plan analysis describes the plans being used by the
novice. The bug analysis interprets each bug in terms of all plausible bug
generators. An entire analyzed protocol is shown. Based on that and
three other protocols, the predictions of the theory are evaluated. Also,
that analysis is used to comment on general patterns of plan usage and bug
generator coverage.
The dissertation concludes by summarizing what has been accom-
plished, the implications of the work, and future directions for the work. A
plan-based novice programming environment and tutor is described as a
natural extension of this work.
</bodyText>
<table confidence="0.699920363636364">
Computationally Efficient and
Linguistically Adequate Parsing of
Some Natural Language Structures
Pradip Dey
University of Pennsylvania Ph.D. 1984,
204 pages
Computer Science; Language, Linguis-
tics
SO DAI V46(01), SecB, pp231
University Microfilms Order Number
ADG85-05054. 8507
</table>
<bodyText confidence="0.998985419354839">
Computationally efficient and linguistically adequate strategies for
parsing sentences with word order variations and gapped structures
are presented. At first the general aspects of the strategies are
presented at an abstract level. Then, the strategies are applied to
Hindi. The strategies are implemented in the Augmented Transition
Network (ATN) formalism. However, they are general enough to be
implemented in other parsers. The standard ATN is inefficient due
to its usual backtracking, and so we add look-ahead facilities to
reduce backtracking. We also use temporary registers to hold well-
formed substrings temporarily until a decision can be taken about
them.
A problem in parsing word order variation is that subject, object, etc.,
cannot be identified from their position in the sentence. Postpositions and
inflection often help to identify them. But, there is a large body of data in
Hindi for which semantic information is required in addition to syntactic
information to make these identifications. Our analysis of Hindi shows that
in order to check grammatical agreement the parser must identify the
subject and object. In order to identify the subject and object the parser
must have access to semantic information. That means, in order to check
grammatical agreement the parser must have access to semantic informa-
tion. This has serious consequences for the relation between syntax and
semantics.
As a general strategy we suggest that, for parsing free constituent
orders, the right hand side of a phrase structure rule can be treated as a set.
The parser can proceed by checking set membership. By imposing
restrictions on sets, other word order variations can also be parsed. The
basic strategy is compatible with ID/LP analysis of Gazdar and Pullum
(see Pullum 1983), but we do not use metagrammar. We show that &apos;flat
structures&apos; are suitable for languages with word order variations. The flat
structure hypothesis helps to parse gapping, because structural matching
required for gapping is limited to higher constituents.
</bodyText>
<page confidence="0.937587">
162 Computational Linguistics, Volume 12, Number 2, April-June 1986
</page>
<note confidence="0.641541">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.9053665">
Grammatical phenomena often interact with each other in intricate
ways. We investigate several interacting phenomena, such as word order
variation, grammatical agreement, relative clause constructions, dative
constructions and gapping, in a coordinated way.
</bodyText>
<figure confidence="0.98097255">
A Computational Theory of Meta-
phor Comprehension and Analogical
Reasoning
Bipin Indurkhya
University of Massachusetts Ph.D.
1985, 207 pages
Computer Science
SO DAI V46(03), SecB, pp898
University Microfilms Order Number
ADG85-09557. 8509
Knowledge Representation and Intel-
ligent Systems: from Semantic
Networks to Cognitive Maps
James Richard Levenick
The University of Michigan Ph.D. 1985,
253 pages
Computer Science
DAI V46(04), SecB, pp1242
University Microfilms Order Number
ADG85-12454. 8510
</figure>
<bodyText confidence="0.993105294117647">
In this thesis we propose a formal theory of metaphors and analogies.
We start from the assumption that in a metaphor, or an analogy,
some terms of one domain (source domain) are applied to terms of
another domain (target domain). We address the problem of repre-
senting the information contained in a metaphor, or an analogy, and
the means of computing it from the domains&apos; knowledge.
We describe a formalism, called Schema-Language SL, for repre-
senting domain knowledge which is based on the First Order Predi-
cate Calculus. We then develop a theory of Constrained Semantic
Transference CST which shows how the terms and structural relation-
ships of the source domain can be coherently transferred to the target
domain. The concept of a T-MAP, which is a partial coherent mapping
from the terms of the source domain to the target domain, lies at the heart
of CST.
We introduce two operators, called Augmentation and Positing Struc-
ture, that make it possible to create a new structure in the target domain
constrained by the structure of the source domain. We show how to char-
acterize metaphors and analogies by using T-MAPs which can explain many
cognitive properties associated with them. We also present a characteriza-
tion of metaphorical truth and metaphorical inference in CST. A major
limitation of CST is that the notion of coherency is not computational.
We propose a theory of Approximate Semantic Transference AST,
which is derived from CST by replacing the coherency requirement on
T-MAPs by approximate coherency. The partial approximate-coherent
mappings of AST, called AT-MAPs, are computational and can be used as a
basis for developing models of cognitive processes involved in compre-
hending metaphors and analogies. We propose two alternative formu-
lations of approximate coherency. Based on one of these versions, we
present several algorithms, and principles that can be used in designing
algorithms, for computing AT-MAPs from the knowledge of the source and
target domains.
Al systems have long relied on propositional semantic network know-
ledge representation. Although many Al projects produce impressive
results, they tend to be difficult to generalize and have yielded only
meagre progress towards a theory of intelligence. The field lacks
coherence in definitions, assumptions, and methods. A systematic
treatment of the underlying knowledge representation issues appears
essential for the development of a more unified theory.
This dissertation considers knowledge networks in the context of
an attempt to model and produce intelligent, adaptive behavior.
Natural intelligent systems build perceptual and predictive capacity
on the basis of ordinary experience, and function routinely in ill-defined,
context sensitive situations. Propositional Al systems have not demon-
strated these capabilities; this may be inevitable given their underlying
knowledge representations. To facilitate an analysis of these issues a set of
parameters characterizing the space of such networks is developed. The
networks so defined range from Quillian&apos;s semantic network to the more
theoretically based cognitive map
A network activity passing simulation (NAPS) was developed to investi-
gate the effects of these various parameters on performance. NAPS
searches for subgoals between two given locations in a familiar environ-
ment. This wayfinding task is a simplification of the more general problem
Computational Linguistics, Volume 12, Number 2, April-June 1986 163
The FINITE STRING Newsletter Abstracts of Current Literature
solving method of searching for subgoals between a perceived state and a
desired state. NAPS inputs a set of knowledge representation parameters
and constructs the sort of network specified for use in testing. Then given
a start and a goal location, NAPS propagates activity in the net to select a
subgoal.
Experiments were performed in several environments; networks were
compared in terms of speed, reliability, flexibility and robustness. The
results were somewhat surprising; marker passing semantic networks are
more reliable than activity passing cognitive maps in simple environments
but prove to be less reliable in complex situations. The semantic networks
are shown to be rigid and inflexible, and so unsuitable for use in unpredict-
able, or difficult environments without the inclusion of additional mech-
anisms. The cognitive maps by contrast, are able to handle unexpected
environmental vagaries without the intervention of an intelligent executive.
</bodyText>
<figure confidence="0.738647055555556">
A Data Base System for Small Inter-
active Computers
John Robert Levine
Yale University Ph.D. 1984, 193 pages
Computer Science
DAI V46(05), SecB, pp1618
University Microfilms Order Number
ADG85-14672. 8511
The Organization of Knowledge in a
Multi-lingual, Integrated Parser
Steven Leo Lytinen
Yale University Ph.D. 1984, 314 pages
Computer Science
DAI V46(05), SecB, pp1618
University Microfilms Order Number
ADG85-14879. 8511
Functional Entity Relationship Model
Masanobu Matsuo
</figure>
<affiliation confidence="0.555646">
University of California, Santa Barbara
</affiliation>
<bodyText confidence="0.998894170731707">
A data base system oriented toward flexible single-user systems is
described, in three parts. First is a network data structure with
extensive provision for dynamic restructuring and omitted data.
Second is a graphical inquiry language that allows the user literally to
navigate among his data in an imaginary space. The inquiry language
allows manipulation of complex network structured data at the high
level usually associated with relational data bases. Last is a report
language that allows the user to draw his report on the screen in an
intuitively appealing way, without having to specify many of the details of
the reports to be generated. The three parts share tree-structured
selections from the underlying network data. An implementation mostly in
Lisp is described in detail, with sketches of how it could be implemented in
other languages.
A controversy has existed over the interaction of syntax and seman-
tics in natural language understanding systems. On the one hand,
theories of integrated parsing have argued that syntactic and seman-
tic processing must take place at the same time. In addition, these
theories have also argued that syntactic and semantic knowledge
should be mixed together, and that the role of syntax should be
completely subservient to semantic processing. On the other hand,
opponents of this theory argue that parsing should be more modular,
with syntactic and semantic processing taking place separately. Along with
this processing modularity, these opponents also argue that syntactic and
semantic knowledge should be more modular, and that syntax, since it is
largely autonomous from semantics, plays a more important role in natural
language understanding.
This thesis presents a theory of natural language understanding which is
a compromise between these two views. I argue that natural language
understanding should be integrated, in the sense that syntactic and seman-
tic processing should take place at the same time. However, instead of
mixing syntactic and semantic knowledge together in the knowledge base
of a parser, I argue that power can be gained by organizing syntax and
semantics as two largely separate bodies of knowledge, which are
combined only at the time of processing. The result is a parser which
retains the predictive power gained by using semantic information during
syntactic processing, but which is more robust in parsing complex syntactic
constructions, and which is more amenable to the organization of know-
ledge about more than one language.
In this thesis a new data model called FERM (Functional Entity
Relationship Model) is introduced in order to improve the semantic
modeling capability of database systems. FERM is an integration
</bodyText>
<page confidence="0.907736">
164 Computational Linguistics, Volume 12, Number 2, April-J-ne 1986
</page>
<table confidence="0.973023041666667">
The FINITE STRING Newsletter Abstracts of Current Literature
Ph.D. 1984, 213 pages
Computer Science
DAI V46(03), SecB, pp899
University Microfilms Order Number
ADG85-09444. 8509
A Modal Temporal Logic for Rea-
soning about Changing Databases
with Applications to Natural
Language Question Answering
Eric Keener Mays
University of Pennsylvania Ph.D. 1984,
136 pages
Computer Science
DAI V46(01), SecB, pp235
University Microfilms Order Number
ADG85-05104. 8507
Managing Permanent Objects
Nathaniel William Mishkin
Yale University Ph.D. 1984, 160 pages
Computer Science
DAI V46(05), SecB, pp1619
University Microfilms Order Number
ADG85-14880. 8511
</table>
<bodyText confidence="0.9884074">
and extension of the Functional Data Model and the Entity Relation-
ship Model. FERM consists of a small number of primitive elements
such as entity sets, attribute types, and database storage functions
together with a set of integrity constraints called domain-range
constraints. Functional relationships between entities and attributes
of an entity are represented in FERM by database storage functions. High-
level semantics such as ISA relationships, existence dependencies, manda-
tory relationships, case and disjointedness relationships are all expressed in
FERM in terms of domain-range constraints on database storage functions.
Thus, consistent database storage states with respect to such high-level
semantics can be studied by analyzing the effect of database update oper-
ations on domain-range constraints.
This thesis gives a formal definition of FERM including update
(insertion, deletion, replacement) operations. This remedies two major
shortcomings of previous work in the Functional and Entity Relationship
Model area: (1) the lack of a formal definition; (2) the absence of update
operations in these models.
A major emphasis in this thesis is placed upon the study of correct
update operations. A major contribution is the introduction of a finite set
of update operations called unit update operations which have three impor-
tant properties: (1) they are all consistent (i.e., correct) transactions with
respect to a given set of domain-range constraints; (2) they are minimal in
the sense that no unit update operation can be simplified any further with-
out becoming incorrect, and (3) all other consistent transactions can be
constructed from unit update operations using a given set of correctness-
preserving constructors (completeness property). An algorithm for gener-
ating a complete and minimal set of unit operations is given and
correctness of this algorithm is proven.
These results may be used as a basis for the development of a FERM
database design methodology. The Database Administrator can construct
user-level correct transactions by composing unit operations generated
automatically. This approach may ultimately result in improved productiv-
ity and reliability of database application development.
A database which models a changing world must evolve in corre-
spondence to the world. Previous work on natural language question
answering systems for databases has largely ignored the issues which
arise when the database is viewed as a dynamic (rather than a static)
object. We investigate the question answering behaviors that become
possible with the ability to represent and reason about the possible
evolution of a database. These behaviors include offering to monitor
for a possible future state of the database as an indirect response to a
query, and directly answering questions about prior and future possi-
bility. We apply a propositional modal temporal logic that captures
possibility and temporality to represent and reason about dynamic
databases, and present a sound axiomatization and proof procedure.
This work describes a programming system that facilitates the
management of data objects that live across multiple invocations of
programs that read and modify those objects; we call such data
objects &amp;quot;permanent objects&amp;quot;. Typically, programmers needing to
save data objects permanently do so either (1) by writing an ad hoc
set of procedures that convert data from some internal representation
to some external representation (and back), or (2) by interfacing
their programs with an existing database system. We discuss the problems
encountered by a programmer adopting either of these strategies, and we
describe our system whose design is an attempt to strike a balance between
Computational Linguistics, Volume 12, Number 2, April-June 1986 165
The FINITE STRING Newsletter Abstracts of Current Literature
the flexibility of the ad hoc approach and the rigidity of the approach that
employs a database.
A key goal of our work is the design and implementation of a system
that makes the manipulation of permanent objects nearly as easy and flexi-
ble as the manipulation of &amp;quot;transient&amp;quot; objects — i.e. the memory resident
data structures that programmers are accustomed to dealing with. We wish
to hide the details associated with the fact that permanent objects must
•
have their permanent home in a disk file system.
Our system is written in T, a dialect of Scheme, which is in turn a dialect
of Lisp and runs on the Apollo workstation. The system provides tools to
make it relatively easy to write T programs that manipulate these perma-
nent objects.
A secondary goal of our work is to support distributed computing by
allowing multiple processors to have access to permanent objects. While
the system does not address all the issues associated with distributed
computing, we believe that the mechanisms provided can be effectively
used in the course of solving certain problems in a distributed way.
</bodyText>
<subsectionHeader confidence="0.5901925">
Schema-based Problem Solving
Daniel Martin Russell
</subsectionHeader>
<footnote confidence="0.327179666666667">
The University of Rochester Ph.D.
1985, 186 pages
Computer Science
DAI V46(04), SecB, pp1245
University Microfilms Order Number
ADG85-11910. 8510
</footnote>
<subsectionHeader confidence="0.942496666666667">
An Analysis of the Problem-Solving
Technique of a Talmudical Expert
Steven Ira Levenson
</subsectionHeader>
<footnote confidence="0.956387">
New York University Ph.D. 1984,
420 pages
Education, Curriculum and Instruction
DA! V46(01), SecA, pp62
University Microfilms Order Number
ADG85-05431. 8507
</footnote>
<bodyText confidence="0.999253871794871">
Much evidence supports the use of schemata as a basic element of
human problem solving. Yet, little work has been done to show how
schematic information can be represented, manipulated or used for
problem solving.
A schema maps goals to abstract action sequences. Schema-based
problem solving is the process of identifying appropriate schemata,
and then adapting that schematic knowledge to the particular circum-
stances of a problem. Schemata adapt to the problem environment in
three ways: (1) Multiple schema expansions are pursued simultaneously
and only the best expansion is selected for execution; (2) As a schema
expands, constraints defining and limiting future expansion are established.
Constraint satisfaction ensures that the plan will be internally consistent
and still attain the goal; (3) Schema components may be selectively deleted
or integrated in response to goal requirements or constraints set up during
problem solving. The three methods translate a selected schema into a
fully-developed plan.
However, problem solving in complex domains requires an ability to
switch rapidly and reliably between problem solving and error recovery.
Plan generation and execution are tightly interwoven to allow dynamic
error recovery through plan alteration and replanning. A fully-developed
problem solution never exists at any one time. Instead, the plan constantly
evolves in response to execution and problem solving requirements.
Because deviation from a plan due to inaccurate world models or incorrect
planning is so common, error recovery at many levels is seen to be a
normal, rather than exceptional, part of problem solving. Expanding alter-
nate plans in parallel and distributing control over many sites allows the
system to be responsive to error and quick in execution.
Methods for deriving plans from schemata and a problem solving philos-
ophy are demonstrated in a program, SHEM, which generates and executes
plans to assemble small block figures in a simulated world.
A theory of expert skill in the Brisker style of Talmudic study is pre-
sented. It identifies particular problem-solving skills an expert uses
in analyzing a Talmudical text. Within the framework of this theory,
a more explicit model of the analysis process was developed. The
model is based on a production system and describes the solution path
of the expert as recorded in verbal protocols. Content analysis of the
protocols was used as the research methodology. The implications
of the model are discussed for the training of Talmudical students
in general problem-solving skills by teaching them computer
</bodyText>
<page confidence="0.950753">
166 Computational Linguistics, Volume 12, Number 2, April-June 1986
</page>
<note confidence="0.68873">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.588728">
programming, for the differential model of Intelligent Computer Assisted
Instruction and for Talmudical pedagogy.
</bodyText>
<table confidence="0.965167842105263">
The Script Schema in Children&apos;s
Comprehension and Memory
Joel A. Seltzer
City University of New York
Ph.D. 1985, 120 pages
Education, Psychology
DAT V46(05), SecA, pp1232
University Microfilms Order Number
ADG85-15658. 8511
The Semantic Features of Text:
Their Interaction and Influence
on Comprehending
Karen M. Feathers
Indiana University ED.D. 1985,
786 pages
Education, Reading
DAI V46(04), SecA, pp937
University Microfilms Order Number
ADG85-07850. 8510
</table>
<bodyText confidence="0.99059125">
This study examined how script strength, prior scriptal knowledge,
and age differences affect children&apos;s comprehension and memory,
and relate to reading skill. A script schema represents stereotypical
action sequences of familiar events that are goal-oriented, for exam-
ple, going-to-a-restaurant. Script strength refers to a script&apos;s ability
to evoke a familiar temporal-causal sequence of events. Strong —
canonical — schemata facilitate story recall. Prior knowledge and
experience affect performance as measured by inference-making abil-
ity, recall, and errors in recall with words, sentences, and text. Skilled
readers spontaneously use strategies that facilitate comprehension and
recall.
A strong script was predicted to facilitate comprehension and recall of a
picture series. Prior scriptal knowledge was expected to facilitate perform-
ance, and older children were predicted to have higher comprehension and
recall than their younger counterparts.
The subjects were 139 second- and fourth-grade children. They were
shown a picture series evoking the script getting-ready-for-school-in-the-
morning. Within each grade, children were randomly assigned to one of
six conditions in which a script-header — or title — was presented before or
after exposure to the stimuli and varied by three levels of canonical
strength. The children were asked to generate a story as a measure of their
comprehension and were given traditional memory tasks to measure verbal
recall, visual recognition, and serial reconstruction of the pictures.
As predicted, the strong script-header was found to facilitate compre-
hension and serial reconstruction ability. Children in the weak script-head-
er condition produced more intrusions in their stories. Prior knowledge
facilitated only the ability to produce more detailed information. Recall
and recognition were not affected by the treatments. Fourth graders
performed better than second graders on most tasks. A modest relation-
ship between second grade task performance and reading ability was
noted.
It was concluded the children&apos;s level of information processing must be
consistent throughout a task to facilitate comprehension and recall. Scripts
that organize new information to fit the learner&apos;s knowledge base may be a
useful pedagogical tool.
Research in the last ten years has begun to focus on text as a
complete semantic unity. A variety of techniques for viewing text
from this new semantic perspective have been proposed. Research
using these techniques suggests the viability of approaching the read-
ing process from this perspective. However, because of materials and
procedures used and a focus on comprehension as measured by recall
and recognition, these studies leave unanswered the question of the
feasibility of applying the procedures to longer texts as well as the
more important question of how the semantic features interact and
influence the actual process of reading. This study investigated the
impact of semantic features of discourse on in-process reading behavior as
measured by the Reader Miscue Inventory. A 1300 word story was
analyzed using six different techniques (propositional analysis, macrostruc-
ture, story schema, cohesion analysis, propositional mapping, and concep-
tual chaining). Twenty fourth grade subjects read the story orally and
retold it. Cross tabulations, multiple regression analysis, and factor analy-
sis were used to consider the semantic features in relation to subjects&apos;
reading behavior. Results suggest that these features do influence reader
behavior, but that this influence is highly interactive in nature. That is,
Computational Linguistics, Volume 12, Number 2, April-June 1986 167
The FINITE STRING Newsletter Abstracts of Current Literature
reading behavior is related not to one semantic feature but to multiple and
complex patterns of features. These findings are significant since they
suggest a need to reconceptualize current views of language processing and
text evaluation.
</bodyText>
<figure confidence="0.969864111111111">
Reading, Writing, and Mutual
Knowledge
Gordon Philip Thomas
University of Minnesota Ph.D. 1985,
169 pages
Language, General
DAI V46(04), SecA, pp961
University Microfilms Order Number
ADG85-12082. 8510
An Operator-Argument Grammar of
Quantity Expressions
Michiko Kosaka
New York University Ph.D. 1984,
257 pages
Language, Linguistics
DAI V46(01), SecA, pp140
University Microfilms Order Number
ADG85-05504. 8507
</figure>
<bodyText confidence="0.999440096153846">
&amp;quot;Mutual Knowledge&amp;quot;, a topic in the philosophy of language, is know-
ledge that is known by at least two persons in any meaningful situ-
ation and known to be known by both those persons. Mutual
knowledge encompasses such issues in the philosophy of language as
how intention and convention contribute to meaning and where
meaning itself resides. These issues are important to composition
theory as well, for the field deals with the practical application of
how writers make meaning for their readers through texts. This
dissertation explains certain philosophical approaches to these issues,
while developing and modifying these concepts so that they can be applied
to the reading-writing situation. In addition it uses them to examine two
other theories in composition: Linda Flower and John Hayes&apos;s cognitive
process model of composing and E.D. Hirsch&apos;s views on &amp;quot;relative
readability&amp;quot; and their application to composition instruction.
The first chapter traces the history of the notion of mutual knowledge,
explaining its significance to H.P. Grice&apos;s general theory of meaning and
arguing against the infinite regress that some philosophers have claimed
occurs with mutual knowledge. The chapter also defines and distinguishes
three types of mutual knowledge necessary to the reading-writing situation:
&amp;quot;world knowledge&amp;quot;, &amp;quot;knowledge of conventions&amp;quot;, and &amp;quot;knowledge of
language&amp;quot;. Finally, it amplifies the concept of &amp;quot;world knowledge&amp;quot;, giving
examples of how it can work for or against a writer.
The second chapter explores the nature of conventions in language,
building on the work of David K. Lewis. It argues against confusing the
forms that conventions assume with the conventions themselves, which are
formal regularities used to convey writers&apos; intentions to particular audi-
ences. The third chapter develops a theory of meaning for the reading-
writing situation, by applying the speech-act theories of J.L. Austin and of
Kent Bach and Robert M. Harnish.
Chapter 4 incorporates the notion of mutual knowledge into Flower and
Hayes&apos;s model, elaborating on their concepts of meaning and the long-term
memory. Chapter 5 uses Hirsch&apos;s 1977 work as an example to argue
against emphasizing the formal properties of texts in the teaching of
composition. It argues instead that successful writing is a communicative
intention, conveyed through mutually known, linguistic conventions.
Within the language of quantitative description, exemplified by the
use of quantity expressions in portions of the scientific literature, this
thesis provides a grammatical analysis of quantity expressions and fits
them into an overall grammatical analysis of English sentences. It
provides a set of word classes, and the grammatical relations which
relate these classes to each other and to the overall grammatical
structures of English sentences.
The set of quantity expressions dealt with in this study includes
numbers, numerical expressions, universal quantifiers, quantity adjec-
tives, quantity verbs, comparatives, superlatives, and others.
This thesis contains and validates two major hypotheses with respect to
the treatment of quantity expressions. One hypothesis is the existence of a
subgrammar of quantity expressions. The second hypothesis deals with the
possibility of the regularization of the surface grammar by positing implicit
elements in the underlying representation.
The third major contribution of this study comes from a finding that in
some cases the choice of analysis within the available alternatives could be
</bodyText>
<page confidence="0.934588">
168 Computational Linguistics, Volume 12, Number 2, April-June 1986
</page>
<note confidence="0.76614">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.603696">
identified with the occurrence of different classes of verbs operating on a
particular class of quantity expressions.
</bodyText>
<table confidence="0.913556333333333">
A Speech Act Theory Based Inter-
pretation Model for Written Texts
Takashi Manabe
The University of Texas at Arlington
Ph.D. 1984, 405 pages
Language, Linguistics
DAI V46(05), SecA, pp1265
University Microfilms Order Number
ADG85-15329. 8511
The Modal Verbs: Univocal Lexical
Items
Heather McCallum-Bayliss
Georgetown University Ph.D. 1984,
267 pages
Language, Linguistics
DAI V46(05), SecA, pp1265
University Microfilms Order Number
ADG85-1569I. 8511
</table>
<bodyText confidence="0.992097436619718">
In recent years, so called Speech Act Theory (a branch of the philos-
ophy of language) has focused on such concepts as &apos;intention&apos; (or
&apos;intentionality&apos;), &apos;interaction&apos; and &apos;comprehension&apos;. The author
believes that this theory gives the theoretical basis for an interpreta-
tion model which is developed in this study.
After human speech activities are placed in the larger context of
human actions, a model which explains the performing process of
speech acts is presented. In this model, four levels of intentionality
are posited: utterance intent, propositional intent, illocutionary
intent, and perlocutionary intent. These levels of intentionality are insepa-
rable from each other and operate simultaneously in performing speech
acts. Speech acts are categorized according to their illocutionary intents.
Six major categories are suggested: Representatives, Directives, Commis-
sives, Expressives, Declarations, and Responsives.
The model of the speaker presented above is completed when the model
of the hearer (or interpreter) is added to it. The combined model, which
can be called a cyclic or interactive model, is developed. Corresponding to
the four levels of intentions on the part of the speaker, four levels of
effects are posited in order to explain the &apos;comprehension&apos; process of the
hearer. These four levels of effects are: utterance effect, propositional
effect, illocutionary effect, and perlocutionary effect. It is contended that
only after this stage of comprehension can the hearer make a legitimate
response to the speaker by his &apos;decision&apos; and &apos;response action&apos;. The proc-
ess of comprehension, then, is explained extensively, paying particular
attention to the concepts of &apos;context&apos; and &apos;inference&apos;.
The main contributions of the study are: (i) the resolutions of some of
the lingering theoretical problems in Speech Act Theory; (ii) the develop-
ment of an interactive model of speech acts, which is applied to text analy-
sis; and (iii) the development of such concepts as &apos;author&apos;s intent structure&apos;
and &apos;referential intent structure&apos; for the application of Speech Act Theory
to text analysis.
This study is about the modal verbs of American English: can, may,
could, might, must, will, shall, would and should. Many traditional
analyses consider the modals ambiguous between the so-called epis-
temic and root interpretations (e.g., may of &apos;possibility&apos; and &apos;per-
mission&apos;, respectively). This study demonstrates, however, that the modal
verbs are not ambiguous but systematically polysemous, a type of
non-ambiguity. Each modal has a singulary semantic representa-
tion from which the various modal interpretations are contextually
derived. These interpretations depend on two classes of world, not lin-
guistic, knowledge; as such, they need not be specified in the linguistic
description of the language but will be accounted for in the pragmatics of
the language.
Each modal is a univocal marker of probability. Together, they consti-
tute a class of items that exhibits semantic gradience and manifest both the
semantic and conversational characteristics typical of quantitative scales.
Individually, the modal verbs are distinguished in part by different degrees
of probability. However, several items can represent the same notion of
probability (e.g., can, may, could and might all correspond to the concept
of &apos;possibility&apos;), so other features must distinguish among items that are
apparent semantic equivalents. This analysis demonstrates that the
concept of grounds and the semantics of the past marker are what differ-
entiate parallel items.
Computational Linguistics, Volume 12, Number 2, April-June 1986 169
The FINITE STRING Newsletter Abstracts of Current Literature
The notion of grounds is a semantic concept that signals that the speak-
er has significant, relevant information available to him that serves as testi-
mony to the validity of his statement. Such knowledge accords the speaker
the status of &amp;quot;expert&amp;quot;. This analysis also shows that the past marker has
retained its function of signalling various types of remoteness in the case of
the modal verbs. The past-marked modals (e.g., could) are therefore not
independent semantic units (as often claimed) but are composite semantic
entities.
Probability, grounds, and the semantics of the past marker explain the
great variety of modal interpretation, the difficulty in isolating a specific
range of applicability of the modals and the contrastive behavior of seem-
ingly parallel items.
It is from these univocal semantic representations that the interpreta-
tions that have been attributed to the modal verbs are contextually derived.
This study demonstrates that these interpretations are reasonable and
predictable from the univocal semantic representations proposed.
</bodyText>
<figure confidence="0.6871736">
The Pronoun and the Topic of
Discourse: a Functional Perspective
on Text
Mary Ng En Tzu
The University of Wisconsin - Madison
Ph.D. 1984, 108 pages
Language, Linguistics
DAI V46(02), SecA, pp413
University Microfilms Order Number
ADG85-00839. 8508
Temporal Inferences in Compu-
tational Linguistic Information
Processing
Klaus Karl Obermeier
The Ohio State University Ph.D. 1984,
190 pages
Language, Linguistics
DAI V46(01), SecA, pp142
University Microfilms Order Number
ADG85-04060. 8507
</figure>
<bodyText confidence="0.9999269">
This thesis proposes to show through the presentation of &apos;logical
proofs&apos; that the formal, surface concatenation of pronouns in a
discourse/text is patterned to reveal the underlying continuity in a
particular semantic category — the topic of discourse. In so doing, a
new approach for finding the topic in discourse/text is presented —
specifically through the analysis of the pattern of pronominalization
in discourse/text.
In the course of this thesis, we will show how a whole theory on
discourse/text grammar must take into account both its formal and
structural level of representation, as well as its functional and seman-
tic level of representation. This is explicated in terms of its organizational
structure — its structural relationships (cohesive and staging relationships)
and its semantic content (the topic).
Then, through investigating the nature and the &apos;capabilities&apos; of the
pronoun, one of many phoric referential forms in discourse/text, it will be
demonstrated that it is both a referential element that explicates the
&apos;wholeness&apos; in text organization as well as a language form that represents
an underlying functional and semantic category — the topic.
Finally, through a look at the actual use of the pronoun in
discourse/text, it will be argued that the surface concatenation of pronomi-
nals in a discourse/text is patterned to reveal the underlying continuity in
its topic.
This dissertation aims for an integration of insights gained from
linguistic, psychological, and Artificial Intelligence based research to
provide a pragmatic theory and mental model of how natural language
processing and temporal inferences can be explained within the
framework of computational information processing. A pragmatic theory
focuses on the information from the context (e.g., co-text, text-type,
and audience) to explain linguistic behavior. A mental model
provides an internal representation of the state of affairs that are
are described in a given sentence.
The objectives of this research are twofold, whereby the computer
program is meant to be a particular implementation of a general natural
language processing system which could be used for a variety of domains.
The first objective is to show how an integration of linguistic and extra-lin-
guistic knowledge achieves a form of comprehension, where comprehen-
sion is characterized as a conversion of information based on knowledge
from one representation into another. The second objective is to show
that such a procedural approach is a basis for an event-based theory for
temporal information processing.
</bodyText>
<page confidence="0.8902">
170 Computational Linguistics, Volume 12, Number 2, April-June 1986
</page>
<bodyText confidence="0.936094714285714">
The FINITE STRING Newsletter Abstracts of Current Literature
The computer program (implemented in ELISP on EFRL on a DEC
20/60) performs the following tasks: (1) parse a text from a medical jour-
nal while using linguistic and extra-linguistic knowledge; (2) map the
parsed linguistic structure into an event-representation; (3) draw temporal
and factual inferences within the domain of liver diseases; (4) create and
update a data base containing the pertinent information about a patient.
</bodyText>
<figure confidence="0.992950380952381">
Computer-Assisted Instruction for
Learners of English as a Second
Language: Design and Field-Test of
a Program to Review Word Order
in a Sentence
Andrew Martin Soria
Georgetown University Ph.D. 1984,
250 pages
Language, Linguistics
DAI V46(05), SecA, pp1266
University Microfilms Order Number
ADG85-15690. 8511
The Nature of Subjects, Topics and
Agents: a Cognitive Explanation
Jeanne Hillecidena Van Oosten
University of California, Berkeley Ph.D.
1984, 343 pages
Language, Linguistics
DAI V46(04), SecA, pp969
University Microfilms Order Number
ADG85-13027. 8510
</figure>
<bodyText confidence="0.997202216216216">
The best existing computer-assisted language learning programs take
advantage of the computer&apos;s motivational sound, graphics, and game-
like atmosphere. They are highly interactive and individualized, and
keep track of scores and grades. However, these programs use little
or no linguistic knowledge in responding to a student.
In this dissertation I have written a linguistically sensitive micro-
computer program in BASIC. The intelligence in the program simu-
lates an English instructor&apos;s knowledge of sentence structure over a
precisely defined range of words. The program presents a list of words
to the student, who arranges them to create a sentence. Words
may be used more than once. The program&apos;s expert knowledge
is stored in the form of lists. The program parses the sentence by
matching it against the lists, indicating correct noun phrases, prepositional
phrases, and relative clauses to the student. It also shows errors of omis-
sion and adjective order. It then gives an overall judgement on the gram-
matical correctness of the sentence, based on the word order.
The program offers collaborative guidance by judging the sentence, and
advising the student. This linguistic component of the program adds a
pedagogically significant dimension to answer processing. The student is
free to experiment and review word order within a range of sentence struc-
tures. Field-testing indicates that students enjoy the freedom of composing
their own sentences. Although the program accurately recognizes an
almost infinite set of correct sentences, students requested a broader
vocabulary to work with, and more available sentence structures. These
results confirm the educational need for this type of program.
This dissertation describes the program and the results of field-testing.
The first three chapters discuss the context, contributions, and evaluation
of the program. The fourth chapter suggests future directions. The
program is divided into separate modules which may be changed independ-
ently. They may also be extracted and applied to other problems involving
specific sequences.
The last chapter is useful in adapting the program to meet new needs. It
is a technical discussion of how the program operates, discussing and
cross-referencing the variables for each module. The program listing for
the Apple II microcomputer is included in the appendix.
English subjects form a category that is both meaning-based and
grammaticized. This position, demonstrated in this dissertation by an
examination of the uses of seven constructions of English — existen-
tials, it-clefts, two types of property-factoring, the Tough-Construc-
tion, the Patient-Subject Construction and passive — is a departure
from the transformational tradition, Relational Grammar and Role-
and-Reference Grammar. The transformational tradition holds that
grammatical relations are to be defined purely structurally and
syntactically; Relational Grammar holds that subjects are totally
grammaticized (primitive), and Role-and-Reference Grammar maintains
that subjects are totally meaning-based (derived from pragmatic and
semantic notions) and not grammaticized at all.
The English subject is a category with prototype structure; the central
members of the prototype are both prototypical agents and prototypical
topics. Prototypical subjects are found in prototypical basic sentences
Computational Linguistics, Volume 12, Number 2, April-June 1986 171
The FINITE STRING Newsletter Abstracts of Current Literature
(defined in this dissertation); each special construction, as tested by the
seven special constructions mentioned above, has an individual meaning
for its subject which deviates from but is motivated by the meaning of the
subjects of basic sentences. (Both basic sentences and special
constructions can themselves have deviations from the prototype.)
I also redefine the notion of &amp;quot;topic&amp;quot; in terms of a prototype containing
such pragmatic notions as focus of attention, old information, focus of
interest, perspective, salience, and aboutness. Prototypical topics have all
these features; nonprototypical topics do not have all of them. These prag-
matic features have semantic reflexes like referentiality and definiteness
(and a strong tendency towards agency). I distinguish between discourse
and sentence topics; the former are in the normal case layered within a
single discourse. I also distinguish between superordinate, basic-level, and
subordinate topics. Superordinate topics are semantic structures like sche-
mata or scenes; basic-level topics are individual participants in a superordi-
nate topic (typically human); and subordinate topics are aspects or parts of
basic-level topics. Since basic-level topics are prototypical sentence topics,
it is this level that has typically been identified as &amp;quot;topic&amp;quot; in the past.
I corroborate and expand the notion that the category agent also has
prototype structure, and propose, on the basis of the special constructions
investigated, that the notion of &amp;quot;primary responsibility&amp;quot; rather than inten-
tionality or volition is the central characteristic of the category of agents.
</bodyText>
<figure confidence="0.991559705882353">
Representational Semantics
Barry E. Brown
The University of Rochester Ph.D.
1985, 239 pages
Philosophy
DAI V46(04), SecA, pp997
University Microfilms Order Number
ADG85-11889. 8510
Learning to Understand Speech
Sounds: a Theory and Model
Gary L. Bradshaw
Carnegie-Mellon University Ph.D.
1984, 123 pages
DAI V46(05), SecB, pp1720
Psychology, Experimental
University Microfilms Order Number
ADG85-13677. 8511
</figure>
<bodyText confidence="0.998263852941177">
I present and defend an original semantic theory which assigns repre-
sentatives to expressions, in addition to referents. The theory is
nominalistic — i.e., it avoids reference to possible worlds and other
abstract entities — and yet is strong enough, I claim, to serve as a
theory of meaning. More precisely, it provides a means for interpret-
ing, in a nominalistically acceptable manner, the non-extensional
linguistic contexts in which the &amp;quot;meanings&amp;quot; of expressions are
supposed to play a semantic role.
The intuitive ancestry of the theory can be traced back to an analysis of
meaning first proposed by Nelson Goodman and later expanded by Rolf
Eberle. I construct a precise formal semantics which embodies the basic
ideas of these earlier proposals, and apply this theory to the interpretation
of a language which contains one primitive non-extensional predicate —
about. In addition, I furnish a rigorous axiomatic treatment of this
language, and demonstrate formally the soundness and completeness of
this axiomatic theory relative to the semantics.
Theories of human speech perception have emphasized the role of
innate feature detectors in speech comprehension. Empirical
evidence suggests that theories based on specialized feature detectors
are wrong, and that human listeners improve in their ability to identi-
fy the basic sounds of their language. A learning theory of speech
perception is proposed to account for the evidence. To test the theo-
ry, a computer simulation, NEXUS, was created. When provided
with a simple vocabulary of the names of the letters of the alphabet,
NEXUS was able to create descriptions of all words, identify the
similarities between words, and simplify the network by eliminating redun-
dant information. The resulting word network was used to classify new
instances of speech. Performance of NEXUS was superior to that of a
state-of-the-art speech recognition system, Cicada, on both speakers test-
ed. NEXUS serves as a sufficiency proof of the learning theory, although
the lack of detailed learning data precludes stronger comparisons with
human performance. NEXUS also demonstrates that learning heuristics
can be very useful in building computer systems to perform perceptual
tasks, such as speech recognition or vision. These heuristics do not require
</bodyText>
<page confidence="0.928026">
172 Computational Linguistics, Volume 12, Number 2, April-June 1986
</page>
<note confidence="0.58864">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<reference confidence="0.906888045454545">
statistical assumptions about the form of the distribution underlying the
data.
Referential Choice and Focus of
Attention in Narratives
Ellen Palmer Francik
Stanford University Ph.D. 1985,
110 pages
Psychology, Experimental
DAI V46(04), SecB, pp1363
University Microfilms Order Number
ADG85-11308. 8510
Interlingual Semantic Facilitation:
Evidence for a Common Represen-
tational System in the Bilingual
Lexicon
Mario Rey
Florida Atlantic University M.A. 1985,
104 pages
Psychology, Experimental
MM V23(03), pp405
University Microfilms Order Number
ADGI3-25149. 8510
</reference>
<bodyText confidence="0.99720121875">
English forms for definite reference range from descriptions (the
artist) to names (Columbine) to reduced forms carrying little infor-
mation (such as the pronoun she). Speakers can choose more or less
explicit forms depending upon what other information in the
discourse will help the addressee to determine the referent. One
important source of information is focus, the importance of a referent
at a particular moment in the discourse.
This experiment used simple narratives to compare speakers&apos; use
of three sources of information: discourse organization, lexical cues
(e.g. gender), and recency of mention. Forty university students were each
given a story (a 10-page picture booklet) with two equally active charac-
ters, and were asked to tell it from one character&apos;s viewpoint. That is,
speakers organized their narratives around a protagonist. Since protagon-
ists are important for story planning, comprehension, and recall, they
should tend to become focal and be referred to with reduced forms
(pronouns). The availability of gender information was manipulated by
creating two versions of each story, one with same-sex characters and one
with opposite-sex characters. Recency of mention was determined from
each speaker&apos;s narrative.
Speakers mark the protagonist&apos;s importance in several ways: they
mention the protagonist more often, foreground the protagonist syntac-
tically, and pronominalize the protagonist more readily in a variety of
circumstances. Recency of mention also leads speakers to pronominalize
the character. But when recency is taken into account, speakers still pron-
ominalize protagonists more frequently. The cue of gender (when the two
characters are of opposite sexes) is most useful when speakers switch
reference from one character to another. Yet though gender cues make
pronouns unambiguous for both characters, protagonists are still pronomi-
nalized more often. The impact of protagonist status on speakers&apos; refer-
ences and the subtle interaction of these three sources of information lead
us to reject several simple models of referential choice.
(No abstract.)
</bodyText>
<page confidence="0.283907">
Computational Linguistics, Volume 12, Number 2, April-June 1986 173
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.8325905">ABSTRACTS OF CURRENT LITERATURE following Technical Reports cover work done under the described on page 136. Reports are</title>
<author confidence="0.57187">available from</author>
<affiliation confidence="0.9994675">Department of Computer and Information Service University of Pennsylvania</affiliation>
<address confidence="0.998267">Philadelphia, PA 19104</address>
<title confidence="0.979387666666667">Interactive Classification: A Technique for the Acquisition and Maintenance of Knowledge Bases</title>
<author confidence="0.999893">Tim Fink</author>
<pubnum confidence="0.967734">MS-CIS-84-17</pubnum>
<title confidence="0.984327666666667">Correcting Object-Related Misconceptions: How Should The System Respond?</title>
<author confidence="0.999434">Kathleen F McCoy</author>
<pubnum confidence="0.963937">MS-CIS-84-18</pubnum>
<title confidence="0.996704">Default Reasoning in Interaction</title>
<author confidence="0.987545">Aravind Joshi</author>
<author confidence="0.987545">Bonnie Webber</author>
<author confidence="0.987545">Ralph Weischedel</author>
<pubnum confidence="0.938338">MS-CIS-84-58</pubnum>
<title confidence="0.999646">Preventing False Inferences</title>
<author confidence="0.98424">Aravind Joshi</author>
<author confidence="0.98424">Bonnie Webber</author>
<author confidence="0.98424">Ralph M Weischedel</author>
<pubnum confidence="0.542766">MS-CIS-84-59</pubnum>
<abstract confidence="0.99143916">The practical application of frame-based knowledge-based systems, such as in expert systems, requires the maintenance of potentially very large amounts of declarative knowledge stored in their knowledge bases (ICBs). As a KB grows in size and complexity, it becomes more difficult to maintain and extend. Even someone who is familiar with the representation and the contents of the existing KB may introduce inconsistencies and errors whenever an addition or modification is made. This paper describes an approach to this problem based on a tool called an interactive classifier. An interactive classifier uses the contents of the existing KB and knowledge about its representation to assist the person who is maintaining the KB in describing new KB objects. The interactive classifier will identify the appropriate taxonomic location for the newly described object and add it to the KB. The new object is allowed to be a generalization of existing KB objects, enabling the system to learn more about existing objects. The ideas have been tested in a system call KuBIC, for Knowledge Base Interactive Classifier, and are being extended to a more complete knowledge representation language. This paper describes a computational method for correcting users&apos; misconceptions concerning the objects modeled by a computer system. The method involves classifying object-related misconceptions according to the knowledge-base feature involved in the incorrect information. For each resulting class, sub-types are identified, according to the structure of the knowledge base, which indicate what information may be supporting the misconception and, therefore, what information to include in the response. Such a characterization, along with a model of what the user knows, enables the system to reason in a domain-independent way about how best to correct the user. Nonmonotonic reasoning is usually studied in the context of a logical system in its own right or as reasoning done by an agent, in which the agent reasons about the world from partial information and hence may draw conclusions unsupported by traditional logic. The main point of departure here is looking at nonmonotonic reasoning in the context of interacting with another agent. This information is partial, in that the other agent neither will nor can make everything explicit. Knowing this, the agent may attempt to derive more from the interaction than what has been made explicit, by reasoning by default about what has been made explicit (often by contrast with what he assumes would have been made explicit, were something else the case). Thus there can be rules for default reasoning that are operative in the interactive situation (&amp;quot;interactional defaults&amp;quot;) that are not operative with only a single agent. In cooperative man-machine interaction, it is taken as necessary that a system truthfully and informatively respond to a user&apos;s question. It is not, however, sufficient. In particular, if the system has reason to believe that its planned response might lead the user to draw an inference that it knows to be false, then it must block it by modifying or adding to its response. The problem is that a system neither can nor should explore all conclusions Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature a user might possibly draw: its reasoning must be constrained in some systematic and well motivated way.</abstract>
<title confidence="0.994092">Living Up To Expectations: Computing Expert Responses</title>
<author confidence="0.992716">Aravind Joshi</author>
<author confidence="0.992716">Bonnie Webber</author>
<author confidence="0.992716">Ralph Weischedel</author>
<pubnum confidence="0.875773">MS-CIS-84-60</pubnum>
<title confidence="0.996683">A Modal Temporal Logic for Reasoning about Changing Databases with Applications to Natural Language Question Answering</title>
<author confidence="0.999409">Eric Mays</author>
<author confidence="0.999409">Aravind Joshi</author>
<author confidence="0.999409">Bonnie Webber</author>
<pubnum confidence="0.973933">MS-CIS-85-01</pubnum>
<title confidence="0.997883">Explaining Concepts in Expert Systems:</title>
<author confidence="0.999648">Robert Rubinoff</author>
<address confidence="0.420256">MS-CIS-85-06, LINC LAB 02</address>
<title confidence="0.9759015">Linguistic Relevance of Tree Adjoining Grammars</title>
<author confidence="0.999966">Anthony S Kroch</author>
<affiliation confidence="0.997135">Department of Linguistics</affiliation>
<author confidence="0.998389">Aravind K Joshi</author>
<affiliation confidence="0.9840025">Department of Computer and Information Science</affiliation>
<address confidence="0.899706">MS-CIS-85-16, LINC LAB 03</address>
<title confidence="0.987115">A Computational Logic Approach to Syntax and Semantics</title>
<author confidence="0.994056">Dale A Miller</author>
<author confidence="0.994056">Gopalan Nadathur</author>
<abstract confidence="0.978531461538462">MS-CIS-85 - 17 In cooperative man-machine interaction, it is necessary but not sufficient for a system to respond truthfully and informatively to a user&apos;s question. In particular, if the system has reason to believe that its planned response might mislead the user, then it must block that conclusion by modifying its response. This paper focuses on identifying and avoiding potentially misleading responses by acknowledging types of &amp;quot;informing behavior&amp;quot; usually expected of an expert. We attempt to give a formal account of several types of assertions that should be included in response to questions concerning the achievement of some goal (in addition to the simple answer), lest the questioner otherwise be misled. A database that models a changing world must evolve in correspondence to the world. Previous work on natural language question answering systems for databases has largely ignored the issues which arise when the database is viewed as a dynamic (rather than a static) object. We investigate the question answering behaviors that become possible with the ability to represent and reason about the possible evolution of a database. These behaviors include offering to monitor for a possible future state of the database as an indirect response to a query, and directly answering questions about prior and future possibility. We apply a propositional modal temporal logic that captures possibility and temporality to represent and reason about dynamic databases, and present a sound axiomatization and proof, and proof procedure. Existing expert systems provide limited explanatory ability. They can explain the specific reasoning the system uses, but if the user is confused about the concepts and terms the system is using, no help is available. The allows users to ask for explanations of specific concepts. The system generates the explanations by examining the rule base, selecting rules that are relevant to the concept asked about. These rules are then turned into English by various simple translation schemes and presented to the user, providing an explanation of how the concept is used by the system. In this paper the linguistic significance of the Tree Adjoining Grammar been investigated. An important property of that it defines a constrained theory of syntactic embedding, one requiring that embedded structures be composed out of elementary structures in a fixed way, and one that forces co-occurrence relations between elements that are separated in surface constituent structures to be stated broadly as constraints on elementary trees in which those elements are co-present. extra generative power of context-free grammar emerges as a corollary of factoring recursion and co-occurrence relations. The linguistic details specifically discussed are raising constructions, passive, It is well known that higher-order logics are very expensive, and for this reason have been used to represent many problems in mathematics and theoretical computer science. In the latter domain, higher-order logics are often used to describe the semantics of first-order logics, natural languages, or programs, since the formalization of such semantics needs a recourse to quantification over the domain of functions and sets. In these settings, higher-order logic has generally been limited to a descriptive role. Once the formalization is made, little has been made of it computationally, because there is abundant evidence that theorem proving in high- Linguistics, Volume 12, Number 2, April-June 1986</abstract>
<title confidence="0.998343333333333">The FINITE STRING Newsletter Abstracts of Current Literature The Role of Perspective in Responding to Property Misconceptions</title>
<author confidence="0.999849">Kathleen F McCoy</author>
<date confidence="0.783724">MS-CIS-85-31, May 1985</date>
<title confidence="0.9970125">Some Computational Properties of Tree Adjoining Grammars</title>
<author confidence="0.999511">K Vijay-Shankar</author>
<author confidence="0.999511">A Joshi</author>
<pubnum confidence="0.944655">MS-CIS-85-07</pubnum>
<title confidence="0.992801">Grammar, Phrase Structure</title>
<author confidence="0.999877">Aravind K Joshi</author>
<pubnum confidence="0.955172">MS-CIS-85-45</pubnum>
<title confidence="0.9860595">Question, Answer and Responses: Interacting with Knowledge Base Systems</title>
<author confidence="0.99947">Bonnie Lynn Webber</author>
<address confidence="0.614252">MS-CIS-85-50, LINC LAB 04</address>
<title confidence="0.97267">A Theory of Scalar Implicature</title>
<author confidence="0.989849">Julia Bell Hirschberg</author>
<pubnum confidence="0.62272">MS-CIS-85-56</pubnum>
<abstract confidence="0.989807408163265">er-order logics is very difficult. In this paper we look at a sublogic of a particular higher-order logic that is derived from Church&apos;s Theory of Types, and examine its representational power and its computational tractability. This sublogic can also be described as Horn clauses logic extended with quantifications over function variables and A-contraction. We shall present a sound and complete theorem prover for this logic, which uses higher-order unification and may be described as an extension of a unification procedure for the typed A-calculus. There are at least three ways in which this logic is different from the first-order logic it generalizes. First it possesses function variables which can be instantiated with A-terms and evaluated through A-contractions. This provides the logic with a new source of computation. Second, since A-terms do not have most general unifiers, the process of finding appropriate unifiers must branch, and hence involves real search. This facet provides a new source of nondeterminism in specifying computations. Finally, this logic can directly encode firstorder logic in its term structure and can manipulate such terms in logically meaningful ways. We illustrate this with examples taken from knowledge representation and natural language parsing. In order to adequately respond to misconceptions involving an object&apos;s properties, we must have a context-sensitive method for determining object similarity. Such a method is introduced here. Some of the necessary contextual information is captured by a new notion of object perspective. It is shown how object perspective can be used to account for different responses to a given misconception in different contexts. Tree Adjoining Grammar (TAG) is a formalism for natural language gram- Some of the basic notions of introduced in Joshi, Levy, and Takahashi (1975) and by Joshi (1983). A detailed investigation of the relevance of been carried out in Kroch and Joshi (1985). In this paper, we describe some new results for TAGS, especially in following areas: (1) parsing complexity of (2) some closure for TAGs, (3) the relationship to Head grammars. Phrase-structure trees (phrase-markers) provide structural descriptions for sentences. Phrase-structure trees can be generated by phrase-structure grammars. Phrase-structure trees can be shown to be appropriate to characterize structural descriptions for sentences, including those aspects which are usually characterized by transformational grammars, by making certain to increasing their power, or by generating them from elementary trees (phrase-markers) by a suitable rule of composition, increasing the power only mildly beyond that of CFGs. Structural descriptions provided by phrase-structure trees are used explicitly or implicitly in natural language processing systems. The purpose of this chapter is to examine the character of informationseeking interactions between a user and a knowledge base system (KBS). In doing so, I advocate that a clear distinction be made between an answer to a question and a response. The chapter characterizes questions, answers, and responses, the role they play in effective information interchanges, and what is involved in facilitating such interactions between user (No abstract.) Relationship Between Tree Adjoining Adjoining Grammars (TAG) and Head Grammars were intro-</abstract>
<note confidence="0.361658">Linguistics, Volume 12, Number 2, April-June 1986</note>
<title confidence="0.998682">The FINITE STRING Newsletter Abstracts of Current Literature Grammars and Head Grammars</title>
<author confidence="0.9974165">K Vijay-Shanker</author>
<author confidence="0.9974165">David J Weir</author>
<author confidence="0.9974165">Aravind K Joshi</author>
<pubnum confidence="0.505118">MS-CIS-86-01, LINC LAB 06</pubnum>
<title confidence="0.921734">Natural Language Interactions with Artificial Experts</title>
<author confidence="0.9979335">Tim Finin</author>
<author confidence="0.9979335">Aravind K Jashi</author>
<author confidence="0.9979335">Bonnie Lynn Webber</author>
<address confidence="0.58774">MS-CIS-86-16, LINC LAB 08</address>
<title confidence="0.996226">Higher-Order Logic Programming</title>
<author confidence="0.932766">Dale A Miller</author>
<author confidence="0.932766">Gopalan Nadathur</author>
<title confidence="0.420551">Some Uses of Higher-Order Logic in Computational Linguistics</title>
<author confidence="0.904596">Dale A Miller</author>
<author confidence="0.904596">Gopalan Nadathur</author>
<pubnum confidence="0.457254">MS-CIS-86-31, LINC LAB 08</pubnum>
<title confidence="0.9950375">Some Aspects Of Default Reasoning in Interactive Discourse</title>
<author confidence="0.999198">Aravind K Jashi</author>
<author confidence="0.999198">Bonnie L Webber</author>
<author confidence="0.999198">Ralph M Weischedel</author>
<abstract confidence="0.999259689655173">In cooperative interaction, it is taken and informatively respond to a user&apos;s cient. In particular, if the system has response might lead the user to draw as necessary that a system truthfully question. It is not, however, suffireason to believe that its planned an inference that it knows to be false, duced to capture certain structural properties of natural languages. These formalisms, which were developed independently, appear to be quite different notationally. In this paper we discuss the formal relationship the class of languages generated TAGs (TAL) and the class of languages generated by HGs (HL). In particular, we show that HLs are included in TALs and that TAGs are equivalent to a modification of HGs called Modified Head Grammars (MHG). The inclusion of MHL in HL, and thus the equivalence of HGs and TAGs, in the most general case remains to be established. We show that this relationship is very close both linguistically and formally, the difference hinging on the status of heads of empty strings and whether one deals with heads directly or with the left and right wrapping positions around the head. The aim of this paper is to justify why Natural Language NLP interaction, of a very rich functionality, is critical to the effective use of Expert Systems and to describe what is needed and what has been done to support such interaction. Interactive functions discussed here include defining terms, paraphrasing, correcting misconceptions, avoiding misconceptions and modifying questions. In this paper we consider the problem of extending Prolog to include predicate and function variables and typed A-terms. For this purpose, we use a higher-order logic to describe a generalization to first-order Horn clauses. We show that this extension possesses certain desirable computational properties. Specifically, we show that the familiar operational and least fixpoint semantics can be given to these clauses. A language, XProlog, that is based on this generalization is then presented, and several examples of its use are provided. We also discuss an interpreter for this language in which new sources of branching and backtracking must be accommodated. An experimental interpreter has been constructed for the language, and all the examples in this paper have been tested using it. Consideration of the question of meaning in the framework of linguistics often requires an allusion to sets and other higher-order notions. The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems that are either based on first-order logic or that use mechanisms whose formal justifications are to be provided after the fact. In this paper we shall consider the use of a higher-order logic for this task. We first present a version of definite clauses (positive Horn clauses) that is based on this logic. Predicate and function variables may occur in such clauses; the terms in the language are the typed A-terms. Such term structures have a richness that may be exploited in representing meanings. We also describe a higher-order logic programming language, called XProlog, which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter. A virtue of this language is that it is possible to write programs in it that integrate syntactic and semantic analyses into one computational paradigm. This is to be contrasted with the more common practice of using two entirely different computation paradigms, such as DCGs or ATNs for parsing and frames or semantic nets for semantic processing. We illustrate such an integration in this language by considering a simple example, and we claim that its use makes the task of providing formal justifications for the computations specified much more direct.</abstract>
<note confidence="0.660397">Linguistics, Volume 12, Number 2, April-June 1986</note>
<title confidence="0.997983">The FINITE STRING Newsletter Abstracts of Current Literature GUMS,: A General User Modeling System</title>
<author confidence="0.999927">Tim Fin in</author>
<author confidence="0.999927">David Drager</author>
<pubnum confidence="0.965613">MS-CIS-86-35</pubnum>
<title confidence="0.993642">Breaking the Primitive Concept Barrier</title>
<author confidence="0.998433">Robert Kass</author>
<author confidence="0.998433">Ron Katriel</author>
<author confidence="0.998433">Tim Finin</author>
<pubnum confidence="0.488874">MS-CIS-86-36</pubnum>
<abstract confidence="0.977972897435897">then it must block it by modifying or adding to its response. In this paper we investigate several aspects of such reasoning in interactive discourse. This paper describes the construction of a MUMBLE-based (McDonald 1983) tactical component for the TEXT text generation system (McKeown 1985). This new component, which produces fluent English sentences from the sequence of structured message units output from TEXT&apos;s strategic component, has produced a 60-fold speed-up in sentence production. Adapting MUMBLE required work on each of the three parts of the MUMBLE framework: the interpreter, the grammar, and the dictionary. It also provided some insight into the generation process and the consequences of MUMBLE&apos;s commitment to a deterministic model. This paper describes a general architecture of a domain independent system for building and maintaining long term models of individual users. The user modeling system is intended to provide a well-defined set of services for an application system which is interacting with various users and has a need to build and maintain models of them. As the application system interacts with a user, it can acquire knowledge of him and pass that knowledge on to the user model maintenance system for incorporation. We describe a prototype general user modeling system we have implemented in Prolog. This system satisfies some of the desirable characteristics we discuss. Building and maintaining a large knowledge base of general information requires a knowledge representation system with precise semahtics and an easy knowledge acquisition procedure. Systems such as KL-ONE meet these criteria by using a classifier to install new concepts into a taxonomic structure. These systems use a formal notion of a definition for concepts. Unfortunately, many concepts do not seem to have such precise definitions, and end up represented as primitive concepts. Primitive concepts form a barrier to classification, forcing the user to manually classify a new concept with respect to all primitive concepts in the knowledge base. We propose an extension to KL-ONE that retains its soundness and greatly reduces the burden on the user during knowledge acquisition. This extension consists of adding an explicit definitional component to concepts and relaxing the strictness of concept definitions themselves. The relaxed definition reduces the number primitive concepts in a knowledge base, enables the classifier to handle concepts that do not have complete definitions, and enhances the usefulness of an interactive classifier. MS-CIS-86-27 (revised version of MS-CIS-84-58)</abstract>
<title confidence="0.970972">Adapting MUMBLE: Experience with Natural Language Generation</title>
<author confidence="0.999857">Robert Rubinoff</author>
<degree confidence="0.140165">MS-CIS-86-32, LINC LAB 09 The following Technical Reports are available from</degree>
<affiliation confidence="0.998053">Computer Science Department Boston University</affiliation>
<address confidence="0.999763">Boston, MA 02215</address>
<note confidence="0.816577">Please enclose a check, made payable to Boston University, for the total amount due ($3.00 per report to cover cost of copying and postage). Constrained Semantic Transference: A Formal Theory of Metaphors Bipin Indurkhya In this paper we propose a formal theory of metaphors called Constrained Semantic Transference (CST). We start from the assumptions that meta-phors are characterized by the description of one domain, called the target domain, in terms of another domain, called the source domain; and that a metaphor works by transferring a set of structural relationships from the source domain to the target domain coherently. Report 85-008 Starting from these assumptions, we formally define the concept of T-MAPs which are partial coherent mappings from the source domain to target domain. We also define two operators, called and Computational Linguistics, Volume Number 2, April-June 1986</note>
<title confidence="0.854319">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<abstract confidence="0.995059857142857">Structure, that define a given adding new structure to the target domain. We show how T-MAPs can be used to characterize metaphorical interpretations of a given set of sentences. This characterization allows several cognitive features of metaphors to be described in CST. In particular, our characterization used the same criterion for deciding metaphorical truth as for literal truths.</abstract>
<title confidence="0.929698">Approximate Semantic Transference: A Computational Theory of Metaphors and Analogies</title>
<author confidence="0.506309">Bipin Indurkhya</author>
<pubnum confidence="0.990379">Report 85-012</pubnum>
<title confidence="0.956237">Computational Linguistics Technical Notes</title>
<author confidence="0.989509">Weiguo Wang</author>
<pubnum confidence="0.995601">Report 85-013,</pubnum>
<date confidence="0.991636">November 1985</date>
<title confidence="0.986924">Machine Translation in the Semantic Definite Clause Grammars Formalism</title>
<author confidence="0.64711">Xiuming Huang</author>
<abstract confidence="0.996728619047619">In this paper we start from the assumption that in a metaphor, or an analogy, some terms belonging to one domain (source domain) are used to refer to objects other than their conventional referents belonging to a possibly different domain (target domain). We describe a formalism, which is based on the First Order Predicate Calculus, for representing knowledge structure associated with a domain and then develop a theory of Constrained Semantic Transference (CST), which allows the terms and the relationships of the source domain to be transferred across to the target domain. We show how metaphors and analogies can be characterized in CST in such a way that many of their cognitive properties can be explained. We then propose a theory of approximate Semantic Transference (AST), which is a computational version of CST and is derived from it by replacing coherence requirement with We show how AST can be used as a basis for designing models of cognitive processes involved in comprehending metaphors and analogies. This technical report contains two technical notes on the analysis of the Kilbury algorithm for parsing ID/LP Grammars which is part of the GPSG framework. In the first one, two implementations of this algorithm are given, one in C-prolog, one in Interlisp-D. In the second one, the algorithm is compared against the Earley-Shieber algorithm on efficiency. The paper claims that the right attachment rules for phrases originally suggested by Frazier and Fodor are wrong, and that none of the subsequent patchings of the rules by syntactic methods have improved the situation. For each rule there are perfectly straightforward and indefinitely large classes of simple counter-examples. We then examine suggestions by Ford et al., Schubert and Hirst which are quasi-semantic in nature and which we consider ingenious but unsatisfactory. We offer a straightforward solution within the framework of preference semantics, and argue that the principal issue is not the type and nature of information required to get appropriate phrase attachments, but the issue of where to store the information and with what process to apply it. We present a Prolog implementation of a best first algorithm covering the data and contrast it with closely related ones, all of which are based on the preferences of nouns and prepositions, as well as verbs. The paper describes the SDCG (Semantic Definite Clause Grammars), a for Natural Language Processing and the XTRA Chinese Sentence TRAnslator) machine translation (MT) system based on it. The system translates general domain English sentences into grammatical Chinese sentences in a fully automatic manner. It is written in Prolog and implemented on the DEC-10, the GEC, and the SUN workstation, The following reports are available from</abstract>
<affiliation confidence="0.999934">Computing Research Laboratory</affiliation>
<address confidence="0.998925">Box 3CRL</address>
<affiliation confidence="0.996163">New Mexico State University</affiliation>
<address confidence="0.996008">Las Cruces, NM 88003</address>
<title confidence="0.903945">Syntax, Preference and Right Attachment</title>
<author confidence="0.968994">Yorick Wilks</author>
<author confidence="0.968994">Xiuming Huang</author>
<author confidence="0.968994">Dan Fass</author>
<pubnum confidence="0.872519">MCCS-85-5</pubnum>
<title confidence="0.6623565">Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature</title>
<author confidence="0.834501">SDCG is an augmentation of the DCG</author>
<affiliation confidence="0.534951">mar) of Pereira et al. (1980), which in turn is based on CFG (Context Free</affiliation>
<abstract confidence="0.86142975">Implemented in Prolog, the SDCG is highly suitable for general, and MT in particular. A wide range of linguistic phenomena is covered by the XTRA system, including multiple word senses, coordinate constructions, and prepositional phrase attachment, among others.</abstract>
<note confidence="0.787937714285714">Bad Metaphors: Chomsky and Artificial Intelligence Yorick Wilks MCCS-85-8 appear in S. Mogdil (ed.): Chomsky: Consensus and Controversy (with replies by Chomsky)</note>
<title confidence="0.850498">Collative Semantics</title>
<author confidence="0.887662">Dan Fats</author>
<pubnum confidence="0.908904">MCCS-85-23</pubnum>
<title confidence="0.9940545">Relevance, Points of View and Speech Acts: An Artificial Intelligence View</title>
<author confidence="0.941829">Yorick Wilks</author>
<pubnum confidence="0.967449">MCCS-85-25</pubnum>
<title confidence="0.996771">Impatient Users and Foreign-Speak</title>
<author confidence="0.999882">Brian M Slator</author>
<abstract confidence="0.99920955319149">The paper argues that the historical divisions between, on the one hand, the cluster of approaches to language understanding by computer known as Artificial Intelligence and, on the other, the Transformational Grammar system of Chomsky were caused not so much by matters of principle (as between scientific linguistics and computational applications) or by methodology, as by Chomsky&apos;s attachment over the years to a succession of unfortunate metaphors to explain his position. As recent developments in linguistics have shown, once these are removed, there are no issues of principle (though there may continue to be differences of fact and evidence) between linguistics, on the one hand, and Artificial Intelligence and Computational Linguistics on the other. Collative Semantics (CS) is a new domain independent semantics for natural language understanding. CS can resolve instances of word-sense and case ambiguity and offers an approach to discriminating between a subset of semantic relations that can exist between word-senses: conventional meaningful semantic relations, semantic redundancy, contrariness and contradiction (it cannot differentiate between the two), metonymy, metaphor, and meaninglessness. Collative Semantics is described. The word-sense is the basic unit of representation. Sense-frames, the sole form of knowledge structure, perform a double function: they are dictionary entries for individual word-senses and, in certain situations, behave like semantic primitives. Semantic vectors are scores that represent the degree of match between word-senses (and their associated sense-frames). Brief examples are given of how metonymic relations are computed, and how metaphor is recognised. The paper compares two approaches to modelling human discourse and, more particularly, dialogue: one is the relevance logic of Sperber and Wilson, the other the environments and points-of-view approach of Wilks and Bien. Although both descend from general insights of Grice that understanding is a matter of inference from what is said and what is assumed, the paper criticizes the Sperber and Wilson work on the ground that it is weak exactly where Grice is weak and that, contrary to its authors&apos; claims, it is not an information processing model, and does not model the beliefs of individual speakers and hearers. The latter system, like in to do that, and it is argued that any system that models discourse must do so. There is a class of Natural Language interface that translates English input into mnemonic formal language commands. These interfaces appear &amp;quot;front-end&amp;quot; of data bases and other applications software. Translation time, system response time, and user keyboard time, can all be improved if users acquire and use the mnemonics of the applications software. This paper argues that users, when shown the mnemonic output of their English input, will learn to communicate on a mnemonic level much the same way native speakers learn to speak on the level of intelligent but inarticulate foreigners. Foreign-Speak (so called) will act as a CAI teaching tool to help users more effectively use systems and might usefully be included as part of the design of these Natural Language interfaces.</abstract>
<note confidence="0.679574">Linguistics, Volume 12, Number 2, April-June 1986</note>
<title confidence="0.996593">The FINITE STRING Newsletter Abstracts of Current Literature The Effects of Restricted Syntax on Human-Computer Interaction</title>
<author confidence="0.999901">Gregg D Bailey</author>
<pubnum confidence="0.962657">MCCS-85-36</pubnum>
<title confidence="0.985423">Parsing in Parallel</title>
<author confidence="0.908048">Xiuming Huang</author>
<author confidence="0.908048">Louise Guthrie</author>
<pubnum confidence="0.943333">MCCS-85-40</pubnum>
<title confidence="0.997021">Natural Language Processing and Expert Systems</title>
<author confidence="0.999718">Jerry T Ball</author>
<pubnum confidence="0.572637">MCCS-85-41</pubnum>
<abstract confidence="0.998586461538461">The study compared the performance of 48 computer novices on two different problem solving tasks that involve computer interactions. One third of the subjects interacted with very limited natural language restrictions. Another group (16 subjects) interacted with command-oriented restriction. The final group (16 subjects) had object-oriented restrictions. The results of the study indicate that command restriction may facilitate performance over both unrestricted and object-oriented conditions. This paper is a description of a parallel model for natural language parsing, and a design for its implementation on the Hypercube multiprocessor. The parallel model is based on the Semantic Definite Clause Grammar formalism and integrates syntax and semantics through the communication of processes. The main processes, of which there are six, contain either purely syntactic or purely semantic information, giving the advantage of simple, transparent algorithms dedicated to only one aspect of parsing. Communication between processes is used to impose semantic constraints on the syntactic processes. Language Processing fall within the domain of problems suitable for solution using techniques developed for use in Expert Given an design involving the integration of syntax, semantics and pragmatics, a production system formalism augmented with techniques for managing a large solution space with interacting subproblems and reliable data is needed. The combined techniques of constraint propagation — to constrain the solution space, predictive selection of alternatives — to avoid least-commitment deadlock, and dependency-directed backtracking — to recover from selection of failed alternatives, are recommended.</abstract>
<affiliation confidence="0.912185">The following reports are available from the Department of Information Science, University of Constance: Universitaet Konstanz Informationswissenschaft Projekt TOPIC II</affiliation>
<address confidence="0.911724">Postfach 5560</address>
<abstract confidence="0.461055">D-7750 Konstanz 1, F.R.G. Orders from non-profit organizations (universities, governments, etc.) are handled free of charge. Requests from other organizations or from individuals should be accompanied by payment for postage and handling costs ($5.00 per report). TOPIC II / TOPOGRAPHIC II. Automatische Textkondensierimg und textorientiertes Informationsmanagement</abstract>
<note confidence="0.636127166666667">(Projektziele - State of the Art) U. Hahn, R. Hammwoehner, R. Kithlen, U. Reimer, U. Thiel Bericht TOPIC-12/844 &amp;TOPOGRAPHIC-3/84, in German, Dec. 1984, 72 pp.</note>
<title confidence="0.8943515">On Formal Semantic Properties of a Frame Data Model</title>
<author confidence="0.989766">Ulrich Reimer</author>
<author confidence="0.989766">Udo Hahn</author>
<abstract confidence="0.898240909090909">A concise overview is given of the aims of the second phase of the text condensation (automatic abstracting) and information retrieval projects of the Information Science Department at the University of Constance (FRG) After providing a state-of-the-art report of current string-processing, statistical, linguistical, knowledge-based, and cognitive approaches to full text analysis and retrieval in information systems, the conception of a knowledge-based full text information system is outlined. Combining both, sophisticated text analysis as well as information retrieval procedures, text analysis is dealt with applying a word expert system to text cohesion and text coherence phenomena for text parsing, a frame representation model for text representation, and appropriate text condensation, for text summarization (project II). information management on the other hand is based on various forms of graphical man-machine interaction and alternative ways of graphics-based text inforretrieval (project Standard knowledge representation languages (e.g., KRL, FRL) are almost exclusively characterized by syntactic specifications but are seriously lacking of an explicit formal semantics. The formal description of the frame Linguistics, Volume 12, Number 2, April-June 1986 The FINITE STRING Newsletter Abstracts of Current Literature Bericht TOPIC-13/84, in English, Dec. 1984, 24 pp.</abstract>
<title confidence="0.776708">The TOPIC Project: Text-Oriented Procedures for Information Management and Condensation of Expository Texts</title>
<note confidence="0.66719125">(Final Report) Udo Hahn, Ulrich Reimer Bericht TOPIC-17/85, in English, May 1985, 114 pp.</note>
<title confidence="0.816946666666667">Enhancing Knowledge Base Reliability: A Claim for Specifying the Empirical Semantics of Knowledge Representation Models</title>
<author confidence="0.896976">Ulrich Reimer</author>
<note confidence="0.8380836">Bericht TOPIC-18/86, in English, May 1986 Udo Hahn, Ulrich Reimer Berticht TOPIC-19/86, in English, April 1986, 35 pp.</note>
<title confidence="0.82931825">On Lexically Distributed Text Parsing: Computational Model for the Analysis of Textuality on the Level of Text Cohesion and Text Coherence</title>
<abstract confidence="0.994375545454546">data model outlined in this paper is given with emphasis on explicit semantic specifications applying a combination of a denotational and an axiomatic approach. After introducing the formal syntax and the basic conceptual relations (e.g., is-a, instance-of, part-of) of the frame data model, the set of semantic integrity constraints is outlined, finally leading to the specification of a set of basic operations in the frame data model. Based on an abstract data type view of knowledge representation languages, these operations completely specify the frame data model in terms of its behavioral properties. As these operations are the only means to access data in a frame data base, semantic integrity is always guaranteed. After considering requirements for advanced full text information systems, the major methodological concepts underlying the automatic text conden- (automatic abstracting) system outlined: partial parsing as a text-specific parsing strategy, validity and reliability of the knowledge base under the scope of a formally specified frame data model, recognition of text cohesion and coherence phenomena of expository texts by an appropriately tuned text parser, and variable degrees of thematic aggregation of an input text, basically on a level of text summarization comparable to that of structured abstracts (indicative abstracts plus facts acquired from the original text). Subsequently, a coherent formal description of the whole TOPIC system is given that distinguishes three major processes: text parsing based on the close interaction between a frame knowledge base and a word expert parser, text condensation that utilizes various activation, property assignment, and connectivity patterns of the text representation structures, and finally, the construction of the text graph that represents the knowledge structures of text condensates on different levels of thematic abstraction accessible to appropriate information retrieval procedures. Starting from a request for higher functionality of knowledge-based systems, this paper argues for a more elaborate semantic specification of knowledge representation models. Semantics in the understanding of this paper requires the representation constructs to reflect the empirical regularities holding in some domain of discourse. This way the possibility of representing illegal knowledge is reduced and semantic integrity of a knowledge base is increased with an accordingly positive effect on a corresponding application system. The paper further calls for high-level representation constructs to be made available to a knowledge representation model and argues that they are needed to cope with increasing complexity and size of knowledge bases. Finally, integrity preserving operations are demanded as the only means to access a knowledge base so that an abstract data type view on knowledge bases is achieved which guarantees knowledge based always to be in valid states An overview of TOPIC, a knowledge-based text information system for the analysis of German-language texts, is provided. TOPIC supplies text condensates (summaries) on variable degrees of generality and makes available facts acquired from the texts. The presentation focuses on the major methodological principles underlying the design of TOPIC: a frame representation model that incorporates text cohesion and text coherence properties of expository texts, a lexically distributed semantic text grammar in the format of word experts, a model of partial text parsing, and text graphs as appropriate representation structures for text condensates. Adequate linguistic models for text analysis must account for textuality on the level of text cohesion and text coherence, i.e., for local texture and global well-formedness of text propositions by text grammars on a semantic basis. Considering the requirements of corresponding text parsing Linguistics, Volume 12, Number 2, April-June 1986</abstract>
<title confidence="0.990507">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<author confidence="0.98534">Udo Hahn</author>
<note confidence="0.3497055">Bericht TOPIC-20/85, in English, Dec. 1985, 46 pp.</note>
<title confidence="0.959511">A Generalized Word Expert Model of Lexically Distributed Text Parsing</title>
<author confidence="0.970475">Udo Hahn</author>
<abstract confidence="0.981890041666667">Bericht TOPIC-21/86, in English, April 1986, 16 pp. devices, a distributed lexical text grammar is provided in the format of word experts. The current specification level aims at the recognition of semantic and thematic relations holding among nominal expressions of a text. An empirical fragment of the semantic text grammar supplied thus is confined to the description of text cohesion phenomena due to nominal anaphora and lexical cohesion. Repeated application of corresponding word experts yields text representation structures in a frame knowledge base which reveal highly standardized topic/comment patterns, namely thematic progression based on the regular modes of constant theme, linear thematization of themes, and derives themes. Adequate linguistic models for text analysis must account for textuality on the level of text cohesion and text coherence, i.e., for local texture and global well-formedness of text propositions by text grammars on a semantic basis. Considering the requirements of corresponding text parsing devices, a distributed lexical text grammar is provided in the format of word experts. The generalizations proposed concern linguistic generality and methodological requirements of the formal specification of word expert parsers. The approach is illustrated by an informal account of word experts applying to nominal anaphora and lexical cohesion phenomena. Repeated application of corresponding cohesion devices yields regular text coherence structures in a frame knowledge base in terms of basic patterns of thematic progression.</abstract>
<note confidence="0.7979518">Single copies of the following reports and memos can be ordered free of charge from Universitaet des Saarlandes FR 10.2 Informatik IV IM Stadtwald 15 D - 6600 Saarbrucken 11</note>
<title confidence="0.9554715">Federal Republic of Germany Dialog-Based User Models</title>
<author confidence="0.994669">W Wahlster</author>
<author confidence="0.994669">A Kobsa</author>
<pubnum confidence="0.676197">Report No. 3</pubnum>
<note confidence="0.774601666666667">appear in Ferrari, G., Ed., of the IEEE, Special Issue on Natural Processing, 1986</note>
<title confidence="0.821855666666667">SYCON — EM Ralunensystem zur Constraint-Propagierung auf Netzwerken von beliebigen symbolischen Constraints</title>
<author confidence="0.993973">M Fendler</author>
<author confidence="0.993973">R Wichlacz</author>
<pubnum confidence="0.919958">Report No. 4</pubnum>
<note confidence="0.94398325">Stoyan, 9th German Workshop on Artificial Intelligence Dassel/Solling, September 1985 Springer, Heidelberg, 1986</note>
<abstract confidence="0.942139375">Entwurf eines aktiven, wissenbasierten Hilfesystems für SINIX investigates several approaches to user modeling in natural-language dialog systems. First, reasons are pointed out why user modeling has become so important in the last few years, and definitions are proposed the notions of model modeling. techniques for constructing user models in the course of a dialog are presented and recent proposals for representing a wide range of assumptions about a user&apos;s beliefs and goals in a system&apos;s knowledge base are surveyed. Examples for the application of user models in systems developed to date are presented, and some social implications are discussed. Finally, unsolved problems, like coping with collective beliefs or resource-limited processes, are investigated, and prospects for application-oriented research are outlined. Constraint-propagation on networks has been successfully tested as a processing mechanism in several fields of Artificial Intelligence. This paper presents SYCON, with which symbolic constraints of any kind can be</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>statistical assumptions about the form of the distribution underlying the data.</title>
<marker></marker>
<rawString>statistical assumptions about the form of the distribution underlying the data.</rawString>
</citation>
<citation valid="true">
<title>Referential Choice and Focus of Attention in Narratives Ellen Palmer Francik Stanford University Ph.D.</title>
<date>1985</date>
<booktitle>Psychology, Experimental DAI V46(04), SecB,</booktitle>
<pages>110</pages>
<contexts>
<context position="10906" citStr="(1985)" startWordPosition="1649" endWordPosition="1649">ext-sensitive method for determining object similarity. Such a method is introduced here. Some of the necessary contextual information is captured by a new notion of object perspective. It is shown how object perspective can be used to account for different responses to a given misconception in different contexts. Tree Adjoining Grammar (TAG) is a formalism for natural language grammars. Some of the basic notions of TAGs were introduced in Joshi, Levy, and Takahashi (1975) and by Joshi (1983). A detailed investigation of the linguistic relevance of TAGs has been carried out in Kroch and Joshi (1985). In this paper, we describe some new results for TAGS, especially in the following areas: (1) parsing complexity of TAGs, (2) some closure results for TAGs, and (3) the relationship to Head grammars. Phrase-structure trees (phrase-markers) provide structural descriptions for sentences. Phrase-structure trees can be generated by phrase-structure grammars. Phrase-structure trees can be shown to be appropriate to characterize structural descriptions for sentences, including those aspects which are usually characterized by transformational grammars, by making certain amendations to CFGs, without </context>
</contexts>
<marker>1985</marker>
<rawString>Referential Choice and Focus of Attention in Narratives Ellen Palmer Francik Stanford University Ph.D. 1985, 110 pages Psychology, Experimental DAI V46(04), SecB, pp1363</rawString>
</citation>
<citation valid="false">
<title>Order Number ADG85-11308. 8510 Interlingual Semantic Facilitation: Evidence for a Common Representational System in the Bilingual Lexicon</title>
<institution>University Microfilms</institution>
<marker></marker>
<rawString>University Microfilms Order Number ADG85-11308. 8510 Interlingual Semantic Facilitation: Evidence for a Common Representational System in the Bilingual Lexicon</rawString>
</citation>
<citation valid="false">
<date>1985</date>
<pages>104</pages>
<institution>Mario Rey Florida Atlantic University M.A.</institution>
<contexts>
<context position="10906" citStr="(1985)" startWordPosition="1649" endWordPosition="1649">ext-sensitive method for determining object similarity. Such a method is introduced here. Some of the necessary contextual information is captured by a new notion of object perspective. It is shown how object perspective can be used to account for different responses to a given misconception in different contexts. Tree Adjoining Grammar (TAG) is a formalism for natural language grammars. Some of the basic notions of TAGs were introduced in Joshi, Levy, and Takahashi (1975) and by Joshi (1983). A detailed investigation of the linguistic relevance of TAGs has been carried out in Kroch and Joshi (1985). In this paper, we describe some new results for TAGS, especially in the following areas: (1) parsing complexity of TAGs, (2) some closure results for TAGs, and (3) the relationship to Head grammars. Phrase-structure trees (phrase-markers) provide structural descriptions for sentences. Phrase-structure trees can be generated by phrase-structure grammars. Phrase-structure trees can be shown to be appropriate to characterize structural descriptions for sentences, including those aspects which are usually characterized by transformational grammars, by making certain amendations to CFGs, without </context>
</contexts>
<marker>1985</marker>
<rawString>Mario Rey Florida Atlantic University M.A. 1985, 104 pages</rawString>
</citation>
<citation valid="false">
<authors>
<author>Psychology</author>
</authors>
<journal>Experimental MM</journal>
<volume>23</volume>
<issue>03</issue>
<pages>405</pages>
<institution>University Microfilms Order Number</institution>
<marker>Psychology, </marker>
<rawString>Psychology, Experimental MM V23(03), pp405 University Microfilms Order Number ADGI3-25149. 8510</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>