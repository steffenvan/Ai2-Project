<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.984889">
Fine-grained Tree-to-String Translation Rule Extraction
</title>
<author confidence="0.999227">
Xianchao Wu† Takuya Matsuzaki† Jun’ichi Tsujii†$*
</author>
<affiliation confidence="0.9817645">
†Department of Computer Science, The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan
$School of Computer Science, University of Manchester
*National Centre for Text Mining (NaCTeM)
</affiliation>
<address confidence="0.56802">
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK
</address>
<email confidence="0.985324">
{wxc, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.99737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999618777777778">
Tree-to-string translation rules are widely
used in linguistically syntax-based statis-
tical machine translation systems. In this
paper, we propose to use deep syntac-
tic information for obtaining fine-grained
translation rules. A head-driven phrase
structure grammar (HPSG) parser is used
to obtain the deep syntactic information,
which includes a fine-grained description
of the syntactic property and a semantic
representation of a sentence. We extract
fine-grained rules from aligned HPSG
tree/forest-string pairs and use them in
our tree-to-string and string-to-tree sys-
tems. Extensive experiments on large-
scale bidirectional Japanese-English trans-
lations testified the effectiveness of our ap-
proach.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991952631579">
Tree-to-string translation rules are generic and ap-
plicable to numerous linguistically syntax-based
Statistical Machine Translation (SMT) systems,
such as string-to-tree translation (Galley et al.,
2004; Galley et al., 2006; Chiang et al., 2009),
tree-to-string translation (Liu et al., 2006; Huang
et al., 2006), and forest-to-string translation (Mi et
al., 2008; Mi and Huang, 2008). The algorithms
proposed by Galley et al. (2004; 2006) are fre-
quently used for extracting minimal and composed
rules from aligned 1-best tree-string pairs. Deal-
ing with the parse error problem and rule sparse-
ness problem, Mi and Huang (2008) replaced the
1-best parse tree with a packed forest which com-
pactly encodes exponentially many parses for tree-
to-string rule extraction.
However, current tree-to-string rules only make
use of Probabilistic Context-Free Grammar tree
fragments, in which part-of-speech (POS) or
</bodyText>
<figure confidence="0.4105556">
koroshita korosareta
(active) (passive)
VBN(killed) 6 (6/10,6/6) 4 (4/10,4/4)
VBN(killed:active) 5 (5/6,5/6) 1 (1/6,1/4)
VBN(killed:passive) 1 (1/4,1/6) 3 (3/4,3/4)
</figure>
<tableCaption confidence="0.630254">
Table 1: Bidirectional translation probabilities of
</tableCaption>
<bodyText confidence="0.9922838125">
rules, denoted in the brackets, change when voice
is attached to “killed”.
phrasal tags are used as the tree node labels. As
will be testified by our experiments, we argue that
the simple POS/phrasal tags are too coarse to re-
flect the accurate translation probabilities of the
translation rules.
For example, as shown in Table 1, sup-
pose a simple tree fragment “VBN(killed)” ap-
pears 6 times with “koroshita”, which is a
Japanese translation of an active form of “killed”,
and 4 times with “korosareta”, which is a
Japanese translation of a passive form of “killed”.
Then, without larger tree fragments, we will
more frequently translate “VBN(killed)” into “ko-
roshita” (with a probability of 0.6). But,
“VBN(killed)” is indeed separable into two fine-
grained tree fragments of “VBN(killed:active)”
and “VBN(killed:passive)”1. Consequently,
“VBN(killed:active)” appears 5 times with “ko-
roshita” and 1 time with “korosareta”; and
“VBN(killed:passive)” appears 1 time with “ko-
roshita” and 3 times with “korosareta”. Now, by
attaching the voice information to “killed”, we are
gaining a rule set that is more appropriate to reflect
the real translation situations.
This motivates our proposal of using deep syn-
tactic information to obtain a fine-grained trans-
lation rule set. We name the information such as
the voice of a verb in a tree fragment as deep syn-
tactic information. We use a head-driven phrase
structure grammar (HPSG) parser to obtain the
</bodyText>
<footnote confidence="0.831288">
&apos;For example, “John has killed Mary.” versus “John was
killed by Mary.”
</footnote>
<page confidence="0.961426">
325
</page>
<note confidence="0.9444965">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 325–334,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999881458333333">
deep syntactic information of an English sentence,
which includes a fine-grained description of the
syntactic property and a semantic representation
of the sentence. We extract fine-grained trans-
lation rules from aligned HPSG tree/forest-string
pairs. We localize an HPSG tree/forest to make
it segmentable at any nodes to fit the extraction
algorithms described in (Galley et al., 2006; Mi
and Huang, 2008). We also propose a linear-time
algorithm for extracting composed rules guided
by predicate-argument structures. The effective-
ness of the rules are testified in our tree-to-string
and string-to-tree systems, taking bidirectional
Japanese-English translations as our test cases.
This paper is organized as follows. In Section 2,
we briefly review the tree-to-string and string-to-
tree translation frameworks, tree-to-string rule ex-
traction algorithms, and rich syntactic information
previously used for SMT. The HPSG grammar and
our proposal of fine-grained rule extraction algo-
rithms are described in Section 3. Section 4 gives
the experiments for applying fine-grained transla-
tion rules to large-scale Japanese-English transla-
tion tasks. Finally, we conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.922853">
2.1 Tree-to-string and string-to-tree
translations
</subsectionHeader>
<bodyText confidence="0.999784666666667">
Tree-to-string translation (Liu et al., 2006; Huang
et al., 2006) first uses a parser to parse a source
sentence into a 1-best tree and then searches for
the best derivation that segments and converts the
tree into a target string. In contrast, string-to-tree
translation (Galley et al., 2004; Galley et al., 2006;
Chiang et al., 2009) is like bilingual parsing. That
is, giving a (bilingual) translation grammar and a
source sentence, we are trying to construct a parse
forest in the target language. Consequently, the
translation results can be collected from the leaves
of the parse forest.
Figure 1 illustrates the training and decoding
processes of bidirectional Japanese-English trans-
lations. The English sentence is “John killed
Mary” and the Japanese sentence is “jyon ha mari
wo koroshita”, in which the function words “ha”
and “wo” are not aligned with any English word.
</bodyText>
<subsectionHeader confidence="0.997394">
2.2 Tree/forest-based rule extraction
</subsectionHeader>
<bodyText confidence="0.999498">
Galley et al. (2004) proposed the GHKM algo-
rithm for extracting (minimal) tree-to-string trans-
lation rules from a tuple of ⟨F, Et, A⟩, where F =
</bodyText>
<figure confidence="0.58567">
John killed Mary
</figure>
<figureCaption confidence="0.886363">
Figure 1: Illustration of the training and decod-
ing processes for tree-to-string and string-to-tree
translations.
</figureCaption>
<bodyText confidence="0.999080071428571">
f1 is a sentence of a foreign language other than
English, Et is a 1-best parse tree of an English sen-
tence E = el, and A = {(j, i)} is an alignment
between the words in F and E.
The basic idea of GHKM algorithm is to de-
compose Et into a series of tree fragments, each
of which will form a rule with its corresponding
translation in the foreign language. A is used as a
constraint to guide the segmentation procedure, so
that the root node of every tree fragment of Et ex-
actly corresponds to a contiguous span on the for-
eign language side. Based on this consideration, a
frontier set (fs) is defined to be a set of nodes n in
Et that satisfies the following constraint:
</bodyText>
<equation confidence="0.966945">
fs = {n|span(n) n comp span(n) = ϕ}. (1)
</equation>
<bodyText confidence="0.999598">
Here, span(n) is defined by the indices of the first
and last word in F that are reachable from a node
n, and comp span(n) is defined to be the comple-
ment set of span(n), i.e., the union of the spans
of all nodes n′ in Et that are neither descendants
nor ancestors of n. span(n) and comp span(n)
of each n can be computed by first a bottom-up
exploration and then a top-down traversal of Et.
By restricting each fragment so that it only takes
</bodyText>
<figure confidence="0.999517666666666">
Apply
rules
Testing
Training
S
xa x1
NP VP
xa
VP
S
Extract
rules
V NP
x1
xa
NP
VP
NP
V NP
John
John killed
Mary
V
killed
NP
Mary
jyon ha mari wo koroshita
Aligned tree-string pair:
x1
x1 xa
CKY decoding
John
killed
Mary
NP
NP
V
NP
V
VP
NP
tree-to-string
string-to-tree
VP
S
parsing
Bottom-up
decoding
</figure>
<page confidence="0.724997">
326
</page>
<figure confidence="0.97033772">
HEAD C8
SEM_HEAD C8
CAT S
XCAT
SCHEMA head_mod
HEAD C3
SEM_HEAD C3
CAT S
XCAT
SCHEMA subj_head
C7
C0
C1
t0
HEAD C2
SEM_HEAD C2
CAT NP
XCAT
SCHEMA empty_sp ec_head
C2
John
CAT N
POS NNP
BASE john
LEXENTRY [D&lt;
N.3sg&gt;]_lan
PRED noun_arg0
HEAD t0
SEM_HEAD t0
CAT NX
XCAT
-0.001
C8
0.00
HEAD C9
SEM_HEAD C9
CAT S
XCAT
SCHEMA subj_head
0.81
t3
1. C0(X0:C1, X1:C3) 4 X0 X1
2. C1(X0:C2) 4 X0
3. C2(t0) 4
4. C3(X0:C4, X1:C5) 4 X1 X0
5. C4(t1) 4
6. C5(X0:C6) 4 X0
7. C6(t2) 4
killed
CAT V
POS VBD
BASE kill
LEXENTRY [NP.nom
&lt;V.bs e&gt;]_lcm-
past_verb_rule
PRED verb_arg1
TENSE past
ASPECT none
VOICE active
AUX minus
ARG1 C1
C9
HEAD t3
SEM_HEAD t3
CAT VP
XCAT
2.77
4.52
-3.47
tl
C4
killed
CAT V
POS VBD
BASE kill
LEXENTRY [NP.nom
&lt;V.bse&gt; NP.acc]
_lxm-past_verb_rule
PRED verb_arg12
TENSE past
ASPECT none
VOICE active
AUX minus
ARG1 C1
ARG2 C5
John
HEAD t1
SEM_HEAD t1
CAT VX
XCAT
C3
kidded
-0.03
HEAD C4
SEM_HEAD C4
CAT VP
XCAT
SCHEMA head_comp
Mary
X0 X1
C0
C1 C3
C4 C5
t�
2.25
C5
HEAD C6
SEM_HEAD C6
CAT NP
XCAT
SCHEMA empty_sp ec_h ead
t2
C6
Mary
CAT N
POS NNP
BASE mary
LEXENTRY
[D&lt;N.3sg&gt;]_lcm
PRED noun_arg0
HEAD t2
SEM_HEAD t2
CAT NX
XCAT
minimum
Covering tree
0
-0.07
C10
HEAD C11
SEM_HEAD C11
CAT NP
XCAT
SCHEMA empty_spec_head
t4
C11
Mary
CAT N
POS NNP
BASE mary
LEXENTRY
V[D&lt;N.3sg&gt;]
PRED noun_arg0
HEAD t4
SEM_HEAD t4
CAT NX
XCAT
0
-2.82
An HPSG-tree based minimal rule set A PAS-based composed rul
</figure>
<figureCaption confidence="0.998886">
Figure 2: Illustration of an aligned HPSG forest-string pair. The forest includes two parse trees by taking
</figureCaption>
<bodyText confidence="0.998138393939394">
“Mary” as a modifier (t3, t4) or an argument (t1, t2) of “killed”. Arrows with broken lines denote the PAS
dependencies from the terminal node t1 to its argument nodes (c1 and c5). The scores of the hyperedges
are attached to the forest as well.
the nodes in fs as the root and leaf nodes, a well-
formed fragmentation of Et is generated. With
fs computed, rules are extracted through a depth-
first traversal of Et: we cut Et at all nodes in fs
to form tree fragments and extract a rule for each
fragment. These extracted rules are called minimal
rules (Galley et al., 2004). For example, the 1-
best tree (with gray nodes) in Figure 2 is cut into 7
pieces, each of which corresponds to the tree frag-
ment in a rule (bottom-left corner of the figure).
In order to include richer context information
and account for multiple interpretations of un-
aligned words of foreign language, minimal rules
which share adjacent tree fragments are connected
together to form composed rules (Galley et al.,
2006). For each aligned tree-string pair, Gal-
ley et al. (2006) constructed a derivation-forest,
in which composed rules were generated, un-
aligned words of foreign language were consis-
tently attached, and the translation probabilities
of rules were estimated by using Expectation-
Maximization (EM) (Dempster et al., 1977) train-
ing. For example, by combining the minimal rules
of 1, 4, and 5, we obtain a composed rule, as
shown in the bottom-right corner of Figure 2.
Considering the parse error problem in the
1-best or k-best parse trees, Mi and Huang
(2008) extracted tree-to-string translation rules
from aligned packed forest-string pairs. A for-
est compactly encodes exponentially many trees
</bodyText>
<page confidence="0.990848">
327
</page>
<bodyText confidence="0.99996775">
rather than the 1-best tree used by Galley et al.
(2004; 2006). Two problems were managed to
be tackled during extracting rules from an aligned
forest-string pair: where to cut and how to cut.
Equation 1 was used again to compute a frontier
node set to determine where to cut the packed
forest into a number of tree-fragments. The dif-
ference with tree-based rule extraction is that the
nodes in a packed forest (which is a hypergraph)
now are hypernodes, which can take a set of in-
coming hyperedges. Then, by limiting each frag-
ment to be a tree and whose root/leaf hypernodes
all appearing in the frontier set, the packed forest
can be segmented properly into a set of tree frag-
ments, each of which can be used to generate a
tree-to-string translation rule.
</bodyText>
<subsectionHeader confidence="0.991303">
2.3 Rich syntactic information for SMT
</subsectionHeader>
<bodyText confidence="0.999951490196079">
Before describing our approaches of applying
deep syntactic information yielded by an HPSG
parser for fine-grained rule extraction, we would
like to briefly review what kinds of deep syntactic
information have been employed for SMT.
Two kinds of supertags, from Lexicalized Tree-
Adjoining Grammar and Combinatory Categorial
Grammar (CCG), have been used as lexical syn-
tactic descriptions (Hassan et al., 2007) for phrase-
based SMT (Koehn et al., 2007). By introduc-
ing supertags into the target language side, i.e.,
the target language model and the target side
of the phrase table, significant improvement was
achieved for Arabic-to-English translation. Birch
et al. (2007) also reported a significant improve-
ment for Dutch-English translation by applying
CCG supertags at a word level to a factorized SMT
system (Koehn et al., 2007).
In this paper, we also make use of supertags
on the English language side. In an HPSG
parse tree, these lexical syntactic descriptions
are included in the LEXENTRY feature (re-
fer to Table 2) of a lexical node (Matsuzaki
et al., 2007). For example, the LEXEN-
TRY feature of “t1:killed” takes the value of
[NP.nom&lt;V.bse&gt;NP.acc]_lxm-past
_verb_rule in Figure 2. In which,
[NP.nom&lt;V.bse&gt;NP.acc] is an HPSG
style supertag, which tells us that the base form
of “killed” needs a nominative NP in the left hand
side and an accessorial NP in the right hand side.
The major differences are that, we use a larger
feature set (Table 2) including the supertags for
fine-grained tree-to-string rule extraction, rather
than string-to-string translation (Hassan et al.,
2007; Birch et al., 2007).
The Logon project2 (Oepen et al., 2007) for
Norwegian-English translation integrates in-depth
grammatical analysis of Norwegian (using lexi-
cal functional grammar, similar to (Riezler and
Maxwell, 2006)) with semantic representations in
the minimal recursion semantics framework, and
fully grammar-based generation for English using
HPSG. A hybrid (of rule-based and data-driven)
architecture with a semantic transfer backbone is
taken as the vantage point of this project. In
contrast, the fine-grained tree-to-string translation
rule extraction approaches in this paper are to-
tally data-driven, and easily applicable to numer-
ous language pairs by taking English as the source
or target language.
</bodyText>
<sectionHeader confidence="0.990784" genericHeader="method">
3 Fine-grained rule extraction
</sectionHeader>
<bodyText confidence="0.999950333333333">
We now introduce the deep syntactic informa-
tion generated by an HPSG parser and then de-
scribe our approaches for fine-grained tree-to-
string rule extraction. Especially, we localize an
HPSG tree/forest to fit the extraction algorithms
described in (Galley et al., 2006; Mi and Huang,
2008). Also, we propose a linear-time com-
posed rule extraction algorithm by making use of
predicate-argument structures.
</bodyText>
<subsectionHeader confidence="0.5764965">
3.1 Deep syntactic information by HPSG
parsing
</subsectionHeader>
<bodyText confidence="0.999964888888889">
Head-driven phrase structure grammar (HPSG) is
a lexicalist grammar framework. In HPSG, lin-
guistic entities such as words and phrases are rep-
resented by a data structure called a sign. A sign
gives a factored representation of the syntactic fea-
tures of a word/phrase, as well as a representation
of their semantic content. Phrases and words rep-
resented by signs are composed into larger phrases
by applications of schemata. The semantic rep-
resentation of the new phrase is calculated at the
same time. As such, an HPSG parse tree/forest
can be considered as a tree/forest of signs (c.f. the
HPSG forest in Figure 2).
An HPSG parse tree/forest has two attractive
properties as a representation of an English sen-
tence in syntax-based SMT. First, we can carefully
control the condition of the application of a trans-
lation rule by exploiting the fine-grained syntactic
</bodyText>
<footnote confidence="0.981436">
2http://www.emmtee.net/
</footnote>
<page confidence="0.980808">
328
</page>
<table confidence="0.986775444444444">
Feature Description
CAT phrasal category
XCAT fine-grained phrasal category
SCHEMA name of the schema applied in the node
HEAD pointer to the head daughter
SEM HEAD pointer to the semantic head daughter
CAT syntactic category
POS Penn Treebank-style part-of-speech tag
BASE base form
</table>
<tableCaption confidence="0.620724692307692">
TENSE tense of a verb (past, present, untensed)
ASPECT aspect of a verb (none, perfect,
progressive, perfect-progressive)
VOICE voice of a verb (passive, active)
AUX auxiliary verb or not (minus, modal,
have, be, do, to, copular)
LEXENTRY lexical entry, with supertags embedded
PRED type of a predicate
ARG⟨x⟩ pointer to semantic arguments, x = L.�
Table 2: Syntactic/semantic features extracted
from HPSG signs that are included in the output
of Enju. Features in phrasal nodes (top) and lexi-
cal nodes (bottom) are listed separately.
</tableCaption>
<bodyText confidence="0.999302966666667">
description in the English parse tree/forest, as well
as those in the translation rules. Second, we can
identify sub-trees in a parse tree/forest that cor-
respond to basic units of the semantics, namely
sub-trees covering a predicate and its arguments,
by using the semantic representation given in the
signs. We expect that extraction of translation
rules based on such semantically-connected sub-
trees will give a compact and effective set of trans-
lation rules.
A sign in the HPSG tree/forest is represented by
a typed feature structure (TFS) (Carpenter, 1992).
A TFS is a directed-acyclic graph (DAG) wherein
the edges are labeled with feature names and the
nodes (feature values) are typed. In the original
HPSG formalism, the types are defined in a hierar-
chy and the DAG can have arbitrary shape (e.g., it
can be of any depth). We however use a simplified
form of TFS, for simplicity of the algorithms. In
the simplified form, a TFS is converted to a (flat)
set of pairs of feature names and their values. Ta-
ble 2 lists the features used in this paper, which
are a subset of those in the original output from an
HPSG parser, Enju3. The HPSG forest shown in
Figure 2 is in this simplified format. An impor-
tant detail is that we allow a feature value to be a
pointer to another (simplified) TFS. Such pointer-
valued features are necessary for denoting the se-
mantics, as explained shortly.
In the Enju English HPSG grammar (Miyao et
</bodyText>
<footnote confidence="0.898493">
3http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
</footnote>
<note confidence="0.813953">
ARG2
</note>
<figureCaption confidence="0.885124333333333">
Figure 3: Predicate argument structures for the
sentences of “John killed Mary” and “She ignored
the fact that I wanted to dispute”.
</figureCaption>
<bodyText confidence="0.998954642857143">
al., 2003) used in this paper, the semantic content
of a sentence/phrase is represented by a predicate-
argument structure (PAS). Figure 3 shows the PAS
of the example sentence in Figure 2, “John killed
Mary”, and a more complex PAS for another sen-
tence, “She ignored the fact that I wanted to dis-
pute”, which is adopted from (Miyao et al., 2003).
In an HPSG tree/forest, each leaf node generally
introduces a predicate, which is represented by
the pair of LEXENTRY (lexical entry) feature and
PRED (predicate type) feature. The arguments of
a predicate are designated by the pointers from the
ARG⟨x⟩ features in a leaf node to non-terminal
nodes.
</bodyText>
<subsectionHeader confidence="0.999185">
3.2 Localize HPSG forest
</subsectionHeader>
<bodyText confidence="0.999948153846154">
Our fine-grained translation rule extraction algo-
rithm is sketched in Algorithm 1. Considering that
a parse tree is a trivial packed forest, we only use
the term forest to expand our discussion, hereafter.
Recall that there are pointer-valued features in the
TFSs (Table 2) which prevent arbitrary segmenta-
tion of a packed forest. Hence, we have to localize
an HPSG forest.
For example, there are ARG pointers from t1 to
c1 and c5 in the HPSG forest of Figure 2. How-
ever, the three nodes are not included in one (min-
imal) translation rule. This problem is caused
by not considering the predicate argument depen-
dency among t1, c1, and c5 while performing the
GHKM algorithm. We can combine several min-
imal rules (Galley et al., 2006) together to ad-
dress this dependency. Yet we have a faster way
to tackle PASs, as will be described in the next
subsection.
Even if we omit ARG, there are still two kinds
of pointer-valued features in TFSs, HEAD and
SEM HEAD. Localizing these pointer-valuedfea-
tures is straightforward, since during parsing, the
HEAD and SEM HEAD of a node are automati-
cally transferred to its mother node. That is, the
syntactic and semantic head of a node only take
</bodyText>
<figure confidence="0.941720045454546">
John
kill
ARG2 Mary
ARG1
ARG1
She
ignore
ARG2
fact
I
ARG1
dispute
ARG1
want
ARG2
329
Algorithm 1 Fine-grained rule extraction
Input: HPSG tree/forest Ef, foreign sentence F, and align-
ment A
Output: a PAS-based rule set R1 and/or a tree-rule set R2
1: if Ef is an HPSG tree then
2: E′f =localize Tree(Ef)
</figure>
<listItem confidence="0.958807111111111">
3: R1 = PASR extraction(E ′f, F, A) &gt; Algorithm 2
4: E′′f = ignore PAS(E′f)
5: R2 = TR extraction(E′′f , F, A) &gt; composed rule ex-
traction algorithm in (Galley et al., 2006)
6: else if Ef is an HPSG forest then
7: E′ f = localize Forest(Ef);
8: R2 = forest based rule extraction(E′f, F, A) &gt; Algo-
rithm 1 in (Mi and Huang, 2008)
9: end if
</listItem>
<bodyText confidence="0.9984205">
the identifier of the daughter node as the values.
For example, HEAD and SEM HEAD of node c0
take the identical value to be c3 in Figure 2.
To extract tree-to-string rules from the tree
structures of an HPSG forest, our solution is to
pre-process an HPSG forest in the following way:
</bodyText>
<listItem confidence="0.989218857142857">
• for a phrasal hypernode, replace its HEAD
and SEM HEAD value with L, R, or S,
which respectively represent left daughter,
right daughter, or single daughter (line 2 and
7); and,
• for a lexical node, ARG(x) and PRED fea-
tures are ignored (line 4).
</listItem>
<bodyText confidence="0.9994376">
A pure syntactic-based HPSG forest without any
pointer-valued features can be yielded through this
pre-processing for the consequent execution of the
extraction algorithms (Galley et al., 2006; Mi and
Huang, 2008).
</bodyText>
<subsectionHeader confidence="0.999314">
3.3 Predicate-argument structures
</subsectionHeader>
<bodyText confidence="0.975093823529412">
In order to extract translation rules from PASs,
we want to localize a predicate word and its ar-
guments into one tree fragment. For example, in
Figure 2, we can use a tree fragment which takes
c0 as its root node and c1, t1, and c5 on its yield (=
leaf nodes of a tree fragment) to cover “killed” and
its subject and direct object arguments. We define
this kind of tree fragment to be a minimum cov-
ering tree. For example, the minimum covering
tree of {t1, c1, c5} is shown in the bottom-right
corner of Figure 2. The definition supplies us a
linear-time algorithm to directly find the tree frag-
ment that covers a PAS during both rule extracting
and rule matching when decoding an HPSG tree.
Algorithm 2 PASR extraction
Input: HPSG tree Et, foreign sentence F, and alignment A
Output: a PAS-based rule set R
</bodyText>
<listItem confidence="0.9385581">
1: R = {}
2: for node n G Leaves(Et) do
3: if Open(n.ARG) then
4: Tc = MinimumCoveringTree(Et, n, n.ARGs)
5: if root and leaf nodes of Tc are in fs then
6: generate a rule r using fragment Tc
7: R.append(r)
8: end if
9: end if
10: end for
</listItem>
<bodyText confidence="0.9992745">
See (Wu, 2010) for more examples of minimum
covering trees.
Taking a minimum covering tree as the tree
fragment, we can easily build a tree-to-string
translation rule that reflects the semantic depen-
dency of a PAS. The algorithm of PAS-based
rule (PASR) extraction is sketched in Algorithm
2. Suppose we are given a tuple of (F, Et, A).
Et is pre-processed by replacing HEAD and
SEM HEAD to be L, R, or S, and computing the
span and comp span of each node.
We extract PAS-based rules through one-time
traversal of the leaf nodes in Et (line 2). For each
leaf node n, we extract a minimum covering tree
T, if n contains at least one argument. That is, at
least one ARG(x) takes the value of some node
identifier, where x ranges 1 over 4 (line 3). Then,
we require the root and yield nodes of T, being in
the frontier set of Et (line 5). Based on T,, we can
easily build a tree-to-string translation rule by fur-
ther completing the right-hand-side string by sort-
ing the spans of T,’s leaf nodes, lexicalizing the
terminal node’s span(s), and assigning a variable
to each non-terminal node’s span. Maximum like-
lihood estimation is used to calculate the transla-
tion probabilities of each rule.
An example of PAS-based rule is shown in the
bottom-right corner of Figure 2. In the rule, the
subject and direct-object of “killed” are general-
ized into two variables, x0 and x1.
</bodyText>
<sectionHeader confidence="0.999904" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999798">
4.1 Translation models
</subsectionHeader>
<bodyText confidence="0.999779666666667">
We use a tree-to-string model and a string-to-tree
model for bidirectional Japanese-English transla-
tions. Both models use a phrase translation table
(PTT), an HPSG tree-based rule set (TRS), and
a PAS-based rule set (PRS). Since the three rule
sets are independently extracted and estimated, we
</bodyText>
<page confidence="0.994117">
330
</page>
<bodyText confidence="0.998909142857143">
use Minimum Error Rate Training (MERT) (Och,
2003) to tune the weights of the features from the
three rule sets on the development set.
Given a 1-best (localized) HPSG tree Et, the
tree-to-string decoder searches for the optimal
derivation d∗ that transforms Et into a Japanese
string among the set of all possible derivations D:
</bodyText>
<equation confidence="0.990049666666667">
d∗ =arg max {λ1 log pLM(τ(d)) + λ2|τ(d)|
d∈D
+ log s(d|Et)}. (2)
</equation>
<bodyText confidence="0.999980833333333">
Here, the first item is the language model (LM)
probability where τ(d) is the target string of
derivation d; the second item is the translation
length penalty; and the third item is the transla-
tion score, which is decomposed into a product of
feature values of rules:
</bodyText>
<equation confidence="0.976939">
s(d|Et) = ∏ f(r∈PTT)f(r∈TRS)f(r∈PRS).
r∈d
</equation>
<bodyText confidence="0.999109875">
This equation reflects that the translation rules in
one d come from three sets. Inspired by (Liu et
al., 2009b), it is appealing to combine these rule
sets together in one decoder because PTT provides
excellent rule coverages while TRS and PRS offer
linguistically motivated phrase selections and non-
local reorderings. Each f(r) is in turn a product of
five features:
</bodyText>
<equation confidence="0.992929">
f(r) = p(s|t)λ3 · p(t|s)λ4 · l(s|t)λ5 · l(t|s)λ6 · eλ7.
</equation>
<bodyText confidence="0.999934076923077">
Here, s/t represent the source/target part of a rule
in PTT, TRS, or PRS; p(·|·) and l(·|·) are transla-
tion probabilities and lexical weights of rules from
PTT, TRS, and PRS. The derivation length penalty
is controlled by λ7.
In our string-to-tree model, for efficient decod-
ing with integrated n-gram LM, we follow (Zhang
et al., 2006) and inversely binarize all translation
rules into Chomsky Normal Forms that contain
at most two variables and can be incrementally
scored by LM. In order to make use of the bina-
rized rules in the CKY decoding, we add two kinds
of glues rules:
</bodyText>
<equation confidence="0.9971815">
S → Xm(1), Xm(1),
S → S(1)Xm(2), S(1)Xm(2).
</equation>
<bodyText confidence="0.98870525">
Here Xm ranges over the nonterminals appearing
in a binarized rule set. These glue rules can be
seen as an extension from X to {Xm}of the two
glue rules described in (Chiang, 2007).
The string-to-tree decoder searches for the op-
timal derivation d∗ that parses a Japanese string
F into a packed forest of the set of all possible
derivations D:
</bodyText>
<equation confidence="0.999463333333333">
d∗ = arg max{λ1 log pLM(τ(d)) + λ2|τ(d)|
d∈D
+ λ3g(d) + log s(d|F)}. (3)
</equation>
<bodyText confidence="0.9999458">
This formula differs from Equation 2 by replacing
Et with F in s(d|·) and adding g(d), which is the
number of glue rules used in d. Further definitions
of s(d|F) and f(r) are identical with those used
in Equation 2.
</bodyText>
<subsectionHeader confidence="0.999421">
4.2 Decoding algorithms
</subsectionHeader>
<bodyText confidence="0.999932594594594">
In our translation models, we have made use
of three kinds of translation rule sets which are
trained separately. We perform derivation-level
combination as described in (Liu et al., 2009b) for
mixing different types of translation rules within
one derivation.
For tree-to-string translation, we use a bottom-
up beam search algorithm (Liu et al., 2006) for
decoding an HPSG tree Et. We keep at most 10
best derivations with distinct τ(d)s at each node.
Recall the definition of minimum covering tree,
which supports a faster way to retrieve available
rules from PRS without generating all the sub-
trees. That is, when node n fortunately to be the
root of some minimum covering tree(s), we use the
tree(s) to seek available PAS-based rules in PRS.
We keep a hash-table with the key to be the node
identifier of n and the value to be a priority queue
of available PAS-based rules. The hash-table is
easy to be filled by one-time traversal of the termi-
nal nodes in Et. At each terminal node, we seek
its minimum covering tree, retrieve PRS, and up-
date the hash-table. For example, suppose we are
decoding an HPSG tree (with gray nodes) shown
in Figure 2. At t1, we can extract its minimum
covering tree with the root node to be c0, then take
this tree fragment as the key to retrieve PRS, and
consequently put c0 and the available rules in the
hash-table. When decoding at c0, we can directly
access the hash-table looking for available PAS-
based rules.
In contrast, we use a CKY-style algorithm with
beam-pruning and cube-pruning (Chiang, 2007)
to decode Japanese sentences. For each Japanese
sentence F, the output of the chart-parsing algo-
rithm is expressed as a hypergraph representing a
set of derivations. Given such a hypergraph, we
</bodyText>
<page confidence="0.995305">
331
</page>
<table confidence="0.9988745">
Train Dev. Test
# of sentences 994K 2K 2K
# of Jp words 28.2M 57.4K 57.1K
# of En words 24.7M 50.3K 49.9K
</table>
<tableCaption confidence="0.999845">
Table 3: Statistics of the JST corpus.
</tableCaption>
<bodyText confidence="0.99813325">
use the Algorithm 3 described in (Huang and Chi-
ang, 2005) to extract its k-best (k = 500 in our
experiments) derivations. Since different deriva-
tions may lead to the same target language string,
we further adopt Algorithm 3’s modification, i.e.,
keep a hash-table to maintain the unique target
sentences (Huang et al., 2006), to efficiently gen-
erate the unique k-best translations.
</bodyText>
<subsectionHeader confidence="0.999091">
4.3 Setups
</subsectionHeader>
<bodyText confidence="0.999942096774194">
The JST Japanese-English paper abstract corpus4,
which consists of one million parallel sentences,
was used for training and testing. This corpus
was constructed from a Japanese-English paper
abstract corpus by using the method of Utiyama
and Isahara (2007). Table 3 shows the statistics
of this corpus. Making use of Enju 2.3.1, we suc-
cessfully parsed 987,401 English sentences in the
training set, with a parse rate of 99.3%. We mod-
ified this parser to output a packed forest for each
English sentence.
We executed GIZA++ (Och and Ney, 2003) and
grow-diag-final-and balancing strategy (Koehn et
al., 2007) on the training set to obtain a phrase-
aligned parallel corpus, from which bidirectional
phrase translation tables were estimated. SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) was em-
ployed to train 5-gram English and Japanese LMs
on the training set. We evaluated the translation
quality using the case-insensitive BLEU-4 metric
(Papineni et al., 2002). The MERT toolkit we used
is Z-mert5 (Zaidan, 2009).
The baseline system for comparison is Joshua
(Li et al., 2009), a freely available decoder for hi-
erarchical phrase-based SMT (Chiang, 2005). We
respectively extracted 4.5M and 5.3M translation
rules from the training set for the 4K English and
Japanese sentences in the development and test
sets. We used the default configuration of Joshua,
expect setting the maximum number of items/rules
and the k of k-best outputs to be the identical
</bodyText>
<footnote confidence="0.997875">
4http://www.jst.go.jp. The corpus can be conditionally
obtained from NTCIR-7 patent translation workshop home-
page: http://research.nii.ac.jp/ntcir/permission/ntcir-7/perm-
en-PATMT.html.
5http://www.cs.jhu.edu/ ozaidan/zmert/
</footnote>
<note confidence="0.734848">
PRS CS3 C3 FS F
</note>
<bodyText confidence="0.842481">
tree nodes TFS POS TFS POS TFS
Table 4: Statistics of several kinds of tree-to-string
rules. Here, the number is in million level and the
time is in hour.
200 for English-to-Japanese translation and 500
for Japanese-to-English translation.
We used four dual core Xeon machines
(4x3.0GHzx2CPU, 4x64GB memory) to run all
the experiments.
</bodyText>
<sectionHeader confidence="0.557662" genericHeader="evaluation">
4.4 Results
</sectionHeader>
<bodyText confidence="0.980572">
Table 4 illustrates the statistics of several transla-
tion rule sets, which are classified by:
</bodyText>
<listItem confidence="0.917768769230769">
• using TFSs or simple POS/phrasal tags (an-
notated by a superscript S) to represent tree
nodes;
• composed rules (PRS) extracted from the
PAS of 1-best HPSG trees;
• composed rules (C3), extracted from the tree
structures of 1-best HPSG trees, and 3 is the
maximum number of internal nodes in the
tree fragments; and
• forest-based rules (F), where the packed
forests are pre-pruned by the marginal
probability-based inside-outside algorithm
used in (Mi and Huang, 2008).
</listItem>
<bodyText confidence="0.999649411764706">
Table 5 reports the BLEU-4 scores achieved by
decoding the test set making use of Joshua and our
systems (t2s = tree-to-string and s2t = string-to-
tree) under numerous rule sets. We analyze this
table in terms of several aspects to prove the effec-
tiveness of deep syntactic information for SMT.
Let’s first look at the performance of TFSs. We
take CS3 and Fs as approximations of CFG-based
translation rules. Comparing the BLEU-4 scores
of PTT+CS3 and PTT+C3, we gained 0.56 (t2s)
and 0.57 (s2t) BLEU-4 points which are signifi-
cant improvements (p &lt; 0.05). Furthermore, we
gained 0.50 (t2s) and 0.62 (s2t) BLEU-4 points
from PTT+Fs to PTT+F, which are also signif-
icant improvements (p &lt; 0.05). The rich fea-
tures included in TFSs contribute to these im-
provements.
</bodyText>
<table confidence="0.866760823529412">
# rules
# tree types
extract time
0.9 62.1 83.9 92.5 103.7
0.4 23.5 34.7 40.6 45.2
3.5 - 98.6 - 121.2
332
Systems BLEU-t2s Decoding BLEU-s2t
Joshua 21.79 0.486 19.73
PTT 18.40 0.013 17.21
PTT+PRS 22.12 0.031 19.33
PTT+CS 23.56 2.686 20.59
3
PTT+C3 24.12 2.753 21.16
PTT+C3+PRS 24.13 2.930 21.20
PTT+FS 24.25 3.241 22.05
PTT+F 24.75 3.470 22.67
</table>
<tableCaption confidence="0.973607">
Table 5: BLEU-4 scores (%) achieved by Joshua
</tableCaption>
<bodyText confidence="0.9988735">
and our systems under numerous rule configura-
tions. The decoding time (seconds per sentence)
of tree-to-string translation is listed as well.
Also, BLEU-4 scores were inspiringly in-
creased 3.72 (t2s) and 2.12 (s2t) points by append-
ing PRS to PTT, comparing PTT with PTT+PRS.
Furthermore, in Table 5, the decoding time (sec-
onds per sentence) of tree-to-string translation by
using PTT+PRS is more than 86 times faster than
using the other tree-to-string rule sets. This sug-
gests that the direct generation of minimum cover-
ing trees for rule matching is extremely faster than
generating all subtrees of a tree node. Note that
PTT performed extremely bad compared with all
other systems or tree-based rule sets. The major
reason is that we did not perform any reordering
or distorting during decoding with PTT.
However, in both t2s and s2t systems, the
BLEU-4 score benefits of PRS were covered by
the composed rules: both PTT+C3 and PTT+C3
performed significant better (p &lt; 0.01) than
PTT+PRS, and there are no significant differences
when appending PRS to PTT+C3. The reason is
obvious: PRS is only a small subset of the com-
posed rules, and the probabilities of rules in PRS
were estimated by maximum likelihood, which is
fast but biased compared with EM based estima-
tion (Galley et al., 2006).
Finally, by using PTT+F, our systems achieved
the best BLEU-4 scores of 24.75% (t2s) and
22.67% (s2t), both are significantly better (p &lt;
0.01) than that achieved by Joshua.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985772727273">
We have proposed approaches of using deep syn-
tactic information for extracting fine-grained tree-
to-string translation rules from aligned HPSG
forest-string pairs. The main contributions are the
applications of GHKM-related algorithms (Galley
et al., 2006; Mi and Huang, 2008) to HPSG forests
and a linear-time algorithm for extracting com-
posed rules from predicate-argument structures.
We applied our fine-grained translation rules to a
tree-to-string system and an Hiero-style string-to-
tree system. Extensive experiments on large-scale
bidirectional Japanese-English translations testi-
fied the significant improvements on BLEU score.
We argue the fine-grained translation rules are
generic and applicable to many syntax-based SMT
frameworks such as the forest-to-string model (Mi
et al., 2008). Furthermore, it will be interesting
to extract fine-grained tree-to-tree translation rules
by integrating deep syntactic information in the
source and/or target language side(s). These tree-
to-tree rules are applicable for forest-to-tree trans-
lation models (Liu et al., 2009a).
</bodyText>
<sectionHeader confidence="0.999507" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999221888888889">
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan) and Japanese/Chinese Machine Translation
Project in Special Coordination Funds for Pro-
moting Science and Technology (MEXT, Japan),
and Microsoft Research Asia Machine Translation
Theme. The first author thanks Naoaki Okazaki
and Yusuke Miyao for their help and the anony-
mous reviewers for improving the earlier version.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999936782608696">
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. Ccg supertags in factored statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 9–
16, June.
Bob Carpenter. 1992. The Logic of Typed Feature
Structures. Cambridge University Press.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 218–
226, June.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, pages 263–270, Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Lingustics, 33(2):201–228.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Soci-
ety, 39:1–38.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings of HLT-NAACL.
</reference>
<page confidence="0.991112">
333
</page>
<reference confidence="0.999944642105264">
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL, pages 961–968, Sydney.
Hany Hassan, Khalil Sima’an, and Andy Way. 2007.
Supertagged phrase-based statistical machine trans-
lation. In Proceedings ofACL, pages 288–295, June.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of 7th AMTA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL 2007 Demo and Poster Ses-
sions, pages 177–180.
Zhifei Li, Chris Callison-Burch, Chris Dyery, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N. G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Demonstration of joshua: An open
source toolkit for parsing-based machine translation.
In Proceedings of the ACL-IJCNLP 2009 Software
Demonstrations, pages 25–28, August.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
transaltion. In Proceedings of COLING-ACL, pages
609–616, Sydney, Australia.
Yang Liu, Yajuan L¨u, and Qun Liu. 2009a. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL-IJCNLP, pages 558–566, August.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009b.
Joint decoding with multiple translation models. In
Proceedings of ACL-IJCNLP, pages 576–584, Au-
gust.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2007. Efficient hpsg parsing with supertagging and
cfg-filtering. In Proceedings of IJCAI, pages 1671–
1676, January.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 206–214, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192–199, Columbus, Ohio.
Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsu-
jii. 2003. Probabilistic modeling of argument struc-
tures including non-local dependencies. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, pages 285–
291, Borovets.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160–167.
Stephan Oepen, Erik Velldal, Jan Tore Lønning, Paul
Meurer, and Victoria Ros´en. 2007. Towards hy-
brid quality-oriented machine translation - on lin-
guistics and probabilities in mt. In Proceedings
of the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI-07), September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311–318.
Stefan Riezler and John T. Maxwell, III. 2006. Gram-
matical machine translation. In Proceedings of HLT-
NAACL, pages 248–255.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, pages
901–904.
Masao Utiyama and Hitoshi Isahara. 2007. A
japanese-english patent parallel corpus. In Proceed-
ings of MT Summit XI, pages 475–482, Copenhagen.
Xianchao Wu. 2010. Statistical Machine Transla-
tion Using Large-Scale Lexicon and Deep Syntactic
Structures. Ph.D. dissertation. Department of Com-
puter Science, The University of Tokyo.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of HLT-NAACL,
pages 256–263, June.
</reference>
<page confidence="0.999061">
334
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.493795">
<title confidence="0.904883">Fine-grained Tree-to-String Translation Rule Extraction</title>
<affiliation confidence="0.8631">of Computer Science, The University of Tokyo</affiliation>
<address confidence="0.900954">7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan</address>
<affiliation confidence="0.9939395">of Computer Science, University of Manchester Centre for Text Mining (NaCTeM)</affiliation>
<address confidence="0.99506">Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK</address>
<email confidence="0.972034">matuzaki,</email>
<abstract confidence="0.982830263157895">Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. In this we propose to use syntacinformation obtaining fine-grained translation rules. A head-driven phrase structure grammar (HPSG) parser is used to obtain the deep syntactic information, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English translations testified the effectiveness of our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Ccg supertags in factored statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="12547" citStr="Birch et al. (2007)" startWordPosition="2038" endWordPosition="2041">ntactic information yielded by an HPSG parser for fine-grained rule extraction, we would like to briefly review what kinds of deep syntactic information have been employed for SMT. Two kinds of supertags, from Lexicalized TreeAdjoining Grammar and Combinatory Categorial Grammar (CCG), have been used as lexical syntactic descriptions (Hassan et al., 2007) for phrasebased SMT (Koehn et al., 2007). By introducing supertags into the target language side, i.e., the target language model and the target side of the phrase table, significant improvement was achieved for Arabic-to-English translation. Birch et al. (2007) also reported a significant improvement for Dutch-English translation by applying CCG supertags at a word level to a factorized SMT system (Koehn et al., 2007). In this paper, we also make use of supertags on the English language side. In an HPSG parse tree, these lexical syntactic descriptions are included in the LEXENTRY feature (refer to Table 2) of a lexical node (Matsuzaki et al., 2007). For example, the LEXENTRY feature of “t1:killed” takes the value of [NP.nom&lt;V.bse&gt;NP.acc]_lxm-past _verb_rule in Figure 2. In which, [NP.nom&lt;V.bse&gt;NP.acc] is an HPSG style supertag, which tells us that t</context>
</contexts>
<marker>Birch, Osborne, Koehn, 2007</marker>
<rawString>Alexandra Birch, Miles Osborne, and Philipp Koehn. 2007. Ccg supertags in factored statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 9– 16, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>The Logic of Typed Feature Structures.</title>
<date>1992</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="16940" citStr="Carpenter, 1992" startWordPosition="2725" endWordPosition="2726">(top) and lexical nodes (bottom) are listed separately. description in the English parse tree/forest, as well as those in the translation rules. Second, we can identify sub-trees in a parse tree/forest that correspond to basic units of the semantics, namely sub-trees covering a predicate and its arguments, by using the semantic representation given in the signs. We expect that extraction of translation rules based on such semantically-connected subtrees will give a compact and effective set of translation rules. A sign in the HPSG tree/forest is represented by a typed feature structure (TFS) (Carpenter, 1992). A TFS is a directed-acyclic graph (DAG) wherein the edges are labeled with feature names and the nodes (feature values) are typed. In the original HPSG formalism, the types are defined in a hierarchy and the DAG can have arbitrary shape (e.g., it can be of any depth). We however use a simplified form of TFS, for simplicity of the algorithms. In the simplified form, a TFS is converted to a (flat) set of pairs of feature names and their values. Table 2 lists the features used in this paper, which are a subset of those in the original output from an HPSG parser, Enju3. The HPSG forest shown in </context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Bob Carpenter. 1992. The Logic of Typed Feature Structures. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>218--226</pages>
<contexts>
<context position="1404" citStr="Chiang et al., 2009" startWordPosition="179" endWordPosition="182"> a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English translations testified the effectiveness of our approach. 1 Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009), tree-to-string translation (Liu et al., 2006; Huang et al., 2006), and forest-to-string translation (Mi et al., 2008; Mi and Huang, 2008). The algorithms proposed by Galley et al. (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. However, current tree-to-string rules only make use of Probabilistic Context-Free</context>
<context position="5569" citStr="Chiang et al., 2009" startWordPosition="804" endWordPosition="807">d rule extraction algorithms are described in Section 3. Section 4 gives the experiments for applying fine-grained translation rules to large-scale Japanese-English translation tasks. Finally, we conclude in Section 5. 2 Related Work 2.1 Tree-to-string and string-to-tree translations Tree-to-string translation (Liu et al., 2006; Huang et al., 2006) first uses a parser to parse a source sentence into a 1-best tree and then searches for the best derivation that segments and converts the tree into a target string. In contrast, string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009) is like bilingual parsing. That is, giving a (bilingual) translation grammar and a source sentence, we are trying to construct a parse forest in the target language. Consequently, the translation results can be collected from the leaves of the parse forest. Figure 1 illustrates the training and decoding processes of bidirectional Japanese-English translations. The English sentence is “John killed Mary” and the Japanese sentence is “jyon ha mari wo koroshita”, in which the function words “ha” and “wo” are not aligned with any English word. 2.2 Tree/forest-based rule extraction Galley et al. (2</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of HLT-NAACL, pages 218– 226, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="28382" citStr="Chiang, 2005" startWordPosition="4744" endWordPosition="4746">ules in the hash-table. When decoding at c0, we can directly access the hash-table looking for available PASbased rules. In contrast, we use a CKY-style algorithm with beam-pruning and cube-pruning (Chiang, 2007) to decode Japanese sentences. For each Japanese sentence F, the output of the chart-parsing algorithm is expressed as a hypergraph representing a set of derivations. Given such a hypergraph, we 331 Train Dev. Test # of sentences 994K 2K 2K # of Jp words 28.2M 57.4K 57.1K # of En words 24.7M 50.3K 49.9K Table 3: Statistics of the JST corpus. use the Algorithm 3 described in (Huang and Chiang, 2005) to extract its k-best (k = 500 in our experiments) derivations. Since different derivations may lead to the same target language string, we further adopt Algorithm 3’s modification, i.e., keep a hash-table to maintain the unique target sentences (Huang et al., 2006), to efficiently generate the unique k-best translations. 4.3 Setups The JST Japanese-English paper abstract corpus4, which consists of one million parallel sentences, was used for training and testing. This corpus was constructed from a Japanese-English paper abstract corpus by using the method of Utiyama and Isahara (2007). Table</context>
<context position="29871" citStr="Chiang, 2005" startWordPosition="4980" endWordPosition="4981">03) and grow-diag-final-and balancing strategy (Koehn et al., 2007) on the training set to obtain a phrasealigned parallel corpus, from which bidirectional phrase translation tables were estimated. SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train 5-gram English and Japanese LMs on the training set. We evaluated the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). The MERT toolkit we used is Z-mert5 (Zaidan, 2009). The baseline system for comparison is Joshua (Li et al., 2009), a freely available decoder for hierarchical phrase-based SMT (Chiang, 2005). We respectively extracted 4.5M and 5.3M translation rules from the training set for the 4K English and Japanese sentences in the development and test sets. We used the default configuration of Joshua, expect setting the maximum number of items/rules and the k of k-best outputs to be the identical 4http://www.jst.go.jp. The corpus can be conditionally obtained from NTCIR-7 patent translation workshop homepage: http://research.nii.ac.jp/ntcir/permission/ntcir-7/permen-PATMT.html. 5http://www.cs.jhu.edu/ ozaidan/zmert/ PRS CS3 C3 FS F tree nodes TFS POS TFS POS TFS Table 4: Statistics of severa</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL, pages 263–270, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Lingustics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="25966" citStr="Chiang, 2007" startWordPosition="4319" endWordPosition="4320">length penalty is controlled by λ7. In our string-to-tree model, for efficient decoding with integrated n-gram LM, we follow (Zhang et al., 2006) and inversely binarize all translation rules into Chomsky Normal Forms that contain at most two variables and can be incrementally scored by LM. In order to make use of the binarized rules in the CKY decoding, we add two kinds of glues rules: S → Xm(1), Xm(1), S → S(1)Xm(2), S(1)Xm(2). Here Xm ranges over the nonterminals appearing in a binarized rule set. These glue rules can be seen as an extension from X to {Xm}of the two glue rules described in (Chiang, 2007). The string-to-tree decoder searches for the optimal derivation d∗ that parses a Japanese string F into a packed forest of the set of all possible derivations D: d∗ = arg max{λ1 log pLM(τ(d)) + λ2|τ(d)| d∈D + λ3g(d) + log s(d|F)}. (3) This formula differs from Equation 2 by replacing Et with F in s(d|·) and adding g(d), which is the number of glue rules used in d. Further definitions of s(d|F) and f(r) are identical with those used in Equation 2. 4.2 Decoding algorithms In our translation models, we have made use of three kinds of translation rule sets which are trained separately. We perform</context>
<context position="27981" citStr="Chiang, 2007" startWordPosition="4672" endWordPosition="4673">raversal of the terminal nodes in Et. At each terminal node, we seek its minimum covering tree, retrieve PRS, and update the hash-table. For example, suppose we are decoding an HPSG tree (with gray nodes) shown in Figure 2. At t1, we can extract its minimum covering tree with the root node to be c0, then take this tree fragment as the key to retrieve PRS, and consequently put c0 and the available rules in the hash-table. When decoding at c0, we can directly access the hash-table looking for available PASbased rules. In contrast, we use a CKY-style algorithm with beam-pruning and cube-pruning (Chiang, 2007) to decode Japanese sentences. For each Japanese sentence F, the output of the chart-parsing algorithm is expressed as a hypergraph representing a set of derivations. Given such a hypergraph, we 331 Train Dev. Test # of sentences 994K 2K 2K # of Jp words 28.2M 57.4K 57.1K # of En words 24.7M 50.3K 49.9K Table 3: Statistics of the JST corpus. use the Algorithm 3 described in (Huang and Chiang, 2005) to extract its k-best (k = 500 in our experiments) derivations. Since different derivations may lead to the same target language string, we further adopt Algorithm 3’s modification, i.e., keep a has</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Lingustics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="10699" citStr="Dempster et al., 1977" startWordPosition="1733" endWordPosition="1736">he tree fragment in a rule (bottom-left corner of the figure). In order to include richer context information and account for multiple interpretations of unaligned words of foreign language, minimal rules which share adjacent tree fragments are connected together to form composed rules (Galley et al., 2006). For each aligned tree-string pair, Galley et al. (2006) constructed a derivation-forest, in which composed rules were generated, unaligned words of foreign language were consistently attached, and the translation probabilities of rules were estimated by using ExpectationMaximization (EM) (Dempster et al., 1977) training. For example, by combining the minimal rules of 1, 4, and 5, we obtain a composed rule, as shown in the bottom-right corner of Figure 2. Considering the parse error problem in the 1-best or k-best parse trees, Mi and Huang (2008) extracted tree-to-string translation rules from aligned packed forest-string pairs. A forest compactly encodes exponentially many trees 327 rather than the 1-best tree used by Galley et al. (2004; 2006). Two problems were managed to be tackled during extracting rules from an aligned forest-string pair: where to cut and how to cut. Equation 1 was used again t</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="1361" citStr="Galley et al., 2004" startWordPosition="171" endWordPosition="174">deep syntactic information, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English translations testified the effectiveness of our approach. 1 Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009), tree-to-string translation (Liu et al., 2006; Huang et al., 2006), and forest-to-string translation (Mi et al., 2008; Mi and Huang, 2008). The algorithms proposed by Galley et al. (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. However, current tree-to-string rules </context>
<context position="5526" citStr="Galley et al., 2004" startWordPosition="796" endWordPosition="799">SG grammar and our proposal of fine-grained rule extraction algorithms are described in Section 3. Section 4 gives the experiments for applying fine-grained translation rules to large-scale Japanese-English translation tasks. Finally, we conclude in Section 5. 2 Related Work 2.1 Tree-to-string and string-to-tree translations Tree-to-string translation (Liu et al., 2006; Huang et al., 2006) first uses a parser to parse a source sentence into a 1-best tree and then searches for the best derivation that segments and converts the tree into a target string. In contrast, string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009) is like bilingual parsing. That is, giving a (bilingual) translation grammar and a source sentence, we are trying to construct a parse forest in the target language. Consequently, the translation results can be collected from the leaves of the parse forest. Figure 1 illustrates the training and decoding processes of bidirectional Japanese-English translations. The English sentence is “John killed Mary” and the Japanese sentence is “jyon ha mari wo koroshita”, in which the function words “ha” and “wo” are not aligned with any English word. 2.2 Tree/fo</context>
<context position="9963" citStr="Galley et al., 2004" startWordPosition="1616" endWordPosition="1619">r. The forest includes two parse trees by taking “Mary” as a modifier (t3, t4) or an argument (t1, t2) of “killed”. Arrows with broken lines denote the PAS dependencies from the terminal node t1 to its argument nodes (c1 and c5). The scores of the hyperedges are attached to the forest as well. the nodes in fs as the root and leaf nodes, a wellformed fragmentation of Et is generated. With fs computed, rules are extracted through a depthfirst traversal of Et: we cut Et at all nodes in fs to form tree fragments and extract a rule for each fragment. These extracted rules are called minimal rules (Galley et al., 2004). For example, the 1- best tree (with gray nodes) in Figure 2 is cut into 7 pieces, each of which corresponds to the tree fragment in a rule (bottom-left corner of the figure). In order to include richer context information and account for multiple interpretations of unaligned words of foreign language, minimal rules which share adjacent tree fragments are connected together to form composed rules (Galley et al., 2006). For each aligned tree-string pair, Galley et al. (2006) constructed a derivation-forest, in which composed rules were generated, unaligned words of foreign language were consis</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>961--968</pages>
<location>Sydney.</location>
<contexts>
<context position="1382" citStr="Galley et al., 2006" startWordPosition="175" endWordPosition="178">ation, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English translations testified the effectiveness of our approach. 1 Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009), tree-to-string translation (Liu et al., 2006; Huang et al., 2006), and forest-to-string translation (Mi et al., 2008; Mi and Huang, 2008). The algorithms proposed by Galley et al. (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. However, current tree-to-string rules only make use of Prob</context>
<context position="4374" citStr="Galley et al., 2006" startWordPosition="628" endWordPosition="631"> Mary.” versus “John was killed by Mary.” 325 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 325–334, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics deep syntactic information of an English sentence, which includes a fine-grained description of the syntactic property and a semantic representation of the sentence. We extract fine-grained translation rules from aligned HPSG tree/forest-string pairs. We localize an HPSG tree/forest to make it segmentable at any nodes to fit the extraction algorithms described in (Galley et al., 2006; Mi and Huang, 2008). We also propose a linear-time algorithm for extracting composed rules guided by predicate-argument structures. The effectiveness of the rules are testified in our tree-to-string and string-to-tree systems, taking bidirectional Japanese-English translations as our test cases. This paper is organized as follows. In Section 2, we briefly review the tree-to-string and string-totree translation frameworks, tree-to-string rule extraction algorithms, and rich syntactic information previously used for SMT. The HPSG grammar and our proposal of fine-grained rule extraction algorit</context>
<context position="10385" citStr="Galley et al., 2006" startWordPosition="1687" endWordPosition="1690">d through a depthfirst traversal of Et: we cut Et at all nodes in fs to form tree fragments and extract a rule for each fragment. These extracted rules are called minimal rules (Galley et al., 2004). For example, the 1- best tree (with gray nodes) in Figure 2 is cut into 7 pieces, each of which corresponds to the tree fragment in a rule (bottom-left corner of the figure). In order to include richer context information and account for multiple interpretations of unaligned words of foreign language, minimal rules which share adjacent tree fragments are connected together to form composed rules (Galley et al., 2006). For each aligned tree-string pair, Galley et al. (2006) constructed a derivation-forest, in which composed rules were generated, unaligned words of foreign language were consistently attached, and the translation probabilities of rules were estimated by using ExpectationMaximization (EM) (Dempster et al., 1977) training. For example, by combining the minimal rules of 1, 4, and 5, we obtain a composed rule, as shown in the bottom-right corner of Figure 2. Considering the parse error problem in the 1-best or k-best parse trees, Mi and Huang (2008) extracted tree-to-string translation rules fro</context>
<context position="14484" citStr="Galley et al., 2006" startWordPosition="2335" endWordPosition="2338">n) architecture with a semantic transfer backbone is taken as the vantage point of this project. In contrast, the fine-grained tree-to-string translation rule extraction approaches in this paper are totally data-driven, and easily applicable to numerous language pairs by taking English as the source or target language. 3 Fine-grained rule extraction We now introduce the deep syntactic information generated by an HPSG parser and then describe our approaches for fine-grained tree-tostring rule extraction. Especially, we localize an HPSG tree/forest to fit the extraction algorithms described in (Galley et al., 2006; Mi and Huang, 2008). Also, we propose a linear-time composed rule extraction algorithm by making use of predicate-argument structures. 3.1 Deep syntactic information by HPSG parsing Head-driven phrase structure grammar (HPSG) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure called a sign. A sign gives a factored representation of the syntactic features of a word/phrase, as well as a representation of their semantic content. Phrases and words represented by signs are composed into larger phrases by applications of sc</context>
<context position="19407" citStr="Galley et al., 2006" startWordPosition="3148" endWordPosition="3151"> is a trivial packed forest, we only use the term forest to expand our discussion, hereafter. Recall that there are pointer-valued features in the TFSs (Table 2) which prevent arbitrary segmentation of a packed forest. Hence, we have to localize an HPSG forest. For example, there are ARG pointers from t1 to c1 and c5 in the HPSG forest of Figure 2. However, the three nodes are not included in one (minimal) translation rule. This problem is caused by not considering the predicate argument dependency among t1, c1, and c5 while performing the GHKM algorithm. We can combine several minimal rules (Galley et al., 2006) together to address this dependency. Yet we have a faster way to tackle PASs, as will be described in the next subsection. Even if we omit ARG, there are still two kinds of pointer-valued features in TFSs, HEAD and SEM HEAD. Localizing these pointer-valuedfeatures is straightforward, since during parsing, the HEAD and SEM HEAD of a node are automatically transferred to its mother node. That is, the syntactic and semantic head of a node only take John kill ARG2 Mary ARG1 ARG1 She ignore ARG2 fact I ARG1 dispute ARG1 want ARG2 329 Algorithm 1 Fine-grained rule extraction Input: HPSG tree/forest</context>
<context position="21224" citStr="Galley et al., 2006" startWordPosition="3479" endWordPosition="3482">tical value to be c3 in Figure 2. To extract tree-to-string rules from the tree structures of an HPSG forest, our solution is to pre-process an HPSG forest in the following way: • for a phrasal hypernode, replace its HEAD and SEM HEAD value with L, R, or S, which respectively represent left daughter, right daughter, or single daughter (line 2 and 7); and, • for a lexical node, ARG(x) and PRED features are ignored (line 4). A pure syntactic-based HPSG forest without any pointer-valued features can be yielded through this pre-processing for the consequent execution of the extraction algorithms (Galley et al., 2006; Mi and Huang, 2008). 3.3 Predicate-argument structures In order to extract translation rules from PASs, we want to localize a predicate word and its arguments into one tree fragment. For example, in Figure 2, we can use a tree fragment which takes c0 as its root node and c1, t1, and c5 on its yield (= leaf nodes of a tree fragment) to cover “killed” and its subject and direct object arguments. We define this kind of tree fragment to be a minimum covering tree. For example, the minimum covering tree of {t1, c1, c5} is shown in the bottom-right corner of Figure 2. The definition supplies us a </context>
<context position="33769" citStr="Galley et al., 2006" startWordPosition="5612" endWordPosition="5615">her systems or tree-based rule sets. The major reason is that we did not perform any reordering or distorting during decoding with PTT. However, in both t2s and s2t systems, the BLEU-4 score benefits of PRS were covered by the composed rules: both PTT+C3 and PTT+C3 performed significant better (p &lt; 0.01) than PTT+PRS, and there are no significant differences when appending PRS to PTT+C3. The reason is obvious: PRS is only a small subset of the composed rules, and the probabilities of rules in PRS were estimated by maximum likelihood, which is fast but biased compared with EM based estimation (Galley et al., 2006). Finally, by using PTT+F, our systems achieved the best BLEU-4 scores of 24.75% (t2s) and 22.67% (s2t), both are significantly better (p &lt; 0.01) than that achieved by Joshua. 5 Conclusion We have proposed approaches of using deep syntactic information for extracting fine-grained treeto-string translation rules from aligned HPSG forest-string pairs. The main contributions are the applications of GHKM-related algorithms (Galley et al., 2006; Mi and Huang, 2008) to HPSG forests and a linear-time algorithm for extracting composed rules from predicate-argument structures. We applied our fine-grain</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-ACL, pages 961–968, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hany Hassan</author>
<author>Khalil Sima’an</author>
<author>Andy Way</author>
</authors>
<title>Supertagged phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>288--295</pages>
<marker>Hassan, Sima’an, Way, 2007</marker>
<rawString>Hany Hassan, Khalil Sima’an, and Andy Way. 2007. Supertagged phrase-based statistical machine translation. In Proceedings ofACL, pages 288–295, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="28382" citStr="Huang and Chiang, 2005" startWordPosition="4742" endWordPosition="4746">vailable rules in the hash-table. When decoding at c0, we can directly access the hash-table looking for available PASbased rules. In contrast, we use a CKY-style algorithm with beam-pruning and cube-pruning (Chiang, 2007) to decode Japanese sentences. For each Japanese sentence F, the output of the chart-parsing algorithm is expressed as a hypergraph representing a set of derivations. Given such a hypergraph, we 331 Train Dev. Test # of sentences 994K 2K 2K # of Jp words 28.2M 57.4K 57.1K # of En words 24.7M 50.3K 49.9K Table 3: Statistics of the JST corpus. use the Algorithm 3 described in (Huang and Chiang, 2005) to extract its k-best (k = 500 in our experiments) derivations. Since different derivations may lead to the same target language string, we further adopt Algorithm 3’s modification, i.e., keep a hash-table to maintain the unique target sentences (Huang et al., 2006), to efficiently generate the unique k-best translations. 4.3 Setups The JST Japanese-English paper abstract corpus4, which consists of one million parallel sentences, was used for training and testing. This corpus was constructed from a Japanese-English paper abstract corpus by using the method of Utiyama and Isahara (2007). Table</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of 7th AMTA.</booktitle>
<contexts>
<context position="1471" citStr="Huang et al., 2006" startWordPosition="189" endWordPosition="192"> representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English translations testified the effectiveness of our approach. 1 Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009), tree-to-string translation (Liu et al., 2006; Huang et al., 2006), and forest-to-string translation (Mi et al., 2008; Mi and Huang, 2008). The algorithms proposed by Galley et al. (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. However, current tree-to-string rules only make use of Probabilistic Context-Free Grammar tree fragments, in which part-of-speech (POS) or koroshita</context>
<context position="5299" citStr="Huang et al., 2006" startWordPosition="758" endWordPosition="761">per is organized as follows. In Section 2, we briefly review the tree-to-string and string-totree translation frameworks, tree-to-string rule extraction algorithms, and rich syntactic information previously used for SMT. The HPSG grammar and our proposal of fine-grained rule extraction algorithms are described in Section 3. Section 4 gives the experiments for applying fine-grained translation rules to large-scale Japanese-English translation tasks. Finally, we conclude in Section 5. 2 Related Work 2.1 Tree-to-string and string-to-tree translations Tree-to-string translation (Liu et al., 2006; Huang et al., 2006) first uses a parser to parse a source sentence into a 1-best tree and then searches for the best derivation that segments and converts the tree into a target string. In contrast, string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009) is like bilingual parsing. That is, giving a (bilingual) translation grammar and a source sentence, we are trying to construct a parse forest in the target language. Consequently, the translation results can be collected from the leaves of the parse forest. Figure 1 illustrates the training and decoding processes of bidirection</context>
<context position="28649" citStr="Huang et al., 2006" startWordPosition="4786" endWordPosition="4789">sentence F, the output of the chart-parsing algorithm is expressed as a hypergraph representing a set of derivations. Given such a hypergraph, we 331 Train Dev. Test # of sentences 994K 2K 2K # of Jp words 28.2M 57.4K 57.1K # of En words 24.7M 50.3K 49.9K Table 3: Statistics of the JST corpus. use the Algorithm 3 described in (Huang and Chiang, 2005) to extract its k-best (k = 500 in our experiments) derivations. Since different derivations may lead to the same target language string, we further adopt Algorithm 3’s modification, i.e., keep a hash-table to maintain the unique target sentences (Huang et al., 2006), to efficiently generate the unique k-best translations. 4.3 Setups The JST Japanese-English paper abstract corpus4, which consists of one million parallel sentences, was used for training and testing. This corpus was constructed from a Japanese-English paper abstract corpus by using the method of Utiyama and Isahara (2007). Table 3 shows the statistics of this corpus. Making use of Enju 2.3.1, we successfully parsed 987,401 English sentences in the training set, with a parse rate of 99.3%. We modified this parser to output a packed forest for each English sentence. We executed GIZA++ (Och an</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of 7th AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra</location>
<contexts>
<context position="12325" citStr="Koehn et al., 2007" startWordPosition="2005" endWordPosition="2008">t can be segmented properly into a set of tree fragments, each of which can be used to generate a tree-to-string translation rule. 2.3 Rich syntactic information for SMT Before describing our approaches of applying deep syntactic information yielded by an HPSG parser for fine-grained rule extraction, we would like to briefly review what kinds of deep syntactic information have been employed for SMT. Two kinds of supertags, from Lexicalized TreeAdjoining Grammar and Combinatory Categorial Grammar (CCG), have been used as lexical syntactic descriptions (Hassan et al., 2007) for phrasebased SMT (Koehn et al., 2007). By introducing supertags into the target language side, i.e., the target language model and the target side of the phrase table, significant improvement was achieved for Arabic-to-English translation. Birch et al. (2007) also reported a significant improvement for Dutch-English translation by applying CCG supertags at a word level to a factorized SMT system (Koehn et al., 2007). In this paper, we also make use of supertags on the English language side. In an HPSG parse tree, these lexical syntactic descriptions are included in the LEXENTRY feature (refer to Table 2) of a lexical node (Matsuz</context>
<context position="29325" citStr="Koehn et al., 2007" startWordPosition="4893" endWordPosition="4896"> 4.3 Setups The JST Japanese-English paper abstract corpus4, which consists of one million parallel sentences, was used for training and testing. This corpus was constructed from a Japanese-English paper abstract corpus by using the method of Utiyama and Isahara (2007). Table 3 shows the statistics of this corpus. Making use of Enju 2.3.1, we successfully parsed 987,401 English sentences in the training set, with a parse rate of 99.3%. We modified this parser to output a packed forest for each English sentence. We executed GIZA++ (Och and Ney, 2003) and grow-diag-final-and balancing strategy (Koehn et al., 2007) on the training set to obtain a phrasealigned parallel corpus, from which bidirectional phrase translation tables were estimated. SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train 5-gram English and Japanese LMs on the training set. We evaluated the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). The MERT toolkit we used is Z-mert5 (Zaidan, 2009). The baseline system for comparison is Joshua (Li et al., 2009), a freely available decoder for hierarchical phrase-based SMT (Chiang, 2005). We respectively extracted 4.5M and 5.3M translation </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyery</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren N G Thornton</author>
<author>Jonathan Weese</author>
<author>Omar F Zaidan</author>
</authors>
<title>Demonstration of joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Software Demonstrations,</booktitle>
<pages>25--28</pages>
<contexts>
<context position="29794" citStr="Li et al., 2009" startWordPosition="4967" endWordPosition="4970">t a packed forest for each English sentence. We executed GIZA++ (Och and Ney, 2003) and grow-diag-final-and balancing strategy (Koehn et al., 2007) on the training set to obtain a phrasealigned parallel corpus, from which bidirectional phrase translation tables were estimated. SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train 5-gram English and Japanese LMs on the training set. We evaluated the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). The MERT toolkit we used is Z-mert5 (Zaidan, 2009). The baseline system for comparison is Joshua (Li et al., 2009), a freely available decoder for hierarchical phrase-based SMT (Chiang, 2005). We respectively extracted 4.5M and 5.3M translation rules from the training set for the 4K English and Japanese sentences in the development and test sets. We used the default configuration of Joshua, expect setting the maximum number of items/rules and the k of k-best outputs to be the identical 4http://www.jst.go.jp. The corpus can be conditionally obtained from NTCIR-7 patent translation workshop homepage: http://research.nii.ac.jp/ntcir/permission/ntcir-7/permen-PATMT.html. 5http://www.cs.jhu.edu/ ozaidan/zmert/</context>
</contexts>
<marker>Li, Callison-Burch, Dyery, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyery, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan. 2009. Demonstration of joshua: An open source toolkit for parsing-based machine translation. In Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25–28, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment templates for statistical machine transaltion.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>609--616</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1450" citStr="Liu et al., 2006" startWordPosition="185" endWordPosition="188">rty and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English translations testified the effectiveness of our approach. 1 Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009), tree-to-string translation (Liu et al., 2006; Huang et al., 2006), and forest-to-string translation (Mi et al., 2008; Mi and Huang, 2008). The algorithms proposed by Galley et al. (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. However, current tree-to-string rules only make use of Probabilistic Context-Free Grammar tree fragments, in which part-of-spee</context>
<context position="5278" citStr="Liu et al., 2006" startWordPosition="754" endWordPosition="757">est cases. This paper is organized as follows. In Section 2, we briefly review the tree-to-string and string-totree translation frameworks, tree-to-string rule extraction algorithms, and rich syntactic information previously used for SMT. The HPSG grammar and our proposal of fine-grained rule extraction algorithms are described in Section 3. Section 4 gives the experiments for applying fine-grained translation rules to large-scale Japanese-English translation tasks. Finally, we conclude in Section 5. 2 Related Work 2.1 Tree-to-string and string-to-tree translations Tree-to-string translation (Liu et al., 2006; Huang et al., 2006) first uses a parser to parse a source sentence into a 1-best tree and then searches for the best derivation that segments and converts the tree into a target string. In contrast, string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009) is like bilingual parsing. That is, giving a (bilingual) translation grammar and a source sentence, we are trying to construct a parse forest in the target language. Consequently, the translation results can be collected from the leaves of the parse forest. Figure 1 illustrates the training and decoding pro</context>
<context position="26793" citStr="Liu et al., 2006" startWordPosition="4458" endWordPosition="4461">λ3g(d) + log s(d|F)}. (3) This formula differs from Equation 2 by replacing Et with F in s(d|·) and adding g(d), which is the number of glue rules used in d. Further definitions of s(d|F) and f(r) are identical with those used in Equation 2. 4.2 Decoding algorithms In our translation models, we have made use of three kinds of translation rule sets which are trained separately. We perform derivation-level combination as described in (Liu et al., 2009b) for mixing different types of translation rules within one derivation. For tree-to-string translation, we use a bottomup beam search algorithm (Liu et al., 2006) for decoding an HPSG tree Et. We keep at most 10 best derivations with distinct τ(d)s at each node. Recall the definition of minimum covering tree, which supports a faster way to retrieve available rules from PRS without generating all the subtrees. That is, when node n fortunately to be the root of some minimum covering tree(s), we use the tree(s) to seek available PAS-based rules in PRS. We keep a hash-table with the key to be the node identifier of n and the value to be a priority queue of available PAS-based rules. The hash-table is easy to be filled by one-time traversal of the terminal </context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment templates for statistical machine transaltion. In Proceedings of COLING-ACL, pages 609–616, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Improving tree-to-tree translation with packed forests.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>558--566</pages>
<marker>Liu, L¨u, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan L¨u, and Qun Liu. 2009a. Improving tree-to-tree translation with packed forests. In Proceedings of ACL-IJCNLP, pages 558–566, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Haitao Mi</author>
<author>Yang Feng</author>
<author>Qun Liu</author>
</authors>
<title>Joint decoding with multiple translation models.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>576--584</pages>
<contexts>
<context position="24848" citStr="Liu et al., 2009" startWordPosition="4123" endWordPosition="4126">oder searches for the optimal derivation d∗ that transforms Et into a Japanese string among the set of all possible derivations D: d∗ =arg max {λ1 log pLM(τ(d)) + λ2|τ(d)| d∈D + log s(d|Et)}. (2) Here, the first item is the language model (LM) probability where τ(d) is the target string of derivation d; the second item is the translation length penalty; and the third item is the translation score, which is decomposed into a product of feature values of rules: s(d|Et) = ∏ f(r∈PTT)f(r∈TRS)f(r∈PRS). r∈d This equation reflects that the translation rules in one d come from three sets. Inspired by (Liu et al., 2009b), it is appealing to combine these rule sets together in one decoder because PTT provides excellent rule coverages while TRS and PRS offer linguistically motivated phrase selections and nonlocal reorderings. Each f(r) is in turn a product of five features: f(r) = p(s|t)λ3 · p(t|s)λ4 · l(s|t)λ5 · l(t|s)λ6 · eλ7. Here, s/t represent the source/target part of a rule in PTT, TRS, or PRS; p(·|·) and l(·|·) are translation probabilities and lexical weights of rules from PTT, TRS, and PRS. The derivation length penalty is controlled by λ7. In our string-to-tree model, for efficient decoding with in</context>
<context position="26629" citStr="Liu et al., 2009" startWordPosition="4433" endWordPosition="4436">ptimal derivation d∗ that parses a Japanese string F into a packed forest of the set of all possible derivations D: d∗ = arg max{λ1 log pLM(τ(d)) + λ2|τ(d)| d∈D + λ3g(d) + log s(d|F)}. (3) This formula differs from Equation 2 by replacing Et with F in s(d|·) and adding g(d), which is the number of glue rules used in d. Further definitions of s(d|F) and f(r) are identical with those used in Equation 2. 4.2 Decoding algorithms In our translation models, we have made use of three kinds of translation rule sets which are trained separately. We perform derivation-level combination as described in (Liu et al., 2009b) for mixing different types of translation rules within one derivation. For tree-to-string translation, we use a bottomup beam search algorithm (Liu et al., 2006) for decoding an HPSG tree Et. We keep at most 10 best derivations with distinct τ(d)s at each node. Recall the definition of minimum covering tree, which supports a faster way to retrieve available rules from PRS without generating all the subtrees. That is, when node n fortunately to be the root of some minimum covering tree(s), we use the tree(s) to seek available PAS-based rules in PRS. We keep a hash-table with the key to be th</context>
</contexts>
<marker>Liu, Mi, Feng, Liu, 2009</marker>
<rawString>Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009b. Joint decoding with multiple translation models. In Proceedings of ACL-IJCNLP, pages 576–584, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Efficient hpsg parsing with supertagging and cfg-filtering.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1671--1676</pages>
<contexts>
<context position="12942" citStr="Matsuzaki et al., 2007" startWordPosition="2106" endWordPosition="2109"> 2007). By introducing supertags into the target language side, i.e., the target language model and the target side of the phrase table, significant improvement was achieved for Arabic-to-English translation. Birch et al. (2007) also reported a significant improvement for Dutch-English translation by applying CCG supertags at a word level to a factorized SMT system (Koehn et al., 2007). In this paper, we also make use of supertags on the English language side. In an HPSG parse tree, these lexical syntactic descriptions are included in the LEXENTRY feature (refer to Table 2) of a lexical node (Matsuzaki et al., 2007). For example, the LEXENTRY feature of “t1:killed” takes the value of [NP.nom&lt;V.bse&gt;NP.acc]_lxm-past _verb_rule in Figure 2. In which, [NP.nom&lt;V.bse&gt;NP.acc] is an HPSG style supertag, which tells us that the base form of “killed” needs a nominative NP in the left hand side and an accessorial NP in the right hand side. The major differences are that, we use a larger feature set (Table 2) including the supertags for fine-grained tree-to-string rule extraction, rather than string-to-string translation (Hassan et al., 2007; Birch et al., 2007). The Logon project2 (Oepen et al., 2007) for Norwegian</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2007</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Efficient hpsg parsing with supertagging and cfg-filtering. In Proceedings of IJCAI, pages 1671– 1676, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>206--214</pages>
<contexts>
<context position="1543" citStr="Mi and Huang, 2008" startWordPosition="200" endWordPosition="203">d HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English translations testified the effectiveness of our approach. 1 Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009), tree-to-string translation (Liu et al., 2006; Huang et al., 2006), and forest-to-string translation (Mi et al., 2008; Mi and Huang, 2008). The algorithms proposed by Galley et al. (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. However, current tree-to-string rules only make use of Probabilistic Context-Free Grammar tree fragments, in which part-of-speech (POS) or koroshita korosareta (active) (passive) VBN(killed) 6 (6/10,6/6) 4 (4/10,4/4) VBN</context>
<context position="4395" citStr="Mi and Huang, 2008" startWordPosition="632" endWordPosition="635">was killed by Mary.” 325 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 325–334, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics deep syntactic information of an English sentence, which includes a fine-grained description of the syntactic property and a semantic representation of the sentence. We extract fine-grained translation rules from aligned HPSG tree/forest-string pairs. We localize an HPSG tree/forest to make it segmentable at any nodes to fit the extraction algorithms described in (Galley et al., 2006; Mi and Huang, 2008). We also propose a linear-time algorithm for extracting composed rules guided by predicate-argument structures. The effectiveness of the rules are testified in our tree-to-string and string-to-tree systems, taking bidirectional Japanese-English translations as our test cases. This paper is organized as follows. In Section 2, we briefly review the tree-to-string and string-totree translation frameworks, tree-to-string rule extraction algorithms, and rich syntactic information previously used for SMT. The HPSG grammar and our proposal of fine-grained rule extraction algorithms are described in </context>
<context position="10938" citStr="Mi and Huang (2008)" startWordPosition="1777" endWordPosition="1780">re connected together to form composed rules (Galley et al., 2006). For each aligned tree-string pair, Galley et al. (2006) constructed a derivation-forest, in which composed rules were generated, unaligned words of foreign language were consistently attached, and the translation probabilities of rules were estimated by using ExpectationMaximization (EM) (Dempster et al., 1977) training. For example, by combining the minimal rules of 1, 4, and 5, we obtain a composed rule, as shown in the bottom-right corner of Figure 2. Considering the parse error problem in the 1-best or k-best parse trees, Mi and Huang (2008) extracted tree-to-string translation rules from aligned packed forest-string pairs. A forest compactly encodes exponentially many trees 327 rather than the 1-best tree used by Galley et al. (2004; 2006). Two problems were managed to be tackled during extracting rules from an aligned forest-string pair: where to cut and how to cut. Equation 1 was used again to compute a frontier node set to determine where to cut the packed forest into a number of tree-fragments. The difference with tree-based rule extraction is that the nodes in a packed forest (which is a hypergraph) now are hypernodes, whic</context>
<context position="14505" citStr="Mi and Huang, 2008" startWordPosition="2339" endWordPosition="2342">a semantic transfer backbone is taken as the vantage point of this project. In contrast, the fine-grained tree-to-string translation rule extraction approaches in this paper are totally data-driven, and easily applicable to numerous language pairs by taking English as the source or target language. 3 Fine-grained rule extraction We now introduce the deep syntactic information generated by an HPSG parser and then describe our approaches for fine-grained tree-tostring rule extraction. Especially, we localize an HPSG tree/forest to fit the extraction algorithms described in (Galley et al., 2006; Mi and Huang, 2008). Also, we propose a linear-time composed rule extraction algorithm by making use of predicate-argument structures. 3.1 Deep syntactic information by HPSG parsing Head-driven phrase structure grammar (HPSG) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure called a sign. A sign gives a factored representation of the syntactic features of a word/phrase, as well as a representation of their semantic content. Phrases and words represented by signs are composed into larger phrases by applications of schemata. The semantic </context>
<context position="20488" citStr="Mi and Huang, 2008" startWordPosition="3351" endWordPosition="3354">RG2 Mary ARG1 ARG1 She ignore ARG2 fact I ARG1 dispute ARG1 want ARG2 329 Algorithm 1 Fine-grained rule extraction Input: HPSG tree/forest Ef, foreign sentence F, and alignment A Output: a PAS-based rule set R1 and/or a tree-rule set R2 1: if Ef is an HPSG tree then 2: E′f =localize Tree(Ef) 3: R1 = PASR extraction(E ′f, F, A) &gt; Algorithm 2 4: E′′f = ignore PAS(E′f) 5: R2 = TR extraction(E′′f , F, A) &gt; composed rule extraction algorithm in (Galley et al., 2006) 6: else if Ef is an HPSG forest then 7: E′ f = localize Forest(Ef); 8: R2 = forest based rule extraction(E′f, F, A) &gt; Algorithm 1 in (Mi and Huang, 2008) 9: end if the identifier of the daughter node as the values. For example, HEAD and SEM HEAD of node c0 take the identical value to be c3 in Figure 2. To extract tree-to-string rules from the tree structures of an HPSG forest, our solution is to pre-process an HPSG forest in the following way: • for a phrasal hypernode, replace its HEAD and SEM HEAD value with L, R, or S, which respectively represent left daughter, right daughter, or single daughter (line 2 and 7); and, • for a lexical node, ARG(x) and PRED features are ignored (line 4). A pure syntactic-based HPSG forest without any pointer-v</context>
<context position="31323" citStr="Mi and Huang, 2008" startWordPosition="5198" endWordPosition="5201">4x64GB memory) to run all the experiments. 4.4 Results Table 4 illustrates the statistics of several translation rule sets, which are classified by: • using TFSs or simple POS/phrasal tags (annotated by a superscript S) to represent tree nodes; • composed rules (PRS) extracted from the PAS of 1-best HPSG trees; • composed rules (C3), extracted from the tree structures of 1-best HPSG trees, and 3 is the maximum number of internal nodes in the tree fragments; and • forest-based rules (F), where the packed forests are pre-pruned by the marginal probability-based inside-outside algorithm used in (Mi and Huang, 2008). Table 5 reports the BLEU-4 scores achieved by decoding the test set making use of Joshua and our systems (t2s = tree-to-string and s2t = string-totree) under numerous rule sets. We analyze this table in terms of several aspects to prove the effectiveness of deep syntactic information for SMT. Let’s first look at the performance of TFSs. We take CS3 and Fs as approximations of CFG-based translation rules. Comparing the BLEU-4 scores of PTT+CS3 and PTT+C3, we gained 0.56 (t2s) and 0.57 (s2t) BLEU-4 points which are significant improvements (p &lt; 0.05). Furthermore, we gained 0.50 (t2s) and 0.62</context>
<context position="34233" citStr="Mi and Huang, 2008" startWordPosition="5682" endWordPosition="5685">nd the probabilities of rules in PRS were estimated by maximum likelihood, which is fast but biased compared with EM based estimation (Galley et al., 2006). Finally, by using PTT+F, our systems achieved the best BLEU-4 scores of 24.75% (t2s) and 22.67% (s2t), both are significantly better (p &lt; 0.01) than that achieved by Joshua. 5 Conclusion We have proposed approaches of using deep syntactic information for extracting fine-grained treeto-string translation rules from aligned HPSG forest-string pairs. The main contributions are the applications of GHKM-related algorithms (Galley et al., 2006; Mi and Huang, 2008) to HPSG forests and a linear-time algorithm for extracting composed rules from predicate-argument structures. We applied our fine-grained translation rules to a tree-to-string system and an Hiero-style string-totree system. Extensive experiments on large-scale bidirectional Japanese-English translations testified the significant improvements on BLEU score. We argue the fine-grained translation rules are generic and applicable to many syntax-based SMT frameworks such as the forest-to-string model (Mi et al., 2008). Furthermore, it will be interesting to extract fine-grained tree-to-tree transl</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08:HLT,</booktitle>
<pages>192--199</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1522" citStr="Mi et al., 2008" startWordPosition="196" endWordPosition="199">rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English translations testified the effectiveness of our approach. 1 Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009), tree-to-string translation (Liu et al., 2006; Huang et al., 2006), and forest-to-string translation (Mi et al., 2008; Mi and Huang, 2008). The algorithms proposed by Galley et al. (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. However, current tree-to-string rules only make use of Probabilistic Context-Free Grammar tree fragments, in which part-of-speech (POS) or koroshita korosareta (active) (passive) VBN(killed) 6 (6/10,</context>
<context position="34752" citStr="Mi et al., 2008" startWordPosition="5752" endWordPosition="5755">butions are the applications of GHKM-related algorithms (Galley et al., 2006; Mi and Huang, 2008) to HPSG forests and a linear-time algorithm for extracting composed rules from predicate-argument structures. We applied our fine-grained translation rules to a tree-to-string system and an Hiero-style string-totree system. Extensive experiments on large-scale bidirectional Japanese-English translations testified the significant improvements on BLEU score. We argue the fine-grained translation rules are generic and applicable to many syntax-based SMT frameworks such as the forest-to-string model (Mi et al., 2008). Furthermore, it will be interesting to extract fine-grained tree-to-tree translation rules by integrating deep syntactic information in the source and/or target language side(s). These treeto-tree rules are applicable for forest-to-tree translation models (Liu et al., 2009a). Acknowledgments This work was partially supported by Grant-inAid for Specially Promoted Research (MEXT, Japan) and Japanese/Chinese Machine Translation Project in Special Coordination Funds for Promoting Science and Technology (MEXT, Japan), and Microsoft Research Asia Machine Translation Theme. The first author thanks </context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL-08:HLT, pages 192–199, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic modeling of argument structures including non-local dependencies.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>285--291</pages>
<contexts>
<context position="18348" citStr="Miyao et al., 2003" startWordPosition="2969" endWordPosition="2972">ng the semantics, as explained shortly. In the Enju English HPSG grammar (Miyao et 3http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html ARG2 Figure 3: Predicate argument structures for the sentences of “John killed Mary” and “She ignored the fact that I wanted to dispute”. al., 2003) used in this paper, the semantic content of a sentence/phrase is represented by a predicateargument structure (PAS). Figure 3 shows the PAS of the example sentence in Figure 2, “John killed Mary”, and a more complex PAS for another sentence, “She ignored the fact that I wanted to dispute”, which is adopted from (Miyao et al., 2003). In an HPSG tree/forest, each leaf node generally introduces a predicate, which is represented by the pair of LEXENTRY (lexical entry) feature and PRED (predicate type) feature. The arguments of a predicate are designated by the pointers from the ARG⟨x⟩ features in a leaf node to non-terminal nodes. 3.2 Localize HPSG forest Our fine-grained translation rule extraction algorithm is sketched in Algorithm 1. Considering that a parse tree is a trivial packed forest, we only use the term forest to expand our discussion, hereafter. Recall that there are pointer-valued features in the TFSs (Table 2)</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2003</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii. 2003. Probabilistic modeling of argument structures including non-local dependencies. In Proceedings of the International Conference on Recent Advances in Natural Language Processing, pages 285– 291, Borovets.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="29261" citStr="Och and Ney, 2003" startWordPosition="4885" endWordPosition="4888"> 2006), to efficiently generate the unique k-best translations. 4.3 Setups The JST Japanese-English paper abstract corpus4, which consists of one million parallel sentences, was used for training and testing. This corpus was constructed from a Japanese-English paper abstract corpus by using the method of Utiyama and Isahara (2007). Table 3 shows the statistics of this corpus. Making use of Enju 2.3.1, we successfully parsed 987,401 English sentences in the training set, with a parse rate of 99.3%. We modified this parser to output a packed forest for each English sentence. We executed GIZA++ (Och and Ney, 2003) and grow-diag-final-and balancing strategy (Koehn et al., 2007) on the training set to obtain a phrasealigned parallel corpus, from which bidirectional phrase translation tables were estimated. SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train 5-gram English and Japanese LMs on the training set. We evaluated the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). The MERT toolkit we used is Z-mert5 (Zaidan, 2009). The baseline system for comparison is Joshua (Li et al., 2009), a freely available decoder for hierarchical phrase-based SMT (Chi</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="24083" citStr="Och, 2003" startWordPosition="3993" endWordPosition="3994">the translation probabilities of each rule. An example of PAS-based rule is shown in the bottom-right corner of Figure 2. In the rule, the subject and direct-object of “killed” are generalized into two variables, x0 and x1. 4 Experiments 4.1 Translation models We use a tree-to-string model and a string-to-tree model for bidirectional Japanese-English translations. Both models use a phrase translation table (PTT), an HPSG tree-based rule set (TRS), and a PAS-based rule set (PRS). Since the three rule sets are independently extracted and estimated, we 330 use Minimum Error Rate Training (MERT) (Och, 2003) to tune the weights of the features from the three rule sets on the development set. Given a 1-best (localized) HPSG tree Et, the tree-to-string decoder searches for the optimal derivation d∗ that transforms Et into a Japanese string among the set of all possible derivations D: d∗ =arg max {λ1 log pLM(τ(d)) + λ2|τ(d)| d∈D + log s(d|Et)}. (2) Here, the first item is the language model (LM) probability where τ(d) is the target string of derivation d; the second item is the translation length penalty; and the third item is the translation score, which is decomposed into a product of feature valu</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Erik Velldal</author>
<author>Jan Tore Lønning</author>
<author>Paul Meurer</author>
<author>Victoria Ros´en</author>
</authors>
<title>Towards hybrid quality-oriented machine translation - on linguistics and probabilities in mt.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-07),</booktitle>
<marker>Oepen, Velldal, Lønning, Meurer, Ros´en, 2007</marker>
<rawString>Stephan Oepen, Erik Velldal, Jan Tore Lønning, Paul Meurer, and Victoria Ros´en. 2007. Towards hybrid quality-oriented machine translation - on linguistics and probabilities in mt. In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-07), September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="29678" citStr="Papineni et al., 2002" startWordPosition="4947" endWordPosition="4950">ssfully parsed 987,401 English sentences in the training set, with a parse rate of 99.3%. We modified this parser to output a packed forest for each English sentence. We executed GIZA++ (Och and Ney, 2003) and grow-diag-final-and balancing strategy (Koehn et al., 2007) on the training set to obtain a phrasealigned parallel corpus, from which bidirectional phrase translation tables were estimated. SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train 5-gram English and Japanese LMs on the training set. We evaluated the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). The MERT toolkit we used is Z-mert5 (Zaidan, 2009). The baseline system for comparison is Joshua (Li et al., 2009), a freely available decoder for hierarchical phrase-based SMT (Chiang, 2005). We respectively extracted 4.5M and 5.3M translation rules from the training set for the 4K English and Japanese sentences in the development and test sets. We used the default configuration of Joshua, expect setting the maximum number of items/rules and the k of k-best outputs to be the identical 4http://www.jst.go.jp. The corpus can be conditionally obtained from NTCIR-7 patent translation workshop ho</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>Grammatical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>248--255</pages>
<contexts>
<context position="13690" citStr="Riezler and Maxwell, 2006" startWordPosition="2218" endWordPosition="2221">2. In which, [NP.nom&lt;V.bse&gt;NP.acc] is an HPSG style supertag, which tells us that the base form of “killed” needs a nominative NP in the left hand side and an accessorial NP in the right hand side. The major differences are that, we use a larger feature set (Table 2) including the supertags for fine-grained tree-to-string rule extraction, rather than string-to-string translation (Hassan et al., 2007; Birch et al., 2007). The Logon project2 (Oepen et al., 2007) for Norwegian-English translation integrates in-depth grammatical analysis of Norwegian (using lexical functional grammar, similar to (Riezler and Maxwell, 2006)) with semantic representations in the minimal recursion semantics framework, and fully grammar-based generation for English using HPSG. A hybrid (of rule-based and data-driven) architecture with a semantic transfer backbone is taken as the vantage point of this project. In contrast, the fine-grained tree-to-string translation rule extraction approaches in this paper are totally data-driven, and easily applicable to numerous language pairs by taking English as the source or target language. 3 Fine-grained rule extraction We now introduce the deep syntactic information generated by an HPSG pars</context>
</contexts>
<marker>Riezler, Maxwell, 2006</marker>
<rawString>Stefan Riezler and John T. Maxwell, III. 2006. Grammatical machine translation. In Proceedings of HLTNAACL, pages 248–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="29501" citStr="Stolcke, 2002" startWordPosition="4921" endWordPosition="4922">Japanese-English paper abstract corpus by using the method of Utiyama and Isahara (2007). Table 3 shows the statistics of this corpus. Making use of Enju 2.3.1, we successfully parsed 987,401 English sentences in the training set, with a parse rate of 99.3%. We modified this parser to output a packed forest for each English sentence. We executed GIZA++ (Och and Ney, 2003) and grow-diag-final-and balancing strategy (Koehn et al., 2007) on the training set to obtain a phrasealigned parallel corpus, from which bidirectional phrase translation tables were estimated. SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train 5-gram English and Japanese LMs on the training set. We evaluated the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). The MERT toolkit we used is Z-mert5 (Zaidan, 2009). The baseline system for comparison is Joshua (Li et al., 2009), a freely available decoder for hierarchical phrase-based SMT (Chiang, 2005). We respectively extracted 4.5M and 5.3M translation rules from the training set for the 4K English and Japanese sentences in the development and test sets. We used the default configuration of Joshua, expect setting the maximum </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A japanese-english patent parallel corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of MT Summit XI,</booktitle>
<pages>475--482</pages>
<location>Copenhagen.</location>
<contexts>
<context position="28975" citStr="Utiyama and Isahara (2007)" startWordPosition="4834" endWordPosition="4837">ribed in (Huang and Chiang, 2005) to extract its k-best (k = 500 in our experiments) derivations. Since different derivations may lead to the same target language string, we further adopt Algorithm 3’s modification, i.e., keep a hash-table to maintain the unique target sentences (Huang et al., 2006), to efficiently generate the unique k-best translations. 4.3 Setups The JST Japanese-English paper abstract corpus4, which consists of one million parallel sentences, was used for training and testing. This corpus was constructed from a Japanese-English paper abstract corpus by using the method of Utiyama and Isahara (2007). Table 3 shows the statistics of this corpus. Making use of Enju 2.3.1, we successfully parsed 987,401 English sentences in the training set, with a parse rate of 99.3%. We modified this parser to output a packed forest for each English sentence. We executed GIZA++ (Och and Ney, 2003) and grow-diag-final-and balancing strategy (Koehn et al., 2007) on the training set to obtain a phrasealigned parallel corpus, from which bidirectional phrase translation tables were estimated. SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train 5-gram English and Japanese LMs on the training set</context>
</contexts>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2007. A japanese-english patent parallel corpus. In Proceedings of MT Summit XI, pages 475–482, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
</authors>
<title>Statistical Machine Translation Using Large-Scale Lexicon and Deep Syntactic Structures.</title>
<date>2010</date>
<tech>Ph.D. dissertation.</tech>
<institution>Department of Computer Science, The University of Tokyo.</institution>
<contexts>
<context position="22342" citStr="Wu, 2010" startWordPosition="3692" endWordPosition="3693">{t1, c1, c5} is shown in the bottom-right corner of Figure 2. The definition supplies us a linear-time algorithm to directly find the tree fragment that covers a PAS during both rule extracting and rule matching when decoding an HPSG tree. Algorithm 2 PASR extraction Input: HPSG tree Et, foreign sentence F, and alignment A Output: a PAS-based rule set R 1: R = {} 2: for node n G Leaves(Et) do 3: if Open(n.ARG) then 4: Tc = MinimumCoveringTree(Et, n, n.ARGs) 5: if root and leaf nodes of Tc are in fs then 6: generate a rule r using fragment Tc 7: R.append(r) 8: end if 9: end if 10: end for See (Wu, 2010) for more examples of minimum covering trees. Taking a minimum covering tree as the tree fragment, we can easily build a tree-to-string translation rule that reflects the semantic dependency of a PAS. The algorithm of PAS-based rule (PASR) extraction is sketched in Algorithm 2. Suppose we are given a tuple of (F, Et, A). Et is pre-processed by replacing HEAD and SEM HEAD to be L, R, or S, and computing the span and comp span of each node. We extract PAS-based rules through one-time traversal of the leaf nodes in Et (line 2). For each leaf node n, we extract a minimum covering tree T, if n cont</context>
</contexts>
<marker>Wu, 2010</marker>
<rawString>Xianchao Wu. 2010. Statistical Machine Translation Using Large-Scale Lexicon and Deep Syntactic Structures. Ph.D. dissertation. Department of Computer Science, The University of Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
</authors>
<title>Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--79</pages>
<contexts>
<context position="29730" citStr="Zaidan, 2009" startWordPosition="4958" endWordPosition="4959"> with a parse rate of 99.3%. We modified this parser to output a packed forest for each English sentence. We executed GIZA++ (Och and Ney, 2003) and grow-diag-final-and balancing strategy (Koehn et al., 2007) on the training set to obtain a phrasealigned parallel corpus, from which bidirectional phrase translation tables were estimated. SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train 5-gram English and Japanese LMs on the training set. We evaluated the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). The MERT toolkit we used is Z-mert5 (Zaidan, 2009). The baseline system for comparison is Joshua (Li et al., 2009), a freely available decoder for hierarchical phrase-based SMT (Chiang, 2005). We respectively extracted 4.5M and 5.3M translation rules from the training set for the 4K English and Japanese sentences in the development and test sets. We used the default configuration of Joshua, expect setting the maximum number of items/rules and the k of k-best outputs to be the identical 4http://www.jst.go.jp. The corpus can be conditionally obtained from NTCIR-7 patent translation workshop homepage: http://research.nii.ac.jp/ntcir/permission/n</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar F. Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>256--263</pages>
<contexts>
<context position="25498" citStr="Zhang et al., 2006" startWordPosition="4232" endWordPosition="4235">these rule sets together in one decoder because PTT provides excellent rule coverages while TRS and PRS offer linguistically motivated phrase selections and nonlocal reorderings. Each f(r) is in turn a product of five features: f(r) = p(s|t)λ3 · p(t|s)λ4 · l(s|t)λ5 · l(t|s)λ6 · eλ7. Here, s/t represent the source/target part of a rule in PTT, TRS, or PRS; p(·|·) and l(·|·) are translation probabilities and lexical weights of rules from PTT, TRS, and PRS. The derivation length penalty is controlled by λ7. In our string-to-tree model, for efficient decoding with integrated n-gram LM, we follow (Zhang et al., 2006) and inversely binarize all translation rules into Chomsky Normal Forms that contain at most two variables and can be incrementally scored by LM. In order to make use of the binarized rules in the CKY decoding, we add two kinds of glues rules: S → Xm(1), Xm(1), S → S(1)Xm(2), S(1)Xm(2). Here Xm ranges over the nonterminals appearing in a binarized rule set. These glue rules can be seen as an extension from X to {Xm}of the two glue rules described in (Chiang, 2007). The string-to-tree decoder searches for the optimal derivation d∗ that parses a Japanese string F into a packed forest of the set </context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of HLT-NAACL, pages 256–263, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>