<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000083">
<title confidence="0.993787">
Exploring the Potential of Intractable Parsers
</title>
<author confidence="0.994626">
Mark Hopkins
</author>
<affiliation confidence="0.9888325">
Dept. of Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.688035">
Saarbr¨ucken, Germany
</address>
<email confidence="0.996312">
mhopkins@coli.uni-sb.de
</email>
<author confidence="0.983107">
Jonas Kuhn
</author>
<affiliation confidence="0.9802425">
Dept. of Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.685094">
Saarbr¨ucken, Germany
</address>
<email confidence="0.995872">
jonask@coli.uni-sb.de
</email>
<sectionHeader confidence="0.99473" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999914727272727">
We revisit the idea of history-based pars-
ing, and present a history-based parsing
framework that strives to be simple, gen-
eral, and flexible. We also provide a de-
coder for this probability model that is
linear-space, optimal, and anytime. A
parser based on this framework, when
evaluated on Section 23 of the Penn Tree-
bank, compares favorably with other state-
of-the-art approaches, in terms of both ac-
curacy and speed.
</bodyText>
<sectionHeader confidence="0.998427" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.979465363636363">
Much of the current research into probabilis-
tic parsing is founded on probabilistic context-
free grammars (PCFGs) (Collins, 1996; Charniak,
1997; Collins, 1999; Charniak, 2000; Charniak,
2001; Klein and Manning, 2003). For instance,
consider the parse tree in Figure 1. One way to de-
compose this parse tree is to view it as a sequence
of applications of CFG rules. For this particular
tree, we could view it as the application of rule
“NP → NP PP,” followed by rule “NP → DT NN,”
followed by rule “DT → that,” and so forth. Hence
instead of analyzing P(tree), we deal with the
more modular:
P(NP → NP PP, NP → DT NN,
DT → that, NN → money, PP → IN NP,
IN → in, NP → DT NN, DT → the,
NN → market)
Obviously this joint distribution is just as diffi-
cult to assess and compute with as P(tree). How-
ever there exist cubic-time dynamic programming
algorithms to find the most likely parse if we as-
sume that all CFG rule applications are marginally
</bodyText>
<figure confidence="0.780339666666667">
NP
NP PP
the market
</figure>
<figureCaption confidence="0.999849">
Figure 1: Example parse tree.
</figureCaption>
<bodyText confidence="0.99993812">
independent of one another. The problem, of
course, with this simplification is that although
it is computationally attractive, it is usually too
strong of an independence assumption. To miti-
gate this loss of context, without sacrificing algo-
rithmic tractability, typically researchers annotate
the nodes of the parse tree with contextual infor-
mation. A simple example is the annotation of
nodes with their parent labels (Johnson, 1998).
The choice of which annotations to use is
one of the main features that distinguish parsers
based on this approach. Generally, this approach
has proven quite effective in producing English
phrase-structure grammar parsers that perform
well on the Penn Treebank.
One drawback of this approach is its inflexibil-
ity. Because we are adding probabilistic context
by changing the data itself, we make our data in-
creasingly sparse as we add features. Thus we are
constrained from adding too many features, be-
cause at some point we will not have enough data
to sustain them. We must strike a delicate bal-
ance between how much context we want to in-
clude versus how much we dare to partition our
data set.
</bodyText>
<figure confidence="0.996178666666667">
DT
NN
NP
IN
that
money
in
DT
NN
</figure>
<page confidence="0.986194">
369
</page>
<note confidence="0.744183">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 369–376,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.993836109090909">
The major alternative to PCFG-based ap-
proaches are so-called history-based parsers
(Black et al., 1993). These parsers differ from
PCFG parsers in that they incorporate context by
using a more complex probability model, rather
than by modifying the data itself. The tradeoff to
using a more powerful probabilistic model is that
one can no longer employ dynamic programming
to find the most probable parse. Thus one trades
assurances of polynomial running time for greater
modeling flexibility.
There are two canonical parsers that fall into
this category: the decision-tree parser of (Mager-
man, 1995), and the maximum-entropy parser of
(Ratnaparkhi, 1997). Both showed decent results
on parsing the Penn Treebank, but in the decade
since these papers were published, history-based
parsers have been largely ignored by the research
community in favor of PCFG-based approaches.
There are several reasons why this may be. First
is naturally the matter of time efficiency. Mager-
man reports decent parsing times, but for the pur-
poses of efficiency, must restrict his results to sen-
tences of length 40 or less. Furthermore, his two-
phase stack decoder is a bit complicated and is ac-
knowledged to require too much memory to han-
dle certain sentences. Ratnaparkhi is vague about
the running time performance of his parser, stat-
ing that it is “observed linear-time,” but in any
event, provides only a heuristic, not a complete al-
gorithm.
Next is the matter of flexibility. The main ad-
vantage of abandoning PCFGs is the opportunity
to have a more flexible and adaptable probabilis-
tic parsing model. Unfortunately, both Magerman
and Ratnaparkhi’s models are rather specific and
complicated. Ratnaparkhi’s, for instance, consists
of the interleaved sequence of four different types
of tree construction operations. Furthermore, both
are inextricably tied to the learning procedure that
they employ (decision trees for Magerman, maxi-
mum entropy for Ratnaparkhi).
In this work, our goal is to revisit history-based
parsers, and provide a general-purpose framework
that is (a) simple, (b) fast, (c) space-efficient and
(d) easily adaptable to new domains. As a method
of evaluation, we use this framework with a very
simple set of features to see how well it performs
(both in terms of accuracy and running time) on
the Penn Treebank. The overarching goal is to de-
velop a history-based hierarchical labeling frame-
work that is viable not only for parsing, but for
other application areas that current rely on dy-
namic programming, like phrase-based machine
translation.
</bodyText>
<sectionHeader confidence="0.972224" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.998994545454546">
For the following discussion, it will be useful to
establish some terminology and notational con-
ventions. Typically we will represent variables
with capital letters (e.g. X, Y ) and sets of vari-
ables with bold-faced capital letters (e.g. X,
Y). The domain of a variable X will be denoted
dom(X), and typically we will use the lower-case
correspondent (in this case, x) to denote a value in
the domain of X. A partial assignment (or simply
assignment) of a set X of variables is a function
w that maps a subset W of the variables of X
to values in their respective domains. We define
dom(w) = W. When W = X, then we say that
w is a full assignment of X. The trivial assign-
ment of X makes no variable assignments.
Let w(X) denote the value that partial assign-
ment w assigns to variable X. For value x E
dom(X), let w[X = x] denote the assignment
identical to w except that w[X = x](X) = x.
For a set Y of variables, let w Y denote the re-
striction of partial assignment w to the variables
in dom(w) n Y.
</bodyText>
<sectionHeader confidence="0.99193" genericHeader="method">
3 The Generative Model
</sectionHeader>
<bodyText confidence="0.999818142857143">
The goal of this section is to develop a probabilis-
tic process that generates labeled trees in a manner
considerably different from PCFGs. We will use
the tree in Figure 2 to motivate our model. In this
example, nodes of the tree are labeled with either
an A or a B. We can represent this tree using two
charts. One chart labels each span with a boolean
value, such that a span is labeled true iff it is a
constituent in the tree. The other chart labels each
span with a label from our labeling scheme (A or
B) or with the value null (to represent that the
span is unlabeled). We show these charts in Fig-
ure 3. Notice that we may want to have more than
one labeling scheme. For instance, in the parse
tree of Figure 1, there are three different types of
labels: word labels, preterminal labels, and nonter-
minal labels. Thus we would use four 5x5 charts
instead of two 3x3 charts to represent that tree.
We will pause here and generalize these con-
cepts. Define a labeling scheme as a set of symbols
including a special symbol null (this will desig-
</bodyText>
<page confidence="0.998046">
370
</page>
<figureCaption confidence="0.959835">
Figure 2: Example labeled tree.
</figureCaption>
<figure confidence="0.435052">
1 2 3 1 2 3
1 true true true 1 A B A
2 - true false 2 - B null
3 - - true 3 - - B
</figure>
<figureCaption confidence="0.948652">
Figure 3: Chart representation of the example tree:
</figureCaption>
<bodyText confidence="0.997957382022473">
the left chart tells us which spans are tree con-
stituents, and the right chart tells us the labels of
the spans (null means unlabeled).
nate that a given span is unlabeled). For instance,
we can define L1 = {null, A, B} to be a labeling
scheme for the example tree.
Let L = {L1, L2, ...Lm} be a set of labeling
schemes. Define a model variable of L as a sym-
bol of the form Sij or Lkij, for positive integers i,
j, k, such that i &lt; j and k &lt; m. Model vari-
ables of the form Sij indicate whether span (i, j)
is a tree constituent, hence the domain of Sij is
{true, false}. Such variables correspond to en-
tries in the left chart of Figure 3. Model variables
of the form Lk ij indicate which label from scheme
Lk is assigned to span (i, j), hence the domain of
model variable Lk ij is Lk. Such variables corre-
spond to entries in the right chart of Figure 3. Here
we have only one labeling scheme.
Let VL be the (countably infinite) set of model
variables of L. Usually we are interested in trees
over a given sentence of finite length n. Let VnL
denote the finite subset of VL that includes pre-
cisely the model variables of the form Sij or Lkij,
where j &lt; n.
Basically then, our model consists of two types
of decisions: (1) whether a span should be labeled,
and (2) if so, what label(s) the span should have.
Let us proceed with our example. To generate the
tree of Figure 2, the first decision we need to make
is how many leaves it will have (or equivalently,
how large our tables will be). We assume that we
have a probability distribution PN over the set of
positive integers. For our example tree, we draw
the value 3, with probability PN (3).
Now that we know our tree will have three
leaves, we can now decide which spans will be
constituents and what labels they will have. In
other words, we assign values to the variables in
V3L. First we need to choose the order in which
we will make these assignments. For our exam-
ple, we will assign model variables in the follow-
ing order: S11, L111, S22, L122, S33, L133, S12, L112,
S23, L123, S13, L113. A detailed look at this assign-
ment process should help clarify the details of the
model.
Assigning S11: The first model variable in our
order is S11. In other words, we need to decide
whether the span (1, 1) should be a constituent.
We could let this decision be probabilistically de-
termined, but recall that we are trying to gener-
ate a well-formed tree, thus the leaves and the root
should always be considered constituents. To han-
dle situations when we would like to make deter-
ministic variable assignments, we supply an aux-
illiary function A that tells us (given a model vari-
able X and the history of decisions made so far)
whether X should be automatically determined,
and if so, what value it should be assigned. In our
running example, we ask A whether S11 should be
automatically determined, given the previous as-
signments made (so far only the value chosen for
n, which was 3). The so-called auto-assignment
function A responds (since S11 is a leaf span) that
S11 should be automatically assigned the value
true, making span (1, 1) a constituent.
Assigning L111: Next we want to assign a la-
bel to the first leaf of our tree. There is no com-
pelling reason to deterministically assign this la-
bel. Therefore, the auto-assignment function A
declines to assign a value to L111, and we pro-
ceed to assign its value probabilistically. For this
task, we would like a probability distribution over
the labels of labeling scheme L1 = {null, A, B},
conditioned on the decision history so far. The dif-
ficulty is that it is clearly impractical to learn con-
ditional distributions over every conceivable his-
tory of variable assignments. So first we distill
the important features from an assignment history.
For instance, one such feature (though possibly
not a good one) could be whether an odd or an
even number of nodes have so far been labeled
with an A. Our conditional probability distribu-
tion is conditioned on the values of these features,
instead of the entire assignment history. Consider
specifically model variable L111. We compute its
features (an even number of nodes – zero – have
so far been labeled with an A), and then we use
these feature values to access the relevant prob-
</bodyText>
<figure confidence="0.993901">
A
B
B
A B
</figure>
<page confidence="0.991192">
371
</page>
<bodyText confidence="0.999624446808511">
ability distribution over {null, A, B}. Drawing
from this conditional distribution, we probabilis-
tically assign the value A to variable L111.
Assigning S22, L122, S33, L133: We proceed in
this way to assign values to S22, L122, S33, L133 (the
S-variables deterministically, and the L1-variables
probabilistically).
Assigning S12: Next comes model variable
S12. Here, there is no reason to deterministically
dictate whether span (1, 2) is a constituent or not.
Both should be considered options. Hence we
treat this situation the same as for the L1 variables.
First we extract the relevant features from the as-
signment history. We then use these features to
access the correct probability distribution over the
domain of S12 (namely {true, false}). Drawing
from this conditional distribution, we probabilis-
tically assign the value true to S12, making span
(1, 2) a constituent in our tree.
Assigning L112: We proceed to probabilisti-
cally assign the value B to L112, in the same man-
ner as we did with the other L1 model variables.
Assigning S23: Now we must determine
whether span (2,3) is a constituent. We could
again probabilistically assign a value to S23 as we
did for S12, but this could result in a hierarchi-
cal structure in which both spans (1, 2) and (2, 3)
are constituents, which is not a tree. For trees,
we cannot allow two model variables Sij and Skl
to both be assigned true if they properly over-
lap, i.e. their spans overlap and one is not a sub-
span of the other. Fortunately we have already es-
tablished auto-assignment function A, and so we
simply need to ensure that it automatically assigns
the value false to model variable Skl if a prop-
erly overlapping model variable Sij has previously
been assigned the value true.
Assigning L123, S13, L113: In this manner, we
can complete our variable assignments: L123 is au-
tomatically determined (since span (2,3) is not a
constituent, it should not get a label), as is S13 (to
ensure a rooted tree), while the label of the root is
probabilistically assigned.
We can summarize this generative process as a
general modeling tool. Define a hierarchical la-
beling process (HLP) as a 5-tuple hL, &lt;, A, F, Pi
where:
</bodyText>
<listItem confidence="0.7638416">
• L = {L1, L2, ..., Lm} is a finite set of label-
ing schemes.
• &lt; is a model order, defined as a total ordering
of the model variables VL such that for all
HLPGEN(HLP H = hL, &lt;, A, F, Pi):
1. Choose a positive integer n from distribution
PN. Let x be the trivial assignment of VL.
2. In the order defined by &lt;, compute step 3 for
each model variable Y of Vn L.
3. If A(Y, x, n) = htrue, yi for some y in the
domain of model variable Y , then let x =
x[Y = y]. Otherwise assign a value to Y
from its domain:
(a) If Y = Sij, then let x = x[Sij = sij],
where sij is a value drawn from distri-
bution PS(s|FS(x, i, j, n)).
(b) If Y = Lkij, then let x = x[Lk ij= lkij],
where lk ij is a value drawn from distribu-
tion Pk(lk|Fk(x, i, j, n)).
4. Return hn, xi.
</listItem>
<figureCaption confidence="0.99449">
Figure 4: Pseudocode for the generative process.
</figureCaption>
<bodyText confidence="0.964377333333333">
i, j, k: Sij &lt; Lk ij(i.e. we decide whether
a span is a constituent before attempting to
label it).
</bodyText>
<listItem confidence="0.987699791666667">
• A is an auto-assignment function. Specifi-
cally A takes three arguments: a model vari-
able Y of VL, a partial assignment x of VL,
and integer n. The function A maps this 3-
tuple to false if the variable Y should not be
automatically assigned a value based on the
current history, or the pair htrue, yi, where y
is the value in the domain of Y that should be
automatically assigned to Y .
• F = {FS,F1,F2, ...,Fm} is a set of fea-
ture functions. Specifically, Fk (resp., FS)
takes four arguments: a partial assignment
x of VL, and integers i , j , n such that
1 ≤ i ≤ j ≤ n. It maps this 4-tuple to a
full assignment fk (resp., fS) of some finite
set Fk (resp., FS) of feature variables.
• P = {PN, PS, P1, P2, ..., Pm} is a set of
probability distributions. PN is a marginal
probability distribution over the set of pos-
itive integers, whereas {PS, P1, P2, ..., Pm}
are conditional probability distributions.
Specifically, Pk (respectively, PS) is a func-
tion that takes as its argument a full assign-
ment fk (resp., fS) of feature set Fk (resp.,
</listItem>
<page confidence="0.94524">
372
</page>
<bodyText confidence="0.323317">
A(variable Y , assignment x, int n):
</bodyText>
<listItem confidence="0.996681">
1. If Y = Sij, and there exists a properly
overlapping model variable Skl such that
x(Skl) = true, then return htrue, falsei.
2. If Y = Sii or Y = S1n, then return
htrue, truei.
3. If Y = Lkij, and x(Sij) = false, then return
htrue, nulli.
4. Else return false.
</listItem>
<figureCaption confidence="0.997466">
Figure 5: An example auto-assignment function.
</figureCaption>
<bodyText confidence="0.926031769230769">
FS). It maps this to a probability distribution
over dom(Lk) (resp., {true, false}).
An HLP probabilistically generates an assign-
ment of its model variables using the generative
process shown in Figure 4. Taking an HLP H =
hL, &lt;, A, F, Pi as input, HLPGEN outputs an in-
teger n, and an H-labeling x of length n, defined
as a full assignment of VnL.
Given the auto-assignment function in Figure 5,
every H-labeling generated by HLPGEN can be
viewed as a labeled tree using the interpretation:
span (i, j) is a constituent iff Sij = true; span
(i, j) has label lk ∈ dom(Lk) iff Lk ij = lk.
</bodyText>
<sectionHeader confidence="0.994016" genericHeader="method">
4 Learning
</sectionHeader>
<bodyText confidence="0.99927372">
The generative story from the previous section al-
lows us to express the probability of a labeled tree
as P(n, x), where x is an H-labeling of length n.
For model variable X, define V&lt;L (X) as the sub-
set of VL appearing before X in model order &lt;.
With the help of this terminology, we can decom-
pose P(n, x) into the following product:
where fSij = FS(x|V&lt;� (Sij), i, j, n) and
fk ij= Fk(x|V&lt;�(Lkij),i,j,n) and Y is the sub-
set of VnL that was not automatically assigned by
HLPGEN.
Usually in parsing, we are interested in comput-
ing the most likely tree given a specific sentence.
In our framework, this generalizes to computing:
argmaxXP(x|n, w), where w is a subassignment
of an H-labeling x of length n. In natural lan-
guage parsing, w could specify the constituency
and word labels of the leaf-level spans. This would
be equivalent to asking: given a sentence, what is
its most likely parse?
Let W = dom(w) and suppose that we choose
a model order &lt; such that for every pair of model
variables W ∈ W, X ∈ VL\W, either W &lt; X
or W is always auto-assigned. Then P(x|n, w)
can be expressed as:
</bodyText>
<equation confidence="0.996723">
ri PS(x(Sij)|fSij)
SijEY\W
ri· Pk(x(Lkij)|fkij)
LkijEY\W
</equation>
<bodyText confidence="0.999861888888889">
Hence the distributions we need to learn
are probability distributions PS(sij|fS) and
Pk(lkij|fk). This is fairly straightforward. Given
a data bank consisting of labeled trees (such as
the Penn Treebank), we simply convert each tree
into its H-labeling and use the probabilistically
determined variable assignments to compile our
training instances. In this way, we compile k + 1
sets of training instances that we can use to induce
PS, and the Pk distributions. The choice of which
learning technique to use is up to the personal
preference of the user. The only requirement
is that it must return a conditional probability
distribution, and not a hard classification. Tech-
niques that allow this include relative frequency,
maximum entropy models, and decision trees.
For our experiments, we used maximum entropy
learning. Specifics are deferred to Section 6.
</bodyText>
<sectionHeader confidence="0.99411" genericHeader="method">
5 Decoding
</sectionHeader>
<bodyText confidence="0.994543333333333">
For the PCFG parsing model, we can find
argmaxtreeP(tree|sentence) using a cubic-time
dynamic programming-based algorithm. By
adopting a more flexible probabilistic model, we
sacrifice polynomial-time guarantees. The central
question driving this paper is whether we can jetti-
son these guarantees and still obtain good perfor-
mance in practice. For the decoding of the prob-
abilistic model of the previous section, we choose
a depth-first branch-and-bound approach, specif-
ically because of two advantages. First, this ap-
proach takes linear space. Second, it is anytime,
</bodyText>
<equation confidence="0.710864">
riP0(n) · PS(x(Sij)|fSij) Pk(x(L)|f)
SijEY 11 · ij ij
LkijEY
</equation>
<page confidence="0.989776">
373
</page>
<bodyText confidence="0.678709">
HLPDECODE(HLP H, int n, assignment w):
</bodyText>
<listItem confidence="0.914635761904762">
1. Initialize stack S with the pair (x∅,1), where
x∅ is the trivial assignment of VL. Let
xbest = x∅; let pbest = 0. Until stack S is
empty, repeat steps 2 to 4.
2. Pop topmost pair (x, p) from stack S.
3. If p &gt; pbest and x is an H-labeling of length
n, then: let xbest = x; let pbest = p.
4. If p &gt; pbest and x is not yet a H-labeling of
length n, then:
(a) Let Y be the earliest variable in VnL (ac-
cording to model order &lt;) unassigned
by x.
(b) If Y E dom(w), then push pair (x[Y =
w(Y )], p) onto stack S.
(c) Else if A(Y, x, n) = (true, y) for some
value y E dom(Y), then push pair
(x[Y = y], p) onto stack S.
(d) Otherwise for every value y E dom(Y),
push pair (x[Y = y], p·q(y)) onto stack
S in ascending order of the value of
q(y), where:
</listItem>
<equation confidence="0.8719285">
PS(y |FS (x, i, j, n)) if Y = Sij
q(y) = � Pk(y|Fk(x, i,j, n)) if Y = Lk ij
</equation>
<sectionHeader confidence="0.580048" genericHeader="method">
5. Return xbest.
</sectionHeader>
<figureCaption confidence="0.98856">
Figure 6: Pseudocode for the decoder.
</figureCaption>
<bodyText confidence="0.998845939393939">
i.e. it finds a (typically good) solution early and
improves this solution as the search progresses.
Thus if one does not wish the spend the time to
run the search to completion (and ensure optimal-
ity), one can use this algorithm easily as a heuristic
by halting prematurely and taking the best solution
found thus far.
The search space is simple to define. Given an
HLP H, the search algorithm simply makes as-
signments to the model variables (depth-first) in
the order defined by &lt;.
This search space can clearly grow to be quite
large, however in practice the search speed is
improved drastically by using branch-and-bound
backtracking. Namely, at any choice point in the
search space, we first choose the least cost child
to expand (i.e. we make the most probable assign-
ment). In this way, we quickly obtain a greedy
solution (in linear time). After that point, we can
continue to keep track of the best solution we have
found so far, and if at any point we reach an inter-
nal node of our search tree with partial cost greater
than the total cost of our best solution, we can dis-
card this node and discontinue exploration of that
subtree. This technique can result in a significant
aggregrate savings of computation time, depend-
ing on the nature of the cost function.
Figure 6 shows the pseudocode for the depth-
first branch-and-bound decoder. For an HLP H =
(L, &lt;, A, F, P), a positive integer n, and a partial
assignment w of VnL, the call HLPDECODE(H, n,
w) returns the H-labeling x of length n such that
P(x|n, w) is maximized.
</bodyText>
<sectionHeader confidence="0.9989" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999984794117647">
We employed a familiar experimental set-up. For
training, we used sections 2–21 of the WSJ section
of the Penn treebank. As a development set, we
used the first 20 files of section 22, and then saved
section 23 for testing the final model. One uncon-
ventional preprocessing step was taken. Namely,
for the entire treebank, we compressed all unary
chains into a single node, labeled with the label of
the node furthest from the root. We did so in or-
der to simplify our experiments, since the frame-
work outlined in this paper allows only one label
per labeling scheme per span. Thus by avoiding
unary chains, we avoid the need for many label-
ing schemes or more complicated compound la-
bels (labels like “NP-NN”). Since our goal here
was not to create a parsing tool but rather to ex-
plore the viability of this approach, this seemed a
fair concession. It should be noted that it is indeed
possible to create a fully general parser using our
framework (for instance, by using the above idea
of compound labels for unary chains).
The main difficulty with this compromise is that
it renders the familiar metrics of labeled preci-
sion and labeled recall incomparable with previ-
ous work (i.e. the LP of a set of candidate parses
with respect to the unmodified test set differs from
the LP with respect to the preprocessed test set).
This would be a major problem, were it not for
the existence of other metrics which measure only
the quality of a parser’s recursive decomposition
of a sentence. Fortunately, such metrics do exist,
thus we used cross-bracketing statistics as the ba-
sic measure of quality for our parser. The cross-
bracketing score of a set of candidate parses with
</bodyText>
<page confidence="0.964395">
374
</page>
<equation confidence="0.9964862">
word(i+k) = w word(j+k) = w
preterminal(i+k) = p preterminal(j+k) = p
label(i+k) = l label(j+k) = l
category(i+k) = c category(j+k) = c
signature(i,i+k) = s
</equation>
<bodyText confidence="0.97602792">
Figure 7: Basic feature templates used to deter-
mine constituency and labeling of span (i, j). k is
an arbitrary integer.
respect to the unmodified test set is identical to the
cross-bracketing score with respect to the prepro-
cessed test set, hence our preprocessing causes no
comparability problems as viewed by this metric.
For our parsing model, we used an HLP x =
(L, &lt;, A, F, P) with the following parameters. L
consisted of three labeling schemes: the set Lwd
of word labels, the set Lpt of preterminal labels,
and the set Lnt of nonterminal labels. The or-
der &lt; of the model variables was the unique or-
der such that for all suitable integers i, j, k, l: (1)
Sij &lt; Lwd
ij &lt; Lpt
ij &lt; Lnt
ij , (2) Lnt
ij &lt; Skl iff
span (i, j) is strictly shorter than span (k, l) or they
have the same length and integer i is less than inte-
ger k. For auto-assignment function A, we essen-
tially used the function in Figure 5, modified so
that it automatically assigned null to model vari-
ables Lwd
ij and Lpt ijfor i =� j (i.e. no preterminal or
word tagging of internal nodes), and to model vari-
ables Lnt
ii (i.e. no nonterminal tagging of leaves,
rendered unnecessary by our preprocessing step).
Rather than incorporate part-of-speech tagging
into the search process, we opted to pretag the sen-
tences of our development and test sets with an
off-the-shelf tagger, namely the Brill tagger (Brill,
1994). Thus the object of our computation was
HLPDECODE(x, n, w), where n was the length
of the sentence, and partial assignment w speci-
fied the word and PT labels of the leaves. Given
this partial assignment, the job of HLPDECODE
was to find the most probable assignment of model
variables Sij and Lnt
ij for 1 &lt; i &lt; j &lt; n.
The two probability models, PS and Pnt, were
trained in the manner described in Section 4.
Two decisions needed to be made: which fea-
tures to use and which learning technique to em-
ploy. As for the learning technique, we used
maximum entropy models, specifically the imple-
mentation called MegaM provided by Hal Daume
(Daum´e III, 2004). For PS, we needed features
</bodyText>
<table confidence="0.997673875">
&lt; 40 0CB &lt; 100 0CB
CB CB
Magerman (1995) 1.26 56.6
Collins (1996) 1.14 59.9
Klein/Manning (2003) 1.10 60.3 1.31 57.2
this paper 1.09 58.2 1.25 55.2
Charniak (1997) 1.00 62.1
Collins (1999) 0.90 67.1
</table>
<figureCaption confidence="0.9845725">
Figure 8: Cross-bracketing results for Section 23
of the Penn Treebank.
</figureCaption>
<bodyText confidence="0.999684380952382">
that would be relevant to deciding whether a given
span (i, j) should be considered a constituent. The
basic building blocks we used are depicted in Fig-
ure 7. A few words of explanation are in or-
der. By label(k), we mean the highest nonter-
minal label so far assigned that covers word k, or
if such a label does not yet exist, then the preter-
minal label of k (recall that our model order was
bottom-up). By category(k), we mean the cat-
egory of the preterminal label of word k (given
a coarser, hand-made categorization of pretermi-
nal labels that grouped all noun tags into one
category, all verb tags into another, etc.). By
signature(k, m), where k &lt; m, we mean the
sequence (label(k), label(k + 1), ..., label(m)),
from which all consecutive sequences of identi-
cal labels are compressed into a single label. For
instance, (IN, NP, NP, V P, V P) would become
(IN, NP, V P). Ad-hoc conjunctions of these ba-
sic binary features were used as features for our
probability model PS. In total, approximately
800,000 such conjunctions were used.
For Pnt, we needed features that would be rele-
vant to deciding which nonterminal label to give
to a given constituent span. For this somewhat
simpler task, we used a subset of the basic fea-
tures used for PS, shown in bold in Figure 7. Ad-
hoc conjunctions of these boldface binary features
were used as features for our probability model
Pnt. In total, approximately 100,000 such con-
junctions were used.
As mentioned earlier, we used cross-bracketing
statistics as our basis of comparision. These re-
sults as shown in Figure 8. CB denotes the av-
erage cross-bracketing, i.e. the overall percent-
age of candidate constituents that properly overlap
with a constituent in the gold parse. 0CB denotes
the percentage of sentences in the test set that ex-
hibit no cross-bracketing. With a simple feature
set, we manage to obtain performance compara-
ble to the unlexicalized PCFG parser of (Klein and
Manning, 2003) on the set of sentences of length
</bodyText>
<page confidence="0.996864">
375
</page>
<bodyText confidence="0.999154">
40 or less. On the subset of Section 23 consist-
ing of sentences of length 100 or less, our parser
slightly outperforms their results in terms of av-
erage cross-bracketing. Interestingly, our parser
has a lower percentage of sentences exhibiting no
cross bracketing. To reconcile this result with the
superior overall cross-bracketing score, it would
appear that when our parser does make bracketing
errors, the errors tend to be less severe.
The surprise was how quickly the parser per-
formed. Despite its exponential worst-case time
bounds, the search space turned out to be quite
conducive to depth-first branch-and-bound prun-
ing. Using an unoptimized Java implementation
on a 4x Opteron 848 with 16GB of RAM, the
parser required (on average) less than 0.26 sec-
onds per sentence to optimally parse the subset of
Section 23 comprised of sentences of 40 words or
less. It required an average of 0.48 seconds per
sentence to optimally parse the sentences of 100
words or less (an average of less than 3.5 seconds
per sentence for those sentences of length 41-100).
As noted earlier, the parser requires space linear in
the size of the sentence.
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999992256410256">
This project began with a question: can we de-
velop a history-based parsing framework that is
simple, general, and effective? We sought to
provide a versatile probabilistic framework that
would be free from the constraints that dynamic
programming places on PCFG-based approaches.
The work presented in this paper gives favorable
evidence that more flexible (and worst-case in-
tractable) probabilistic approaches can indeed per-
form well in practice, both in terms of running
time and parsing quality.
We can extend this research in multiple direc-
tions. First, the set of features we selected were
chosen with simplicity in mind, to see how well a
simple and unadorned set of features would work,
given our probabilistic model. A next step would
be a more carefully considered feature set. For in-
stance, although lexical information was used, it
was employed in only a most basic sense. There
was no attempt to use head information, which has
been so successful in PCFG parsing methods.
Another parameter to experiment with is the
model order, i.e. the order in which the model vari-
ables are assigned. In this work, we explored only
one specific order (the left-to-right, leaves-to-head
assignment) but in principle there are many other
feasible orders. For instance, one could try a top-
down approach, or a bottom-up approach in which
internal nodes are assigned immediately after all
of their descendants’ values have been determined.
Throughout this paper, we strove to present the
model in a very general manner. There is no rea-
son why this framework cannot be tried in other
application areas that rely on dynamic program-
ming techniques to perform hierarchical labeling,
such as phrase-based machine translation. Apply-
ing this framework to such application areas, as
well as developing a general-purpose parser based
on HLPs, are the subject of our continuing work.
</bodyText>
<sectionHeader confidence="0.999199" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999737823529412">
Ezra Black, Fred Jelinek, John Lafferty, David M.
Magerman, Robert Mercer, and Salim Roukos.
1993. Towards history-based grammars: using
richer models for probabilistic parsing. In Proc.
ACL.
Eric Brill. 1994. Some advances in rule-based part of
speech tagging. In Proc. AAAI.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proc.
AAAI.
Eugene Charniak. 2000. A maximum entropy-inspired
parser. In Proc. NAACL.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proc. ACL.
Michael Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proc. ACL.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Hal Daum´e III. 2004. Notes on CG and LM-BFGS op-
timization of logistic regression. Paper available at
http://www.isi.edu/ hdaume/docs/daume04cg-
bfgs.ps, implementation available at
http://www.isi.edu/ hdaume/megam/, August.
Mark Johnson. 1998. Pcfg models of linguistic
tree representations. Computational Linguistics,
24:613–632.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. ACL.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proc. ACL.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proc. EMNLP.
</reference>
<page confidence="0.999109">
376
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.871330">
<title confidence="0.999952">Exploring the Potential of Intractable Parsers</title>
<author confidence="0.999915">Mark Hopkins</author>
<affiliation confidence="0.999939">Dept. of Computational Linguistics Saarland University</affiliation>
<address confidence="0.931801">Saarbr¨ucken, Germany</address>
<email confidence="0.998335">mhopkins@coli.uni-sb.de</email>
<author confidence="0.999958">Jonas Kuhn</author>
<affiliation confidence="0.9999675">Dept. of Computational Linguistics Saarland University</affiliation>
<address confidence="0.981472">Saarbr¨ucken, Germany</address>
<email confidence="0.999479">jonask@coli.uni-sb.de</email>
<abstract confidence="0.996032583333333">We revisit the idea of history-based parsing, and present a history-based parsing framework that strives to be simple, general, and flexible. We also provide a decoder for this probability model that is linear-space, optimal, and anytime. A parser based on this framework, when evaluated on Section 23 of the Penn Treebank, compares favorably with other stateof-the-art approaches, in terms of both accuracy and speed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Fred Jelinek</author>
<author>John Lafferty</author>
<author>David M Magerman</author>
<author>Robert Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Towards history-based grammars: using richer models for probabilistic parsing. In</title>
<date>1993</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="3128" citStr="Black et al., 1993" startWordPosition="513" endWordPosition="516">ng the data itself, we make our data increasingly sparse as we add features. Thus we are constrained from adding too many features, because at some point we will not have enough data to sustain them. We must strike a delicate balance between how much context we want to include versus how much we dare to partition our data set. DT NN NP IN that money in DT NN 369 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 369–376, Sydney, July 2006. c�2006 Association for Computational Linguistics The major alternative to PCFG-based approaches are so-called history-based parsers (Black et al., 1993). These parsers differ from PCFG parsers in that they incorporate context by using a more complex probability model, rather than by modifying the data itself. The tradeoff to using a more powerful probabilistic model is that one can no longer employ dynamic programming to find the most probable parse. Thus one trades assurances of polynomial running time for greater modeling flexibility. There are two canonical parsers that fall into this category: the decision-tree parser of (Magerman, 1995), and the maximum-entropy parser of (Ratnaparkhi, 1997). Both showed decent results on parsing the Penn</context>
</contexts>
<marker>Black, Jelinek, Lafferty, Magerman, Mercer, Roukos, 1993</marker>
<rawString>Ezra Black, Fred Jelinek, John Lafferty, David M. Magerman, Robert Mercer, and Salim Roukos. 1993. Towards history-based grammars: using richer models for probabilistic parsing. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Some advances in rule-based part of speech tagging.</title>
<date>1994</date>
<booktitle>In Proc. AAAI.</booktitle>
<contexts>
<context position="25471" citStr="Brill, 1994" startWordPosition="4568" endWordPosition="4569">ave the same length and integer i is less than integer k. For auto-assignment function A, we essentially used the function in Figure 5, modified so that it automatically assigned null to model variables Lwd ij and Lpt ijfor i =� j (i.e. no preterminal or word tagging of internal nodes), and to model variables Lnt ii (i.e. no nonterminal tagging of leaves, rendered unnecessary by our preprocessing step). Rather than incorporate part-of-speech tagging into the search process, we opted to pretag the sentences of our development and test sets with an off-the-shelf tagger, namely the Brill tagger (Brill, 1994). Thus the object of our computation was HLPDECODE(x, n, w), where n was the length of the sentence, and partial assignment w specified the word and PT labels of the leaves. Given this partial assignment, the job of HLPDECODE was to find the most probable assignment of model variables Sij and Lnt ij for 1 &lt; i &lt; j &lt; n. The two probability models, PS and Pnt, were trained in the manner described in Section 4. Two decisions needed to be made: which features to use and which learning technique to employ. As for the learning technique, we used maximum entropy models, specifically the implementation</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>Eric Brill. 1994. Some advances in rule-based part of speech tagging. In Proc. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proc. AAAI.</booktitle>
<contexts>
<context position="857" citStr="Charniak, 1997" startWordPosition="120" endWordPosition="121">rmany jonask@coli.uni-sb.de Abstract We revisit the idea of history-based parsing, and present a history-based parsing framework that strives to be simple, general, and flexible. We also provide a decoder for this probability model that is linear-space, optimal, and anytime. A parser based on this framework, when evaluated on Section 23 of the Penn Treebank, compares favorably with other stateof-the-art approaches, in terms of both accuracy and speed. 1 Introduction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1996; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001; Klein and Manning, 2003). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the application of rule “NP → NP PP,” followed by rule “NP → DT NN,” followed by rule “DT → that,” and so forth. Hence instead of analyzing P(tree), we deal with the more modular: P(NP → NP PP, NP → DT NN, DT → that, NN → money, PP → IN NP, IN → in, NP → DT NN, DT → the, NN → market) Obviously this joint distribution is just as dif</context>
<context position="26317" citStr="Charniak (1997)" startWordPosition="4722" endWordPosition="4723">o find the most probable assignment of model variables Sij and Lnt ij for 1 &lt; i &lt; j &lt; n. The two probability models, PS and Pnt, were trained in the manner described in Section 4. Two decisions needed to be made: which features to use and which learning technique to employ. As for the learning technique, we used maximum entropy models, specifically the implementation called MegaM provided by Hal Daume (Daum´e III, 2004). For PS, we needed features &lt; 40 0CB &lt; 100 0CB CB CB Magerman (1995) 1.26 56.6 Collins (1996) 1.14 59.9 Klein/Manning (2003) 1.10 60.3 1.31 57.2 this paper 1.09 58.2 1.25 55.2 Charniak (1997) 1.00 62.1 Collins (1999) 0.90 67.1 Figure 8: Cross-bracketing results for Section 23 of the Penn Treebank. that would be relevant to deciding whether a given span (i, j) should be considered a constituent. The basic building blocks we used are depicted in Figure 7. A few words of explanation are in order. By label(k), we mean the highest nonterminal label so far assigned that covers word k, or if such a label does not yet exist, then the preterminal label of k (recall that our model order was bottom-up). By category(k), we mean the category of the preterminal label of word k (given a coarser,</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proc. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="888" citStr="Charniak, 2000" startWordPosition="124" endWordPosition="125">tract We revisit the idea of history-based parsing, and present a history-based parsing framework that strives to be simple, general, and flexible. We also provide a decoder for this probability model that is linear-space, optimal, and anytime. A parser based on this framework, when evaluated on Section 23 of the Penn Treebank, compares favorably with other stateof-the-art approaches, in terms of both accuracy and speed. 1 Introduction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1996; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001; Klein and Manning, 2003). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the application of rule “NP → NP PP,” followed by rule “NP → DT NN,” followed by rule “DT → that,” and so forth. Hence instead of analyzing P(tree), we deal with the more modular: P(NP → NP PP, NP → DT NN, DT → that, NN → money, PP → IN NP, IN → in, NP → DT NN, DT → the, NN → market) Obviously this joint distribution is just as difficult to assess and compute wi</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum entropy-inspired parser. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="904" citStr="Charniak, 2001" startWordPosition="126" endWordPosition="127"> the idea of history-based parsing, and present a history-based parsing framework that strives to be simple, general, and flexible. We also provide a decoder for this probability model that is linear-space, optimal, and anytime. A parser based on this framework, when evaluated on Section 23 of the Penn Treebank, compares favorably with other stateof-the-art approaches, in terms of both accuracy and speed. 1 Introduction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1996; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001; Klein and Manning, 2003). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the application of rule “NP → NP PP,” followed by rule “NP → DT NN,” followed by rule “DT → that,” and so forth. Hence instead of analyzing P(tree), we deal with the more modular: P(NP → NP PP, NP → DT NN, DT → that, NN → money, PP → IN NP, IN → in, NP → DT NN, DT → the, NN → market) Obviously this joint distribution is just as difficult to assess and compute with as P(tree). H</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="841" citStr="Collins, 1996" startWordPosition="118" endWordPosition="119">aarbr¨ucken, Germany jonask@coli.uni-sb.de Abstract We revisit the idea of history-based parsing, and present a history-based parsing framework that strives to be simple, general, and flexible. We also provide a decoder for this probability model that is linear-space, optimal, and anytime. A parser based on this framework, when evaluated on Section 23 of the Penn Treebank, compares favorably with other stateof-the-art approaches, in terms of both accuracy and speed. 1 Introduction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1996; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001; Klein and Manning, 2003). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the application of rule “NP → NP PP,” followed by rule “NP → DT NN,” followed by rule “DT → that,” and so forth. Hence instead of analyzing P(tree), we deal with the more modular: P(NP → NP PP, NP → DT NN, DT → that, NN → money, PP → IN NP, IN → in, NP → DT NN, DT → the, NN → market) Obviously this joint distributio</context>
<context position="26219" citStr="Collins (1996)" startWordPosition="4706" endWordPosition="4707">d the word and PT labels of the leaves. Given this partial assignment, the job of HLPDECODE was to find the most probable assignment of model variables Sij and Lnt ij for 1 &lt; i &lt; j &lt; n. The two probability models, PS and Pnt, were trained in the manner described in Section 4. Two decisions needed to be made: which features to use and which learning technique to employ. As for the learning technique, we used maximum entropy models, specifically the implementation called MegaM provided by Hal Daume (Daum´e III, 2004). For PS, we needed features &lt; 40 0CB &lt; 100 0CB CB CB Magerman (1995) 1.26 56.6 Collins (1996) 1.14 59.9 Klein/Manning (2003) 1.10 60.3 1.31 57.2 this paper 1.09 58.2 1.25 55.2 Charniak (1997) 1.00 62.1 Collins (1999) 0.90 67.1 Figure 8: Cross-bracketing results for Section 23 of the Penn Treebank. that would be relevant to deciding whether a given span (i, j) should be considered a constituent. The basic building blocks we used are depicted in Figure 7. A few words of explanation are in order. By label(k), we mean the highest nonterminal label so far assigned that covers word k, or if such a label does not yet exist, then the preterminal label of k (recall that our model order was bot</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="872" citStr="Collins, 1999" startWordPosition="122" endWordPosition="123">i.uni-sb.de Abstract We revisit the idea of history-based parsing, and present a history-based parsing framework that strives to be simple, general, and flexible. We also provide a decoder for this probability model that is linear-space, optimal, and anytime. A parser based on this framework, when evaluated on Section 23 of the Penn Treebank, compares favorably with other stateof-the-art approaches, in terms of both accuracy and speed. 1 Introduction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1996; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001; Klein and Manning, 2003). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the application of rule “NP → NP PP,” followed by rule “NP → DT NN,” followed by rule “DT → that,” and so forth. Hence instead of analyzing P(tree), we deal with the more modular: P(NP → NP PP, NP → DT NN, DT → that, NN → money, PP → IN NP, IN → in, NP → DT NN, DT → the, NN → market) Obviously this joint distribution is just as difficult to asses</context>
<context position="26342" citStr="Collins (1999)" startWordPosition="4726" endWordPosition="4727">ssignment of model variables Sij and Lnt ij for 1 &lt; i &lt; j &lt; n. The two probability models, PS and Pnt, were trained in the manner described in Section 4. Two decisions needed to be made: which features to use and which learning technique to employ. As for the learning technique, we used maximum entropy models, specifically the implementation called MegaM provided by Hal Daume (Daum´e III, 2004). For PS, we needed features &lt; 40 0CB &lt; 100 0CB CB CB Magerman (1995) 1.26 56.6 Collins (1996) 1.14 59.9 Klein/Manning (2003) 1.10 60.3 1.31 57.2 this paper 1.09 58.2 1.25 55.2 Charniak (1997) 1.00 62.1 Collins (1999) 0.90 67.1 Figure 8: Cross-bracketing results for Section 23 of the Penn Treebank. that would be relevant to deciding whether a given span (i, j) should be considered a constituent. The basic building blocks we used are depicted in Figure 7. A few words of explanation are in order. By label(k), we mean the highest nonterminal label so far assigned that covers word k, or if such a label does not yet exist, then the preterminal label of k (recall that our model order was bottom-up). By category(k), we mean the category of the preterminal label of word k (given a coarser, hand-made categorization</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Notes on CG and LM-BFGS optimization of logistic regression. Paper available at http://www.isi.edu/ hdaume/docs/daume04cgbfgs.ps, implementation available at http://www.isi.edu/ hdaume/megam/,</title>
<date>2004</date>
<marker>Daum´e, 2004</marker>
<rawString>Hal Daum´e III. 2004. Notes on CG and LM-BFGS optimization of logistic regression. Paper available at http://www.isi.edu/ hdaume/docs/daume04cgbfgs.ps, implementation available at http://www.isi.edu/ hdaume/megam/, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Pcfg models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--613</pages>
<contexts>
<context position="2140" citStr="Johnson, 1998" startWordPosition="351" endWordPosition="352">ic-time dynamic programming algorithms to find the most likely parse if we assume that all CFG rule applications are marginally NP NP PP the market Figure 1: Example parse tree. independent of one another. The problem, of course, with this simplification is that although it is computationally attractive, it is usually too strong of an independence assumption. To mitigate this loss of context, without sacrificing algorithmic tractability, typically researchers annotate the nodes of the parse tree with contextual information. A simple example is the annotation of nodes with their parent labels (Johnson, 1998). The choice of which annotations to use is one of the main features that distinguish parsers based on this approach. Generally, this approach has proven quite effective in producing English phrase-structure grammar parsers that perform well on the Penn Treebank. One drawback of this approach is its inflexibility. Because we are adding probabilistic context by changing the data itself, we make our data increasingly sparse as we add features. Thus we are constrained from adding too many features, because at some point we will not have enough data to sustain them. We must strike a delicate balan</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. Pcfg models of linguistic tree representations. Computational Linguistics, 24:613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="930" citStr="Klein and Manning, 2003" startWordPosition="128" endWordPosition="131">tory-based parsing, and present a history-based parsing framework that strives to be simple, general, and flexible. We also provide a decoder for this probability model that is linear-space, optimal, and anytime. A parser based on this framework, when evaluated on Section 23 of the Penn Treebank, compares favorably with other stateof-the-art approaches, in terms of both accuracy and speed. 1 Introduction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1996; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001; Klein and Manning, 2003). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the application of rule “NP → NP PP,” followed by rule “NP → DT NN,” followed by rule “DT → that,” and so forth. Hence instead of analyzing P(tree), we deal with the more modular: P(NP → NP PP, NP → DT NN, DT → that, NN → money, PP → IN NP, IN → in, NP → DT NN, DT → the, NN → market) Obviously this joint distribution is just as difficult to assess and compute with as P(tree). However there exist cubic-t</context>
<context position="28364" citStr="Klein and Manning, 2003" startWordPosition="5075" endWordPosition="5078">y features were used as features for our probability model Pnt. In total, approximately 100,000 such conjunctions were used. As mentioned earlier, we used cross-bracketing statistics as our basis of comparision. These results as shown in Figure 8. CB denotes the average cross-bracketing, i.e. the overall percentage of candidate constituents that properly overlap with a constituent in the gold parse. 0CB denotes the percentage of sentences in the test set that exhibit no cross-bracketing. With a simple feature set, we manage to obtain performance comparable to the unlexicalized PCFG parser of (Klein and Manning, 2003) on the set of sentences of length 375 40 or less. On the subset of Section 23 consisting of sentences of length 100 or less, our parser slightly outperforms their results in terms of average cross-bracketing. Interestingly, our parser has a lower percentage of sentences exhibiting no cross bracketing. To reconcile this result with the superior overall cross-bracketing score, it would appear that when our parser does make bracketing errors, the errors tend to be less severe. The surprise was how quickly the parser performed. Despite its exponential worst-case time bounds, the search space turn</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="3625" citStr="Magerman, 1995" startWordPosition="592" endWordPosition="594">al Linguistics The major alternative to PCFG-based approaches are so-called history-based parsers (Black et al., 1993). These parsers differ from PCFG parsers in that they incorporate context by using a more complex probability model, rather than by modifying the data itself. The tradeoff to using a more powerful probabilistic model is that one can no longer employ dynamic programming to find the most probable parse. Thus one trades assurances of polynomial running time for greater modeling flexibility. There are two canonical parsers that fall into this category: the decision-tree parser of (Magerman, 1995), and the maximum-entropy parser of (Ratnaparkhi, 1997). Both showed decent results on parsing the Penn Treebank, but in the decade since these papers were published, history-based parsers have been largely ignored by the research community in favor of PCFG-based approaches. There are several reasons why this may be. First is naturally the matter of time efficiency. Magerman reports decent parsing times, but for the purposes of efficiency, must restrict his results to sentences of length 40 or less. Furthermore, his twophase stack decoder is a bit complicated and is acknowledged to require too</context>
<context position="26194" citStr="Magerman (1995)" startWordPosition="4702" endWordPosition="4703">tial assignment w specified the word and PT labels of the leaves. Given this partial assignment, the job of HLPDECODE was to find the most probable assignment of model variables Sij and Lnt ij for 1 &lt; i &lt; j &lt; n. The two probability models, PS and Pnt, were trained in the manner described in Section 4. Two decisions needed to be made: which features to use and which learning technique to employ. As for the learning technique, we used maximum entropy models, specifically the implementation called MegaM provided by Hal Daume (Daum´e III, 2004). For PS, we needed features &lt; 40 0CB &lt; 100 0CB CB CB Magerman (1995) 1.26 56.6 Collins (1996) 1.14 59.9 Klein/Manning (2003) 1.10 60.3 1.31 57.2 this paper 1.09 58.2 1.25 55.2 Charniak (1997) 1.00 62.1 Collins (1999) 0.90 67.1 Figure 8: Cross-bracketing results for Section 23 of the Penn Treebank. that would be relevant to deciding whether a given span (i, j) should be considered a constituent. The basic building blocks we used are depicted in Figure 7. A few words of explanation are in order. By label(k), we mean the highest nonterminal label so far assigned that covers word k, or if such a label does not yet exist, then the preterminal label of k (recall tha</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical decision-tree models for parsing. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="3680" citStr="Ratnaparkhi, 1997" startWordPosition="600" endWordPosition="601">pproaches are so-called history-based parsers (Black et al., 1993). These parsers differ from PCFG parsers in that they incorporate context by using a more complex probability model, rather than by modifying the data itself. The tradeoff to using a more powerful probabilistic model is that one can no longer employ dynamic programming to find the most probable parse. Thus one trades assurances of polynomial running time for greater modeling flexibility. There are two canonical parsers that fall into this category: the decision-tree parser of (Magerman, 1995), and the maximum-entropy parser of (Ratnaparkhi, 1997). Both showed decent results on parsing the Penn Treebank, but in the decade since these papers were published, history-based parsers have been largely ignored by the research community in favor of PCFG-based approaches. There are several reasons why this may be. First is naturally the matter of time efficiency. Magerman reports decent parsing times, but for the purposes of efficiency, must restrict his results to sentences of length 40 or less. Furthermore, his twophase stack decoder is a bit complicated and is acknowledged to require too much memory to handle certain sentences. Ratnaparkhi i</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proc. EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>