<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000147">
<title confidence="0.958807">
Unsupervised Language Model Adaptation Incorporating
Named Entity Information
</title>
<author confidence="0.996764">
Feifan Liu and Yang Liu
</author>
<affiliation confidence="0.9974075">
Department of Computer Science
The University of Texas at Dallas, Richardson, TX, USA
</affiliation>
<email confidence="0.998925">
{ffliu,yangl}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.99666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953769230769">
Language model (LM) adaptation is im-
portant for both speech and language
processing. It is often achieved by com-
bining a generic LM with a topic-specific
model that is more relevant to the target
document. Unlike previous work on un-
supervised LM adaptation, this paper in-
vestigates how effectively using named
entity (NE) information, instead of con-
sidering all the words, helps LM adapta-
tion. We evaluate two latent topic analysis
approaches in this paper, namely, cluster-
ing and Latent Dirichlet Allocation
(LDA). In addition, a new dynamically
adapted weighting scheme for topic mix-
ture models is proposed based on LDA
topic analysis. Our experimental results
show that the NE-driven LM adaptation
framework outperforms the baseline ge-
neric LM. The best result is obtained us-
ing the LDA-based approach by
expanding the named entities with syntac-
tically filtered words, together with using
a large number of topics, which yields a
perplexity reduction of 14.23% compared
to the baseline generic LM.
</bodyText>
<sectionHeader confidence="0.999163" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992288372093">
Language model (LM) adaptation plays an impor-
tant role in speech recognition and many natural
language processing tasks, such as machine trans-
lation and information retrieval. Statistical N-gram
LMs have been widely used; however, they capture
only local contextual information. In addition, even
with the increasing amount of LM training data,
there is often a mismatch problem because of dif-
ferences in domain, topics, or styles. Adaptation of
LM, therefore, is very important in order to better
deal with a variety of topics and styles.
Many studies have been conducted for LM ad-
aptation. One method is supervised LM adaptation,
where topic information is typically available and a
topic specific LM is interpolated with the generic
LM (Kneser and Steinbiss, 1993; Suzuki and Gao,
2005). In contrast, various unsupervised ap-
proaches perform latent topic analysis for LM ad-
aptation. To identify implicit topics from the
unlabeled corpus, one simple technique is to group
the documents into topic clusters by assigning only
one topic label to a document (Iyer and Ostendorf,
1996). Recently several other methods in the line
of latent semantic analysis have been proposed and
used in LM adaptation, such as latent semantic
analysis (LSA) (Bellegarda, 2000), probabilistic
latent semantic analysis (PLSA) (Gildea and Hof-
mann, 1999), and LDA (Blei et al., 2003). Most of
these existing approaches are based on the “bag of
words” model to represent documents, where all
the words are treated equally and no relation or
association between words is considered.
Unlike prior work in LM adaptation, this paper
investigates how to effectively leverage named
entity information for latent topic analysis. Named
entities are very common in domains such as
newswire or broadcast news, and carry valuable
information, which we hypothesize is topic indica-
tive and useful for latent topic analysis. We com-
pare different latent topic generation approaches as
well as model adaptation methods, and propose an
LDA based dynamic weighting method for the
topic mixture model. Furthermore, we expand
</bodyText>
<page confidence="0.945242">
672
</page>
<note confidence="0.926123">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 672–679,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999987428571429">
named entities by incorporating other content
words, in order to capture more topic information.
Our experimental results show that the proposed
method of incorporating named information in LM
adaptation is effective. In addition, we find that for
the LDA based adaptation scheme, adding more
content words and increasing the number of topics
can further improve the performance significantly.
The paper is organized as follows. In Section 2
we review some related work. Section 3 describes
in detail our unsupervised LM adaptation approach
using named entities. Experimental results are pre-
sented and discussed in Section 4. Conclusion and
future work appear in Section 5.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999986580645161">
There has been a lot of previous related work on
LM adaptation. Suzuki and Gao (2005) compared
different supervised LM adaptation approaches,
and showed that three discriminative methods sig-
nificantly outperform the maximum a posteriori
(MAP) method. For unsupervised LM adaptation,
an earlier attempt is a cache-based model (Kuhn
and Mori, 1990), developed based on the assump-
tion that words appearing earlier in a document are
likely to appear again. The cache concept has also
been used to increase the probability of unseen but
topically related words, for example, the trigger-
based LM adaptation using the maximum entropy
approach (Rosenfeld, 1996).
Latent topic analysis has recently been investi-
gated extensively for language modeling. Iyer and
Ostendorf (1996) used hard clustering to obtain
topic clusters for LM adaptation, where a single
topic is assigned to each document. Bellegarda
(2000) employed Latent Semantic Analysis (LSA)
to map documents into implicit topic sub-spaces
and demonstrated significant reduction in perplex-
ity and word error rate (WER). Its probabilistic
extension, PLSA, is powerful for characterizing
topics and documents in a probabilistic space and
has been used in LM adaptation. For example,
Gildea and Hofmann (1999) reported noticeable
perplexity reduction via a dynamic combination of
many unigram topic models with a generic trigram
model. Proposed by Blei et al. (2003), Latent
Dirichlet Allocation (LDA) loosens the constraint
of the document-specific fixed weights by using a
prior distribution and has quickly become one of
the most popular probabilistic text modeling tech-
niques. LDA can overcome the drawbacks in the
PLSA model, and has been shown to outperform
PLSA in corpus perplexity and text classification
experiments (Blei et al., 2003). Tam and Schultz
(2005) successfully applied the LDA model to un-
supervised LM adaptation by interpolating the
background LM with the dynamic unigram LM
estimated by the LDA model. Hsu and Glass (2006)
investigated using hidden Markov model with
LDA to allow for both topic and style adaptation.
Mrva and Woodland (2006) achieved WER reduc-
tion on broadcast conversation recognition using
an LDA based adaptation approach that effectively
combined the LMs trained from corpora with dif-
ferent styles: broadcast news and broadcast con-
versation data.
In this paper, we investigate unsupervised LM
adaptation using clustering and LDA based topic
analysis. Unlike the clustering based interpolation
method as in (Iyer and Ostendorf, 1996), we ex-
plore different distance measure methods for topic
analysis. Different from the LDA based framework
as in (Tam and Schultz, 2005), we propose a novel
dynamic weighting scheme for the topic adapted
LM. More importantly, the focus of our work is to
investigate the role of named entity information in
LM adaptation, which to our knowledge has not
been explored.
</bodyText>
<sectionHeader confidence="0.9809045" genericHeader="method">
3 Unsupervised LM Adaptation Integrat-
ing Named Entities (NEs)
</sectionHeader>
<subsectionHeader confidence="0.9958985">
3.1 Overview of the NE-driven LM Adapta-
tion Framework
</subsectionHeader>
<bodyText confidence="0.9999686875">
Figure 1 shows our unsupervised LM adaptation
framework using NEs. For training, we use the text
collection to train the generic word-based N-gram
LM. Then we apply named entity recognition
(NER) and topic analysis to train multiple topic
specific N-gram LMs. During testing, NER is per-
formed on each test document, and then a dynami-
cally adaptive LM based on the topic analysis
result is combined with the general LM. Note that
in this figure, we evaluate the performance of LM
adaptation using the perplexity measure. We will
evaluate this framework for N-best or lattice res-
coring in speech recognition in the future.
In our experiments, different topic analysis
methods combined with different topic matching
and adaptive schemes result in several LM adapta-
</bodyText>
<page confidence="0.998666">
673
</page>
<bodyText confidence="0.8885755">
tion paradigms, which are described below in de-
tails.
</bodyText>
<figureCaption confidence="0.996103">
Figure 1. Framework of NE-driven LM adaptation.
</figureCaption>
<subsectionHeader confidence="0.99222">
3.2 NE-based Clustering for LM Adaptation
</subsectionHeader>
<bodyText confidence="0.999566928571429">
Clustering is a simple unsupervised topic analysis
method. We use NEs to construct feature vectors
for the documents, rather than considering all the
words as in most previous work. We use the
CLUTO1 toolkit to perform clustering. It finds a
predefined number of clusters based on a specific
criterion, for which we chose the following func-
tion:
where K is the desired number of clusters, Si is the
set of documents belonging to the ith cluster, v and
u represent two documents, and sim(v, u) is the
similarity between them. We use the cosine dis-
tance to measure the similarity between two docu-
ments:
</bodyText>
<equation confidence="0.998571571428571">
r r
= r
       ||
v u
v u
⋅
⋅ r (1)
</equation>
<bodyText confidence="0.9999434">
where vr and ur are the feature vectors represent-
ing the two documents respectively, in our experi-
ments composed of NEs. For clustering, the
elements in every feature vector are scaled based
on their term frequency and inverse document fre-
</bodyText>
<footnote confidence="0.429595">
1 Available at http://glaros.dtc.umn.edu/gkhome/views/cluto
</footnote>
<bodyText confidence="0.997986454545455">
quency, a concept widely used in information re-
trieval.
After clustering, we train an N-gram LM, called
a topic LM, for each cluster using the documents in
it.
During testing, we identify the ‘topic’ for the
test document, and interpolate the topic specific
LM with the background LM, that is, if the test
document belongs to the cluster S*, we can predict
a word wk in the document given the word’s his-
tory hk using the following equation:
</bodyText>
<equation confidence="0.995228333333333">
p(wk  |hk ) = λpGeneral (wk  |hk )
+ −
(1 λ)pTopic −S* (wk  |hk) (2)
</equation>
<bodyText confidence="0.9604005">
where λ is the interpolation weight.
We investigate two approaches to find the topic
assignment S* for a given test document.
(A) cross-entropy measure
For a test document d=w1,w2,...,wn with a word
distribution pd(w) and a cluster S with a topic LM
ps(w), the cross entropy CE(d, S) can be computed
as:
</bodyText>
<equation confidence="0.993561">
n
CE(d, S) = H(pd , ps) = −∑ pd (wi) log2 (ps (wi ))
i=1
</equation>
<bodyText confidence="0.999988166666667">
From the information theoretic perspective, the
cluster with the lower cross entropy value is ex-
pected to be more topically correlated to the test
document. For each test document, we compute the
cross entropy values according to different clusters,
and select the cluster S* that satisfies:
</bodyText>
<equation confidence="0.995565222222222">
S* = arg min CE(d, S
i)
1≤ ≤
i K
(B) cosine similarity
For each cluster, its centroid can be obtained by:
ni
1 ∑u
k=1
</equation>
<bodyText confidence="0.9752247">
where uik is the vector for the
document in the ith
cluster, and ni is the number of documents in the ith
cluster. The distance between the test document
and a cluster can then be easily measured by the
cosine similari
kth
ty function as in Equation (1). Our
goal here is to find the cluster S* which the test
document is closest to, that is,
</bodyText>
<figure confidence="0.982760533333333">
Generic N-gram
Training
Training Text Test Text
Topic Model
Training
Latent Topic
Analysis
NER NER
Topic Matching
Topic Model
Adaptation
Model
Interpolation
Compute
Perplexity
</figure>
<equation confidence="0.891630071428572">
K
(S1S2LSk)* =argmax∑ ∑sim(v,u)
i = 1 v u S
, ∈ i
sim (v, u)
ik
ni
r
S =arg max
1≤ ≤
i K  ||d ||⋅  ||cvi ||
*
d ⋅ cvi
r
</equation>
<page confidence="0.988552">
674
</page>
<bodyText confidence="0.894463">
where d is the feature vector for the test document. model using the mixture of LMs from different
topics:
</bodyText>
<subsectionHeader confidence="0.974826">
3.3 NE-based LDA for LM Adaptation
</subsectionHeader>
<bodyText confidence="0.9996015">
LDA model (Blei et al., 2003) has been introduced
as a new, semantically consistent generative model,
which overcomes overfitting and the problem of
generating new documents in PLSA. It is a three-
level hierarchical Bayesian model. Based on the
LDA model, a document d is generated as follows.
</bodyText>
<listItem confidence="0.980527">
• Sample a vector of K topic mixture weights
θ from a prior Dirichlet distribution with
parameter α :
</listItem>
<equation confidence="0.9986626">
K
f ( ; )
θ α = ∏θαk−
k
k=1
</equation>
<listItem confidence="0.96944425">
• For each word w in d, pick a topic k from the
multinomial distribution θ .
• Pick a word w from the multinomial distri-
bution βw,k given the kth topic.
</listItem>
<bodyText confidence="0.932260666666667">
For a document d=w1,w2,...wn, the LDA model
assigns it the following probability:
p d
</bodyText>
<equation confidence="0.991293833333333">
( ) = ⎛n K
∫ ∏∑
θ ⎝ i = =
⎜
1 1
k
</equation>
<bodyText confidence="0.998931833333333">
We use the MATLAB topic Toolbox 1.3 (Grif-
fiths et al., 2004) in the training set to obtain the
document-topic matrix, DP, and the word-topic
matrix, WP. Note that here “words” correspond to
the elements in the feature vector used to represent
the document (e.g., NEs). In the DP matrix, an en-
try cik represents the counts of words in a document
di that are from a topic zk (k=1,2,...,K). In the WP
matrix, an entry fjk represents the frequency of a
word wj generated from a topic zk (k=1,2,...,K)
over the training set.
For training, we assign a topic zi* to a document
</bodyText>
<equation confidence="0.703055333333333">
*
di such that zi = argmaxcik . Based on the docu-
1≤k≤K
</equation>
<bodyText confidence="0.9999227">
ments belonging to the different topics, K topic N-
gram LMs are trained. This “hard clustering” strat-
egy allows us to train an LM that accounts for all
the words rather than simply those NEs used in
LDA analysis, as well as use higher order N-gram
LMs, unlike the ‘unigram’ based LDA in previous
work.
For a test document d = w1,w2,...,wn that is gen-
erated by multiple topics under the LDA assump-
tion, we formulate a dynamically adapted topic
</bodyText>
<equation confidence="0.99400475">
K
p (  |) γ ( |
w h = × p w h
LDA adapt k k
− i z k k
i
∑=
i 1
</equation>
<bodyText confidence="0.999264333333333">
where pzi (wk  |hk) stands for the ith topic LM, and
γi is the mixture weight. Different from the idea of
dynamic topic adaptation in (Tam and Schultz,
2005), we propose a new weighting scheme to cal-
culate γi that directly uses the two resulting matri-
ces from LDA analysis during training:
</bodyText>
<equation confidence="0.730854">
n
∑ f ∑ freq ( )
jp wq
p 1 q=1
</equation>
<bodyText confidence="0.9995692">
where freq(wj) is the frequency of a word wj in the
document d. Other notations are consistent with the
previous definitions.
Then we interpolate this adapted topic model
with the generic LM, similar to Equation (2):
</bodyText>
<equation confidence="0.998544">
p(wk  |hk ) = λ pGeneral (wk  |hk )
− λ )pLDA−adapt (wk  |hk ) (3)
</equation>
<sectionHeader confidence="0.998857" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.929218">
4.1 Experimental Setup
</subsectionHeader>
<table confidence="0.99561">
# of files # of words # of NEs
Training Data 23,985 7,345,644 590,656
Test Data 2,661 831,283 65,867
</table>
<tableCaption confidence="0.999834">
Table 1. Statistics of our experimental data.
</tableCaption>
<bodyText confidence="0.999859">
The data set we used is the LDC Mandarin TDT4
corpus, consisting of 337 broadcast news shows
with transcriptions. These files were split into
small pieces, which we call documents here, ac-
cording to the topic segmentation information
marked in the LDC’s transcription. In total, there
are 26,646 such documents in our data set. We
randomly chose 2661 files as the test data (which
is balanced for different news sources). The rest
was used for topic analysis and also generic LM
training. Punctuation marks were used to deter-
mine sentences in the transcriptions. We used the
NYU NE tagger (Ji and Grishman, 2005) to recog-
nize four kinds of NEs: Person, Location, Organi-
</bodyText>
<figure confidence="0.9689424">
)
n
γk = p(zk  |wj)p(wj  |d
)
∑=
j 1
, p(wj  |d) =
1 p ( zk  |) = fjk
w j
K
(wj)
freq
βwik ⋅ θk ⎞ ⎟ f θ α dθ + (1
( ; )
⎠
</figure>
<page confidence="0.992947">
675
</page>
<bodyText confidence="0.999841777777778">
zation, and Geo-political. Table 1 shows the statis-
tics of the data set in our experiments.
We trained trigram LMs using the SRILM tool-
kit (Stolcke, 2002). A fixed weight (i.e., λ in
Equation (2) and (3)) was used for the entire test
set when interpolating the generic LM with the
adapted topic LM. Perplexity was used to measure
the performance of different adapted LMs in our
experiments.
</bodyText>
<subsectionHeader confidence="0.950382">
4.2 Latent Topic Analysis Results
</subsectionHeader>
<table confidence="0.980397948717949">
Topic # of Top 10 Descriptive Items
Files (Translated from Chinese)
Cluster- 1 3526 U.S., Israel, Washington, Palestine,
ing Bush, Clinton, Gore, Voice of Amer-
Based ica, Mid-East, Republican Party
2 3067 Taiwan, Taipei, Mainland, Taipei
City, Chinese People’s Broadcasting
Station, Shuibian Chen, the Execu-
tive Yuan, the Legislative Yuan, De-
mocratic Progressive Party,
Nationalist Party
3 4857 Singapore, Japan, Hong Kong, Indo-
nesia, Asia, Tokyo, Malaysia, Thai-
land, World, China
4 4495 World, German, Landon, Russia,
France, England, Xinhua News
Agency, Europe, U.S., Italy
5 7586 China, Beijing, Nation, China Central
Television Station, Xinhua News
Agency, Shanghai, World, State
Council, Zemin Jiang, Beijing City
LDA 1 5859 China, Japan, Hong Kong, Beijing,
Based Shanghai, World, Zemin Jiang, Ma-
cao, China Central Television Sta-
tion, Africa
2 3794 U.S., Bush, World, Gore, South
Korea, North Korea, Clinton, George
Walker Bush, Asia, Thailand
3 4640 Singapore, Indonesia, Team, Israel,
Europe, Germany, England, France,
Palestine, Wahid
4 4623 Taiwan, Russia, Mainland, India,
Taipei, Shuibian Chen, Philippine,
Estrada, Communist Party of China,
RUS.
5 4729 Xinhua News Agency, Nation, Bei-
jing, World, Canada, Sydney, Brazil,
Beijing City, Education Ministry,
Cuba
</table>
<tableCaption confidence="0.990068">
Table 2. Topic analysis results using clustering
</tableCaption>
<bodyText confidence="0.970656741935484">
and LDA (the number of documents and the top 10
words (NEs) in each cluster).
For latent topic analysis, we investigated two ap-
proaches using named entities, i.e., clustering and
LDA. 5 latent topics were used in both approaches.
Table 2 illustrates the resulting topics using the top
10 words in each topic. We can see that the words
in the same cluster share some similarity and that
the words in different clusters seem to be ‘topi-
cally’ different. Note that errors from automatic
NE recognition may impact the clustering results.
For example, ‘队/team’ in the table (in topic 3 in
LDA results) is an error and is less discriminative
for topic analysis.
Table 3 shows the perplexity of the test set us-
ing the background LM (baseline) and each of the
topic LMs, from clustering and LDA respectively.
We can see that for the entire test set, a topic LM
generally performs much worse than the generic
LM. This is expected, since the size of a topic clus-
ter is much smaller than that of the entire training
set, and the test set may contain documents from
different topics. However, we found that when us-
ing an optimal topic model (i.e., the topic LM that
yields the lowest perplexity among the 5 topic
LMs), 23.45% of the documents in the test set have
a lower perplexity value than that obtained from
the generic LM. This suggests that a topic model
could benefit LM adaptation and motivates a dy-
namic topic adaptation approach for different test
documents.
</bodyText>
<table confidence="0.95787225">
Perplexity
Baseline 502.02
CL-1 1054.36
CL-2 1399.16
CL-3 919.237
CL-4 962.996
CL-5 981.072
LDA-1 1224.54
LDA-2 1375.97
LDA-3 1330.44
LDA-4 1328.81
LDA-5 1287.05
</table>
<tableCaption confidence="0.969813">
Table 3. Perplexity results using the baseline LM
vs. the single topic LMs.
</tableCaption>
<subsectionHeader confidence="0.997113">
4.3 Clustering vs. LDA Based LM Adaptation
</subsectionHeader>
<bodyText confidence="0.9999244">
In this section, we compare three LM adaptation
paradigms. As we discussed in Section 3, two of
them are clustering based topic analysis, but using
different strategies to choose the optimal cluster;
and the third one is based on LDA analysis that
</bodyText>
<page confidence="0.998207">
676
</page>
<bodyText confidence="0.997569818181818">
uses a dynamic weighting scheme for adapted
topic mixture model.
Figure 2 shows the perplexity results using dif-
ferent interpolation parameters with the general
LM. 5 topics were used in both clustering and
LDA based approaches (as in Section 4.2). “CL-
CE” means clustering based topic analysis via
cross entropy criterion, “CL-Cos” represents clus-
tering based topic analysis via cosine distance cri-
terion, and “LDA-MIX” denotes LDA based topic
mixture model, which uses 5 mixture topic LMs.
</bodyText>
<figureCaption confidence="0.822509333333333">
Figure 2. Perplexity using different LM adaptation
approaches and different interpolation weights λ
with the general LM.
</figureCaption>
<bodyText confidence="0.9998515">
We observe that all three adaptation approaches
outperform the baseline when using a proper inter-
polation weight. “CL-CE” yields the best perplex-
ity of 469.75 when λ is 0.5, a reduction of 6.46%
against the baseline perplexity of 502.02. For clus-
tering based adaptation, between the two strategies
used to determine the topic for a test document,
“CL-CE” outperforms “CL-Cos”. This indicates
that the cosine distance measure using only names
is less effective than cross entropy for LM adapta-
tion. In addition, cosine similarity does not match
perplexity as well as the CE-based distance meas-
ure. Similarly, for the LDA based approach, using
only NEs may not be sufficient to find appropriate
weights for the topic model. This also explains the
bigger interpolation weight for the general LM in
CL-Cos and LDA-MIX than that in “CL-CE”.
For a fair comparison between the clustering
and LDA based LM adaptation approaches, we
also evaluated using the topic mixture model for
the clustering based approach and using only one
topic in the LDA based method. For clustering
based adaptation, we constructed topic mixture
models using the weights obtained from a linear
normalization of the two distance measures pre-
sented in Section 3.2. In order to use only one topic
model in LDA based adaptation, we chose the
topic cluster that has the largest weight in the
adapted topic mixture model (as in Sec 3.3). Table
4 shows the perplexity for the three approaches
(CL-Cos, CL-CE, and LDA) using the mixture
topic models versus a single topic LM. We observe
similar trends as in Figure 2 when changing the
interpolation weight λ with the generic LM; there-
fore, in Table 4 we only present results for one op-
timal interpolation weight.
</bodyText>
<table confidence="0.99057675">
Single-Topic Mixture-Topic
CL-Cos (λ =0.7) 498.01 497.86
CL-CE (λ =0.5) 469.75 483.09
LDA (λ =0.7) 488.96 489.14
</table>
<tableCaption confidence="0.90486">
Table 4. Perplexity results using the adapted topic
</tableCaption>
<bodyText confidence="0.983306823529412">
model (single vs. mixture) for clustering and LDA
based approaches.
We can see from Table 4 that using the mixture
model in clustering based adaptation does not im-
prove performance. This may be attributed to how
the interpolation weights are calculated. For ex-
ample, only names are used in cosine distance,
and the normalized distance may not be appropri-
ate weights. We also notice negligible difference
when only using one topic in the LDA based
framework. This might be because of the small
number of topics currently used. Intuitively, using
a mixture model should yield better performance,
since LDA itself is based on the assumption of
generating words from multiple topics. We will
investigate the impact of the number of topics on
LM adaptation in Section 4.5.
</bodyText>
<subsectionHeader confidence="0.9925695">
4.4 Effect of Different Feature Configura-
tions on LM Adaptation
</subsectionHeader>
<bodyText confidence="0.994388">
We suspect that using only named entities may not
provide enough information about the ‘topics’ of
the documents, therefore we investigate expanding
the feature vectors with other words. Since gener-
ally content words are more indicative of the topic
of a document than function words, we used a POS
tagger (Hillard et al., 2006) to select words for la-
tent topic analysis. We kept words with three POS
classes: noun (NN, NR, NT), verb (VV), and modi-
</bodyText>
<figure confidence="0.991397133333333">
Perplexity
490
480
470
460
450
440
540
530
520
510
500
0.4 0.5 0.6 0.7 0.8
λ
Baseline CL-CE CL-Cos LDA-MIX
</figure>
<page confidence="0.991779">
677
</page>
<bodyText confidence="0.999921578947369">
fier (JJ), selected from the LDC POS set2. This is
similar to the removal of stop words widely used in
information retrieval.
Figure 3 shows the perplexity results for three
different feature configurations, namely, all-words
(w), names (n), and names plus syntactically fil-
tered items (n+), for the CL-CE and LDA based
approaches. The LDA based LM adaptation para-
digm supports our hypothesis. Using named infor-
mation instead of all the words seems to efficiently
eliminate redundant information and achieve better
performance. In addition, expanding named enti-
ties with syntactically filtered items yields further
improvement. For CL-CE, using named informa-
tion achieves the best result among the three con-
figurations. This might be because that the
clustering method is less powerful in analyzing the
principal components as well as dealing with re-
dundant information than the LDA model.
</bodyText>
<figureCaption confidence="0.8052105">
Figure 3. Comparison of perplexity using different
feature configurations.
</figureCaption>
<subsectionHeader confidence="0.9667985">
4.5 Impact of Predefined Topic Number on
LM Adaptation
</subsectionHeader>
<bodyText confidence="0.999873153846154">
LDA based topic analysis typically uses a large
number of topics to capture the fine grained topic
space. In this section, we evaluate the effect of the
number of topics on LM adaptation. For compari-
son, we evaluate this for both LDA and CL-CE,
similar to Section 4.3. We use the “n+” feature
configuration as in Section 4.4, that is, names plus
POS filtered items. When using a single-topic
adapted model in the LDA or CL-CE based ap-
proach, finer-grained topic analysis (i.e., increasing
the number of topics) leads to worse performance
mainly because of the smaller clusters for each
topic; therefore, we only show results here using
</bodyText>
<footnote confidence="0.620318">
2 See http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf
</footnote>
<bodyText confidence="0.999076619047619">
the mixture topic adapted models. Figure 4 shows
the perplexity results using different numbers of
topics. The interpolation weightλ with the general
LM is 0.5 in all the experiments. For the topic mix-
ture LMs, we used a maximum of 9 mixtures (a
limitation in the current SRILM toolkit) when the
number of topics is greater than 9.
We observe that as the number of topics in-
creases, the perplexity reduces significantly for
LDA. When the number of topics is 50, the
adapted LM using LDA achieves a perplexity re-
duction of 11.35% compared to using 5 topics, and
14.23% against the baseline generic LM. Therefore,
using finer-grained multiple topics in dynamic ad-
aptation improves system performance. When the
number of topics increases further, e.g., to 100, the
performance degrades slightly. This might be due
to the limitation of the number of the topic mix-
tures used. A similar trend is observable for the
CL-CE approach, but the effect of the topic num-
ber is much greater in LDA than CL-CE.
</bodyText>
<figureCaption confidence="0.6301805">
Figure 4. Perplexity results using different prede-
fined numbers of topics for LDA and CL-CE.
</figureCaption>
<subsectionHeader confidence="0.889615">
4.6 Discussion
</subsectionHeader>
<bodyText confidence="0.999933714285714">
As we know, although there is an increasing
amount of training data available for LM training,
it is still only for limited domains and styles. Creat-
ing new training data for different domains is time
consuming and labor intensive, therefore it is very
important to develop algorithms for LM adaptation.
We investigate leveraging named entities in the
LM adaptation task. Though some errors of NER
may be introduced, our experimental results have
shown that exploring named information for topic
analysis is promising for LM adaptation.
Furthermore, this framework may have other
advantages. For speech recognition, using NEs for
topic analysis can be less vulnerable to recognition
</bodyText>
<figure confidence="0.9812965625">
Perplexity
495
490
485
480
475
470
465
460
505
500
0.4 0.5 0.6 0.7 0.8
CL-CE(w) CL-CE(n) CL-CE(n+)
LDA-MIX(w) LDA-MIX(n) LDA-MIX(n+)
λ
Perplexity
480
460
440
420
400
500 485.7
483.1 485.1
n=5 n=10 n=20 n=50 n=100
# of Topics
477.2
LDA CL-CE
445.8
471.8 467.2
430.6
435.2
477.3
</figure>
<page confidence="0.993199">
678
</page>
<bodyText confidence="0.999966142857143">
errors. For instance, we may add a simple module
to compute the similarity between two NEs based
on the word tokens or phonetics, and thus compen-
sate the recognition errors inside NEs. Whereas,
word-based models, such as the traditional cache
LMs, may be more sensitive to recognition errors
that are likely to have a negative impact on the
prediction of the current word. From this point of
view, our framework can potentially be more ro-
bust in the speech processing task. In addition, the
number of NEs in a document is much smaller than
that of the words, as shown in Table 1; hence, us-
ing NEs can also reduce the computational com-
plexity, in particular in topic analysis for training.
</bodyText>
<sectionHeader confidence="0.995923" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999988272727273">
We compared several unsupervised LM adaptation
methods leveraging named entities, and proposed a
new dynamic weighting scheme for topic mixture
model based on LDA topic analysis. Experimental
results have shown that the NE-driven LM adapta-
tion approach outperforms using all the words, and
yields perplexity reduction compared to the base-
line generic LM. In addition, we find that for the
LDA based method, adding other content words,
combined with an increased number of topics, can
further improve the performance, achieving up to
14.23% perplexity reduction compared to the base-
line LM.
The experiments in this paper combine models
primarily through simple linear interpolation. Thus
one direction of our future work is to develop algo-
rithms to automatically learn appropriate interpola-
tion weights. In addition, our work in this paper
has only showed promising results in perplexity
reduction. We will investigate using this frame-
work of LM adaptation for N-best or lattice rescor-
ing in speech recognition.
</bodyText>
<sectionHeader confidence="0.996522" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999960428571429">
We thank Mari Ostendorf, Mei-Yuh Hwang, and
Wen Wang for useful discussions, and Heng Ji for
sharing the Mandarin named entity tagger. This
work is supported by DARPA under Contract No.
HR0011-06-C-0023. Any opinions expressed in
this material are those of the authors and do not
necessarily reflect the views of DARPA.
</bodyText>
<sectionHeader confidence="0.994319" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999914791666667">
J. Bellegarda. 2000. Exploiting Latent Semantic Infor-
mation in Statistical Language Modeling. In IEEE
Transactions on Speech and Audio Processing.
88(80):1279-1296.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
Allocation. Journal of Machine Learning Research.
3:993-1022.
D. Gildea and T. Hofmann. 1999. Topic-Based Lan-
guage Models using EM. In Proc. of Eurospeech.
T. Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum.
2004. Integrating Topics and Syntax. Adv. in Neural
Information Processing Systems. 17:537-544.
D. Hillard, Z. Huang, H. Ji, R. Grishman, D. Hakkani-
Tur, M. Harper, M. Ostendorf, and W. Wang. 2006.
Impact of Automatic Comma Prediction on
POS/Name Tagging of Speech. In Proc. of the First
Workshop on Spoken Language Technology (SLT).
P. Hsu and J. Glass. 2006. Style &amp; Topic Language
Model Adaptation using HMM-LDA. In Proc. of
EMNLP, pp:373-381.
R. Iyer and M. Ostendorf. 1996. Modeling Long Dis-
tance Dependence in Language: Topic Mixtures vs.
Dynamic Cache Models. In Proc. of ICSLP.
H. Ji and R. Grishman. 2005. Improving NameTagging
by Reference Resolution and Relation Detection. In
Proc. of ACL. pp: 411-418.
R. Kneser and V. Steinbiss. 1993. On the Dynamic Ad-
aptation of Stochastic language models. In Proc. of
ICASSP, Vol 2, pp: 586-589.
R. Kuhn and R.D. Mori. 1990. A Cache-Based Natural
Language Model for Speech Recognition. In IEEE
Transactions on Pattern Analysis and Machine Intel-
ligence, 12: 570-583.
D. Mrva and P.C. Woodland. 2006. Unsupervised Lan-
guage Model Adaptation for Mandarin Broadcast
Conversation Transcription. In Proc. of
INTERSPEECH, pp:2206-2209.
R. Rosenfeld. 1996. A Maximum Entropy Approach to
Adaptive Statistical Language Modeling. Computer,
Speech and Language, 10:187-228.
A. Stolcke. 2002. SRILM – An Extensible Language
Modeling Toolkit. In Proc. of ICSLP.
H. Suzuki and J. Gao. 2005. A Comparative Study on
Language Model Adaptation Techniques Using New
Evaluation Metrics, In Proc. of HLT/EMNLP.
Y.C. Tam and T. Schultz. 2005. Dynamic Language
Model Adaptation Using Variational Bayes Inference.
In Proc. of INTERSPEECH, pp:5-8.
</reference>
<page confidence="0.999037">
679
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902483">
<title confidence="0.9996525">Unsupervised Language Model Adaptation Incorporating Named Entity Information</title>
<author confidence="0.998082">Feifan Liu</author>
<author confidence="0.998082">Yang Liu</author>
<affiliation confidence="0.974967">Department of Computer Science The University of Texas at Dallas, Richardson, TX, USA</affiliation>
<email confidence="0.999668">ffliu@hlt.utdallas.edu</email>
<email confidence="0.999668">yangl@hlt.utdallas.edu</email>
<abstract confidence="0.998066703703704">Language model (LM) adaptation is important for both speech and language processing. It is often achieved by combining a generic LM with a topic-specific model that is more relevant to the target document. Unlike previous work on unsupervised LM adaptation, this paper investigates how effectively using named entity (NE) information, instead of considering all the words, helps LM adaptation. We evaluate two latent topic analysis approaches in this paper, namely, clustering and Latent Dirichlet Allocation (LDA). In addition, a new dynamically adapted weighting scheme for topic mixture models is proposed based on LDA topic analysis. Our experimental results show that the NE-driven LM adaptation framework outperforms the baseline generic LM. The best result is obtained using the LDA-based approach by expanding the named entities with syntactically filtered words, together with using a large number of topics, which yields a perplexity reduction of 14.23% compared to the baseline generic LM.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bellegarda</author>
</authors>
<title>Exploiting Latent Semantic Information in Statistical Language Modeling.</title>
<date>2000</date>
<booktitle>In IEEE Transactions on Speech and Audio Processing.</booktitle>
<pages>88--80</pages>
<contexts>
<context position="2501" citStr="Bellegarda, 2000" startWordPosition="386" endWordPosition="387">pic information is typically available and a topic specific LM is interpolated with the generic LM (Kneser and Steinbiss, 1993; Suzuki and Gao, 2005). In contrast, various unsupervised approaches perform latent topic analysis for LM adaptation. To identify implicit topics from the unlabeled corpus, one simple technique is to group the documents into topic clusters by assigning only one topic label to a document (Iyer and Ostendorf, 1996). Recently several other methods in the line of latent semantic analysis have been proposed and used in LM adaptation, such as latent semantic analysis (LSA) (Bellegarda, 2000), probabilistic latent semantic analysis (PLSA) (Gildea and Hofmann, 1999), and LDA (Blei et al., 2003). Most of these existing approaches are based on the “bag of words” model to represent documents, where all the words are treated equally and no relation or association between words is considered. Unlike prior work in LM adaptation, this paper investigates how to effectively leverage named entity information for latent topic analysis. Named entities are very common in domains such as newswire or broadcast news, and carry valuable information, which we hypothesize is topic indicative and usef</context>
<context position="5108" citStr="Bellegarda (2000)" startWordPosition="783" endWordPosition="784"> earlier attempt is a cache-based model (Kuhn and Mori, 1990), developed based on the assumption that words appearing earlier in a document are likely to appear again. The cache concept has also been used to increase the probability of unseen but topically related words, for example, the triggerbased LM adaptation using the maximum entropy approach (Rosenfeld, 1996). Latent topic analysis has recently been investigated extensively for language modeling. Iyer and Ostendorf (1996) used hard clustering to obtain topic clusters for LM adaptation, where a single topic is assigned to each document. Bellegarda (2000) employed Latent Semantic Analysis (LSA) to map documents into implicit topic sub-spaces and demonstrated significant reduction in perplexity and word error rate (WER). Its probabilistic extension, PLSA, is powerful for characterizing topics and documents in a probabilistic space and has been used in LM adaptation. For example, Gildea and Hofmann (1999) reported noticeable perplexity reduction via a dynamic combination of many unigram topic models with a generic trigram model. Proposed by Blei et al. (2003), Latent Dirichlet Allocation (LDA) loosens the constraint of the document-specific fixe</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>J. Bellegarda. 2000. Exploiting Latent Semantic Information in Statistical Language Modeling. In IEEE Transactions on Speech and Audio Processing. 88(80):1279-1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<pages>3--993</pages>
<contexts>
<context position="2604" citStr="Blei et al., 2003" startWordPosition="400" endWordPosition="403">eser and Steinbiss, 1993; Suzuki and Gao, 2005). In contrast, various unsupervised approaches perform latent topic analysis for LM adaptation. To identify implicit topics from the unlabeled corpus, one simple technique is to group the documents into topic clusters by assigning only one topic label to a document (Iyer and Ostendorf, 1996). Recently several other methods in the line of latent semantic analysis have been proposed and used in LM adaptation, such as latent semantic analysis (LSA) (Bellegarda, 2000), probabilistic latent semantic analysis (PLSA) (Gildea and Hofmann, 1999), and LDA (Blei et al., 2003). Most of these existing approaches are based on the “bag of words” model to represent documents, where all the words are treated equally and no relation or association between words is considered. Unlike prior work in LM adaptation, this paper investigates how to effectively leverage named entity information for latent topic analysis. Named entities are very common in domains such as newswire or broadcast news, and carry valuable information, which we hypothesize is topic indicative and useful for latent topic analysis. We compare different latent topic generation approaches as well as model </context>
<context position="5620" citStr="Blei et al. (2003)" startWordPosition="857" endWordPosition="860">ain topic clusters for LM adaptation, where a single topic is assigned to each document. Bellegarda (2000) employed Latent Semantic Analysis (LSA) to map documents into implicit topic sub-spaces and demonstrated significant reduction in perplexity and word error rate (WER). Its probabilistic extension, PLSA, is powerful for characterizing topics and documents in a probabilistic space and has been used in LM adaptation. For example, Gildea and Hofmann (1999) reported noticeable perplexity reduction via a dynamic combination of many unigram topic models with a generic trigram model. Proposed by Blei et al. (2003), Latent Dirichlet Allocation (LDA) loosens the constraint of the document-specific fixed weights by using a prior distribution and has quickly become one of the most popular probabilistic text modeling techniques. LDA can overcome the drawbacks in the PLSA model, and has been shown to outperform PLSA in corpus perplexity and text classification experiments (Blei et al., 2003). Tam and Schultz (2005) successfully applied the LDA model to unsupervised LM adaptation by interpolating the background LM with the dynamic unigram LM estimated by the LDA model. Hsu and Glass (2006) investigated using </context>
<context position="11142" citStr="Blei et al., 2003" startWordPosition="1805" endWordPosition="1808">d by the cosine similari kth ty function as in Equation (1). Our goal here is to find the cluster S* which the test document is closest to, that is, Generic N-gram Training Training Text Test Text Topic Model Training Latent Topic Analysis NER NER Topic Matching Topic Model Adaptation Model Interpolation Compute Perplexity K (S1S2LSk)* =argmax∑ ∑sim(v,u) i = 1 v u S , ∈ i sim (v, u) ik ni r S =arg max 1≤ ≤ i K ||d ||⋅ ||cvi || * d ⋅ cvi r 674 where d is the feature vector for the test document. model using the mixture of LMs from different topics: 3.3 NE-based LDA for LM Adaptation LDA model (Blei et al., 2003) has been introduced as a new, semantically consistent generative model, which overcomes overfitting and the problem of generating new documents in PLSA. It is a threelevel hierarchical Bayesian model. Based on the LDA model, a document d is generated as follows. • Sample a vector of K topic mixture weights θ from a prior Dirichlet distribution with parameter α : K f ( ; ) θ α = ∏θαk− k k=1 • For each word w in d, pick a topic k from the multinomial distribution θ . • Pick a word w from the multinomial distribution βw,k given the kth topic. For a document d=w1,w2,...wn, the LDA model assigns i</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research. 3:993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>T Hofmann</author>
</authors>
<title>Topic-Based Language Models using EM.</title>
<date>1999</date>
<booktitle>In Proc. of Eurospeech.</booktitle>
<contexts>
<context position="2575" citStr="Gildea and Hofmann, 1999" startWordPosition="393" endWordPosition="397">interpolated with the generic LM (Kneser and Steinbiss, 1993; Suzuki and Gao, 2005). In contrast, various unsupervised approaches perform latent topic analysis for LM adaptation. To identify implicit topics from the unlabeled corpus, one simple technique is to group the documents into topic clusters by assigning only one topic label to a document (Iyer and Ostendorf, 1996). Recently several other methods in the line of latent semantic analysis have been proposed and used in LM adaptation, such as latent semantic analysis (LSA) (Bellegarda, 2000), probabilistic latent semantic analysis (PLSA) (Gildea and Hofmann, 1999), and LDA (Blei et al., 2003). Most of these existing approaches are based on the “bag of words” model to represent documents, where all the words are treated equally and no relation or association between words is considered. Unlike prior work in LM adaptation, this paper investigates how to effectively leverage named entity information for latent topic analysis. Named entities are very common in domains such as newswire or broadcast news, and carry valuable information, which we hypothesize is topic indicative and useful for latent topic analysis. We compare different latent topic generation</context>
<context position="5463" citStr="Gildea and Hofmann (1999)" startWordPosition="833" endWordPosition="836">ach (Rosenfeld, 1996). Latent topic analysis has recently been investigated extensively for language modeling. Iyer and Ostendorf (1996) used hard clustering to obtain topic clusters for LM adaptation, where a single topic is assigned to each document. Bellegarda (2000) employed Latent Semantic Analysis (LSA) to map documents into implicit topic sub-spaces and demonstrated significant reduction in perplexity and word error rate (WER). Its probabilistic extension, PLSA, is powerful for characterizing topics and documents in a probabilistic space and has been used in LM adaptation. For example, Gildea and Hofmann (1999) reported noticeable perplexity reduction via a dynamic combination of many unigram topic models with a generic trigram model. Proposed by Blei et al. (2003), Latent Dirichlet Allocation (LDA) loosens the constraint of the document-specific fixed weights by using a prior distribution and has quickly become one of the most popular probabilistic text modeling techniques. LDA can overcome the drawbacks in the PLSA model, and has been shown to outperform PLSA in corpus perplexity and text classification experiments (Blei et al., 2003). Tam and Schultz (2005) successfully applied the LDA model to u</context>
</contexts>
<marker>Gildea, Hofmann, 1999</marker>
<rawString>D. Gildea and T. Hofmann. 1999. Topic-Based Language Models using EM. In Proc. of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Griffiths</author>
<author>M Steyvers</author>
<author>D Blei</author>
<author>J Tenenbaum</author>
</authors>
<date>2004</date>
<booktitle>Integrating Topics and Syntax. Adv. in Neural Information Processing Systems.</booktitle>
<pages>17--537</pages>
<contexts>
<context position="11869" citStr="Griffiths et al., 2004" startWordPosition="1950" endWordPosition="1954"> the problem of generating new documents in PLSA. It is a threelevel hierarchical Bayesian model. Based on the LDA model, a document d is generated as follows. • Sample a vector of K topic mixture weights θ from a prior Dirichlet distribution with parameter α : K f ( ; ) θ α = ∏θαk− k k=1 • For each word w in d, pick a topic k from the multinomial distribution θ . • Pick a word w from the multinomial distribution βw,k given the kth topic. For a document d=w1,w2,...wn, the LDA model assigns it the following probability: p d ( ) = ⎛n K ∫ ∏∑ θ ⎝ i = = ⎜ 1 1 k We use the MATLAB topic Toolbox 1.3 (Griffiths et al., 2004) in the training set to obtain the document-topic matrix, DP, and the word-topic matrix, WP. Note that here “words” correspond to the elements in the feature vector used to represent the document (e.g., NEs). In the DP matrix, an entry cik represents the counts of words in a document di that are from a topic zk (k=1,2,...,K). In the WP matrix, an entry fjk represents the frequency of a word wj generated from a topic zk (k=1,2,...,K) over the training set. For training, we assign a topic zi* to a document * di such that zi = argmaxcik . Based on the docu1≤k≤K ments belonging to the different to</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2004</marker>
<rawString>T. Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum. 2004. Integrating Topics and Syntax. Adv. in Neural Information Processing Systems. 17:537-544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hillard</author>
<author>Z Huang</author>
<author>H Ji</author>
<author>R Grishman</author>
<author>D HakkaniTur</author>
<author>M Harper</author>
<author>M Ostendorf</author>
<author>W Wang</author>
</authors>
<title>Impact of Automatic Comma Prediction on POS/Name Tagging of Speech.</title>
<date>2006</date>
<booktitle>In Proc. of the First Workshop on Spoken Language Technology (SLT).</booktitle>
<contexts>
<context position="21912" citStr="Hillard et al., 2006" startWordPosition="3673" endWordPosition="3676">using a mixture model should yield better performance, since LDA itself is based on the assumption of generating words from multiple topics. We will investigate the impact of the number of topics on LM adaptation in Section 4.5. 4.4 Effect of Different Feature Configurations on LM Adaptation We suspect that using only named entities may not provide enough information about the ‘topics’ of the documents, therefore we investigate expanding the feature vectors with other words. Since generally content words are more indicative of the topic of a document than function words, we used a POS tagger (Hillard et al., 2006) to select words for latent topic analysis. We kept words with three POS classes: noun (NN, NR, NT), verb (VV), and modiPerplexity 490 480 470 460 450 440 540 530 520 510 500 0.4 0.5 0.6 0.7 0.8 λ Baseline CL-CE CL-Cos LDA-MIX 677 fier (JJ), selected from the LDC POS set2. This is similar to the removal of stop words widely used in information retrieval. Figure 3 shows the perplexity results for three different feature configurations, namely, all-words (w), names (n), and names plus syntactically filtered items (n+), for the CL-CE and LDA based approaches. The LDA based LM adaptation paradigm </context>
</contexts>
<marker>Hillard, Huang, Ji, Grishman, HakkaniTur, Harper, Ostendorf, Wang, 2006</marker>
<rawString>D. Hillard, Z. Huang, H. Ji, R. Grishman, D. HakkaniTur, M. Harper, M. Ostendorf, and W. Wang. 2006. Impact of Automatic Comma Prediction on POS/Name Tagging of Speech. In Proc. of the First Workshop on Spoken Language Technology (SLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hsu</author>
<author>J Glass</author>
</authors>
<title>Style &amp; Topic Language Model Adaptation using HMM-LDA.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>373--381</pages>
<contexts>
<context position="6200" citStr="Hsu and Glass (2006)" startWordPosition="948" endWordPosition="951">am model. Proposed by Blei et al. (2003), Latent Dirichlet Allocation (LDA) loosens the constraint of the document-specific fixed weights by using a prior distribution and has quickly become one of the most popular probabilistic text modeling techniques. LDA can overcome the drawbacks in the PLSA model, and has been shown to outperform PLSA in corpus perplexity and text classification experiments (Blei et al., 2003). Tam and Schultz (2005) successfully applied the LDA model to unsupervised LM adaptation by interpolating the background LM with the dynamic unigram LM estimated by the LDA model. Hsu and Glass (2006) investigated using hidden Markov model with LDA to allow for both topic and style adaptation. Mrva and Woodland (2006) achieved WER reduction on broadcast conversation recognition using an LDA based adaptation approach that effectively combined the LMs trained from corpora with different styles: broadcast news and broadcast conversation data. In this paper, we investigate unsupervised LM adaptation using clustering and LDA based topic analysis. Unlike the clustering based interpolation method as in (Iyer and Ostendorf, 1996), we explore different distance measure methods for topic analysis. D</context>
</contexts>
<marker>Hsu, Glass, 2006</marker>
<rawString>P. Hsu and J. Glass. 2006. Style &amp; Topic Language Model Adaptation using HMM-LDA. In Proc. of EMNLP, pp:373-381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iyer</author>
<author>M Ostendorf</author>
</authors>
<title>Modeling Long Distance Dependence in Language: Topic Mixtures vs. Dynamic Cache Models.</title>
<date>1996</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="2325" citStr="Iyer and Ostendorf, 1996" startWordPosition="357" endWordPosition="360">ore, is very important in order to better deal with a variety of topics and styles. Many studies have been conducted for LM adaptation. One method is supervised LM adaptation, where topic information is typically available and a topic specific LM is interpolated with the generic LM (Kneser and Steinbiss, 1993; Suzuki and Gao, 2005). In contrast, various unsupervised approaches perform latent topic analysis for LM adaptation. To identify implicit topics from the unlabeled corpus, one simple technique is to group the documents into topic clusters by assigning only one topic label to a document (Iyer and Ostendorf, 1996). Recently several other methods in the line of latent semantic analysis have been proposed and used in LM adaptation, such as latent semantic analysis (LSA) (Bellegarda, 2000), probabilistic latent semantic analysis (PLSA) (Gildea and Hofmann, 1999), and LDA (Blei et al., 2003). Most of these existing approaches are based on the “bag of words” model to represent documents, where all the words are treated equally and no relation or association between words is considered. Unlike prior work in LM adaptation, this paper investigates how to effectively leverage named entity information for latent</context>
<context position="4974" citStr="Iyer and Ostendorf (1996)" startWordPosition="760" endWordPosition="763">nd showed that three discriminative methods significantly outperform the maximum a posteriori (MAP) method. For unsupervised LM adaptation, an earlier attempt is a cache-based model (Kuhn and Mori, 1990), developed based on the assumption that words appearing earlier in a document are likely to appear again. The cache concept has also been used to increase the probability of unseen but topically related words, for example, the triggerbased LM adaptation using the maximum entropy approach (Rosenfeld, 1996). Latent topic analysis has recently been investigated extensively for language modeling. Iyer and Ostendorf (1996) used hard clustering to obtain topic clusters for LM adaptation, where a single topic is assigned to each document. Bellegarda (2000) employed Latent Semantic Analysis (LSA) to map documents into implicit topic sub-spaces and demonstrated significant reduction in perplexity and word error rate (WER). Its probabilistic extension, PLSA, is powerful for characterizing topics and documents in a probabilistic space and has been used in LM adaptation. For example, Gildea and Hofmann (1999) reported noticeable perplexity reduction via a dynamic combination of many unigram topic models with a generic</context>
<context position="6731" citStr="Iyer and Ostendorf, 1996" startWordPosition="1027" endWordPosition="1030">the background LM with the dynamic unigram LM estimated by the LDA model. Hsu and Glass (2006) investigated using hidden Markov model with LDA to allow for both topic and style adaptation. Mrva and Woodland (2006) achieved WER reduction on broadcast conversation recognition using an LDA based adaptation approach that effectively combined the LMs trained from corpora with different styles: broadcast news and broadcast conversation data. In this paper, we investigate unsupervised LM adaptation using clustering and LDA based topic analysis. Unlike the clustering based interpolation method as in (Iyer and Ostendorf, 1996), we explore different distance measure methods for topic analysis. Different from the LDA based framework as in (Tam and Schultz, 2005), we propose a novel dynamic weighting scheme for the topic adapted LM. More importantly, the focus of our work is to investigate the role of named entity information in LM adaptation, which to our knowledge has not been explored. 3 Unsupervised LM Adaptation Integrating Named Entities (NEs) 3.1 Overview of the NE-driven LM Adaptation Framework Figure 1 shows our unsupervised LM adaptation framework using NEs. For training, we use the text collection to train </context>
</contexts>
<marker>Iyer, Ostendorf, 1996</marker>
<rawString>R. Iyer and M. Ostendorf. 1996. Modeling Long Distance Dependence in Language: Topic Mixtures vs. Dynamic Cache Models. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
</authors>
<title>Improving NameTagging by Reference Resolution and Relation Detection.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<pages>411--418</pages>
<contexts>
<context position="14335" citStr="Ji and Grishman, 2005" startWordPosition="2413" endWordPosition="2416">tal data. The data set we used is the LDC Mandarin TDT4 corpus, consisting of 337 broadcast news shows with transcriptions. These files were split into small pieces, which we call documents here, according to the topic segmentation information marked in the LDC’s transcription. In total, there are 26,646 such documents in our data set. We randomly chose 2661 files as the test data (which is balanced for different news sources). The rest was used for topic analysis and also generic LM training. Punctuation marks were used to determine sentences in the transcriptions. We used the NYU NE tagger (Ji and Grishman, 2005) to recognize four kinds of NEs: Person, Location, Organi) n γk = p(zk |wj)p(wj |d ) ∑= j 1 , p(wj |d) = 1 p ( zk |) = fjk w j K (wj) freq βwik ⋅ θk ⎞ ⎟ f θ α dθ + (1 ( ; ) ⎠ 675 zation, and Geo-political. Table 1 shows the statistics of the data set in our experiments. We trained trigram LMs using the SRILM toolkit (Stolcke, 2002). A fixed weight (i.e., λ in Equation (2) and (3)) was used for the entire test set when interpolating the generic LM with the adapted topic LM. Perplexity was used to measure the performance of different adapted LMs in our experiments. 4.2 Latent Topic Analysis Resu</context>
</contexts>
<marker>Ji, Grishman, 2005</marker>
<rawString>H. Ji and R. Grishman. 2005. Improving NameTagging by Reference Resolution and Relation Detection. In Proc. of ACL. pp: 411-418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>V Steinbiss</author>
</authors>
<title>On the Dynamic Adaptation of Stochastic language models.</title>
<date>1993</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<volume>2</volume>
<pages>586--589</pages>
<contexts>
<context position="2010" citStr="Kneser and Steinbiss, 1993" startWordPosition="307" endWordPosition="310">nslation and information retrieval. Statistical N-gram LMs have been widely used; however, they capture only local contextual information. In addition, even with the increasing amount of LM training data, there is often a mismatch problem because of differences in domain, topics, or styles. Adaptation of LM, therefore, is very important in order to better deal with a variety of topics and styles. Many studies have been conducted for LM adaptation. One method is supervised LM adaptation, where topic information is typically available and a topic specific LM is interpolated with the generic LM (Kneser and Steinbiss, 1993; Suzuki and Gao, 2005). In contrast, various unsupervised approaches perform latent topic analysis for LM adaptation. To identify implicit topics from the unlabeled corpus, one simple technique is to group the documents into topic clusters by assigning only one topic label to a document (Iyer and Ostendorf, 1996). Recently several other methods in the line of latent semantic analysis have been proposed and used in LM adaptation, such as latent semantic analysis (LSA) (Bellegarda, 2000), probabilistic latent semantic analysis (PLSA) (Gildea and Hofmann, 1999), and LDA (Blei et al., 2003). Most</context>
</contexts>
<marker>Kneser, Steinbiss, 1993</marker>
<rawString>R. Kneser and V. Steinbiss. 1993. On the Dynamic Adaptation of Stochastic language models. In Proc. of ICASSP, Vol 2, pp: 586-589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>R D Mori</author>
</authors>
<title>A Cache-Based Natural Language Model for Speech Recognition. In</title>
<date>1990</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>12</volume>
<pages>570--583</pages>
<contexts>
<context position="4552" citStr="Kuhn and Mori, 1990" startWordPosition="695" endWordPosition="698">d as follows. In Section 2 we review some related work. Section 3 describes in detail our unsupervised LM adaptation approach using named entities. Experimental results are presented and discussed in Section 4. Conclusion and future work appear in Section 5. 2 Related Work There has been a lot of previous related work on LM adaptation. Suzuki and Gao (2005) compared different supervised LM adaptation approaches, and showed that three discriminative methods significantly outperform the maximum a posteriori (MAP) method. For unsupervised LM adaptation, an earlier attempt is a cache-based model (Kuhn and Mori, 1990), developed based on the assumption that words appearing earlier in a document are likely to appear again. The cache concept has also been used to increase the probability of unseen but topically related words, for example, the triggerbased LM adaptation using the maximum entropy approach (Rosenfeld, 1996). Latent topic analysis has recently been investigated extensively for language modeling. Iyer and Ostendorf (1996) used hard clustering to obtain topic clusters for LM adaptation, where a single topic is assigned to each document. Bellegarda (2000) employed Latent Semantic Analysis (LSA) to </context>
</contexts>
<marker>Kuhn, Mori, 1990</marker>
<rawString>R. Kuhn and R.D. Mori. 1990. A Cache-Based Natural Language Model for Speech Recognition. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 12: 570-583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mrva</author>
<author>P C Woodland</author>
</authors>
<title>Unsupervised Language Model Adaptation for Mandarin Broadcast Conversation Transcription.</title>
<date>2006</date>
<booktitle>In Proc. of INTERSPEECH,</booktitle>
<pages>2206--2209</pages>
<contexts>
<context position="6319" citStr="Mrva and Woodland (2006)" startWordPosition="967" endWordPosition="970">specific fixed weights by using a prior distribution and has quickly become one of the most popular probabilistic text modeling techniques. LDA can overcome the drawbacks in the PLSA model, and has been shown to outperform PLSA in corpus perplexity and text classification experiments (Blei et al., 2003). Tam and Schultz (2005) successfully applied the LDA model to unsupervised LM adaptation by interpolating the background LM with the dynamic unigram LM estimated by the LDA model. Hsu and Glass (2006) investigated using hidden Markov model with LDA to allow for both topic and style adaptation. Mrva and Woodland (2006) achieved WER reduction on broadcast conversation recognition using an LDA based adaptation approach that effectively combined the LMs trained from corpora with different styles: broadcast news and broadcast conversation data. In this paper, we investigate unsupervised LM adaptation using clustering and LDA based topic analysis. Unlike the clustering based interpolation method as in (Iyer and Ostendorf, 1996), we explore different distance measure methods for topic analysis. Different from the LDA based framework as in (Tam and Schultz, 2005), we propose a novel dynamic weighting scheme for th</context>
</contexts>
<marker>Mrva, Woodland, 2006</marker>
<rawString>D. Mrva and P.C. Woodland. 2006. Unsupervised Language Model Adaptation for Mandarin Broadcast Conversation Transcription. In Proc. of INTERSPEECH, pp:2206-2209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A Maximum Entropy Approach to Adaptive Statistical Language Modeling.</title>
<date>1996</date>
<journal>Computer, Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="4859" citStr="Rosenfeld, 1996" startWordPosition="746" endWordPosition="747">ted work on LM adaptation. Suzuki and Gao (2005) compared different supervised LM adaptation approaches, and showed that three discriminative methods significantly outperform the maximum a posteriori (MAP) method. For unsupervised LM adaptation, an earlier attempt is a cache-based model (Kuhn and Mori, 1990), developed based on the assumption that words appearing earlier in a document are likely to appear again. The cache concept has also been used to increase the probability of unseen but topically related words, for example, the triggerbased LM adaptation using the maximum entropy approach (Rosenfeld, 1996). Latent topic analysis has recently been investigated extensively for language modeling. Iyer and Ostendorf (1996) used hard clustering to obtain topic clusters for LM adaptation, where a single topic is assigned to each document. Bellegarda (2000) employed Latent Semantic Analysis (LSA) to map documents into implicit topic sub-spaces and demonstrated significant reduction in perplexity and word error rate (WER). Its probabilistic extension, PLSA, is powerful for characterizing topics and documents in a probabilistic space and has been used in LM adaptation. For example, Gildea and Hofmann (1</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>R. Rosenfeld. 1996. A Maximum Entropy Approach to Adaptive Statistical Language Modeling. Computer, Speech and Language, 10:187-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="14668" citStr="Stolcke, 2002" startWordPosition="2495" endWordPosition="2496">randomly chose 2661 files as the test data (which is balanced for different news sources). The rest was used for topic analysis and also generic LM training. Punctuation marks were used to determine sentences in the transcriptions. We used the NYU NE tagger (Ji and Grishman, 2005) to recognize four kinds of NEs: Person, Location, Organi) n γk = p(zk |wj)p(wj |d ) ∑= j 1 , p(wj |d) = 1 p ( zk |) = fjk w j K (wj) freq βwik ⋅ θk ⎞ ⎟ f θ α dθ + (1 ( ; ) ⎠ 675 zation, and Geo-political. Table 1 shows the statistics of the data set in our experiments. We trained trigram LMs using the SRILM toolkit (Stolcke, 2002). A fixed weight (i.e., λ in Equation (2) and (3)) was used for the entire test set when interpolating the generic LM with the adapted topic LM. Perplexity was used to measure the performance of different adapted LMs in our experiments. 4.2 Latent Topic Analysis Results Topic # of Top 10 Descriptive Items Files (Translated from Chinese) Cluster- 1 3526 U.S., Israel, Washington, Palestine, ing Bush, Clinton, Gore, Voice of AmerBased ica, Mid-East, Republican Party 2 3067 Taiwan, Taipei, Mainland, Taipei City, Chinese People’s Broadcasting Station, Shuibian Chen, the Executive Yuan, the Legislat</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Suzuki</author>
<author>J Gao</author>
</authors>
<title>A Comparative Study on Language Model Adaptation Techniques Using New Evaluation Metrics,</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP.</booktitle>
<contexts>
<context position="2033" citStr="Suzuki and Gao, 2005" startWordPosition="311" endWordPosition="314">rieval. Statistical N-gram LMs have been widely used; however, they capture only local contextual information. In addition, even with the increasing amount of LM training data, there is often a mismatch problem because of differences in domain, topics, or styles. Adaptation of LM, therefore, is very important in order to better deal with a variety of topics and styles. Many studies have been conducted for LM adaptation. One method is supervised LM adaptation, where topic information is typically available and a topic specific LM is interpolated with the generic LM (Kneser and Steinbiss, 1993; Suzuki and Gao, 2005). In contrast, various unsupervised approaches perform latent topic analysis for LM adaptation. To identify implicit topics from the unlabeled corpus, one simple technique is to group the documents into topic clusters by assigning only one topic label to a document (Iyer and Ostendorf, 1996). Recently several other methods in the line of latent semantic analysis have been proposed and used in LM adaptation, such as latent semantic analysis (LSA) (Bellegarda, 2000), probabilistic latent semantic analysis (PLSA) (Gildea and Hofmann, 1999), and LDA (Blei et al., 2003). Most of these existing appr</context>
<context position="4291" citStr="Suzuki and Gao (2005)" startWordPosition="659" endWordPosition="662">f incorporating named information in LM adaptation is effective. In addition, we find that for the LDA based adaptation scheme, adding more content words and increasing the number of topics can further improve the performance significantly. The paper is organized as follows. In Section 2 we review some related work. Section 3 describes in detail our unsupervised LM adaptation approach using named entities. Experimental results are presented and discussed in Section 4. Conclusion and future work appear in Section 5. 2 Related Work There has been a lot of previous related work on LM adaptation. Suzuki and Gao (2005) compared different supervised LM adaptation approaches, and showed that three discriminative methods significantly outperform the maximum a posteriori (MAP) method. For unsupervised LM adaptation, an earlier attempt is a cache-based model (Kuhn and Mori, 1990), developed based on the assumption that words appearing earlier in a document are likely to appear again. The cache concept has also been used to increase the probability of unseen but topically related words, for example, the triggerbased LM adaptation using the maximum entropy approach (Rosenfeld, 1996). Latent topic analysis has rece</context>
</contexts>
<marker>Suzuki, Gao, 2005</marker>
<rawString>H. Suzuki and J. Gao. 2005. A Comparative Study on Language Model Adaptation Techniques Using New Evaluation Metrics, In Proc. of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Tam</author>
<author>T Schultz</author>
</authors>
<title>Dynamic Language Model Adaptation Using Variational Bayes Inference.</title>
<date>2005</date>
<booktitle>In Proc. of INTERSPEECH,</booktitle>
<pages>5--8</pages>
<contexts>
<context position="6023" citStr="Tam and Schultz (2005)" startWordPosition="919" endWordPosition="922"> used in LM adaptation. For example, Gildea and Hofmann (1999) reported noticeable perplexity reduction via a dynamic combination of many unigram topic models with a generic trigram model. Proposed by Blei et al. (2003), Latent Dirichlet Allocation (LDA) loosens the constraint of the document-specific fixed weights by using a prior distribution and has quickly become one of the most popular probabilistic text modeling techniques. LDA can overcome the drawbacks in the PLSA model, and has been shown to outperform PLSA in corpus perplexity and text classification experiments (Blei et al., 2003). Tam and Schultz (2005) successfully applied the LDA model to unsupervised LM adaptation by interpolating the background LM with the dynamic unigram LM estimated by the LDA model. Hsu and Glass (2006) investigated using hidden Markov model with LDA to allow for both topic and style adaptation. Mrva and Woodland (2006) achieved WER reduction on broadcast conversation recognition using an LDA based adaptation approach that effectively combined the LMs trained from corpora with different styles: broadcast news and broadcast conversation data. In this paper, we investigate unsupervised LM adaptation using clustering and</context>
<context position="13094" citStr="Tam and Schultz, 2005" startWordPosition="2192" endWordPosition="2195">s, K topic Ngram LMs are trained. This “hard clustering” strategy allows us to train an LM that accounts for all the words rather than simply those NEs used in LDA analysis, as well as use higher order N-gram LMs, unlike the ‘unigram’ based LDA in previous work. For a test document d = w1,w2,...,wn that is generated by multiple topics under the LDA assumption, we formulate a dynamically adapted topic K p ( |) γ ( | w h = × p w h LDA adapt k k − i z k k i ∑= i 1 where pzi (wk |hk) stands for the ith topic LM, and γi is the mixture weight. Different from the idea of dynamic topic adaptation in (Tam and Schultz, 2005), we propose a new weighting scheme to calculate γi that directly uses the two resulting matrices from LDA analysis during training: n ∑ f ∑ freq ( ) jp wq p 1 q=1 where freq(wj) is the frequency of a word wj in the document d. Other notations are consistent with the previous definitions. Then we interpolate this adapted topic model with the generic LM, similar to Equation (2): p(wk |hk ) = λ pGeneral (wk |hk ) − λ )pLDA−adapt (wk |hk ) (3) 4 Experiments 4.1 Experimental Setup # of files # of words # of NEs Training Data 23,985 7,345,644 590,656 Test Data 2,661 831,283 65,867 Table 1. Statisti</context>
</contexts>
<marker>Tam, Schultz, 2005</marker>
<rawString>Y.C. Tam and T. Schultz. 2005. Dynamic Language Model Adaptation Using Variational Bayes Inference. In Proc. of INTERSPEECH, pp:5-8.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>