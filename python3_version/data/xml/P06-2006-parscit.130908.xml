<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022540">
<title confidence="0.9995765">
Evaluating the Accuracy of an Unlexicalized
Statistical Parser on the PARC DepBank
</title>
<author confidence="0.996145">
Ted Briscoe
</author>
<affiliation confidence="0.993816">
Computer Laboratory
University of Cambridge
</affiliation>
<sectionHeader confidence="0.978756" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999870117647059">
We evaluate the accuracy of an unlexi-
calized statistical parser, trained on 4K
treebanked sentences from balanced data
and tested on the PARC DepBank. We
demonstrate that a parser which is compet-
itive in accuracy (without sacrificing pro-
cessing speed) can be quickly tuned with-
out reliance on large in-domain manually-
constructed treebanks. This makes it more
practical to use statistical parsers in ap-
plications that need access to aspects of
predicate-argument structure. The com-
parison of systems using DepBank is not
straightforward, so we extend and validate
DepBank and highlight a number of repre-
sentation and scoring issues for relational
evaluation schemes.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999714761904762">
Considerable progress has been made in accu-
rate statistical parsing of realistic texts, yield-
ing rooted, hierarchical and/or relational repre-
sentations of full sentences. However, much
of this progress has been made with systems
based on large lexicalized probabilistic context-
free like (PCFG-like) models trained on the Wall
Street Journal (WSJ) subset of the Penn Tree-
Bank (PTB). Evaluation of these systems has been
mostly in terms of the PARSEVAL scheme using
tree similarity measures of (labelled) precision and
recall and crossing bracket rate applied to section
23 of the WSJ PTB. (See e.g. Collins (1999) for
detailed exposition of one such very fruitful line
of research.)
We evaluate the comparative accuracy of an un-
lexicalized statistical parser trained on a smaller
treebank and tested on a subset of section 23 of
the WSJ using a relational evaluation scheme. We
demonstrate that a parser which is competitive
in accuracy (without sacrificing processing speed)
</bodyText>
<author confidence="0.799708">
John Carroll
</author>
<affiliation confidence="0.943645">
School of Informatics
University of Sussex
</affiliation>
<bodyText confidence="0.999837947368421">
can be quickly developed without reliance on large
in-domain manually-constructed treebanks. This
makes it more practical to use statistical parsers in
diverse applications needing access to aspects of
predicate-argument structure.
We define a lexicalized statistical parser as one
which utilizes probabilistic parameters concerning
lexical subcategorization and/or bilexical relations
over tree configurations. Current lexicalized sta-
tistical parsers developed, trained and tested on
PTB achieve a labelled Fl-score – the harmonic
mean of labelled precision and recall – of around
90%. Klein and Manning (2003) argue that such
results represent about 4% absolute improvement
over a carefully constructed unlexicalized PCFG-
like model trained and tested in the same man-
ner.1 Gildea (2001) shows that WSJ-derived bilex-
ical parameters in Collins’ (1999) Model 1 parser
contribute less than 1% to parse selection accu-
racy when test data is in the same domain, and
yield no improvement for test data selected from
the Brown Corpus. Bikel (2004) shows that, in
Collins’ (1999) Model 2, bilexical parameters con-
tribute less than 0.5% to accuracy on in-domain
data while lexical subcategorization-like parame-
ters contribute just over 1%.
Several alternative relational evaluation
schemes have been developed (e.g. Carroll et al.,
1998; Lin, 1998). However, until recently, no
WSJ data has been carefully annotated to support
relational evaluation. King et al. (2003) describe
the PARC 700 Dependency Bank (hereinafter
DepBank), which consists of 700 WSJ sentences
randomly drawn from section 23. These sentences
have been annotated with syntactic features and
with bilexical head-dependent relations derived
from the F-structure representation of Lexical
Functional Grammar (LFG). DepBank facilitates
</bodyText>
<footnote confidence="0.989413">
1Klein and Manning retained some functional tag infor-
mation from PTB, so it could be argued that their model re-
mains ‘mildly’ lexicalized since functional tags encode some
subcategorization information.
</footnote>
<page confidence="0.983388">
41
</page>
<note confidence="0.726682">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 41–48,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999846894736842">
comparison of PCFG-like statistical parsers
developed from the PTB with other parsers whose
output is not designed to yield PTB-style trees,
using an evaluation which is closer to the protypi-
cal parsing task of recovering predicate-argument
structure.
Kaplan et al. (2004) compare the accuracy and
speed of the PARC XLE Parser to Collins’ Model
3 parser. They develop transformation rules for
both, designed to map native output to a subset of
the features and relations in DepBank. They com-
pare performance of a grammatically cut-down
and complete version of the XLE parser to the
publically available version of Collins’ parser.
One fifth of DepBank is held out to optimize the
speed and accuracy of the three systems. They
conclude from the results of these experiments that
the cut-down XLE parser is two-thirds the speed
of Collins’ Model 3 but 12% more accurate, while
the complete XLE system is 20% more accurate
but five times slower. F1-score percentages range
from the mid- to high-70s, suggesting that the re-
lational evaluation is harder than PARSEVAL.
Both Collins’ Model 3 and the XLE Parser use
lexicalized models for parse selection trained on
the rest of the WSJ PTB. Therefore, although Ka-
plan et al. demonstrate an improvement in accu-
racy at some cost to speed, there remain questions
concerning viability for applications, at some re-
move from the financial news domain, for which
substantial treebanks are not available. The parser
we deploy, like the XLE one, is based on a
manually-defined feature-based unification gram-
mar. However, the approach is somewhat differ-
ent, making maximal use of more generic struc-
tural rather than lexical information, both within
the grammar and the probabilistic parse selection
model. Here we compare the accuracy of our
parser with Kaplan et al.’s results, by repeating
their experiment with our parser. This compari-
son is not straightforward, given both the system-
specific nature of some of the annotation in Dep-
Bank and the scoring reported. We, therefore, ex-
tend DepBank with a set of grammatical relations
derived from our own system output and highlight
how issues of representation and scoring can affect
results and their interpretation.
In §2, we describe our development method-
ology and the resulting system in greater detail.
§3 describes the extended Depbank that we have
developed and motivates our additions. §2.4 dis-
cusses how we trained and tuned our current sys-
tem and describes our limited use of information
derived from WSJ text. §4 details the various ex-
periments undertaken with the extended DepBank
and gives detailed results. §5 discusses these re-
sults and proposes further lines of research.
</bodyText>
<sectionHeader confidence="0.827761" genericHeader="method">
2 Unlexicalized Statistical Parsing
</sectionHeader>
<subsectionHeader confidence="0.995041">
2.1 System Architecture
</subsectionHeader>
<bodyText confidence="0.999958606060606">
Both the XLE system and Collins’ Model 3 pre-
process textual input before parsing. Similarly,
our baseline system consists of a pipeline of mod-
ules. First, text is tokenized using a deterministic
finite-state transducer. Second, tokens are part-of-
speech and punctuation (PoS) tagged using a 1st-
order Hidden Markov Model (HMM) utilizing a
lexicon of just over 50K words and an unknown
word handling module. Third, deterministic mor-
phological analysis is performed on each token-
tag pair with a finite-state transducer. Fourth, the
lattice of lemma-affix-tags is parsed using a gram-
mar over such tags. Finally, the n-best parses are
computed from the parse forest using a probabilis-
tic parse selection model conditioned on the struc-
tural parse context. The output of the parser can be
displayed as syntactic trees, and/or factored into a
sequence of bilexical grammatical relations (GRs)
between lexical heads and their dependents.
The full system can be extended in a variety of
ways – for example, by pruning PoS tags but al-
lowing multiple tag possibilities per word as in-
put to the parser, by incorporating lexical subcate-
gorization into parse selection, by computing GR
weights based on the proportion and probability
of the n-best analyses yielding them, and so forth
– broadly trading accuracy and greater domain-
dependence against speed and reduced sensitivity
to domain-specific lexical behaviour (Briscoe and
Carroll, 2002; Carroll and Briscoe, 2002; Watson
et al., 2005; Watson, 2006). However, in this pa-
per we focus exclusively on the baseline unlexical-
ized system.
</bodyText>
<subsectionHeader confidence="0.994981">
2.2 Grammar Development
</subsectionHeader>
<bodyText confidence="0.999992285714286">
The grammar is expressed in a feature-based, uni-
fication formalism. There are currently 676 phrase
structure rule schemata, 15 feature propagation
rules, 30 default feature value rules, 22 category
expansion rules and 41 feature types which to-
gether define 1124 compiled phrase structure rules
in which categories are represented as sets of fea-
</bodyText>
<page confidence="0.998318">
42
</page>
<bodyText confidence="0.995879472222222">
tures, that is, attribute-value pairs, possibly with
variable values, possibly bound between mother
and one or more daughter categories. 142 of the
phrase structure schemata are manually identified
as peripheral rather than core rules of English
grammar. Categories are matched using fixed-
arity term unification at parse time.
The lexical categories of the grammar consist
of feature-based descriptions of the 149 PoS tags
and 13 punctuation tags (a subset of the CLAWS
tagset, see e.g. Sampson, 1995) which constitute
the preterminals of the grammar. The number
of distinct lexical categories associated with each
preterminal varies from 1 for some function words
through to around 35 as, for instance, tags for main
verbs are associated with a VSUBCAT attribute tak-
ing 33 possible values. The grammar is designed
to enumerate possible valencies for predicates by
including separate rules for each pattern of pos-
sible complementation in English. The distinc-
tion between arguments and adjuncts is expressed
by adjunction of adjuncts to maximal projections
(XP —* XP Adjunct) as opposed to government of
arguments (i.e. arguments are sisters within X]
projections; X] —* X0 Arg]... ArgN).
Each phrase structure schema is associated with
one or more GR specifications which can be con-
ditioned on feature values instantiated at parse
time and which yield a rule-to-rule mapping from
local trees to GRs. The set of GRs associated with
a given derivation define a connected, directed
graph with individual nodes representing lemma-
affix-tags and arcs representing named grammati-
cal relations. The encoding of this mapping within
the grammar is similar to that of F-structure map-
ping in LFG. However, the connected graph is not
constructed and completeness and coherence con-
straints are not used to filter the phrase structure
derivation space.
The grammar finds at least one parse rooted in
the start category for 85% of the Susanne treebank,
a 140K word balanced subset of the Brown Cor-
pus, which we have used for development (Samp-
son, 1995). Much of the remaining data consists
of phrasal fragments marked as independent text
sentences, for example in dialogue. Grammati-
cal coverage includes the majority of construction
types of English, however the handling of some
unbounded dependency constructions, particularly
comparatives and equatives, is limited because of
the lack of fine-grained subcategorization infor-
mation in the PoS tags and by the need to balance
depth of analysis against the size of the deriva-
tion space. On the Susanne corpus, the geometric
mean of the number of analyses for a sentence of
length n is 1.31&apos;. The microaveraged F1-score for
GR extraction on held-out data from Susanne is
76.5% (see section 4.2 for details of the evaluation
scheme).
The system has been used to analyse about 150
million words of English text drawn primarily
from the PTB, TREC, BNC, and Reuters RCV1
datasets in connection with a variety of projects.
The grammar and PoS tagger lexicon have been
incrementally improved by manually examining
cases of parse failure on these datasets. How-
ever, the effort invested amounts to a few days’
effort for each new dataset as opposed to the main
grammar development effort, centred on Susanne,
which has extended over some years and now
amounts to about 2 years’ effort (see Briscoe, 2006
for further details).
</bodyText>
<subsectionHeader confidence="0.992662">
2.3 Parser
</subsectionHeader>
<bodyText confidence="0.999976321428571">
To build the parsing module, the unification gram-
mar is automatically converted into an atomic-
categoried context free ‘backbone’, and a non-
deterministic LALR(1) table is constructed from
this, which is used to drive the parser. The residue
of features not incorporated into the backbone
are unified on each rule application (reduce ac-
tion). In practice, the parser takes average time
roughly quadratic in the length of the input to cre-
ate a packed parse forest represented as a graph-
structured stack. The statistical disambiguation
phase is trained on Susanne treebank bracketings,
producing a probabilistic generalized LALR(1)
parser (e.g. Inui et al., 1997) which associates
probabilities with alternative actions in the LR ta-
ble.
The parser is passed as input the sequence of
most probable lemma-affix-tags found by the tag-
ger. During parsing, probabilities are assigned
to subanalyses based on the the LR table actions
that derived them. The n-best (i.e. most proba-
ble) parses are extracted by a dynamic program-
ming procedure over subanalyses (represented by
nodes in the parse forest). The search is effi-
cient since probabilities are associated with single
nodes in the parse forest and no weight function
over ancestor or sibling nodes is needed. Proba-
bilities capture structural context, since nodes in
</bodyText>
<page confidence="0.998712">
43
</page>
<bodyText confidence="0.999956714285714">
the parse forest partially encode a configuration of
the graph-structured stack and lookahead symbol,
so that, unlike a standard PCFG, the model dis-
criminates between derivations which only differ
in the order of application of the same rules and
also conditions rule application on the PoS tag of
the lookahead token.
When there is no parse rooted in the start cat-
egory, the parser returns a connected sequence
of partial parses which covers the input based
on subanalysis probability and a preference for
longer and non-lexical subanalysis combinations
(e.g. Kiefer et al., 1999). In these cases, the GR
graph will not be fully connected.
</bodyText>
<subsectionHeader confidence="0.989014">
2.4 Tuning and Training Method
</subsectionHeader>
<bodyText confidence="0.999992596491228">
The HMM tagger has been trained on 3M words
of balanced text drawn from the LOB, BNC and
Susanne corpora, which are available with hand-
corrected CLAWS tags. The parser has been
trained from 1.9K trees for sentences from Su-
sanne that were interactively parsed to manually
obtain the correct derivation, and also from 2.1K
further sentences with unlabelled bracketings de-
rived from the Susanne treebank. These brack-
etings guide the parser to one or possibly sev-
eral closely-matching derivations and these are
used to derive probabilities for the LR table us-
ing (weighted) Laplace estimation. Actions in the
table involving rules marked as peripheral are as-
signed a uniform low prior probability to ensure
that derivations involving such rules are consis-
tently lower ranked than those involving only core
rules.
To improve performance on WSJ text, we exam-
ined some parse failures from sections other than
section 23 to identify patterns of consistent fail-
ure. We then manually modified and extended the
grammar with a further 6 rules, mostly to handle
cases of indirect and direct quotation that are very
common in this dataset. This involved 3 days’
work. Once completed, the parser was retrained
on the original data. A subsequent limited inspec-
tion of top-ranked parses led us to disable 6 ex-
isting rules which applied too freely to the WSJ
text; these were designed to analyse auxiliary el-
lipsis which appears to be rare in this genre. We
also catalogued incorrect PoS tags from WSJ parse
failures and manually modified the tagger lexicon
where appropriate. These modifications mostly
consisted of adjusting lexical probabilities of ex-
tant entries with highly-skewed distributions. We
also added some tags to extant entries for infre-
quent words. These modifications took a further
day. The tag transition probabilities were not rees-
timated. Thus, we have made no use of the PTB
itself and only limited use of WSJ text.
This method of grammar and lexicon devel-
opment incrementally improves the overall per-
formance of the system averaged across all the
datasets that it has been applied to. It is very
likely that retraining the PoS tagger on the WSJ
and retraining the parser using PTB would yield
a system which would perform more effectively
on DepBank. However, one of our goals is to
demonstrate that an unlexicalized parser trained
on a modest amount of annotated text from other
sources, coupled to a tagger also trained on
generic, balanced data, can perform competitively
with systems which have been (almost) entirely
developed and trained using PTB, whether or not
these systems deploy hand-crafted grammars or
ones derived automatically from treebanks.
</bodyText>
<sectionHeader confidence="0.987968" genericHeader="method">
3 Extending and Validating DepBank
</sectionHeader>
<bodyText confidence="0.999951074074074">
DepBank was constructed by parsing the selected
section 23 WSJ sentences with the XLE system
and outputting syntactic features and bilexical re-
lations from the F-structure found by the parser.
These features and relations were subsequently
checked, corrected and extended interactively with
the aid of software tools (King et al., 2003).
The choice of relations and features is based
quite closely on LFG and, in fact, overlaps sub-
stantially with the GR output of our parser. Fig-
ure 1 illustrates some DepBank annotations used
in the experiment reported by Kaplan et al. and
our hand-corrected GR output for the example
Ten of the nation’s governors meanwhile called
on the justices to reject efforts to limit abortions.
We have kept the GR representation simpler and
more readable by suppressing lemmatization, to-
ken numbering and PoS tags, but have left the
DepBank annotations unmodified.
The example illustrates some differences be-
tween the schemes. For instance, the subj and
ncsubj relations overlap as both annotations con-
tain such a relation between call(ed) and Ten), but
the GR annotation also includes this relation be-
tween limit and effort(s) and reject and justice(s),
while DepBank links these two verbs to a variable
pro. This reflects a difference of philosophy about
</bodyText>
<page confidence="0.997428">
44
</page>
<tableCaption confidence="0.966977571428571">
DepBank: obl(call-0, on-2)
stmt_type(call-0, declarative)
subj(call-0, ten-1)
tense(call-0, past)
number_type(ten-1, cardinal)
obl(ten-1, governor-35)
obj(on-2, justice-30)
obj(limit-7, abortion-15)
subj(limit-7, pro-21)
obj(reject-8, effort-10)
subj(reject-8, pro-27)
adegree(meanwhile-9, positive)
num(effort-10, pl)
xcomp(effort-10, limit-7)
</tableCaption>
<figure confidence="0.841850133333333">
GR: (ncsubj called Ten _)
(ncsubj reject justices _)
(ncsubj limit efforts _)
(iobj called on)
(xcomp to called reject)
(dobj reject efforts)
(xmod to efforts limit)
(dobj limit abortions)
(dobj on justices)
(det justices the)
(ta bal governors meanwhile)
(ncmod poss governors nation)
(iobj Ten of)
(dobj of governors)
(det nation the)
</figure>
<figureCaption confidence="0.999996">
Figure 1: DepBank and GR annotations.
</figureCaption>
<bodyText confidence="0.999970833333334">
resolution of such ‘understood’ relations in differ-
ent constructions. Viewed as output appropriate to
specific applications, either approach is justifiable.
However, for evaluation, these DepBank relations
add little or no information not already specified
by the xcomp relations in which these verbs also
appear as dependents. On the other hand, Dep-
Bank includes an adjunct relation between mean-
while and call(ed), while the GR annotation treats
meanwhile as a text adjunct (ta) of governors, de-
limited by balanced commas, following Nunberg’s
(1990) text grammar but conveying less informa-
tion here.
There are also issues of incompatible tokeniza-
tion and lemmatization between the systems and
of differing syntactic annotation of similar infor-
mation, which lead to problems mapping between
our GR output and the current DepBank. Finally,
differences in the linguistic intuitions of the an-
notators and errors of commission or omission
on both sides can only be uncovered by manual
comparison of output (e.g. xmod vs. xcomp for
limit efforts above). Thus we reannotated the Dep-
Bank sentences with GRs using our current sys-
tem, and then corrected and extended this anno-
tation utilizing a software tool to highlight dif-
ferences between the extant annotations and our
own.2 This exercise, though time-consuming, un-
covered problems in both annotations, and yields
a doubly-annotated and potentially more valuable
resource in which annotation disagreements over
complex attachment decisions, for instance, can be
inspected.
The GR scheme includes one feature in Dep-
Bank (passive), several splits of relations in Dep-
Bank, such as adjunct, adds some of DepBank’s
featural information, such as subord form, as a
subtype slot of a relation (ccomp), merges Dep-
Bank’s oblique with iobj, and so forth. But it
does not explicitly include all the features of Dep-
Bank or even of the reduced set of semantically-
relevant features used in the experiments and eval-
uation reported in Kaplan et al.. Most of these
features can be computed from the full GR repre-
sentation of bilexical relations between numbered
lemma-affix-tags output by the parser. For in-
stance, num features, such as the plurality of jus-
tices in the example, can be computed from the
full det GR (det justice+s NN2:4 the AT:3)
based on the CLAWS tag (NN2 indicating ‘plu-
ral’) selected for output. The few features that can-
not be computed from GRs and CLAWS tags di-
rectly, such as stmt type, could be computed from
the derivation tree.
</bodyText>
<sectionHeader confidence="0.999894" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.992457">
4.1 Experimental Design
</subsectionHeader>
<bodyText confidence="0.999985363636364">
We selected the same 560 sentences as test data as
Kaplan et al., and all modifications that we made
to our system (see §2.4) were made on the basis
of (very limited) information from other sections
of WSJ text.3 We have made no use of the further
140 held out sentences in DepBank. The results
we report below are derived by choosing the most
probable tag for each word returned by the PoS
tagger and by choosing the unweighted GR set re-
turned for the most probable parse with no lexical
information guiding parse ranking.
</bodyText>
<sectionHeader confidence="0.717604" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.9849325">
Our parser produced rooted sentential analyses for
84% of the test items; actual coverage is higher
</bodyText>
<footnote confidence="0.999634166666667">
2The new version of DepBank along with evaluation
software is included in the current RASP distribution:
www.informatics.susx.ac.uk/research/nlp/rasp
3The PARC group kindly supplied us with the experimen-
tal data files they used to facilitate accurate reproduction of
this experiment.
</footnote>
<page confidence="0.997791">
45
</page>
<table confidence="0.999674405405405">
Relation Precision Recall Fl P R Fl Relation
mod 75.4 71.2 73.3
ncmod 72.9 67.9 70.3
xmod 47.7 45.5 46.6
cmod 51.4 31.6 39.1
pmod 30.8 33.3 32.0
det 88.7 91.1 89.9
arg mod 71.9 67.9 69.9
arg 76.0 73.4 74.6
subj 80.1 66.6 72.7 73 73 73
ncsubj 80.5 66.8 73.0
xsubj 50.0 28.6 36.4
csubj 20.0 50.0 28.6
subj or dobj 82.1 74.9 78.4
comp 74.5 76.4 75.5
obj 78.4 77.9 78.1
dobj 83.4 81.4 82.4 75 75 75 obj
obj2 24.2 38.1 29.6 42 36 39 obj-theta
iobj 68.2 68.1 68.2 64 83 72 obl
clausal 63.5 71.6 67.3
xcomp 75.0 76.4 75.7 74 73 74
ccomp 51.2 65.6 57.5 78 64 70 comp
pcomp 69.6 66.7 68.1
aux 92.8 90.5 91.6
conj 71.7 71.0 71.4 68 62 65
ta 39.1 48.2 43.2
passive 93.6 70.6 80.5 80 83 82
adegree 89.2 72.4 79.9 81 72 76
coord form 92.3 85.7 88.9 92 93 93
num 92.2 89.8 91.0 86 87 86
number type 86.3 92.7 89.4 96 95 96
precoord form 100.0 16.7 28.6 100 50 67
pron form 92.1 91.9 92.0 88 89 89
prt form 71.1 58.7 64.3 72 65 68
subord form 60.7 48.1 53.6
macroaverage 69.0 63.4 66.1
microaverage 81.5 78.1 79.7 80 79 79
</table>
<bodyText confidence="0.999914433333334">
croaverages. The macroaverage is calculated by
taking the average of each measure for each indi-
vidual relation and feature; the microaverage mea-
sures are calculated from the counts for all rela-
tions and features.4 Indentation of GRs shows
degree of specificity of the relation. Thus, mod
scores are microaveraged over the counts for the
five fully specified modifier relations listed imme-
diately after it in Table 1. This allows comparison
of overall accuracy on modifiers with, for instance
overall accuracy on arguments. Figures in italics
to the right are discussed in the next section.
Kaplan et al.’s microaveraged scores for
Collins’ Model 3 and the cut-down and complete
versions of the XLE parser are given in Table 2,
along with the microaveraged scores for our parser
from Table 1. Our system’s accuracy results (eval-
uated on the reannotated DepBank) are better than
those for Collins and the cut-down XLE, and very
similar overall to the complete XLE (evaluated
on DepBank). Speed of processing is also very
competitive.5 These results demonstrate that a
statistical parser with roughly state-of-the-art ac-
curacy can be constructed without the need for
large in-domain treebanks. However, the perfor-
mance of the system, as measured by microrav-
eraged F1-score on GR extraction alone, has de-
clined by 2.7% over the held-out Susanne data,
so even the unlexicalized parser is by no means
domain-independent.
</bodyText>
<tableCaption confidence="0.680320333333333">
Table 1: Accuracy of our parser, and where
roughly comparable, the XLE as reported by King
et al.
</tableCaption>
<bodyText confidence="0.999383411764706">
than this since some of the test sentences are el-
liptical or fragmentary, but in many cases are rec-
ognized as single complete constituents. Kaplan
et al. report that the complete XLE system finds
rooted analyses for 79% of section 23 of the WSJ
but do not report coverage just for the test sen-
tences. The XLE parser uses several performance
optimizations which mean that processing of sub-
analyses in longer sentences can be curtailed or
preempted, so that it is not clear what proportion
of the remaining data is outside grammatical cov-
erage.
Table 1 shows accuracy results for each indi-
vidual relation and feature, starting with the GR
bilexical relations in the extended DepBank and
followed by most DepBank features reported by
Kaplan et al., and finally overall macro- and mi-
</bodyText>
<subsectionHeader confidence="0.998364">
4.3 Evaluation Issues
</subsectionHeader>
<bodyText confidence="0.999819272727273">
The DepBank num feature on nouns is evalu-
ated by Kaplan et al. on the grounds that it is
semantically-relevant for applications. There are
over 5K num features in DepBank so the overall
microaveraged scores for a system will be signifi-
cantly affected by accuracy on num. We expected
our system, which incorporates a tagger with good
empirical (97.1%) accuracy on the test data, to re-
cover this feature with 95% accuracy or better, as
it will correlate with tags NNx1 and NNx2 (where
‘x’ represents zero or more capitals in the CLAWS
</bodyText>
<footnote confidence="0.9910856">
4We did not compute the remaining DepBank features
stmt type, tense, prog or perf as these rely on information
that can only be extracted from the derivation tree rather than
the GR set.
5Processing time for our system was 61 seconds on one
2.2GHz Opteron CPU (comprising tokenization, tagging,
morphology, and parsing, including module startup over-
heads). Allowing for slightly different CPUs, this is 2.5–10
times faster than the Collins and XLE parsers, as reported by
Kaplan et al.
</footnote>
<page confidence="0.997749">
46
</page>
<table confidence="0.9998304">
System Eval corpus Precision Recall F1
Collins DepBank 78.3 71.2 74.6
Cut-down XLE DepBank 79.1 76.2 77.6
Complete XLE DepBank 79.4 79.8 79.6
Our system DepBank/GR 81.5 78.1 79.7
</table>
<tableCaption confidence="0.999579">
Table 2: Microaveraged overall scores from Kaplan et al. and for our system.
</tableCaption>
<bodyText confidence="0.999952927710844">
tagset). However, DepBank treats the majority
of prenominal modifiers as adjectives rather than
nouns and, therefore, associates them with an ade-
gree rather than a num feature. The PoS tag se-
lected depends primarily on the relative lexical
probabilities of each tag for a given lexical item
recorded in the tagger lexicon. But, regardless
of this lexical decision, the correct GR is recov-
ered, and neither adegree(positive) or num(sg)
add anything semantically-relevant when the lex-
ical item is a nominal premodifier. A strategy
which only provided a num feature for nominal
heads would be both more semantically-relevant
and would also yield higher precision (95.2%).
However, recall (48.4%) then suffers against Dep-
Bank as noun premodifiers have a num feature.
Therefore, in the results presented in Table 1 we
have not counted cases where either DepBank or
our system assign a premodifier adegree(positive)
or num(sg).
There are similar issues with other DepBank
features and relations. For instance, the form of
a subordinator with clausal complements is anno-
tated as a relation between verb and subordina-
tor, while there is a separate comp relation be-
tween verb and complement head. The GR rep-
resentation adds the subordinator as a subtype of
ccomp recording essentially identical information
in a single relation. So evaluation scores based on
aggregated counts of correct decisions will be dou-
bled for a system which structures this informa-
tion as in DepBank. However, reproducing the ex-
act DepBank subord form relation from the GR
ccomp one is non-trivial because DepBank treats
modal auxiliaries as syntactic heads while the GR-
scheme treats the main verb as head in all ccomp
relations. We have not attempted to compensate
for any further such discrepancies other than the
one discussed in the previous paragraph. However,
we do believe that they collectively damage scores
for our system.
As King et al. note, it is difficult to identify
such informational redundancies to avoid double-
counting and to eradicate all system specific bi-
ases. However, reporting precision, recall and F1-
scores for each relation and feature separately and
microaveraging these scores on the basis of a hi-
erarchy, as in our GR scheme, ameliorates many
of these problems and gives a better indication
of the strengths and weaknesses of a particular
parser, which may also be useful in a decision
about its usefulness for a specific application. Un-
fortunately, Kaplan et al. do not report their re-
sults broken down by relation or feature so it is
not possible, for example, on the basis of the ar-
guments made above, to choose to compare the
performance of our system on ccomp to theirs for
comp, ignoring subord form. King et al. do re-
port individual results for selected features and re-
lations from an evaluation of the complete XLE
parser on all 700 DepBank sentences with an al-
most identical overall microaveraged F1 score of
79.5%, suggesting that these results provide a rea-
sonably accurate idea of the XLE parser’s relative
performance on different features and relations.
Where we believe that the information captured
by a DepBank feature or relation is roughly com-
parable to that expressed by a GR in our extended
DepBank, we have included King et al.’s scores
in the rightmost column in Table 1 for compari-
son purposes. Even if these features and relations
were drawn from the same experiment, however,
they would still not be exactly comparable. For in-
stance, as discussed in §3 nearly half (just over 1K)
the DepBank subj relations include pro as one el-
ement, mostly double counting a corresponding
xcomp relation. On the other hand, our ta rela-
tion syntactically underspecifies many DepBank
adjunct relations. Nevertheless, it is possible to
see, for instance, that while both parsers perform
badly on second objects ours is worse, presumably
because of lack of lexical subcategorization infor-
mation.
</bodyText>
<page confidence="0.999152">
47
</page>
<sectionHeader confidence="0.999456" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999957357142857">
We have demonstrated that an unlexicalized parser
with minimal manual modification for WSJ text –
but no tuning of performance to optimize on this
dataset alone, and no use of PTB – can achieve
accuracy competitive with parsers employing lex-
icalized statistical models trained on PTB.
We speculate that we achieve these results be-
cause our system is engineered to make minimal
use of lexical information both in the grammar and
in parse ranking, because the grammar has been
developed to constrain ambiguity despite this lack
of lexical information, and because we can com-
pute the full packed parse forest for all the test sen-
tences efficiently (without sacrificing speed of pro-
cessing with respect to other statistical parsers).
These advantages appear to effectively offset the
disadvantage of relying on a coarser, purely struc-
tural model for probabilistic parse selection. In fu-
ture work, we hope to improve the accuracy of the
system by adding lexical information to the statis-
tical parse selection component without exploiting
in-domain treebanks.
Clearly, more work is needed to enable more
accurate, informative, objective and wider com-
parison of extant parsers. More recent PTB-based
parsers show small improvements over Collins’
Model 3 using PARSEVAL, while Clark and Cur-
ran (2004) and Miyao and Tsujii (2005) report
84% and 86.7% F1-scores respectively for their
own relational evaluations on section 23 of WSJ.
However, it is impossible to meaningfully com-
pare these results to those reported here. The rean-
notated DepBank potentially supports evaluations
which score according to the degree of agreement
between this and the original annotation and/or de-
velopment of future consensual versions through
collaborative reannotation by the research com-
munity. We have also highlighted difficulties for
relational evaluation schemes and argued that pre-
senting individual scores for (classes of) relations
and features is both more informative and facili-
tates system comparisons.
</bodyText>
<sectionHeader confidence="0.999816" genericHeader="references">
6 References
</sectionHeader>
<reference confidence="0.999672347222223">
Bikel, D.. 2004. Intricacies of Collins’ parsing model, Com-
putational Linguistics, 30(4):479–512.
Briscoe, E.J.. 2006. An introduction to tag sequence gram-
mars and the RASP system parser, University of Cam-
bridge, Computer Laboratory Technical Report 662.
Briscoe, E.J. and J. Carroll. 2002. Robust accurate statistical
annotation of general text. In Proceedings of the 3rd Int.
Conf. on Language Resources and Evaluation (LREC),
Las Palmas, Gran Canaria. 1499–1504.
Carroll, J. and E.J. Briscoe. 2002. High precision extraction
of grammatical relations. In Proceedings of the 19th Int.
Conf. on Computational Linguistics (COLING), Taipei,
Taiwan. 134–140.
Carroll, J., E. Briscoe and A. Sanfilippo. 1998. Parser evalu-
ation: a survey and a new proposal. In Proceedings of the
1st International Conference on Language Resources and
Evaluation, Granada, Spain. 447–454.
Clark, S. and J. Curran. 2004. The importance of supertag-
ging for wide-coverage CCG parsing. In Proceedings of
the 20th International Conference on Computational Lin-
guistics (COLING-04), Geneva, Switzerland. 282–288.
Collins, M.. 1999. Head-driven Statistical Models for Nat-
ural Language Parsing. PhD Dissertation, Computer and
Information Science, University of Pennsylvania.
Gildea, D.. 2001. Corpus variation and parser performance.
In Proceedings of the Empirical Methods in Natural Lan-
guage Processing (EMNLP’01), Pittsburgh, PA.
Inui, K., V. Sornlertlamvanich, H. Tanaka and T. Tokunaga.
1997. A new formalization of probabilistic GLR parsing.
In Proceedings of the 5th International Workshop on Pars-
ing Technologies (IWPT’97), Boston, MA. 123–134.
Kaplan, R., S. Riezler, T. H. King, J. Maxwell III, A. Vasser-
man and R. Crouch. 2004. Speed and accuracy in shal-
low and deep stochastic parsing. In Proceedings of the
HLT Conference and the 4th Annual Meeting of the North
American Chapter of the ACL (HLT-NAACL’04), Boston,
MA.
Kiefer, B., H-U. Krieger, J. Carroll and R. Malouf. 1999.
A bag of useful techniques for efficient and robust pars-
ing. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, University of
Maryland. 473–480.
King, T. H., R. Crouch, S. Riezler, M. Dalrymple and R. Ka-
plan. 2003. The PARC700 Dependency Bank. In Pro-
ceedings of the 4th International Workshop on Linguisti-
cally Interpreted Corpora (LINC-03), Budapest, Hungary.
Klein, D. and C. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, Sapporo,
Japan. 423–430.
Lin, D.. 1998. Dependency-based evaluation of MINIPAR.
In Proceedings of the Workshop at LREC’98 on The Eval-
uation of Parsing Systems, Granada, Spain.
Manning, C. and H. Sch¨utze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press, Cambridge,
MA.
Miyao, Y. and J. Tsujii. 2005. Probabilistic disambiguation
models for wide-coverage HPSG parsing. In Proceedings
of the 43rd Annual Meeting of the Association for Compu-
tational Linguistics, Ann Arbor, MI. 83–90.
Nunberg, G.. 1990. The Linguistics of Punctuation. CSLI
Lecture Notes 18, Stanford, CA.
Sampson, G.. 1995. English for the Computer. Oxford Uni-
versity Press, Oxford, UK.
Watson, R.. 2006. Part-of-speech tagging models for parsing.
In Proceedings of the 9th Conference of Computational
Linguistics in the UK (CLUK’06), Open University, Mil-
ton Keynes.
Watson, R., J. Carroll and E.J. Briscoe. 2005. Efficient ex-
traction of grammatical relations. In Proceedings of the
9th Int. Workshop on Parsing Technologies (IWPT’05),
Vancouver, Ca..
</reference>
<page confidence="0.999347">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978759">
<title confidence="0.992362">Evaluating the Accuracy of an Unlexicalized Statistical Parser on the PARC DepBank</title>
<author confidence="0.999896">Ted Briscoe</author>
<affiliation confidence="0.999859">Computer Laboratory University of Cambridge</affiliation>
<abstract confidence="0.999675055555556">We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank. We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manuallyconstructed treebanks. This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure. The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model, Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="2899" citStr="Bikel (2004)" startWordPosition="437" endWordPosition="438">ized statistical parsers developed, trained and tested on PTB achieve a labelled Fl-score – the harmonic mean of labelled precision and recall – of around 90%. Klein and Manning (2003) argue that such results represent about 4% absolute improvement over a carefully constructed unlexicalized PCFGlike model trained and tested in the same manner.1 Gildea (2001) shows that WSJ-derived bilexical parameters in Collins’ (1999) Model 1 parser contribute less than 1% to parse selection accuracy when test data is in the same domain, and yield no improvement for test data selected from the Brown Corpus. Bikel (2004) shows that, in Collins’ (1999) Model 2, bilexical parameters contribute less than 0.5% to accuracy on in-domain data while lexical subcategorization-like parameters contribute just over 1%. Several alternative relational evaluation schemes have been developed (e.g. Carroll et al., 1998; Lin, 1998). However, until recently, no WSJ data has been carefully annotated to support relational evaluation. King et al. (2003) describe the PARC 700 Dependency Bank (hereinafter DepBank), which consists of 700 WSJ sentences randomly drawn from section 23. These sentences have been annotated with syntactic </context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Bikel, D.. 2004. Intricacies of Collins’ parsing model, Computational Linguistics, 30(4):479–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
</authors>
<title>An introduction to tag sequence grammars and the RASP system parser,</title>
<date>2006</date>
<tech>Technical Report 662.</tech>
<institution>University of Cambridge, Computer Laboratory</institution>
<contexts>
<context position="12011" citStr="Briscoe, 2006" startWordPosition="1873" endWordPosition="1874">section 4.2 for details of the evaluation scheme). The system has been used to analyse about 150 million words of English text drawn primarily from the PTB, TREC, BNC, and Reuters RCV1 datasets in connection with a variety of projects. The grammar and PoS tagger lexicon have been incrementally improved by manually examining cases of parse failure on these datasets. However, the effort invested amounts to a few days’ effort for each new dataset as opposed to the main grammar development effort, centred on Susanne, which has extended over some years and now amounts to about 2 years’ effort (see Briscoe, 2006 for further details). 2.3 Parser To build the parsing module, the unification grammar is automatically converted into an atomiccategoried context free ‘backbone’, and a nondeterministic LALR(1) table is constructed from this, which is used to drive the parser. The residue of features not incorporated into the backbone are unified on each rule application (reduce action). In practice, the parser takes average time roughly quadratic in the length of the input to create a packed parse forest represented as a graphstructured stack. The statistical disambiguation phase is trained on Susanne treeba</context>
</contexts>
<marker>Briscoe, 2006</marker>
<rawString>Briscoe, E.J.. 2006. An introduction to tag sequence grammars and the RASP system parser, University of Cambridge, Computer Laboratory Technical Report 662.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd Int. Conf. on Language Resources and Evaluation (LREC), Las Palmas, Gran Canaria.</booktitle>
<pages>1499--1504</pages>
<contexts>
<context position="8165" citStr="Briscoe and Carroll, 2002" startWordPosition="1256" endWordPosition="1259">ic trees, and/or factored into a sequence of bilexical grammatical relations (GRs) between lexical heads and their dependents. The full system can be extended in a variety of ways – for example, by pruning PoS tags but allowing multiple tag possibilities per word as input to the parser, by incorporating lexical subcategorization into parse selection, by computing GR weights based on the proportion and probability of the n-best analyses yielding them, and so forth – broadly trading accuracy and greater domaindependence against speed and reduced sensitivity to domain-specific lexical behaviour (Briscoe and Carroll, 2002; Carroll and Briscoe, 2002; Watson et al., 2005; Watson, 2006). However, in this paper we focus exclusively on the baseline unlexicalized system. 2.2 Grammar Development The grammar is expressed in a feature-based, unification formalism. There are currently 676 phrase structure rule schemata, 15 feature propagation rules, 30 default feature value rules, 22 category expansion rules and 41 feature types which together define 1124 compiled phrase structure rules in which categories are represented as sets of fea42 tures, that is, attribute-value pairs, possibly with variable values, possibly bou</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Briscoe, E.J. and J. Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd Int. Conf. on Language Resources and Evaluation (LREC), Las Palmas, Gran Canaria. 1499–1504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>E J Briscoe</author>
</authors>
<title>High precision extraction of grammatical relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th Int. Conf. on Computational Linguistics (COLING),</booktitle>
<location>Taipei,</location>
<contexts>
<context position="8192" citStr="Carroll and Briscoe, 2002" startWordPosition="1260" endWordPosition="1263">nto a sequence of bilexical grammatical relations (GRs) between lexical heads and their dependents. The full system can be extended in a variety of ways – for example, by pruning PoS tags but allowing multiple tag possibilities per word as input to the parser, by incorporating lexical subcategorization into parse selection, by computing GR weights based on the proportion and probability of the n-best analyses yielding them, and so forth – broadly trading accuracy and greater domaindependence against speed and reduced sensitivity to domain-specific lexical behaviour (Briscoe and Carroll, 2002; Carroll and Briscoe, 2002; Watson et al., 2005; Watson, 2006). However, in this paper we focus exclusively on the baseline unlexicalized system. 2.2 Grammar Development The grammar is expressed in a feature-based, unification formalism. There are currently 676 phrase structure rule schemata, 15 feature propagation rules, 30 default feature value rules, 22 category expansion rules and 41 feature types which together define 1124 compiled phrase structure rules in which categories are represented as sets of fea42 tures, that is, attribute-value pairs, possibly with variable values, possibly bound between mother and one o</context>
</contexts>
<marker>Carroll, Briscoe, 2002</marker>
<rawString>Carroll, J. and E.J. Briscoe. 2002. High precision extraction of grammatical relations. In Proceedings of the 19th Int. Conf. on Computational Linguistics (COLING), Taipei, Taiwan. 134–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>E Briscoe</author>
<author>A Sanfilippo</author>
</authors>
<title>Parser evaluation: a survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference on Language Resources and Evaluation,</booktitle>
<pages>447--454</pages>
<location>Granada,</location>
<contexts>
<context position="3186" citStr="Carroll et al., 1998" startWordPosition="476" endWordPosition="479">nlexicalized PCFGlike model trained and tested in the same manner.1 Gildea (2001) shows that WSJ-derived bilexical parameters in Collins’ (1999) Model 1 parser contribute less than 1% to parse selection accuracy when test data is in the same domain, and yield no improvement for test data selected from the Brown Corpus. Bikel (2004) shows that, in Collins’ (1999) Model 2, bilexical parameters contribute less than 0.5% to accuracy on in-domain data while lexical subcategorization-like parameters contribute just over 1%. Several alternative relational evaluation schemes have been developed (e.g. Carroll et al., 1998; Lin, 1998). However, until recently, no WSJ data has been carefully annotated to support relational evaluation. King et al. (2003) describe the PARC 700 Dependency Bank (hereinafter DepBank), which consists of 700 WSJ sentences randomly drawn from section 23. These sentences have been annotated with syntactic features and with bilexical head-dependent relations derived from the F-structure representation of Lexical Functional Grammar (LFG). DepBank facilitates 1Klein and Manning retained some functional tag information from PTB, so it could be argued that their model remains ‘mildly’ lexical</context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>Carroll, J., E. Briscoe and A. Sanfilippo. 1998. Parser evaluation: a survey and a new proposal. In Proceedings of the 1st International Conference on Language Resources and Evaluation, Granada, Spain. 447–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04),</booktitle>
<location>Geneva,</location>
<contexts>
<context position="31989" citStr="Clark and Curran (2004)" startWordPosition="5137" endWordPosition="5141">ed of processing with respect to other statistical parsers). These advantages appear to effectively offset the disadvantage of relying on a coarser, purely structural model for probabilistic parse selection. In future work, we hope to improve the accuracy of the system by adding lexical information to the statistical parse selection component without exploiting in-domain treebanks. Clearly, more work is needed to enable more accurate, informative, objective and wider comparison of extant parsers. More recent PTB-based parsers show small improvements over Collins’ Model 3 using PARSEVAL, while Clark and Curran (2004) and Miyao and Tsujii (2005) report 84% and 86.7% F1-scores respectively for their own relational evaluations on section 23 of WSJ. However, it is impossible to meaningfully compare these results to those reported here. The reannotated DepBank potentially supports evaluations which score according to the degree of agreement between this and the original annotation and/or development of future consensual versions through collaborative reannotation by the research community. We have also highlighted difficulties for relational evaluation schemes and argued that presenting individual scores for (</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Clark, S. and J. Curran. 2004. The importance of supertagging for wide-coverage CCG parsing. In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04), Geneva, Switzerland. 282–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>PhD Dissertation,</tech>
<institution>Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="1441" citStr="Collins (1999)" startWordPosition="219" endWordPosition="220">n Considerable progress has been made in accurate statistical parsing of realistic texts, yielding rooted, hierarchical and/or relational representations of full sentences. However, much of this progress has been made with systems based on large lexicalized probabilistic contextfree like (PCFG-like) models trained on the Wall Street Journal (WSJ) subset of the Penn TreeBank (PTB). Evaluation of these systems has been mostly in terms of the PARSEVAL scheme using tree similarity measures of (labelled) precision and recall and crossing bracket rate applied to section 23 of the WSJ PTB. (See e.g. Collins (1999) for detailed exposition of one such very fruitful line of research.) We evaluate the comparative accuracy of an unlexicalized statistical parser trained on a smaller treebank and tested on a subset of section 23 of the WSJ using a relational evaluation scheme. We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) John Carroll School of Informatics University of Sussex can be quickly developed without reliance on large in-domain manually-constructed treebanks. This makes it more practical to use statistical parsers in diverse applications needing </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, M.. 1999. Head-driven Statistical Models for Natural Language Parsing. PhD Dissertation, Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP’01),</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2647" citStr="Gildea (2001)" startWordPosition="394" endWordPosition="395">eeding access to aspects of predicate-argument structure. We define a lexicalized statistical parser as one which utilizes probabilistic parameters concerning lexical subcategorization and/or bilexical relations over tree configurations. Current lexicalized statistical parsers developed, trained and tested on PTB achieve a labelled Fl-score – the harmonic mean of labelled precision and recall – of around 90%. Klein and Manning (2003) argue that such results represent about 4% absolute improvement over a carefully constructed unlexicalized PCFGlike model trained and tested in the same manner.1 Gildea (2001) shows that WSJ-derived bilexical parameters in Collins’ (1999) Model 1 parser contribute less than 1% to parse selection accuracy when test data is in the same domain, and yield no improvement for test data selected from the Brown Corpus. Bikel (2004) shows that, in Collins’ (1999) Model 2, bilexical parameters contribute less than 0.5% to accuracy on in-domain data while lexical subcategorization-like parameters contribute just over 1%. Several alternative relational evaluation schemes have been developed (e.g. Carroll et al., 1998; Lin, 1998). However, until recently, no WSJ data has been c</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Gildea, D.. 2001. Corpus variation and parser performance. In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP’01), Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Inui</author>
<author>V Sornlertlamvanich</author>
<author>H Tanaka</author>
<author>T Tokunaga</author>
</authors>
<title>A new formalization of probabilistic GLR parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th International Workshop on Parsing Technologies (IWPT’97),</booktitle>
<pages>123--134</pages>
<location>Boston, MA.</location>
<contexts>
<context position="12704" citStr="Inui et al., 1997" startWordPosition="1979" endWordPosition="1982">ion grammar is automatically converted into an atomiccategoried context free ‘backbone’, and a nondeterministic LALR(1) table is constructed from this, which is used to drive the parser. The residue of features not incorporated into the backbone are unified on each rule application (reduce action). In practice, the parser takes average time roughly quadratic in the length of the input to create a packed parse forest represented as a graphstructured stack. The statistical disambiguation phase is trained on Susanne treebank bracketings, producing a probabilistic generalized LALR(1) parser (e.g. Inui et al., 1997) which associates probabilities with alternative actions in the LR table. The parser is passed as input the sequence of most probable lemma-affix-tags found by the tagger. During parsing, probabilities are assigned to subanalyses based on the the LR table actions that derived them. The n-best (i.e. most probable) parses are extracted by a dynamic programming procedure over subanalyses (represented by nodes in the parse forest). The search is efficient since probabilities are associated with single nodes in the parse forest and no weight function over ancestor or sibling nodes is needed. Probab</context>
</contexts>
<marker>Inui, Sornlertlamvanich, Tanaka, Tokunaga, 1997</marker>
<rawString>Inui, K., V. Sornlertlamvanich, H. Tanaka and T. Tokunaga. 1997. A new formalization of probabilistic GLR parsing. In Proceedings of the 5th International Workshop on Parsing Technologies (IWPT’97), Boston, MA. 123–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
<author>S Riezler</author>
<author>T H King</author>
<author>J Maxwell A Vasserman</author>
<author>R Crouch</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT Conference and the 4th Annual Meeting of the North American Chapter of the ACL (HLT-NAACL’04),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="4280" citStr="Kaplan et al. (2004)" startWordPosition="631" endWordPosition="634">and Manning retained some functional tag information from PTB, so it could be argued that their model remains ‘mildly’ lexicalized since functional tags encode some subcategorization information. 41 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 41–48, Sydney, July 2006. c�2006 Association for Computational Linguistics comparison of PCFG-like statistical parsers developed from the PTB with other parsers whose output is not designed to yield PTB-style trees, using an evaluation which is closer to the protypical parsing task of recovering predicate-argument structure. Kaplan et al. (2004) compare the accuracy and speed of the PARC XLE Parser to Collins’ Model 3 parser. They develop transformation rules for both, designed to map native output to a subset of the features and relations in DepBank. They compare performance of a grammatically cut-down and complete version of the XLE parser to the publically available version of Collins’ parser. One fifth of DepBank is held out to optimize the speed and accuracy of the three systems. They conclude from the results of these experiments that the cut-down XLE parser is two-thirds the speed of Collins’ Model 3 but 12% more accurate, whi</context>
</contexts>
<marker>Kaplan, Riezler, King, Vasserman, Crouch, 2004</marker>
<rawString>Kaplan, R., S. Riezler, T. H. King, J. Maxwell III, A. Vasserman and R. Crouch. 2004. Speed and accuracy in shallow and deep stochastic parsing. In Proceedings of the HLT Conference and the 4th Annual Meeting of the North American Chapter of the ACL (HLT-NAACL’04), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kiefer</author>
<author>H-U Krieger</author>
<author>J Carroll</author>
<author>R Malouf</author>
</authors>
<title>A bag of useful techniques for efficient and robust parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association</booktitle>
<pages>473--480</pages>
<institution>for Computational Linguistics, University of Maryland.</institution>
<contexts>
<context position="13939" citStr="Kiefer et al., 1999" startWordPosition="2177" endWordPosition="2180"> structural context, since nodes in 43 the parse forest partially encode a configuration of the graph-structured stack and lookahead symbol, so that, unlike a standard PCFG, the model discriminates between derivations which only differ in the order of application of the same rules and also conditions rule application on the PoS tag of the lookahead token. When there is no parse rooted in the start category, the parser returns a connected sequence of partial parses which covers the input based on subanalysis probability and a preference for longer and non-lexical subanalysis combinations (e.g. Kiefer et al., 1999). In these cases, the GR graph will not be fully connected. 2.4 Tuning and Training Method The HMM tagger has been trained on 3M words of balanced text drawn from the LOB, BNC and Susanne corpora, which are available with handcorrected CLAWS tags. The parser has been trained from 1.9K trees for sentences from Susanne that were interactively parsed to manually obtain the correct derivation, and also from 2.1K further sentences with unlabelled bracketings derived from the Susanne treebank. These bracketings guide the parser to one or possibly several closely-matching derivations and these are us</context>
</contexts>
<marker>Kiefer, Krieger, Carroll, Malouf, 1999</marker>
<rawString>Kiefer, B., H-U. Krieger, J. Carroll and R. Malouf. 1999. A bag of useful techniques for efficient and robust parsing. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, University of Maryland. 473–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H King</author>
<author>R Crouch</author>
<author>S Riezler</author>
<author>M Dalrymple</author>
<author>R Kaplan</author>
</authors>
<title>The PARC700 Dependency Bank.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03),</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="3318" citStr="King et al. (2003)" startWordPosition="496" endWordPosition="499">ns’ (1999) Model 1 parser contribute less than 1% to parse selection accuracy when test data is in the same domain, and yield no improvement for test data selected from the Brown Corpus. Bikel (2004) shows that, in Collins’ (1999) Model 2, bilexical parameters contribute less than 0.5% to accuracy on in-domain data while lexical subcategorization-like parameters contribute just over 1%. Several alternative relational evaluation schemes have been developed (e.g. Carroll et al., 1998; Lin, 1998). However, until recently, no WSJ data has been carefully annotated to support relational evaluation. King et al. (2003) describe the PARC 700 Dependency Bank (hereinafter DepBank), which consists of 700 WSJ sentences randomly drawn from section 23. These sentences have been annotated with syntactic features and with bilexical head-dependent relations derived from the F-structure representation of Lexical Functional Grammar (LFG). DepBank facilitates 1Klein and Manning retained some functional tag information from PTB, so it could be argued that their model remains ‘mildly’ lexicalized since functional tags encode some subcategorization information. 41 Proceedings of the COLING/ACL 2006 Main Conference Poster S</context>
<context position="17065" citStr="King et al., 2003" startWordPosition="2684" endWordPosition="2687">o trained on generic, balanced data, can perform competitively with systems which have been (almost) entirely developed and trained using PTB, whether or not these systems deploy hand-crafted grammars or ones derived automatically from treebanks. 3 Extending and Validating DepBank DepBank was constructed by parsing the selected section 23 WSJ sentences with the XLE system and outputting syntactic features and bilexical relations from the F-structure found by the parser. These features and relations were subsequently checked, corrected and extended interactively with the aid of software tools (King et al., 2003). The choice of relations and features is based quite closely on LFG and, in fact, overlaps substantially with the GR output of our parser. Figure 1 illustrates some DepBank annotations used in the experiment reported by Kaplan et al. and our hand-corrected GR output for the example Ten of the nation’s governors meanwhile called on the justices to reject efforts to limit abortions. We have kept the GR representation simpler and more readable by suppressing lemmatization, token numbering and PoS tags, but have left the DepBank annotations unmodified. The example illustrates some differences bet</context>
</contexts>
<marker>King, Crouch, Riezler, Dalrymple, Kaplan, 2003</marker>
<rawString>King, T. H., R. Crouch, S. Riezler, M. Dalrymple and R. Kaplan. 2003. The PARC700 Dependency Bank. In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03), Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo,</location>
<contexts>
<context position="2471" citStr="Klein and Manning (2003)" startWordPosition="365" endWordPosition="368">ty of Sussex can be quickly developed without reliance on large in-domain manually-constructed treebanks. This makes it more practical to use statistical parsers in diverse applications needing access to aspects of predicate-argument structure. We define a lexicalized statistical parser as one which utilizes probabilistic parameters concerning lexical subcategorization and/or bilexical relations over tree configurations. Current lexicalized statistical parsers developed, trained and tested on PTB achieve a labelled Fl-score – the harmonic mean of labelled precision and recall – of around 90%. Klein and Manning (2003) argue that such results represent about 4% absolute improvement over a carefully constructed unlexicalized PCFGlike model trained and tested in the same manner.1 Gildea (2001) shows that WSJ-derived bilexical parameters in Collins’ (1999) Model 1 parser contribute less than 1% to parse selection accuracy when test data is in the same domain, and yield no improvement for test data selected from the Brown Corpus. Bikel (2004) shows that, in Collins’ (1999) Model 2, bilexical parameters contribute less than 0.5% to accuracy on in-domain data while lexical subcategorization-like parameters contri</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D. and C. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, Sapporo, Japan. 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop at LREC’98 on The Evaluation of Parsing Systems,</booktitle>
<location>Granada,</location>
<contexts>
<context position="3198" citStr="Lin, 1998" startWordPosition="480" endWordPosition="481">model trained and tested in the same manner.1 Gildea (2001) shows that WSJ-derived bilexical parameters in Collins’ (1999) Model 1 parser contribute less than 1% to parse selection accuracy when test data is in the same domain, and yield no improvement for test data selected from the Brown Corpus. Bikel (2004) shows that, in Collins’ (1999) Model 2, bilexical parameters contribute less than 0.5% to accuracy on in-domain data while lexical subcategorization-like parameters contribute just over 1%. Several alternative relational evaluation schemes have been developed (e.g. Carroll et al., 1998; Lin, 1998). However, until recently, no WSJ data has been carefully annotated to support relational evaluation. King et al. (2003) describe the PARC 700 Dependency Bank (hereinafter DepBank), which consists of 700 WSJ sentences randomly drawn from section 23. These sentences have been annotated with syntactic features and with bilexical head-dependent relations derived from the F-structure representation of Lexical Functional Grammar (LFG). DepBank facilitates 1Klein and Manning retained some functional tag information from PTB, so it could be argued that their model remains ‘mildly’ lexicalized since f</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D.. 1998. Dependency-based evaluation of MINIPAR. In Proceedings of the Workshop at LREC’98 on The Evaluation of Parsing Systems, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Manning, C. and H. Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>83--90</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="32017" citStr="Miyao and Tsujii (2005)" startWordPosition="5143" endWordPosition="5146">t to other statistical parsers). These advantages appear to effectively offset the disadvantage of relying on a coarser, purely structural model for probabilistic parse selection. In future work, we hope to improve the accuracy of the system by adding lexical information to the statistical parse selection component without exploiting in-domain treebanks. Clearly, more work is needed to enable more accurate, informative, objective and wider comparison of extant parsers. More recent PTB-based parsers show small improvements over Collins’ Model 3 using PARSEVAL, while Clark and Curran (2004) and Miyao and Tsujii (2005) report 84% and 86.7% F1-scores respectively for their own relational evaluations on section 23 of WSJ. However, it is impossible to meaningfully compare these results to those reported here. The reannotated DepBank potentially supports evaluations which score according to the degree of agreement between this and the original annotation and/or development of future consensual versions through collaborative reannotation by the research community. We have also highlighted difficulties for relational evaluation schemes and argued that presenting individual scores for (classes of) relations and fe</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Miyao, Y. and J. Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, Ann Arbor, MI. 83–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Nunberg</author>
</authors>
<title>The Linguistics of Punctuation.</title>
<date>1990</date>
<booktitle>CSLI Lecture Notes 18,</booktitle>
<location>Stanford, CA.</location>
<marker>Nunberg, 1990</marker>
<rawString>Nunberg, G.. 1990. The Linguistics of Punctuation. CSLI Lecture Notes 18, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
</authors>
<title>English for the Computer.</title>
<date>1995</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="9184" citStr="Sampson, 1995" startWordPosition="1416" endWordPosition="1417"> which together define 1124 compiled phrase structure rules in which categories are represented as sets of fea42 tures, that is, attribute-value pairs, possibly with variable values, possibly bound between mother and one or more daughter categories. 142 of the phrase structure schemata are manually identified as peripheral rather than core rules of English grammar. Categories are matched using fixedarity term unification at parse time. The lexical categories of the grammar consist of feature-based descriptions of the 149 PoS tags and 13 punctuation tags (a subset of the CLAWS tagset, see e.g. Sampson, 1995) which constitute the preterminals of the grammar. The number of distinct lexical categories associated with each preterminal varies from 1 for some function words through to around 35 as, for instance, tags for main verbs are associated with a VSUBCAT attribute taking 33 possible values. The grammar is designed to enumerate possible valencies for predicates by including separate rules for each pattern of possible complementation in English. The distinction between arguments and adjuncts is expressed by adjunction of adjuncts to maximal projections (XP —* XP Adjunct) as opposed to government o</context>
<context position="10719" citStr="Sampson, 1995" startWordPosition="1662" endWordPosition="1664">ted with a given derivation define a connected, directed graph with individual nodes representing lemmaaffix-tags and arcs representing named grammatical relations. The encoding of this mapping within the grammar is similar to that of F-structure mapping in LFG. However, the connected graph is not constructed and completeness and coherence constraints are not used to filter the phrase structure derivation space. The grammar finds at least one parse rooted in the start category for 85% of the Susanne treebank, a 140K word balanced subset of the Brown Corpus, which we have used for development (Sampson, 1995). Much of the remaining data consists of phrasal fragments marked as independent text sentences, for example in dialogue. Grammatical coverage includes the majority of construction types of English, however the handling of some unbounded dependency constructions, particularly comparatives and equatives, is limited because of the lack of fine-grained subcategorization information in the PoS tags and by the need to balance depth of analysis against the size of the derivation space. On the Susanne corpus, the geometric mean of the number of analyses for a sentence of length n is 1.31&apos;. The microa</context>
</contexts>
<marker>Sampson, 1995</marker>
<rawString>Sampson, G.. 1995. English for the Computer. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Watson</author>
</authors>
<title>Part-of-speech tagging models for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 9th Conference of Computational Linguistics in the UK (CLUK’06),</booktitle>
<institution>Open University, Milton Keynes.</institution>
<contexts>
<context position="8228" citStr="Watson, 2006" startWordPosition="1268" endWordPosition="1269">s (GRs) between lexical heads and their dependents. The full system can be extended in a variety of ways – for example, by pruning PoS tags but allowing multiple tag possibilities per word as input to the parser, by incorporating lexical subcategorization into parse selection, by computing GR weights based on the proportion and probability of the n-best analyses yielding them, and so forth – broadly trading accuracy and greater domaindependence against speed and reduced sensitivity to domain-specific lexical behaviour (Briscoe and Carroll, 2002; Carroll and Briscoe, 2002; Watson et al., 2005; Watson, 2006). However, in this paper we focus exclusively on the baseline unlexicalized system. 2.2 Grammar Development The grammar is expressed in a feature-based, unification formalism. There are currently 676 phrase structure rule schemata, 15 feature propagation rules, 30 default feature value rules, 22 category expansion rules and 41 feature types which together define 1124 compiled phrase structure rules in which categories are represented as sets of fea42 tures, that is, attribute-value pairs, possibly with variable values, possibly bound between mother and one or more daughter categories. 142 of t</context>
</contexts>
<marker>Watson, 2006</marker>
<rawString>Watson, R.. 2006. Part-of-speech tagging models for parsing. In Proceedings of the 9th Conference of Computational Linguistics in the UK (CLUK’06), Open University, Milton Keynes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Watson</author>
<author>J Carroll</author>
<author>E J Briscoe</author>
</authors>
<title>Efficient extraction of grammatical relations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Int. Workshop on Parsing Technologies (IWPT’05),</booktitle>
<location>Vancouver, Ca..</location>
<contexts>
<context position="8213" citStr="Watson et al., 2005" startWordPosition="1264" endWordPosition="1267"> grammatical relations (GRs) between lexical heads and their dependents. The full system can be extended in a variety of ways – for example, by pruning PoS tags but allowing multiple tag possibilities per word as input to the parser, by incorporating lexical subcategorization into parse selection, by computing GR weights based on the proportion and probability of the n-best analyses yielding them, and so forth – broadly trading accuracy and greater domaindependence against speed and reduced sensitivity to domain-specific lexical behaviour (Briscoe and Carroll, 2002; Carroll and Briscoe, 2002; Watson et al., 2005; Watson, 2006). However, in this paper we focus exclusively on the baseline unlexicalized system. 2.2 Grammar Development The grammar is expressed in a feature-based, unification formalism. There are currently 676 phrase structure rule schemata, 15 feature propagation rules, 30 default feature value rules, 22 category expansion rules and 41 feature types which together define 1124 compiled phrase structure rules in which categories are represented as sets of fea42 tures, that is, attribute-value pairs, possibly with variable values, possibly bound between mother and one or more daughter categ</context>
</contexts>
<marker>Watson, Carroll, Briscoe, 2005</marker>
<rawString>Watson, R., J. Carroll and E.J. Briscoe. 2005. Efficient extraction of grammatical relations. In Proceedings of the 9th Int. Workshop on Parsing Technologies (IWPT’05), Vancouver, Ca..</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>