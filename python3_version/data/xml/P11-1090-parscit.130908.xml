<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000980">
<title confidence="0.9954525">
Unsupervised Bilingual Morpheme Segmentation and Alignment with
Context-rich Hidden Semi-Markov Models
</title>
<author confidence="0.712636">
Kristina Toutanova
</author>
<affiliation confidence="0.623039">
Microsoft Research
</affiliation>
<address confidence="0.899085">
Redmond, WA 98502
</address>
<email confidence="0.9658">
kristout@microsoft.com
</email>
<author confidence="0.984743">
Jason Naradowsky*
</author>
<affiliation confidence="0.998766">
Department of Computer Science
University of Massachusetts Amherst
</affiliation>
<address confidence="0.730592">
Amherst, MA 01003
</address>
<email confidence="0.998404">
narad@cs.umass.edu
</email>
<sectionHeader confidence="0.993873" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997865470588235">
This paper describes an unsupervised dynamic
graphical model for morphological segmen-
tation and bilingual morpheme alignment for
statistical machine translation. The model ex-
tends Hidden Semi-Markov chain models by
using factored output nodes and special struc-
tures for its conditional probability distribu-
tions. It relies on morpho-syntactic and lex-
ical source-side information (part-of-speech,
morphological segmentation) while learning a
morpheme segmentation over the target lan-
guage. Our model outperforms a competi-
tive word alignment system in alignment qual-
ity. Used in a monolingual morphological seg-
mentation setting it substantially improves ac-
curacy over previous state-of-the-art models
on three Arabic and Hebrew datasets.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999923857142857">
An enduring problem in statistical machine trans-
lation is sparsity. The word alignment models of
modern MT systems attempt to capture p(ei|fj),
the probability that token ei is a translation of fj.
Underlying these models is the assumption that the
word-based tokenization of each sentence is, if not
optimal, at least appropriate for specifying a concep-
tual mapping between the two languages.
However, when translating between unrelated lan-
guages – a common task – disparate morphological
systems can place an asymmetric conceptual bur-
den on words, making the lexicon of one language
much more coarse. This intensifies the problem of
sparsity as the large number of word forms created
</bodyText>
<footnote confidence="0.961226">
This research was conducted during the author’s internship
at Microsoft Research
</footnote>
<page confidence="0.993316">
895
</page>
<bodyText confidence="0.999768125">
through morphologically productive processes hin-
ders attempts to find concise mappings between con-
cepts.
For instance, Bulgarian adjectives may contain
markings for gender, number, and definiteness. The
following tree illustrates nine realized forms of the
Bulgarian word for red, with each leaf listing the
definite and indefinite markings.
</bodyText>
<tableCaption confidence="0.973982">
Table 1: Bulgarian forms of red
</tableCaption>
<equation confidence="0.46898">
cherven(iq)(iqt) chervena(ta) cherveno(to) cherveni(te)
</equation>
<bodyText confidence="0.99992495">
Contrast this with English, in which this informa-
tion is marked either on the modified word or by sep-
arate function words.
In comparison to a language which isn’t mor-
phologically productive on adjectives, the alignment
model must observe nine times as much data (as-
suming uniform distribution of the inflected forms)
to yield a comparable statistic. In an area of research
where the amount of data available plays a large role
in a system’s overall performance, this sparsity can
be extremely problematic. Further complications are
created when lexical sparsity is compounded with
the desire to build up alignments over increasingly
larger contiguous phrases.
To address this issue we propose an alternative
to word alignment: morpheme alignment, an align-
ment that operates over the smallest meaningful sub-
sequences of words. By striving to keep a direct 1-
to-1 mapping between corresponding semantic units
across languages, we hope to find better estimates
</bodyText>
<figure confidence="0.992037">
Masculine
Feminine
Singular
Root
Neuter
Plural
</figure>
<note confidence="0.9004115">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 895–904,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.983383970588235">
DET ADJ NN
PRN VB INF
VB PRN
cherven i te
gePBeH W Te
the
red
flower
tsvet
gBeT
s
ia
A
h
e
they
sr~d
L)&amp;quot;u
�
Is
want
4
y
nA^
to
��
teach
nw
��
dyr
yu
him
4
y
</figure>
<figureCaption confidence="0.967552142857143">
Figure 1: A depiction of morpheme-level alignment. Here dark lines indicate the more stem-focused alignment
strategy of a traditional word or phrasal alignment model, while thin lines indicate a more fine-grained alignment
across morphemes. In the alignment between English and Bulgarian (a) the morpheme-specific alignment reduces
sparsity in the adjective and noun (red flowers) by isolating the stems from their inflected forms. Despite Arabic
exhibiting templatic morphology, there are still phenomena which can be accounted for with a simpler segmentational
approach. The Arabic alignment (b) demonstrates how the plural marker on English they would normally create
sparsity by being marked in three additional places, two of them inflections in larger wordforms.
</figureCaption>
<bodyText confidence="0.98695875">
for the alignment statistics. Our results show that
this improves alignment quality.
In the following sections we describe an un-
supervised dynamic graphical model approach to
monolingual morphological segmentation and bilin-
gual morpheme alignment using a linguistically mo-
tivated statistical model. In a bilingual setting,
the model relies on morpho-syntactic and lexical
source-side information (part-of-speech, morpho-
logical segmentation, dependency analysis) while
learning a morpheme segmentation over the tar-
get language. In a monolingual setting we intro-
duce effective use of context by feature-rich mod-
eling of the probabilities of morphemes, morpheme-
transitions, and word boundaries. These additional
sources of information provide powerful bias for un-
supervised learning, without increasing the asymp-
totic running time of the inference algorithm.
Used as a monolingual model, our system sig-
nificantly improves the state-of-the-art segmenta-
tion performance on three Arabic and Hebrew data-
sets. Used as a bilingual model, our system out-
performs the state-of-the-art WDHMM (He, 2007)
word alignment model as measured by alignment er-
ror rate (AER).
In agreement with some previous work on to-
kenization/morpheme segmentation for alignment
(Chung and Gildea, 2009; Habash and Sadat, 2006),
we find that the best segmentation for alignment
does not coincide with the gold-standard segmenta-
tion and our bilingual model does not outperform
our monolingual model in segmentation F-Measure.
</bodyText>
<sectionHeader confidence="0.976962" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.998904846153846">
Our model defines the probability of a target lan-
guage sequence of words (each consisting of a se-
quence of morphemes), and alignment from target
to source morphemes, given a source language se-
quence of words (each consisting of a sequence of
morphemes).
An example morpheme segmentation and align-
ment of phrases in English-Arabic and English-
Bulgarian is shown in Figure 1. In our task setting,
the words of the source and target language as well
as the morpheme segmentation of the source (En-
glish) language are given. The morpheme segmen-
tation of the target language and the alignments be-
tween source and target morphemes are hidden.
The source-side input, which we assume to be
English, is processed with a gold morphological
segmentation, part-of-speech, and dependency tree
analysis. While these tools are unavailable in
resource-poor languages, they are often available for
at least one of the modeled languages in common
translation tasks. This additional information then
provides a source of features and conditioning infor-
mation for the translation model.
Our model is derived from the hidden-markov
model for word alignment (Vogel et al., 1996; Och
and Ney, 2000). Based on it, we define a dynamic
</bodyText>
<page confidence="0.999119">
896
</page>
<figureCaption confidence="0.99254025">
Figure 2: A graphical depiction of the model generating
the transliteration of the first Bulgarian word from Figure
1. Trigram dependencies and some incoming/outgoing
arcs have been omitted for clarity.
</figureCaption>
<bodyText confidence="0.999967095238096">
graphical model which lets us encode more lin-
guistic intuition about morpheme segmentation and
alignment: (i) we extend it to a hidden semi-markov
model to account for hidden target morpheme seg-
mentation; (ii) we introduce an additional observa-
tion layer to model observed word boundaries and
thus truly represent target sentences as words com-
posed of morphemes, instead of just a sequence
of tokens; (iii) we employ hierarchically smoothed
models and log-linear models to capture broader
context and to better represent the morpho-syntactic
mapping between source and target languages. (iv)
we enrich the hidden state space of the model to en-
code morpheme types {prefix,suffix,stem}, in ad-
dition to morpheme alignment and segmentation in-
formation.
Before defining our model formally, we introduce
some notation. Each possible morphological seg-
mentation and alignment for a given sentence pair
can be described by the following random variables:
Let µ1µ2 ... µI denote I morphemes in the seg-
mentation of the target sentence. For the Example
in Figure 1 (a) I=5 and µ1=cherven, µ2=i ... , and
µ5=ia. Let b1, b2,... bI denote Bernoulli variables
indicating whether there is a word boundary after
morpheme µi. For our example, b3 = 1, b5 = 1,
and the other bi are 0. Let c1,c2,...,cT denote
the non-space characters in the target string, and
wb1, ... , wbT denote Bernoulli variables indicating
whether there is a word boundary after the corre-
sponding target character. For our example, T = 14
(for the Cyrillic version) and the only wb variables
that are on are wb9 and wb14. The c and wb vari-
ables are observed. Let s1s2 ... sT denote Bernoulli
segmentation variables indicating whether there is a
morpheme boundary after the corresponding char-
acter. The values of the hidden segmentation vari-
ables s together with the values of the observed c
and wb variables uniquely define the values of the
morpheme variables µi and the word boundary vari-
ables bi. Naturally we enforce the constraint that
a given word boundary wbt = 1 entails a segmen-
tation boundary st = 1. If we use bold letters
to indicate a vector of corresponding variables, we
have that c, wb, s=µ, b. We will define the assumed
parametric form of the learned distribution using the
µ, b but the inference algorithms are implemented
in terms of the s and wb variables.
We denote the observed source language mor-
phemes by e1 ... eJ. Our model makes use of ad-
ditional information from the source which we will
mention when necessary.
The last part of the hidden model state repre-
sents the alignment between target and source mor-
phemes and the type of target morphemes. Let
tai = [ai, ti], i = 1... I indicate a factored state
where ai represents one of the J source words (or
NULL) and ti represents one of the three morpheme
types {prefix,suffix,stem}. ai is the source mor-
pheme aligned to µi and ti is the type of µi.
We are finally ready to define the desired proba-
bility of target morphemes, morpheme types, align-
ments, and word boundaries given source:
</bodyText>
<equation confidence="0.995056">
I
P(µ, ta, b|e) = PT (µi|tai, bi−1, bi−2, µi−1, e)
i=1
· PB(bi|µi, µi−1, tai, bi−1, bi−2, e)
· PD(tai|tai−1, bi−1, e) · LP(|µi|)
</equation>
<bodyText confidence="0.9999705">
We now describe each of the factors used by our
model in more detail. The formulation makes ex-
plicit the full extent of dependencies we have ex-
plored in this work. By simplifying the factors
</bodyText>
<equation confidence="0.952357136363636">
the red flower
s
a1
a2 a3
= 2 = 4 = 1
t1
t2 t3
= stem
= sufÞx = sufÞx
μ1
= &apos;cherven&apos;
μ2
= &apos;i&apos;
μ3
= &apos;te&apos;
b1
b2
b3
= OFF
= OFF
= ON
cherven.i.te
</equation>
<page confidence="0.972937">
897
</page>
<bodyText confidence="0.998495">
we can recover several previously used models for
monolingual segmentation and bilingual joint seg-
mentation and alignment. We discuss the relation-
ship of this model to prior work and study the impact
of the novel components in our experiments.
When the source sentence is assumed to be empty
(and thus contains no morphemes to align to) our
model turns into a monolingual morpheme segmen-
tation model, which we show exceeds the perfor-
mance of previous state-of-the-art models. When we
remove the word boundary component, reduce the
order of the alignment transition, omit the morpho-
logical type component of the state space, and retain
only minimal dependencies in the morpheme trans-
lation model, we recover the joint tokenization and
alignment model based on IBM Model-1 proposed
by (Chung and Gildea, 2009).
</bodyText>
<subsectionHeader confidence="0.957056">
2.1 Morpheme Translation Model
</subsectionHeader>
<bodyText confidence="0.99954985">
In the model equation, PT denotes the morpheme
translation probability. The standard dependence on
the aligned source morpheme is represented as a de-
pendence on the state tai and the whole annotated
source sentence e. We experimented with multiple
options for the amount of conditioning context to be
included. When most context is used, there is a bi-
gram dependency of target language morphemes as
well as dependence on two previous boundary vari-
ables and dependence on the aligned source mor-
pheme eai as well as its POS tag.
When multiple conditioning variables are used we
assume a special linearly interpolated backoff form
of the model, similar to models routinely used in lan-
guage modeling.
As an example, suppose we estimate the mor-
pheme translation probability as PT (pi|eai, ti). We
estimate this in the M-step, given expected joint
counts 4i, eai, ti) and marginal counts derived
from these as follows:
</bodyText>
<equation confidence="0.5202375">
c(µi ,eai ,ti)+α2 P2 (µi  |ti )
PT (pi  |eai, ti) = c(eai,ti)+α2
</equation>
<bodyText confidence="0.998727">
The lower order distributions are estimated recur-
sively in a similar way:
</bodyText>
<equation confidence="0.99069325">
P2(pi|ti) = c(µi,ti)+α�P�(µi)
c(ti)+α�
P1(pi) = c(µi)+α0P0(µi)
c(.)+α0
</equation>
<bodyText confidence="0.997296333333333">
For P0 we used a unigram character language
model. This hierarchical smoothing can be seen
as an approximation to hierarchical Dirichlet priors
with maximum aposteriori estimation.
Note how our explicit treatment of word bound-
ary variables bi allows us to use a higher order de-
pendence on these variables. If word boundaries are
treated as morphemes on their own, we would need
to have a four-gram model on target morphemes to
represent this dependency which we are now repre-
senting using only a bigram model on hidden mor-
phemes.
</bodyText>
<subsectionHeader confidence="0.999199">
2.2 Word Boundary Generation Model
</subsectionHeader>
<bodyText confidence="0.999938151515152">
The PB distribution denotes the probability of gen-
erating word boundaries. As a sequence model of
sentences the basic hidden semi-markov model com-
pletely ignores word boundaries. However, they can
be powerful predictors of morpheme segments (by
for example, indicating that common prefixes fol-
low word boundaries, or that common suffixes pre-
cede them). The log-linear model of (Poon et al.,
2009) uses word boundaries as observed left and
right context features, and Morfessor (Creutz and
Lagus, 2007) includes boundaries as special bound-
ary symbols which can inform about the morpheme
state of a morpheme (but not its identity).
Our model includes a special generative process
for boundaries which is conditioned not only on the
previous morpheme state but also the previous two
morphemes and other boundaries. Due to the fact
that boundaries are observed their inclusion in the
model does not increase the complexity of inference.
The inclusion of this distribution lets us estimate
the likelihood of a word consisting of one,two,three,
or more morphemes. It also allows the estimation of
likelihood that particular morphemes are in the be-
ginning/middle/end of words. Through the included
factored state variable tai word boundaries can also
inform about the likelihood of a morpheme aligned
to a source word of a particular pos tag to end a
word. We discuss the particular conditioning con-
text for this distribution we found most helpful in
our experiments.
Similarly to the PT distribution, we make use of
multiple context vectors by hierarchical smoothing
of distributions of different granularities.
</bodyText>
<page confidence="0.992001">
898
</page>
<subsectionHeader confidence="0.864196">
2.3 Distortion Model
</subsectionHeader>
<bodyText confidence="0.999844441860465">
PD indicates the distortion modeling distribution
we use. 1 Traditional distortion models represent
P(aj|aj_1i e), the probability of an alignment given
the previous alignment, to bias the model away from
placing large distances between the aligned tokens
of consecutively sequenced tokens. In addition to
modeling a larger state space to also predict mor-
pheme types, we extend this model by using a spe-
cial log-linear model form which allows the integra-
tion of rich morpho-syntactic context. Log-linear
models have been previously used in unsupervised
learning for local multinomial distributions like this
one in e.g. (Berg-Kirkpatrick et al., 2010), and for
global distributions in (Poon et al., 2009).
The special log-linear form allows the inclusion
of features targeted at learning the transitions among
morpheme types and the transitions between corre-
sponding source morphemes. The set of features
with example values for this model is depicted in
Table 3. The example is focussed on the features
firing for the transition from the Bulgarian suffix
te aligned to the first English morpheme µi_1 =
te, ti_1=suffix, ai_1=1, to the Bulgarian root tsvet
aligned to the third English morpheme µi = tsvet,
ti=root, ai=3. The first feature is the absolute dif-
ference between ai and ai_1 + 1 and is similar to
information used in other HMM word alignment
models (Och and Ney, 2000) as well as phrase-
translation models (Koehn, 2004). The alignment
positions ai are defined as indices of the aligned
source morphemes. We additionally compute distor-
tion in terms of distance in number of source words
that are skipped. This distance corresponds to the
feature name WORD DISTANCE. Looking at both
kinds of distance is useful to capture the intuition
that consecutive morphemes in the same target word
should prefer to have a higher proximity of their
aligned source words, as compared to consecutive
morphemes which are not part of the same target
word. The binned distances look at the sign of the
distortion and bin the jumps into 5 bins, pooling the
distances greater than 2 together. The feature SAME
TARGET WORD indicates whether the two consecu-
</bodyText>
<footnote confidence="0.914691333333333">
1To reduce complexity of exposition we have omitted the
final transition to a special state beyond the source sentence end
after the last target morpheme.
</footnote>
<table confidence="0.996543181818182">
Feature Value
MORPH DISTANCE 1
WORD DISTANCE 1
BINNED MORPH DISTANCE fore1
BINNED WORD DISTANCE fore1
MORPH STATE TRANSITION suffix-root
SAME TARGET WORD False
POS TAG TRANSITION DET-NN
DEP RELATION DET+—NN
NULL ALIGNMENT False
conjunctions ...
</table>
<figureCaption confidence="0.997286">
Figure 3: Features in log-linear distortion model firing
for the transition from te:suffix:1 to tsvet:root:3 in the
example sentence pair in Figure 1a.
</figureCaption>
<bodyText confidence="0.9999272">
tive morphemes are part of the same word. In this
case, they are not. This feature is not useful on its
own because it does not distinguish between differ-
ent alignment possibilities for tai, but is useful in
conjunction with other features to differentiate the
transition behaviors within and across target words.
The DEP RELATION feature indicates the direct de-
pendency relation between the source words con-
taining the aligned source morphemes, if such rela-
tionship exists. We also represent alignments to null
and have one null for each source word, similarly to
(Och and Ney, 2000) and have a feature to indicate
null. Additionally, we make use of several feature
conjunctions involving the null, same target word,
and distance features.
</bodyText>
<subsectionHeader confidence="0.997231">
2.4 Length Penalty
</subsectionHeader>
<bodyText confidence="0.9999536">
Following (Chung and Gildea, 2009) and (Liang and
Klein, 2009) we use an exponential length penalty
on morpheme lengths to bias the model away from
the maximum likelihood under-segmentation solu-
tion. The form of the penalty is:
</bodyText>
<equation confidence="0.939004">
LP(|µi|) = 1
e,µ�,lp
</equation>
<bodyText confidence="0.996770666666667">
Here lp is a hyper-parameter indicating the power
that the morpheme length is raised to. We fit this pa-
rameter using an annotated development set, to op-
timize morpheme-segmentation F1. The model is
extremely sensitive to this value and performs quite
poorly if such penalty is not used.
</bodyText>
<sectionHeader confidence="0.684445" genericHeader="background">
2.5 Inference
</sectionHeader>
<bodyText confidence="0.9992125">
We perform inference by EM training on the aligned
sentence pairs. In the E-step we compute expected
</bodyText>
<page confidence="0.978371">
899
</page>
<bodyText confidence="0.991483290322581">
counts of all hidden variable configurations that are 3 Evaluation
relevant for our model. In the M-step we re-estimate For a majority of our testing we borrow the paral-
the model parameters (using LBFGS in the M-step lel phrases corpus used in previous work (Snyder
for the distortion model and using count interpola- and Barzilay, 2008), which we refer to as S&amp;B.
tion for the translation and word-boundary models). The corpus consists of 6,139 short phrases drawn
The computation of expectations in the E-step from English, Hebrew, and Arabic translations of
is of the same order as an order two semi-markov the Bible. We use an unmodified version of this
chain model using hidden state labels of cardinality corpus for the purpose of comparing morphological
(J x 3 = number of source morphemes times num- segmentation accuracy. For evaluating morpheme
ber of target morpheme types). The running time alignment accuracy, we have also augmented the En-
of the forward and backward dynamic programming glish/Arabic subset of the corpus with a gold stan-
passes is T x l2 x (3J)2, where T is the length of dard alignment between morphemes. Here mor-
the target sentence in characters, J is the number phological segmentations were obtained using the
of source morphemes, and l is the maximum mor- previously-annotated gold standard Arabic morpho-
pheme length. Space does not permit the complete logical segmentation, while the English was prepro-
listing of the dynamic programming solution but it cessed with a morphological analyzer and then fur-
is not hard to derive by starting from the dynamic ther hand annotated with corrections by two native
program for the IBM-1 like tokenization model of speakers. Morphological alignments were manually
(Chung and Gildea, 2009) and extending it to ac- annotated. Additionally, we evaluate monolingual
count for the higher order on morphemes and the segmentation models on the full Arabic Treebank
factored alignment state space. (ATB), also used for unsupervised morpheme seg-
Even though the inference algorithm is low poly- mentation in (Poon et al., 2009).
nomial it is still much more expensive than the infer- 4 Results
ence for an HMM model for word-alignment with- 4.1 Morpheme Segmentation
out segmentation. To reduce the running time of the We begin by evaluating a series of models which are
model we limit the space of considered morpheme simplifications of our complete model, to assess the
boundaries as follows: impact of individual modeling decisions. We focus
Given the target side of the corpus, we derive a first on a monolingual setting, where the source sen-
list of K most frequent prefixes and suffixes using a tence aligned to each target sentence is empty.
simple trie-based method proposed by (Schone and Unigram Model with Length Penalty
Jurafsky, 2000).2 After we determine a list of al- The first model we study is the unigram mono-
lowed prefixes and suffixes we restrict our model to lingual segmentation model using an exponential
allow only segmentations of the form: ((p*)r(s*))+ length penalty as proposed by (Liang and Klein,
where p and s belong to the allowed prefixes and 2009; Chung and Gildea, 2009), which has been
suffixes and r can match any substring. shown to be quite accurate. We refer to this model as
We determine the number of prefixes and suffixes Model-UP (for unigram with penalty). It defines the
to consider using the maximum recall achievable by probability of a target morpheme sequence as fol-
limiting the segmentation points in this way. Re- lows: (p1 ... pI) = (1 − 0) rIi�1 0PT(pi)LP(JpiJ)
stricting the allowable segmentations in this way not This model can be (almost) recovered as a spe-
only improves the speed of inference but also leads cial case of our full model, if we drop the transition
to improvements in segmentation accuracy. and word boundary probabilities, do not model mor-
pheme types, and use no conditioning for the mor-
pheme translation model. The only parameter not
present in our model is the probability 0 of gener-
ating a morpheme as opposed to stopping to gener-
2Words are inserted into a trie with each complete branch
naturally identifying a potential suffix, inclusive of its sub-
branches. The list comprises of the K most frequent of these
complete branches. Inserting the reversed words will then yield
potential prefixes.
900
ate morphemes (with probability 1 − 0). We exper-
imented with this additional parameter, but found it
had no significant impact on performance, and so we
do not report results including it.
We select the value of the length penalty power
by a gird search in the range 1.1 to 2.0, using .1 in-
crements and choosing the values resulting in best
performance on a development set containing 500
phrase pairs for each language. We also select the
optimal number of prefixes/suffixes to consider by
measuring performance on the development set. 3
</bodyText>
<subsectionHeader confidence="0.831926">
Morpheme Type Models
</subsectionHeader>
<bodyText confidence="0.999979727272727">
The next model we consider is similar to the un-
igram model with penalty, but introduces the use
of the hidden ta states which indicate only mor-
pheme types in the monolingual setting. We use
the ta states and test different configurations to de-
rive the best set of features that can be used in the
distortion model utilizing these states, and the mor-
pheme translation model. We consider two vari-
ants: (1) Model-HMMP-basic (for HMM model
with length penalty), which includes the hidden
states but uses them with a simple uniform transition
matrix P (tai|tai−1,bi−1) (uniform over allowable
transitions but forbidding the prefixes from transi-
tioning directly to suffixes, and preventing suffixes
from immediately following a word boundary), and
(2) a richer model Model-HMMP which is allowed
to learn a log-linear distortion model and a feature
rich translation model as detailed in the model defi-
nition. This model is allowed to use word boundary
information for conditioning (because word bound-
aries are observed), but does not include the PB pre-
dictive word boundary distribution.
</bodyText>
<subsectionHeader confidence="0.988357">
Full Model with Word Boundaries
</subsectionHeader>
<bodyText confidence="0.999974833333333">
Finally we consider our full monolingual model
which also includes the distribution predicting word
boundary variables bi. We term this model Model-
FullMono. We detail the best context features for
the conditional PD distribution for each language.
We initialize this model with the morpheme trans-
</bodyText>
<tableCaption confidence="0.621262">
3For the S&amp;B Arabic dataset, we selected to use seven pre-
fixes and seven suffixes, which correspond to maximum achiev-
able recall of 95.3. For the S&amp;B Hebrew dataset, we used six
prefixes and six suffixes, for a maximum recall of 94.3. The
Arabic treebank data required a larger number of affixes: we
used seven prefixes and 20 suffixes, for a maximum recall of
98.3.
</tableCaption>
<bodyText confidence="0.996548386363637">
lation unigram distribution of ModelHMMP-basic,
trained for 5 iterations.
Table 4 details the test set results of the different
model configurations, as well as previously reported
results on these datasets. For our main results we use
the automatically derived list of prefixes and suffixes
to limit segmentation points. The names of models
that use such limited lists are prefixed by Dict in the
Table. For comparison, we also report the results
achieved by models that do not limit the segmenta-
tion points in this way.
As we can see the unigram model with penalty,
Dict-Model-UP, is already very strong, especially
on the S&amp;B Arabic dataset. When the segmenta-
tion points are not limited, its performance is much
worse. The introduction of hidden morpheme states
in Dict-HMMP-basic gives substantial improvement
on Arabic and does not change results much on the
other datasets. A small improvement is observed
for the unconstrained models.4 When our model in-
cludes all components except word boundary pre-
diction, Dict-Model-HMMP, the results are substan-
tially improved on all languages. Model-HMMP is
also the first unconstrained model in our sequence
to approach or surpass previous state-of-the-art seg-
mentation performance.
Finally, when the full model Dict-MonoFull is
used, we achieve a substantial improvement over
the previous state-of-the-art results on all three cor-
pora, a 6.5 point improvement on Arabic, 6.2 point
improvement on Hebrew, and a 9.3 point improve-
ment on ATB. The best configuration of this model
uses the same distortion model for all languages: us-
ing the morph state transition and boundary features.
The translation models used only ti for Hebrew and
ATB and ti and µi−1 for Arabic. Word bound-
ary was predicted using ti in Arabic and Hebrew,
and additionally using bi−1 and bi−2 for ATB. The
unconstrained models without affix dictionaries are
also very strong, outperforming previous state-of-
the-art models. For ATB, the unconstrained model
slightly outperforms the constrained one.
The segmentation errors made by this system shed
light on how it might be improved. We find the dis-
</bodyText>
<footnote confidence="0.993488">
4Note that the inclusion of states in HMMP-basic only
serves to provide a different distribution over the number of
morphemes in a word, so it is interesting it can have a positive
impact.
</footnote>
<page confidence="0.985576">
901
</page>
<table confidence="0.999798846153846">
P Arabic P Hebrew P ATB F1
R F1 R F1 R
UP 88.1 55.1 67.8 43.2 87.6 57.9 79.0 54.6 64.6
Dict-UP 85.8 73.1 78.9 57.0 79.4 66.3 61.6 91.0 73.5
HMMP-basic 83.3 58.0 68.4 43.5 87.8 58.2 79.0 54.9 64.8
Dict-HMMP-basic 84.8 76.3 80.3 56.9 78.8 66.1 69.3 76.2 72.6
HMMP 73.6 76.9 75.2 70.2 73.0 71.6 94.0 76.1 84.1
Dict-HMMP 82.4 81.3 81.8 62.7 77.6 69.4 85.2 85.8 85.5
MonoFull 80.5 87.3 83.8 72.2 71.7 72.0 86.2 88.5 87.4
Dict-MonoFull 86.1 83.2 84.6 73.7 72.5 73.1 92.9 81.8 87.0
Poon et. al 76.0 80.2 78.1 67.6 66.1 66.9 88.5 69.2 77.7
S&amp;B-Best 67.8 77.3 72.2 64.9 62.9 63.9 – – –
Morfessor 71.1 60.5 65.4 65.4 57.7 61.3 77.4 72.6 74.9
</table>
<figureCaption confidence="0.995195666666667">
Figure 4: Results on morphological segmentation achieved by monolingual variants of our model (top) with results
from prior work are included for comparison (bottom). Results from models with a small, automatically-derived list
of possible prefixes and suffixes are labeled as ”Dict-” followed by the model name.
</figureCaption>
<bodyText confidence="0.999877896551724">
tributions over the frequencies of particular errors
follow a Zipfian skew across both S&amp;B datasets,
with the Arabic being more pronounced (the most
frequent error being made 27 times, with 627 er-
rors being made just once) in comparison with the
Hebrew (with the most frequent error being made
19 times, and with 856 isolated errors). However,
in both the Arabic and Hebrew S&amp;B tasks we find
that a tendency to over-segment certain characters
off of their correct morphemes and on to other fre-
quently occurring, yet incorrect, particles is actually
the cause of many of these isolated errors. In Ara-
bic the system tends to over segment the character
aleph (totally about 300 errors combined). In He-
brew the source of error is not as overwhelmingly
directed at a single character, but yod and he, the
latter functioning quite similarly to the problematic
Arabic character and frequently turn up in the corre-
sponding places of cognate words in Biblical texts.
We should note that our models select a large
number of hyper-parameters on an annotated devel-
opment set, including length penalty, hierarchical
smoothing parameters α, and the subset of variables
to use in each of three component sub-models. This
might in part explain their advantage over previous-
state-of-the-art models, which might use fewer (e.g.
(Poon et al., 2009) and (Snyder and Barzilay, 2008))
or no specifically tuned for these datasets hyper-
parameters (Morfessor (Creutz and Lagus, 2007)).
</bodyText>
<subsectionHeader confidence="0.924643">
4.2 Alignment
</subsectionHeader>
<bodyText confidence="0.999828733333333">
Next we evaluate our full bilingual model and a sim-
pler variant on the task of word alignment. We use
the morpheme-level annotation of the S&amp;B English-
Arabic dataset and project the morpheme alignments
to word alignments. We can thus compare align-
ment performance of the results of different segmen-
tations. Additionally, we evaluate against a state-
of-the-art word alignment system WDHMM (He,
2007), which performs comparably or better than
IBM-Model4. The table in Figure 5 presents the re-
sults. In addition to reporting alignment error rate
for different segmentation models, we report their
morphological segmentation F1.
The word-alignment WDHMM model performs
best when aligning English words to Arabic words
(using Arabic as source). In this direction it is
able to capture the many-to-one correspondence be-
tween English words and arabic morphemes. When
we combine alignments in both directions using the
standard grow-diag-final method, the error goes up.
We compare the (Chung and Gildea, 2009) model
(termed Model-1) to our full bilingual model. We
can recover Model-1 similarly to Model-UP, except
now every morpheme is conditioned on an aligned
source morpheme. Our full bilingual model outper-
forms Model-1 in both AER and segmentation F1.
The specific form of the full model was selected as
in the previous experiments, by choosing the model
with best segmentations of the development set.
For Arabic, the best model conditions target mor-
</bodyText>
<page confidence="0.989665">
902
</page>
<table confidence="0.999345833333333">
Align P Arabic P R F1 P Hebrew
Align R AER R F1
Model-1 (C&amp;G 09) 91.6 81.2 13.9 72.4 76.2 74.3 61.0 71.8 65.9
Bilingual full 91.0 88.3 10.3 90.0 72.0 80.0 63.3 71.2 67.0
WDHMM E-to-A 82.4 96.7 11.1
WDHMM GDF 82.1 94.6 12.1
</table>
<figureCaption confidence="0.996892">
Figure 5: Alignment Error Rate (AER) and morphological segmentation F1 achieved by bilingual variants of our
model. AER performance of WDHMM is also reported. Gold standard alignments are not available for the Hebrew
data set.
</figureCaption>
<bodyText confidence="0.996477964285714">
phemes on source morphemes only, uses the bound-
ary model with conditioning on number of mor-
phemes in the word, aligned source part-of-speech,
and type of target morpheme. The distortion model
uses both morpheme and word-based absolute dis-
tortion, binned distortion, morpheme types of states,
and aligned source-part-of-speech tags. Our best
model for Arabic outperforms WDHMM in word
alignment error rate. For Hebrew, the best model
uses a similar boundary model configuration but a
simpler uniform transition distortion distribution.
Note that the bilingual models perform worse than
the monolingual ones in segmentation F1. This
finding is in line with previous work showing that
the best segmentation for MT does not necessarily
agree with a particular linguistic convention about
what morphemes should contain (Chung and Gildea,
2009; Habash and Sadat, 2006), but contradicts
other results (Snyder and Barzilay, 2008). Further
experimentation is required to make a general claim.
We should note that the Arabic dataset used
for word-alignment evaluation is unconventionally
small and noisy (the sentences are very short
phrases, automatically extracted using GIZA++).
Thus the phrases might not be really translations,
and the sentence length is much smaller than in stan-
dard parallel corpora. This warrants further model
evaluation in a large-scale alignment setting.
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999979576923077">
This work is most closely related to the unsupervised
tokenization and alignment models of Chung and
Gildea (2009), Xu et al. (2008), Snyder and Barzilay
(2008), and Nguyen et al. (2010).
Chung &amp; Gildea (2009) introduce a unigram
model of tokenization based on IBM Model-1,which
is a special case of our model. Snyder and Barzi-
lay (2008) proposes a hierarchical Bayesian model
that combines the learning of monolingual segmen-
tations and a cross-lingual alignment; their model is
very different from ours.
Incorporating morphological information into
MT has received reasonable attention. For exam-
ple, Goldwater &amp; McClosky (2005) show improve-
ments when preprocessing Czech input to reflect
a morphological decomposition using combinations
of lemmatization, pseudowords, and morphemes.
Yeniterzi and Oflazer (2010) bridge the morpholog-
ical disparity between languages in a unique way
by effectively aligning English syntactic elements
(function words connected by dependency relations)
to Turkish morphemes, using rule-based postpro-
cessing of standard word alignment. Our work is
partly inspired by that work and attempts to auto-
mate both the morpho-syntactic alignment and mor-
phological analysis tasks.
</bodyText>
<sectionHeader confidence="0.999578" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999953">
We have described an unsupervised model for mor-
pheme segmentation and alignment based on Hid-
den Semi-Markov Models. Our model makes use
of linguistic information to improve alignment qual-
ity. On the task of monolingual morphological seg-
mentation it produces a new state-of-the-art level on
three datasets. The model shows quantitative im-
provements in both word segmentation and word
alignment, but its true potential lies in its finer-
grained interpretation of word alignment, which will
hopefully yield improvements in translation quality.
</bodyText>
<sectionHeader confidence="0.996925" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99992625">
We thank the ACL reviewers for their valuable
comments on earlier versions of this paper, and
Michael J. Burling for his contributions as a corpus
annotator and to the Arabic aspects of this paper.
</bodyText>
<page confidence="0.998354">
903
</page>
<sectionHeader confidence="0.995855" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997492">
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cote,
John DeNero, and Dan Klein. 2010. Unsupervised
learning with features. In Proceedings of the North
American chapter of the Association for Computa-
tional Linguistics (NAACL).
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Trans. Speech Lang. Process.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
North American Chapter of the Association for Com-
putational Linguistics.
Xiaodong He. 2007. Using word-dependent transition
models in HMM based word alignment for statistical
machine translation. In ACL 2nd Statistical MT work-
shop, pages 80–87.
Philip Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA.
P. Liang and D. Klein. 2009. Online EM for unsu-
pervised models. In North American Association for
Computational Linguistics (NAACL).
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for machine
translation. In Proceedings of the International Con-
ference on Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In In Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In North American Chap-
ter of the Association for Computation Linguistics
- Human Language Technologies 2009 conference
(NAACL/HLT-09).
Patrick Schone and Daniel Jurafsky. 2000. Knowlege-
free induction of morphology using latent semantic
analysis. In Proceedings of the Conference on Compu-
tational Natural Language Learning (CoNLL-2000).
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In ACL.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In In COLING 96: The 16th Int. Conf. on Com-
putational Linguistics.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann
Ney. 2008. Bayesian semi-supervised chinese word
segmentation for statistical machine translation. In
COLING.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from english to turkish. In
Proceedings of Association of Computational Linguis-
tics.
</reference>
<page confidence="0.998546">
904
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.870721">
<title confidence="0.985129">Unsupervised Bilingual Morpheme Segmentation and Alignment Context-rich Hidden Semi-Markov Models</title>
<author confidence="0.965542">Kristina</author>
<affiliation confidence="0.938749">Microsoft</affiliation>
<address confidence="0.99088">Redmond, WA</address>
<email confidence="0.99995">kristout@microsoft.com</email>
<affiliation confidence="0.99991">Department of Computer University of Massachusetts</affiliation>
<address confidence="0.994667">Amherst, MA</address>
<email confidence="0.99916">narad@cs.umass.edu</email>
<abstract confidence="0.999278277777778">This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cote</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="15590" citStr="Berg-Kirkpatrick et al., 2010" startWordPosition="2474" endWordPosition="2477">ortion modeling distribution we use. 1 Traditional distortion models represent P(aj|aj_1i e), the probability of an alignment given the previous alignment, to bias the model away from placing large distances between the aligned tokens of consecutively sequenced tokens. In addition to modeling a larger state space to also predict morpheme types, we extend this model by using a special log-linear model form which allows the integration of rich morpho-syntactic context. Log-linear models have been previously used in unsupervised learning for local multinomial distributions like this one in e.g. (Berg-Kirkpatrick et al., 2010), and for global distributions in (Poon et al., 2009). The special log-linear form allows the inclusion of features targeted at learning the transitions among morpheme types and the transitions between corresponding source morphemes. The set of features with example values for this model is depicted in Table 3. The example is focussed on the features firing for the transition from the Bulgarian suffix te aligned to the first English morpheme µi_1 = te, ti_1=suffix, ai_1=1, to the Bulgarian root tsvet aligned to the third English morpheme µi = tsvet, ti=root, ai=3. The first feature is the abso</context>
</contexts>
<marker>Berg-Kirkpatrick, Bouchard-Cote, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cote, John DeNero, and Dan Klein. 2010. Unsupervised learning with features. In Proceedings of the North American chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Unsupervised tokenization for machine translation.</title>
<date>2009</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5652" citStr="Chung and Gildea, 2009" startWordPosition="828" endWordPosition="831">emes, morphemetransitions, and word boundaries. These additional sources of information provide powerful bias for unsupervised learning, without increasing the asymptotic running time of the inference algorithm. Used as a monolingual model, our system significantly improves the state-of-the-art segmentation performance on three Arabic and Hebrew datasets. Used as a bilingual model, our system outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER). In agreement with some previous work on tokenization/morpheme segmentation for alignment (Chung and Gildea, 2009; Habash and Sadat, 2006), we find that the best segmentation for alignment does not coincide with the gold-standard segmentation and our bilingual model does not outperform our monolingual model in segmentation F-Measure. 2 Model Our model defines the probability of a target language sequence of words (each consisting of a sequence of morphemes), and alignment from target to source morphemes, given a source language sequence of words (each consisting of a sequence of morphemes). An example morpheme segmentation and alignment of phrases in English-Arabic and EnglishBulgarian is shown in Figure</context>
<context position="11589" citStr="Chung and Gildea, 2009" startWordPosition="1838" endWordPosition="1841">tudy the impact of the novel components in our experiments. When the source sentence is assumed to be empty (and thus contains no morphemes to align to) our model turns into a monolingual morpheme segmentation model, which we show exceeds the performance of previous state-of-the-art models. When we remove the word boundary component, reduce the order of the alignment transition, omit the morphological type component of the state space, and retain only minimal dependencies in the morpheme translation model, we recover the joint tokenization and alignment model based on IBM Model-1 proposed by (Chung and Gildea, 2009). 2.1 Morpheme Translation Model In the model equation, PT denotes the morpheme translation probability. The standard dependence on the aligned source morpheme is represented as a dependence on the state tai and the whole annotated source sentence e. We experimented with multiple options for the amount of conditioning context to be included. When most context is used, there is a bigram dependency of target language morphemes as well as dependence on two previous boundary variables and dependence on the aligned source morpheme eai as well as its POS tag. When multiple conditioning variables are</context>
<context position="18430" citStr="Chung and Gildea, 2009" startWordPosition="2936" endWordPosition="2939">sibilities for tai, but is useful in conjunction with other features to differentiate the transition behaviors within and across target words. The DEP RELATION feature indicates the direct dependency relation between the source words containing the aligned source morphemes, if such relationship exists. We also represent alignments to null and have one null for each source word, similarly to (Och and Ney, 2000) and have a feature to indicate null. Additionally, we make use of several feature conjunctions involving the null, same target word, and distance features. 2.4 Length Penalty Following (Chung and Gildea, 2009) and (Liang and Klein, 2009) we use an exponential length penalty on morpheme lengths to bias the model away from the maximum likelihood under-segmentation solution. The form of the penalty is: LP(|µi|) = 1 e,µ�,lp Here lp is a hyper-parameter indicating the power that the morpheme length is raised to. We fit this parameter using an annotated development set, to optimize morpheme-segmentation F1. The model is extremely sensitive to this value and performs quite poorly if such penalty is not used. 2.5 Inference We perform inference by EM training on the aligned sentence pairs. In the E-step we </context>
<context position="20812" citStr="Chung and Gildea, 2009" startWordPosition="3326" endWordPosition="3329">e morthe target sentence in characters, J is the number phological segmentations were obtained using the of source morphemes, and l is the maximum mor- previously-annotated gold standard Arabic morphopheme length. Space does not permit the complete logical segmentation, while the English was preprolisting of the dynamic programming solution but it cessed with a morphological analyzer and then furis not hard to derive by starting from the dynamic ther hand annotated with corrections by two native program for the IBM-1 like tokenization model of speakers. Morphological alignments were manually (Chung and Gildea, 2009) and extending it to ac- annotated. Additionally, we evaluate monolingual count for the higher order on morphemes and the segmentation models on the full Arabic Treebank factored alignment state space. (ATB), also used for unsupervised morpheme segEven though the inference algorithm is low poly- mentation in (Poon et al., 2009). nomial it is still much more expensive than the infer- 4 Results ence for an HMM model for word-alignment with- 4.1 Morpheme Segmentation out segmentation. To reduce the running time of the We begin by evaluating a series of models which are model we limit the space of</context>
<context position="22218" citStr="Chung and Gildea, 2009" startWordPosition="3557" endWordPosition="3560"> we derive a first on a monolingual setting, where the source senlist of K most frequent prefixes and suffixes using a tence aligned to each target sentence is empty. simple trie-based method proposed by (Schone and Unigram Model with Length Penalty Jurafsky, 2000).2 After we determine a list of al- The first model we study is the unigram monolowed prefixes and suffixes we restrict our model to lingual segmentation model using an exponential allow only segmentations of the form: ((p*)r(s*))+ length penalty as proposed by (Liang and Klein, where p and s belong to the allowed prefixes and 2009; Chung and Gildea, 2009), which has been suffixes and r can match any substring. shown to be quite accurate. We refer to this model as We determine the number of prefixes and suffixes Model-UP (for unigram with penalty). It defines the to consider using the maximum recall achievable by probability of a target morpheme sequence as follimiting the segmentation points in this way. Re- lows: (p1 ... pI) = (1 − 0) rIi�1 0PT(pi)LP(JpiJ) stricting the allowable segmentations in this way not This model can be (almost) recovered as a speonly improves the speed of inference but also leads cial case of our full model, if we dro</context>
<context position="31451" citStr="Chung and Gildea, 2009" startWordPosition="5074" endWordPosition="5077">em WDHMM (He, 2007), which performs comparably or better than IBM-Model4. The table in Figure 5 presents the results. In addition to reporting alignment error rate for different segmentation models, we report their morphological segmentation F1. The word-alignment WDHMM model performs best when aligning English words to Arabic words (using Arabic as source). In this direction it is able to capture the many-to-one correspondence between English words and arabic morphemes. When we combine alignments in both directions using the standard grow-diag-final method, the error goes up. We compare the (Chung and Gildea, 2009) model (termed Model-1) to our full bilingual model. We can recover Model-1 similarly to Model-UP, except now every morpheme is conditioned on an aligned source morpheme. Our full bilingual model outperforms Model-1 in both AER and segmentation F1. The specific form of the full model was selected as in the previous experiments, by choosing the model with best segmentations of the development set. For Arabic, the best model conditions target mor902 Align P Arabic P R F1 P Hebrew Align R AER R F1 Model-1 (C&amp;G 09) 91.6 81.2 13.9 72.4 76.2 74.3 61.0 71.8 65.9 Bilingual full 91.0 88.3 10.3 90.0 72.</context>
<context position="33190" citStr="Chung and Gildea, 2009" startWordPosition="5354" endWordPosition="5357">word-based absolute distortion, binned distortion, morpheme types of states, and aligned source-part-of-speech tags. Our best model for Arabic outperforms WDHMM in word alignment error rate. For Hebrew, the best model uses a similar boundary model configuration but a simpler uniform transition distortion distribution. Note that the bilingual models perform worse than the monolingual ones in segmentation F1. This finding is in line with previous work showing that the best segmentation for MT does not necessarily agree with a particular linguistic convention about what morphemes should contain (Chung and Gildea, 2009; Habash and Sadat, 2006), but contradicts other results (Snyder and Barzilay, 2008). Further experimentation is required to make a general claim. We should note that the Arabic dataset used for word-alignment evaluation is unconventionally small and noisy (the sentences are very short phrases, automatically extracted using GIZA++). Thus the phrases might not be really translations, and the sentence length is much smaller than in standard parallel corpora. This warrants further model evaluation in a large-scale alignment setting. 5 Related Work This work is most closely related to the unsuperv</context>
<context position="33950" citStr="Chung &amp; Gildea (2009)" startWordPosition="5470" endWordPosition="5473"> claim. We should note that the Arabic dataset used for word-alignment evaluation is unconventionally small and noisy (the sentences are very short phrases, automatically extracted using GIZA++). Thus the phrases might not be really translations, and the sentence length is much smaller than in standard parallel corpora. This warrants further model evaluation in a large-scale alignment setting. 5 Related Work This work is most closely related to the unsupervised tokenization and alignment models of Chung and Gildea (2009), Xu et al. (2008), Snyder and Barzilay (2008), and Nguyen et al. (2010). Chung &amp; Gildea (2009) introduce a unigram model of tokenization based on IBM Model-1,which is a special case of our model. Snyder and Barzilay (2008) proposes a hierarchical Bayesian model that combines the learning of monolingual segmentations and a cross-lingual alignment; their model is very different from ours. Incorporating morphological information into MT has received reasonable attention. For example, Goldwater &amp; McClosky (2005) show improvements when preprocessing Czech input to reflect a morphological decomposition using combinations of lemmatization, pseudowords, and morphemes. Yeniterzi and Oflazer (20</context>
</contexts>
<marker>Chung, Gildea, 2009</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2009. Unsupervised tokenization for machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Trans. Speech Lang. Process.</journal>
<contexts>
<context position="13809" citStr="Creutz and Lagus, 2007" startWordPosition="2198" endWordPosition="2201">ependency which we are now representing using only a bigram model on hidden morphemes. 2.2 Word Boundary Generation Model The PB distribution denotes the probability of generating word boundaries. As a sequence model of sentences the basic hidden semi-markov model completely ignores word boundaries. However, they can be powerful predictors of morpheme segments (by for example, indicating that common prefixes follow word boundaries, or that common suffixes precede them). The log-linear model of (Poon et al., 2009) uses word boundaries as observed left and right context features, and Morfessor (Creutz and Lagus, 2007) includes boundaries as special boundary symbols which can inform about the morpheme state of a morpheme (but not its identity). Our model includes a special generative process for boundaries which is conditioned not only on the previous morpheme state but also the previous two morphemes and other boundaries. Due to the fact that boundaries are observed their inclusion in the model does not increase the complexity of inference. The inclusion of this distribution lets us estimate the likelihood of a word consisting of one,two,three, or more morphemes. It also allows the estimation of likelihood</context>
<context position="30434" citStr="Creutz and Lagus, 2007" startWordPosition="4917" endWordPosition="4920">to the problematic Arabic character and frequently turn up in the corresponding places of cognate words in Biblical texts. We should note that our models select a large number of hyper-parameters on an annotated development set, including length penalty, hierarchical smoothing parameters α, and the subset of variables to use in each of three component sub-models. This might in part explain their advantage over previousstate-of-the-art models, which might use fewer (e.g. (Poon et al., 2009) and (Snyder and Barzilay, 2008)) or no specifically tuned for these datasets hyperparameters (Morfessor (Creutz and Lagus, 2007)). 4.2 Alignment Next we evaluate our full bilingual model and a simpler variant on the task of word alignment. We use the morpheme-level annotation of the S&amp;B EnglishArabic dataset and project the morpheme alignments to word alignments. We can thus compare alignment performance of the results of different segmentations. Additionally, we evaluate against a stateof-the-art word alignment system WDHMM (He, 2007), which performs comparably or better than IBM-Model4. The table in Figure 5 presents the results. In addition to reporting alignment error rate for different segmentation models, we repo</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Trans. Speech Lang. Process.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Fatiha Sadat</author>
</authors>
<title>Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5677" citStr="Habash and Sadat, 2006" startWordPosition="832" endWordPosition="835">s, and word boundaries. These additional sources of information provide powerful bias for unsupervised learning, without increasing the asymptotic running time of the inference algorithm. Used as a monolingual model, our system significantly improves the state-of-the-art segmentation performance on three Arabic and Hebrew datasets. Used as a bilingual model, our system outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER). In agreement with some previous work on tokenization/morpheme segmentation for alignment (Chung and Gildea, 2009; Habash and Sadat, 2006), we find that the best segmentation for alignment does not coincide with the gold-standard segmentation and our bilingual model does not outperform our monolingual model in segmentation F-Measure. 2 Model Our model defines the probability of a target language sequence of words (each consisting of a sequence of morphemes), and alignment from target to source morphemes, given a source language sequence of words (each consisting of a sequence of morphemes). An example morpheme segmentation and alignment of phrases in English-Arabic and EnglishBulgarian is shown in Figure 1. In our task setting, </context>
<context position="33215" citStr="Habash and Sadat, 2006" startWordPosition="5358" endWordPosition="5361">ortion, binned distortion, morpheme types of states, and aligned source-part-of-speech tags. Our best model for Arabic outperforms WDHMM in word alignment error rate. For Hebrew, the best model uses a similar boundary model configuration but a simpler uniform transition distortion distribution. Note that the bilingual models perform worse than the monolingual ones in segmentation F1. This finding is in line with previous work showing that the best segmentation for MT does not necessarily agree with a particular linguistic convention about what morphemes should contain (Chung and Gildea, 2009; Habash and Sadat, 2006), but contradicts other results (Snyder and Barzilay, 2008). Further experimentation is required to make a general claim. We should note that the Arabic dataset used for word-alignment evaluation is unconventionally small and noisy (the sentences are very short phrases, automatically extracted using GIZA++). Thus the phrases might not be really translations, and the sentence length is much smaller than in standard parallel corpora. This warrants further model evaluation in a large-scale alignment setting. 5 Related Work This work is most closely related to the unsupervised tokenization and ali</context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>Nizar Habash and Fatiha Sadat. 2006. Arabic preprocessing schemes for statistical machine translation. In North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
</authors>
<title>Using word-dependent transition models in HMM based word alignment for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL 2nd Statistical MT workshop,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="5475" citStr="He, 2007" startWordPosition="803" endWordPosition="804">orpheme segmentation over the target language. In a monolingual setting we introduce effective use of context by feature-rich modeling of the probabilities of morphemes, morphemetransitions, and word boundaries. These additional sources of information provide powerful bias for unsupervised learning, without increasing the asymptotic running time of the inference algorithm. Used as a monolingual model, our system significantly improves the state-of-the-art segmentation performance on three Arabic and Hebrew datasets. Used as a bilingual model, our system outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER). In agreement with some previous work on tokenization/morpheme segmentation for alignment (Chung and Gildea, 2009; Habash and Sadat, 2006), we find that the best segmentation for alignment does not coincide with the gold-standard segmentation and our bilingual model does not outperform our monolingual model in segmentation F-Measure. 2 Model Our model defines the probability of a target language sequence of words (each consisting of a sequence of morphemes), and alignment from target to source morphemes, given a source language se</context>
<context position="30847" citStr="He, 2007" startWordPosition="4985" endWordPosition="4986">-of-the-art models, which might use fewer (e.g. (Poon et al., 2009) and (Snyder and Barzilay, 2008)) or no specifically tuned for these datasets hyperparameters (Morfessor (Creutz and Lagus, 2007)). 4.2 Alignment Next we evaluate our full bilingual model and a simpler variant on the task of word alignment. We use the morpheme-level annotation of the S&amp;B EnglishArabic dataset and project the morpheme alignments to word alignments. We can thus compare alignment performance of the results of different segmentations. Additionally, we evaluate against a stateof-the-art word alignment system WDHMM (He, 2007), which performs comparably or better than IBM-Model4. The table in Figure 5 presents the results. In addition to reporting alignment error rate for different segmentation models, we report their morphological segmentation F1. The word-alignment WDHMM model performs best when aligning English words to Arabic words (using Arabic as source). In this direction it is able to capture the many-to-one correspondence between English words and arabic morphemes. When we combine alignments in both directions using the standard grow-diag-final method, the error goes up. We compare the (Chung and Gildea, 2</context>
</contexts>
<marker>He, 2007</marker>
<rawString>Xiaodong He. 2007. Using word-dependent transition models in HMM based word alignment for statistical machine translation. In ACL 2nd Statistical MT workshop, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="16369" citStr="Koehn, 2004" startWordPosition="2606" endWordPosition="2607"> types and the transitions between corresponding source morphemes. The set of features with example values for this model is depicted in Table 3. The example is focussed on the features firing for the transition from the Bulgarian suffix te aligned to the first English morpheme µi_1 = te, ti_1=suffix, ai_1=1, to the Bulgarian root tsvet aligned to the third English morpheme µi = tsvet, ti=root, ai=3. The first feature is the absolute difference between ai and ai_1 + 1 and is similar to information used in other HMM word alignment models (Och and Ney, 2000) as well as phrasetranslation models (Koehn, 2004). The alignment positions ai are defined as indices of the aligned source morphemes. We additionally compute distortion in terms of distance in number of source words that are skipped. This distance corresponds to the feature name WORD DISTANCE. Looking at both kinds of distance is useful to capture the intuition that consecutive morphemes in the same target word should prefer to have a higher proximity of their aligned source words, as compared to consecutive morphemes which are not part of the same target word. The binned distances look at the sign of the distortion and bin the jumps into 5 </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philip Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>Online EM for unsupervised models.</title>
<date>2009</date>
<booktitle>In North American Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="18458" citStr="Liang and Klein, 2009" startWordPosition="2941" endWordPosition="2944">eful in conjunction with other features to differentiate the transition behaviors within and across target words. The DEP RELATION feature indicates the direct dependency relation between the source words containing the aligned source morphemes, if such relationship exists. We also represent alignments to null and have one null for each source word, similarly to (Och and Ney, 2000) and have a feature to indicate null. Additionally, we make use of several feature conjunctions involving the null, same target word, and distance features. 2.4 Length Penalty Following (Chung and Gildea, 2009) and (Liang and Klein, 2009) we use an exponential length penalty on morpheme lengths to bias the model away from the maximum likelihood under-segmentation solution. The form of the penalty is: LP(|µi|) = 1 e,µ�,lp Here lp is a hyper-parameter indicating the power that the morpheme length is raised to. We fit this parameter using an annotated development set, to optimize morpheme-segmentation F1. The model is extremely sensitive to this value and performs quite poorly if such penalty is not used. 2.5 Inference We perform inference by EM training on the aligned sentence pairs. In the E-step we compute expected 899 counts </context>
</contexts>
<marker>Liang, Klein, 2009</marker>
<rawString>P. Liang and D. Klein. 2009. Online EM for unsupervised models. In North American Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>ThuyLinh Nguyen</author>
<author>Stephan Vogel</author>
<author>Noah A Smith</author>
</authors>
<title>Nonparametric word segmentation for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="33927" citStr="Nguyen et al. (2010)" startWordPosition="5466" endWordPosition="5469">ired to make a general claim. We should note that the Arabic dataset used for word-alignment evaluation is unconventionally small and noisy (the sentences are very short phrases, automatically extracted using GIZA++). Thus the phrases might not be really translations, and the sentence length is much smaller than in standard parallel corpora. This warrants further model evaluation in a large-scale alignment setting. 5 Related Work This work is most closely related to the unsupervised tokenization and alignment models of Chung and Gildea (2009), Xu et al. (2008), Snyder and Barzilay (2008), and Nguyen et al. (2010). Chung &amp; Gildea (2009) introduce a unigram model of tokenization based on IBM Model-1,which is a special case of our model. Snyder and Barzilay (2008) proposes a hierarchical Bayesian model that combines the learning of monolingual segmentations and a cross-lingual alignment; their model is very different from ours. Incorporating morphological information into MT has received reasonable attention. For example, Goldwater &amp; McClosky (2005) show improvements when preprocessing Czech input to reflect a morphological decomposition using combinations of lemmatization, pseudowords, and morphemes. Ye</context>
</contexts>
<marker>Nguyen, Vogel, Smith, 2010</marker>
<rawString>ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith. 2010. Nonparametric word segmentation for machine translation. In Proceedings of the International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models. In</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7056" citStr="Och and Ney, 2000" startWordPosition="1051" endWordPosition="1054">get language and the alignments between source and target morphemes are hidden. The source-side input, which we assume to be English, is processed with a gold morphological segmentation, part-of-speech, and dependency tree analysis. While these tools are unavailable in resource-poor languages, they are often available for at least one of the modeled languages in common translation tasks. This additional information then provides a source of features and conditioning information for the translation model. Our model is derived from the hidden-markov model for word alignment (Vogel et al., 1996; Och and Ney, 2000). Based on it, we define a dynamic 896 Figure 2: A graphical depiction of the model generating the transliteration of the first Bulgarian word from Figure 1. Trigram dependencies and some incoming/outgoing arcs have been omitted for clarity. graphical model which lets us encode more linguistic intuition about morpheme segmentation and alignment: (i) we extend it to a hidden semi-markov model to account for hidden target morpheme segmentation; (ii) we introduce an additional observation layer to model observed word boundaries and thus truly represent target sentences as words composed of morphe</context>
<context position="16319" citStr="Och and Ney, 2000" startWordPosition="2596" endWordPosition="2599">ures targeted at learning the transitions among morpheme types and the transitions between corresponding source morphemes. The set of features with example values for this model is depicted in Table 3. The example is focussed on the features firing for the transition from the Bulgarian suffix te aligned to the first English morpheme µi_1 = te, ti_1=suffix, ai_1=1, to the Bulgarian root tsvet aligned to the third English morpheme µi = tsvet, ti=root, ai=3. The first feature is the absolute difference between ai and ai_1 + 1 and is similar to information used in other HMM word alignment models (Och and Ney, 2000) as well as phrasetranslation models (Koehn, 2004). The alignment positions ai are defined as indices of the aligned source morphemes. We additionally compute distortion in terms of distance in number of source words that are skipped. This distance corresponds to the feature name WORD DISTANCE. Looking at both kinds of distance is useful to capture the intuition that consecutive morphemes in the same target word should prefer to have a higher proximity of their aligned source words, as compared to consecutive morphemes which are not part of the same target word. The binned distances look at th</context>
<context position="18220" citStr="Och and Ney, 2000" startWordPosition="2904" endWordPosition="2907">ample sentence pair in Figure 1a. tive morphemes are part of the same word. In this case, they are not. This feature is not useful on its own because it does not distinguish between different alignment possibilities for tai, but is useful in conjunction with other features to differentiate the transition behaviors within and across target words. The DEP RELATION feature indicates the direct dependency relation between the source words containing the aligned source morphemes, if such relationship exists. We also represent alignments to null and have one null for each source word, similarly to (Och and Ney, 2000) and have a feature to indicate null. Additionally, we make use of several feature conjunctions involving the null, same target word, and distance features. 2.4 Length Penalty Following (Chung and Gildea, 2009) and (Liang and Klein, 2009) we use an exponential length penalty on morpheme lengths to bias the model away from the maximum likelihood under-segmentation solution. The form of the penalty is: LP(|µi|) = 1 e,µ�,lp Here lp is a hyper-parameter indicating the power that the morpheme length is raised to. We fit this parameter using an annotated development set, to optimize morpheme-segment</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In North American Chapter of the Association for Computation Linguistics - Human Language Technologies</booktitle>
<contexts>
<context position="13704" citStr="Poon et al., 2009" startWordPosition="2182" endWordPosition="2185">phemes on their own, we would need to have a four-gram model on target morphemes to represent this dependency which we are now representing using only a bigram model on hidden morphemes. 2.2 Word Boundary Generation Model The PB distribution denotes the probability of generating word boundaries. As a sequence model of sentences the basic hidden semi-markov model completely ignores word boundaries. However, they can be powerful predictors of morpheme segments (by for example, indicating that common prefixes follow word boundaries, or that common suffixes precede them). The log-linear model of (Poon et al., 2009) uses word boundaries as observed left and right context features, and Morfessor (Creutz and Lagus, 2007) includes boundaries as special boundary symbols which can inform about the morpheme state of a morpheme (but not its identity). Our model includes a special generative process for boundaries which is conditioned not only on the previous morpheme state but also the previous two morphemes and other boundaries. Due to the fact that boundaries are observed their inclusion in the model does not increase the complexity of inference. The inclusion of this distribution lets us estimate the likelih</context>
<context position="15643" citStr="Poon et al., 2009" startWordPosition="2483" endWordPosition="2486">els represent P(aj|aj_1i e), the probability of an alignment given the previous alignment, to bias the model away from placing large distances between the aligned tokens of consecutively sequenced tokens. In addition to modeling a larger state space to also predict morpheme types, we extend this model by using a special log-linear model form which allows the integration of rich morpho-syntactic context. Log-linear models have been previously used in unsupervised learning for local multinomial distributions like this one in e.g. (Berg-Kirkpatrick et al., 2010), and for global distributions in (Poon et al., 2009). The special log-linear form allows the inclusion of features targeted at learning the transitions among morpheme types and the transitions between corresponding source morphemes. The set of features with example values for this model is depicted in Table 3. The example is focussed on the features firing for the transition from the Bulgarian suffix te aligned to the first English morpheme µi_1 = te, ti_1=suffix, ai_1=1, to the Bulgarian root tsvet aligned to the third English morpheme µi = tsvet, ti=root, ai=3. The first feature is the absolute difference between ai and ai_1 + 1 and is simila</context>
<context position="21141" citStr="Poon et al., 2009" startWordPosition="3377" endWordPosition="3380">solution but it cessed with a morphological analyzer and then furis not hard to derive by starting from the dynamic ther hand annotated with corrections by two native program for the IBM-1 like tokenization model of speakers. Morphological alignments were manually (Chung and Gildea, 2009) and extending it to ac- annotated. Additionally, we evaluate monolingual count for the higher order on morphemes and the segmentation models on the full Arabic Treebank factored alignment state space. (ATB), also used for unsupervised morpheme segEven though the inference algorithm is low poly- mentation in (Poon et al., 2009). nomial it is still much more expensive than the infer- 4 Results ence for an HMM model for word-alignment with- 4.1 Morpheme Segmentation out segmentation. To reduce the running time of the We begin by evaluating a series of models which are model we limit the space of considered morpheme simplifications of our complete model, to assess the boundaries as follows: impact of individual modeling decisions. We focus Given the target side of the corpus, we derive a first on a monolingual setting, where the source senlist of K most frequent prefixes and suffixes using a tence aligned to each targe</context>
<context position="30305" citStr="Poon et al., 2009" startWordPosition="4898" endWordPosition="4901">ce of error is not as overwhelmingly directed at a single character, but yod and he, the latter functioning quite similarly to the problematic Arabic character and frequently turn up in the corresponding places of cognate words in Biblical texts. We should note that our models select a large number of hyper-parameters on an annotated development set, including length penalty, hierarchical smoothing parameters α, and the subset of variables to use in each of three component sub-models. This might in part explain their advantage over previousstate-of-the-art models, which might use fewer (e.g. (Poon et al., 2009) and (Snyder and Barzilay, 2008)) or no specifically tuned for these datasets hyperparameters (Morfessor (Creutz and Lagus, 2007)). 4.2 Alignment Next we evaluate our full bilingual model and a simpler variant on the task of word alignment. We use the morpheme-level annotation of the S&amp;B EnglishArabic dataset and project the morpheme alignments to word alignments. We can thus compare alignment performance of the results of different segmentations. Additionally, we evaluate against a stateof-the-art word alignment system WDHMM (He, 2007), which performs comparably or better than IBM-Model4. The</context>
</contexts>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Hoifung Poon, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In North American Chapter of the Association for Computation Linguistics - Human Language Technologies 2009 conference (NAACL/HLT-09).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Knowlegefree induction of morphology using latent semantic analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning (CoNLL-2000).</booktitle>
<marker>Schone, Jurafsky, 2000</marker>
<rawString>Patrick Schone and Daniel Jurafsky. 2000. Knowlegefree induction of morphology using latent semantic analysis. In Proceedings of the Conference on Computational Natural Language Learning (CoNLL-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="30337" citStr="Snyder and Barzilay, 2008" startWordPosition="4903" endWordPosition="4906">erwhelmingly directed at a single character, but yod and he, the latter functioning quite similarly to the problematic Arabic character and frequently turn up in the corresponding places of cognate words in Biblical texts. We should note that our models select a large number of hyper-parameters on an annotated development set, including length penalty, hierarchical smoothing parameters α, and the subset of variables to use in each of three component sub-models. This might in part explain their advantage over previousstate-of-the-art models, which might use fewer (e.g. (Poon et al., 2009) and (Snyder and Barzilay, 2008)) or no specifically tuned for these datasets hyperparameters (Morfessor (Creutz and Lagus, 2007)). 4.2 Alignment Next we evaluate our full bilingual model and a simpler variant on the task of word alignment. We use the morpheme-level annotation of the S&amp;B EnglishArabic dataset and project the morpheme alignments to word alignments. We can thus compare alignment performance of the results of different segmentations. Additionally, we evaluate against a stateof-the-art word alignment system WDHMM (He, 2007), which performs comparably or better than IBM-Model4. The table in Figure 5 presents the </context>
<context position="33274" citStr="Snyder and Barzilay, 2008" startWordPosition="5366" endWordPosition="5369"> aligned source-part-of-speech tags. Our best model for Arabic outperforms WDHMM in word alignment error rate. For Hebrew, the best model uses a similar boundary model configuration but a simpler uniform transition distortion distribution. Note that the bilingual models perform worse than the monolingual ones in segmentation F1. This finding is in line with previous work showing that the best segmentation for MT does not necessarily agree with a particular linguistic convention about what morphemes should contain (Chung and Gildea, 2009; Habash and Sadat, 2006), but contradicts other results (Snyder and Barzilay, 2008). Further experimentation is required to make a general claim. We should note that the Arabic dataset used for word-alignment evaluation is unconventionally small and noisy (the sentences are very short phrases, automatically extracted using GIZA++). Thus the phrases might not be really translations, and the sentence length is much smaller than in standard parallel corpora. This warrants further model evaluation in a large-scale alignment setting. 5 Related Work This work is most closely related to the unsupervised tokenization and alignment models of Chung and Gildea (2009), Xu et al. (2008),</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In In COLING 96: The 16th Int. Conf. on Computational Linguistics.</booktitle>
<contexts>
<context position="7036" citStr="Vogel et al., 1996" startWordPosition="1047" endWordPosition="1050">mentation of the target language and the alignments between source and target morphemes are hidden. The source-side input, which we assume to be English, is processed with a gold morphological segmentation, part-of-speech, and dependency tree analysis. While these tools are unavailable in resource-poor languages, they are often available for at least one of the modeled languages in common translation tasks. This additional information then provides a source of features and conditioning information for the translation model. Our model is derived from the hidden-markov model for word alignment (Vogel et al., 1996; Och and Ney, 2000). Based on it, we define a dynamic 896 Figure 2: A graphical depiction of the model generating the transliteration of the first Bulgarian word from Figure 1. Trigram dependencies and some incoming/outgoing arcs have been omitted for clarity. graphical model which lets us encode more linguistic intuition about morpheme segmentation and alignment: (i) we extend it to a hidden semi-markov model to account for hidden target morpheme segmentation; (ii) we introduce an additional observation layer to model observed word boundaries and thus truly represent target sentences as word</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In In COLING 96: The 16th Int. Conf. on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Hermann Ney</author>
</authors>
<title>Bayesian semi-supervised chinese word segmentation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="33873" citStr="Xu et al. (2008)" startWordPosition="5457" endWordPosition="5460">d Barzilay, 2008). Further experimentation is required to make a general claim. We should note that the Arabic dataset used for word-alignment evaluation is unconventionally small and noisy (the sentences are very short phrases, automatically extracted using GIZA++). Thus the phrases might not be really translations, and the sentence length is much smaller than in standard parallel corpora. This warrants further model evaluation in a large-scale alignment setting. 5 Related Work This work is most closely related to the unsupervised tokenization and alignment models of Chung and Gildea (2009), Xu et al. (2008), Snyder and Barzilay (2008), and Nguyen et al. (2010). Chung &amp; Gildea (2009) introduce a unigram model of tokenization based on IBM Model-1,which is a special case of our model. Snyder and Barzilay (2008) proposes a hierarchical Bayesian model that combines the learning of monolingual segmentations and a cross-lingual alignment; their model is very different from ours. Incorporating morphological information into MT has received reasonable attention. For example, Goldwater &amp; McClosky (2005) show improvements when preprocessing Czech input to reflect a morphological decomposition using combina</context>
</contexts>
<marker>Xu, Gao, Toutanova, Ney, 2008</marker>
<rawString>Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann Ney. 2008. Bayesian semi-supervised chinese word segmentation for statistical machine translation. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reyyan Yeniterzi</author>
<author>Kemal Oflazer</author>
</authors>
<title>Syntax-tomorphology mapping in factored phrase-based statistical machine translation from english to turkish.</title>
<date>2010</date>
<booktitle>In Proceedings of Association of Computational Linguistics.</booktitle>
<contexts>
<context position="34553" citStr="Yeniterzi and Oflazer (2010)" startWordPosition="5555" endWordPosition="5558">0). Chung &amp; Gildea (2009) introduce a unigram model of tokenization based on IBM Model-1,which is a special case of our model. Snyder and Barzilay (2008) proposes a hierarchical Bayesian model that combines the learning of monolingual segmentations and a cross-lingual alignment; their model is very different from ours. Incorporating morphological information into MT has received reasonable attention. For example, Goldwater &amp; McClosky (2005) show improvements when preprocessing Czech input to reflect a morphological decomposition using combinations of lemmatization, pseudowords, and morphemes. Yeniterzi and Oflazer (2010) bridge the morphological disparity between languages in a unique way by effectively aligning English syntactic elements (function words connected by dependency relations) to Turkish morphemes, using rule-based postprocessing of standard word alignment. Our work is partly inspired by that work and attempts to automate both the morpho-syntactic alignment and morphological analysis tasks. 6 Conclusion We have described an unsupervised model for morpheme segmentation and alignment based on Hidden Semi-Markov Models. Our model makes use of linguistic information to improve alignment quality. On th</context>
</contexts>
<marker>Yeniterzi, Oflazer, 2010</marker>
<rawString>Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-tomorphology mapping in factored phrase-based statistical machine translation from english to turkish. In Proceedings of Association of Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>