<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.975923">
Selecting Query Term Alterations for Web Search by Exploiting Query
Contexts
</title>
<author confidence="0.932666">
Guihong Cao Stephen Robertson Jian-Yun Nie
</author>
<affiliation confidence="0.840531666666667">
Dept. of Computer Science and Microsoft Research at Dept. of Computer Science and
Operations Research Cambridge Operations Research
University of Montreal, Canada Cambridge, UK University of Montreal, Canada
</affiliation>
<email confidence="0.990775">
caogui@iro.umontreal.ca ser@microsoft.com nie@iro.umontreal.ca
</email>
<sectionHeader confidence="0.998562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998378">
Query expansion by word alterations (alterna-
tive forms of a word) is often used in Web
search to replace word stemming. This allows
users to specify particular word forms in a
query. However, if many alterations are
added, query traffic will be greatly increased.
In this paper, we propose methods to select
only a few useful word alterations for query
expansion. The selection is made according to
the appropriateness of the alteration to the
query context (using a bigram language
model), or according to its expected impact
on the retrieval effectiveness (using a regres-
sion model). Our experiments on two TREC
collections will show that both methods only
select a few expansion terms, but the retrieval
effectiveness can be improved significantly.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999817411764706">
Word stemming is a basic NLP technique used in
most of Information Retrieval (IR) systems. It
transforms words into their root forms so as to in-
crease the chance to match similar words/terms
that are morphological variants. For example, with
stemming, “controlling” can match “controlled”
because both have the same root “control”. Most
stemmers, such as the Porter stemmer (Porter,
1980) and Krovetz stemmer (Krovetz, 1993), deal
with stemming by stripping word suffixes accord-
ing to a set of morphological rules. Rule-based ap-
proaches are intuitive and easy to implement.
However, while in general, most words can be
stemmed correctly; there is often erroneous stem-
ming that unifies unrelated words. For instance,
“jobs” is stemmed to “job” in both “find jobs in
Apple” and “Steve Jobs at Apple”. This is particu-
larly problematic in Web search, where users often
use special or new words in their queries. A stan-
dard stemmer such as Porter’s will wrongly stem
them.
To better determine stemming rules, Xu and
Croft (1998) propose a selective stemming method
based on corpus analysis. They refine the Porter
stemmer by means of word clustering: words are
first clustered according to their co-occurrences in
the text collection. Only word variants belonging
to the same cluster will be conflated.
Despite this improvement, the basic idea of
word stemming is to transform words in both doc-
uments and queries to a standard form. Once this is
done, there is no means for users to require a spe-
cific word form in a query – the word form will be
automatically transformed, otherwise, it will not
match documents. This approach does not seem to
be appropriate for Web search, where users often
specify particular word forms in their queries. An
example of this is a quoted query such as “Steve
Jobs”, or “US Policy”. If documents are stemmed,
many pages about job offerings or US police may
be returned (“policy” conflates with “police” in
Porter stemmer). Another drawback of stemming is
that it usually enhances recall, but may hurt preci-
sion (Kraaij and Pohlmann, 1996). However, gen-
eral Web search is basically a precision-oriented
task.
One alternative approach to word stemming is to
do query expansion at query time. The original
query terms are expanded by their related forms
having the same root. All expansions can be com-
bined by the Boolean operator “OR”. For example,
</bodyText>
<page confidence="0.968624">
148
</page>
<note confidence="0.71604">
Proceedings of ACL-08: HLT, pages 148–155,
</note>
<page confidence="0.525761">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.998333548387097">
the query “controlling acid rain” can be expanded
to “(control OR controlling OR controller OR con-
trolled OR controls) (acid OR acidic OR acidify)
(rain OR raining OR rained OR rains)”. We will
call each such expansion term an alteration to the
original query term. Once a set of possible altera-
tions is determined, the simplest approach to per-
form expansion is to add all possible alterations.
We call this approach Naive Expansion. One can
easily show that stemming at indexing time is
equivalent to Naive Expansion at retrieval time.
This approach has been adopted by most commer-
cial search engines (Peng et al., 2007). However,
the expansion approaches proposed previously can
have several serious problems: First, they usually
do not consider expansion ambiguity – each query
term is usually expanded independently. However,
some expansion terms may not be appropriate. The
case of “Steve Jobs” is one such example, for
which the word “job” can be proposed as an ex-
pansion term. Second, as each query term may
have several alterations, the naïve approach using
all the alterations will create a very long query. As
a consequence, query traffic (the time required for
the evaluation of a query) is greatly increased.
Query traffic is a critical problem, as each search
engine serves millions of users at the same time. It
is important to limit the query traffic as much as
possible.
In practice, we can observe that some word al-
terations are irrelevant and undesirable (as in the
“Steve Jobs” case), and some other alterations have
little impact on the retrieval effectiveness (for ex-
ample, if we expand a word by a rarely used word
form). In this study, we will address these two
problems. Our goal is to select only appropriate
word alterations to be used in query expansion.
This is done for two purposes: On the one hand,
we want to limit query traffic as much as possible
when query expansion is performed. On the other
hand, we also want to remove irrelevant expansion
terms so that fewer irrelevant documents will be
retrieved, thereby improve the retrieval effective-
ness.
To deal with the two problems we mentioned
above, we will propose two methods to select al-
terations. In the first method, we make use of the
query context to select only the alterations that fit
the query. The query context is modeled by a bi-
gram language model. To reduce query traffic, we
select only one alteration for each query term,
which is the most coherent with the bigram model.
We call this model Bigram Expansion. Despite the
fact that this method adds far fewer expansion
terms than the naïve expansion, our experiments
will show that we can achieve comparable or even
better retrieval effectiveness.
Both the Naive Expansion and the Bigram Ex-
pansion determine word alterations solely accord-
ing to general knowledge about the language
(bigram model or morphological rules), and no
consideration about the possible effect of the ex-
pansion term is made. In practice, some alterations
will have virtually no impact on retrieval effec-
tiveness. They can be ignored. Therefore, in our
second method, we will try to predict whether an
alteration will have some positive impact on re-
trieval effectiveness. Only the alterations with pos-
itive impact will be retained. In this paper, we will
use a regression model to predict the impact on
retrieval effectiveness. Compared to the bigram
expansion method, the regression method results in
even fewer alterations, but experiments show that
the retrieval effectiveness is even better.
Experiments will be conducted on two TREC
collections, Gov2 data for Web Track and
TREC6&amp;7&amp;8 for ad-hoc retrieval. The results
show that the two methods we propose both out-
perform the original queries significantly with less
than two alterations per query on average. Com-
pared to the Naive Expansion method, the two me-
thods can perform at least equally well, while
query traffic is dramatically reduced.
In the following section, we provide a brief re-
view of related work. Section 3 shows how to gen-
erate alteration candidates using a similar approach
to Xu and Croft’s corpus analysis (1998). In sec-
tion 4 and 5, we describe the Bigram Expansion
method and Regression method respectively. Sec-
tion 6 presents some experiments on TREC
benchmarks to evaluate our methods. Section 7
concludes this paper and suggests some avenues
for future work.
</bodyText>
<sectionHeader confidence="0.999913" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999309">
Many stemmers have been implemented and used
as standard processing in IR. Among them, the
Porter stemmer (Porter, 1980) is the most widely
used. It strips term suffixes step-by-step according
to a set of morphological rules. However, the Por-
ter stemmer sometimes wrongly transforms a term
into an unrelated root. For example, it will unify
</bodyText>
<page confidence="0.998474">
149
</page>
<bodyText confidence="0.999975903225807">
“news” and “new”, “execute” and “executive”. On
the other hand, it may miss some conflations, such
as “mice” and “mouse”, “europe” and “european”.
Krovetz (1993) developed another stemmer, which
uses a machine-readable dictionary, to improve the
Porter stemmer. It avoids some of the Porter
stemmer’s wrong stripping, but does not produce
consistent improvement in IR experiments.
Both stemmers use generic rules for English to
strip each word in isolation. In practice, the re-
quired stemming may vary from one text collection
to another. Therefore, attempts have been made to
use corpus analysis to improve existing rule-based
stemmers. Xu and Croft (1998) create equivalence
clusters of words which are morphologically simi-
lar and occur in similar contexts.
As we stated earlier, the stemming-based IR ap-
proaches are not well suited to Web search. Query
expansion has been used as an alternative (Peng et
al. 2007). To limit the number of expansion terms,
and thus the query traffic, Peng et al. only use al-
terations for some of the query words: They seg-
ment each query into phrases and only the head
word in each phrase is expanded. The assumptions
are: 1)Queries issued in Web search often consist
of noun phrases. 2) Only the head word in the noun
phrase varies in form and needs to be expanded.
However, both assumptions may be questionable.
Their experiments did not show that the two as-
sumptions hold.
Stemming is related to query expansion or query
reformulation (Jones et al., 2006; Anick, 2003; Xu
and Croft, 1996), although the latter is not limited
to word variants. If the expansion terms used are
those that are variant forms of a word, then query
expansion can produce the same effect as word
stemming. However, if we add all possible word
alterations, query expansion/reformulation will run
the risk of adding many unrelated terms to the
original query, which may result in both heavy
traffic and topic drift. Therefore, we need a way to
select the most appropriate expansion terms. In
(Peng et al. 2007), a bigram language model is
used to determine the alteration of the head word
that best fits the query. In this paper, one of the
proposed methods will also use a bigram language
model of the query to determine the appropriate
alteration candidates. However, in our approach,
alterations are not limited to head words. In addi-
tion, we will also propose a supervised learning
method to predict if an alteration will have a posi-
tive impact on retrieval effectiveness. To our
knowledge, no previous method uses the same ap-
proach.
In the following sections, we will describe our
approach, which consists of two steps: the genera-
tion of alteration candidates, and the selection of
appropriate alterations for a query. The first step is
query-independent using corpus analysis, while the
second step is query-dependent. The selected word
alterations will be OR-ed with the original query
words.
</bodyText>
<sectionHeader confidence="0.981755" genericHeader="method">
3 Generating Alteration Candidates
</sectionHeader>
<bodyText confidence="0.999965434782609">
Our method to generate alteration candidates can
be described as follows. First, we do word cluster-
ing using a Porter stemmer. All words in the vo-
cabulary sharing the same root form are grouped
together. Then we do corpus analysis to filter out
the words which are clustered incorrectly, accord-
ing to word distributional similarity, following (Xu
and Croft, 1998; Lin 1998). The rationale behind
this is that words sharing the same meaning tend to
occur in the same contexts.
The context of each word in the vocabulary is
represented by a vector containing the frequencies
of the context words which co-occur with the word
within a predefined window in a training corpus.
The window size is set empirically at 3 words and
the training corpus is about 1/10 of the GOV2 cor-
pus (see section 5 for details about the collection).
Similarity is measured by the cosine distance be-
tween two vectors. For each word, we select at
most 5 similar words as alteration candidates.
In the next sections, we will further consider ways
to select appropriate alterations according to the
query.
</bodyText>
<sectionHeader confidence="0.9314425" genericHeader="method">
4 Bigram Expansion Model for Alteration
Selection
</sectionHeader>
<bodyText confidence="0.999839454545455">
In this section, we try to select the most suitable
alterations according to the query context. The
query context is modeled by a bigram language
model as in (Peng et al. 2007).
Given a query described by a sequence of
words, we consider each of the query word as rep-
resenting a concept c;. In addition to the given
word form, c; can also be expressed by other alter-
native forms. However, the appropriate alterations
do not only depend on the original word of c;, but
also on other query words or their alterations.
</bodyText>
<page confidence="0.994937">
150
</page>
<figureCaption confidence="0.818689">
Figure 1: Considering all Combinations to Calculate the
Plausibility of Alterations
</figureCaption>
<bodyText confidence="0.9998359">
Accordingly, a confidence weight is determined
for each alteration candidate. For example, in the
query “Steve Jobs at Apple”, the alteration “job” of
“jobs” should have a low confidence; while in the
query “finding jobs in Apple”, it should have a
high confidence.
One way to measure the confidence of an altera-
tion is the plausibility of its appearing in the query.
Since each concept may be expressed by several
alterations, we consider all the alterations of con-
text concepts when calculating the plausibility of a
given word. Suppose we have the query “control-
ling acid rain”. The second concept has two altera-
tions - “acidify” and “acidic”. For each of the
alterations, our method will consider all the com-
binations with other words, as illustrated in figure
1, where each combination is shown as a path.
More precisely, for a query of n words (or their
corresponding concepts), let ei,j∈ci, j=1,2,...,|ci |be
the alterations of concept ci. Then we have:
</bodyText>
<subsectionHeader confidence="0.609862">
P(eij ) E |c1 |1,j1=1 E2Zj2=1 ... E|i-114−1=1 Ei+1,ji+1=1 ... (1)
</subsectionHeader>
<bodyText confidence="0.999948">
passing through ei,j. For simplicity, we abbreviate it
as e1e2...ei...en. In this work, we used bigram lan-
guage model to calculate the probability of each
path. Then we have:
</bodyText>
<equation confidence="0.931558333333333">
P e e e e P e
( , ,..., ,..., ) ( )
= ∏n = P e e
(  |) (2)
1 2 i n 1 −
k 2 k k 1
</equation>
<bodyText confidence="0.999443444444445">
P(ek|ek-1) is estimated with a back-off bigram lan-
guage model (Goodman, 2001). In the experiments
with TREC6&amp;7&amp;8, we train the model with all
text collections; while in the experiments with
Gov2 data, we only used about 1/10 of the GOV2
data to train the bigram model because the whole
Gov2 collection is too large.
Directly calculating P(eij) by summing the prob-
abilities of all paths passing through eij is an NP
problem (Rabiner, 1989), and is intractable if the
query is long. Therefore, we use the forward-
backward algorithm (Bishop, 2006) to calculate
P(eij) in a more efficient way. After calculating
P(eij) for each ci, we select one alteration which
has the highest probability. We limit the number of
additional alterations to 1 in order to limit query
traffic. Our experiments will show that this is often
sufficient.
</bodyText>
<sectionHeader confidence="0.997212" genericHeader="method">
5 Regression Model for Alteration Selec-
tion
</sectionHeader>
<bodyText confidence="0.999980736842105">
None of the previous selection methods considers
how well an alteration would perform in retrieval.
The Bigram Expansion model assumes that the
query replaced with better alterations should have
a higher likelihood. This approach belongs to the
family of unsupervised learning. In this section, we
introduce a method belonging to supervised learn-
ing family. This method develops a regression
model from a set of training data, and it is capable
of predicting the expected change in performance
when the original query is augmented by this al-
teration. The performance change is measured by
the difference in the Mean Average Precision
(MAP) between the augmented and the original
query. The training instances are defined by the
original query string, an original query term under
consideration and one alteration to the query term.
A set of features will be used, which will be de-
fined later in this section.
</bodyText>
<subsectionHeader confidence="0.931342">
5.1 Linear Regression Model
</subsectionHeader>
<bodyText confidence="0.999956818181818">
The goal of the regression model is to predict the
performance change when a query term is aug-
mented with an alteration. There are several re-
gression models, ranging from the simplest linear
regression model to non-linear alternatives, such as
a neural network (Duda et al., 2001), a Regression
SVM (Bishop, 2006). For simplicity, we use linear
regression model here. We denote an instance in
the feature space as X, and the weights of features
are denoted as W. Then the linear regression model
is defined as:
</bodyText>
<equation confidence="0.794261">
f(X)=WTX (3)
</equation>
<bodyText confidence="0.999892333333333">
where WT is the transpose of W. However, we will
have a technical problem if we set the target value
to the performance change directly: The range of
</bodyText>
<figure confidence="0.972621888888889">
control
controller
acidify
acidic
rain
rains
raining
controlling
controlled
</figure>
<equation confidence="0.827227133333333">
P e e
( , ,..., ,..., )
e e
n j
, 1
= 1, j j
2, ij
, n j
,
1 2 i n
n
In equation 1, e1,j1 , e2,j2 ,..., ei,ji ,..., en,jn is a path
...E
|
|cn
</equation>
<page confidence="0.975986">
151
</page>
<bodyText confidence="0.999933">
values of f(X) is (−∞,+∞) , while the range of per-
formance change is [-1,1]. The two value ranges do
not match. This inconsistency may result in severe
problems when the scales of feature values vary
dramatically (Duda et al., 2001). To solve this
problem, we do a simple transformation on the per-
formance change. Let the change be y ∈ [−1,1] , then
the transformed performance change is:
</bodyText>
<equation confidence="0.9924645">
ϕ(y) = log 1 + y + γ y ∈ −
[ 1,1] (4)
1− +
y γ
</equation>
<bodyText confidence="0.9999832">
where γ is a very small positive real number (set to
be 1e-37 in the experiments), which acts as a
smoothing factor. The value of ϕ( y)can be an arbi-
trary real number. ϕ( y) is a monotonic function
defined in the range of [-1,1]. Moreover, the fixed
point of ϕ( y) is 0, i.e., ϕ(y) = y when y=0. This
property is nice; it means that the expansion brings
positive improvement if and only if f(X)&gt;0, which
makes it easy to determine which alteration is bet-
ter.
We train the regression model by minimizing
the mean square error. Suppose there are training
instances X1,X2,...,Xm, and the corresponding per-
formance change is yi, i=1,2,...,m. We calculate
the mean square error with the following equation:
</bodyText>
<equation confidence="0.923375666666667">
2
err W
( ) =∑m = 1 WTX
( −ϕ( ))
y (5)
i i i
</equation>
<bodyText confidence="0.726287">
Then the optimal weight is defined as:
</bodyText>
<equation confidence="0.957028916666667">
W * = arg min err(W ) (6)
m
∑= W X
T
( − ϕ ( ))
y
W i 1 i i
the gradient is zero (Bazaraa et al
., 2006). Then we
have:
ea�
(
i))XT
So,
*
∂
−ϕ
y
∑ =
* T m T = ∑ m = 1 y X
ϕ( )
T
W 1 X X
i i i i i i
</equation>
<bodyText confidence="0.996242571428571">
In fact, ∑=m1XiXT is a square matrix, we denote
The matrix XXT is an
square matrix, where l
is the number of features. In our experiments, we
only use three features. Therefore the optimal
weights can be calculated efficiently even we have
a large number of training instan
</bodyText>
<equation confidence="0.986513714285714">
* ( ) 1 ϕ ( )
−
= [ ∑= ]
m
W XX T 1 y X (7)
i i i
l×l
</equation>
<bodyText confidence="0.82146">
ces.
</bodyText>
<subsectionHeader confidence="0.390839">
152 5.2 Constructing Training Data
</subsectionHeader>
<bodyText confidence="0.993415892857143">
As a supervised learning method, the regression
model is trained with a set of training data. We
illustrate here the procedure to generate training
instances with an example.
Given a query
acid
we obtain
the MAP of the original query at first. Then we
augment the query with an alteration to the original
term (one term at a time) at each time. We retain
the MAP of the augmented query and compare it
with the original query to obtain the performance
change. For this query, we expand
by
and get an augmented query
OR control) acid
We can obtain the dif-
ference between the MAP of the augmented query
and that of the original query. By doing this, we
can generate a series of training instances consist-
ing of the original query string, the original query
term under consideration, its alteration and the per-
formance change, for example:
&lt;controlling acid rain, controlling, control, 0.05&gt;
Note that we use MAP to measure performance,
but we could well use other metrics such as NDCG
(Peng et al., 2007) or P@N (precision at top-N
documents).
</bodyText>
<subsectionHeader confidence="0.990611">
5.3 Features Used for Regression Model
</subsectionHeader>
<bodyText confidence="0.999272714285714">
Three features are used. The first feature reflects to
what degree an alteration is coherent with the other
acid
is
window (90 words) in the corpus. That is:
where
foll
</bodyText>
<figure confidence="0.862304384615385">
“controlling
rain”,
“controlling”
“control”
“(control-
ling
rain”.
“controlling
rain”,
“acidic”
log(count(controlling...acidic...rain|window)+0.5)
“...”
(Rijsbergen,
</figure>
<bodyText confidence="0.9585366">
ows:
terms. For example, for the query
the coherence of the alteration
measured by the logarithm of its co-occurrence
with the other query terms within a predefined
</bodyText>
<equation confidence="0.8599906">
P(controlling ... acidic ... rain

|window)   P(controlling)P(acidic)P(rain) 
W
Because err(W) is a convex function of W, it has
</equation>
<bodyText confidence="0.956782941176471">
a global minimum and obtains its minimum when
it as XXT. Then we have:
means there may be some words be-
tween two query terms. Word order is ignored.
The second feature is an extension to point-wise
mutual information
1979), defined as
log
where
is the
co-occurrence probability of the trigram containing
acidic within a predefined window (50 words).
P(controlling), p(acidic), P(rain) are probabilities
of the three words in the collection. The three
words are defined as: the term under consideration,
the first term to the left of that term, and the first
term to the right. If a query contains less than
</bodyText>
<equation confidence="0.751603833333333">
P(controlling...acidic...rain|window)
3
= arg min
2
0
=
</equation>
<bodyText confidence="0.999957714285714">
terms or the term under consideration is the begin-
ning/ending term in the query, we will set the
probability of the missed term/terms to be 1.
Therefore, it becomes point-wise mutual informa-
tion when the query contains only two terms. In
fact, this feature is supplemental to the first feature.
When the query is very long and the first feature
always obtains a value of log(0.5), so it does not
have any discriminative ability. On the other hand,
the second feature helps because it can capture
some co-occurrence information no matter how
long the query is.
The last feature is the bias, whose value is al-
ways set to be 1.0.
The regression model is trained in a leave-one-
out cross-validation manner on three collections;
each of them is used in turn as a test collection
while the two others are used for training. For
each incoming query, the regression model pre-
dicts the expected performance change when one
alteration is used. For each query term, we only
select the alteration with the largest positive per-
formance change. If none of its alterations produce
a positive performance change, we do not expand
the query term. This selection is therefore more
restrictive than the Bigram Expansion Model.
Nevertheless, our experiments show that it im-
proves retrieval effectiveness further.
</bodyText>
<sectionHeader confidence="0.99985" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999922">
6.1 Experiment Settings
</subsectionHeader>
<bodyText confidence="0.999642473684211">
In this section, our aim is to evaluate the two con-
text-sensitive word alteration selection methods.
The ideal evaluation corpus should be composed of
some Web data. Unfortunately, such data are not
publicly available and the results also could not be
compared with other published results. Therefore,
we use two TREC collections. The first one is the
ad-hoc retrieval test collections used for
TREC6&amp;7&amp; 8. This collection is relative small and
homogeneous. The second one is the Gov2 data. It
is obtained by crawling the entire .gov domain and
has been used for three TREC Terabyte tracks
(TREC2004-2006). Table 1 shows some statistics
of the two collections. For each collection, we use
150 queries. Since the Regression model needs
some data for training, we divided the queries into
three parts, each containing 50 queries. We then
use leave-one-out cross-validation. The evaluation
metrics shown below are the average value of the
</bodyText>
<table confidence="0.998853333333333">
Name Description Size #Doc Query
(GB)
TREC6 TREC disk4&amp;5, 1.7 500,447 301-450
&amp;7&amp;8 Newpapers
Gov2 2004 crawl of entire 427 25,205,179 701-850
.gov domain
</table>
<tableCaption confidence="0.825136">
Table1: Overview of Test Collections
</tableCaption>
<bodyText confidence="0.9997185">
three-fold cross-validation. Because the queries in
Web are usually very short, we use only the title
field of each query.
To correspond to Web search practice, both
documents and queries are not stemmed. We do
not filter the stop words either.
Two main metrics are used: the Mean Average
Precision (MAP) for the top 1000 documents to
measure retrieval effectiveness, and the number of
terms in the query to reflect query traffic. In addi-
tion, we also provide precision for the top 30 doc-
uments (P@30) to show the impact on top ranked
documents. We also conducted t-tests to determine
whether the improvement is statistically significant.
The Indri 2.5 search engine (Strohman et al.,
2004) is used as our basic retrieval system. It pro-
vides for a rich query language allowing disjunc-
tive combinations of words in queries.
</bodyText>
<subsectionHeader confidence="0.999687">
6.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.997840857142857">
The first baseline method we compare with only
uses the original query, which is named Original.
In addition to this, we also compare with the fol-
lowing methods:
Naïve Exp: The Naïve expansion model expands
each query term with all terms in the vocabu-
lary sharing the same root with it. This model is
equivalent to the traditional stemming method.
UMASS: This is the result reported in (Metzler et al.,
2006) using Porter stemming for both document
and query terms. This reflects a state-of-the-art
result using Porter stemming.
Similarity: We select the alterations (at most 5)
with the highest similarity to the original term.
This is the method described in section 3.
The two methods we propose in this paper are the
following ones:
Bigram Exp: the alteration is chosen by a Bigram
Expansion model.
Regression: the alteration is chosen by a Regres-
sion model.
</bodyText>
<page confidence="0.996366">
153
</page>
<table confidence="0.999395857142857">
Model P@30 #term MAP Imp.
Original 0.4701 158 0.2440 ----
UMASS 0.2666 9.26
Naïve Exp 0.4714 1345 0.2653 8.73
Similarity 0.4900 303 0.2689 10.20*
Bigram Exp 0.5007 303 0.2751 12.75**
Regression 0.5054 237 0.2773 13.65**
</table>
<tableCaption confidence="0.986571">
Table 2: Results of Query 701-750 Over Gov2 Data
</tableCaption>
<table confidence="0.999937857142857">
Model P@30 #term MAP Imp.
Original 0.4907 158 0.2738 ----
UMASS 0.3251 18.73
Naive Exp 0.5213 1167 0.3224 17.75**
Similarity 0.5140 290 0.3043 11.14**
Bigram Exp. 0.5153 290 0.3107 13.47**
Regression 0.5140 256 0.3144 14.82**
</table>
<tableCaption confidence="0.99848">
Table 3: Results of Query 751-800 over Gov2 Data
</tableCaption>
<table confidence="0.999924">
Model P@30 #term MAP Imp.
Original 0.4710 154 0.2887 ----
UMASS 0.2996 3.78
Naïve Exp 0.4633 1225 0.2999 3.87
Similarity 0.4710 288 0.2976 3.08
Bigram Exp 0.4730 288 0.3137 8.66**
Regression 0.4748 237 0.3118 8.00*
</table>
<tableCaption confidence="0.997508">
Table 4: Results of Query 801-850 over Gov2 Data
</tableCaption>
<table confidence="0.999911333333333">
Model P@30 #term MAP Imp.
Original 0.2673 137 0.1669 ----
Naïve Exp 0.3053 783 0.2146 28.57**
Similarity 0.3007 255 0.2020 21.03**
Bigram Exp 0.3033 255 0.2091 25.28**
Regression 0.3113 224 0.2161 29.48**
</table>
<tableCaption confidence="0.997327">
Table 5: Results of Query 301-350 over TREC6&amp;7&amp;8
</tableCaption>
<table confidence="0.999926">
Model P@30 #term MAP Imp.
Original 0.2820 126 0.1639
Naive Exp 0.2787 736 0.1665 1.59
Similarity 0.2867 244 0.1650 0.67
Bigram Exp. 0.2800 244 0.1641 0.12
Regression 0.2867 214 0.1664 1.53
</table>
<tableCaption confidence="0.989924">
Table 6: Results of Query 351-400 over TREC6&amp;7&amp;8
</tableCaption>
<table confidence="0.999633666666667">
Model P@30 #term MAP Imp.
Original 0.2833 124 0.1759
Naïve Exp 0.3167 685 0.2138 21.55**
Similarity 0.3080 240 0.2066 17.45**
Bigram Exp 0.3133 240 0.2080 18.25**
Regression 0.3220 187 0.2144 21.88**
</table>
<tableCaption confidence="0.596517">
Table7: Results of Query 401-450 over TREC6&amp;7&amp;8
</tableCaption>
<bodyText confidence="0.999241454545455">
Tables 2, 3, 4 show the results of Gov2 data
while table 5, 6, 7 show the results of the
TREC6&amp;7&amp;8 collection. In the tables, the * mark
indicates that the improvement over the original
model is statistically significant with p-value&lt;0.05,
and ** means the p-values&lt;0.01.
From the tables, we see that both word stem-
ming (UMASS) and expansion with word altera-
tions can improve MAP for all six tasks. In most
cases (except in table 4 and 6), it also improve the
precision of top ranked documents. This shows the
usefulness of word stemming or word alteration
expansion for IR.
We can make several additional observations:
1). Stemming Vs Expansion. UMASS uses docu-
ment and query stemming while Naive Exp uses
expansion by word alteration. We stated that both
approaches are equivalent. The equivalence is
confirmed by our experiment results: for all Gov2
collections, these approaches perform equiva-
lently.
2). The Similarity model performs very well. Com-
pared with the Naïve Expansion model, it pro-
duces quite similar retrieval effectiveness, while
the query traffic is dramatically reduced. This
approach is similar to the work of Xu and Croft
(1998), and can be considered as another state-of-
the-art result.
3). In comparison, the Bigram Expansion model
performs better than the Similarity model. This
shows that it is useful to consider query context
in selecting word alterations.
4). The Regression model performs the best of all
the models. Compared with the Original query, it
adds fewer than 2 alterations for each query on
average (since each group has 50 queries); never-
theless we obtained improvements on all the six
collections. Moreover, the improvements on five
collections are statistically significant. It also per-
forms slightly better than the Similarity and Bi-
gram Expansion methods, but with fewer
alterations. This shows that the supervised learn-
ing approach, if used in the correct way, is supe-
rior to an unsupervised approach. Another
advantage over the two other models is that the
Regression model can reduce the number of al-
terations further. Because the Regression model
selects alterations according to their expected
improvement, the improvement of the alterations
to one query term can be compared with that of
the alterations to other query terms. Therefore,
we can select at most one optimal alteration for
the whole query. However, with the Similarity or
Bigram Expansion models, the selection value,
either similarity or query likelihood, cannot be
</bodyText>
<page confidence="0.99915">
154
</page>
<bodyText confidence="0.999671666666667">
compared across the query terms. As a conse-
quence, more alterations need to be selected,
leading to heavier query traffic.
</bodyText>
<sectionHeader confidence="0.999066" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999961581818182">
Traditional IR approaches stem terms in both doc-
uments and queries. This approach is appropriate
for general purpose IR, but is ill-suited for the spe-
cific retrieval needs in Web search such as quoted
queries or queries with a specific word form that
should not be stemmed. The current practice in
Web search is not to stem words in index, but ra-
ther to perform a form of expansion using word
alteration.
However, a naïve expansion will result in many
alterations and this will increase the query traffic.
This paper has proposed two alternative methods
to select precise alterations by considering the
query context. We seek to produce similar or better
improvements in retrieval effectiveness, while lim-
iting the query traffic.
In the first method proposed – the Bigram Ex-
pansion model, query context is modeled by a bi-
gram language model. For each query term, the
selected alteration is the one which maximizes the
query likelihood. In the second method - Regres-
sion model, we fit a regression model to calculate
the expected improvement when the original query
is expanded by an alteration. Only the alteration
that is expected to yield the largest improvement to
retrieval effectiveness is added.
The proposed methods were evaluated on two
TREC benchmarks: the ad-hoc retrieval test collec-
tion for TREC6&amp;7&amp;8 and the Gov2 data. Our ex-
perimental results show that both proposed
methods perform significantly better than the orig-
inal queries. Compared with traditional word
stemming or the naïve expansion approach, our
methods can not only improve retrieval effective-
ness, but also greatly reduce the query traffic.
This work shows that query expansion with
word alterations is a reasonable alternative to word
stemming. It is possible to limit the query traffic by
a query-dependent selection of word alterations.
Our work shows that both unsupervised and super-
vised learning can be used to perform alteration
selection.
Our methods can be further improved in several
aspects. For example, we could integrate other fea-
tures in the regression model, and use other non-
linear regression models, such as Bayesian regres-
sion models (e.g. Gaussian Process regression)
(Rasmussen and Williams, 2006). The additional
advantage of these models is that we can not only
obtain the expected improvement in retrieval effec-
tiveness for an alteration, but also the probability
of obtaining an improvement (i.e. the robustness of
the alteration).
Finally, it would be interesting to test the ap-
proaches using real Web data.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999917431818182">
Anick, P. (2003) Using Terminological Feedback for
Web Search Refinement: a Log-based Study. In
SIGIR, pp. 88-95.
Bazaraa, M., Sherali, H., and Shett, C. (2006). Nonlin-
ear Programming, Theory and Algorithms. John
Wiley &amp; Sons Inc.
Bishop, C. (2006). Pattern Recognition and Machine
Learning. Springer.
Duda, R., Hart, P., and Stork, D. (2001). Pattern Clas-
sification, John Wiley &amp; Sons, Inc.
Goodman, J. (2001). A Bit of Progress in Language
Modeling. Technical report.
Jones, R., Rey, B., Madani, O., and Greiner, W. (2006).
Generating Query Substitutions. In WWW2006, pp.
387-396
Kraaij, W. and Pohlmann, R. (1996) Viewing Stemming
as Recall Enhancement. Proc. SIGIR, pp. 40-48.
Krovetz, R. (1993). Viewing Morphology as an Infer-
ence Process. Proc. ACM SIGIR, pp. 191-202.
Lin, D. (1998). Automatic Retrieval and Clustering of
Similar Words. In COLING-ACL, pp. 768-774.
Metzler, D., Strohman, T. and Croft, B. (2006). Indri
TREC Notebook 2006: Lessons learned from Three
Terabyte Tracks. In the Proceedings of TREC 2006.
Peng, F., Ahmed, N., Li, X., and Lu, Y. (2007). Context
Sensitive Stemming for Web Search. Proc. ACM
SIGIR, pp. 639-636 .
Porter, M. (1980) An Algorithm for Suffix Stripping.
Program, 14(3): 130-137.
Rabiner, L. (1989). A Tutorial on Hidden Markov Mod-
els and Selected Applications in Speech Recognition.
In Proceedings of IEEE Vol. 77(2), pp. 257-286.
Rijsbergen, V. (1979). Information Retrieval. Butter-
worths, second version.
Strohman, T., Metzler, D. and Turtle, H., and Croft, B.
(2004). Indri: A Language Model-based Search En-
gine for Complex Queries. In Proceedings of the In-
ternational conference on Intelligence Analysis.
Xu, J. and Croft, B. (1996). Query Expansion Using
Local and Global Document Analysis. Proc. ACM
SIGIR, pp. 4-11.
Xu, J. and Croft, B. (1998). Corpus-based Stemming
Using Co-occurrence of Word Variants. ACM
TOIS, 16(1): 61-81.
</reference>
<page confidence="0.999014">
155
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.801319">
<title confidence="0.9970775">Selecting Query Term Alterations for Web Search by Exploiting Query Contexts</title>
<author confidence="0.998637">Guihong Cao Stephen Robertson Jian-Yun Nie</author>
<affiliation confidence="0.963843666666667">Dept. of Computer Science and Microsoft Research at Dept. of Computer Science and Operations Research Cambridge Operations Research University of Montreal, Canada Cambridge, UK University of Montreal, Canada</affiliation>
<email confidence="0.91448">caogui@iro.umontreal.caser@microsoft.comnie@iro.umontreal.ca</email>
<abstract confidence="0.999732555555556">Query expansion by word alterations (alternative forms of a word) is often used in Web search to replace word stemming. This allows users to specify particular word forms in a query. However, if many alterations are added, query traffic will be greatly increased. In this paper, we propose methods to select only a few useful word alterations for query expansion. The selection is made according to the appropriateness of the alteration to the query context (using a bigram language model), or according to its expected impact on the retrieval effectiveness (using a regression model). Our experiments on two TREC collections will show that both methods only select a few expansion terms, but the retrieval effectiveness can be improved significantly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Anick</author>
</authors>
<title>Using Terminological Feedback for Web Search Refinement: a Log-based Study.</title>
<date>2003</date>
<booktitle>In SIGIR,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="9898" citStr="Anick, 2003" startWordPosition="1614" endWordPosition="1615">tive (Peng et al. 2007). To limit the number of expansion terms, and thus the query traffic, Peng et al. only use alterations for some of the query words: They segment each query into phrases and only the head word in each phrase is expanded. The assumptions are: 1)Queries issued in Web search often consist of noun phrases. 2) Only the head word in the noun phrase varies in form and needs to be expanded. However, both assumptions may be questionable. Their experiments did not show that the two assumptions hold. Stemming is related to query expansion or query reformulation (Jones et al., 2006; Anick, 2003; Xu and Croft, 1996), although the latter is not limited to word variants. If the expansion terms used are those that are variant forms of a word, then query expansion can produce the same effect as word stemming. However, if we add all possible word alterations, query expansion/reformulation will run the risk of adding many unrelated terms to the original query, which may result in both heavy traffic and topic drift. Therefore, we need a way to select the most appropriate expansion terms. In (Peng et al. 2007), a bigram language model is used to determine the alteration of the head word that</context>
</contexts>
<marker>Anick, 2003</marker>
<rawString>Anick, P. (2003) Using Terminological Feedback for Web Search Refinement: a Log-based Study. In SIGIR, pp. 88-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bazaraa</author>
<author>H Sherali</author>
<author>C Shett</author>
</authors>
<title>Nonlinear Programming, Theory and Algorithms.</title>
<date>2006</date>
<publisher>John Wiley &amp; Sons Inc.</publisher>
<marker>Bazaraa, Sherali, Shett, 2006</marker>
<rawString>Bazaraa, M., Sherali, H., and Shett, C. (2006). Nonlinear Programming, Theory and Algorithms. John Wiley &amp; Sons Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="14896" citStr="Bishop, 2006" startWordPosition="2473" endWordPosition="2474">P e ( , ,..., ,..., ) ( ) = ∏n = P e e ( |) (2) 1 2 i n 1 − k 2 k k 1 P(ek|ek-1) is estimated with a back-off bigram language model (Goodman, 2001). In the experiments with TREC6&amp;7&amp;8, we train the model with all text collections; while in the experiments with Gov2 data, we only used about 1/10 of the GOV2 data to train the bigram model because the whole Gov2 collection is too large. Directly calculating P(eij) by summing the probabilities of all paths passing through eij is an NP problem (Rabiner, 1989), and is intractable if the query is long. Therefore, we use the forwardbackward algorithm (Bishop, 2006) to calculate P(eij) in a more efficient way. After calculating P(eij) for each ci, we select one alteration which has the highest probability. We limit the number of additional alterations to 1 in order to limit query traffic. Our experiments will show that this is often sufficient. 5 Regression Model for Alteration Selection None of the previous selection methods considers how well an alteration would perform in retrieval. The Bigram Expansion model assumes that the query replaced with better alterations should have a higher likelihood. This approach belongs to the family of unsupervised lea</context>
<context position="16475" citStr="Bishop, 2006" startWordPosition="2730" endWordPosition="2731">P) between the augmented and the original query. The training instances are defined by the original query string, an original query term under consideration and one alteration to the query term. A set of features will be used, which will be defined later in this section. 5.1 Linear Regression Model The goal of the regression model is to predict the performance change when a query term is augmented with an alteration. There are several regression models, ranging from the simplest linear regression model to non-linear alternatives, such as a neural network (Duda et al., 2001), a Regression SVM (Bishop, 2006). For simplicity, we use linear regression model here. We denote an instance in the feature space as X, and the weights of features are denoted as W. Then the linear regression model is defined as: f(X)=WTX (3) where WT is the transpose of W. However, we will have a technical problem if we set the target value to the performance change directly: The range of control controller acidify acidic rain rains raining controlling controlled P e e ( , ,..., ,..., ) e e n j , 1 = 1, j j 2, ij , n j , 1 2 i n n In equation 1, e1,j1 , e2,j2 ,..., ei,ji ,..., en,jn is a path ...E | |cn 151 values of f(X) i</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Bishop, C. (2006). Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Duda</author>
<author>P Hart</author>
<author>D Stork</author>
</authors>
<title>Pattern Classification,</title>
<date>2001</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<contexts>
<context position="16442" citStr="Duda et al., 2001" startWordPosition="2723" endWordPosition="2726">ence in the Mean Average Precision (MAP) between the augmented and the original query. The training instances are defined by the original query string, an original query term under consideration and one alteration to the query term. A set of features will be used, which will be defined later in this section. 5.1 Linear Regression Model The goal of the regression model is to predict the performance change when a query term is augmented with an alteration. There are several regression models, ranging from the simplest linear regression model to non-linear alternatives, such as a neural network (Duda et al., 2001), a Regression SVM (Bishop, 2006). For simplicity, we use linear regression model here. We denote an instance in the feature space as X, and the weights of features are denoted as W. Then the linear regression model is defined as: f(X)=WTX (3) where WT is the transpose of W. However, we will have a technical problem if we set the target value to the performance change directly: The range of control controller acidify acidic rain rains raining controlling controlled P e e ( , ,..., ,..., ) e e n j , 1 = 1, j j 2, ij , n j , 1 2 i n n In equation 1, e1,j1 , e2,j2 ,..., ei,ji ,..., en,jn is a pat</context>
</contexts>
<marker>Duda, Hart, Stork, 2001</marker>
<rawString>Duda, R., Hart, P., and Stork, D. (2001). Pattern Classification, John Wiley &amp; Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>A Bit of Progress in Language Modeling.</title>
<date>2001</date>
<tech>Technical report.</tech>
<contexts>
<context position="14430" citStr="Goodman, 2001" startWordPosition="2394" endWordPosition="2395">lustrated in figure 1, where each combination is shown as a path. More precisely, for a query of n words (or their corresponding concepts), let ei,j∈ci, j=1,2,...,|ci |be the alterations of concept ci. Then we have: P(eij ) E |c1 |1,j1=1 E2Zj2=1 ... E|i-114−1=1 Ei+1,ji+1=1 ... (1) passing through ei,j. For simplicity, we abbreviate it as e1e2...ei...en. In this work, we used bigram language model to calculate the probability of each path. Then we have: P e e e e P e ( , ,..., ,..., ) ( ) = ∏n = P e e ( |) (2) 1 2 i n 1 − k 2 k k 1 P(ek|ek-1) is estimated with a back-off bigram language model (Goodman, 2001). In the experiments with TREC6&amp;7&amp;8, we train the model with all text collections; while in the experiments with Gov2 data, we only used about 1/10 of the GOV2 data to train the bigram model because the whole Gov2 collection is too large. Directly calculating P(eij) by summing the probabilities of all paths passing through eij is an NP problem (Rabiner, 1989), and is intractable if the query is long. Therefore, we use the forwardbackward algorithm (Bishop, 2006) to calculate P(eij) in a more efficient way. After calculating P(eij) for each ci, we select one alteration which has the highest pro</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Goodman, J. (2001). A Bit of Progress in Language Modeling. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jones</author>
<author>B Rey</author>
<author>O Madani</author>
<author>W Greiner</author>
</authors>
<title>Generating Query Substitutions.</title>
<date>2006</date>
<booktitle>In WWW2006,</booktitle>
<pages>387--396</pages>
<contexts>
<context position="9885" citStr="Jones et al., 2006" startWordPosition="1610" endWordPosition="1613">n used as an alternative (Peng et al. 2007). To limit the number of expansion terms, and thus the query traffic, Peng et al. only use alterations for some of the query words: They segment each query into phrases and only the head word in each phrase is expanded. The assumptions are: 1)Queries issued in Web search often consist of noun phrases. 2) Only the head word in the noun phrase varies in form and needs to be expanded. However, both assumptions may be questionable. Their experiments did not show that the two assumptions hold. Stemming is related to query expansion or query reformulation (Jones et al., 2006; Anick, 2003; Xu and Croft, 1996), although the latter is not limited to word variants. If the expansion terms used are those that are variant forms of a word, then query expansion can produce the same effect as word stemming. However, if we add all possible word alterations, query expansion/reformulation will run the risk of adding many unrelated terms to the original query, which may result in both heavy traffic and topic drift. Therefore, we need a way to select the most appropriate expansion terms. In (Peng et al. 2007), a bigram language model is used to determine the alteration of the h</context>
</contexts>
<marker>Jones, Rey, Madani, Greiner, 2006</marker>
<rawString>Jones, R., Rey, B., Madani, O., and Greiner, W. (2006). Generating Query Substitutions. In WWW2006, pp. 387-396</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kraaij</author>
<author>R Pohlmann</author>
</authors>
<title>Viewing Stemming as Recall Enhancement.</title>
<date>1996</date>
<booktitle>Proc. SIGIR,</booktitle>
<pages>40--48</pages>
<contexts>
<context position="3234" citStr="Kraaij and Pohlmann, 1996" startWordPosition="511" endWordPosition="514">done, there is no means for users to require a specific word form in a query – the word form will be automatically transformed, otherwise, it will not match documents. This approach does not seem to be appropriate for Web search, where users often specify particular word forms in their queries. An example of this is a quoted query such as “Steve Jobs”, or “US Policy”. If documents are stemmed, many pages about job offerings or US police may be returned (“policy” conflates with “police” in Porter stemmer). Another drawback of stemming is that it usually enhances recall, but may hurt precision (Kraaij and Pohlmann, 1996). However, general Web search is basically a precision-oriented task. One alternative approach to word stemming is to do query expansion at query time. The original query terms are expanded by their related forms having the same root. All expansions can be combined by the Boolean operator “OR”. For example, 148 Proceedings of ACL-08: HLT, pages 148–155, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics the query “controlling acid rain” can be expanded to “(control OR controlling OR controller OR controlled OR controls) (acid OR acidic OR acidify) (rain OR raining</context>
</contexts>
<marker>Kraaij, Pohlmann, 1996</marker>
<rawString>Kraaij, W. and Pohlmann, R. (1996) Viewing Stemming as Recall Enhancement. Proc. SIGIR, pp. 40-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Krovetz</author>
</authors>
<title>Viewing Morphology as an Inference Process.</title>
<date>1993</date>
<booktitle>Proc. ACM SIGIR,</booktitle>
<pages>191--202</pages>
<contexts>
<context position="1591" citStr="Krovetz, 1993" startWordPosition="236" endWordPosition="237"> model). Our experiments on two TREC collections will show that both methods only select a few expansion terms, but the retrieval effectiveness can be improved significantly. 1 Introduction Word stemming is a basic NLP technique used in most of Information Retrieval (IR) systems. It transforms words into their root forms so as to increase the chance to match similar words/terms that are morphological variants. For example, with stemming, “controlling” can match “controlled” because both have the same root “control”. Most stemmers, such as the Porter stemmer (Porter, 1980) and Krovetz stemmer (Krovetz, 1993), deal with stemming by stripping word suffixes according to a set of morphological rules. Rule-based approaches are intuitive and easy to implement. However, while in general, most words can be stemmed correctly; there is often erroneous stemming that unifies unrelated words. For instance, “jobs” is stemmed to “job” in both “find jobs in Apple” and “Steve Jobs at Apple”. This is particularly problematic in Web search, where users often use special or new words in their queries. A standard stemmer such as Porter’s will wrongly stem them. To better determine stemming rules, Xu and Croft (1998) </context>
<context position="8555" citStr="Krovetz (1993)" startWordPosition="1391" endWordPosition="1392">luate our methods. Section 7 concludes this paper and suggests some avenues for future work. 2 Related Work Many stemmers have been implemented and used as standard processing in IR. Among them, the Porter stemmer (Porter, 1980) is the most widely used. It strips term suffixes step-by-step according to a set of morphological rules. However, the Porter stemmer sometimes wrongly transforms a term into an unrelated root. For example, it will unify 149 “news” and “new”, “execute” and “executive”. On the other hand, it may miss some conflations, such as “mice” and “mouse”, “europe” and “european”. Krovetz (1993) developed another stemmer, which uses a machine-readable dictionary, to improve the Porter stemmer. It avoids some of the Porter stemmer’s wrong stripping, but does not produce consistent improvement in IR experiments. Both stemmers use generic rules for English to strip each word in isolation. In practice, the required stemming may vary from one text collection to another. Therefore, attempts have been made to use corpus analysis to improve existing rule-based stemmers. Xu and Croft (1998) create equivalence clusters of words which are morphologically similar and occur in similar contexts. A</context>
</contexts>
<marker>Krovetz, 1993</marker>
<rawString>Krovetz, R. (1993). Viewing Morphology as an Inference Process. Proc. ACM SIGIR, pp. 191-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="11713" citStr="Lin 1998" startWordPosition="1913" endWordPosition="1914">alterations for a query. The first step is query-independent using corpus analysis, while the second step is query-dependent. The selected word alterations will be OR-ed with the original query words. 3 Generating Alteration Candidates Our method to generate alteration candidates can be described as follows. First, we do word clustering using a Porter stemmer. All words in the vocabulary sharing the same root form are grouped together. Then we do corpus analysis to filter out the words which are clustered incorrectly, according to word distributional similarity, following (Xu and Croft, 1998; Lin 1998). The rationale behind this is that words sharing the same meaning tend to occur in the same contexts. The context of each word in the vocabulary is represented by a vector containing the frequencies of the context words which co-occur with the word within a predefined window in a training corpus. The window size is set empirically at 3 words and the training corpus is about 1/10 of the GOV2 corpus (see section 5 for details about the collection). Similarity is measured by the cosine distance between two vectors. For each word, we select at most 5 similar words as alteration candidates. In the</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. (1998). Automatic Retrieval and Clustering of Similar Words. In COLING-ACL, pp. 768-774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Metzler</author>
<author>T Strohman</author>
<author>B Croft</author>
</authors>
<title>Indri TREC Notebook 2006: Lessons learned from Three Terabyte Tracks.</title>
<date>2006</date>
<booktitle>In the Proceedings of TREC</booktitle>
<contexts>
<context position="24935" citStr="Metzler et al., 2006" startWordPosition="4235" endWordPosition="4238">nt. The Indri 2.5 search engine (Strohman et al., 2004) is used as our basic retrieval system. It provides for a rich query language allowing disjunctive combinations of words in queries. 6.2 Experimental Results The first baseline method we compare with only uses the original query, which is named Original. In addition to this, we also compare with the following methods: Naïve Exp: The Naïve expansion model expands each query term with all terms in the vocabulary sharing the same root with it. This model is equivalent to the traditional stemming method. UMASS: This is the result reported in (Metzler et al., 2006) using Porter stemming for both document and query terms. This reflects a state-of-the-art result using Porter stemming. Similarity: We select the alterations (at most 5) with the highest similarity to the original term. This is the method described in section 3. The two methods we propose in this paper are the following ones: Bigram Exp: the alteration is chosen by a Bigram Expansion model. Regression: the alteration is chosen by a Regression model. 153 Model P@30 #term MAP Imp. Original 0.4701 158 0.2440 ---- UMASS 0.2666 9.26 Naïve Exp 0.4714 1345 0.2653 8.73 Similarity 0.4900 303 0.2689 10</context>
</contexts>
<marker>Metzler, Strohman, Croft, 2006</marker>
<rawString>Metzler, D., Strohman, T. and Croft, B. (2006). Indri TREC Notebook 2006: Lessons learned from Three Terabyte Tracks. In the Proceedings of TREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>N Ahmed</author>
<author>X Li</author>
<author>Y Lu</author>
</authors>
<title>Context Sensitive Stemming for Web Search.</title>
<date>2007</date>
<booktitle>Proc. ACM SIGIR,</booktitle>
<pages>639--636</pages>
<contexts>
<context position="4292" citStr="Peng et al., 2007" startWordPosition="684" endWordPosition="687"> query “controlling acid rain” can be expanded to “(control OR controlling OR controller OR controlled OR controls) (acid OR acidic OR acidify) (rain OR raining OR rained OR rains)”. We will call each such expansion term an alteration to the original query term. Once a set of possible alterations is determined, the simplest approach to perform expansion is to add all possible alterations. We call this approach Naive Expansion. One can easily show that stemming at indexing time is equivalent to Naive Expansion at retrieval time. This approach has been adopted by most commercial search engines (Peng et al., 2007). However, the expansion approaches proposed previously can have several serious problems: First, they usually do not consider expansion ambiguity – each query term is usually expanded independently. However, some expansion terms may not be appropriate. The case of “Steve Jobs” is one such example, for which the word “job” can be proposed as an expansion term. Second, as each query term may have several alterations, the naïve approach using all the alterations will create a very long query. As a consequence, query traffic (the time required for the evaluation of a query) is greatly increased. </context>
<context position="9310" citStr="Peng et al. 2007" startWordPosition="1508" endWordPosition="1511">s wrong stripping, but does not produce consistent improvement in IR experiments. Both stemmers use generic rules for English to strip each word in isolation. In practice, the required stemming may vary from one text collection to another. Therefore, attempts have been made to use corpus analysis to improve existing rule-based stemmers. Xu and Croft (1998) create equivalence clusters of words which are morphologically similar and occur in similar contexts. As we stated earlier, the stemming-based IR approaches are not well suited to Web search. Query expansion has been used as an alternative (Peng et al. 2007). To limit the number of expansion terms, and thus the query traffic, Peng et al. only use alterations for some of the query words: They segment each query into phrases and only the head word in each phrase is expanded. The assumptions are: 1)Queries issued in Web search often consist of noun phrases. 2) Only the head word in the noun phrase varies in form and needs to be expanded. However, both assumptions may be questionable. Their experiments did not show that the two assumptions hold. Stemming is related to query expansion or query reformulation (Jones et al., 2006; Anick, 2003; Xu and Cro</context>
<context position="12643" citStr="Peng et al. 2007" startWordPosition="2071" endWordPosition="2074">e is set empirically at 3 words and the training corpus is about 1/10 of the GOV2 corpus (see section 5 for details about the collection). Similarity is measured by the cosine distance between two vectors. For each word, we select at most 5 similar words as alteration candidates. In the next sections, we will further consider ways to select appropriate alterations according to the query. 4 Bigram Expansion Model for Alteration Selection In this section, we try to select the most suitable alterations according to the query context. The query context is modeled by a bigram language model as in (Peng et al. 2007). Given a query described by a sequence of words, we consider each of the query word as representing a concept c;. In addition to the given word form, c; can also be expressed by other alternative forms. However, the appropriate alterations do not only depend on the original word of c;, but also on other query words or their alterations. 150 Figure 1: Considering all Combinations to Calculate the Plausibility of Alterations Accordingly, a confidence weight is determined for each alteration candidate. For example, in the query “Steve Jobs at Apple”, the alteration “job” of “jobs” should have a </context>
<context position="19848" citStr="Peng et al., 2007" startWordPosition="3407" endWordPosition="3410">ry and compare it with the original query to obtain the performance change. For this query, we expand by and get an augmented query OR control) acid We can obtain the difference between the MAP of the augmented query and that of the original query. By doing this, we can generate a series of training instances consisting of the original query string, the original query term under consideration, its alteration and the performance change, for example: &lt;controlling acid rain, controlling, control, 0.05&gt; Note that we use MAP to measure performance, but we could well use other metrics such as NDCG (Peng et al., 2007) or P@N (precision at top-N documents). 5.3 Features Used for Regression Model Three features are used. The first feature reflects to what degree an alteration is coherent with the other acid is window (90 words) in the corpus. That is: where foll “controlling rain”, “controlling” “control” “(controlling rain”. “controlling rain”, “acidic” log(count(controlling...acidic...rain|window)+0.5) “...” (Rijsbergen, ows: terms. For example, for the query the coherence of the alteration measured by the logarithm of its co-occurrence with the other query terms within a predefined P(controlling ... acidi</context>
</contexts>
<marker>Peng, Ahmed, Li, Lu, 2007</marker>
<rawString>Peng, F., Ahmed, N., Li, X., and Lu, Y. (2007). Context Sensitive Stemming for Web Search. Proc. ACM SIGIR, pp. 639-636 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Porter</author>
</authors>
<title>An Algorithm for Suffix Stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>130--137</pages>
<contexts>
<context position="1555" citStr="Porter, 1980" startWordPosition="231" endWordPosition="232">l effectiveness (using a regression model). Our experiments on two TREC collections will show that both methods only select a few expansion terms, but the retrieval effectiveness can be improved significantly. 1 Introduction Word stemming is a basic NLP technique used in most of Information Retrieval (IR) systems. It transforms words into their root forms so as to increase the chance to match similar words/terms that are morphological variants. For example, with stemming, “controlling” can match “controlled” because both have the same root “control”. Most stemmers, such as the Porter stemmer (Porter, 1980) and Krovetz stemmer (Krovetz, 1993), deal with stemming by stripping word suffixes according to a set of morphological rules. Rule-based approaches are intuitive and easy to implement. However, while in general, most words can be stemmed correctly; there is often erroneous stemming that unifies unrelated words. For instance, “jobs” is stemmed to “job” in both “find jobs in Apple” and “Steve Jobs at Apple”. This is particularly problematic in Web search, where users often use special or new words in their queries. A standard stemmer such as Porter’s will wrongly stem them. To better determine </context>
<context position="8169" citStr="Porter, 1980" startWordPosition="1329" endWordPosition="1330">uery traffic is dramatically reduced. In the following section, we provide a brief review of related work. Section 3 shows how to generate alteration candidates using a similar approach to Xu and Croft’s corpus analysis (1998). In section 4 and 5, we describe the Bigram Expansion method and Regression method respectively. Section 6 presents some experiments on TREC benchmarks to evaluate our methods. Section 7 concludes this paper and suggests some avenues for future work. 2 Related Work Many stemmers have been implemented and used as standard processing in IR. Among them, the Porter stemmer (Porter, 1980) is the most widely used. It strips term suffixes step-by-step according to a set of morphological rules. However, the Porter stemmer sometimes wrongly transforms a term into an unrelated root. For example, it will unify 149 “news” and “new”, “execute” and “executive”. On the other hand, it may miss some conflations, such as “mice” and “mouse”, “europe” and “european”. Krovetz (1993) developed another stemmer, which uses a machine-readable dictionary, to improve the Porter stemmer. It avoids some of the Porter stemmer’s wrong stripping, but does not produce consistent improvement in IR experim</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Porter, M. (1980) An Algorithm for Suffix Stripping. Program, 14(3): 130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
</authors>
<title>A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.</title>
<date>1989</date>
<booktitle>In Proceedings of IEEE</booktitle>
<volume>77</volume>
<issue>2</issue>
<pages>257--286</pages>
<contexts>
<context position="14791" citStr="Rabiner, 1989" startWordPosition="2456" endWordPosition="2457">is work, we used bigram language model to calculate the probability of each path. Then we have: P e e e e P e ( , ,..., ,..., ) ( ) = ∏n = P e e ( |) (2) 1 2 i n 1 − k 2 k k 1 P(ek|ek-1) is estimated with a back-off bigram language model (Goodman, 2001). In the experiments with TREC6&amp;7&amp;8, we train the model with all text collections; while in the experiments with Gov2 data, we only used about 1/10 of the GOV2 data to train the bigram model because the whole Gov2 collection is too large. Directly calculating P(eij) by summing the probabilities of all paths passing through eij is an NP problem (Rabiner, 1989), and is intractable if the query is long. Therefore, we use the forwardbackward algorithm (Bishop, 2006) to calculate P(eij) in a more efficient way. After calculating P(eij) for each ci, we select one alteration which has the highest probability. We limit the number of additional alterations to 1 in order to limit query traffic. Our experiments will show that this is often sufficient. 5 Regression Model for Alteration Selection None of the previous selection methods considers how well an alteration would perform in retrieval. The Bigram Expansion model assumes that the query replaced with be</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner, L. (1989). A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. In Proceedings of IEEE Vol. 77(2), pp. 257-286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rijsbergen</author>
</authors>
<title>Information Retrieval. Butterworths, second version.</title>
<date>1979</date>
<marker>Rijsbergen, 1979</marker>
<rawString>Rijsbergen, V. (1979). Information Retrieval. Butterworths, second version.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strohman</author>
<author>D Metzler</author>
<author>H Turtle</author>
<author>B Croft</author>
</authors>
<title>Indri: A Language Model-based Search Engine for Complex Queries.</title>
<date>2004</date>
<booktitle>In Proceedings of the International conference on Intelligence Analysis.</booktitle>
<contexts>
<context position="24369" citStr="Strohman et al., 2004" startWordPosition="4138" endWordPosition="4141">very short, we use only the title field of each query. To correspond to Web search practice, both documents and queries are not stemmed. We do not filter the stop words either. Two main metrics are used: the Mean Average Precision (MAP) for the top 1000 documents to measure retrieval effectiveness, and the number of terms in the query to reflect query traffic. In addition, we also provide precision for the top 30 documents (P@30) to show the impact on top ranked documents. We also conducted t-tests to determine whether the improvement is statistically significant. The Indri 2.5 search engine (Strohman et al., 2004) is used as our basic retrieval system. It provides for a rich query language allowing disjunctive combinations of words in queries. 6.2 Experimental Results The first baseline method we compare with only uses the original query, which is named Original. In addition to this, we also compare with the following methods: Naïve Exp: The Naïve expansion model expands each query term with all terms in the vocabulary sharing the same root with it. This model is equivalent to the traditional stemming method. UMASS: This is the result reported in (Metzler et al., 2006) using Porter stemming for both do</context>
</contexts>
<marker>Strohman, Metzler, Turtle, Croft, 2004</marker>
<rawString>Strohman, T., Metzler, D. and Turtle, H., and Croft, B. (2004). Indri: A Language Model-based Search Engine for Complex Queries. In Proceedings of the International conference on Intelligence Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>B Croft</author>
</authors>
<title>Query Expansion Using Local and Global Document Analysis.</title>
<date>1996</date>
<booktitle>Proc. ACM SIGIR,</booktitle>
<pages>4--11</pages>
<contexts>
<context position="9919" citStr="Xu and Croft, 1996" startWordPosition="1616" endWordPosition="1619"> al. 2007). To limit the number of expansion terms, and thus the query traffic, Peng et al. only use alterations for some of the query words: They segment each query into phrases and only the head word in each phrase is expanded. The assumptions are: 1)Queries issued in Web search often consist of noun phrases. 2) Only the head word in the noun phrase varies in form and needs to be expanded. However, both assumptions may be questionable. Their experiments did not show that the two assumptions hold. Stemming is related to query expansion or query reformulation (Jones et al., 2006; Anick, 2003; Xu and Croft, 1996), although the latter is not limited to word variants. If the expansion terms used are those that are variant forms of a word, then query expansion can produce the same effect as word stemming. However, if we add all possible word alterations, query expansion/reformulation will run the risk of adding many unrelated terms to the original query, which may result in both heavy traffic and topic drift. Therefore, we need a way to select the most appropriate expansion terms. In (Peng et al. 2007), a bigram language model is used to determine the alteration of the head word that best fits the query.</context>
</contexts>
<marker>Xu, Croft, 1996</marker>
<rawString>Xu, J. and Croft, B. (1996). Query Expansion Using Local and Global Document Analysis. Proc. ACM SIGIR, pp. 4-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>B Croft</author>
</authors>
<title>Corpus-based Stemming Using Co-occurrence of Word Variants.</title>
<date>1998</date>
<journal>ACM TOIS,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>61--81</pages>
<contexts>
<context position="2190" citStr="Xu and Croft (1998)" startWordPosition="335" endWordPosition="338">mmer (Krovetz, 1993), deal with stemming by stripping word suffixes according to a set of morphological rules. Rule-based approaches are intuitive and easy to implement. However, while in general, most words can be stemmed correctly; there is often erroneous stemming that unifies unrelated words. For instance, “jobs” is stemmed to “job” in both “find jobs in Apple” and “Steve Jobs at Apple”. This is particularly problematic in Web search, where users often use special or new words in their queries. A standard stemmer such as Porter’s will wrongly stem them. To better determine stemming rules, Xu and Croft (1998) propose a selective stemming method based on corpus analysis. They refine the Porter stemmer by means of word clustering: words are first clustered according to their co-occurrences in the text collection. Only word variants belonging to the same cluster will be conflated. Despite this improvement, the basic idea of word stemming is to transform words in both documents and queries to a standard form. Once this is done, there is no means for users to require a specific word form in a query – the word form will be automatically transformed, otherwise, it will not match documents. This approach </context>
<context position="9051" citStr="Xu and Croft (1998)" startWordPosition="1465" endWordPosition="1468">tive”. On the other hand, it may miss some conflations, such as “mice” and “mouse”, “europe” and “european”. Krovetz (1993) developed another stemmer, which uses a machine-readable dictionary, to improve the Porter stemmer. It avoids some of the Porter stemmer’s wrong stripping, but does not produce consistent improvement in IR experiments. Both stemmers use generic rules for English to strip each word in isolation. In practice, the required stemming may vary from one text collection to another. Therefore, attempts have been made to use corpus analysis to improve existing rule-based stemmers. Xu and Croft (1998) create equivalence clusters of words which are morphologically similar and occur in similar contexts. As we stated earlier, the stemming-based IR approaches are not well suited to Web search. Query expansion has been used as an alternative (Peng et al. 2007). To limit the number of expansion terms, and thus the query traffic, Peng et al. only use alterations for some of the query words: They segment each query into phrases and only the head word in each phrase is expanded. The assumptions are: 1)Queries issued in Web search often consist of noun phrases. 2) Only the head word in the noun phra</context>
<context position="11702" citStr="Xu and Croft, 1998" startWordPosition="1909" endWordPosition="1912">tion of appropriate alterations for a query. The first step is query-independent using corpus analysis, while the second step is query-dependent. The selected word alterations will be OR-ed with the original query words. 3 Generating Alteration Candidates Our method to generate alteration candidates can be described as follows. First, we do word clustering using a Porter stemmer. All words in the vocabulary sharing the same root form are grouped together. Then we do corpus analysis to filter out the words which are clustered incorrectly, according to word distributional similarity, following (Xu and Croft, 1998; Lin 1998). The rationale behind this is that words sharing the same meaning tend to occur in the same contexts. The context of each word in the vocabulary is represented by a vector containing the frequencies of the context words which co-occur with the word within a predefined window in a training corpus. The window size is set empirically at 3 words and the training corpus is about 1/10 of the GOV2 corpus (see section 5 for details about the collection). Similarity is measured by the cosine distance between two vectors. For each word, we select at most 5 similar words as alteration candida</context>
<context position="28092" citStr="Xu and Croft (1998)" startWordPosition="4749" endWordPosition="4752">emming or word alteration expansion for IR. We can make several additional observations: 1). Stemming Vs Expansion. UMASS uses document and query stemming while Naive Exp uses expansion by word alteration. We stated that both approaches are equivalent. The equivalence is confirmed by our experiment results: for all Gov2 collections, these approaches perform equivalently. 2). The Similarity model performs very well. Compared with the Naïve Expansion model, it produces quite similar retrieval effectiveness, while the query traffic is dramatically reduced. This approach is similar to the work of Xu and Croft (1998), and can be considered as another state-ofthe-art result. 3). In comparison, the Bigram Expansion model performs better than the Similarity model. This shows that it is useful to consider query context in selecting word alterations. 4). The Regression model performs the best of all the models. Compared with the Original query, it adds fewer than 2 alterations for each query on average (since each group has 50 queries); nevertheless we obtained improvements on all the six collections. Moreover, the improvements on five collections are statistically significant. It also performs slightly better</context>
</contexts>
<marker>Xu, Croft, 1998</marker>
<rawString>Xu, J. and Croft, B. (1998). Corpus-based Stemming Using Co-occurrence of Word Variants. ACM TOIS, 16(1): 61-81.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>