<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003951">
<title confidence="0.998074">
SimCompass: Using Deep Learning Word Embeddings
to Assess Cross-level Similarity
</title>
<author confidence="0.975286">
Carmen Banea, Di Chen, Claire Cardie Janyce Wiebe
</author>
<affiliation confidence="0.7835595">
Rada Mihalcea∗ Cornell University University of Pittsburgh
University of Michigan Ithaca, NY Pittsburgh, PA
</affiliation>
<address confidence="0.631017">
Ann Arbor, MI
</address>
<sectionHeader confidence="0.940214" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998617">
This article presents our team’s partici-
pating system at SemEval-2014 Task 3.
Using a meta-learning framework, we
experiment with traditional knowledge-
based metrics, as well as novel corpus-
based measures based on deep learning
paradigms, paired with varying degrees of
context expansion. The framework en-
abled us to reach the highest overall per-
formance among all competing systems.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.976433652173913">
Semantic textual similarity is one of the key
components behind a multitude of natural lan-
guage processing applications, such as informa-
tion retrieval (Salton and Lesk, 1971), relevance
feedback and text classification (Rocchio, 1971),
word sense disambiguation (Lesk, 1986; Schutze,
1998), summarization (Salton et al., 1997; Lin
and Hovy, 2003), automatic evaluation of machine
translation (Papineni et al., 2002), plagiarism de-
tection (Nawab et al., 2011), and more.
To date, semantic similarity research has pri-
marily focused on comparing text snippets of simi-
lar length (see the semantic textual similarity tasks
organized during *Sem 2013 (Agirre et al., 2013)
and SemEval 2012 (Agirre et al., 2012)). Yet,
as new challenges emerge, such as augmenting a
knowledge-base with textual evidence, assessing
similarity across different context granularities is
gaining traction. The SemEval Cross-level seman-
tic similarity task is aimed at this latter scenario,
and is described in more details in the task paper
(Jurgens et al., 2014).
∗{carmennb,chenditc,mihalcea}@umich.edu
</bodyText>
<footnote confidence="0.5382265">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<sectionHeader confidence="0.99885" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999729088235294">
Over the past years, the research community has
focused on computing semantic relatedness us-
ing methods that are either knowledge-based or
corpus-based. Knowledge-based methods derive
a measure of relatedness by utilizing lexical re-
sources and ontologies such as WordNet (Miller,
1995) or Roget (Rog, 1995) to measure defi-
nitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy
as a measure of specificity. There are many
knowledge-based measures that were proposed in
the past, e.g., (Leacock and Chodorow, 1998;
Lesk, 1986; Resnik, 1995; Jiang and Conrath,
1997; Lin, 1998; Jarmasz and Szpakowicz, 2003;
Hughes and Ramage, 2007).
On the other side, corpus-based measures such
as Latent Semantic Analysis (LSA) (Landauer
and Dumais, 1997), Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007),
Salient Semantic Analysis (SSA) (Hassan and
Mihalcea, 2011), Pointwise Mutual Informa-
tion (PMI) (Church and Hanks, 1990), PMI-IR
(Turney, 2001), Second Order PMI (Islam and
Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al., 1998) and distributional
similarity (Lin, 1998) employ probabilistic ap-
proaches to decode the semantics of words. They
consist of unsupervised methods that utilize the
contextual information and patterns observed in
raw text to build semantic profiles of words. Un-
like knowledge-based methods, which suffer from
limited coverage, corpus-based measures are able
to induce the similarity between any two words, as
long as they appear in the corpus used for training.
</bodyText>
<sectionHeader confidence="0.988924" genericHeader="method">
3 System Description
</sectionHeader>
<subsectionHeader confidence="0.987259">
3.1 Generic Features
</subsectionHeader>
<bodyText confidence="0.98797">
Our system employs both knowledge and corpus-
based measures as detailed below.
</bodyText>
<page confidence="0.945663">
560
</page>
<note confidence="0.8023575">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560–565,
Dublin, Ireland, August 23-24, 2014.
</note>
<sectionHeader confidence="0.505009" genericHeader="method">
Knowledge-based features
</sectionHeader>
<bodyText confidence="0.9997886">
Knowledge-based metrics were shown to provide
high correlation scores with the goldstandard in
text similarity tasks (Agirre et al., 2012; Agirre et
al., 2013). We used three WordNet-based simi-
larity measures that employ information content.
We chose these metrics because they are able to
incorporate external information derived from a
large corpus: Resnik (Resnik, 1995) (RES), Lin
(Lin, 1998) (LIN), and Jiang &amp; Conrath (Jiang
and Conrath, 1997) (JCN).
</bodyText>
<subsectionHeader confidence="0.832273">
Corpus based features
</subsectionHeader>
<bodyText confidence="0.999911297297297">
Our corpus based features are derived from a
deep learning vector space model that is able to
“understand” word meaning without human in-
put. Distributed word embeddings are learned us-
ing a skip-gram recurrent neural net architecture
running over a large raw corpus (Mikolov et al.,
2013b; Mikolov et al., 2013a). A primary advan-
tage of such a model is that, by breaking away
from the typical n-gram model that sees individual
units with no relationship to each other, it is able to
generalize and produce word vectors that are simi-
lar for related words, thus encoding linguistic reg-
ularities and patterns (Mikolov et al., 2013b). For
example, vec(Madrid)-vec(Spain)+vec(France) is
closer to vec(Paris) than any other word vec-
tor (Mikolov et al., 2013a). We used the pre-
trained Google News word2vec model (WTV )
built over a 100 billion words corpus, and con-
taining 3 million 300-dimension vectors for words
and phrases. The model is distributed with the
word2vec toolkit. 1
Since the methods outlined above provide similar-
ity scores at the sense or word level, we derive text
level metrics by employing two methods.
VectorSum. We add the vectors corresponding to
the non-stopwords tokens in bag of words (BOW)
A and B, resulting in vectors VA and VB, respec-
tively. The assumption is that these vectors are
able to capture the semantic meaning associated
with the contexts, enabling us to gauge their relat-
edness using cosine similarity.
Align. Given two BOW A and B as input, we
compare them using a word-alignment-based sim-
ilarity measure (Mihalcea et al., 2006). We calcu-
late the pairwise similarity between the words in
A and B, and match each word in A with its most
similar counterpart in B. For corpus-based fea-
</bodyText>
<footnote confidence="0.905908">
1https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.999957">
tures, the similarity measure represents the aver-
age over these scores, while for knowledge-based
measures, we consider the top 40% ranking pairs.
We use the DKPro Similarity package (B¨ar et
al., 2013) to compute knowledge-based metrics,
and the word2vec implementation from the Gen-
sim toolkit (Rehurek and Sojka, 2010).
</bodyText>
<subsectionHeader confidence="0.999268">
3.2 Feature Variations
</subsectionHeader>
<bodyText confidence="0.949835761904762">
Since our system participated in all four lexical
levels evaluations, we describe below the modifi-
cations pertaining to each.
word2sense. At the word2sense level, we em-
ploy both knowledge and corpus-based features.
Since the information available in each pair is ex-
tremely limited (only a word and a sense key)
we infuse contextual information by drawing on
WordNet (Miller, 1995). In WordNet, the sense
of each word is encapsulated in a uniquely iden-
tifiable synset, consisting of the definition (gloss),
usage examples and its synonyms. We can derive
three variations (where the word and sense com-
ponents are represented by BOW A and B, respec-
tively): a) no expansion (A={word}, B={sense}),
b) expand right (R) (A={word}, B={sense gloss
&amp; example}), c) expand left (L) &amp; right (R)
(A={word glosses &amp; examples}, B={sense gloss
&amp; example}). After applying the Align method,
we obtain measures JNC, LIN, RES and
WTV 1; VectorSum results in WTV 2.
phrase2word. As this lexical level also suf-
fers from low context, we adapt the above vari-
ations, where the phrase and word components
are represented by BOW A and BOW B, re-
spectively. Thus, we have: a) no expan-
sion (A={phrase}, B={word}), b) expand R
(A={phrase}, B={word glosses and examples}),
c) expand L &amp; R (A={phrase glosses &amp; exam-
ples}, B={word glosses and examples}). We ex-
tract the same measures as for word2sense.
sentence2phrase. For this variation, we use only
corpus based measures; BOW A represents the
sentence component, B, the phrase. Since there is
sufficient context available, we follow the no ex-
pansion variation, and obtain metrics WTV 1 (by
applying Align) and WTV2 (using VectorSum).
paragraph2sentence. At this level, due to the
long context that entails one-to-many mappings
between the words in the sentence and those in
the paragraph, we use a text clustering technique
prior to calculating the features’ weights.
</bodyText>
<page confidence="0.986067">
561
</page>
<bodyText confidence="0.989215366666667">
a) no clustering. We use only corpus based mea-
sures, where the paragraph represents BOW A,
and the sentence represents BOW B. Then we ap-
ply Align and VectorSum, resulting in WTV 1 and
WTV 2, respectively.
b) paragraph centroids extraction. Since the
longer text contains more information compared
to the shorter one, we extract k topic vectors after
K-means clustering the left context.2 These cen-
troids are able to model topics permeating across
sentences, and by comparing them with the word
vectors pertaining to the short text, we seek to cap-
ture how much of the information is covered in the
shorter text. Each word is paired with the centroid
that it is closest to, and the average is computed
over these scores, resulting in WTV 3.
c) sentence centroids extraction. Under a dif-
ferent scenario, assuming that one sentence cov-
ers only a few strongly expressed topics, unlike
a paragraph that may digress and introduce unre-
lated noise, we apply clustering on the short text.
The centroids thus obtained are able to capture
the essence of the sentence, so when compared to
every word in the paragraph, we can gauge how
much of the short text is reflected in the longer
one. Each centroid is paired with the word that it is
most similar to, and we average these scores, thus
obtaining WTV 4. In a way, methods b) and c)
provide a macro, respectively micro view of how
the topics are reflected across the two spans of text.
</bodyText>
<subsectionHeader confidence="0.999521">
3.3 Meta-learning
</subsectionHeader>
<bodyText confidence="0.9999941875">
The measures of similarity described above pro-
vide a single score per each long text - short text
pair in the training and test data. These scores then
become features for a meta-learner, which is able
to optimize their impact on the prediction process.
We experimented with multiple regression algo-
rithms by conducting 10 fold cross-validation on
the training data. The strongest performer across
all lexical levels was Gaussian processes with a
radial basis function (RBF) kernel. Gaussian pro-
cesses regression is an efficient probabilistic pre-
diction framework that assumes a Gaussian pro-
cess prior on the unobservable (latent) functions
and a likelihood function that accounts for noise.
An individual classifier3 was trained for each lex-
ical level and applied to the test data sets.
</bodyText>
<footnote confidence="0.976452">
2Implementation provided in the Scikit library (Pedregosa
et al., 2011), where k is set to 3.
3Implementation available in the WEKA machine learn-
ing software (Hall et al., 2009) using the default parameters.
</footnote>
<sectionHeader confidence="0.962022" genericHeader="method">
4 Evaluations &amp; Discussion
</sectionHeader>
<bodyText confidence="0.99972828">
Our system participated in all cross-level subtasks
under the name SimCompass, competing with 37
other systems developed by 20 teams.
Figure 1 highlights the Pearson correlations at
the four lexical levels between the gold standard
and each similarity measure introduced in Section
3, as well as the predictions ensuing as a result
of meta-learning. The left and right histograms in
each subfigure present the scores obtained on the
train and test data, respectively.
In the case of word2sense train data, we no-
tice that expanding the context provides additional
information and improves the correlation results.
For corpus-based measures, the correlations are
stronger when the expansion involves only the
right side of the tuple, namely the sense. We
notice an increase of 0.04 correlation points for
WTV1 and 0.09 for WTV2. As soon as the word
is expanded as well, the context incorporates too
much noise, and the correlation levels drop. In
the case of knowledge-based measures, expanding
the context does not seem to impact the results.
However, these trends do not carry out to the test
data, where the corpus-based features without ex-
pansion reach a correlation higher than 0.3, while
the knowledge-based features score significantly
lower (by 0.16). Once all these measures are used
as features in a meta learner (All) using Gaus-
sian processes regression (GP), the correlation in-
creases over the level attained by the best perform-
ing individual feature, reaching 0.45 on the train
data and 0.36 on the test data. SimCompass ranks
second in this subtask’s evaluations, falling short
of the leading system by 0.025 correlation points.
Turning now to the phrase2word subfigure, we
notice that the context already carries sufficient
information, and expanding it causes the perfor-
mance to drop (the more extensive the expan-
sion, the steeper the drop). Unlike the scenario
encountered for word2sense, the trend observed
here on the training data also gets mirrored in the
test data. Same as before, knowledge-based mea-
sures have a significantly lower performance, but
deep learning-based features based on word2vec
(WTV) only show a correlation variation by at
most 0.05, proving their robustness. Leveraging
all the features in a meta-learning framework en-
ables the system to predict stronger scores for both
the train and the test data (0.48 and 0.42, respec-
tively). Actually, for this variation, SimCompass
</bodyText>
<page confidence="0.98237">
562
</page>
<figure confidence="0.999839793103448">
word2sense
phrase2word
sentence2phrase
paragraph2sentence
No expansion Expansion R Expansion L&amp;R All
0.5
0.8
0.4
0.6
0.3
0.4
0.2
0.2
0.1
0
0
Train Test
Train Test
0.5
0.4
0.3
0.2
0.1
0
0.8
0.6
0.4
0.2
0
</figure>
<figureCaption confidence="0.695206">
Figure 1: Pearson correlation of individual measures on the train and test data sets. As these measures be-
come features in a regression algorithm (GP), prediction correlations are included as well. BL represents
the baseline computed by the organizers.
</figureCaption>
<bodyText confidence="0.998433916666667">
obtains the highest score among all competing sys-
tems, surpassing the second best by 0.10.
Noticing that expansion is not helpful when suf-
ficient context is available, for the next variations
we use the original tuples. Also, due to the re-
duced impact of knowledge-based features on the
training outcome, we only focus on deep learning
features (WTV1, WTV2, WTV3, WTV4).
Shifting to sentence2phrase, WTV2 (con-
structed using VectorSum) is the top perform-
ing feature, surpassing the baseline by 0.19,
and attaining 0.69 and 0.73 on the train and
test sets, respectively. Despite also considering
a lower performing feature (WTV1), the meta-
learner maintains high scores, surpassing the cor-
relation achieved on the train data by 0.04 (from
0.70 to 0.74). In this variation, our system ranks
fifth, at 0.035 from the top system.
For the paragraph2sentence variation, due to
the availability of longer contexts, we introduce
WTV3 and WTV4 that are based on clustering the
left and the right sides of the tuple, respectively.
WTV2 fares slightly better than WTV3 and WTV4.
WTV1 surpasses the baseline this time, leaving its
mark on the decision process. When training the
GP learner on all features, we obtain 0.78 correla-
tion on the train data, and 0.81 on test data, 0.10
higher than those attained by the individual fea-
tures alone. SimCompass ranks seventh in perfor-
mance on this subtask, at 0.026 from the first.
Considering the overall system performance,
SimCompass is remarkably versatile, ranking
among the top at each lexical level, and taking the
first place in the SemEval Task 3 overall evalu-
ation with respect to both Pearson (0.58 average
correlation) and Spearman correlations.
</bodyText>
<sectionHeader confidence="0.993854" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999825625">
We described SimCompass, the system we partic-
ipated with at SemEval-2014 Task 3. Our exper-
iments suggest that traditional knowledge-based
features are cornered by novel corpus-based word
meaning representations, such as word2vec, which
emerge as efficient and strong performers under
a variety of scenarios. We also explored whether
context expansion is beneficial to the cross-level
similarity task, and remarked that only when the
context is particularly short, this enrichment is vi-
able. However, in a meta-learning framework, the
information permeating from a set of similarity
measures exposed to varying context expansions
can attain a higher performance than possible with
individual signals. Overall, our system ranked first
among 21 teams and 38 systems.
</bodyText>
<sectionHeader confidence="0.989709" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.930081142857143">
This material is based in part upon work sup-
ported by National Science Foundation CAREER
award #1361274 and IIS award #1018613 and
by DARPA-BAA-12-47 DEFT grant #12475008.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
</bodyText>
<page confidence="0.996773">
563
</page>
<bodyText confidence="0.840353">
of the National Science Foundation or the Defense
Advanced Research Projects Agency.
</bodyText>
<sectionHeader confidence="0.924211" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99764672815534">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational Se-
mantics (*SEM 2012), Montreal, Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic Textual Similarity, including a Pi-
lot on Typed-Similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013).
Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2013.
DKPro Similarity: An open source framework for
text similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 121–126,
Sofia, Bulgaria.
Curt Burgess, Kay Livesay, and Kevin Lund. 1998.
Explorations in context space: words, sentences,
discourse. Discourse Processes, 25(2):211–257.
Kenneth Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22–29.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, pages 1606–1611, Hyderabad, India.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1).
Samer Hassan and Rada Mihalcea. 2011. Measuring
semantic relatedness using salient encyclopedic con-
cepts. Artificial Intelligence, Special Issue.
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic knowledge with random graph walks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, Prague, Czech
Republic.
Aminul Islam and Diana Zaiu Inkpen. 2006. Second
order co-occurrence PMI for determining the seman-
tic similarity of words. In Proceedings of the Fifth
Conference on Language Resources and Evaluation,
volume 2, pages 1033–1038, Genoa, Italy, July.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s
thesaurus and semantic similarity. In Proceedings
of the conference on Recent Advances in Natural
Language Processing RANLP-2003, Borovetz, Bul-
garia, September.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceeding of the International Confer-
ence Research on Computational Linguistics (RO-
CLING X), Taiwan.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato’s problem: The latent semantic
analysis theory of acquisition, induction, and repre-
sentation of knowledge. Psychological Review, 104.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet sense similarity
for word sense identification. In WordNet, An Elec-
tronic Lexical Database. The MIT Press.
Michael E. Lesk. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: How to
tell a pine cone from an ice cream cone. In Pro-
ceedings of the SIGDOC Conference 1986, Toronto,
June.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings ofHuman Lan-
guage Technology Conference (HLT-NAACL 2003),
Edmonton, Canada, May.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296–304, Madison, Wisconsin.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In American
Association for Artificial Intelligence (AAAI-2006),
Boston, MA.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Distributed Representations of Words
and Phrases and their Compositionality . In NIPS,
pages 3111–3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In NAACL HLT, pages 746–
751, Atlanta, GA, USA.
George A. Miller. 1995. WordNet: a Lexical database
for English. Communications of the Association for
Computing Machinery, 38(11):39–41.
Rao Muhammad Adeel Nawab, Mark Stevenson, and
Paul Clough. 2011. External plagiarism detection
using information retrieval and sequence alignment:
</reference>
<page confidence="0.983361">
564
</page>
<reference confidence="0.998857959183673">
Notebook for PAN at CLEF 2011. In Proceedings
of the 5th International Workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuse
(PAN 2011).
Kishore. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
PA.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ´Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. The Jour-
nal of Machine Learning Research, 12:2825–2830.
Radim Rehurek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Valletta,
Malta, May. ELRA.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence, pages 448–453, Montreal,
Quebec, Canada. Morgan Kaufmann Publishers Inc.
J. Rocchio, 1971. Relevance feedback in information
retrieval. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
1995. Roget’s II: The New Thesaurus. Houghton Mif-
flin.
Gerard Salton and Michael E. Lesk, 1971. The SMART
Retrieval System: Experiments in Automatic Doc-
ument Processing, chapter Computer evaluation of
indexing and text processing. Prentice Hall, Ing. En-
glewood Cliffs, New Jersey.
Gerard Salton, Amit Singhal, Mandar Mitra, and Chris
Buckley. 1997. Automatic text structuring and sum-
marization. Information Processing and Manage-
ment, 2(32).
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
124.
Peter D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML’01), pages 491–502, Freiburg, Germany.
</reference>
<page confidence="0.998484">
565
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930619">
<title confidence="0.9764085">SimCompass: Using Deep Learning Word to Assess Cross-level Similarity</title>
<author confidence="0.999044">Carmen Banea</author>
<author confidence="0.999044">Claire Cardie Janyce Wiebe Di_Chen</author>
<affiliation confidence="0.999197">University University of Pittsburgh University of Michigan Ithaca, NY Pittsburgh,</affiliation>
<address confidence="0.983329">Ann Arbor, MI</address>
<abstract confidence="0.999473727272727">This article presents our team’s participating system at SemEval-2014 Task 3. Using a meta-learning framework, we experiment with traditional knowledgebased metrics, as well as novel corpusbased measures based on deep learning paradigms, paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 Task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012),</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="1365" citStr="Agirre et al., 2012" startWordPosition="198" endWordPosition="201">l language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this latter scenario, and is described in more details in the task paper (Jurgens et al., 2014). ∗{carmennb,chenditc,mihalcea}@umich.edu This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 2 Related Work O</context>
<context position="3935" citStr="Agirre et al., 2012" startWordPosition="569" endWordPosition="572">knowledge-based methods, which suffer from limited coverage, corpus-based measures are able to induce the similarity between any two words, as long as they appear in the corpus used for training. 3 System Description 3.1 Generic Features Our system employs both knowledge and corpusbased measures as detailed below. 560 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560–565, Dublin, Ireland, August 23-24, 2014. Knowledge-based features Knowledge-based metrics were shown to provide high correlation scores with the goldstandard in text similarity tasks (Agirre et al., 2012; Agirre et al., 2013). We used three WordNet-based similarity measures that employ information content. We chose these metrics because they are able to incorporate external information derived from a large corpus: Resnik (Resnik, 1995) (RES), Lin (Lin, 1998) (LIN), and Jiang &amp; Conrath (Jiang and Conrath, 1997) (JCN). Corpus based features Our corpus based features are derived from a deep learning vector space model that is able to “understand” word meaning without human input. Distributed word embeddings are learned using a skip-gram recurrent neural net architecture running over a large raw </context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A pilot on semantic textual similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>Shared Task: Semantic Textual Similarity, including a Pilot on Typed-Similarity.</title>
<date>2013</date>
<booktitle>In The Second Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<publisher>SEM</publisher>
<contexts>
<context position="1326" citStr="Agirre et al., 2013" startWordPosition="191" endWordPosition="194">components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this latter scenario, and is described in more details in the task paper (Jurgens et al., 2014). ∗{carmennb,chenditc,mihalcea}@umich.edu This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommon</context>
<context position="3957" citStr="Agirre et al., 2013" startWordPosition="573" endWordPosition="576">ds, which suffer from limited coverage, corpus-based measures are able to induce the similarity between any two words, as long as they appear in the corpus used for training. 3 System Description 3.1 Generic Features Our system employs both knowledge and corpusbased measures as detailed below. 560 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560–565, Dublin, Ireland, August 23-24, 2014. Knowledge-based features Knowledge-based metrics were shown to provide high correlation scores with the goldstandard in text similarity tasks (Agirre et al., 2012; Agirre et al., 2013). We used three WordNet-based similarity measures that employ information content. We chose these metrics because they are able to incorporate external information derived from a large corpus: Resnik (Resnik, 1995) (RES), Lin (Lin, 1998) (LIN), and Jiang &amp; Conrath (Jiang and Conrath, 1997) (JCN). Corpus based features Our corpus based features are derived from a deep learning vector space model that is able to “understand” word meaning without human input. Distributed word embeddings are learned using a skip-gram recurrent neural net architecture running over a large raw corpus (Mikolov et al.</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 Shared Task: Semantic Textual Similarity, including a Pilot on Typed-Similarity. In The Second Joint Conference on Lexical and Computational Semantics (*SEM 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>DKPro Similarity: An open source framework for text similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>121--126</pages>
<location>Sofia, Bulgaria.</location>
<marker>B¨ar, Zesch, Gurevych, 2013</marker>
<rawString>Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2013. DKPro Similarity: An open source framework for text similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 121–126, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Curt Burgess</author>
<author>Kay Livesay</author>
<author>Kevin Lund</author>
</authors>
<title>Explorations in context space: words, sentences, discourse.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="3052" citStr="Burgess et al., 1998" startWordPosition="441" endWordPosition="444">based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledge-based methods, which suffer from limited coverage, corpus-based measures are able to induce the similarity between any two words, as long as they appear in the corpus used for training. 3 System Description 3.1 Generic Features Our system employs both knowledge and corpusbased measures as detailed below. 560 Proceedings of t</context>
</contexts>
<marker>Burgess, Livesay, Lund, 1998</marker>
<rawString>Curt Burgess, Kay Livesay, and Kevin Lund. 1998. Explorations in context space: words, sentences, discourse. Discourse Processes, 25(2):211–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="2929" citStr="Church and Hanks, 1990" startWordPosition="422" endWordPosition="425">rm distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledge-based methods, which suffer from limited coverage, corpus-based measures are able to induce the similarity between any two words, as long as they appear in the corpus used for training. 3 System Descript</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1606--1611</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="2807" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="405" endWordPosition="408">y utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledge-based methods, which suffer from limited coverage, corpus-based measures are able</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipediabased explicit semantic analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 1606–1611, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="10676" citStr="Hall et al., 2009" startWordPosition="1678" endWordPosition="1681"> the training data. The strongest performer across all lexical levels was Gaussian processes with a radial basis function (RBF) kernel. Gaussian processes regression is an efficient probabilistic prediction framework that assumes a Gaussian process prior on the unobservable (latent) functions and a likelihood function that accounts for noise. An individual classifier3 was trained for each lexical level and applied to the test data sets. 2Implementation provided in the Scikit library (Pedregosa et al., 2011), where k is set to 3. 3Implementation available in the WEKA machine learning software (Hall et al., 2009) using the default parameters. 4 Evaluations &amp; Discussion Our system participated in all cross-level subtasks under the name SimCompass, competing with 37 other systems developed by 20 teams. Figure 1 highlights the Pearson correlations at the four lexical levels between the gold standard and each similarity measure introduced in Section 3, as well as the predictions ensuing as a result of meta-learning. The left and right histograms in each subfigure present the scores obtained on the train and test data, respectively. In the case of word2sense train data, we notice that expanding the context</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samer Hassan</author>
<author>Rada Mihalcea</author>
</authors>
<title>Measuring semantic relatedness using salient encyclopedic concepts.</title>
<date>2011</date>
<journal>Artificial Intelligence, Special Issue.</journal>
<contexts>
<context position="2868" citStr="Hassan and Mihalcea, 2011" startWordPosition="413" endWordPosition="416">, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledge-based methods, which suffer from limited coverage, corpus-based measures are able to induce the similarity between any two words, as long as t</context>
</contexts>
<marker>Hassan, Mihalcea, 2011</marker>
<rawString>Samer Hassan and Rada Mihalcea. 2011. Measuring semantic relatedness using salient encyclopedic concepts. Artificial Intelligence, Special Issue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thad Hughes</author>
<author>Daniel Ramage</author>
</authors>
<title>Lexical semantic knowledge with random graph walks.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2629" citStr="Hughes and Ramage, 2007" startWordPosition="380" endWordPosition="383">ty has focused on computing semantic relatedness using methods that are either knowledge-based or corpus-based. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual inf</context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>Thad Hughes and Daniel Ramage. 2007. Lexical semantic knowledge with random graph walks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Zaiu Inkpen</author>
</authors>
<title>Second order co-occurrence PMI for determining the semantic similarity of words.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth Conference on Language Resources and Evaluation,</booktitle>
<volume>2</volume>
<pages>1033--1038</pages>
<location>Genoa, Italy,</location>
<contexts>
<context position="2995" citStr="Islam and Inkpen, 2006" startWordPosition="432" endWordPosition="435">nomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledge-based methods, which suffer from limited coverage, corpus-based measures are able to induce the similarity between any two words, as long as they appear in the corpus used for training. 3 System Description 3.1 Generic Features Our system employs both knowledge and cor</context>
</contexts>
<marker>Islam, Inkpen, 2006</marker>
<rawString>Aminul Islam and Diana Zaiu Inkpen. 2006. Second order co-occurrence PMI for determining the semantic similarity of words. In Proceedings of the Fifth Conference on Language Resources and Evaluation, volume 2, pages 1033–1038, Genoa, Italy, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Jarmasz</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Roget’s thesaurus and semantic similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the conference on Recent Advances in Natural Language Processing RANLP-2003,</booktitle>
<location>Borovetz, Bulgaria,</location>
<contexts>
<context position="2603" citStr="Jarmasz and Szpakowicz, 2003" startWordPosition="376" endWordPosition="379">st years, the research community has focused on computing semantic relatedness using methods that are either knowledge-based or corpus-based. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that </context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s thesaurus and semantic similarity. In Proceedings of the conference on Recent Advances in Natural Language Processing RANLP-2003, Borovetz, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceeding of the International Conference Research on Computational Linguistics (ROCLING X),</booktitle>
<contexts>
<context position="2562" citStr="Jiang and Conrath, 1997" startWordPosition="370" endWordPosition="373">s/by/4.0/ 2 Related Work Over the past years, the research community has focused on computing semantic relatedness using methods that are either knowledge-based or corpus-based. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. T</context>
<context position="4247" citStr="Jiang and Conrath, 1997" startWordPosition="617" endWordPosition="620">d below. 560 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560–565, Dublin, Ireland, August 23-24, 2014. Knowledge-based features Knowledge-based metrics were shown to provide high correlation scores with the goldstandard in text similarity tasks (Agirre et al., 2012; Agirre et al., 2013). We used three WordNet-based similarity measures that employ information content. We chose these metrics because they are able to incorporate external information derived from a large corpus: Resnik (Resnik, 1995) (RES), Lin (Lin, 1998) (LIN), and Jiang &amp; Conrath (Jiang and Conrath, 1997) (JCN). Corpus based features Our corpus based features are derived from a deep learning vector space model that is able to “understand” word meaning without human input. Distributed word embeddings are learned using a skip-gram recurrent neural net architecture running over a large raw corpus (Mikolov et al., 2013b; Mikolov et al., 2013a). A primary advantage of such a model is that, by breaking away from the typical n-gram model that sees individual units with no relationship to each other, it is able to generalize and produce word vectors that are similar for related words, thus encoding li</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceeding of the International Conference Research on Computational Linguistics (ROCLING X), Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval-2014 Task 3: Cross-Level Semantic Similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1695" citStr="Jurgens et al., 2014" startWordPosition="247" endWordPosition="250">), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this latter scenario, and is described in more details in the task paper (Jurgens et al., 2014). ∗{carmennb,chenditc,mihalcea}@umich.edu This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 2 Related Work Over the past years, the research community has focused on computing semantic relatedness using methods that are either knowledge-based or corpus-based. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional </context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. SemEval-2014 Task 3: Cross-Level Semantic Similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<contexts>
<context position="2738" citStr="Landauer and Dumais, 1997" startWordPosition="396" endWordPosition="399">sed. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledge-based method</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet sense similarity for word sense identification. In WordNet, An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="2511" citStr="Leacock and Chodorow, 1998" startWordPosition="362" endWordPosition="365">s. Licence details: http://creativecommons.org/licenses/by/4.0/ 2 Related Work Over the past years, the research community has focused on computing semantic relatedness using methods that are either knowledge-based or corpus-based. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabili</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and WordNet sense similarity for word sense identification. In WordNet, An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the SIGDOC Conference</booktitle>
<location>Toronto,</location>
<contexts>
<context position="933" citStr="Lesk, 1986" startWordPosition="133" endWordPosition="134">val-2014 Task 3. Using a meta-learning framework, we experiment with traditional knowledgebased metrics, as well as novel corpusbased measures based on deep learning paradigms, paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining trac</context>
<context position="2523" citStr="Lesk, 1986" startWordPosition="366" endWordPosition="367">reativecommons.org/licenses/by/4.0/ 2 Related Work Over the past years, the research community has focused on computing semantic relatedness using methods that are either knowledge-based or corpus-based. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approac</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael E. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the SIGDOC Conference 1986, Toronto, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram cooccurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings ofHuman Language Technology Conference (HLT-NAACL 2003),</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="1006" citStr="Lin and Hovy, 2003" startWordPosition="142" endWordPosition="145">with traditional knowledgebased metrics, as well as novel corpusbased measures based on deep learning paradigms, paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this l</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings ofHuman Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<location>Madison, Wisconsin.</location>
<contexts>
<context position="2573" citStr="Lin, 1998" startWordPosition="374" endWordPosition="375">Over the past years, the research community has focused on computing semantic relatedness using methods that are either knowledge-based or corpus-based. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist</context>
<context position="4194" citStr="Lin, 1998" startWordPosition="610" endWordPosition="611">dge and corpusbased measures as detailed below. 560 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560–565, Dublin, Ireland, August 23-24, 2014. Knowledge-based features Knowledge-based metrics were shown to provide high correlation scores with the goldstandard in text similarity tasks (Agirre et al., 2012; Agirre et al., 2013). We used three WordNet-based similarity measures that employ information content. We chose these metrics because they are able to incorporate external information derived from a large corpus: Resnik (Resnik, 1995) (RES), Lin (Lin, 1998) (LIN), and Jiang &amp; Conrath (Jiang and Conrath, 1997) (JCN). Corpus based features Our corpus based features are derived from a deep learning vector space model that is able to “understand” word meaning without human input. Distributed word embeddings are learned using a skip-gram recurrent neural net architecture running over a large raw corpus (Mikolov et al., 2013b; Mikolov et al., 2013a). A primary advantage of such a model is that, by breaking away from the typical n-gram model that sees individual units with no relationship to each other, it is able to generalize and produce word vectors</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the Fifteenth International Conference on Machine Learning, pages 296–304, Madison, Wisconsin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In American Association for Artificial Intelligence (AAAI-2006),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="5840" citStr="Mihalcea et al., 2006" startWordPosition="881" endWordPosition="884">ted with the word2vec toolkit. 1 Since the methods outlined above provide similarity scores at the sense or word level, we derive text level metrics by employing two methods. VectorSum. We add the vectors corresponding to the non-stopwords tokens in bag of words (BOW) A and B, resulting in vectors VA and VB, respectively. The assumption is that these vectors are able to capture the semantic meaning associated with the contexts, enabling us to gauge their relatedness using cosine similarity. Align. Given two BOW A and B as input, we compare them using a word-alignment-based similarity measure (Mihalcea et al., 2006). We calculate the pairwise similarity between the words in A and B, and match each word in A with its most similar counterpart in B. For corpus-based fea1https://code.google.com/p/word2vec/ tures, the similarity measure represents the average over these scores, while for knowledge-based measures, we consider the top 40% ranking pairs. We use the DKPro Similarity package (B¨ar et al., 2013) to compute knowledge-based metrics, and the word2vec implementation from the Gensim toolkit (Rehurek and Sojka, 2010). 3.2 Feature Variations Since our system participated in all four lexical levels evaluat</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In American Association for Artificial Intelligence (AAAI-2006), Boston, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<booktitle>2013a. Distributed Representations of Words and Phrases and their Compositionality . In NIPS,</booktitle>
<pages>3111--3119</pages>
<marker>Mikolov, Chen, Corrado, Dean, </marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Distributed Representations of Words and Phrases and their Compositionality . In NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In NAACL HLT,</booktitle>
<pages>746--751</pages>
<location>Atlanta, GA, USA.</location>
<contexts>
<context position="4563" citStr="Mikolov et al., 2013" startWordPosition="668" endWordPosition="671"> et al., 2013). We used three WordNet-based similarity measures that employ information content. We chose these metrics because they are able to incorporate external information derived from a large corpus: Resnik (Resnik, 1995) (RES), Lin (Lin, 1998) (LIN), and Jiang &amp; Conrath (Jiang and Conrath, 1997) (JCN). Corpus based features Our corpus based features are derived from a deep learning vector space model that is able to “understand” word meaning without human input. Distributed word embeddings are learned using a skip-gram recurrent neural net architecture running over a large raw corpus (Mikolov et al., 2013b; Mikolov et al., 2013a). A primary advantage of such a model is that, by breaking away from the typical n-gram model that sees individual units with no relationship to each other, it is able to generalize and produce word vectors that are similar for related words, thus encoding linguistic regularities and patterns (Mikolov et al., 2013b). For example, vec(Madrid)-vec(Spain)+vec(France) is closer to vec(Paris) than any other word vector (Mikolov et al., 2013a). We used the pretrained Google News word2vec model (WTV ) built over a 100 billion words corpus, and containing 3 million 300-dimensi</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In NAACL HLT, pages 746– 751, Atlanta, GA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a Lexical database for English.</title>
<date>1995</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2249" citStr="Miller, 1995" startWordPosition="321" endWordPosition="322">ed in more details in the task paper (Jurgens et al., 2014). ∗{carmennb,chenditc,mihalcea}@umich.edu This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 2 Related Work Over the past years, the research community has focused on computing semantic relatedness using methods that are either knowledge-based or corpus-based. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan </context>
<context position="6756" citStr="Miller, 1995" startWordPosition="1025" endWordPosition="1026">onsider the top 40% ranking pairs. We use the DKPro Similarity package (B¨ar et al., 2013) to compute knowledge-based metrics, and the word2vec implementation from the Gensim toolkit (Rehurek and Sojka, 2010). 3.2 Feature Variations Since our system participated in all four lexical levels evaluations, we describe below the modifications pertaining to each. word2sense. At the word2sense level, we employ both knowledge and corpus-based features. Since the information available in each pair is extremely limited (only a word and a sense key) we infuse contextual information by drawing on WordNet (Miller, 1995). In WordNet, the sense of each word is encapsulated in a uniquely identifiable synset, consisting of the definition (gloss), usage examples and its synonyms. We can derive three variations (where the word and sense components are represented by BOW A and B, respectively): a) no expansion (A={word}, B={sense}), b) expand right (R) (A={word}, B={sense gloss &amp; example}), c) expand left (L) &amp; right (R) (A={word glosses &amp; examples}, B={sense gloss &amp; example}). After applying the Align method, we obtain measures JNC, LIN, RES and WTV 1; VectorSum results in WTV 2. phrase2word. As this lexical level</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a Lexical database for English. Communications of the Association for Computing Machinery, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rao Muhammad Adeel Nawab</author>
<author>Mark Stevenson</author>
<author>Paul Clough</author>
</authors>
<title>External plagiarism detection using information retrieval and sequence alignment: Notebook for PAN at CLEF</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse (PAN</booktitle>
<contexts>
<context position="1118" citStr="Nawab et al., 2011" startWordPosition="158" endWordPosition="161"> paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this latter scenario, and is described in more details in the task paper (Jurgens et al., 2014). ∗{carmennb,chenditc,m</context>
</contexts>
<marker>Nawab, Stevenson, Clough, 2011</marker>
<rawString>Rao Muhammad Adeel Nawab, Mark Stevenson, and Paul Clough. 2011. External plagiarism detection using information retrieval and sequence alignment: Notebook for PAN at CLEF 2011. In Proceedings of the 5th International Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse (PAN 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Salim Roukos Papineni</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1075" citStr="Papineni et al., 2002" startWordPosition="151" endWordPosition="154">sed measures based on deep learning paradigms, paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this latter scenario, and is described in more details in the task paper (J</context>
</contexts>
<marker>Papineni, Ward, Zhu, 2002</marker>
<rawString>Kishore. Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. The Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim Rehurek</author>
<author>Petr Sojka</author>
</authors>
<title>Software framework for topic modelling with large corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<publisher>ELRA.</publisher>
<location>Valletta, Malta,</location>
<contexts>
<context position="6351" citStr="Rehurek and Sojka, 2010" startWordPosition="960" endWordPosition="963">two BOW A and B as input, we compare them using a word-alignment-based similarity measure (Mihalcea et al., 2006). We calculate the pairwise similarity between the words in A and B, and match each word in A with its most similar counterpart in B. For corpus-based fea1https://code.google.com/p/word2vec/ tures, the similarity measure represents the average over these scores, while for knowledge-based measures, we consider the top 40% ranking pairs. We use the DKPro Similarity package (B¨ar et al., 2013) to compute knowledge-based metrics, and the word2vec implementation from the Gensim toolkit (Rehurek and Sojka, 2010). 3.2 Feature Variations Since our system participated in all four lexical levels evaluations, we describe below the modifications pertaining to each. word2sense. At the word2sense level, we employ both knowledge and corpus-based features. Since the information available in each pair is extremely limited (only a word and a sense key) we infuse contextual information by drawing on WordNet (Miller, 1995). In WordNet, the sense of each word is encapsulated in a uniquely identifiable synset, consisting of the definition (gloss), usage examples and its synonyms. We can derive three variations (wher</context>
</contexts>
<marker>Rehurek, Sojka, 2010</marker>
<rawString>Radim Rehurek and Petr Sojka. 2010. Software framework for topic modelling with large corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="2537" citStr="Resnik, 1995" startWordPosition="368" endWordPosition="369">ns.org/licenses/by/4.0/ 2 Related Work Over the past years, the research community has focused on computing semantic relatedness using methods that are either knowledge-based or corpus-based. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode </context>
<context position="4171" citStr="Resnik, 1995" startWordPosition="606" endWordPosition="607">system employs both knowledge and corpusbased measures as detailed below. 560 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560–565, Dublin, Ireland, August 23-24, 2014. Knowledge-based features Knowledge-based metrics were shown to provide high correlation scores with the goldstandard in text similarity tasks (Agirre et al., 2012; Agirre et al., 2013). We used three WordNet-based similarity measures that employ information content. We chose these metrics because they are able to incorporate external information derived from a large corpus: Resnik (Resnik, 1995) (RES), Lin (Lin, 1998) (LIN), and Jiang &amp; Conrath (Jiang and Conrath, 1997) (JCN). Corpus based features Our corpus based features are derived from a deep learning vector space model that is able to “understand” word meaning without human input. Distributed word embeddings are learned using a skip-gram recurrent neural net architecture running over a large raw corpus (Mikolov et al., 2013b; Mikolov et al., 2013a). A primary advantage of such a model is that, by breaking away from the typical n-gram model that sees individual units with no relationship to each other, it is able to generalize a</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448–453, Montreal, Quebec, Canada. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rocchio</author>
</authors>
<title>Relevance feedback in information retrieval.</title>
<date>1971</date>
<publisher>Prentice Hall,</publisher>
<location>Ing. Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="894" citStr="Rocchio, 1971" startWordPosition="128" endWordPosition="129">nts our team’s participating system at SemEval-2014 Task 3. Using a meta-learning framework, we experiment with traditional knowledgebased metrics, as well as novel corpusbased measures based on deep learning paradigms, paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across differen</context>
</contexts>
<marker>Rocchio, 1971</marker>
<rawString>J. Rocchio, 1971. Relevance feedback in information retrieval. Prentice Hall, Ing. Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<title>Roget’s II: The New Thesaurus.</title>
<date>1995</date>
<publisher>Houghton Mifflin.</publisher>
<marker>1995</marker>
<rawString>1995. Roget’s II: The New Thesaurus. Houghton Mifflin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael E Lesk</author>
</authors>
<title>The SMART Retrieval System: Experiments in Automatic Document Processing, chapter Computer evaluation of indexing and text processing.</title>
<date>1971</date>
<publisher>Prentice Hall,</publisher>
<location>Ing. Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="834" citStr="Salton and Lesk, 1971" startWordPosition="119" endWordPosition="122"> Ithaca, NY Pittsburgh, PA Ann Arbor, MI Abstract This article presents our team’s participating system at SemEval-2014 Task 3. Using a meta-learning framework, we experiment with traditional knowledgebased metrics, as well as novel corpusbased measures based on deep learning paradigms, paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base</context>
</contexts>
<marker>Salton, Lesk, 1971</marker>
<rawString>Gerard Salton and Michael E. Lesk, 1971. The SMART Retrieval System: Experiments in Automatic Document Processing, chapter Computer evaluation of indexing and text processing. Prentice Hall, Ing. Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Amit Singhal</author>
<author>Mandar Mitra</author>
<author>Chris Buckley</author>
</authors>
<title>Automatic text structuring and summarization.</title>
<date>1997</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>2</volume>
<issue>32</issue>
<contexts>
<context position="985" citStr="Salton et al., 1997" startWordPosition="138" endWordPosition="141">ework, we experiment with traditional knowledgebased metrics, as well as novel corpusbased measures based on deep learning paradigms, paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity ta</context>
</contexts>
<marker>Salton, Singhal, Mitra, Buckley, 1997</marker>
<rawString>Gerard Salton, Amit Singhal, Mandar Mitra, and Chris Buckley. 1997. Automatic text structuring and summarization. Information Processing and Management, 2(32).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>124</pages>
<contexts>
<context position="949" citStr="Schutze, 1998" startWordPosition="135" endWordPosition="136">k 3. Using a meta-learning framework, we experiment with traditional knowledgebased metrics, as well as novel corpusbased measures based on deep learning paradigms, paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEva</context>
</contexts>
<marker>Schutze, 1998</marker>
<rawString>Hinrich Schutze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning (ECML’01),</booktitle>
<pages>491--502</pages>
<location>Freiburg, Germany.</location>
<contexts>
<context position="2952" citStr="Turney, 2001" startWordPosition="427" endWordPosition="428">xonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledge-based methods, which suffer from limited coverage, corpus-based measures are able to induce the similarity between any two words, as long as they appear in the corpus used for training. 3 System Description 3.1 Generic Feature</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the 12th European Conference on Machine Learning (ECML’01), pages 491–502, Freiburg, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>