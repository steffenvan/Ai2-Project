<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001533">
<title confidence="0.997599">
Unsupervized Word Segmentation:
the case for Mandarin Chinese
</title>
<author confidence="0.901898">
Pierre Magistry
</author>
<affiliation confidence="0.702099">
Alpage, INRIA &amp; Univ. Paris 7,
</affiliation>
<address confidence="0.9912205">
175 rue du Chevaleret,
75013 Paris, France
</address>
<email confidence="0.995304">
pierre.magistry@inria.fr
</email>
<author confidence="0.604542">
Benoît Sagot
</author>
<affiliation confidence="0.476908">
Alpage, INRIA &amp; Univ. Paris 7,
</affiliation>
<address confidence="0.988531">
175 rue du Chevaleret,
75013 Paris, France
</address>
<email confidence="0.998433">
benoit.sagot@inria.fr
</email>
<sectionHeader confidence="0.995492" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.902889941176471">
In this paper, we present an unsupervized seg-
mentation system tested on Mandarin Chi-
nese. Following Harris&apos;s Hypothesis in Kempe
(1999) and Tanaka-Ishii&apos;s (2005) reformulation,
we base our work on the Variation of Branching
Entropy. We improve on (Jin and Tanaka-Ishii,
2006) by adding normalization and viterbi-
decoding. This enable us to remove most of
the thresholds and parameters from their model
and to reach near state-of-the-art results (Wang
et al., 2011) with a simpler system. We provide
evaluation on different corpora available from
the Segmentation bake-off II (Emerson, 2005)
and define a more precise topline for the task
using cross-trained supervized system available
off-the-shelf (Zhang and Clark, 2010; Zhao and
Kit, 2008; Huang and Zhao, 2007)
</bodyText>
<sectionHeader confidence="0.998886" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999989125">
The Chinese script has no explicit “word” bound-
aries. Therefore, tokenization itself, although the
very first step of many text processing systems, is
a challenging task. Supervized segmentation sys-
tems exist but rely on manually segmented corpora,
which are often specific to a genre or a domain and
use many different segmentation guidelines. In order
to deal with a larger variety of genres and domains,
or to tackle more theoretic questions about linguistic
units, unsupervized segmentation is still an impor-
tant issue. After a short review of the corresponding
literature in Section 2, we discuss the challenging is-
sue of evaluating unsupervized word segmentation
systems in Section 3. Section 4 and Section 5 present
the core of our system. Finally, in Section 6, we de-
tail and discuss our results.
</bodyText>
<sectionHeader confidence="0.47938" genericHeader="method">
2 State of the Art
</sectionHeader>
<bodyText confidence="0.999368294117647">
Unsupervized word segmentation systems tend to
make use of three different types of information: the
cohesion of the resulting units (e.g., Mutual Infor-
mation, as in (Sproat and Shih, 1990)), the degree of
separation between the resulting units (e.g., Acces-
sor Variety, see (Feng et al., 2004)) and the proba-
bility of a segmentation given a string (Goldwater et
al., 2006; Mochihashi et al., 2009).
A recently published work by Wang et al. (2011)
introduce ESA: “Evaluation, Selection, Adjust-
ment.” This method combines cohesion and separa-
tion measures in a “goodness” metric that is maxi-
mized during an iterative process. This work is the
current state-of-the-art in unsupervized segmenta-
tion of Mandarin Chinese data.
The main drawbacks of ESA are the need to iterate
the process on the corpus around 10 times to reach
good performance levels and the need to set a param-
eter that balances the impact of the cohesion measure
w.r.t. the separation measure. Empirically, a corre-
lation is found between the parameter and the size of
the corpus but this correlation depends on the script
used in the corpus (it changes if Latin letters and
Arabic numbers are taken into account during pre-
processing or not). Moreover, computing this cor-
relation and finding the best value for the parameter
(i.e., what the authors call the proper exponent) re-
quires a manually segmented training corpus. There-
fore, this proper exponent may not be easily available
in all situations. However, if we only consider their
experiments using settings similar to ours, their re-
sults consistently lie around an f-score of 0.80.
An older approach, introduced by Jin and Tanaka-
Ishii (2006), solely relies on a separation measure
</bodyText>
<page confidence="0.993647">
383
</page>
<note confidence="0.6844855">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 383–387,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999789131578948">
that is directly inspired by a linguistic hypothesis for-
mulated by Harris (1955). In Tanaka-Ishii (2005)
(following Kempe (1999)) who use Branching En-
tropy (BE), this hypothesis goes as follows: if se-
quences produced by human language were random,
we would expect the Branching Entropy of a se-
quence (estimated from the n-grams in a corpus)
to decrease as we increase the length of the se-
quence. Therefore the variation of the branching en-
tropy (VBE) should be negative. When we observe
that it is not the case, Harris hypothesizes that we
are at a linguistic boundary. Following this hypoth-
esis, (Jin and Tanaka-Ishii, 2006) propose a system
that segments when BE is rising or when it reach a
certain maximum.
The main drawback of Jin and Tanaka-Ishii (2006)
model is that segmentation decisions are taken very
locally1 and do not depend on neighboring cuts.
Moreover, this system also also relies on parameters,
namely the threshold on the VBE above which the
system decides to segment (in their system, this is
when VBE&gt; 0). In theory, we could expect a de-
creasing BE and look for a less decreasing value (or
on the contrary, rising at least to some extent). A
threshold of 0 can be seen as a default value. Fi-
nally, Jin and Tanaka-Ishii do not take in account that
VBE of n-gram may not be directly comparable to
the VBE of m-grams if m # n. A normalization is
needed (as in (Cohen et al., 2002)).
Due to space constraints, we shall not describe
here other systems than those by Wang et al. (2011)
and Jin and Tanaka-Ishii (2006). A more compre-
hensive state of the art can be found in (Zhao and
Kit, 2008) and (Wang et al., 2011).
In this paper we will show that we can correct the
drawbacks of Jin and Tanaka-Ishii (2006) model and
reach performances comparable to those of Wang et
al. (2011) with as simpler system.
</bodyText>
<sectionHeader confidence="0.997624" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9996875">
In this paper, in order to be comparable with
Wang et al. (2011), we evaluate our system against
the corpora from the Second International Chi-
nese Word Segmentation Bakeoff (Emerson, 2005).
These corpora cover 4 different segmentation guide-
lines from various origins: Academia Sinica (AS),
City-University of Hong-Kong (CITYU), Microsoft
Research (MSR) and Peking University (PKU).
</bodyText>
<footnote confidence="0.593549">
1Jin (2007) uses self-training with MDL to address this issue.
</footnote>
<bodyText confidence="0.999762795918368">
Evaluating unsupervized systems is a challenge by
itself. As an agreement on the exact definition of
what a word is remains hard to reach, various seg-
mentation guidelines have been proposed and fol-
lowed for the annotation of different corpora. The
evaluation of supervized systems can be achieved on
any corpus using any guidelines: when trained on
data that follows particular guidelines, the resulting
system will follow as well as possible these guide-
lines, and can be evaluated on data annotated accord-
ingly. However, for unsupervized systems, there is
no reason why a system should be closer to one ref-
erence than another or even not to lie somewhere
in between the different existing guidelines. Huang
and Zhao (2007) propose to use cross-training of a
supervized segmentation system in order to have an
estimation of the consistency between different seg-
mentation guidelines, and therefore an upper bound
of what can be expected from an unsupervized sys-
tem (Zhao and Kit, 2008). The average consistency
is found to be as low as 0.85 (f-score). Therefore
this figure can be considered as a sensible topline for
unsupervized systems. The standard baseline which
consists in segmenting each character leads to a base-
line around 0.35 (f-score) — almost half of the to-
kens in a manually segmented corpus are unigrams.
Per word-length evaluation is also important as
units of various lengths tend to have different distri-
butions. We used ZPAR (Zhang and Clark, 2010) on
the four corpora from the Second Bakeoff to repro-
duce Huang and Zhao&apos;s (2007) experiments, but also
to measure cross-corpus consistency at a per-word-
length level. Our overall results are comparable to
what Huang and Zhao (2007) report. However, the
consistency is quickly falling for longer words: on
unigrams, f-scores range from 0.81 to 0.90 (the same
as the overall results). We get slightly higher figures
on bigrams (0.85–0.92) but much lower on trigrams
with only 0.59–0.79. In a segmented Chinese text,
most of the tokens are uni- and bigrams but most of
the types are bi- and trigrams (as unigrams are often
high frequency grammatical words and trigrams the
result of more or less productive affixations). There-
fore the results of evaluations only based on tokens
do not suffer much from poor performances on tri-
grams even if a large part of the lexicon may be in-
correctly processed.
Another issue about the evaluation and compari-
son of unsupervized systems is to try and remain fair
</bodyText>
<page confidence="0.995059">
384
</page>
<bodyText confidence="0.99994068">
in terms of preprocessing and prior knowledge given
to the systems. For example, Wang et al. (2011)
used different levels of preprocessing (which they
call “settings”). In their settings 1 and 2, Wang et
al. (2011) try not to rely on punctuation and char-
acter encoding information (such as distinguishing
Latin and Chinese characters). However, they opti-
mize their parameter for each setting. We therefore
consider that their system does take into account the
level of processing which is performed on Latin char-
acters and Arabic numbers, and therefore “knows”
whether to expect such characters or not. In set-
ting 3 they add the knowledge of punctuation as clear
boundaries and in setting 4 they preprocess Arabic
and Latin and obtain better, more consistent and less
questionable results.
As we are more interested in reducing the amount
of human labor needed than in achieving by all
means fully unsupervized learning, we do not re-
frain from performing basic and straightforward pre-
processing such as detection of punctuation marks,
Latin characters and Arabic numbers.2 Therefore,
our experiments rely on settings similar to their set-
tings 3 and 4, and are evaluated against the same
corpora.
</bodyText>
<sectionHeader confidence="0.9528195" genericHeader="method">
4 Normalized Variation of Branching
Entropy (nVBE)
</sectionHeader>
<bodyText confidence="0.995811857142857">
Our system builds upon Harris&apos;s (1955) hypothesis
and its reformulation by Kempe (1999) and Tanaka-
Ishii (2005). Let us now define formally the notions
underlying our system.
Given an n-gram x0..n = x0..1 x1..2 ... xn−1..n
with a left context X→, we define its Right Branching
Entropy (RBE) as:
</bodyText>
<equation confidence="0.995592">
h→(x0..n) = H(X→  |x0..n)
∑= − P(x  |x0..n) log P(x  |x0..n).
x∈x—
</equation>
<bodyText confidence="0.936135666666667">
The Left Branching Entropy (LBE) is defined in a
symmetric way: if we note X← the right context of
x0..n, its LBE is defined as:/
</bodyText>
<equation confidence="0.997763">
h←(x0..n) = H(X←  |x0..n).
</equation>
<bodyText confidence="0.999304333333333">
The RBE (resp. LBE) can be considered as x0..n&apos;s
Branching Entropy (BE) when reading from left to
right (resp. right to left).
</bodyText>
<footnote confidence="0.5814795">
2Simple regular expressions could also be considered to deal
with unambiguous cases of numbers and dates in Chinese script.
</footnote>
<bodyText confidence="0.9997915">
From h→(x0..n) and h→(x0..n−1) on the one hand,
and from h←(x0..n) and h←(x1..n) we estimate the
Variation ofBranching Entropy (VBE) in both direc-
tions, defined as follows:
</bodyText>
<equation confidence="0.999991">
Sh→(x0..n) = h→(x0..n) − h→(x0..n−1)
Sh←(x0..n) = h←(x0..n) − h←(x1..n).
</equation>
<bodyText confidence="0.998795">
The VBEs are not directly comparable for strings
of different lengths and need to be normalized. In
this work, we recenter them around 0 with respect to
the length of the string by substracting the mean of
the VBEs of the strings of the same length. Writing
</bodyText>
<equation confidence="0.366249">
�Sh→(x) and �
</equation>
<bodyText confidence="0.997606769230769">
string x, or nVBEs, are then defined as follow (we
only defined �Sh←(x) for clarity reasons): for each
length k and each k-gram x such that len(x) = k,
�Sh→(x) = Sh→(x)−µ→,k, where µ→,k is the mean
of the values of Sh→(x) of all k-grams x.
Note that we use and normalize the variation of
branching entropy and not the branching entropy it-
self. Doing so would break the Harris&apos;s hypothesis as
we would not expect �h(x0..n) &lt; �h(x0..n−1) in non-
boundary situation anymore. Many studies use di-
rectly the branching entropy (normalized or not) and
report results that are below state-of-the-art systems
(Cohen et al., 2002).
</bodyText>
<sectionHeader confidence="0.975039" genericHeader="method">
5 Decoding algorithm
</sectionHeader>
<bodyText confidence="0.969292727272727">
If we follow Harris&apos;s hypothesis and consider com-
plex morphological word structures, we expect a
large VBE at the boundaries of interesting units and
more unstable variations inside “words.” This expec-
tation was confirmed by empirical data visualization.
For different lengths of n-grams, we compared the
distributions of the VBEs at different positions inside
the n-gram and at its boundaries. By plotting density
distributions for words vs. non-words, we observed
that the VBE at both boundaries were the most dis-
criminative value. Therefore, we decided to take in
account the VBE only at the word-candidate bound-
aries (left and right) and not to consider the inner val-
ues. Two interesting consequences of this decision
are: first, all Sh(x) can be precomputed as they do
not depend on the context. Second, best segmenta-
tion can be computed using dynamic programming.
Since we consider the VBE only at words bound-
ary, we can define for any n-gram w its autonomy as
a(x) = �S←h(x) + �Sh→(x). The more an n-gram is
autonomous, the more likely it is to be a word.
Sh←(x). The normalized VBEs for the
</bodyText>
<page confidence="0.984966">
385
</page>
<bodyText confidence="0.9995936">
With this measure, we can redefine the sentence
segmentation problem as the maximization of the au-
tonomy measure of its words. For a character se-
quence s, if we call Seg(s) the set of all the possible
segmentations, then we are looking for:
</bodyText>
<equation confidence="0.9508325">
∑arg max a(wi) · len(wi),
WESeg(s) ,WiEW
</equation>
<bodyText confidence="0.999866333333333">
where W is the segmentation corresponding to the
sequence of words w0ws ... w,,,,, and len(wi) is the
length of a word wi used here to be able to com-
pare segmentations resulting in a different number
of words. This best segmentation can be computed
easily using dynamic programming.
</bodyText>
<sectionHeader confidence="0.9991" genericHeader="conclusions">
6 Results and discussion
</sectionHeader>
<bodyText confidence="0.999983363636364">
We tested our system against the data from the 4 cor-
pora of the Second Bakeoff, in both settings 3 and 4,
as described in Section 3. Overall results are given
in Table 1 and per-word-length results in Table 2.
Our results (nVBE) show significant improve-
ments over Jin&apos;s (2006) strategy (VBE &gt; 0) and
are closely competing with ESA. But contrarily to
ESA (Wang et al., 2011), it does not require multi-
ple iterations on the corpus and it does not rely on
any parameters. This shows that we can rely solely
on a separation measure and get high segmentation
scores. When maximized over a sentence, this mea-
sure captures at least in part what can be modeled by
a cohesion measure without the need for fine-tuning
the balance between the two.
The evolution of the results w.r.t. word length is
consistent with the supervized cross-evaluation re-
sults of the various segmentation guidelines as per-
formed in Section 3.
Due to space constraints, we cannot detail here a
qualitative analysis of the results. We can simply
mention that the errors we observed are consistent
with previous systems based on Harris&apos;s hypothesis
(see (Magistry and Sagot, 2011) and Jin (2007) for a
longer discussion). Many errors are related to dates
and Chinese numbers. This could and should be
dealt with during preprocessing. Other errors often
involve frequent grammatical morphemes or produc-
tive affixes. These errors are often interesting for lin-
guists and could be studied as such and/or corrected
in a post-processing stage that would introduce lin-
guistic knowledge. Indeed, unlike content words,
grammatical morphemes belongs to closed classes,
</bodyText>
<table confidence="0.9992364">
System AS CITYU PKU MSR
Setting 3
ESA worst 0.729 0.795 0.781 0.768
ESA best 0.782 0.816 0.795 0.802
nVBE 0.758 0.775 0.781 0.798
Setting 4
VBE &gt; 0 0.63 0.640 0.703 0.713
ESA worst 0.732 0.809 0.784 0.784
ESA best 0.786 0.829 0.800 0.818
nVBE 0.766 0.767 0.800 0.813
</table>
<tableCaption confidence="0.979666428571429">
Table 1: Evaluation on the Second Bakeoff data with
Wang et al.&apos;s (2011) settings. “Worst” and “best” give the
range of the reported results with differents values of the
parameter in Wang et al.&apos;s system. VBE &gt; 0 correspond
to a cut whenever BE is raising. nVBE corresponds to our
proposal, based on normalized VBE with maximization at
word boundaries. Recall that the topline is around 0.85
</tableCaption>
<table confidence="0.9997124">
Corpus overall unigrams bigrams trigrams
AS 0.766 0.741 0.828 0.494
CITYU 0.767 0.739 0.834 0.555
PKU 0.800 0.789 0.855 0.451
MSR 0.813 0.823 0.856 0.482
</table>
<tableCaption confidence="0.953675333333333">
Table 2: Per word-length details of our results with our
nVBE algorithm and setting 4. Recall that the toplines
are respectively 0.85, 0.81, 0.85 and 0.59 (see Section 3)
</tableCaption>
<bodyText confidence="0.999943714285714">
therefore introducing this linguistic knowledge into
the system may be of great help without requiring
to much human effort. A sensible way to go in that
direction would be to let unsupervized system deal
with open classes and process closed classes with a
symbolic or supervized module.
One can also observe that our system performs bet-
ter on PKU and MSR corpora. As PKU is the small-
est corpus and AS the biggest, size alone cannot ex-
plain this result. However, PKU is more consistent
in genre as it contains only articles from the Peo-
ple&apos;s Daily. On the other end, AS is a balanced cor-
pus with a greater variety in many aspects. CITYU
Corpus is almost as small as PKU but contains arti-
cles from newspapers of various Mandarin Chinese
speaking communities where great variation is to be
expected. This suggest that consistency of the input
data is as important as the amount of data. This hy-
pothesis has to be confirmed in futur studies. If it is,
automatic clustering of the input data may be an im-
portant pre-processing step for this kind of systems.
</bodyText>
<page confidence="0.998012">
386
</page>
<sectionHeader confidence="0.98967" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999427119402985">
Paul Cohen, Brent Heeringa, and Niall Adams. 2002.
An unsupervised algorithm for segmenting categorical
timeseries into episodes. Pattern Detection and Dis-
covery, page 117–133.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of the
Fourth SIGHAN Workshop on Chinese Language Pro-
cessing, volume 133.
Haodi Feng, Kang Chen, Xiaotie Deng, and Weiming
Zheng. 2004. Accessor variety criteria for Chi-
nese word extraction. Computational Linguistics,
30(1):75–93.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics, page 673–680.
Zellig S. Harris. 1955. From phoneme to morpheme.
Language, 31(2):190–222.
Changning. Huang and Hai Zhao. 2007. 中文分词十年
回顾 (Chinese word segmentation: A decade review).
Journal of Chinese Information Processing, 21(3):8–
20.
Zhihui Jin and Kumiko Tanaka-Ishii. 2006. Unsuper-
vised segmentation of Chinese text by use of branching
entropy. In Proceedings of the COLING/ACL on Main
conference poster sessions, page 428–435.
Zhihui Jin. 2007. A Study On Unsupervised Segmenta-
tion Of Text Using Contextual Complexity. Ph.D. the-
sis, University of Tokyo.
André Kempe. 1999. Experiments in unsupervised
entropy-based corpus segmentation. In Workshop of
EACL in Computational Natural Language Learning,
page 7–13.
Pierre Magistry and Benoît Sagot. 2011. Segmentation
et induction de lexique non-supervisées du mandarin.
In TALN&apos;2011 - Traitement Automatique des Langues
Naturelles, Montpellier, France, June. ATALA.
Daichi Mochihashi, Takeshi. Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1-Volume 1, page 100–108.
Richard W. Sproat and Chilin Shih. 1990. A statis-
tical method for finding word boundaries in Chinese
text. Computer Processing of Chinese and Oriental
Languages, 4(4):336–351.
Kumiko Tanaka-Ishii. 2005. Entropy as an indicator of
context boundaries: An experiment using a web search
engine. In IJCNLP, page 93–105.
Hanshi Wang, Jian Zhu, Shiping Tang, and Xiaozhong
Fan. 2011. A new unsupervised approach to word
segmentation. Computational Linguistics, 37(3):421–
454.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using a
single discriminative model. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, page 843–852.
Hai Zhao and Chunyu Kit. 2008. An empirical compar-
ison of goodness measures for unsupervised Chinese
word segmentation with a unified framework. In The
Third International Joint Conference on Natural Lan-
guage Processing (IJCNLP2008), Hyderabad, India.
</reference>
<page confidence="0.998288">
387
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.171140">
<title confidence="0.916589">Unsupervized Word the case for Mandarin Chinese</title>
<author confidence="0.862063">Pierre</author>
<affiliation confidence="0.921699">Alpage, INRIA &amp; Univ. Paris</affiliation>
<address confidence="0.9899055">175 rue du 75013 Paris,</address>
<email confidence="0.994336">pierre.magistry@inria.fr</email>
<affiliation confidence="0.723963">Benoît Alpage, INRIA &amp; Univ. Paris</affiliation>
<address confidence="0.986159">175 rue du 75013 Paris,</address>
<email confidence="0.998656">benoit.sagot@inria.fr</email>
<abstract confidence="0.992500882352941">In this paper, we present an unsupervized segmentation system tested on Mandarin Chinese. Following Harris&apos;s Hypothesis in Kempe (1999) and Tanaka-Ishii&apos;s (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin and Tanaka-Ishii, 2006) by adding normalization and viterbidecoding. This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on different corpora available from the Segmentation bake-off II (Emerson, 2005) and define a more precise topline for the task using cross-trained supervized system available off-the-shelf (Zhang and Clark, 2010; Zhao and</abstract>
<intro confidence="0.440172">Kit, 2008; Huang and Zhao, 2007</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paul Cohen</author>
<author>Brent Heeringa</author>
<author>Niall Adams</author>
</authors>
<title>An unsupervised algorithm for segmenting categorical timeseries into episodes. Pattern Detection and Discovery,</title>
<date>2002</date>
<pages>117--133</pages>
<contexts>
<context position="5195" citStr="Cohen et al., 2002" startWordPosition="846" endWordPosition="849">tation decisions are taken very locally1 and do not depend on neighboring cuts. Moreover, this system also also relies on parameters, namely the threshold on the VBE above which the system decides to segment (in their system, this is when VBE&gt; 0). In theory, we could expect a decreasing BE and look for a less decreasing value (or on the contrary, rising at least to some extent). A threshold of 0 can be seen as a default value. Finally, Jin and Tanaka-Ishii do not take in account that VBE of n-gram may not be directly comparable to the VBE of m-grams if m # n. A normalization is needed (as in (Cohen et al., 2002)). Due to space constraints, we shall not describe here other systems than those by Wang et al. (2011) and Jin and Tanaka-Ishii (2006). A more comprehensive state of the art can be found in (Zhao and Kit, 2008) and (Wang et al., 2011). In this paper we will show that we can correct the drawbacks of Jin and Tanaka-Ishii (2006) model and reach performances comparable to those of Wang et al. (2011) with as simpler system. 3 Evaluation In this paper, in order to be comparable with Wang et al. (2011), we evaluate our system against the corpora from the Second International Chinese Word Segmentation</context>
<context position="11690" citStr="Cohen et al., 2002" startWordPosition="1927" endWordPosition="1930">� string x, or nVBEs, are then defined as follow (we only defined �Sh←(x) for clarity reasons): for each length k and each k-gram x such that len(x) = k, �Sh→(x) = Sh→(x)−µ→,k, where µ→,k is the mean of the values of Sh→(x) of all k-grams x. Note that we use and normalize the variation of branching entropy and not the branching entropy itself. Doing so would break the Harris&apos;s hypothesis as we would not expect �h(x0..n) &lt; �h(x0..n−1) in nonboundary situation anymore. Many studies use directly the branching entropy (normalized or not) and report results that are below state-of-the-art systems (Cohen et al., 2002). 5 Decoding algorithm If we follow Harris&apos;s hypothesis and consider complex morphological word structures, we expect a large VBE at the boundaries of interesting units and more unstable variations inside “words.” This expectation was confirmed by empirical data visualization. For different lengths of n-grams, we compared the distributions of the VBEs at different positions inside the n-gram and at its boundaries. By plotting density distributions for words vs. non-words, we observed that the VBE at both boundaries were the most discriminative value. Therefore, we decided to take in account th</context>
</contexts>
<marker>Cohen, Heeringa, Adams, 2002</marker>
<rawString>Paul Cohen, Brent Heeringa, and Niall Adams. 2002. An unsupervised algorithm for segmenting categorical timeseries into episodes. Pattern Detection and Discovery, page 117–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>133</volume>
<contexts>
<context position="884" citStr="Emerson, 2005" startWordPosition="129" endWordPosition="130">.sagot@inria.fr Abstract In this paper, we present an unsupervized segmentation system tested on Mandarin Chinese. Following Harris&apos;s Hypothesis in Kempe (1999) and Tanaka-Ishii&apos;s (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin and Tanaka-Ishii, 2006) by adding normalization and viterbidecoding. This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on different corpora available from the Segmentation bake-off II (Emerson, 2005) and define a more precise topline for the task using cross-trained supervized system available off-the-shelf (Zhang and Clark, 2010; Zhao and Kit, 2008; Huang and Zhao, 2007) 1 Introduction The Chinese script has no explicit “word” boundaries. Therefore, tokenization itself, although the very first step of many text processing systems, is a challenging task. Supervized segmentation systems exist but rely on manually segmented corpora, which are often specific to a genre or a domain and use many different segmentation guidelines. In order to deal with a larger variety of genres and domains, or</context>
<context position="5819" citStr="Emerson, 2005" startWordPosition="958" endWordPosition="959">space constraints, we shall not describe here other systems than those by Wang et al. (2011) and Jin and Tanaka-Ishii (2006). A more comprehensive state of the art can be found in (Zhao and Kit, 2008) and (Wang et al., 2011). In this paper we will show that we can correct the drawbacks of Jin and Tanaka-Ishii (2006) model and reach performances comparable to those of Wang et al. (2011) with as simpler system. 3 Evaluation In this paper, in order to be comparable with Wang et al. (2011), we evaluate our system against the corpora from the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005). These corpora cover 4 different segmentation guidelines from various origins: Academia Sinica (AS), City-University of Hong-Kong (CITYU), Microsoft Research (MSR) and Peking University (PKU). 1Jin (2007) uses self-training with MDL to address this issue. Evaluating unsupervized systems is a challenge by itself. As an agreement on the exact definition of what a word is remains hard to reach, various segmentation guidelines have been proposed and followed for the annotation of different corpora. The evaluation of supervized systems can be achieved on any corpus using any guidelines: when train</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haodi Feng</author>
<author>Kang Chen</author>
<author>Xiaotie Deng</author>
<author>Weiming Zheng</author>
</authors>
<title>Accessor variety criteria for Chinese word extraction.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="2192" citStr="Feng et al., 2004" startWordPosition="338" endWordPosition="341">till an important issue. After a short review of the corresponding literature in Section 2, we discuss the challenging issue of evaluating unsupervized word segmentation systems in Section 3. Section 4 and Section 5 present the core of our system. Finally, in Section 6, we detail and discuss our results. 2 State of the Art Unsupervized word segmentation systems tend to make use of three different types of information: the cohesion of the resulting units (e.g., Mutual Information, as in (Sproat and Shih, 1990)), the degree of separation between the resulting units (e.g., Accessor Variety, see (Feng et al., 2004)) and the probability of a segmentation given a string (Goldwater et al., 2006; Mochihashi et al., 2009). A recently published work by Wang et al. (2011) introduce ESA: “Evaluation, Selection, Adjustment.” This method combines cohesion and separation measures in a “goodness” metric that is maximized during an iterative process. This work is the current state-of-the-art in unsupervized segmentation of Mandarin Chinese data. The main drawbacks of ESA are the need to iterate the process on the corpus around 10 times to reach good performance levels and the need to set a parameter that balances th</context>
</contexts>
<marker>Feng, Chen, Deng, Zheng, 2004</marker>
<rawString>Haodi Feng, Kang Chen, Xiaotie Deng, and Weiming Zheng. 2004. Accessor variety criteria for Chinese word extraction. Computational Linguistics, 30(1):75–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>673--680</pages>
<contexts>
<context position="2270" citStr="Goldwater et al., 2006" startWordPosition="352" endWordPosition="355">ure in Section 2, we discuss the challenging issue of evaluating unsupervized word segmentation systems in Section 3. Section 4 and Section 5 present the core of our system. Finally, in Section 6, we detail and discuss our results. 2 State of the Art Unsupervized word segmentation systems tend to make use of three different types of information: the cohesion of the resulting units (e.g., Mutual Information, as in (Sproat and Shih, 1990)), the degree of separation between the resulting units (e.g., Accessor Variety, see (Feng et al., 2004)) and the probability of a segmentation given a string (Goldwater et al., 2006; Mochihashi et al., 2009). A recently published work by Wang et al. (2011) introduce ESA: “Evaluation, Selection, Adjustment.” This method combines cohesion and separation measures in a “goodness” metric that is maximized during an iterative process. This work is the current state-of-the-art in unsupervized segmentation of Mandarin Chinese data. The main drawbacks of ESA are the need to iterate the process on the corpus around 10 times to reach good performance levels and the need to set a parameter that balances the impact of the cohesion measure w.r.t. the separation measure. Empirically, a</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, page 673–680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>From phoneme to morpheme.</title>
<date>1955</date>
<journal>Language,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="3876" citStr="Harris (1955)" startWordPosition="612" endWordPosition="613">ed training corpus. Therefore, this proper exponent may not be easily available in all situations. However, if we only consider their experiments using settings similar to ours, their results consistently lie around an f-score of 0.80. An older approach, introduced by Jin and TanakaIshii (2006), solely relies on a separation measure 383 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 383–387, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics that is directly inspired by a linguistic hypothesis formulated by Harris (1955). In Tanaka-Ishii (2005) (following Kempe (1999)) who use Branching Entropy (BE), this hypothesis goes as follows: if sequences produced by human language were random, we would expect the Branching Entropy of a sequence (estimated from the n-grams in a corpus) to decrease as we increase the length of the sequence. Therefore the variation of the branching entropy (VBE) should be negative. When we observe that it is not the case, Harris hypothesizes that we are at a linguistic boundary. Following this hypothesis, (Jin and Tanaka-Ishii, 2006) propose a system that segments when BE is rising or wh</context>
</contexts>
<marker>Harris, 1955</marker>
<rawString>Zellig S. Harris. 1955. From phoneme to morpheme. Language, 31(2):190–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huang</author>
<author>Hai Zhao</author>
</authors>
<title>(Chinese word segmentation: A decade review).</title>
<date>2007</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>21</volume>
<issue>3</issue>
<pages>20</pages>
<contexts>
<context position="1059" citStr="Huang and Zhao, 2007" startWordPosition="154" endWordPosition="157">aka-Ishii&apos;s (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin and Tanaka-Ishii, 2006) by adding normalization and viterbidecoding. This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on different corpora available from the Segmentation bake-off II (Emerson, 2005) and define a more precise topline for the task using cross-trained supervized system available off-the-shelf (Zhang and Clark, 2010; Zhao and Kit, 2008; Huang and Zhao, 2007) 1 Introduction The Chinese script has no explicit “word” boundaries. Therefore, tokenization itself, although the very first step of many text processing systems, is a challenging task. Supervized segmentation systems exist but rely on manually segmented corpora, which are often specific to a genre or a domain and use many different segmentation guidelines. In order to deal with a larger variety of genres and domains, or to tackle more theoretic questions about linguistic units, unsupervized segmentation is still an important issue. After a short review of the corresponding literature in Sect</context>
<context position="6799" citStr="Huang and Zhao (2007)" startWordPosition="1110" endWordPosition="1113">hat a word is remains hard to reach, various segmentation guidelines have been proposed and followed for the annotation of different corpora. The evaluation of supervized systems can be achieved on any corpus using any guidelines: when trained on data that follows particular guidelines, the resulting system will follow as well as possible these guidelines, and can be evaluated on data annotated accordingly. However, for unsupervized systems, there is no reason why a system should be closer to one reference than another or even not to lie somewhere in between the different existing guidelines. Huang and Zhao (2007) propose to use cross-training of a supervized segmentation system in order to have an estimation of the consistency between different segmentation guidelines, and therefore an upper bound of what can be expected from an unsupervized system (Zhao and Kit, 2008). The average consistency is found to be as low as 0.85 (f-score). Therefore this figure can be considered as a sensible topline for unsupervized systems. The standard baseline which consists in segmenting each character leads to a baseline around 0.35 (f-score) — almost half of the tokens in a manually segmented corpus are unigrams. Per</context>
</contexts>
<marker>Huang, Zhao, 2007</marker>
<rawString>Changning. Huang and Hai Zhao. 2007. 中文分词十年 回顾 (Chinese word segmentation: A decade review). Journal of Chinese Information Processing, 21(3):8– 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhihui Jin</author>
<author>Kumiko Tanaka-Ishii</author>
</authors>
<title>Unsupervised segmentation of Chinese text by use of branching entropy.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>428--435</pages>
<contexts>
<context position="4421" citStr="Jin and Tanaka-Ishii, 2006" startWordPosition="702" endWordPosition="705">s that is directly inspired by a linguistic hypothesis formulated by Harris (1955). In Tanaka-Ishii (2005) (following Kempe (1999)) who use Branching Entropy (BE), this hypothesis goes as follows: if sequences produced by human language were random, we would expect the Branching Entropy of a sequence (estimated from the n-grams in a corpus) to decrease as we increase the length of the sequence. Therefore the variation of the branching entropy (VBE) should be negative. When we observe that it is not the case, Harris hypothesizes that we are at a linguistic boundary. Following this hypothesis, (Jin and Tanaka-Ishii, 2006) propose a system that segments when BE is rising or when it reach a certain maximum. The main drawback of Jin and Tanaka-Ishii (2006) model is that segmentation decisions are taken very locally1 and do not depend on neighboring cuts. Moreover, this system also also relies on parameters, namely the threshold on the VBE above which the system decides to segment (in their system, this is when VBE&gt; 0). In theory, we could expect a decreasing BE and look for a less decreasing value (or on the contrary, rising at least to some extent). A threshold of 0 can be seen as a default value. Finally, Jin a</context>
</contexts>
<marker>Jin, Tanaka-Ishii, 2006</marker>
<rawString>Zhihui Jin and Kumiko Tanaka-Ishii. 2006. Unsupervised segmentation of Chinese text by use of branching entropy. In Proceedings of the COLING/ACL on Main conference poster sessions, page 428–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhihui Jin</author>
</authors>
<title>A Study On Unsupervised Segmentation Of Text Using Contextual Complexity.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Tokyo.</institution>
<contexts>
<context position="6024" citStr="Jin (2007)" startWordPosition="985" endWordPosition="986">g et al., 2011). In this paper we will show that we can correct the drawbacks of Jin and Tanaka-Ishii (2006) model and reach performances comparable to those of Wang et al. (2011) with as simpler system. 3 Evaluation In this paper, in order to be comparable with Wang et al. (2011), we evaluate our system against the corpora from the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005). These corpora cover 4 different segmentation guidelines from various origins: Academia Sinica (AS), City-University of Hong-Kong (CITYU), Microsoft Research (MSR) and Peking University (PKU). 1Jin (2007) uses self-training with MDL to address this issue. Evaluating unsupervized systems is a challenge by itself. As an agreement on the exact definition of what a word is remains hard to reach, various segmentation guidelines have been proposed and followed for the annotation of different corpora. The evaluation of supervized systems can be achieved on any corpus using any guidelines: when trained on data that follows particular guidelines, the resulting system will follow as well as possible these guidelines, and can be evaluated on data annotated accordingly. However, for unsupervized systems, </context>
<context position="14565" citStr="Jin (2007)" startWordPosition="2421" endWordPosition="2422">tion scores. When maximized over a sentence, this measure captures at least in part what can be modeled by a cohesion measure without the need for fine-tuning the balance between the two. The evolution of the results w.r.t. word length is consistent with the supervized cross-evaluation results of the various segmentation guidelines as performed in Section 3. Due to space constraints, we cannot detail here a qualitative analysis of the results. We can simply mention that the errors we observed are consistent with previous systems based on Harris&apos;s hypothesis (see (Magistry and Sagot, 2011) and Jin (2007) for a longer discussion). Many errors are related to dates and Chinese numbers. This could and should be dealt with during preprocessing. Other errors often involve frequent grammatical morphemes or productive affixes. These errors are often interesting for linguists and could be studied as such and/or corrected in a post-processing stage that would introduce linguistic knowledge. Indeed, unlike content words, grammatical morphemes belongs to closed classes, System AS CITYU PKU MSR Setting 3 ESA worst 0.729 0.795 0.781 0.768 ESA best 0.782 0.816 0.795 0.802 nVBE 0.758 0.775 0.781 0.798 Settin</context>
</contexts>
<marker>Jin, 2007</marker>
<rawString>Zhihui Jin. 2007. A Study On Unsupervised Segmentation Of Text Using Contextual Complexity. Ph.D. thesis, University of Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André Kempe</author>
</authors>
<title>Experiments in unsupervised entropy-based corpus segmentation.</title>
<date>1999</date>
<booktitle>In Workshop of EACL in Computational Natural Language Learning,</booktitle>
<pages>7--13</pages>
<contexts>
<context position="3924" citStr="Kempe (1999)" startWordPosition="618" endWordPosition="619">nt may not be easily available in all situations. However, if we only consider their experiments using settings similar to ours, their results consistently lie around an f-score of 0.80. An older approach, introduced by Jin and TanakaIshii (2006), solely relies on a separation measure 383 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 383–387, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics that is directly inspired by a linguistic hypothesis formulated by Harris (1955). In Tanaka-Ishii (2005) (following Kempe (1999)) who use Branching Entropy (BE), this hypothesis goes as follows: if sequences produced by human language were random, we would expect the Branching Entropy of a sequence (estimated from the n-grams in a corpus) to decrease as we increase the length of the sequence. Therefore the variation of the branching entropy (VBE) should be negative. When we observe that it is not the case, Harris hypothesizes that we are at a linguistic boundary. Following this hypothesis, (Jin and Tanaka-Ishii, 2006) propose a system that segments when BE is rising or when it reach a certain maximum. The main drawback</context>
<context position="9877" citStr="Kempe (1999)" startWordPosition="1617" endWordPosition="1618">n and obtain better, more consistent and less questionable results. As we are more interested in reducing the amount of human labor needed than in achieving by all means fully unsupervized learning, we do not refrain from performing basic and straightforward preprocessing such as detection of punctuation marks, Latin characters and Arabic numbers.2 Therefore, our experiments rely on settings similar to their settings 3 and 4, and are evaluated against the same corpora. 4 Normalized Variation of Branching Entropy (nVBE) Our system builds upon Harris&apos;s (1955) hypothesis and its reformulation by Kempe (1999) and TanakaIshii (2005). Let us now define formally the notions underlying our system. Given an n-gram x0..n = x0..1 x1..2 ... xn−1..n with a left context X→, we define its Right Branching Entropy (RBE) as: h→(x0..n) = H(X→ |x0..n) ∑= − P(x |x0..n) log P(x |x0..n). x∈x— The Left Branching Entropy (LBE) is defined in a symmetric way: if we note X← the right context of x0..n, its LBE is defined as:/ h←(x0..n) = H(X← |x0..n). The RBE (resp. LBE) can be considered as x0..n&apos;s Branching Entropy (BE) when reading from left to right (resp. right to left). 2Simple regular expressions could also be cons</context>
</contexts>
<marker>Kempe, 1999</marker>
<rawString>André Kempe. 1999. Experiments in unsupervised entropy-based corpus segmentation. In Workshop of EACL in Computational Natural Language Learning, page 7–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Magistry</author>
<author>Benoît Sagot</author>
</authors>
<title>Segmentation et induction de lexique non-supervisées du mandarin.</title>
<date>2011</date>
<booktitle>In TALN&apos;2011 - Traitement Automatique des Langues Naturelles,</booktitle>
<publisher>ATALA.</publisher>
<location>Montpellier, France,</location>
<contexts>
<context position="14550" citStr="Magistry and Sagot, 2011" startWordPosition="2416" endWordPosition="2419"> measure and get high segmentation scores. When maximized over a sentence, this measure captures at least in part what can be modeled by a cohesion measure without the need for fine-tuning the balance between the two. The evolution of the results w.r.t. word length is consistent with the supervized cross-evaluation results of the various segmentation guidelines as performed in Section 3. Due to space constraints, we cannot detail here a qualitative analysis of the results. We can simply mention that the errors we observed are consistent with previous systems based on Harris&apos;s hypothesis (see (Magistry and Sagot, 2011) and Jin (2007) for a longer discussion). Many errors are related to dates and Chinese numbers. This could and should be dealt with during preprocessing. Other errors often involve frequent grammatical morphemes or productive affixes. These errors are often interesting for linguists and could be studied as such and/or corrected in a post-processing stage that would introduce linguistic knowledge. Indeed, unlike content words, grammatical morphemes belongs to closed classes, System AS CITYU PKU MSR Setting 3 ESA worst 0.729 0.795 0.781 0.768 ESA best 0.782 0.816 0.795 0.802 nVBE 0.758 0.775 0.7</context>
</contexts>
<marker>Magistry, Sagot, 2011</marker>
<rawString>Pierre Magistry and Benoît Sagot. 2011. Segmentation et induction de lexique non-supervisées du mandarin. In TALN&apos;2011 - Traitement Automatique des Langues Naturelles, Montpellier, France, June. ATALA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>100--108</pages>
<marker>Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi. Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, page 100–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard W Sproat</author>
<author>Chilin Shih</author>
</authors>
<title>A statistical method for finding word boundaries</title>
<date>1990</date>
<booktitle>in Chinese text. Computer Processing of Chinese and Oriental Languages,</booktitle>
<pages>4--4</pages>
<contexts>
<context position="2088" citStr="Sproat and Shih, 1990" startWordPosition="321" endWordPosition="324">es and domains, or to tackle more theoretic questions about linguistic units, unsupervized segmentation is still an important issue. After a short review of the corresponding literature in Section 2, we discuss the challenging issue of evaluating unsupervized word segmentation systems in Section 3. Section 4 and Section 5 present the core of our system. Finally, in Section 6, we detail and discuss our results. 2 State of the Art Unsupervized word segmentation systems tend to make use of three different types of information: the cohesion of the resulting units (e.g., Mutual Information, as in (Sproat and Shih, 1990)), the degree of separation between the resulting units (e.g., Accessor Variety, see (Feng et al., 2004)) and the probability of a segmentation given a string (Goldwater et al., 2006; Mochihashi et al., 2009). A recently published work by Wang et al. (2011) introduce ESA: “Evaluation, Selection, Adjustment.” This method combines cohesion and separation measures in a “goodness” metric that is maximized during an iterative process. This work is the current state-of-the-art in unsupervized segmentation of Mandarin Chinese data. The main drawbacks of ESA are the need to iterate the process on the </context>
</contexts>
<marker>Sproat, Shih, 1990</marker>
<rawString>Richard W. Sproat and Chilin Shih. 1990. A statistical method for finding word boundaries in Chinese text. Computer Processing of Chinese and Oriental Languages, 4(4):336–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kumiko Tanaka-Ishii</author>
</authors>
<title>Entropy as an indicator of context boundaries: An experiment using a web search engine.</title>
<date>2005</date>
<booktitle>In IJCNLP,</booktitle>
<pages>93--105</pages>
<contexts>
<context position="3900" citStr="Tanaka-Ishii (2005)" startWordPosition="615" endWordPosition="616">. Therefore, this proper exponent may not be easily available in all situations. However, if we only consider their experiments using settings similar to ours, their results consistently lie around an f-score of 0.80. An older approach, introduced by Jin and TanakaIshii (2006), solely relies on a separation measure 383 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 383–387, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics that is directly inspired by a linguistic hypothesis formulated by Harris (1955). In Tanaka-Ishii (2005) (following Kempe (1999)) who use Branching Entropy (BE), this hypothesis goes as follows: if sequences produced by human language were random, we would expect the Branching Entropy of a sequence (estimated from the n-grams in a corpus) to decrease as we increase the length of the sequence. Therefore the variation of the branching entropy (VBE) should be negative. When we observe that it is not the case, Harris hypothesizes that we are at a linguistic boundary. Following this hypothesis, (Jin and Tanaka-Ishii, 2006) propose a system that segments when BE is rising or when it reach a certain ma</context>
</contexts>
<marker>Tanaka-Ishii, 2005</marker>
<rawString>Kumiko Tanaka-Ishii. 2005. Entropy as an indicator of context boundaries: An experiment using a web search engine. In IJCNLP, page 93–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanshi Wang</author>
<author>Jian Zhu</author>
<author>Shiping Tang</author>
<author>Xiaozhong Fan</author>
</authors>
<title>A new unsupervised approach to word segmentation.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>3</issue>
<pages>454</pages>
<contexts>
<context position="758" citStr="Wang et al., 2011" startWordPosition="109" endWordPosition="112">ris, France pierre.magistry@inria.fr Benoît Sagot Alpage, INRIA &amp; Univ. Paris 7, 175 rue du Chevaleret, 75013 Paris, France benoit.sagot@inria.fr Abstract In this paper, we present an unsupervized segmentation system tested on Mandarin Chinese. Following Harris&apos;s Hypothesis in Kempe (1999) and Tanaka-Ishii&apos;s (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin and Tanaka-Ishii, 2006) by adding normalization and viterbidecoding. This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on different corpora available from the Segmentation bake-off II (Emerson, 2005) and define a more precise topline for the task using cross-trained supervized system available off-the-shelf (Zhang and Clark, 2010; Zhao and Kit, 2008; Huang and Zhao, 2007) 1 Introduction The Chinese script has no explicit “word” boundaries. Therefore, tokenization itself, although the very first step of many text processing systems, is a challenging task. Supervized segmentation systems exist but rely on manually segmented corpora, which are often specific to a genr</context>
<context position="2345" citStr="Wang et al. (2011)" startWordPosition="365" endWordPosition="368">ord segmentation systems in Section 3. Section 4 and Section 5 present the core of our system. Finally, in Section 6, we detail and discuss our results. 2 State of the Art Unsupervized word segmentation systems tend to make use of three different types of information: the cohesion of the resulting units (e.g., Mutual Information, as in (Sproat and Shih, 1990)), the degree of separation between the resulting units (e.g., Accessor Variety, see (Feng et al., 2004)) and the probability of a segmentation given a string (Goldwater et al., 2006; Mochihashi et al., 2009). A recently published work by Wang et al. (2011) introduce ESA: “Evaluation, Selection, Adjustment.” This method combines cohesion and separation measures in a “goodness” metric that is maximized during an iterative process. This work is the current state-of-the-art in unsupervized segmentation of Mandarin Chinese data. The main drawbacks of ESA are the need to iterate the process on the corpus around 10 times to reach good performance levels and the need to set a parameter that balances the impact of the cohesion measure w.r.t. the separation measure. Empirically, a correlation is found between the parameter and the size of the corpus but </context>
<context position="5297" citStr="Wang et al. (2011)" startWordPosition="864" endWordPosition="867">lso also relies on parameters, namely the threshold on the VBE above which the system decides to segment (in their system, this is when VBE&gt; 0). In theory, we could expect a decreasing BE and look for a less decreasing value (or on the contrary, rising at least to some extent). A threshold of 0 can be seen as a default value. Finally, Jin and Tanaka-Ishii do not take in account that VBE of n-gram may not be directly comparable to the VBE of m-grams if m # n. A normalization is needed (as in (Cohen et al., 2002)). Due to space constraints, we shall not describe here other systems than those by Wang et al. (2011) and Jin and Tanaka-Ishii (2006). A more comprehensive state of the art can be found in (Zhao and Kit, 2008) and (Wang et al., 2011). In this paper we will show that we can correct the drawbacks of Jin and Tanaka-Ishii (2006) model and reach performances comparable to those of Wang et al. (2011) with as simpler system. 3 Evaluation In this paper, in order to be comparable with Wang et al. (2011), we evaluate our system against the corpora from the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005). These corpora cover 4 different segmentation guidelines from various origins</context>
<context position="8642" citStr="Wang et al. (2011)" startWordPosition="1418" endWordPosition="1421">79. In a segmented Chinese text, most of the tokens are uni- and bigrams but most of the types are bi- and trigrams (as unigrams are often high frequency grammatical words and trigrams the result of more or less productive affixations). Therefore the results of evaluations only based on tokens do not suffer much from poor performances on trigrams even if a large part of the lexicon may be incorrectly processed. Another issue about the evaluation and comparison of unsupervized systems is to try and remain fair 384 in terms of preprocessing and prior knowledge given to the systems. For example, Wang et al. (2011) used different levels of preprocessing (which they call “settings”). In their settings 1 and 2, Wang et al. (2011) try not to rely on punctuation and character encoding information (such as distinguishing Latin and Chinese characters). However, they optimize their parameter for each setting. We therefore consider that their system does take into account the level of processing which is performed on Latin characters and Arabic numbers, and therefore “knows” whether to expect such characters or not. In setting 3 they add the knowledge of punctuation as clear boundaries and in setting 4 they pre</context>
<context position="13779" citStr="Wang et al., 2011" startWordPosition="2287" endWordPosition="2290">rds w0ws ... w,,,,, and len(wi) is the length of a word wi used here to be able to compare segmentations resulting in a different number of words. This best segmentation can be computed easily using dynamic programming. 6 Results and discussion We tested our system against the data from the 4 corpora of the Second Bakeoff, in both settings 3 and 4, as described in Section 3. Overall results are given in Table 1 and per-word-length results in Table 2. Our results (nVBE) show significant improvements over Jin&apos;s (2006) strategy (VBE &gt; 0) and are closely competing with ESA. But contrarily to ESA (Wang et al., 2011), it does not require multiple iterations on the corpus and it does not rely on any parameters. This shows that we can rely solely on a separation measure and get high segmentation scores. When maximized over a sentence, this measure captures at least in part what can be modeled by a cohesion measure without the need for fine-tuning the balance between the two. The evolution of the results w.r.t. word length is consistent with the supervized cross-evaluation results of the various segmentation guidelines as performed in Section 3. Due to space constraints, we cannot detail here a qualitative a</context>
</contexts>
<marker>Wang, Zhu, Tang, Fan, 2011</marker>
<rawString>Hanshi Wang, Jian Zhu, Shiping Tang, and Xiaozhong Fan. 2011. A new unsupervised approach to word segmentation. Computational Linguistics, 37(3):421– 454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and POS-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>843--852</pages>
<contexts>
<context position="1016" citStr="Zhang and Clark, 2010" startWordPosition="146" endWordPosition="149">Harris&apos;s Hypothesis in Kempe (1999) and Tanaka-Ishii&apos;s (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin and Tanaka-Ishii, 2006) by adding normalization and viterbidecoding. This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on different corpora available from the Segmentation bake-off II (Emerson, 2005) and define a more precise topline for the task using cross-trained supervized system available off-the-shelf (Zhang and Clark, 2010; Zhao and Kit, 2008; Huang and Zhao, 2007) 1 Introduction The Chinese script has no explicit “word” boundaries. Therefore, tokenization itself, although the very first step of many text processing systems, is a challenging task. Supervized segmentation systems exist but rely on manually segmented corpora, which are often specific to a genre or a domain and use many different segmentation guidelines. In order to deal with a larger variety of genres and domains, or to tackle more theoretic questions about linguistic units, unsupervized segmentation is still an important issue. After a short rev</context>
<context position="7543" citStr="Zhang and Clark, 2010" startWordPosition="1232" endWordPosition="1235">en different segmentation guidelines, and therefore an upper bound of what can be expected from an unsupervized system (Zhao and Kit, 2008). The average consistency is found to be as low as 0.85 (f-score). Therefore this figure can be considered as a sensible topline for unsupervized systems. The standard baseline which consists in segmenting each character leads to a baseline around 0.35 (f-score) — almost half of the tokens in a manually segmented corpus are unigrams. Per word-length evaluation is also important as units of various lengths tend to have different distributions. We used ZPAR (Zhang and Clark, 2010) on the four corpora from the Second Bakeoff to reproduce Huang and Zhao&apos;s (2007) experiments, but also to measure cross-corpus consistency at a per-wordlength level. Our overall results are comparable to what Huang and Zhao (2007) report. However, the consistency is quickly falling for longer words: on unigrams, f-scores range from 0.81 to 0.90 (the same as the overall results). We get slightly higher figures on bigrams (0.85–0.92) but much lower on trigrams with only 0.59–0.79. In a segmented Chinese text, most of the tokens are uni- and bigrams but most of the types are bi- and trigrams (as</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, page 843–852.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>An empirical comparison of goodness measures for unsupervised Chinese word segmentation with a unified framework.</title>
<date>2008</date>
<booktitle>In The Third International Joint Conference on Natural Language Processing (IJCNLP2008),</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="1036" citStr="Zhao and Kit, 2008" startWordPosition="150" endWordPosition="153">Kempe (1999) and Tanaka-Ishii&apos;s (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin and Tanaka-Ishii, 2006) by adding normalization and viterbidecoding. This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on different corpora available from the Segmentation bake-off II (Emerson, 2005) and define a more precise topline for the task using cross-trained supervized system available off-the-shelf (Zhang and Clark, 2010; Zhao and Kit, 2008; Huang and Zhao, 2007) 1 Introduction The Chinese script has no explicit “word” boundaries. Therefore, tokenization itself, although the very first step of many text processing systems, is a challenging task. Supervized segmentation systems exist but rely on manually segmented corpora, which are often specific to a genre or a domain and use many different segmentation guidelines. In order to deal with a larger variety of genres and domains, or to tackle more theoretic questions about linguistic units, unsupervized segmentation is still an important issue. After a short review of the correspon</context>
<context position="5405" citStr="Zhao and Kit, 2008" startWordPosition="885" endWordPosition="888">n their system, this is when VBE&gt; 0). In theory, we could expect a decreasing BE and look for a less decreasing value (or on the contrary, rising at least to some extent). A threshold of 0 can be seen as a default value. Finally, Jin and Tanaka-Ishii do not take in account that VBE of n-gram may not be directly comparable to the VBE of m-grams if m # n. A normalization is needed (as in (Cohen et al., 2002)). Due to space constraints, we shall not describe here other systems than those by Wang et al. (2011) and Jin and Tanaka-Ishii (2006). A more comprehensive state of the art can be found in (Zhao and Kit, 2008) and (Wang et al., 2011). In this paper we will show that we can correct the drawbacks of Jin and Tanaka-Ishii (2006) model and reach performances comparable to those of Wang et al. (2011) with as simpler system. 3 Evaluation In this paper, in order to be comparable with Wang et al. (2011), we evaluate our system against the corpora from the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005). These corpora cover 4 different segmentation guidelines from various origins: Academia Sinica (AS), City-University of Hong-Kong (CITYU), Microsoft Research (MSR) and Peking University</context>
<context position="7060" citStr="Zhao and Kit, 2008" startWordPosition="1152" endWordPosition="1155">s particular guidelines, the resulting system will follow as well as possible these guidelines, and can be evaluated on data annotated accordingly. However, for unsupervized systems, there is no reason why a system should be closer to one reference than another or even not to lie somewhere in between the different existing guidelines. Huang and Zhao (2007) propose to use cross-training of a supervized segmentation system in order to have an estimation of the consistency between different segmentation guidelines, and therefore an upper bound of what can be expected from an unsupervized system (Zhao and Kit, 2008). The average consistency is found to be as low as 0.85 (f-score). Therefore this figure can be considered as a sensible topline for unsupervized systems. The standard baseline which consists in segmenting each character leads to a baseline around 0.35 (f-score) — almost half of the tokens in a manually segmented corpus are unigrams. Per word-length evaluation is also important as units of various lengths tend to have different distributions. We used ZPAR (Zhang and Clark, 2010) on the four corpora from the Second Bakeoff to reproduce Huang and Zhao&apos;s (2007) experiments, but also to measure cr</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. An empirical comparison of goodness measures for unsupervised Chinese word segmentation with a unified framework. In The Third International Joint Conference on Natural Language Processing (IJCNLP2008), Hyderabad, India.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>