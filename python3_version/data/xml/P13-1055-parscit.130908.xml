<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000984">
<title confidence="0.9781805">
Identifying Bad Semantic Neighbors for Improving Distributional
Thesauri
</title>
<author confidence="0.703396">
Olivier Ferret
</author>
<note confidence="0.926241">
CEA, LIST, Vision and Content Engineering Laboratory,
Gif-sur-Yvette, F-91191 France.
</note>
<email confidence="0.988904">
olivier.ferret@cea.fr
</email>
<sectionHeader confidence="0.993653" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896181818182">
Distributional thesauri are now widely
used in a large number of Natural Lan-
guage Processing tasks. However, they
are far from containing only interesting
semantic relations. As a consequence,
improving such thesaurus is an impor-
tant issue that is mainly tackled indirectly
through the improvement of semantic sim-
ilarity measures. In this article, we pro-
pose a more direct approach focusing on
the identification of the neighbors of a
thesaurus entry that are not semantically
linked to this entry. This identification re-
lies on a discriminative classifier trained
from unsupervised selected examples for
building a distributional model of the entry
in texts. Its bad neighbors are found by ap-
plying this classifier to a representative set
of occurrences of each of these neighbors.
We evaluate the interest of this method for
a large set of English nouns with various
frequencies.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933375">
The work we present in this article focuses on the
automatic building of a thesaurus from a corpus.
As illustrated by Table 1, such thesaurus gives for
each of its entries a list of words, called seman-
tic neighbors, that are supposed to be semanti-
cally linked to the entry. Generally, each neigh-
bor is associated with a weight that characterizes
the strength of its link with the entry and all the
neighbors of an entry are sorted according to the
decreasing order of their weight.
The term semantic neighbor is very generic and
can have two main interpretations according to the
kind of semantic relations it is based on: one re-
lies only on paradigmatic relations, such as hy-
pernymy or synonymy, while the other consid-
ers syntagmatic relations, called collocation rela-
tions by (Halliday and Hasan, 1976) in the context
of lexical cohesion or “non-classical relations” by
(Morris and Hirst, 2004). The distinction between
these two interpretations refers to the distinction
between the notions of semantic similarity and se-
mantic relatedness as it was done in (Budanitsky
and Hirst, 2006) or in (Zesch and Gurevych, 2010)
for instance. However, the limit between these two
notions is sometimes hard to find in existing work
as terms semantic similarity and semantic relat-
edness are often used interchangeably. Moreover,
semantic similarity is frequently considered as in-
cluded into semantic relatedness and the two prob-
lems are often tackled by using the same methods.
In the remainder of this article, we will use the
term semantic similarity with its generic sense and
the term semantic relatedness for referring more
specifically to similarity based on syntagmatic re-
lations.
Following work such as (Grefenstette, 1994), a
widespread way to build a thesaurus from a cor-
pus is to use a semantic similarity measure for ex-
tracting the semantic neighbors of the entries of
the thesaurus. Three main ways of implement-
ing such measures can be distinguished. The first
one relies on handcrafted resources in which se-
mantic relations are clearly identified. Work based
on WordNet-like lexical networks for building se-
mantic similarity measures such as (Budanitsky
and Hirst, 2006) or (Pedersen et al., 2004) falls
into this category. These measures typically ex-
ploit the hierarchical structure of these networks,
based on hypernymy relations. The second ap-
proach makes use of a less structured source of
knowledge about words such as the definitions of
classical dictionaries or the glosses of WordNet.
WordNet’s glosses were used to support Lesk-
like measures in (Banerjee and Pedersen, 2003)
and more recently, measures were also defined
from Wikipedia or Wiktionaries (Gabrilovich and
</bodyText>
<page confidence="0.968339">
561
</page>
<note confidence="0.923567">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–571,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.993735414634146">
Markovitch, 2007). The last option is the corpus-
based approach, based on the distributional hy-
pothesis (Firth, 1957): each word is characterized
by the set of contexts from a corpus in which it ap-
pears and the semantic similarity of two words is
computed from the contexts they share. This per-
spective was first adopted by (Grefenstette, 1994)
and (Lin, 1998) and then, explored in details in
(Curran and Moens, 2002b), (Weeds, 2003) or
(Heylen et al., 2008).
The problem of improving the results of the
“classical” implementation of the distributional
approach as it can be found in (Curran and Moens,
2002a) for instance was already tackled by some
work. A part of these proposals focus on the
weighting of the elements that are part of the
contexts of words such as (Broda et al., 2009),
in which the weights of context elements are
turned into ranks, or (Zhitomirsky-Geffet and Da-
gan, 2009), followed and extended by (Yamamoto
and Asakura, 2010), that proposes a bootstrap-
ping method for modifying the weights of con-
text elements according to the semantic neighbors
found by an initial distributional similarity mea-
sure. However, another part of these proposals
implies more radical changes. The use of dimen-
sionality reduction techniques, for instance Latent
Semantic Analysis in (Pad´o and Lapata, 2007), the
multi-prototype (Reisinger and Mooney, 2010) or
examplar-based models (Erk and Pado, 2010), the
Deep Learning approach of (Huang et al., 2012) or
the redefinition of the distributional approach in a
Bayesian framework (Kazama et al., 2010) can be
classified into this second category.
The work we present in this article takes place
in the framework defined by (Grefenstette, 1994)
for implementing the distributional approach but
proposes a new method for improving a thesaurus
built in this context based on the identification of
its bad semantic neighbors rather than on the adap-
tation of the weight of their features.
</bodyText>
<sectionHeader confidence="0.98479" genericHeader="introduction">
2 Principles
</sectionHeader>
<bodyText confidence="0.99997206779661">
Our work shares with (Zhitomirsky-Geffet and
Dagan, 2009) the use of a kind of bootstrapping as
it starts from a distributional thesaurus and to some
extent, exploits it for its improvement. However, it
adopts a more indirect approach: instead of select-
ing the “best” semantic neighbors of an entry in
the thesaurus for adapting the weights of distribu-
tional context elements, it focuses on the detection
of its bad semantic neighbors, that is to say the
neighbors of the entry that are actually not seman-
tically similar to the entry. In Table 1, waterworks
for the entry cabdriver and hollowness for the en-
try machination are two examples of such kind of
neighbors. By discarding these bad neighbors or
at least by downgrading them, the rank of true se-
mantic neighbors is expected to be lower. This
makes the thesaurus more interesting to use since
the quality of such thesaurus strongly decreases as
the rank of the neighbors of its entries increases
(see Section 4.1 for an illustration), which means
in practice that only the first neighbors of an entry
can be generally exploited.
The approach we propose for identifying the
bad semantic neighbors of a thesaurus entry re-
lies on the distributional hypothesis, as the method
for the initial building of the thesaurus, but im-
plements it in a different way. This hypothesis
roughly specifies that from a semantic viewpoint,
the meaning of a word can be characterized by the
set of contexts in which this word occurs. As a
consequence, two words are considered as seman-
tically similar if they occur in a large enough set
of shared contexts. In work such as (Curran and
Moens, 2002a), this hypothesis is implemented by
collecting for each entry the words it co-occurs
with in a large corpus. This co-occurrence can
be based either on the position of the word in the
text in relation to the entry or on the presence
of a syntactic relation between the entry and the
word. As a result, the distributional representa-
tion of a word takes the unstructured form of a
bag of words or the more structured form of a
set of pairs {syntactic relation, word}. A vari-
ant of this approach was proposed in (Kazama et
al., 2010) where the distributional representation
of a word is modeled as a multinomial distribution
with Dirichlet as prior.
However, this approach globally faces a certain
lack of diversity and complexity of the features of
its models. For instance, features such as ngrams
of words or ngrams of parts of speech are not con-
sidered whereas they are widely used in tasks such
as word sense disambiguation (WSD) for instance,
probably because they would lead to very large
models and because similarity measures such as
the Cosine measure are not necessarily suitable
for heterogeneous representations (Alexandrescu
and Kirchhoff, 2007). Hence, we propose in this
article to build a discriminative model for repre-
</bodyText>
<page confidence="0.982894">
562
</page>
<table confidence="0.647089">
abnormality defect [0.30], disorder [0.23], deformity [0.22], mutation [0.21], prolapse [0.21], anomaly [0.21] ...
agreement accord [0.44], deal [0.41], pact [0.38], treaty [0.36], negotiation [0.35], proposal [0.32], arrangement [0.30] ...
cabdriver waterworks [0.23], toolmaker [0.22], weaponeer [0.17], valkyry [0.17], wang [0.17], amusement-park [0.17] ...
machination hollowness [0.15], share-price [0.12], clockmaker [0.12], huguenot [0.12], wrangling [0.12], alternation [0.12] ...
</table>
<tableCaption confidence="0.996507">
Table 1: First neighbors of some entries of the distributional thesaurus of section 3.2
</tableCaption>
<bodyText confidence="0.99996985">
senting the contexts of a word since this kind of
models are known to integrate easily a wide set
of different types of features. This model aims
more precisely at discriminating from a semantic
viewpoint a word in context, i.e. in a sentence,
from all other words and more particularly, from
those of its neighbors in a distributional thesaurus
that are likely to be actually not semantically sim-
ilar to it. The underlying hypothesis follows the
distributional principles: a word and a synonym
should appear in the same contexts, which means
that they are characterized by the same features.
As a consequence, a model based on these fea-
tures that can identify a word in a sentence is likely
to identify also a synonym of this word in a sen-
tence, and by extension, to identify a word that
is paradigmatically linked to it. More precisely,
we found that such model is specifically effective
for discarding the bad neighbors of the entries of a
distributional thesaurus.
</bodyText>
<sectionHeader confidence="0.876826" genericHeader="method">
3 Improving a distributional thesaurus
</sectionHeader>
<subsectionHeader confidence="0.985363">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999949117647059">
The principles presented in the previous section
face one major problem compared to the “classi-
cal” distributional approach: the semantic similar-
ity of two words can be evaluated directly by com-
puting the similarity of their distributional repre-
sentations. However, in our case, since this rep-
resentation is a discriminative model, the similar-
ity of two words can not be evaluated through the
direct comparison of their models. These models
have to be applied to words in context for being
exploited. As a consequence, for deciding whether
a neighbor of a thesaurus entry is a bad neighbor
or not, the discriminative model of the entry has
to be applied to occurrences of this neighbor in
texts. Hence, the method we propose for improv-
ing a distributional thesaurus applies the following
process to each of its entries:
</bodyText>
<listItem confidence="0.995318666666667">
• building of a classifier for determining
whether a word in a sentence corresponds or
not to the entry;
• selection of a set of examples sentences for
each of the neighbors of the entry in the the-
saurus;
• application of the classifier to these sen-
tences;
• identification of bad neighbors according to
the results of the classifier;
• reranking of entry’s neighbors according to
bad neighbors.
</listItem>
<subsectionHeader confidence="0.999884">
3.2 Building of the initial thesaurus
</subsectionHeader>
<bodyText confidence="0.999773137931035">
Before introducing our method for improving dis-
tributional thesauri, we first present the way we
build such a thesaurus. As in (Lin, 1998) or (Cur-
ran and Moens, 2002a), this building is based on
the definition of a semantic similarity measure
from a corpus. The corpus used for defining this
measure was the AQUAINT-2 corpus, a middle-
size corpus made of around 380 million words
coming from news articles. Although our target
language is English, we chose to limit deliber-
ately the level of the tools applied for preprocess-
ing texts to part-of-speech tagging and lemmati-
zation to make possible the transposition of our
method to a large set of languages. This seems
to be a reasonable compromise between the ap-
proach of (Freitag et al., 2005), in which none
normalization of words is done, and the more
widespread use of syntactic parsers in work such
as (Lin, 1998). More precisely, we used TreeTag-
ger (Schmid, 1994) for performing the linguistic
preprocessing of the AQUAINT-2 corpus.
For the extraction of distributional data and the
characteristics of the distributional similarity mea-
sure, we adopted the options of (Ferret, 2010), re-
sulting from a kind of grid search procedure per-
formed with the extended TOEFL test proposed in
(Freitag et al., 2005) as an optimization objective.
More precisely, the following characteristics were
taken:
</bodyText>
<listItem confidence="0.943318">
• distributional contexts made of the co-
occurrents collected in a 3 word window cen-
tered on each occurrence in the corpus of the
target word. These co-occurrents were re-
stricted to nouns, verbs and adjectives;
• soft filtering of contexts: removal of co-
occurrents with only one occurrence;
• weighting function of co-occurrents in con-
</listItem>
<page confidence="0.982527">
563
</page>
<bodyText confidence="0.8868795">
texts = Pointwise Mutual Information (PMI)
between the target word and the co-occurrent;
</bodyText>
<listItem confidence="0.781387">
• similarity measure between contexts, for
evaluating the semantic similarity of two
words = Cosine measure.
</listItem>
<bodyText confidence="0.999986181818182">
The building of our initial thesaurus from the
similarity measure above was performed classi-
cally by extracting the closest semantic neighbors
of each of its entries. More precisely, the selected
measure was computed between each entry and
its possible neighbors. These neighbors were then
ranked in the decreasing order of the values of this
measure and the first 100 neighbors were kept as
the semantic neighbors of the entry. Both entries
and possible neighbors were AQUAINT-2 nouns
whose frequency was higher than 10.
</bodyText>
<subsectionHeader confidence="0.798478">
3.3 Building a discriminative model of words
in context
</subsectionHeader>
<bodyText confidence="0.99999071875">
As mentioned in section 3.1, the starting point of
our reranking process is the definition of a model
for determining to what extent a word in a sen-
tence, which is not supposed to be known in the
context of this task, corresponds or not to a refer-
ence word E. This task can also be viewed as a
tagging task in which the occurrences of a target
word T are labeled with two tags: E and notE.
In the context of our global objective, we are not
of course interested by this task itself but rather by
the fact that such classifier is likely to model the
contexts in which E occurs and as a consequence,
is also likely to model its meaning according to the
distributional hypothesis.
A step further, such classifier can be viewed
as a means for testing whether or not a word
has the same meaning as E. This is a problem
close to WSD as it is performed in the context of
the pseudo-word disambiguation paradigm (Gale
et al., 1992): a pseudo-word is created with two
senses, E and notE, notE corresponding to one
or several words that are supposed to be represen-
tative of a meaning different from the meaning of
E. The objective is then to build a classifier for
distinguishing the pseudo-senses E and notE. As
a consequence of this view, we adopt the same
kind of features as the ones used for WSD for
building our classifier. More precisely, we follow
(Lee and Ng, 2002), a reference work for WSD,
by adopting a Support Vector Machines (SVM)
classifier with a linear kernel and three kinds of
features for characterizing each considered occur-
</bodyText>
<listItem confidence="0.88527225">
rence in a text of the reference word E:
• neighboring words;
• Part-of-Speech (POS) of neighboring words;
• local collocations.
</listItem>
<bodyText confidence="0.999883170212766">
Only features based on syntactic relations are
not taken from (Lee and Ng, 2002) since their use
would have not been coherent with the window
based approach of the building of our initial the-
saurus.
For the neighboring words features, we con-
sider all plain words (common and proper nouns,
verbs and adjectives) and adverbs that are present
in the same sentence of an occurrence of E. Each
neighboring word is represented under its lemma
form as a binary feature whose value is equal to 1
when it is present in the same sentence as E.
For the second type of features, we take more
precisely the POS of the three words before E and
those of the three words after E. Each pair {POS,
position} corresponds to a binary feature for the
SVM classifier. A special empty symbol is used
for the POS when the position goes beyond the end
or the beginning of the current sentence. Since we
analyze texts with TreeTagger, the tagset is very
close to the set of Penn Treebank tags.
Finally, the local collocations features corre-
spond to pairs of words, named collocations, in
the neighborhood of E. A collocation is speci-
fied by the notation Ci,j, with i and j referring to
the position of the first and the second word of the
collocation. In our case, i and j take their values
in the interval [−3,+3], similarly to POS. More
precisely, the following 11 types of collocations
are extracted for each occurrence of E: C−1,−1,
C1,1, C−2,−2, C2,2 C−2,−1, C−1,1, C1,2, C−3,−1,
C−2,1, C−1,2 and C1,3. As for POS, a special
empty symbol stands for words beyond the end
or the beginning of the sentence and similarly to
neighboring words features, words in collocations
are given under their lemma form. Each instance
of the 11 types of collocations is represented by a
tuple (lemma1, position1, lemma2, position2) and
leads to a binary feature for the SVM classifier.
In accordance with the process of section 3.1,
a specific SVM classifier is trained for each entry
of our initial thesaurus, which requires the unsu-
pervised selection of a set of positive and nega-
tive examples. The case of positive examples is
simple: a fixed number of sentences containing at
least one occurrence of the target entry are ran-
domly chosen in the corpus used for building our
</bodyText>
<page confidence="0.990883">
564
</page>
<bodyText confidence="0.999990678571429">
initial thesaurus and the first occurrence of this en-
try in the sentence is taken as a positive example.
Since we want to characterize words as much as
possible from a semantic viewpoint, the selection
of negative examples is guided by our initial the-
saurus. Choosing a neighbor of the entry with a
high rank would guarantee in principle few false
negative examples, that is to say words1 which are
semantically similar to the entry, since the number
of such neighbors strongly decreases as the rank
of neighbors increases as we will illustrate it in
section 4.1. In practice, taking neighbors with a
rather small rank as negative examples is a bet-
ter option because these examples are more useful
in terms of discrimination as they are close to the
transition zone between negative and positive ex-
amples. Moreover, in order to limit the risk of se-
lecting only false negative examples, three neigh-
bors are taken as negative examples, at ranks 10,
15 and 202. For each of these negative examples,
a fixed number of sentences is selected follow-
ing the same principles as for positive examples,
which means that on average, the number of neg-
ative examples is equal to three times the number
of positive examples. This ratio reflects the fact
that among the neighbors of an entry, the number
of those that are semantically similar to the entry
is far lower than the number of those that are not.
</bodyText>
<subsectionHeader confidence="0.980554">
3.4 Identification of bad neighbors and
thesaurus reranking
</subsectionHeader>
<bodyText confidence="0.999009666666667">
Once a word-in-context classifier was trained for
an entry, it is used for identifying the bad neigh-
bors of this entry, that is to say the neighbors that
are not semantically similar to it. As this classifier
can only be applied to words in context, a fixed
number of representative occurrences have to be
selected from our reference corpus for each neigh-
bor of the entry. This selection is performed sim-
ilarly to the selection of positive and negative ex-
amples in the previous section. The application of
our word-in-context classifier to each of these oc-
currences determines whether the context of this
occurrence is likely to be compatible with the con-
text of an occurrence of the entry.
In practice, the decision of the classifier is rarely
</bodyText>
<footnote confidence="0.8402245">
1More precisely, an example here is an occurrence of a
word in a text but by extension, we also use the term example
for referring to the word itself.
2It should be noted that these ranks come from the eval-
uation of section 4.1 but their choice is not the result of an
optimization process.
</footnote>
<bodyText confidence="0.999942037037037">
positive, which is not surprising: even if two
words are semantically equivalent, each one is
characterized by specific usages, especially in a
given corpus, and some features of our classifier,
such as the collocation features, are more likely
to capture such specificities than the unigrams of
“classical” distributional contexts. As a conse-
quence, we consider that a positive outcome of our
classifier is a significant hint about the presence of
a word that is semantically similar to the entry and
we keep a neighbor as a “good” neighbor if at least
a fixed number G of its occurrences, among those
selected as reference, are tagged positively by our
word-in-context classifier. Conversely, a neighbor
is defined as “bad” if the number of its reference
occurrences tagged positively by our classifier is
lower or equal to G.
The neighbors of an entry identified as bad
neighbors are not fully discarded. They are rather
downgraded to the end of the list of neighbors.
Among the downgraded neighbors, their initial or-
der is left unchanged. It should be noted that
the word-in-context classifier is not applied to the
neighbors whose occurrences are used for its train-
ing as it would frequently lead to downgrade these
neighbors, which is not necessarily optimum as we
chose them with a rather low rank.
</bodyText>
<sectionHeader confidence="0.995868" genericHeader="method">
4 Experiments and evaluation
</sectionHeader>
<subsectionHeader confidence="0.91324">
4.1 Initial thesaurus evaluation
</subsectionHeader>
<bodyText confidence="0.999794529411765">
Table 2 shows the results of the evaluation of our
initial thesaurus, achieved by comparing the se-
lected semantic neighbors with two complemen-
tary reference resources: WordNet 3.0 synonyms
(Miller, 1990) [W], which characterize a semantic
similarity based on paradigmatic relations, and the
Moby thesaurus (Ward, 1996) [M], which gathers
a larger set of types of relations and is more rep-
resentative of semantic relatedness3. The fourth
column of Table 2, which gives the average num-
ber of synonyms and similar words in our refer-
ences for the AQUAINT-2 nouns, also illustrates
the difference of these two resources in terms of
richness. A fusion of the two resources is also
considered [WM]. As our objective is to evalu-
ate the extracted semantic neighbors and not the
ability to rebuild the reference resources, these re-
</bodyText>
<footnote confidence="0.994451">
3The Moby thesaurus includes more precisely both
paradigmatic and syntactic relations but we will sometimes
use the term synonym as a shortcut for referring to all the
words associated to one of its entries.
</footnote>
<page confidence="0.972059">
565
</page>
<table confidence="0.999307214285714">
freq. ref. #eval. #syn. / recall R-prec. MAP P@1 P@5 P@10 P@100
words word
W 10,473 2.9 24.6 8.2 9.8 11.7 5.1 3.4 0.7
all M 9,216 50.0 9.5 6.7 3.2 24.1 16.4 13.0 4.8
# 14,670 WM 12,243 38.7 9.8 7.7 5.6 22.5 14.1 10.8 3.8
W 3,690 3.7 28.3 11.1 12.5 17.2 7.7 5.1 1.0
high M 3,732 69.4 11.4 10.2 4.9 41.3 28.0 21.9 7.9
# 4,378 WM 4,164 63.2 11.5 11.0 6.5 41.3 26.8 20.8 7.3
W 3,732 2.6 28.6 10.4 12.5 13.6 5.8 3.7 0.7
middle M 3,306 41.3 9.3 6.5 3.1 18.7 13.1 10.4 3.8
# 5,175 WM 4,392 32.0 9.8 9.3 7.4 20.9 12.3 9.3 3.2
W 3,051 2.3 11.9 2.1 3.3 2.6 1.2 0.9 0.3
low M 2,178 30.1 2.8 1.2 0.5 2.5 1.5 1.5 0.9
# 5,117 WM 3,687 18.9 3.5 2.1 2.4 3.3 1.7 1.5 0.7
</table>
<tableCaption confidence="0.999809">
Table 2: Evaluation of semantic neighbor extraction
</tableCaption>
<bodyText confidence="0.999980980769231">
sources were filtered to discard entries and syn-
onyms that are not part of the AQUAINT-2 vo-
cabulary (see the difference between the number
of words in the first column and the number of
evaluated words of the third column). Since the
frequency of words is an important factor in dis-
tributional approaches, we give our results glob-
ally but also for three ranges of frequencies that
split our set of nouns into roughly equal parts:
high frequency (frequency &gt; 1000), middle fre-
quency (100 &lt; frequency &lt; 1000) and low fre-
quency (10 &lt; frequency &lt; 100). These results
take the form of several measures and start at the
fifth column by the proportion of the synonyms
and similar words of our references that are found
among the first 100 extracted neighbors of each
noun. As these neighbors are ranked according to
their similarity value with their target word, the
evaluation measures are taken from the Informa-
tion Retrieval field by replacing documents with
synonyms and queries with target words (see the
four last columns of Table 2). The R-precision (R-
prec.) is the precision after the first R neighbors
were retrieved, R being the number of reference
synonyms; the Mean Average Precision (MAP) is
the average of the precision value after a reference
synonym is found; precision at different cut-offs is
given for the 1, 5, 10 and 100 first neighbors. All
these values are given as percentages.
The results of Table 2 lead to three main ob-
servations. First, the level of results heavily de-
pends on the frequency range of target words:
the best results are obtained for high frequency
words while evaluation measures significantly de-
crease for words whose frequency is low. Sec-
ond, the characteristics of the reference resources
have a significant impact on results. WordNet
provides a restricted number of synonyms for
each noun while the Moby thesaurus contains for
each entry a large number of synonyms and sim-
ilar words. As a consequence, the precisions
at different cut-offs have a significantly higher
value with Moby as reference than with Word-
Net as reference. Finally, the results of Ta-
ble 2 are compatible with those of (Lin, 1998)
for instance (R-prec. = 11.6 and MAP = 8.1
with WM as reference for all entries of the the-
saurus at http://webdocs.cs.ualberta.
ca/lindek/Downloads/sim.tgz) if we
take into account the fact that the thesaurus of Lin
was built from a much larger corpus and with syn-
tactic co-occurrences.
</bodyText>
<subsectionHeader confidence="0.98367">
4.2 Implementation issues
</subsectionHeader>
<bodyText confidence="0.999991823529412">
The implementation of the method we have pre-
sented in section 3 raises several issues. One of
these concerns the occurrences to select from texts
of both the entries of the thesaurus and their neigh-
bors. These occurrences are used both for the
training of our word-in-context classifier and for
the identification of bad neighbors. In practice, we
extract randomly from our reference corpus, i.e.
the AQUAINT-2 corpus, a fixed number of sen-
tences, equal to 250, for each word of the vocab-
ulary of our initial thesaurus and exploit them for
the two tasks. This extraction is performed on the
basis of the lemma form of these words. It should
be noted that 250 is the upper limit of the num-
ber of occurrences by word since the frequency
in the corpus of many words is lower than 250.
When this limit is not reached, all the available oc-
</bodyText>
<page confidence="0.996403">
566
</page>
<bodyText confidence="0.996866368421053">
currences are taken, which may be no more than
11 occurrences for certain low-frequency words.
The upper limit of 250 is halfway between the 385
training examples on average for the Lexical Sam-
ple Task of Senseval 1 and the 118 training exam-
ples on average for the same task of Senseval 2.
The training of our word-in-context classifier
is also an important issue. As mentioned before,
this classifier is a linear SVM. Hence, only its C
regularization parameter can be optimized. Since
we have one specific classifier for each thesaurus
entry, such optimization has globally a high cost,
even for a linear kernel. Hence, we have first eval-
uated through a 5-fold cross-validation method the
results of these classifiers with a default value of
C, equal to 1. Table 3 gives their average accu-
racy value along with their standard deviation for
all the entries of the thesaurus and for the three
frequency ranges of Table 2.
</bodyText>
<table confidence="0.508367666666667">
all high middle low
accuracy 86.2 86.1 86.0 86.5
standard deviation 6.1 4.2 5.7 7.6
</table>
<tableCaption confidence="0.996482">
Table 3: Results of word-in-context classifiers
</tableCaption>
<bodyText confidence="0.9999598">
This table shows a global high level of result
along with similar values for all the frequency
ranges of entries4. Hence, we have decided not to
optimize the C parameter and to adopt the default
value of 1 for all the word-in-context classifiers.
</bodyText>
<figure confidence="0.645737">
G threshold
</figure>
<figureCaption confidence="0.9884005">
Figure 1: R-precision and MAP for various values
of the G threshold
</figureCaption>
<bodyText confidence="0.998312333333333">
The last and the most important implementation
issue is the setting of the threshold G for deter-
mining whether a neighbor is likely to be a bad
</bodyText>
<tableCaption confidence="0.51633525">
4The standard deviation is a little bit higher for the lowest
frequencies but it should be noted that the low number of
examples for low frequency entries does not seem to have
a strong impact on the results of such classifier.
</tableCaption>
<bodyText confidence="0.999879833333333">
neighbor. For this setting, we have randomly cho-
sen a subset of 859 entries of our initial thesaurus
that corresponds to 10% of the entries with at least
one true neighbor in any of our references. Fig-
ure 1 gives the results of the reranked thesaurus
for these entries in terms of R-precision and MAP
against reference W5 for various values of G. Al-
though the level of these measures does not change
a lot for G &gt; 5, the graph of Figure 1 shows that
G = 15 appears to be an optimal value. Hence,
this is the value used for the detailed evaluation of
the next section.
</bodyText>
<subsectionHeader confidence="0.996857">
4.3 Evaluation of the reranked thesaurus
</subsectionHeader>
<bodyText confidence="0.999979125">
Table 4 gives the evaluation of the application of
our reranking method to the initial thesaurus ac-
cording to the same principles as in section 4.1.
The value of each measure comes with its differ-
ence with the corresponding value for the initial
thesaurus. As the recall measure and the precision
for the last rank do not change in a reranking pro-
cess, they are not given again.
The first thing to notice is that at the global
scale, all measures for all references are signifi-
cantly improved6, which means that our hypothe-
sis about the possibility for a discriminative clas-
sifier to capture the meaning of a word tends to
be validated. It is an interesting result since the
features upon which this classifier was built were
taken from WSD and were not specifically se-
lected for this task. As a consequence, there is
probably some room for improvement.
If we go into details, Table 4 clearly shows two
main trends. First, the improvement of results is
particularly effective for middle frequency entries,
then for low frequency and finally, for high fre-
quency entries. Because of their already high level
in the initial thesaurus, results for high frequency
entries are difficult to improve but it is important
to note that our selection of bad neighbors has a
very low error rate, which at least preserves these
results. This is confirmed by the fact that, with
WordNet as reference, only 744 neighbors were
found wrongly downgraded, spread over 686 en-
tries, which represents only 5% of all downgraded
neighbors. The second main trend of Table 4 con-
</bodyText>
<footnote confidence="0.908998714285714">
5The use of W as reference is justified by the fact that the
number of synonyms for an entry in W is more compatible,
especially for R-precision, with the real use of the resulting
thesaurus in an application.
6The statistical significance of differences with the initial
thesaurus was evaluated by a paired Wilcoxon test with p-
value &lt; 0.05 and &lt; 0.01 († and ‡ for non significance).
</footnote>
<figure confidence="0.9942435">
0 5 10 15 20
16
15
14
13
12
R−prec. (W)
MAP (W)
</figure>
<page confidence="0.980161">
567
</page>
<table confidence="0.999169923076923">
freq. ref. R-prec. MAP P@1 P@5 P@10
W 9.1 (0.9) 10.7 (0.9) 12.8 (1.1) 5.6 (0.5) 3.7 (0.3)
all M 7.2 (0.5) 3.5 (0.3) 26.5 (2.4) 17.9 (1.5) 14.0 (1.0)
WM 8.4 (0.7) 6.1 (0.5) 24.8 (2.3) 15.4 (1.3) 11.7 (0.9)
W 11.3 (0.2) † 12.6 (0.1) 17.3 (0.1) $ 7.8 (0.1) $ 5.1 (0.0)
high M 10.3 (0.1) 4.9 (0.0) 42.1 (0.8) 28.4 (0.4) 22.1 (0.2)
WM 11.1 (0.1) 6.6 (0.1) 42.0 (0.7) 27.2 (0.4) 20.9 (0.1)
W 11.8 (1.4) 13.8 (1.3) 15.7 (2.1) 6.5 (0.7) 4.1 (0.4)
middle M 7.3 (0.8) 3.6 (0.5) 23.3 (4.6) 16.0 (2.9) 12.4 (2.0)
WM 10.3 (1.0) 8.1 (0.7) 25.1 (4.2) 14.6 (2.3) 10.9 (1.6)
W 3.2 (1.1) 4.6 (1.3) 3.9 (1.3) 1.8 (0.6) 1.3 (0.4)
low M 1.8 (0.6) 0.8 (0.3) 4.4 (1.9) 2.9 (1.4) 2.6 (1.1)
WM 3.1 (1.0) 3.3 (0.9) 5.1 (1.8) 2.9 (1.2) 2.3 (0.8)
</table>
<tableCaption confidence="0.999886">
Table 4: Results of the reranking of semantic neighbors
</tableCaption>
<bodyText confidence="0.999291307692308">
cerns the type of semantic relations: results with
Moby as reference are improved in a larger ex-
tent than results with WordNet as reference. This
suggests that our procedure is more effective for
semantically related words than for semantically
similar words, which can be considered as a lit-
tle bit surprising since the notion of context in our
discriminative classifier seems a priori more strict
than in “classical” distributional contexts. How-
ever, this point must be investigated further as a
significant part of the relations in Moby, even if
they do no represent the largest part of them, are
paradigmatic relations.
</bodyText>
<table confidence="0.9998748">
WordNet respect, admiration, regard
Moby admiration, appreciation, accep-
tance, dignity, regard, respect, ac-
count, adherence, consideration,
estimate, estimation, fame, great-
ness, reverence + 79 words more
initial cordiality, gratitude, admiration,
comradeship, back-scratching,
perplexity, respect, ruination,
appreciation, neighbourliness ...
reranking gratitude, admiration, respect,
appreciation, neighborliness, trust,
empathy, goodwill, reciprocity,
half-staff, affection, self-esteem,
reverence, longing, regard ...
</table>
<tableCaption confidence="0.7162915">
Table 5: Impact of our reranking for the entry es-
teem
</tableCaption>
<bodyText confidence="0.997227941176471">
Table 5 illustrates more precisely the impact of
our reranking procedure for the middle frequency
entry esteem. Its WordNet row gives all the refer-
ence synonyms for this entry in WordNet while its
Moby row gives the first reference related words
for this entry in Moby. In our initial thesaurus, the
first two neighbors of esteem that are present in our
reference resources are admiration (rank 3) and re-
spect (rank 7). The reranking produces a thesaurus
in which these two words appear as the second
and the third neighbors of the entry because neigh-
bors without clear relation with it such as back-
scratching were downgraded while its third syn-
onym in WordNet is raised from rank 22 to rank
15. Moreover, the number of neighbors among the
first 15 ones that are present in Moby increases
from 3 to 5.
</bodyText>
<sectionHeader confidence="0.999805" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999870818181818">
The building of distributional thesaurus is gener-
ally viewed as an application or a mode of eval-
uation of work about semantic similarity or se-
mantic relatedness. As a consequence, the im-
provement of such thesaurus is generally not di-
rectly addressed but is a possible consequence
of the improvement of semantic similarity mea-
sures. However, the extent of this improvement
is rarely evaluated as most of the work about se-
mantic similarity is evaluated on datasets such as
the WordSim-353 test collection (Gabrilovich and
Markovitch, 2007), which are only partially repre-
sentative of the results for thesaurus building.
If we consider more specifically the problem of
improving semantic similarity, and by the way the-
sauri, in a given paradigm, (Broda et al., 2009),
(Zhitomirsky-Geffet and Dagan, 2009) and (Ya-
mamoto and Asakura, 2010), which all take place
in the paradigm defined by (Grefenstette, 1994),
are the closest works to ours. (Broda et al., 2009)
proposes a new weighting scheme of words in
distributional contexts that replaces the weight of
</bodyText>
<page confidence="0.992392">
568
</page>
<bodyText confidence="0.999972285714286">
word by a function of its rank in the context, which
is a way to be less dependent on the values of a par-
ticular weighting function. (Zhitomirsky-Geffet
and Dagan, 2009) shares with our work the use
of bootstrapping by relying on an initial thesaurus
to derive means of improving it. More specifi-
cally, (Zhitomirsky-Geffet and Dagan, 2009) as-
sumes that the first neighbors of an entry are more
relevant than the others and as a consequence, that
their most significant features are also representa-
tive of the meaning of the entry. The neighbors
of the entry are reranked according to this hypoth-
esis by increasing the weight of these features to
favor their influence in the distributional contexts
that support the evaluation of the similarity be-
tween the entry and its neighbors. (Yamamoto and
Asakura, 2010) is a variant of (Zhitomirsky-Geffet
and Dagan, 2009) that takes into account a larger
number of features for the reranking process. One
main difference between all these works and ours
is that they assume that the initial thesaurus was
built by relying on distributional contexts repre-
sented as bags-of-words. Our method does not
make this assumption as its reranking is based on
a classifier built in an unsupervised way7 from and
applied to the corpus used for building the initial
thesaurus. As a consequence, it could even be ap-
plied to other paradigms than (Grefenstette, 1994).
If we focus more specifically on the improve-
ment of distributional thesauri, (Ferret, 2012) is
the most comparable work to ours, both because
it is specifically focused on this task and it is
based on the same evaluation framework. (Fer-
ret, 2012) selects in an unsupervised way a set
of positive and negative examples of semantically
similar words from the initial thesaurus, uses them
for training a classifier deciding whether or not a
pair of words are semantically similar and finally,
applies this classifier to the neighbors of each en-
try for reranking them. One of the objectives of
(Ferret, 2012) was to rebalance the initial the-
saurus in favor of low frequency entries. Although
this objective was reached, the resulting thesaurus
tends to have a lower performance than the initial
thesaurus for high frequency entries and for syn-
onyms. The problem with high frequency entries
comes from the fact that applying a machine learn-
ing classifier to its training examples does not lead
to a perfect result. The problem with synonyms
</bodyText>
<footnote confidence="0.987837">
7It is a supervised classifier but its training set is selected
in an unsupervised way.
</footnote>
<bodyText confidence="0.999639863636364">
arises from the imbalance between semantic simi-
larity and semantic relatedness among training ex-
amples: most of selected examples were pairs of
words linked by semantic relatedness because this
kind of relations are more frequent among seman-
tic neighbors than relations based on semantic sim-
ilarity.
In both cases, the method proposed in (Ferret,
2012) faces the problem of relying only on the dis-
tributional thesaurus it tries to improve. This is an
important difference with the method presented in
this article, which mainly exploits the context of
the occurrences of words in the corpus used for the
building the initial thesaurus. As a consequence, at
a global scale, our reranked thesaurus outperforms
the final thesaurus of (Ferret, 2012) for nearly all
measures. The only exceptions are the P@1 values
for M and WM as reference. However, it should be
noted that values for both MAP and R-precision,
which are more reliable measures than P@1, are
identical for the two thesauri and the same refer-
ences.
</bodyText>
<sectionHeader confidence="0.985815" genericHeader="conclusions">
6 Conclusion and perspectives
</sectionHeader>
<bodyText confidence="0.999979045454545">
In this article, we have presented a new approach
for reranking the semantic neighbors of a distribu-
tional thesaurus. This approach relies on the unsu-
pervised building of discriminative classifiers ded-
icated to the identification of its entries in texts,
with the objective to characterize their meaning
according to the distributional hypothesis. The
classifier built for an entry is then applied to a
set of occurrences of its neighbors for identifying
and downgrading those that are not semantically
related to the entry. The proposed method was
tested on a large thesaurus of nouns for English
and led to a significant improvement of this the-
saurus, especially for middle and low frequency
entries and for semantic relatedness. We plan to
extend this work by taking into account the no-
tion of word sense as it is done in (Reisinger and
Mooney, 2010) or (Huang et al., 2012): since we
rely on occurrences of words in texts, this exten-
sion should be quite straightforward by turning our
word-in-context classifiers into true word sense
classifiers.
</bodyText>
<sectionHeader confidence="0.998318" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9489685">
This work was partly supported by the project
ANR ASFALDA ANR-12-CORD-0023.
</bodyText>
<page confidence="0.997657">
569
</page>
<sectionHeader confidence="0.989338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999595842592593">
Andrei Alexandrescu and Katrin Kirchhoff. 2007.
Data-driven graph construction for semi-supervised
graph-based learning in NLP. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL HLT 2007), pages 204–
211, Rochester, New York.
Satanjeev Bano Banerjee and Ted Pedersen. 2003. Ex-
tended gloss overlaps as a measure of semantic relat-
edness. In Eighteenth International Conference on
Artificial Intelligence (IJCAI-03), Acapulco, Mex-
ico.
Bartosz Broda, Maciej Piasecki, and Stan Szpakow-
icz. 2009. Rank-Based Transformation in Measur-
ing Semantic Relatedness. In 22nd Canadian Con-
ference on Artificial Intelligence, pages 187–190.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13–
47.
James Curran and Marc Moens. 2002a. Scaling con-
text space. In 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-02), pages
231–238, Philadelphia, Pennsylvania, USA.
James R. Curran and Marc Moens. 2002b. Improve-
ments in automatic thesaurus extraction. In Work-
shop of the ACL Special Interest Group on the Lexi-
con (SIGLEX), pages 59–66, Philadelphia, USA.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In 481h An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), short paper, pages 92–97,
Uppsala, Sweden, July.
Olivier Ferret. 2010. Testing semantic similarity mea-
sures for extracting synonyms from a corpus. In
Seventh conference on International Language Re-
sources and Evaluation (LREC’10), Valletta, Malta.
Olivier Ferret. 2012. Combining bootstrapping and
feature selection for improving a distributional the-
saurus. In 201h European Conference on Artificial
Intelligence (ECAI2012), pages 336–341, Montpel-
lier, France.
John R. Firth, 1957. Studies in Linguistic Analysis,
chapter A synopsis of linguistic theory 1930-1955,
pages 1–32. Blackwell, Oxford.
Dayne Freitag, Matthias Blume, John Byrnes, Ed-
mond Chow, Sadik Kapadia, Richard Rohwer, and
Zhiqiang Wang. 2005. New experiments in distribu-
tional representations of synonymy. In Ninth Con-
ference on Computational Natural Language Learn-
ing (CoNLL), pages 25–32, Ann Arbor, Michigan,
USA.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In 201h Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI 2007), pages 6–12.
William A Gale, Kenneth W Church, and David
Yarowsky. 1992. Work on statistical methods for
word sense disambiguation. In AAAI Fall Sympo-
sium on Probabilistic Approaches to Natural Lan-
guage, pages 54–60.
Gregory Grefenstette. 1994. Explorations in auto-
matic thesaurus discovery. Kluwer Academic Pub-
lishers.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Kris Heylen, Yves Peirsmany, Dirk Geeraerts, and
Dirk Speelman. 2008. Modelling Word Similarity:
An Evaluation of Automatic Synonymy Extraction
Algorithms. In Sixth conference on International
Language Resources and Evaluation (LREC 2008),
Marrakech, Morocco.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In 50th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’12), pages
873–882.
Jun’ichi Kazama, Stijn De Saeger, Kow Kuroda,
Masaki Murata, and Kentaro Torisawa. 2010. A
bayesian method for robust estimation of distribu-
tional similarities. In 481h Annual Meeting of the
Association for Computational Linguistics, pages
247–256, Uppsala, Sweden.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning
algorithms for word sense disambiguation. In 2002
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002), pages 41–48.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In 171h International Confer-
ence on Computational Linguistics and 361h Annual
Meeting of the Association for Computational Lin-
guistics (ACL-COLING’98), pages 768–774, Mon-
tral, Canada.
George A. Miller. 1990. WordNet: An On-Line Lex-
ical Database. International Journal of Lexicogra-
phy, 3(4).
Jane Morris and Graeme Hirst. 2004. Non-
classical lexical semantic relations. In Workshop
on Computational Lexical Semantics of Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 46–51, Boston, MA.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.
</reference>
<page confidence="0.963182">
570
</page>
<reference confidence="0.999803741935484">
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the
relatedness of concepts. In HLT-NAACL 2004,
demonstration papers, pages 38–41, Boston, Mas-
sachusetts, USA.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL 2010), pages 109–117, Los Angeles,
California, June.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing.
Grady Ward. 1996. Moby thesaurus. Moby Project.
Julie Weeds. 2003. Measures and Applications of Lex-
ical Distributional Similarity. Ph.D. thesis, Depart-
ment of Informatics, University of Sussex.
Kazuhide Yamamoto and Takeshi Asakura. 2010.
Even unassociated features can improve lexical dis-
tributional similarity. In Second Workshop on
NLP Challenges in the Information Explosion Era
(NLPIX 2010), pages 32–39, Beijing, China.
Torsten Zesch and Iryna Gurevych. 2010. Wisdom
of crowds versus wisdom of linguists - measuring
the semantic relatdness of words. Natural Language
Engineering, 16(1):25–59.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009.
Bootstrapping Distributional Feature Vector Quality.
Computational Linguistics, 35(3):435–461.
</reference>
<page confidence="0.997864">
571
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.712177">
<title confidence="0.95413">Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</title>
<author confidence="0.955864">Olivier</author>
<affiliation confidence="0.879897">CEA, LIST, Vision and Content Engineering</affiliation>
<address confidence="0.989376">Gif-sur-Yvette, F-91191</address>
<email confidence="0.994934">olivier.ferret@cea.fr</email>
<abstract confidence="0.995200695652174">Distributional thesauri are now widely used in a large number of Natural Language Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. In this article, we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrei Alexandrescu</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Data-driven graph construction for semi-supervised graph-based learning in NLP.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT</booktitle>
<pages>204--211</pages>
<location>Rochester, New York.</location>
<contexts>
<context position="8713" citStr="Alexandrescu and Kirchhoff, 2007" startWordPosition="1407" endWordPosition="1410">, 2010) where the distributional representation of a word is modeled as a multinomial distribution with Dirichlet as prior. However, this approach globally faces a certain lack of diversity and complexity of the features of its models. For instance, features such as ngrams of words or ngrams of parts of speech are not considered whereas they are widely used in tasks such as word sense disambiguation (WSD) for instance, probably because they would lead to very large models and because similarity measures such as the Cosine measure are not necessarily suitable for heterogeneous representations (Alexandrescu and Kirchhoff, 2007). Hence, we propose in this article to build a discriminative model for repre562 abnormality defect [0.30], disorder [0.23], deformity [0.22], mutation [0.21], prolapse [0.21], anomaly [0.21] ... agreement accord [0.44], deal [0.41], pact [0.38], treaty [0.36], negotiation [0.35], proposal [0.32], arrangement [0.30] ... cabdriver waterworks [0.23], toolmaker [0.22], weaponeer [0.17], valkyry [0.17], wang [0.17], amusement-park [0.17] ... machination hollowness [0.15], share-price [0.12], clockmaker [0.12], huguenot [0.12], wrangling [0.12], alternation [0.12] ... Table 1: First neighbors of so</context>
</contexts>
<marker>Alexandrescu, Kirchhoff, 2007</marker>
<rawString>Andrei Alexandrescu and Katrin Kirchhoff. 2007. Data-driven graph construction for semi-supervised graph-based learning in NLP. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2007), pages 204– 211, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Bano Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Eighteenth International Conference on Artificial Intelligence (IJCAI-03),</booktitle>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="3689" citStr="Banerjee and Pedersen, 2003" startWordPosition="580" endWordPosition="583">first one relies on handcrafted resources in which semantic relations are clearly identified. Work based on WordNet-like lexical networks for building semantic similarity measures such as (Budanitsky and Hirst, 2006) or (Pedersen et al., 2004) falls into this category. These measures typically exploit the hierarchical structure of these networks, based on hypernymy relations. The second approach makes use of a less structured source of knowledge about words such as the definitions of classical dictionaries or the glosses of WordNet. WordNet’s glosses were used to support Lesklike measures in (Banerjee and Pedersen, 2003) and more recently, measures were also defined from Wikipedia or Wiktionaries (Gabrilovich and 561 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–571, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Markovitch, 2007). The last option is the corpusbased approach, based on the distributional hypothesis (Firth, 1957): each word is characterized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share. This perspective was first ad</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Bano Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Eighteenth International Conference on Artificial Intelligence (IJCAI-03), Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bartosz Broda</author>
<author>Maciej Piasecki</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Rank-Based Transformation in Measuring Semantic Relatedness.</title>
<date>2009</date>
<booktitle>In 22nd Canadian Conference on Artificial Intelligence,</booktitle>
<pages>187--190</pages>
<contexts>
<context position="4764" citStr="Broda et al., 2009" startWordPosition="754" endWordPosition="757">om a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of context elements according to the semantic neighbors found by an initial distributional similarity measure. However, another part of these proposals implies more radical changes. The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pad´o and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk</context>
<context position="34804" citStr="Broda et al., 2009" startWordPosition="5879" endWordPosition="5882">ntic relatedness. As a consequence, the improvement of such thesaurus is generally not directly addressed but is a possible consequence of the improvement of semantic similarity measures. However, the extent of this improvement is rarely evaluated as most of the work about semantic similarity is evaluated on datasets such as the WordSim-353 test collection (Gabrilovich and Markovitch, 2007), which are only partially representative of the results for thesaurus building. If we consider more specifically the problem of improving semantic similarity, and by the way thesauri, in a given paradigm, (Broda et al., 2009), (Zhitomirsky-Geffet and Dagan, 2009) and (Yamamoto and Asakura, 2010), which all take place in the paradigm defined by (Grefenstette, 1994), are the closest works to ours. (Broda et al., 2009) proposes a new weighting scheme of words in distributional contexts that replaces the weight of 568 word by a function of its rank in the context, which is a way to be less dependent on the values of a particular weighting function. (Zhitomirsky-Geffet and Dagan, 2009) shares with our work the use of bootstrapping by relying on an initial thesaurus to derive means of improving it. More specifically, (Z</context>
</contexts>
<marker>Broda, Piasecki, Szpakowicz, 2009</marker>
<rawString>Bartosz Broda, Maciej Piasecki, and Stan Szpakowicz. 2009. Rank-Based Transformation in Measuring Semantic Relatedness. In 22nd Canadian Conference on Artificial Intelligence, pages 187–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating wordnet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<pages>47</pages>
<contexts>
<context position="2190" citStr="Budanitsky and Hirst, 2006" startWordPosition="342" endWordPosition="345">der of their weight. The term semantic neighbor is very generic and can have two main interpretations according to the kind of semantic relations it is based on: one relies only on paradigmatic relations, such as hypernymy or synonymy, while the other considers syntagmatic relations, called collocation relations by (Halliday and Hasan, 1976) in the context of lexical cohesion or “non-classical relations” by (Morris and Hirst, 2004). The distinction between these two interpretations refers to the distinction between the notions of semantic similarity and semantic relatedness as it was done in (Budanitsky and Hirst, 2006) or in (Zesch and Gurevych, 2010) for instance. However, the limit between these two notions is sometimes hard to find in existing work as terms semantic similarity and semantic relatedness are often used interchangeably. Moreover, semantic similarity is frequently considered as included into semantic relatedness and the two problems are often tackled by using the same methods. In the remainder of this article, we will use the term semantic similarity with its generic sense and the term semantic relatedness for referring more specifically to similarity based on syntagmatic relations. Following</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13– 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Marc Moens</author>
</authors>
<title>Scaling context space.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting of the Association for Computational Linguistics (ACL-02),</booktitle>
<pages>231--238</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="4391" citStr="Curran and Moens, 2002" startWordPosition="689" endWordPosition="692">abrilovich and 561 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–571, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Markovitch, 2007). The last option is the corpusbased approach, based on the distributional hypothesis (Firth, 1957): each word is characterized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of</context>
<context position="7556" citStr="Curran and Moens, 2002" startWordPosition="1213" endWordPosition="1216">actice that only the first neighbors of an entry can be generally exploited. The approach we propose for identifying the bad semantic neighbors of a thesaurus entry relies on the distributional hypothesis, as the method for the initial building of the thesaurus, but implements it in a different way. This hypothesis roughly specifies that from a semantic viewpoint, the meaning of a word can be characterized by the set of contexts in which this word occurs. As a consequence, two words are considered as semantically similar if they occur in a large enough set of shared contexts. In work such as (Curran and Moens, 2002a), this hypothesis is implemented by collecting for each entry the words it co-occurs with in a large corpus. This co-occurrence can be based either on the position of the word in the text in relation to the entry or on the presence of a syntactic relation between the entry and the word. As a result, the distributional representation of a word takes the unstructured form of a bag of words or the more structured form of a set of pairs {syntactic relation, word}. A variant of this approach was proposed in (Kazama et al., 2010) where the distributional representation of a word is modeled as a mu</context>
<context position="11811" citStr="Curran and Moens, 2002" startWordPosition="1905" endWordPosition="1909">o each of its entries: • building of a classifier for determining whether a word in a sentence corresponds or not to the entry; • selection of a set of examples sentences for each of the neighbors of the entry in the thesaurus; • application of the classifier to these sentences; • identification of bad neighbors according to the results of the classifier; • reranking of entry’s neighbors according to bad neighbors. 3.2 Building of the initial thesaurus Before introducing our method for improving distributional thesauri, we first present the way we build such a thesaurus. As in (Lin, 1998) or (Curran and Moens, 2002a), this building is based on the definition of a semantic similarity measure from a corpus. The corpus used for defining this measure was the AQUAINT-2 corpus, a middlesize corpus made of around 380 million words coming from news articles. Although our target language is English, we chose to limit deliberately the level of the tools applied for preprocessing texts to part-of-speech tagging and lemmatization to make possible the transposition of our method to a large set of languages. This seems to be a reasonable compromise between the approach of (Freitag et al., 2005), in which none normali</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James Curran and Marc Moens. 2002a. Scaling context space. In 40th Annual Meeting of the Association for Computational Linguistics (ACL-02), pages 231–238, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Marc Moens</author>
</authors>
<title>Improvements in automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In Workshop of the ACL Special Interest Group on the Lexicon (SIGLEX),</booktitle>
<pages>59--66</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="4391" citStr="Curran and Moens, 2002" startWordPosition="689" endWordPosition="692">abrilovich and 561 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–571, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Markovitch, 2007). The last option is the corpusbased approach, based on the distributional hypothesis (Firth, 1957): each word is characterized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of</context>
<context position="7556" citStr="Curran and Moens, 2002" startWordPosition="1213" endWordPosition="1216">actice that only the first neighbors of an entry can be generally exploited. The approach we propose for identifying the bad semantic neighbors of a thesaurus entry relies on the distributional hypothesis, as the method for the initial building of the thesaurus, but implements it in a different way. This hypothesis roughly specifies that from a semantic viewpoint, the meaning of a word can be characterized by the set of contexts in which this word occurs. As a consequence, two words are considered as semantically similar if they occur in a large enough set of shared contexts. In work such as (Curran and Moens, 2002a), this hypothesis is implemented by collecting for each entry the words it co-occurs with in a large corpus. This co-occurrence can be based either on the position of the word in the text in relation to the entry or on the presence of a syntactic relation between the entry and the word. As a result, the distributional representation of a word takes the unstructured form of a bag of words or the more structured form of a set of pairs {syntactic relation, word}. A variant of this approach was proposed in (Kazama et al., 2010) where the distributional representation of a word is modeled as a mu</context>
<context position="11811" citStr="Curran and Moens, 2002" startWordPosition="1905" endWordPosition="1909">o each of its entries: • building of a classifier for determining whether a word in a sentence corresponds or not to the entry; • selection of a set of examples sentences for each of the neighbors of the entry in the thesaurus; • application of the classifier to these sentences; • identification of bad neighbors according to the results of the classifier; • reranking of entry’s neighbors according to bad neighbors. 3.2 Building of the initial thesaurus Before introducing our method for improving distributional thesauri, we first present the way we build such a thesaurus. As in (Lin, 1998) or (Curran and Moens, 2002a), this building is based on the definition of a semantic similarity measure from a corpus. The corpus used for defining this measure was the AQUAINT-2 corpus, a middlesize corpus made of around 380 million words coming from news articles. Although our target language is English, we chose to limit deliberately the level of the tools applied for preprocessing texts to part-of-speech tagging and lemmatization to make possible the transposition of our method to a large set of languages. This seems to be a reasonable compromise between the approach of (Freitag et al., 2005), in which none normali</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James R. Curran and Marc Moens. 2002b. Improvements in automatic thesaurus extraction. In Workshop of the ACL Special Interest Group on the Lexicon (SIGLEX), pages 59–66, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pado</author>
</authors>
<title>Exemplar-based models for word meaning in context.</title>
<date>2010</date>
<booktitle>In 481h Annual Meeting of the Association for Computational Linguistics (ACL 2010), short paper,</booktitle>
<pages>92--97</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5380" citStr="Erk and Pado, 2010" startWordPosition="846" endWordPosition="849">09), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of context elements according to the semantic neighbors found by an initial distributional similarity measure. However, another part of these proposals implies more radical changes. The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pad´o and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al., 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al., 2010) can be classified into this second category. The work we present in this article takes place in the framework defined by (Grefenstette, 1994) for implementing the distributional approach but proposes a new method for improving a thesaurus built in this context based on the identification of its bad semantic neighbors rather than on the adaptation of the weight of their features. 2 Principles Our work shares with (Zhitomirsky-Geffet and Dagan, 200</context>
</contexts>
<marker>Erk, Pado, 2010</marker>
<rawString>Katrin Erk and Sebastian Pado. 2010. Exemplar-based models for word meaning in context. In 481h Annual Meeting of the Association for Computational Linguistics (ACL 2010), short paper, pages 92–97, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
</authors>
<title>Testing semantic similarity measures for extracting synonyms from a corpus.</title>
<date>2010</date>
<booktitle>In Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="12781" citStr="Ferret, 2010" startWordPosition="2067" endWordPosition="2068">sing texts to part-of-speech tagging and lemmatization to make possible the transposition of our method to a large set of languages. This seems to be a reasonable compromise between the approach of (Freitag et al., 2005), in which none normalization of words is done, and the more widespread use of syntactic parsers in work such as (Lin, 1998). More precisely, we used TreeTagger (Schmid, 1994) for performing the linguistic preprocessing of the AQUAINT-2 corpus. For the extraction of distributional data and the characteristics of the distributional similarity measure, we adopted the options of (Ferret, 2010), resulting from a kind of grid search procedure performed with the extended TOEFL test proposed in (Freitag et al., 2005) as an optimization objective. More precisely, the following characteristics were taken: • distributional contexts made of the cooccurrents collected in a 3 word window centered on each occurrence in the corpus of the target word. These co-occurrents were restricted to nouns, verbs and adjectives; • soft filtering of contexts: removal of cooccurrents with only one occurrence; • weighting function of co-occurrents in con563 texts = Pointwise Mutual Information (PMI) between </context>
</contexts>
<marker>Ferret, 2010</marker>
<rawString>Olivier Ferret. 2010. Testing semantic similarity measures for extracting synonyms from a corpus. In Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
</authors>
<title>Combining bootstrapping and feature selection for improving a distributional thesaurus. In</title>
<date>2012</date>
<booktitle>201h European Conference on Artificial Intelligence (ECAI2012),</booktitle>
<pages>336--341</pages>
<location>Montpellier, France.</location>
<contexts>
<context position="36585" citStr="Ferret, 2012" startWordPosition="6175" endWordPosition="6176">s into account a larger number of features for the reranking process. One main difference between all these works and ours is that they assume that the initial thesaurus was built by relying on distributional contexts represented as bags-of-words. Our method does not make this assumption as its reranking is based on a classifier built in an unsupervised way7 from and applied to the corpus used for building the initial thesaurus. As a consequence, it could even be applied to other paradigms than (Grefenstette, 1994). If we focus more specifically on the improvement of distributional thesauri, (Ferret, 2012) is the most comparable work to ours, both because it is specifically focused on this task and it is based on the same evaluation framework. (Ferret, 2012) selects in an unsupervised way a set of positive and negative examples of semantically similar words from the initial thesaurus, uses them for training a classifier deciding whether or not a pair of words are semantically similar and finally, applies this classifier to the neighbors of each entry for reranking them. One of the objectives of (Ferret, 2012) was to rebalance the initial thesaurus in favor of low frequency entries. Although thi</context>
<context position="37970" citStr="Ferret, 2012" startWordPosition="6403" endWordPosition="6404">high frequency entries comes from the fact that applying a machine learning classifier to its training examples does not lead to a perfect result. The problem with synonyms 7It is a supervised classifier but its training set is selected in an unsupervised way. arises from the imbalance between semantic similarity and semantic relatedness among training examples: most of selected examples were pairs of words linked by semantic relatedness because this kind of relations are more frequent among semantic neighbors than relations based on semantic similarity. In both cases, the method proposed in (Ferret, 2012) faces the problem of relying only on the distributional thesaurus it tries to improve. This is an important difference with the method presented in this article, which mainly exploits the context of the occurrences of words in the corpus used for the building the initial thesaurus. As a consequence, at a global scale, our reranked thesaurus outperforms the final thesaurus of (Ferret, 2012) for nearly all measures. The only exceptions are the P@1 values for M and WM as reference. However, it should be noted that values for both MAP and R-precision, which are more reliable measures than P@1, ar</context>
</contexts>
<marker>Ferret, 2012</marker>
<rawString>Olivier Ferret. 2012. Combining bootstrapping and feature selection for improving a distributional thesaurus. In 201h European Conference on Artificial Intelligence (ECAI2012), pages 336–341, Montpellier, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<title>Studies in Linguistic Analysis, chapter A synopsis of linguistic theory 1930-1955,</title>
<date>1957</date>
<pages>1--32</pages>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="4091" citStr="Firth, 1957" startWordPosition="638" endWordPosition="639"> structured source of knowledge about words such as the definitions of classical dictionaries or the glosses of WordNet. WordNet’s glosses were used to support Lesklike measures in (Banerjee and Pedersen, 2003) and more recently, measures were also defined from Wikipedia or Wiktionaries (Gabrilovich and 561 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–571, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Markovitch, 2007). The last option is the corpusbased approach, based on the distributional hypothesis (Firth, 1957): each word is characterized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the ele</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John R. Firth, 1957. Studies in Linguistic Analysis, chapter A synopsis of linguistic theory 1930-1955, pages 1–32. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
<author>Matthias Blume</author>
<author>John Byrnes</author>
<author>Edmond Chow</author>
<author>Sadik Kapadia</author>
<author>Richard Rohwer</author>
<author>Zhiqiang Wang</author>
</authors>
<title>New experiments in distributional representations of synonymy.</title>
<date>2005</date>
<booktitle>In Ninth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>25--32</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="12388" citStr="Freitag et al., 2005" startWordPosition="2004" endWordPosition="2007">As in (Lin, 1998) or (Curran and Moens, 2002a), this building is based on the definition of a semantic similarity measure from a corpus. The corpus used for defining this measure was the AQUAINT-2 corpus, a middlesize corpus made of around 380 million words coming from news articles. Although our target language is English, we chose to limit deliberately the level of the tools applied for preprocessing texts to part-of-speech tagging and lemmatization to make possible the transposition of our method to a large set of languages. This seems to be a reasonable compromise between the approach of (Freitag et al., 2005), in which none normalization of words is done, and the more widespread use of syntactic parsers in work such as (Lin, 1998). More precisely, we used TreeTagger (Schmid, 1994) for performing the linguistic preprocessing of the AQUAINT-2 corpus. For the extraction of distributional data and the characteristics of the distributional similarity measure, we adopted the options of (Ferret, 2010), resulting from a kind of grid search procedure performed with the extended TOEFL test proposed in (Freitag et al., 2005) as an optimization objective. More precisely, the following characteristics were tak</context>
</contexts>
<marker>Freitag, Blume, Byrnes, Chow, Kapadia, Rohwer, Wang, 2005</marker>
<rawString>Dayne Freitag, Matthias Blume, John Byrnes, Edmond Chow, Sadik Kapadia, Richard Rohwer, and Zhiqiang Wang. 2005. New experiments in distributional representations of synonymy. In Ninth Conference on Computational Natural Language Learning (CoNLL), pages 25–32, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In 201h International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<pages>6--12</pages>
<contexts>
<context position="34578" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="5842" endWordPosition="5845">eighbors among the first 15 ones that are present in Moby increases from 3 to 5. 5 Related work The building of distributional thesaurus is generally viewed as an application or a mode of evaluation of work about semantic similarity or semantic relatedness. As a consequence, the improvement of such thesaurus is generally not directly addressed but is a possible consequence of the improvement of semantic similarity measures. However, the extent of this improvement is rarely evaluated as most of the work about semantic similarity is evaluated on datasets such as the WordSim-353 test collection (Gabrilovich and Markovitch, 2007), which are only partially representative of the results for thesaurus building. If we consider more specifically the problem of improving semantic similarity, and by the way thesauri, in a given paradigm, (Broda et al., 2009), (Zhitomirsky-Geffet and Dagan, 2009) and (Yamamoto and Asakura, 2010), which all take place in the paradigm defined by (Grefenstette, 1994), are the closest works to ours. (Broda et al., 2009) proposes a new weighting scheme of words in distributional contexts that replaces the weight of 568 word by a function of its rank in the context, which is a way to be less depend</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipediabased explicit semantic analysis. In 201h International Joint Conference on Artificial Intelligence (IJCAI 2007), pages 6–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>Work on statistical methods for word sense disambiguation.</title>
<date>1992</date>
<booktitle>In AAAI Fall Symposium on Probabilistic Approaches to Natural Language,</booktitle>
<pages>54--60</pages>
<contexts>
<context position="15029" citStr="Gale et al., 1992" startWordPosition="2452" endWordPosition="2455">k in which the occurrences of a target word T are labeled with two tags: E and notE. In the context of our global objective, we are not of course interested by this task itself but rather by the fact that such classifier is likely to model the contexts in which E occurs and as a consequence, is also likely to model its meaning according to the distributional hypothesis. A step further, such classifier can be viewed as a means for testing whether or not a word has the same meaning as E. This is a problem close to WSD as it is performed in the context of the pseudo-word disambiguation paradigm (Gale et al., 1992): a pseudo-word is created with two senses, E and notE, notE corresponding to one or several words that are supposed to be representative of a meaning different from the meaning of E. The objective is then to build a classifier for distinguishing the pseudo-senses E and notE. As a consequence of this view, we adopt the same kind of features as the ones used for WSD for building our classifier. More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each con</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A Gale, Kenneth W Church, and David Yarowsky. 1992. Work on statistical methods for word sense disambiguation. In AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 54–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in automatic thesaurus discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="2824" citStr="Grefenstette, 1994" startWordPosition="443" endWordPosition="444">nd Gurevych, 2010) for instance. However, the limit between these two notions is sometimes hard to find in existing work as terms semantic similarity and semantic relatedness are often used interchangeably. Moreover, semantic similarity is frequently considered as included into semantic relatedness and the two problems are often tackled by using the same methods. In the remainder of this article, we will use the term semantic similarity with its generic sense and the term semantic relatedness for referring more specifically to similarity based on syntagmatic relations. Following work such as (Grefenstette, 1994), a widespread way to build a thesaurus from a corpus is to use a semantic similarity measure for extracting the semantic neighbors of the entries of the thesaurus. Three main ways of implementing such measures can be distinguished. The first one relies on handcrafted resources in which semantic relations are clearly identified. Work based on WordNet-like lexical networks for building semantic similarity measures such as (Budanitsky and Hirst, 2006) or (Pedersen et al., 2004) falls into this category. These measures typically exploit the hierarchical structure of these networks, based on hyper</context>
<context position="4318" citStr="Grefenstette, 1994" startWordPosition="678" endWordPosition="679">recently, measures were also defined from Wikipedia or Wiktionaries (Gabrilovich and 561 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–571, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Markovitch, 2007). The last option is the corpusbased approach, based on the distributional hypothesis (Firth, 1957): each word is characterized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura,</context>
<context position="5671" citStr="Grefenstette, 1994" startWordPosition="895" endWordPosition="896">y an initial distributional similarity measure. However, another part of these proposals implies more radical changes. The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pad´o and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al., 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al., 2010) can be classified into this second category. The work we present in this article takes place in the framework defined by (Grefenstette, 1994) for implementing the distributional approach but proposes a new method for improving a thesaurus built in this context based on the identification of its bad semantic neighbors rather than on the adaptation of the weight of their features. 2 Principles Our work shares with (Zhitomirsky-Geffet and Dagan, 2009) the use of a kind of bootstrapping as it starts from a distributional thesaurus and to some extent, exploits it for its improvement. However, it adopts a more indirect approach: instead of selecting the “best” semantic neighbors of an entry in the thesaurus for adapting the weights of di</context>
<context position="34945" citStr="Grefenstette, 1994" startWordPosition="5902" endWordPosition="5903">e improvement of semantic similarity measures. However, the extent of this improvement is rarely evaluated as most of the work about semantic similarity is evaluated on datasets such as the WordSim-353 test collection (Gabrilovich and Markovitch, 2007), which are only partially representative of the results for thesaurus building. If we consider more specifically the problem of improving semantic similarity, and by the way thesauri, in a given paradigm, (Broda et al., 2009), (Zhitomirsky-Geffet and Dagan, 2009) and (Yamamoto and Asakura, 2010), which all take place in the paradigm defined by (Grefenstette, 1994), are the closest works to ours. (Broda et al., 2009) proposes a new weighting scheme of words in distributional contexts that replaces the weight of 568 word by a function of its rank in the context, which is a way to be less dependent on the values of a particular weighting function. (Zhitomirsky-Geffet and Dagan, 2009) shares with our work the use of bootstrapping by relying on an initial thesaurus to derive means of improving it. More specifically, (Zhitomirsky-Geffet and Dagan, 2009) assumes that the first neighbors of an entry are more relevant than the others and as a consequence, that </context>
<context position="36492" citStr="Grefenstette, 1994" startWordPosition="6161" endWordPosition="6162">ghbors. (Yamamoto and Asakura, 2010) is a variant of (Zhitomirsky-Geffet and Dagan, 2009) that takes into account a larger number of features for the reranking process. One main difference between all these works and ours is that they assume that the initial thesaurus was built by relying on distributional contexts represented as bags-of-words. Our method does not make this assumption as its reranking is based on a classifier built in an unsupervised way7 from and applied to the corpus used for building the initial thesaurus. As a consequence, it could even be applied to other paradigms than (Grefenstette, 1994). If we focus more specifically on the improvement of distributional thesauri, (Ferret, 2012) is the most comparable work to ours, both because it is specifically focused on this task and it is based on the same evaluation framework. (Ferret, 2012) selects in an unsupervised way a set of positive and negative examples of semantically similar words from the initial thesaurus, uses them for training a classifier deciding whether or not a pair of words are semantically similar and finally, applies this classifier to the neighbors of each entry for reranking them. One of the objectives of (Ferret,</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in automatic thesaurus discovery. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English. Longman,</booktitle>
<location>London.</location>
<contexts>
<context position="1906" citStr="Halliday and Hasan, 1976" startWordPosition="299" endWordPosition="302">of words, called semantic neighbors, that are supposed to be semantically linked to the entry. Generally, each neighbor is associated with a weight that characterizes the strength of its link with the entry and all the neighbors of an entry are sorted according to the decreasing order of their weight. The term semantic neighbor is very generic and can have two main interpretations according to the kind of semantic relations it is based on: one relies only on paradigmatic relations, such as hypernymy or synonymy, while the other considers syntagmatic relations, called collocation relations by (Halliday and Hasan, 1976) in the context of lexical cohesion or “non-classical relations” by (Morris and Hirst, 2004). The distinction between these two interpretations refers to the distinction between the notions of semantic similarity and semantic relatedness as it was done in (Budanitsky and Hirst, 2006) or in (Zesch and Gurevych, 2010) for instance. However, the limit between these two notions is sometimes hard to find in existing work as terms semantic similarity and semantic relatedness are often used interchangeably. Moreover, semantic similarity is frequently considered as included into semantic relatedness a</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Michael A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kris Heylen</author>
<author>Yves Peirsmany</author>
<author>Dirk Geeraerts</author>
<author>Dirk Speelman</author>
</authors>
<title>Modelling Word Similarity: An Evaluation of Automatic Synonymy Extraction Algorithms.</title>
<date>2008</date>
<booktitle>In Sixth conference on International Language Resources and Evaluation (LREC</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="4433" citStr="Heylen et al., 2008" startWordPosition="696" endWordPosition="699">nnual Meeting of the Association for Computational Linguistics, pages 561–571, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Markovitch, 2007). The last option is the corpusbased approach, based on the distributional hypothesis (Firth, 1957): each word is characterized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of context elements according to the semanti</context>
</contexts>
<marker>Heylen, Peirsmany, Geeraerts, Speelman, 2008</marker>
<rawString>Kris Heylen, Yves Peirsmany, Dirk Geeraerts, and Dirk Speelman. 2008. Modelling Word Similarity: An Evaluation of Automatic Synonymy Extraction Algorithms. In Sixth conference on International Language Resources and Evaluation (LREC 2008), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In 50th Annual Meeting of the Association for Computational Linguistics (ACL’12),</booktitle>
<pages>873--882</pages>
<contexts>
<context position="5432" citStr="Huang et al., 2012" startWordPosition="855" endWordPosition="858">rned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of context elements according to the semantic neighbors found by an initial distributional similarity measure. However, another part of these proposals implies more radical changes. The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pad´o and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al., 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al., 2010) can be classified into this second category. The work we present in this article takes place in the framework defined by (Grefenstette, 1994) for implementing the distributional approach but proposes a new method for improving a thesaurus built in this context based on the identification of its bad semantic neighbors rather than on the adaptation of the weight of their features. 2 Principles Our work shares with (Zhitomirsky-Geffet and Dagan, 2009) the use of a kind of bootstrapping as it starts f</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In 50th Annual Meeting of the Association for Computational Linguistics (ACL’12), pages 873–882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Stijn De Saeger</author>
<author>Kow Kuroda</author>
<author>Masaki Murata</author>
<author>Kentaro Torisawa</author>
</authors>
<title>A bayesian method for robust estimation of distributional similarities.</title>
<date>2010</date>
<booktitle>In 481h Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>247--256</pages>
<location>Uppsala,</location>
<marker>Kazama, De Saeger, Kuroda, Murata, Torisawa, 2010</marker>
<rawString>Jun’ichi Kazama, Stijn De Saeger, Kow Kuroda, Masaki Murata, and Kentaro Torisawa. 2010. A bayesian method for robust estimation of distributional similarities. In 481h Annual Meeting of the Association for Computational Linguistics, pages 247–256, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>41--48</pages>
<contexts>
<context position="15470" citStr="Lee and Ng, 2002" startWordPosition="2531" endWordPosition="2534">whether or not a word has the same meaning as E. This is a problem close to WSD as it is performed in the context of the pseudo-word disambiguation paradigm (Gale et al., 1992): a pseudo-word is created with two senses, E and notE, notE corresponding to one or several words that are supposed to be representative of a meaning different from the meaning of E. The objective is then to build a classifier for distinguishing the pseudo-senses E and notE. As a consequence of this view, we adopt the same kind of features as the ones used for WSD for building our classifier. More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word E: • neighboring words; • Part-of-Speech (POS) of neighboring words; • local collocations. Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus. For the neighboring words features, we consider all plain words (common and proper nouns, verbs and </context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In 171h International Conference on Computational Linguistics and 361h Annual Meeting of the Association for Computational Linguistics (ACL-COLING’98),</booktitle>
<pages>768--774</pages>
<location>Montral, Canada.</location>
<contexts>
<context position="4334" citStr="Lin, 1998" startWordPosition="681" endWordPosition="682">lso defined from Wikipedia or Wiktionaries (Gabrilovich and 561 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–571, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Markovitch, 2007). The last option is the corpusbased approach, based on the distributional hypothesis (Firth, 1957): each word is characterized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that pro</context>
<context position="11784" citStr="Lin, 1998" startWordPosition="1902" endWordPosition="1903">owing process to each of its entries: • building of a classifier for determining whether a word in a sentence corresponds or not to the entry; • selection of a set of examples sentences for each of the neighbors of the entry in the thesaurus; • application of the classifier to these sentences; • identification of bad neighbors according to the results of the classifier; • reranking of entry’s neighbors according to bad neighbors. 3.2 Building of the initial thesaurus Before introducing our method for improving distributional thesauri, we first present the way we build such a thesaurus. As in (Lin, 1998) or (Curran and Moens, 2002a), this building is based on the definition of a semantic similarity measure from a corpus. The corpus used for defining this measure was the AQUAINT-2 corpus, a middlesize corpus made of around 380 million words coming from news articles. Although our target language is English, we chose to limit deliberately the level of the tools applied for preprocessing texts to part-of-speech tagging and lemmatization to make possible the transposition of our method to a large set of languages. This seems to be a reasonable compromise between the approach of (Freitag et al., 2</context>
<context position="25749" citStr="Lin, 1998" startWordPosition="4320" endWordPosition="4321">rds: the best results are obtained for high frequency words while evaluation measures significantly decrease for words whose frequency is low. Second, the characteristics of the reference resources have a significant impact on results. WordNet provides a restricted number of synonyms for each noun while the Moby thesaurus contains for each entry a large number of synonyms and similar words. As a consequence, the precisions at different cut-offs have a significantly higher value with Moby as reference than with WordNet as reference. Finally, the results of Table 2 are compatible with those of (Lin, 1998) for instance (R-prec. = 11.6 and MAP = 8.1 with WM as reference for all entries of the thesaurus at http://webdocs.cs.ualberta. ca/lindek/Downloads/sim.tgz) if we take into account the fact that the thesaurus of Lin was built from a much larger corpus and with syntactic co-occurrences. 4.2 Implementation issues The implementation of the method we have presented in section 3 raises several issues. One of these concerns the occurrences to select from texts of both the entries of the thesaurus and their neighbors. These occurrences are used both for the training of our word-in-context classifier</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In 171h International Conference on Computational Linguistics and 361h Annual Meeting of the Association for Computational Linguistics (ACL-COLING’98), pages 768–774, Montral, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: An On-Line Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="22073" citStr="Miller, 1990" startWordPosition="3664" endWordPosition="3665">of neighbors. Among the downgraded neighbors, their initial order is left unchanged. It should be noted that the word-in-context classifier is not applied to the neighbors whose occurrences are used for its training as it would frequently lead to downgrade these neighbors, which is not necessarily optimum as we chose them with a rather low rank. 4 Experiments and evaluation 4.1 Initial thesaurus evaluation Table 2 shows the results of the evaluation of our initial thesaurus, achieved by comparing the selected semantic neighbors with two complementary reference resources: WordNet 3.0 synonyms (Miller, 1990) [W], which characterize a semantic similarity based on paradigmatic relations, and the Moby thesaurus (Ward, 1996) [M], which gathers a larger set of types of relations and is more representative of semantic relatedness3. The fourth column of Table 2, which gives the average number of synonyms and similar words in our references for the AQUAINT-2 nouns, also illustrates the difference of these two resources in terms of richness. A fusion of the two resources is also considered [WM]. As our objective is to evaluate the extracted semantic neighbors and not the ability to rebuild the reference r</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George A. Miller. 1990. WordNet: An On-Line Lexical Database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Nonclassical lexical semantic relations.</title>
<date>2004</date>
<booktitle>In Workshop on Computational Lexical Semantics of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>46--51</pages>
<location>Boston, MA.</location>
<contexts>
<context position="1998" citStr="Morris and Hirst, 2004" startWordPosition="313" endWordPosition="316"> Generally, each neighbor is associated with a weight that characterizes the strength of its link with the entry and all the neighbors of an entry are sorted according to the decreasing order of their weight. The term semantic neighbor is very generic and can have two main interpretations according to the kind of semantic relations it is based on: one relies only on paradigmatic relations, such as hypernymy or synonymy, while the other considers syntagmatic relations, called collocation relations by (Halliday and Hasan, 1976) in the context of lexical cohesion or “non-classical relations” by (Morris and Hirst, 2004). The distinction between these two interpretations refers to the distinction between the notions of semantic similarity and semantic relatedness as it was done in (Budanitsky and Hirst, 2006) or in (Zesch and Gurevych, 2010) for instance. However, the limit between these two notions is sometimes hard to find in existing work as terms semantic similarity and semantic relatedness are often used interchangeably. Moreover, semantic similarity is frequently considered as included into semantic relatedness and the two problems are often tackled by using the same methods. In the remainder of this ar</context>
</contexts>
<marker>Morris, Hirst, 2004</marker>
<rawString>Jane Morris and Graeme Hirst. 2004. Nonclassical lexical semantic relations. In Workshop on Computational Lexical Semantics of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 46–51, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity - measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004, demonstration papers,</booktitle>
<pages>38--41</pages>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="3304" citStr="Pedersen et al., 2004" startWordPosition="520" endWordPosition="523">semantic relatedness for referring more specifically to similarity based on syntagmatic relations. Following work such as (Grefenstette, 1994), a widespread way to build a thesaurus from a corpus is to use a semantic similarity measure for extracting the semantic neighbors of the entries of the thesaurus. Three main ways of implementing such measures can be distinguished. The first one relies on handcrafted resources in which semantic relations are clearly identified. Work based on WordNet-like lexical networks for building semantic similarity measures such as (Budanitsky and Hirst, 2006) or (Pedersen et al., 2004) falls into this category. These measures typically exploit the hierarchical structure of these networks, based on hypernymy relations. The second approach makes use of a less structured source of knowledge about words such as the definitions of classical dictionaries or the glosses of WordNet. WordNet’s glosses were used to support Lesklike measures in (Banerjee and Pedersen, 2003) and more recently, measures were also defined from Wikipedia or Wiktionaries (Gabrilovich and 561 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–571, Sofia, Bulga</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity - measuring the relatedness of concepts. In HLT-NAACL 2004, demonstration papers, pages 38–41, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2010),</booktitle>
<pages>109--117</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="5334" citStr="Reisinger and Mooney, 2010" startWordPosition="839" endWordPosition="842">art of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of context elements according to the semantic neighbors found by an initial distributional similarity measure. However, another part of these proposals implies more radical changes. The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pad´o and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al., 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al., 2010) can be classified into this second category. The work we present in this article takes place in the framework defined by (Grefenstette, 1994) for implementing the distributional approach but proposes a new method for improving a thesaurus built in this context based on the identification of its bad semantic neighbors rather than on the adaptation of the weight of their features. 2 Principles Our work </context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010. Multi-prototype vector-space models of word meaning. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2010), pages 109–117, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing. Grady Ward.</booktitle>
<contexts>
<context position="12563" citStr="Schmid, 1994" startWordPosition="2036" endWordPosition="2037">the AQUAINT-2 corpus, a middlesize corpus made of around 380 million words coming from news articles. Although our target language is English, we chose to limit deliberately the level of the tools applied for preprocessing texts to part-of-speech tagging and lemmatization to make possible the transposition of our method to a large set of languages. This seems to be a reasonable compromise between the approach of (Freitag et al., 2005), in which none normalization of words is done, and the more widespread use of syntactic parsers in work such as (Lin, 1998). More precisely, we used TreeTagger (Schmid, 1994) for performing the linguistic preprocessing of the AQUAINT-2 corpus. For the extraction of distributional data and the characteristics of the distributional similarity measure, we adopted the options of (Ferret, 2010), resulting from a kind of grid search procedure performed with the extended TOEFL test proposed in (Freitag et al., 2005) as an optimization objective. More precisely, the following characteristics were taken: • distributional contexts made of the cooccurrents collected in a 3 word window centered on each occurrence in the corpus of the target word. These co-occurrents were rest</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In International Conference on New Methods in Language Processing. Grady Ward. 1996. Moby thesaurus. Moby Project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
</authors>
<title>Measures and Applications of Lexical Distributional Similarity.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Informatics, University of Sussex.</institution>
<contexts>
<context position="4408" citStr="Weeds, 2003" startWordPosition="693" endWordPosition="694">ngs of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–571, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Markovitch, 2007). The last option is the corpusbased approach, based on the distributional hypothesis (Firth, 1957): each word is characterized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of context elements</context>
</contexts>
<marker>Weeds, 2003</marker>
<rawString>Julie Weeds. 2003. Measures and Applications of Lexical Distributional Similarity. Ph.D. thesis, Department of Informatics, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuhide Yamamoto</author>
<author>Takeshi Asakura</author>
</authors>
<title>Even unassociated features can improve lexical distributional similarity.</title>
<date>2010</date>
<booktitle>In Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010),</booktitle>
<pages>32--39</pages>
<location>Beijing, China.</location>
<contexts>
<context position="4924" citStr="Yamamoto and Asakura, 2010" startWordPosition="779" endWordPosition="782"> (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of context elements according to the semantic neighbors found by an initial distributional similarity measure. However, another part of these proposals implies more radical changes. The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pad´o and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al., 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al., </context>
<context position="34875" citStr="Yamamoto and Asakura, 2010" startWordPosition="5888" endWordPosition="5892">saurus is generally not directly addressed but is a possible consequence of the improvement of semantic similarity measures. However, the extent of this improvement is rarely evaluated as most of the work about semantic similarity is evaluated on datasets such as the WordSim-353 test collection (Gabrilovich and Markovitch, 2007), which are only partially representative of the results for thesaurus building. If we consider more specifically the problem of improving semantic similarity, and by the way thesauri, in a given paradigm, (Broda et al., 2009), (Zhitomirsky-Geffet and Dagan, 2009) and (Yamamoto and Asakura, 2010), which all take place in the paradigm defined by (Grefenstette, 1994), are the closest works to ours. (Broda et al., 2009) proposes a new weighting scheme of words in distributional contexts that replaces the weight of 568 word by a function of its rank in the context, which is a way to be less dependent on the values of a particular weighting function. (Zhitomirsky-Geffet and Dagan, 2009) shares with our work the use of bootstrapping by relying on an initial thesaurus to derive means of improving it. More specifically, (Zhitomirsky-Geffet and Dagan, 2009) assumes that the first neighbors of </context>
</contexts>
<marker>Yamamoto, Asakura, 2010</marker>
<rawString>Kazuhide Yamamoto and Takeshi Asakura. 2010. Even unassociated features can improve lexical distributional similarity. In Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 32–39, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Wisdom of crowds versus wisdom of linguists - measuring the semantic relatdness of words.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="2223" citStr="Zesch and Gurevych, 2010" startWordPosition="348" endWordPosition="351">tic neighbor is very generic and can have two main interpretations according to the kind of semantic relations it is based on: one relies only on paradigmatic relations, such as hypernymy or synonymy, while the other considers syntagmatic relations, called collocation relations by (Halliday and Hasan, 1976) in the context of lexical cohesion or “non-classical relations” by (Morris and Hirst, 2004). The distinction between these two interpretations refers to the distinction between the notions of semantic similarity and semantic relatedness as it was done in (Budanitsky and Hirst, 2006) or in (Zesch and Gurevych, 2010) for instance. However, the limit between these two notions is sometimes hard to find in existing work as terms semantic similarity and semantic relatedness are often used interchangeably. Moreover, semantic similarity is frequently considered as included into semantic relatedness and the two problems are often tackled by using the same methods. In the remainder of this article, we will use the term semantic similarity with its generic sense and the term semantic relatedness for referring more specifically to similarity based on syntagmatic relations. Following work such as (Grefenstette, 1994</context>
</contexts>
<marker>Zesch, Gurevych, 2010</marker>
<rawString>Torsten Zesch and Iryna Gurevych. 2010. Wisdom of crowds versus wisdom of linguists - measuring the semantic relatdness of words. Natural Language Engineering, 16(1):25–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Zhitomirsky-Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>Bootstrapping Distributional Feature Vector Quality.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="4869" citStr="Zhitomirsky-Geffet and Dagan, 2009" startWordPosition="770" endWordPosition="774"> the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of context elements according to the semantic neighbors found by an initial distributional similarity measure. However, another part of these proposals implies more radical changes. The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pad´o and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al., 2012) or the redefinition of the distribut</context>
<context position="34842" citStr="Zhitomirsky-Geffet and Dagan, 2009" startWordPosition="5883" endWordPosition="5886"> consequence, the improvement of such thesaurus is generally not directly addressed but is a possible consequence of the improvement of semantic similarity measures. However, the extent of this improvement is rarely evaluated as most of the work about semantic similarity is evaluated on datasets such as the WordSim-353 test collection (Gabrilovich and Markovitch, 2007), which are only partially representative of the results for thesaurus building. If we consider more specifically the problem of improving semantic similarity, and by the way thesauri, in a given paradigm, (Broda et al., 2009), (Zhitomirsky-Geffet and Dagan, 2009) and (Yamamoto and Asakura, 2010), which all take place in the paradigm defined by (Grefenstette, 1994), are the closest works to ours. (Broda et al., 2009) proposes a new weighting scheme of words in distributional contexts that replaces the weight of 568 word by a function of its rank in the context, which is a way to be less dependent on the values of a particular weighting function. (Zhitomirsky-Geffet and Dagan, 2009) shares with our work the use of bootstrapping by relying on an initial thesaurus to derive means of improving it. More specifically, (Zhitomirsky-Geffet and Dagan, 2009) ass</context>
</contexts>
<marker>Zhitomirsky-Geffet, Dagan, 2009</marker>
<rawString>Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Bootstrapping Distributional Feature Vector Quality. Computational Linguistics, 35(3):435–461.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>