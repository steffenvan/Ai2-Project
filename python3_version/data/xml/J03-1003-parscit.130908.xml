<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998889">
Graph-Based Generation of Referring
Expressions
</title>
<author confidence="0.996889">
Emiel Krahmer∗ Sebastiaan van Erk†
</author>
<affiliation confidence="0.999377">
Tilburg University Eindhoven University of Technology
</affiliation>
<author confidence="0.985722">
Andr´e Verleg‡
</author>
<affiliation confidence="0.975049">
Eindhoven University of Technology
</affiliation>
<bodyText confidence="0.989025583333333">
This article describes a new approach to the generation of referring expressions. We propose to
formalize a scene (consisting of a set of objects with various properties and relations) as a labeled
directed graph and describe content selection (which properties to include in a referring expression)
as a subgraph construction problem. Cost functions are used to guide the search process and to
give preference to some solutions over others. The current approach has four main advantages:
(1) Graph structures have been studied extensively, and by moving to a graph perspective we
get direct access to the many theories and algorithms for dealing with graphs; (2) many existing
generation algorithms can be reformulated in terms of graphs, and this enhances comparison and
integration of the various approaches; (3) the graph perspective allows us to solve a number of
problems that have plagued earlier algorithms for the generation of referring expressions; and
(4) the combined use of graphs and cost functions paves the way for an integration of rule-based
generation techniques with more recent stochastic approaches.
</bodyText>
<sectionHeader confidence="0.997657" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999899875">
The generation of referring expressions is one of the most common tasks in natural
language generation and has been addressed by many researchers in the past two
decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992),
Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune
(1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present
a general, graph-theoretic approach to the generation of referring expressions. We pro-
pose to formalize a scene (i.e., a domain of objects and their properties and relations)
as a labeled directed graph and describe the content selection problem (which proper-
ties and relations to include in a description for an object?) as a subgraph construction
problem. The graph perspective has four main advantages. (1) There are many at-
tractive and well-understood algorithms for dealing with graph structures (see, e.g.,
Gibbons [1985], Cormen, Leiserson, and Rivest [1990], or Chartrand and Oellermann
[1993]). In this article, we describe a straightforward branch and bound algorithm for
finding the relevant subgraphs in which cost functions are used to guide the search
process. (2) By defining different cost functions for the graph perspective, we can simu-
late (and improve) some of the well-known algorithms for the generation of referring
</bodyText>
<footnote confidence="0.6838254">
∗ Communication and Cognition/Computational Linguistics, Faculty of Arts, Tilburg University, Tilburg,
The Netherlands. E-mail: E.J.Krahmer@uvt.nl.
† Tijgerstraat 2, NL-5645 CK, Eindhoven, The Netherlands. E-mail: sebster@sebster.com.
‡ Ranonkelstraat 67, NL-5644 LB, Eindhoven, The Netherlands. E-mail: andre@astygian.nl.
© 2003 Association for Computational Linguistics
</footnote>
<note confidence="0.828863">
Computational Linguistics Volume 29, Number 1
</note>
<bodyText confidence="0.999885277777778">
expressions mentioned above. This facilitates the formal comparison of these algo-
rithms and makes it easier to transfer results from one algorithm to another. (3) The
graph perspective provides a clean solution for some problems that have plagued ear-
lier algorithms. For instance, the generation of relational expressions (i.e., referring
expressions that include references to other objects) is enhanced by the fact that both
properties and relations are formalized in the same way, namely, as edges in a graph.
(4) The combined use of graphs and cost functions paves the way for a natural inte-
gration of traditional rule-based approaches to generating referring expressions and
more recent statistical approaches, such as Langkilde and Knight (1998) and Malouf
(2000), in a single algorithm.
The outline of this article is as follows. In Section 2 the content selection problem
for generating referring expressions is explained, and some well-known solutions to
the problem are discussed. In Section 3, we describe how scenes can be modeled
as labeled directed graphs and show how content selection can be formalized as a
subgraph construction problem. Section 4 contains a sketch of the basic generation
algorithm, which is illustrated with a worked example. In Section 5 various ways to
formalize cost functions are discussed and compared. We end with some concluding
remarks and a discussion of future research directions in Section 6.
</bodyText>
<sectionHeader confidence="0.982742" genericHeader="method">
2. Generating Referring Expressions
</sectionHeader>
<bodyText confidence="0.936228862068965">
There are many different algorithms for the generation of referring expressions, each
with its own objectives: Some aim at producing the shortest possible description (e.g.,
Dale’s [1992] full brevity algorithm), others focus on psychological realism (e.g., Dale
and Reiter’s [1995] incremental algorithm) or realistic output (e.g., Horacek 1997). The
degree of detail in which the various algorithms are described differs considerably.
Some algorithms are fully formalized and come with explicit characterizations of their
complexity (e.g., Dale and Reiter 1995; van Deemter 2000); others are more conceptual
and concentrate on exploring new directions (e.g., Stone and Webber 1998). Despite
such differences, most algorithms deal with the same problem definition. They take
as input a single object v (the target object) for which a referring expression is to be
generated and a set of objects (the distractors) from which the target object needs to
be distinguished (we use the terminology from Dale and Reiter [1995]). The task of
the algorithm is to determine which set of properties is needed to single out the target
object v from the distractors. This is known as the content determination problem for
referring expressions. On the basis of this set of properties a distinguishing descrip-
tion for v can be generated. Most algorithms do not address the surface realization
problem (how the selected properties should be realized in natural language) in much
detail; it is usually assumed that once the content for a referring expression has been
determined, a standard realizer such as KPML (Bateman 1997) or SURGE (Elhaded and
Robin 1997) can convert the meaning representation to natural language.
Consider the example scene in Figure 1. In this scene, as in any other scene, we
see a finite domain of entities D with properties P. In this particular scene, D =
{d1,d2,d3,d4} is the set of entities and P = { dog, cat, brown, black+white, large, small }
is the set of properties. A scene is usually represented as a database (or knowledge
base) listing the properties of each element in D. Thus:
d1: dog (d1) small (d1) brown (d1)
d2: dog (d2) large (d2) brown (d2)
d3: dog (d3) large (d3) black+white (d3)
d4: cat (d4) small (d4) brown (d4)
</bodyText>
<page confidence="0.996159">
54
</page>
<note confidence="0.594821">
Krahmer, van Erk, and Verleg Graph-Based Generation
d1 d2 d3 d4
</note>
<figureCaption confidence="0.979265">
Figure 1
</figureCaption>
<bodyText confidence="0.998971894736842">
A simple example scene consisting of some domestic animals.
In what is probably the key reference on the topic, Dale and Reiter (1995) describe
and discuss a number of algorithms for the generation of referring expressions. One
of these is the full brevity algorithm (originally due to Dale 1992). This algorithm first
tries to generate a distinguishing description for the target object v using one single
property. If this fails, it considers all possible combinations of two properties to see if
any of these suffices for the generation of a distinguishing description, and so on. It
is readily seen that this algorithm will output the shortest possible description, if one
exists. Suppose the full brevity algorithm is used to generate a description for d1 in
Figure 1. There is no single property that distinguishes the target object d1 from the
distractors {d2, d3, d4}. But when considering all pairs of properties the algorithm will
find that one such pair rules out all distractors, namely, small and dog; “the small dog”
is a successful and minimal distinguishing description for d1.
Dale and Reiter point out that the full brevity algorithm is both computationally
infeasible (NP hard) and psychologically unrealistic. They offer the incremental algo-
rithm as an alternative. The incremental algorithm considers properties for selection
in a predetermined order, based on the idea that human speakers and listeners prefer
certain kinds of properties (or attributes) when describing objects from a given do-
main. For instance, when discussing domestic animals, it seems likely that a human
speaker would first describe an animal by its type (is it a dog? is it a cat?). If that
does not suffice, first absolute attributes like color are tried, followed by relative ones
such as size. In sum: The list of preferred attributes for our example domain would
be ( type, color, size ). Essentially, the incremental algorithm iterates through this list,
and for each property it encounters, it determines whether adding this property to
the properties selected so far would rule out any of the remaining distractors. If so,
it is included in the list of selected properties. There is one exception to this general
strategy: Type information is always included, even if it rules out no distractors. The
algorithm stops when all distractors are ruled out (success) or when the end of the
list of preferred attributes is reached (failure).
Suppose we apply the incremental algorithm to d1 from Figure 1 with ( type, color,
size ) as preferred attributes. The type of d1 listed in the database is dog. This property
is selected (since type information is always selected). It rules out d4 (which is a cat).
Next we consider the color of d1; the animal is brown. This property rules out d3 (which
is a black and white dog) and is selected. Finally, we consider the size of our target ob-
ject, which is small. This properly rules out the remaining distractor d2 (which is a large
brown dog) and hence is included as well. At this point, all distractors are ruled out
(success!), and the set of selected properties is {dog, brown, small}, which a linguistic
realizer might express as “the small brown dog.” This is a successful distinguishing
</bodyText>
<page confidence="0.991429">
55
</page>
<figure confidence="0.914726">
Computational Linguistics Volume 29, Number 1
d3
� 1 � 1
d4
d1 d2
</figure>
<figureCaption confidence="0.960131">
Figure 2
</figureCaption>
<subsectionHeader confidence="0.462564">
Another scene: Two dogs and two doghouses (from Krahmer and Theune [2002]).
</subsectionHeader>
<bodyText confidence="0.999967864864865">
description but not a minimal one: The property brown is, strictly speaking, made re-
dundant by the later inclusion of the property small. Since there is no backtracking
in the incremental algorithm however, every selected property is realized (hence “in-
cremental”). This aspect is largely responsible for the computational efficiency of the
algorithm (it has a polynomial complexity), but Dale and Reiter (1995, page 248) also
claim that it is “psychologically realistic.” They point out that sometimes people may
describe an object as “the white bird” even though the simpler “the bird” would have
been sufficient (cf. Pechmann [1989]; see, however, Krahmer and Theune [2002] for
discussion).
Even though there are various useful and interesting algorithms for the genera-
tion of referring expressions, a number of open questions remain. Recently there has
been an increased interest in statistical approaches to natural language generation.
For example, Malouf (2000) has shown that large corpora can be used to determine
the order of realization of sequences of prenominal adjectives. It is unclear how such
statistical work on generation can be combined with older, rule-based work such as
the algorithms just discussed. In addition, many algorithms still have difficulties with
the generation of relational descriptions (descriptions that include references to other
objects to single out the target object from its distractors). To illustrate the problem,
consider the scene depicted in Figure 2. In this scene we again see a finite domain
of entities D with certain properties P. Here, D = {d1, d2, d3, d4} is the set of entities,
and P = { dog, doghouse, small, large, brown, white } is the set of properties. Clearly
no algorithm can generate a distinguishing description referring to d1 on this basis.
Intuitively, d1 can be distinguished from d2 only using its relation to the doghouse d3.
To facilitate this we extend the scene description with a set of relations R = { left of,
right of, contain, in }.
A few algorithms have been developed that address the issue of relational descrip-
tions. The earliest is from Dale and Haddock (1992), who offer an extension of the full
brevity algorithm. The Dale and Haddock algorithm has a problem with infinite re-
cursions; it may produce descriptions like “the dog in the doghouse that contains a
dog that is inside a doghouse....” Dale and Haddock, somewhat ad hoc, solve this
problem by stipulating that a property or relation may be used only once. Krahmer
and Theune (2002) (see also Theune [2000]) describe an extension of the incremental
algorithm that allows for relational descriptions. Their extension suffers from what
may be called the problem of forced incrementality: When a first relation fails to rule
out all remaining distractors, additional relations will be tried incrementally. Although
it could be argued that incremental selection of properties is psychologically plausi-
ble, it seems less plausible for relations. It is unlikely that someone would describe an
</bodyText>
<page confidence="0.995092">
56
</page>
<note confidence="0.740879">
Krahmer, van Erk, and Verleg Graph-Based Generation
</note>
<figureCaption confidence="0.830943">
Figure 3
</figureCaption>
<bodyText confidence="0.93681775">
A graph representation of the scene in Figure 2.
object as “the dog next to the tree in front of the garage” in a situation in which “the
dog in front of the garage” would suffice. As we shall argue, the graph perspective
provides a clean solution for these problems.
</bodyText>
<sectionHeader confidence="0.860093" genericHeader="method">
3. Generating Referring Expressions Using Graphs
</sectionHeader>
<bodyText confidence="0.999595722222222">
In the previous section we saw that a scene can be described in terms of a domain
of entities D with properties P and relations R. Such a scene can be represented as
a labeled directed graph (see, e.g., Wilson [1996] for a gentle introduction or Berge
[1985] for a more specialized one). Let L = P U R be the set of labels with P and R
disjoint (i.e., P fl R = 0). Then G = (VG, EG) is a labeled directed graph, where VG C D
is the set of vertices (or nodes) and EG C VG x L x VG is the set of labeled directed
edges (or arcs). Where this can be done without creating confusion, the graph subscript
is omitted. Throughout this article we use the following notations. If G = (V, E) is a
graph and e = (v, l, w) an edge (with l E L), then the extension of G with e, denoted
as G + e, is the graph (V U {v, w}, E U {e}). Moreover, with EG(v, w) we refer to the set
of edges in EG from v to w; that is, EG(v, w) = {e E EG  |e = (v, l, w), for l E L}.
The scene given in Figure 2, for example, can now be represented by the graph
in Figure 3. This graph models the respective spatial relations between the two chi-
huahuas, between the two doghouses, and between each dog and the nearest dog-
house. For the sake of transparency we have not modeled the relations between the
dogs and the distant doghouses (i.e., between d1 and d4 and between d2 and d3). (It is
worth stressing that adding these edges would not result in different outcomes in the
discussion below). Note that properties (such as dog) are always modeled as loops,
</bodyText>
<figure confidence="0.991307789473684">
doghouse
doghouse
chihuahua
chihuahua
dog
brown
small
next-to
left-of
small
dog
brown
d1���� 911
d2
right-of
next-to
left-of
contains in
next-to
next-to
right-of
next-to
left-of
d3 0 0
d4
large
right-of
large
white
white
next-to
57
Computational Linguistics Volume 29, Number 1
next_to
chihuahua
dog doghouse
in
(iii)
</figure>
<figureCaption confidence="0.990738">
Figure 4
</figureCaption>
<bodyText confidence="0.993052814814815">
Some graphs for referring expressions, with circles around the intended referent.
that is, as edges that start and end in the same vertex. Relations may have different
start and end vertices, but they do not have to (consider potentially reflexive relations
such as shave). Finally, note that the graph sometimes contains properties of various
levels of specificity (e.g., chihuahua and dog). This aspect of scene graphs will be further
discussed in Section 5.
Now the content determination problem for referring expressions can be formu-
lated as a graph construction problem. To decide which information to include in a
referring expression for an object v E V, we construct a connected directed labeled
graph over the set of labels L and an arbitrary set of vertices, but including v. A graph
is connected iff there is a path (a list of vertices in which each vertex has an edge
from itself to the next vertex) between each pair of vertices. Informally, we say that a
vertex (“the intended referent”) from a graph H refers to a given entity in the scene
graph G iff the graph H can be “placed over” the scene graph G in such a way that the
vertex being referred to is “placed over” the vertex of the given entity in G and each
edge from H with label l can be “placed over” an edge from G with the same label.
Furthermore, a vertex-graph pair is distinguishing iff it refers to exactly one vertex in
the scene graph.
Consider the three vertex-graph pairs in Figure 4, in which circled vertices stand
for the intended referent. Graph (i) refers to all vertices of the graph in Figure 3
(every object in the scene is next to some other object), graph (ii) can refer to both d1
and d2, and graph (iii) is distinguishing in that it can refer only to d1. Note that the
graphs might be realized as something next to something else, a chihuahua, and the dog
in the doghouse, respectively. Here we concentrate on the generation of distinguishing
vertex-graph pairs.
Formally, the notion that a graph H = (VH, EH) can be “placed over” another
graph G = ft, EG) corresponds to the notion of a subgraph isomorphism (see, e.g.,
</bodyText>
<page confidence="0.982942">
58
</page>
<figure confidence="0.5058285">
Krahmer, van Erk, and Verleg Graph-Based Generation
in
</figure>
<figureCaption confidence="0.818558">
Figure 5
</figureCaption>
<bodyText confidence="0.99580384">
Three distinguishing vertex-graph pairs referring to d1 in Figure 3.
Read and Corneil [1977] for an overview). H can be “placed over” G iff there exists a
subgraph G&apos; = (VG,, EG,) of G such that H is isomorphic to G&apos;. H is isomorphic to G&apos; iff
there exists a bijection 7r: VH → VG, such that for all vertices v,w E VH and all l E L:
(v, l, w) E EH �* (7r.v,l, 7r.w) E EG
In words: The bijective function 7r maps all the vertices in H to corresponding vertices
in G&apos; in such a way that any edge with label l between vertices v and w in H is matched
with an edge with the same label between the G&apos; counterparts of v and w (i.e., 7r.v and
7r.w, respectively). When H is isomorphic to some subgraph of G by an isomorphism
7r, we write H C,r G.
Given a graph H and a vertex v in H, and a graph G and a vertex w in G, we
define that the pair (v, H) refers to the pair (w, G) iff H is connected and H C,r G and
7r.v = w. Furthermore, (v, H) uniquely refers to (w, G) (i.e., (v, H) is distinguishing)
iff (v, H) refers to (w, G) and there is no vertex w&apos; in G different from w such that
(v, H) refers to (w&apos;, G). The problem considered in this article can now be formalized
as follows: Given a graph G and a vertex w in G, find a pair (v, H) such that (v, H)
uniquely refers to (w, G).
Consider, for instance, the task of finding a pair (v,H) that uniquely refers to the
vertex labeled d1 in Figure 3. It is easily seen that there are a number of such pairs,
three of which are depicted in Figure 5. We would like to have a mechanism that
allows us to give certain solutions to this kind of task preference over other solutions.
For this purpose we shall use cost functions. In general, a cost function is a function
that assigns to each subgraph of a scene graph a non-negative number. As we shall
see, by defining cost functions in different ways, we can mimic various algorithms for
the generation of referring expressions known from the literature.
</bodyText>
<figure confidence="0.934847909090909">
dog
doghouse
dog doghouse
in
brown
small
in
large
white
59
Computational Linguistics Volume 29, Number 1
</figure>
<subsectionHeader confidence="0.996384">
3.1 A Note on the Problem Complexity
</subsectionHeader>
<bodyText confidence="0.99993804">
The basic decision problem for subgraph isomorphism (i.e., testing whether a graph
H is isomorphic to a subgraph of G) is known to be NP-complete (see, e.g., Garey
and Johnson [1979]). Here we are interested in connected H, but unfortunately that
restriction does not reduce the theoretical complexity. Note that this characterization
of the worst-case complexity holds for graphs in which all edges have the same label; in
that case each edge from H can potentially be matched to any edge from G. The best-
case complexity is given when each edge is uniquely labeled. In practice, the situation
will most often be somewhere between these extremes. In general, we can say that the
more diverse the labeling of edges in the graph of a particular scene is, the sooner a
distinguishing vertex-graph pair will be found.
It is worth pointing out that there are various alternatives to full subgraph iso-
morphism that have a lower complexity. For instance, as soon as an upper bound
K is defined on the number of edges in a distinguishing graph, the problem loses
its intractability (for relatively small K) and becomes solvable, in the worst case, in
polynomial O(nK) time, where n is number of edges in the graph G. Restricting the
problem in such a way is rather harmless for our current purposes, as it prohibits the
generation only of distinguishing descriptions with more than K properties, and for
all practical purposes K can be small (referring expressions usually express a limited
number of properties).
Defining an upper bound K, however, does have a disadvantage: We lose com-
pleteness (see van Deemter [2002]). In particular, the algorithm will fail for objects
that can be uniquely described only with K + 1 (or more) edges. Of course, one could
argue that in such cases objects should be distinguished using other means (e.g., by
pointing). Nevertheless, it is worthwhile to look for classes of graphs for which the
subgraph isomorphism problem can be solved more efficiently, without postulating
upper bounds. For instance, if G and H are planar (simple) graphs the problem can
be solved in time linear in the number of vertices of G (Eppstein 1999). Basically, a
planar graph is one that can be drawn on a plane in such a way that there are no
crossing edges (thus, for instance, the graph in Figure 3 is planar, as is any graph
with only four vertices). In general, there is no a priori reason to assume that our
scene representations will be planar. Yet every nonplanar graph can be modified into
a closely related planar one. We briefly address planarization of scene graphs in the
Appendix.
A final alternative is worth mentioning. The general approach to the problem of
subgraph isomorphism detection assumes that both graphs are given on-line. For our
current purposes, however, it may happen that the scene graph is fixed and known
beforehand, and only the referring graph is unknown and given on-line. Messmer
and Bunke (1995, 1998) describe a method that converts the known graph (or model
graph, as they call it) into a decision tree. At run time, the input graph is classified by
the decision tree, which detects subgraph isomorphisms. The disadvantage of this ap-
proach is that the decision tree may contain, in the worst case, an exponential number
of nodes. But the main advantage is that the complexity of the new subgraph iso-
morphism algorithm is only quadratic in the number of vertices of the input referring
graph. Note that with this approach we do not lose information from the scene graph,
nor do we lose completeness.
In sum, the basic approach to subgraph isomorphisms is NP-complete, but there
exist various reformulations of the problem that can be solved more efficiently. Decid-
ing which (combination) of these is the most suitable in practice, however, is beyond
the scope of this article. Finally, it is worth stressing that the NP-completeness is due
to the presence of edges representing relations between different vertices. If we re-
</bodyText>
<page confidence="0.968718">
60
</page>
<note confidence="0.504164">
Krahmer, van Erk, and Verleg Graph-Based Generation
</note>
<bodyText confidence="0.985693">
strict the approach to properties (looping edges), testing for subgraph isomorphisms
becomes trivial.
</bodyText>
<sectionHeader confidence="0.70711" genericHeader="method">
4. A Sketch of a Branch and Bound Generation Algorithm
</sectionHeader>
<bodyText confidence="0.9995024">
In this section we give a high-level sketch of the graph-based generation algorithm.
The algorithm (called makeReferringExpression) consists of two main components,
a subgraph construction algorithm (called findGraph) and a subgraph isomorphism
testing algorithm (called matchGraphs). For expository reasons we do not address
optimization strategies (but see Section 6).
</bodyText>
<subsectionHeader confidence="0.999426">
4.1 The Basic Idea
</subsectionHeader>
<bodyText confidence="0.999994166666667">
We assume that a scene graph G = (VG, EG) is given. The algorithm systematically
tries all relevant subgraphs H of the scene graph G by starting with the subgraph
containing only the vertex v (the target object) and expanding it recursively by trying
to add edges from G that are adjacent to the subgraph H constructed up to that point.
In this way we know that the results will be a connected subgraph. We refer to this set
of adjacent edges as the H neighbors in G (denoted as G.neighbors(H)). Formally:
</bodyText>
<equation confidence="0.979648">
U U EG(v, w)
G.neighbors(H) = wEVG
vEVH
</equation>
<bodyText confidence="0.998797333333333">
The algorithm returns the cheapest (least expensive) distinguishing subgraph H that
refers to v, if such a distinguishing graph exists; otherwise it returns the undefined
null graph -L.
</bodyText>
<subsectionHeader confidence="0.997172">
4.2 Cost Functions
</subsectionHeader>
<bodyText confidence="0.9996095">
We use cost functions to guide the search process and to give preference to some
solutions over others. If H = (VH, EH) is a subgraph of G, then the costs of H, denoted
as cost(H), can be given by summing over the costs associated with the vertices and
edges of H. Formally:
</bodyText>
<equation confidence="0.9964365">
cost(H) = E Ecost(v) + cost(e)
vEVH eEEH
</equation>
<bodyText confidence="0.980103">
In fact, this is only one possible way to define a cost function. The only hard require-
ment cost functions have to fulfill is monotonicity. That is, adding an edge e to a graph
G should never result in a graph cheaper than G. Formally:
dG&apos; C G de E EG: cost(G&apos;) &lt; cost(G&apos; + e)
The monotonicity assumption helps reduce the search space, since extensions of sub-
graphs with a cost greater than the best subgraph found up to that point can safely
be ignored. Naturally, the costs of the undefined graph (cost(-L)) are not defined. It is
worth stressing that the cost function is global: It determines the costs of entire graphs.
This implies that the cheapest distinguishing graph is not necessarily the smallest dis-
tinguishing graph; a graph consisting of two or more edges may be cheaper than a
graph containing one expensive edge.
</bodyText>
<subsectionHeader confidence="0.995694">
4.3 Worked Example
</subsectionHeader>
<bodyText confidence="0.999947">
We now illustrate the algorithm with an example. Suppose the scene graph G is as
given in Figure 3 and that we want to generate a referring expression for object d1
</bodyText>
<page confidence="0.990925">
61
</page>
<note confidence="0.573948">
Computational Linguistics Volume 29, Number 1
</note>
<equation confidence="0.980980789473684">
makeReferringExpression(v) {
bestGraph := 1;
H := ({v},0);
return findGraph(v, bestGraph, H);
}
findGraph(v, bestGraph, H) {
if [bestGraph =� 1 and cost(bestGraph) &lt; cost(H)]
then return bestGraph
fi;
distractors := { n  |n E VG n matchGraphs(v, H, n, G) n n =� v};
if distractors = 0 then return H fi;
for each edge e E G.neighbors(H) do
I := findGraph(v, bestGraph, H + e);
if [bestGraph = 1 or cost(I) &lt; cost(bestGraph)]
then bestGraph := I
fi;
rof;
return bestGraph;
}
</equation>
<figureCaption confidence="0.822036">
Figure 6
</figureCaption>
<bodyText confidence="0.998381">
Sketch of the main function (makeReferringExpression) and the subgraph construction
function (findGraph).
in this graph. Let us assume for the sake of illustration that the cost function is de-
fined in such a way that adding a vertex or an edge always costs one point. Thus,
for each v E VH and for each e E EH: cost(v) = cost(e) = 1. (In the next section we
describe a number of more interesting cost functions and discuss the impact these
have on the output of the algorithm.) We call the function makeReferringExpression
(given in Figure 6) with d1 as parameter. In this function the variable bestGraph (for
the best solution found up to that point) is initialized as the null graph (there is no
best solution yet), and the variable H (for the distinguishing subgraph under construc-
tion) is initialized as the graph containing only vertex d1 (i.e., (i) in Figure 7). Then
the function findGraph (see also Figure 6) is called, with parameters d1, bestGraph,
and H.
To begin with, whether a first non-null bestGraph has been found is checked and,
if one has, whether the costs of H (the graph under construction) are higher than
the costs of the bestGraph found up to that point. If the costs of H are higher, it is
not worth extending H since, due to the monotonicity constraint, it will never end
up being cheaper than the current bestGraph. During the first iteration we have no
non-null bestGraph, so we continue. Next the set of distractors is calculated. In terms
of the graph perspective, this is the set of vertices in the scene graph G (other than
the target vertex v) to which the graph H refers. It is easily seen that the initial value
of H refers to every vertex in G. Hence, as one would expect, the initial set of distrac-
tors is VG\{d1} = {d2,d3,d4}. Then the current set of distractors is checked to deter-
mine whether it is empty. If it is, we have managed to find a distinguishing graph,
which is subsequently stored in the variable bestGraph. In the first iteration, this is
obviously not the case, and we continue, recursively trying to extend H by adding
</bodyText>
<page confidence="0.975168">
62
</page>
<figure confidence="0.989587545454545">
Krahmer, van Erk, and Verleg Graph-Based Generation
H =
(i)
left_of
brown
chihuahua
H =
(ii)
in
H =
(iii)
</figure>
<figureCaption confidence="0.999258">
Figure 7
</figureCaption>
<bodyText confidence="0.994463944444445">
Three values for H in the generation process for d1.
adjacent (neighboring) edges until either a distinguishing graph has been constructed
(all distractors are ruled out) or the costs of H exceed the costs of the bestGraph found
so far.
While bestGraph is still the null graph, the algorithm continues until H is a distin-
guishing graph. Which is the first distinguishing graph to be found (if more than one
exists) depends on the order in which the adjacent edges are tried (see also Section 5.1).
Suppose for the sake of argument that the first distinguishing graph to be found is (ii)
in Figure 7. This graph is returned and stored in bestGraph. The costs associated with
this graph are five points (two vertices and three edges). At this stage in the generation
process only graphs with costs lower than five points are worth investigating. In fact,
there are only a few distinguishing graphs that cost less than this. After a number of
iterations the algorithm will find the cheapest solution (given this particular, simple
definition of the cost function), which is (iii) in Figure 7. That this distinguishing graph
does not include type information does not necessarily mean that such information
should not be realized. It means only that type information is, strictly speaking, not
necessary to distinguish the intended referent from the distractors. We return to this
issue in Section 5.2.
</bodyText>
<subsectionHeader confidence="0.995917">
4.4 Subgraph Isomorphism Testing
</subsectionHeader>
<bodyText confidence="0.9997874">
Figure 8 sketches the part of the algorithm that tests for subgraph isomorphism, match-
Graphs. This function is called each time the distractor set is calculated. It tests whether
the pair (v,H) can refer to (w, G), or put differently, it checks whether there exists an
isomorphism 7r such that H Cπ G with 7r.v = w. The function matchGraphs first deter-
mines whether the looping edges starting from vertex v match those of w. If EH(v, v)
is not a subset of EG(w,w) (e.g., v is a dog and w is a doghouse), we can immediately
discard the matching. Otherwise we start with the matching 7r.v = w and try to expand
it recursively. At each recursion step a fresh and as yet unmatched vertex y from VH
is selected that is adjacent to one of the vertices in the current domain of 7r (notated
dom(7r)). For each y we calculate the set Z of possible vertices in G to which y can be
</bodyText>
<page confidence="0.993932">
63
</page>
<note confidence="0.566311">
Computational Linguistics Volume 29, Number 1
</note>
<equation confidence="0.8186953">
matchGraphs(v, H, w, G) {
if EH(v,v) ⊆� EG(w,w) then return false fi;
7 r:= {v �--&gt; w} ;
Y := H.neighbors(v);
return matchHelper(7r, Y, G, H);
}
matchHelper(7r, Y, G, H) {
if  |Dom (7r) |= |H |then return true fi;
if Y = ∅ then return false fi;
choose a fresh, unmatched y from Y;
Z := {z ∈ VG  |y might be matched to z };
for each z ∈ Z do
if z is a valid extension of the mapping
then if matchHelper(7r ∪ {y �--&gt; z}, Y, G, H)
then return true
fi;
fi;
rof;
return false;
}
</equation>
<figureCaption confidence="0.919395">
Figure 8
</figureCaption>
<bodyText confidence="0.934588">
Sketch of the function testing for subgraph isomorphism (matchGraphs).
matched. This set consists of all the vertices in G that have the same looping edges as
y and the same edges to and from other vertices in the domain of the current matching
function 7r. Formally:
</bodyText>
<equation confidence="0.99190825">
Z := {z  |z ∈ VG ∧ EH(y,y) ⊆ EG(z,z)∧
∀h ∈ H.neighbors(y) ∩ dom(7r) :
[ EH(y,h) ⊆ EG(z,7r.h) ∧ EH(h,y) ⊆ EG(7r.h,z) ]
}
</equation>
<bodyText confidence="0.996038571428572">
(The H.neighbors(y) are the vertices in H that are adjacent to y, that is, those vertices
that are connected to y by an edge.) The matching can now possibly be extended with
7r.y = z, for each z ∈ Z. The algorithm then branches over all these possibilities. Once
a mapping 7r has been found that has exactly as many elements as H has vertices,
we have found a subgraph isomorphism. If there are still unmatched vertices in H,
or if all possible extensions with vertex y have been checked and no matching can be
found, the test for subgraph isomorphism has failed.
</bodyText>
<subsectionHeader confidence="0.993742">
4.5 A Note on the Implementation
</subsectionHeader>
<bodyText confidence="0.999957428571428">
The algorithm outlined in Figures 6 and 8 has been implemented in Java 2 (J2SE,
version 1.4). The implemented version of the algorithm is actually more efficient than
the sketch suggests, because various calculations need not be repeated in each iteration
(the set of distractors and the set G.neighbors(H), for example). In addition, the user
has the possibility of specifying the cost function in whatever way he or she sees fit.
A full-fledged performance analysis of the current implementation is beyond the
scope of this article. Such an analysis would be complicated by the fact that there
</bodyText>
<page confidence="0.999356">
64
</page>
<note confidence="0.901719">
Krahmer, van Erk, and Verleg Graph-Based Generation
</note>
<tableCaption confidence="0.998036">
Table 1
</tableCaption>
<table confidence="0.6138874">
Average times needed to find the cheapest distinguishing referring graphs for objects in seven
test scene graphs of increasing complexity.
Test scene Vertices Edges Average times
1 4 28 52 ms.
2 8 56 82 ms.
3 16 112 123 ms.
4 32 224 254 ms.
5 64 448 725 ms.
6 128 896 1,023 ms.
7 256 1,792 2,363 ms.
</table>
<bodyText confidence="0.999741625">
are many kinds of graphs (dense, sparse, (un)connected, etc.), and the performance
results will vary with the properties of the scene graph. If the scene graph is fully
connected, finding distinguishing graphs is much harder then when it is fully un-
connected. Nevertheless, to give the reader some insight into the performance of the
implementation, we applied it to seven test scene graphs of a specific form. The first
is our running example: the graph in Figure 3. The other six are obtained by scaling
up this graph and permutating (plus, if necessary, adding) properties to make sure
that each object can be uniquely described. This implies that only the first test graph
is fully connected. The other graphs consist of n4 connected subgraphs, where n is the
number of vertices in the graph. Thus, the bigger graphs are relatively less complex
than the smaller ones. Each graph consists of 4n looping edges (representing proper-
ties) and 3n nonlooping edges (representing spatial relations), again with n the number
of vertices.
The test was performed on a Windows 2000 PC with a 900 mHz AMD Athlon
Processor and 128 Mb RAM. The results are given in Table 1. We measured the system
time right before and right after the call of the main function makeReferringExpres-
sion. We computed the differences between these two times for a number of target
objects from the scene graphs (4 and 8 objects for the first two graphs, 16 for the
remaining ones). Note that this measurement does not include the time Java requires
for initialization or background activities such as garbage collection. Table 1 shows
that even for the larger graphs, the program is able to find minimal distinguishing
graphs relatively quickly. The current implementation is a straightforward one (see
also Section 6), so optimization, possibly in combination with heuristics, is likely to
show further improvements in performance.
</bodyText>
<sectionHeader confidence="0.854592" genericHeader="method">
5. Cost Functions and Search Strategies
</sectionHeader>
<subsectionHeader confidence="0.996001">
5.1 Full (Relational) Brevity Algorithm and Greedy Heuristics
</subsectionHeader>
<bodyText confidence="0.999753666666667">
The algorithm described in the previous section (with the uniform cost function as-
signing one point to each edge and vertex) can be seen as a generalization of Dale’s
(1992) full brevity algorithm, in the sense that there is a guarantee that the algorithm
will output the shortest possible description, if one exists. It is also an extension of the
full brevity algorithm, since it allows for relational descriptions. In this respect it is
comparable to the Dale and Haddock (1991) algorithm, granted that here the prob-
lems with infinite recursions do not arise, since a particular edge is either present in
a graph or not. Moreover, the approach is fully general and applies to n-ary relations
(and relation/property combinations) as well.
</bodyText>
<page confidence="0.998713">
65
</page>
<note confidence="0.640993">
Computational Linguistics Volume 29, Number 1
</note>
<bodyText confidence="0.999850833333333">
It is worth noting that Dale’s (1992) greedy heuristic algorithm (also discussed
in Dale and Reiter [1995]) can be cast in the graph framework as well. In fact, this
would give us a handle on the order in which different adjacent edges could be tried.
The edges associated with the intended referent should be sorted on their descriptive
power, which is inversely proportional to the number of occurrences of that particular
edge in the scene graph. The algorithm then adds the most discriminating edge (i.e., the
one removing most distractors) first. If there are various equally distinguishing edges,
the cheapest one is added. This process is then repeated until a distinguishing graph
is found. In fact, such a greedy strategy could be used to produce a first nonminimal
distinguishing graph. Subsequently we could call findGraph with this graph as initial
value of bestGraph, instead of the null graph ⊥. In this way, we would be able to find
minimal graphs more efficiently.
</bodyText>
<subsectionHeader confidence="0.984058">
5.2 Incremental Algorithm
</subsectionHeader>
<bodyText confidence="0.990879142857143">
The characteristic properties of Dale and Reiter’s (1995) incremental algorithm can be
incorporated into the graph framework as follows. First, the list of preferred attributes
can be modeled in terms of the cost function: All type edges should be cheaper than
all other edges. In fact, (basic level, see below) type edges could be free. Moreover,
the edges corresponding to absolute properties (color) should cost less than those
corresponding to relative ones (size). This gives us exactly the effect of having a list
of preferred attributes ( type, color, size ). It also implies that the type of an object is
always included if it is in any way distinguishing. That by itself does not guarantee
that type is always included. The incremental nature of the incremental algorithm can be
obtained by ordering edges with respect to their costs. Now the cheapest edges (i.e,
those expressing type information) should be tried first, and more expensive edges
should be tried later. In addition, the algorithm should terminate as soon as it has
found a distinguishing graph. This would guarantee that bargain type loops are always
included, and the algorithm would output (iii) from Figure 4 instead of (iii) from
</bodyText>
<figureCaption confidence="0.568236">
Figure 7 when applied to d1 from Figure 3.
</figureCaption>
<bodyText confidence="0.999876590909091">
Another characteristic property of the original incremental algorithm (not dis-
cussed in section 2) is the use of a subsumption hierarchy. A chihuahua, for instance,
can be referred to as either a chihuahua or a dog. The latter has a special status and
is called the basic level value (see, e.g., Rosch [1978]). According to Dale and Reiter
(1995) and Reiter (1990), human speakers have a general preference for basic level val-
ues and move to more specific (subsumed) values only if these are more informative.
This notion of a subsumption hierarchy can be modeled using the cost function. For
a given attribute, the basic level edges should be assigned the lowest costs, and those
farthest away from the basic level edge should have the highest costs. This implies
that adding an edge labeled dog is cheaper than adding an edge labeled chihuahua.
Hence a chihuahua edge will be selected only when there are fewer (or less expensive)
additional edges required to construct a distinguishing graph than would be the case
for a graph including a dog edge. Note that (assuming that the scene representation
is well defined) a distinguishing graph can never contain both a dog and a chihuahua
edge, since there will always be a cheaper distinguishing graph omitting one of the
two edges.
In sum, we can recast the incremental algorithm quite easily in terms of graphs.
The original incremental algorithm operates only on properties (looped edges in graph
terminology). Recall that when all edges in a scene graph are of the looping variety,
testing for subgraph isomorphism becomes trivial. The graph-theoretical reformulation
of the incremental algorithm does not fully exploit the possibilities offered by the graph
framework and the use of cost functions. First, from the graph-theoretical perspective,
</bodyText>
<page confidence="0.965941">
66
</page>
<note confidence="0.795944">
Krahmer, van Erk, and Verleg Graph-Based Generation
</note>
<bodyText confidence="0.9998874">
the generation of relational descriptions poses no problems. Note that the use of a
cost function to simulate subsumption hierarchies for properties carries over directly
to relations; for instance, the costs of adding an edge labeled next to should be less
than those of adding one labeled left of or right of. Hence, next to will be preferred,
unless using left of or right of requires fewer (or less expensive) additional edges for
the construction of a distinguishing graph. Another advantage of the way the graph-
based algorithm models the list of preferred attributes is that finer-grained distinctions
can be made than can with the incremental algorithm. In particular, we are not forced
to say that values of the attribute type are always cheaper than values of the attribute
color. Instead of assigning costs to attributes, we can assign costs to values of attributes.
This gives us the freedom to assign edges labeled with a common color value (e.g.,
brown) a lower cost than edges labeled with obscure type values, such as Polish owczarek
nizinny sheepdog. This implies that it will be cheaper to construct a distinguishing graph
referring to an object using two cheap edges (the brown dog) than with one particularly
expensive edge (the Polish owczarek nizinny sheepdog).
</bodyText>
<subsectionHeader confidence="0.999843">
5.3 Aspects of Other Algorithms
</subsectionHeader>
<bodyText confidence="0.986634633333333">
Various aspects of other algorithms can be captured in the graph-based algorithm as
well. To further illustrate the flexibility of the graph perspective, we briefly discuss
two such aspects.
5.3.1 Plurals. Van Deemter’s (2000) proposal to generate distinguishing plural de-
scriptions (such as the dogs) can be modeled using graphs in the following way. Van
Deemter’s algorithm takes as input a set of target objects, which, in our case, translates
into a set of vertices W from the scene graph (W C_ VG). Now the algorithm tries to
generate a vertex-graph pair (v, H) that uniquely refers to (W, G). The definition of
“uniquely referring graphs” has to be generalized slightly to accommodate plurals.
The constructed subgraph should refer to each of the vertices in the set W, but not to
any of the vertices in the scene graph outside this set. Formally, (v, H) uniquely refers
to (W, G) iff H is connected, and for each w E W there is a bijection π such that H Cπ G,
with π.v = w and there is no w&apos; E G\W such that (v, H) refers to (w&apos;, G). Observe that
the singular case defined in Section 4 is obtained by restricting W to singleton sets.
In this way, the basic algorithm can generate both singular and plural distinguishing
descriptions.
5.3.2 Context and Salience. An object that has been mentioned in recent context is
linguistically salient and hence can often be referred to using fewer properties; an
animal that is first described as “the large black dog with the hanging ears” may
subsequently be referred to using an anaphoric description such as “the dog.” Krahmer
and Theune (2002) model this phenomenon by assigning salience weights (sws) to
objects. For this purpose they use a version of centering theory (Grosz, Joshi, and
Weinstein 1995) augmented with a recency effect essentially due to Haji˘cov´a (1993).
Krahmer and Theune then define the set of distractors as the set of objects with a
salience weight higher than or equal to that of the target object. In terms of the graph-
theoretical framework, this would go as follows. First, we assign salience weights
to the vertices in the scene graph using the salience weights definition proposed by
Krahmer and Theune. Subsequently, in the sketch of the basic algorithm (Figure 6),
the set of distractors should be redefined as follows:
In I n E VG n matchGraphs(v, H, n, G) n n =� v n sw(n) &gt; sw(v)}
</bodyText>
<page confidence="0.998077">
67
</page>
<note confidence="0.729838">
Computational Linguistics Volume 29, Number 1
</note>
<bodyText confidence="0.99974225">
That is, the distractor set is restricted to those vertices n in the scene graph G that
currently are at least as salient as the target object v. For target objects that are linguis-
tically salient, this will typically lead to a reduction of the distractor set. Consequently,
distinguishing graphs for these target objects will generally be smaller than those for
nonsalient objects. Moreover, we will be able to find distinguishing graphs for a salient
object v relatively fast, since we already have a distinguishing graph (constructed for
the first definite reference to v) and we can use this graph as our initial value of
bestGraph.
</bodyText>
<subsectionHeader confidence="0.993335">
5.4 Stochastic Cost Functions
</subsectionHeader>
<bodyText confidence="0.999979">
One of the important open questions in natural language generation is how the com-
mon rule-based approaches to generation can be combined with recent insights from
statistical natural language processing (see, e.g., Langkilde and Knight [1998] and Mal-
ouf [2000] for partial answers). The approach proposed in this article makes it possible
to combine graph reformulations of well-known rule-based generation algorithms with
stochastic cost functions (the result resembles a Markov model). Such a cost function
could be derived from a sufficiently large corpus. For instance, as a first approximation
we could define the costs of adding an edge e in terms of the probability P(e) that e
occurs in a distinguishing description (estimated by counting occurrences):
</bodyText>
<equation confidence="0.98921">
cost(e) = −log2(P(e))
</equation>
<bodyText confidence="0.999993142857143">
Thus, properties that occur frequently are cheap; properties that are relatively rare are
expensive. In this way, we would probably derive that polish owczarek nizinny sheepdog
indeed costs more (and is thus less likely to be selected) than brown.
Even though this first approximation already has some interesting consequences,
it is probably not enough to obtain a plausible and useful cost function. For instance, it
is unlikely that the co-occurrence of edges is fully independent; a husky is likely to be
white, and a chihuahua is not. Such dependencies are not modeled by the definition
given above. In addition, properties referring to size such as small and large probably
occur more often in a corpus than properties referring to colors such as brown or yellow,
which at first sight appears to run counter to the earlier observation that speakers
generally prefer absolute properties over relative ones. The reason for this, however,
is probably that there are simply fewer ways to describe the size than there are to
describe the color of objects. Searching for a more sophisticated method of defining
stochastic cost functions is therefore an interesting line of future research.
</bodyText>
<subsectionHeader confidence="0.853408">
6. Concluding Remarks and Future Research
</subsectionHeader>
<bodyText confidence="0.999955916666666">
In this article, we have presented a new approach to the content determination problem
for referring expressions. We proposed to model scenes as labeled directed graphs, in
which objects are represented as vertices and the properties and relations of these
objects are represented as edges. The problem of finding a referring expression for
an object is treated as finding a subgraph of the scene graph that is isomorphic to
the intended referent but not to any other object. The theoretical complexity of this
reformulation of the content determination problem is NP-complete, but there exist
various restrictions (planar graphs, decision trees for fixed scene graphs, upper bound
to the number of edges in a distinguishing graph) that have a polynomial complexity.
We have described a general and fully implemented algorithm, based on the sub-
graph isomorphism idea, consisting of two main functions: one that constructs refer-
ring graphs and one that tests for subgraph isomorphisms. Cost functions are used to
</bodyText>
<page confidence="0.997544">
68
</page>
<note confidence="0.815497">
Krahmer, van Erk, and Verleg Graph-Based Generation
</note>
<bodyText confidence="0.999980980392157">
guide the search process and to give preference to some solutions over others. Opti-
mization has not been the focus of this article, but we came across various heuristic
strategies that would speed up the algorithm. For instance, we can try edges in the
order determined by the cost function (from cheap to more expensive), and we can use
a greedy algorithm to find a first distinguishing graph quickly. In general, one of the
advantages of the graph perspective is that many efficient algorithms for dealing with
graph structures are known. We can use those algorithms to formulate more efficient
versions of the subgraph construction component (perhaps using the method of Tarjan
[1972]; see also Sedgewick [1988]) and of the subgraph isomorphism testing component
(e.g., using the aforementioned approach of Messmer and Bunke [1995, 1998]).
The graph perspective has a number of attractive properties. (1) By reformulating
the content determination problem as a graph construction problem, we can directly
apply the many techniques and algorithms for dealing with graph structures. (2) The
use of cost functions allows us to model different search methods, each restricting
the search space in its own way. By defining cost functions in different ways, we
can mimic and extend various well-known algorithms from the literature (see also
Krahmer, van Erk, and Verleg [2001]). (3) The generation of relational descriptions is
straightforward; the problems that plague some other algorithms for the generation
of relational descriptions do not arise. Moreover, the approach to relations proposed
here is fully general: It applies to all n-ary relations, not just binary ones. (4) The use
of cost functions paves the way for integrating statistical information directly into the
generation process. In fact, performing experiments with various ways to estimate
stochastic cost functions from corpora is one path for future research that we have
identified.
Besides looking for graph-based optimizations and performing experiments with
stochastic cost functions, there are three other lines for future research we would
like to mention. The first concerns the construction of scene graphs. How should the
decision be made as to which aspects of a scene to represent in the graph? Naturally,
the algorithm can only refer to entities that are modeled in the scene graph, but
representing every possible object in a single graph will lead to an explosion of edges
and vertices. Perhaps some notion of focus of attention can be used to restrict the scene
graph. It would also be interesting to look for automatic methods for the construction
of scene graphs. We might use computer vision algorithms (see, e.g., Faugeras [1993]),
which are often graph-based themselves, for this purpose. For example, Bauckhage et
al. (1999) describe an assembly system in which computer vision is used to convert
a workspace with various building blocks into a labeled directed scene graph. Note
that this approach is also able to deal with dynamic scenes; it can track changes in the
workspace (which is required for handling the assembly process).
Another issue that we have not discussed in much detail is linguistic realization.
How should the information contained in a referring graph be expressed in natural
language? So far, we have assumed that a distinguishing graph can simply be con-
structed first and subsequently fed into a realization engine. There may, however,
be certain dependencies between content selection and realization (see, e.g., Horacek
[1997] and Krahmer and Theune [2002]). One way to take these dependencies into ac-
count would be to reformulate the cost function in such a way that it promotes graphs
that can easily be realized and punishes graphs that are more difficult to realize.
A final aspect of the graph model that deserves further investigation is based on
the fact that we can look at a graph such as that in Figure 3 as a Kripke model.
Kripke models are used in model-theoretic semantics for modal logics. The advantage
of looking at graphs such as that in Figure 3 as Kripke models is that we can use tools
from modal logic to reason about these structures. For example, we can reformulate
</bodyText>
<page confidence="0.995573">
69
</page>
<note confidence="0.635957">
Computational Linguistics Volume 29, Number 1
</note>
<bodyText confidence="0.815908">
the problem of determining the content of a distinguishing description in terms of
hybrid logic (see, e.g., Blackburn [2000]) as follows:
@iϕ ∧ Aj(i =� j — _@jϕ)
In words: When we want to refer to vertex i, we are looking for that distinguishing
formula ϕ that is true of (“at”) i but not of any j different from i. One advantage of this
logical perspective is that logical properties that are not covered by most generation
algorithms (such as not having a certain property; see van Deemter [2002]) fit in very
well with this perspective.
</bodyText>
<sectionHeader confidence="0.604364" genericHeader="method">
Appendix: Planarizing Scene Graphs
</sectionHeader>
<bodyText confidence="0.999988869565217">
Planar graphs may be relevant for our current purposes, since subgraph isomorphism
can be tested more efficiently on planar graphs than on arbitrary graphs. There are
two ways in which a nonplanar graph G can be turned into a planar one G&apos; (see
Liebers [2001] for a recent overview of planarization algorithms): Either the graph G
can be pruned (using vertex or edge deletion) or it can be extended (for instance,
using vertex splitting or by inserting vertices at crossings). A disadvantage of the
extension approach is that we lose the intuitive one-to-one correspondence between
potential target objects and vertices in the scene graph, since the additional vertices
only serve the purpose of planarizing the graph and do not represent objects in a scene.
A disadvantage of the pruning approach is that we lose information. The presence of a
cost function, however, is potentially very useful, since it allows us to avoid eliminating
comparatively cheap (and thus more frequently selected) edges.
Here, for the sake of illustration, we briefly describe a weighted greedy pruning
algorithm that turns an arbitrary scene graph G = (VG, EG) with n vertices and m
edges into a planar graph G&apos; = (VG,, EGI) with n vertices and at most m edges. We
start from the graph G&apos; = (VG, OG), where OG is the set of looping edges from the
scene graph, that is, OG = UvEVG EG(v,v). Next we order the remaining edges from
the scene graph RG = EG\OG with respect to their costs; the cheapest one comes first,
the more expensive ones come later, in order of increasing expense. For each e E RG,
we check whether G&apos; + e is planar (e.g., using the algorithm from Hopcroft and Tarjan
[1974]). If it is, e is added to EGI. The algorithm terminates when RG = 0. The result is
a maximal planar subgraph G&apos; of the scene graph G that differs from G only possibly
in the deletion of certain relatively expensive nonlooping (relational) edges.
</bodyText>
<sectionHeader confidence="0.983287" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999635444444445">
We would like to thank Dennis van Oort
and Denis Gerritsen for their help in the
implementation and Alexander Koller and
Kees van Deemter for some very useful
discussions. Thanks are also due to Paul
Piwek, Mari¨et Theune, Ielka van der Sluis,
and the anonymous reviewers for helpful
comments on an earlier version of this
article.
</bodyText>
<sectionHeader confidence="0.992174" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991784052631579">
Appelt, Douglas E. 1985. Planning English
referring expressions. Artificial Intelligence,
26:1–33.
Bateman, John. 1997. Enabling technology
for multilingual natural language
generation: The KPML development
environment. Natural Language
Engineering, 3:15–55.
Bateman, John. 1999. Using aggregation for
selecting content when generating
referring expressions. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics (ACL’99),
University of Maryland.
Bauckhage, Christian, Jannik Fritsch,
Franz Kummert, and Gerhard Sagerer.
1999. Towards a vision system for
supervising assembly processes. In
Proceedings of the Symposium on
</reference>
<page confidence="0.994278">
70
</page>
<note confidence="0.856094">
Krahmer, van Erk, and Verleg Graph-Based Generation
</note>
<reference confidence="0.99827224590164">
Intelligent Robotic Systems (SIRS’99), pages
89–98.
Berge, Claude. 1985. Graphs. North-Holland,
Amsterdam.
Blackburn, Patrick. 2000. Representation,
reasoning, and relational structure: A
hybrid logic manifesto. Logic Journal of the
IGPL, 8(3):339–365.
Chartrand, Gary and Ortrud Oellermann.
1993. Applied and Algorithmic Graph Theory.
McGraw-Hill, New York.
Cormen, Thomas H., Charles E. Leiserson,
and Ronald L. Rivest. 1990. Introduction to
Algorithms. MIT Press, Cambridge.
Dale, Robert. 1992. Generating Referring
Expressions: Constructing Descriptions in a
Domain of Objects and Processes. MIT Press,
Cambridge.
Dale, Robert and Nicholas Haddock. 1991.
Generating referring expressions
involving relations. In Proceedings of the
Fifth Conference of the European Chapter
of the Association for Computational
Linguistics (EACL’91), pages 161–166,
Berlin.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
Gricean maxims in the generation of
referring expressions. Cognitive Science,
18:233–263.
Elhaded, Michael and Jacques Robin. 1997.
SURGE: A comprehensive plug-in
syntactic realization component for text
generation. Technical Report 96-03,
Computer Science Department,
Ben-Gurion University, Beer Sheva, Israel.
Eppstein, David. 1999. Subgraph
isomorphism in planar graphs and related
problems. Journal of Graph Algorithms and
Applications, 3(3):1–27.
Faugeras, Olivier. 1993. Three-Dimensional
Computer Vision. MIT Press, Cambridge.
Garey, Michael R. and David S. Johnson.
1979. Computers and Intractability: A Guide
to the Theory of NP-Completeness. W.H.
Freeman, New York.
Gibbons, Alan M. 1985. Algorithmic Graph
Theory. Cambridge University Press,
Cambridge.
Grosz, Barbara J., Aravind K. Joshi, and
Scott Weinstein. 1995. Centering: A
framework for modeling the local
coherence of discourse. Computational
Linguistics, 21(2):203–225.
Haji˘covi, Eva. 1993. Issues of Sentence
Structure and Discourse Patterns. Charles
University, Prague.
Hopcroft, John E. and Robert Tarjan. 1974.
Efficient planarity testing. Journal of the
Association for Computing Machinery,
21:549–568.
Horacek, Helmut. 1997. An algorithm for
generating referential descriptions with
flexible interfaces. In Proceedings of the 35th
Annual Meeting of the Association for
Computational Linguistics (ACL’97),
pages 206–213, Madrid.
Krahmer, Emiel and Mari¨et Theune. 1998.
Context sensitive generation of
descriptions. In Proceedings of the Fifth
International Conference on Spoken Language
Processing (ICSLP’98), pages 1151–1154,
Sydney.
Krahmer, Emiel and Mari¨et Theune. 2002.
Efficient context-sensitive generation of
descriptions. In Kees van Deemter and
Rodger Kibble, editors, Information
Sharing: Givenness and Newness
in Language Processing. CSLI Publications,
Stanford, California, pages 223–264.
Krahmer, Emiel, Sebastiaan van Erk, and
Andre´ Verleg. 2001. A meta-algorithm for
the generation of referring expressions. In
Proceedings of the Eighth European Workshop
on Natural Language Generation
(EWNLG’01), Toulouse.
Langkilde, Irene and Kevin Knight. 1998.
The practical value of n-grams in
generation. In Proceedings of the Ninth
International Workshop on Natural Language
Generation (INLG’98), pages 248–255,
Niagara-on-the-Lake, Ontario.
Liebers, Annegret. 2001. Planarizing graphs.
Journal of Graph Algorithms and Applications,
5(1):1–74.
Malouf, Rob. 2000. The order of prenominal
adjectives in natural language generation.
In Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics
(ACL’00), Hong Kong.
Messmer, Bruno T. and Horst Bunke. 1995.
Subgraph isomorphism in polynomial
time. Technical Report IAM 95-003,
University of Bern, Institute of Computer
Science and Applied Mathematics, Bern,
Switzerland.
Messmer, Bruno T. and Horst Bunke. 1998.
A new algorithm for error-tolerant
subgraph isomorphism detection. IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 20(5):493–504.
Pechmann, Thomas. 1989. Incremental
speech production and referential
overspecification. Linguistics, 27:98–110.
Read, Ronald C. and Derek G. Corneil.
1977. The graph isomorphism disease.
Journal of Graph Theory, 1(1):339–363.
Reiter, Ehud. 1990. The computational
complexity of avoiding conversational
implicatures. In Proceedings of the 28th
Annual Meeting of the Association for
Computational Linguistics (ACL’90),
</reference>
<page confidence="0.959628">
71
</page>
<note confidence="0.345721">
Computational Linguistics Volume 29, Number 1
</note>
<reference confidence="0.998464441176471">
pages 97–104.
Rosch, Eleanor. 1978. Principles of
categorization. In E. Rosch and B. Lloyd,
editors, Cognition and Categorization.
Lawrence Erlbaum, Hillsdale, New Jersey,
pages 27–48.
Sedgewick, Robert. 1988. Algorithms.
Addison-Wesley, Reading, Massachusetts,
second edition.
Stone, Matthew and Bonnie Webber. 1998.
Textual economy through close coupling
of syntax and semantics. In Proceedings of
the Ninth International Workshop on Natural
Language Generation (INLG’98),
pages 178–187, Niagara-on-the-Lake,
Ontario.
Tarjan, Robert E. 1972. Depth-first search
and linear time algorithms. SIAM Journal
on Computing, 1(2):146–160.
Theune, Mari¨et. 2000. From Data to Speech:
Language Generation in Context. Ph.D.
thesis, Eindhoven University of
Technology, Eindhoven, The Netherlands.
van Deemter, Kees. 2000. Generating vague
descriptions. In Proceedings of the First
International Natural Language Generation
Conference (INLG’00), Mitzpe Ramon.
van Deemter, Kees. 2002. Generating
referring expressions: Boolean extensions
of the incremental algorithm.
Computational Linguistics, 28(1):37–52.
Wilson, Robin J. 1996. Introduction to Graph
Theory. Longman, Harlow, England,
fourth edition.
</reference>
<page confidence="0.998716">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.942203">
<title confidence="0.998132">Graph-Based Generation of Referring Expressions</title>
<author confidence="0.999177">Sebastiaan van</author>
<affiliation confidence="0.9993405">Tilburg University Eindhoven University of Technology Eindhoven University of Technology</affiliation>
<abstract confidence="0.995517">This article describes a new approach to the generation of referring expressions. We propose to formalize a scene (consisting of a set of objects with various properties and relations) as a labeled directed graph and describe content selection (which properties to include in a referring expression) as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The current approach has four main advantages: (1) Graph structures have been studied extensively, and by moving to a graph perspective we get direct access to the many theories and algorithms for dealing with graphs; (2) many existing generation algorithms can be reformulated in terms of graphs, and this enhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
</authors>
<title>Planning English referring expressions.</title>
<date>1985</date>
<journal>Artificial Intelligence,</journal>
<pages>26--1</pages>
<contexts>
<context position="1520" citStr="Appelt (1985)" startWordPosition="227" endWordPosition="228">in terms of graphs, and this enhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph persp</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>Appelt, Douglas E. 1985. Planning English referring expressions. Artificial Intelligence, 26:1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bateman</author>
</authors>
<title>Enabling technology for multilingual natural language generation: The KPML development environment.</title>
<date>1997</date>
<journal>Natural Language Engineering,</journal>
<pages>3--15</pages>
<contexts>
<context position="6193" citStr="Bateman 1997" startWordPosition="932" endWordPosition="933">ale and Reiter [1995]). The task of the algorithm is to determine which set of properties is needed to single out the target object v from the distractors. This is known as the content determination problem for referring expressions. On the basis of this set of properties a distinguishing description for v can be generated. Most algorithms do not address the surface realization problem (how the selected properties should be realized in natural language) in much detail; it is usually assumed that once the content for a referring expression has been determined, a standard realizer such as KPML (Bateman 1997) or SURGE (Elhaded and Robin 1997) can convert the meaning representation to natural language. Consider the example scene in Figure 1. In this scene, as in any other scene, we see a finite domain of entities D with properties P. In this particular scene, D = {d1,d2,d3,d4} is the set of entities and P = { dog, cat, brown, black+white, large, small } is the set of properties. A scene is usually represented as a database (or knowledge base) listing the properties of each element in D. Thus: d1: dog (d1) small (d1) brown (d1) d2: dog (d2) large (d2) brown (d2) d3: dog (d3) large (d3) black+white (</context>
</contexts>
<marker>Bateman, 1997</marker>
<rawString>Bateman, John. 1997. Enabling technology for multilingual natural language generation: The KPML development environment. Natural Language Engineering, 3:15–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bateman</author>
</authors>
<title>Using aggregation for selecting content when generating referring expressions.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99),</booktitle>
<institution>University of Maryland.</institution>
<contexts>
<context position="1687" citStr="Bateman (1999)" startWordPosition="252" endWordPosition="253">e plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph perspective has four main advantages. (1) There are many attractive and well-understood algorithms for dealing with graph structures (see, e.g., Gibbons [1985], Cormen, Lei</context>
</contexts>
<marker>Bateman, 1999</marker>
<rawString>Bateman, John. 1999. Using aggregation for selecting content when generating referring expressions. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99), University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Bauckhage</author>
<author>Jannik Fritsch</author>
<author>Franz Kummert</author>
<author>Gerhard Sagerer</author>
</authors>
<title>Towards a vision system for supervising assembly processes.</title>
<date>1999</date>
<booktitle>In Proceedings of the Symposium on Intelligent Robotic Systems (SIRS’99),</booktitle>
<pages>89--98</pages>
<contexts>
<context position="51189" citStr="Bauckhage et al. (1999)" startWordPosition="8613" endWordPosition="8616">ould the decision be made as to which aspects of a scene to represent in the graph? Naturally, the algorithm can only refer to entities that are modeled in the scene graph, but representing every possible object in a single graph will lead to an explosion of edges and vertices. Perhaps some notion of focus of attention can be used to restrict the scene graph. It would also be interesting to look for automatic methods for the construction of scene graphs. We might use computer vision algorithms (see, e.g., Faugeras [1993]), which are often graph-based themselves, for this purpose. For example, Bauckhage et al. (1999) describe an assembly system in which computer vision is used to convert a workspace with various building blocks into a labeled directed scene graph. Note that this approach is also able to deal with dynamic scenes; it can track changes in the workspace (which is required for handling the assembly process). Another issue that we have not discussed in much detail is linguistic realization. How should the information contained in a referring graph be expressed in natural language? So far, we have assumed that a distinguishing graph can simply be constructed first and subsequently fed into a rea</context>
</contexts>
<marker>Bauckhage, Fritsch, Kummert, Sagerer, 1999</marker>
<rawString>Bauckhage, Christian, Jannik Fritsch, Franz Kummert, and Gerhard Sagerer. 1999. Towards a vision system for supervising assembly processes. In Proceedings of the Symposium on Intelligent Robotic Systems (SIRS’99), pages 89–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude Berge</author>
</authors>
<date>1985</date>
<publisher>Graphs. North-Holland,</publisher>
<location>Amsterdam.</location>
<marker>Berge, 1985</marker>
<rawString>Berge, Claude. 1985. Graphs. North-Holland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Blackburn</author>
</authors>
<title>Representation, reasoning, and relational structure: A hybrid logic manifesto.</title>
<date>2000</date>
<journal>Logic Journal of the IGPL,</journal>
<volume>8</volume>
<issue>3</issue>
<marker>Blackburn, 2000</marker>
<rawString>Blackburn, Patrick. 2000. Representation, reasoning, and relational structure: A hybrid logic manifesto. Logic Journal of the IGPL, 8(3):339–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary Chartrand</author>
<author>Ortrud Oellermann</author>
</authors>
<title>Applied and Algorithmic Graph Theory.</title>
<date>1993</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<marker>Chartrand, Oellermann, 1993</marker>
<rawString>Chartrand, Gary and Ortrud Oellermann. 1993. Applied and Algorithmic Graph Theory. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>Cormen, Thomas H., Charles E. Leiserson, and Ronald L. Rivest. 1990. Introduction to Algorithms. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes.</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1573" citStr="Dale (1992)" startWordPosition="235" endWordPosition="236">tegration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph perspective has four main advantages. (1) There are many a</context>
<context position="7204" citStr="Dale 1992" startWordPosition="1111" endWordPosition="1112">sented as a database (or knowledge base) listing the properties of each element in D. Thus: d1: dog (d1) small (d1) brown (d1) d2: dog (d2) large (d2) brown (d2) d3: dog (d3) large (d3) black+white (d3) d4: cat (d4) small (d4) brown (d4) 54 Krahmer, van Erk, and Verleg Graph-Based Generation d1 d2 d3 d4 Figure 1 A simple example scene consisting of some domestic animals. In what is probably the key reference on the topic, Dale and Reiter (1995) describe and discuss a number of algorithms for the generation of referring expressions. One of these is the full brevity algorithm (originally due to Dale 1992). This algorithm first tries to generate a distinguishing description for the target object v using one single property. If this fails, it considers all possible combinations of two properties to see if any of these suffices for the generation of a distinguishing description, and so on. It is readily seen that this algorithm will output the shortest possible description, if one exists. Suppose the full brevity algorithm is used to generate a description for d1 in Figure 1. There is no single property that distinguishes the target object d1 from the distractors {d2, d3, d4}. But when considerin</context>
</contexts>
<marker>Dale, 1992</marker>
<rawString>Dale, Robert. 1992. Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Nicholas Haddock</author>
</authors>
<title>Generating referring expressions involving relations.</title>
<date>1991</date>
<booktitle>In Proceedings of the Fifth Conference of the European Chapter of the Association for Computational Linguistics (EACL’91),</booktitle>
<pages>161--166</pages>
<location>Berlin.</location>
<contexts>
<context position="1560" citStr="Dale and Haddock (1991)" startWordPosition="231" endWordPosition="234">nhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph perspective has four main advantages. (1) The</context>
<context position="36469" citStr="Dale and Haddock (1991)" startWordPosition="6223" endWordPosition="6226">ow further improvements in performance. 5. Cost Functions and Search Strategies 5.1 Full (Relational) Brevity Algorithm and Greedy Heuristics The algorithm described in the previous section (with the uniform cost function assigning one point to each edge and vertex) can be seen as a generalization of Dale’s (1992) full brevity algorithm, in the sense that there is a guarantee that the algorithm will output the shortest possible description, if one exists. It is also an extension of the full brevity algorithm, since it allows for relational descriptions. In this respect it is comparable to the Dale and Haddock (1991) algorithm, granted that here the problems with infinite recursions do not arise, since a particular edge is either present in a graph or not. Moreover, the approach is fully general and applies to n-ary relations (and relation/property combinations) as well. 65 Computational Linguistics Volume 29, Number 1 It is worth noting that Dale’s (1992) greedy heuristic algorithm (also discussed in Dale and Reiter [1995]) can be cast in the graph framework as well. In fact, this would give us a handle on the order in which different adjacent edges could be tried. The edges associated with the intended </context>
</contexts>
<marker>Dale, Haddock, 1991</marker>
<rawString>Dale, Robert and Nicholas Haddock. 1991. Generating referring expressions involving relations. In Proceedings of the Fifth Conference of the European Chapter of the Association for Computational Linguistics (EACL’91), pages 161–166, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<pages>18--233</pages>
<contexts>
<context position="1597" citStr="Dale and Reiter (1995)" startWordPosition="237" endWordPosition="240">the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph perspective has four main advantages. (1) There are many attractive and well-under</context>
<context position="5143" citStr="Dale and Reiter 1995" startWordPosition="759" endWordPosition="762">ections in Section 6. 2. Generating Referring Expressions There are many different algorithms for the generation of referring expressions, each with its own objectives: Some aim at producing the shortest possible description (e.g., Dale’s [1992] full brevity algorithm), others focus on psychological realism (e.g., Dale and Reiter’s [1995] incremental algorithm) or realistic output (e.g., Horacek 1997). The degree of detail in which the various algorithms are described differs considerably. Some algorithms are fully formalized and come with explicit characterizations of their complexity (e.g., Dale and Reiter 1995; van Deemter 2000); others are more conceptual and concentrate on exploring new directions (e.g., Stone and Webber 1998). Despite such differences, most algorithms deal with the same problem definition. They take as input a single object v (the target object) for which a referring expression is to be generated and a set of objects (the distractors) from which the target object needs to be distinguished (we use the terminology from Dale and Reiter [1995]). The task of the algorithm is to determine which set of properties is needed to single out the target object v from the distractors. This is</context>
<context position="7042" citStr="Dale and Reiter (1995)" startWordPosition="1083" endWordPosition="1086">. In this particular scene, D = {d1,d2,d3,d4} is the set of entities and P = { dog, cat, brown, black+white, large, small } is the set of properties. A scene is usually represented as a database (or knowledge base) listing the properties of each element in D. Thus: d1: dog (d1) small (d1) brown (d1) d2: dog (d2) large (d2) brown (d2) d3: dog (d3) large (d3) black+white (d3) d4: cat (d4) small (d4) brown (d4) 54 Krahmer, van Erk, and Verleg Graph-Based Generation d1 d2 d3 d4 Figure 1 A simple example scene consisting of some domestic animals. In what is probably the key reference on the topic, Dale and Reiter (1995) describe and discuss a number of algorithms for the generation of referring expressions. One of these is the full brevity algorithm (originally due to Dale 1992). This algorithm first tries to generate a distinguishing description for the target object v using one single property. If this fails, it considers all possible combinations of two properties to see if any of these suffices for the generation of a distinguishing description, and so on. It is readily seen that this algorithm will output the shortest possible description, if one exists. Suppose the full brevity algorithm is used to gen</context>
<context position="10708" citStr="Dale and Reiter (1995" startWordPosition="1690" endWordPosition="1693">he small brown dog.” This is a successful distinguishing 55 Computational Linguistics Volume 29, Number 1 d3 � 1 � 1 d4 d1 d2 Figure 2 Another scene: Two dogs and two doghouses (from Krahmer and Theune [2002]). description but not a minimal one: The property brown is, strictly speaking, made redundant by the later inclusion of the property small. Since there is no backtracking in the incremental algorithm however, every selected property is realized (hence “incremental”). This aspect is largely responsible for the computational efficiency of the algorithm (it has a polynomial complexity), but Dale and Reiter (1995, page 248) also claim that it is “psychologically realistic.” They point out that sometimes people may describe an object as “the white bird” even though the simpler “the bird” would have been sufficient (cf. Pechmann [1989]; see, however, Krahmer and Theune [2002] for discussion). Even though there are various useful and interesting algorithms for the generation of referring expressions, a number of open questions remain. Recently there has been an increased interest in statistical approaches to natural language generation. For example, Malouf (2000) has shown that large corpora can be used </context>
<context position="39384" citStr="Dale and Reiter (1995)" startWordPosition="6702" endWordPosition="6705">. In addition, the algorithm should terminate as soon as it has found a distinguishing graph. This would guarantee that bargain type loops are always included, and the algorithm would output (iii) from Figure 4 instead of (iii) from Figure 7 when applied to d1 from Figure 3. Another characteristic property of the original incremental algorithm (not discussed in section 2) is the use of a subsumption hierarchy. A chihuahua, for instance, can be referred to as either a chihuahua or a dog. The latter has a special status and is called the basic level value (see, e.g., Rosch [1978]). According to Dale and Reiter (1995) and Reiter (1990), human speakers have a general preference for basic level values and move to more specific (subsumed) values only if these are more informative. This notion of a subsumption hierarchy can be modeled using the cost function. For a given attribute, the basic level edges should be assigned the lowest costs, and those farthest away from the basic level edge should have the highest costs. This implies that adding an edge labeled dog is cheaper than adding an edge labeled chihuahua. Hence a chihuahua edge will be selected only when there are fewer (or less expensive) additional ed</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Dale, Robert and Ehud Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 18:233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhaded</author>
<author>Jacques Robin</author>
</authors>
<title>SURGE: A comprehensive plug-in syntactic realization component for text generation.</title>
<date>1997</date>
<tech>Technical Report 96-03,</tech>
<institution>Computer Science Department, Ben-Gurion University, Beer Sheva, Israel.</institution>
<contexts>
<context position="6227" citStr="Elhaded and Robin 1997" startWordPosition="936" endWordPosition="939">The task of the algorithm is to determine which set of properties is needed to single out the target object v from the distractors. This is known as the content determination problem for referring expressions. On the basis of this set of properties a distinguishing description for v can be generated. Most algorithms do not address the surface realization problem (how the selected properties should be realized in natural language) in much detail; it is usually assumed that once the content for a referring expression has been determined, a standard realizer such as KPML (Bateman 1997) or SURGE (Elhaded and Robin 1997) can convert the meaning representation to natural language. Consider the example scene in Figure 1. In this scene, as in any other scene, we see a finite domain of entities D with properties P. In this particular scene, D = {d1,d2,d3,d4} is the set of entities and P = { dog, cat, brown, black+white, large, small } is the set of properties. A scene is usually represented as a database (or knowledge base) listing the properties of each element in D. Thus: d1: dog (d1) small (d1) brown (d1) d2: dog (d2) large (d2) brown (d2) d3: dog (d3) large (d3) black+white (d3) d4: cat (d4) small (d4) brown </context>
</contexts>
<marker>Elhaded, Robin, 1997</marker>
<rawString>Elhaded, Michael and Jacques Robin. 1997. SURGE: A comprehensive plug-in syntactic realization component for text generation. Technical Report 96-03, Computer Science Department, Ben-Gurion University, Beer Sheva, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Eppstein</author>
</authors>
<title>Subgraph isomorphism in planar graphs and related problems.</title>
<date>1999</date>
<journal>Journal of Graph Algorithms and Applications,</journal>
<volume>3</volume>
<issue>3</issue>
<contexts>
<context position="21986" citStr="Eppstein 1999" startWordPosition="3704" endWordPosition="3705">does have a disadvantage: We lose completeness (see van Deemter [2002]). In particular, the algorithm will fail for objects that can be uniquely described only with K + 1 (or more) edges. Of course, one could argue that in such cases objects should be distinguished using other means (e.g., by pointing). Nevertheless, it is worthwhile to look for classes of graphs for which the subgraph isomorphism problem can be solved more efficiently, without postulating upper bounds. For instance, if G and H are planar (simple) graphs the problem can be solved in time linear in the number of vertices of G (Eppstein 1999). Basically, a planar graph is one that can be drawn on a plane in such a way that there are no crossing edges (thus, for instance, the graph in Figure 3 is planar, as is any graph with only four vertices). In general, there is no a priori reason to assume that our scene representations will be planar. Yet every nonplanar graph can be modified into a closely related planar one. We briefly address planarization of scene graphs in the Appendix. A final alternative is worth mentioning. The general approach to the problem of subgraph isomorphism detection assumes that both graphs are given on-line</context>
</contexts>
<marker>Eppstein, 1999</marker>
<rawString>Eppstein, David. 1999. Subgraph isomorphism in planar graphs and related problems. Journal of Graph Algorithms and Applications, 3(3):1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Faugeras</author>
</authors>
<title>Three-Dimensional Computer Vision.</title>
<date>1993</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<marker>Faugeras, 1993</marker>
<rawString>Faugeras, Olivier. 1993. Three-Dimensional Computer Vision. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Garey</author>
<author>David S Johnson</author>
</authors>
<title>Computers and Intractability: A Guide to the Theory of NP-Completeness.</title>
<date>1979</date>
<publisher>W.H. Freeman,</publisher>
<location>New York.</location>
<marker>Garey, Johnson, 1979</marker>
<rawString>Garey, Michael R. and David S. Johnson. 1979. Computers and Intractability: A Guide to the Theory of NP-Completeness. W.H. Freeman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan M Gibbons</author>
</authors>
<title>Algorithmic Graph Theory.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<marker>Gibbons, 1985</marker>
<rawString>Gibbons, Alan M. 1985. Algorithmic Graph Theory. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Grosz, Barbara J., Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Haji˘covi</author>
</authors>
<title>Issues of Sentence Structure and Discourse Patterns.</title>
<date>1993</date>
<institution>Charles University,</institution>
<location>Prague.</location>
<marker>Haji˘covi, 1993</marker>
<rawString>Haji˘covi, Eva. 1993. Issues of Sentence Structure and Discourse Patterns. Charles University, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hopcroft</author>
<author>Robert Tarjan</author>
</authors>
<title>Efficient planarity testing.</title>
<date>1974</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<pages>21--549</pages>
<marker>Hopcroft, Tarjan, 1974</marker>
<rawString>Hopcroft, John E. and Robert Tarjan. 1974. Efficient planarity testing. Journal of the Association for Computing Machinery, 21:549–568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Horacek</author>
</authors>
<title>An algorithm for generating referential descriptions with flexible interfaces.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL’97),</booktitle>
<pages>206--213</pages>
<location>Madrid.</location>
<contexts>
<context position="1613" citStr="Horacek (1997)" startWordPosition="241" endWordPosition="242">(3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph perspective has four main advantages. (1) There are many attractive and well-understood algorithms</context>
<context position="4927" citStr="Horacek 1997" startWordPosition="730" endWordPosition="731">orithm, which is illustrated with a worked example. In Section 5 various ways to formalize cost functions are discussed and compared. We end with some concluding remarks and a discussion of future research directions in Section 6. 2. Generating Referring Expressions There are many different algorithms for the generation of referring expressions, each with its own objectives: Some aim at producing the shortest possible description (e.g., Dale’s [1992] full brevity algorithm), others focus on psychological realism (e.g., Dale and Reiter’s [1995] incremental algorithm) or realistic output (e.g., Horacek 1997). The degree of detail in which the various algorithms are described differs considerably. Some algorithms are fully formalized and come with explicit characterizations of their complexity (e.g., Dale and Reiter 1995; van Deemter 2000); others are more conceptual and concentrate on exploring new directions (e.g., Stone and Webber 1998). Despite such differences, most algorithms deal with the same problem definition. They take as input a single object v (the target object) for which a referring expression is to be generated and a set of objects (the distractors) from which the target object nee</context>
</contexts>
<marker>Horacek, 1997</marker>
<rawString>Horacek, Helmut. 1997. An algorithm for generating referential descriptions with flexible interfaces. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL’97), pages 206–213, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Mari¨et Theune</author>
</authors>
<title>Context sensitive generation of descriptions.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifth International Conference on Spoken Language Processing (ICSLP’98),</booktitle>
<pages>1151--1154</pages>
<location>Sydney.</location>
<contexts>
<context position="1664" citStr="Krahmer and Theune (1998" startWordPosition="247" endWordPosition="250">lve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph perspective has four main advantages. (1) There are many attractive and well-understood algorithms for dealing with graph structures (see, e.g., Gibb</context>
</contexts>
<marker>Krahmer, Theune, 1998</marker>
<rawString>Krahmer, Emiel and Mari¨et Theune. 1998. Context sensitive generation of descriptions. In Proceedings of the Fifth International Conference on Spoken Language Processing (ICSLP’98), pages 1151–1154, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Mari¨et Theune</author>
</authors>
<title>Efficient context-sensitive generation of descriptions.</title>
<date>2002</date>
<booktitle>Information Sharing: Givenness and Newness in Language Processing. CSLI Publications,</booktitle>
<pages>223--264</pages>
<editor>In Kees van Deemter and Rodger Kibble, editors,</editor>
<location>Stanford, California,</location>
<contexts>
<context position="12826" citStr="Krahmer and Theune (2002)" startWordPosition="2038" endWordPosition="2041">facilitate this we extend the scene description with a set of relations R = { left of, right of, contain, in }. A few algorithms have been developed that address the issue of relational descriptions. The earliest is from Dale and Haddock (1992), who offer an extension of the full brevity algorithm. The Dale and Haddock algorithm has a problem with infinite recursions; it may produce descriptions like “the dog in the doghouse that contains a dog that is inside a doghouse....” Dale and Haddock, somewhat ad hoc, solve this problem by stipulating that a property or relation may be used only once. Krahmer and Theune (2002) (see also Theune [2000]) describe an extension of the incremental algorithm that allows for relational descriptions. Their extension suffers from what may be called the problem of forced incrementality: When a first relation fails to rule out all remaining distractors, additional relations will be tried incrementally. Although it could be argued that incremental selection of properties is psychologically plausible, it seems less plausible for relations. It is unlikely that someone would describe an 56 Krahmer, van Erk, and Verleg Graph-Based Generation Figure 3 A graph representation of the s</context>
<context position="43802" citStr="Krahmer and Theune (2002)" startWordPosition="7436" endWordPosition="7439">th π.v = w and there is no w&apos; E G\W such that (v, H) refers to (w&apos;, G). Observe that the singular case defined in Section 4 is obtained by restricting W to singleton sets. In this way, the basic algorithm can generate both singular and plural distinguishing descriptions. 5.3.2 Context and Salience. An object that has been mentioned in recent context is linguistically salient and hence can often be referred to using fewer properties; an animal that is first described as “the large black dog with the hanging ears” may subsequently be referred to using an anaphoric description such as “the dog.” Krahmer and Theune (2002) model this phenomenon by assigning salience weights (sws) to objects. For this purpose they use a version of centering theory (Grosz, Joshi, and Weinstein 1995) augmented with a recency effect essentially due to Haji˘cov´a (1993). Krahmer and Theune then define the set of distractors as the set of objects with a salience weight higher than or equal to that of the target object. In terms of the graphtheoretical framework, this would go as follows. First, we assign salience weights to the vertices in the scene graph using the salience weights definition proposed by Krahmer and Theune. Subsequen</context>
</contexts>
<marker>Krahmer, Theune, 2002</marker>
<rawString>Krahmer, Emiel and Mari¨et Theune. 2002. Efficient context-sensitive generation of descriptions. In Kees van Deemter and Rodger Kibble, editors, Information Sharing: Givenness and Newness in Language Processing. CSLI Publications, Stanford, California, pages 223–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Sebastiaan van Erk</author>
<author>Andre´ Verleg</author>
</authors>
<title>A meta-algorithm for the generation of referring expressions.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighth European Workshop on Natural Language Generation (EWNLG’01),</booktitle>
<location>Toulouse.</location>
<marker>Krahmer, van Erk, Verleg, 2001</marker>
<rawString>Krahmer, Emiel, Sebastiaan van Erk, and Andre´ Verleg. 2001. A meta-algorithm for the generation of referring expressions. In Proceedings of the Eighth European Workshop on Natural Language Generation (EWNLG’01), Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>The practical value of n-grams in generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the Ninth International Workshop on Natural Language Generation (INLG’98),</booktitle>
<pages>248--255</pages>
<location>Niagara-on-the-Lake, Ontario.</location>
<contexts>
<context position="3855" citStr="Langkilde and Knight (1998)" startWordPosition="566" endWordPosition="569">lts from one algorithm to another. (3) The graph perspective provides a clean solution for some problems that have plagued earlier algorithms. For instance, the generation of relational expressions (i.e., referring expressions that include references to other objects) is enhanced by the fact that both properties and relations are formalized in the same way, namely, as edges in a graph. (4) The combined use of graphs and cost functions paves the way for a natural integration of traditional rule-based approaches to generating referring expressions and more recent statistical approaches, such as Langkilde and Knight (1998) and Malouf (2000), in a single algorithm. The outline of this article is as follows. In Section 2 the content selection problem for generating referring expressions is explained, and some well-known solutions to the problem are discussed. In Section 3, we describe how scenes can be modeled as labeled directed graphs and show how content selection can be formalized as a subgraph construction problem. Section 4 contains a sketch of the basic generation algorithm, which is illustrated with a worked example. In Section 5 various ways to formalize cost functions are discussed and compared. We end </context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde, Irene and Kevin Knight. 1998. The practical value of n-grams in generation. In Proceedings of the Ninth International Workshop on Natural Language Generation (INLG’98), pages 248–255, Niagara-on-the-Lake, Ontario.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annegret Liebers</author>
</authors>
<title>Planarizing graphs.</title>
<date>2001</date>
<journal>Journal of Graph Algorithms and Applications,</journal>
<volume>5</volume>
<issue>1</issue>
<marker>Liebers, 2001</marker>
<rawString>Liebers, Annegret. 2001. Planarizing graphs. Journal of Graph Algorithms and Applications, 5(1):1–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Malouf</author>
</authors>
<title>The order of prenominal adjectives in natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL’00),</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="3873" citStr="Malouf (2000)" startWordPosition="571" endWordPosition="572">r. (3) The graph perspective provides a clean solution for some problems that have plagued earlier algorithms. For instance, the generation of relational expressions (i.e., referring expressions that include references to other objects) is enhanced by the fact that both properties and relations are formalized in the same way, namely, as edges in a graph. (4) The combined use of graphs and cost functions paves the way for a natural integration of traditional rule-based approaches to generating referring expressions and more recent statistical approaches, such as Langkilde and Knight (1998) and Malouf (2000), in a single algorithm. The outline of this article is as follows. In Section 2 the content selection problem for generating referring expressions is explained, and some well-known solutions to the problem are discussed. In Section 3, we describe how scenes can be modeled as labeled directed graphs and show how content selection can be formalized as a subgraph construction problem. Section 4 contains a sketch of the basic generation algorithm, which is illustrated with a worked example. In Section 5 various ways to formalize cost functions are discussed and compared. We end with some concludi</context>
<context position="11266" citStr="Malouf (2000)" startWordPosition="1776" endWordPosition="1777">has a polynomial complexity), but Dale and Reiter (1995, page 248) also claim that it is “psychologically realistic.” They point out that sometimes people may describe an object as “the white bird” even though the simpler “the bird” would have been sufficient (cf. Pechmann [1989]; see, however, Krahmer and Theune [2002] for discussion). Even though there are various useful and interesting algorithms for the generation of referring expressions, a number of open questions remain. Recently there has been an increased interest in statistical approaches to natural language generation. For example, Malouf (2000) has shown that large corpora can be used to determine the order of realization of sequences of prenominal adjectives. It is unclear how such statistical work on generation can be combined with older, rule-based work such as the algorithms just discussed. In addition, many algorithms still have difficulties with the generation of relational descriptions (descriptions that include references to other objects to single out the target object from its distractors). To illustrate the problem, consider the scene depicted in Figure 2. In this scene we again see a finite domain of entities D with cert</context>
</contexts>
<marker>Malouf, 2000</marker>
<rawString>Malouf, Rob. 2000. The order of prenominal adjectives in natural language generation. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL’00), Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno T Messmer</author>
<author>Horst Bunke</author>
</authors>
<title>Subgraph isomorphism in polynomial time.</title>
<date>1995</date>
<tech>Technical Report IAM 95-003,</tech>
<institution>University of Bern, Institute of Computer Science and Applied Mathematics,</institution>
<location>Bern,</location>
<contexts>
<context position="22771" citStr="Messmer and Bunke (1995" startWordPosition="3837" endWordPosition="3840">r, as is any graph with only four vertices). In general, there is no a priori reason to assume that our scene representations will be planar. Yet every nonplanar graph can be modified into a closely related planar one. We briefly address planarization of scene graphs in the Appendix. A final alternative is worth mentioning. The general approach to the problem of subgraph isomorphism detection assumes that both graphs are given on-line. For our current purposes, however, it may happen that the scene graph is fixed and known beforehand, and only the referring graph is unknown and given on-line. Messmer and Bunke (1995, 1998) describe a method that converts the known graph (or model graph, as they call it) into a decision tree. At run time, the input graph is classified by the decision tree, which detects subgraph isomorphisms. The disadvantage of this approach is that the decision tree may contain, in the worst case, an exponential number of nodes. But the main advantage is that the complexity of the new subgraph isomorphism algorithm is only quadratic in the number of vertices of the input referring graph. Note that with this approach we do not lose information from the scene graph, nor do we lose complet</context>
</contexts>
<marker>Messmer, Bunke, 1995</marker>
<rawString>Messmer, Bruno T. and Horst Bunke. 1995. Subgraph isomorphism in polynomial time. Technical Report IAM 95-003, University of Bern, Institute of Computer Science and Applied Mathematics, Bern, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno T Messmer</author>
<author>Horst Bunke</author>
</authors>
<title>A new algorithm for error-tolerant subgraph isomorphism detection.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<issue>5</issue>
<marker>Messmer, Bunke, 1998</marker>
<rawString>Messmer, Bruno T. and Horst Bunke. 1998. A new algorithm for error-tolerant subgraph isomorphism detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(5):493–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Pechmann</author>
</authors>
<title>Incremental speech production and referential overspecification.</title>
<date>1989</date>
<journal>Linguistics,</journal>
<pages>27--98</pages>
<marker>Pechmann, 1989</marker>
<rawString>Pechmann, Thomas. 1989. Incremental speech production and referential overspecification. Linguistics, 27:98–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald C Read</author>
<author>Derek G Corneil</author>
</authors>
<title>The graph isomorphism disease.</title>
<date>1977</date>
<journal>Journal of Graph Theory,</journal>
<volume>1</volume>
<issue>1</issue>
<marker>Read, Corneil, 1977</marker>
<rawString>Read, Ronald C. and Derek G. Corneil. 1977. The graph isomorphism disease. Journal of Graph Theory, 1(1):339–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>The computational complexity of avoiding conversational implicatures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics (ACL’90),</booktitle>
<pages>97--104</pages>
<contexts>
<context position="1535" citStr="Reiter (1990)" startWordPosition="229" endWordPosition="230">phs, and this enhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph perspective has four</context>
<context position="39402" citStr="Reiter (1990)" startWordPosition="6707" endWordPosition="6708">m should terminate as soon as it has found a distinguishing graph. This would guarantee that bargain type loops are always included, and the algorithm would output (iii) from Figure 4 instead of (iii) from Figure 7 when applied to d1 from Figure 3. Another characteristic property of the original incremental algorithm (not discussed in section 2) is the use of a subsumption hierarchy. A chihuahua, for instance, can be referred to as either a chihuahua or a dog. The latter has a special status and is called the basic level value (see, e.g., Rosch [1978]). According to Dale and Reiter (1995) and Reiter (1990), human speakers have a general preference for basic level values and move to more specific (subsumed) values only if these are more informative. This notion of a subsumption hierarchy can be modeled using the cost function. For a given attribute, the basic level edges should be assigned the lowest costs, and those farthest away from the basic level edge should have the highest costs. This implies that adding an edge labeled dog is cheaper than adding an edge labeled chihuahua. Hence a chihuahua edge will be selected only when there are fewer (or less expensive) additional edges required to co</context>
</contexts>
<marker>Reiter, 1990</marker>
<rawString>Reiter, Ehud. 1990. The computational complexity of avoiding conversational implicatures. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics (ACL’90), pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleanor Rosch</author>
</authors>
<title>Principles of categorization.</title>
<date>1978</date>
<booktitle>Cognition and Categorization. Lawrence Erlbaum,</booktitle>
<pages>27--48</pages>
<editor>In E. Rosch and B. Lloyd, editors,</editor>
<location>Hillsdale, New Jersey,</location>
<marker>Rosch, 1978</marker>
<rawString>Rosch, Eleanor. 1978. Principles of categorization. In E. Rosch and B. Lloyd, editors, Cognition and Categorization. Lawrence Erlbaum, Hillsdale, New Jersey, pages 27–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Sedgewick</author>
</authors>
<date>1988</date>
<publisher>Algorithms. Addison-Wesley,</publisher>
<location>Reading, Massachusetts,</location>
<note>second edition.</note>
<marker>Sedgewick, 1988</marker>
<rawString>Sedgewick, Robert. 1988. Algorithms. Addison-Wesley, Reading, Massachusetts, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
<author>Bonnie Webber</author>
</authors>
<title>Textual economy through close coupling of syntax and semantics.</title>
<date>1998</date>
<booktitle>In Proceedings of the Ninth International Workshop on Natural Language Generation (INLG’98),</booktitle>
<contexts>
<context position="1638" citStr="Stone and Webber (1998)" startWordPosition="243" endWordPosition="246">rspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph perspective has four main advantages. (1) There are many attractive and well-understood algorithms for dealing with graph s</context>
<context position="5264" citStr="Stone and Webber 1998" startWordPosition="777" endWordPosition="780">erring expressions, each with its own objectives: Some aim at producing the shortest possible description (e.g., Dale’s [1992] full brevity algorithm), others focus on psychological realism (e.g., Dale and Reiter’s [1995] incremental algorithm) or realistic output (e.g., Horacek 1997). The degree of detail in which the various algorithms are described differs considerably. Some algorithms are fully formalized and come with explicit characterizations of their complexity (e.g., Dale and Reiter 1995; van Deemter 2000); others are more conceptual and concentrate on exploring new directions (e.g., Stone and Webber 1998). Despite such differences, most algorithms deal with the same problem definition. They take as input a single object v (the target object) for which a referring expression is to be generated and a set of objects (the distractors) from which the target object needs to be distinguished (we use the terminology from Dale and Reiter [1995]). The task of the algorithm is to determine which set of properties is needed to single out the target object v from the distractors. This is known as the content determination problem for referring expressions. On the basis of this set of properties a distingui</context>
</contexts>
<marker>Stone, Webber, 1998</marker>
<rawString>Stone, Matthew and Bonnie Webber. 1998. Textual economy through close coupling of syntax and semantics. In Proceedings of the Ninth International Workshop on Natural Language Generation (INLG’98),</rawString>
</citation>
<citation valid="false">
<pages>178--187</pages>
<location>Niagara-on-the-Lake, Ontario.</location>
<marker></marker>
<rawString>pages 178–187, Niagara-on-the-Lake, Ontario.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Tarjan</author>
</authors>
<title>Depth-first search and linear time algorithms.</title>
<date>1972</date>
<journal>SIAM Journal on Computing,</journal>
<volume>1</volume>
<issue>2</issue>
<marker>Tarjan, 1972</marker>
<rawString>Tarjan, Robert E. 1972. Depth-first search and linear time algorithms. SIAM Journal on Computing, 1(2):146–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari¨et Theune</author>
</authors>
<title>From Data to Speech: Language Generation in Context.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Eindhoven University of Technology,</institution>
<location>Eindhoven, The Netherlands.</location>
<marker>Theune, 2000</marker>
<rawString>Theune, Mari¨et. 2000. From Data to Speech: Language Generation in Context. Ph.D. thesis, Eindhoven University of Technology, Eindhoven, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
</authors>
<title>Generating vague descriptions.</title>
<date>2000</date>
<booktitle>In Proceedings of the First International Natural Language Generation Conference (INLG’00),</booktitle>
<location>Mitzpe Ramon.</location>
<marker>van Deemter, 2000</marker>
<rawString>van Deemter, Kees. 2000. Generating vague descriptions. In Proceedings of the First International Natural Language Generation Conference (INLG’00), Mitzpe Ramon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
</authors>
<title>Generating referring expressions: Boolean extensions of the incremental algorithm.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<marker>van Deemter, 2002</marker>
<rawString>van Deemter, Kees. 2002. Generating referring expressions: Boolean extensions of the incremental algorithm. Computational Linguistics, 28(1):37–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin J Wilson</author>
</authors>
<title>Introduction to Graph Theory.</title>
<date>1996</date>
<location>Longman, Harlow, England,</location>
<note>fourth edition.</note>
<marker>Wilson, 1996</marker>
<rawString>Wilson, Robin J. 1996. Introduction to Graph Theory. Longman, Harlow, England, fourth edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>