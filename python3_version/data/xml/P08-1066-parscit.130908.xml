<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9996455">
A New String-to-Dependency Machine Translation Algorithm
with a Target Dependency Language Model
</title>
<author confidence="0.88529">
Libin Shen
</author>
<affiliation confidence="0.612644">
BBN Technologies
</affiliation>
<address confidence="0.678869">
Cambridge, MA 02138, USA
</address>
<email confidence="0.994004">
lshen@bbn.com
</email>
<author confidence="0.790456">
Jinxi Xu
</author>
<affiliation confidence="0.547659">
BBN Technologies
</affiliation>
<address confidence="0.706755">
Cambridge, MA 02138, USA
</address>
<email confidence="0.994948">
jxu@bbn.com
</email>
<author confidence="0.83322">
Ralph Weischedel
</author>
<affiliation confidence="0.576504">
BBN Technologies
</affiliation>
<address confidence="0.705367">
Cambridge, MA 02138, USA
</address>
<email confidence="0.997039">
weisched@bbn.com
</email>
<sectionHeader confidence="0.993834" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999343153846154">
In this paper, we propose a novel string-to-
dependency algorithm for statistical machine
translation. With this new framework, we em-
ploy a target dependency language model dur-
ing decoding to exploit long distance word
relations, which are unavailable with a tra-
ditional n-gram language model. Our ex-
periments show that the string-to-dependency
decoder achieves 1.48 point improvement in
BLEU and 2.53 point improvement in TER
compared to a standard hierarchical string-to-
string system on the NIST 04 Chinese-English
evaluation set.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99969441025641">
In recent years, hierarchical methods have been suc-
cessfully applied to Statistical Machine Translation
(Graehl and Knight, 2004; Chiang, 2005; Ding and
Palmer, 2005; Quirk et al., 2005). In some language
pairs, i.e. Chinese-to-English translation, state-of-
the-art hierarchical systems show significant advan-
tage over phrasal systems in MT accuracy. For ex-
ample, Chiang (2007) showed that the Hiero system
achieved about 1 to 3 point improvement in BLEU
on the NIST 03/04/05 Chinese-English evaluation
sets compared to a start-of-the-art phrasal system.
Our work extends the hierarchical MT approach.
We propose a string-to-dependency model for MT,
which employs rules that represent the source side
as strings and the target side as dependency struc-
tures. We restrict the target side to the so called well-
formed dependency structures, in order to cover a
large set of non-constituent transfer rules (Marcu et
al., 2006), and enable efficient decoding through dy-
namic programming. We incorporate a dependency
language model during decoding, in order to exploit
long-distance word relations which are unavailable
with a traditional n-gram language model on target
strings.
For comparison purposes, we replicated the Hiero
decoder (Chiang, 2005) as our baseline. Our string-
to-dependency decoder shows 1.48 point improve-
ment in BLEU and 2.53 point improvement in TER
on the NIST 04 Chinese-English MT evaluation set.
In the rest of this section, we will briefly dis-
cuss previous work on hierarchical MT and de-
pendency representations, which motivated our re-
search. In section 2, we introduce the model of
string-to-dependency decoding. Section 3 illustrates
of the use of dependency language models. In sec-
tion 4, we describe the implementation details of our
MT system. We discuss experimental results in sec-
tion 5, compare to related work in section 6, and
draw conclusions in section 7.
</bodyText>
<subsectionHeader confidence="0.993766">
1.1 Hierarchical Machine Translation
</subsectionHeader>
<bodyText confidence="0.9925552">
Graehl and Knight (2004) proposed the use of target-
tree-to-source-string transducers (xRS) to model
translation. In xRS rules, the right-hand-side(rhs)
of the target side is a tree with non-terminals(NTs),
while the rhs of the source side is a string with
NTs. Galley et al. (2006) extended this string-to-tree
model by using Context-Free parse trees to represent
the target side. A tree could represent multi-level
transfer rules.
The Hiero decoder (Chiang, 2007) does not re-
quire explicit syntactic representation on either side
of the rules. Both source and target are strings with
NTs. Decoding is solved as chart parsing. Hiero can
be viewed as a hierarchical string-to-string model.
Ding and Palmer (2005) and Quirk et al. (2005)
</bodyText>
<page confidence="0.982446">
577
</page>
<note confidence="0.687367">
Proceedings ofACL-08: HLT, pages 577–585,
</note>
<page confidence="0.516052">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<figure confidence="0.996519833333333">
boy
find
will
it
interesting
the
</figure>
<figureCaption confidence="0.9822315">
Figure 1: The dependency tree for sentence the boy will
find it interesting
</figureCaption>
<bodyText confidence="0.9999845">
followed the tree-to-tree approach (Shieber and Sch-
abes, 1990) for translation. In their models, depen-
dency treelets are used to represent both the source
and the target sides. Decoding is implemented as
tree transduction preceded by source side depen-
dency parsing. While tree-to-tree models can rep-
resent richer structural information, existing tree-to-
tree models did not show advantage over string-to-
tree models on translation accuracy due to a much
larger search space.
One of the motivations of our work is to achieve
desirable trade-off between model capability and
search space through the use of the so called well-
formed dependency structures in rule representation.
</bodyText>
<subsectionHeader confidence="0.978453">
1.2 Dependency Trees
</subsectionHeader>
<bodyText confidence="0.999924333333333">
Dependency trees reveal long-distance relations be-
tween words. For a given sentence, each word has a
parent word which it depends on, except for the root
word.
Figure 1 shows an example of a dependency tree.
Arrows point from the child to the parent. In this
example, the word find is the root.
Dependency trees are simpler in form than CFG
trees since there are no constituent labels. However,
dependency relations directly model semantic struc-
ture of a sentence. As such, dependency trees are a
desirable prior model of the target sentence.
</bodyText>
<subsectionHeader confidence="0.9628585">
1.3 Motivations for Well-Formed Dependency
Structures
</subsectionHeader>
<bodyText confidence="0.999632333333333">
We restrict ourselves to the so-called well-formed
target dependency structures based on the following
considerations.
</bodyText>
<subsectionHeader confidence="0.980359">
Dynamic Programming
</subsectionHeader>
<bodyText confidence="0.999851733333333">
In (Ding and Palmer, 2005; Quirk et al., 2005),
there is no restriction on dependency treelets used in
transfer rules except for the size limit. This may re-
sult in a high dimensionality in hypothesis represen-
tation and make it hard to employ shared structures
for efficient dynamic programming.
In (Galley et al., 2004), rules contain NT slots and
combination is only allowed at those slots. There-
fore, the search space becomes much smaller. Fur-
thermore, shared structures can be easily defined
based on the labels of the slots.
In order to take advantage of dynamic program-
ming, we fixed the positions onto which another an-
other tree could be attached by specifying NTs in
dependency trees.
</bodyText>
<subsectionHeader confidence="0.981388">
Rule Coverage
</subsectionHeader>
<bodyText confidence="0.9999512">
Marcu et al. (2006) showed that many useful
phrasal rules cannot be represented as hierarchical
rules with the existing representation methods, even
with composed transfer rules (Galley et al., 2006).
For example, the following rule
</bodyText>
<listItem confidence="0.721903666666667">
• &lt;(hong)Chinese, (DT(the) JJ(red))English&gt;
is not a valid string-to-tree transfer rule since the red
is a partial constituent.
</listItem>
<bodyText confidence="0.999641055555556">
A number of techniques have been proposed to
improve rule coverage. (Marcu et al., 2006) and
(Galley et al., 2006) introduced artificial constituent
nodes dominating the phrase of interest. The bi-
narization method used by Wang et al. (2007) can
cover many non-constituent rules also, but not all of
them. For example, it cannot handle the above ex-
ample. DeNeefe et al. (2007) showed that the best
results were obtained by combing these methods.
In this paper, we use well-formed dependency
structures to handle the coverage of non-constituent
rules. The use of dependency structures is due to the
flexibility of dependency trees as a representation
method which does not rely on constituents (Fox,
2002; Ding and Palmer, 2005; Quirk et al., 2005).
The well-formedness of the dependency structures
enables efficient decoding through dynamic pro-
gramming.
</bodyText>
<sectionHeader confidence="0.996911" genericHeader="introduction">
2 String-to-Dependency Translation
</sectionHeader>
<subsectionHeader confidence="0.978753">
2.1 Transfer Rules with Well-Formed
Dependency Structures
</subsectionHeader>
<bodyText confidence="0.8274715">
A string-to-dependency grammar G is a 4-tuple
G =&lt; R, X, Tf, Te &gt;, where R is a set of transfer
rules. X is the only non-terminal, which is similar
to the Hiero system (Chiang, 2007). Tf is a set of
</bodyText>
<page confidence="0.993492">
578
</page>
<bodyText confidence="0.989511291666667">
terminals in the source language, and Te is a set of
terminals in the target language1.
A string-to-dependency transfer rule R E R is a
4-tuple R =&lt; 5f, 5e, D, A &gt;, where 5f E (Tf U
{X})+ is a source string, 5e E (Te U {X})+ is a
target string, D represents the dependency structure
for 5e, and A is the alignment between 5f and 5e.
Non-terminal alignments in A must be one-to-one.
In order to exclude undesirable structures, we
only allow 5e whose dependency structure D is
well-formed, which we will define below. In addi-
tion, the same well-formedness requirement will be
applied to partial decoding results. Thus, we will be
able to employ shared structures to merge multiple
partial results.
Based on the results in previous work (DeNeefe
et al., 2007), we want to keep two kinds of depen-
dency structures. In one kind, we keep dependency
trees with a sub-root, where all the children of the
sub-root are complete. We call them fixed depen-
dency structures because the head is known or fixed.
In the other, we keep dependency structures of sib-
ling nodes of a common head, but the head itself is
unspecified or floating. Each of the siblings must be
a complete constituent. We call them floating de-
pendency structures. Floating structures can repre-
sent many linguistically meaningful non-constituent
structures: for example, like the red, a modifier of
a noun. Only those two kinds of dependency struc-
tures are well-formed structures in our system.
Furthermore, we operate over well-formed struc-
tures in a bottom-up style in decoding. However,
the description given above does not provide a clear
definition on how to combine those two types of
structures. In the rest of this section, we will pro-
vide formal definitions of well-formed structures and
combinatory operations over them, so that we can
easily manipulate well-formed structures in decod-
ing. Formal definitions also allow us to easily ex-
tend the framework to incorporate a dependency lan-
guage model in decoding. Examples will be pro-
vided along with the formal definitions.
Consider a sentence 5 = w1w2...wn. Let
d1d2...dn represent the parent word IDs for each
word. For example, d4 = 2 means that w4 depends
&apos;We ignore the left hand side here because there is only one
non-terminal X. Of course, this formalism can be extended to
have multiple NTs.
</bodyText>
<figure confidence="0.9795305">
find find
boy will it boy
the
(a) (b) (c)
</figure>
<figureCaption confidence="0.923068">
Figure 2: Fixed dependency structures
</figureCaption>
<figure confidence="0.990413">
boy will it interesting
the
(a) (b)
</figure>
<figureCaption confidence="0.8889895">
Figure 3: Floating dependency structures
on w2. If wi is a root, we define di = 0.
</figureCaption>
<bodyText confidence="0.940382">
Definition 1 A dependency structure di..j is fixed
on head h, where h E [i, j], or fixed for short, if
and only if it meets the following conditions
</bodyText>
<listItem confidence="0.999855">
• dh E� [i, j]
• bk E [i, j] and k =� h, dk E [i, j]
• bk E� [i, j], dk = h or dk E� [i, j]
</listItem>
<bodyText confidence="0.96957">
In addition, we say the category of di..j is
(−, h, −), where − means this field is undefined.
</bodyText>
<construct confidence="0.98027475">
Definition 2 A dependency structure di...dj is float-
ing with children C, for a non-empty set C C_
{i, ..., j}, or floating for short, if and only if it meets
the following conditions
</construct>
<listItem confidence="0.999567">
• ]h E� [i, j], s.t.bk E C, dk = h
• bk E [i, j] and k E� C, dk E [i, j]
• bk E� [i,j],dk E� [i, j]
</listItem>
<bodyText confidence="0.999533428571429">
We say the category of di..j is (C, −, −) if j &lt; h,
or (−, −, C) otherwise. A category is composed of
the three fields (A, h, B), where h is used to repre-
sent the head, and A and B are designed to model
left and right dependents of the head respectively.
A dependency structure is well-formed if and
only if it is either fixed or floating.
</bodyText>
<subsectionHeader confidence="0.748571">
Examples
</subsectionHeader>
<bodyText confidence="0.996271333333333">
We can represent dependency structures with
graphs. Figure 2 shows examples of fixed structures,
Figure 3 shows examples of floating structures, and
Figure 4 shows ill-formed dependency structures.
It is easy to verify that the structures in Figures
2 and 3 are well-formed. 4(a) is ill-formed because
</bodyText>
<page confidence="0.986832">
579
</page>
<figure confidence="0.999119375">
the
(a) (b)
will it
interesting
LC
RC
find
LA RA
LA RA
boy
LA
find
boy
find
will
interesting
</figure>
<figureCaption confidence="0.999979">
Figure 4: Ill-formed dependency structures
</figureCaption>
<bodyText confidence="0.9997425">
boy does not have its child word the in the tree. 4(b)
is ill-formed because it is not a continuous segment.
As for the example the red mentioned above, it is
a well-formed floating dependency structure.
</bodyText>
<subsectionHeader confidence="0.9986845">
2.2 Operations on Well-Formed Dependency
Structures and Categories
</subsectionHeader>
<bodyText confidence="0.999958192307693">
One of the purposes of introducing floating depen-
dency structures is that siblings having a common
parent will become a well-defined entity, although
they are not considered a constituent. We always
build well-formed partial structures on the target
side in decoding. Furthermore, we combine partial
dependency structures in a way such that we can ob-
tain all possible well-formed but no ill-formed de-
pendency structures during bottom-up decoding.
The solution is to employ categories introduced
above. Each well-formed dependency structure has
a category. We can apply four combinatory oper-
ations over the categories. If we can combine two
categories with a certain category operation, we can
use a corresponding tree operation to combine two
dependency structures. The category of the com-
bined dependency structure is the result of the com-
binatory category operations.
We first introduce three meta category operations.
Two of them are unary operations, left raising (LR)
and right raising (RR), and one is the binary opera-
tion unification (UF).
First, the raising operations are used to turn a
completed fixed structure into a floating structure.
It is easy to verify the following theorem according
to the definitions.
</bodyText>
<construct confidence="0.9111038">
Theorem 1 A fixed structure with category
(−, h, −) for span [i, j] is also a floating structure
with children {h} if there are no outside words
depending on word h.
dk 0 [i, j], dk =� h. (1)
</construct>
<bodyText confidence="0.6739405">
Therefore we can always raise a fixed structure if we
assume it is complete, i.e. (1) holds.
</bodyText>
<figureCaption confidence="0.825129">
Figure 5: A dependency tree with flexible combination
Definition 3 Meta Category Operations
</figureCaption>
<listItem confidence="0.984784">
• LR((−, h, −)) = ({h}, −, −)
• RR((−, h, −)) = (−, −, {h})
• UF((A1, h1, B1), (A2, h2, B2)) = NORM((A1 u
A2, h1 u h2, B1 u B2))
</listItem>
<bodyText confidence="0.975296833333334">
Unification is well-defined if and only if we can
unify all three elements and the result is a valid fixed
or floating category. For example, we can unify a
fixed structure with a floating structure or two float-
ing structures in the same direction, but we cannot
unify two fixed structures.
</bodyText>
<equation confidence="0.8989272">
h1 if h2 = −
h2 if h1 = −
undefined otherwise
A1 if A2 = −
A2 if A1 = −
A1 U A2 otherwise
(−, h, −) if h =� −
(A, −, −) if h = −, B = −
(−, −, B) if h = −, A = −
undefined otherwise
</equation>
<bodyText confidence="0.999967578947368">
Next we introduce the four tree operations on de-
pendency structures. Instead of providing the formal
definition, we use figures to illustrate these opera-
tions to make it easy to understand. Figure 1 shows
a traditional dependency tree. Figure 5 shows the
four operations to combine partial dependency struc-
tures, which are left adjoining (LA), right adjoining
(RA), left concatenation (LC) and right concatena-
tion (RC).
Child and parent subtrees can be combined with
adjoining which is similar to the traditional depen-
dency formalism. We can either adjoin a fixed struc-
ture or a floating structure to the head of a fixed
structure.
Complete siblings can be combined via concate-
nation. We can concatenate two fixed structures, one
fixed structure with one floating structure, or two
floating structures in the same direction. The flex-
ibility of the order of operation allows us to take ad-
</bodyText>
<figure confidence="0.5672675">
�
�h1 u h2 = �
A1 u A2 = I
NORM((A, h, B)) = �
�� �
���
</figure>
<page confidence="0.728226">
580
</page>
<figureCaption confidence="0.844432">
Figure 6: Operations over well-formed structures
vantage of various translation fragments encoded in
transfer rules.
Figure 6 shows alternative ways of applying op-
erations on well-formed structures to build larger
structures in a bottom-up style. Numbers represent
the order of operation.
</figureCaption>
<bodyText confidence="0.999434">
We use the same names for the operations on cat-
egories for the sake of convenience. We can easily
use the meta category operations to define the four
combinatory operations. The definition of the oper-
ations in the left direction is as follows. Those in the
right direction are similar.
</bodyText>
<equation confidence="0.961519153846154">
Definition 4 Combinatory category operations
LA((A1, −, −), (−, h2, −))
= UF((A1, −, −), (−, h2, −))
LA((−, h1, −), (−, h2, −))
= UF(LR((−, h1, −)), (− , h2, −))
LC((A1, −, −), (A2, −, −))
= UF((A1, −, −), (A2, −, −))
LC((A1, −, −), (−, h2, −))
= UF((A1, −, −), LR((−, h2, −)))
LC((−, h1, −), (A2, −, −))
= UF(LR((−, h1, −)), (A2, −, −))
LC((−, h1, −), (−, h2, −))
= UF(LR((−, h1, −)), LR((−, h2, −)))
</equation>
<bodyText confidence="0.9991528">
It is easy to verify the soundness and complete-
ness of category operations based on one-to-one
mapping of the conditions in the definitions of cor-
responding operations on dependency structures and
on categories.
</bodyText>
<construct confidence="0.8967376">
Theorem 2 (soundness and completeness)
Suppose X and Y are well-formed dependency
structures. OP(cat(X), cat(Y)) is well-defined for
a given operation OP if and only if OP(X, Y ) is
well-defined. Furthermore,
</construct>
<equation confidence="0.917657">
cat(OP(X, Y )) = OP(cat(X), cat(Y))
</equation>
<bodyText confidence="0.995060277777778">
Suppose we have a dependency tree for a red apple,
where both a and red depend on apple. There are
two ways to compute the category of this string from
the bottom up.
cat(Da red apple)
= LA(cat(Da),LA(cat(Dred),cat(Dapple)))
= LA(LC(cat(Da), cat(Dred)), cat(Dapple))
Based on Theorem 2, it follows that combinatory
operation of categories has the confluence property,
since the result dependency structure is determined.
Corollary 1 (confluence) The category of a well-
formed dependency tree does not depend on the or-
der of category calculation.
With categories, we can easily track the types of
dependency structures and constrain operations in
decoding. For example, we have a rule with depen-
dency structure find +— X, where X right adjoins
to find. Suppose we have two floating structures2,
</bodyText>
<equation confidence="0.999904">
cat(X1) = ({he, will}, −, −)
cat(X2) = (−, −, {it, interesting})
</equation>
<bodyText confidence="0.9998035">
We can replace X by X2, but not by X1 based on
the definition of category operations.
</bodyText>
<subsectionHeader confidence="0.997488">
2.3 Rule Extraction
</subsectionHeader>
<bodyText confidence="0.999977307692308">
Now we explain how we get the string-to-
dependency rules from training data. The procedure
is similar to (Chiang, 2007) except that we maintain
tree structures on the target side, instead of strings.
Given sentence-aligned bi-lingual training data,
we first use GIZA++ (Och and Ney, 2003) to gen-
erate word level alignment. We use a statistical CFG
parser to parse the English side of the training data,
and extract dependency trees with Magerman’s rules
(1995). Then we use heuristic rules to extract trans-
ferrules recursively based on the GIZA alignment
and the target dependency trees. The rule extraction
procedure is as follows.
</bodyText>
<listItem confidence="0.736392">
1. Initialization:
</listItem>
<bodyText confidence="0.591615">
All the 4-tuples (P?&apos;,�
</bodyText>
<equation confidence="0.544755">
� , P�,n
� , D, A) are valid
</equation>
<bodyText confidence="0.569461">
phrase alignments, where source phrase P ?&apos;,� �is
</bodyText>
<footnote confidence="0.9646565">
2Here we use words instead of word indexes in categories to
make the example easy to understand.
</footnote>
<figure confidence="0.966046695652174">
(a) (b)
the
LA
boy
3 2
1
LA
will
LA
find
the
LA
3
boy
1
LC
will
LA
find
2
581
find
find
</figure>
<figureCaption confidence="0.9851745">
Figure 7: Replacing it with X in Dl
aligned to target phrase P m,n
</figureCaption>
<bodyText confidence="0.9912592">
e under alignment3
A, and D, the dependency structure for P m,n
e ,
is well-formed. All valid phrase templates are
valid rules templates.
</bodyText>
<sectionHeader confidence="0.89041" genericHeader="method">
2. Inference:
</sectionHeader>
<bodyText confidence="0.9566943125">
Let (Pi,j
f , Pm,n
e , D1, A) be a valid rule tem-
plate, and (Pp,q
f , Ps,t
e , D2, A) a valid phrase
alignment, where [p, q] C [i, j], [s, t] C [m, n],
D2 is a sub-structure of D1, and at least one
word in Pi,j
f but not in Pp,q
f is aligned.
We create a new valid rule template
(P0 f, P0e, D0, A), where we obtain Pf0 by
replacing Pp,q
f with label X in Pi,j
f , and obtain
</bodyText>
<figure confidence="0.501278333333333">
Pe0 by replacing P s,t
e with X in Pm,n
e . Further-
</figure>
<figureCaption confidence="0.959452333333333">
more, We obtain D0 by replacing sub-structure
D2 with X in D14. An example is shown in
Figure 7.
</figureCaption>
<bodyText confidence="0.999901666666667">
Among all valid rule templates, we collect those
that contain at most two NTs and at most seven ele-
ments in the source as transfer rules in our system.
</bodyText>
<subsectionHeader confidence="0.963367">
2.4 Decoding
</subsectionHeader>
<bodyText confidence="0.999686">
Following previous work on hierarchical MT (Chi-
ang, 2005; Galley et al., 2006), we solve decoding
as chart parsing. We view target dependency as the
hidden structure of source fragments.
The parser scans all source cells in a bottom-up
style, and checks matched transfer rules according to
the source side. Once there is a completed rule, we
build a larger dependency structure by substituting
component dependency structures for corresponding
NTs in the target dependency structure of rules.
Hypothesis dependency structures are organized
in a shared forest, or AND-OR structures. An AND-
</bodyText>
<subsectionHeader confidence="0.723405">
3By Pi,j
</subsectionHeader>
<bodyText confidence="0.716337333333333">
f aligned to Pm,n
e , we mean all words in Pi,j
f are
either aligned to words in Pm,n
e or unaligned, and vice versa.
Furthermore, at least one word in Pi,j
</bodyText>
<footnote confidence="0.8279568">
f is aligned to a word in
Pm,n .
e
4If D2 is a floating structure, we need to merge several
dependency links into one.
</footnote>
<bodyText confidence="0.994549105263158">
structure represents an application of a rule over
component OR-structures, and an OR-structure rep-
resents a set of alternative AND-structures with the
same state. A state means a n-tuple that character-
izes the information that will be inquired by up-level
AND-structures.
Supposing we use a traditional tri-gram language
model in decoding, we need to specify the leftmost
two words and the rightmost two words in a state.
Since we only have a single NT X in the formalism
described above, we do not need to add the NT la-
bel in states. However, we need to specify one of
the three types of the dependency structure: fixed,
floating on the left side, or floating on the right side.
This information is encoded in the category of the
dependency structure.
In the next section, we will explain how to ex-
tend categories and states to exploit a dependency
language model during decoding.
</bodyText>
<sectionHeader confidence="0.995607" genericHeader="method">
3 Dependency Language Model
</sectionHeader>
<bodyText confidence="0.9970445">
For the dependency tree in Figure 1, we calculate the
probability of the tree as follows
</bodyText>
<equation confidence="0.988329">
Prob = PT (find)
</equation>
<bodyText confidence="0.893870454545455">
×PL(will|find-as-head)
×PL(boy|will, find-as-head)
×PL(the|boy-as-head)
×PR(it|find-as-head)
×PR(interesting|it, find-as-head)
Here PT(x) is the probability that word x is the
root of a dependency tree. PL and PR are left and
right side generative probabilities respectively. Let
wh be the head, and wL1wL2...wLn be the children
on the left side from the nearest to the farthest. Sup-
pose we use a tri-gram dependency LM,
</bodyText>
<equation confidence="0.9971635">
PL(wL1wL2...wLn|wh-as-head)
= PL(wL1|wh-as-head)
×PL(wL2|wL1, wh-as-head)
×... × PL(wLn|wLn−1, wLn−2) (2)
</equation>
<bodyText confidence="0.9998046">
wh-as-head represents wh used as the head, and
it is different from wh in the dependency language
model. The right side probability is similar.
In order to calculate the dependency language
model score, or depLM score for short, on the fly for
</bodyText>
<figure confidence="0.6765896">
it
it
interesting
X interesting
(D’)
</figure>
<page confidence="0.992636">
582
</page>
<bodyText confidence="0.999863444444444">
partial hypotheses in a bottom-up decoding, we need
to save more information in categories and states.
We use a 5-tuple (LF, LN, h, RN, RF) to repre-
sent the category of a dependency structure. h rep-
resents the head. LF and RF represent the farthest
two children on the left and right sides respectively.
Similarly, LN and RN represent the nearest two
children on the left and right sides respectively. The
three types of categories are as follows.
</bodyText>
<listItem confidence="0.999962">
• fixed: (LF, −, h, −, RF)
• floating left: (LF, LN, −, −, −)
• floating right: (−, −, −, RN, RF)
</listItem>
<bodyText confidence="0.9990188">
Similar operations as described in Section 2.2 are
used to keep track of the head and boundary child
nodes which are then used to compute depLM scores
in decoding. Due to the limit of space, we skip the
details here.
</bodyText>
<sectionHeader confidence="0.695495" genericHeader="method">
4 Implementation Details
Features
</sectionHeader>
<listItem confidence="0.998308444444444">
1. Probability of the source side given the target
side of a rule
2. Probability of the target side given the source
side of a rule
3. Word alignment probability
4. Number of target words
5. Number of concatenation rules used
6. Language model score
7. Dependency language model score
</listItem>
<bodyText confidence="0.9753099">
8. Discount on ill-formed dependency structures
We have eight features in our system. The values of
the first four features are accumulated on the rules
used in a translation. Following (Chiang, 2005),
we also use concatenation rules like X —* XX for
backup. The 5th feature counts the number of con-
catenation rules used in a translation. In our sys-
tem, we allow substitutions of dependency struc-
tures with unmatched categories, but there is a dis-
count for such substitutions.
</bodyText>
<subsectionHeader confidence="0.935905">
Weight Optimization
</subsectionHeader>
<bodyText confidence="0.9420881">
We tune the weights with several rounds of
decoding-optimization. Following (Och, 2003), the
k-best results are accumulated as the input of the op-
timizer. Powell’s method is used for optimization
with 20 random starting points around the weight
vector of the last iteration.
Rescoring
We rescore 1000-best translations (Huang and
Chiang, 2005) by replacing the 3-gram LM score
with the 5-gram LM score computed offline.
</bodyText>
<sectionHeader confidence="0.997211" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.996831">
We carried out experiments on three models.
</bodyText>
<listItem confidence="0.999252285714286">
• baseline: replication of the Hiero system.
• filtered: a string-to-string MT system as in
baseline. However, we only keep the transfer
rules whose target side can be generated by a
well-formed dependency structure.
• str-dep: a string-to-dependency system with a
dependency LM.
</listItem>
<bodyText confidence="0.999986413793103">
We take the replicated Hiero system as our
baseline because it is the closest to our string-to-
dependency model. They have similar rule extrac-
tion and decoding algorithms. Both systems use
only one non-terminal label in rules. The major dif-
ference is in the representation of target structures.
We use dependency structures instead of strings;
thus, the comparison will show the contribution of
using dependency information in decoding.
All models are tuned on BLEU (Papineni et al.,
2001), and evaluated on both BLEU and Translation
Error Rate (TER) (Snover et al., 2006) so that we
could detect over-tuning on one metric.
We used part of the NIST 2006 Chinese-
English large track data as well as some LDC
corpora collected for the DARPA GALE program
(LDC2005E83, LDC2006E34 and LDC2006G05)
as our bilingual training data. It contains about
178M/191M words in source/target. Hierarchical
rules were extracted from a subset which has about
35M/41M words5, and the rest of the training data
were used to extract phrasal rules as in (Och, 2003;
Chiang, 2005). The English side of this subset was
also used to train a 3-gram dependency LM. Tra-
ditional 3-gram and 5-gram LMs were trained on a
corpus of 6G words composed of the LDC Gigaword
corpus and text downloaded from Web (Bulyko et
al., 2007). We tuned the weights on NIST MT05
and tested on MT04.
</bodyText>
<footnote confidence="0.997507">
5It includes eight corpora: LDC2002E18, LDC2003E07,
LDC2004T08 HK News, LDC2005E83, LDC2005T06,
LDC2005T10, LDC2006E34, and LDC2006G05
</footnote>
<page confidence="0.992332">
583
</page>
<table confidence="0.9960295">
Model #Rules
baseline 140M
filtered 26M
str-dep 27M
</table>
<tableCaption confidence="0.993803">
Table 1: Number of transfer rules
</tableCaption>
<table confidence="0.9997796">
Model BLEU% TER%
lower mixed lower mixed
Decoding (3-gram LM)
baseline 38.18 35.77 58.91 56.60
filtered 37.92 35.48 57.80 55.43
str-dep 39.52 37.25 56.27 54.07
Rescoring (5-gram LM)
baseline 40.53 38.26 56.35 54.15
filtered 40.49 38.26 55.57 53.47
str-dep 41.60 39.47 55.06 52.96
</table>
<tableCaption confidence="0.998923">
Table 2: BLEU and TER scores on the test set.
</tableCaption>
<bodyText confidence="0.997719913043479">
Table 1 shows the number of transfer rules ex-
tracted from the training data for the tuning and
test sets. The constraint of well-formed dependency
structures greatly reduced the size of the rule set. Al-
though the rule size increased a little bit after incor-
porating dependency structures in rules, the size of
string-to-dependency rule set is less than 20% of the
baseline rule set size.
Table 2 shows the BLEU and TER scores
on MT04. On decoding output, the string-to-
dependency system achieved 1.48 point improve-
ment in BLEU and 2.53 point improvement in
TER compared to the baseline hierarchical string-
to-string system. After 5-gram rescoring, it achieved
1.21 point improvement in BLEU and 1.19 improve-
ment in TER. The filtered model does not show im-
provement on BLEU. The filtered string-to-string
rules can be viewed the string projection of string-
to-dependency rules. It means that just using depen-
dency structure does not provide an improvement on
performance. However, dependency structures al-
low the use of a dependency LM which gives rise to
significant improvement.
</bodyText>
<sectionHeader confidence="0.999837" genericHeader="discussions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99990368">
The well-formed dependency structures defined here
are similar to the data structures in previous work on
mono-lingual parsing (Eisner and Satta, 1999; Mc-
Donald et al., 2005). However, here we have fixed
structures growing on both sides to exploit various
translation fragments learned in the training data,
while the operations in mono-lingual parsing were
designed to avoid artificial ambiguity of derivation.
Charniak et al. (2003) described a two-step string-
to-CFG-tree translation model which employed a
syntax-based language model to select the best
translation from a target parse forest built in the first
step. Only translation probability P(FIE) was em-
ployed in the construction of the target forest due to
the complexity of the syntax-based LM. Since our
dependency LM models structures over target words
directly based on dependency trees, we can build a
single-step system. This dependency LM can also
be used in hierarchical MT systems using lexical-
ized CFG trees.
The use of a dependency LM in MT is similar to
the use of a structured LM in ASR (Xu et al., 2002),
which was also designed to exploit long-distance re-
lations. The depLM is used in a bottom-up style,
while SLM is employed in a left-to-right style.
</bodyText>
<sectionHeader confidence="0.996845" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999740823529412">
In this paper, we propose a novel string-to-
dependency algorithm for statistical machine trans-
lation. For comparison purposes, we replicated
the Hiero system as described in (Chiang, 2005).
Our string-to-dependency system generates 80%
fewer rules, and achieves 1.48 point improvement in
BLEU and 2.53 point improvement in TER on the
decoding output on the NIST 04 Chinese-English
evaluation set.
Dependency structures provide a desirable plat-
form to employ linguistic knowledge in MT. In the
future, we will continue our research in this direction
to carry out translation with deeper features, for ex-
ample, propositional structures (Palmer et al., 2005).
We believe that the fixed and floating structures pro-
posed in this paper can be extended to model predi-
cates and arguments.
</bodyText>
<sectionHeader confidence="0.998838" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999436285714286">
This work was supported by DARPA/IPTO Contract
No. HR0011-06-C-0022 under the GALE program.
We are grateful to Roger Bock, Ivan Bulyko, Mike
Kayser, John Makhoul, Spyros Matsoukas, Antti-
Veikko Rosti, Rich Schwartz and Bing Zhang for
their help in running the experiments and construc-
tive comments to improve this paper.
</bodyText>
<page confidence="0.997575">
584
</page>
<sectionHeader confidence="0.995884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99981234375">
I. Bulyko, S. Matsoukas, R. Schwartz, L. Nguyen, and
J. Makhoul. 2007. Language model adaptation in
machine translation from speech. In Proceedings of
the 32nd IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP).
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for statistical machine transla-
tion. In Proceedings ofMT Summit IX.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the
43th Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
S. DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007.
What can syntax-based mt learn from phrase-based
mt? In Proceedings of the 2007 Conference of Em-
pirical Methods in Natural Language Processing.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proceedings of the 43th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 541–548, Ann Arbor, Michigan, June.
J. Eisner and G. Satta. 1999. Efficient parsing for bilex-
ical context-free grammars and head automaton gram-
mars. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics (ACL).
H. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of the 2002 Conference of
Empirical Methods in Natural Language Processing.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What’s in a translation rule? In Proceedings of the
2004 Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefea,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic models. In COLING-
ACL ’06: Proceedings of 44th Annual Meeting of the
Association for Computational Linguistics and 21st
Int. Conf. on Computational Linguistics.
J. Graehl and K. Knight. 2004. Training tree transducers.
In Proceedings of the 2004 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics.
L. Huang and D. Chiang. 2005. Better k-best parsing.
In Proceedings of the 9th International Workshop on
Parsing Technologies.
D. Magerman. 1995. Statistical decision-tree models for
parsing. In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics.
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
SPMT: Statistical machine translation with syntacti-
fied target language phraases. In Proceedings of the
2006 Conference of Empirical Methods in Natural
Language Processing.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Pro-
ceedings ofthe 43th Annual Meeting of the Association
for Computational Linguistics (ACL).
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for sta-
tistical machine translation. In Erhard W. Hinrichs
and Dan Roth, editors, Proceedings of the 41st Annual
Meeting ofthe Association for Computational Linguis-
tics (ACL), pages 160–167, Sapporo, Japan, July.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
K. Papineni, S. Roukos, and T. Ward. 2001. Bleu: a
method for automatic evaluation of machine transla-
tion. IBM Research Report, RC22176.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 271–279, Ann Arbor, Michigan,
June.
S. Shieber and Y. Schabes. 1990. Synchronous tree ad-
joining grammars. In Proceedings of COLING ’90:
The 13th Int. Conf. on Computational Linguistics.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings ofAssoci-
ation for Machine Translation in the Americas.
W. Wang, K. Knight, and D. Marcu. 2007. Binarizing
syntax trees to improve syntax-based machine transla-
tion accuracy. In Proceedings of the 2007 Conference
of Empirical Methods in Natural Language Process-
ing.
P. Xu, C. Chelba, and F. Jelinek. 2002. A study on richer
syntactic dependencies for structured language model-
ing. In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL).
</reference>
<page confidence="0.998748">
585
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931882">
<title confidence="0.9994555">A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model</title>
<author confidence="0.999151">Libin Shen</author>
<affiliation confidence="0.991484">BBN Technologies</affiliation>
<address confidence="0.99998">Cambridge, MA 02138, USA</address>
<email confidence="0.999856">lshen@bbn.com</email>
<author confidence="0.997705">Jinxi Xu</author>
<affiliation confidence="0.992628">BBN Technologies</affiliation>
<address confidence="0.999987">Cambridge, MA 02138, USA</address>
<email confidence="0.999821">jxu@bbn.com</email>
<author confidence="0.999758">Ralph Weischedel</author>
<affiliation confidence="0.981017">BBN Technologies</affiliation>
<address confidence="0.999979">Cambridge, MA 02138, USA</address>
<email confidence="0.999912">weisched@bbn.com</email>
<abstract confidence="0.997748142857143">In this paper, we propose a novel string-todependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Bulyko</author>
<author>S Matsoukas</author>
<author>R Schwartz</author>
<author>L Nguyen</author>
<author>J Makhoul</author>
</authors>
<title>Language model adaptation in machine translation from speech.</title>
<date>2007</date>
<booktitle>In Proceedings of the 32nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="25317" citStr="Bulyko et al., 2007" startWordPosition="4296" endWordPosition="4299">e track data as well as some LDC corpora collected for the DARPA GALE program (LDC2005E83, LDC2006E34 and LDC2006G05) as our bilingual training data. It contains about 178M/191M words in source/target. Hierarchical rules were extracted from a subset which has about 35M/41M words5, and the rest of the training data were used to extract phrasal rules as in (Och, 2003; Chiang, 2005). The English side of this subset was also used to train a 3-gram dependency LM. Traditional 3-gram and 5-gram LMs were trained on a corpus of 6G words composed of the LDC Gigaword corpus and text downloaded from Web (Bulyko et al., 2007). We tuned the weights on NIST MT05 and tested on MT04. 5It includes eight corpora: LDC2002E18, LDC2003E07, LDC2004T08 HK News, LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E34, and LDC2006G05 583 Model #Rules baseline 140M filtered 26M str-dep 27M Table 1: Number of transfer rules Model BLEU% TER% lower mixed lower mixed Decoding (3-gram LM) baseline 38.18 35.77 58.91 56.60 filtered 37.92 35.48 57.80 55.43 str-dep 39.52 37.25 56.27 54.07 Rescoring (5-gram LM) baseline 40.53 38.26 56.35 54.15 filtered 40.49 38.26 55.57 53.47 str-dep 41.60 39.47 55.06 52.96 Table 2: BLEU and TER scores on the tes</context>
</contexts>
<marker>Bulyko, Matsoukas, Schwartz, Nguyen, Makhoul, 2007</marker>
<rawString>I. Bulyko, S. Matsoukas, R. Schwartz, L. Nguyen, and J. Makhoul. 2007. Language model adaptation in machine translation from speech. In Proceedings of the 32nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>K Knight</author>
<author>K Yamada</author>
</authors>
<title>Syntaxbased language models for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofMT Summit IX.</booktitle>
<contexts>
<context position="27448" citStr="Charniak et al. (2003)" startWordPosition="4633" endWordPosition="4636">endency structure does not provide an improvement on performance. However, dependency structures allow the use of a dependency LM which gives rise to significant improvement. 6 Discussion The well-formed dependency structures defined here are similar to the data structures in previous work on mono-lingual parsing (Eisner and Satta, 1999; McDonald et al., 2005). However, here we have fixed structures growing on both sides to exploit various translation fragments learned in the training data, while the operations in mono-lingual parsing were designed to avoid artificial ambiguity of derivation. Charniak et al. (2003) described a two-step stringto-CFG-tree translation model which employed a syntax-based language model to select the best translation from a target parse forest built in the first step. Only translation probability P(FIE) was employed in the construction of the target forest due to the complexity of the syntax-based LM. Since our dependency LM models structures over target words directly based on dependency trees, we can build a single-step system. This dependency LM can also be used in hierarchical MT systems using lexicalized CFG trees. The use of a dependency LM in MT is similar to the use </context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>E. Charniak, K. Knight, and K. Yamada. 2003. Syntaxbased language models for statistical machine translation. In Proceedings ofMT Summit IX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1000" citStr="Chiang, 2005" startWordPosition="140" endWordPosition="141">cal machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. 1 Introduction In recent years, hierarchical methods have been successfully applied to Statistical Machine Translation (Graehl and Knight, 2004; Chiang, 2005; Ding and Palmer, 2005; Quirk et al., 2005). In some language pairs, i.e. Chinese-to-English translation, state-ofthe-art hierarchical systems show significant advantage over phrasal systems in MT accuracy. For example, Chiang (2007) showed that the Hiero system achieved about 1 to 3 point improvement in BLEU on the NIST 03/04/05 Chinese-English evaluation sets compared to a start-of-the-art phrasal system. Our work extends the hierarchical MT approach. We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependen</context>
<context position="19042" citStr="Chiang, 2005" startWordPosition="3250" endWordPosition="3252">], [s, t] C [m, n], D2 is a sub-structure of D1, and at least one word in Pi,j f but not in Pp,q f is aligned. We create a new valid rule template (P0 f, P0e, D0, A), where we obtain Pf0 by replacing Pp,q f with label X in Pi,j f , and obtain Pe0 by replacing P s,t e with X in Pm,n e . Furthermore, We obtain D0 by replacing sub-structure D2 with X in D14. An example is shown in Figure 7. Among all valid rule templates, we collect those that contain at most two NTs and at most seven elements in the source as transfer rules in our system. 2.4 Decoding Following previous work on hierarchical MT (Chiang, 2005; Galley et al., 2006), we solve decoding as chart parsing. We view target dependency as the hidden structure of source fragments. The parser scans all source cells in a bottom-up style, and checks matched transfer rules according to the source side. Once there is a completed rule, we build a larger dependency structure by substituting component dependency structures for corresponding NTs in the target dependency structure of rules. Hypothesis dependency structures are organized in a shared forest, or AND-OR structures. An AND3By Pi,j f aligned to Pm,n e , we mean all words in Pi,j f are eithe</context>
<context position="22970" citStr="Chiang, 2005" startWordPosition="3915" endWordPosition="3916">sed to compute depLM scores in decoding. Due to the limit of space, we skip the details here. 4 Implementation Details Features 1. Probability of the source side given the target side of a rule 2. Probability of the target side given the source side of a rule 3. Word alignment probability 4. Number of target words 5. Number of concatenation rules used 6. Language model score 7. Dependency language model score 8. Discount on ill-formed dependency structures We have eight features in our system. The values of the first four features are accumulated on the rules used in a translation. Following (Chiang, 2005), we also use concatenation rules like X —* XX for backup. The 5th feature counts the number of concatenation rules used in a translation. In our system, we allow substitutions of dependency structures with unmatched categories, but there is a discount for such substitutions. Weight Optimization We tune the weights with several rounds of decoding-optimization. Following (Och, 2003), the k-best results are accumulated as the input of the optimizer. Powell’s method is used for optimization with 20 random starting points around the weight vector of the last iteration. Rescoring We rescore 1000-be</context>
<context position="25079" citStr="Chiang, 2005" startWordPosition="4254" endWordPosition="4255">odels are tuned on BLEU (Papineni et al., 2001), and evaluated on both BLEU and Translation Error Rate (TER) (Snover et al., 2006) so that we could detect over-tuning on one metric. We used part of the NIST 2006 ChineseEnglish large track data as well as some LDC corpora collected for the DARPA GALE program (LDC2005E83, LDC2006E34 and LDC2006G05) as our bilingual training data. It contains about 178M/191M words in source/target. Hierarchical rules were extracted from a subset which has about 35M/41M words5, and the rest of the training data were used to extract phrasal rules as in (Och, 2003; Chiang, 2005). The English side of this subset was also used to train a 3-gram dependency LM. Traditional 3-gram and 5-gram LMs were trained on a corpus of 6G words composed of the LDC Gigaword corpus and text downloaded from Web (Bulyko et al., 2007). We tuned the weights on NIST MT05 and tested on MT04. 5It includes eight corpora: LDC2002E18, LDC2003E07, LDC2004T08 HK News, LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E34, and LDC2006G05 583 Model #Rules baseline 140M filtered 26M str-dep 27M Table 1: Number of transfer rules Model BLEU% TER% lower mixed lower mixed Decoding (3-gram LM) baseline 38.18 35.7</context>
<context position="28458" citStr="Chiang, 2005" startWordPosition="4802" endWordPosition="4803">on dependency trees, we can build a single-step system. This dependency LM can also be used in hierarchical MT systems using lexicalized CFG trees. The use of a dependency LM in MT is similar to the use of a structured LM in ASR (Xu et al., 2002), which was also designed to exploit long-distance relations. The depLM is used in a bottom-up style, while SLM is employed in a left-to-right style. 7 Conclusions and Future Work In this paper, we propose a novel string-todependency algorithm for statistical machine translation. For comparison purposes, we replicated the Hiero system as described in (Chiang, 2005). Our string-to-dependency system generates 80% fewer rules, and achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER on the decoding output on the NIST 04 Chinese-English evaluation set. Dependency structures provide a desirable platform to employ linguistic knowledge in MT. In the future, we will continue our research in this direction to carry out translation with deeper features, for example, propositional structures (Palmer et al., 2005). We believe that the fixed and floating structures proposed in this paper can be extended to model predicates and arguments. Acknowl</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1234" citStr="Chiang (2007)" startWordPosition="174" endWordPosition="175">how that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. 1 Introduction In recent years, hierarchical methods have been successfully applied to Statistical Machine Translation (Graehl and Knight, 2004; Chiang, 2005; Ding and Palmer, 2005; Quirk et al., 2005). In some language pairs, i.e. Chinese-to-English translation, state-ofthe-art hierarchical systems show significant advantage over phrasal systems in MT accuracy. For example, Chiang (2007) showed that the Hiero system achieved about 1 to 3 point improvement in BLEU on the NIST 03/04/05 Chinese-English evaluation sets compared to a start-of-the-art phrasal system. Our work extends the hierarchical MT approach. We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependency structures. We restrict the target side to the so called wellformed dependency structures, in order to cover a large set of non-constituent transfer rules (Marcu et al., 2006), and enable efficient decoding through dynamic programm</context>
<context position="3246" citStr="Chiang, 2007" startWordPosition="488" endWordPosition="489">uss experimental results in section 5, compare to related work in section 6, and draw conclusions in section 7. 1.1 Hierarchical Machine Translation Graehl and Knight (2004) proposed the use of targettree-to-source-string transducers (xRS) to model translation. In xRS rules, the right-hand-side(rhs) of the target side is a tree with non-terminals(NTs), while the rhs of the source side is a string with NTs. Galley et al. (2006) extended this string-to-tree model by using Context-Free parse trees to represent the target side. A tree could represent multi-level transfer rules. The Hiero decoder (Chiang, 2007) does not require explicit syntactic representation on either side of the rules. Both source and target are strings with NTs. Decoding is solved as chart parsing. Hiero can be viewed as a hierarchical string-to-string model. Ding and Palmer (2005) and Quirk et al. (2005) 577 Proceedings ofACL-08: HLT, pages 577–585, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics boy find will it interesting the Figure 1: The dependency tree for sentence the boy will find it interesting followed the tree-to-tree approach (Shieber and Schabes, 1990) for translation. In their mod</context>
<context position="7380" citStr="Chiang, 2007" startWordPosition="1143" endWordPosition="1144">coverage of non-constituent rules. The use of dependency structures is due to the flexibility of dependency trees as a representation method which does not rely on constituents (Fox, 2002; Ding and Palmer, 2005; Quirk et al., 2005). The well-formedness of the dependency structures enables efficient decoding through dynamic programming. 2 String-to-Dependency Translation 2.1 Transfer Rules with Well-Formed Dependency Structures A string-to-dependency grammar G is a 4-tuple G =&lt; R, X, Tf, Te &gt;, where R is a set of transfer rules. X is the only non-terminal, which is similar to the Hiero system (Chiang, 2007). Tf is a set of 578 terminals in the source language, and Te is a set of terminals in the target language1. A string-to-dependency transfer rule R E R is a 4-tuple R =&lt; 5f, 5e, D, A &gt;, where 5f E (Tf U {X})+ is a source string, 5e E (Te U {X})+ is a target string, D represents the dependency structure for 5e, and A is the alignment between 5f and 5e. Non-terminal alignments in A must be one-to-one. In order to exclude undesirable structures, we only allow 5e whose dependency structure D is well-formed, which we will define below. In addition, the same well-formedness requirement will be appli</context>
<context position="17258" citStr="Chiang, 2007" startWordPosition="2903" endWordPosition="2904">ormed dependency tree does not depend on the order of category calculation. With categories, we can easily track the types of dependency structures and constrain operations in decoding. For example, we have a rule with dependency structure find +— X, where X right adjoins to find. Suppose we have two floating structures2, cat(X1) = ({he, will}, −, −) cat(X2) = (−, −, {it, interesting}) We can replace X by X2, but not by X1 based on the definition of category operations. 2.3 Rule Extraction Now we explain how we get the string-todependency rules from training data. The procedure is similar to (Chiang, 2007) except that we maintain tree structures on the target side, instead of strings. Given sentence-aligned bi-lingual training data, we first use GIZA++ (Och and Ney, 2003) to generate word level alignment. We use a statistical CFG parser to parse the English side of the training data, and extract dependency trees with Magerman’s rules (1995). Then we use heuristic rules to extract transferrules recursively based on the GIZA alignment and the target dependency trees. The rule extraction procedure is as follows. 1. Initialization: All the 4-tuples (P?&apos;,� � , P�,n � , D, A) are valid phrase alignme</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S DeNeefe</author>
<author>K Knight</author>
<author>W Wang</author>
<author>D Marcu</author>
</authors>
<title>What can syntax-based mt learn from phrase-based mt?</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Conference of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6627" citStr="DeNeefe et al. (2007)" startWordPosition="1023" endWordPosition="1026">ting representation methods, even with composed transfer rules (Galley et al., 2006). For example, the following rule • &lt;(hong)Chinese, (DT(the) JJ(red))English&gt; is not a valid string-to-tree transfer rule since the red is a partial constituent. A number of techniques have been proposed to improve rule coverage. (Marcu et al., 2006) and (Galley et al., 2006) introduced artificial constituent nodes dominating the phrase of interest. The binarization method used by Wang et al. (2007) can cover many non-constituent rules also, but not all of them. For example, it cannot handle the above example. DeNeefe et al. (2007) showed that the best results were obtained by combing these methods. In this paper, we use well-formed dependency structures to handle the coverage of non-constituent rules. The use of dependency structures is due to the flexibility of dependency trees as a representation method which does not rely on constituents (Fox, 2002; Ding and Palmer, 2005; Quirk et al., 2005). The well-formedness of the dependency structures enables efficient decoding through dynamic programming. 2 String-to-Dependency Translation 2.1 Transfer Rules with Well-Formed Dependency Structures A string-to-dependency gramma</context>
<context position="8157" citStr="DeNeefe et al., 2007" startWordPosition="1282" endWordPosition="1285"> 4-tuple R =&lt; 5f, 5e, D, A &gt;, where 5f E (Tf U {X})+ is a source string, 5e E (Te U {X})+ is a target string, D represents the dependency structure for 5e, and A is the alignment between 5f and 5e. Non-terminal alignments in A must be one-to-one. In order to exclude undesirable structures, we only allow 5e whose dependency structure D is well-formed, which we will define below. In addition, the same well-formedness requirement will be applied to partial decoding results. Thus, we will be able to employ shared structures to merge multiple partial results. Based on the results in previous work (DeNeefe et al., 2007), we want to keep two kinds of dependency structures. In one kind, we keep dependency trees with a sub-root, where all the children of the sub-root are complete. We call them fixed dependency structures because the head is known or fixed. In the other, we keep dependency structures of sibling nodes of a common head, but the head itself is unspecified or floating. Each of the siblings must be a complete constituent. We call them floating dependency structures. Floating structures can represent many linguistically meaningful non-constituent structures: for example, like the red, a modifier of a </context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>S. DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007. What can syntax-based mt learn from phrase-based mt? In Proceedings of the 2007 Conference of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ding</author>
<author>M Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>541--548</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1023" citStr="Ding and Palmer, 2005" startWordPosition="142" endWordPosition="145">anslation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. 1 Introduction In recent years, hierarchical methods have been successfully applied to Statistical Machine Translation (Graehl and Knight, 2004; Chiang, 2005; Ding and Palmer, 2005; Quirk et al., 2005). In some language pairs, i.e. Chinese-to-English translation, state-ofthe-art hierarchical systems show significant advantage over phrasal systems in MT accuracy. For example, Chiang (2007) showed that the Hiero system achieved about 1 to 3 point improvement in BLEU on the NIST 03/04/05 Chinese-English evaluation sets compared to a start-of-the-art phrasal system. Our work extends the hierarchical MT approach. We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependency structures. We restr</context>
<context position="3493" citStr="Ding and Palmer (2005)" startWordPosition="526" endWordPosition="529">) to model translation. In xRS rules, the right-hand-side(rhs) of the target side is a tree with non-terminals(NTs), while the rhs of the source side is a string with NTs. Galley et al. (2006) extended this string-to-tree model by using Context-Free parse trees to represent the target side. A tree could represent multi-level transfer rules. The Hiero decoder (Chiang, 2007) does not require explicit syntactic representation on either side of the rules. Both source and target are strings with NTs. Decoding is solved as chart parsing. Hiero can be viewed as a hierarchical string-to-string model. Ding and Palmer (2005) and Quirk et al. (2005) 577 Proceedings ofACL-08: HLT, pages 577–585, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics boy find will it interesting the Figure 1: The dependency tree for sentence the boy will find it interesting followed the tree-to-tree approach (Shieber and Schabes, 1990) for translation. In their models, dependency treelets are used to represent both the source and the target sides. Decoding is implemented as tree transduction preceded by source side dependency parsing. While tree-to-tree models can represent richer structural information, ex</context>
<context position="5210" citStr="Ding and Palmer, 2005" startWordPosition="793" endWordPosition="796"> for the root word. Figure 1 shows an example of a dependency tree. Arrows point from the child to the parent. In this example, the word find is the root. Dependency trees are simpler in form than CFG trees since there are no constituent labels. However, dependency relations directly model semantic structure of a sentence. As such, dependency trees are a desirable prior model of the target sentence. 1.3 Motivations for Well-Formed Dependency Structures We restrict ourselves to the so-called well-formed target dependency structures based on the following considerations. Dynamic Programming In (Ding and Palmer, 2005; Quirk et al., 2005), there is no restriction on dependency treelets used in transfer rules except for the size limit. This may result in a high dimensionality in hypothesis representation and make it hard to employ shared structures for efficient dynamic programming. In (Galley et al., 2004), rules contain NT slots and combination is only allowed at those slots. Therefore, the search space becomes much smaller. Furthermore, shared structures can be easily defined based on the labels of the slots. In order to take advantage of dynamic programming, we fixed the positions onto which another ano</context>
<context position="6977" citStr="Ding and Palmer, 2005" startWordPosition="1078" endWordPosition="1081">al., 2006) introduced artificial constituent nodes dominating the phrase of interest. The binarization method used by Wang et al. (2007) can cover many non-constituent rules also, but not all of them. For example, it cannot handle the above example. DeNeefe et al. (2007) showed that the best results were obtained by combing these methods. In this paper, we use well-formed dependency structures to handle the coverage of non-constituent rules. The use of dependency structures is due to the flexibility of dependency trees as a representation method which does not rely on constituents (Fox, 2002; Ding and Palmer, 2005; Quirk et al., 2005). The well-formedness of the dependency structures enables efficient decoding through dynamic programming. 2 String-to-Dependency Translation 2.1 Transfer Rules with Well-Formed Dependency Structures A string-to-dependency grammar G is a 4-tuple G =&lt; R, X, Tf, Te &gt;, where R is a set of transfer rules. X is the only non-terminal, which is similar to the Hiero system (Chiang, 2007). Tf is a set of 578 terminals in the source language, and Te is a set of terminals in the target language1. A string-to-dependency transfer rule R E R is a 4-tuple R =&lt; 5f, 5e, D, A &gt;, where 5f E </context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Y. Ding and M. Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL), pages 541–548, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="27164" citStr="Eisner and Satta, 1999" startWordPosition="4590" endWordPosition="4593">em. After 5-gram rescoring, it achieved 1.21 point improvement in BLEU and 1.19 improvement in TER. The filtered model does not show improvement on BLEU. The filtered string-to-string rules can be viewed the string projection of stringto-dependency rules. It means that just using dependency structure does not provide an improvement on performance. However, dependency structures allow the use of a dependency LM which gives rise to significant improvement. 6 Discussion The well-formed dependency structures defined here are similar to the data structures in previous work on mono-lingual parsing (Eisner and Satta, 1999; McDonald et al., 2005). However, here we have fixed structures growing on both sides to exploit various translation fragments learned in the training data, while the operations in mono-lingual parsing were designed to avoid artificial ambiguity of derivation. Charniak et al. (2003) described a two-step stringto-CFG-tree translation model which employed a syntax-based language model to select the best translation from a target parse forest built in the first step. Only translation probability P(FIE) was employed in the construction of the target forest due to the complexity of the syntax-base</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6954" citStr="Fox, 2002" startWordPosition="1076" endWordPosition="1077">(Galley et al., 2006) introduced artificial constituent nodes dominating the phrase of interest. The binarization method used by Wang et al. (2007) can cover many non-constituent rules also, but not all of them. For example, it cannot handle the above example. DeNeefe et al. (2007) showed that the best results were obtained by combing these methods. In this paper, we use well-formed dependency structures to handle the coverage of non-constituent rules. The use of dependency structures is due to the flexibility of dependency trees as a representation method which does not rely on constituents (Fox, 2002; Ding and Palmer, 2005; Quirk et al., 2005). The well-formedness of the dependency structures enables efficient decoding through dynamic programming. 2 String-to-Dependency Translation 2.1 Transfer Rules with Well-Formed Dependency Structures A string-to-dependency grammar G is a 4-tuple G =&lt; R, X, Tf, Te &gt;, where R is a set of transfer rules. X is the only non-terminal, which is similar to the Hiero system (Chiang, 2007). Tf is a set of 578 terminals in the source language, and Te is a set of terminals in the target language1. A string-to-dependency transfer rule R E R is a 4-tuple R =&lt; 5f, </context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>H. Fox. 2002. Phrasal cohesion and statistical machine translation. In Proceedings of the 2002 Conference of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>M Hopkins</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5504" citStr="Galley et al., 2004" startWordPosition="842" endWordPosition="845">ntic structure of a sentence. As such, dependency trees are a desirable prior model of the target sentence. 1.3 Motivations for Well-Formed Dependency Structures We restrict ourselves to the so-called well-formed target dependency structures based on the following considerations. Dynamic Programming In (Ding and Palmer, 2005; Quirk et al., 2005), there is no restriction on dependency treelets used in transfer rules except for the size limit. This may result in a high dimensionality in hypothesis representation and make it hard to employ shared structures for efficient dynamic programming. In (Galley et al., 2004), rules contain NT slots and combination is only allowed at those slots. Therefore, the search space becomes much smaller. Furthermore, shared structures can be easily defined based on the labels of the slots. In order to take advantage of dynamic programming, we fixed the positions onto which another another tree could be attached by specifying NTs in dependency trees. Rule Coverage Marcu et al. (2006) showed that many useful phrasal rules cannot be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al., 2006). For example,</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004. What’s in a translation rule? In Proceedings of the 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefea</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic models.</title>
<date>2006</date>
<booktitle>In COLINGACL ’06: Proceedings of 44th Annual Meeting of the Association for Computational Linguistics and 21st Int. Conf. on Computational Linguistics.</booktitle>
<contexts>
<context position="3063" citStr="Galley et al. (2006)" startWordPosition="460" endWordPosition="463">uce the model of string-to-dependency decoding. Section 3 illustrates of the use of dependency language models. In section 4, we describe the implementation details of our MT system. We discuss experimental results in section 5, compare to related work in section 6, and draw conclusions in section 7. 1.1 Hierarchical Machine Translation Graehl and Knight (2004) proposed the use of targettree-to-source-string transducers (xRS) to model translation. In xRS rules, the right-hand-side(rhs) of the target side is a tree with non-terminals(NTs), while the rhs of the source side is a string with NTs. Galley et al. (2006) extended this string-to-tree model by using Context-Free parse trees to represent the target side. A tree could represent multi-level transfer rules. The Hiero decoder (Chiang, 2007) does not require explicit syntactic representation on either side of the rules. Both source and target are strings with NTs. Decoding is solved as chart parsing. Hiero can be viewed as a hierarchical string-to-string model. Ding and Palmer (2005) and Quirk et al. (2005) 577 Proceedings ofACL-08: HLT, pages 577–585, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics boy find will it i</context>
<context position="6090" citStr="Galley et al., 2006" startWordPosition="937" endWordPosition="940">gramming. In (Galley et al., 2004), rules contain NT slots and combination is only allowed at those slots. Therefore, the search space becomes much smaller. Furthermore, shared structures can be easily defined based on the labels of the slots. In order to take advantage of dynamic programming, we fixed the positions onto which another another tree could be attached by specifying NTs in dependency trees. Rule Coverage Marcu et al. (2006) showed that many useful phrasal rules cannot be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al., 2006). For example, the following rule • &lt;(hong)Chinese, (DT(the) JJ(red))English&gt; is not a valid string-to-tree transfer rule since the red is a partial constituent. A number of techniques have been proposed to improve rule coverage. (Marcu et al., 2006) and (Galley et al., 2006) introduced artificial constituent nodes dominating the phrase of interest. The binarization method used by Wang et al. (2007) can cover many non-constituent rules also, but not all of them. For example, it cannot handle the above example. DeNeefe et al. (2007) showed that the best results were obtained by combing these me</context>
<context position="19064" citStr="Galley et al., 2006" startWordPosition="3253" endWordPosition="3256">, n], D2 is a sub-structure of D1, and at least one word in Pi,j f but not in Pp,q f is aligned. We create a new valid rule template (P0 f, P0e, D0, A), where we obtain Pf0 by replacing Pp,q f with label X in Pi,j f , and obtain Pe0 by replacing P s,t e with X in Pm,n e . Furthermore, We obtain D0 by replacing sub-structure D2 with X in D14. An example is shown in Figure 7. Among all valid rule templates, we collect those that contain at most two NTs and at most seven elements in the source as transfer rules in our system. 2.4 Decoding Following previous work on hierarchical MT (Chiang, 2005; Galley et al., 2006), we solve decoding as chart parsing. We view target dependency as the hidden structure of source fragments. The parser scans all source cells in a bottom-up style, and checks matched transfer rules according to the source side. Once there is a completed rule, we build a larger dependency structure by substituting component dependency structures for corresponding NTs in the target dependency structure of rules. Hypothesis dependency structures are organized in a shared forest, or AND-OR structures. An AND3By Pi,j f aligned to Pm,n e , we mean all words in Pi,j f are either aligned to words in </context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefea, Wang, Thayer, 2006</marker>
<rawString>M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefea, W. Wang, and I. Thayer. 2006. Scalable inference and training of context-rich syntactic models. In COLINGACL ’06: Proceedings of 44th Annual Meeting of the Association for Computational Linguistics and 21st Int. Conf. on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Graehl</author>
<author>K Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="986" citStr="Graehl and Knight, 2004" startWordPosition="136" endWordPosition="139">cy algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. 1 Introduction In recent years, hierarchical methods have been successfully applied to Statistical Machine Translation (Graehl and Knight, 2004; Chiang, 2005; Ding and Palmer, 2005; Quirk et al., 2005). In some language pairs, i.e. Chinese-to-English translation, state-ofthe-art hierarchical systems show significant advantage over phrasal systems in MT accuracy. For example, Chiang (2007) showed that the Hiero system achieved about 1 to 3 point improvement in BLEU on the NIST 03/04/05 Chinese-English evaluation sets compared to a start-of-the-art phrasal system. Our work extends the hierarchical MT approach. We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target si</context>
<context position="2806" citStr="Graehl and Knight (2004)" startWordPosition="419" endWordPosition="422"> BLEU and 2.53 point improvement in TER on the NIST 04 Chinese-English MT evaluation set. In the rest of this section, we will briefly discuss previous work on hierarchical MT and dependency representations, which motivated our research. In section 2, we introduce the model of string-to-dependency decoding. Section 3 illustrates of the use of dependency language models. In section 4, we describe the implementation details of our MT system. We discuss experimental results in section 5, compare to related work in section 6, and draw conclusions in section 7. 1.1 Hierarchical Machine Translation Graehl and Knight (2004) proposed the use of targettree-to-source-string transducers (xRS) to model translation. In xRS rules, the right-hand-side(rhs) of the target side is a tree with non-terminals(NTs), while the rhs of the source side is a string with NTs. Galley et al. (2006) extended this string-to-tree model by using Context-Free parse trees to represent the target side. A tree could represent multi-level transfer rules. The Hiero decoder (Chiang, 2007) does not require explicit syntactic representation on either side of the rules. Both source and target are strings with NTs. Decoding is solved as chart parsin</context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>J. Graehl and K. Knight. 2004. Training tree transducers. In Proceedings of the 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="23610" citStr="Huang and Chiang, 2005" startWordPosition="4015" endWordPosition="4018">catenation rules like X —* XX for backup. The 5th feature counts the number of concatenation rules used in a translation. In our system, we allow substitutions of dependency structures with unmatched categories, but there is a discount for such substitutions. Weight Optimization We tune the weights with several rounds of decoding-optimization. Following (Och, 2003), the k-best results are accumulated as the input of the optimizer. Powell’s method is used for optimization with 20 random starting points around the weight vector of the last iteration. Rescoring We rescore 1000-best translations (Huang and Chiang, 2005) by replacing the 3-gram LM score with the 5-gram LM score computed offline. 5 Experiments We carried out experiments on three models. • baseline: replication of the Hiero system. • filtered: a string-to-string MT system as in baseline. However, we only keep the transfer rules whose target side can be generated by a well-formed dependency structure. • str-dep: a string-to-dependency system with a dependency LM. We take the replicated Hiero system as our baseline because it is the closest to our string-todependency model. They have similar rule extraction and decoding algorithms. Both systems u</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>L. Huang and D. Chiang. 2005. Better k-best parsing. In Proceedings of the 9th International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Magerman, 1995</marker>
<rawString>D. Magerman. 1995. Statistical decision-tree models for parsing. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wang</author>
<author>A Echihabi</author>
<author>K Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phraases.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1778" citStr="Marcu et al., 2006" startWordPosition="259" endWordPosition="262">t advantage over phrasal systems in MT accuracy. For example, Chiang (2007) showed that the Hiero system achieved about 1 to 3 point improvement in BLEU on the NIST 03/04/05 Chinese-English evaluation sets compared to a start-of-the-art phrasal system. Our work extends the hierarchical MT approach. We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependency structures. We restrict the target side to the so called wellformed dependency structures, in order to cover a large set of non-constituent transfer rules (Marcu et al., 2006), and enable efficient decoding through dynamic programming. We incorporate a dependency language model during decoding, in order to exploit long-distance word relations which are unavailable with a traditional n-gram language model on target strings. For comparison purposes, we replicated the Hiero decoder (Chiang, 2005) as our baseline. Our stringto-dependency decoder shows 1.48 point improvement in BLEU and 2.53 point improvement in TER on the NIST 04 Chinese-English MT evaluation set. In the rest of this section, we will briefly discuss previous work on hierarchical MT and dependency repre</context>
<context position="5910" citStr="Marcu et al. (2006)" startWordPosition="911" endWordPosition="914">nsfer rules except for the size limit. This may result in a high dimensionality in hypothesis representation and make it hard to employ shared structures for efficient dynamic programming. In (Galley et al., 2004), rules contain NT slots and combination is only allowed at those slots. Therefore, the search space becomes much smaller. Furthermore, shared structures can be easily defined based on the labels of the slots. In order to take advantage of dynamic programming, we fixed the positions onto which another another tree could be attached by specifying NTs in dependency trees. Rule Coverage Marcu et al. (2006) showed that many useful phrasal rules cannot be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al., 2006). For example, the following rule • &lt;(hong)Chinese, (DT(the) JJ(red))English&gt; is not a valid string-to-tree transfer rule since the red is a partial constituent. A number of techniques have been proposed to improve rule coverage. (Marcu et al., 2006) and (Galley et al., 2006) introduced artificial constituent nodes dominating the phrase of interest. The binarization method used by Wang et al. (2007) can cover many no</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006. SPMT: Statistical machine translation with syntactified target language phraases. In Proceedings of the 2006 Conference of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofthe 43th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="27188" citStr="McDonald et al., 2005" startWordPosition="4594" endWordPosition="4598">ng, it achieved 1.21 point improvement in BLEU and 1.19 improvement in TER. The filtered model does not show improvement on BLEU. The filtered string-to-string rules can be viewed the string projection of stringto-dependency rules. It means that just using dependency structure does not provide an improvement on performance. However, dependency structures allow the use of a dependency LM which gives rise to significant improvement. 6 Discussion The well-formed dependency structures defined here are similar to the data structures in previous work on mono-lingual parsing (Eisner and Satta, 1999; McDonald et al., 2005). However, here we have fixed structures growing on both sides to exploit various translation fragments learned in the training data, while the operations in mono-lingual parsing were designed to avoid artificial ambiguity of derivation. Charniak et al. (2003) described a two-step stringto-CFG-tree translation model which employed a syntax-based language model to select the best translation from a target parse forest built in the first step. Only translation probability P(FIE) was employed in the construction of the target forest due to the complexity of the syntax-based LM. Since our dependen</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings ofthe 43th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="17427" citStr="Och and Ney, 2003" startWordPosition="2927" endWordPosition="2930">erations in decoding. For example, we have a rule with dependency structure find +— X, where X right adjoins to find. Suppose we have two floating structures2, cat(X1) = ({he, will}, −, −) cat(X2) = (−, −, {it, interesting}) We can replace X by X2, but not by X1 based on the definition of category operations. 2.3 Rule Extraction Now we explain how we get the string-todependency rules from training data. The procedure is similar to (Chiang, 2007) except that we maintain tree structures on the target side, instead of strings. Given sentence-aligned bi-lingual training data, we first use GIZA++ (Och and Ney, 2003) to generate word level alignment. We use a statistical CFG parser to parse the English side of the training data, and extract dependency trees with Magerman’s rules (1995). Then we use heuristic rules to extract transferrules recursively based on the GIZA alignment and the target dependency trees. The rule extraction procedure is as follows. 1. Initialization: All the 4-tuples (P?&apos;,� � , P�,n � , D, A) are valid phrase alignments, where source phrase P ?&apos;,� �is 2Here we use words instead of word indexes in categories to make the example easy to understand. (a) (b) the LA boy 3 2 1 LA will LA </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<editor>In Erhard W. Hinrichs and Dan Roth, editors,</editor>
<location>Sapporo, Japan,</location>
<contexts>
<context position="23354" citStr="Och, 2003" startWordPosition="3977" endWordPosition="3978">ency language model score 8. Discount on ill-formed dependency structures We have eight features in our system. The values of the first four features are accumulated on the rules used in a translation. Following (Chiang, 2005), we also use concatenation rules like X —* XX for backup. The 5th feature counts the number of concatenation rules used in a translation. In our system, we allow substitutions of dependency structures with unmatched categories, but there is a discount for such substitutions. Weight Optimization We tune the weights with several rounds of decoding-optimization. Following (Och, 2003), the k-best results are accumulated as the input of the optimizer. Powell’s method is used for optimization with 20 random starting points around the weight vector of the last iteration. Rescoring We rescore 1000-best translations (Huang and Chiang, 2005) by replacing the 3-gram LM score with the 5-gram LM score computed offline. 5 Experiments We carried out experiments on three models. • baseline: replication of the Hiero system. • filtered: a string-to-string MT system as in baseline. However, we only keep the transfer rules whose target side can be generated by a well-formed dependency str</context>
<context position="25064" citStr="Och, 2003" startWordPosition="4252" endWordPosition="4253">ding. All models are tuned on BLEU (Papineni et al., 2001), and evaluated on both BLEU and Translation Error Rate (TER) (Snover et al., 2006) so that we could detect over-tuning on one metric. We used part of the NIST 2006 ChineseEnglish large track data as well as some LDC corpora collected for the DARPA GALE program (LDC2005E83, LDC2006E34 and LDC2006G05) as our bilingual training data. It contains about 178M/191M words in source/target. Hierarchical rules were extracted from a subset which has about 35M/41M words5, and the rest of the training data were used to extract phrasal rules as in (Och, 2003; Chiang, 2005). The English side of this subset was also used to train a 3-gram dependency LM. Traditional 3-gram and 5-gram LMs were trained on a corpus of 6G words composed of the LDC Gigaword corpus and text downloaded from Web (Bulyko et al., 2007). We tuned the weights on NIST MT05 and tested on MT04. 5It includes eight corpora: LDC2002E18, LDC2003E07, LDC2004T08 HK News, LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E34, and LDC2006G05 583 Model #Rules baseline 140M filtered 26M str-dep 27M Table 1: Number of transfer rules Model BLEU% TER% lower mixed lower mixed Decoding (3-gram LM) base</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training for statistical machine translation. In Erhard W. Hinrichs and Dan Roth, editors, Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research</journal>
<tech>Report, RC22176.</tech>
<contexts>
<context position="24513" citStr="Papineni et al., 2001" startWordPosition="4158" endWordPosition="4161">se target side can be generated by a well-formed dependency structure. • str-dep: a string-to-dependency system with a dependency LM. We take the replicated Hiero system as our baseline because it is the closest to our string-todependency model. They have similar rule extraction and decoding algorithms. Both systems use only one non-terminal label in rules. The major difference is in the representation of target structures. We use dependency structures instead of strings; thus, the comparison will show the contribution of using dependency information in decoding. All models are tuned on BLEU (Papineni et al., 2001), and evaluated on both BLEU and Translation Error Rate (TER) (Snover et al., 2006) so that we could detect over-tuning on one metric. We used part of the NIST 2006 ChineseEnglish large track data as well as some LDC corpora collected for the DARPA GALE program (LDC2005E83, LDC2006E34 and LDC2006G05) as our bilingual training data. It contains about 178M/191M words in source/target. Hierarchical rules were extracted from a subset which has about 35M/41M words5, and the rest of the training data were used to extract phrasal rules as in (Och, 2003; Chiang, 2005). The English side of this subset </context>
</contexts>
<marker>Papineni, Roukos, Ward, 2001</marker>
<rawString>K. Papineni, S. Roukos, and T. Ward. 2001. Bleu: a method for automatic evaluation of machine translation. IBM Research Report, RC22176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quirk</author>
<author>A Menezes</author>
<author>C Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>271--279</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1044" citStr="Quirk et al., 2005" startWordPosition="146" endWordPosition="149">w framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. 1 Introduction In recent years, hierarchical methods have been successfully applied to Statistical Machine Translation (Graehl and Knight, 2004; Chiang, 2005; Ding and Palmer, 2005; Quirk et al., 2005). In some language pairs, i.e. Chinese-to-English translation, state-ofthe-art hierarchical systems show significant advantage over phrasal systems in MT accuracy. For example, Chiang (2007) showed that the Hiero system achieved about 1 to 3 point improvement in BLEU on the NIST 03/04/05 Chinese-English evaluation sets compared to a start-of-the-art phrasal system. Our work extends the hierarchical MT approach. We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependency structures. We restrict the target side t</context>
<context position="3517" citStr="Quirk et al. (2005)" startWordPosition="531" endWordPosition="534">xRS rules, the right-hand-side(rhs) of the target side is a tree with non-terminals(NTs), while the rhs of the source side is a string with NTs. Galley et al. (2006) extended this string-to-tree model by using Context-Free parse trees to represent the target side. A tree could represent multi-level transfer rules. The Hiero decoder (Chiang, 2007) does not require explicit syntactic representation on either side of the rules. Both source and target are strings with NTs. Decoding is solved as chart parsing. Hiero can be viewed as a hierarchical string-to-string model. Ding and Palmer (2005) and Quirk et al. (2005) 577 Proceedings ofACL-08: HLT, pages 577–585, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics boy find will it interesting the Figure 1: The dependency tree for sentence the boy will find it interesting followed the tree-to-tree approach (Shieber and Schabes, 1990) for translation. In their models, dependency treelets are used to represent both the source and the target sides. Decoding is implemented as tree transduction preceded by source side dependency parsing. While tree-to-tree models can represent richer structural information, existing tree-totree model</context>
<context position="5231" citStr="Quirk et al., 2005" startWordPosition="797" endWordPosition="800">ure 1 shows an example of a dependency tree. Arrows point from the child to the parent. In this example, the word find is the root. Dependency trees are simpler in form than CFG trees since there are no constituent labels. However, dependency relations directly model semantic structure of a sentence. As such, dependency trees are a desirable prior model of the target sentence. 1.3 Motivations for Well-Formed Dependency Structures We restrict ourselves to the so-called well-formed target dependency structures based on the following considerations. Dynamic Programming In (Ding and Palmer, 2005; Quirk et al., 2005), there is no restriction on dependency treelets used in transfer rules except for the size limit. This may result in a high dimensionality in hypothesis representation and make it hard to employ shared structures for efficient dynamic programming. In (Galley et al., 2004), rules contain NT slots and combination is only allowed at those slots. Therefore, the search space becomes much smaller. Furthermore, shared structures can be easily defined based on the labels of the slots. In order to take advantage of dynamic programming, we fixed the positions onto which another another tree could be at</context>
<context position="6998" citStr="Quirk et al., 2005" startWordPosition="1082" endWordPosition="1085">rtificial constituent nodes dominating the phrase of interest. The binarization method used by Wang et al. (2007) can cover many non-constituent rules also, but not all of them. For example, it cannot handle the above example. DeNeefe et al. (2007) showed that the best results were obtained by combing these methods. In this paper, we use well-formed dependency structures to handle the coverage of non-constituent rules. The use of dependency structures is due to the flexibility of dependency trees as a representation method which does not rely on constituents (Fox, 2002; Ding and Palmer, 2005; Quirk et al., 2005). The well-formedness of the dependency structures enables efficient decoding through dynamic programming. 2 String-to-Dependency Translation 2.1 Transfer Rules with Well-Formed Dependency Structures A string-to-dependency grammar G is a 4-tuple G =&lt; R, X, Tf, Te &gt;, where R is a set of transfer rules. X is the only non-terminal, which is similar to the Hiero system (Chiang, 2007). Tf is a set of 578 terminals in the source language, and Te is a set of terminals in the target language1. A string-to-dependency transfer rule R E R is a 4-tuple R =&lt; 5f, 5e, D, A &gt;, where 5f E (Tf U {X})+ is a sour</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL), pages 271–279, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
<author>Y Schabes</author>
</authors>
<title>Synchronous tree adjoining grammars.</title>
<date>1990</date>
<booktitle>In Proceedings of COLING ’90: The 13th Int. Conf. on Computational Linguistics.</booktitle>
<contexts>
<context position="3816" citStr="Shieber and Schabes, 1990" startWordPosition="574" endWordPosition="578">i-level transfer rules. The Hiero decoder (Chiang, 2007) does not require explicit syntactic representation on either side of the rules. Both source and target are strings with NTs. Decoding is solved as chart parsing. Hiero can be viewed as a hierarchical string-to-string model. Ding and Palmer (2005) and Quirk et al. (2005) 577 Proceedings ofACL-08: HLT, pages 577–585, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics boy find will it interesting the Figure 1: The dependency tree for sentence the boy will find it interesting followed the tree-to-tree approach (Shieber and Schabes, 1990) for translation. In their models, dependency treelets are used to represent both the source and the target sides. Decoding is implemented as tree transduction preceded by source side dependency parsing. While tree-to-tree models can represent richer structural information, existing tree-totree models did not show advantage over string-totree models on translation accuracy due to a much larger search space. One of the motivations of our work is to achieve desirable trade-off between model capability and search space through the use of the so called wellformed dependency structures in rule repr</context>
</contexts>
<marker>Shieber, Schabes, 1990</marker>
<rawString>S. Shieber and Y. Schabes. 1990. Synchronous tree adjoining grammars. In Proceedings of COLING ’90: The 13th Int. Conf. on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="24596" citStr="Snover et al., 2006" startWordPosition="4172" endWordPosition="4175">tring-to-dependency system with a dependency LM. We take the replicated Hiero system as our baseline because it is the closest to our string-todependency model. They have similar rule extraction and decoding algorithms. Both systems use only one non-terminal label in rules. The major difference is in the representation of target structures. We use dependency structures instead of strings; thus, the comparison will show the contribution of using dependency information in decoding. All models are tuned on BLEU (Papineni et al., 2001), and evaluated on both BLEU and Translation Error Rate (TER) (Snover et al., 2006) so that we could detect over-tuning on one metric. We used part of the NIST 2006 ChineseEnglish large track data as well as some LDC corpora collected for the DARPA GALE program (LDC2005E83, LDC2006E34 and LDC2006G05) as our bilingual training data. It contains about 178M/191M words in source/target. Hierarchical rules were extracted from a subset which has about 35M/41M words5, and the rest of the training data were used to extract phrasal rules as in (Och, 2003; Chiang, 2005). The English side of this subset was also used to train a 3-gram dependency LM. Traditional 3-gram and 5-gram LMs we</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociation for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Binarizing syntax trees to improve syntax-based machine translation accuracy.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Conference of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6492" citStr="Wang et al. (2007)" startWordPosition="999" endWordPosition="1002">s. Rule Coverage Marcu et al. (2006) showed that many useful phrasal rules cannot be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al., 2006). For example, the following rule • &lt;(hong)Chinese, (DT(the) JJ(red))English&gt; is not a valid string-to-tree transfer rule since the red is a partial constituent. A number of techniques have been proposed to improve rule coverage. (Marcu et al., 2006) and (Galley et al., 2006) introduced artificial constituent nodes dominating the phrase of interest. The binarization method used by Wang et al. (2007) can cover many non-constituent rules also, but not all of them. For example, it cannot handle the above example. DeNeefe et al. (2007) showed that the best results were obtained by combing these methods. In this paper, we use well-formed dependency structures to handle the coverage of non-constituent rules. The use of dependency structures is due to the flexibility of dependency trees as a representation method which does not rely on constituents (Fox, 2002; Ding and Palmer, 2005; Quirk et al., 2005). The well-formedness of the dependency structures enables efficient decoding through dynamic </context>
</contexts>
<marker>Wang, Knight, Marcu, 2007</marker>
<rawString>W. Wang, K. Knight, and D. Marcu. 2007. Binarizing syntax trees to improve syntax-based machine translation accuracy. In Proceedings of the 2007 Conference of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Xu</author>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>A study on richer syntactic dependencies for structured language modeling.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="28091" citStr="Xu et al., 2002" startWordPosition="4742" endWordPosition="4745">gto-CFG-tree translation model which employed a syntax-based language model to select the best translation from a target parse forest built in the first step. Only translation probability P(FIE) was employed in the construction of the target forest due to the complexity of the syntax-based LM. Since our dependency LM models structures over target words directly based on dependency trees, we can build a single-step system. This dependency LM can also be used in hierarchical MT systems using lexicalized CFG trees. The use of a dependency LM in MT is similar to the use of a structured LM in ASR (Xu et al., 2002), which was also designed to exploit long-distance relations. The depLM is used in a bottom-up style, while SLM is employed in a left-to-right style. 7 Conclusions and Future Work In this paper, we propose a novel string-todependency algorithm for statistical machine translation. For comparison purposes, we replicated the Hiero system as described in (Chiang, 2005). Our string-to-dependency system generates 80% fewer rules, and achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER on the decoding output on the NIST 04 Chinese-English evaluation set. Dependency structures pr</context>
</contexts>
<marker>Xu, Chelba, Jelinek, 2002</marker>
<rawString>P. Xu, C. Chelba, and F. Jelinek. 2002. A study on richer syntactic dependencies for structured language modeling. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>