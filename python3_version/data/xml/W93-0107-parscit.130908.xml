<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.38054">
HIERARCHICAL CLUSTERING OF VERBS
</note>
<author confidence="0.902482">
Roberto Basili (*)
Maria Teresa Pazienza (*)
Paola Velardi (**)
</author>
<bodyText confidence="0.978646588235294">
(*) Universita&apos; di Roma Tor Vergata, Italy
(**) Universita&apos; di Ancona, Italy
Abstract.
In this paper we present an unsupervised
learning algorithm for incremental concept
formation, based on an augmented version
of COBWEB. The algorithm is applied to
the task of acquiring a verb taxonomy
through the systematic observation of verb
usages in corpora.
Using a Machine Learning methodology for
a Natural language problem required
adjustments on both sides. In fact, concept
formation algorithms assume the input
information as being stable, unambiguous
and complete. At the opposite, linguistic
data are ambiguous, incomplete, and
possibly erroneous.
A NL processor is used to extract semi-
automatically from corpora the thematic
roles of verbs and derive a feature-vector
representation of verb instances. In order
to account for multiple instances of the
same verb, the measure of category utility,
defined in COBWEB, has been augmented
with the notion of memory inertia. Memory
inertia models the influence that previously
classified instances of a given verb have on
the classification of subsequent instances of
the same verb. Finally, a method is defined
to identify the basic-level classes of an
acquired hierarchy, i.e. those bringing the
most predictive information about their
members.
</bodyText>
<sectionHeader confidence="0.998704" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999570913043478">
The design of word-sense taxonomies is
acknowledged as one of the most difficult
(and frustrating) tasks in NLP systems.
The decision to assign a word to a category
is far from being straightforward
(Nirenburg and Raskin (1987)) and often
the lexicon builders do not use consistent
classification principia.
Automatic approaches to the acquisition of
word taxonomies have generally made use
of machine readable dictionaries (MRD) ,
for the typical definitory nature of MRD
texts. For example, in Byrd et al., (1987)
and other similar studies the category of a
word is acquired from the first few words
of a dictionary definition. Besides the well
known problems of inconsistency and
circularity of definitions, an inherent
difficulty with this approach is that verbs
can hardly be defined in terms of genus and
differentiae. Verb semantics resides in the
nature of the event they describe, that is
better expressed by the roles played by its
arguments in a sentence. Psycholinguistic
studies on verb semantics outline the
relevance of thematic roles, especially in
categorisation activities Keil, (1989),
Jackendoff (1983) and indicate the
argument structure of verbs as playing a
central role in language acquisition Pinker
(1989). In NLP, representing verb
semantics with their thematic roles is a
consolidated practice, even though
theoretical researches (Pustejovski (1991))
propose more rich and formal
representation frameworks.
More recent papers Hindle (1990), Pereira
and Tishby (1992) proposed to cluster
nouns on the basis of a metric derived from
the distribution of subject, verb and object
in the texts. Both papers use as a source of
information large corpora, but differ in the
type of statistical approach used to
determine word similarity. These studies,
though valuable, leave several open
problems:
</bodyText>
<page confidence="0.991211">
70
</page>
<listItem confidence="0.961918142857143">
1) A metric of conceptual closeness based
on mere syntactic similarity is
questionable, particularly if applied to
verbs. In fact, the argument structure
of verbs is variegated and poorly
overlapping. Furthermore, subject and
object relations do not fully
characterize many verbs.
2) Many events accumulate statistical
evidence only in very large corpora,
even though in Pereira and Tishby
(1992) the adopted notion of
distributional similarity in part avoids
this problem.
3) The description of a word is an
&amp;quot;agglomerate&amp;quot; of its occurrences in the
corpus, and it is not possible to
discriminate different senses.
4) None of the aforementioned studies
provide a method to describe and
evaluate the derived categories.
</listItem>
<bodyText confidence="0.999824270833334">
As a result, the acquired classifications
seem of little use for a large-scale NLP
system, and even for a linguist that is in
charge of deriving the taxonomy.
Our research is an attempt to overcome in
part the aforementioned limitations. We
present a corpus-driven unsupervised
learning algorithm based on a modified
version of COBWEB Fisher (1987),
Gennari et al. (1989). The algorithm learns
verb classifications through the systematic
observation of verb usages in sentences.
The algorithm has been tested on two
domains with very different linguistic
styles, a commercial and a legal corpus of
about 500,000 words each.
In section 2 we highlight the advantages
that concept formation algorithms, like
COBWEB, have over &amp;quot;agglomerate&amp;quot;
statistical approaches. However, using a
Machine Learning methodology for a
Natural Language Processing problem
required adjustments on both sides. Raw
texts representing instances of verb usages
have been processed to fit the feature-vector
like representation needed for concept
formation algorithms. The NL processor
used for this task is briefly summarized in
section2.1. Similarly, it was necessary to
adapt COBWEB to the linguistic nature of
the classification activity, since, for
example, the algorithm does not
discriminate different instances of the same
entity, i.e. polysemic. verbs, nor identical
instances of different entities, i.e. verbs
with the same pattern of use. These
modifications are discussed in sections 2.1
trough 2.3. Finally, in section 3 we present
a method to identify the basic-level
categories of a classification, i.e. those that
are repository of most of the lexical
information about their members.
Class descriptions and basic-level
categories, as derived by our clustering
algorithm, are in our view greatly helpful at
addressing the intuition of a linguist
towards the relevant taxonomic relations in
a given language domain.
</bodyText>
<sectionHeader confidence="0.6768975" genericHeader="method">
2. CIA UL Al: An algorithm to
acquire word clusters
</sectionHeader>
<bodyText confidence="0.912422652173913">
Incremental example-based learning
algorithms, like COBWEB Fisher (1987),
seem more adequate than other Machine
Learning and Statistical methods to the task
of acquiring word taxonomies from
corpora. COBWEB has several desirable
features:
a)Incrementality, since whenever new data
are available, the system updates its
classification;
b) A formal description of the acquired
clusters;
c) The notion of category utility, used to
select among competing classifications.
b) and c) are particularly relevant to our
linguistic problem, as remarked in the
Introduction.
On the other side, applying COBWEB to
verb classification is not straightforward.
First, there is a knowledge representation
problem, that is common to most Machine
Learning algorithms: Input instances must
be pre-coded (manually) using a feature-
</bodyText>
<footnote confidence="0.992087">
1 Ciaula stands for Concept formation Algorithm
Used for Language Acquisition, and has been
inspired by the tale &amp;quot;Ciaula scopre la luna&amp;quot; by
Luigi Pirandello (1922).
</footnote>
<page confidence="0.999229">
71
</page>
<bodyText confidence="0.992968909090909">
vector like representation. This limited the
use of such algorithms in many real world
problems. In the specific case we are
analyzing, a manual codification of verb
instances is not realistic on a large scale.
Second, the algorithm does not distinguish
multiple usages of the same verb, nor
different verbs that are found with the
same pattern of use, since different
instances with the same feature vector are
taken as identical. The motivation is that
concept formation algorithms as COBWEB
assume the input information as being
stable, unambiguous, and complete. At the
opposite, our data do not exhibit a stable
behaviour, they are ambiguous,
incomplete, and possibly misleading, since
errors in codification of verb instances may
well be possible.
In the following sections we will discuss
the methods by which we attempted to
overcome these obstacles.
</bodyText>
<subsectionHeader confidence="0.99904">
2.1 Representing verb instances
</subsectionHeader>
<bodyText confidence="0.998290885245901">
This section describes the formal
representation of verb instances and verb
clusters in CIAULA.
Verb usages input to the clustering
algorithm are represented by their thematic
roles, acquired semi-automatically from
corpora by a process that has been
described in Basili, (1992a), (1992b), (in
press). In short, sentences including verbs
are processed as follows:
First, a (general-purpose) morphologic and
a partial syntactic analyzer Bashi, (1992b)
extracts from the sentences in the corpus all
the elementary syntactic relations (esl) in
which a word participates. Syntactic
relations are word pairs and triples
augmented with a syntactic information,
e.g. for the verb to carry: N_V(
company,carry) V_N(carry,food)
V_N(carry,goods) V_prep_N(carry,
with,truck), etc.
Each syntactic relation is stored with its
frequency of occurrence in the corpus.
Ambiguous relations are weighted by a ilk
factor, where k is the number of competing
esl in a sentence.
Second, the verb arguments are tagged by
hand using 10-12 &amp;quot;naive&amp;quot; conceptual
types (semantic tags), such as: ACT,
PLACE, HUMAN_ENTITY, GOOD, etc.
Conceptual types are not the same for every
domain, even though the commercial and
legal domains have many common types.
Syntactic relations between words are
validated in terms of semantic relations
between word classes using a set of semi-
automatically acquired selectional rules
Basili, (1992a). For example,
V_prep_N(carry,with,truck) is accepted as
an istance of the high-level selectional rule
[ACT1--&gt;(INSTRUMENT)-
&gt;[MACHINE]. The relation: [carry]-
&gt;(INSTRUMENT)-&gt;[truck] is acquired as
part of the argument structure of the verb to
carry. In other published papers we
demonstrated that the use of semantic tags
greatly increase the statistical stability of the
data, and add predictive power to the
acquired information on word usages, at
the price of a limited manual work (the
semantic tagging).
For the purpose of this paper, the
interesting aspect is that single instances of
verb usages (local2 meanings) are validated
on the basis of a global analysis of the
corpus. This considerably reduces (though
does not eliminate) the presence of
erroneous instances.
The detected thematic roles of a verb v in a
sentence are represented by the feature-
vector:
</bodyText>
<listItem confidence="0.992499">
(1) v / (R.it:Catjt) he I, jte J t=1,2,...,n
• where kit are the thematic roles (AGENT,
</listItem>
<bodyText confidence="0.9587042">
INSTRUMENT etc.)3 and Catjt are the
conceptual types of the words to which v is
related semantically. For example, the
2 i.e. meanings that are completely described within
a single sentence of the corpus
3 The roles used are an extension of Sowa&apos;s
conceptual relations (Sowa 19841. Details on the
set of conceptual relations used and a corpus-based
method to select a domain-appropriate set, are
provided in other papers.
</bodyText>
<page confidence="0.990125">
72
</page>
<bodyText confidence="0.949432215384615">
following sentence in the commercial
domain:
&amp;quot;... la ditta produce beni di consumo con
macchinari elettromeccanici..&amp;quot;
&amp;quot;... the company produces goods with
electromechanical machines..&amp;quot;
originates the instance:
produce/(AGENT:HUMAN_ENTITY,
OBJECT:GOODS,
INSTRUMENT:MACHINE)
Configurations in which words of the same
conceptual type play the same roles are
strong suggestion of semantic similarity
between the related events. The
categorisation process must capture this
similarity among local meanings of verbs.
The representation of verb clusters follows
the scheme adopted in COBWEB. Each
target class is represented by the probability
that its members (i.e. verbs) are seen with a
set of typical roles. Given the set {ki}je
of thematic roles and the set (Catj ) je j of
conceptual types, a target class cif for our
clustering system is given by the following
(2) cif = &lt; cce, Vce, Sce &gt;
or equivalently by
(2)&apos; &lt;c, [x]ij, V, S&gt;
A class is represented in COBWEB by the
matrix [x]ij, showing the distribution of
probability among relations (11.i) and
conceptual types (Cats). The additional
parameters Vce and cce are introduced to
account for multiple instances of the same
verb in a class. cce is the cardinality (i.e.
the number of different instance members
of if), and Vce is the set of pairs &lt;v, v#&gt;
such that it exists at least one instance
v / (Ri:Catj)
classified in CC and v# is the number of
such instances.
Finally, Sceis the set of &apos;&apos;subtypes. The
definitions of the empty class (3.1) and of
the top node of the taxonomy (3.2) follows
from (2)
(3.1) &lt;0,[x]ij,{0},{0)&gt;
with xij=0 for each i,j
(3.2) &lt;Ntot, [x]ij, V. S&gt;
where Ntot is the number of available
instances in the corpus, V is the set of
verbs with their absolute occurrences.
An excerpt of a class acquired from the
legal domain is showed in Fig. 1. The
semantic types used in this domain are
listed in the figure.
Special type of classes are those in which
only a verb has been classified, that we will
call singleton classes. A singleton class is a
class ce=&lt;c,[x]ii,V,S&gt; for which
card(V)=1. It will be denoted by {v } where
v is the only member of (whatever its
occurrences) Se For a singleton class it is
clearly true that S={ø}. Note that a
singleton class is different from an instance
because any number of instances of the
verb v can be classified in (v).
</bodyText>
<subsectionHeader confidence="0.8839945">
2.2 Measuring the utility of a
classification
</subsectionHeader>
<bodyText confidence="0.99999475">
As remarked in the introduction, a useful
property of concept formation algorithms,
with respect to agglomerate statistical
approaches, is the use of formal methods
that guide the classification choices.
Quantitative approaches to model human
choices in categorisation have been adopted
in psychological models of conceptual
development. In her seminal work, Rosch
(1976) introduced a metrics of preference,
the category cue validity, expressed by the
sum of expectations of observing some
feature in the class members. This value is
maximum for the so-called basic level
categories. A later development, used in
COBWEB, introduces the notion of
category utility, derived from the
application of the Bayes law to the
expression of the predictive power of a
given classification. Given a classification
</bodyText>
<page confidence="0.988968">
73
</page>
<figure confidence="0.92635125">
into K classes, the category utility is given
by:
(4) Eprob(Ck)Eprob(attrFva1)C,)
ji
</figure>
<figureCaption confidence="0.9294708">
In COBWEB, a hill climbing algorithm is
defined to maximize the category utility of a
resulting classification. The following
expression is used to discriminate among
conflicting clusters:
</figureCaption>
<figure confidence="0.80851">
prob(C ir)Eprob(attr Fv 2 k) -Eprob(attr Fv al)2
(5)
</figure>
<tableCaption confidence="0.635150571428571">
The clusters that maximize the above
quantity provide the system with the
capability of deriving the best predictive
taxonomy with respect to the set of i
attributes and j values. This evaluation
maximizes infra-class similarity and intra-
class dissimilarity.
</tableCaption>
<table confidence="0.982586791666667">
Class: 123 Father class: 7
Cardinality: 18 (5) Level: 2 Tau: 1.00 Omega: 0.28
A D RE G HE AE S RE TE P AM Q M
AGEN: 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
AFF: 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
FI_S; 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
MANN: 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
FI_D: 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
FI_L: 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
REF: 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
REC: 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
CAUSE: 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
LOC: 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Heads:
- approvare (occ 8) %to approve
- stabilire (occ 7) %to establish, to decide
- prevedere (occ 1) %to foresee
- disporre (occ 1) %to dispose
- dichiarare (occ 1) %to declare
LMENDA (semantic types for the legal domain):
A=ACT, D=DOCUMENT, RE=REAL ESTATE, G=GOODS, HE=HUMAN_ENTITY,
AE=ABSTRACT_ENTITY, S=STATE, AM=AMOUNT, TE=TEMPORAL_ENTITY,
P=PLACES, Q=QUALITY, M=MANNER
- Fig 1. Example of cluster produced by the system -
</table>
<bodyText confidence="0.944697941176471">
The notion of category utility adopted in
COBWEB, however, does not fully cope
with our linguistic problem. As remarked in
the previous section, multiple instances of
the same entity are not considered in
COBWEB. In order to account for multiple
instances of a verb, we introduced the
notion of mnemonic inertia. The mnemonic
inertia models an inertial trend attracting a
new instance of an already classified verb
in the class where it was previously
classified.
Given the incoming instance
v / (12.i:Catj)
and a current classification in the set of
classes cigc, for each k the mnemonic inertia
is modelled by:
</bodyText>
<listItem confidence="0.412299">
(6) 1-1k(v) = #v ck
</listItem>
<bodyText confidence="0.991165625">
where #v is the number of instances of the
verb v already classified in c‘k and ck is the
cardinality of c6k.
(6) expresses a fuzzy membership of v to
the class cific. The more instances of v are
classified into iek, the more future
observations of v will be attracted by ctgic. A
suitable combination of the mnemonic
</bodyText>
<page confidence="0.993567">
74
</page>
<bodyText confidence="0.995751125">
inertia and the category utility provides our
system with generalization capabilities
along with the &amp;quot;conservative&amp;quot; policy of
leaving different verb instances separate.
The desired effect within the data is that
slightly different usages of a verb are
classified in the same cluster, while
remarkable differences result in different
classifications.
The global measure of category utility, used
by the CIAULA algorithm during
classification, can now be defined. Let v /
(124:Catj) be the incoming instance, cific be
the set of classes, and let cu(v,k) be the
category utility as defined in (5), the
measure 1.t, given by
</bodyText>
<equation confidence="0.837234">
(7) = vcu(v,k) + (1-v)mc(v) VE [OM
</equation>
<bodyText confidence="0.950762609756097">
expresses the global utility of the
classification obtained by assigning the
instance v to the class Ac. (7) is a distance
metrics among instances and classes.
2.3 The incremental clustering
algorithm.
The algorithm for the incremental clustering
of verb instances follows the approach used
in COBWEB. Given a new incoming
instance I and a current valid classification
{ c6k Ike K, the system evaluates the utility
of the new classification obtained by
inserting I in each class. The maximum
utility value corresponds to the best
predictive configuration of classes. A
further attempt is made to change the
current configuration (introducing a new
class, merging the two best candidate for
the classification or splitting the best classes
in the set of its son) to improve the
predictivity. The main difference with
respect to COBWEB, due to the linguistic
nature of the problem at hand, concern the
procedure to evaluate the utility of a
temporary classification and the MERGE
operator, as it applies to singleton classes.
The description of the algorithm is given in
Appendix 1. Auxiliary procedures are
omitted for brevity.
According to (7), the procedure
G_UTILITY(x, I, Se, s.5q1, v) evaluates the
utility of the classification as a combination
of the category utility and the inertial factor
introduced in (6). Current values
experimented for v are 0.90-0.75.
Figure 2 shows the difference between the
standard MERGE operation, identical to
that used in COBWEB, and the elementary
MERGE between two singleton classes, as
defined in CIAULA.
- Fig. 2: Merge (a) vs. Elementary Merge (b) -
</bodyText>
<figure confidence="0.996433">
(b)
(a)
</figure>
<sectionHeader confidence="0.923449" genericHeader="method">
3. Experimental Results.
</sectionHeader>
<bodyText confidence="0.999801909090909">
The algorithm has been experimented on
two corpora of about 500,000 words each,
a legal and a commercial domain, that
exhibit very different linguistic styles and
verb usages. Only verbs for which at least
65 instances in each corpus have been
considered, in order to further reduce
parsing errors. Notice however that the use
of semantic tags in corpus parsing reduces
considerably the noise, with respect to
other corpus-based approaches.
</bodyText>
<page confidence="0.997413">
75
</page>
<bodyText confidence="0.999948041666667">
In the first experiment, CIAULA classifies
3325 examples of 371 verbs, from the legal
corpus. In the second, it receives 1296
examples of 41 verbs from the commercial
corpus. Upon a careful analysis of the
clusters obtained from each domain, the
resulting classifications were judged quite
expressive, and semantically biased from
the target linguistic domains, a part from
some noise due to wrong semantic
interpretation of elementary syntactic
structures Basili et al., (1992a). However,
the granularity of the description of the final
taxonomy is too fine, to be usefully
imported in the type hierarchy of a NLP
system. Furthermore, the order of
presentation of the different examples
strongly influences the final result4. In
order to derive reliable results we must find
some invariant with respect to the
presentation order. An additional
requirement is to define some objective
measure of the quality of the acquired
classification, other than the personal
judgement of the authors.
In this section we define a measure of the
class informative power, able to capture the
most relevant levels of the hierarchy. The
idea is to extract from the hierarchy the
basic level classes, or classes that are
repository of the most relevant lexical
information about their members. We
define basic level classes of the
classification those bringing most predictive
and stable information with respect to the
presentation order.
The notion of basic level classes has been
introduced in Rosch (1978). She
experimentally demonstrated that some
conceptual categories are more meaningful
than others as for the quantity of
information they bring about their
members. Membership to such classes
implies a grater number of attributes to be
inherited by instances of the domain. These
classes appear at the intermediate levels of a
taxonomy: for example within the vague
notion of animal, classes such dog or cat
</bodyText>
<footnote confidence="0.776748">
4 This is an inherent problem with concept
formation algorithms
</footnote>
<bodyText confidence="0.9992841">
seem to concentrate the major part of
information about their members, with
respect for example to the class of
mammals Lakoff (1987).
But what is a basic-level class for verbs? A
formal definition for these more
representative classes, able to guide the
intuition of the linguist in the categorisation
activity has been attempted, and will be
discussed in the next section.
</bodyText>
<subsectionHeader confidence="0.550906">
3.1. Basic level categories of
verbs.
</subsectionHeader>
<bodyText confidence="0.97507264516129">
The information conveyed by the derived
clusters, ce=&lt;c,[x]ij,V,S&gt;, is in the
distributions of the matrices [x]ij, and in the
set V. Two examples may be helpful at
distinguishing classes that are more
selective, from other more vague clusters.
Let (el be a singleton class, with
WI =&lt;1,[xl],V1, I 01&gt;. This clearly implies
that [xl] is binary. This class is highly
typical, as it is strongly characterized by its
only instance, but it has no generalization
power. Given, for example, a class
ce1=&lt;10,[x2],V2,S&gt; for which the
cardinality of a V2 is 10, and let [x2] be
such that for each couple &lt;i,j&gt; for which
x2v0, it follows x2ij=1/10. This class is
scarcely typical but has a strong
generalization power, as it clusters verbs
that show no overlaps between the thematic
roles they are represented by. We can say
that typicality is signaled by high values of
roles-types probabilities (i.e.
xij=prob((R.i:Catj) I ce) ), while the
generalization power co of a class
is related to the
following quantity:
(8) co = card(V)/c
To quantify the typicality of a class
the following definitions
are useful. Given a threshold cm [0,1], the
typicality of c&apos;is given by:
</bodyText>
<page confidence="0.962181">
76
</page>
<bodyText confidence="0.995834666666667">
The values y=0.6 and 8=0.75 have been
empirically selected as producing the most
stable results in both corpora.
</bodyText>
<figure confidence="0.725069444444444">
(9) Tie = E&lt;i,j&gt;E Tce Xii card(Tcd
where Tce is the typicality set of CC i.e.
{&lt;i,j&gt; I xij &gt;a).
DEF (Basic-level verb category). Given
two thresholds y , 8 [ 0 , 1],
is a basic-level category
for the related taxonomy iff:
(10.1) co &lt; y (generalization power)
(10.2) tie &gt;8 (typicality)
</figure>
<figureCaption confidence="0.795462">
Like all the classes derived by the algorithm
of section 2.3, each basic-level category
determines two fuzzy
membership values of the verb v included
in V. The local membership of v to ce,
</figureCaption>
<figure confidence="0.5171952">
gice(v), is defined by:
(11) uice(v)= #v/ max{# I &lt;w, #w&gt;e V}
The global membership of v to
is :
(12) 112ce(v) = #v nv,
</figure>
<bodyText confidence="0.998837777777778">
where nv is the number of different
instances of v in the learning set. (11)
depends on the contribution of v to the
distribution of probabilities [x]ij, i.e. it
measures the adherence of v to the
prototype. (12) determines how typical is
the classification of v in ce, with respect to
all the observations of v in the corpus. Low
values of the global membership are useful
at identifying instances of v that are likely
to be originated by parsing errors.
Given a classification 9&amp;quot;Of extended sets of
linguistic instances, the definition (10)
identifies all the basic-level classes.
Repeated experiment over the two corpora
demonstrated that these classes are
substantially invariant with respect to the
presentation order of the instances.
</bodyText>
<sectionHeader confidence="0.99859" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.995038954545455">
The Appendix 2 shows all the basic level
categories derived from a small learning
set, named DPR633, that belongs to the
legal corpus. CIAULA receives in input
293 examples of 30 verbs. The reason for
showing DPR633 rather than an excerpt of
the results derived from the full corpus is
that there was no objective way to select
among the over 300 basic level classes. In
Appendix 2, the relatively low values of j.ti
and u2 are due to the exiguity of the
example set, rather than to errors in
parsing, as remarked in the previous
section. Of corse, the basic-level classes
extracted from the larger corpora exhibit a
more striking similarity among their
members, indicated by highest values of
global and local membership. An example
of cluster extracted from the whole legal
corpus was shown in Figure 1.
The example shown in Appendix 2 is
however &amp;quot;good enough&amp;quot; to highlight some
interesting property of our clustering
method. Each cluster has a semantic
description, and the degree of local and
global membership of verbs give an
objective measure of the similarity among
cluster members. It is interesting to observe
that the algorithm classifies in distinct
clusters different verb usages. For
example, the cluster 4 and the cluster 6
classify two different usages of the verb
indicare, e.g. indicare un&apos;ammontare (to
indicate an amount) and indicare un motivo
(to specify a motivation), where
&amp;quot;ammontare&amp;quot; is a type of AMOUNT(AM)
and &amp;quot; motivo&amp;quot; is a type of
ABSTRACT_ENTITY (AE).
The two clusters 13 and 14 capture the
physical and abstract use of eseguire, e.g.
eseguire un&apos;opera (to build a
bui/ding(=REAL_ESTATE) vrs. eseguire
un pagamento (to make a
payment(=AMOUNT,ACT)).
</bodyText>
<page confidence="0.996661">
77
</page>
<bodyText confidence="0.991409771428571">
The clusters 3 and 6 classify two uses of
the verb tenere, i.e. tenere un registro (to
keep a record(=DOCUMENT) vrs. tenere
un discorso (to hold a
speech(=ABSTRACT_ENTITY)). Many
other (often domain-dependent) examples
are reflected in the derived classification.
To sum up, we believe that CIAULA has
several advantages over other clustering
algorithms presented in literature.
(1) The derived clusters have a semantic
description, i.e. the predicted thematic
roles of its members.
(2) The clustering algorithm incrementally
assigns instances to classes, evaluating
its choices on the basis of a formal
criterium, the global utility.
(3) The defined measures of typicality and
generalization power make it possible
to select the basic-level classes of a
hierarchy, i.e. those that are repository
of most lexical information about their
members. These classes demonstrated
substantially stable with respect to the
order of presentation ofption, i.e. the
predicted thematic roles of its
members.
(4) It is possible to discriminate different
usages of verbs, since verb instances
are considered individually.
The hierarchy, as obtained by CIAULA, is
not usable tout court by a NLP system,
however class descriptions and basic-level
categories appear to be greatly useful at
addressing the intuition of the linguist
</bodyText>
<sectionHeader confidence="0.995007" genericHeader="method">
References.
</sectionHeader>
<reference confidence="0.998811526315789">
R. Basili, M.T. Pazienza, P. Velardi, (1992a)
Computational Lexicons: the neat examples and the
odd exemplars, Proc. of 3rd. Conf. on Applied
NLP, 1992.
R. Basili, M.T. Pazienza, P. Velardi, (1992b) &amp;quot;A
shallow Syntax to extract word associations from
corpora&amp;quot;, in Literary and Linguistic Computing,
vol. 2, 1992
R. Basili, M.T. Pazienza, P. Velardi, (in press)
Semi-automatic extraction of linguistic information
for syntactic disambiguation, Applied Artificial
Intelligence.
R. Byrd, N. Calzolari, M. Chodorow, I. Klavans,
M. Neff, 0. Rizk, (1987), &amp;quot;Large lexicons for
Natural Language Procesing: Utilizing the
Grammar COding System of LDOCE&amp;quot;, In
Computational Linguistics, December 1987
Dubois, D., Prade, H. (1988), &amp;quot;Possibility Theory:
an Approach to Computerized Processing of
Uncertainty&amp;quot;, Plenum Press, New York, 1988.
Fisher, D., (1987), Knowledge acquisition via
incremental conceptual clustering, Machine
Learning, 2, 1987.
J. Gennari, P. Langley, D. Fisher, (1989), Model
of incremental Concept Formation, in Artificial
Intelligence n.1-3, 1989.
Jacobs, P., (1991), &amp;quot;Integrating language and
meaning in structured inheritance networks&amp;quot;, in
&amp;quot;Principles of Semantic Networks&amp;quot;, J. Sowa Ed.,
Morgan ICauffmann, 1991.
R. Jackendoff, (1983), &amp;quot;Semantics and cognition&amp;quot;,
MIT Press, 1983.
D. Hindle, (1990), Noun classification from
predicate argument structures, in Proc. of ACL ,
1990
F. Keil, (1989),Concepts, kinds and cognitive
development, The MIT press, 1989.
G. Lakoff, (1987),Woman, fire and dangerous
things, University of Chicago Press, 1987.
S. Nirenburg, V. Raskin, (1987), The subworld
concept lexicon and the lexicon management
system, In Computational Linguistics, n. 13,
December 1987
F.Pereira, H. Tishby, (1992), &amp;quot;Distributional
similarity, Phase Transition and Hierarchical
Clustering&amp;quot;; in Proc. of AAAI Fall Symposium
Series, Probabilistic Approaches to Natural
Language, Cambridge, October, 1992.
S.Pinker, (1989), Learnability and Cognition - The
Acquisition of Argument Structure, MIT Press,
1989.
Luigi Pirandello, (1922), Novelle per un anno,
Editore R. Bemporad, Mondadori, 1922.
Pustejovsky, J., (1991), &amp;quot;The Generative Lexicon&amp;quot;,
Computational Linguistics, vol. 17, n. 4, 1991.
E. Rosch, (1978), Principle of categorization, in
Cognition and Categorization, Erlbaum 1978.
</reference>
<page confidence="0.99915">
78
</page>
<sectionHeader confidence="0.938927" genericHeader="method">
Appendix 1: The Algorithm for Conceptual Clustering of Verb Semantic
Instances
</sectionHeader>
<bodyText confidence="0.406808333333333">
Input: root node of the current taxonomy
I, Unclassified verb semantic instance
v(I), verb head of the instance I
</bodyText>
<figure confidence="0.8622934">
Output: An exhaustive conceptual classification of the incoming instances.
Variables: Se Sf.X.At classes of the taxonomy
x, p, s, n, m, q measures of global utility of a classification
CIAU LA( 5, I, v)
.LE 99%-s is a terminal
MEN
I.E.*&amp;quot;.&apos; is the singleton {v(I)}
THEN
INC ORPORATE(S, I)
ELSE NEW TERMINAL(, I)
INCORPORATE(51, I)
ELSE
INCORPORATE(.5;y1, I)
FOR EACH subtype ceof.*.
G _UTILITYx, I. ce,
Let p the best score x for classifying I in the class
s the second best score x for classifying I in the class.?
n the score x for classifyng I in a new node...X., subtype of
m the score x of classifying I in node merge between &amp;quot;and SP
the score x in classifying I in a classification obtained removing from the
current level and picking up the set of its son to the previous &amp;quot;level
iE p is the highest score
THEN
CIAULA( 9, I, v)
ELSE .LE n is the highest score
THEN initialize,./rwith values shown by I
ELSE I.E m is the highest score
THEN
MERGE( .4( -9, g7s.9,-
CIA ULA( .A, I, v)
ELSE 1E q is the highest score
THEN
SPLIT(
CIAU LA( gis, I, v)
END
</figure>
<page confidence="0.983641">
79
</page>
<table confidence="0.6912561">
Appendix 2: Basic level classes derived from the DPR633 Corpus
Class: 1 Card: 3 Omega=0.67 Tau: 1.00
PROTOTYPE (i.e., Predicted Roles):
- (FIG_DEST) -- [A]
-- (RECIPIENT) -- [HE]
Verbs (local - global degree membership):
dichiarare (0.50 - 0.20)
applicare (1.00 - 0.07)
Class: 2 Card: 10 Omega=0.50 Tau: 1.00
PROTOTYPE (i.e., Predicted Roles):
- (OBJ) -- (AM]
-- (RECIPIENT) -- [HE]
Verbs (local - global degree membership):
richiedere (0.33 - 0.25)
eseguire (0.33 - 0.06)
applicare (1.00 - 0.11)
versare (0.66 - 0.13)
pagare (1.00 - 0.30)
Class: 3 Card: 14 Omega=0.50 Tau: 1.00
PROTOTYPE (i.e., Predicted Roles):
- (OBJ) [D]
-- (REFERENCE) -- [D]
Verbs (local - global degree membership):
tenere (0.25 - 0.11)
applicare (0.25 - 0.03)
allegare (0.50 - 0.33)
prevedere (1.00 - 0.44)
emettere (0.25 - 0.07)
indicare (0.50 - 0.03)
eseguire (0.75 - 0.20)
</table>
<reference confidence="0.78584675">
Class: 4 Card: 9 Omega=0.33 Tau: 1.00
PROTOTYPE (i.e., Predicted Roles):
- (OBJ) -- [AM]
-- (REFERENCE) -- [D]
Verbs (local - global degree membership):
applicare (1.00 - 0.19)
indicare (0.60 - 0.05)
prevedere (0.20 - 0.11)
Class: 5 Card: 3 Omega=0.67 Tau: 0.89
PROTOTYPE (i.e., Predicted Roles):
- (OBJ) -- [A, AM]
-- (REFERENCE) -- [D]
Verbs (local - global degree membership):
ammettere (1.00 - 0.66)
modificare (0.50 - 0.50)
Class: 6 Card: 8 Omega=0.62 Tau: 1.00
PROTOTYPE (i.e., Predicted Roles):
- (OBJ) [AE]
-- (REFERENCE) -- [D]
Verbs (local - global degree membership):
operare (0.25 - 0.09)
effettuare (0.25 - 0.01)
richiedere (0.25 - 0.25)
tenere (0.25 - 0.11)
indicare (1.00 - 0.07)
Class: 7 Card: 3 Omega=0.67 Tau: 0.78
PROTOTYPE (i.e., Predicted Roles):
(OBJ) -- [RE]
(FIG_LOC) -- (HE]
-- (REFERENCE) -- [D]
Verbs (local - global degree membership):
comprendere (0.50 - 0.05)
indicare (1.00 - 0.03)
Class: 8 Card: 18 Omega=0.28 Tau: 1.00
PROTOTYPE (i.e., Predicted Roles):
- (SUBJ) -- [HE]
-- (MANNER) -- [D]
Verbs (local - global degree membership):
prevedere (0.12 - 0.11)
disporre (0.12 - 0.33)
approvare (1.00 - 0.88)
stabilire (0.87 - 0.38)
dichiarare (0.12 - 0.20)
Class: 9 Card: 3 Omega=0.67 Tau: 0.78
PROTOTYPE (i.e., Predicted Roles):
- (SUBJ) -- [HE]
- (MANNER) -- [A]
-- (LOCATION) -- [_]
Verbs (local - global degree membership):
rendere (0.50 - 0.20)
operare (1.00 - 0.18)
Class: 10 Card: 3 Omega=0.67 Tau: 0.78
PROTOTYPE (i.e., Predicted Roles):
(OBJ) bbiM]
- (FIG_DEST) -- [A]
-- (REFERENCE) -- [D]
Verbs (local - global degree membership):
versare (0.50 - 0.06)
operare (1.00 - 0.18)
Class: 11 Card: 6 Omega=0.67 Tau: 1.00
PROTOTYPE (i.e., Predicted Roles):
(OBJ) [D]
- (FIG_DEST) [A]
Verbs (local - global degree membership):
presentare (0.33 - 0.06)
tenere (0.33 - 0.11)
emettere (1.00 - 0.23)
indicare (0.33 - 0.01)
Class: 12 Card: 6 Omega=0.50 Tau: 0.78
PROTOTYPE (i.e., Predicted Roles):
(OBJ) -- [A, AM]
(MANNER) -- [A]
Verbs (local - global degree membership):
versare (0.25 - 0.06)
eseguire (0.25 - 0.06)
effettuare (1.00 - 0.07)
</reference>
<page confidence="0.964467">
80
</page>
<table confidence="0.836820411764706">
Class: 13 Card: 3 Omega=0.67 Tau: 1.00
PROTOTYPE (i.e., Predicted Roles):
- (OBJ) -- [RE]
- (MANNER) -- [A]
Verbs (local - global degree membership):
eseguire (0.50 - 0.06)
comprendere (1.00 - 0.10)
Class: 14 Card: 11 Omega=0.55 Tau: 1.00
PROTOTYPE (i.e., Predicted Roles):
- (OBJ) -- (A, AM]
Verbs (local - global degree membership):
produrre (0.16 - 0.11)
considerare (0.16 - 0.16)
applicare (0.16 - 0.03)
eseguire (0.16 - 0.06)
effettuare (1.00 - 0.11)
indicare (0.16 - 0.01)
</table>
<page confidence="0.994835">
81
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.432826">
<title confidence="0.999744">HIERARCHICAL CLUSTERING OF VERBS</title>
<author confidence="0.995284666666667">Roberto Basili Maria Teresa Pazienza Paola Velardi</author>
<affiliation confidence="0.700897">(*) Universita&apos; di Roma Tor Vergata, (**) Universita&apos; di Ancona, Italy</affiliation>
<abstract confidence="0.9990570625">In this paper we present an unsupervised learning algorithm for incremental concept formation, based on an augmented version of COBWEB. The algorithm is applied to the task of acquiring a verb taxonomy through the systematic observation of verb usages in corpora. Using a Machine Learning methodology for a Natural language problem required adjustments on both sides. In fact, concept formation algorithms assume the input information as being stable, unambiguous and complete. At the opposite, linguistic data are ambiguous, incomplete, and possibly erroneous. A NL processor is used to extract semicorpora the thematic roles of verbs and derive a feature-vector representation of verb instances. In order to account for multiple instances of the same verb, the measure of category utility, defined in COBWEB, has been augmented with the notion of memory inertia. Memory inertia models the influence that previously classified instances of a given verb have on the classification of subsequent instances of the same verb. Finally, a method is defined identify the of an acquired hierarchy, i.e. those bringing the most predictive information about their members.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>M T Pazienza</author>
<author>P Velardi</author>
</authors>
<title>Computational Lexicons: the neat examples and the odd exemplars,</title>
<date>1992</date>
<booktitle>Proc. of 3rd. Conf. on Applied NLP,</booktitle>
<contexts>
<context position="19727" citStr="Basili et al., (1992" startWordPosition="3115" endWordPosition="3118">tice however that the use of semantic tags in corpus parsing reduces considerably the noise, with respect to other corpus-based approaches. 75 In the first experiment, CIAULA classifies 3325 examples of 371 verbs, from the legal corpus. In the second, it receives 1296 examples of 41 verbs from the commercial corpus. Upon a careful analysis of the clusters obtained from each domain, the resulting classifications were judged quite expressive, and semantically biased from the target linguistic domains, a part from some noise due to wrong semantic interpretation of elementary syntactic structures Basili et al., (1992a). However, the granularity of the description of the final taxonomy is too fine, to be usefully imported in the type hierarchy of a NLP system. Furthermore, the order of presentation of the different examples strongly influences the final result4. In order to derive reliable results we must find some invariant with respect to the presentation order. An additional requirement is to define some objective measure of the quality of the acquired classification, other than the personal judgement of the authors. In this section we define a measure of the class informative power, able to capture the</context>
</contexts>
<marker>Basili, Pazienza, Velardi, 1992</marker>
<rawString>R. Basili, M.T. Pazienza, P. Velardi, (1992a) Computational Lexicons: the neat examples and the odd exemplars, Proc. of 3rd. Conf. on Applied NLP, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>M T Pazienza</author>
<author>P Velardi</author>
</authors>
<title>A shallow Syntax to extract word associations from corpora&amp;quot;,</title>
<date>1992</date>
<booktitle>in Literary and Linguistic Computing,</booktitle>
<volume>2</volume>
<contexts>
<context position="19727" citStr="Basili et al., (1992" startWordPosition="3115" endWordPosition="3118">tice however that the use of semantic tags in corpus parsing reduces considerably the noise, with respect to other corpus-based approaches. 75 In the first experiment, CIAULA classifies 3325 examples of 371 verbs, from the legal corpus. In the second, it receives 1296 examples of 41 verbs from the commercial corpus. Upon a careful analysis of the clusters obtained from each domain, the resulting classifications were judged quite expressive, and semantically biased from the target linguistic domains, a part from some noise due to wrong semantic interpretation of elementary syntactic structures Basili et al., (1992a). However, the granularity of the description of the final taxonomy is too fine, to be usefully imported in the type hierarchy of a NLP system. Furthermore, the order of presentation of the different examples strongly influences the final result4. In order to derive reliable results we must find some invariant with respect to the presentation order. An additional requirement is to define some objective measure of the quality of the acquired classification, other than the personal judgement of the authors. In this section we define a measure of the class informative power, able to capture the</context>
</contexts>
<marker>Basili, Pazienza, Velardi, 1992</marker>
<rawString>R. Basili, M.T. Pazienza, P. Velardi, (1992b) &amp;quot;A shallow Syntax to extract word associations from corpora&amp;quot;, in Literary and Linguistic Computing, vol. 2, 1992</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Basili</author>
<author>M T Pazienza</author>
<author>P Velardi</author>
</authors>
<title>(in press) Semi-automatic extraction of linguistic information for syntactic disambiguation,</title>
<journal>Applied Artificial Intelligence.</journal>
<marker>Basili, Pazienza, Velardi, </marker>
<rawString>R. Basili, M.T. Pazienza, P. Velardi, (in press) Semi-automatic extraction of linguistic information for syntactic disambiguation, Applied Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Byrd</author>
<author>N Calzolari</author>
<author>M Chodorow</author>
<author>I Klavans</author>
<author>M Neff</author>
</authors>
<title>Large lexicons for Natural Language Procesing: Utilizing the Grammar COding System of LDOCE&amp;quot;,</title>
<date>1987</date>
<booktitle>In Computational Linguistics,</booktitle>
<contexts>
<context position="1921" citStr="Byrd et al., (1987)" startWordPosition="288" endWordPosition="291">red hierarchy, i.e. those bringing the most predictive information about their members. 1. Introduction The design of word-sense taxonomies is acknowledged as one of the most difficult (and frustrating) tasks in NLP systems. The decision to assign a word to a category is far from being straightforward (Nirenburg and Raskin (1987)) and often the lexicon builders do not use consistent classification principia. Automatic approaches to the acquisition of word taxonomies have generally made use of machine readable dictionaries (MRD) , for the typical definitory nature of MRD texts. For example, in Byrd et al., (1987) and other similar studies the category of a word is acquired from the first few words of a dictionary definition. Besides the well known problems of inconsistency and circularity of definitions, an inherent difficulty with this approach is that verbs can hardly be defined in terms of genus and differentiae. Verb semantics resides in the nature of the event they describe, that is better expressed by the roles played by its arguments in a sentence. Psycholinguistic studies on verb semantics outline the relevance of thematic roles, especially in categorisation activities Keil, (1989), Jackendoff</context>
</contexts>
<marker>Byrd, Calzolari, Chodorow, Klavans, Neff, 1987</marker>
<rawString>R. Byrd, N. Calzolari, M. Chodorow, I. Klavans, M. Neff, 0. Rizk, (1987), &amp;quot;Large lexicons for Natural Language Procesing: Utilizing the Grammar COding System of LDOCE&amp;quot;, In Computational Linguistics, December 1987</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dubois</author>
<author>H Prade</author>
</authors>
<title>Possibility Theory: an Approach to Computerized Processing of Uncertainty&amp;quot;,</title>
<date>1988</date>
<publisher>Plenum Press,</publisher>
<location>New York,</location>
<marker>Dubois, Prade, 1988</marker>
<rawString>Dubois, D., Prade, H. (1988), &amp;quot;Possibility Theory: an Approach to Computerized Processing of Uncertainty&amp;quot;, Plenum Press, New York, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Fisher</author>
</authors>
<title>Knowledge acquisition via incremental conceptual clustering,</title>
<date>1987</date>
<booktitle>Machine Learning,</booktitle>
<volume>2</volume>
<contexts>
<context position="4294" citStr="Fisher (1987)" startWordPosition="656" endWordPosition="657">avoids this problem. 3) The description of a word is an &amp;quot;agglomerate&amp;quot; of its occurrences in the corpus, and it is not possible to discriminate different senses. 4) None of the aforementioned studies provide a method to describe and evaluate the derived categories. As a result, the acquired classifications seem of little use for a large-scale NLP system, and even for a linguist that is in charge of deriving the taxonomy. Our research is an attempt to overcome in part the aforementioned limitations. We present a corpus-driven unsupervised learning algorithm based on a modified version of COBWEB Fisher (1987), Gennari et al. (1989). The algorithm learns verb classifications through the systematic observation of verb usages in sentences. The algorithm has been tested on two domains with very different linguistic styles, a commercial and a legal corpus of about 500,000 words each. In section 2 we highlight the advantages that concept formation algorithms, like COBWEB, have over &amp;quot;agglomerate&amp;quot; statistical approaches. However, using a Machine Learning methodology for a Natural Language Processing problem required adjustments on both sides. Raw texts representing instances of verb usages have been proce</context>
<context position="5972" citStr="Fisher (1987)" startWordPosition="903" endWordPosition="904">of use. These modifications are discussed in sections 2.1 trough 2.3. Finally, in section 3 we present a method to identify the basic-level categories of a classification, i.e. those that are repository of most of the lexical information about their members. Class descriptions and basic-level categories, as derived by our clustering algorithm, are in our view greatly helpful at addressing the intuition of a linguist towards the relevant taxonomic relations in a given language domain. 2. CIA UL Al: An algorithm to acquire word clusters Incremental example-based learning algorithms, like COBWEB Fisher (1987), seem more adequate than other Machine Learning and Statistical methods to the task of acquiring word taxonomies from corpora. COBWEB has several desirable features: a)Incrementality, since whenever new data are available, the system updates its classification; b) A formal description of the acquired clusters; c) The notion of category utility, used to select among competing classifications. b) and c) are particularly relevant to our linguistic problem, as remarked in the Introduction. On the other side, applying COBWEB to verb classification is not straightforward. First, there is a knowledg</context>
</contexts>
<marker>Fisher, 1987</marker>
<rawString>Fisher, D., (1987), Knowledge acquisition via incremental conceptual clustering, Machine Learning, 2, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gennari</author>
<author>P Langley</author>
<author>D Fisher</author>
</authors>
<title>Model of incremental Concept Formation,</title>
<date>1989</date>
<booktitle>in Artificial Intelligence n.1-3,</booktitle>
<contexts>
<context position="4317" citStr="Gennari et al. (1989)" startWordPosition="658" endWordPosition="661">blem. 3) The description of a word is an &amp;quot;agglomerate&amp;quot; of its occurrences in the corpus, and it is not possible to discriminate different senses. 4) None of the aforementioned studies provide a method to describe and evaluate the derived categories. As a result, the acquired classifications seem of little use for a large-scale NLP system, and even for a linguist that is in charge of deriving the taxonomy. Our research is an attempt to overcome in part the aforementioned limitations. We present a corpus-driven unsupervised learning algorithm based on a modified version of COBWEB Fisher (1987), Gennari et al. (1989). The algorithm learns verb classifications through the systematic observation of verb usages in sentences. The algorithm has been tested on two domains with very different linguistic styles, a commercial and a legal corpus of about 500,000 words each. In section 2 we highlight the advantages that concept formation algorithms, like COBWEB, have over &amp;quot;agglomerate&amp;quot; statistical approaches. However, using a Machine Learning methodology for a Natural Language Processing problem required adjustments on both sides. Raw texts representing instances of verb usages have been processed to fit the feature</context>
</contexts>
<marker>Gennari, Langley, Fisher, 1989</marker>
<rawString>J. Gennari, P. Langley, D. Fisher, (1989), Model of incremental Concept Formation, in Artificial Intelligence n.1-3, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jacobs</author>
</authors>
<title>Integrating language and meaning in structured inheritance networks&amp;quot;, in &amp;quot;Principles of Semantic Networks&amp;quot;,</title>
<date>1991</date>
<marker>Jacobs, 1991</marker>
<rawString>Jacobs, P., (1991), &amp;quot;Integrating language and meaning in structured inheritance networks&amp;quot;, in &amp;quot;Principles of Semantic Networks&amp;quot;, J. Sowa Ed., Morgan ICauffmann, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>Semantics and cognition&amp;quot;,</title>
<date>1983</date>
<publisher>MIT Press,</publisher>
<contexts>
<context position="2528" citStr="Jackendoff (1983)" startWordPosition="384" endWordPosition="385">l., (1987) and other similar studies the category of a word is acquired from the first few words of a dictionary definition. Besides the well known problems of inconsistency and circularity of definitions, an inherent difficulty with this approach is that verbs can hardly be defined in terms of genus and differentiae. Verb semantics resides in the nature of the event they describe, that is better expressed by the roles played by its arguments in a sentence. Psycholinguistic studies on verb semantics outline the relevance of thematic roles, especially in categorisation activities Keil, (1989), Jackendoff (1983) and indicate the argument structure of verbs as playing a central role in language acquisition Pinker (1989). In NLP, representing verb semantics with their thematic roles is a consolidated practice, even though theoretical researches (Pustejovski (1991)) propose more rich and formal representation frameworks. More recent papers Hindle (1990), Pereira and Tishby (1992) proposed to cluster nouns on the basis of a metric derived from the distribution of subject, verb and object in the texts. Both papers use as a source of information large corpora, but differ in the type of statistical approach</context>
</contexts>
<marker>Jackendoff, 1983</marker>
<rawString>R. Jackendoff, (1983), &amp;quot;Semantics and cognition&amp;quot;, MIT Press, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicate argument structures, in</title>
<date>1990</date>
<booktitle>Proc. of ACL ,</booktitle>
<contexts>
<context position="2873" citStr="Hindle (1990)" startWordPosition="432" endWordPosition="433"> nature of the event they describe, that is better expressed by the roles played by its arguments in a sentence. Psycholinguistic studies on verb semantics outline the relevance of thematic roles, especially in categorisation activities Keil, (1989), Jackendoff (1983) and indicate the argument structure of verbs as playing a central role in language acquisition Pinker (1989). In NLP, representing verb semantics with their thematic roles is a consolidated practice, even though theoretical researches (Pustejovski (1991)) propose more rich and formal representation frameworks. More recent papers Hindle (1990), Pereira and Tishby (1992) proposed to cluster nouns on the basis of a metric derived from the distribution of subject, verb and object in the texts. Both papers use as a source of information large corpora, but differ in the type of statistical approach used to determine word similarity. These studies, though valuable, leave several open problems: 70 1) A metric of conceptual closeness based on mere syntactic similarity is questionable, particularly if applied to verbs. In fact, the argument structure of verbs is variegated and poorly overlapping. Furthermore, subject and object relations do</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>D. Hindle, (1990), Noun classification from predicate argument structures, in Proc. of ACL , 1990</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Keil</author>
</authors>
<title>kinds and cognitive development,</title>
<date>1989</date>
<publisher>The MIT press,</publisher>
<contexts>
<context position="2509" citStr="Keil, (1989)" startWordPosition="382" endWordPosition="383">, in Byrd et al., (1987) and other similar studies the category of a word is acquired from the first few words of a dictionary definition. Besides the well known problems of inconsistency and circularity of definitions, an inherent difficulty with this approach is that verbs can hardly be defined in terms of genus and differentiae. Verb semantics resides in the nature of the event they describe, that is better expressed by the roles played by its arguments in a sentence. Psycholinguistic studies on verb semantics outline the relevance of thematic roles, especially in categorisation activities Keil, (1989), Jackendoff (1983) and indicate the argument structure of verbs as playing a central role in language acquisition Pinker (1989). In NLP, representing verb semantics with their thematic roles is a consolidated practice, even though theoretical researches (Pustejovski (1991)) propose more rich and formal representation frameworks. More recent papers Hindle (1990), Pereira and Tishby (1992) proposed to cluster nouns on the basis of a metric derived from the distribution of subject, verb and object in the texts. Both papers use as a source of information large corpora, but differ in the type of s</context>
</contexts>
<marker>Keil, 1989</marker>
<rawString>F. Keil, (1989),Concepts, kinds and cognitive development, The MIT press, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lakoff</author>
</authors>
<title>fire and dangerous things,</title>
<date>1987</date>
<publisher>University of Chicago Press,</publisher>
<contexts>
<context position="21347" citStr="Lakoff (1987)" startWordPosition="3374" endWordPosition="3375">h (1978). She experimentally demonstrated that some conceptual categories are more meaningful than others as for the quantity of information they bring about their members. Membership to such classes implies a grater number of attributes to be inherited by instances of the domain. These classes appear at the intermediate levels of a taxonomy: for example within the vague notion of animal, classes such dog or cat 4 This is an inherent problem with concept formation algorithms seem to concentrate the major part of information about their members, with respect for example to the class of mammals Lakoff (1987). But what is a basic-level class for verbs? A formal definition for these more representative classes, able to guide the intuition of the linguist in the categorisation activity has been attempted, and will be discussed in the next section. 3.1. Basic level categories of verbs. The information conveyed by the derived clusters, ce=&lt;c,[x]ij,V,S&gt;, is in the distributions of the matrices [x]ij, and in the set V. Two examples may be helpful at distinguishing classes that are more selective, from other more vague clusters. Let (el be a singleton class, with WI =&lt;1,[xl],V1, I 01&gt;. This clearly impli</context>
</contexts>
<marker>Lakoff, 1987</marker>
<rawString>G. Lakoff, (1987),Woman, fire and dangerous things, University of Chicago Press, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nirenburg</author>
<author>V Raskin</author>
</authors>
<title>The subworld concept lexicon and the lexicon management system,</title>
<date>1987</date>
<booktitle>In Computational Linguistics, n. 13,</booktitle>
<contexts>
<context position="1633" citStr="Nirenburg and Raskin (1987)" startWordPosition="244" endWordPosition="247">COBWEB, has been augmented with the notion of memory inertia. Memory inertia models the influence that previously classified instances of a given verb have on the classification of subsequent instances of the same verb. Finally, a method is defined to identify the basic-level classes of an acquired hierarchy, i.e. those bringing the most predictive information about their members. 1. Introduction The design of word-sense taxonomies is acknowledged as one of the most difficult (and frustrating) tasks in NLP systems. The decision to assign a word to a category is far from being straightforward (Nirenburg and Raskin (1987)) and often the lexicon builders do not use consistent classification principia. Automatic approaches to the acquisition of word taxonomies have generally made use of machine readable dictionaries (MRD) , for the typical definitory nature of MRD texts. For example, in Byrd et al., (1987) and other similar studies the category of a word is acquired from the first few words of a dictionary definition. Besides the well known problems of inconsistency and circularity of definitions, an inherent difficulty with this approach is that verbs can hardly be defined in terms of genus and differentiae. Ve</context>
</contexts>
<marker>Nirenburg, Raskin, 1987</marker>
<rawString>S. Nirenburg, V. Raskin, (1987), The subworld concept lexicon and the lexicon management system, In Computational Linguistics, n. 13, December 1987</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tishby F Pereira</author>
</authors>
<title>Distributional similarity, Phase Transition and Hierarchical Clustering&amp;quot;; in</title>
<date>1992</date>
<booktitle>Proc. of AAAI Fall Symposium Series, Probabilistic Approaches to Natural Language,</booktitle>
<location>Cambridge,</location>
<marker>Pereira, 1992</marker>
<rawString>F.Pereira, H. Tishby, (1992), &amp;quot;Distributional similarity, Phase Transition and Hierarchical Clustering&amp;quot;; in Proc. of AAAI Fall Symposium Series, Probabilistic Approaches to Natural Language, Cambridge, October, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pinker</author>
</authors>
<title>Learnability and Cognition - The Acquisition of Argument Structure,</title>
<date>1989</date>
<publisher>MIT Press,</publisher>
<contexts>
<context position="2637" citStr="Pinker (1989)" startWordPosition="401" endWordPosition="402">definition. Besides the well known problems of inconsistency and circularity of definitions, an inherent difficulty with this approach is that verbs can hardly be defined in terms of genus and differentiae. Verb semantics resides in the nature of the event they describe, that is better expressed by the roles played by its arguments in a sentence. Psycholinguistic studies on verb semantics outline the relevance of thematic roles, especially in categorisation activities Keil, (1989), Jackendoff (1983) and indicate the argument structure of verbs as playing a central role in language acquisition Pinker (1989). In NLP, representing verb semantics with their thematic roles is a consolidated practice, even though theoretical researches (Pustejovski (1991)) propose more rich and formal representation frameworks. More recent papers Hindle (1990), Pereira and Tishby (1992) proposed to cluster nouns on the basis of a metric derived from the distribution of subject, verb and object in the texts. Both papers use as a source of information large corpora, but differ in the type of statistical approach used to determine word similarity. These studies, though valuable, leave several open problems: 70 1) A metr</context>
</contexts>
<marker>Pinker, 1989</marker>
<rawString>S.Pinker, (1989), Learnability and Cognition - The Acquisition of Argument Structure, MIT Press, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luigi Pirandello</author>
</authors>
<title>Novelle per un anno,</title>
<date>1922</date>
<journal>Editore</journal>
<contexts>
<context position="6873" citStr="Pirandello (1922)" startWordPosition="1036" endWordPosition="1037">on of the acquired clusters; c) The notion of category utility, used to select among competing classifications. b) and c) are particularly relevant to our linguistic problem, as remarked in the Introduction. On the other side, applying COBWEB to verb classification is not straightforward. First, there is a knowledge representation problem, that is common to most Machine Learning algorithms: Input instances must be pre-coded (manually) using a feature1 Ciaula stands for Concept formation Algorithm Used for Language Acquisition, and has been inspired by the tale &amp;quot;Ciaula scopre la luna&amp;quot; by Luigi Pirandello (1922). 71 vector like representation. This limited the use of such algorithms in many real world problems. In the specific case we are analyzing, a manual codification of verb instances is not realistic on a large scale. Second, the algorithm does not distinguish multiple usages of the same verb, nor different verbs that are found with the same pattern of use, since different instances with the same feature vector are taken as identical. The motivation is that concept formation algorithms as COBWEB assume the input information as being stable, unambiguous, and complete. At the opposite, our data do</context>
</contexts>
<marker>Pirandello, 1922</marker>
<rawString>Luigi Pirandello, (1922), Novelle per un anno, Editore R. Bemporad, Mondadori, 1922.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>The Generative Lexicon&amp;quot;,</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<marker>Pustejovsky, 1991</marker>
<rawString>Pustejovsky, J., (1991), &amp;quot;The Generative Lexicon&amp;quot;, Computational Linguistics, vol. 17, n. 4, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rosch</author>
</authors>
<title>Principle of categorization, in Cognition and Categorization,</title>
<date>1978</date>
<location>Erlbaum</location>
<contexts>
<context position="20742" citStr="Rosch (1978)" startWordPosition="3278" endWordPosition="3279">tive measure of the quality of the acquired classification, other than the personal judgement of the authors. In this section we define a measure of the class informative power, able to capture the most relevant levels of the hierarchy. The idea is to extract from the hierarchy the basic level classes, or classes that are repository of the most relevant lexical information about their members. We define basic level classes of the classification those bringing most predictive and stable information with respect to the presentation order. The notion of basic level classes has been introduced in Rosch (1978). She experimentally demonstrated that some conceptual categories are more meaningful than others as for the quantity of information they bring about their members. Membership to such classes implies a grater number of attributes to be inherited by instances of the domain. These classes appear at the intermediate levels of a taxonomy: for example within the vague notion of animal, classes such dog or cat 4 This is an inherent problem with concept formation algorithms seem to concentrate the major part of information about their members, with respect for example to the class of mammals Lakoff (</context>
</contexts>
<marker>Rosch, 1978</marker>
<rawString>E. Rosch, (1978), Principle of categorization, in Cognition and Categorization, Erlbaum 1978.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Class</author>
</authors>
<title>Card: 9 Omega=0.33 Tau: 1.00 PROTOTYPE (i.e., Predicted Roles): - (OBJ) -- [AM] -- (REFERENCE) -- [D] Verbs (local - global degree membership):</title>
<booktitle>applicare (1.00 - 0.19) indicare (0.60 - 0.05) prevedere</booktitle>
<pages>0--20</pages>
<marker>Class, </marker>
<rawString>Class: 4 Card: 9 Omega=0.33 Tau: 1.00 PROTOTYPE (i.e., Predicted Roles): - (OBJ) -- [AM] -- (REFERENCE) -- [D] Verbs (local - global degree membership): applicare (1.00 - 0.19) indicare (0.60 - 0.05) prevedere (0.20 - 0.11)</rawString>
</citation>
<citation valid="false">
<authors>
<author>Class</author>
</authors>
<title>Card: 3 Omega=0.67 Tau: 0.89 PROTOTYPE (i.e., Predicted Roles): - (OBJ) -- [A, AM] -- (REFERENCE) -- [D] Verbs (local - global degree membership): ammettere (1.00 -</title>
<booktitle>0.66) modificare (0.50 - 0.50) Class: 6 Card: 8 Omega=0.62 Tau: 1.00 PROTOTYPE (i.e., Predicted Roles): - (OBJ) [AE] -- (REFERENCE) -- [D] Verbs (local - global degree membership): operare (0.25 - 0.09) effettuare (0.25 - 0.01) richiedere (0.25 - 0.25) tenere (0.25 - 0.11) indicare (1.00 - 0.07)</booktitle>
<marker>Class, </marker>
<rawString>Class: 5 Card: 3 Omega=0.67 Tau: 0.89 PROTOTYPE (i.e., Predicted Roles): - (OBJ) -- [A, AM] -- (REFERENCE) -- [D] Verbs (local - global degree membership): ammettere (1.00 - 0.66) modificare (0.50 - 0.50) Class: 6 Card: 8 Omega=0.62 Tau: 1.00 PROTOTYPE (i.e., Predicted Roles): - (OBJ) [AE] -- (REFERENCE) -- [D] Verbs (local - global degree membership): operare (0.25 - 0.09) effettuare (0.25 - 0.01) richiedere (0.25 - 0.25) tenere (0.25 - 0.11) indicare (1.00 - 0.07)</rawString>
</citation>
<citation valid="false">
<title>Verbs (local - global degree membership):</title>
<booktitle>Class: 7 Card: 3 Omega=0.67 Tau: 0.78 PROTOTYPE</booktitle>
<pages>0--12</pages>
<marker></marker>
<rawString>Class: 7 Card: 3 Omega=0.67 Tau: 0.78 PROTOTYPE (i.e., Predicted Roles): (OBJ) -- [RE] (FIG_LOC) -- (HE] -- (REFERENCE) -- [D] Verbs (local - global degree membership): comprendere (0.50 - 0.05) indicare (1.00 - 0.03) Class: 8 Card: 18 Omega=0.28 Tau: 1.00 PROTOTYPE (i.e., Predicted Roles): - (SUBJ) -- [HE] -- (MANNER) -- [D] Verbs (local - global degree membership): prevedere (0.12 - 0.11) disporre (0.12 - 0.33) approvare (1.00 - 0.88) stabilire (0.87 - 0.38) dichiarare (0.12 - 0.20)</rawString>
</citation>
<citation valid="true">
<title>Class: 9 Card: 3 Omega=0.67 Tau: 0.78 PROTOTYPE (i.e., Predicted Roles): - (SUBJ) -- [HE] - (MANNER) -- [A] -- (LOCATION) -- [_] Verbs (local - global degree membership): rendere (0.50 - 0.20) operare</title>
<date></date>
<pages>0--18</pages>
<marker></marker>
<rawString>Class: 9 Card: 3 Omega=0.67 Tau: 0.78 PROTOTYPE (i.e., Predicted Roles): - (SUBJ) -- [HE] - (MANNER) -- [A] -- (LOCATION) -- [_] Verbs (local - global degree membership): rendere (0.50 - 0.20) operare (1.00 - 0.18)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Class</author>
</authors>
<title>Card: 3 Omega=0.67 Tau: 0.78 PROTOTYPE (i.e., Predicted Roles): (OBJ) bbiM] - (FIG_DEST) -- [A] -- (REFERENCE) -- [D] Verbs (local - global degree membership): versare (0.50 - 0.06) operare</title>
<date></date>
<pages>0--18</pages>
<marker>Class, </marker>
<rawString>Class: 10 Card: 3 Omega=0.67 Tau: 0.78 PROTOTYPE (i.e., Predicted Roles): (OBJ) bbiM] - (FIG_DEST) -- [A] -- (REFERENCE) -- [D] Verbs (local - global degree membership): versare (0.50 - 0.06) operare (1.00 - 0.18)</rawString>
</citation>
<citation valid="false">
<authors>
<author>Class</author>
</authors>
<title>Card: 6 Omega=0.67 Tau: 1.00 PROTOTYPE (i.e., Predicted Roles): (OBJ) [D] - (FIG_DEST) [A] Verbs (local - global degree membership):</title>
<booktitle>presentare (0.33 - 0.06) tenere (0.33 - 0.11) emettere (1.00 - 0.23) indicare</booktitle>
<pages>0--33</pages>
<marker>Class, </marker>
<rawString>Class: 11 Card: 6 Omega=0.67 Tau: 1.00 PROTOTYPE (i.e., Predicted Roles): (OBJ) [D] - (FIG_DEST) [A] Verbs (local - global degree membership): presentare (0.33 - 0.06) tenere (0.33 - 0.11) emettere (1.00 - 0.23) indicare (0.33 - 0.01)</rawString>
</citation>
<citation valid="false">
<authors>
<author>Class</author>
</authors>
<title>Card: 6 Omega=0.50 Tau: 0.78 PROTOTYPE (i.e., Predicted Roles): (OBJ) -- [A, AM] (MANNER) -- [A] Verbs (local - global degree membership):</title>
<booktitle>versare (0.25 - 0.06) eseguire (0.25 - 0.06) effettuare (1.00 - 0.07)</booktitle>
<marker>Class, </marker>
<rawString>Class: 12 Card: 6 Omega=0.50 Tau: 0.78 PROTOTYPE (i.e., Predicted Roles): (OBJ) -- [A, AM] (MANNER) -- [A] Verbs (local - global degree membership): versare (0.25 - 0.06) eseguire (0.25 - 0.06) effettuare (1.00 - 0.07)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>