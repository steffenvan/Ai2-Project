<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020713">
<title confidence="0.9777525">
Phrase-Based Query Degradation Modeling for
Vocabulary-Independent Ranked Utterance Retrieval
</title>
<author confidence="0.980595">
J. Scott Olsson
</author>
<affiliation confidence="0.9218125">
HLT Center of Excellence
Johns Hopkins University
</affiliation>
<address confidence="0.838584">
Baltimore, MD 21211, USA
</address>
<email confidence="0.999335">
solsson@jhu.edu
</email>
<author confidence="0.912493">
Douglas W. Oard
</author>
<affiliation confidence="0.995798">
College of Information Studies
University of Maryland
</affiliation>
<address confidence="0.935355">
College Park, MD 15213, USA
</address>
<email confidence="0.999527">
oard@umd.edu
</email>
<sectionHeader confidence="0.997398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952761904762">
This paper introduces a new approach to rank-
ing speech utterances by a system’s confi-
dence that they contain a spoken word. Multi-
ple alternate pronunciations, or degradations,
of a query word’s phoneme sequence are hy-
pothesized and incorporated into the ranking
function. We consider two methods for hy-
pothesizing these degradations, the best of
which is constructed using factored phrase-
based statistical machine translation. We show
that this approach is able to significantly im-
prove upon a state-of-the-art baseline tech-
nique in an evaluation on held-out speech.
We evaluate our systems using three differ-
ent methods for indexing the speech utter-
ances (using phoneme, phoneme multigram,
and word recognition), and find that degrada-
tion modeling shows particular promise for lo-
cating out-of-vocabulary words when the un-
derlying indexing system is constructed with
standard word-based speech recognition.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999814625">
Our goal is to find short speech utterances which
contain a query word. We accomplish this goal
by ranking the set of utterances by our confidence
that they contain the query word, a task known as
Ranked Utterance Retrieval (RUR). In particular,
we are interested in the case when the user’s query
word can not be anticipated by a Large Vocabulary
Continuous Speech Recognizer’s (LVCSR) decod-
ing dictionary, so that the word is said to be Out-Of-
Vocabulary (OOV).
Rare words tend to be the most informative, but
are also most likely to be OOV. When words are
OOV, we must use vocabulary-independent tech-
niques to locate them. One popular approach is to
search for the words in output from a phoneme rec-
ognizer (Ng and Zue, 2000), although this suffers
from the low accuracy typical of phoneme recogni-
tion. We consider two methods for handling this in-
accuracy. First, we compare an RUR indexing sys-
tem using phonemes with two systems using longer
recognition units: words or phoneme multigrams.
Second, we consider several methods for handling
the recognition inaccuracy in the utterance rank-
ing function itself. Our baseline generative model
handles errorful recognition by estimating term fre-
quencies from smoothed language models trained
on phoneme lattices. Our new approach, which we
call query degradation, hypothesizes many alternate
“pronunciations” for the query word and incorpo-
rates them into the ranking function. These degra-
dations are translations of the lexical phoneme se-
quence into the errorful recognition language, which
we hypothesize using a factored phrase-based statis-
tical machine translation system.
Our speech collection is a set of oral history
interviews from the MALACH collection (Byrne
et al., 2004), which has previously been used for
ad hoc speech retrieval evaluations using one-best
word level transcripts (Pecina et al., 2007; Olsson,
2008a) and for vocabulary-independent RUR (Ols-
son, 2008b). The interviews were conducted with
survivors and witnesses of the Holocaust, who dis-
cuss their experiences before, during, and after the
Second World War. Their speech is predominately
spontaneous and conversational. It is often also
emotional and heavily accented. Because the speech
contains many words unlikely to occur within a gen-
eral purpose speech recognition lexicon, it repre-
</bodyText>
<page confidence="0.965066">
182
</page>
<note confidence="0.891313">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 182–190,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999849536231884">
sents an excellent collection for RUR evaluation.
We were graciously permitted to use BBN Tech-
nology’s speech recognition system Byblos (Prasad
et al., 2005; Matsoukas et al., 2005) for our speech
recognition experiments. We train on approximately
200 hours of transcribed audio excerpted from about
800 unique speakers in the MALACH collection. To
provide a realistic set of OOV query words, we use
an LVCSR dictionary previously constructed for a
different topic domain (broadcast news and conver-
sational telephone speech) and discard all utterances
in our acoustic training data which are not covered
by this dictionary. New acoustic and language mod-
els are trained for each of the phoneme, multigram
and word recognition systems.
The output of LVCSR is a lattice of recogni-
tion hypotheses for each test speech utterance. A
lattice is a directed acyclic graph that is used to
compactly represent the search space for a speech
recognition system. Each node represents a point in
time and arcs between nodes indicates a word oc-
curs between the connected nodes’ times. Arcs are
weighted by the probability of the word occurring,
so that the so-called “one-best” path through the lat-
tice (what a system might return as a transcription)
is the path through the lattice having highest proba-
bility under the acoustic and language models. Each
RUR model we consider is constructed using the ex-
pected counts of a query word’s phoneme sequences
in these recognition lattices. We consider three ap-
proaches to producing these phoneme lattices, using
standard word-based LVCSR, phoneme recognition,
and LVCSR using phoneme multigrams. Our word
system’s dictionary contains about 50,000 entries,
while the phoneme system contains 39 phonemes
from the ARPABET set.
Originally proposed by Deligne and Bimbot
(1997) to model variable length regularities in
streams of symbols (e.g., words, graphemes, or
phonemes), phoneme multigrams are short se-
quences of one or more phonemes. We produce a
set of “phoneme transcripts” by replacing transcript
words with their lexical pronunciation. The set of
multigrams is learned by then choosing a maximum-
likelihood segmentation of these training phoneme
transcripts, where the segmentation is viewed as hid-
den data in an Expectation-Maximization algorithm.
The set of all continuous phonemes occurring be-
tween segment boundaries is then chosen as our
multigram dictionary. This multigram recognition
dictionary contains 16,409 entries.
After we have obtained each recognition lat-
tice, our indexing approach follows that of Olsson
(2008b). Namely, for the word and multigram sys-
tems, we first expand lattice arcs containing multi-
ple phones to produce a lattice having only single
phonemes on its arcs. Then, we compute the ex-
pected count of all phoneme n-grams n ≤ 5 in the
lattice. These n-grams and their counts are inserted
in our inverted index for retrieval.
This paper is organized as follows. In Section 2
we introduce our baseline RUR methods. In Sec-
tion 3 we introduce our query degradation approach.
We introduce our experimental validation in Sec-
tion 4 and our results in Section 5. We find that
using phrase-based query degradations can signifi-
cantly improve upon a strong RUR baseline. Finally,
in Section 6 we conclude and outline several direc-
tions for future work.
</bodyText>
<sectionHeader confidence="0.99361" genericHeader="method">
2 Generative Baseline
</sectionHeader>
<bodyText confidence="0.999926714285714">
Each method we present in this paper ranks the ut-
terances by the term’s estimated frequency within
the corresponding phoneme lattice. This general
approach has previously been considered (Yu and
Seide, 2005; Saraclar and Sproat, 2004), on the ba-
sis that it provides a minimum Bayes-risk ranking
criterion (Yu et al., Sept 2005; Robertson, 1977) for
the utterances. What differs for each method is the
particular estimator of term frequency which is used.
We first outline our baseline approach, a generative
model for term frequency estimation.
Recall that our vocabulary-independent indices
contain the expected counts of phoneme sequences
from our recognition lattices. Yu and Seide (2005)
used these expected phoneme sequence counts to es-
timate term frequency in the following way. For a
t_fG is
estimated as tfG(Q, L) = P(Q|L) · NL, where NL
is an estimate for the number of words in the utter-
ance. The conditional P(Q|L) is modeled as an or-
der M phoneme level language model,
</bodyText>
<equation confidence="0.9578325">
Pˆ(Q|L) = �l P˜(qZ|qZ−nor+1,...,qZ−1,L), (1)
Z=1
query term Q and lattice L, term frequency
_
</equation>
<page confidence="0.990456">
183
</page>
<bodyText confidence="0.818652333333333">
so that �tfG(Q, L) ≈ Pˆ(Q|L) · NL. The probabil-
ity of a query phoneme qj being generated, given
that the phoneme sequence qj_M+1, ... , qj_1 =
</bodyText>
<equation confidence="0.966265333333333">
1 was observed, is estimated as
_M+1
.
EP,c[C(qj_1
j_M+1)]
Here, EP,c[C(qj_1
</equation>
<bodyText confidence="0.999379615384616">
j_M+1)] denotes the expected count
in lattice L of the phoneme sequence qj_1
j_M+1. We
compute these counts using a variant of the forward-
backward algorithm, which is implemented by the
SRI language modeling toolkit (Stolcke, 2002).
In practice, because of data sparsity, the language
model in Equation 1 must be modified to include
smoothing for unseen phoneme sequences. We use a
backoff M-gram model with Witten-Bell discount-
ing (Witten and Bell, 1991). We set the phoneme
language model’s order to M = 5, which gave good
results in previous work (Yu and Seide, 2005).
</bodyText>
<sectionHeader confidence="0.99292" genericHeader="method">
3 Incorporating Query Degradations
</sectionHeader>
<bodyText confidence="0.999376243902439">
One problem with the generative approach is that
recognition error is not modeled (apart from the un-
certainty captured in the phoneme lattice). The es-
sential problem is that while the method hopes to
model P(Q|L), it is in fact only able to model the
probability of one degradation H in the lattice, that
is P(H|L). We define a query degradation as any
phoneme sequence (including the lexical sequence)
which may, with some estimated probability, occur
in an errorful phonemic representation of the audio
(either a one-best or lattice hypothesis). Because of
speaker variation and because recognition is error-
ful, we ought to also consider non-lexical degrada-
tions of the query phoneme sequence. That is, we
should incorporate P(H|Q) in our ranking function.
It has previously been demonstrated that allow-
ing for phoneme confusability can significantly in-
crease spoken term detection performance on one-
best phoneme transcripts (Chaudhari and Picheny,
2007; Schone et al., 2005) and in phonemic lat-
tices (Foote et al., 1997). These methods work by
allowing weighted substitution costs in minimum-
edit-distance matching. Previously, these substitu-
tion costs have been maximum-likelihood estimates
of P(H|Q) for each phoneme, where P(H|Q) is
easily computed from a phoneme confusion matrix
after aligning the reference and one-best hypothesis
transcript under a minimum edit distance criterion.
Similar methods have also been used in other lan-
guage processing applications. For example, in (Ko-
lak, 2005), one-for-one character substitutions, in-
sertions and deletions were considered in a genera-
tive model of errors in OCR.
In this work, because we are focused on construct-
ing inverted indices of audio files (for speed and
to conserve space), we must generalize our method
of incorporating query degradations in the ranking
function. Given a degradation model P(H|Q), we
take as our ranking function the expectation of the
generative baseline estimate NL · Pˆ(H|L) with re-
spect to P(H|Q),
</bodyText>
<equation confidence="0.983230666666667">
� �
Pˆ(H|L) · NL · P(H|Q), (2)
HEW
</equation>
<bodyText confidence="0.999893172413793">
where H is the set of degradations. Note that, while
we consider the expected value of our baseline term
frequency estimator with respect to P(H|Q), this
general approach could be used with any other term
frequency estimator.
Our formulation is similar to approaches taken
in OCR document retrieval, using degradations of
character sequences (Darwish and Magdy, 2007;
Darwish, 2003). For vocabulary-independent spo-
ken term detection, perhaps the most closely re-
lated formulation is provided by (Mamou and Ram-
abhadran, 2008). In that work, they ranked ut-
terances by the weighted average of their match-
ing score, where the weights were confidences from
a grapheme to phoneme system’s first several hy-
potheses for a word’s pronunciation. The match-
ing scores were edit distances, where substitution
costs were weighted using phoneme confusability.
Accordingly, their formulation was not aimed at ac-
counting for errors in recognition per se, but rather
for errors in hypothesizing pronunciations. We ex-
pect this accounts for their lack of significant im-
provement using the method.
Since we don’t want to sum over all possible
recognition hypotheses H, we might instead sum
over the smallest set H such that EHEW P(H|Q) ≥
γ. That is, we could take the most probable degra-
dations until their cumulative probability exceeds
some threshold γ. In practice, however, because
</bodyText>
<equation confidence="0.997631">
��,,j-1 EP,c [C(qjj_M+1)]
P˜(qj  |Yj_M+1, L) =
�
�tfG(Q, L) =
</equation>
<page confidence="0.9858">
184
</page>
<bodyText confidence="0.999936428571429">
degradation probabilities can be poorly scaled, we
instead take a fixed number of degradations and
normalize their scores. When a query is issued,
we apply a degradation model to learn the top few
phoneme sequences &apos;H that are most likely to have
been recognized, under the model. In the machine
translation literature, this process is commonly re-
ferred to as decoding.
We now turn to the modeling of query degrada-
tions H given a phoneme sequence Q, P(HIQ).
First, we consider a simple baseline approach in Sec-
tion 3.1. Then, in Section 3.2, we propose a more
powerful technique, using state-of-the-art machine
translation methods to hypothesize our degradations.
</bodyText>
<subsectionHeader confidence="0.995507">
3.1 Baseline Query Degradations
</subsectionHeader>
<bodyText confidence="0.99626075">
Schone et al. (2005) used phoneme confusion ma-
trices created by aligning hypothesized and refer-
ence phoneme transcripts to weight edit costs for a
minimum-edit distance based search in a one-best
phoneme transcript. Foote et al. (1997) had previ-
ously used phoneme lattices, although with ad hoc
edit costs and without efficient indexing. In this
work, we do not want to linearly scan each phoneme
lattice for our query’s phoneme sequence, preferring
instead to look up sequences in the inverted indices
containing phoneme sequences.
Our baseline degradation approach is related to
the edit-cost approach taken by (Schone et al.,
2005), although we generalize it so that it may be
applied within Equation 2 and we consider speech
recognition hypotheses beyond the one-best hypoth-
esis. First, we randomly generate N traversals of
each phonemic recognition lattice. These traver-
sals are random paths through the lattice (i.e., we
start at the beginning of the lattice and move to the
next node, where our choice is weighted by the out-
going arcs’ probabilities). Then, we align each of
these traversals with its reference transcript using a
minimum-edit distance criterion. Phone confusion
matrices are then tabulated from the aggregated in-
sertion, substitution, and deletion counts across all
traversals of all lattices. From these confusion ma-
trices, we compute unsmoothed estimates of P(h|r),
the probability of a phoneme h being hypothesized
given a reference phoneme r.
Making an independence assumption, our base-
line degradation model for a query with m
</bodyText>
<table confidence="0.975288">
AY K M AA N
Vowel Consonant Semi-vowel Vowel Semi-vowel
Dipthong Voiceless plosive Nasal Back vowel Nasal
</table>
<figureCaption confidence="0.98914">
Figure 1: Three levels of annotation used by the factored
phrase-based query degradation model.
</figureCaption>
<bodyText confidence="0.986557333333333">
phonemes is then P(HIQ) = Hmi=, P(hi|ri). We
efficiently compute the most probable degradations
for a query Q using a lattice of possible degrada-
tions and the forward backward algorithm. We call
this baseline degradation approach CMQD (Confu-
sion Matrix based Query Degradation).
</bodyText>
<subsectionHeader confidence="0.998421">
3.2 Phrase-Based Query Degradation
</subsectionHeader>
<bodyText confidence="0.997897323529412">
One problem with CMQD is that we only allow in-
sertions, deletions, and one-for-one substitutions. It
may be, however, that certain pairs of phonemes
are commonly hypothesized for a particular refer-
ence phoneme (in the language of statistical machine
translation, we might say that we should allow some
non-zero fertility). Second, there is nothing to dis-
courage query degradations which are unlikely un-
der an (errorful) language model—that is, degrada-
tions that are not observed in the speech hypothe-
ses. Finally, CMQD doesn’t account for similarities
between phoneme classes. While some of these de-
ficiencies could be addressed with an extension to
CMQD (e.g., by expanding the degradation lattices
to include language model scores), we can do bet-
ter using a more powerful modeling framework. In
particular, we adopt the approach of phrase-based
statistical machine translation (Koehn et al., 2003;
Koehn and Hoang, 2007). This approach allows
for multiple-phoneme to multiple-phoneme substi-
tutions, as well as the soft incorporation of addi-
tional linguistic knowledge (e.g., phoneme classes).
This is related to previous work allowing higher or-
der phoneme confusions in bigram or trigram con-
texts (Chaudhari and Picheny, 2007), although they
used a fuzzy edit distance measure and did not in-
corporate other evidence in their model (e.g., the
phoneme language model score). The reader is re-
ferred to (Koehn and Hoang, 2007; Koehn et al.,
2007) for detailed information about phrase-based
statistical machine translation. We give a brief out-
line here, sufficient only to provide background for
our query degradation application.
Statistical machine translation systems work by
</bodyText>
<page confidence="0.995596">
185
</page>
<bodyText confidence="0.999537947368421">
converting a source-language sentence into the most
probable target-language sentence, under a model
whose parameters are estimated using example sen-
tence pairs. Phrase-based machine translation is one
variant of this statistical approach, wherein multiple-
word phrases rather than isolated words are the
basic translation unit. These phrases are gener-
ally not linguistically motivated, but rather learned
from co-occurrences in the paired example transla-
tion sentences. We apply the same machinery to hy-
pothesize our pronunciation degradations, where we
now translate from the “source-language” reference
phoneme sequence Q to the hypothesized “target-
language” phoneme sequence H.
Phrase-based translation is based on the noisy
channel model, where Bayes rule is used to refor-
mulate the translation probability for translating a
reference query Q into a hypothesized phoneme se-
quence H as
</bodyText>
<equation confidence="0.9179595">
arg max P(H|Q) = arg max P(Q|H)P(H).
H H
</equation>
<bodyText confidence="0.999971125">
Here, for example, P(H) is the language model
probability of a degradation H and P(Q|H) is the
conditional probability of the reference sequence Q
given H. More generally however, we can incorpo-
rate other feature functions of H and Q, hi(H, Q),
and with varying weights. This is implemented us-
ing a log-linear model for P(H|Q), where the model
covariates are the functions hi(H, Q), so that
</bodyText>
<equation confidence="0.9927136">
P(H|Q) = Z exp
1
i=1
n
Aihi(H, Q)
</equation>
<bodyText confidence="0.999858241935484">
The parameters Ai are estimated by MLE and the
normalizing Z need not be computed (because we
will take the argmax). Example feature functions in-
clude the language model probability of the hypoth-
esis and a hypothesis length penalty.
In addition to feature functions being defined on
the surface level of the phonemes, they may also be
defined on non-surface annotation levels, called fac-
tors. In a word translation setting, the intuition is
that statistics from morphological variants of a lex-
ical form ought to contribute to statistics for other
variants. For example, if we have never seen the
word houses in language model training, but have
examples of house, we still can expect houses are to
be more probable than houses fly. In other words,
factors allow us to collect improved statistics on
sparse data. While sparsity might appear to be less
of a problem for phoneme degradation modeling
(because the token inventory is comparatively very
small), we nevertheless may benefit from this ap-
proach, particularly because we expect to rely on
higher order language models and because we have
rather little training data: only 22,810 transcribed
utterances (about 600k reference phonemes).
In our case, we use two additional annotation lay-
ers, based on a simple grouping of phonemes into
broad classes. We consider the phoneme itself, the
broad distinction of vowel and consonant, and a finer
grained set of classes (e.g., front vowels, central
vowels, voiceless and voiced fricatives). Figure 1
shows the three annotation layers we consider for an
example reference phoneme sequence. After map-
ping the reference and hypothesized phonemes to
each of these additional factor levels, we train lan-
guage models on each of the three factor levels of
the hypothesized phonemes. The language models
for each of these factor levels are then incorporated
as features in the translation model.
We use the open source toolkit Moses (Koehn
et al., 2007) as our phrase-based machine transla-
tion system. We used the SRI language model-
ing toolkit to estimate interpolated 5-gram language
models (for each factor level), and smoothed our
estimates with Witten-Bell discounting (Witten and
Bell, 1991). We used the default parameter settings
for Moses’s training, with the exception of modi-
fying GTZA++’s default maximum fertility from 10
to 4 (since we don’t expect one reference phoneme
to align to 10 degraded phonemes). We used default
decoding settings, apart from setting the distortion
penalty to prevent any reorderings (since alignments
are logically constrained to never cross). For the rest
of this chapter, we refer to our phrase-based query
degradation model as PBQD. We denote the phrase-
based model using factors as PBQD-Fac.
Figure 2 shows an example alignment learned
for a reference and one-best phonemic transcript.
The reference utterance “snow white and the seven
dwarves” is recognized (approximately) as “no
white a the second walks”. Note that the phrase-
based system is learning not only acoustically plau-
sible confusions, but critically, also confusions aris-
</bodyText>
<page confidence="0.993188">
186
</page>
<figure confidence="0.862955333333333">
N OW W AY T AX DH AX S EH K AX N D W AO K S
S N OW W AY T AE N D DH AX S EH V AX N D W OW R F S
snow white and the seven dwarves
</figure>
<figureCaption confidence="0.994742">
Figure 2: An alignment of hypothesized and reference phoneme transcripts from the multigram phoneme recognizer,
for the phrase-based query degradation model.
</figureCaption>
<bodyText confidence="0.999305210526316">
ing from the phonemic recognition system’s pe-
culiar construction. For example, while V and
K may not be acoustically similar, they are still
confusable—within the context of S EH—because
multigram language model data has many exam-
ples of the word second. Moreover, while the word
dwarves (D-W-OW-R-F-S) is not present in the
dictionary, the words dwarf (D-W-AO-R-F) and
dwarfed (D-W-AO-R-F-T) are present (N.B., the
change of vowel from AO to OW between the OOV
and in vocabulary pronunciations). While CMQD
would have to allow a deletion and two substitutions
(without any context) to obtain the correct degrada-
tion, the phrase-based system can align the complete
phrase pair from training and exploit context. Here,
for example, it is highly probable that the errorfully
hypothesized phonemes W AO will be followed by
K, because of the prevalence of walk in language
model data.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.995596132075472">
An appropriate and commonly used measure for
RUR is Mean Average Precision (MAP). Given a
ranked list of utterances being searched through, we
define the precision at position i in the list as the pro-
portion of the top i utterances which actually contain
the corresponding query word. Average Precision
(AP) is the average of the precision values computed
for each position containing a relevant utterance. To
assess the effectiveness of a system across multi-
ple queries, Mean Average Precision is defined as
the arithmetic mean of per-query average precision,
�
MAP = 1 S,, APS,,. Throughout this paper, when
S,,
we report statistically significant improvements in
MAP, we are comparing AP for paired queries us-
ing a Wilcoxon signed rank test at α = 0.05.
Note, RUR is different than spoken term detec-
tion in two ways, and thus warrants an evaluation
measure (e.g., MAP) different than standard spoken
term detection measures (such as NIST’s actual term
weighted value (Fiscus et al., 2006)). First, STD
measures require locating a term with granularity
finer than that of an utterance. Second, STD mea-
sures are computed using a fixed detection thresh-
old. This latter requirement will be unnecessary in
many applications (e.g., where a user might prefer
to decide themselves when to stop reading down
the ranked list of retrieved utterances) and unlikely
to be helpful for downstream evidence combination
(where we may prefer to keep all putative hits and
weight them by some measure of confidence).
For our evaluation, we consider retrieving
short utterances from seventeen fully transcribed
MALACH interviews. Our query set contains all
single words occurring in these interviews that are
OOV with respect to the word dictionary. This
gives us a total of 261 query terms for evalua-
tion. Note, query words are also not present in
the multigram training transcripts, in any language
model training data, or in any transcripts used for
degradation modeling. Some example query words
include BUCHENWALD, KINDERTRANSPORT, and
SONDERKOMMANDO.
To train our degradation models, we used a held
out set of 22,810 manually transcribed utterances.
We run each recognition system (phoneme, multi-
gram, and word) on these utterances and, for each,
train separate degradation models using the aligned
reference and hypothesis transcripts. For CMQD,
we computed 100 random traversals on each lattice,
giving us a total of 2,281,000 hypothesis and refer-
ence pairs to align for our confusion matrices.
</bodyText>
<sectionHeader confidence="0.999909" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999695">
We first consider an intrinsic measure of the three
speech recognition systems we consider, namely
Phoneme Error Rate (PER). Phoneme Error Rate
is calculated by first producing an alignment of
</bodyText>
<page confidence="0.995494">
187
</page>
<bodyText confidence="0.994274430107527">
the hypothesis and reference phoneme transcripts.
The counts of each error type are used to compute
PER = 100 · S+D+I
N , where S, D, I are the num-
ber of substitutions, insertions, and deletions respec-
tively, while N is the phoneme length of the refer-
ence. Results are shown in Table 1. First, we see that
the PER for the multigram system is roughly half
that of the phoneme-only system. Second, we find
that the word system achieves a considerably lower
PER than the multigram system. We note, however,
that since these are not true phonemes (but rather
phonemes copied over from pronunciation dictionar-
ies and word transcripts), we must cautiously inter-
pret these results. In particular, it seems reasonable
that this framework will overestimate the strength
of the word based system. For comparison, on the
same train/test partition, our word-level system had
a word error rate of 31.63. Note, however, that au-
tomatic word transcripts can not contain our OOV
query words, so word error rate is reported only to
give a sense of the difficulty of the recognition task.
Table 1 shows our baseline RUR evaluation re-
sults. First, we find that the generative model yields
statistically significantly higher MAP using words
or multigrams than phonemes. This is almost cer-
tainly due to the considerably improved phoneme
recognition afforded by longer recognition units.
Second, many more unique phoneme sequences typ-
ically occur in phoneme lattices than in their word
or multigram counterparts. We expect this will in-
crease the false alarm rate for the phoneme system,
thus decreasing MAP.
Surprisingly, while the word-based recognition
system achieved considerably lower phoneme er-
ror rates than the multigram system (see Table 1),
the word-based generative model was in fact in-
distinguishable from the same model using multi-
grams. We speculate that this is because the method,
as it is essentially a language modeling approach,
is sensitive to data sparsity and requires appropri-
ate smoothing. Because multigram lattices incor-
porate smaller recognition units, which are not con-
strained to be English words, they naturally produce
smoother phoneme language models than a word-
based system. On the other hand, the multigram
system is also not statistically significantly better
than the word-based generative model, suggesting
this may be a promising area for future work.
Table 1 shows results using our degradation mod-
els. Query degradation appears to help all sys-
tems with respect to the generative baseline. This
agrees with our intuition that, for RUR, low MAP on
OOV terms is predominately driven by low recall.1
Note that, at one degradation, CMQD has the same
MAP as the generative model, since the most prob-
able degradation under CMQD is almost always the
reference phoneme sequence. Because the CMQD
model can easily hypothesize implausible degrada-
tions, we see the MAP increases modestly with a
few degradations, but then MAP decreases. In con-
trast, the MAP of the phrase-based system (PBQD-
Fac) increases through to 500 query degradations us-
ing multigrams. The phonemic system appears to
achieve its peak MAP with fewer degradations, but
also has a considerably lower best value.
The non-factored phrase-based system PBQD
achieves a peak MAP considerably larger than the
peak CMQD approach. And, likewise, using addi-
tional factor levels (PBQD-Fac) also considerably
improves performance. Note especially that, using
multiple factor levels, we not only achieve a higher
MAP, but also a higher MAP when only a few degra-
dations are possible.
To account for errors in phonemic recognition, we
have taken two steps. First, we used longer recog-
nition units which we found significantly improved
MAP while using our baseline RUR technique. As
a second method for handling recognition errors,
we also considered variants of our ranking func-
tion. In particular, we incorporated query degrada-
tions hypothesized using factored phrase-based ma-
chine translation. Comparing the MAP for PBQD-
Fac with MAP using the generative baseline for the
most improved indexing system (the word system),
we find that this degradation approach again statisti-
cally significantly improved MAP. That is, these two
strategies for handling recognition errors in RUR ap-
pear to work well in combination.
Although we focused on vocabulary-independent
RUR, downstream tasks such as ad hoc speech
retrieval will also want to incorporate evidence
from in-vocabulary query words. This makes
</bodyText>
<footnote confidence="0.9534785">
1We note however that the preferred operating point in the
tradeoff between precision and recall will be task specific. For
example, it is known that precision errors become increasingly
important as collection size grows (Shao et al., 2008).
</footnote>
<page confidence="0.982601">
188
</page>
<table confidence="0.999271857142857">
Method Phone Source PER QD Model Baseline Query Degradations
1 5 50 500
Degraded Model Phonemes 64.4 PBQD-Fac 0.0387 0.0479 0.0581 0.0614 0.0612
Multigrams 32.1 CMQD 0.1258 0.1258 0.1272 0.1158 0.0991
Multigrams 32.1 PBQD 0.1258 0.1160 0.1283 0.1347 0.1317
Multigrams 32.1 PBQD-Fac 0.1258 0.1238 0.1399 0.1510 0.1527
Words 20.5 PBQD-Fac 0.1255 0.1162 0.1509 0.1787 0.1753
</table>
<tableCaption confidence="0.9886155">
Table 1: PER and MAP results for baseline and degradation models. The best result for each indexing approach is
shown in bold.
</tableCaption>
<bodyText confidence="0.999606857142857">
our query degradation approach which indexed
phonemes from word-based LVCSR particularly at-
tractive. Not only did it achieve the best MAP in
our evaluation, but this approach also allows us to
construct recognition lattices for both in and out-of-
vocabulary query words without running a second,
costly, recognition step.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999982576923077">
Our goal in this work was to rank utterances by our
confidence that they contained a previously unseen
query word. We proposed a new approach to this
task using hypothesized degradations of the query
word’s phoneme sequence, which we produced us-
ing a factored phrase-based machine translation
model. This approach was principally motivated by
the mismatch between the query’s phonemes and
the recognition phoneme sequences due to errorful
speech indexing. Our approach was constructed and
evaluated using phoneme-, multigram-, and word-
based indexing, and significant improvements in
MAP using each indexing system were achieved.
Critically, these significant improvements were in
addition to the significant gains we achieved by con-
structing our index with longer recognition units.
While PBQD-Fac outperformed CMQD averag-
ing over all queries in our evaluation, as expected,
there may be particular query words for which this
is not the case. Table 2 shows example degrada-
tions using both the CMQD and PBQD-Fac degra-
dation models for multigrams. The query word is
Mengele. We see that CMQD degradations are near
(in an edit distance sense) to the reference pronun-
ciation (M-EH-NX-EY-L-EH), while the phrase-
based degradations tend to sound like commonly oc-
</bodyText>
<sectionHeader confidence="0.869241" genericHeader="acknowledgments">
CMQD Phrase-based
</sectionHeader>
<table confidence="0.6114962">
M-EH-NX-EY-L-EH M-EH-N-T-AX-L
M-EH-NX-EY-L M-EH-N-T-AX-L-AA-T
M-NX-EY-L-EH AH-AH-AH-AH-M-EH-N-T-AX-L
M-EH-NX-EY-EH M-EH-N-DH-EY-L-EH
M-EH-NX-L-EH M-EH-N-T-AX-L-IY
</table>
<tableCaption confidence="0.968540666666667">
Table 2: The top five degradations and associated proba-
bilities using the CMQD and PBQD-Fac models, for the
term Mengele using multigram indexing.
</tableCaption>
<bodyText confidence="0.999761956521739">
curring words (mental, meant a lot, men they...,
mentally). In this case, the lexical phoneme se-
quence does not occur in the PBQD-Fac degrada-
tions until degradation nineteen. Because delet-
ing EH has the same cost irrespective of context
for CMQD, both CMQD degradations 2 and 3 are
given the same pronunciation weight. Here, CMQD
performs considerably better, achieving an average
precision of 0.1707, while PBQD-Fac obtains only
0.0300. This suggests that occasionally the phrase-
based language model may exert too much influence
on the degradations, which is likely to increase the
incidence of false alarms. One solution, for future
work, might be to incorporate a false alarm model
(e.g., down-weighting putative occurrences which
look suspiciously like non-query words). Second,
we might consider training the degradation model
in a discriminative framework (e.g., training to op-
timize a measure that will penalize degradations
which cause false alarms, even if they are good can-
didates from the perspective of MLE). We hope that
the ideas presented in this paper will provide a solid
foundation for this future work.
</bodyText>
<page confidence="0.99846">
189
</page>
<sectionHeader confidence="0.977339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998686320754717">
W. Byrne et al. 2004. Automatic Recognition of Spon-
taneous Speech for Access to Multilingual Oral His-
tory Archives. IEEE Transactions on Speech and Au-
dio Processing, Special Issue on Spontaneous Speech
Processing, 12(4):420–435, July.
U.V. Chaudhari and M. Picheny. 2007. Improvements in
phone based audio search via constrained match with
high order confusion estimates. Automatic Speech
Recognition &amp; Understanding, 2007. ASRU. IEEE
Workshop on, pages 665–670, Dec.
Kareem Darwish and Walid Magdy. 2007. Error cor-
rection vs. query garbling for Arabic OCR document
retrieval. ACM Trans. Inf. Syst., 26(1):5.
Kareem M. Darwish. 2003. Probabilistic Methods for
Searching OCR-Degraded Arabic Text. Ph.D. thesis,
University of Maryland, College Park, MD, USA. Di-
rected by Bruce Jacob and Douglas W. Oard.
S. Deligne and F. Bimbot. 1997. Inference of Variable-
length Acoustic Units for Continuous Speech Recog-
nition. In ICASSP ’97: Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech, and Signal
Processing, pages 1731–1734, Munich, Germany.
Jonathan Fiscus et al. 2006. English Spoken Term De-
tection 2006 Results. In Presentation at NIST’s 2006
STD Eval Workshop.
J.T. Foote et al. 1997. Unconstrained keyword spot-
ting using phone lattices with application to spoken
document retrieval. Computer Speech and Language,
11:207—224.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In EMNLP ’07: Conference on Empiri-
cal Methods in Natural Language Processing, June.
Philipp Koehn et al. 2003. Statistical phrase-based
translation. In NAACL ’03: Proceedings of the 2003
Conference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology, pages 48–54, Morristown, NJ,
USA. Association for Computational Linguistics.
Philipp Koehn et al. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In ACL ’07: Pro-
ceedings of the 2007 Conference of the Association
for Computational Linguistics, demonstration session,
June.
Okan Kolak. 2005. Rapid Resource Transfer for Mul-
tilingual Natural Language Processing. Ph.D. thesis,
University of Maryland, College Park, MD, USA. Di-
rected by Philip Resnik.
Jonathan Mamou and Bhuvana Ramabhadran. 2008.
Phonetic Query Expansion for Spoken Document Re-
trieval. In Interspeech ’08: Conference of the Interna-
tional Speech Communication Association.
Spyros Matsoukas et al. 2005. The 2004 BBN 1xRT
Recognition Systems for English Broadcast News and
Conversational Telephone Speech. In Interspeech ’05:
Conference of the International Speech Communica-
tion Association, pages 1641–1644.
K. Ng and V.W. Zue. 2000. Subword-based approaches
for spoken document retrieval. Speech Commun.,
32(3):157–186.
J. Scott Olsson. 2008a. Combining Speech Retrieval Re-
sults with Generalized Additive Models. In ACL ’08:
Proceedings of the 2008 Conference of the Association
for Computational Linguistics.
J. Scott Olsson. 2008b. Vocabulary Independent Dis-
criminative Term Frequency Estimation. In Inter-
speech ’08: Conference of the International Speech
Communication Association.
Pavel Pecina, Petra Hoffmannova, Gareth J.F. Jones, Jian-
qiang Wang, and Douglas W. Oard. 2007. Overview
of the CLEF-2007 Cross-Language Speech Retrieval
Track. In Proceedings of the CLEF 2007 Workshop
on Cross-Language Information Retrieval and Evalu-
ation, September.
R. Prasad et al. 2005. The 2004 BBN/LIMSI 20xRT En-
glish Conversational Telephone Speech Recognition
System. In Interspeech ’05: Conference of the Inter-
national Speech Communication Association.
S.E. Robertson. 1977. The Probability Ranking Princi-
ple in IR. Journal of Documentation, pages 281–286.
M. Saraclar and R. Sproat. 2004. Lattice-Based Search
for Spoken Utterance Retrieval. In NAACL ’04: Pro-
ceedings of the 2004 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology.
P. Schone et al. 2005. Searching Conversational Tele-
phone Speech in Any of the World’s Languages.
Jian Shao et al. 2008. Towards Vocabulary-Independent
Speech Indexing for Large-Scale Repositories. In In-
terspeech ’08: Conference of the International Speech
Communication Association.
A. Stolcke. 2002. SRILM – an extensible language mod-
eling toolkit. In ICSLP ’02: Proceedings of 2002 In-
ternational Conference on Spoken Language Process-
ing.
I. H. Witten and T. C. Bell. 1991. The Zero-Frequency
Problem: Estimating the Probabilities of Novel Events
in Adaptive Text Compression. IEEE Trans. Informa-
tion Theory, 37(4):1085–1094.
Peng Yu and Frank Seide. 2005. Fast Two-
Stage Vocabulary-Independent Search In Spontaneous
Speech. In ICASSP ’05: Proceedings of the 2005
IEEE International Conference on Acoustics, Speech,
and Signal Processing.
P. Yu et al. Sept. 2005. Vocabulary-Independent Index-
ing of Spontaneous Speech. IEEE Transactions on
Speech and Audio Processing, 13(5):635–643.
</reference>
<page confidence="0.997863">
190
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.711334">
<title confidence="0.9970415">Phrase-Based Query Degradation Modeling Vocabulary-Independent Ranked Utterance Retrieval</title>
<author confidence="0.999793">J Scott</author>
<affiliation confidence="0.863462">HLT Center of Johns Hopkins</affiliation>
<address confidence="0.990996">Baltimore, MD 21211,</address>
<email confidence="0.999787">solsson@jhu.edu</email>
<author confidence="0.999908">Douglas W Oard</author>
<affiliation confidence="0.999912">College of Information University of</affiliation>
<address confidence="0.996788">College Park, MD 15213,</address>
<email confidence="0.998942">oard@umd.edu</email>
<abstract confidence="0.999616727272727">This paper introduces a new approach to ranking speech utterances by a system’s confidence that they contain a spoken word. Multiple alternate pronunciations, or degradations, of a query word’s phoneme sequence are hypothesized and incorporated into the ranking function. We consider two methods for hypothesizing these degradations, the best of which is constructed using factored phrasebased statistical machine translation. We show that this approach is able to significantly improve upon a state-of-the-art baseline technique in an evaluation on held-out speech. We evaluate our systems using three different methods for indexing the speech utterances (using phoneme, phoneme multigram, and word recognition), and find that degradation modeling shows particular promise for locating out-of-vocabulary words when the underlying indexing system is constructed with standard word-based speech recognition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Byrne</author>
</authors>
<title>Automatic Recognition of Spontaneous Speech for Access to Multilingual Oral History Archives.</title>
<date>2004</date>
<journal>IEEE Transactions on Speech and Audio Processing, Special Issue on Spontaneous Speech Processing,</journal>
<volume>12</volume>
<issue>4</issue>
<marker>Byrne, 2004</marker>
<rawString>W. Byrne et al. 2004. Automatic Recognition of Spontaneous Speech for Access to Multilingual Oral History Archives. IEEE Transactions on Speech and Audio Processing, Special Issue on Spontaneous Speech Processing, 12(4):420–435, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U V Chaudhari</author>
<author>M Picheny</author>
</authors>
<title>Improvements in phone based audio search via constrained match with high order confusion estimates. Automatic Speech Recognition &amp; Understanding,</title>
<date>2007</date>
<booktitle>ASRU. IEEE Workshop on,</booktitle>
<pages>665--670</pages>
<contexts>
<context position="9940" citStr="Chaudhari and Picheny, 2007" startWordPosition="1562" endWordPosition="1565">query degradation as any phoneme sequence (including the lexical sequence) which may, with some estimated probability, occur in an errorful phonemic representation of the audio (either a one-best or lattice hypothesis). Because of speaker variation and because recognition is errorful, we ought to also consider non-lexical degradations of the query phoneme sequence. That is, we should incorporate P(H|Q) in our ranking function. It has previously been demonstrated that allowing for phoneme confusability can significantly increase spoken term detection performance on onebest phoneme transcripts (Chaudhari and Picheny, 2007; Schone et al., 2005) and in phonemic lattices (Foote et al., 1997). These methods work by allowing weighted substitution costs in minimumedit-distance matching. Previously, these substitution costs have been maximum-likelihood estimates of P(H|Q) for each phoneme, where P(H|Q) is easily computed from a phoneme confusion matrix after aligning the reference and one-best hypothesis transcript under a minimum edit distance criterion. Similar methods have also been used in other language processing applications. For example, in (Kolak, 2005), one-for-one character substitutions, insertions and de</context>
<context position="16430" citStr="Chaudhari and Picheny, 2007" startWordPosition="2582" endWordPosition="2585">iciencies could be addressed with an extension to CMQD (e.g., by expanding the degradation lattices to include language model scores), we can do better using a more powerful modeling framework. In particular, we adopt the approach of phrase-based statistical machine translation (Koehn et al., 2003; Koehn and Hoang, 2007). This approach allows for multiple-phoneme to multiple-phoneme substitutions, as well as the soft incorporation of additional linguistic knowledge (e.g., phoneme classes). This is related to previous work allowing higher order phoneme confusions in bigram or trigram contexts (Chaudhari and Picheny, 2007), although they used a fuzzy edit distance measure and did not incorporate other evidence in their model (e.g., the phoneme language model score). The reader is referred to (Koehn and Hoang, 2007; Koehn et al., 2007) for detailed information about phrase-based statistical machine translation. We give a brief outline here, sufficient only to provide background for our query degradation application. Statistical machine translation systems work by 185 converting a source-language sentence into the most probable target-language sentence, under a model whose parameters are estimated using example s</context>
</contexts>
<marker>Chaudhari, Picheny, 2007</marker>
<rawString>U.V. Chaudhari and M. Picheny. 2007. Improvements in phone based audio search via constrained match with high order confusion estimates. Automatic Speech Recognition &amp; Understanding, 2007. ASRU. IEEE Workshop on, pages 665–670, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kareem Darwish</author>
<author>Walid Magdy</author>
</authors>
<title>Error correction vs. query garbling for Arabic OCR document retrieval.</title>
<date>2007</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="11371" citStr="Darwish and Magdy, 2007" startWordPosition="1787" endWordPosition="1790">ethod of incorporating query degradations in the ranking function. Given a degradation model P(H|Q), we take as our ranking function the expectation of the generative baseline estimate NL · Pˆ(H|L) with respect to P(H|Q), � � Pˆ(H|L) · NL · P(H|Q), (2) HEW where H is the set of degradations. Note that, while we consider the expected value of our baseline term frequency estimator with respect to P(H|Q), this general approach could be used with any other term frequency estimator. Our formulation is similar to approaches taken in OCR document retrieval, using degradations of character sequences (Darwish and Magdy, 2007; Darwish, 2003). For vocabulary-independent spoken term detection, perhaps the most closely related formulation is provided by (Mamou and Ramabhadran, 2008). In that work, they ranked utterances by the weighted average of their matching score, where the weights were confidences from a grapheme to phoneme system’s first several hypotheses for a word’s pronunciation. The matching scores were edit distances, where substitution costs were weighted using phoneme confusability. Accordingly, their formulation was not aimed at accounting for errors in recognition per se, but rather for errors in hypo</context>
</contexts>
<marker>Darwish, Magdy, 2007</marker>
<rawString>Kareem Darwish and Walid Magdy. 2007. Error correction vs. query garbling for Arabic OCR document retrieval. ACM Trans. Inf. Syst., 26(1):5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kareem M Darwish</author>
</authors>
<title>Probabilistic Methods for Searching OCR-Degraded Arabic Text.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland,</institution>
<location>College Park, MD, USA.</location>
<note>Directed by</note>
<contexts>
<context position="11387" citStr="Darwish, 2003" startWordPosition="1791" endWordPosition="1792">ery degradations in the ranking function. Given a degradation model P(H|Q), we take as our ranking function the expectation of the generative baseline estimate NL · Pˆ(H|L) with respect to P(H|Q), � � Pˆ(H|L) · NL · P(H|Q), (2) HEW where H is the set of degradations. Note that, while we consider the expected value of our baseline term frequency estimator with respect to P(H|Q), this general approach could be used with any other term frequency estimator. Our formulation is similar to approaches taken in OCR document retrieval, using degradations of character sequences (Darwish and Magdy, 2007; Darwish, 2003). For vocabulary-independent spoken term detection, perhaps the most closely related formulation is provided by (Mamou and Ramabhadran, 2008). In that work, they ranked utterances by the weighted average of their matching score, where the weights were confidences from a grapheme to phoneme system’s first several hypotheses for a word’s pronunciation. The matching scores were edit distances, where substitution costs were weighted using phoneme confusability. Accordingly, their formulation was not aimed at accounting for errors in recognition per se, but rather for errors in hypothesizing pronun</context>
</contexts>
<marker>Darwish, 2003</marker>
<rawString>Kareem M. Darwish. 2003. Probabilistic Methods for Searching OCR-Degraded Arabic Text. Ph.D. thesis, University of Maryland, College Park, MD, USA. Directed by Bruce Jacob and Douglas W. Oard.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deligne</author>
<author>F Bimbot</author>
</authors>
<title>Inference of Variablelength Acoustic Units for Continuous Speech Recognition.</title>
<date>1997</date>
<booktitle>In ICASSP ’97: Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>1731--1734</pages>
<location>Munich, Germany.</location>
<contexts>
<context position="5552" citStr="Deligne and Bimbot (1997)" startWordPosition="852" endWordPosition="855">he lattice (what a system might return as a transcription) is the path through the lattice having highest probability under the acoustic and language models. Each RUR model we consider is constructed using the expected counts of a query word’s phoneme sequences in these recognition lattices. We consider three approaches to producing these phoneme lattices, using standard word-based LVCSR, phoneme recognition, and LVCSR using phoneme multigrams. Our word system’s dictionary contains about 50,000 entries, while the phoneme system contains 39 phonemes from the ARPABET set. Originally proposed by Deligne and Bimbot (1997) to model variable length regularities in streams of symbols (e.g., words, graphemes, or phonemes), phoneme multigrams are short sequences of one or more phonemes. We produce a set of “phoneme transcripts” by replacing transcript words with their lexical pronunciation. The set of multigrams is learned by then choosing a maximumlikelihood segmentation of these training phoneme transcripts, where the segmentation is viewed as hidden data in an Expectation-Maximization algorithm. The set of all continuous phonemes occurring between segment boundaries is then chosen as our multigram dictionary. Th</context>
</contexts>
<marker>Deligne, Bimbot, 1997</marker>
<rawString>S. Deligne and F. Bimbot. 1997. Inference of Variablelength Acoustic Units for Continuous Speech Recognition. In ICASSP ’97: Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 1731–1734, Munich, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Fiscus</author>
</authors>
<title>English Spoken Term Detection</title>
<date>2006</date>
<booktitle>In Presentation at NIST’s 2006 STD Eval Workshop.</booktitle>
<marker>Fiscus, 2006</marker>
<rawString>Jonathan Fiscus et al. 2006. English Spoken Term Detection 2006 Results. In Presentation at NIST’s 2006 STD Eval Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Foote</author>
</authors>
<title>Unconstrained keyword spotting using phone lattices with application to spoken document retrieval.</title>
<date>1997</date>
<journal>Computer Speech and Language,</journal>
<pages>11--207</pages>
<marker>Foote, 1997</marker>
<rawString>J.T. Foote et al. 1997. Unconstrained keyword spotting using phone lattices with application to spoken document retrieval. Computer Speech and Language, 11:207—224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored Translation Models.</title>
<date>2007</date>
<booktitle>In EMNLP ’07: Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="16124" citStr="Koehn and Hoang, 2007" startWordPosition="2537" endWordPosition="2540">e non-zero fertility). Second, there is nothing to discourage query degradations which are unlikely under an (errorful) language model—that is, degradations that are not observed in the speech hypotheses. Finally, CMQD doesn’t account for similarities between phoneme classes. While some of these deficiencies could be addressed with an extension to CMQD (e.g., by expanding the degradation lattices to include language model scores), we can do better using a more powerful modeling framework. In particular, we adopt the approach of phrase-based statistical machine translation (Koehn et al., 2003; Koehn and Hoang, 2007). This approach allows for multiple-phoneme to multiple-phoneme substitutions, as well as the soft incorporation of additional linguistic knowledge (e.g., phoneme classes). This is related to previous work allowing higher order phoneme confusions in bigram or trigram contexts (Chaudhari and Picheny, 2007), although they used a fuzzy edit distance measure and did not incorporate other evidence in their model (e.g., the phoneme language model score). The reader is referred to (Koehn and Hoang, 2007; Koehn et al., 2007) for detailed information about phrase-based statistical machine translation. </context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored Translation Models. In EMNLP ’07: Conference on Empirical Methods in Natural Language Processing, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Koehn, 2003</marker>
<rawString>Philipp Koehn et al. 2003. Statistical phrase-based translation. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation. In</title>
<date>2007</date>
<booktitle>ACL ’07: Proceedings of the 2007 Conference of the Association for Computational Linguistics, demonstration session,</booktitle>
<marker>Koehn, 2007</marker>
<rawString>Philipp Koehn et al. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL ’07: Proceedings of the 2007 Conference of the Association for Computational Linguistics, demonstration session, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okan Kolak</author>
</authors>
<title>Rapid Resource Transfer for Multilingual Natural Language Processing.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland,</institution>
<location>College Park, MD, USA.</location>
<note>Directed by Philip Resnik.</note>
<contexts>
<context position="10484" citStr="Kolak, 2005" startWordPosition="1644" endWordPosition="1646">formance on onebest phoneme transcripts (Chaudhari and Picheny, 2007; Schone et al., 2005) and in phonemic lattices (Foote et al., 1997). These methods work by allowing weighted substitution costs in minimumedit-distance matching. Previously, these substitution costs have been maximum-likelihood estimates of P(H|Q) for each phoneme, where P(H|Q) is easily computed from a phoneme confusion matrix after aligning the reference and one-best hypothesis transcript under a minimum edit distance criterion. Similar methods have also been used in other language processing applications. For example, in (Kolak, 2005), one-for-one character substitutions, insertions and deletions were considered in a generative model of errors in OCR. In this work, because we are focused on constructing inverted indices of audio files (for speed and to conserve space), we must generalize our method of incorporating query degradations in the ranking function. Given a degradation model P(H|Q), we take as our ranking function the expectation of the generative baseline estimate NL · Pˆ(H|L) with respect to P(H|Q), � � Pˆ(H|L) · NL · P(H|Q), (2) HEW where H is the set of degradations. Note that, while we consider the expected v</context>
</contexts>
<marker>Kolak, 2005</marker>
<rawString>Okan Kolak. 2005. Rapid Resource Transfer for Multilingual Natural Language Processing. Ph.D. thesis, University of Maryland, College Park, MD, USA. Directed by Philip Resnik.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Mamou</author>
<author>Bhuvana Ramabhadran</author>
</authors>
<title>Phonetic Query Expansion for Spoken Document Retrieval.</title>
<date>2008</date>
<booktitle>In Interspeech ’08: Conference of the International Speech Communication Association.</booktitle>
<contexts>
<context position="11528" citStr="Mamou and Ramabhadran, 2008" startWordPosition="1809" endWordPosition="1813">the generative baseline estimate NL · Pˆ(H|L) with respect to P(H|Q), � � Pˆ(H|L) · NL · P(H|Q), (2) HEW where H is the set of degradations. Note that, while we consider the expected value of our baseline term frequency estimator with respect to P(H|Q), this general approach could be used with any other term frequency estimator. Our formulation is similar to approaches taken in OCR document retrieval, using degradations of character sequences (Darwish and Magdy, 2007; Darwish, 2003). For vocabulary-independent spoken term detection, perhaps the most closely related formulation is provided by (Mamou and Ramabhadran, 2008). In that work, they ranked utterances by the weighted average of their matching score, where the weights were confidences from a grapheme to phoneme system’s first several hypotheses for a word’s pronunciation. The matching scores were edit distances, where substitution costs were weighted using phoneme confusability. Accordingly, their formulation was not aimed at accounting for errors in recognition per se, but rather for errors in hypothesizing pronunciations. We expect this accounts for their lack of significant improvement using the method. Since we don’t want to sum over all possible re</context>
</contexts>
<marker>Mamou, Ramabhadran, 2008</marker>
<rawString>Jonathan Mamou and Bhuvana Ramabhadran. 2008. Phonetic Query Expansion for Spoken Document Retrieval. In Interspeech ’08: Conference of the International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
</authors>
<date>2005</date>
<booktitle>The 2004 BBN 1xRT Recognition Systems for English Broadcast News and Conversational Telephone Speech. In Interspeech ’05: Conference of the International Speech Communication Association,</booktitle>
<pages>1641--1644</pages>
<marker>Matsoukas, 2005</marker>
<rawString>Spyros Matsoukas et al. 2005. The 2004 BBN 1xRT Recognition Systems for English Broadcast News and Conversational Telephone Speech. In Interspeech ’05: Conference of the International Speech Communication Association, pages 1641–1644.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ng</author>
<author>V W Zue</author>
</authors>
<title>Subword-based approaches for spoken document retrieval.</title>
<date>2000</date>
<journal>Speech Commun.,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="1970" citStr="Ng and Zue, 2000" startWordPosition="304" endWordPosition="307"> set of utterances by our confidence that they contain the query word, a task known as Ranked Utterance Retrieval (RUR). In particular, we are interested in the case when the user’s query word can not be anticipated by a Large Vocabulary Continuous Speech Recognizer’s (LVCSR) decoding dictionary, so that the word is said to be Out-OfVocabulary (OOV). Rare words tend to be the most informative, but are also most likely to be OOV. When words are OOV, we must use vocabulary-independent techniques to locate them. One popular approach is to search for the words in output from a phoneme recognizer (Ng and Zue, 2000), although this suffers from the low accuracy typical of phoneme recognition. We consider two methods for handling this inaccuracy. First, we compare an RUR indexing system using phonemes with two systems using longer recognition units: words or phoneme multigrams. Second, we consider several methods for handling the recognition inaccuracy in the utterance ranking function itself. Our baseline generative model handles errorful recognition by estimating term frequencies from smoothed language models trained on phoneme lattices. Our new approach, which we call query degradation, hypothesizes man</context>
</contexts>
<marker>Ng, Zue, 2000</marker>
<rawString>K. Ng and V.W. Zue. 2000. Subword-based approaches for spoken document retrieval. Speech Commun., 32(3):157–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Scott Olsson</author>
</authors>
<title>Combining Speech Retrieval Results with Generalized Additive Models.</title>
<date>2008</date>
<booktitle>In ACL ’08: Proceedings of the 2008 Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3116" citStr="Olsson, 2008" startWordPosition="477" endWordPosition="478">ur new approach, which we call query degradation, hypothesizes many alternate “pronunciations” for the query word and incorporates them into the ranking function. These degradations are translations of the lexical phoneme sequence into the errorful recognition language, which we hypothesize using a factored phrase-based statistical machine translation system. Our speech collection is a set of oral history interviews from the MALACH collection (Byrne et al., 2004), which has previously been used for ad hoc speech retrieval evaluations using one-best word level transcripts (Pecina et al., 2007; Olsson, 2008a) and for vocabulary-independent RUR (Olsson, 2008b). The interviews were conducted with survivors and witnesses of the Holocaust, who discuss their experiences before, during, and after the Second World War. Their speech is predominately spontaneous and conversational. It is often also emotional and heavily accented. Because the speech contains many words unlikely to occur within a general purpose speech recognition lexicon, it repre182 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 182–190, Boulder, Colorado, June 2009. c�2009 Associa</context>
<context position="6312" citStr="Olsson (2008" startWordPosition="966" endWordPosition="967"> more phonemes. We produce a set of “phoneme transcripts” by replacing transcript words with their lexical pronunciation. The set of multigrams is learned by then choosing a maximumlikelihood segmentation of these training phoneme transcripts, where the segmentation is viewed as hidden data in an Expectation-Maximization algorithm. The set of all continuous phonemes occurring between segment boundaries is then chosen as our multigram dictionary. This multigram recognition dictionary contains 16,409 entries. After we have obtained each recognition lattice, our indexing approach follows that of Olsson (2008b). Namely, for the word and multigram systems, we first expand lattice arcs containing multiple phones to produce a lattice having only single phonemes on its arcs. Then, we compute the expected count of all phoneme n-grams n ≤ 5 in the lattice. These n-grams and their counts are inserted in our inverted index for retrieval. This paper is organized as follows. In Section 2 we introduce our baseline RUR methods. In Section 3 we introduce our query degradation approach. We introduce our experimental validation in Section 4 and our results in Section 5. We find that using phrase-based query degr</context>
</contexts>
<marker>Olsson, 2008</marker>
<rawString>J. Scott Olsson. 2008a. Combining Speech Retrieval Results with Generalized Additive Models. In ACL ’08: Proceedings of the 2008 Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Scott Olsson</author>
</authors>
<title>Vocabulary Independent Discriminative Term Frequency Estimation.</title>
<date>2008</date>
<booktitle>In Interspeech ’08: Conference of the International Speech Communication Association.</booktitle>
<contexts>
<context position="3116" citStr="Olsson, 2008" startWordPosition="477" endWordPosition="478">ur new approach, which we call query degradation, hypothesizes many alternate “pronunciations” for the query word and incorporates them into the ranking function. These degradations are translations of the lexical phoneme sequence into the errorful recognition language, which we hypothesize using a factored phrase-based statistical machine translation system. Our speech collection is a set of oral history interviews from the MALACH collection (Byrne et al., 2004), which has previously been used for ad hoc speech retrieval evaluations using one-best word level transcripts (Pecina et al., 2007; Olsson, 2008a) and for vocabulary-independent RUR (Olsson, 2008b). The interviews were conducted with survivors and witnesses of the Holocaust, who discuss their experiences before, during, and after the Second World War. Their speech is predominately spontaneous and conversational. It is often also emotional and heavily accented. Because the speech contains many words unlikely to occur within a general purpose speech recognition lexicon, it repre182 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 182–190, Boulder, Colorado, June 2009. c�2009 Associa</context>
<context position="6312" citStr="Olsson (2008" startWordPosition="966" endWordPosition="967"> more phonemes. We produce a set of “phoneme transcripts” by replacing transcript words with their lexical pronunciation. The set of multigrams is learned by then choosing a maximumlikelihood segmentation of these training phoneme transcripts, where the segmentation is viewed as hidden data in an Expectation-Maximization algorithm. The set of all continuous phonemes occurring between segment boundaries is then chosen as our multigram dictionary. This multigram recognition dictionary contains 16,409 entries. After we have obtained each recognition lattice, our indexing approach follows that of Olsson (2008b). Namely, for the word and multigram systems, we first expand lattice arcs containing multiple phones to produce a lattice having only single phonemes on its arcs. Then, we compute the expected count of all phoneme n-grams n ≤ 5 in the lattice. These n-grams and their counts are inserted in our inverted index for retrieval. This paper is organized as follows. In Section 2 we introduce our baseline RUR methods. In Section 3 we introduce our query degradation approach. We introduce our experimental validation in Section 4 and our results in Section 5. We find that using phrase-based query degr</context>
</contexts>
<marker>Olsson, 2008</marker>
<rawString>J. Scott Olsson. 2008b. Vocabulary Independent Discriminative Term Frequency Estimation. In Interspeech ’08: Conference of the International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Pecina</author>
<author>Petra Hoffmannova</author>
<author>Gareth J F Jones</author>
<author>Jianqiang Wang</author>
<author>Douglas W Oard</author>
</authors>
<title>Overview of the CLEF-2007 Cross-Language Speech Retrieval Track.</title>
<date>2007</date>
<booktitle>In Proceedings of the CLEF 2007 Workshop on Cross-Language Information Retrieval and Evaluation,</booktitle>
<contexts>
<context position="3102" citStr="Pecina et al., 2007" startWordPosition="473" endWordPosition="476">n phoneme lattices. Our new approach, which we call query degradation, hypothesizes many alternate “pronunciations” for the query word and incorporates them into the ranking function. These degradations are translations of the lexical phoneme sequence into the errorful recognition language, which we hypothesize using a factored phrase-based statistical machine translation system. Our speech collection is a set of oral history interviews from the MALACH collection (Byrne et al., 2004), which has previously been used for ad hoc speech retrieval evaluations using one-best word level transcripts (Pecina et al., 2007; Olsson, 2008a) and for vocabulary-independent RUR (Olsson, 2008b). The interviews were conducted with survivors and witnesses of the Holocaust, who discuss their experiences before, during, and after the Second World War. Their speech is predominately spontaneous and conversational. It is often also emotional and heavily accented. Because the speech contains many words unlikely to occur within a general purpose speech recognition lexicon, it repre182 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 182–190, Boulder, Colorado, June 2009. </context>
</contexts>
<marker>Pecina, Hoffmannova, Jones, Wang, Oard, 2007</marker>
<rawString>Pavel Pecina, Petra Hoffmannova, Gareth J.F. Jones, Jianqiang Wang, and Douglas W. Oard. 2007. Overview of the CLEF-2007 Cross-Language Speech Retrieval Track. In Proceedings of the CLEF 2007 Workshop on Cross-Language Information Retrieval and Evaluation, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
</authors>
<title>The</title>
<date>2005</date>
<booktitle>In Interspeech ’05: Conference of the International Speech Communication Association.</booktitle>
<marker>Prasad, 2005</marker>
<rawString>R. Prasad et al. 2005. The 2004 BBN/LIMSI 20xRT English Conversational Telephone Speech Recognition System. In Interspeech ’05: Conference of the International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
</authors>
<title>The Probability Ranking Principle in IR.</title>
<date>1977</date>
<journal>Journal of Documentation,</journal>
<pages>281--286</pages>
<contexts>
<context position="7423" citStr="Robertson, 1977" startWordPosition="1152" endWordPosition="1153">rimental validation in Section 4 and our results in Section 5. We find that using phrase-based query degradations can significantly improve upon a strong RUR baseline. Finally, in Section 6 we conclude and outline several directions for future work. 2 Generative Baseline Each method we present in this paper ranks the utterances by the term’s estimated frequency within the corresponding phoneme lattice. This general approach has previously been considered (Yu and Seide, 2005; Saraclar and Sproat, 2004), on the basis that it provides a minimum Bayes-risk ranking criterion (Yu et al., Sept 2005; Robertson, 1977) for the utterances. What differs for each method is the particular estimator of term frequency which is used. We first outline our baseline approach, a generative model for term frequency estimation. Recall that our vocabulary-independent indices contain the expected counts of phoneme sequences from our recognition lattices. Yu and Seide (2005) used these expected phoneme sequence counts to estimate term frequency in the following way. For a t_fG is estimated as tfG(Q, L) = P(Q|L) · NL, where NL is an estimate for the number of words in the utterance. The conditional P(Q|L) is modeled as an o</context>
</contexts>
<marker>Robertson, 1977</marker>
<rawString>S.E. Robertson. 1977. The Probability Ranking Principle in IR. Journal of Documentation, pages 281–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Saraclar</author>
<author>R Sproat</author>
</authors>
<title>Lattice-Based Search for Spoken Utterance Retrieval.</title>
<date>2004</date>
<booktitle>In NAACL ’04: Proceedings of the 2004 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology.</booktitle>
<contexts>
<context position="7313" citStr="Saraclar and Sproat, 2004" startWordPosition="1131" endWordPosition="1134">2 we introduce our baseline RUR methods. In Section 3 we introduce our query degradation approach. We introduce our experimental validation in Section 4 and our results in Section 5. We find that using phrase-based query degradations can significantly improve upon a strong RUR baseline. Finally, in Section 6 we conclude and outline several directions for future work. 2 Generative Baseline Each method we present in this paper ranks the utterances by the term’s estimated frequency within the corresponding phoneme lattice. This general approach has previously been considered (Yu and Seide, 2005; Saraclar and Sproat, 2004), on the basis that it provides a minimum Bayes-risk ranking criterion (Yu et al., Sept 2005; Robertson, 1977) for the utterances. What differs for each method is the particular estimator of term frequency which is used. We first outline our baseline approach, a generative model for term frequency estimation. Recall that our vocabulary-independent indices contain the expected counts of phoneme sequences from our recognition lattices. Yu and Seide (2005) used these expected phoneme sequence counts to estimate term frequency in the following way. For a t_fG is estimated as tfG(Q, L) = P(Q|L) · N</context>
</contexts>
<marker>Saraclar, Sproat, 2004</marker>
<rawString>M. Saraclar and R. Sproat. 2004. Lattice-Based Search for Spoken Utterance Retrieval. In NAACL ’04: Proceedings of the 2004 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Schone</author>
</authors>
<title>Searching Conversational Telephone Speech in Any of the World’s Languages.</title>
<date>2005</date>
<marker>Schone, 2005</marker>
<rawString>P. Schone et al. 2005. Searching Conversational Telephone Speech in Any of the World’s Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Shao</author>
</authors>
<title>Towards Vocabulary-Independent Speech Indexing for Large-Scale Repositories.</title>
<date>2008</date>
<booktitle>In Interspeech ’08: Conference of the International Speech Communication Association.</booktitle>
<marker>Shao, 2008</marker>
<rawString>Jian Shao et al. 2008. Towards Vocabulary-Independent Speech Indexing for Large-Scale Repositories. In Interspeech ’08: Conference of the International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In ICSLP ’02: Proceedings of 2002 International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="8609" citStr="Stolcke, 2002" startWordPosition="1351" endWordPosition="1352">al P(Q|L) is modeled as an order M phoneme level language model, Pˆ(Q|L) = �l P˜(qZ|qZ−nor+1,...,qZ−1,L), (1) Z=1 query term Q and lattice L, term frequency _ 183 so that �tfG(Q, L) ≈ Pˆ(Q|L) · NL. The probability of a query phoneme qj being generated, given that the phoneme sequence qj_M+1, ... , qj_1 = 1 was observed, is estimated as _M+1 . EP,c[C(qj_1 j_M+1)] Here, EP,c[C(qj_1 j_M+1)] denotes the expected count in lattice L of the phoneme sequence qj_1 j_M+1. We compute these counts using a variant of the forwardbackward algorithm, which is implemented by the SRI language modeling toolkit (Stolcke, 2002). In practice, because of data sparsity, the language model in Equation 1 must be modified to include smoothing for unseen phoneme sequences. We use a backoff M-gram model with Witten-Bell discounting (Witten and Bell, 1991). We set the phoneme language model’s order to M = 5, which gave good results in previous work (Yu and Seide, 2005). 3 Incorporating Query Degradations One problem with the generative approach is that recognition error is not modeled (apart from the uncertainty captured in the phoneme lattice). The essential problem is that while the method hopes to model P(Q|L), it is in f</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In ICSLP ’02: Proceedings of 2002 International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>T C Bell</author>
</authors>
<title>The Zero-Frequency Problem: Estimating the Probabilities of Novel Events in Adaptive Text Compression.</title>
<date>1991</date>
<journal>IEEE Trans. Information Theory,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="8833" citStr="Witten and Bell, 1991" startWordPosition="1385" endWordPosition="1388"> a query phoneme qj being generated, given that the phoneme sequence qj_M+1, ... , qj_1 = 1 was observed, is estimated as _M+1 . EP,c[C(qj_1 j_M+1)] Here, EP,c[C(qj_1 j_M+1)] denotes the expected count in lattice L of the phoneme sequence qj_1 j_M+1. We compute these counts using a variant of the forwardbackward algorithm, which is implemented by the SRI language modeling toolkit (Stolcke, 2002). In practice, because of data sparsity, the language model in Equation 1 must be modified to include smoothing for unseen phoneme sequences. We use a backoff M-gram model with Witten-Bell discounting (Witten and Bell, 1991). We set the phoneme language model’s order to M = 5, which gave good results in previous work (Yu and Seide, 2005). 3 Incorporating Query Degradations One problem with the generative approach is that recognition error is not modeled (apart from the uncertainty captured in the phoneme lattice). The essential problem is that while the method hopes to model P(Q|L), it is in fact only able to model the probability of one degradation H in the lattice, that is P(H|L). We define a query degradation as any phoneme sequence (including the lexical sequence) which may, with some estimated probability, o</context>
<context position="20424" citStr="Witten and Bell, 1991" startWordPosition="3218" endWordPosition="3221">ence phoneme sequence. After mapping the reference and hypothesized phonemes to each of these additional factor levels, we train language models on each of the three factor levels of the hypothesized phonemes. The language models for each of these factor levels are then incorporated as features in the translation model. We use the open source toolkit Moses (Koehn et al., 2007) as our phrase-based machine translation system. We used the SRI language modeling toolkit to estimate interpolated 5-gram language models (for each factor level), and smoothed our estimates with Witten-Bell discounting (Witten and Bell, 1991). We used the default parameter settings for Moses’s training, with the exception of modifying GTZA++’s default maximum fertility from 10 to 4 (since we don’t expect one reference phoneme to align to 10 degraded phonemes). We used default decoding settings, apart from setting the distortion penalty to prevent any reorderings (since alignments are logically constrained to never cross). For the rest of this chapter, we refer to our phrase-based query degradation model as PBQD. We denote the phrasebased model using factors as PBQD-Fac. Figure 2 shows an example alignment learned for a reference a</context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>I. H. Witten and T. C. Bell. 1991. The Zero-Frequency Problem: Estimating the Probabilities of Novel Events in Adaptive Text Compression. IEEE Trans. Information Theory, 37(4):1085–1094.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Yu</author>
<author>Frank Seide</author>
</authors>
<title>Fast TwoStage Vocabulary-Independent Search In Spontaneous Speech.</title>
<date>2005</date>
<booktitle>In ICASSP ’05: Proceedings of the 2005 IEEE International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="7285" citStr="Yu and Seide, 2005" startWordPosition="1127" endWordPosition="1130">follows. In Section 2 we introduce our baseline RUR methods. In Section 3 we introduce our query degradation approach. We introduce our experimental validation in Section 4 and our results in Section 5. We find that using phrase-based query degradations can significantly improve upon a strong RUR baseline. Finally, in Section 6 we conclude and outline several directions for future work. 2 Generative Baseline Each method we present in this paper ranks the utterances by the term’s estimated frequency within the corresponding phoneme lattice. This general approach has previously been considered (Yu and Seide, 2005; Saraclar and Sproat, 2004), on the basis that it provides a minimum Bayes-risk ranking criterion (Yu et al., Sept 2005; Robertson, 1977) for the utterances. What differs for each method is the particular estimator of term frequency which is used. We first outline our baseline approach, a generative model for term frequency estimation. Recall that our vocabulary-independent indices contain the expected counts of phoneme sequences from our recognition lattices. Yu and Seide (2005) used these expected phoneme sequence counts to estimate term frequency in the following way. For a t_fG is estimat</context>
<context position="8948" citStr="Yu and Seide, 2005" startWordPosition="1407" endWordPosition="1410"> as _M+1 . EP,c[C(qj_1 j_M+1)] Here, EP,c[C(qj_1 j_M+1)] denotes the expected count in lattice L of the phoneme sequence qj_1 j_M+1. We compute these counts using a variant of the forwardbackward algorithm, which is implemented by the SRI language modeling toolkit (Stolcke, 2002). In practice, because of data sparsity, the language model in Equation 1 must be modified to include smoothing for unseen phoneme sequences. We use a backoff M-gram model with Witten-Bell discounting (Witten and Bell, 1991). We set the phoneme language model’s order to M = 5, which gave good results in previous work (Yu and Seide, 2005). 3 Incorporating Query Degradations One problem with the generative approach is that recognition error is not modeled (apart from the uncertainty captured in the phoneme lattice). The essential problem is that while the method hopes to model P(Q|L), it is in fact only able to model the probability of one degradation H in the lattice, that is P(H|L). We define a query degradation as any phoneme sequence (including the lexical sequence) which may, with some estimated probability, occur in an errorful phonemic representation of the audio (either a one-best or lattice hypothesis). Because of spea</context>
</contexts>
<marker>Yu, Seide, 2005</marker>
<rawString>Peng Yu and Frank Seide. 2005. Fast TwoStage Vocabulary-Independent Search In Spontaneous Speech. In ICASSP ’05: Proceedings of the 2005 IEEE International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Yu</author>
</authors>
<title>Vocabulary-Independent Indexing of Spontaneous Speech.</title>
<date>2005</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>13</volume>
<issue>5</issue>
<marker>Yu, 2005</marker>
<rawString>P. Yu et al. Sept. 2005. Vocabulary-Independent Indexing of Spontaneous Speech. IEEE Transactions on Speech and Audio Processing, 13(5):635–643.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>