<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.314071">
<title confidence="0.996556">
A SNAPSHOT OF KDS
A KNOWLEDGE DELIVERY SYSTEM
</title>
<note confidence="0.98153025">
James A. Moore and William C. Mann
USC/Information Sciences Institute
Marina del Rey, CA
June, 1979
</note>
<sectionHeader confidence="0.93717" genericHeader="abstract">
SUMMARY
</sectionHeader>
<bodyText confidence="0.999922909090909">
KM is a computer program which creates
multi-paragraph, Natural Language text from • computer
representation of knowledge to be delivered. We have
addressed a number of issues not previously encountered in
the generation of Natural Language at the multi-sentence
level, viz: ordering among sentences and the scope of each,
quality comparisons between alternative aggregations of
sub-sentential units, the coordination of communication
with non-linguistic activities by a goal-pursuing planner,
and the use of dynamic models of speaker and hearer to shape
the text to the task at hand.
</bodyText>
<sectionHeader confidence="0.988878" genericHeader="keywords">
STATEMENT OF THE PROBLEM
</sectionHeader>
<bodyText confidence="0.984342">
The task of KDS is to generate English text under the
following constraints:
</bodyText>
<listItem confidence="0.984078">
1. The source of information is a semantic net, having no
a priori structuring to facilitate the outputting task.
2. The text is produced to satisfy an explicit goal held by
the text generating system, which describes a desired
cognitive state of the reader.
3. To achieve the desired state of the reader requires more
than a single sentence.
</listItem>
<sectionHeader confidence="0.996935" genericHeader="introduction">
RESULTS
</sectionHeader>
<bodyText confidence="0.99981025">
This is not the forum for a extensive analysis of our
results: for details, see Mann and Moore (1979). However, to
communicate the flavor of what we have
accomplished—from the motivating goal:
</bodyText>
<sectionHeader confidence="0.63687" genericHeader="method">
(WANTS SPEAKER
(KNOWS HEARER
</sectionHeader>
<subsectionHeader confidence="0.605202">
F M2E-AL ARM-SC E N
</subsectionHeader>
<bodyText confidence="0.9915805">
and about two pages of formal propositions describing the
&amp;quot;Fire-alarm scene&amp;quot;, KDS generated the following:
</bodyText>
<construct confidence="0.988356571428571">
Whenever there is a fire, the alarm system if started
which sounds the alarm bell and starts the timer. When it is
ninety seconds after the timer if started, unless the alarm
system is cancelled it calls Wells Fargo. When Wells Fargo
Is called, it calls the Fire Dept. .
When you hear the alarm bell or you smell smoke, stop
everything, determine whether there is a fire and decide to
permit the alarm system or to cancel it. When you determine
whether there is a fire, if there is, permit the alarm system;
otherwise Cancel It. .W When you permit the alarm system, call
the Fire Dept. if possible and follow the evacuation
procedure. When you cancel the alarm system, if it is more
than ninety seconds since the timer is started, the alarm
system calls Wells Fargo; otherwise continue everything.
</construct>
<bodyText confidence="0.9983565">
This represents the most elaborate performance of KDS to
date.
</bodyText>
<sectionHeader confidence="0.909517" genericHeader="method">
SYSTEM DESIGN
</sectionHeader>
<bodyText confidence="0.999371928571429">
The KDS organization reflects our novel paradigm:
FRAGMENT- AND-COMPOSE. KDS decomposes the original
network into fragments then orders and aggregates these
according to the dictates of the text-producing task, not
according to the needs for which the internal representation
was originally conceived. KDS has shown the feasibility of
this approach.
The KDS organization is a simple pipeline: FRAGMENT,
PLAN, FILTER, HILL-CLIMB, and OUTPUT.
FRAGMENT transforms the selected portion of the
semantic net into an unordered set of propositions which
correspond, roughly, to minimal sentences.
PLAN uses goal-sensitive rules to impose an ordering on
this set of fragments. A typical planning rule is:
&amp;quot;When conveying a scene in which the hearer is to
identify himself with one of the actors, express all
propositions involving that actor AFTER those which
do not, and separate these two partitions by a
paragraph break&amp;quot;.
FILTER, deletes from the set, all propositions currently
represented as known by the hearer.
HILL-CLIMB coordinates two sub-activities:
AGGREGATOR applies rules to combine two or three
fragments into a single one. A typical aggregation rule is:
&amp;quot;The two fragments &apos;x does A&apos; and &apos;x does B&apos; can be
combined into a single fragment: &apos;x does A and IP&amp;quot;.
PREFERENCER evaluates each proposed new fragment,
producing a numerical measure of its &amp;quot;goodness&amp;quot;. A typical
preference rule is:
&amp;quot;When instructing the hearer, increase the
accumulating measure by 10 for each occurrence of
the symbol &apos;YOU–.
HILL-CLIMB uses AGGREGATOR to generate new candidate
sets of fragments, and PREFERENCER, to determine which
new set presents the best one-step improvement over the
current set.
The objective function of HILL-CLIMB has been
enlarged to also take into account the COST OF FOREGONE
OPPORTUNITIES. This has drastically improved the initial
performance, since the topology abounds with local maxima.
KDS has used, at one time or another, on the order of 10
planning rules, 30 aggregation rules and 7 preference rules.
</bodyText>
<page confidence="0.994811">
51
</page>
<bodyText confidence="0.998916">
The aggregation and preference rules are directly
analogous to the capabilities of linguistic competence and
performance, respectively.
OUTPUT is a simple (two pages of LISP) text generator
driven by a context free grammar.
</bodyText>
<sectionHeader confidence="0.996302" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.978415">
The work reported here was supported by NSF Grant
MCS- 78-07332.
</bodyText>
<sectionHeader confidence="0.999647" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.997331466666667">
Levin, J. A.. and Goldman, N. M., Process models of reference
in context, ISI/RR-78-72, Information Sciences
Institute, Marina del Rey, CA, 1978.
Levin, J. A., and Moore, J. A., Dialogue Games, meta-
communication structures for natural language
Interaction, Cognitive Science, 1,4, 1978.
Mann, W. C., Moore, J. A., and Levin, J. A., A comprehension
model for human dialogue, in Proc. IJCAI-V,
Cambridge, MA, 1977.
Mann, W. C.. and Moore, J. A.. Computer generation of
multi-paragraph English text, in preparation.
Moore. J. A., Levin, J. A.. and Mann, W. C., A Goal-oriented
model of human dialogue, MCI. microfiche 87, 1977.
Moore, J. A., Communication as a problem-solving activity,
in preparation.
</reference>
<page confidence="0.998839">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.009052">
<title confidence="0.998547">A SNAPSHOT OF KDS A KNOWLEDGE DELIVERY SYSTEM</title>
<author confidence="0.999993">James A Moore</author>
<author confidence="0.999993">William C Mann</author>
<affiliation confidence="0.99756">USC/Information Sciences Institute</affiliation>
<address confidence="0.624947">Marina del Rey, CA</address>
<date confidence="0.995262">June, 1979</date>
<abstract confidence="0.991174711340206">SUMMARY KM is a computer program which creates multi-paragraph, Natural Language text from • computer representation of knowledge to be delivered. We have addressed a number of issues not previously encountered in the generation of Natural Language at the multi-sentence level, viz: ordering among sentences and the scope of each, quality comparisons between alternative aggregations of sub-sentential units, the coordination of communication with non-linguistic activities by a goal-pursuing planner, and the use of dynamic models of speaker and hearer to shape the text to the task at hand. STATEMENT OF THE PROBLEM The task of KDS is to generate English text under the following constraints: 1. The source of information is a semantic net, having no a priori structuring to facilitate the outputting task. 2. The text is produced to satisfy an explicit goal held by the text generating system, which describes a desired cognitive state of the reader. 3. To achieve the desired state of the reader requires more than a single sentence. RESULTS This is not the forum for a extensive analysis of our results: for details, see Mann and Moore (1979). However, to communicate the flavor of what we have accomplished—from the motivating goal: (WANTS SPEAKER (KNOWS HEARER F M2E-AL ARM-SC E N and about two pages of formal propositions describing the &amp;quot;Fire-alarm scene&amp;quot;, KDS generated the following: there is a fire, the alarm system which sounds the alarm bell and starts the timer. When it is seconds after the timer unless the alarm system is cancelled it calls Wells Fargo. When Wells Fargo Is called, it calls the Fire Dept. . When you hear the alarm bell or you smell smoke, stop everything, determine whether there is a fire and decide to permit the alarm system or to cancel it. When you determine whether there is a fire, if there is, permit the alarm system; Cancel It. .W you alarm system, call Fire possible and follow the evacuation procedure. When you cancel the alarm system, if it is more than ninety seconds since the timer is started, the alarm system calls Wells Fargo; otherwise continue everything. This represents the most elaborate performance of KDS to date. SYSTEM DESIGN The KDS organization reflects our novel paradigm: FRAGMENT- AND-COMPOSE. KDS decomposes the original network into fragments then orders and aggregates these according to the dictates of the text-producing task, not according to the needs for which the internal representation was originally conceived. KDS has shown the feasibility of this approach. The KDS organization is a simple pipeline: FRAGMENT, PLAN, FILTER, HILL-CLIMB, and OUTPUT. FRAGMENT transforms the selected portion of the semantic net into an unordered set of propositions which correspond, roughly, to minimal sentences. PLAN uses goal-sensitive rules to impose an ordering on this set of fragments. A typical planning rule is: &amp;quot;When conveying a scene in which the hearer is to identify himself with one of the actors, express all propositions involving that actor AFTER those which do not, and separate these two partitions by a paragraph break&amp;quot;. FILTER, deletes from the set, all propositions currently represented as known by the hearer. HILL-CLIMB coordinates two sub-activities: AGGREGATOR applies rules to combine two or three fragments into a single one. A typical aggregation rule is: &amp;quot;The two fragments &apos;x does A&apos; and &apos;x does B&apos; can be combined into a single fragment: &apos;x does A and IP&amp;quot;. PREFERENCER evaluates each proposed new fragment, producing a numerical measure of its &amp;quot;goodness&amp;quot;. A typical preference rule is: &amp;quot;When instructing the hearer, increase the accumulating measure by 10 for each occurrence of symbol HILL-CLIMB uses AGGREGATOR to generate new candidate sets of fragments, and PREFERENCER, to determine which new set presents the best one-step improvement over the current set. objective function of HILL-CLIMB enlarged to also take into account the COST OF FOREGONE OPPORTUNITIES. This has drastically improved the initial performance, since the topology abounds with local maxima. used, at one time or another, on the order of 10 planning rules, 30 aggregation rules and 7 preference rules. 51 The aggregation and preference rules are directly analogous to the capabilities of linguistic competence and performance, respectively. OUTPUT is a simple (two pages of LISP) text generator driven by a context free grammar.</abstract>
<note confidence="0.895134">ACKNOWLEDGMENTS work reported here was supported by MCS- 78-07332. REFERENCES Levin, J. A.. and Goldman, N. M., Process models of reference in context, ISI/RR-78-72, Information Sciences Institute, Marina del Rey, CA, 1978. Levin, J. A., and Moore, J. A., Dialogue Games, metacommunication structures for natural language Interaction, Cognitive Science, 1,4, 1978.</note>
<author confidence="0.247111">W C Mann</author>
<author confidence="0.247111">J A Moore</author>
<author confidence="0.247111">J A Levin</author>
<author confidence="0.247111">A comprehension</author>
<affiliation confidence="0.706092">model for human dialogue, in Proc. IJCAI-V,</affiliation>
<address confidence="0.815061">Cambridge, MA, 1977.</address>
<abstract confidence="0.741918666666667">Mann, W. C.. and Moore, J. A.. Computer generation of multi-paragraph English text, in preparation. Moore. J. A., Levin, J. A.. and Mann, W. C., A Goal-oriented model of human dialogue, MCI. microfiche 87, 1977. Moore, J. A., Communication as a problem-solving activity, in preparation.</abstract>
<intro confidence="0.880963">52</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J A Levin</author>
<author>N M Goldman</author>
</authors>
<title>Process models of reference in context, ISI/RR-78-72, Information Sciences Institute,</title>
<date>1978</date>
<location>Marina del Rey, CA,</location>
<marker>Levin, Goldman, 1978</marker>
<rawString>Levin, J. A.. and Goldman, N. M., Process models of reference in context, ISI/RR-78-72, Information Sciences Institute, Marina del Rey, CA, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Levin</author>
<author>J A Moore</author>
</authors>
<title>Dialogue Games, metacommunication structures for natural language Interaction,</title>
<date>1978</date>
<journal>Cognitive Science,</journal>
<volume>1</volume>
<marker>Levin, Moore, 1978</marker>
<rawString>Levin, J. A., and Moore, J. A., Dialogue Games, metacommunication structures for natural language Interaction, Cognitive Science, 1,4, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>J A Moore</author>
<author>J A Levin</author>
</authors>
<title>A comprehension model for human dialogue, in</title>
<date>1977</date>
<booktitle>Proc. IJCAI-V,</booktitle>
<location>Cambridge, MA,</location>
<marker>Mann, Moore, Levin, 1977</marker>
<rawString>Mann, W. C., Moore, J. A., and Levin, J. A., A comprehension model for human dialogue, in Proc. IJCAI-V, Cambridge, MA, 1977.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W C Mann</author>
<author>J A Moore</author>
</authors>
<title>Computer generation of multi-paragraph English text, in preparation.</title>
<marker>Mann, Moore, </marker>
<rawString>Mann, W. C.. and Moore, J. A.. Computer generation of multi-paragraph English text, in preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A</author>
<author>J A Levin</author>
<author>W C Mann</author>
</authors>
<title>A Goal-oriented model of human dialogue, MCI. microfiche 87,</title>
<date>1977</date>
<marker>A, Levin, Mann, 1977</marker>
<rawString>Moore. J. A., Levin, J. A.. and Mann, W. C., A Goal-oriented model of human dialogue, MCI. microfiche 87, 1977.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J A Moore</author>
</authors>
<title>Communication as a problem-solving activity, in preparation.</title>
<marker>Moore, </marker>
<rawString>Moore, J. A., Communication as a problem-solving activity, in preparation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>