<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999681">
A Feedback-Augmented Method for Detecting Errors in the Writing of
Learners of English
</title>
<author confidence="0.985683">
Ryo Nagata
</author>
<affiliation confidence="0.975827">
Hyogo University of Teacher Education
</affiliation>
<address confidence="0.442638">
6731494, Japan
</address>
<email confidence="0.781753">
rnagata@hyogo-u.ac.jp
</email>
<author confidence="0.987969">
Koichiro Morihiro
</author>
<affiliation confidence="0.994937">
Hyogo University of Teacher Education
</affiliation>
<address confidence="0.507234">
6731494, Japan
</address>
<email confidence="0.906854">
mori@hyogo-u.ac.jp
</email>
<sectionHeader confidence="0.991194" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999162">
This paper proposes a method for detect-
ing errors in article usage and singular plu-
ral usage based on the mass count distinc-
tion. First, it learns decision lists from
training data generated automatically to
distinguish mass and count nouns. Then,
in order to improve its performance, it is
augmented by feedback that is obtained
from the writing of learners. Finally, it de-
tects errors by applying rules to the mass
count distinction. Experiments show that
it achieves a recall of 0.71 and a preci-
sion of 0.72 and outperforms other meth-
ods used for comparison when augmented
by feedback.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985316631578947">
Although several researchers (Kawai et al., 1984;
McCoy et al., 1996; Schneider and McCoy, 1998;
Tschichold et al., 1997) have shown that rule-
based methods are effective to detecting gram-
matical errors in the writing of learners of En-
glish, it has been pointed out that it is hard to
write rules for detecting errors concerning the ar-
ticles and singular plural usage. To be precise, it
is hard to write rules for distinguishing mass and
count nouns which are particularly important in
detecting these errors (Kawai et al., 1984). The
major reason for this is that whether a noun is a
mass noun or a count noun greatly depends on its
meaning or its surrounding context (refer to Al-
lan (1980) and Bond (2005) for details of the mass
count distinction).
The above errors are very common among
Japanese learners of English (Kawai et al., 1984;
Izumi et al., 2003). This is perhaps because the
</bodyText>
<note confidence="0.346391666666667">
Atsuo Kawai
Mie University
5148507, Japan
</note>
<email confidence="0.926651">
kawai@ai.info.mie-u.ac.jp
</email>
<sectionHeader confidence="0.495589666666667" genericHeader="introduction">
Naoki Isu
Mie University
5148507, Japan
</sectionHeader>
<email confidence="0.95492">
isu@ai.info.mie-u.ac.jp
</email>
<bodyText confidence="0.9998585">
Japanese language does not have a mass count dis-
tinction system similar to that of English. Thus, it
is favorable for error detection systems aiming at
Japanese learners to be capable of detecting these
errors. In other words, such systems need to some-
how distinguish mass and count nouns.
This paper proposes a method for distinguishing
mass and count nouns in context to complement
the conventional rules for detecting grammatical
errors. In this method, first, training data, which
consist of instances of mass and count nouns, are
automatically generated from a corpus. Then,
decision lists for distinguishing mass and count
nouns are learned from the training data. Finally,
the decision lists are used with the conventional
rules to detect the target errors.
The proposed method requires a corpus to learn
decision lists for distinguishing mass and count
nouns. General corpora such as newspaper ar-
ticles can be used for the purpose. However,
a drawback to it is that there are differences in
character between general corpora and the writ-
ing of non-native learners of English (Granger,
1998; Chodorow and Leacock, 2000). For in-
stance, Chodorow and Leacock (2000) point out
that the word concentrate is usually used as a noun
in a general corpus whereas it is a verb 91% of
the time in essays written by non-native learners
of English. Consequently, the differences affect
the performance of the proposed method.
In order to reduce the drawback, the proposed
method is augmented by feedback; it takes as feed-
back learners’ essays whose errors are corrected
by a teacher of English (hereafter, referred to as
the feedback corpus). In essence, the feedback
corpus could be added to a general corpus to gen-
erate training data. Or, ideally training data could
be generated only from the feedback corpus just as
</bodyText>
<page confidence="0.972028">
241
</page>
<note confidence="0.535995">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 241–248,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999869363636364">
from a general corpus. However, this causes a se-
rious problem in practice since the size of the feed-
back corpus is normally far smaller than that of a
general corpus. To make it practical, this paper
discusses the problem and explores its solution.
The rest of this paper is structured as follows.
Section 2 describes the method for detecting the
target errors based on the mass count distinction.
Section 3 explains how the method is augmented
by feedback. Section 4 discusses experiments con-
ducted to evaluate the proposed method.
</bodyText>
<sectionHeader confidence="0.967471" genericHeader="method">
2 Method for detecting the target errors
</sectionHeader>
<subsectionHeader confidence="0.999302">
2.1 Generating training data
</subsectionHeader>
<bodyText confidence="0.99871290625">
First, instances of the target noun that head their
noun phrase (NP) are collected from a corpus with
their surrounding words. This can be simply done
by an existing chunker or parser.
Then, the collected instances are tagged with
mass or count by the following tagging rules. For
example, the underlined chicken:
... are a lot of chickens in the roost ...
is tagged as
... are a lot of chickens/count in the roost ...
because it is in plural form.
We have made tagging rules based on linguistic
knowledge (Huddleston and Pullum, 2002). Fig-
ure 1 and Table 1 represent the tagging rules. Fig-
ure 1 shows the framework of the tagging rules.
Each node in Figure 1 represents a question ap-
plied to the instance in question. For example, the
root node reads “Is the instance in question plu-
ral?”. Each leaf represents a result of the classi-
fication. For example, if the answer is yes at the
root node, the instance in question is tagged with
count. Otherwise, the question at the lower node
is applied and so on. The tagging rules do not
classify instances as mass or count in some cases.
These unclassified instances are tagged with the
symbol “?”. Unfortunately, they cannot readily be
included in training data. For simplicity of imple-
mentation, they are excluded from training data1.
Note that the tagging rules can be used only for
generating training data. They cannot be used to
distinguish mass and count nouns in the writing
of learners of English for the purpose of detecting
</bodyText>
<footnote confidence="0.81343675">
&apos;According to experiments we have conducted, approxi-
mately 30% of instances are tagged with “?” on average. It is
highly possible that performance of the proposed method will
improve if these instances are included in the training data.
</footnote>
<bodyText confidence="0.999667166666667">
the target errors since they are based on the articles
and the distinction between singular and plural.
Finally, the tagged instances are stored in a file
with their surrounding words. Each line of it con-
sists of one of the tagged instances and its sur-
rounding words as in the above chicken example.
</bodyText>
<subsectionHeader confidence="0.999902">
2.2 Learning Decision Lists
</subsectionHeader>
<bodyText confidence="0.999968666666666">
In the proposed method, decision lists are used for
distinguishing mass and count nouns. One of the
reasons for the use of decision lists is that they
have been shown to be effective to the word sense
disambiguation task and the mass count distinc-
tion is highly related to word sense as we will see
in this section. Another reason is that rules for dis-
tinguishing mass and count nouns are observable
in decision lists, which helps understand and im-
prove the proposed method.
A decision list consists of a set of rules. Each
rule matches the template as follows:
</bodyText>
<subsectionHeader confidence="0.496646">
If a condition is true, then a decision (1)
</subsectionHeader>
<bodyText confidence="0.999348">
To define the template in the proposed method,
let us have a look at the following two examples:
</bodyText>
<listItem confidence="0.520887">
1. I read the paper.
2. The paper is made of hemp pulp.
</listItem>
<bodyText confidence="0.999871434782609">
The underlined papers in both sentences cannot
simply be classified as mass or count by the tag-
ging rules presented in Section 2.1 because both
are singular and modified by the definite article.
Nevertheless, we can tell that the former is a count
noun and the latter is a mass noun from the con-
texts. This suggests that the mass count distinc-
tion is often determined by words surrounding the
target noun. In example 1, we can tell that the pa-
per refers to something that can be read such as
a newspaper or a scientific paper from read, and
therefore it is a count noun. Likewise, in exam-
ple 2, we can tell that the paper refers to a certain
substance from made and pulp, and therefore it is
a mass noun.
Taking this observation into account, we define
the template based on words surrounding the tar-
get noun. To formalize the template, we will use
a random variable that takes either or
to denote that the target noun is a mass noun
or a count noun, respectively. We will also use
and to denote a word and a certain context
around the target noun, respectively. We define
</bodyText>
<page confidence="0.998345">
242
</page>
<figureCaption confidence="0.740857">
Figure 1: Framework of the tagging rules
</figureCaption>
<tableCaption confidence="0.843183">
Table 1: Words used in the tagging rules
</tableCaption>
<figure confidence="0.988628411764706">
(a) (b) (c)
the indefinite article much the definite article
another less demonstrative adjectives
one enough possessive adjectives
each sufficient interrogative adjectives
– –quantifiers
– – ’s genitives
?
? MASS
COUNT
plural?
�
yes
COUNT
�
modified by a little?
yes� no
no
modified by one of the words
in Table 1(a)?
modified by one of the words
in Table 1(b)?
modified by one of the words
in Table 1(c)?
�
yes
�
yes
no
�
yes
no
MASS
no
</figure>
<bodyText confidence="0.988490642857143">
three types of : ,, and that denote the
contexts consisting of the noun phrase that the tar-
get noun heads, words to the left of the noun
phrase, and words to its right, respectively. Then
the template is formalized by:
If word appears in context of the target noun,
then it is distinguished as
for the target noun chicken when .
In addition, a default rule is defined. It is based
on the target noun itself and used when no other
applicable rules are found in the decision list for
the target noun. It is defined by
Hereafter, to keep the notation simple, it will be
abbreviated to
</bodyText>
<figure confidence="0.762450333333333">
major
(3)
(2)
</figure>
<bodyText confidence="0.95563137037037">
Now rules that match the template can be ob-
tained from the training data. All we need to do
is to collect words in from the training data.
Here, the words in Table 1 are excluded. Also,
function words (except prepositions), cardinal and
quasi-cardinal numerals, and the target noun are
excluded. All words are reduced to their mor-
phological stem and converted entirely to lower
case when collected. For example, the following
tagged instance:
She ate fried chicken/mass for dinner.
would give a set of rules that match the template:
where and major denote the target noun and
the majority of in the training data, respec-
tively. Equation (3) reads “If the target noun ap-
pears, then it is distinguished by the majority”.
The log-likelihood ratio (Yarowsky, 1995) de-
cides in which order rules are applied to the target
noun in novel context. It is defined bye
(4)
where is the exclusive event of and
is the probability that the target noun
is used as when appears in the context .
It is important to exercise some care in estimat-
ing . In principle, we could simply
2For the default rule, the log-likelihood ratio is defined by
replacing and withand major, respectively.
</bodyText>
<page confidence="0.992093">
243
</page>
<bodyText confidence="0.990670851851852">
count the number of times that appears in the
context of the target noun used as in the
training data. However, this estimate can be unre-
liable, when does not appear often in the con-
text. To solve this problem, using a smoothing pa-
rameter (Yarowsky, 1996), is esti-
mated by&apos;
(5)
where and are occurrences of
appearing in and those in of the target noun
used as , respectively. The constant is the
number of possible classes, that is, (
or ) in our case, and introduced to satisfy
. In this paper, is
set to 1.
Rules in a decision list are sorted in descending
order by the log-likelihood ratio. They are tested
on the target noun in novel context in this order.
Rules sorted below the default rule are discarded
because they are never used as we will see in Sec-
tion 2.3.
Table 2 shows part of a decision list for the tar-
get noun chicken that was learned from a subset
of the BNC (British National Corpus) (Burnard,
1995). Note that the rules are divided into two
columns for the purpose of illustration in Table 2;
in practice, they are merged into one.
</bodyText>
<tableCaption confidence="0.996286">
Table 2: Rules in a decision list
</tableCaption>
<figure confidence="0.983487428571428">
Count
LLR
1.49
1.32
1.23
1.23
1.18
</figure>
<bodyText confidence="0.997142666666667">
On one hand, we associate the words in the left
half with food or cooking. On the other hand,
we associate those in the right half with animals
or birds. From this observation, we can say that
chicken in the sense of an animal or a bird is a
count noun but a mass noun when referring to food
</bodyText>
<footnote confidence="0.9875255">
3The probability for the default rule is estimated just as
the log-likelihood ratio for the default rule above.
4It depends on the target noun how many rules are dis-
carded.
</footnote>
<bodyText confidence="0.8207945">
or cooking, which agrees with the knowledge pre-
sented in previous work (Ostler and Atkins, 1991).
</bodyText>
<subsectionHeader confidence="0.999083">
2.3 Distinguishing mass and count nouns
</subsectionHeader>
<bodyText confidence="0.9999687">
To distinguish the target noun in novel context,
each rule in the decision list is tested on it in the
sorted order until the first applicable one is found.
It is distinguished according to the first applicable
one. Ties are broken by the rules below.
It should be noted that rules sorted below the
default rule are never used because the default rule
is always applicable to the target noun. This is the
reason why rules sorted below the default rule are
discarded as mentioned in Section 2.2.
</bodyText>
<subsectionHeader confidence="0.997948">
2.4 Detecting the target errors
</subsectionHeader>
<bodyText confidence="0.960879260869565">
The target errors are detected by the following
three steps. Rules in each step are examined on
each target noun in the target text.
In the first step, any mass noun in plural form is
detected as an errors. If an error is detected in this
step, the rest of the steps are not applied.
In the second step, errors are detected by the
rules described in Table 3. The symbol “ ” in Ta-
ble 3 denotes that the combination of the corre-
sponding row and column is erroneous. For exam-
ple, the fifth row denotes that singular and plural
count nouns modified by much are erroneous. The
symbol “–” denotes that no error can be detected
by the table. If one of the rules in Table 3 is applied
to the target noun, the third step is not applied.
In the third step, errors are detected by the rules
described in Table 4. The symbols “ ” and “–”
are the same as in Table 3.
In addition, the indefinite article that modifies
other than the head noun is judged to be erroneous
5Mass nouns can be used in plural in some cases. How-
ever, they are rare especially in the writing of learners of En-
glish.
</bodyText>
<figure confidence="0.927946826086956">
Sing.
Pattern Sing. Pl.
Count
Mass
another, each, one –
all, enough, sufficient –
that, this –
few, many, several –
these, those –
various, numerous –
cardinal numbers exc. one –
much
–
–
–
Mass LLR
1.49
1.28
target noun: chicken,
LLR (Log-Likelihood Ratio)
1.23
1.23
1.18 Table 3: Detection rules (i)
</figure>
<page confidence="0.901914">
244
</page>
<tableCaption confidence="0.878446">
Table 4: Detection rules (ii)
</tableCaption>
<figure confidence="0.994612857142857">
Singular
a/an the
Plural
a/an the
Mass – –
Count – –
– –
</figure>
<bodyText confidence="0.9933362">
To formalize the interpolated probability, we
will use the symbols , , , and to de-
note the conditional probabilities estimated from
the feedback corpus and the general corpus, and
their confidences, respectively. Then, the interpo-
lated probability is estimated by7
(e.g., *an expensive). Likewise, the definite article
that modifies other than the head noun or adjective
is judged to be erroneous (e.g., *the them). Also,
we have made exceptions to the rules. The follow-
ing combinations are excluded from the detection
in the second and third steps: head nouns modified
by interrogative adjectives (e.g., what), possessive
adjectives (e.g., my), ’s genitives, “some”, “any”,
or “no”.
</bodyText>
<sectionHeader confidence="0.999723" genericHeader="method">
3 Feedback-augmented method
</sectionHeader>
<bodyText confidence="0.99996636">
As mentioned in Section 1, the proposed method
takes the feedback corpus6 as feedback to improve
its performance. In essence, decision lists could be
learned from a corpus consisting of a general cor-
pus and the feedback corpus. However, since the
size of the feedback corpus is normally far smaller
than that of general corpora, so is the effect of the
feedback corpus on . This means that
the feedback corpus hardly has effect on the per-
formance.
Instead, can be estimated by in-
terpolating the probabilities estimated from the
feedback corpus and the general corpus accord-
ing to confidences of their estimates. It is favor-
able that the interpolated probability approaches
to the probability estimated from the feedback cor-
pus as its confidence increases; the more confident
its estimate is, the more effect it has on the inter-
polated probability. Here, confidence of ratio
is measured by the reciprocal of variance of the
ratio (Tanaka, 1977). Variance is calculated by
where denotes the number of samples used for
calculating the ratio. Therefore, confidence of the
estimate of the conditional probability used in the
proposed method is measured by
</bodyText>
<footnote confidence="0.685992">
6The feedback corpus refers to learners’ essays whose er-
rors are corrected as mentioned in Section 1.
</footnote>
<bodyText confidence="0.99983406060606">
In Equation (8), the effect of on becomes
large as its confidence increases. It should also be
noted that when its confidence exceeds that of ,
the general corpus is no longer used in the inter-
polated probability.
A problem that arises in Equation (8) is that
hardly has effect on when a much larger general
corpus is used than the feedback corpus even if
is estimated with a sufficient confidence. For ex-
ample, estimated from 100 samples, which are
a relatively large number for estimating a proba-
bility, hardly has effect on when is estimated
from 10000 samples; roughly, has a ef-
fect of on .
One way to prevent this is to limit the effect of
to some extent. It can be realized by taking the
log of in Equation (8). That is, the interpolated
probability is estimated by
It is arguable what base of the log should be used.
In this paper, it is set to 2 so that the effect of on
the interpolated probability becomes large when
the confidence of the estimate of the conditional
probability estimated from the feedback corpus is
small (that is, when there is little data in the feed-
back corpus for the estimate)8.
In summary, Equation (9) interpolates between
the conditional probabilities estimated from the
feedback corpus and the general corpus in the
feedback-augmented method. The interpolated
probability is then used to calculate the log-
likelihood ratio. Doing so, the proposed method
takes the feedback corpus as feedback to improve
its performance.
</bodyText>
<footnote confidence="0.652455857142857">
7In general, the interpolated probability needs to be nor-
malized to satisfy . In our case, however, it is al-
ways satisfied without normalization since
and
are satisfied.
8We tested several bases in the experiments and found
there were little difference in performance between them.
</footnote>
<page confidence="0.99882">
245
</page>
<sectionHeader confidence="0.999503" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997585">
4.1 Experimental Conditions
</subsectionHeader>
<bodyText confidence="0.923450703703704">
A set of essays9 written by Japanese learners of
English was used as the target essays in the exper-
iments. It consisted of 47 essays (3180 words) on
the topic traveling. A native speaker of English
who was a professional rewriter of English recog-
nized 105 target errors in it.
The written part of the British National Corpus
(BNC) (Burnard, 1995) was used to learn deci-
sion lists. Sentences the OAK system10, which
was used to extract NPs from the corpus, failed
to analyze were excluded. After these operations,
the size of the corpus approximately amounted to
80 million words. Hereafter, the corpus will be
referred to as the BNC.
As another corpus, the English concept explica-
tion in the EDR English-Japanese Bilingual dic-
tionary and the EDR corpus (1993) were used; it
will be referred to as the EDR corpus, hereafter.
Its size amounted to about 3 million words.
Performance of the proposed method was eval-
uated by recall and precision. Recall is defined by
No. of target errors detected correctly (10)
No. of target errors in the target essays
Precision is defined by
No. of target errors detected correctly
(11)
No. of detected errors
</bodyText>
<subsectionHeader confidence="0.968676">
4.2 Experimental Procedures
</subsectionHeader>
<bodyText confidence="0.9998552">
First, decision lists for each target noun in the tar-
get essays were learned from the BNC11. To ex-
tract noun phrases and their head nouns, the OAK
system was used. An optimal value for (window
size of context) was estimated as follows. For 25
nouns shown in (Huddleston and Pullum, 2002) as
examples of nouns used as both mass and count
nouns, accuracy on the BNC was calculated us-
ing ten-fold cross validation. As a result of set-
ting small ( ), medium ( ), and large
( ) window sizes, it turned out that
maximized the average accuracy. Following this
result, was selected in the experiments.
Second, the target nouns were distinguished
whether they were mass or count by the learned
</bodyText>
<footnote confidence="0.685863666666667">
9http://www.eng.ritsumei.ac.jp/lcorpus/.
10OAK System Homepage: http://nlp.cs.nyu.edu/oak/.
11If no instance of the target noun is found in the gen-
eral corpora (and also in the feedback corpus in case of the
feedback-augmented method), the target noun is ignored in
the error detection procedure.
</footnote>
<bodyText confidence="0.972593346153846">
decision lists, and then the target errors were de-
tected by applying the detection rules to the mass
count distinction. As a preprocessing, spelling er-
rors were corrected using a spell checker. The re-
sults of the detection were compared to those done
by the native-speaker of English. From the com-
parison, recall and precision were calculated.
Then, the feedback-augmented method was
evaluated on the same target essays. Each target
essay in turn was left out, and all the remaining
target essays were used as a feedback corpus. The
target errors in the left-out essay were detected us-
ing the feedback-augmented method. The results
of all 47 detections were integrated into one to cal-
culate overall performance. This way of feedback
can be regarded as that one uses revised essays
previously written in a class to detect errors in es-
says on the same topic written in other classes.
Finally, the above two methods were compared
with their seven variants shown in Table 5. “DL”
in Table 5 refers to the nine decision list based
methods (the above two methods and their seven
variants). The words in brackets denote the cor-
pora used to learn decision lists; the symbol “+FB”
means that the feedback corpus was simply added
to the general corpus. The subscripts and
indicate that the feedback was done by using
Equation (8) and Equation (9), respectively.
In addition to the seven variants, two kinds of
earlier method were used for comparison. One
was one (Kawai et al., 1984) of the rule-based
methods. It judges singular head nouns with no
determiner to be erroneous since missing articles
are most common in the writing of Japanese learn-
ers of English. In the experiments, this was imple-
mented by treating all nouns as count nouns and
applying the same detection rules as in the pro-
posed method to the countability.
The other was a web-based method (Lapata and
Keller, 2005)12 for generating articles. It retrieves
web counts for queries consisting of two words
preceding the NP that the target noun head, one
of the articles (a/an, the, ), and the core NP
to generate articles. All queries are performed as
exact matches using quotation marks and submit-
ted to the Google search engine in lower case. For
example, in the case of “*She is good student.”, it
retrieves web counts for “she is a good student”,
12There are other statistical methods that can be used for
comparison including Lee (2004) and Minnen (2000). Lapata
and Keller (2005) report that the web-based method is the
best performing article generation method.
</bodyText>
<page confidence="0.996514">
246
</page>
<bodyText confidence="0.999980285714286">
“she is the good student”, and “she is good stu-
dent”. Then, it generates the article that maxi-
mizes the web counts. We extended it to make
it capable of detecting our target errors. First, the
singular/plural distinction was taken into account
in the queries (e.g., “she is a good students”, “she
is the good students”, and “she is good students”
in addition to the above three queries). The one(s)
that maximized the web counts was judged to be
correct; the rest were judged to be erroneous. Sec-
ond, if determiners other than the articles modify
head nouns, only the distinction between singu-
lar and plural was taken into account (e.g., “he
has some book” vs “he has some books”). In the
case of “much/many”, the target noun in singular
form modified by “much” and that in plural form
modified by “many” were compared (e.g., “he has
much furniture” vs “he has many furnitures). Fi-
nally, some rules were used to detect literal errors.
For example, plural head nouns modified by “this”
were judged to be erroneous.
</bodyText>
<subsectionHeader confidence="0.99976">
4.3 Experimental Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999185833333333">
Table 5 shows the experimental results. “Rule-
based” and “Web-based” in Table 5 refer to the
rule-based method and the web-based method, re-
spectively. The other symbols are as already ex-
plained in Section 4.2.
As we can see from Table 5, all the decision
list based methods outperform the earlier methods.
The rule-based method treated all nouns as count
nouns, and thus it did not work well at all on mass
nouns. This caused a lot of false-positives and
false-negatives. The web-based method suffered
a lot from other errors than the target errors since
</bodyText>
<tableCaption confidence="0.994146">
Table 5: Experimental results
</tableCaption>
<table confidence="0.998709">
Method Recall Precision
DL (BNC) 0.66 0.65
DL (BNC+FB) 0.66 0.65
DL (BNC) 0.66 0.65
DL (BNC) 0.69 0.70
DL (EDR) 0.70 0.68
DL (EDR+FB) 0.71 0.69
DL (EDR) 0.71 0.70
DL (EDR) 0.71 0.72
DL (FB) 0.43 0.76
Rule-based 0.59 0.39
Web-based 0.49 0.53
</table>
<bodyText confidence="0.998627603773585">
it implicitly assumed that there were no errors ex-
cept the target errors. Contrary to this assumption,
not only did the target essays contain the target er-
rors but also other errors since they were written
by Japanese learners of English. This indicate that
the queries often contained the other errors when
web counts were retrieved. These errors made the
web counts useless, and thus it did not perform
well. By contrast, the decision list based meth-
ods did because they distinguished mass and count
nouns by one of the words around the target noun
that was most likely to be effective according to
the log-likelihood ratio13; the best performing de-
cision list based method (DL (EDR)) is sig-
nificantly superior to the best performing14 non-
decision list based method (Web-based) in both re-
call and precision at the 99% confidence level.
Table 5 also shows that the feedback-augmented
methods benefit from feedback. Only an exception
is “DL (BNC)”. The reason is that the size of
BNC is far larger than that of the feedback cor-
pus and thus it did not affect the performance.
This also explains that simply adding the feed-
back corpus to the general corpus achieved little
or no improvement as “DL (EDR+FB)” and “DL
(BNC+FB)” show. Unlike these, both “DL
(BNC)” and “DL (EDR)” benefit from feed-
back since the effect of the general corpus is lim-
ited to some extent by the log function in Equa-
tion (9). Because of this, both benefit from feed-
back despite the differences in size between the
feedback corpus and the general corpus.
Although the experimental results have shown
that the feedback-augmented method is effective
to detecting the target errors in the writing of
Japanese learners of English, even the best per-
forming method (DL (EDR)) made 30 false-
negatives and 29 false-positives. About 70% of
the false-negatives were errors that required other
sources of information than the mass count dis-
tinction to be detected. For example, extra def-
inite articles (e.g., *the traveling) cannot be de-
tected even if the correct mass count distinction is
given. Thus, only a little improvement is expected
in recall however much feedback corpus data be-
come available. On the other hand, most of the
13Indeed, words around the target noun were effective. The
default rules were used about 60% and 30% of the time in
“DL (EDR)” and “DL (BNC)”, respectively; when only the
default rules were used, “DL (EDR)” (“DL (BNC)”) achieved
0.66 (0.56) in recall and 0.58 (0.53) in precision.
14“Best performing” here means best performing in terms
of -measure.
</bodyText>
<page confidence="0.990491">
247
</page>
<bodyText confidence="0.99954275">
false-positives were due to the decision lists them-
selves. Considering this, it is highly possible that
precision will improve as the size of the feedback
corpus increases.
</bodyText>
<sectionHeader confidence="0.992281" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999892172413793">
Japan electronic dictionary research institute ltd,
Tokyo.
S. Granger. 1998. Prefabricated patterns in advanced
EFL writing: collocations and formulae. In A. P.
Cowie, editor, Phraseology: theory, analysis, and
applications, pages 145–160. Clarendon Press.
This paper has proposed a feedback-augmented
method for distinguishing mass and count nouns
to complement the conventional rules for detect-
ing grammatical errors. The experiments have
shown that the proposed method detected 71% of
the target errors in the writing of Japanese learn-
ers of English with a precision of 72% when it
was augmented by feedback. From the results,
we conclude that the feedback-augmented method
is effective to detecting errors concerning the ar-
ticles and singular plural usage in the writing of
Japanese learners of English.
Although it is not taken into account in this pa-
per, the feedback corpus contains further useful in-
formation. For example, we can obtain training
data consisting of instances of errors by compar-
ing the feedback corpus with its original corpus.
Also, comparing it with the results of detection,
we can know performance of each rule used in
the detection, which make it possible to increase
or decrease their log-likelihood ratios according to
their performance. We will investigate how to ex-
ploit these sources of information in future work.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99996325">
The authors would like to thank Sekine Satoshi
who has developed the OAK System. The authors
also would like to thank three anonymous review-
ers for their useful comments on this paper.
</bodyText>
<sectionHeader confidence="0.999284" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999830300000001">
K. Allan. 1980. Nouns and countability. J. Linguistic
Society ofAmerica, 56(3):541–567.
F. Bond. 2005. Translating the Untranslatable. CSLI
publications, Stanford.
L. Burnard. 1995. Users Reference Guide for the
British National Corpus. version 1.0. Oxford Uni-
versity Computing Services, Oxford.
M. Chodorow and C. Leacock. 2000. An unsupervised
method for detecting grammatical errors. In Proc. of
1st Meeting of the North America Chapter of ACL,
pages 140–147.
Japan electronic dictionary research institute ltd. 1993.
EDR electronic dictionary specifications guide.
R. Huddleston and G.K. Pullum. 2002. The Cam-
bridge Grammar of the English Language. Cam-
bridge University Press, Cambridge.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and
H. Isahara. 2003. Automatic error detection in the
Japanese learners’ English spoken data. In Proc. of
41st Annual Meeting ofACL, pages 145–148.
A. Kawai, K. Sugihara, and N. Sugie. 1984. ASPEC-I:
An error detection system for English composition.
IPSJ Journal (in Japanese), 25(6):1072–1079.
M. Lapata and F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1):1–31.
J. Lee. 2004. Automatic article restoration. In Proc. of
the Human Language Technology Conference of the
North American Chapter ofACL, pages 31–36.
K.F. McCoy, C.A. Pennington, and L.Z. Suri. 1996.
English error correction: A syntactic user model
based on principled “mal-rule” scoring. In Proc.
of 5th International Conference on User Modeling,
pages 69–66.
G. Minnen, F. Bond, and A. Copestake. 2000.
Memory-based learning for article generation. In
Proc. of CoNLL-2000 and LLL-2000 workshop,
pages 43–48.
N. Ostler and B.T.S Atkins. 1991. Predictable mean-
ing shift: Some linguistic properties of lexical impli-
cation rules. In Proc. of 1st SIGLEX Workshop on
Lexical Semantics and Knowledge Representation,
pages 87–100.
D. Schneider and K.F. McCoy. 1998. Recognizing
syntactic errors in the writing of second language
learners. In Proc. of 17th International Conference
on Computational Linguistics, pages 1198–1205.
Y. Tanaka. 1977. Psychological methods (in
Japanese). University of Tokyo Press.
C. Tschichold, F. Bodmer, E. Cornu, F. Grosjean,
L. Grosjean, N. Kubler, N. Lewy, and C. Tschumi.
1997. Developing a new grammar checker for En-
glish as a second language. In Proc. of the From Re-
search to CommercialApplications Workshop, pages
7–12.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
33rd Annual Meeting ofACL, pages 189–196.
D. Yarowsky. 1996. Homograph Disambiguation in
Speech Synthesis. Springer-Verlag.
</reference>
<page confidence="0.997009">
248
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.718049">
<title confidence="0.9933515">A Feedback-Augmented Method for Detecting Errors in the Writing of Learners of English</title>
<author confidence="0.976694">Ryo Nagata</author>
<affiliation confidence="0.999803">Hyogo University of Teacher Education</affiliation>
<address confidence="0.999548">6731494, Japan</address>
<email confidence="0.982995">rnagata@hyogo-u.ac.jp</email>
<author confidence="0.816081">Koichiro Morihiro</author>
<affiliation confidence="0.999622">Hyogo University of Teacher Education</affiliation>
<address confidence="0.998524">6731494, Japan</address>
<email confidence="0.989059">mori@hyogo-u.ac.jp</email>
<abstract confidence="0.995574375">This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction. First, it learns decision lists from training data generated automatically to distinguish mass and count nouns. Then, in order to improve its performance, it is augmented by feedback that is obtained from the writing of learners. Finally, it detects errors by applying rules to the mass count distinction. Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Allan</author>
</authors>
<title>Nouns and countability.</title>
<date>1980</date>
<journal>J. Linguistic Society ofAmerica,</journal>
<volume>56</volume>
<issue>3</issue>
<contexts>
<context position="1566" citStr="Allan (1980)" startWordPosition="255" endWordPosition="257">r and McCoy, 1998; Tschichold et al., 1997) have shown that rulebased methods are effective to detecting grammatical errors in the writing of learners of English, it has been pointed out that it is hard to write rules for detecting errors concerning the articles and singular plural usage. To be precise, it is hard to write rules for distinguishing mass and count nouns which are particularly important in detecting these errors (Kawai et al., 1984). The major reason for this is that whether a noun is a mass noun or a count noun greatly depends on its meaning or its surrounding context (refer to Allan (1980) and Bond (2005) for details of the mass count distinction). The above errors are very common among Japanese learners of English (Kawai et al., 1984; Izumi et al., 2003). This is perhaps because the Atsuo Kawai Mie University 5148507, Japan kawai@ai.info.mie-u.ac.jp Naoki Isu Mie University 5148507, Japan isu@ai.info.mie-u.ac.jp Japanese language does not have a mass count distinction system similar to that of English. Thus, it is favorable for error detection systems aiming at Japanese learners to be capable of detecting these errors. In other words, such systems need to somehow distinguish m</context>
</contexts>
<marker>Allan, 1980</marker>
<rawString>K. Allan. 1980. Nouns and countability. J. Linguistic Society ofAmerica, 56(3):541–567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Bond</author>
</authors>
<title>Translating the Untranslatable. CSLI publications,</title>
<date>2005</date>
<location>Stanford.</location>
<contexts>
<context position="1582" citStr="Bond (2005)" startWordPosition="259" endWordPosition="260">; Tschichold et al., 1997) have shown that rulebased methods are effective to detecting grammatical errors in the writing of learners of English, it has been pointed out that it is hard to write rules for detecting errors concerning the articles and singular plural usage. To be precise, it is hard to write rules for distinguishing mass and count nouns which are particularly important in detecting these errors (Kawai et al., 1984). The major reason for this is that whether a noun is a mass noun or a count noun greatly depends on its meaning or its surrounding context (refer to Allan (1980) and Bond (2005) for details of the mass count distinction). The above errors are very common among Japanese learners of English (Kawai et al., 1984; Izumi et al., 2003). This is perhaps because the Atsuo Kawai Mie University 5148507, Japan kawai@ai.info.mie-u.ac.jp Naoki Isu Mie University 5148507, Japan isu@ai.info.mie-u.ac.jp Japanese language does not have a mass count distinction system similar to that of English. Thus, it is favorable for error detection systems aiming at Japanese learners to be capable of detecting these errors. In other words, such systems need to somehow distinguish mass and count no</context>
</contexts>
<marker>Bond, 2005</marker>
<rawString>F. Bond. 2005. Translating the Untranslatable. CSLI publications, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Burnard</author>
</authors>
<title>Users Reference Guide for the British National Corpus. version 1.0.</title>
<date>1995</date>
<institution>Oxford University Computing Services,</institution>
<location>Oxford.</location>
<contexts>
<context position="11602" citStr="Burnard, 1995" startWordPosition="2010" endWordPosition="2011">f appearing in and those in of the target noun used as , respectively. The constant is the number of possible classes, that is, ( or ) in our case, and introduced to satisfy . In this paper, is set to 1. Rules in a decision list are sorted in descending order by the log-likelihood ratio. They are tested on the target noun in novel context in this order. Rules sorted below the default rule are discarded because they are never used as we will see in Section 2.3. Table 2 shows part of a decision list for the target noun chicken that was learned from a subset of the BNC (British National Corpus) (Burnard, 1995). Note that the rules are divided into two columns for the purpose of illustration in Table 2; in practice, they are merged into one. Table 2: Rules in a decision list Count LLR 1.49 1.32 1.23 1.23 1.18 On one hand, we associate the words in the left half with food or cooking. On the other hand, we associate those in the right half with animals or birds. From this observation, we can say that chicken in the sense of an animal or a bird is a count noun but a mass noun when referring to food 3The probability for the default rule is estimated just as the log-likelihood ratio for the default rule </context>
<context position="18500" citStr="Burnard, 1995" startWordPosition="3214" endWordPosition="3215">ized to satisfy . In our case, however, it is always satisfied without normalization since and are satisfied. 8We tested several bases in the experiments and found there were little difference in performance between them. 245 4 Experiments 4.1 Experimental Conditions A set of essays9 written by Japanese learners of English was used as the target essays in the experiments. It consisted of 47 essays (3180 words) on the topic traveling. A native speaker of English who was a professional rewriter of English recognized 105 target errors in it. The written part of the British National Corpus (BNC) (Burnard, 1995) was used to learn decision lists. Sentences the OAK system10, which was used to extract NPs from the corpus, failed to analyze were excluded. After these operations, the size of the corpus approximately amounted to 80 million words. Hereafter, the corpus will be referred to as the BNC. As another corpus, the English concept explication in the EDR English-Japanese Bilingual dictionary and the EDR corpus (1993) were used; it will be referred to as the EDR corpus, hereafter. Its size amounted to about 3 million words. Performance of the proposed method was evaluated by recall and precision. Reca</context>
</contexts>
<marker>Burnard, 1995</marker>
<rawString>L. Burnard. 1995. Users Reference Guide for the British National Corpus. version 1.0. Oxford University Computing Services, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>An unsupervised method for detecting grammatical errors.</title>
<date>2000</date>
<booktitle>In Proc. of 1st Meeting of the North America Chapter of ACL,</booktitle>
<pages>140--147</pages>
<contexts>
<context position="3023" citStr="Chodorow and Leacock, 2000" startWordPosition="483" endWordPosition="486">tances of mass and count nouns, are automatically generated from a corpus. Then, decision lists for distinguishing mass and count nouns are learned from the training data. Finally, the decision lists are used with the conventional rules to detect the target errors. The proposed method requires a corpus to learn decision lists for distinguishing mass and count nouns. General corpora such as newspaper articles can be used for the purpose. However, a drawback to it is that there are differences in character between general corpora and the writing of non-native learners of English (Granger, 1998; Chodorow and Leacock, 2000). For instance, Chodorow and Leacock (2000) point out that the word concentrate is usually used as a noun in a general corpus whereas it is a verb 91% of the time in essays written by non-native learners of English. Consequently, the differences affect the performance of the proposed method. In order to reduce the drawback, the proposed method is augmented by feedback; it takes as feedback learners’ essays whose errors are corrected by a teacher of English (hereafter, referred to as the feedback corpus). In essence, the feedback corpus could be added to a general corpus to generate training da</context>
</contexts>
<marker>Chodorow, Leacock, 2000</marker>
<rawString>M. Chodorow and C. Leacock. 2000. An unsupervised method for detecting grammatical errors. In Proc. of 1st Meeting of the North America Chapter of ACL, pages 140–147.</rawString>
</citation>
<citation valid="true">
<title>electronic dictionary research institute ltd.</title>
<date></date>
<marker></marker>
<rawString>Japan electronic dictionary research institute ltd. 1993. EDR electronic dictionary specifications guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Huddleston</author>
<author>G K Pullum</author>
</authors>
<title>The Cambridge Grammar of the English Language.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="5049" citStr="Huddleston and Pullum, 2002" startWordPosition="822" endWordPosition="825">e the proposed method. 2 Method for detecting the target errors 2.1 Generating training data First, instances of the target noun that head their noun phrase (NP) are collected from a corpus with their surrounding words. This can be simply done by an existing chunker or parser. Then, the collected instances are tagged with mass or count by the following tagging rules. For example, the underlined chicken: ... are a lot of chickens in the roost ... is tagged as ... are a lot of chickens/count in the roost ... because it is in plural form. We have made tagging rules based on linguistic knowledge (Huddleston and Pullum, 2002). Figure 1 and Table 1 represent the tagging rules. Figure 1 shows the framework of the tagging rules. Each node in Figure 1 represents a question applied to the instance in question. For example, the root node reads “Is the instance in question plural?”. Each leaf represents a result of the classification. For example, if the answer is yes at the root node, the instance in question is tagged with count. Otherwise, the question at the lower node is applied and so on. The tagging rules do not classify instances as mass or count in some cases. These unclassified instances are tagged with the sym</context>
<context position="19611" citStr="Huddleston and Pullum, 2002" startWordPosition="3402" endWordPosition="3405">ize amounted to about 3 million words. Performance of the proposed method was evaluated by recall and precision. Recall is defined by No. of target errors detected correctly (10) No. of target errors in the target essays Precision is defined by No. of target errors detected correctly (11) No. of detected errors 4.2 Experimental Procedures First, decision lists for each target noun in the target essays were learned from the BNC11. To extract noun phrases and their head nouns, the OAK system was used. An optimal value for (window size of context) was estimated as follows. For 25 nouns shown in (Huddleston and Pullum, 2002) as examples of nouns used as both mass and count nouns, accuracy on the BNC was calculated using ten-fold cross validation. As a result of setting small ( ), medium ( ), and large ( ) window sizes, it turned out that maximized the average accuracy. Following this result, was selected in the experiments. Second, the target nouns were distinguished whether they were mass or count by the learned 9http://www.eng.ritsumei.ac.jp/lcorpus/. 10OAK System Homepage: http://nlp.cs.nyu.edu/oak/. 11If no instance of the target noun is found in the general corpora (and also in the feedback corpus in case of</context>
</contexts>
<marker>Huddleston, Pullum, 2002</marker>
<rawString>R. Huddleston and G.K. Pullum. 2002. The Cambridge Grammar of the English Language. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>T Saiga</author>
<author>T Supnithi</author>
<author>H Isahara</author>
</authors>
<title>Automatic error detection in the Japanese learners’ English spoken data.</title>
<date>2003</date>
<booktitle>In Proc. of 41st Annual Meeting ofACL,</booktitle>
<pages>145--148</pages>
<contexts>
<context position="1735" citStr="Izumi et al., 2003" startWordPosition="283" endWordPosition="286"> it has been pointed out that it is hard to write rules for detecting errors concerning the articles and singular plural usage. To be precise, it is hard to write rules for distinguishing mass and count nouns which are particularly important in detecting these errors (Kawai et al., 1984). The major reason for this is that whether a noun is a mass noun or a count noun greatly depends on its meaning or its surrounding context (refer to Allan (1980) and Bond (2005) for details of the mass count distinction). The above errors are very common among Japanese learners of English (Kawai et al., 1984; Izumi et al., 2003). This is perhaps because the Atsuo Kawai Mie University 5148507, Japan kawai@ai.info.mie-u.ac.jp Naoki Isu Mie University 5148507, Japan isu@ai.info.mie-u.ac.jp Japanese language does not have a mass count distinction system similar to that of English. Thus, it is favorable for error detection systems aiming at Japanese learners to be capable of detecting these errors. In other words, such systems need to somehow distinguish mass and count nouns. This paper proposes a method for distinguishing mass and count nouns in context to complement the conventional rules for detecting grammatical error</context>
</contexts>
<marker>Izumi, Uchimoto, Saiga, Supnithi, Isahara, 2003</marker>
<rawString>E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isahara. 2003. Automatic error detection in the Japanese learners’ English spoken data. In Proc. of 41st Annual Meeting ofACL, pages 145–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kawai</author>
<author>K Sugihara</author>
<author>N Sugie</author>
</authors>
<title>ASPEC-I: An error detection system for English composition.</title>
<date>1984</date>
<journal>IPSJ Journal (in Japanese),</journal>
<volume>25</volume>
<issue>6</issue>
<contexts>
<context position="924" citStr="Kawai et al., 1984" startWordPosition="139" endWordPosition="142">etecting errors in article usage and singular plural usage based on the mass count distinction. First, it learns decision lists from training data generated automatically to distinguish mass and count nouns. Then, in order to improve its performance, it is augmented by feedback that is obtained from the writing of learners. Finally, it detects errors by applying rules to the mass count distinction. Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback. 1 Introduction Although several researchers (Kawai et al., 1984; McCoy et al., 1996; Schneider and McCoy, 1998; Tschichold et al., 1997) have shown that rulebased methods are effective to detecting grammatical errors in the writing of learners of English, it has been pointed out that it is hard to write rules for detecting errors concerning the articles and singular plural usage. To be precise, it is hard to write rules for distinguishing mass and count nouns which are particularly important in detecting these errors (Kawai et al., 1984). The major reason for this is that whether a noun is a mass noun or a count noun greatly depends on its meaning or its </context>
<context position="21778" citStr="Kawai et al., 1984" startWordPosition="3765" endWordPosition="3768">c written in other classes. Finally, the above two methods were compared with their seven variants shown in Table 5. “DL” in Table 5 refers to the nine decision list based methods (the above two methods and their seven variants). The words in brackets denote the corpora used to learn decision lists; the symbol “+FB” means that the feedback corpus was simply added to the general corpus. The subscripts and indicate that the feedback was done by using Equation (8) and Equation (9), respectively. In addition to the seven variants, two kinds of earlier method were used for comparison. One was one (Kawai et al., 1984) of the rule-based methods. It judges singular head nouns with no determiner to be erroneous since missing articles are most common in the writing of Japanese learners of English. In the experiments, this was implemented by treating all nouns as count nouns and applying the same detection rules as in the proposed method to the countability. The other was a web-based method (Lapata and Keller, 2005)12 for generating articles. It retrieves web counts for queries consisting of two words preceding the NP that the target noun head, one of the articles (a/an, the, ), and the core NP to generate arti</context>
</contexts>
<marker>Kawai, Sugihara, Sugie, 1984</marker>
<rawString>A. Kawai, K. Sugihara, and N. Sugie. 1984. ASPEC-I: An error detection system for English composition. IPSJ Journal (in Japanese), 25(6):1072–1079.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>F Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="22179" citStr="Lapata and Keller, 2005" startWordPosition="3834" endWordPosition="3837">scripts and indicate that the feedback was done by using Equation (8) and Equation (9), respectively. In addition to the seven variants, two kinds of earlier method were used for comparison. One was one (Kawai et al., 1984) of the rule-based methods. It judges singular head nouns with no determiner to be erroneous since missing articles are most common in the writing of Japanese learners of English. In the experiments, this was implemented by treating all nouns as count nouns and applying the same detection rules as in the proposed method to the countability. The other was a web-based method (Lapata and Keller, 2005)12 for generating articles. It retrieves web counts for queries consisting of two words preceding the NP that the target noun head, one of the articles (a/an, the, ), and the core NP to generate articles. All queries are performed as exact matches using quotation marks and submitted to the Google search engine in lower case. For example, in the case of “*She is good student.”, it retrieves web counts for “she is a good student”, 12There are other statistical methods that can be used for comparison including Lee (2004) and Minnen (2000). Lapata and Keller (2005) report that the web-based method</context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>M. Lapata and F. Keller. 2005. Web-based models for natural language processing. ACM Transactions on Speech and Language Processing, 2(1):1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
</authors>
<title>Automatic article restoration.</title>
<date>2004</date>
<booktitle>In Proc. of the Human Language Technology Conference of the North American Chapter ofACL,</booktitle>
<pages>31--36</pages>
<contexts>
<context position="22702" citStr="Lee (2004)" startWordPosition="3926" endWordPosition="3927"> method to the countability. The other was a web-based method (Lapata and Keller, 2005)12 for generating articles. It retrieves web counts for queries consisting of two words preceding the NP that the target noun head, one of the articles (a/an, the, ), and the core NP to generate articles. All queries are performed as exact matches using quotation marks and submitted to the Google search engine in lower case. For example, in the case of “*She is good student.”, it retrieves web counts for “she is a good student”, 12There are other statistical methods that can be used for comparison including Lee (2004) and Minnen (2000). Lapata and Keller (2005) report that the web-based method is the best performing article generation method. 246 “she is the good student”, and “she is good student”. Then, it generates the article that maximizes the web counts. We extended it to make it capable of detecting our target errors. First, the singular/plural distinction was taken into account in the queries (e.g., “she is a good students”, “she is the good students”, and “she is good students” in addition to the above three queries). The one(s) that maximized the web counts was judged to be correct; the rest were</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>J. Lee. 2004. Automatic article restoration. In Proc. of the Human Language Technology Conference of the North American Chapter ofACL, pages 31–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K F McCoy</author>
<author>C A Pennington</author>
<author>L Z Suri</author>
</authors>
<title>English error correction: A syntactic user model based on principled “mal-rule” scoring.</title>
<date>1996</date>
<booktitle>In Proc. of 5th International Conference on User Modeling,</booktitle>
<pages>69--66</pages>
<contexts>
<context position="944" citStr="McCoy et al., 1996" startWordPosition="143" endWordPosition="146">rticle usage and singular plural usage based on the mass count distinction. First, it learns decision lists from training data generated automatically to distinguish mass and count nouns. Then, in order to improve its performance, it is augmented by feedback that is obtained from the writing of learners. Finally, it detects errors by applying rules to the mass count distinction. Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback. 1 Introduction Although several researchers (Kawai et al., 1984; McCoy et al., 1996; Schneider and McCoy, 1998; Tschichold et al., 1997) have shown that rulebased methods are effective to detecting grammatical errors in the writing of learners of English, it has been pointed out that it is hard to write rules for detecting errors concerning the articles and singular plural usage. To be precise, it is hard to write rules for distinguishing mass and count nouns which are particularly important in detecting these errors (Kawai et al., 1984). The major reason for this is that whether a noun is a mass noun or a count noun greatly depends on its meaning or its surrounding context </context>
</contexts>
<marker>McCoy, Pennington, Suri, 1996</marker>
<rawString>K.F. McCoy, C.A. Pennington, and L.Z. Suri. 1996. English error correction: A syntactic user model based on principled “mal-rule” scoring. In Proc. of 5th International Conference on User Modeling, pages 69–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>F Bond</author>
<author>A Copestake</author>
</authors>
<title>Memory-based learning for article generation.</title>
<date>2000</date>
<booktitle>In Proc. of CoNLL-2000 and LLL-2000 workshop,</booktitle>
<pages>43--48</pages>
<marker>Minnen, Bond, Copestake, 2000</marker>
<rawString>G. Minnen, F. Bond, and A. Copestake. 2000. Memory-based learning for article generation. In Proc. of CoNLL-2000 and LLL-2000 workshop, pages 43–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ostler</author>
<author>B T S Atkins</author>
</authors>
<title>Predictable meaning shift: Some linguistic properties of lexical implication rules.</title>
<date>1991</date>
<booktitle>In Proc. of 1st SIGLEX Workshop on Lexical Semantics and Knowledge Representation,</booktitle>
<pages>87--100</pages>
<contexts>
<context position="12366" citStr="Ostler and Atkins, 1991" startWordPosition="2151" endWordPosition="2154">2: Rules in a decision list Count LLR 1.49 1.32 1.23 1.23 1.18 On one hand, we associate the words in the left half with food or cooking. On the other hand, we associate those in the right half with animals or birds. From this observation, we can say that chicken in the sense of an animal or a bird is a count noun but a mass noun when referring to food 3The probability for the default rule is estimated just as the log-likelihood ratio for the default rule above. 4It depends on the target noun how many rules are discarded. or cooking, which agrees with the knowledge presented in previous work (Ostler and Atkins, 1991). 2.3 Distinguishing mass and count nouns To distinguish the target noun in novel context, each rule in the decision list is tested on it in the sorted order until the first applicable one is found. It is distinguished according to the first applicable one. Ties are broken by the rules below. It should be noted that rules sorted below the default rule are never used because the default rule is always applicable to the target noun. This is the reason why rules sorted below the default rule are discarded as mentioned in Section 2.2. 2.4 Detecting the target errors The target errors are detected </context>
</contexts>
<marker>Ostler, Atkins, 1991</marker>
<rawString>N. Ostler and B.T.S Atkins. 1991. Predictable meaning shift: Some linguistic properties of lexical implication rules. In Proc. of 1st SIGLEX Workshop on Lexical Semantics and Knowledge Representation, pages 87–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Schneider</author>
<author>K F McCoy</author>
</authors>
<title>Recognizing syntactic errors in the writing of second language learners.</title>
<date>1998</date>
<booktitle>In Proc. of 17th International Conference on Computational Linguistics,</booktitle>
<pages>1198--1205</pages>
<contexts>
<context position="971" citStr="Schneider and McCoy, 1998" startWordPosition="147" endWordPosition="150">gular plural usage based on the mass count distinction. First, it learns decision lists from training data generated automatically to distinguish mass and count nouns. Then, in order to improve its performance, it is augmented by feedback that is obtained from the writing of learners. Finally, it detects errors by applying rules to the mass count distinction. Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback. 1 Introduction Although several researchers (Kawai et al., 1984; McCoy et al., 1996; Schneider and McCoy, 1998; Tschichold et al., 1997) have shown that rulebased methods are effective to detecting grammatical errors in the writing of learners of English, it has been pointed out that it is hard to write rules for detecting errors concerning the articles and singular plural usage. To be precise, it is hard to write rules for distinguishing mass and count nouns which are particularly important in detecting these errors (Kawai et al., 1984). The major reason for this is that whether a noun is a mass noun or a count noun greatly depends on its meaning or its surrounding context (refer to Allan (1980) and </context>
</contexts>
<marker>Schneider, McCoy, 1998</marker>
<rawString>D. Schneider and K.F. McCoy. 1998. Recognizing syntactic errors in the writing of second language learners. In Proc. of 17th International Conference on Computational Linguistics, pages 1198–1205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tanaka</author>
</authors>
<title>Psychological methods (in Japanese).</title>
<date>1977</date>
<publisher>University of Tokyo Press.</publisher>
<contexts>
<context position="16060" citStr="Tanaka, 1977" startWordPosition="2801" endWordPosition="2802">o is the effect of the feedback corpus on . This means that the feedback corpus hardly has effect on the performance. Instead, can be estimated by interpolating the probabilities estimated from the feedback corpus and the general corpus according to confidences of their estimates. It is favorable that the interpolated probability approaches to the probability estimated from the feedback corpus as its confidence increases; the more confident its estimate is, the more effect it has on the interpolated probability. Here, confidence of ratio is measured by the reciprocal of variance of the ratio (Tanaka, 1977). Variance is calculated by where denotes the number of samples used for calculating the ratio. Therefore, confidence of the estimate of the conditional probability used in the proposed method is measured by 6The feedback corpus refers to learners’ essays whose errors are corrected as mentioned in Section 1. In Equation (8), the effect of on becomes large as its confidence increases. It should also be noted that when its confidence exceeds that of , the general corpus is no longer used in the interpolated probability. A problem that arises in Equation (8) is that hardly has effect on when a mu</context>
</contexts>
<marker>Tanaka, 1977</marker>
<rawString>Y. Tanaka. 1977. Psychological methods (in Japanese). University of Tokyo Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tschichold</author>
<author>F Bodmer</author>
<author>E Cornu</author>
<author>F Grosjean</author>
<author>L Grosjean</author>
<author>N Kubler</author>
<author>N Lewy</author>
<author>C Tschumi</author>
</authors>
<title>Developing a new grammar checker for English as a second language.</title>
<date>1997</date>
<booktitle>In Proc. of the From Research to CommercialApplications Workshop,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="997" citStr="Tschichold et al., 1997" startWordPosition="151" endWordPosition="154"> the mass count distinction. First, it learns decision lists from training data generated automatically to distinguish mass and count nouns. Then, in order to improve its performance, it is augmented by feedback that is obtained from the writing of learners. Finally, it detects errors by applying rules to the mass count distinction. Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback. 1 Introduction Although several researchers (Kawai et al., 1984; McCoy et al., 1996; Schneider and McCoy, 1998; Tschichold et al., 1997) have shown that rulebased methods are effective to detecting grammatical errors in the writing of learners of English, it has been pointed out that it is hard to write rules for detecting errors concerning the articles and singular plural usage. To be precise, it is hard to write rules for distinguishing mass and count nouns which are particularly important in detecting these errors (Kawai et al., 1984). The major reason for this is that whether a noun is a mass noun or a count noun greatly depends on its meaning or its surrounding context (refer to Allan (1980) and Bond (2005) for details of</context>
</contexts>
<marker>Tschichold, Bodmer, Cornu, Grosjean, Grosjean, Kubler, Lewy, Tschumi, 1997</marker>
<rawString>C. Tschichold, F. Bodmer, E. Cornu, F. Grosjean, L. Grosjean, N. Kubler, N. Lewy, and C. Tschumi. 1997. Developing a new grammar checker for English as a second language. In Proc. of the From Research to CommercialApplications Workshop, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proc. of 33rd Annual Meeting ofACL,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="10272" citStr="Yarowsky, 1995" startWordPosition="1761" endWordPosition="1762">the words in Table 1 are excluded. Also, function words (except prepositions), cardinal and quasi-cardinal numerals, and the target noun are excluded. All words are reduced to their morphological stem and converted entirely to lower case when collected. For example, the following tagged instance: She ate fried chicken/mass for dinner. would give a set of rules that match the template: where and major denote the target noun and the majority of in the training data, respectively. Equation (3) reads “If the target noun appears, then it is distinguished by the majority”. The log-likelihood ratio (Yarowsky, 1995) decides in which order rules are applied to the target noun in novel context. It is defined bye (4) where is the exclusive event of and is the probability that the target noun is used as when appears in the context . It is important to exercise some care in estimating . In principle, we could simply 2For the default rule, the log-likelihood ratio is defined by replacing and withand major, respectively. 243 count the number of times that appears in the context of the target noun used as in the training data. However, this estimate can be unreliable, when does not appear often in the context. T</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. of 33rd Annual Meeting ofACL, pages 189–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Homograph Disambiguation in Speech Synthesis.</title>
<date>1996</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="10938" citStr="Yarowsky, 1996" startWordPosition="1883" endWordPosition="1884">rget noun in novel context. It is defined bye (4) where is the exclusive event of and is the probability that the target noun is used as when appears in the context . It is important to exercise some care in estimating . In principle, we could simply 2For the default rule, the log-likelihood ratio is defined by replacing and withand major, respectively. 243 count the number of times that appears in the context of the target noun used as in the training data. However, this estimate can be unreliable, when does not appear often in the context. To solve this problem, using a smoothing parameter (Yarowsky, 1996), is estimated by&apos; (5) where and are occurrences of appearing in and those in of the target noun used as , respectively. The constant is the number of possible classes, that is, ( or ) in our case, and introduced to satisfy . In this paper, is set to 1. Rules in a decision list are sorted in descending order by the log-likelihood ratio. They are tested on the target noun in novel context in this order. Rules sorted below the default rule are discarded because they are never used as we will see in Section 2.3. Table 2 shows part of a decision list for the target noun chicken that was learned fr</context>
</contexts>
<marker>Yarowsky, 1996</marker>
<rawString>D. Yarowsky. 1996. Homograph Disambiguation in Speech Synthesis. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>