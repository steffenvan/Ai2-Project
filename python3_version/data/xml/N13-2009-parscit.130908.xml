<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003618">
<title confidence="0.996953">
Large-Scale Paraphrasing for Natural Language Understanding
</title>
<author confidence="0.979022">
Juri Ganitkevitch
</author>
<affiliation confidence="0.947233">
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<email confidence="0.992094">
juri@cs.jhu.edu
</email>
<sectionHeader confidence="0.997325" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999532409090909">
We examine the application of data-driven
paraphrasing to natural language understand-
ing. We leverage bilingual parallel corpora
to extract a large collection of syntactic para-
phrase pairs, and introduce an adaptation
scheme that allows us to tackle a variety of
text transformation tasks via paraphrasing. We
evaluate our system on the sentence compres-
sion task. Further, we use distributional sim-
ilarity measures based on context vectors de-
rived from large monolingual corpora to anno-
tate our paraphrases with an orthogonal source
of information. This yields significant im-
provements in our compression system’s out-
put quality, achieving state-of-the-art perfor-
mance. Finally, we propose a refinement of
our paraphrases by classifying them into nat-
ural logic entailment relations. By extend-
ing the synchronous parsing paradigm towards
these entailment relations, we will enable our
system to perform recognition of textual en-
tailment.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940333333333">
In this work, we propose an extension of current
paraphrasing methods to tackle natural language un-
derstanding problems. We create a large set of para-
phrase pairs in a data-driven fashion, rank them
based on a variety of similarity metrics, and attach
an entailment relation to each pair, facilitating nat-
ural logic inference. The resulting resource has po-
tential applications to a variety of NLP applications,
including summarization, query expansion, question
answering, and recognizing textual entailment.
Specifically, we build on Callison-Burch (2007)’s
pivot-based paraphrase extraction method, which
uses bilingual parallel data to learn English phrase
pairs that share the same meaning. Our approach ex-
tends the pivot method to learn meaning-preserving
</bodyText>
<page confidence="0.988442">
62
</page>
<bodyText confidence="0.999372">
syntactic transformations in English. We repre-
sent these using synchronous context-free grammars
(SCFGs). This representation allows us to re-use
a lot of machine translation machinery to perform
monolingual text-to-text generation. We demon-
strate the method on a sentence compression task
(Ganitkevitch et al., 2011).
To improve the system, we then incorporate fea-
tures based on monolingual distributional similar-
ity. This orthogonal source of signal allows us to
re-scores the bilingually-extracted paraphrases us-
ing information drawn from large monolingual cor-
pora. We show that the monolingual distributional
scores yield significant improvements over a base-
line that scores paraphrases only with bilingually-
extracted features (Ganitkevitch et al., 2012).
Further, we propose a semantics for paraphras-
ing by classifying each paraphrase pair with one
of the entailment relation types defined by natural
logic (MacCartney, 2009). Natural logic is used
to perform inference over pairs of natural language
phrases, like our paraphrase pairs. It defines a set of
relations including, equivalence (≡), forward- and
backward-entailments (C, antonyms (∧), and
others. We will build a classifier for our paraphrases
that uses features extracted from annotated resources
like WordNet and distributional information gath-
ered over large text corpora to assign one or more
entailment relations to each paraphrase pair. We will
evaluate the entailment assignments by applying this
enhanced paraphrasing system to the task of recog-
nizing textual entailment (RTE).
</bodyText>
<sectionHeader confidence="0.982924" genericHeader="introduction">
2 Extraction of Syntactic Paraphrases
from Bitexts
</sectionHeader>
<bodyText confidence="0.993476">
A variety of different types of corpora have been
used to automatically induce paraphrase collections
for English (see Madnani and Dorr (2010) for a sur-
</bodyText>
<note confidence="0.8543555">
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 62–68,
Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.904999">
Figure 1: An example of pivot-based phrasal paraphrase
extraction – we assume English phrases that translate to
a common German phrase to be paraphrases. Thus we
extract “imprisoned” as a paraphrase of “thrown into jail.”
</figureCaption>
<bodyText confidence="0.971737">
vey of these methods). Bannard and Callison-Burch
(2005) extracted phrasal paraphrases from bitext by
using foreign language phrases as a pivot: if two
English phrases e1 and e2 both translate to a for-
eign phrase f, they assume that e1 and e2 are para-
phrases of one another. Figure 1 gives an example
of a phrasal paraphrase extracted by Bannard and
Callison-Burch (2005).
Since “thrown into jail” is aligned to multiple
German phrases, and since each of those German
phrases align back to a variety of English phrases,
the method extracts a wide range of possible para-
phrases including good paraphrase like: imprisoned
and thrown into prison. It also produces less good
paraphrases like: in jail and put in prison for, and
bad paraphrases, such as maltreated and protec-
tion, because of noisy/inaccurate word alignments
and other problems. To rank these, Bannard and
Callison-Burch (2005) derive a paraphrase probabil-
ity p(e1|e2):
</bodyText>
<equation confidence="0.9662865">
�p(e2|e1) ≈ p(e2|f)p(f|e1), (1)
f
</equation>
<bodyText confidence="0.99929">
where the p(ei|f) and p(f|ei) are translation proba-
bilities estimated from the bitext (Brown et al., 1990;
Koehn et al., 2003).
We extend this method to extract syntactic para-
phrases (Ganitkevitch et al., 2011). Table 1
shows example paraphrases produced by our sys-
tem. While phrasal systems memorize phrase pairs
without any further generalization, a syntactic para-
phrasing system can learn more generic patterns.
These can be better applied to unseen data. The
paraphrases implementing the possessive rule and
</bodyText>
<table confidence="0.999537">
Possessive rule
NP the NN of the NNP the NNP’s NN
NP the NP made by NN the NN’s NP
Dative shift
VP give NN to NP give NP the NN
VP provide NP1 to NP2 give NP2 NP1
Partitive constructions
NP CD of the NN CD NN
NP all NN all of the NN
Reduced relative clause
SBAR/S although PRP VBP that although PRP VBP
ADJP very JJ that S JJ S
</table>
<tableCaption confidence="0.9952682">
Table 1: A selection of example paraphrase patterns ex-
tracted by our system. These rules demonstrate that, us-
ing the pivot approach from Figure 1, our system is capa-
ble of learning meaning-preserving syntactic transforma-
tions in English.
</tableCaption>
<bodyText confidence="0.9999658">
the dative shift shown in Table 1 are good examples
of this: the two noun-phrase arguments to the ex-
pressions are abstracted to nonterminals while each
rule’s lexicalization provides an appropriate frame
of evidence for the transform.
</bodyText>
<subsectionHeader confidence="0.968144">
2.1 Formal Representation
</subsectionHeader>
<bodyText confidence="0.999895764705882">
In this proposal we focus on a paraphrase
model based on synchronous context-free gram-
mar (SCFG). The SCFG formalism (Aho and Ull-
man, 1972) was repopularized for statistical ma-
chine translation by (Chiang, 2005). An probabilis-
tic SCFG g contains rules r of the form r = C →
(-y, α, ∼, w). A rule r’s left-hand side C is a nonter-
minal, while its right-hands sides -y and α can be
mixed strings of words and nonterminal symbols.
There is a one-to-one correspondency between the
nonterminals in -y and α. Each rule is assigned a
cost wr ≥ 0, reflecting its likelihood.
To compute the cost wr of the application of a
rule r, we define a set of feature functions c� =
{c1...cN} that are combined in a log-linear model.
The model weights are set to maximize a task-
dependent objective function.
</bodyText>
<subsectionHeader confidence="0.9954685">
2.2 Syntactic Paraphrase Rules via Bilingual
Pivoting
</subsectionHeader>
<bodyText confidence="0.9950804">
Our paraphrase acquisition method is based on the
extraction of syntactic translation rules in statistical
machine translation (SMT). In SMT, SCFG rules are
extracted from English-foreign sentence pairs that
are automatically parsed and word-aligned. For a
</bodyText>
<figure confidence="0.979256416666667">
... 5 farmers were
... fünf Landwirte
... oder wurden
... or have been
thrown into jail
festgenommen
festgenommen
imprisoned
in Ireland ...
, weil ...
, gefoltert ...
, tortured ...
</figure>
<page confidence="0.991488">
63
</page>
<table confidence="0.999040571428571">
CR Meaning Grammar
Reference 0.80 4.80 4.54
ILP 0.74 3.44 3.41
PP 0.78 3.53 2.98
PP + n-gram 0.80 3.65 3.16
PP + syntax 0.79 3.70 3.26
Random Deletions 0.78 2.91 2.53
</table>
<tableCaption confidence="0.8194986">
Table 2: Results of the human evaluation on longer com-
pressions: pairwise compression ratios (CR), meaning
and grammaticality scores. Bold indicates a statistically
significant best result at p &lt; 0.05. The scores range from
1 to 5, 5 being perfect.
</tableCaption>
<bodyText confidence="0.999904272727273">
foreign phrase the corresponding English phrase is
found via the word alignments. This phrase pair
is turned into an SCFG rule by assigning a left-
hand side nonterminal symbol, corresponding to
the syntactic constituent that dominates the English
phrase. To introduce nonterminals into the right-
hand sides of the rule, we can replace correspond-
ing sub-phrases in the English and foreign phrases
with nonterminal symbols. Doing this for all sen-
tence pairs in a bilingual parallel corpus results in a
translation grammar that serves as the basis for syn-
tactic machine translation.
To create a paraphrase grammar from a transla-
tion grammar, we extend the syntactically informed
pivot approach of (Callison-Burch, 2008) to the
SCFG model: for each pair of translation rules r1
and r2 with matching left-hand side nonterminal C
and foreign language right-hand side -y: r1 = C –+
(-y, α1, —1, �61) and r2 = C –+ (-y, α2, —2, 02),
we pivot over -y and create a paraphrase rule rp:
rp = C –+ (α1, α2, —, –ϕ). We estimate the cost
for rp following Equation 1.
</bodyText>
<subsectionHeader confidence="0.997362">
2.3 Task-Based Evaluation
</subsectionHeader>
<bodyText confidence="0.99992975">
Sharing its SCFG formalism permits us to re-use
much of SMT’s machinery for paraphrasing appli-
cations, including decoding and minimum error rate
training. This allows us to easily tackle a variety of
monolingual text-to-text generation tasks, which can
be cast as sentential paraphrasing with task-specific
constraints or goals.
For our evaluation, we apply our paraphrase sys-
tem to sentence compression. However, to success-
fully use paraphrases for sentence compression, we
need to adapt the system to suit the task. We intro-
duce a four-point adaptation scheme for text-to-text
</bodyText>
<figureCaption confidence="0.9956885">
Figure 2: An example of a synchronous paraphrastic
derivation in sentence compression.
</figureCaption>
<bodyText confidence="0.539222">
generation via paraphrases, suggesting:
</bodyText>
<listItem confidence="0.947419692307692">
• The use task-targeted features that capture in-
formation pertinent to the text transformation.
For sentence compression the features include
word count and length-difference features.
• An objective function that takes into account
the contraints imposed by the task. We use
PR´ECIS, an augmentation of the BLEU metric,
which introduces a verbosity penalty.
• Development data that represents the precise
transformations we seek to model. We use a set
of human-made example compressions mined
from translation references.
• Optionally, grammar augmentations that allow
</listItem>
<bodyText confidence="0.992225266666667">
for the incorporation of effects that the learned
paraphrase grammar cannot capture. We exper-
imented with automatically generated deletion
rules.
Applying the above adaptations to our generic para-
phraser (PP), quickly yields a sentence compression
system that performs on par with a state-of-the-art
integer linear programming-based (ILP) compres-
sion system (Clarke and Lapata, 2008). As Table 2
shows, human evaluation results suggest that our
system outperforms the contrast system in meaning
retention. However, it suffers losses in grammatical-
ity. Figure 2 shows an example derivation produced
as a result of applying our paraphrase rules in the
decoding process.
</bodyText>
<sectionHeader confidence="0.972910666666667" genericHeader="method">
3 Integrating Monolingual Distributional
Similarity into Bilingually Extracted
Paraphrases
</sectionHeader>
<table confidence="0.8258643125">
Distributional similarity-based methods (Lin and
Pantel, 2001; Bhagat and Ravichandran, 2008) rely
NP VP
NP
NP
DT+NNP
CD NNS JJ DT NNP
the prophet mohammad
twelve
of the cartoons that are offensive to
cartoons insulting the prophet mohammad
12
CD NNS JJ DT NNP
NP
DT+NNP
NP VP NP
</table>
<page confidence="0.996177">
64
</page>
<bodyText confidence="0.999711727272727">
on the assumption that similar expressions appear
in similar contexts – a signal that is orthogonal to
bilingual pivot information we have considered thus
far. However, the monolingual distributional signal
is noisy: it suffers from problems such as mistaking
cousin expressions or antonyms (such as (rise, fall)
or (boy, girl)) for paraphrases. We circumvent this
issue by starting with a paraphrase grammar ex-
tracted from bilingual data and reranking it with in-
formation based on distributional similarity (Gan-
itkevitch et al., 2012).
</bodyText>
<subsectionHeader confidence="0.997521">
3.1 Distributional Similarity
</subsectionHeader>
<bodyText confidence="0.999987315789474">
In order to compute the similarity of two expressions
e1 and e2, their respective occurrences across a cor-
pus are aggregated in context vectors cc1 and cc2. The
cci are typically vectors in a high-dimensional fea-
ture space with features like counts for words seen
within a window of an ei. For parsed data more so-
phisticated features based on syntax and dependency
structure around an occurrence are possible. The
comparison of e1 and e2 is then made by comput-
ing the cosine similarity between cc1 and cc2.
Over large corpora the context vectors for even
moderately frequent ei can grow unmanageably
large. Locality sensitive hashing provides a way of
dealing with this problem: instead of retaining the
explicit sparse high-dimensional cci, we use a ran-
dom projection h(·) to convert them into compact bit
signatures in a dense b-dimensional boolean space
in which approximate similarity calculation is pos-
sible.
</bodyText>
<subsectionHeader confidence="0.9945255">
3.2 Integrating Similarity with Syntactic
Paraphrases
</subsectionHeader>
<bodyText confidence="0.999773461538461">
In order to incorporate distributional similarity in-
formation into the paraphrasing system, we need
to calculate similarity scores for the paraphrastic
SCFG rules in our grammar. For rules with purely
lexical right-hand sides e1 and e2 this is a simple
task, and the similarity score sim(e1, e2) can be di-
rectly included in the rule’s feature vector cp. How-
ever, if e1 and e2 are long, their occurrences be-
come sparse and their similarity can no longer be
reliably estimated. In our case, the right-hand sides
of our rules also contain non-terminal symbols and
re-ordered phrases, so computing a similarity score
is not straightforward.
</bodyText>
<figureCaption confidence="0.9395795">
Figure 3: An example of the n-gram feature extraction
on an n-gram corpus. Here, “the long-term” is seen pre-
ceded by “revise” (43 times) and followed by “plans” (97
times).
</figureCaption>
<bodyText confidence="0.999963375">
Our solution is to decompose the discontinuous
patterns that make up the right-hand sides of a rule r
into pairs of contiguous phrases, for which we then
look up distributional signatures and compute sim-
ilarity scores. To avoid comparing unrelated pairs,
we require the phrase pairs to be consistent with a to-
ken alignment a, defined and computed analogously
to word alignments in machine translation.
</bodyText>
<subsectionHeader confidence="0.9982645">
3.3 Data Sets and Types of Distributional
Signatures
</subsectionHeader>
<bodyText confidence="0.999987157894737">
We investigate the impact of the data and feature set
used to construct distributional signatures. In partic-
ular we contrast two approaches: a large collection
of distributional signatures with a relatively simple
feature set, and a much smaller set of signatures with
a rich, syntactically informed feature set.
The larger n-gram model is drawn from a web-
scale n-gram corpus (Brants and Franz, 2006; Lin et
al., 2010). Figure 3 illustrates this feature extraction
approach. The resulting collection comprises distri-
butional signatures for the 200 million most frequent
1-to-4-grams in the n-gram corpus.
For the syntactically informed model, we use
the constituency and dependency parses provided
in the Annotated Gigaword corpus (Napoles et al.,
2012). Figure 4 illustrates this model’s feature ex-
traction for an example phrase occurrence. Using
this method we extract distributional signatures for
over 12 million 1-to-4-gram phrases.
</bodyText>
<subsectionHeader confidence="0.732774">
3.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.9998125">
For evaluation, we follow the task-based approach
taken in Section 2 and apply the similarity-scored
</bodyText>
<figure confidence="0.997125411764706">
25 achieve
the long-term
L-achieve = 25 R-plans = 97
~sig the long-term = L-revise = 43 R-goals = 23
L-confirmed = 64 R-investment = 10
the long-term
the long-term
the long-term
the long-term
the long-term
goals 23
plans 97
investment 10
43
revise
64
confirmed
</figure>
<page confidence="0.941842">
65
</page>
<figureCaption confidence="0.985955">
Figure 4: An example of the syntactic feature-set. The
</figureCaption>
<bodyText confidence="0.924722291666667">
phrase “the long-term” is annotated with position-aware
lexical and part-of-speech n-gram features, labeled de-
pendency links, and features derived from the phrase’s
CCG label (NP/NN).
paraphrases to sentence compression. The distri-
butional similarity scores are incorporated into the
paraphrasing system as additional rule features into
the log-linear model. The task-targeted parameter
tuning thus results in a reranking of the rules that
takes into consideration, the distributional informa-
tion, bilingual alignment-based paraphrase probabil-
ities, and compression-centric features.
Table 2 shows comparison of the bilingual base-
line paraphrase grammar (PP), the reranked gram-
mars based on signatures extracted from the Google
n-grams (n-gram), the richer signatures drawn from
Annotated Gigaword (Syntax), and Clarke and La-
pata (2008)’s compression system (ILP). In both
cases, the inclusion of distributional similarity in-
formation results in significantly better output gram-
maticality and meaning retention. Despite its lower
coverage (12 versus 200 million phrases), the syn-
tactic distributional similarity outperforms the sim-
pler Google n-gram signatures.
</bodyText>
<subsectionHeader confidence="0.662935">
3.5 PPDB
</subsectionHeader>
<bodyText confidence="0.99995875">
To facilitate a more widespread use of paraphrases,
we release a collection of ranked paraphrases ob-
tained by the methods outlined in Sections 2 and 3
to the public (Ganitkevitch et al., 2013).
</bodyText>
<sectionHeader confidence="0.932552" genericHeader="method">
4 Paraphrasing with Natural Logic
</sectionHeader>
<bodyText confidence="0.999981461538462">
In the previously derived paraphrase grammar it is
assumed that all rules imply the semantic equiva-
lence of two textual expressions. The varying de-
grees of confidence our system has in this relation-
ship are evidenced by the paraphrase probabilities
and similarity scores. However, the grammar can
also contain rules that in fact represent a range of se-
mantic relationships, including hypernym- hyponym
relationships, such as India – this country.
To better model such cases we propose an anno-
tation of each paraphrase rule with explicit relation
labels based on natural logic. Natural logic (Mac-
Cartney, 2009) defines a set of pairwise relations be-
tween textual expressions, such as equivalence (≡),
forward (C) and backward (�) entailment, negation
∧) and others. These relations can be used to not
only detect semantic equivalence, but also infer en-
tailment. Our resulting system will be able to tackle
tasks like RTE, where the more a fine-grained reso-
lution of semantic relationships is crucial to perfor-
mance.
We favor a classification-based approach to this
problem: for each pair of paraphrases in the gram-
mar, we extract a feature vector that aims to capture
information about the semantic relationship in the
rule. Using a manually annotated development set
of paraphrases with relation labels, we train a clas-
sifier to discriminate between the different natural
logic relations.
We propose to leverage both labeled and unla-
beled data resources to extract useful features for
the classification. Annotated resources like Word-
Net can be used to derive a catalog of word and
phrase pairs with known entailment relationships,
for instance (India, country, C). Using word align-
ments between our paraphrase pairs, we can estab-
lish what portions of a pair have labels in WordNet
and retain corresponding features.
To leverage unlabeled data, we propose extending
our notion of distributional similarity. Previously,
we used cosine similarity to compare the signatures
of two phrases. However, cosine similarity is a sym-
metric measure, and it is unlikely to prove helpful
for determining the (asymmetric) entailment direc-
tionality of a paraphrase pair (i.e. whether it is a
hypo- or hypernym relation). We therefore propose
to extract a variety of asymmetric similarity fea-
tures from distributional contexts. Specifically, we
seek a measure that compares both the similarity and
the “breadth” of two vectors. Assuming that wider
breadth implies a hypernym, i.e. a C-entailment, the
scores produced by such a measure can be highly
</bodyText>
<figure confidence="0.99794252631579">
VP
PP
NP
det
amod
VBG IN TO DT
JJ NN
holding on to the long-term investment
lex-R-investment
pos-L-IN-TO
pos-L-TO
lex-L-on-to
lex-L-to
dep-det-R-NN dep-amod-R-NN
syn-gov-NP syn-miss-L-NN
~sig the long-term =
dep-det-R-investment
dep-amod-R-investment
pos-R-NN
</figure>
<page confidence="0.777143">
66
</page>
<table confidence="0.9979137">
Paraphrase rules Entailment classification
CD -+ twelve 12 twelve - 12
JJ -+ e editorial e A editorial
NNS -+ illustrations cartoons illustrations A cartoons
JJ insulting offensive insulting - / @ offensive
NP the prophet muhammad the prophet - muhammad
VB caused sparked caused A sparked
NP unrest riots unrest A riots
PP e in Denmark e A in Denmark
NP CD(-) NNS(A) CD(-) of the NNS(A) twelve illustrations A 12 of the cartoons
</table>
<figureCaption confidence="0.90606825">
Figure 5: Our system will use synchronous parsing and paraphrase grammars to perform natural language inference.
Each paraphrase transformation will be classified with a natural logic entailment relation. These will be joined bottom-
up, as illustrated by the last rule, where the join of the smaller constituents ≡ x A results in A for the larger phrase
pairs. This process will be propagated up the trees to determine if the hypothesis can be inferred from the premise.
</figureCaption>
<figure confidence="0.99891341025641">
twelve illustrations
insulting
the prophet
caused
unrest
riots in Denmark
were
sparked
by
12
of the
editorial cartoons
that were
offensive
to
muhammad
NP
PP
VB
CD
JJ
NNS JJ
NP
NP
NP
NP
NP
VP
S
S
NP
NP
NP
NP
VP
NP
VB
NP PP
CD JJ NNS JJ
</figure>
<bodyText confidence="0.999812833333333">
informative for our classification problem. Asym-
metric measures like Tversky indices (Tolias et al.,
2001) appear well-suited to the problem. We will
investigate application of Tversky indices to our dis-
tributional signatures and their usefulness for entail-
ment relation classification.
</bodyText>
<subsectionHeader confidence="0.955361">
4.1 Task-Based Evaluation
</subsectionHeader>
<bodyText confidence="0.99999355">
We propose evaluating the resulting system on tex-
tual entailment recognition. To do this, we cast the
RTE task as a synchronous parsing problem, as illus-
trated in Figure 5. We will extend the notion of syn-
chronous parsing towards resolving entailments, and
define and implement a compositional join operator
m to compute entailment relations over synchronous
derivations from the individual rule entailments.
While the assumption of a synchronous parse
structure is likely to be valid for translations and
paraphrases, we do not expect it to straightforwardly
hold for entailment recognition. We will thus in-
vestigate the limits of the synchronous assumption
over RTE data. Furthermore, to expand the sys-
tem’s coverage in a first step, we propose a simple
relaxation of the synchronousness requirement via
entailment-less “glue rules.” These rules, similar to
out-of-vocabulary rules in translation, will allow us
to include potentially unrelated or unrecognized por-
tions of the input into the synchronous parse.
</bodyText>
<sectionHeader confidence="0.999349" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99999280952381">
We have described an extension of the state of the art
in paraphrasing in a number of important ways: we
leverage large bilingual data sets to extract linguis-
tically expressive high-coverage paraphrases based
on an SCFG formalism. On an example text-to-
text generation task, sentence compression, we show
that an easily adapted paraphrase system achieves
state of the art meaning retention. Further, we in-
clude a complementary data source, monolingual
corpora, to augment the quality of the previously
obtained paraphrase grammar. The resulting sys-
tem is shown to perform significantly better than
the purely bilingual paraphrases, in both meaning
retention and grammaticality, achieving results on
par with the state of the art. Finally, we propose
an extension of SCFG-based paraphrasing towards
a more fine grained semantic representation using a
classification-based approach. In extending the syn-
chronous parsing methodology, we outline the ex-
pansion of the paraphraser towards a system capable
of tackling entailment recognition tasks.
</bodyText>
<page confidence="0.999287">
67
</page>
<sectionHeader confidence="0.999106" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999850916666667">
The ideas described in this paper were developed in
collaboration with Benjamin Van Durme and Chris
Callison-Burch. This material is based on research
sponsored by the NSF under grant IIS-1249516
and DARPA under agreement number FA8750-13-
2-0017 (the DEFT program). The U.S. Government
is authorized to reproduce and distribute reprints for
Governmental purposes. The views and conclusions
contained in this publication are those of the authors
and should not be interpreted as representing official
policies or endorsements of DARPA, the NSF, or the
U.S. Government.
</bodyText>
<sectionHeader confidence="0.999671" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999908950819673">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL/HLT.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1.
Peter Brown, John Cocke, Stephen Della Pietra, Vincent
Della Pietra, Frederick Jelinek, Robert Mercer, and
Paul Poossin. 1990. A statistical approach to language
translation. Computational Linguistics, 16(2), June.
Chris Callison-Burch. 2007. Paraphrasing and Trans-
lation. Ph.D. thesis, University of Edinburgh, Edin-
burgh, Scotland.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273–381.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2012. Monolingual distributional
similarity for text-to-text generation. In Proceedings
of *SEM. Association for Computational Linguistics.
Juri Ganitkevitch, Chris Callison-Burch, and Benjamin
Van Durme. 2013. Ppdb: The paraphrase database. In
Proceedings of HLT/NAACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of LREC.
Bill MacCartney. 2009. Natural language inference.
Ph.D. thesis, Stanford University.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341–388.
Courtney Napoles, Matt Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In Proceedings
of AKBC-WEKEX 2012.
Yannis A. Tolias, Stavros M. Panas, and Lefteri H.
Tsoukalas. 2001. Generalized fuzzy indices for simi-
larity matching. Fuzzy Sets and Systems, 120(2):255–
270.
</reference>
<page confidence="0.999441">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.274158">
<title confidence="0.999333">Large-Scale Paraphrasing for Natural Language Understanding</title>
<author confidence="0.901478">Juri</author>
<affiliation confidence="0.889167">Center for Language and Speech</affiliation>
<author confidence="0.478834">Johns Hopkins</author>
<email confidence="0.999814">juri@cs.jhu.edu</email>
<abstract confidence="0.989228956521739">We examine the application of data-driven paraphrasing to natural language understanding. We leverage bilingual parallel corpora to extract a large collection of syntactic paraphrase pairs, and introduce an adaptation scheme that allows us to tackle a variety of text transformation tasks via paraphrasing. We evaluate our system on the sentence compression task. Further, we use distributional similarity measures based on context vectors derived from large monolingual corpora to annotate our paraphrases with an orthogonal source of information. This yields significant improvements in our compression system’s output quality, achieving state-of-the-art performance. Finally, we propose a refinement of our paraphrases by classifying them into natural logic entailment relations. By extending the synchronous parsing paradigm towards these entailment relations, we will enable our system to perform recognition of textual entailment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling.</booktitle>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="6462" citStr="Aho and Ullman, 1972" startWordPosition="989" endWordPosition="993">ion of example paraphrase patterns extracted by our system. These rules demonstrate that, using the pivot approach from Figure 1, our system is capable of learning meaning-preserving syntactic transformations in English. the dative shift shown in Table 1 are good examples of this: the two noun-phrase arguments to the expressions are abstracted to nonterminals while each rule’s lexicalization provides an appropriate frame of evidence for the transform. 2.1 Formal Representation In this proposal we focus on a paraphrase model based on synchronous context-free grammar (SCFG). The SCFG formalism (Aho and Ullman, 1972) was repopularized for statistical machine translation by (Chiang, 2005). An probabilistic SCFG g contains rules r of the form r = C → (-y, α, ∼, w). A rule r’s left-hand side C is a nonterminal, while its right-hands sides -y and α can be mixed strings of words and nonterminal symbols. There is a one-to-one correspondency between the nonterminals in -y and α. Each rule is assigned a cost wr ≥ 0, reflecting its likelihood. To compute the cost wr of the application of a rule r, we define a set of feature functions c� = {c1...cN} that are combined in a log-linear model. The model weights are set</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4075" citStr="Bannard and Callison-Burch (2005)" startWordPosition="588" endWordPosition="591">xtraction of Syntactic Paraphrases from Bitexts A variety of different types of corpora have been used to automatically induce paraphrase collections for English (see Madnani and Dorr (2010) for a surProceedings of the NAACL HLT 2013 Student Research Workshop, pages 62–68, Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics Figure 1: An example of pivot-based phrasal paraphrase extraction – we assume English phrases that translate to a common German phrase to be paraphrases. Thus we extract “imprisoned” as a paraphrase of “thrown into jail.” vey of these methods). Bannard and Callison-Burch (2005) extracted phrasal paraphrases from bitext by using foreign language phrases as a pivot: if two English phrases e1 and e2 both translate to a foreign phrase f, they assume that e1 and e2 are paraphrases of one another. Figure 1 gives an example of a phrasal paraphrase extracted by Bannard and Callison-Burch (2005). Since “thrown into jail” is aligned to multiple German phrases, and since each of those German phrases align back to a variety of English phrases, the method extracts a wide range of possible paraphrases including good paraphrase like: imprisoned and thrown into prison. It also prod</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Large scale acquisition of paraphrases for learning surface patterns.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT.</booktitle>
<contexts>
<context position="11227" citStr="Bhagat and Ravichandran, 2008" startWordPosition="1747" endWordPosition="1750">ds a sentence compression system that performs on par with a state-of-the-art integer linear programming-based (ILP) compression system (Clarke and Lapata, 2008). As Table 2 shows, human evaluation results suggest that our system outperforms the contrast system in meaning retention. However, it suffers losses in grammaticality. Figure 2 shows an example derivation produced as a result of applying our paraphrase rules in the decoding process. 3 Integrating Monolingual Distributional Similarity into Bilingually Extracted Paraphrases Distributional similarity-based methods (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) rely NP VP NP NP DT+NNP CD NNS JJ DT NNP the prophet mohammad twelve of the cartoons that are offensive to cartoons insulting the prophet mohammad 12 CD NNS JJ DT NNP NP DT+NNP NP VP NP 64 on the assumption that similar expressions appear in similar contexts – a signal that is orthogonal to bilingual pivot information we have considered thus far. However, the monolingual distributional signal is noisy: it suffers from problems such as mistaking cousin expressions or antonyms (such as (rise, fall) or (boy, girl)) for paraphrases. We circumvent this issue by starting with a paraphrase grammar e</context>
</contexts>
<marker>Bhagat, Ravichandran, 2008</marker>
<rawString>Rahul Bhagat and Deepak Ravichandran. 2008. Large scale acquisition of paraphrases for learning surface patterns. In Proceedings of ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram version 1.</title>
<date>2006</date>
<contexts>
<context position="14616" citStr="Brants and Franz, 2006" startWordPosition="2298" endWordPosition="2301">aring unrelated pairs, we require the phrase pairs to be consistent with a token alignment a, defined and computed analogously to word alignments in machine translation. 3.3 Data Sets and Types of Distributional Signatures We investigate the impact of the data and feature set used to construct distributional signatures. In particular we contrast two approaches: a large collection of distributional signatures with a relatively simple feature set, and a much smaller set of signatures with a rich, syntactically informed feature set. The larger n-gram model is drawn from a webscale n-gram corpus (Brants and Franz, 2006; Lin et al., 2010). Figure 3 illustrates this feature extraction approach. The resulting collection comprises distributional signatures for the 200 million most frequent 1-to-4-grams in the n-gram corpus. For the syntactically informed model, we use the constituency and dependency parses provided in the Annotated Gigaword corpus (Napoles et al., 2012). Figure 4 illustrates this model’s feature extraction for an example phrase occurrence. Using this method we extract distributional signatures for over 12 million 1-to-4-gram phrases. 3.4 Evaluation For evaluation, we follow the task-based appro</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram version 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>John Cocke</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Frederick Jelinek</author>
<author>Robert Mercer</author>
<author>Paul Poossin</author>
</authors>
<title>A statistical approach to language translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="5089" citStr="Brown et al., 1990" startWordPosition="753" endWordPosition="756">of those German phrases align back to a variety of English phrases, the method extracts a wide range of possible paraphrases including good paraphrase like: imprisoned and thrown into prison. It also produces less good paraphrases like: in jail and put in prison for, and bad paraphrases, such as maltreated and protection, because of noisy/inaccurate word alignments and other problems. To rank these, Bannard and Callison-Burch (2005) derive a paraphrase probability p(e1|e2): �p(e2|e1) ≈ p(e2|f)p(f|e1), (1) f where the p(ei|f) and p(f|ei) are translation probabilities estimated from the bitext (Brown et al., 1990; Koehn et al., 2003). We extend this method to extract syntactic paraphrases (Ganitkevitch et al., 2011). Table 1 shows example paraphrases produced by our system. While phrasal systems memorize phrase pairs without any further generalization, a syntactic paraphrasing system can learn more generic patterns. These can be better applied to unseen data. The paraphrases implementing the possessive rule and Possessive rule NP the NN of the NNP the NNP’s NN NP the NP made by NN the NN’s NP Dative shift VP give NN to NP give NP the NN VP provide NP1 to NP2 give NP2 NP1 Partitive constructions NP CD </context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Mercer, Poossin, 1990</marker>
<rawString>Peter Brown, John Cocke, Stephen Della Pietra, Vincent Della Pietra, Frederick Jelinek, Robert Mercer, and Paul Poossin. 1990. A statistical approach to language translation. Computational Linguistics, 16(2), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing and Translation.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh,</institution>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="1678" citStr="Callison-Burch (2007)" startWordPosition="240" endWordPosition="241">tem to perform recognition of textual entailment. 1 Introduction In this work, we propose an extension of current paraphrasing methods to tackle natural language understanding problems. We create a large set of paraphrase pairs in a data-driven fashion, rank them based on a variety of similarity metrics, and attach an entailment relation to each pair, facilitating natural logic inference. The resulting resource has potential applications to a variety of NLP applications, including summarization, query expansion, question answering, and recognizing textual entailment. Specifically, we build on Callison-Burch (2007)’s pivot-based paraphrase extraction method, which uses bilingual parallel data to learn English phrase pairs that share the same meaning. Our approach extends the pivot method to learn meaning-preserving 62 syntactic transformations in English. We represent these using synchronous context-free grammars (SCFGs). This representation allows us to re-use a lot of machine translation machinery to perform monolingual text-to-text generation. We demonstrate the method on a sentence compression task (Ganitkevitch et al., 2011). To improve the system, we then incorporate features based on monolingual </context>
</contexts>
<marker>Callison-Burch, 2007</marker>
<rawString>Chris Callison-Burch. 2007. Paraphrasing and Translation. Ph.D. thesis, University of Edinburgh, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Syntactic constraints on paraphrases extracted from parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="8737" citStr="Callison-Burch, 2008" startWordPosition="1372" endWordPosition="1373"> pair is turned into an SCFG rule by assigning a lefthand side nonterminal symbol, corresponding to the syntactic constituent that dominates the English phrase. To introduce nonterminals into the righthand sides of the rule, we can replace corresponding sub-phrases in the English and foreign phrases with nonterminal symbols. Doing this for all sentence pairs in a bilingual parallel corpus results in a translation grammar that serves as the basis for syntactic machine translation. To create a paraphrase grammar from a translation grammar, we extend the syntactically informed pivot approach of (Callison-Burch, 2008) to the SCFG model: for each pair of translation rules r1 and r2 with matching left-hand side nonterminal C and foreign language right-hand side -y: r1 = C –+ (-y, α1, —1, �61) and r2 = C –+ (-y, α2, —2, 02), we pivot over -y and create a paraphrase rule rp: rp = C –+ (α1, α2, —, –ϕ). We estimate the cost for rp following Equation 1. 2.3 Task-Based Evaluation Sharing its SCFG formalism permits us to re-use much of SMT’s machinery for paraphrasing applications, including decoding and minimum error rate training. This allows us to easily tackle a variety of monolingual text-to-text generation ta</context>
</contexts>
<marker>Callison-Burch, 2008</marker>
<rawString>Chris Callison-Burch. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6534" citStr="Chiang, 2005" startWordPosition="1002" endWordPosition="1003">ate that, using the pivot approach from Figure 1, our system is capable of learning meaning-preserving syntactic transformations in English. the dative shift shown in Table 1 are good examples of this: the two noun-phrase arguments to the expressions are abstracted to nonterminals while each rule’s lexicalization provides an appropriate frame of evidence for the transform. 2.1 Formal Representation In this proposal we focus on a paraphrase model based on synchronous context-free grammar (SCFG). The SCFG formalism (Aho and Ullman, 1972) was repopularized for statistical machine translation by (Chiang, 2005). An probabilistic SCFG g contains rules r of the form r = C → (-y, α, ∼, w). A rule r’s left-hand side C is a nonterminal, while its right-hands sides -y and α can be mixed strings of words and nonterminal symbols. There is a one-to-one correspondency between the nonterminals in -y and α. Each rule is assigned a cost wr ≥ 0, reflecting its likelihood. To compute the cost wr of the application of a rule r, we define a set of feature functions c� = {c1...cN} that are combined in a log-linear model. The model weights are set to maximize a taskdependent objective function. 2.2 Syntactic Paraphras</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>31--273</pages>
<contexts>
<context position="10758" citStr="Clarke and Lapata, 2008" startWordPosition="1683" endWordPosition="1686">troduces a verbosity penalty. • Development data that represents the precise transformations we seek to model. We use a set of human-made example compressions mined from translation references. • Optionally, grammar augmentations that allow for the incorporation of effects that the learned paraphrase grammar cannot capture. We experimented with automatically generated deletion rules. Applying the above adaptations to our generic paraphraser (PP), quickly yields a sentence compression system that performs on par with a state-of-the-art integer linear programming-based (ILP) compression system (Clarke and Lapata, 2008). As Table 2 shows, human evaluation results suggest that our system outperforms the contrast system in meaning retention. However, it suffers losses in grammaticality. Figure 2 shows an example derivation produced as a result of applying our paraphrase rules in the decoding process. 3 Integrating Monolingual Distributional Similarity into Bilingually Extracted Paraphrases Distributional similarity-based methods (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) rely NP VP NP NP DT+NNP CD NNS JJ DT NNP the prophet mohammad twelve of the cartoons that are offensive to cartoons insulting the </context>
<context position="16426" citStr="Clarke and Lapata (2008)" startWordPosition="2562" endWordPosition="2566">ression. The distributional similarity scores are incorporated into the paraphrasing system as additional rule features into the log-linear model. The task-targeted parameter tuning thus results in a reranking of the rules that takes into consideration, the distributional information, bilingual alignment-based paraphrase probabilities, and compression-centric features. Table 2 shows comparison of the bilingual baseline paraphrase grammar (PP), the reranked grammars based on signatures extracted from the Google n-grams (n-gram), the richer signatures drawn from Annotated Gigaword (Syntax), and Clarke and Lapata (2008)’s compression system (ILP). In both cases, the inclusion of distributional similarity information results in significantly better output grammaticality and meaning retention. Despite its lower coverage (12 versus 200 million phrases), the syntactic distributional similarity outperforms the simpler Google n-gram signatures. 3.5 PPDB To facilitate a more widespread use of paraphrases, we release a collection of ranked paraphrases obtained by the methods outlined in Sections 2 and 3 to the public (Ganitkevitch et al., 2013). 4 Paraphrasing with Natural Logic In the previously derived paraphrase </context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Journal of Artificial Intelligence Research, 31:273–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Chris Callison-Burch</author>
<author>Courtney Napoles</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Ganitkevitch, Callison-Burch, Napoles, Van Durme, 2011</marker>
<rawString>Juri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van Durme. 2011. Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Monolingual distributional similarity for text-to-text generation.</title>
<date>2012</date>
<booktitle>In Proceedings of *SEM. Association for Computational Linguistics.</booktitle>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2012</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2012. Monolingual distributional similarity for text-to-text generation. In Proceedings of *SEM. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Chris Callison-Burch</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Ppdb: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<marker>Ganitkevitch, Callison-Burch, Van Durme, 2013</marker>
<rawString>Juri Ganitkevitch, Chris Callison-Burch, and Benjamin Van Durme. 2013. Ppdb: The paraphrase database. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="5110" citStr="Koehn et al., 2003" startWordPosition="757" endWordPosition="760">ses align back to a variety of English phrases, the method extracts a wide range of possible paraphrases including good paraphrase like: imprisoned and thrown into prison. It also produces less good paraphrases like: in jail and put in prison for, and bad paraphrases, such as maltreated and protection, because of noisy/inaccurate word alignments and other problems. To rank these, Bannard and Callison-Burch (2005) derive a paraphrase probability p(e1|e2): �p(e2|e1) ≈ p(e2|f)p(f|e1), (1) f where the p(ei|f) and p(f|ei) are translation probabilities estimated from the bitext (Brown et al., 1990; Koehn et al., 2003). We extend this method to extract syntactic paraphrases (Ganitkevitch et al., 2011). Table 1 shows example paraphrases produced by our system. While phrasal systems memorize phrase pairs without any further generalization, a syntactic paraphrasing system can learn more generic patterns. These can be better applied to unseen data. The paraphrases implementing the possessive rule and Possessive rule NP the NN of the NNP the NNP’s NN NP the NP made by NN the NN’s NP Dative shift VP give NN to NP give NP the NN VP provide NP1 to NP2 give NP2 NP1 Partitive constructions NP CD of the NN CD NN NP al</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules from text. Natural Language Engineering.</title>
<date>2001</date>
<contexts>
<context position="11195" citStr="Lin and Pantel, 2001" startWordPosition="1743" endWordPosition="1746">ser (PP), quickly yields a sentence compression system that performs on par with a state-of-the-art integer linear programming-based (ILP) compression system (Clarke and Lapata, 2008). As Table 2 shows, human evaluation results suggest that our system outperforms the contrast system in meaning retention. However, it suffers losses in grammaticality. Figure 2 shows an example derivation produced as a result of applying our paraphrase rules in the decoding process. 3 Integrating Monolingual Distributional Similarity into Bilingually Extracted Paraphrases Distributional similarity-based methods (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) rely NP VP NP NP DT+NNP CD NNS JJ DT NNP the prophet mohammad twelve of the cartoons that are offensive to cartoons insulting the prophet mohammad 12 CD NNS JJ DT NNP NP DT+NNP NP VP NP 64 on the assumption that similar expressions appear in similar contexts – a signal that is orthogonal to bilingual pivot information we have considered thus far. However, the monolingual distributional signal is noisy: it suffers from problems such as mistaking cousin expressions or antonyms (such as (rise, fall) or (boy, girl)) for paraphrases. We circumvent this issue by star</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules from text. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Kenneth Church</author>
<author>Heng Ji</author>
<author>Satoshi Sekine</author>
<author>David Yarowsky</author>
<author>Shane Bergsma</author>
</authors>
<title>New tools for web-scale n-grams.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<institution>Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant</institution>
<contexts>
<context position="14635" citStr="Lin et al., 2010" startWordPosition="2302" endWordPosition="2305">e require the phrase pairs to be consistent with a token alignment a, defined and computed analogously to word alignments in machine translation. 3.3 Data Sets and Types of Distributional Signatures We investigate the impact of the data and feature set used to construct distributional signatures. In particular we contrast two approaches: a large collection of distributional signatures with a relatively simple feature set, and a much smaller set of signatures with a rich, syntactically informed feature set. The larger n-gram model is drawn from a webscale n-gram corpus (Brants and Franz, 2006; Lin et al., 2010). Figure 3 illustrates this feature extraction approach. The resulting collection comprises distributional signatures for the 200 million most frequent 1-to-4-grams in the n-gram corpus. For the syntactically informed model, we use the constituency and dependency parses provided in the Annotated Gigaword corpus (Napoles et al., 2012). Figure 4 illustrates this model’s feature extraction for an example phrase occurrence. Using this method we extract distributional signatures for over 12 million 1-to-4-gram phrases. 3.4 Evaluation For evaluation, we follow the task-based approach taken in Sectio</context>
</contexts>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, 2010</marker>
<rawString>Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale. 2010. New tools for web-scale n-grams. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
</authors>
<title>Natural language inference.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="2816" citStr="MacCartney, 2009" startWordPosition="402" endWordPosition="403">011). To improve the system, we then incorporate features based on monolingual distributional similarity. This orthogonal source of signal allows us to re-scores the bilingually-extracted paraphrases using information drawn from large monolingual corpora. We show that the monolingual distributional scores yield significant improvements over a baseline that scores paraphrases only with bilinguallyextracted features (Ganitkevitch et al., 2012). Further, we propose a semantics for paraphrasing by classifying each paraphrase pair with one of the entailment relation types defined by natural logic (MacCartney, 2009). Natural logic is used to perform inference over pairs of natural language phrases, like our paraphrase pairs. It defines a set of relations including, equivalence (≡), forward- and backward-entailments (C, antonyms (∧), and others. We will build a classifier for our paraphrases that uses features extracted from annotated resources like WordNet and distributional information gathered over large text corpora to assign one or more entailment relations to each paraphrase pair. We will evaluate the entailment assignments by applying this enhanced paraphrasing system to the task of recognizing tex</context>
<context position="17598" citStr="MacCartney, 2009" startWordPosition="2745" endWordPosition="2747">l Logic In the previously derived paraphrase grammar it is assumed that all rules imply the semantic equivalence of two textual expressions. The varying degrees of confidence our system has in this relationship are evidenced by the paraphrase probabilities and similarity scores. However, the grammar can also contain rules that in fact represent a range of semantic relationships, including hypernym- hyponym relationships, such as India – this country. To better model such cases we propose an annotation of each paraphrase rule with explicit relation labels based on natural logic. Natural logic (MacCartney, 2009) defines a set of pairwise relations between textual expressions, such as equivalence (≡), forward (C) and backward (�) entailment, negation ∧) and others. These relations can be used to not only detect semantic equivalence, but also infer entailment. Our resulting system will be able to tackle tasks like RTE, where the more a fine-grained resolution of semantic relationships is crucial to performance. We favor a classification-based approach to this problem: for each pair of paraphrases in the grammar, we extract a feature vector that aims to capture information about the semantic relationshi</context>
</contexts>
<marker>MacCartney, 2009</marker>
<rawString>Bill MacCartney. 2009. Natural language inference. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of data-driven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="3632" citStr="Madnani and Dorr (2010)" startWordPosition="521" endWordPosition="524">ntailments (C, antonyms (∧), and others. We will build a classifier for our paraphrases that uses features extracted from annotated resources like WordNet and distributional information gathered over large text corpora to assign one or more entailment relations to each paraphrase pair. We will evaluate the entailment assignments by applying this enhanced paraphrasing system to the task of recognizing textual entailment (RTE). 2 Extraction of Syntactic Paraphrases from Bitexts A variety of different types of corpora have been used to automatically induce paraphrase collections for English (see Madnani and Dorr (2010) for a surProceedings of the NAACL HLT 2013 Student Research Workshop, pages 62–68, Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics Figure 1: An example of pivot-based phrasal paraphrase extraction – we assume English phrases that translate to a common German phrase to be paraphrases. Thus we extract “imprisoned” as a paraphrase of “thrown into jail.” vey of these methods). Bannard and Callison-Burch (2005) extracted phrasal paraphrases from bitext by using foreign language phrases as a pivot: if two English phrases e1 and e2 both translate to a foreign phrase </context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics, 36(3):341–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Matt Gormley</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Annotated gigaword.</title>
<date>2012</date>
<booktitle>In Proceedings of AKBC-WEKEX</booktitle>
<marker>Napoles, Gormley, Van Durme, 2012</marker>
<rawString>Courtney Napoles, Matt Gormley, and Benjamin Van Durme. 2012. Annotated gigaword. In Proceedings of AKBC-WEKEX 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannis A Tolias</author>
<author>Stavros M Panas</author>
<author>Lefteri H Tsoukalas</author>
</authors>
<title>Generalized fuzzy indices for similarity matching. Fuzzy Sets and Systems,</title>
<date>2001</date>
<volume>120</volume>
<issue>2</issue>
<pages>270</pages>
<contexts>
<context position="21024" citStr="Tolias et al., 2001" startWordPosition="3299" endWordPosition="3302">ent relation. These will be joined bottomup, as illustrated by the last rule, where the join of the smaller constituents ≡ x A results in A for the larger phrase pairs. This process will be propagated up the trees to determine if the hypothesis can be inferred from the premise. twelve illustrations insulting the prophet caused unrest riots in Denmark were sparked by 12 of the editorial cartoons that were offensive to muhammad NP PP VB CD JJ NNS JJ NP NP NP NP NP VP S S NP NP NP NP VP NP VB NP PP CD JJ NNS JJ informative for our classification problem. Asymmetric measures like Tversky indices (Tolias et al., 2001) appear well-suited to the problem. We will investigate application of Tversky indices to our distributional signatures and their usefulness for entailment relation classification. 4.1 Task-Based Evaluation We propose evaluating the resulting system on textual entailment recognition. To do this, we cast the RTE task as a synchronous parsing problem, as illustrated in Figure 5. We will extend the notion of synchronous parsing towards resolving entailments, and define and implement a compositional join operator m to compute entailment relations over synchronous derivations from the individual ru</context>
</contexts>
<marker>Tolias, Panas, Tsoukalas, 2001</marker>
<rawString>Yannis A. Tolias, Stavros M. Panas, and Lefteri H. Tsoukalas. 2001. Generalized fuzzy indices for similarity matching. Fuzzy Sets and Systems, 120(2):255– 270.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>