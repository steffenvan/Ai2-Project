<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004218">
<title confidence="0.9898425">
Argumentative Feedback: A Linguistically-motivated Term
Expansion for Information Retrieval
</title>
<author confidence="0.99692">
Patrick Ruch, Imad Tbahriti, Julien Gobeill Alan R. Aronson
</author>
<affiliation confidence="0.9783455">
Medical Informatics Service Lister Hill Center
University of Geneva National Library of Medicine
</affiliation>
<address confidence="0.964719">
24 Micheli du Crest 8600 Rockville Pike
1201 Geneva Bethesda, MD 20894
Switzerland USA
</address>
<email confidence="0.995191">
{patrick.ruch,julien.gobeill,imad.tbahriti}@hcuge.ch alan@nlm.nih.gov
</email>
<sectionHeader confidence="0.979537" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999840933333333">
We report on the development of a new au-
tomatic feedback model to improve informa-
tion retrieval in digital libraries. Our hy-
pothesis is that some particular sentences,
selected based on argumentative criteria,
can be more useful than others to perform
well-known feedback information retrieval
tasks. The argumentative model we ex-
plore is based on four disjunct classes, which
has been very regularly observed in scien-
tific reports: PURPOSE, METHODS, RE-
SULTS, CONCLUSION. To test this hy-
pothesis, we use the Rocchio algorithm as
baseline. While Rocchio selects the fea-
tures to be added to the original query
based on statistical evidence, we propose
to base our feature selection also on argu-
mentative criteria. Thus, we restrict the ex-
pansion on features appearing only in sen-
tences classified into one of our argumen-
tative categories. Our results, obtained on
the OHSUMED collection, show a signifi-
cant improvement when expansion is based
on PURPOSE (mean average precision =
+23%) and CONCLUSION (mean average
precision = +41%) contents rather than on
other argumentative contents. These results
suggest that argumentation is an important
linguistic dimension that could benefit in-
formation retrieval.
</bodyText>
<sectionHeader confidence="0.997331" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960018181818">
Information retrieval (IR) is a challenging en-
deavor due to problems caused by the underly-
ing expressiveness of all natural languages. One
of these problems, synonymy, is that authors
and users frequently employ different words or
expressions to refer to the same meaning (acci-
dent may be expressed as event, incident, prob-
lem, difficulty, unfortunate situation, the subject
of your last letter, what happened last week, etc.)
(Furnas et al., 1987). Another problem is ambi-
guity, where a specific term may have several
(and sometimes contradictory) meanings and
interpretations (e.g., the word horse as in Tro-
jan horse, light horse, to work like a horse, horse
about). In order to obtain better meaning-based
matches between queries and documents, vari-
ous propositions have been suggested, usually
without giving any consideration to the under-
lying domain.
During our participation in different interna-
tional evaluation campaigns such as the TREC
Genomics track (Hersh, 2005), the BioCreative
initiative (Hirschman et al., 2005), as well as
in our attempts to deliver advanced search
tools for biologists (Ruch, 2006) and health-
care providers (Ruch, 2002) (Ruch, 2004), we
were more concerned with domain-specific in-
formation retrieval in which systems must re-
turn a ranked list of MEDLINE records in re-
sponse to an expert’s information request. This
involved a set of available queries describing
typical search interests, in which gene, pro-
tein names, and diseases were often essential
for an effective retrieval. Biomedical publica-
tions however tend to generate new informa-
tion very rapidly and also use a wide varia-
tion in terminology, thus leading to the cur-
rent situation whereby a large number of names,
symbols and synonyms are used to denote the
same concepts. Current solutions to these issues
can be classified into domain-specific strate-
gies, such as thesaurus-based expansion, and
domain-independent strategies, such as blind-
feedback. By proposing to explore a third type
of approach, which attempts to take advan-
tage of argumentative specificities of scientific
reports, our study initiates a new research di-
rection for natural language processing applied
to information retrieval.
The rest of this paper is organized as follows.
Section 2 presents some related work in infor-
mation retrieval and in argumentative parsing,
while Section 3 depicts the main characteristics
of our test collection and the metrics used in
our experiments. Section 4 details the strategy
</bodyText>
<page confidence="0.98666">
675
</page>
<note confidence="0.724838">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 675–682,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.99878725">
used to develop our improved feedback method.
Section 5 reports on results obtained by varying
our model and Section 6 contains conclusions on
our experiments.
</bodyText>
<sectionHeader confidence="0.979298" genericHeader="introduction">
2 Related works
</sectionHeader>
<bodyText confidence="0.99977625">
Our basic experimental hypothesis is that some
particular sentences, selected based on argu-
mentative categories, can be more useful than
others to support well-known feedback informa-
tion retrieval tasks. It means that selecting sen-
tences based on argumentative categories can
help focusing on content-bearing sections of sci-
entific articles.
</bodyText>
<subsectionHeader confidence="0.970286">
2.1 Argumentation
</subsectionHeader>
<bodyText confidence="0.999976028571428">
Originally inspired by corpus linguistics studies
(Orasan, 2001), which suggests that scientific
reports (in chemistry, linguistics, computer sci-
ences, medicine...) exhibit a very regular logi-
cal distribution -confirmed by studies conducted
on biomedical corpora (Swales, 1990) and by
ANSI/ISO professional standards - the argu-
mentative model we experiment is based on four
disjunct classes: PURPOSE, METHODS, RE-
SULTS, CONCLUSION.
Argumentation belongs to discourse analy-
sisi, with fairly complex computational mod-
els such as the implementation of the rhetori-
cal structure theory proposed by (Marcu, 1997),
which proposes dozens of rhetorical classes.
More recent advances were applied to docu-
ment summarization. Of particular interest for
our approach, Teufel and Moens (Teufel and
Moens, 1999) propose using a list of manually
crafted triggers (using both words and expres-
sions such as we argued, in this article, the
paper is an attempt to, we aim at, etc.) to
automatically structure scientific articles into
a lighter model, with only seven categories:
BACKGROUND, TOPIC, RELATED WORK,
PURPOSE, METHOD, RESULT, and CON-
CLUSION.
More recently and for knowledge discovery in
molecular biology, more elaborated models were
proposed by (Mizuta and Collier, 2004) (Mizuta
et al., 2005) and by (Lisacek et al., 2005) for
novelty-detection. (McKnight and Srinivasan,
2003) propose a model very similar to our four-
class model but is inspired by clinical trials.
Preliminary applications were proposed for bib-
</bodyText>
<footnote confidence="0.524894666666667">
&apos;After Aristotle, discourses structured following an
appropriate argumentative distribution belong to logics,
while ill-defined ones belong to rhetorics.
</footnote>
<bodyText confidence="0.998982625">
liometrics and related-article search (Tbahriti
et al., 2004) (Tbahriti et al., 2005), informa-
tion extraction and passage retrieval (Ruch et
al., 2005b). In these studies, sentences were se-
lected as the basic classification unit in order
to avoid as far as possible co-reference issues
(Hirst, 1981), which hinder readibity of auto-
matically generated and extracted sentences.
</bodyText>
<subsectionHeader confidence="0.998385">
2.2 Q uery expansion
</subsectionHeader>
<bodyText confidence="0.999873022727273">
Various query expansion techniques have been
suggested to provide a better match between
user information needs and documents, and to
increase retrieval effectiveness. The general
principle is to expand the query using words
or phrases having a similar or related meaning
to those appearing in the original request. Vari-
ous empirical studies based on different IR mod-
els or collections have shown that this type of
search strategy should usually be effective in en-
hancing retrieval performance. Scheme propo-
sitions such as this should consider the various
relationships between words as well as term se-
lection mechanisms and term weighting schemes
(Robertson, 1990). The specific answers found
to these questions may vary; thus a variety
of query expansion approaches were suggested
(Efthimiadis, 1996).
In a first attempt to find related search terms,
we might ask the user to select additional terms
to be included in a new query, e.g. (Velez et
al., 1997). This could be handled interactively
through displaying a ranked list of retrieved
items returned by the first query. Voorhees
(Voorhees, 1994) proposed basing a scheme
based on the WordNet thesaurus. The au-
thor demonstrated that terms having a lexical-
semantic relation with the original query words
(extracted from a synonym relationship) pro-
vided very little improvement (around 1% when
compared to the original unexpanded query).
As a second strategy for expanding the orig-
inal query, Rocchio (Rocchio, 1971) proposed
accounting for the relevance or irrelevance of
top-ranked documents, according to the user’s
manual input. In this case, a new query was
automatically built in the form of a linear com-
bination of the term included in the previous
query and terms automatically extracted from
both the relevant documents (with a positive
weight) and non-relevant items (with a nega-
tive weight). Empirical studies (e.g., (Salton
and Buckley, 1990)) demonstrated that such an
approach is usually quite effective, and could
</bodyText>
<page confidence="0.996106">
676
</page>
<bodyText confidence="0.999908605633803">
be used more than once per query (Aalbers-
berg, 1992). Buckley et al. (Singhal et al.,
1996b) suggested that we could assume, with-
out even looking at them or asking the user, that
the top k ranked documents are relevant. De-
noted the pseudo-relevance feedback or blind-
query expansion approach, this approach is usu-
ally effective, at least when handling relatively
large text collections.
As a third source, we might use large text
corpora to derive various term-term relation-
ships, using statistically or information-based
measures (Jones, 1971), (Manning and Schutze,
2000). For example, (Qiu and Frei, 1993)
suggested that terms to be added to a new
query could be extracted from a similarity the-
saurus automatically built through calculating
co-occurrence frequencies in the search collec-
tion. The underlying effect was to add idiosyn-
cratic terms to the underlying document col-
lection, related to the query terms by language
use. When using such query expansion ap-
proaches, we can assume that the new terms are
more appropriate for the retrieval of pertinent
items than are lexically or semantically related
terms provided by a general thesaurus or dic-
tionary. To complement this global document
analysis, (Croft, 1998) suggested that text pas-
sages (with a text window size of between 100
to 300 words) be taken into account. This local
document analysis seemed to be more effective
than a global term relationship generation.
As a forth source of additional terms, we
might account for specific user information
needs and/or the underlying domain. In this
vein, (Liu and Chu, 2005) suggested that terms
related to the user’s intention or scenario might
be included. In the medical domain, it was ob-
served that users looking for information usu-
ally have an underlying scenario in mind (or
a typical medical task). Knowing that the
number of scenarios for a user is rather lim-
ited (e.g., diagnosis, treatment, etiology), the
authors suggested automatically building a se-
mantic network based on a domain-specific the-
saurus (using the Unified Medical Language
System (UMLS) in this case). The effective-
ness of this strategy would of course depend
on the quality and completeness of domain-
specific knowledge sources. Using the well-
known term frequency (tf)/inverse document
frequency (idf) retrieval model, the domain-
specific query-expansion scheme suggested by
Liu and Chu (2005) produces better retrieval
performance than a scheme based on statis-
tics (MAP: 0.408 without query expansion,
0.433 using statistical methods and 0.452 with
domain-specific approaches).
In these different query expansion ap-
proaches, various underlying parameters must
be specified, and generally there is no sin-
gle theory able to help us find the most ap-
propriate values. Recent empirical studies
conducted in the context of the TREC Ge-
nomics track, using the OHSUGEN collection
(Hersh, 2005), show that neither blind expan-
sion (Rocchio), nor domain-specific query ex-
pansion (thesaurus-based Gene and Protein ex-
pansion) seem appropriate to improve retrieval
effectiveness (Aronson et al., 2006) (Abdou et
al., 2006).
</bodyText>
<sectionHeader confidence="0.919625" genericHeader="method">
3 Data and metrics
</sectionHeader>
<bodyText confidence="0.999983142857143">
To test our hypothesis, we used the OHSUMED
collection (Hersh et al., 1994), originally devel-
oped for the TREC topic detection track, which
is the most popular information retrieval collec-
tion for evaluating information search in library
corpora. Alternative collections (cf. (Savoy,
2005)), such as the French Amaryllis collection,
are usually smaller and/or not appropriate to
evaluate our argumentative classifier, which can
only process English documents. Other MED-
LINE collections, which can be regarded as sim-
ilar in size or larger, such as the TREC Ge-
nomics 2004 and 2005 collections are unfortu-
nately more domain-specific since information
requests in these collection are usually target-
ing a particular gene or gene product.
Among the 348,566 MEDLINE citations of
the OHSUMED collection, we use the 233,455
records provided with an abstract. An exam-
ple of a MEDLINE citation is given in Table 1:
only Title, Abstract, MeSH and Chemical (RN)
fields of MEDLINE records were used for index-
ing. Out of the 105 queries of the OHSUMED
collection, only 101 queries have at least one
positive relevance judgement, therefore we used
only this subset for our experiments. The sub-
set has been randomly split into a training set
(75 queries), which is used to select the different
parameters of our retrieval model, and a test set
(26 queries), used for our final evaluation.
As usual in information retrieval evaluations,
the mean average precision, which computes the
precision of the engine at different levels (0%,
10%, 20%... 100%) of recall, will be used in our
experiments. The precision of the top returned
</bodyText>
<page confidence="0.993826">
677
</page>
<bodyText confidence="0.999659692307692">
Title: Computerized extraction of coded find-
ings from free-text radiologic reports. Work in
progress.
Abstract: A computerized data acquisition
tool, the special purpose radiology understand-
ing system (SPRUS), has been implemented as
a module in the Health Evaluation through Log-
ical Processing Hospital Information System.
This tool uses semantic information from a di-
agnostic expert system to parse free-text radi-
ology reports and to extract and encode both
the findings and the radiologists’ interpreta-
tions. These coded findings and interpretations
are then stored in a clinical data base. The sys-
tem recognizes both radiologic findings and di-
agnostic interpretations. Initial tests showed a
true-positive rate of 87% for radiographic find-
ings and a bad data rate of 5%. Diagnostic in-
terpretations are recognized at a rate of 95%
with a bad data rate of 6%. Testing suggests
that these rates can be improved through en-
hancements to the system’s thesaurus and the
computerized medical knowledge that drives it.
This system holds promise as a tool to obtain
coded radiologic data for research, medical au-
dit, and patient care.
</bodyText>
<listItem confidence="0.580691">
MeSH Terms: Artificial Intelligence*; Deci-
sion Support Techniques; Diagnosis, Computer-
Assisted; Documentation; Expert Systems; Hos-
pital Information Systems*; Human; Natural
Language Processing*; Online Systems; Radi-
ology Information Systems*.
</listItem>
<tableCaption confidence="0.84215975">
Table 1: MEDLINE records with, title, abstract
and keyword fields as provided by MEDLINE
librarians: major concepts are marked with *;
Subheadings and checktags are removed.
</tableCaption>
<bodyText confidence="0.99937325">
document, which is obviously of major impor-
tance is also provided together with the total
number of relevant retrieved documents for each
evaluated run.
</bodyText>
<sectionHeader confidence="0.998466" genericHeader="method">
4 Methods
</sectionHeader>
<bodyText confidence="0.999963785714286">
To test our experimental hypothesis, we use the
Rocchio algorithm as baseline. In addition, we
also provide the score obtained by the engine
before the feedback step. This measure is nec-
essary to verify that feedback is useful for query-
ing the OHSUMED collection and to establish a
strong baseline. While Rocchio selects the fea-
tures to be added to the original queries based
on pure statistical analysis, we propose to base
our feature expansion also on argumentative cri-
teria. That is, we overweight features appear-
ing in sentences classified in a particular argu-
mentative category by the argumentative cate-
gorizer.
</bodyText>
<subsectionHeader confidence="0.995153">
4.1 Retrieval engine and indexing units
</subsectionHeader>
<bodyText confidence="0.999908214285714">
The easyIR system is a standard vector-space
engine (Ruch, 2004), which computes state-
of-the-art tf.idf and probabilistic weighting
schema. All experiments were conducted with
pivoted normalization (Singhal et al., 1996a),
which has recently shown some effectiveness
on MEDLINE corpora (Aronson et al., 2006).
Query and document weighings are provided in
Equation (1): the dtu formula is applied to the
documents, while the dtn formula is applied to
the query; t the number of indexing terms, dfj
the number of documents in which the term tj;
pivot and slope are constants (fixed at pivot =
0.14, slope = 146).
</bodyText>
<subsubsectionHeader confidence="0.660668">
(Ln(Ln(t fij)+1)+1) ·idfj
</subsubsectionHeader>
<bodyText confidence="0.985506722222222">
dtu: wij = (1−slope)·pivot+slope·nti
dtn: wij = idfj · (Ln(Ln(tfif) + 1) + 1)
As already observed in several linguistically-
motivated studies (Hull, 1996), we observe that
common stemming methods do not perform well
on MEDLINE collections (Abdou et al., 2006),
therefore indexing units are stored in the in-
verted file using a simple S-stemmer (Harman,
1991), which basically handles most frequent
plural forms and exceptions of the English lan-
guage such as -ies, -es and -s and exclude end-
ings such as -aies, -eies, -ss, etc. This simple
normalization procedure performs better than
others and better than no stemming. We also
use a slightly modified standard stopword list of
544 items, where strings such as a, which stands
for alpha in chemistry and is relevant in biomed-
ical expressions such as vitamin a.
</bodyText>
<subsectionHeader confidence="0.995451">
4.2 Argumentative categorizer
</subsectionHeader>
<bodyText confidence="0.999948307692308">
The argumentative classifier ranks and catego-
rizes abstract sentences as to their argumenta-
tive classes. To implement our argumentative
categorizer, we rely on four binary Bayesian
classifiers, which use lexical features, and a
Markov model, which models the logical distri-
bution of the argumentative classes in MED-
LINE abstracts. A comprehensive description
of the classifier with feature selection and com-
parative evaluation can be found in (Ruch et
al., 2005a)
To train the classifier, we obtained 19,555 ex-
plicitly structured abstracts from MEDLINE. A
</bodyText>
<equation confidence="0.901296">
(1)
</equation>
<page confidence="0.985056">
678
</page>
<bodyText confidence="0.901666086956522">
Abstract: PURPOSE: The overall prognosis
for patients with congestive heart failure is poor.
Defining specific populations that might demon-
strate improved survival has been difficult [...]
PATIENTS AND METHODS: We identified 11
patients with severe congestive heart failure (av-
erage ejection fraction 21.9 +/- 4.23% (+/- SD)
who developed spontaneous, marked improve-
ment over a period of follow-up lasting 4.25 +/-
1.49 years [...] RESULTS: During the follow-up
period, the average ejection fraction improved
in 11 patients from 21.9 +/- 4.23% to 56.64
+/- 10.22%. Late follow-up indicates an aver-
age ejection fraction of 52.6 +/- 8.55% for the
group [...] CONCLUSIONS: We conclude that
selected patients with severe congestive heart
failure can markedly improve their left ventric-
ular function in association with complete reso-
lution of heart failure [...]
Table 2: MEDLINE records with explicit ar-
gumentative markers: PURPOSE, (PATIENTS
and) METHODS, RESULTS and CONCLU-
SION.
</bodyText>
<table confidence="0.998798916666667">
Bayesian classifier
PURP. METH. RESU. CONC.
PURP. 80.65 % 0 % 3.23 % 16 %
METH. 8 % 78 % 8 % 6 %
RESU. 18.58 % 5.31 % 52.21 % 23.89 %
CONC. 18.18 % 0 % 2.27 % 79.55 %
Bayesian classifier with Markov model
PURP. METH. RESU. CONC.
PURP. 93.35 % 0 % 3.23 % 3 %
METH. 3 % 78 % 8 % 6 %
RESU. 12.73 % 2.07 % 57.15 % 10.01 %
CONC. 2.27 % 0 % 2.27 % 95.45 %
</table>
<tableCaption confidence="0.998636">
Table 3: Confusion matrix for argumentative
</tableCaption>
<bodyText confidence="0.985335928571429">
classification. The harmonic means between re-
call and precision score (or F-score) is in the
range of 85% for the combined system.
conjunctive query was used to combine the fol-
lowing four strings: PURPOSE:, METHODS:,
RESULTS:, CONCLUSION:. From the original
set, we retained 12,000 abstracts used for train-
ing our categorizer, and 1,200 were used for fine-
tuning and evaluating the categorizer, following
removal of explicit argumentative markers. An
example of an abstract, structured with explicit
argumentative labels, is given in Table 2. The
per-class performance of the categorizer is given
by a contingency matrix in Table 3.
</bodyText>
<subsectionHeader confidence="0.986595">
4.3 Rocchio feedback
</subsectionHeader>
<bodyText confidence="0.9999719">
Various general query expansion approaches
have been suggested, and in this paper we com-
pared ours with that of Rocchio. In this latter
case, the system was allowed to add m terms ex-
tracted from the k best-ranked abstracts from
the original query. Each new query was derived
by applying the following formula (Equation 2):
Q&apos; = α · Q + (0/k) · E kj = 1wij (2), in which
Q&apos; denotes the new query built from the previ-
ous query Q, and wij denotes the indexing term
weight attached to the term tj in the document
Di. By direct use of the training data, we de-
termine the optimal values of our model: m =
10, k = 15. In our experiments, we fixed α =
2.0, Q = 0.75. Without feedback the mean av-
erage precision of the evaluation run is 0.3066,
the Rocchio feedback (mean average precision =
0.353) represents an improvement of about 15%
(cf. Table 5), which is statistically2 significant
(p &lt; 0.05).
</bodyText>
<subsectionHeader confidence="0.4530475">
4.4 Argumentative selection for
feedback
</subsectionHeader>
<bodyText confidence="0.999980678571429">
To apply our argumentation-driven feedback
strategy, we first have to classify the top-ranked
abstracts into our four argumentative moves:
PURPOSE, METHODS, RESULTS, and CON-
CLUSION. For the argumentative feedback, dif-
ferent m and k values are recomputed on the
training queries, depending on the argumenta-
tive category we want to over-weight. The ba-
sic segment is the sentence; therefore the ab-
stract is split into a set of sentences before being
processed by the argumentative classifier. The
sentence splitter simply applies as set of regu-
lar expressions to locate sentence boundaries.
The precision of this simple sentence splitter
equals 97% on MEDLINE abstracts. In this
setting only one argumentative category is at-
tributed to each sentence, which makes the de-
cision model binary.
Table 4 shows the output of the argumenta-
tive classifier when applied to an abstract. To
determine the respective value of each argumen-
tative contents for feedback, the argumenta-
tive categorizer parses each top-ranked abstract.
These abstracts are then used to generate four
groups of sentences. Each group corresponds to
a unique argumentative class. Each argumenta-
tive index contains sentences classified in one of
four argumentative classes. Because argumen-
</bodyText>
<footnote confidence="0.9756115">
2Tests are computed using a non-parametric signed
test, cf. (Zobel, 1998) for more details.
</footnote>
<page confidence="0.99471">
679
</page>
<table confidence="0.91280275">
CONCLUSION (00160116) The highly favorable pathologic stage
(RI-RII, 58%) and the fact that the majority of patients were
alive and disease-free suggested a more favorable prognosis
for this type of renal cell carcinoma.
METHODS (00160119) Tumors were classified according to
well-established histologic criteria to determine stage of
disease; the system proposed by Robson was used.
METHODS (00162303) Of 250 renal cell carcinomas analyzed,
36 were classified as chromophobe renal cell carcinoma,
representing 14% of the group studied.
PURPOSE (00156456) In this study, we analyzed 250 renal cell
carcinomas to a) determine frequency of CCRC at our Hospital
and b) analyze clinical and pathologic features of CCRCs.
PURPOSE (00167817) Chromophobe renal cell carcinoma (CCRC)
comprises 5% of neoplasms of renal tubular epithelium. CCRC
may have a slightly better prognosis than clear cell carcinoma,
but outcome data are limited.
RESULTS (00155338) Robson staging was possible in all cases,
and 10 patients were stage 1) 11 stage II; 10 stage III, and
five stage IV.
</table>
<tableCaption confidence="0.988799">
Table 4: Output of the argumentative catego-
</tableCaption>
<bodyText confidence="0.993775916666667">
rizer when applied to an argumentatively struc-
tured abstract after removal of explicit mark-
ers. For each row, the attributed class is fol-
lowed by the score for the class, followed by the
extracted text segment. The reader can com-
pare this categorization with argumentative la-
bels as provided in the original abstract (PMID
12404725).
tative classes are equally distributed in MED-
LINE abstracts, each index contains approxi-
mately a quarter of the top-ranked abstracts
collection.
</bodyText>
<sectionHeader confidence="0.994835" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.998360423076923">
All results are computed using the treceval pro-
gram, using the top 1000 retrieved documents
for each evaluation query. We mainly evaluate
the impact of varying the feedback category on
the retrieval effectiveness, so we separately ex-
pand our queries based a single category. Query
expansion based on RESULTS or METHODS
sentences does not result in any improvement.
On the contrary, expansion based on PURPOSE
sentences improve the Rocchio baseline by +
23%, which is again significant (p &lt; 0.05). But
the main improvement is observed when CON-
CLUSION sentences are used to generate the
expansion, with a remarkable gain of 41% when
compared to Rocchio. We also observe in Table
5 that other measures (top precision) and num-
ber of relevant retrieved articles do confirm this
trend.
For the PURPOSE category, the optimal k
parameter, computed on the test queries was
11. For the CONCLUSION category, the opti-
mal k parameter, computed on the test queries
was 10. The difference between the m values be-
tween Rocchio feedback and the argumentative
feedback, respectively 15 vs. 11 and 10 for Roc-
chio, PURPOSE, CONCLUSION sentences can
</bodyText>
<table confidence="0.99961925">
No feeback
Relevant Top Mean average
retrieved precision precision
1020 0.3871 0.3066
Rocchio feedback
Relevant Top Mean average
retrieved precision precision
1112 0.4020 0.353
Argumentative feedback: PURPOSE
Relevant Top Mean average
retrieved precision precision
1136 0.485 0.4353
Argumentative feedback: CONCLUSION
Relevant Top Mean average
retrieved precision precision
1143 0.550 0.4999
</table>
<tableCaption confidence="0.999243">
Table 5: Results without feedback, with Roc-
</tableCaption>
<bodyText confidence="0.995937578947369">
chio and with argumentative feedback applied
on PURPOSE and CONCLUSION sentences.
The number of relevant document for all queries
is 1178.
be explained by the fact that less textual mate-
rial is available when a particular class of sen-
tences is selected; therefore the number of words
that should be added to the original query is
more targeted.
From a more general perspective, the impor-
tance of CONCLUSION and PURPOSE sen-
tences is consistent with other studies, which
aimed at selecting highly content bearing sen-
tences for information extraction (Ruch et al.,
2005b). This result is also consistent with
the state-of-the-art in automatic summariza-
tion, which tends to prefer sentences appearing
at the beginning or at the end of documents to
generate summaries.
</bodyText>
<sectionHeader confidence="0.998847" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99999425">
We have reported on the evaluation of a
new linguistically-motivated feedback strategy,
which selects highly-content bearing features for
expansion based on argumentative criteria. Our
simple model is based on four classes, which
have been reported very stable in scientific re-
ports of all kinds. Our results suggest that
argumentation-driven expansion can improve
retrieval effectiveness of search engines by more
than 40%. The proposed methods open new
research directions and are generally promis-
ing for natural language processing applied to
information retrieval, whose positive impact is
still to be confirmed (Strzalkowski et al., 1998).
Finally, the proposed methods are important
from a theoretical perspective, if we consider
</bodyText>
<page confidence="0.991882">
680
</page>
<bodyText confidence="0.999845">
that it initiates a genre-specific paradigm as
opposed to the usual information retrieval ty-
pology, which distinguishes between domain-
specific and domain-independent approaches.
</bodyText>
<sectionHeader confidence="0.993796" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999950545454546">
The first author was supported by a visiting
faculty grant (ORAU) at the Lister Hill Cen-
ter of the National Library of Medicine in 2005.
We would like to thank Dina Demner-Fushman,
Susanne M. Humphrey, Jimmy Lin, Hongfang
Liu, Miguel E. Ruiz, Lawrence H. Smith, Lor-
raine K. Tanabe, W. John Wilbur for the fruit-
ful discussions we had during our weekly TREC
meetings at the NLM. The study has also been
partially supported by the Swiss National Foun-
dation (Grant 3200-065228).
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999483640449438">
I Aalbersberg. 1992. Incremental Relevance
Feedback. In SIGIR, pages 11–22.
S Abdou, P Ruch, and J Savoy. 2006. Gen-
eral vs. Specific Blind Query Expansion for
Biomedical Searches. In TREC 2005.
A Aronson, D Demner-Fushman, S Humphrey,
J Lin, H Liu, P Ruch, M Ruiz, L Smith,
L Tanabe, and J Wilbur. 2006. Fusion
of Knowledge-intensive and Statistical Ap-
proaches for Retrieving and Annotating Tex-
tual Genomics Documents. In TREC 2005.
J Xu B Croft. 1998. Corpus-based stem-
ming using cooccurrence of word variants.
ACM-Transactions on Information Systems,
16(1):61–81.
E Efthimiadis. 1996. Query expansion. Annual
Review of Information Science and Technol-
ogy, 31.
G Furnas, T Landauer, L Gomez, and S Du-
mais. 1987. The vocabulary problem in
human-system communication. Communica-
tions of the ACM, 30(11).
D Harman. 1991. How effective is suffixing ?
JASIS, 42 (1):7–15.
W Hersh, C Buckley, T Leone, and D Hickam.
1994. OHSUMED: An interactive retrieval
evaluation and new large test collection for
research. In SIGIR, pages 192–201.
W Hersh. 2005. Report on the trec 2004 ge-
nomics track. pages 21–24.
Lynette Hirschman, Alexander Yeh, Chris-
tian Blaschke, and Alfonso Valencia. 2005.
Overview of BioCreAtIvE: critical assessment
of information extraction for biology. BMC
Bioinformatics, 6 (suppl. 1).
G Hirst. 1981. Anaphora in Natural Language
Understanding: A Survey. Lecture Notes in
Computer Science 119 - Springer.
D Hull. 1996. Stemming algorithms: A case
study for detailed evaluation. Journal of
the American Society of Information Science,
47(1):70–84.
K Sparck Jones. 1971. Automatic Keyword
Classification for Information Retrieval. But-
terworths.
F Lisacek, C Chichester, A Kaplan, and San-
dor. 2005. Discovering Paradigm Shift Pat-
terns in Biomedical Abstracts: Application
to Neurodegenerative Diseases. In Proceed-
ings of the First International Symposium on
Semantic Mining in Biomedicine (SMBM),
pages 212–217. Morgan Kaufmann.
Z Liu and W Chu. 2005. Knowledge-based
query expansion to support scenario-specific
retrieval of medical free text. ACM-SAC In-
formation Access and Retrieval Track, pages
1076–1083.
C Manning and H Schiitze. 2000. Foundations
of Statistical Natural Language Processing.
MIT Press.
D Marcu. 1997. The Rhetorical Parsing of Nat-
ural Language Texts. pages 96–103.
L McKnight and P Srinivasan. 2003. Cate-
gorization of sentence types in medical ab-
stracts. AMIA Annu Symp Proc., pages 440–
444.
Y Mizuta and N Collier. 2004. Zone iden-
tification in biology articles as a basis for
information extraction. Proceedings of the
joint NLPBA/BioNLP Workshop on Natural
Language for Biomedical Applications, pages
119–125.
Y Mizuta, A Korhonen, T Mullen, and N Col-
lier. 2005. Zone Analysis in Biology Articles
as a Basis for Information Extraction. Inter-
national Journal of Medical Informatics, to
appear.
C Orasan. 2001. Patterns in Scientific Ab-
stracts. In Proceedings of Corpus Linguistics,
pages 433–445.
Y Qiu and H Frei. 1993. Concept based query
expansion. ACM-SIGIR, pages 160–69.
S Robertson. 1990. On term selection for
query expansion. Journal of Documentation,
46(4):359–364.
J Rocchio. 1971. Relevance feedback in infor-
mation retrieval in The SMART Retrieval
System - Experiments in Automatic Docu-
ment Processing. Prentice-Hall.
</reference>
<page confidence="0.978016">
681
</page>
<reference confidence="0.999840985507246">
P Ruch, R Baud, C Chichester, A Geissbiihler,
F Lisacek, J Marty, D Rebholz-Schuhmann,
I Tbahriti, and AL Veuthey. 2005a. Extract-
ing Key Sentences with Latent Argumenta-
tive Structuring. In Medical Informatica Eu-
rope (MIE), pages 835–40.
P Ruch, L Perret, and J Savoy. 2005b. Features
Combination for Extracting Gene Functions
from MEDLINE. In European Colloquium
on Information Retrieval (ECIR), pages 112–
126.
P Ruch. 2002. Using contextual spelling correc-
tion to improve retrieval effectiveness in de-
graded text collections. COLING 2002.
P Ruch. 2004. Query translation by text cate-
gorization. COLING 2004.
P Ruch. 2006. Automatic Assignment of
Biomedical Categories: Toward a Generic
Approach. Bioinformatics, 6.
G Salton and C Buckley. 1990. Improving re-
trieval performance by relevance feedback.
Journal of the American Society for Informa-
tion Science, 41(4).
J Savoy. 2005. Bibliographic database access
using free-text and controlled vocabulary: An
evaluation. Information Processing and Man-
agement, 41(4):873–890.
A Singhal, C Buckley, and M Mitra. 1996a.
Pivoted document length normalization.
ACM-SIGIR, pages 21–29.
C Buckley A Singhal, M Mitra, and G Salton.
1996b. New retrieval approaches using smart.
In Proceedings of TREC-4.
T Strzalkowski, G Stein, G Bowden Wise,
J Perez Carballo, P Tapanainen, T Jarvinen,
A Voutilainen, and J Karlgren. 1998. Natu-
ral language information retrieval: TREC-7
report. In Text REtrieval Conference, pages
164–173.
J Swales. 1990. Genre Analysis: English in
Academic and Research Settings. Cambridge
University Press.
I Tbahriti, C Chichester, F Lisacek, and
P Ruch. 2004. Using Argumention to
Retrieve Articles with Similar Citations
from MEDLINE. Proceedings of the joint
NLPBA/BioNLP Workshop on Natural Lan-
guage for Biomedical Applications.
I Tbahriti, C Chichester, F Lisacek, and
P Ruch. 2005. Using Argumentation to Re-
trieve Articles with Similar Citations: an In-
quiry into Improving Related Articles Search
in the MEDLINE Digital Library. Interna-
tional Journal of Medical Informatics, to ap-
pear.
S Teufel and M Moens. 1999. Argumenta-
tive Classification of Extracted Sentences as
a First Step Towards Flexible Abstracting.
Advances in Automatic Text Summarization,
MIT Press, pages 155–171.
B Velez, R Weiss, M Sheldon, and D Gifford.
1997. Fast and effective query refinement. In
ACM SIGIR, pages 6–15.
E Voorhees. 1994. Query expansion using
lexical-semantic relations. In ACM SIGIR,
pages 61–69.
J Zobel. 1998. How reliable are large-scale
information retrieval experiments? ACM-
SIGIR, pages 307–314.
</reference>
<page confidence="0.997967">
682
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.700212">
<title confidence="0.9993005">Argumentative Feedback: A Linguistically-motivated Term Expansion for Information Retrieval</title>
<author confidence="0.999981">Patrick Ruch</author>
<author confidence="0.999981">Imad Tbahriti</author>
<author confidence="0.999981">Julien Gobeill Alan R Aronson</author>
<affiliation confidence="0.9973925">Medical Informatics Service Lister Hill Center University of Geneva National Library of Medicine</affiliation>
<address confidence="0.926394">24 Micheli du Crest 8600 Rockville Pike 1201 Geneva Bethesda, MD 20894 Switzerland USA</address>
<email confidence="0.995448">alan@nlm.nih.gov</email>
<abstract confidence="0.996904096774194">We report on the development of a new automatic feedback model to improve information retrieval in digital libraries. Our hypothesis is that some particular sentences, selected based on argumentative criteria, can be more useful than others to perform well-known feedback information retrieval tasks. The argumentative model we explore is based on four disjunct classes, which has been very regularly observed in scientific reports: PURPOSE, METHODS, RE- SULTS, CONCLUSION. To test this hypothesis, we use the Rocchio algorithm as baseline. While Rocchio selects the features to be added to the original query based on statistical evidence, we propose to base our feature selection also on argumentative criteria. Thus, we restrict the expansion on features appearing only in sentences classified into one of our argumentative categories. Our results, obtained on the OHSUMED collection, show a significant improvement when expansion is based on PURPOSE (mean average precision = +23%) and CONCLUSION (mean average precision = +41%) contents rather than on other argumentative contents. These results suggest that argumentation is an important linguistic dimension that could benefit information retrieval.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Aalbersberg</author>
</authors>
<title>Incremental Relevance Feedback. In</title>
<date>1992</date>
<booktitle>SIGIR,</booktitle>
<pages>11--22</pages>
<contexts>
<context position="8900" citStr="Aalbersberg, 1992" startWordPosition="1344" endWordPosition="1346">nding the original query, Rocchio (Rocchio, 1971) proposed accounting for the relevance or irrelevance of top-ranked documents, according to the user’s manual input. In this case, a new query was automatically built in the form of a linear combination of the term included in the previous query and terms automatically extracted from both the relevant documents (with a positive weight) and non-relevant items (with a negative weight). Empirical studies (e.g., (Salton and Buckley, 1990)) demonstrated that such an approach is usually quite effective, and could 676 be used more than once per query (Aalbersberg, 1992). Buckley et al. (Singhal et al., 1996b) suggested that we could assume, without even looking at them or asking the user, that the top k ranked documents are relevant. Denoted the pseudo-relevance feedback or blindquery expansion approach, this approach is usually effective, at least when handling relatively large text collections. As a third source, we might use large text corpora to derive various term-term relationships, using statistically or information-based measures (Jones, 1971), (Manning and Schutze, 2000). For example, (Qiu and Frei, 1993) suggested that terms to be added to a new qu</context>
</contexts>
<marker>Aalbersberg, 1992</marker>
<rawString>I Aalbersberg. 1992. Incremental Relevance Feedback. In SIGIR, pages 11–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abdou</author>
<author>P Ruch</author>
<author>J Savoy</author>
</authors>
<title>General vs. Specific Blind Query Expansion for Biomedical Searches. In TREC</title>
<date>2006</date>
<contexts>
<context position="11940" citStr="Abdou et al., 2006" startWordPosition="1820" endWordPosition="1823">ry expansion, 0.433 using statistical methods and 0.452 with domain-specific approaches). In these different query expansion approaches, various underlying parameters must be specified, and generally there is no single theory able to help us find the most appropriate values. Recent empirical studies conducted in the context of the TREC Genomics track, using the OHSUGEN collection (Hersh, 2005), show that neither blind expansion (Rocchio), nor domain-specific query expansion (thesaurus-based Gene and Protein expansion) seem appropriate to improve retrieval effectiveness (Aronson et al., 2006) (Abdou et al., 2006). 3 Data and metrics To test our hypothesis, we used the OHSUMED collection (Hersh et al., 1994), originally developed for the TREC topic detection track, which is the most popular information retrieval collection for evaluating information search in library corpora. Alternative collections (cf. (Savoy, 2005)), such as the French Amaryllis collection, are usually smaller and/or not appropriate to evaluate our argumentative classifier, which can only process English documents. Other MEDLINE collections, which can be regarded as similar in size or larger, such as the TREC Genomics 2004 and 2005 </context>
<context position="16839" citStr="Abdou et al., 2006" startWordPosition="2592" endWordPosition="2595">E corpora (Aronson et al., 2006). Query and document weighings are provided in Equation (1): the dtu formula is applied to the documents, while the dtn formula is applied to the query; t the number of indexing terms, dfj the number of documents in which the term tj; pivot and slope are constants (fixed at pivot = 0.14, slope = 146). (Ln(Ln(t fij)+1)+1) ·idfj dtu: wij = (1−slope)·pivot+slope·nti dtn: wij = idfj · (Ln(Ln(tfif) + 1) + 1) As already observed in several linguisticallymotivated studies (Hull, 1996), we observe that common stemming methods do not perform well on MEDLINE collections (Abdou et al., 2006), therefore indexing units are stored in the inverted file using a simple S-stemmer (Harman, 1991), which basically handles most frequent plural forms and exceptions of the English language such as -ies, -es and -s and exclude endings such as -aies, -eies, -ss, etc. This simple normalization procedure performs better than others and better than no stemming. We also use a slightly modified standard stopword list of 544 items, where strings such as a, which stands for alpha in chemistry and is relevant in biomedical expressions such as vitamin a. 4.2 Argumentative categorizer The argumentative c</context>
</contexts>
<marker>Abdou, Ruch, Savoy, 2006</marker>
<rawString>S Abdou, P Ruch, and J Savoy. 2006. General vs. Specific Blind Query Expansion for Biomedical Searches. In TREC 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Aronson</author>
<author>D Demner-Fushman</author>
<author>S Humphrey</author>
<author>J Lin</author>
<author>H Liu</author>
<author>P Ruch</author>
<author>M Ruiz</author>
<author>L Smith</author>
<author>L Tanabe</author>
<author>J Wilbur</author>
</authors>
<title>Fusion of Knowledge-intensive and Statistical Approaches for Retrieving and Annotating Textual Genomics Documents. In TREC</title>
<date>2006</date>
<contexts>
<context position="11919" citStr="Aronson et al., 2006" startWordPosition="1816" endWordPosition="1819">(MAP: 0.408 without query expansion, 0.433 using statistical methods and 0.452 with domain-specific approaches). In these different query expansion approaches, various underlying parameters must be specified, and generally there is no single theory able to help us find the most appropriate values. Recent empirical studies conducted in the context of the TREC Genomics track, using the OHSUGEN collection (Hersh, 2005), show that neither blind expansion (Rocchio), nor domain-specific query expansion (thesaurus-based Gene and Protein expansion) seem appropriate to improve retrieval effectiveness (Aronson et al., 2006) (Abdou et al., 2006). 3 Data and metrics To test our hypothesis, we used the OHSUMED collection (Hersh et al., 1994), originally developed for the TREC topic detection track, which is the most popular information retrieval collection for evaluating information search in library corpora. Alternative collections (cf. (Savoy, 2005)), such as the French Amaryllis collection, are usually smaller and/or not appropriate to evaluate our argumentative classifier, which can only process English documents. Other MEDLINE collections, which can be regarded as similar in size or larger, such as the TREC Ge</context>
<context position="16252" citStr="Aronson et al., 2006" startWordPosition="2493" endWordPosition="2496"> to the original queries based on pure statistical analysis, we propose to base our feature expansion also on argumentative criteria. That is, we overweight features appearing in sentences classified in a particular argumentative category by the argumentative categorizer. 4.1 Retrieval engine and indexing units The easyIR system is a standard vector-space engine (Ruch, 2004), which computes stateof-the-art tf.idf and probabilistic weighting schema. All experiments were conducted with pivoted normalization (Singhal et al., 1996a), which has recently shown some effectiveness on MEDLINE corpora (Aronson et al., 2006). Query and document weighings are provided in Equation (1): the dtu formula is applied to the documents, while the dtn formula is applied to the query; t the number of indexing terms, dfj the number of documents in which the term tj; pivot and slope are constants (fixed at pivot = 0.14, slope = 146). (Ln(Ln(t fij)+1)+1) ·idfj dtu: wij = (1−slope)·pivot+slope·nti dtn: wij = idfj · (Ln(Ln(tfif) + 1) + 1) As already observed in several linguisticallymotivated studies (Hull, 1996), we observe that common stemming methods do not perform well on MEDLINE collections (Abdou et al., 2006), therefore i</context>
</contexts>
<marker>Aronson, Demner-Fushman, Humphrey, Lin, Liu, Ruch, Ruiz, Smith, Tanabe, Wilbur, 2006</marker>
<rawString>A Aronson, D Demner-Fushman, S Humphrey, J Lin, H Liu, P Ruch, M Ruiz, L Smith, L Tanabe, and J Wilbur. 2006. Fusion of Knowledge-intensive and Statistical Approaches for Retrieving and Annotating Textual Genomics Documents. In TREC 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu B Croft</author>
</authors>
<title>Corpus-based stemming using cooccurrence of word variants.</title>
<date>1998</date>
<journal>ACM-Transactions on Information Systems,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="10068" citStr="Croft, 1998" startWordPosition="1530" endWordPosition="1531">suggested that terms to be added to a new query could be extracted from a similarity thesaurus automatically built through calculating co-occurrence frequencies in the search collection. The underlying effect was to add idiosyncratic terms to the underlying document collection, related to the query terms by language use. When using such query expansion approaches, we can assume that the new terms are more appropriate for the retrieval of pertinent items than are lexically or semantically related terms provided by a general thesaurus or dictionary. To complement this global document analysis, (Croft, 1998) suggested that text passages (with a text window size of between 100 to 300 words) be taken into account. This local document analysis seemed to be more effective than a global term relationship generation. As a forth source of additional terms, we might account for specific user information needs and/or the underlying domain. In this vein, (Liu and Chu, 2005) suggested that terms related to the user’s intention or scenario might be included. In the medical domain, it was observed that users looking for information usually have an underlying scenario in mind (or a typical medical task). Knowi</context>
</contexts>
<marker>Croft, 1998</marker>
<rawString>J Xu B Croft. 1998. Corpus-based stemming using cooccurrence of word variants. ACM-Transactions on Information Systems, 16(1):61–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Efthimiadis</author>
</authors>
<title>Query expansion.</title>
<date>1996</date>
<journal>Annual Review of Information Science and Technology,</journal>
<volume>31</volume>
<contexts>
<context position="7663" citStr="Efthimiadis, 1996" startWordPosition="1146" endWordPosition="1147">ciple is to expand the query using words or phrases having a similar or related meaning to those appearing in the original request. Various empirical studies based on different IR models or collections have shown that this type of search strategy should usually be effective in enhancing retrieval performance. Scheme propositions such as this should consider the various relationships between words as well as term selection mechanisms and term weighting schemes (Robertson, 1990). The specific answers found to these questions may vary; thus a variety of query expansion approaches were suggested (Efthimiadis, 1996). In a first attempt to find related search terms, we might ask the user to select additional terms to be included in a new query, e.g. (Velez et al., 1997). This could be handled interactively through displaying a ranked list of retrieved items returned by the first query. Voorhees (Voorhees, 1994) proposed basing a scheme based on the WordNet thesaurus. The author demonstrated that terms having a lexicalsemantic relation with the original query words (extracted from a synonym relationship) provided very little improvement (around 1% when compared to the original unexpanded query). As a secon</context>
</contexts>
<marker>Efthimiadis, 1996</marker>
<rawString>E Efthimiadis. 1996. Query expansion. Annual Review of Information Science and Technology, 31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Furnas</author>
<author>T Landauer</author>
<author>L Gomez</author>
<author>S Dumais</author>
</authors>
<title>The vocabulary problem in human-system communication.</title>
<date>1987</date>
<journal>Communications of the ACM,</journal>
<volume>30</volume>
<issue>11</issue>
<contexts>
<context position="2083" citStr="Furnas et al., 1987" startWordPosition="305" endWordPosition="308">han on other argumentative contents. These results suggest that argumentation is an important linguistic dimension that could benefit information retrieval. 1 Introduction Information retrieval (IR) is a challenging endeavor due to problems caused by the underlying expressiveness of all natural languages. One of these problems, synonymy, is that authors and users frequently employ different words or expressions to refer to the same meaning (accident may be expressed as event, incident, problem, difficulty, unfortunate situation, the subject of your last letter, what happened last week, etc.) (Furnas et al., 1987). Another problem is ambiguity, where a specific term may have several (and sometimes contradictory) meanings and interpretations (e.g., the word horse as in Trojan horse, light horse, to work like a horse, horse about). In order to obtain better meaning-based matches between queries and documents, various propositions have been suggested, usually without giving any consideration to the underlying domain. During our participation in different international evaluation campaigns such as the TREC Genomics track (Hersh, 2005), the BioCreative initiative (Hirschman et al., 2005), as well as in our </context>
</contexts>
<marker>Furnas, Landauer, Gomez, Dumais, 1987</marker>
<rawString>G Furnas, T Landauer, L Gomez, and S Dumais. 1987. The vocabulary problem in human-system communication. Communications of the ACM, 30(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harman</author>
</authors>
<title>How effective is suffixing ?</title>
<date>1991</date>
<journal>JASIS,</journal>
<volume>42</volume>
<pages>1--7</pages>
<contexts>
<context position="16937" citStr="Harman, 1991" startWordPosition="2610" endWordPosition="2611">mula is applied to the documents, while the dtn formula is applied to the query; t the number of indexing terms, dfj the number of documents in which the term tj; pivot and slope are constants (fixed at pivot = 0.14, slope = 146). (Ln(Ln(t fij)+1)+1) ·idfj dtu: wij = (1−slope)·pivot+slope·nti dtn: wij = idfj · (Ln(Ln(tfif) + 1) + 1) As already observed in several linguisticallymotivated studies (Hull, 1996), we observe that common stemming methods do not perform well on MEDLINE collections (Abdou et al., 2006), therefore indexing units are stored in the inverted file using a simple S-stemmer (Harman, 1991), which basically handles most frequent plural forms and exceptions of the English language such as -ies, -es and -s and exclude endings such as -aies, -eies, -ss, etc. This simple normalization procedure performs better than others and better than no stemming. We also use a slightly modified standard stopword list of 544 items, where strings such as a, which stands for alpha in chemistry and is relevant in biomedical expressions such as vitamin a. 4.2 Argumentative categorizer The argumentative classifier ranks and categorizes abstract sentences as to their argumentative classes. To implement</context>
</contexts>
<marker>Harman, 1991</marker>
<rawString>D Harman. 1991. How effective is suffixing ? JASIS, 42 (1):7–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hersh</author>
<author>C Buckley</author>
<author>T Leone</author>
<author>D Hickam</author>
</authors>
<title>OHSUMED: An interactive retrieval evaluation and new large test collection for research.</title>
<date>1994</date>
<booktitle>In SIGIR,</booktitle>
<pages>192--201</pages>
<contexts>
<context position="12036" citStr="Hersh et al., 1994" startWordPosition="1837" endWordPosition="1840">se different query expansion approaches, various underlying parameters must be specified, and generally there is no single theory able to help us find the most appropriate values. Recent empirical studies conducted in the context of the TREC Genomics track, using the OHSUGEN collection (Hersh, 2005), show that neither blind expansion (Rocchio), nor domain-specific query expansion (thesaurus-based Gene and Protein expansion) seem appropriate to improve retrieval effectiveness (Aronson et al., 2006) (Abdou et al., 2006). 3 Data and metrics To test our hypothesis, we used the OHSUMED collection (Hersh et al., 1994), originally developed for the TREC topic detection track, which is the most popular information retrieval collection for evaluating information search in library corpora. Alternative collections (cf. (Savoy, 2005)), such as the French Amaryllis collection, are usually smaller and/or not appropriate to evaluate our argumentative classifier, which can only process English documents. Other MEDLINE collections, which can be regarded as similar in size or larger, such as the TREC Genomics 2004 and 2005 collections are unfortunately more domain-specific since information requests in these collectio</context>
</contexts>
<marker>Hersh, Buckley, Leone, Hickam, 1994</marker>
<rawString>W Hersh, C Buckley, T Leone, and D Hickam. 1994. OHSUMED: An interactive retrieval evaluation and new large test collection for research. In SIGIR, pages 192–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hersh</author>
</authors>
<title>Report on the trec 2004 genomics track.</title>
<date>2005</date>
<pages>21--24</pages>
<contexts>
<context position="2610" citStr="Hersh, 2005" startWordPosition="387" endWordPosition="388">the subject of your last letter, what happened last week, etc.) (Furnas et al., 1987). Another problem is ambiguity, where a specific term may have several (and sometimes contradictory) meanings and interpretations (e.g., the word horse as in Trojan horse, light horse, to work like a horse, horse about). In order to obtain better meaning-based matches between queries and documents, various propositions have been suggested, usually without giving any consideration to the underlying domain. During our participation in different international evaluation campaigns such as the TREC Genomics track (Hersh, 2005), the BioCreative initiative (Hirschman et al., 2005), as well as in our attempts to deliver advanced search tools for biologists (Ruch, 2006) and healthcare providers (Ruch, 2002) (Ruch, 2004), we were more concerned with domain-specific information retrieval in which systems must return a ranked list of MEDLINE records in response to an expert’s information request. This involved a set of available queries describing typical search interests, in which gene, protein names, and diseases were often essential for an effective retrieval. Biomedical publications however tend to generate new inform</context>
<context position="11717" citStr="Hersh, 2005" startWordPosition="1790" endWordPosition="1791">erse document frequency (idf) retrieval model, the domainspecific query-expansion scheme suggested by Liu and Chu (2005) produces better retrieval performance than a scheme based on statistics (MAP: 0.408 without query expansion, 0.433 using statistical methods and 0.452 with domain-specific approaches). In these different query expansion approaches, various underlying parameters must be specified, and generally there is no single theory able to help us find the most appropriate values. Recent empirical studies conducted in the context of the TREC Genomics track, using the OHSUGEN collection (Hersh, 2005), show that neither blind expansion (Rocchio), nor domain-specific query expansion (thesaurus-based Gene and Protein expansion) seem appropriate to improve retrieval effectiveness (Aronson et al., 2006) (Abdou et al., 2006). 3 Data and metrics To test our hypothesis, we used the OHSUMED collection (Hersh et al., 1994), originally developed for the TREC topic detection track, which is the most popular information retrieval collection for evaluating information search in library corpora. Alternative collections (cf. (Savoy, 2005)), such as the French Amaryllis collection, are usually smaller and</context>
</contexts>
<marker>Hersh, 2005</marker>
<rawString>W Hersh. 2005. Report on the trec 2004 genomics track. pages 21–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Alexander Yeh</author>
<author>Christian Blaschke</author>
<author>Alfonso Valencia</author>
</authors>
<title>Overview of BioCreAtIvE: critical assessment of information extraction for biology.</title>
<date>2005</date>
<journal>BMC Bioinformatics,</journal>
<volume>6</volume>
<contexts>
<context position="2663" citStr="Hirschman et al., 2005" startWordPosition="392" endWordPosition="395">ened last week, etc.) (Furnas et al., 1987). Another problem is ambiguity, where a specific term may have several (and sometimes contradictory) meanings and interpretations (e.g., the word horse as in Trojan horse, light horse, to work like a horse, horse about). In order to obtain better meaning-based matches between queries and documents, various propositions have been suggested, usually without giving any consideration to the underlying domain. During our participation in different international evaluation campaigns such as the TREC Genomics track (Hersh, 2005), the BioCreative initiative (Hirschman et al., 2005), as well as in our attempts to deliver advanced search tools for biologists (Ruch, 2006) and healthcare providers (Ruch, 2002) (Ruch, 2004), we were more concerned with domain-specific information retrieval in which systems must return a ranked list of MEDLINE records in response to an expert’s information request. This involved a set of available queries describing typical search interests, in which gene, protein names, and diseases were often essential for an effective retrieval. Biomedical publications however tend to generate new information very rapidly and also use a wide variation in t</context>
</contexts>
<marker>Hirschman, Yeh, Blaschke, Valencia, 2005</marker>
<rawString>Lynette Hirschman, Alexander Yeh, Christian Blaschke, and Alfonso Valencia. 2005. Overview of BioCreAtIvE: critical assessment of information extraction for biology. BMC Bioinformatics, 6 (suppl. 1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
</authors>
<title>Anaphora in Natural Language Understanding: A Survey.</title>
<date>1981</date>
<journal>Lecture Notes in Computer Science</journal>
<volume>119</volume>
<publisher>Springer.</publisher>
<contexts>
<context position="6763" citStr="Hirst, 1981" startWordPosition="1009" endWordPosition="1010">ht and Srinivasan, 2003) propose a model very similar to our fourclass model but is inspired by clinical trials. Preliminary applications were proposed for bib&apos;After Aristotle, discourses structured following an appropriate argumentative distribution belong to logics, while ill-defined ones belong to rhetorics. liometrics and related-article search (Tbahriti et al., 2004) (Tbahriti et al., 2005), information extraction and passage retrieval (Ruch et al., 2005b). In these studies, sentences were selected as the basic classification unit in order to avoid as far as possible co-reference issues (Hirst, 1981), which hinder readibity of automatically generated and extracted sentences. 2.2 Q uery expansion Various query expansion techniques have been suggested to provide a better match between user information needs and documents, and to increase retrieval effectiveness. The general principle is to expand the query using words or phrases having a similar or related meaning to those appearing in the original request. Various empirical studies based on different IR models or collections have shown that this type of search strategy should usually be effective in enhancing retrieval performance. Scheme </context>
</contexts>
<marker>Hirst, 1981</marker>
<rawString>G Hirst. 1981. Anaphora in Natural Language Understanding: A Survey. Lecture Notes in Computer Science 119 - Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hull</author>
</authors>
<title>Stemming algorithms: A case study for detailed evaluation.</title>
<date>1996</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="16734" citStr="Hull, 1996" startWordPosition="2577" endWordPosition="2578">oted normalization (Singhal et al., 1996a), which has recently shown some effectiveness on MEDLINE corpora (Aronson et al., 2006). Query and document weighings are provided in Equation (1): the dtu formula is applied to the documents, while the dtn formula is applied to the query; t the number of indexing terms, dfj the number of documents in which the term tj; pivot and slope are constants (fixed at pivot = 0.14, slope = 146). (Ln(Ln(t fij)+1)+1) ·idfj dtu: wij = (1−slope)·pivot+slope·nti dtn: wij = idfj · (Ln(Ln(tfif) + 1) + 1) As already observed in several linguisticallymotivated studies (Hull, 1996), we observe that common stemming methods do not perform well on MEDLINE collections (Abdou et al., 2006), therefore indexing units are stored in the inverted file using a simple S-stemmer (Harman, 1991), which basically handles most frequent plural forms and exceptions of the English language such as -ies, -es and -s and exclude endings such as -aies, -eies, -ss, etc. This simple normalization procedure performs better than others and better than no stemming. We also use a slightly modified standard stopword list of 544 items, where strings such as a, which stands for alpha in chemistry and i</context>
</contexts>
<marker>Hull, 1996</marker>
<rawString>D Hull. 1996. Stemming algorithms: A case study for detailed evaluation. Journal of the American Society of Information Science, 47(1):70–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck Jones</author>
</authors>
<title>Automatic Keyword Classification for Information Retrieval.</title>
<date>1971</date>
<publisher>Butterworths.</publisher>
<contexts>
<context position="9391" citStr="Jones, 1971" startWordPosition="1422" endWordPosition="1423">ated that such an approach is usually quite effective, and could 676 be used more than once per query (Aalbersberg, 1992). Buckley et al. (Singhal et al., 1996b) suggested that we could assume, without even looking at them or asking the user, that the top k ranked documents are relevant. Denoted the pseudo-relevance feedback or blindquery expansion approach, this approach is usually effective, at least when handling relatively large text collections. As a third source, we might use large text corpora to derive various term-term relationships, using statistically or information-based measures (Jones, 1971), (Manning and Schutze, 2000). For example, (Qiu and Frei, 1993) suggested that terms to be added to a new query could be extracted from a similarity thesaurus automatically built through calculating co-occurrence frequencies in the search collection. The underlying effect was to add idiosyncratic terms to the underlying document collection, related to the query terms by language use. When using such query expansion approaches, we can assume that the new terms are more appropriate for the retrieval of pertinent items than are lexically or semantically related terms provided by a general thesau</context>
</contexts>
<marker>Jones, 1971</marker>
<rawString>K Sparck Jones. 1971. Automatic Keyword Classification for Information Retrieval. Butterworths.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Lisacek</author>
<author>C Chichester</author>
<author>A Kaplan</author>
<author>Sandor</author>
</authors>
<title>Discovering Paradigm Shift Patterns in Biomedical Abstracts: Application to Neurodegenerative Diseases.</title>
<date>2005</date>
<booktitle>In Proceedings of the First International Symposium on Semantic Mining in Biomedicine (SMBM),</booktitle>
<pages>212--217</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="6120" citStr="Lisacek et al., 2005" startWordPosition="915" endWordPosition="918">ment summarization. Of particular interest for our approach, Teufel and Moens (Teufel and Moens, 1999) propose using a list of manually crafted triggers (using both words and expressions such as we argued, in this article, the paper is an attempt to, we aim at, etc.) to automatically structure scientific articles into a lighter model, with only seven categories: BACKGROUND, TOPIC, RELATED WORK, PURPOSE, METHOD, RESULT, and CONCLUSION. More recently and for knowledge discovery in molecular biology, more elaborated models were proposed by (Mizuta and Collier, 2004) (Mizuta et al., 2005) and by (Lisacek et al., 2005) for novelty-detection. (McKnight and Srinivasan, 2003) propose a model very similar to our fourclass model but is inspired by clinical trials. Preliminary applications were proposed for bib&apos;After Aristotle, discourses structured following an appropriate argumentative distribution belong to logics, while ill-defined ones belong to rhetorics. liometrics and related-article search (Tbahriti et al., 2004) (Tbahriti et al., 2005), information extraction and passage retrieval (Ruch et al., 2005b). In these studies, sentences were selected as the basic classification unit in order to avoid as far as</context>
</contexts>
<marker>Lisacek, Chichester, Kaplan, Sandor, 2005</marker>
<rawString>F Lisacek, C Chichester, A Kaplan, and Sandor. 2005. Discovering Paradigm Shift Patterns in Biomedical Abstracts: Application to Neurodegenerative Diseases. In Proceedings of the First International Symposium on Semantic Mining in Biomedicine (SMBM), pages 212–217. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Liu</author>
<author>W Chu</author>
</authors>
<title>Knowledge-based query expansion to support scenario-specific retrieval of medical free text. ACM-SAC Information Access and Retrieval Track,</title>
<date>2005</date>
<pages>1076--1083</pages>
<contexts>
<context position="10431" citStr="Liu and Chu, 2005" startWordPosition="1589" endWordPosition="1592">proaches, we can assume that the new terms are more appropriate for the retrieval of pertinent items than are lexically or semantically related terms provided by a general thesaurus or dictionary. To complement this global document analysis, (Croft, 1998) suggested that text passages (with a text window size of between 100 to 300 words) be taken into account. This local document analysis seemed to be more effective than a global term relationship generation. As a forth source of additional terms, we might account for specific user information needs and/or the underlying domain. In this vein, (Liu and Chu, 2005) suggested that terms related to the user’s intention or scenario might be included. In the medical domain, it was observed that users looking for information usually have an underlying scenario in mind (or a typical medical task). Knowing that the number of scenarios for a user is rather limited (e.g., diagnosis, treatment, etiology), the authors suggested automatically building a semantic network based on a domain-specific thesaurus (using the Unified Medical Language System (UMLS) in this case). The effectiveness of this strategy would of course depend on the quality and completeness of dom</context>
</contexts>
<marker>Liu, Chu, 2005</marker>
<rawString>Z Liu and W Chu. 2005. Knowledge-based query expansion to support scenario-specific retrieval of medical free text. ACM-SAC Information Access and Retrieval Track, pages 1076–1083.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schiitze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<marker>Manning, Schiitze, 2000</marker>
<rawString>C Manning and H Schiitze. 2000. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The Rhetorical Parsing of Natural Language Texts.</title>
<date>1997</date>
<pages>96--103</pages>
<contexts>
<context position="5411" citStr="Marcu, 1997" startWordPosition="806" endWordPosition="807">tation Originally inspired by corpus linguistics studies (Orasan, 2001), which suggests that scientific reports (in chemistry, linguistics, computer sciences, medicine...) exhibit a very regular logical distribution -confirmed by studies conducted on biomedical corpora (Swales, 1990) and by ANSI/ISO professional standards - the argumentative model we experiment is based on four disjunct classes: PURPOSE, METHODS, RESULTS, CONCLUSION. Argumentation belongs to discourse analysisi, with fairly complex computational models such as the implementation of the rhetorical structure theory proposed by (Marcu, 1997), which proposes dozens of rhetorical classes. More recent advances were applied to document summarization. Of particular interest for our approach, Teufel and Moens (Teufel and Moens, 1999) propose using a list of manually crafted triggers (using both words and expressions such as we argued, in this article, the paper is an attempt to, we aim at, etc.) to automatically structure scientific articles into a lighter model, with only seven categories: BACKGROUND, TOPIC, RELATED WORK, PURPOSE, METHOD, RESULT, and CONCLUSION. More recently and for knowledge discovery in molecular biology, more elab</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>D Marcu. 1997. The Rhetorical Parsing of Natural Language Texts. pages 96–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L McKnight</author>
<author>P Srinivasan</author>
</authors>
<title>Categorization of sentence types in medical abstracts. AMIA Annu Symp Proc.,</title>
<date>2003</date>
<pages>440--444</pages>
<contexts>
<context position="6175" citStr="McKnight and Srinivasan, 2003" startWordPosition="921" endWordPosition="924"> our approach, Teufel and Moens (Teufel and Moens, 1999) propose using a list of manually crafted triggers (using both words and expressions such as we argued, in this article, the paper is an attempt to, we aim at, etc.) to automatically structure scientific articles into a lighter model, with only seven categories: BACKGROUND, TOPIC, RELATED WORK, PURPOSE, METHOD, RESULT, and CONCLUSION. More recently and for knowledge discovery in molecular biology, more elaborated models were proposed by (Mizuta and Collier, 2004) (Mizuta et al., 2005) and by (Lisacek et al., 2005) for novelty-detection. (McKnight and Srinivasan, 2003) propose a model very similar to our fourclass model but is inspired by clinical trials. Preliminary applications were proposed for bib&apos;After Aristotle, discourses structured following an appropriate argumentative distribution belong to logics, while ill-defined ones belong to rhetorics. liometrics and related-article search (Tbahriti et al., 2004) (Tbahriti et al., 2005), information extraction and passage retrieval (Ruch et al., 2005b). In these studies, sentences were selected as the basic classification unit in order to avoid as far as possible co-reference issues (Hirst, 1981), which hind</context>
</contexts>
<marker>McKnight, Srinivasan, 2003</marker>
<rawString>L McKnight and P Srinivasan. 2003. Categorization of sentence types in medical abstracts. AMIA Annu Symp Proc., pages 440– 444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Mizuta</author>
<author>N Collier</author>
</authors>
<title>Zone identification in biology articles as a basis for information extraction.</title>
<date>2004</date>
<booktitle>Proceedings of the joint NLPBA/BioNLP Workshop on Natural Language for Biomedical Applications,</booktitle>
<pages>119--125</pages>
<contexts>
<context position="6068" citStr="Mizuta and Collier, 2004" startWordPosition="905" endWordPosition="908">rical classes. More recent advances were applied to document summarization. Of particular interest for our approach, Teufel and Moens (Teufel and Moens, 1999) propose using a list of manually crafted triggers (using both words and expressions such as we argued, in this article, the paper is an attempt to, we aim at, etc.) to automatically structure scientific articles into a lighter model, with only seven categories: BACKGROUND, TOPIC, RELATED WORK, PURPOSE, METHOD, RESULT, and CONCLUSION. More recently and for knowledge discovery in molecular biology, more elaborated models were proposed by (Mizuta and Collier, 2004) (Mizuta et al., 2005) and by (Lisacek et al., 2005) for novelty-detection. (McKnight and Srinivasan, 2003) propose a model very similar to our fourclass model but is inspired by clinical trials. Preliminary applications were proposed for bib&apos;After Aristotle, discourses structured following an appropriate argumentative distribution belong to logics, while ill-defined ones belong to rhetorics. liometrics and related-article search (Tbahriti et al., 2004) (Tbahriti et al., 2005), information extraction and passage retrieval (Ruch et al., 2005b). In these studies, sentences were selected as the b</context>
</contexts>
<marker>Mizuta, Collier, 2004</marker>
<rawString>Y Mizuta and N Collier. 2004. Zone identification in biology articles as a basis for information extraction. Proceedings of the joint NLPBA/BioNLP Workshop on Natural Language for Biomedical Applications, pages 119–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Mizuta</author>
<author>A Korhonen</author>
<author>T Mullen</author>
<author>N Collier</author>
</authors>
<title>Zone Analysis in Biology Articles as a Basis for Information Extraction.</title>
<date>2005</date>
<journal>International Journal of Medical Informatics,</journal>
<note>to appear.</note>
<contexts>
<context position="6090" citStr="Mizuta et al., 2005" startWordPosition="909" endWordPosition="912">advances were applied to document summarization. Of particular interest for our approach, Teufel and Moens (Teufel and Moens, 1999) propose using a list of manually crafted triggers (using both words and expressions such as we argued, in this article, the paper is an attempt to, we aim at, etc.) to automatically structure scientific articles into a lighter model, with only seven categories: BACKGROUND, TOPIC, RELATED WORK, PURPOSE, METHOD, RESULT, and CONCLUSION. More recently and for knowledge discovery in molecular biology, more elaborated models were proposed by (Mizuta and Collier, 2004) (Mizuta et al., 2005) and by (Lisacek et al., 2005) for novelty-detection. (McKnight and Srinivasan, 2003) propose a model very similar to our fourclass model but is inspired by clinical trials. Preliminary applications were proposed for bib&apos;After Aristotle, discourses structured following an appropriate argumentative distribution belong to logics, while ill-defined ones belong to rhetorics. liometrics and related-article search (Tbahriti et al., 2004) (Tbahriti et al., 2005), information extraction and passage retrieval (Ruch et al., 2005b). In these studies, sentences were selected as the basic classification un</context>
</contexts>
<marker>Mizuta, Korhonen, Mullen, Collier, 2005</marker>
<rawString>Y Mizuta, A Korhonen, T Mullen, and N Collier. 2005. Zone Analysis in Biology Articles as a Basis for Information Extraction. International Journal of Medical Informatics, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Orasan</author>
</authors>
<title>Patterns in Scientific Abstracts.</title>
<date>2001</date>
<booktitle>In Proceedings of Corpus Linguistics,</booktitle>
<pages>433--445</pages>
<contexts>
<context position="4870" citStr="Orasan, 2001" startWordPosition="729" endWordPosition="730"> Linguistics used to develop our improved feedback method. Section 5 reports on results obtained by varying our model and Section 6 contains conclusions on our experiments. 2 Related works Our basic experimental hypothesis is that some particular sentences, selected based on argumentative categories, can be more useful than others to support well-known feedback information retrieval tasks. It means that selecting sentences based on argumentative categories can help focusing on content-bearing sections of scientific articles. 2.1 Argumentation Originally inspired by corpus linguistics studies (Orasan, 2001), which suggests that scientific reports (in chemistry, linguistics, computer sciences, medicine...) exhibit a very regular logical distribution -confirmed by studies conducted on biomedical corpora (Swales, 1990) and by ANSI/ISO professional standards - the argumentative model we experiment is based on four disjunct classes: PURPOSE, METHODS, RESULTS, CONCLUSION. Argumentation belongs to discourse analysisi, with fairly complex computational models such as the implementation of the rhetorical structure theory proposed by (Marcu, 1997), which proposes dozens of rhetorical classes. More recent </context>
</contexts>
<marker>Orasan, 2001</marker>
<rawString>C Orasan. 2001. Patterns in Scientific Abstracts. In Proceedings of Corpus Linguistics, pages 433–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Qiu</author>
<author>H Frei</author>
</authors>
<title>Concept based query expansion.</title>
<date>1993</date>
<pages>160--69</pages>
<publisher>ACM-SIGIR,</publisher>
<contexts>
<context position="9455" citStr="Qiu and Frei, 1993" startWordPosition="1430" endWordPosition="1433">d could 676 be used more than once per query (Aalbersberg, 1992). Buckley et al. (Singhal et al., 1996b) suggested that we could assume, without even looking at them or asking the user, that the top k ranked documents are relevant. Denoted the pseudo-relevance feedback or blindquery expansion approach, this approach is usually effective, at least when handling relatively large text collections. As a third source, we might use large text corpora to derive various term-term relationships, using statistically or information-based measures (Jones, 1971), (Manning and Schutze, 2000). For example, (Qiu and Frei, 1993) suggested that terms to be added to a new query could be extracted from a similarity thesaurus automatically built through calculating co-occurrence frequencies in the search collection. The underlying effect was to add idiosyncratic terms to the underlying document collection, related to the query terms by language use. When using such query expansion approaches, we can assume that the new terms are more appropriate for the retrieval of pertinent items than are lexically or semantically related terms provided by a general thesaurus or dictionary. To complement this global document analysis, </context>
</contexts>
<marker>Qiu, Frei, 1993</marker>
<rawString>Y Qiu and H Frei. 1993. Concept based query expansion. ACM-SIGIR, pages 160–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Robertson</author>
</authors>
<title>On term selection for query expansion.</title>
<date>1990</date>
<journal>Journal of Documentation,</journal>
<volume>46</volume>
<issue>4</issue>
<contexts>
<context position="7526" citStr="Robertson, 1990" startWordPosition="1126" endWordPosition="1127">ested to provide a better match between user information needs and documents, and to increase retrieval effectiveness. The general principle is to expand the query using words or phrases having a similar or related meaning to those appearing in the original request. Various empirical studies based on different IR models or collections have shown that this type of search strategy should usually be effective in enhancing retrieval performance. Scheme propositions such as this should consider the various relationships between words as well as term selection mechanisms and term weighting schemes (Robertson, 1990). The specific answers found to these questions may vary; thus a variety of query expansion approaches were suggested (Efthimiadis, 1996). In a first attempt to find related search terms, we might ask the user to select additional terms to be included in a new query, e.g. (Velez et al., 1997). This could be handled interactively through displaying a ranked list of retrieved items returned by the first query. Voorhees (Voorhees, 1994) proposed basing a scheme based on the WordNet thesaurus. The author demonstrated that terms having a lexicalsemantic relation with the original query words (extra</context>
</contexts>
<marker>Robertson, 1990</marker>
<rawString>S Robertson. 1990. On term selection for query expansion. Journal of Documentation, 46(4):359–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rocchio</author>
</authors>
<title>Relevance feedback in information retrieval</title>
<date>1971</date>
<booktitle>in The SMART Retrieval System - Experiments in Automatic Document Processing.</booktitle>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="8331" citStr="Rocchio, 1971" startWordPosition="1254" endWordPosition="1255">might ask the user to select additional terms to be included in a new query, e.g. (Velez et al., 1997). This could be handled interactively through displaying a ranked list of retrieved items returned by the first query. Voorhees (Voorhees, 1994) proposed basing a scheme based on the WordNet thesaurus. The author demonstrated that terms having a lexicalsemantic relation with the original query words (extracted from a synonym relationship) provided very little improvement (around 1% when compared to the original unexpanded query). As a second strategy for expanding the original query, Rocchio (Rocchio, 1971) proposed accounting for the relevance or irrelevance of top-ranked documents, according to the user’s manual input. In this case, a new query was automatically built in the form of a linear combination of the term included in the previous query and terms automatically extracted from both the relevant documents (with a positive weight) and non-relevant items (with a negative weight). Empirical studies (e.g., (Salton and Buckley, 1990)) demonstrated that such an approach is usually quite effective, and could 676 be used more than once per query (Aalbersberg, 1992). Buckley et al. (Singhal et al</context>
</contexts>
<marker>Rocchio, 1971</marker>
<rawString>J Rocchio. 1971. Relevance feedback in information retrieval in The SMART Retrieval System - Experiments in Automatic Document Processing. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ruch</author>
<author>R Baud</author>
<author>C Chichester</author>
<author>A Geissbiihler</author>
<author>F Lisacek</author>
<author>J Marty</author>
<author>D Rebholz-Schuhmann</author>
<author>I Tbahriti</author>
<author>AL Veuthey</author>
</authors>
<date>2005</date>
<booktitle>Extracting Key Sentences with Latent Argumentative Structuring. In Medical Informatica Europe (MIE),</booktitle>
<pages>835--40</pages>
<contexts>
<context position="6614" citStr="Ruch et al., 2005" startWordPosition="983" endWordPosition="986">ogy, more elaborated models were proposed by (Mizuta and Collier, 2004) (Mizuta et al., 2005) and by (Lisacek et al., 2005) for novelty-detection. (McKnight and Srinivasan, 2003) propose a model very similar to our fourclass model but is inspired by clinical trials. Preliminary applications were proposed for bib&apos;After Aristotle, discourses structured following an appropriate argumentative distribution belong to logics, while ill-defined ones belong to rhetorics. liometrics and related-article search (Tbahriti et al., 2004) (Tbahriti et al., 2005), information extraction and passage retrieval (Ruch et al., 2005b). In these studies, sentences were selected as the basic classification unit in order to avoid as far as possible co-reference issues (Hirst, 1981), which hinder readibity of automatically generated and extracted sentences. 2.2 Q uery expansion Various query expansion techniques have been suggested to provide a better match between user information needs and documents, and to increase retrieval effectiveness. The general principle is to expand the query using words or phrases having a similar or related meaning to those appearing in the original request. Various empirical studies based on di</context>
<context position="17881" citStr="Ruch et al., 2005" startWordPosition="2758" endWordPosition="2761">ms, where strings such as a, which stands for alpha in chemistry and is relevant in biomedical expressions such as vitamin a. 4.2 Argumentative categorizer The argumentative classifier ranks and categorizes abstract sentences as to their argumentative classes. To implement our argumentative categorizer, we rely on four binary Bayesian classifiers, which use lexical features, and a Markov model, which models the logical distribution of the argumentative classes in MEDLINE abstracts. A comprehensive description of the classifier with feature selection and comparative evaluation can be found in (Ruch et al., 2005a) To train the classifier, we obtained 19,555 explicitly structured abstracts from MEDLINE. A (1) 678 Abstract: PURPOSE: The overall prognosis for patients with congestive heart failure is poor. Defining specific populations that might demonstrate improved survival has been difficult [...] PATIENTS AND METHODS: We identified 11 patients with severe congestive heart failure (average ejection fraction 21.9 +/- 4.23% (+/- SD) who developed spontaneous, marked improvement over a period of follow-up lasting 4.25 +/- 1.49 years [...] RESULTS: During the follow-up period, the average ejection fracti</context>
<context position="26028" citStr="Ruch et al., 2005" startWordPosition="4080" endWordPosition="4083"> 0.4999 Table 5: Results without feedback, with Rocchio and with argumentative feedback applied on PURPOSE and CONCLUSION sentences. The number of relevant document for all queries is 1178. be explained by the fact that less textual material is available when a particular class of sentences is selected; therefore the number of words that should be added to the original query is more targeted. From a more general perspective, the importance of CONCLUSION and PURPOSE sentences is consistent with other studies, which aimed at selecting highly content bearing sentences for information extraction (Ruch et al., 2005b). This result is also consistent with the state-of-the-art in automatic summarization, which tends to prefer sentences appearing at the beginning or at the end of documents to generate summaries. 6 Conclusion We have reported on the evaluation of a new linguistically-motivated feedback strategy, which selects highly-content bearing features for expansion based on argumentative criteria. Our simple model is based on four classes, which have been reported very stable in scientific reports of all kinds. Our results suggest that argumentation-driven expansion can improve retrieval effectiveness </context>
</contexts>
<marker>Ruch, Baud, Chichester, Geissbiihler, Lisacek, Marty, Rebholz-Schuhmann, Tbahriti, Veuthey, 2005</marker>
<rawString>P Ruch, R Baud, C Chichester, A Geissbiihler, F Lisacek, J Marty, D Rebholz-Schuhmann, I Tbahriti, and AL Veuthey. 2005a. Extracting Key Sentences with Latent Argumentative Structuring. In Medical Informatica Europe (MIE), pages 835–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ruch</author>
<author>L Perret</author>
<author>J Savoy</author>
</authors>
<title>Features Combination for Extracting Gene Functions from MEDLINE.</title>
<date>2005</date>
<booktitle>In European Colloquium on Information Retrieval (ECIR),</booktitle>
<pages>112--126</pages>
<contexts>
<context position="6614" citStr="Ruch et al., 2005" startWordPosition="983" endWordPosition="986">ogy, more elaborated models were proposed by (Mizuta and Collier, 2004) (Mizuta et al., 2005) and by (Lisacek et al., 2005) for novelty-detection. (McKnight and Srinivasan, 2003) propose a model very similar to our fourclass model but is inspired by clinical trials. Preliminary applications were proposed for bib&apos;After Aristotle, discourses structured following an appropriate argumentative distribution belong to logics, while ill-defined ones belong to rhetorics. liometrics and related-article search (Tbahriti et al., 2004) (Tbahriti et al., 2005), information extraction and passage retrieval (Ruch et al., 2005b). In these studies, sentences were selected as the basic classification unit in order to avoid as far as possible co-reference issues (Hirst, 1981), which hinder readibity of automatically generated and extracted sentences. 2.2 Q uery expansion Various query expansion techniques have been suggested to provide a better match between user information needs and documents, and to increase retrieval effectiveness. The general principle is to expand the query using words or phrases having a similar or related meaning to those appearing in the original request. Various empirical studies based on di</context>
<context position="17881" citStr="Ruch et al., 2005" startWordPosition="2758" endWordPosition="2761">ms, where strings such as a, which stands for alpha in chemistry and is relevant in biomedical expressions such as vitamin a. 4.2 Argumentative categorizer The argumentative classifier ranks and categorizes abstract sentences as to their argumentative classes. To implement our argumentative categorizer, we rely on four binary Bayesian classifiers, which use lexical features, and a Markov model, which models the logical distribution of the argumentative classes in MEDLINE abstracts. A comprehensive description of the classifier with feature selection and comparative evaluation can be found in (Ruch et al., 2005a) To train the classifier, we obtained 19,555 explicitly structured abstracts from MEDLINE. A (1) 678 Abstract: PURPOSE: The overall prognosis for patients with congestive heart failure is poor. Defining specific populations that might demonstrate improved survival has been difficult [...] PATIENTS AND METHODS: We identified 11 patients with severe congestive heart failure (average ejection fraction 21.9 +/- 4.23% (+/- SD) who developed spontaneous, marked improvement over a period of follow-up lasting 4.25 +/- 1.49 years [...] RESULTS: During the follow-up period, the average ejection fracti</context>
<context position="26028" citStr="Ruch et al., 2005" startWordPosition="4080" endWordPosition="4083"> 0.4999 Table 5: Results without feedback, with Rocchio and with argumentative feedback applied on PURPOSE and CONCLUSION sentences. The number of relevant document for all queries is 1178. be explained by the fact that less textual material is available when a particular class of sentences is selected; therefore the number of words that should be added to the original query is more targeted. From a more general perspective, the importance of CONCLUSION and PURPOSE sentences is consistent with other studies, which aimed at selecting highly content bearing sentences for information extraction (Ruch et al., 2005b). This result is also consistent with the state-of-the-art in automatic summarization, which tends to prefer sentences appearing at the beginning or at the end of documents to generate summaries. 6 Conclusion We have reported on the evaluation of a new linguistically-motivated feedback strategy, which selects highly-content bearing features for expansion based on argumentative criteria. Our simple model is based on four classes, which have been reported very stable in scientific reports of all kinds. Our results suggest that argumentation-driven expansion can improve retrieval effectiveness </context>
</contexts>
<marker>Ruch, Perret, Savoy, 2005</marker>
<rawString>P Ruch, L Perret, and J Savoy. 2005b. Features Combination for Extracting Gene Functions from MEDLINE. In European Colloquium on Information Retrieval (ECIR), pages 112– 126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ruch</author>
</authors>
<title>Using contextual spelling correction to improve retrieval effectiveness in degraded text collections. COLING</title>
<date>2002</date>
<contexts>
<context position="2790" citStr="Ruch, 2002" startWordPosition="415" endWordPosition="416">ory) meanings and interpretations (e.g., the word horse as in Trojan horse, light horse, to work like a horse, horse about). In order to obtain better meaning-based matches between queries and documents, various propositions have been suggested, usually without giving any consideration to the underlying domain. During our participation in different international evaluation campaigns such as the TREC Genomics track (Hersh, 2005), the BioCreative initiative (Hirschman et al., 2005), as well as in our attempts to deliver advanced search tools for biologists (Ruch, 2006) and healthcare providers (Ruch, 2002) (Ruch, 2004), we were more concerned with domain-specific information retrieval in which systems must return a ranked list of MEDLINE records in response to an expert’s information request. This involved a set of available queries describing typical search interests, in which gene, protein names, and diseases were often essential for an effective retrieval. Biomedical publications however tend to generate new information very rapidly and also use a wide variation in terminology, thus leading to the current situation whereby a large number of names, symbols and synonyms are used to denote the </context>
</contexts>
<marker>Ruch, 2002</marker>
<rawString>P Ruch. 2002. Using contextual spelling correction to improve retrieval effectiveness in degraded text collections. COLING 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ruch</author>
</authors>
<title>Query translation by text categorization.</title>
<date>2004</date>
<journal>COLING</journal>
<contexts>
<context position="2803" citStr="Ruch, 2004" startWordPosition="417" endWordPosition="418"> and interpretations (e.g., the word horse as in Trojan horse, light horse, to work like a horse, horse about). In order to obtain better meaning-based matches between queries and documents, various propositions have been suggested, usually without giving any consideration to the underlying domain. During our participation in different international evaluation campaigns such as the TREC Genomics track (Hersh, 2005), the BioCreative initiative (Hirschman et al., 2005), as well as in our attempts to deliver advanced search tools for biologists (Ruch, 2006) and healthcare providers (Ruch, 2002) (Ruch, 2004), we were more concerned with domain-specific information retrieval in which systems must return a ranked list of MEDLINE records in response to an expert’s information request. This involved a set of available queries describing typical search interests, in which gene, protein names, and diseases were often essential for an effective retrieval. Biomedical publications however tend to generate new information very rapidly and also use a wide variation in terminology, thus leading to the current situation whereby a large number of names, symbols and synonyms are used to denote the same concepts</context>
<context position="16008" citStr="Ruch, 2004" startWordPosition="2462" endWordPosition="2463"> score obtained by the engine before the feedback step. This measure is necessary to verify that feedback is useful for querying the OHSUMED collection and to establish a strong baseline. While Rocchio selects the features to be added to the original queries based on pure statistical analysis, we propose to base our feature expansion also on argumentative criteria. That is, we overweight features appearing in sentences classified in a particular argumentative category by the argumentative categorizer. 4.1 Retrieval engine and indexing units The easyIR system is a standard vector-space engine (Ruch, 2004), which computes stateof-the-art tf.idf and probabilistic weighting schema. All experiments were conducted with pivoted normalization (Singhal et al., 1996a), which has recently shown some effectiveness on MEDLINE corpora (Aronson et al., 2006). Query and document weighings are provided in Equation (1): the dtu formula is applied to the documents, while the dtn formula is applied to the query; t the number of indexing terms, dfj the number of documents in which the term tj; pivot and slope are constants (fixed at pivot = 0.14, slope = 146). (Ln(Ln(t fij)+1)+1) ·idfj dtu: wij = (1−slope)·pivot+</context>
</contexts>
<marker>Ruch, 2004</marker>
<rawString>P Ruch. 2004. Query translation by text categorization. COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ruch</author>
</authors>
<title>Automatic Assignment of Biomedical Categories: Toward a Generic Approach.</title>
<date>2006</date>
<journal>Bioinformatics,</journal>
<volume>6</volume>
<contexts>
<context position="2752" citStr="Ruch, 2006" startWordPosition="409" endWordPosition="410">have several (and sometimes contradictory) meanings and interpretations (e.g., the word horse as in Trojan horse, light horse, to work like a horse, horse about). In order to obtain better meaning-based matches between queries and documents, various propositions have been suggested, usually without giving any consideration to the underlying domain. During our participation in different international evaluation campaigns such as the TREC Genomics track (Hersh, 2005), the BioCreative initiative (Hirschman et al., 2005), as well as in our attempts to deliver advanced search tools for biologists (Ruch, 2006) and healthcare providers (Ruch, 2002) (Ruch, 2004), we were more concerned with domain-specific information retrieval in which systems must return a ranked list of MEDLINE records in response to an expert’s information request. This involved a set of available queries describing typical search interests, in which gene, protein names, and diseases were often essential for an effective retrieval. Biomedical publications however tend to generate new information very rapidly and also use a wide variation in terminology, thus leading to the current situation whereby a large number of names, symbol</context>
</contexts>
<marker>Ruch, 2006</marker>
<rawString>P Ruch. 2006. Automatic Assignment of Biomedical Categories: Toward a Generic Approach. Bioinformatics, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Improving retrieval performance by relevance feedback.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>4</issue>
<contexts>
<context position="8769" citStr="Salton and Buckley, 1990" startWordPosition="1321" endWordPosition="1324">m relationship) provided very little improvement (around 1% when compared to the original unexpanded query). As a second strategy for expanding the original query, Rocchio (Rocchio, 1971) proposed accounting for the relevance or irrelevance of top-ranked documents, according to the user’s manual input. In this case, a new query was automatically built in the form of a linear combination of the term included in the previous query and terms automatically extracted from both the relevant documents (with a positive weight) and non-relevant items (with a negative weight). Empirical studies (e.g., (Salton and Buckley, 1990)) demonstrated that such an approach is usually quite effective, and could 676 be used more than once per query (Aalbersberg, 1992). Buckley et al. (Singhal et al., 1996b) suggested that we could assume, without even looking at them or asking the user, that the top k ranked documents are relevant. Denoted the pseudo-relevance feedback or blindquery expansion approach, this approach is usually effective, at least when handling relatively large text collections. As a third source, we might use large text corpora to derive various term-term relationships, using statistically or information-based </context>
</contexts>
<marker>Salton, Buckley, 1990</marker>
<rawString>G Salton and C Buckley. 1990. Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science, 41(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Savoy</author>
</authors>
<title>Bibliographic database access using free-text and controlled vocabulary: An evaluation.</title>
<date>2005</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>41--4</pages>
<contexts>
<context position="12250" citStr="Savoy, 2005" startWordPosition="1869" endWordPosition="1870"> context of the TREC Genomics track, using the OHSUGEN collection (Hersh, 2005), show that neither blind expansion (Rocchio), nor domain-specific query expansion (thesaurus-based Gene and Protein expansion) seem appropriate to improve retrieval effectiveness (Aronson et al., 2006) (Abdou et al., 2006). 3 Data and metrics To test our hypothesis, we used the OHSUMED collection (Hersh et al., 1994), originally developed for the TREC topic detection track, which is the most popular information retrieval collection for evaluating information search in library corpora. Alternative collections (cf. (Savoy, 2005)), such as the French Amaryllis collection, are usually smaller and/or not appropriate to evaluate our argumentative classifier, which can only process English documents. Other MEDLINE collections, which can be regarded as similar in size or larger, such as the TREC Genomics 2004 and 2005 collections are unfortunately more domain-specific since information requests in these collection are usually targeting a particular gene or gene product. Among the 348,566 MEDLINE citations of the OHSUMED collection, we use the 233,455 records provided with an abstract. An example of a MEDLINE citation is gi</context>
</contexts>
<marker>Savoy, 2005</marker>
<rawString>J Savoy. 2005. Bibliographic database access using free-text and controlled vocabulary: An evaluation. Information Processing and Management, 41(4):873–890.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Singhal</author>
<author>C Buckley</author>
<author>M Mitra</author>
</authors>
<title>Pivoted document length normalization.</title>
<date>1996</date>
<pages>21--29</pages>
<publisher>ACM-SIGIR,</publisher>
<contexts>
<context position="8938" citStr="Singhal et al., 1996" startWordPosition="1350" endWordPosition="1353">occhio, 1971) proposed accounting for the relevance or irrelevance of top-ranked documents, according to the user’s manual input. In this case, a new query was automatically built in the form of a linear combination of the term included in the previous query and terms automatically extracted from both the relevant documents (with a positive weight) and non-relevant items (with a negative weight). Empirical studies (e.g., (Salton and Buckley, 1990)) demonstrated that such an approach is usually quite effective, and could 676 be used more than once per query (Aalbersberg, 1992). Buckley et al. (Singhal et al., 1996b) suggested that we could assume, without even looking at them or asking the user, that the top k ranked documents are relevant. Denoted the pseudo-relevance feedback or blindquery expansion approach, this approach is usually effective, at least when handling relatively large text collections. As a third source, we might use large text corpora to derive various term-term relationships, using statistically or information-based measures (Jones, 1971), (Manning and Schutze, 2000). For example, (Qiu and Frei, 1993) suggested that terms to be added to a new query could be extracted from a similari</context>
<context position="16163" citStr="Singhal et al., 1996" startWordPosition="2480" endWordPosition="2483">ction and to establish a strong baseline. While Rocchio selects the features to be added to the original queries based on pure statistical analysis, we propose to base our feature expansion also on argumentative criteria. That is, we overweight features appearing in sentences classified in a particular argumentative category by the argumentative categorizer. 4.1 Retrieval engine and indexing units The easyIR system is a standard vector-space engine (Ruch, 2004), which computes stateof-the-art tf.idf and probabilistic weighting schema. All experiments were conducted with pivoted normalization (Singhal et al., 1996a), which has recently shown some effectiveness on MEDLINE corpora (Aronson et al., 2006). Query and document weighings are provided in Equation (1): the dtu formula is applied to the documents, while the dtn formula is applied to the query; t the number of indexing terms, dfj the number of documents in which the term tj; pivot and slope are constants (fixed at pivot = 0.14, slope = 146). (Ln(Ln(t fij)+1)+1) ·idfj dtu: wij = (1−slope)·pivot+slope·nti dtn: wij = idfj · (Ln(Ln(tfif) + 1) + 1) As already observed in several linguisticallymotivated studies (Hull, 1996), we observe that common stem</context>
</contexts>
<marker>Singhal, Buckley, Mitra, 1996</marker>
<rawString>A Singhal, C Buckley, and M Mitra. 1996a. Pivoted document length normalization. ACM-SIGIR, pages 21–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Buckley A Singhal</author>
<author>M Mitra</author>
<author>G Salton</author>
</authors>
<title>New retrieval approaches using smart.</title>
<date>1996</date>
<booktitle>In Proceedings of TREC-4.</booktitle>
<contexts>
<context position="8938" citStr="Singhal et al., 1996" startWordPosition="1350" endWordPosition="1353">occhio, 1971) proposed accounting for the relevance or irrelevance of top-ranked documents, according to the user’s manual input. In this case, a new query was automatically built in the form of a linear combination of the term included in the previous query and terms automatically extracted from both the relevant documents (with a positive weight) and non-relevant items (with a negative weight). Empirical studies (e.g., (Salton and Buckley, 1990)) demonstrated that such an approach is usually quite effective, and could 676 be used more than once per query (Aalbersberg, 1992). Buckley et al. (Singhal et al., 1996b) suggested that we could assume, without even looking at them or asking the user, that the top k ranked documents are relevant. Denoted the pseudo-relevance feedback or blindquery expansion approach, this approach is usually effective, at least when handling relatively large text collections. As a third source, we might use large text corpora to derive various term-term relationships, using statistically or information-based measures (Jones, 1971), (Manning and Schutze, 2000). For example, (Qiu and Frei, 1993) suggested that terms to be added to a new query could be extracted from a similari</context>
<context position="16163" citStr="Singhal et al., 1996" startWordPosition="2480" endWordPosition="2483">ction and to establish a strong baseline. While Rocchio selects the features to be added to the original queries based on pure statistical analysis, we propose to base our feature expansion also on argumentative criteria. That is, we overweight features appearing in sentences classified in a particular argumentative category by the argumentative categorizer. 4.1 Retrieval engine and indexing units The easyIR system is a standard vector-space engine (Ruch, 2004), which computes stateof-the-art tf.idf and probabilistic weighting schema. All experiments were conducted with pivoted normalization (Singhal et al., 1996a), which has recently shown some effectiveness on MEDLINE corpora (Aronson et al., 2006). Query and document weighings are provided in Equation (1): the dtu formula is applied to the documents, while the dtn formula is applied to the query; t the number of indexing terms, dfj the number of documents in which the term tj; pivot and slope are constants (fixed at pivot = 0.14, slope = 146). (Ln(Ln(t fij)+1)+1) ·idfj dtu: wij = (1−slope)·pivot+slope·nti dtn: wij = idfj · (Ln(Ln(tfif) + 1) + 1) As already observed in several linguisticallymotivated studies (Hull, 1996), we observe that common stem</context>
</contexts>
<marker>Singhal, Mitra, Salton, 1996</marker>
<rawString>C Buckley A Singhal, M Mitra, and G Salton. 1996b. New retrieval approaches using smart. In Proceedings of TREC-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strzalkowski</author>
<author>G Stein</author>
<author>G Bowden Wise</author>
<author>J Perez Carballo</author>
<author>P Tapanainen</author>
<author>T Jarvinen</author>
<author>A Voutilainen</author>
<author>J Karlgren</author>
</authors>
<title>Natural language information retrieval: TREC-7 report.</title>
<date>1998</date>
<booktitle>In Text REtrieval Conference,</booktitle>
<pages>164--173</pages>
<contexts>
<context position="26882" citStr="Strzalkowski et al., 1998" startWordPosition="4205" endWordPosition="4208">on the evaluation of a new linguistically-motivated feedback strategy, which selects highly-content bearing features for expansion based on argumentative criteria. Our simple model is based on four classes, which have been reported very stable in scientific reports of all kinds. Our results suggest that argumentation-driven expansion can improve retrieval effectiveness of search engines by more than 40%. The proposed methods open new research directions and are generally promising for natural language processing applied to information retrieval, whose positive impact is still to be confirmed (Strzalkowski et al., 1998). Finally, the proposed methods are important from a theoretical perspective, if we consider 680 that it initiates a genre-specific paradigm as opposed to the usual information retrieval typology, which distinguishes between domainspecific and domain-independent approaches. Acknowledgements The first author was supported by a visiting faculty grant (ORAU) at the Lister Hill Center of the National Library of Medicine in 2005. We would like to thank Dina Demner-Fushman, Susanne M. Humphrey, Jimmy Lin, Hongfang Liu, Miguel E. Ruiz, Lawrence H. Smith, Lorraine K. Tanabe, W. John Wilbur for the fru</context>
</contexts>
<marker>Strzalkowski, Stein, Wise, Carballo, Tapanainen, Jarvinen, Voutilainen, Karlgren, 1998</marker>
<rawString>T Strzalkowski, G Stein, G Bowden Wise, J Perez Carballo, P Tapanainen, T Jarvinen, A Voutilainen, and J Karlgren. 1998. Natural language information retrieval: TREC-7 report. In Text REtrieval Conference, pages 164–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Swales</author>
</authors>
<title>Genre Analysis: English in Academic and Research Settings.</title>
<date>1990</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5083" citStr="Swales, 1990" startWordPosition="757" endWordPosition="758">ypothesis is that some particular sentences, selected based on argumentative categories, can be more useful than others to support well-known feedback information retrieval tasks. It means that selecting sentences based on argumentative categories can help focusing on content-bearing sections of scientific articles. 2.1 Argumentation Originally inspired by corpus linguistics studies (Orasan, 2001), which suggests that scientific reports (in chemistry, linguistics, computer sciences, medicine...) exhibit a very regular logical distribution -confirmed by studies conducted on biomedical corpora (Swales, 1990) and by ANSI/ISO professional standards - the argumentative model we experiment is based on four disjunct classes: PURPOSE, METHODS, RESULTS, CONCLUSION. Argumentation belongs to discourse analysisi, with fairly complex computational models such as the implementation of the rhetorical structure theory proposed by (Marcu, 1997), which proposes dozens of rhetorical classes. More recent advances were applied to document summarization. Of particular interest for our approach, Teufel and Moens (Teufel and Moens, 1999) propose using a list of manually crafted triggers (using both words and expressio</context>
</contexts>
<marker>Swales, 1990</marker>
<rawString>J Swales. 1990. Genre Analysis: English in Academic and Research Settings. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tbahriti</author>
<author>C Chichester</author>
<author>F Lisacek</author>
<author>P Ruch</author>
</authors>
<title>Using Argumention to Retrieve Articles with Similar Citations from MEDLINE.</title>
<date>2004</date>
<booktitle>Proceedings of the joint NLPBA/BioNLP Workshop on Natural Language for Biomedical Applications.</booktitle>
<contexts>
<context position="6525" citStr="Tbahriti et al., 2004" startWordPosition="969" endWordPosition="972">E, METHOD, RESULT, and CONCLUSION. More recently and for knowledge discovery in molecular biology, more elaborated models were proposed by (Mizuta and Collier, 2004) (Mizuta et al., 2005) and by (Lisacek et al., 2005) for novelty-detection. (McKnight and Srinivasan, 2003) propose a model very similar to our fourclass model but is inspired by clinical trials. Preliminary applications were proposed for bib&apos;After Aristotle, discourses structured following an appropriate argumentative distribution belong to logics, while ill-defined ones belong to rhetorics. liometrics and related-article search (Tbahriti et al., 2004) (Tbahriti et al., 2005), information extraction and passage retrieval (Ruch et al., 2005b). In these studies, sentences were selected as the basic classification unit in order to avoid as far as possible co-reference issues (Hirst, 1981), which hinder readibity of automatically generated and extracted sentences. 2.2 Q uery expansion Various query expansion techniques have been suggested to provide a better match between user information needs and documents, and to increase retrieval effectiveness. The general principle is to expand the query using words or phrases having a similar or related </context>
</contexts>
<marker>Tbahriti, Chichester, Lisacek, Ruch, 2004</marker>
<rawString>I Tbahriti, C Chichester, F Lisacek, and P Ruch. 2004. Using Argumention to Retrieve Articles with Similar Citations from MEDLINE. Proceedings of the joint NLPBA/BioNLP Workshop on Natural Language for Biomedical Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tbahriti</author>
<author>C Chichester</author>
<author>F Lisacek</author>
<author>P Ruch</author>
</authors>
<title>Using Argumentation to Retrieve Articles with Similar Citations: an Inquiry into Improving Related Articles Search in the MEDLINE Digital Library.</title>
<date>2005</date>
<journal>International Journal of Medical Informatics,</journal>
<note>to appear.</note>
<contexts>
<context position="6549" citStr="Tbahriti et al., 2005" startWordPosition="973" endWordPosition="976">ONCLUSION. More recently and for knowledge discovery in molecular biology, more elaborated models were proposed by (Mizuta and Collier, 2004) (Mizuta et al., 2005) and by (Lisacek et al., 2005) for novelty-detection. (McKnight and Srinivasan, 2003) propose a model very similar to our fourclass model but is inspired by clinical trials. Preliminary applications were proposed for bib&apos;After Aristotle, discourses structured following an appropriate argumentative distribution belong to logics, while ill-defined ones belong to rhetorics. liometrics and related-article search (Tbahriti et al., 2004) (Tbahriti et al., 2005), information extraction and passage retrieval (Ruch et al., 2005b). In these studies, sentences were selected as the basic classification unit in order to avoid as far as possible co-reference issues (Hirst, 1981), which hinder readibity of automatically generated and extracted sentences. 2.2 Q uery expansion Various query expansion techniques have been suggested to provide a better match between user information needs and documents, and to increase retrieval effectiveness. The general principle is to expand the query using words or phrases having a similar or related meaning to those appeari</context>
</contexts>
<marker>Tbahriti, Chichester, Lisacek, Ruch, 2005</marker>
<rawString>I Tbahriti, C Chichester, F Lisacek, and P Ruch. 2005. Using Argumentation to Retrieve Articles with Similar Citations: an Inquiry into Improving Related Articles Search in the MEDLINE Digital Library. International Journal of Medical Informatics, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Argumentative Classification of Extracted Sentences as a First Step Towards Flexible Abstracting. Advances in Automatic Text Summarization,</title>
<date>1999</date>
<pages>155--171</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="5601" citStr="Teufel and Moens, 1999" startWordPosition="832" endWordPosition="835"> a very regular logical distribution -confirmed by studies conducted on biomedical corpora (Swales, 1990) and by ANSI/ISO professional standards - the argumentative model we experiment is based on four disjunct classes: PURPOSE, METHODS, RESULTS, CONCLUSION. Argumentation belongs to discourse analysisi, with fairly complex computational models such as the implementation of the rhetorical structure theory proposed by (Marcu, 1997), which proposes dozens of rhetorical classes. More recent advances were applied to document summarization. Of particular interest for our approach, Teufel and Moens (Teufel and Moens, 1999) propose using a list of manually crafted triggers (using both words and expressions such as we argued, in this article, the paper is an attempt to, we aim at, etc.) to automatically structure scientific articles into a lighter model, with only seven categories: BACKGROUND, TOPIC, RELATED WORK, PURPOSE, METHOD, RESULT, and CONCLUSION. More recently and for knowledge discovery in molecular biology, more elaborated models were proposed by (Mizuta and Collier, 2004) (Mizuta et al., 2005) and by (Lisacek et al., 2005) for novelty-detection. (McKnight and Srinivasan, 2003) propose a model very simi</context>
</contexts>
<marker>Teufel, Moens, 1999</marker>
<rawString>S Teufel and M Moens. 1999. Argumentative Classification of Extracted Sentences as a First Step Towards Flexible Abstracting. Advances in Automatic Text Summarization, MIT Press, pages 155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Velez</author>
<author>R Weiss</author>
<author>M Sheldon</author>
<author>D Gifford</author>
</authors>
<title>Fast and effective query refinement.</title>
<date>1997</date>
<booktitle>In ACM SIGIR,</booktitle>
<pages>6--15</pages>
<contexts>
<context position="7819" citStr="Velez et al., 1997" startWordPosition="1174" endWordPosition="1177">es based on different IR models or collections have shown that this type of search strategy should usually be effective in enhancing retrieval performance. Scheme propositions such as this should consider the various relationships between words as well as term selection mechanisms and term weighting schemes (Robertson, 1990). The specific answers found to these questions may vary; thus a variety of query expansion approaches were suggested (Efthimiadis, 1996). In a first attempt to find related search terms, we might ask the user to select additional terms to be included in a new query, e.g. (Velez et al., 1997). This could be handled interactively through displaying a ranked list of retrieved items returned by the first query. Voorhees (Voorhees, 1994) proposed basing a scheme based on the WordNet thesaurus. The author demonstrated that terms having a lexicalsemantic relation with the original query words (extracted from a synonym relationship) provided very little improvement (around 1% when compared to the original unexpanded query). As a second strategy for expanding the original query, Rocchio (Rocchio, 1971) proposed accounting for the relevance or irrelevance of top-ranked documents, according</context>
</contexts>
<marker>Velez, Weiss, Sheldon, Gifford, 1997</marker>
<rawString>B Velez, R Weiss, M Sheldon, and D Gifford. 1997. Fast and effective query refinement. In ACM SIGIR, pages 6–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Query expansion using lexical-semantic relations.</title>
<date>1994</date>
<booktitle>In ACM SIGIR,</booktitle>
<pages>61--69</pages>
<contexts>
<context position="7963" citStr="Voorhees, 1994" startWordPosition="1197" endWordPosition="1198">rmance. Scheme propositions such as this should consider the various relationships between words as well as term selection mechanisms and term weighting schemes (Robertson, 1990). The specific answers found to these questions may vary; thus a variety of query expansion approaches were suggested (Efthimiadis, 1996). In a first attempt to find related search terms, we might ask the user to select additional terms to be included in a new query, e.g. (Velez et al., 1997). This could be handled interactively through displaying a ranked list of retrieved items returned by the first query. Voorhees (Voorhees, 1994) proposed basing a scheme based on the WordNet thesaurus. The author demonstrated that terms having a lexicalsemantic relation with the original query words (extracted from a synonym relationship) provided very little improvement (around 1% when compared to the original unexpanded query). As a second strategy for expanding the original query, Rocchio (Rocchio, 1971) proposed accounting for the relevance or irrelevance of top-ranked documents, according to the user’s manual input. In this case, a new query was automatically built in the form of a linear combination of the term included in the p</context>
</contexts>
<marker>Voorhees, 1994</marker>
<rawString>E Voorhees. 1994. Query expansion using lexical-semantic relations. In ACM SIGIR, pages 61–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zobel</author>
</authors>
<title>How reliable are large-scale information retrieval experiments?</title>
<date>1998</date>
<pages>307--314</pages>
<publisher>ACMSIGIR,</publisher>
<contexts>
<context position="22261" citStr="Zobel, 1998" startWordPosition="3493" endWordPosition="3494">ategory is attributed to each sentence, which makes the decision model binary. Table 4 shows the output of the argumentative classifier when applied to an abstract. To determine the respective value of each argumentative contents for feedback, the argumentative categorizer parses each top-ranked abstract. These abstracts are then used to generate four groups of sentences. Each group corresponds to a unique argumentative class. Each argumentative index contains sentences classified in one of four argumentative classes. Because argumen2Tests are computed using a non-parametric signed test, cf. (Zobel, 1998) for more details. 679 CONCLUSION (00160116) The highly favorable pathologic stage (RI-RII, 58%) and the fact that the majority of patients were alive and disease-free suggested a more favorable prognosis for this type of renal cell carcinoma. METHODS (00160119) Tumors were classified according to well-established histologic criteria to determine stage of disease; the system proposed by Robson was used. METHODS (00162303) Of 250 renal cell carcinomas analyzed, 36 were classified as chromophobe renal cell carcinoma, representing 14% of the group studied. PURPOSE (00156456) In this study, we ana</context>
</contexts>
<marker>Zobel, 1998</marker>
<rawString>J Zobel. 1998. How reliable are large-scale information retrieval experiments? ACMSIGIR, pages 307–314.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>