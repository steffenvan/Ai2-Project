<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.776832">
Left-to-Right Target Generation for Hierarchical Phrase-based
Translation
</title>
<author confidence="0.49383">
Taro Watanabe Hajime Tsukada Hideki Isozaki
</author>
<address confidence="0.5399275">
2-4, Hikaridai, Seika-cho, Soraku-gun,
Kyoto, JAPAN 619-0237
</address>
<email confidence="0.994516">
{taro,tsukada,isozaki}@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.994769" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999850571428571">
We present a hierarchical phrase-based
statistical machine translation in which a
target sentence is efficiently generated in
left-to-right order. The model is a class
of synchronous-CFG with a Greibach Nor-
mal Form-like structure for the projected
production rule: The paired target-side
of a production rule takes a phrase pre-
fixed form. The decoder for the target-
normalized form is based on an Early-
style top down parser on the source side.
The target-normalized form coupled with
our top down parser implies a left-to-
right generation of translations which en-
ables us a straightforward integration with
ngram language models. Our model was
experimented on a Japanese-to-English
newswire translation task, and showed sta-
tistically significant performance improve-
ments against a phrase-based translation
system.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995082">
In a classical statistical machine translation, a for-
eign language sentence f1J = f1, f2, ...fJ is trans-
lated into another language, i.e. English, eI1 =
e1, e2,..., eI by seeking a maximum likely solution
of:
</bodyText>
<equation confidence="0.99341625">
ˆeI 1= argmax Pr(eI1|f1J) (1)
eI 1
= argmax Pr(f 1 J|eI 1)Pr(eI 1) (2)
eI 1
</equation>
<bodyText confidence="0.9999003">
The source channel approach in Equation 2 inde-
pendently decomposes translation knowledge into
a translation model and a language model, respec-
tively (Brown et al., 1993). The former repre-
sents the correspondence between two languages
and the latter contributes to the fluency of English.
In the state of the art statistical machine transla-
tion, the posterior probability Pr(eI1 |f1J) is directly
maximized using a log-linear combination of fea-
ture functions (Och and Ney, 2002):
</bodyText>
<table confidence="0.751155">
ˆeI 1= argmax ~PM �
eI exp m=1 Amhm(eI 1, f1 J )
1 3
M
Pe′I′ exp Pm=1 Amhm(e′I′
1 ,f1J)
</table>
<bodyText confidence="0.999988961538462">
where hm(eI1, f1J) is a feature function, such as
a ngram language model or a translation model.
When decoding, the denominator is dropped since
it depends only on f1J . Feature function scaling
factors Am are optimized based on a maximum
likely approach (Och and Ney, 2002) or on a direct
error minimization approach (Och, 2003). This
modeling allows the integration of various fea-
ture functions depending on the scenario of how
a translation is constituted.
A phrase-based translation model is one of the
modern approaches which exploits a phrase, a
contiguous sequence of words, as a unit of transla-
tion (Koehn et al., 2003; Zens and Ney, 2003; Till-
man, 2004). The idea is based on a word-based
source channel modeling of Brown et al. (1993):
It assumes that eI1 is segmented into a sequence
of K phrases ¯eK1 . Each phrase ¯ek is transformed
into ¯fk. The translated phrases are reordered to
form f1J. One of the benefits of the modeling is
that the phrase translation unit preserves localized
word reordering. However, it cannot hypothesize
a long-distance reordering required for linguisti-
cally divergent language pairs. For instance, when
translating Japanese to English, a Japanese SOV
structure has to be reordered to match with an En-
</bodyText>
<page confidence="0.954497">
777
</page>
<note confidence="0.5379675">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 777–784,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999472352112676">
glish SVO structure. Such a sentence-wise move-
ment cannot be realized within the phrase-based
modeling.
Chiang (2005) introduced a hierarchical phrase-
based translation model that combined the
strength of the phrase-based approach and a
synchronous-CFG formalism (Aho and Ullman,
1969): A rewrite system initiated from a start
symbol which synchronously rewrites paired non-
terminals. Their translation model is a binarized
synchronous-CFG, or a rank-2 of synchronous-
CFG, in which the right-hand side of a production
rule contains at most two non-terminals. The form
can be regarded as a phrase translation pair with
at most two holes instantiated with other phrases.
The hierarchically combined phrases provide a
sort of reordering constraints that is not directly
modeled by a phrase-based model.
Rules are induced from a bilingual corpus with-
out linguistic clues first by extracting phrase trans-
lation pairs, and then by generalizing extracted
phrases with holes (Chiang, 2005). Even in a
phrase-based model, the number of phrases ex-
tracted from a bilingual corpus is quadratic to
the length of bilingual sentences. The grammar
size for the hierarchical phrase-based model will
be further exploded, since there exists numerous
combination of inserting holes to each rule. The
spuriously increasing grammar size will be prob-
lematic for decoding without certain heuristics,
such as a length based thresholding.
The integration with a ngram language model
further increases the cost of decoding especially
when incorporating a higher order ngram, such as
5-gram. In the hierarchical phrase-based model
(Chiang, 2005), and an inversion transduction
grammar (ITG) (Wu, 1997), the problem is re-
solved by restricting to a binarized form where at
most two non-terminals are allowed in the right-
hand side. However, Huang et al. (2005) reported
that the computational complexity for decoding
amounted to O(J3+3(n−1)) with n-gram even using
a hook technique. The complexity lies in mem-
orizing the ngram’s context for each constituent.
The order of ngram would be a dominant factor
for higher order ngrams.
As an alternative to a binarized form, we
present a target-normalized hierarchical phrase-
based translation model. The model is a class of a
hierarchical phrase-based model, but constrained
so that the English part of the right-hand side
is restricted to a Greibach Normal Form (GNF)-
like structure: A contiguous sequence of termi-
nals, or a phrase, is followed by a string of non-
terminals. The target-normalized form reduces the
number of rules extracted from a bilingual corpus,
but still preserves the strength of the phrase-based
approach. An integration with ngram language
model is straightforward, since the model gener-
ates a translation in left-to-right order. Our de-
coder is based on an Earley-style top down pars-
ing on the foreign language side. The projected
English-side is generated in left-to-right order syn-
chronized with the derivation of the foreign lan-
guage side. The decoder’s implementation is taken
after a decoder for an existing phrase-based model
with a simple modification to account for produc-
tion rules. Experimental results on a Japanese-to-
English newswire translation task showed signif-
icant improvement against a phrase-based model-
ing.
</bodyText>
<sectionHeader confidence="0.993744" genericHeader="method">
2 Translation Model
</sectionHeader>
<bodyText confidence="0.999493">
A weighted synchronous-CFG is a rewrite system
consisting of production rules whose right-hand
side is paired (Aho and Ullman, 1969):
</bodyText>
<equation confidence="0.995752">
X ← (y, α, ∼) (4)
</equation>
<bodyText confidence="0.992648230769231">
where X is a non-terminal, y and α are strings of
terminals and non-terminals. For notational sim-
plicity, we assume that y and α correspond to the
foreign language side and the English side, re-
spectively. ∼ is a one-to-one correspondence for
the non-terminals appeared in y and α. Starting
from an initial non-terminal, each rule rewrites
non-terminals in y and α that are associated with
∼.
Chiang (2005) proposed a hierarchical phrase-
based translation model, a binary synchronous-
CFG, which restricted the form of production rules
as follows:
</bodyText>
<listItem confidence="0.977672571428572">
• Only two types of non-terminals allowed: S
and X.
• Both of the strings y and α must contain at
least one terminal item.
• Rules may have at most two non-terminals
but non-terminals cannot be adjacent for the
foreign language side y.
</listItem>
<bodyText confidence="0.98141">
The production rules are induced from a bilingual
corpus with the help of word alignments. To al-
leviate a data sparseness problem, glue rules are
</bodyText>
<page confidence="0.996116">
778
</page>
<bodyText confidence="0.9979115">
added that prefer combining hierarchical phrases
in a serial manner:
</bodyText>
<equation confidence="0.99782475">
D E
S � S 1 X2,S 1 X2 (5)
D E
S � X1, X1 (6)
</equation>
<bodyText confidence="0.997163833333333">
where boxed indices indicate non-terminal’s link-
ages represented in —.
Our model is based on Chiang (2005)’s frame-
work, but further restricts the form of production
rules so that the aligned right-hand side α follows
a GNF-like structure:
</bodyText>
<equation confidence="0.9926645">
D E
X (-- γ, ¯bβ, — (7)
</equation>
<bodyText confidence="0.999947428571429">
where b¯ is a string of terminals, or a phrase,
and beta is a (possibly empty) string of non-
terminals. The foreign language at right-hand side
γ still takes an arbitrary string of terminals and
non-terminals. The use of a phrase b¯ as a pre-
fix keeps the strength of the phrase-base frame-
work. A contiguous English side coupled with
a (possibly) discontiguous foreign language side
preserves a phrase-bounded local word reordering.
At the same time, the target-normalized frame-
work still combines phrases hierarchically in a re-
stricted manner.
The target-normalized form can be regarded as
a type of rule in which certain non-terminals are
always instantiated with phrase translation pairs.
Thus, we will be able to reduce the number of rules
induced from a bilingual corpus, which, in turn,
help reducing the decoding complexity.
The contiguous phrase-prefixed form generates
English in left-to-right order. Therefore, a decoder
can easily hypothesize a derivation tree integrated
with a ngram language model even with higher or-
der.
Note that we do not imply arbitrary
synchronous-CFGs are transformed into the
target normalized form. The form simply restricts
the grammar extracted from a bilingual corpus
explained in the next section.
</bodyText>
<subsectionHeader confidence="0.994192">
2.1 Rule Extraction
</subsectionHeader>
<bodyText confidence="0.991013388888889">
We present an algorithm to extract production
rules from a bilingual corpus. The procedure is
based on those for the hierarchical phrase-based
translation model (Chiang, 2005).
First, a bilingual corpus is annotated with word
alignments using the method of Koehn et al.
(2003). Many-to-many word alignments are in-
duced by running a one-to-many word alignment
model, such as GIZA++ (Och and Ney, 2003), in
both directions and by combining the results based
on a heuristic (Koehn et al., 2003).
Second, phrase translation pairs are extracted
from the word alignment corpus (Koehn et al.,
2003). The method exhaustively extracts phrase
pairs ( f j+m
j , ei+n
i ) from a sentence pair (f1J , eI1) that
do not violate the word alignment constraints a:
</bodyText>
<equation confidence="0.8808335">
3(i′, j′) E a : j′ E [j, j + m], i′ E [i, i + n]
∄(i′, j′) E a : j′ E [j, j + m],i′ 0 [i,i + n]
</equation>
<bodyText confidence="0.91119275">
∄(i′, j′) E a : j′ 0 [j, j + m], i′ E [i, i + n]
Third, based on the extracted phrases, production
rules are accumulated by computing the “holes”
for contiguous phrases (Chiang, 2005):
</bodyText>
<listItem confidence="0.99939325">
1. A phrase pair (f¯, ¯e) constitutes a rule
X —� Df¯,0
e)
2. A rule X —� (γ, α) and a phrase pair (f¯, ¯e) s.t.
</listItem>
<equation confidence="0.745134">
γ = γ′
fγ′′ and α = ¯e′¯eβ constitutes a rule
D E
X � γ′ Xk γ′′, ¯e′ Xk β
</equation>
<listItem confidence="0.940222666666667">
Following Chiang (2005), we applied constraints
when inducing rules with non-terminals:
• At least one foreign word must be aligned to
an English word.
• Adjacent non-terminals are not allowed for
the foreign language side.
</listItem>
<subsectionHeader confidence="0.993069">
2.2 Phrase-based Rules
</subsectionHeader>
<bodyText confidence="0.9999734">
The rule extraction procedure described in Section
2.1 is a corpus-based, therefore will be easily suf-
fered from a data sparseness problem. The hier-
archical phrase-based model avoided this problem
by introducing the glue rules 5 and 6 that com-
bined hierarchical phrases sequentially (Chiang,
2005).
We use a different method of generalizing pro-
duction rules. When production rules without non-
terminals are extracted in step 1 of Section 2.1,
</bodyText>
<equation confidence="0.969486">
E
X � D f¯ , e¯ (8)
then, we also add production rules as follows:
Df¯ X1, e¯ X1E
X � (9)
D E
X � X1 f¯ , e¯ X1 (10)
D E
X � X1 f¯ X2, e¯ X1 X2 (11)
D E
X � X2 f¯ X1, e¯ X1 X2 (12)
</equation>
<page confidence="0.987937">
779
</page>
<figure confidence="0.9960888">
X7
X7
Mari 53
a*
Japan
possible
X1
X1
X2 it X4
X2
X4
NO X3
X8 £ X5
X3
also
X5
X8
X6 -e ffilb
in
To
X9 -e
is a
X6
X9
The
international
terrorism
A01 threat
(b) A derivation tree representation for Figure 1(a).Indices in
non-terminal X represent the order to perform rewriting.
</figure>
<figureCaption confidence="0.993884">
Figure 1: An example of Japanese-to-English translation by a phrase-based model.
</figureCaption>
<figure confidence="0.9965782">
The international
�
MW TM
terrorism
Q*
Z t
also is a
,kz7 56
possible threat
*9
in Japan
Z ibz
Reference translation: “International terrorism is a threat
even to Japan”
(a) Translation by a phrase-based model.
</figure>
<bodyText confidence="0.9994941">
We call them phrase-based rules, since four types
of rules are generalized directly from phrase trans-
lation pairs.
The class of rules roughly corresponds to the re-
ordering constraints used in a phrase-based model
during decoding. Rules 8 and 9 are sufficient to re-
alize a monotone decoding in which phrase trans-
lation pairs are simply combined sequentially.
With rules 10 and 11, the non-terminal X1 behaves
as a place holder where certain number of foreign
words are skipped. Therefore, those rules real-
ize a window size constraint used in many phrase-
based models (Koehn et al., 2003). The rule 12
further gives an extra freedom for the phrase pair
reordering. The rules 8 through 12 can be in-
terpreted as ITG-constraints where phrase trans-
lation pairs are hierarchically combined either in
a monotonic way or in an inverted manner (Zens
and Ney, 2003; Wu, 1997). Thus, by controlling
what types of phrase-based rules employed in a
grammar, we will be able to simulate a phrase-
based translation model with various constraints.
This reduction is rather natural in that a finite state
transducer, or a phrase-based model, is a subclass
of a synchronous-CFG.
Figure 1(a) shows an example Japanese-to-
English translation by a phrase-based model de-
scribed in Section 5. Using the phrase-based rules,
the translation results is represented as a derivation
tree in Figure 1(b).
</bodyText>
<sectionHeader confidence="0.997796" genericHeader="method">
3 Decoding
</sectionHeader>
<bodyText confidence="0.999746944444444">
Our decoder is an Earley-style top down parser on
the foreign language side with a beam search strat-
egy. Given an input sentence f1J, the decoder seeks
for the best English according to Equation 3 us-
ing the feature functions described in Section 4.
The English output sentence is generated in left-
to-right order in accordance with the derivation of
the foreign language side synchronized with the
cardinality of already translated foreign word po-
sitions.
The decoding process is very similar to those
described in (Koehn et al., 2003): It starts from an
initial empty hypothesis. From an existing hypoth-
esis, new hypothesis is generated by consuming
a production rule that covers untranslated foreign
word positions. The score for the newly generated
hypothesis is updated by combining the scores of
feature functions described in Section 4. The En-
glish side of the rule is simply concatenated to
form a new prefix of English sentence. Hypothe-
ses that consumed m foreign words are stored in a
priority queue Qm.
Hypotheses in Qm undergo two types of prun-
ing: A histogram pruning preserves at most M hy-
potheses in Qm. A threshold pruning discards a hy-
potheses whose score is below the maximum score
of Qm multiplied with a threshold value T. Rules
are constrained by their foreign word span of a
non-terminal. For a rule consisting of more than
two non-terminals, we constrained so that at least
one non-terminal should span at most K words.
The decoder is characterized as a weighted
synchronous-CFG implemented with a push-down
automaton rather a weighted finite state transducer
(Aho and Ullman, 1969). Each hypothesis main-
tains following knowledge:
</bodyText>
<listItem confidence="0.99971775">
• A prefix of English sentence. For space ef-
ficiency, the prefix is represented as a word
graph.
• Partial contexts for each feature function.
</listItem>
<bodyText confidence="0.545715666666667">
For instance, to compute a 5-gram language
model feature, we keep the consecutive last
four words of an English prefix.
</bodyText>
<page confidence="0.929711">
780
</page>
<listItem confidence="0.949226333333333">
• A stack that keeps track of the uncovered for-
eign word spans. The stack for an initial hy-
pothesis is initialized with span [1, J].
</listItem>
<bodyText confidence="0.999960133333334">
When extending a hypothesis, the associated stack
structure is popped. The popped foreign word
span [jl, jr] is used to locate the rules for uncov-
ered foreign word positions. We assume that the
decoder accumulates all the applicable rules from
a large database and stores the extracted rules in a
chart structure. The decoder identifies what rules
to consume when extending a hypothesis using the
chart structure. A new hypothesis is created with
an updated stack by pushing foreign non-terminal
spans: For each rule spanning [jl, jr] at foreign-
side with non-terminal spans of [kl1,kr1], [kl2, kr2], ,
the non-terminal spans are pushed in the reverse
order of the projected English side. For example,
A rule with foreign word non-terminal spans:
</bodyText>
<equation confidence="0.998524">
( �
X — X2 : [kl 2, kr 2] f¯ X1 : [kl 1, kr 1], e¯ X1 X2
</equation>
<bodyText confidence="0.975513517241379">
will update a stack by pushing the foreign word
spans [kl2,kr2] and [kl1, kr1] in order. This ordering
assures that, when popped, the English-side will
be generated in left-to-right order. A hypothesis
with an empty stack implies that the hypothesis
has covered all the foreign words.
Figure 2 illustrates the decoding process for the
derivation tree in Figure 1(b). Starting from the
initial hypothesis of [1,11], the stack is updated in
accordance with non-terminal’s spans. The span
is popped and the rule with the foreign word pan
[1,11] is looked up from the chart structure. The
stack structure for the newly created hypothesis is
updated by pushing non-terminal spans [4,11] and
[1, 2].
Our decoder is based on an in-house devel-
oped phrase-based decoder which uses a bit vec-
tor to represent uncovered foreign word positions
for each hypothesis. We basically replaced the
bit vector structure to the stack structure: Al-
most no modification was required for the word
graph structure and the beam search strategy im-
plemented for a phrase-based modeling. The use
of a stack structure directly models a synchronous-
CFG formalism realized as a push-down automa-
tion, while the bit vector implementation is con-
ceptualized as a finite state transducer. The cost
of decoding with the proposed model is cubic to
foreign language sentence length.
</bodyText>
<subsectionHeader confidence="0.517193">
Rules Stack
</subsectionHeader>
<equation confidence="0.99877675">
( �
X : [1, 11] — X 1 : [1, 2] « X2 : [4, 11], The X 1 X2
( �
X : [1, 2] — WO X 1 : [2,2], international X 1
( �
X : [7, 11] — X 1 : [7,9] 2 ib6,is a X 1
( �
X : [7, 9] — ,RZ7 56 X 1 : [9, 9], possible X1
X : [9, 9] — (*J9, threat) [4, 5]
( �
X : [4, 5] — X 1 : [4,4] 2, in X 1 [4, 4]
X : [4, 4] — (E9*, Japan)
</equation>
<figureCaption confidence="0.814092">
Figure 2: An example decoding process of Fig-
</figureCaption>
<bodyText confidence="0.746158">
ure 1(b) with a stack to keep track of foreign word
spans.
</bodyText>
<sectionHeader confidence="0.996319" genericHeader="method">
4 Feature Functions
</sectionHeader>
<bodyText confidence="0.999960875">
The decoder for our translation model uses a log-
linear combination of feature functions, or sub-
models, to seek for the maximum likely translation
according to Equation 3. This section describes
the models experimented in Section 5, mainly
consisting of count-based models, lexicon-based
models, a language model, reordering models and
length-based models.
</bodyText>
<subsectionHeader confidence="0.900465">
4.1 Count-based Models
</subsectionHeader>
<bodyText confidence="0.999981">
Main feature functions h0(f 1 J|eI1, D) and
h0(eI1 |f1J , D) estimate the likelihood of two
sentences f1J and eI 1over a derivation tree D.
We assume that the production rules in D are
independent of each other:
</bodyText>
<equation confidence="0.97696">
h0(f 1 J|eI1, D) = log H 0(y|α) (13)
(y,α)ED
</equation>
<bodyText confidence="0.8785055">
0(y|α) is estimated through the relative frequency
on a given bilingual corpus.
</bodyText>
<equation confidence="0.992840333333333">
count(y, α)
0(y|α) = (14)
Ey count(y, α)
</equation>
<bodyText confidence="0.9999414">
where count(·) represents the cooccurrence fre-
quency of rules y and α.
The relative count-based probabilities for the
phrase-based rules are simply adopted from the
original probabilities of phrase translation pairs.
</bodyText>
<subsectionHeader confidence="0.817546">
4.2 Lexicon-based Models
</subsectionHeader>
<bodyText confidence="0.999874666666667">
We define lexically weighted feature functions
hw(f1J |eI1, D) and hw(eI1 |f1J, D) applying the inde-
pendence assumption of production rules as in
</bodyText>
<equation confidence="0.976544473684211">
X : [2, 2] — (TM, terrorism) [4, 11]
( �
X : [4, 11] — X2 : [4,5] -i X 1 : [7, 11], also X 1 X2
[7, 11]
[4, 5]
[1, 11]
[1, 2]
[4, 11]
[2, 2]
[4, 11]
[7, 9]
[4, 5]
[9, 9]
[4, 5]
781
Equation 13.
Y
hw(f 1 J|eI 1, D) = log
(Y,α)ED
</equation>
<bodyText confidence="0.9611145">
The lexical weight pw(Y|α) is computed from word
alignments a inside Y and α (Koehn et al., 2003):
</bodyText>
<equation confidence="0.8164684">
pw(Y|α, a) = Y|α ||{j|(i, j) E a}|
i=1 1
∀(i,j)Ea
X t(Yj|αi)
(16)
</equation>
<bodyText confidence="0.921459142857143">
where t(·) is a lexicon model trained from the word
alignment annotated bilingual corpus discussed in
Section 2.1. The alignment a also includes non-
terminal correspondence with t(Xk |Xk) = 1. If we
observed multiple alignment instances for Y and α,
then, we take the maximum of the weights.
pw(Y|α, a) (17)
</bodyText>
<subsectionHeader confidence="0.977022">
4.3 Language Model
</subsectionHeader>
<bodyText confidence="0.999867333333333">
We used mixed-cased n-gram language model. In
case of 5-gram language model, the feature func-
tion is expressed as follows:
</bodyText>
<equation confidence="0.9875775">
hlm(eI 1) = log Y pn(ei|ei−4ei−3ei−2ei−1) (18)
i
</equation>
<subsectionHeader confidence="0.996586">
4.4 Reordering Models
</subsectionHeader>
<bodyText confidence="0.99991975">
In order to limit the reorderings, two feature func-
tions are employed based on the backtracking of
rules during the top-down parsing on foreign lan-
guage side.
</bodyText>
<equation confidence="0.981602">
Xhh(eI1, f1J, D) = height(Di) (19)
DiEback(D)
</equation>
<tableCaption confidence="0.996167">
Table 1: Japanese/English news corpus
</tableCaption>
<table confidence="0.999806454545454">
Japanese English
train sentence 175,384
dictionary + 1,329,519
words 8,373,478 7,222,726
vocabulary 297,646 397,592
dev. sentence 1,500
words 47,081 39,117
OOV 45 149
test sentence 1,500
words 47,033 38,707
OOV 51 127
</table>
<tableCaption confidence="0.779174">
Table 2: Phrases/rules extracted from the
Japanese/English bilingual corpus. Figures do not
include phrase-based rules.
</tableCaption>
<bodyText confidence="0.999633375">
where rule(D) and phrase(D) are the number
of production rules extracted in Section 2.1 and
phrase-based rules generalized in Section 2.2, re-
spectively. The English length feature function
controls the length of output sentence. Two feature
functions based on rule’s counts are hypothesized
to control whether to incorporate a production rule
or a phrase-based rule into D.
</bodyText>
<figure confidence="0.968389333333333">
# rules/phrases
Phrase 5,433,091
Normalized-2 6,225,630
Normalized-3
Hierarchical
6,233,294
12,824,387
pw(Y|α) (15)
pw(Y|α) = max
a
Xhw(eI1, f 1 J, D) = width(Di) (20) 5 Experiments
DiEback(D)
</figure>
<bodyText confidence="0.9999478">
where back(D) is a set of subtrees backtracked
during the derivation of D, and height(Di) and
width(Di) refer the height and width of subtree Di,
respectively. In Figure 1(b), for instance, a rule of
X1 with non-terminals X2 and X4, two rules X2
and X3 spanning two terminal symbols should be
backtracked to proceed to X4. The rationale is that
positive scaling factors prefer a deeper structure
whereby negative scaling factors prefer a mono-
tonized structure.
</bodyText>
<subsectionHeader confidence="0.937149">
4.5 Length-based Models
</subsectionHeader>
<bodyText confidence="0.9997065">
Three trivial length-based feature functions were
used in our experiment.
</bodyText>
<equation confidence="0.999857333333333">
hl(eI1) = I
hr(D) = rule(D)
hp(D) = phrase(D)
</equation>
<bodyText confidence="0.999670714285714">
The bilingual corpus used for our experiments was
obtained from an automatically sentence aligned
Japanese/English Yomiuri newspaper corpus con-
sisting of 180K sentence pairs (refer to Table
1) (Utiyama and Isahara, 2003). From one-to-
one aligned sentences, 1,500 sentence pairs were
sampled for a development set and a test set1.
Since the bilingual corpus is rather small, es-
pecially for the newspaper translation domain,
Japanese/English dictionaries consisting of 1.3M
entries were added into a training set to alleviate
an OOV problem2.
Word alignments were annotated by a HMM
translation model (Och and Ney, 2003). After
</bodyText>
<footnote confidence="0.991734">
1Japanese sentences were segmented by MeCab available
from http://mecab.sourceforge.jp.
2The dictionary entries were compiled from JE-
DICT/JNAMEDICT and an in-house developed dictionary.
</footnote>
<page confidence="0.992631">
782
</page>
<bodyText confidence="0.999984833333334">
the annotation via Viterbi alignments with refine-
ments, phrases translation pairs and production
rules were extracted (refer to Table 2). We per-
formed the rule extraction using the hierarchi-
cal phrase-based constraint (Hierarchical) and our
proposed target-normalized form with 2 and 3
non-terminals (Normalized-2 and Normalized-3).
Phrase translation pairs were also extracted for
comparison (Phrase). We did not threshold the
extracted phrases or rules by their length. Ta-
ble 2 shows that Normalized-2 extracted slightly
larger number of rules than those for phrase-
based model. Including three non-terminals did
not increase the grammar size. The hierarchical
phrase-based translation model extracts twice as
large as our target-normalized formalism. The
target-normalized form is restrictive in that non-
terminals should be consecutive for the English-
side. This property prohibits spuriously extracted
production rules.
Mixed-casing 3-gram/5-gram language models
were estimated from LDC English GigaWord 2 to-
gether with the 100K English articles of Yomiuri
newspaper that were used neither for development
nor test sets 3.
We run the decoder for the target-normalized
hierarchical phrase-based model consisting of at
most two non-terminals, since adding rules with
three non-terminals did not increase the grammar
size. ITG-constraint simulated phrase-based rules
were also included into our grammar. The foreign
word span size was thresholded so that at least one
non-terminal should span at most 7 words.
Our phrase-based model employed all feature
functions for the hierarchical phrase-based system
with additional feature functions:
</bodyText>
<listItem confidence="0.83451775">
• A distortion model that penalizes the re-
ordering of phrases by the number of words
skipped  |j − (j′ + m′) − 1|, where j is the for-
eign word position for a phrase f j+m
</listItem>
<equation confidence="0.356002">
j trans-
lated immediately after a phrase for fj′+m′
j′
(Koehn et al., 2003).
</equation>
<listItem confidence="0.712387">
• Lexicalized reordering models constrain the
reordering of phrases whether to favor mono-
tone, swap or discontinuous positions (Till-
man, 2004).
</listItem>
<bodyText confidence="0.998157">
The phrase-based decoder’s reordering was con-
strained by ITG-constraints with a window size of
</bodyText>
<footnote confidence="0.966082">
3We used SRI ngram language modeling toolkit with lim-
ited vocabulary size.
</footnote>
<tableCaption confidence="0.9272305">
Table 3: Results for the Japanese-to-English
newswire translation task.
</tableCaption>
<table confidence="0.998002666666667">
BLEU NIST
[%]
Phrase 3-gram 7.14 3.21
5-gram 7.33 3.19
Normalized-2 3-gram 10.00 4.11
5-gram 10.26 4.20
</table>
<bodyText confidence="0.9868181875">
7.
The translation results are summarized in Table
3. Two systems were contrasted by 3-gram and 5-
gram language models. Results were evaluated by
ngram precision based metrics, BLEU and NIST,
on the casing preserved single reference test set.
Feature function scaling factors for each system
were optimized on BLEU score under the devel-
opment set using a downhill simplex method. The
differences of translation qualities are statistically
significant at the 95% confidence level (Koehn,
2004). Although the figures presented in Table
3 are rather low, we found that Normalized-2 re-
sulted in statistically significant improvement over
Phrase. Figure 3 shows some translation results
from the test set.
</bodyText>
<sectionHeader confidence="0.999012" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999966958333333">
The target-normalized hierarchical phrase-based
model is based on a more general hierarchical
phrase-based model (Chiang, 2005). The hier-
archically combined phrases can be regarded as
an instance of phrase-based model with a place
holder to constraint reordering. Such reorder-
ing was realized either by an additional constraint
for decoding, such as window constraints, IBM
constraints or ITG-constraints (Zens and Ney,
2003), or by lexicalized reordering feature func-
tions (Tillman, 2004). In the hierarchical phrase-
based model, such reordering is explicitly repre-
sented in each rule.
As experimented in Section 5, the use of the
target-normalized form reduced the grammar size,
but still outperformed a phrase-based system.
Furthermore, the target-normalized form coupled
with our top down parsing on the foreign lan-
guage side allows an easier integration with ngram
language model. A decoder can be implemented
based on a phrase-based model by employing a
stack structure to keep track of untranslated for-
eign word spans.
The target-normalized form can be interpreted
</bodyText>
<page confidence="0.995925">
783
</page>
<reference confidence="0.276804923076923">
Reference: Japan needs to learn a lesson from history to ensure that it not repeat its mistakes .
Phrase: At the same time, it never mistakes that it is necessary to learn lessons from the history of criminal .
Normalized-2: It is necessary to learn lessons from history so as not to repeat similar mistakes in the future .
Reference: The ministries will dispatch design and construction experts to China to train local engineers and to
research technology that is appropriate to China’s economic situation.
Phrase: Japan sent specialists to train local technicians to the project , in addition to the situation in China and
its design methods by exception of study.
Normalized-2: Japan will send experts to study the situation in China , and train Chinese engineers , construction
design and construction methods of the recipient from.
Reference: The Health and Welfare Ministry has decided to invoke the Disaster Relief Law in extending relief
measures to the village and the city of Niigata.
Phrase: The Health and Welfare Ministry in that the Japanese people in the village are made law.
Normalized-2: The Health and Welfare Ministry decided to apply the Disaster Relief Law to the village in Niigata.
</reference>
<figureCaption confidence="0.995559">
Figure 3: Sample translations from two systems: Phrase and Normalized-2
</figureCaption>
<bodyText confidence="0.999979555555556">
as a set of rules that reorders the foreign lan-
guage to match with English language sequen-
tially. Collins et al. (2005) presented a method
with hand-coded rules. Our method directly learns
such serialization rules from a bilingual corpus
without linguistic clues.
The translation quality presented in Section 5
are rather low due to the limited size of the bilin-
gual corpus, and also because of the linguistic dif-
ference of two languages. As our future work,
we are in the process of experimenting our model
for other languages with rich resources, such as
Chinese and Arabic, as well as similar language
pairs, such as French and English. Additional
feature functions will be also investigated that
were proved successful for phrase-based models
together with feature functions useful for a tree-
based modeling.
</bodyText>
<sectionHeader confidence="0.974518" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999869">
We would like to thank to our colleagues, espe-
cially to Hideto Kazawa and Jun Suzuki, for useful
discussions on the hierarchical phrase-based trans-
lation.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999801634615385">
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax
directed translations and the pushdown assembler. J.
Comput. Syst. Sci., 3(1):37–56.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263–311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
ofACL 2005, pages 263–270, Ann Arbor, Michigan,
June.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL 2005, pages 531–540,
Ann Arbor, Michigan, June.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with
hooks. In Proceedings of the Ninth International
Workshop on Parsing Technology, pages 65–73,
Vancouver, British Columbia, October.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
ofNAACL 2003, pages 48–54, Edmonton, Canada.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388–395, Barcelona, Spain, July.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proc. of ACL 2002,
pages 295–302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51,
March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of ACL
2003, pages 160–167.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
HLT-NAACL 2004: Short Papers, pages 101–104,
Boston, Massachusetts, USA, May 2 - May 7.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Proc. of ACL 2003, pages
72–79.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23(3):377–403.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In Proc. ofACL 2003, pages 144–151.
</reference>
<page confidence="0.998328">
784
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.661044">
<title confidence="0.999281">Left-to-Right Target Generation for Hierarchical Phrase-based Translation</title>
<author confidence="0.971743">Taro Watanabe Hajime Tsukada Hideki Isozaki</author>
<address confidence="0.8083005">2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN 619-0237</address>
<abstract confidence="0.998899136363636">We present a hierarchical phrase-based statistical machine translation in which a sentence is generated in left-to-right order. The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule: The paired target-side of a production rule takes a phrase prefixed form. The decoder for the targetnormalized form is based on an Earlystyle top down parser on the source side. The target-normalized form coupled with our top down parser implies a left-toright generation of translations which enables us a straightforward integration with ngram language models. Our model was experimented on a Japanese-to-English newswire translation task, and showed statistically significant performance improvements against a phrase-based translation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Reference: Japan needs to learn a lesson from history to ensure that it not repeat its mistakes . Phrase: At the same time, it never mistakes that it is necessary to learn lessons from the history of criminal . Normalized-2: It is necessary to learn lessons from history so as not to repeat similar mistakes in the future .</title>
<marker></marker>
<rawString>Reference: Japan needs to learn a lesson from history to ensure that it not repeat its mistakes . Phrase: At the same time, it never mistakes that it is necessary to learn lessons from the history of criminal . Normalized-2: It is necessary to learn lessons from history so as not to repeat similar mistakes in the future .</rawString>
</citation>
<citation valid="false">
<title>Reference: The ministries will dispatch design and construction experts to China to train local engineers and to research technology that is appropriate to China’s economic situation.</title>
<marker></marker>
<rawString>Reference: The ministries will dispatch design and construction experts to China to train local engineers and to research technology that is appropriate to China’s economic situation.</rawString>
</citation>
<citation valid="false">
<title>Phrase: Japan sent specialists to train local technicians to the project , in addition to the situation in China and its design methods by exception of study.</title>
<marker></marker>
<rawString>Phrase: Japan sent specialists to train local technicians to the project , in addition to the situation in China and its design methods by exception of study.</rawString>
</citation>
<citation valid="false">
<title>Normalized-2: Japan will send experts to study the situation in China , and train Chinese engineers , construction design and construction methods of the recipient from. Reference: The Health and Welfare Ministry has decided to invoke the Disaster Relief Law in extending relief measures to the village and the city of Niigata. Phrase: The Health and Welfare Ministry in that the Japanese people in the village are made law. Normalized-2: The Health and Welfare Ministry decided to apply the Disaster Relief Law to the village in</title>
<location>Niigata.</location>
<marker></marker>
<rawString>Normalized-2: Japan will send experts to study the situation in China , and train Chinese engineers , construction design and construction methods of the recipient from. Reference: The Health and Welfare Ministry has decided to invoke the Disaster Relief Law in extending relief measures to the village and the city of Niigata. Phrase: The Health and Welfare Ministry in that the Japanese people in the village are made law. Normalized-2: The Health and Welfare Ministry decided to apply the Disaster Relief Law to the village in Niigata.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Syntax directed translations and the pushdown assembler.</title>
<date>1969</date>
<journal>J. Comput. Syst. Sci.,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="3639" citStr="Aho and Ullman, 1969" startWordPosition="564" endWordPosition="567">ent language pairs. For instance, when translating Japanese to English, a Japanese SOV structure has to be reordered to match with an En777 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 777–784, Sydney, July 2006. c�2006 Association for Computational Linguistics glish SVO structure. Such a sentence-wise movement cannot be realized within the phrase-based modeling. Chiang (2005) introduced a hierarchical phrasebased translation model that combined the strength of the phrase-based approach and a synchronous-CFG formalism (Aho and Ullman, 1969): A rewrite system initiated from a start symbol which synchronously rewrites paired nonterminals. Their translation model is a binarized synchronous-CFG, or a rank-2 of synchronousCFG, in which the right-hand side of a production rule contains at most two non-terminals. The form can be regarded as a phrase translation pair with at most two holes instantiated with other phrases. The hierarchically combined phrases provide a sort of reordering constraints that is not directly modeled by a phrase-based model. Rules are induced from a bilingual corpus without linguistic clues first by extracting </context>
<context position="6768" citStr="Aho and Ullman, 1969" startWordPosition="1049" endWordPosition="1052">arley-style top down parsing on the foreign language side. The projected English-side is generated in left-to-right order synchronized with the derivation of the foreign language side. The decoder’s implementation is taken after a decoder for an existing phrase-based model with a simple modification to account for production rules. Experimental results on a Japanese-toEnglish newswire translation task showed significant improvement against a phrase-based modeling. 2 Translation Model A weighted synchronous-CFG is a rewrite system consisting of production rules whose right-hand side is paired (Aho and Ullman, 1969): X ← (y, α, ∼) (4) where X is a non-terminal, y and α are strings of terminals and non-terminals. For notational simplicity, we assume that y and α correspond to the foreign language side and the English side, respectively. ∼ is a one-to-one correspondence for the non-terminals appeared in y and α. Starting from an initial non-terminal, each rule rewrites non-terminals in y and α that are associated with ∼. Chiang (2005) proposed a hierarchical phrasebased translation model, a binary synchronousCFG, which restricted the form of production rules as follows: • Only two types of non-terminals al</context>
<context position="14995" citStr="Aho and Ullman, 1969" startWordPosition="2488" endWordPosition="2491">tored in a priority queue Qm. Hypotheses in Qm undergo two types of pruning: A histogram pruning preserves at most M hypotheses in Qm. A threshold pruning discards a hypotheses whose score is below the maximum score of Qm multiplied with a threshold value T. Rules are constrained by their foreign word span of a non-terminal. For a rule consisting of more than two non-terminals, we constrained so that at least one non-terminal should span at most K words. The decoder is characterized as a weighted synchronous-CFG implemented with a push-down automaton rather a weighted finite state transducer (Aho and Ullman, 1969). Each hypothesis maintains following knowledge: • A prefix of English sentence. For space efficiency, the prefix is represented as a word graph. • Partial contexts for each feature function. For instance, to compute a 5-gram language model feature, we keep the consecutive last four words of an English prefix. 780 • A stack that keeps track of the uncovered foreign word spans. The stack for an initial hypothesis is initialized with span [1, J]. When extending a hypothesis, the associated stack structure is popped. The popped foreign word span [jl, jr] is used to locate the rules for uncovered </context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax directed translations and the pushdown assembler. J. Comput. Syst. Sci., 3(1):37–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1513" citStr="Brown et al., 1993" startWordPosition="220" endWordPosition="223">panese-to-English newswire translation task, and showed statistically significant performance improvements against a phrase-based translation system. 1 Introduction In a classical statistical machine translation, a foreign language sentence f1J = f1, f2, ...fJ is translated into another language, i.e. English, eI1 = e1, e2,..., eI by seeking a maximum likely solution of: ˆeI 1= argmax Pr(eI1|f1J) (1) eI 1 = argmax Pr(f 1 J|eI 1)Pr(eI 1) (2) eI 1 The source channel approach in Equation 2 independently decomposes translation knowledge into a translation model and a language model, respectively (Brown et al., 1993). The former represents the correspondence between two languages and the latter contributes to the fluency of English. In the state of the art statistical machine translation, the posterior probability Pr(eI1 |f1J) is directly maximized using a log-linear combination of feature functions (Och and Ney, 2002): ˆeI 1= argmax ~PM � eI exp m=1 Amhm(eI 1, f1 J ) 1 3 M Pe′I′ exp Pm=1 Amhm(e′I′ 1 ,f1J) where hm(eI1, f1J) is a feature function, such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofACL</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="3472" citStr="Chiang (2005)" startWordPosition="543" endWordPosition="544">t the phrase translation unit preserves localized word reordering. However, it cannot hypothesize a long-distance reordering required for linguistically divergent language pairs. For instance, when translating Japanese to English, a Japanese SOV structure has to be reordered to match with an En777 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 777–784, Sydney, July 2006. c�2006 Association for Computational Linguistics glish SVO structure. Such a sentence-wise movement cannot be realized within the phrase-based modeling. Chiang (2005) introduced a hierarchical phrasebased translation model that combined the strength of the phrase-based approach and a synchronous-CFG formalism (Aho and Ullman, 1969): A rewrite system initiated from a start symbol which synchronously rewrites paired nonterminals. Their translation model is a binarized synchronous-CFG, or a rank-2 of synchronousCFG, in which the right-hand side of a production rule contains at most two non-terminals. The form can be regarded as a phrase translation pair with at most two holes instantiated with other phrases. The hierarchically combined phrases provide a sort </context>
<context position="4969" citStr="Chiang, 2005" startWordPosition="769" endWordPosition="770">d model, the number of phrases extracted from a bilingual corpus is quadratic to the length of bilingual sentences. The grammar size for the hierarchical phrase-based model will be further exploded, since there exists numerous combination of inserting holes to each rule. The spuriously increasing grammar size will be problematic for decoding without certain heuristics, such as a length based thresholding. The integration with a ngram language model further increases the cost of decoding especially when incorporating a higher order ngram, such as 5-gram. In the hierarchical phrase-based model (Chiang, 2005), and an inversion transduction grammar (ITG) (Wu, 1997), the problem is resolved by restricting to a binarized form where at most two non-terminals are allowed in the righthand side. However, Huang et al. (2005) reported that the computational complexity for decoding amounted to O(J3+3(n−1)) with n-gram even using a hook technique. The complexity lies in memorizing the ngram’s context for each constituent. The order of ngram would be a dominant factor for higher order ngrams. As an alternative to a binarized form, we present a target-normalized hierarchical phrasebased translation model. The </context>
<context position="7193" citStr="Chiang (2005)" startWordPosition="1126" endWordPosition="1127">nt against a phrase-based modeling. 2 Translation Model A weighted synchronous-CFG is a rewrite system consisting of production rules whose right-hand side is paired (Aho and Ullman, 1969): X ← (y, α, ∼) (4) where X is a non-terminal, y and α are strings of terminals and non-terminals. For notational simplicity, we assume that y and α correspond to the foreign language side and the English side, respectively. ∼ is a one-to-one correspondence for the non-terminals appeared in y and α. Starting from an initial non-terminal, each rule rewrites non-terminals in y and α that are associated with ∼. Chiang (2005) proposed a hierarchical phrasebased translation model, a binary synchronousCFG, which restricted the form of production rules as follows: • Only two types of non-terminals allowed: S and X. • Both of the strings y and α must contain at least one terminal item. • Rules may have at most two non-terminals but non-terminals cannot be adjacent for the foreign language side y. The production rules are induced from a bilingual corpus with the help of word alignments. To alleviate a data sparseness problem, glue rules are 778 added that prefer combining hierarchical phrases in a serial manner: D E S </context>
<context position="9528" citStr="Chiang, 2005" startWordPosition="1513" endWordPosition="1514">mplexity. The contiguous phrase-prefixed form generates English in left-to-right order. Therefore, a decoder can easily hypothesize a derivation tree integrated with a ngram language model even with higher order. Note that we do not imply arbitrary synchronous-CFGs are transformed into the target normalized form. The form simply restricts the grammar extracted from a bilingual corpus explained in the next section. 2.1 Rule Extraction We present an algorithm to extract production rules from a bilingual corpus. The procedure is based on those for the hierarchical phrase-based translation model (Chiang, 2005). First, a bilingual corpus is annotated with word alignments using the method of Koehn et al. (2003). Many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Koehn et al., 2003). Second, phrase translation pairs are extracted from the word alignment corpus (Koehn et al., 2003). The method exhaustively extracts phrase pairs ( f j+m j , ei+n i ) from a sentence pair (f1J , eI1) that do not violate the word alignment constraints a: 3(i′, j′) E a : j′ E [j, j +</context>
<context position="11112" citStr="Chiang, 2005" startWordPosition="1811" endWordPosition="1812">′ fγ′′ and α = ¯e′¯eβ constitutes a rule D E X � γ′ Xk γ′′, ¯e′ Xk β Following Chiang (2005), we applied constraints when inducing rules with non-terminals: • At least one foreign word must be aligned to an English word. • Adjacent non-terminals are not allowed for the foreign language side. 2.2 Phrase-based Rules The rule extraction procedure described in Section 2.1 is a corpus-based, therefore will be easily suffered from a data sparseness problem. The hierarchical phrase-based model avoided this problem by introducing the glue rules 5 and 6 that combined hierarchical phrases sequentially (Chiang, 2005). We use a different method of generalizing production rules. When production rules without nonterminals are extracted in step 1 of Section 2.1, E X � D f¯ , e¯ (8) then, we also add production rules as follows: Df¯ X1, e¯ X1E X � (9) D E X � X1 f¯ , e¯ X1 (10) D E X � X1 f¯ X2, e¯ X1 X2 (11) D E X � X2 f¯ X1, e¯ X1 X2 (12) 779 X7 X7 Mari 53 a* Japan possible X1 X1 X2 it X4 X2 X4 NO X3 X8 £ X5 X3 also X5 X8 X6 -e ffilb in To X9 -e is a X6 X9 The international terrorism A01 threat (b) A derivation tree representation for Figure 1(a).Indices in non-terminal X represent the order to perform rewri</context>
<context position="25877" citStr="Chiang, 2005" startWordPosition="4285" endWordPosition="4286">ence test set. Feature function scaling factors for each system were optimized on BLEU score under the development set using a downhill simplex method. The differences of translation qualities are statistically significant at the 95% confidence level (Koehn, 2004). Although the figures presented in Table 3 are rather low, we found that Normalized-2 resulted in statistically significant improvement over Phrase. Figure 3 shows some translation results from the test set. 6 Conclusion The target-normalized hierarchical phrase-based model is based on a more general hierarchical phrase-based model (Chiang, 2005). The hierarchically combined phrases can be regarded as an instance of phrase-based model with a place holder to constraint reordering. Such reordering was realized either by an additional constraint for decoding, such as window constraints, IBM constraints or ITG-constraints (Zens and Ney, 2003), or by lexicalized reordering feature functions (Tillman, 2004). In the hierarchical phrasebased model, such reordering is explicitly represented in each rule. As experimented in Section 5, the use of the target-normalized form reduced the grammar size, but still outperformed a phrase-based system. F</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. ofACL 2005, pages 263–270, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>531--540</pages>
<location>Ann Arbor, Michigan,</location>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proc. of ACL 2005, pages 531–540, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Machine translation as lexicalized parsing with hooks.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>65--73</pages>
<location>Vancouver, British Columbia,</location>
<contexts>
<context position="5181" citStr="Huang et al. (2005)" startWordPosition="803" endWordPosition="806">here exists numerous combination of inserting holes to each rule. The spuriously increasing grammar size will be problematic for decoding without certain heuristics, such as a length based thresholding. The integration with a ngram language model further increases the cost of decoding especially when incorporating a higher order ngram, such as 5-gram. In the hierarchical phrase-based model (Chiang, 2005), and an inversion transduction grammar (ITG) (Wu, 1997), the problem is resolved by restricting to a binarized form where at most two non-terminals are allowed in the righthand side. However, Huang et al. (2005) reported that the computational complexity for decoding amounted to O(J3+3(n−1)) with n-gram even using a hook technique. The complexity lies in memorizing the ngram’s context for each constituent. The order of ngram would be a dominant factor for higher order ngrams. As an alternative to a binarized form, we present a target-normalized hierarchical phrasebased translation model. The model is a class of a hierarchical phrase-based model, but constrained so that the English part of the right-hand side is restricted to a Greibach Normal Form (GNF)- like structure: A contiguous sequence of termi</context>
</contexts>
<marker>Huang, Zhang, Gildea, 2005</marker>
<rawString>Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Machine translation as lexicalized parsing with hooks. In Proceedings of the Ninth International Workshop on Parsing Technology, pages 65–73, Vancouver, British Columbia, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. ofNAACL 2003,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2537" citStr="Koehn et al., 2003" startWordPosition="395" endWordPosition="398">(eI1, f1J) is a feature function, such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors Am are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation (Koehn et al., 2003; Zens and Ney, 2003; Tillman, 2004). The idea is based on a word-based source channel modeling of Brown et al. (1993): It assumes that eI1 is segmented into a sequence of K phrases ¯eK1 . Each phrase ¯ek is transformed into ¯fk. The translated phrases are reordered to form f1J. One of the benefits of the modeling is that the phrase translation unit preserves localized word reordering. However, it cannot hypothesize a long-distance reordering required for linguistically divergent language pairs. For instance, when translating Japanese to English, a Japanese SOV structure has to be reordered to</context>
<context position="9629" citStr="Koehn et al. (2003)" startWordPosition="1528" endWordPosition="1531">re, a decoder can easily hypothesize a derivation tree integrated with a ngram language model even with higher order. Note that we do not imply arbitrary synchronous-CFGs are transformed into the target normalized form. The form simply restricts the grammar extracted from a bilingual corpus explained in the next section. 2.1 Rule Extraction We present an algorithm to extract production rules from a bilingual corpus. The procedure is based on those for the hierarchical phrase-based translation model (Chiang, 2005). First, a bilingual corpus is annotated with word alignments using the method of Koehn et al. (2003). Many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Koehn et al., 2003). Second, phrase translation pairs are extracted from the word alignment corpus (Koehn et al., 2003). The method exhaustively extracts phrase pairs ( f j+m j , ei+n i ) from a sentence pair (f1J , eI1) that do not violate the word alignment constraints a: 3(i′, j′) E a : j′ E [j, j + m], i′ E [i, i + n] ∄(i′, j′) E a : j′ E [j, j + m],i′ 0 [i,i + n] ∄(i′, j′) E a : j′ 0 [j, j + m], </context>
<context position="12595" citStr="Koehn et al., 2003" startWordPosition="2091" endWordPosition="2094">nslation by a phrase-based model. We call them phrase-based rules, since four types of rules are generalized directly from phrase translation pairs. The class of rules roughly corresponds to the reordering constraints used in a phrase-based model during decoding. Rules 8 and 9 are sufficient to realize a monotone decoding in which phrase translation pairs are simply combined sequentially. With rules 10 and 11, the non-terminal X1 behaves as a place holder where certain number of foreign words are skipped. Therefore, those rules realize a window size constraint used in many phrasebased models (Koehn et al., 2003). The rule 12 further gives an extra freedom for the phrase pair reordering. The rules 8 through 12 can be interpreted as ITG-constraints where phrase translation pairs are hierarchically combined either in a monotonic way or in an inverted manner (Zens and Ney, 2003; Wu, 1997). Thus, by controlling what types of phrase-based rules employed in a grammar, we will be able to simulate a phrasebased translation model with various constraints. This reduction is rather natural in that a finite state transducer, or a phrase-based model, is a subclass of a synchronous-CFG. Figure 1(a) shows an example</context>
<context position="13926" citStr="Koehn et al., 2003" startWordPosition="2310" endWordPosition="2313">e translation results is represented as a derivation tree in Figure 1(b). 3 Decoding Our decoder is an Earley-style top down parser on the foreign language side with a beam search strategy. Given an input sentence f1J, the decoder seeks for the best English according to Equation 3 using the feature functions described in Section 4. The English output sentence is generated in leftto-right order in accordance with the derivation of the foreign language side synchronized with the cardinality of already translated foreign word positions. The decoding process is very similar to those described in (Koehn et al., 2003): It starts from an initial empty hypothesis. From an existing hypothesis, new hypothesis is generated by consuming a production rule that covers untranslated foreign word positions. The score for the newly generated hypothesis is updated by combining the scores of feature functions described in Section 4. The English side of the rule is simply concatenated to form a new prefix of English sentence. Hypotheses that consumed m foreign words are stored in a priority queue Qm. Hypotheses in Qm undergo two types of pruning: A histogram pruning preserves at most M hypotheses in Qm. A threshold pruni</context>
<context position="19504" citStr="Koehn et al., 2003" startWordPosition="3313" endWordPosition="3316">d probabilities for the phrase-based rules are simply adopted from the original probabilities of phrase translation pairs. 4.2 Lexicon-based Models We define lexically weighted feature functions hw(f1J |eI1, D) and hw(eI1 |f1J, D) applying the independence assumption of production rules as in X : [2, 2] — (TM, terrorism) [4, 11] ( � X : [4, 11] — X2 : [4,5] -i X 1 : [7, 11], also X 1 X2 [7, 11] [4, 5] [1, 11] [1, 2] [4, 11] [2, 2] [4, 11] [7, 9] [4, 5] [9, 9] [4, 5] 781 Equation 13. Y hw(f 1 J|eI 1, D) = log (Y,α)ED The lexical weight pw(Y|α) is computed from word alignments a inside Y and α (Koehn et al., 2003): pw(Y|α, a) = Y|α ||{j|(i, j) E a}| i=1 1 ∀(i,j)Ea X t(Yj|αi) (16) where t(·) is a lexicon model trained from the word alignment annotated bilingual corpus discussed in Section 2.1. The alignment a also includes nonterminal correspondence with t(Xk |Xk) = 1. If we observed multiple alignment instances for Y and α, then, we take the maximum of the weights. pw(Y|α, a) (17) 4.3 Language Model We used mixed-cased n-gram language model. In case of 5-gram language model, the feature function is expressed as follows: hlm(eI 1) = log Y pn(ei|ei−4ei−3ei−2ei−1) (18) i 4.4 Reordering Models In order to </context>
<context position="24543" citStr="Koehn et al., 2003" startWordPosition="4084" endWordPosition="4087">th three non-terminals did not increase the grammar size. ITG-constraint simulated phrase-based rules were also included into our grammar. The foreign word span size was thresholded so that at least one non-terminal should span at most 7 words. Our phrase-based model employed all feature functions for the hierarchical phrase-based system with additional feature functions: • A distortion model that penalizes the reordering of phrases by the number of words skipped |j − (j′ + m′) − 1|, where j is the foreign word position for a phrase f j+m j translated immediately after a phrase for fj′+m′ j′ (Koehn et al., 2003). • Lexicalized reordering models constrain the reordering of phrases whether to favor monotone, swap or discontinuous positions (Tillman, 2004). The phrase-based decoder’s reordering was constrained by ITG-constraints with a window size of 3We used SRI ngram language modeling toolkit with limited vocabulary size. Table 3: Results for the Japanese-to-English newswire translation task. BLEU NIST [%] Phrase 3-gram 7.14 3.21 5-gram 7.33 3.19 Normalized-2 3-gram 10.00 4.11 5-gram 10.26 4.20 7. The translation results are summarized in Table 3. Two systems were contrasted by 3-gram and 5- gram lang</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. ofNAACL 2003, pages 48–54, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="25528" citStr="Koehn, 2004" startWordPosition="4235" endWordPosition="4236">nslation task. BLEU NIST [%] Phrase 3-gram 7.14 3.21 5-gram 7.33 3.19 Normalized-2 3-gram 10.00 4.11 5-gram 10.26 4.20 7. The translation results are summarized in Table 3. Two systems were contrasted by 3-gram and 5- gram language models. Results were evaluated by ngram precision based metrics, BLEU and NIST, on the casing preserved single reference test set. Feature function scaling factors for each system were optimized on BLEU score under the development set using a downhill simplex method. The differences of translation qualities are statistically significant at the 95% confidence level (Koehn, 2004). Although the figures presented in Table 3 are rather low, we found that Normalized-2 resulted in statistically significant improvement over Phrase. Figure 3 shows some translation results from the test set. 6 Conclusion The target-normalized hierarchical phrase-based model is based on a more general hierarchical phrase-based model (Chiang, 2005). The hierarchically combined phrases can be regarded as an instance of phrase-based model with a place holder to constraint reordering. Such reordering was realized either by an additional constraint for decoding, such as window constraints, IBM cons</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP 2004, pages 388–395, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>295--302</pages>
<contexts>
<context position="1821" citStr="Och and Ney, 2002" startWordPosition="268" endWordPosition="271">h, eI1 = e1, e2,..., eI by seeking a maximum likely solution of: ˆeI 1= argmax Pr(eI1|f1J) (1) eI 1 = argmax Pr(f 1 J|eI 1)Pr(eI 1) (2) eI 1 The source channel approach in Equation 2 independently decomposes translation knowledge into a translation model and a language model, respectively (Brown et al., 1993). The former represents the correspondence between two languages and the latter contributes to the fluency of English. In the state of the art statistical machine translation, the posterior probability Pr(eI1 |f1J) is directly maximized using a log-linear combination of feature functions (Och and Ney, 2002): ˆeI 1= argmax ~PM � eI exp m=1 Amhm(eI 1, f1 J ) 1 3 M Pe′I′ exp Pm=1 Amhm(e′I′ 1 ,f1J) where hm(eI1, f1J) is a feature function, such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors Am are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. A phrase-based translation model is one of the mod</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of ACL 2002, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9753" citStr="Och and Ney, 2003" startWordPosition="1548" endWordPosition="1551">at we do not imply arbitrary synchronous-CFGs are transformed into the target normalized form. The form simply restricts the grammar extracted from a bilingual corpus explained in the next section. 2.1 Rule Extraction We present an algorithm to extract production rules from a bilingual corpus. The procedure is based on those for the hierarchical phrase-based translation model (Chiang, 2005). First, a bilingual corpus is annotated with word alignments using the method of Koehn et al. (2003). Many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Koehn et al., 2003). Second, phrase translation pairs are extracted from the word alignment corpus (Koehn et al., 2003). The method exhaustively extracts phrase pairs ( f j+m j , ei+n i ) from a sentence pair (f1J , eI1) that do not violate the word alignment constraints a: 3(i′, j′) E a : j′ E [j, j + m], i′ E [i, i + n] ∄(i′, j′) E a : j′ E [j, j + m],i′ 0 [i,i + n] ∄(i′, j′) E a : j′ 0 [j, j + m], i′ E [i, i + n] Third, based on the extracted phrases, production rules are accumulated by computing the “holes” for contigu</context>
<context position="22462" citStr="Och and Ney, 2003" startWordPosition="3775" endWordPosition="3778"> The bilingual corpus used for our experiments was obtained from an automatically sentence aligned Japanese/English Yomiuri newspaper corpus consisting of 180K sentence pairs (refer to Table 1) (Utiyama and Isahara, 2003). From one-toone aligned sentences, 1,500 sentence pairs were sampled for a development set and a test set1. Since the bilingual corpus is rather small, especially for the newspaper translation domain, Japanese/English dictionaries consisting of 1.3M entries were added into a training set to alleviate an OOV problem2. Word alignments were annotated by a HMM translation model (Och and Ney, 2003). After 1Japanese sentences were segmented by MeCab available from http://mecab.sourceforge.jp. 2The dictionary entries were compiled from JEDICT/JNAMEDICT and an in-house developed dictionary. 782 the annotation via Viterbi alignments with refinements, phrases translation pairs and production rules were extracted (refer to Table 2). We performed the rule extraction using the hierarchical phrase-based constraint (Hierarchical) and our proposed target-normalized form with 2 and 3 non-terminals (Normalized-2 and Normalized-3). Phrase translation pairs were also extracted for comparison (Phrase).</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="2240" citStr="Och, 2003" startWordPosition="348" endWordPosition="349">. In the state of the art statistical machine translation, the posterior probability Pr(eI1 |f1J) is directly maximized using a log-linear combination of feature functions (Och and Ney, 2002): ˆeI 1= argmax ~PM � eI exp m=1 Amhm(eI 1, f1 J ) 1 3 M Pe′I′ exp Pm=1 Amhm(e′I′ 1 ,f1J) where hm(eI1, f1J) is a feature function, such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors Am are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation (Koehn et al., 2003; Zens and Ney, 2003; Tillman, 2004). The idea is based on a word-based source channel modeling of Brown et al. (1993): It assumes that eI1 is segmented into a sequence of K phrases ¯eK1 . Each phrase ¯ek is transformed into ¯fk. The translated phrases are reordered to form f1J. One of the benefits of </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL 2003, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Short Papers,</booktitle>
<pages>101--104</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="2573" citStr="Tillman, 2004" startWordPosition="403" endWordPosition="405">s a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors Am are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation (Koehn et al., 2003; Zens and Ney, 2003; Tillman, 2004). The idea is based on a word-based source channel modeling of Brown et al. (1993): It assumes that eI1 is segmented into a sequence of K phrases ¯eK1 . Each phrase ¯ek is transformed into ¯fk. The translated phrases are reordered to form f1J. One of the benefits of the modeling is that the phrase translation unit preserves localized word reordering. However, it cannot hypothesize a long-distance reordering required for linguistically divergent language pairs. For instance, when translating Japanese to English, a Japanese SOV structure has to be reordered to match with an En777 Proceedings of </context>
<context position="24687" citStr="Tillman, 2004" startWordPosition="4106" endWordPosition="4108">n word span size was thresholded so that at least one non-terminal should span at most 7 words. Our phrase-based model employed all feature functions for the hierarchical phrase-based system with additional feature functions: • A distortion model that penalizes the reordering of phrases by the number of words skipped |j − (j′ + m′) − 1|, where j is the foreign word position for a phrase f j+m j translated immediately after a phrase for fj′+m′ j′ (Koehn et al., 2003). • Lexicalized reordering models constrain the reordering of phrases whether to favor monotone, swap or discontinuous positions (Tillman, 2004). The phrase-based decoder’s reordering was constrained by ITG-constraints with a window size of 3We used SRI ngram language modeling toolkit with limited vocabulary size. Table 3: Results for the Japanese-to-English newswire translation task. BLEU NIST [%] Phrase 3-gram 7.14 3.21 5-gram 7.33 3.19 Normalized-2 3-gram 10.00 4.11 5-gram 10.26 4.20 7. The translation results are summarized in Table 3. Two systems were contrasted by 3-gram and 5- gram language models. Results were evaluated by ngram precision based metrics, BLEU and NIST, on the casing preserved single reference test set. Feature </context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In HLT-NAACL 2004: Short Papers, pages 101–104, Boston, Massachusetts, USA, May 2 - May 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Reliable measures for aligning Japanese-English news articles and sentences.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>72--79</pages>
<contexts>
<context position="22065" citStr="Utiyama and Isahara, 2003" startWordPosition="3713" endWordPosition="3716"> X1 with non-terminals X2 and X4, two rules X2 and X3 spanning two terminal symbols should be backtracked to proceed to X4. The rationale is that positive scaling factors prefer a deeper structure whereby negative scaling factors prefer a monotonized structure. 4.5 Length-based Models Three trivial length-based feature functions were used in our experiment. hl(eI1) = I hr(D) = rule(D) hp(D) = phrase(D) The bilingual corpus used for our experiments was obtained from an automatically sentence aligned Japanese/English Yomiuri newspaper corpus consisting of 180K sentence pairs (refer to Table 1) (Utiyama and Isahara, 2003). From one-toone aligned sentences, 1,500 sentence pairs were sampled for a development set and a test set1. Since the bilingual corpus is rather small, especially for the newspaper translation domain, Japanese/English dictionaries consisting of 1.3M entries were added into a training set to alleviate an OOV problem2. Word alignments were annotated by a HMM translation model (Och and Ney, 2003). After 1Japanese sentences were segmented by MeCab available from http://mecab.sourceforge.jp. 2The dictionary entries were compiled from JEDICT/JNAMEDICT and an in-house developed dictionary. 782 the a</context>
</contexts>
<marker>Utiyama, Isahara, 2003</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2003. Reliable measures for aligning Japanese-English news articles and sentences. In Proc. of ACL 2003, pages 72–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Comput. Linguist.,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="5025" citStr="Wu, 1997" startWordPosition="777" endWordPosition="778">rpus is quadratic to the length of bilingual sentences. The grammar size for the hierarchical phrase-based model will be further exploded, since there exists numerous combination of inserting holes to each rule. The spuriously increasing grammar size will be problematic for decoding without certain heuristics, such as a length based thresholding. The integration with a ngram language model further increases the cost of decoding especially when incorporating a higher order ngram, such as 5-gram. In the hierarchical phrase-based model (Chiang, 2005), and an inversion transduction grammar (ITG) (Wu, 1997), the problem is resolved by restricting to a binarized form where at most two non-terminals are allowed in the righthand side. However, Huang et al. (2005) reported that the computational complexity for decoding amounted to O(J3+3(n−1)) with n-gram even using a hook technique. The complexity lies in memorizing the ngram’s context for each constituent. The order of ngram would be a dominant factor for higher order ngrams. As an alternative to a binarized form, we present a target-normalized hierarchical phrasebased translation model. The model is a class of a hierarchical phrase-based model, b</context>
<context position="12873" citStr="Wu, 1997" startWordPosition="2141" endWordPosition="2142">ient to realize a monotone decoding in which phrase translation pairs are simply combined sequentially. With rules 10 and 11, the non-terminal X1 behaves as a place holder where certain number of foreign words are skipped. Therefore, those rules realize a window size constraint used in many phrasebased models (Koehn et al., 2003). The rule 12 further gives an extra freedom for the phrase pair reordering. The rules 8 through 12 can be interpreted as ITG-constraints where phrase translation pairs are hierarchically combined either in a monotonic way or in an inverted manner (Zens and Ney, 2003; Wu, 1997). Thus, by controlling what types of phrase-based rules employed in a grammar, we will be able to simulate a phrasebased translation model with various constraints. This reduction is rather natural in that a finite state transducer, or a phrase-based model, is a subclass of a synchronous-CFG. Figure 1(a) shows an example Japanese-toEnglish translation by a phrase-based model described in Section 5. Using the phrase-based rules, the translation results is represented as a derivation tree in Figure 1(b). 3 Decoding Our decoder is an Earley-style top down parser on the foreign language side with </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Comput. Linguist., 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>A comparative study on reordering constraints in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ofACL</booktitle>
<pages>144--151</pages>
<contexts>
<context position="2557" citStr="Zens and Ney, 2003" startWordPosition="399" endWordPosition="402">ure function, such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors Am are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation (Koehn et al., 2003; Zens and Ney, 2003; Tillman, 2004). The idea is based on a word-based source channel modeling of Brown et al. (1993): It assumes that eI1 is segmented into a sequence of K phrases ¯eK1 . Each phrase ¯ek is transformed into ¯fk. The translated phrases are reordered to form f1J. One of the benefits of the modeling is that the phrase translation unit preserves localized word reordering. However, it cannot hypothesize a long-distance reordering required for linguistically divergent language pairs. For instance, when translating Japanese to English, a Japanese SOV structure has to be reordered to match with an En777</context>
<context position="12862" citStr="Zens and Ney, 2003" startWordPosition="2137" endWordPosition="2140">s 8 and 9 are sufficient to realize a monotone decoding in which phrase translation pairs are simply combined sequentially. With rules 10 and 11, the non-terminal X1 behaves as a place holder where certain number of foreign words are skipped. Therefore, those rules realize a window size constraint used in many phrasebased models (Koehn et al., 2003). The rule 12 further gives an extra freedom for the phrase pair reordering. The rules 8 through 12 can be interpreted as ITG-constraints where phrase translation pairs are hierarchically combined either in a monotonic way or in an inverted manner (Zens and Ney, 2003; Wu, 1997). Thus, by controlling what types of phrase-based rules employed in a grammar, we will be able to simulate a phrasebased translation model with various constraints. This reduction is rather natural in that a finite state transducer, or a phrase-based model, is a subclass of a synchronous-CFG. Figure 1(a) shows an example Japanese-toEnglish translation by a phrase-based model described in Section 5. Using the phrase-based rules, the translation results is represented as a derivation tree in Figure 1(b). 3 Decoding Our decoder is an Earley-style top down parser on the foreign language</context>
<context position="26175" citStr="Zens and Ney, 2003" startWordPosition="4328" endWordPosition="4331">nted in Table 3 are rather low, we found that Normalized-2 resulted in statistically significant improvement over Phrase. Figure 3 shows some translation results from the test set. 6 Conclusion The target-normalized hierarchical phrase-based model is based on a more general hierarchical phrase-based model (Chiang, 2005). The hierarchically combined phrases can be regarded as an instance of phrase-based model with a place holder to constraint reordering. Such reordering was realized either by an additional constraint for decoding, such as window constraints, IBM constraints or ITG-constraints (Zens and Ney, 2003), or by lexicalized reordering feature functions (Tillman, 2004). In the hierarchical phrasebased model, such reordering is explicitly represented in each rule. As experimented in Section 5, the use of the target-normalized form reduced the grammar size, but still outperformed a phrase-based system. Furthermore, the target-normalized form coupled with our top down parsing on the foreign language side allows an easier integration with ngram language model. A decoder can be implemented based on a phrase-based model by employing a stack structure to keep track of untranslated foreign word spans. </context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>Richard Zens and Hermann Ney. 2003. A comparative study on reordering constraints in statistical machine translation. In Proc. ofACL 2003, pages 144–151.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>