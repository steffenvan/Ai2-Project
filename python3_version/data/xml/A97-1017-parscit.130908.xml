<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9952555">
Probabilistic and Rule-Based Tagger of an Inflective
Language - a Comparison
</title>
<author confidence="0.988776">
Jan Haji and Barbora Hladka
</author>
<affiliation confidence="0.8460155">
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics
</affiliation>
<address confidence="0.57117">
Malostranske nam. 25
CZ-118 00 Prague 1
</address>
<email confidence="0.998801">
e-mail: fhajic,hladkal@ufal.mff.cuni.cz
</email>
<bodyText confidence="0.998244871794872">
(which is usually called taggingl). For example, the
ending &amp;quot;-u&amp;quot; is not only highly ambiguous, but at the
same time it carries complex information: it corre-
sponds to the genitive, the dative and the locative
singular for inanimate nouns, or the dative singu-
lar for animate nouns, or the accusative singular for
feminine nouns, or the first person singular present
tense active participle for certain verbs. There are
two different techniques for text tagging: a stochas-
tic technique and a rule-based technique. Each ap-
proach has some advantages — for stochastic tech-
niques there exists a good theoretical framework,
probabilities provide a straightforward way how to
disambiguate tags for each word and probabilities
can be acquired automatically from the data; for
rule-based techniques the set of meaningful rules is
automatically acquired and there exists an easy way
how to find and implement improvements of the tag-
ger. Small set of rules can be used, in contrast to the
large statistical tables. Given the success of statis-
tical methods in different areas, including text tag-
ging, given the very positive results of English statis-
tical taggers and given the fact that there existed no
statistical tagger for any Slavic language we wanted
to apply statistical methods even for the Czech lan-
guage although it exhibits a rich inflection accom-
panied by a high degree of ambiguity. Originally,
we expected that the result would be plain negative,
getting no more than about two thirds of the tags
correct. However, as we show below, we got bet-
ter results than we had expected. We used the same
statistical approach to tag both the English text and
the Czech text. For English, we obtained results
comparable with the results presented in (Merialdo,
1992) as well as in (Church, 1992). For Czech, we ob-
tained results which are less satisfactory than those
for English. Given the comparability of the accu-
racy of the rule-based part-of-speech (POS) tagger
(Brill, 1992) with the accuracy of the stochastic tag-
</bodyText>
<sectionHeader confidence="0.667105" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983257142857">
We present results of probabilistic tag-
ging of Czech texts in order to show how
these techniques work for one of the highly
morphologically ambiguous inflective lan-
guages. After description of the tag system
used, we show the results of four experi-
ments using a simple probabilistic model
to tag Czech texts (unigram, two bigram
experiments, and a trigram one). For com-
parison, we have applied the same code and
settings to tag an English text (another
four experiments) using the same size of
training and test data in the experiments in
order to avoid any doubt concerning the va-
lidity of the comparison. The experiments
use the source channel model and maxi-
mum likelihood training on a Czech hand-
tagged corpus and on tagged Wall Street
Journal (WSJ) from the LDC collection.
The experiments show (not surprisingly)
that the more training data, the better is
the success rate. The results also indicate
that for inflective languages with 1000+
tags we have to develop a more sophisti-
cated approach in order to get closer to an
acceptable error rate. In order to compare
two different approaches to text tagging —
statistical and rule-based — we modified
Eric Brill&apos;s rule-based part of speech tag-
ger and carried out two more experiments
on the Czech data, obtaining similar results
in terms of the error rate. We have also
run three more experiments with greatly
reduced tagset to get another comparison
based on similar tagset size.
</bodyText>
<footnote confidence="0.891626333333333">
1 INTRODUCTION
&apos;The development of automatic tagging of Czech
is/was supported fully or partially by the fol-
lowing grants/projects: Charles University GAUK
Languages with rich inflection like Czech pose a 39/94, Grant Agency of the Czech Republic GACR
special problem for morphological disambiguation 405/96/K214 and Ministry of Education VS96151.
</footnote>
<page confidence="0.998054">
111
</page>
<bodyText confidence="0.998671666666667">
ger and given the fact that a rule-based POS tagger
has never been used for a Slavic language we have
tried to apply rule-based methods even for Czech.
</bodyText>
<sectionHeader confidence="0.984336" genericHeader="categories and subject descriptors">
2 STATISTICAL EXPERIMENTS
</sectionHeader>
<subsectionHeader confidence="0.815934">
2.1 CZECH EXPERIMENTS
2.1.1 CZECH TAGSET
</subsectionHeader>
<bodyText confidence="0.999660125">
Czech experiment is based upon ten basic POS
classes and the tags describe the possible combina-
tions of morphological categories for each POS class.
In most cases, the first letter of the tag denotes the
part-of-speech; the letters and numbers which follow
it describe combinations of morphological categories
(for a detailed description, see Table 2.1 and Table
2.2).
</bodyText>
<table confidence="0.989698941176471">
Morph. Cat. Poss. Description
Categ. Var. Val.
(see
Tab.
2.2)
gender 9 M masc. anim.
I masc. inanim.
N neuter
F feminine
number n S singular
P plural
tense t M past
P present
F future
mood m 0 indicative
R imperative
case c 1 nominative
</table>
<sectionHeader confidence="0.987213666666667" genericHeader="general terms">
2 genitive
3 dative
4 accusative
5 vocative
6 locative
7 instrumental
</sectionHeader>
<table confidence="0.9919411">
voice s A active voice
P passive voice
polarity a N negative
A affirmative
deg. of comp. d 1 base form
2 comparative
3 superlative
person P 1 1st
2 2nd
3 3rd
</table>
<tableCaption confidence="0.993246">
Table 2.1
</tableCaption>
<footnote confidence="0.503156">
Note especially, that Czech nouns are divided
into four classes according to gender (Sgall, 1967)
and into seven classes according to case.
</footnote>
<table confidence="0.998836782608696">
POS Class
nouns Ngnc
noun, abbreviations NZ
adjectives Agncda
verbs, infinitives VTa
verbs, transgressives VW ntsga
verbs, common V pnstmga
pronouns, personal PPpnc
pronouns, 3rd person PP3gnc
pronouns, possessive PRgncpgn
&amp;quot;svii j&amp;quot; —&amp;quot;his&amp;quot; referring to PSgnc
subject
reflexive particle &amp;quot;se&amp;quot; PEc
pronouns, demonstrative PDgnca
adverbs Oda
conjunctions S
numerals Cgnc
prepositions Rpreposition
interjections F
particles K
sentence boundaries T_SB
punctuation T_IP
unknown tag X
</table>
<tableCaption confidence="0.997947">
Table 2.2
</tableCaption>
<bodyText confidence="0.954582384615385">
Not all possible combinations of morphological
categories are meaningful, however. In addition to
these usual tags we have used special tags for sen-
tence boundaries, punctuation and a so called &amp;quot;un-
known tag&amp;quot;. In the experiments, we used only those
tags which occurred at least once in the training cor-
pus. To illustrate the form of the tagged text, we
present here the following examples from our train-
ing data, with comments:
wordltag #comments
doiRdo #&amp;quot; to&amp;quot;
(prepositions have their
own individuals tags)
</bodyText>
<equation confidence="0.980075">
oddfluINIS2 #&amp;quot; unit&amp;quot;
(noun, masculine inani-
mate, singular, genitive)
kiRk #&amp;quot; for&amp;quot;
(preposition)
snidaniINFS3 #&amp;quot; breakfast&amp;quot;
(noun, feminine, singular,
dative)
pou2ijelV3SAP0MA #&amp;quot; uses&amp;quot;
</equation>
<bodyText confidence="0.66207625">
(verb, 3rd person, singular,
active,
present, indicative, masc.
animate, affirmative)
</bodyText>
<listItem confidence="0.591766333333333">
#&amp;quot; for&amp;quot;
(preposition)
#&amp;quot; us&amp;quot;
</listItem>
<bodyText confidence="0.7888665">
(pronoun, personal, 1st
person, plural, accusative)
</bodyText>
<page confidence="0.862762666666667">
proiRpro
nasIPP1P4
112
</page>
<sectionHeader confidence="0.550742" genericHeader="method">
2.1.2 CZECH TRAINING DATA
</sectionHeader>
<bodyText confidence="0.9996734">
For training, we used the corpus collected dur-
ing the 1960&apos;s and 1970&apos;s in the Institute for Czech
Language at the Czechoslovak Academy of Sciences.
The corpus was originally hand-tagged, including
the lemmatization and syntactic tags. We had to
do some cleaning, which means that we have disre-
garded the lemmatization information and the syn-
tactic tag, as we were interested in words and tags
only. Tags used in this corpus were different from
our suggested tags: number of morphological cate-
gories was higher in the original sample and the no-
tation was also different. Thus we had to carry out
conversions of the original data into the format pre-
sented above, which resulted in the so-called Czech
&amp;quot;modified&amp;quot; corpus, with the following features:
</bodyText>
<table confidence="0.41305075">
tokens 621 015
words 72 445
tags 1 171
average number of tags per token 3.65
</table>
<tableCaption confidence="0.921162">
Table 2.3
</tableCaption>
<bodyText confidence="0.999706">
We used the complete &amp;quot;modified&amp;quot; corpus
(621015 tokens) in the experiments No. 1, No. 3,
No. 4 and a small part of this corpus in the experi-
ment No. 2, as indicated in Table 2.4.
</bodyText>
<figureCaption confidence="0.478589">
tokens 110 874
words 22 530
tags 882
average number of tags per token 2.36
</figureCaption>
<tableCaption confidence="0.866085">
Table 2.4
</tableCaption>
<subsectionHeader confidence="0.902391">
2.2 ENGLISH EXPERIMENTS
2.2.1 ENGLISH TAGSET
</subsectionHeader>
<bodyText confidence="0.9996328">
For the tagging of English texts, we used the
Penn Treebank tagset which contains 36 POS tags
and 12 other tags (for punctuation and the currency
symbol). A detailed description is available in (San-
torini, 1990).
</bodyText>
<sectionHeader confidence="0.40247" genericHeader="method">
2.2.2 ENGLISH TRAINING DATA
</sectionHeader>
<bodyText confidence="0.999558875">
For training in the English experiments, we used
WSJ (Marcus et al., 1993). We had to change the
format of WSJ to prepare it for our tagging soft-
ware. We used a small (100k tokens) part of WSJ in
the experiment No. 6 and the complete corpus (1M
tokens) in the experiments No. 5, No. 7 and No.
8. Table 2.5 contains the basic characteristics of the
training data.
</bodyText>
<table confidence="0.992126875">
Experiment Experiments
No. 6 No. 5, No. 7,
No. 8
tokens 110 530 1 287 749
words 13 582 51 433
tags 45 45
average number 1.72 2.34
of tags per token
</table>
<tableCaption confidence="0.998307">
Table 2.5
</tableCaption>
<subsectionHeader confidence="0.992033">
2.3 CZECH VS ENGLISH
</subsectionHeader>
<bodyText confidence="0.999739333333333">
Differences between Czech as a morphologically am-
biguous inflective language and English as language
with poor inflection are also reflected in the number
of tag bigrams and tag trigrams. The figures given
in Table 2.6 and 2.7 were obtained from the training
files.
</bodyText>
<table confidence="0.967615571428571">
Czech WSJ
corpus
x&lt;=4 24 064 x&lt;=10 459
4&lt;x&lt;=16 5 577 10&lt;x&lt;=100 411
16&lt;x&lt;=64 2 706 100&lt;x&lt;=1000 358
x&gt;64 1 581 x&gt;1000 225
bigrams 33 928 bigrams 1 453
</table>
<tableCaption confidence="0.960003">
Table 2.6 Number of bigrams with frequency x
</tableCaption>
<table confidence="0.999307285714286">
Czech WSJ
corpus
x&lt;=4 155 399 x&lt;=10 11 810
4&lt;x&lt;=16 16 371 10&lt;x&lt;=100 4 571
16&lt;x&lt;=64 4 380 100&lt;x&lt;=1000 1 645
x&gt;64 933 x&gt;1000 231
trigrams 177 083 trigrams 18 257
</table>
<tableCaption confidence="0.996886">
Table 2.7 Number of trigrams with frequency x
</tableCaption>
<bodyText confidence="0.921787090909091">
It is interesting to note the frequencies of the
most ambiguous tokens encountered in the whole
&amp;quot;modified&amp;quot; corpus and to compare them with the
English data. Table 2.8 and Table 2.9 contain the
first tokens with the highest number of possible tags
in the complete Czech &amp;quot;modified&amp;quot; corpus and in the
complete WSJ.
Token Frequency #tags
in train, data in train, data
jejich 1 087 51
jeho 1 087 46
</bodyText>
<table confidence="0.480219">
jeho&apos;Z 163 35
jejich2 150 25
vedouci 193 22
</table>
<tableCaption confidence="0.913263">
Table 2.8
</tableCaption>
<bodyText confidence="0.99994">
In the Czech &amp;quot;modified&amp;quot; corpus, the token &amp;quot;ye-
cloud&amp;quot; appeared 193 times and was tagged by twenty
two different tags: 13 tags for adjective and 9 tags
</bodyText>
<equation confidence="0.818003">
1 1 3
</equation>
<bodyText confidence="0.999731142857143">
for noun. The token &amp;quot;vedouci&amp;quot; means either: &amp;quot;lead-
ing&amp;quot; (adjective) or &amp;quot;manager&amp;quot; or &amp;quot;boss&amp;quot; (noun). The
following columns represent the tags for the token
&amp;quot;vedouci&amp;quot; and their frequencies in the training data;
for example &amp;quot;vedoucr was tagged twice as adjective,
feminine, plural, nominative, first degree, affirma-
tive.
</bodyText>
<table confidence="0.994133666666667">
2 vedoucilAFP11A
4 vedoucilAFP41A
6 vedoucilAFS11A 10 vedouciINFS1
11 vedoucilAFS21A 1 vedouclINFS2
1 vedoucilAFS31A 1 vedouciINFS3
4 vedoucilAFS41A 1 vedouclINFS4
5 vedoucilAFS71A 2 vedouclINFS7
2 vedoucilAIP11A 34 vedouciINMP1
11 vedoucilAMP11A 17 vedouciINMP4
3 vedoucilAMP41A 61 vedouciINMS1
12 vedoucilAMS11A 1 vedouciINMS5
2 vedoucilANP11A
2 vedoucilANS41A
Token Frequency #tags
in train, data in train, data
a 25 791 7
down 1 052 7
put 380 6
set 362 6
that 10 902 6
the 56 265 6
</table>
<tableCaption confidence="0.997306">
Table 2.9
</tableCaption>
<bodyText confidence="0.9999405">
It is clear from these figures that the two lan-
guages in question have quite different properties
and that nothing can be said without really going
through an experiment.
</bodyText>
<subsectionHeader confidence="0.975164">
2.4 THE ALGORITHM
</subsectionHeader>
<bodyText confidence="0.7999996">
We have used the basic source channel model (de-
scribed e.g. in (Merialdo, 1992)). The tagging pro-
cedure 0 selects a sequence of tags T for the sentence
W: : W T . In this case the optimal tagging
procedure is
</bodyText>
<equation confidence="0.99128375">
0(W) = argmaxTPr(TIW) =
= argmaxTPr(TIW)* Pr(W) =
= argmaxTPr(W,T) =
= argmaxTPr(WIT)* Pr(T).
</equation>
<bodyText confidence="0.999243545454545">
Our implementation is based on generating the
(W, T) pairs by means of a probabilistic model
using approximations of probability distributions
Pr (WIT) and Pr(T). The Pr(T) is based on tag bi-
grams and trigrams, and Pr(WIT) is approximated
as the product of Pr(wilti). The parameters have
been estimated by the usual maximum likelihood
training method, i.e. we approximated them as the
relative frequencies found in the training data with
smoothing based on estimated unigram probability
and uniform distributions.
</bodyText>
<subsectionHeader confidence="0.888605">
2.5 THE RESULTS
</subsectionHeader>
<bodyText confidence="0.997176">
The results of the Czech experiments are displayed
in Table 2.10.
</bodyText>
<table confidence="0.965188333333333">
No. 1 I No. 2 No. 3 I No. 4
test data 1 294 1 294 1 294 1 294
(tokens)
prob. unigram bigram bigram trigram
model
incorrect 444 334 239 244
tags
tagging 65.70% 74.19% 81.53% 81.14%
accuracy
</table>
<tableCaption confidence="0.998391">
Table 2.10
</tableCaption>
<bodyText confidence="0.999511375">
These results show, not surprisingly, of course,
that the more data, the better (results experiments
of No.2 vs. No.3), but in order to get better results
for a trigram tag prediction model, we would need
far more data. Clearly, if 88% trigrams occur four
times or less, then the statistics is not reliable. The
following tables show a detailed analysis of the errors
of the trigram experiment.
</bodyText>
<table confidence="0.999581461538462">
A C F K N 0
A 132 0 o o 6 3
C 0 4 o o 1 o
F 0 o o o o o
K 0 o o o o o
N 4 o o o 64 8
0 0 o o o 1 o
P 0 o o o o 3
R 0 o o o 1 1
S o o o 0 o o
V o o o o 3 8
T 0 o o o 1 o
X o o o o o o
</table>
<tableCaption confidence="0.981644">
Table 2.11a
</tableCaption>
<table confidence="0.999554153846154">
PRS V T X
A 2 2 2 2 1 0 50
C 0 0 0 0 0 0 5
F 0 0 0 0 0 0 0
K 0 0 1 0 0 1 2
N 0 4 2 2 5 4 93
0 0 0 0 1 1 0 3
P 19 0 0 0 1 2 23
R 0 0 0 0 0 2 4
S 0 0 0 0 0 2 2
V 0 3 8 28 1 2 53
T 0 0 0 0 0 0 1
X 5 0 1 2 0 0 8
</table>
<tableCaption confidence="0.998673">
Table 2.11b
</tableCaption>
<bodyText confidence="0.886929666666667">
The letters in the first column and row denote
POS classes, the interpunction (T) and the &amp;quot;un-
known tag&amp;quot; (X). The numbers show how many times
the tagger assigned an incorrect POS tag to a to-
ken in the test file. The total number of errors was
244. Altogether, fifty times the adjectives (A) were
</bodyText>
<page confidence="0.994794">
114
</page>
<bodyText confidence="0.998932461538461">
tagged incorrectly, nouns (N) 93 times, numbers (C)
5 times and etc. (see the last unmarked column in
Table 2.11b); to provide a better insight, we should
add that in 32 cases, when the adjective was cor-
rectly tagged as an adjective, but the mistakes ap-
peared in the assignment of morphological categories
(see Table 2.12), 6 times the adjective was tagged as
a noun, twice as a pronoun, 3 times as an adverb
and so on (see the second row in Table 2.11a). A
detailed look at Table 2.12 reveals that for 32 cor-
rectly marked adjectives the mistakes was 17 times
in gender, once in number, three times in gender and
case simultaneously and so on.
</bodyText>
<table confidence="0.480929">
A g n c g&amp;c g&amp;ri c&amp;a g&amp;n&amp;c g&amp;c&amp;d
32 17 1 6 3 2 1 1 1
</table>
<tableCaption confidence="0.841448625">
Table 2.12
Similar tables can be provided for nouns (Table
2.13), numerals (Table 2.14), pronouns(Table 2.15)
and verbs (Table 2.16a, Table 2.16b).
N g n c g&amp;c n&amp;c —&gt; NZ
64 11 5 41 2 4 1
Table 2.13
Table 2.14
</tableCaption>
<table confidence="0.872961444444444">
C g C
4 1 3
P g C g&amp;c PD —&gt; PP
19 8 7 3 1
Table 2.15
V P t n s n&amp;t p&amp;t t&amp;a
22 3 6 5 5 1 1 1
Table 2.16a
V g&amp;a p&amp;n&amp;t V —&gt;VT
</table>
<tableCaption confidence="0.8746635">
6 1 1 4
Table 2.16b
</tableCaption>
<bodyText confidence="0.621663454545455">
The results of our experiments with English are
displayed in Table 2.17.
No. 5 No. 6 No. 7 No. 8
test data 1 294 1 294 1 294 1 294
(tokens)
prob. unigram bigram bigram trigram
model
incorrect 136 81 41 37
tags
tagging 89.5% 93.74% 96.83% 97.14%
accuracy
</bodyText>
<tableCaption confidence="0.870733">
Table 2.17
</tableCaption>
<bodyText confidence="0.731830333333333">
the test data. Cases of incorrect tag assignment are
in boldface.
— Czech
wordlhand tag exp. exp. exp. exp.
No.4 No.3 No.2 No.12
nalRna Rna Rna Rna Rna
</bodyText>
<equation confidence="0.976671575757576">
piideINFS6 NFS6 NFS6 NFS6 NFS6
vlastiINFS2 NFS2 NFS2 NFS2 NFS2
radyINFS2 NFS2 NFS2 NFS2 NFS2
ZenINFP2 NFP2 NFP2 NFP2 NFP2
GustaINFS1 T_SB T_SB AFP21A XX
FueikovaINFS1 NFS1 NFS1 NFP2 NFS1
aISS SS SS SS SS
piedsedaINMS1 NMS1 NMS1 NMS1 NMS1
tivINZ NZ NZ NZ NZ
ssmINZ NZ NZ NZ NZ
JurajINMS1 NMS1 NMS1 NMS1 XX
VarholikINMS1 NMS1 NMS1 NMS1 NMS1
English
word I hand tag exp. exp. exp. exp.
No.8 No.7 No.6 No.5
WithIIN
stockINN
pricesINNS
hoveringIVBG
nearlIN
recordINN
levelsINNS
II,
aIDT
numberINN
oflIN
companiesINNS
havelVBP
been IVBN VBN
announcingIVBG VBG
stockINN NN
splitsINNS NNS
-1.
</equation>
<sectionHeader confidence="0.8886405" genericHeader="method">
2.6 A PROTOTYPE OF RANK XEROX
POS TAGGER FOR CZECH
</sectionHeader>
<bodyText confidence="0.999946125">
(Schiller, 1996) describes the general architecture of
the tool for noun phrase mark-up based on finite-
state techniques and statistical part-of-speech dis-
ambiguation for seven European languages. For
Czech, we created a prototype of the first step of
this process — the part-of-speech (POS) tagger —
using Rank Xerox tools (Tapanainen, 1995), (Cut-
ting et al., 1992).
</bodyText>
<subsubsectionHeader confidence="0.530388">
2.6.1 POS TAGSET
</subsubsectionHeader>
<bodyText confidence="0.9988685">
The first step of POS tagging is obviously a def-
inition of the POS tags. We performed three ex-
</bodyText>
<figure confidence="0.998047383333334">
IN
NN
NNS
VBG
IN
NN
NNS
DT
NN
IN
NNS
VBP
IN
NN
NNS
VBG
IN
NN
NNS
DT
NN
IN
NNS
VBP
VBN
VBG
NN
VBZ
IN
NN
NNS
IN
JJ
NN
NNS
DT
NN
IN
NNS
VBP
VBN
IN
NN
NN
IN
NN
NNS
VBG
IN
NN
NNS
DT
NN
IN
NNS
VBP
VBN
VBG
NN
VBZ
</figure>
<bodyText confidence="0.9911925">
To illustrate the results of our tagging experi-
ments, we present here short examples taken from 2We used a special tag XX for unknown words.
</bodyText>
<page confidence="0.997847">
115
</page>
<bodyText confidence="0.99932775">
periments. These experiments differ in the POS
tagset. During the first experiment we designed
tagset which contains 47 tags. The POS tagset can
be described as follows:
</bodyText>
<table confidence="0.999891294117647">
Category Symbol Pos. Description
Value
case c NOM nominative
GEN genitive
DAT dative
ACC accusative
VOC vocative
LOC locative
INS instrumental
INV invariant
kind of k PAP past
verb paticiple
PRI present
participle
INF infinitive
IMP imperative
TRA transgressive
</table>
<tableCaption confidence="0.954852">
Table 2.18
</tableCaption>
<table confidence="0.9999405">
POS tag Description
NOUN_c nouns + case
ADJ_c adjectives + case
PRON_c pronouns + case
NUM_c numerals + case
VERB _k verbs + kind of verb
ADV adverbs
PROP proper names
PREP prepositions
PSE reflexive particles &amp;quot;se&amp;quot;
CUT clitics
CONJ conjunctions
INTJ interjections
PTCL particles
DATE dates
CM comma
PUNCT interpunction
SENT sentence bundaries
</table>
<tableCaption confidence="0.997964">
Table 2.19
</tableCaption>
<bodyText confidence="0.999978833333333">
The analysis of the results of the first experiment
showed very high ambiguity between the nominative
and accusative cases of nouns, adjectives, pronouns
and numerals. That is why we replaced the tags
for nominative and accusative of nouns, adjectives,
pronouns and numerals by new tags NOUN_NA,
ADJ_NA, PRON_NA and NUM_NA (meaning nom-
inative or accusative, undistinguished). The rest of
the tags stayed unchanged. This led 43 POS tags.
In the third experiment we deleted the morphologi-
cal information for nouns and adjectives alltogether.
This process resulted in the final 34 POS tags.
</bodyText>
<sectionHeader confidence="0.841947" genericHeader="method">
2.6.2 RESULTS
</sectionHeader>
<bodyText confidence="0.99042175">
Figures representing the results of all experi-
ments are presented in the following table. We have
also included the results of English tagging using the
same Xerox tools.
</bodyText>
<table confidence="0.9912375">
language tags ambiguity4 tagging
accuracy
Czech 47 39% 91.7%
Czech 43 36% 93.0%
Czech 34 14% 96.2%
English 76 36% 97.8%
</table>
<tableCaption confidence="0.998516">
Table 2.20
</tableCaption>
<bodyText confidence="0.999955857142857">
The results show that the more radical reduction
of Czech tags (from 1171 to 34) the higher accuracy
of the results and the more comparable are the Czech
and English results. However, the difference in the
error rate is still more than visible — here we can
speculate that the reason is that Czech is &amp;quot;free&amp;quot; word
order language, whereas English is not.
</bodyText>
<sectionHeader confidence="0.946609" genericHeader="method">
3 A RULE-BASED EXPERIMENT
FOR CZECH
</sectionHeader>
<bodyText confidence="0.999599">
A simple rule-based part of speech (RBPOS) tag-
ger is introduced in (Brill, 1992). The accuracy of
this tagger for English is comparable to a stochas-
tic English POS tagger. From our point of view, it
is very interesting to compare the results of Czech
stochastic POS (SPOS) tagger and a modified RB-
POS tagger for Czech.
</bodyText>
<subsectionHeader confidence="0.981701">
3.1 TRAINING DATA
</subsectionHeader>
<bodyText confidence="0.99999275">
We used the same corpus used in the case of the
SPOS tagger for Czech. RBPOS requires different
input format; we thus converted the whole corpus
into this format, preserving the original contents.
</bodyText>
<subsectionHeader confidence="0.994284">
3.2 LEARNING
</subsectionHeader>
<bodyText confidence="0.999959666666667">
It is an obvious fact that the Czech tagset is totally
different from the English tagset. Therefore, we had
to modify the method for the initial guess. For Czech
the algorithm is: &amp;quot;If the word is W_SB (sentence
boundary) assign the tag T_SB, otherwise assign the
tag NNS1.&amp;quot;
</bodyText>
<sectionHeader confidence="0.999651333333333" genericHeader="method">
3.2.1 LEARNING RULES TO PREDICT
THE MOST LIKELY TAG FOR
UNKNOWN WORDS
</sectionHeader>
<bodyText confidence="0.998619666666667">
The first stage of training is learning rules to
predict the most likely tag for unknown words.
These rules operate on word types; for example, if
</bodyText>
<footnote confidence="0.9694525">
3The percentage of ambiguous word forms in the test
file.
</footnote>
<page confidence="0.997909">
116
</page>
<bodyText confidence="0.9946785">
a word ends by &amp;quot;0&amp;quot;, it is probably a masculine ad-
jective. To compare the influence of the size of the
training files on the accuracy of the tagger we per-
formed two subexperiments4:
</bodyText>
<table confidence="0.999056615384616">
No. 1 No. 2
TAGGED-CORPUS 37 971 9 576
(tokens)
TAGGED-CORPUS 15 297 5 031
(words)
TAGGED-CORPUS 738 495
(tags)
UNTAGGED-CORPUS 621 015 621 015
(tokens)
UNTAGGED-CORPUS 72 445 72 445
(words)
LEXRULEOUTFILE 101 75
(rules)
</table>
<tableCaption confidence="0.998702">
Table 3.1
</tableCaption>
<bodyText confidence="0.9822735">
We present here an example of rules taken from
LEXRULEOUTFILE from the exp. No. 1:
</bodyText>
<table confidence="0.9979508">
No. 1 No. 2
TAGGED-CORPUS-2 37 892 9 989
(tokens)
TAGGED-CORPUS-2 12 676 4 635
(words)
TAGGED-CORPUS-2 717 479
(tags)
TAGGED-ENTIRE-CORPUS 621 015 621 015
(tokens)
TAGGED-ENTIRE-CORPUS 72 445 72 445
(words)
TAGGED-ENTIRE-CORPUS 1 171 1 171
(tags)
CONTEXT-RULEFILE 487 61
(rules)
</table>
<tableCaption confidence="0.999234">
Table 3.2
</tableCaption>
<bodyText confidence="0.952037727272727">
We present here an example of the rules taken
from CONTEXT—RULEFILE from the exp. No. 1:
AFP21A AIP21A # change the tag
AFP21A to AIP21A
NEXT1OR2TAG if the following tag is
NIP2 NIP2
u hassuf 1 NIS2
y hassuf 1 NFS2
ho hassuf 2 AIS21A
ach hassuf 3 NFP6
nej addpref 3 02A
# change the tag to NIS2
if the suffix is &amp;quot;u&amp;quot;
# change the tag to NFS2
if the suffix is &amp;quot;y&amp;quot;
# change the tag to AIS21A
if the suffix is &amp;quot;ho&amp;quot;
# change the tag to NFP6
if the suffix is &amp;quot;ach&amp;quot;
# change the tag to 02A
if adding the prefix &amp;quot;nej&amp;quot;
results in a word
</bodyText>
<equation confidence="0.55575">
NIS2 NIS6
PREV10R2OR3TAG
Rv
NIS1 NIS4
PREV10R2TAG
Rna
</equation>
<sectionHeader confidence="0.936099" genericHeader="method">
3.2.3 RESULTS
</sectionHeader>
<bodyText confidence="0.99862975">
# change the tag NIS2
to NIS6
if the preceding tag is
Rv
# change the tag NIS1
to NIS4
if the preceding tag is
Rna
</bodyText>
<sectionHeader confidence="0.762402" genericHeader="method">
3.2.2 LEARNING CONTEXTUAL CUES
</sectionHeader>
<bodyText confidence="0.994469235294118">
The second stage of training is learning rules to
improve tagging accuracy based on contextual cues.
These rules operate on individual word tokens.
4We use the same names of files and variables as
Eric Brill in the rule-based POS tagger&apos;s documenta-
tion. TAGGED-CORPUS — manually tagged train-
ing corpus, UNTAGGED-CORPUS — collection of
all untagged texts, LEXRULEOUTFILE — the list
of transformations to determine the most likely tag
for unknown words, TAGGED-CORPUS-2 — manually
tagged training corpus, TAGGED-CORPUS-ENTIRE
— Czech &amp;quot;modified&amp;quot; corpus (the entire manually tagged
corpus), CONTEXT-RULEFILE — the list of transfor-
mations to improve accuracy based on contextual cues.
The tagger was tested on the same test file as
for the statistical experiments. We obtained the fol-
lowing results:
</bodyText>
<table confidence="0.99146075">
No. 1 No. 2
TEST-FILE 1 294 1 294
errors 262 294
tagging accuracy 79.75% 77.28%
</table>
<tableCaption confidence="0.997564">
Table 3. 3
</tableCaption>
<sectionHeader confidence="0.999153" genericHeader="conclusions">
4 CONCLUSION
</sectionHeader>
<bodyText confidence="0.999940166666667">
The results, though they might seem negative com-
pared to English, are still better than our original ex-
pectations. Before trying some completely different
approach, we would like to improve the current sim-
ple approach by some other simple measures: adding
a morphological analyzer (Hajit., 1994) as a front-
end to the tagger (serving as a &amp;quot;supplier&amp;quot; of pos-
sible tags, instead of just taking all tags occurring
in the training data for a given token), simplifying
the tagset, adding more data. However, the desired
positive effect of some of these measures is not guar-
anteed: for example, the average number of tags per
</bodyText>
<page confidence="0.991619">
117
</page>
<bodyText confidence="0.99925316">
token will increase after a morphological analyzer
is added. Success should be guaranteed, however,
by certain tagset reductions, as the original tagset
(even after the reductions mentioned above) is still
too detailed. This is especially true when comparing
it to English, where some tags represent, in fact, a
set of tags to be discriminated later (if ever). For ex-
ample, the tag VB used in the WSJ corpus actually
means &amp;quot;one of the (five different) tags for 1st person
sg., 2nd person sg., 1st person pl., etc.&amp;quot;. First, we
will reduce the tagset to correspond to our morpho-
logical analyzer which already uses a reduced one.
Then, the tagset will be reduced even further, but
nevertheless, not as much as we did for the Xerox-
tools-based experiment, because that tagset is too
&amp;quot;rough&amp;quot; for many applications, even though the re-
sults are good.
Regarding tagset reduction, we should note that
we haven&apos;t performed a &amp;quot;combined&amp;quot; experiment, i.e.
using the full (1100+) tagset for (thus) &amp;quot;interme-
diate&amp;quot; tagging, but only the reduced tagset for the
final results. However, it can be quite simply derived
from the tables 2.10, 2.11a and 2.11b, that the error
rate would not drop much: it will remain high at
about 6.5% (based on the results of experiment No.
4) using the very small tagset of 12 number or
lines in table 2.11a) tags used for part of speech iden-
tification. This is even much higher than the error
rate reported here for the smallest tagset used in the
&apos;pure&apos; experiment (sect. 2.6, table 2.20), which was
at 3.8%. This suggests that maybe the pure meth-
ods (which are obviously also simple to implement)
are in general better than the &amp;quot;combined&amp;quot; methods.
Another possibility of an improvement is to add
more data to allow for more reliable trigram esti-
mates. We will also add contemporary newspaper
texts to our training data in order to account for
recent language development. Hedging against fail-
ure of all these simple improvements, we are also
working on a different model using independent pre-
dictions for certain grammatical categories (and the
lemma itself), but the final shape of the model has
not yet been determined. This would mean to intro-
duce constraints on possible combinations of mor-
phological categories and take them into account
when &amp;quot;assembling&amp;quot; the final tag.
ACKNOWLEDGMENTS: The authors wish to
thank Eva Haji6ovd for her comments and sugges-
tions and Eric Brill, Jean-Pierre Chanod and Anne
Schiller who made their software tools available.
</bodyText>
<sectionHeader confidence="0.998727" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999778395833333">
Eric Brill. 1992. A Simple Rule-Based Part of
Speech Tagger. In: Proceedings of the Third
Conference on Applied Natural Language Pro-
cessing, Trento, Italy.
Eric Brill. 1993. A Corpus Based Approach To Lan-
guage Learning. PhD Dissertation, Department
of Computer and Information Science, Univer-
sity of Pennsylvania.
Eric Brill. 1994. Some Advances in Transformation-
-Based Part of Speech Tagging. In: Proceedings
of the Twelfth National Conference on Artificial
Intelligence.
Jan Hajie. 1994. Unification Morphology Gram-
mar. PhD Dissertation, Institute of Formal and
Applied Linguistics, Charles University, Prague,
Czech Republic.
Kenneth W. Church. 1992. Current Practice In Part
Of Speech Tagging And Suggestions For The
Future. For Henry Kueera, Studies in Slavic
Philology and Computational Linguistics, Michi-
gan Slavic Publications, Ann Arbor.
Doug Cutting, Julian Kupiec, Jan Pedersen and
Penelope Sibun 1992. A Practical Part-of-
Speech Tagger. In: Proceedings of the Third
Conference on Applied Natural Language Pro-
cessing , Trento, Italy.
Mitchell P. Marcus, Beatrice Santorini, and Mary-
Ann Marcinkiewicz 1993. Building A Large
Annotated Corpus Of English: The Penn Tree-
bank. Computational Linguistics, 19(2):313--
330.
Bernard Merialdo. 1992. Tagging Text With A
Probabilistic Model. Computational Linguis-
tics, 20(2):155-171
Beatrice Santorini. 1990. Part Of Speech Tag-
ging Guidelines For The Penn Treebank Project.
Technical report MS-CIS-90-47, Department of
Computer and Information Science, University
of Pennsylvania.
Anne Schiller. 1996. Multilingual Finite-State Noun
Phrase Extraction. ECAI&apos;96, Budapest, Hun-
gary.
Petr Sgall. 1967. The Generative Description of a
Language and the Czech Declension (In Czech).
Studie a prcice lingvisticke, 6. Prague.
Pasi Tapanainen. 1995. RXRC Finite-State Com-
piler. Technical Report MLTT-20, Rank Xerox
Research Center, Meylen, France.
</reference>
<page confidence="0.996192">
118
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.719252">
<title confidence="0.996267">Probabilistic and Rule-Based Tagger of an Inflective Language a Comparison</title>
<author confidence="0.99052">Haji Hladka</author>
<affiliation confidence="0.9944945">Institute of Formal and Applied Linguistics Faculty of Mathematics and Physics</affiliation>
<address confidence="0.892696">Malostranske nam. 25 CZ-118 00 Prague 1</address>
<email confidence="0.877398">e-mail:fhajic,hladkal@ufal.mff.cuni.cz</email>
<abstract confidence="0.999473133333333">(which is usually called taggingl). For example, the ending &amp;quot;-u&amp;quot; is not only highly ambiguous, but at the same time it carries complex information: it corresponds to the genitive, the dative and the locative singular for inanimate nouns, or the dative singular for animate nouns, or the accusative singular for feminine nouns, or the first person singular present tense active participle for certain verbs. There are two different techniques for text tagging: a stochastic technique and a rule-based technique. Each approach has some advantages — for stochastic techniques there exists a good theoretical framework, probabilities provide a straightforward way how to disambiguate tags for each word and probabilities can be acquired automatically from the data; for rule-based techniques the set of meaningful rules is automatically acquired and there exists an easy way how to find and implement improvements of the tagger. Small set of rules can be used, in contrast to the large statistical tables. Given the success of statistical methods in different areas, including text tagging, given the very positive results of English statistical taggers and given the fact that there existed no statistical tagger for any Slavic language we wanted to apply statistical methods even for the Czech language although it exhibits a rich inflection accompanied by a high degree of ambiguity. Originally, we expected that the result would be plain negative, getting no more than about two thirds of the tags correct. However, as we show below, we got better results than we had expected. We used the same statistical approach to tag both the English text and the Czech text. For English, we obtained results comparable with the results presented in (Merialdo, 1992) as well as in (Church, 1992). For Czech, we obtained results which are less satisfactory than those for English. Given the comparability of the accuracy of the rule-based part-of-speech (POS) tagger 1992) with the accuracy of the stochastic tag- Abstract We present results of probabilistic tagging of Czech texts in order to show how these techniques work for one of the highly morphologically ambiguous inflective languages. After description of the tag system used, we show the results of four experiments using a simple probabilistic model to tag Czech texts (unigram, two bigram experiments, and a trigram one). For comparison, we have applied the same code and settings to tag an English text (another four experiments) using the same size of training and test data in the experiments in order to avoid any doubt concerning the validity of the comparison. The experiments use the source channel model and maximum likelihood training on a Czech handtagged corpus and on tagged Wall Street Journal (WSJ) from the LDC collection. The experiments show (not surprisingly) that the more training data, the better is the success rate. The results also indicate that for inflective languages with 1000+ tags we have to develop a more sophisticated approach in order to get closer to an acceptable error rate. In order to compare two different approaches to text tagging — statistical and rule-based — we modified Eric Brill&apos;s rule-based part of speech tagger and carried out two more experiments on the Czech data, obtaining similar results in terms of the error rate. We have also run three more experiments with greatly reduced tagset to get another comparison based on similar tagset size.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A Simple Rule-Based Part of Speech Tagger. In:</title>
<date>1992</date>
<booktitle>Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="2232" citStr="Brill, 1992" startWordPosition="358" endWordPosition="359"> high degree of ambiguity. Originally, we expected that the result would be plain negative, getting no more than about two thirds of the tags correct. However, as we show below, we got better results than we had expected. We used the same statistical approach to tag both the English text and the Czech text. For English, we obtained results comparable with the results presented in (Merialdo, 1992) as well as in (Church, 1992). For Czech, we obtained results which are less satisfactory than those for English. Given the comparability of the accuracy of the rule-based part-of-speech (POS) tagger (Brill, 1992) with the accuracy of the stochastic tagAbstract We present results of probabilistic tagging of Czech texts in order to show how these techniques work for one of the highly morphologically ambiguous inflective languages. After description of the tag system used, we show the results of four experiments using a simple probabilistic model to tag Czech texts (unigram, two bigram experiments, and a trigram one). For comparison, we have applied the same code and settings to tag an English text (another four experiments) using the same size of training and test data in the experiments in order to avo</context>
<context position="18169" citStr="Brill, 1992" startWordPosition="3236" endWordPosition="3237">erox tools. language tags ambiguity4 tagging accuracy Czech 47 39% 91.7% Czech 43 36% 93.0% Czech 34 14% 96.2% English 76 36% 97.8% Table 2.20 The results show that the more radical reduction of Czech tags (from 1171 to 34) the higher accuracy of the results and the more comparable are the Czech and English results. However, the difference in the error rate is still more than visible — here we can speculate that the reason is that Czech is &amp;quot;free&amp;quot; word order language, whereas English is not. 3 A RULE-BASED EXPERIMENT FOR CZECH A simple rule-based part of speech (RBPOS) tagger is introduced in (Brill, 1992). The accuracy of this tagger for English is comparable to a stochastic English POS tagger. From our point of view, it is very interesting to compare the results of Czech stochastic POS (SPOS) tagger and a modified RBPOS tagger for Czech. 3.1 TRAINING DATA We used the same corpus used in the case of the SPOS tagger for Czech. RBPOS requires different input format; we thus converted the whole corpus into this format, preserving the original contents. 3.2 LEARNING It is an obvious fact that the Czech tagset is totally different from the English tagset. Therefore, we had to modify the method for </context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Eric Brill. 1992. A Simple Rule-Based Part of Speech Tagger. In: Proceedings of the Third Conference on Applied Natural Language Processing, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A Corpus Based Approach To Language Learning.</title>
<date>1993</date>
<tech>PhD Dissertation,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<marker>Brill, 1993</marker>
<rawString>Eric Brill. 1993. A Corpus Based Approach To Language Learning. PhD Dissertation, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Some Advances in Transformation-Based Part of Speech Tagging. In:</title>
<date>1994</date>
<booktitle>Proceedings of the Twelfth National Conference on Artificial Intelligence.</booktitle>
<marker>Brill, 1994</marker>
<rawString>Eric Brill. 1994. Some Advances in Transformation-Based Part of Speech Tagging. In: Proceedings of the Twelfth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajie</author>
</authors>
<title>Unification Morphology Grammar.</title>
<date>1994</date>
<tech>PhD Dissertation,</tech>
<institution>Institute of Formal and Applied Linguistics, Charles University,</institution>
<location>Prague, Czech Republic.</location>
<marker>Hajie, 1994</marker>
<rawString>Jan Hajie. 1994. Unification Morphology Grammar. PhD Dissertation, Institute of Formal and Applied Linguistics, Charles University, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>Current Practice In Part Of Speech Tagging And Suggestions For The Future. For Henry Kueera,</title>
<date>1992</date>
<booktitle>Studies in Slavic Philology and Computational Linguistics,</booktitle>
<publisher>Slavic Publications,</publisher>
<location>Michigan</location>
<contexts>
<context position="2048" citStr="Church, 1992" startWordPosition="329" endWordPosition="330">at there existed no statistical tagger for any Slavic language we wanted to apply statistical methods even for the Czech language although it exhibits a rich inflection accompanied by a high degree of ambiguity. Originally, we expected that the result would be plain negative, getting no more than about two thirds of the tags correct. However, as we show below, we got better results than we had expected. We used the same statistical approach to tag both the English text and the Czech text. For English, we obtained results comparable with the results presented in (Merialdo, 1992) as well as in (Church, 1992). For Czech, we obtained results which are less satisfactory than those for English. Given the comparability of the accuracy of the rule-based part-of-speech (POS) tagger (Brill, 1992) with the accuracy of the stochastic tagAbstract We present results of probabilistic tagging of Czech texts in order to show how these techniques work for one of the highly morphologically ambiguous inflective languages. After description of the tag system used, we show the results of four experiments using a simple probabilistic model to tag Czech texts (unigram, two bigram experiments, and a trigram one). For c</context>
</contexts>
<marker>Church, 1992</marker>
<rawString>Kenneth W. Church. 1992. Current Practice In Part Of Speech Tagging And Suggestions For The Future. For Henry Kueera, Studies in Slavic Philology and Computational Linguistics, Michigan Slavic Publications, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Cutting</author>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Penelope Sibun</author>
</authors>
<title>A Practical Part-ofSpeech Tagger. In:</title>
<date>1992</date>
<booktitle>Proceedings of the Third Conference on Applied Natural Language Processing ,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="15535" citStr="Cutting et al., 1992" startWordPosition="2773" endWordPosition="2777">p. No.8 No.7 No.6 No.5 WithIIN stockINN pricesINNS hoveringIVBG nearlIN recordINN levelsINNS II, aIDT numberINN oflIN companiesINNS havelVBP been IVBN VBN announcingIVBG VBG stockINN NN splitsINNS NNS -1. 2.6 A PROTOTYPE OF RANK XEROX POS TAGGER FOR CZECH (Schiller, 1996) describes the general architecture of the tool for noun phrase mark-up based on finitestate techniques and statistical part-of-speech disambiguation for seven European languages. For Czech, we created a prototype of the first step of this process — the part-of-speech (POS) tagger — using Rank Xerox tools (Tapanainen, 1995), (Cutting et al., 1992). 2.6.1 POS TAGSET The first step of POS tagging is obviously a definition of the POS tags. We performed three exIN NN NNS VBG IN NN NNS DT NN IN NNS VBP IN NN NNS VBG IN NN NNS DT NN IN NNS VBP VBN VBG NN VBZ IN NN NNS IN JJ NN NNS DT NN IN NNS VBP VBN IN NN NN IN NN NNS VBG IN NN NNS DT NN IN NNS VBP VBN VBG NN VBZ To illustrate the results of our tagging experiments, we present here short examples taken from 2We used a special tag XX for unknown words. 115 periments. These experiments differ in the POS tagset. During the first experiment we designed tagset which contains 47 tags. The POS ta</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun 1992. A Practical Part-ofSpeech Tagger. In: Proceedings of the Third Conference on Applied Natural Language Processing , Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>MaryAnn Marcinkiewicz</author>
</authors>
<title>Building A Large Annotated Corpus Of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="8123" citStr="Marcus et al., 1993" startWordPosition="1320" endWordPosition="1323">used the complete &amp;quot;modified&amp;quot; corpus (621015 tokens) in the experiments No. 1, No. 3, No. 4 and a small part of this corpus in the experiment No. 2, as indicated in Table 2.4. tokens 110 874 words 22 530 tags 882 average number of tags per token 2.36 Table 2.4 2.2 ENGLISH EXPERIMENTS 2.2.1 ENGLISH TAGSET For the tagging of English texts, we used the Penn Treebank tagset which contains 36 POS tags and 12 other tags (for punctuation and the currency symbol). A detailed description is available in (Santorini, 1990). 2.2.2 ENGLISH TRAINING DATA For training in the English experiments, we used WSJ (Marcus et al., 1993). We had to change the format of WSJ to prepare it for our tagging software. We used a small (100k tokens) part of WSJ in the experiment No. 6 and the complete corpus (1M tokens) in the experiments No. 5, No. 7 and No. 8. Table 2.5 contains the basic characteristics of the training data. Experiment Experiments No. 6 No. 5, No. 7, No. 8 tokens 110 530 1 287 749 words 13 582 51 433 tags 45 45 average number 1.72 2.34 of tags per token Table 2.5 2.3 CZECH VS ENGLISH Differences between Czech as a morphologically ambiguous inflective language and English as language with poor inflection are also r</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and MaryAnn Marcinkiewicz 1993. Building A Large Annotated Corpus Of English: The Penn Treebank. Computational Linguistics, 19(2):313--330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging Text With A Probabilistic Model.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>20--2</pages>
<contexts>
<context position="2019" citStr="Merialdo, 1992" startWordPosition="323" endWordPosition="324">l taggers and given the fact that there existed no statistical tagger for any Slavic language we wanted to apply statistical methods even for the Czech language although it exhibits a rich inflection accompanied by a high degree of ambiguity. Originally, we expected that the result would be plain negative, getting no more than about two thirds of the tags correct. However, as we show below, we got better results than we had expected. We used the same statistical approach to tag both the English text and the Czech text. For English, we obtained results comparable with the results presented in (Merialdo, 1992) as well as in (Church, 1992). For Czech, we obtained results which are less satisfactory than those for English. Given the comparability of the accuracy of the rule-based part-of-speech (POS) tagger (Brill, 1992) with the accuracy of the stochastic tagAbstract We present results of probabilistic tagging of Czech texts in order to show how these techniques work for one of the highly morphologically ambiguous inflective languages. After description of the tag system used, we show the results of four experiments using a simple probabilistic model to tag Czech texts (unigram, two bigram experimen</context>
<context position="10939" citStr="Merialdo, 1992" startWordPosition="1816" endWordPosition="1817">1 vedouclINFS4 5 vedoucilAFS71A 2 vedouclINFS7 2 vedoucilAIP11A 34 vedouciINMP1 11 vedoucilAMP11A 17 vedouciINMP4 3 vedoucilAMP41A 61 vedouciINMS1 12 vedoucilAMS11A 1 vedouciINMS5 2 vedoucilANP11A 2 vedoucilANS41A Token Frequency #tags in train, data in train, data a 25 791 7 down 1 052 7 put 380 6 set 362 6 that 10 902 6 the 56 265 6 Table 2.9 It is clear from these figures that the two languages in question have quite different properties and that nothing can be said without really going through an experiment. 2.4 THE ALGORITHM We have used the basic source channel model (described e.g. in (Merialdo, 1992)). The tagging procedure 0 selects a sequence of tags T for the sentence W: : W T . In this case the optimal tagging procedure is 0(W) = argmaxTPr(TIW) = = argmaxTPr(TIW)* Pr(W) = = argmaxTPr(W,T) = = argmaxTPr(WIT)* Pr(T). Our implementation is based on generating the (W, T) pairs by means of a probabilistic model using approximations of probability distributions Pr (WIT) and Pr(T). The Pr(T) is based on tag bigrams and trigrams, and Pr(WIT) is approximated as the product of Pr(wilti). The parameters have been estimated by the usual maximum likelihood training method, i.e. we approximated the</context>
</contexts>
<marker>Merialdo, 1992</marker>
<rawString>Bernard Merialdo. 1992. Tagging Text With A Probabilistic Model. Computational Linguistics, 20(2):155-171</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Santorini</author>
</authors>
<title>Part Of Speech Tagging Guidelines For The Penn Treebank Project.</title>
<date>1990</date>
<tech>Technical report MS-CIS-90-47,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="8019" citStr="Santorini, 1990" startWordPosition="1304" endWordPosition="1306">features: tokens 621 015 words 72 445 tags 1 171 average number of tags per token 3.65 Table 2.3 We used the complete &amp;quot;modified&amp;quot; corpus (621015 tokens) in the experiments No. 1, No. 3, No. 4 and a small part of this corpus in the experiment No. 2, as indicated in Table 2.4. tokens 110 874 words 22 530 tags 882 average number of tags per token 2.36 Table 2.4 2.2 ENGLISH EXPERIMENTS 2.2.1 ENGLISH TAGSET For the tagging of English texts, we used the Penn Treebank tagset which contains 36 POS tags and 12 other tags (for punctuation and the currency symbol). A detailed description is available in (Santorini, 1990). 2.2.2 ENGLISH TRAINING DATA For training in the English experiments, we used WSJ (Marcus et al., 1993). We had to change the format of WSJ to prepare it for our tagging software. We used a small (100k tokens) part of WSJ in the experiment No. 6 and the complete corpus (1M tokens) in the experiments No. 5, No. 7 and No. 8. Table 2.5 contains the basic characteristics of the training data. Experiment Experiments No. 6 No. 5, No. 7, No. 8 tokens 110 530 1 287 749 words 13 582 51 433 tags 45 45 average number 1.72 2.34 of tags per token Table 2.5 2.3 CZECH VS ENGLISH Differences between Czech as</context>
</contexts>
<marker>Santorini, 1990</marker>
<rawString>Beatrice Santorini. 1990. Part Of Speech Tagging Guidelines For The Penn Treebank Project. Technical report MS-CIS-90-47, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Schiller</author>
</authors>
<date>1996</date>
<booktitle>Multilingual Finite-State Noun Phrase Extraction. ECAI&apos;96,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="15186" citStr="Schiller, 1996" startWordPosition="2721" endWordPosition="2722">INFS2 NFS2 NFS2 NFS2 NFS2 radyINFS2 NFS2 NFS2 NFS2 NFS2 ZenINFP2 NFP2 NFP2 NFP2 NFP2 GustaINFS1 T_SB T_SB AFP21A XX FueikovaINFS1 NFS1 NFS1 NFP2 NFS1 aISS SS SS SS SS piedsedaINMS1 NMS1 NMS1 NMS1 NMS1 tivINZ NZ NZ NZ NZ ssmINZ NZ NZ NZ NZ JurajINMS1 NMS1 NMS1 NMS1 XX VarholikINMS1 NMS1 NMS1 NMS1 NMS1 English word I hand tag exp. exp. exp. exp. No.8 No.7 No.6 No.5 WithIIN stockINN pricesINNS hoveringIVBG nearlIN recordINN levelsINNS II, aIDT numberINN oflIN companiesINNS havelVBP been IVBN VBN announcingIVBG VBG stockINN NN splitsINNS NNS -1. 2.6 A PROTOTYPE OF RANK XEROX POS TAGGER FOR CZECH (Schiller, 1996) describes the general architecture of the tool for noun phrase mark-up based on finitestate techniques and statistical part-of-speech disambiguation for seven European languages. For Czech, we created a prototype of the first step of this process — the part-of-speech (POS) tagger — using Rank Xerox tools (Tapanainen, 1995), (Cutting et al., 1992). 2.6.1 POS TAGSET The first step of POS tagging is obviously a definition of the POS tags. We performed three exIN NN NNS VBG IN NN NNS DT NN IN NNS VBP IN NN NNS VBG IN NN NNS DT NN IN NNS VBP VBN VBG NN VBZ IN NN NNS IN JJ NN NNS DT NN IN NNS VBP V</context>
</contexts>
<marker>Schiller, 1996</marker>
<rawString>Anne Schiller. 1996. Multilingual Finite-State Noun Phrase Extraction. ECAI&apos;96, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
</authors>
<title>The Generative Description of a Language and the Czech Declension (In Czech). Studie a prcice lingvisticke, 6.</title>
<date>1967</date>
<location>Prague.</location>
<contexts>
<context position="5229" citStr="Sgall, 1967" startWordPosition="862" endWordPosition="863">cription, see Table 2.1 and Table 2.2). Morph. Cat. Poss. Description Categ. Var. Val. (see Tab. 2.2) gender 9 M masc. anim. I masc. inanim. N neuter F feminine number n S singular P plural tense t M past P present F future mood m 0 indicative R imperative case c 1 nominative 2 genitive 3 dative 4 accusative 5 vocative 6 locative 7 instrumental voice s A active voice P passive voice polarity a N negative A affirmative deg. of comp. d 1 base form 2 comparative 3 superlative person P 1 1st 2 2nd 3 3rd Table 2.1 Note especially, that Czech nouns are divided into four classes according to gender (Sgall, 1967) and into seven classes according to case. POS Class nouns Ngnc noun, abbreviations NZ adjectives Agncda verbs, infinitives VTa verbs, transgressives VW ntsga verbs, common V pnstmga pronouns, personal PPpnc pronouns, 3rd person PP3gnc pronouns, possessive PRgncpgn &amp;quot;svii j&amp;quot; —&amp;quot;his&amp;quot; referring to PSgnc subject reflexive particle &amp;quot;se&amp;quot; PEc pronouns, demonstrative PDgnca adverbs Oda conjunctions S numerals Cgnc prepositions Rpreposition interjections F particles K sentence boundaries T_SB punctuation T_IP unknown tag X Table 2.2 Not all possible combinations of morphological categories are meaningfu</context>
</contexts>
<marker>Sgall, 1967</marker>
<rawString>Petr Sgall. 1967. The Generative Description of a Language and the Czech Declension (In Czech). Studie a prcice lingvisticke, 6. Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
</authors>
<title>RXRC Finite-State Compiler.</title>
<date>1995</date>
<tech>Technical Report MLTT-20,</tech>
<institution>Rank Xerox Research Center,</institution>
<location>Meylen, France.</location>
<contexts>
<context position="15511" citStr="Tapanainen, 1995" startWordPosition="2771" endWordPosition="2772">ag exp. exp. exp. exp. No.8 No.7 No.6 No.5 WithIIN stockINN pricesINNS hoveringIVBG nearlIN recordINN levelsINNS II, aIDT numberINN oflIN companiesINNS havelVBP been IVBN VBN announcingIVBG VBG stockINN NN splitsINNS NNS -1. 2.6 A PROTOTYPE OF RANK XEROX POS TAGGER FOR CZECH (Schiller, 1996) describes the general architecture of the tool for noun phrase mark-up based on finitestate techniques and statistical part-of-speech disambiguation for seven European languages. For Czech, we created a prototype of the first step of this process — the part-of-speech (POS) tagger — using Rank Xerox tools (Tapanainen, 1995), (Cutting et al., 1992). 2.6.1 POS TAGSET The first step of POS tagging is obviously a definition of the POS tags. We performed three exIN NN NNS VBG IN NN NNS DT NN IN NNS VBP IN NN NNS VBG IN NN NNS DT NN IN NNS VBP VBN VBG NN VBZ IN NN NNS IN JJ NN NNS DT NN IN NNS VBP VBN IN NN NN IN NN NNS VBG IN NN NNS DT NN IN NNS VBP VBN VBG NN VBZ To illustrate the results of our tagging experiments, we present here short examples taken from 2We used a special tag XX for unknown words. 115 periments. These experiments differ in the POS tagset. During the first experiment we designed tagset which cont</context>
</contexts>
<marker>Tapanainen, 1995</marker>
<rawString>Pasi Tapanainen. 1995. RXRC Finite-State Compiler. Technical Report MLTT-20, Rank Xerox Research Center, Meylen, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>