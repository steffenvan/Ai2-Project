<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004867">
<title confidence="0.999312">
Adaptivity in Question Answering
with User Modelling and a Dialogue Interface
</title>
<author confidence="0.884508">
Silvia Quarteroni and Suresh Manandhar
</author>
<affiliation confidence="0.8887475">
Department of Computer Science
University of York
</affiliation>
<address confidence="0.65337">
York YO10 5DD
UK
</address>
<email confidence="0.994338">
{silvia,suresh}@cs.york.ac.uk
</email>
<sectionHeader confidence="0.993821" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9980258">
Most question answering (QA) and infor-
mation retrieval (IR) systems are insensi-
tive to different users’ needs and prefer-
ences, and also to the existence of multi-
ple, complex or controversial answers. We
introduce adaptivity in QA and IR by cre-
ating a hybrid system based on a dialogue
interface and a user model. Keywords:
question answering, information retrieval,
user modelling, dialogue interfaces.
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975961538462">
While standard information retrieval (IR) systems
present the results of a query in the form of a
ranked list of relevant documents, question an-
swering (QA) systems attempt to return them in
the form of sentences (or paragraphs, or phrases),
responding more precisely to the user’s request.
However, in most state-of-the-art QA systems
the output remains independent of the questioner’s
characteristics, goals and needs. In other words,
there is a lack of user modelling: a 10-year-old and
a University History student would get the same
answer to the question: “When did the Middle
Ages begin?”. Secondly, most of the effort of cur-
rent QA is on factoid questions, i.e. questions con-
cerning people, dates, etc., which can generally be
answered by a short sentence or phrase (Kwok et
al., 2001). The main QA evaluation campaign,
TREC-QA 1, has long focused on this type of
questions, for which the simplifying assumption is
that there exists only one correct answer. Even re-
cent TREC campaigns (Voorhees, 2003; Voorhees,
2004) do not move sufficiently beyond the factoid
approach. They account for two types of non-
factoid questions –list and definitional– but not for
non-factoid answers. In fact, a) TREC defines list
questions as questions requiring multiple factoid
</bodyText>
<footnote confidence="0.905028">
1http://trec.nist.gov
</footnote>
<bodyText confidence="0.99915148">
answers, b) it is clear that a definition question
may be answered by spotting definitional passages
(what is not clear is how to spot them). However,
accounting for the fact that some simple questions
may have complex or controversial answers (e.g.
“What were the causes of World War II?”) remains
an unsolved problem. We argue that in such situa-
tions returning a short paragraph or text snippet is
more appropriate than exact answer spotting. Fi-
nally, QA systems rarely interact with the user:
the typical session involves the user submitting a
query and the system returning a result; the session
is then concluded.
To respond to these deficiencies of existing QA
systems, we propose an adaptive system where a
QA module interacts with a user model and a di-
alogue interface (see Figure 1). The dialogue in-
terface provides the query terms to the QA mod-
ule, and the user model (UM) provides criteria
to adapt query results to the user’s needs. Given
such information, the goal of the QA module is to
be able to discriminate between simple/factoid an-
swers and more complex answers, presenting them
in a TREC-style manner in the first case and more
appropriately in the second.
</bodyText>
<figureCaption confidence="0.979011">
Figure 1: High level system architecture
</figureCaption>
<bodyText confidence="0.998297666666667">
Related work To our knowledge, our system is
among the first to address the need for a different
approach to non-factoid (complex/controversial)
</bodyText>
<figure confidence="0.988662769230769">
QUESTION
PROCESSING
DIALOGUE
INTERFACE
DOCUMENT
RETRIEVAL
ANSWER
EXTRACTION
Answer
USER
MODEL
QA MODULE
Question
</figure>
<page confidence="0.996688">
199
</page>
<bodyText confidence="0.9999811">
answers. Although the three-tiered structure of
our QA module reflects that of a typical web-
based QA system, e.g. MULDER (Kwok et al.,
2001), a significant aspect of novelty in our archi-
tecture is that the QA component is supported by
the user model. Additionally, we drastically re-
duce the amount of linguistic processing applied
during question processing and answer generation,
while giving more relief to the post-retrieval phase
and to the role of the UM.
</bodyText>
<sectionHeader confidence="0.946903" genericHeader="method">
2 User model
</sectionHeader>
<bodyText confidence="0.9987978">
Depending on the application of interest, the UM
can be designed to suit the information needs of
the QA module in different ways. As our current
application, YourQA2, is a learning-oriented, web-
based system, our UM consists of the user’s:
</bodyText>
<listItem confidence="0.833066">
1) age range, a E {7 − 11,11 − 16, adult};
2) reading level, r E {poor, medium, good};
3) webpages of interest/bookmarks, w.
</listItem>
<bodyText confidence="0.982289166666667">
Analogies can be found with the SeAn (Ardissono
et al., 2001) and SiteIF (Magnini and Strapparava,
2001) news recommender systems where age and
browsing history, respectively, are part of the UM.
In this paper we focus on how to filter and adapt
search results using the reading level parameter.
</bodyText>
<sectionHeader confidence="0.997154" genericHeader="method">
3 Dialogue interface
</sectionHeader>
<bodyText confidence="0.999836444444445">
The dialogue component will interact with both
the UM and the QA module. From a UM point of
view, the dialogue history will store previous con-
versations useful to construct and update a model
of the user’s interests, goals and level of under-
standing. From a QA point of view, the main goal
of the dialogue component is to provide users with
a friendly interface to build their requests. A typi-
cal scenario would start this way:
</bodyText>
<listItem confidence="0.671956">
— System: Hi, how can I help you?
— User: I would like to know what books Roald Dahl wrote.
</listItem>
<bodyText confidence="0.999834875">
The query sentence “what books Roald Dahl wrote”, is
thus extracted and handed to the QA module. In a
second phase, the dialogue module is responsible
for providing the answer to the user once the QA
module has generated it. The dialogue manager
consults the UM to decide on the most suitable
formulation of the answer (e.g. short sentences)
and produce the final answer accordingly, e.g.:
</bodyText>
<note confidence="0.478722666666667">
— System: Roald Dahl wrote many books for kids and adults,
including: “The Witches”, “Charlie and the Chocolate Fac-
tory”, and “James and the Giant Peach&amp;quot;.
</note>
<footnote confidence="0.802166">
2http://www.cs.york.ac.uk/aig/aqua
</footnote>
<sectionHeader confidence="0.952141" genericHeader="method">
4 Question Answering Module
</sectionHeader>
<bodyText confidence="0.999766">
The flow between the three QA phases – question
processing, document retrieval and answer gener-
ation – is described below (see Fig. 2).
</bodyText>
<subsectionHeader confidence="0.999505">
4.1 Question processing
</subsectionHeader>
<bodyText confidence="0.9966764">
We perform query expansion, which consists in
creating additional queries using question word
synonyms in the purpose of increasing the recall
of the search engine. Synonyms are obtained via
the WordNet 2.0 3 lexical database.
</bodyText>
<figureCaption confidence="0.994722">
Figure 2: Diagram of the QA module
</figureCaption>
<subsectionHeader confidence="0.858765">
4.2 Retrieval
</subsectionHeader>
<bodyText confidence="0.9994946875">
Document retrieval We retrieve the top 20 doc-
uments returned by Google4 for each query pro-
duced via query expansion. These are processed
in the following steps, which progressively narrow
the part of the text containing relevant informa-
tion.
Keyphrase extraction Once the documents are
retrieved, we perform keyphrase extraction to de-
termine their three most relevant topics using Kea
(Witten et al., 1999), an extractor based on Naïve
Bayes classification.
Estimation of reading levels To adapt the read-
ability of the results to the user, we estimate
the reading difficulty of the retrieved documents
using the Smoothed Unigram Model (Collins-
Thompson and Callan, 2004), which proceeds in
</bodyText>
<footnote confidence="0.9997495">
3http://wordnet.princeton.edu
4http://www.google.com
</footnote>
<figure confidence="0.997425166666667">
RANIING
Ranked
Answer
Candidates
Question
IEYPHRASE
EXTRACTION
CLUSTERING
QUERY
EXPANSION
DOCUMENT
RETRIEVAL
SEMANTIC
SIMILARITY
UM-BASED
FILTERING
ESTIMATION
OF READING
LEVELS
Language
Models
User Model
Reading
Level
</figure>
<page confidence="0.971169">
200
</page>
<bodyText confidence="0.999927368421053">
two phases. 1) In the training phase, sets of repre-
sentative documents are collected for a given num-
ber of reading levels. Then, a unigram language
model is created for each set, i.e. a list of (word
stem, probability) entries for the words appearing
in its documents. Our models account for the fol-
lowing reading levels: poor (suitable for ages 7–
11), medium (ages 11–16) and good (adults). 2)
In the test phase, given an unclassified document
D, its estimated reading level is the model lmi
maximizing the likelihood that D E lmi5.
Clustering We use the extracted topics and es-
timated reading levels as features to apply hierar-
chical clustering on the documents. We use the
WEKA (Witten and Frank, 2000) implementation
of the Cobweb algorithm. This produces a tree
where each leaf corresponds to one document, and
sibling leaves denote documents with similar top-
ics and reading difficulty.
</bodyText>
<subsectionHeader confidence="0.997975">
4.3 Answer extraction
</subsectionHeader>
<bodyText confidence="0.999625476190476">
In this phase, the clustered documents are filtered
based on the user model and answer sentences are
located and formatted for presentation.
UM-based filtering The documents in the clus-
ter tree are filtered according to their reading diffi-
culty: only those compatible with the UM’s read-
ing level are retained for further analysis6.
Semantic similarity Within each of the retained
documents, we seek the sentences which are se-
mantically most relevant to the query by applying
the metric in (Alfonseca et al., 2001): we rep-
resent each document sentence p and the query
q as word sets P = {pw1, ... , pwm} and Q =
{qw1, ... , qwn}. The distance from p to q is then
distq(p) = E1&lt;i&lt;m minj[d(pwi, qwj)], where
d(pwi,qwj) is the word-level distance between
pwi and qwj based on (Jiang and Conrath, 1997).
Ranking Given the query q, we thus locate
in each document D the sentence p* such that
p* = argminpED[distq(p)]; then, distq(p*) be-
comes the document score. Moreover, each clus-
</bodyText>
<footnote confidence="0.83491">
5The likelihood is estimated using the formula:
</footnote>
<equation confidence="0.472379">
Li,D = Ew∈D C(w, D) · log(P(w|lmi)), where w is a
</equation>
<bodyText confidence="0.9881388">
word in the document, C(w, d) is the number of occurrences
of w in D and P(w|lmi) is the probability with which w
occurs in lmi
6However, if their number does not exceed a given thresh-
old, we accept in our candidate set part of the documents hav-
ing the next lowest readability – or a medium readability if the
user’s reading level is low
ter is assigned a score consisting in the maximal
score of the documents composing it. This allows
to rank not only documents, but also clusters, and
present results grouped by cluster in decreasing or-
der of document score.
Answer presentation We present our answers
in an HTML page, where results are listed follow-
ing the ranking described above. Each result con-
sists of the title and clickable URL of the originat-
ing document, and the passage where the sentence
which best answers the query is located and high-
lighted. Question keywords and potentially useful
information such as named entities are in colour.
</bodyText>
<sectionHeader confidence="0.945173" genericHeader="method">
5 Sample result
</sectionHeader>
<bodyText confidence="0.93973547826087">
We have been running our system on a range
of queries, including factoid/simple, complex and
controversial ones. As an example of the latter, we
report the query “Who wrote the Iliad?”, which is
a subject of debate. These are some top results:
— UMgood: “Most Classicists would agree that, whether
there was ever such a composer as &amp;quot;Homer&amp;quot; or not, the
Homeric poems are the product of an oral tradition [...]
Could the Iliad and Odyssey have been oral-formulaic po-
ems, composed on the spot by the poet using a collection of
memorized traditional verses and phases?”
— UMmed: “No reliable ancient evidence for Homer –
[...] General ancient assumption that same poet wrote Il-
iad and Odyssey (and possibly other poems) questioned by
many modern scholars: differences explained biographi-
cally in ancient world (e g wrote Od. in old age); but simi-
larities could be due to imitation.”
— UMpoor: “Homer wrote The Iliad and The Odyssey
(at least, supposedly a blind bard named &amp;quot;Homer&amp;quot; did).”
In the three results, the problem of attribution of
the Iliad is made clearly visible: document pas-
sages provide a context which helps to explain the
controversy at different levels of difficulty.
</bodyText>
<sectionHeader confidence="0.996224" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.9924081">
Since YourQA does not single out one correct an-
swer phrase, TREC evaluation metrics are not suit-
able for it. A user-centred methodology to assess
how individual information needs are met is more
appropriate. We base our evaluation on (Su, 2003),
which proposes a comprehensive search engine
evaluation model, defining the following metrics:
1. Relevance: we define strict precision (P1) as
the ratio between the number of results rated as
relevant and all the returned results, and loose pre-
</bodyText>
<page confidence="0.973131">
201
</page>
<bodyText confidence="0.977325846153846">
Query A9 A, AP
When did the Middle Ages begin? 0,91 0,82 0,68
Who painted the Sistine Chapel? 0,85 0,72 0,79
When did the Romans invade Britain? 0,87 0,74 0,82
Who was a famous cubist? 0,90 0,75 0,85
Who was the first American in space? 0,94 0,80 0,72
Definition of metaphor 0,95 0,81 0,38
average 0,94 0,85 0,72
cision (P2) as the ratio between the number of re-
sults rated as relevant or partially relevant and all
the returned results.
2. User satisfaction: a 7-point Likert scale7 is used
to assess the user’s satisfaction with loose preci-
sion of results (S1) and query success (S2).
3. Reading level accuracy: given the set R of re-
sults returned for a reading level r, Ar is the ratio
between the number of results E R rated by the
users as suitable for r and |R|.
4. Overall utility (U): the search session as a
whole is assessed via a 7-point Likert scale.
We performed our evaluation by running 24
queries (some of which in Tab. 2) on Google and
YourQA and submitting the results –i.e. Google
result page snippets and YourQA passages– of
both to 20 evaluators, along with a questionnaire.
The relevance results (P1 and P2) in Tab. 1 show a
</bodyText>
<table confidence="0.996494666666667">
P1 P2 S1 S2 U
Google 0,39 0,63 4,70 4,61 4,59
YourQA 0,51 0,79 5,39 5,39 5,57
</table>
<tableCaption confidence="0.999832">
Table 1: Evaluation results
</tableCaption>
<bodyText confidence="0.999379571428571">
10-15% difference in favour of YourQA for both
strict and loose precision. The coarse seman-
tic processing applied and context visualisation
thus contribute to creating more relevant passages.
Both user satisfaction results (S1 and S2) in Tab.
1 also denote a higher level of satisfaction tributed
to YourQA. Tab. 2 shows that evaluators found our
</bodyText>
<tableCaption confidence="0.942296">
Table 2: Sample queries and accuracy values
</tableCaption>
<bodyText confidence="0.9388544">
results appropriate for the reading levels to which
they were assigned. The accuracy tended to de-
crease (from 94% to 72%) with the level: it is
indeed more constraining to conform to a lower
reading level than to a higher one. Finally, the
7This measure – ranging from 1= “extremely unsatisfac-
tory” to 7=“extremely satisfactory” – is particularly suitable
to assess how well a system meets user’s search needs.
general satisfaction values for U in Tab. 1 show
an improved preference for YourQA.
</bodyText>
<sectionHeader confidence="0.996734" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999925">
A user-tailored QA system is proposed where a
user model contributes to adapting answers to the
user’s needs and presenting them appropriately.
A preliminary evaluation of our core QA module
shows a positive feedback from human assessors.
Our short term goals involve performing a more
extensive evaluation and implementing a dialogue
interface to improve the system’s interactivity.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883289473684">
E. Alfonseca, M. DeBoni, J.-L. Jara-Valencia, and
S. Manandhar. 2001. A prototype question answer-
ing system using syntactic and semantic information
for answer retrieval. In Text REtrieval Conference.
L. Ardissono, L. Console, and I. Torre. 2001. An adap-
tive system for the personalized access to news. AI
Commun., 14(3):129–147.
K. Collins-Thompson and J. P. Callan. 2004. A lan-
guage modeling approach to predicting reading dif-
ficulty. In Proceedings of HLT/NAACL.
J. J. Jiang and D. W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference Re-
search on Computational Linguistics (ROCLING X).
C. C. T. Kwok, O. Etzioni, and D. S. Weld. 2001. Scal-
ing question answering to the web. In World Wide
Web, pages 150–161.
Bernardo Magnini and Carlo Strapparava. 2001. Im-
proving user modelling with content-based tech-
niques. In UM: Proceedings of the 8th Int. Confer-
ence, volume 2109 of LNCS. Springer.
L. T. Su. 2003. A comprehensive and systematic
model of user evaluation of web search engines: Ii.
an evaluation by undergraduates. J. Am. Soc. Inf.
Sci. Technol., 54(13):1193–1223.
E. M. Voorhees. 2003. Overview of the TREC 2003
question answering track. In Text REtrieval Confer-
ence.
E. M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In Text REtrieval Confer-
ence.
H. Witten and E. Frank. 2000. Data Mining: Practical
Machine Learning Tools and Techniques with Java
Implementation. Morgan Kaufmann.
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and
C. G. Nevill-Manning. 1999. KEA: Practical au-
tomatic keyphrase extraction. In ACM DL, pages
254–255.
</reference>
<page confidence="0.998328">
202
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.754114">
<title confidence="0.9957945">Adaptivity in Question Answering with User Modelling and a Dialogue Interface</title>
<author confidence="0.999678">Silvia Quarteroni</author>
<author confidence="0.999678">Suresh Manandhar</author>
<affiliation confidence="0.9996875">Department of Computer Science University of York</affiliation>
<address confidence="0.8834215">York YO10 5DD UK</address>
<email confidence="0.996352">silvia@cs.york.ac.uk</email>
<email confidence="0.996352">suresh@cs.york.ac.uk</email>
<abstract confidence="0.997529545454546">Most question answering (QA) and information retrieval (IR) systems are insensitive to different users’ needs and preferences, and also to the existence of multiple, complex or controversial answers. We introduce adaptivity in QA and IR by creating a hybrid system based on a dialogue and a user model. question answering, information retrieval, user modelling, dialogue interfaces.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Alfonseca</author>
<author>M DeBoni</author>
<author>J-L Jara-Valencia</author>
<author>S Manandhar</author>
</authors>
<title>A prototype question answering system using syntactic and semantic information for answer retrieval.</title>
<date>2001</date>
<booktitle>In Text REtrieval Conference.</booktitle>
<contexts>
<context position="8523" citStr="Alfonseca et al., 2001" startWordPosition="1371" endWordPosition="1374">cument, and sibling leaves denote documents with similar topics and reading difficulty. 4.3 Answer extraction In this phase, the clustered documents are filtered based on the user model and answer sentences are located and formatted for presentation. UM-based filtering The documents in the cluster tree are filtered according to their reading difficulty: only those compatible with the UM’s reading level are retained for further analysis6. Semantic similarity Within each of the retained documents, we seek the sentences which are semantically most relevant to the query by applying the metric in (Alfonseca et al., 2001): we represent each document sentence p and the query q as word sets P = {pw1, ... , pwm} and Q = {qw1, ... , qwn}. The distance from p to q is then distq(p) = E1&lt;i&lt;m minj[d(pwi, qwj)], where d(pwi,qwj) is the word-level distance between pwi and qwj based on (Jiang and Conrath, 1997). Ranking Given the query q, we thus locate in each document D the sentence p* such that p* = argminpED[distq(p)]; then, distq(p*) becomes the document score. Moreover, each clus5The likelihood is estimated using the formula: Li,D = Ew∈D C(w, D) · log(P(w|lmi)), where w is a word in the document, C(w, d) is the num</context>
</contexts>
<marker>Alfonseca, DeBoni, Jara-Valencia, Manandhar, 2001</marker>
<rawString>E. Alfonseca, M. DeBoni, J.-L. Jara-Valencia, and S. Manandhar. 2001. A prototype question answering system using syntactic and semantic information for answer retrieval. In Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ardissono</author>
<author>L Console</author>
<author>I Torre</author>
</authors>
<title>An adaptive system for the personalized access to news.</title>
<date>2001</date>
<journal>AI Commun.,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="4311" citStr="Ardissono et al., 2001" startWordPosition="693" endWordPosition="696">ly reduce the amount of linguistic processing applied during question processing and answer generation, while giving more relief to the post-retrieval phase and to the role of the UM. 2 User model Depending on the application of interest, the UM can be designed to suit the information needs of the QA module in different ways. As our current application, YourQA2, is a learning-oriented, webbased system, our UM consists of the user’s: 1) age range, a E {7 − 11,11 − 16, adult}; 2) reading level, r E {poor, medium, good}; 3) webpages of interest/bookmarks, w. Analogies can be found with the SeAn (Ardissono et al., 2001) and SiteIF (Magnini and Strapparava, 2001) news recommender systems where age and browsing history, respectively, are part of the UM. In this paper we focus on how to filter and adapt search results using the reading level parameter. 3 Dialogue interface The dialogue component will interact with both the UM and the QA module. From a UM point of view, the dialogue history will store previous conversations useful to construct and update a model of the user’s interests, goals and level of understanding. From a QA point of view, the main goal of the dialogue component is to provide users with a f</context>
</contexts>
<marker>Ardissono, Console, Torre, 2001</marker>
<rawString>L. Ardissono, L. Console, and I. Torre. 2001. An adaptive system for the personalized access to news. AI Commun., 14(3):129–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J P Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>K. Collins-Thompson and J. P. Callan. 2004. A language modeling approach to predicting reading difficulty. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference Research on Computational Linguistics (ROCLING X).</booktitle>
<contexts>
<context position="8807" citStr="Jiang and Conrath, 1997" startWordPosition="1427" endWordPosition="1430"> in the cluster tree are filtered according to their reading difficulty: only those compatible with the UM’s reading level are retained for further analysis6. Semantic similarity Within each of the retained documents, we seek the sentences which are semantically most relevant to the query by applying the metric in (Alfonseca et al., 2001): we represent each document sentence p and the query q as word sets P = {pw1, ... , pwm} and Q = {qw1, ... , qwn}. The distance from p to q is then distq(p) = E1&lt;i&lt;m minj[d(pwi, qwj)], where d(pwi,qwj) is the word-level distance between pwi and qwj based on (Jiang and Conrath, 1997). Ranking Given the query q, we thus locate in each document D the sentence p* such that p* = argminpED[distq(p)]; then, distq(p*) becomes the document score. Moreover, each clus5The likelihood is estimated using the formula: Li,D = Ew∈D C(w, D) · log(P(w|lmi)), where w is a word in the document, C(w, d) is the number of occurrences of w in D and P(w|lmi) is the probability with which w occurs in lmi 6However, if their number does not exceed a given threshold, we accept in our candidate set part of the documents having the next lowest readability – or a medium readability if the user’s reading</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. J. Jiang and D. W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference Research on Computational Linguistics (ROCLING X).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C T Kwok</author>
<author>O Etzioni</author>
<author>D S Weld</author>
</authors>
<title>Scaling question answering to the web. In World Wide Web,</title>
<date>2001</date>
<pages>150--161</pages>
<contexts>
<context position="1434" citStr="Kwok et al., 2001" startWordPosition="220" endWordPosition="223"> return them in the form of sentences (or paragraphs, or phrases), responding more precisely to the user’s request. However, in most state-of-the-art QA systems the output remains independent of the questioner’s characteristics, goals and needs. In other words, there is a lack of user modelling: a 10-year-old and a University History student would get the same answer to the question: “When did the Middle Ages begin?”. Secondly, most of the effort of current QA is on factoid questions, i.e. questions concerning people, dates, etc., which can generally be answered by a short sentence or phrase (Kwok et al., 2001). The main QA evaluation campaign, TREC-QA 1, has long focused on this type of questions, for which the simplifying assumption is that there exists only one correct answer. Even recent TREC campaigns (Voorhees, 2003; Voorhees, 2004) do not move sufficiently beyond the factoid approach. They account for two types of nonfactoid questions –list and definitional– but not for non-factoid answers. In fact, a) TREC defines list questions as questions requiring multiple factoid 1http://trec.nist.gov answers, b) it is clear that a definition question may be answered by spotting definitional passages (w</context>
<context position="3551" citStr="Kwok et al., 2001" startWordPosition="563" endWordPosition="566">e able to discriminate between simple/factoid answers and more complex answers, presenting them in a TREC-style manner in the first case and more appropriately in the second. Figure 1: High level system architecture Related work To our knowledge, our system is among the first to address the need for a different approach to non-factoid (complex/controversial) QUESTION PROCESSING DIALOGUE INTERFACE DOCUMENT RETRIEVAL ANSWER EXTRACTION Answer USER MODEL QA MODULE Question 199 answers. Although the three-tiered structure of our QA module reflects that of a typical webbased QA system, e.g. MULDER (Kwok et al., 2001), a significant aspect of novelty in our architecture is that the QA component is supported by the user model. Additionally, we drastically reduce the amount of linguistic processing applied during question processing and answer generation, while giving more relief to the post-retrieval phase and to the role of the UM. 2 User model Depending on the application of interest, the UM can be designed to suit the information needs of the QA module in different ways. As our current application, YourQA2, is a learning-oriented, webbased system, our UM consists of the user’s: 1) age range, a E {7 − 11,</context>
</contexts>
<marker>Kwok, Etzioni, Weld, 2001</marker>
<rawString>C. C. T. Kwok, O. Etzioni, and D. S. Weld. 2001. Scaling question answering to the web. In World Wide Web, pages 150–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Carlo Strapparava</author>
</authors>
<title>Improving user modelling with content-based techniques.</title>
<date>2001</date>
<booktitle>In UM: Proceedings of the 8th Int. Conference,</booktitle>
<volume>2109</volume>
<publisher>Springer.</publisher>
<contexts>
<context position="4354" citStr="Magnini and Strapparava, 2001" startWordPosition="699" endWordPosition="702">rocessing applied during question processing and answer generation, while giving more relief to the post-retrieval phase and to the role of the UM. 2 User model Depending on the application of interest, the UM can be designed to suit the information needs of the QA module in different ways. As our current application, YourQA2, is a learning-oriented, webbased system, our UM consists of the user’s: 1) age range, a E {7 − 11,11 − 16, adult}; 2) reading level, r E {poor, medium, good}; 3) webpages of interest/bookmarks, w. Analogies can be found with the SeAn (Ardissono et al., 2001) and SiteIF (Magnini and Strapparava, 2001) news recommender systems where age and browsing history, respectively, are part of the UM. In this paper we focus on how to filter and adapt search results using the reading level parameter. 3 Dialogue interface The dialogue component will interact with both the UM and the QA module. From a UM point of view, the dialogue history will store previous conversations useful to construct and update a model of the user’s interests, goals and level of understanding. From a QA point of view, the main goal of the dialogue component is to provide users with a friendly interface to build their requests. </context>
</contexts>
<marker>Magnini, Strapparava, 2001</marker>
<rawString>Bernardo Magnini and Carlo Strapparava. 2001. Improving user modelling with content-based techniques. In UM: Proceedings of the 8th Int. Conference, volume 2109 of LNCS. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L T Su</author>
</authors>
<title>A comprehensive and systematic model of user evaluation of web search engines: Ii. an evaluation by undergraduates.</title>
<date>2003</date>
<journal>J. Am. Soc. Inf. Sci. Technol.,</journal>
<volume>54</volume>
<issue>13</issue>
<contexts>
<context position="11487" citStr="Su, 2003" startWordPosition="1889" endWordPosition="1890">ge); but similarities could be due to imitation.” — UMpoor: “Homer wrote The Iliad and The Odyssey (at least, supposedly a blind bard named &amp;quot;Homer&amp;quot; did).” In the three results, the problem of attribution of the Iliad is made clearly visible: document passages provide a context which helps to explain the controversy at different levels of difficulty. 6 Evaluation Since YourQA does not single out one correct answer phrase, TREC evaluation metrics are not suitable for it. A user-centred methodology to assess how individual information needs are met is more appropriate. We base our evaluation on (Su, 2003), which proposes a comprehensive search engine evaluation model, defining the following metrics: 1. Relevance: we define strict precision (P1) as the ratio between the number of results rated as relevant and all the returned results, and loose pre201 Query A9 A, AP When did the Middle Ages begin? 0,91 0,82 0,68 Who painted the Sistine Chapel? 0,85 0,72 0,79 When did the Romans invade Britain? 0,87 0,74 0,82 Who was a famous cubist? 0,90 0,75 0,85 Who was the first American in space? 0,94 0,80 0,72 Definition of metaphor 0,95 0,81 0,38 average 0,94 0,85 0,72 cision (P2) as the ratio between the</context>
</contexts>
<marker>Su, 2003</marker>
<rawString>L. T. Su. 2003. A comprehensive and systematic model of user evaluation of web search engines: Ii. an evaluation by undergraduates. J. Am. Soc. Inf. Sci. Technol., 54(13):1193–1223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2003</date>
<booktitle>In Text REtrieval Conference.</booktitle>
<contexts>
<context position="1649" citStr="Voorhees, 2003" startWordPosition="257" endWordPosition="258">eristics, goals and needs. In other words, there is a lack of user modelling: a 10-year-old and a University History student would get the same answer to the question: “When did the Middle Ages begin?”. Secondly, most of the effort of current QA is on factoid questions, i.e. questions concerning people, dates, etc., which can generally be answered by a short sentence or phrase (Kwok et al., 2001). The main QA evaluation campaign, TREC-QA 1, has long focused on this type of questions, for which the simplifying assumption is that there exists only one correct answer. Even recent TREC campaigns (Voorhees, 2003; Voorhees, 2004) do not move sufficiently beyond the factoid approach. They account for two types of nonfactoid questions –list and definitional– but not for non-factoid answers. In fact, a) TREC defines list questions as questions requiring multiple factoid 1http://trec.nist.gov answers, b) it is clear that a definition question may be answered by spotting definitional passages (what is not clear is how to spot them). However, accounting for the fact that some simple questions may have complex or controversial answers (e.g. “What were the causes of World War II?”) remains an unsolved problem</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>E. M. Voorhees. 2003. Overview of the TREC 2003 question answering track. In Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2004</date>
<booktitle>In Text REtrieval Conference.</booktitle>
<contexts>
<context position="1666" citStr="Voorhees, 2004" startWordPosition="259" endWordPosition="260">and needs. In other words, there is a lack of user modelling: a 10-year-old and a University History student would get the same answer to the question: “When did the Middle Ages begin?”. Secondly, most of the effort of current QA is on factoid questions, i.e. questions concerning people, dates, etc., which can generally be answered by a short sentence or phrase (Kwok et al., 2001). The main QA evaluation campaign, TREC-QA 1, has long focused on this type of questions, for which the simplifying assumption is that there exists only one correct answer. Even recent TREC campaigns (Voorhees, 2003; Voorhees, 2004) do not move sufficiently beyond the factoid approach. They account for two types of nonfactoid questions –list and definitional– but not for non-factoid answers. In fact, a) TREC defines list questions as questions requiring multiple factoid 1http://trec.nist.gov answers, b) it is clear that a definition question may be answered by spotting definitional passages (what is not clear is how to spot them). However, accounting for the fact that some simple questions may have complex or controversial answers (e.g. “What were the causes of World War II?”) remains an unsolved problem. We argue that i</context>
</contexts>
<marker>Voorhees, 2004</marker>
<rawString>E. M. Voorhees. 2004. Overview of the TREC 2004 question answering track. In Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Witten</author>
<author>E Frank</author>
</authors>
<date>2000</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementation.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="7801" citStr="Witten and Frank, 2000" startWordPosition="1257" endWordPosition="1260">d for a given number of reading levels. Then, a unigram language model is created for each set, i.e. a list of (word stem, probability) entries for the words appearing in its documents. Our models account for the following reading levels: poor (suitable for ages 7– 11), medium (ages 11–16) and good (adults). 2) In the test phase, given an unclassified document D, its estimated reading level is the model lmi maximizing the likelihood that D E lmi5. Clustering We use the extracted topics and estimated reading levels as features to apply hierarchical clustering on the documents. We use the WEKA (Witten and Frank, 2000) implementation of the Cobweb algorithm. This produces a tree where each leaf corresponds to one document, and sibling leaves denote documents with similar topics and reading difficulty. 4.3 Answer extraction In this phase, the clustered documents are filtered based on the user model and answer sentences are located and formatted for presentation. UM-based filtering The documents in the cluster tree are filtered according to their reading difficulty: only those compatible with the UM’s reading level are retained for further analysis6. Semantic similarity Within each of the retained documents, </context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>H. Witten and E. Frank. 2000. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementation. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>G W Paynter</author>
<author>E Frank</author>
<author>C Gutwin</author>
<author>C G Nevill-Manning</author>
</authors>
<title>KEA: Practical automatic keyphrase extraction.</title>
<date>1999</date>
<booktitle>In ACM DL,</booktitle>
<pages>254--255</pages>
<contexts>
<context position="6537" citStr="Witten et al., 1999" startWordPosition="1062" endWordPosition="1065">nal queries using question word synonyms in the purpose of increasing the recall of the search engine. Synonyms are obtained via the WordNet 2.0 3 lexical database. Figure 2: Diagram of the QA module 4.2 Retrieval Document retrieval We retrieve the top 20 documents returned by Google4 for each query produced via query expansion. These are processed in the following steps, which progressively narrow the part of the text containing relevant information. Keyphrase extraction Once the documents are retrieved, we perform keyphrase extraction to determine their three most relevant topics using Kea (Witten et al., 1999), an extractor based on Naïve Bayes classification. Estimation of reading levels To adapt the readability of the results to the user, we estimate the reading difficulty of the retrieved documents using the Smoothed Unigram Model (CollinsThompson and Callan, 2004), which proceeds in 3http://wordnet.princeton.edu 4http://www.google.com RANIING Ranked Answer Candidates Question IEYPHRASE EXTRACTION CLUSTERING QUERY EXPANSION DOCUMENT RETRIEVAL SEMANTIC SIMILARITY UM-BASED FILTERING ESTIMATION OF READING LEVELS Language Models User Model Reading Level 200 two phases. 1) In the training phase, sets</context>
</contexts>
<marker>Witten, Paynter, Frank, Gutwin, Nevill-Manning, 1999</marker>
<rawString>I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G. Nevill-Manning. 1999. KEA: Practical automatic keyphrase extraction. In ACM DL, pages 254–255.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>