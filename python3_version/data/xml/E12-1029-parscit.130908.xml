<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003197">
<title confidence="0.98611">
Bootstrapped Training of Event Extraction Classifiers
</title>
<author confidence="0.998625">
Ruihong Huang and Ellen Riloff
</author>
<affiliation confidence="0.865527333333333">
School of Computing
University of Utah
Salt Lake City, UT 84112
</affiliation>
<email confidence="0.99952">
{huangrh,riloff}@cs.utah.edu
</email>
<sectionHeader confidence="0.997402" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999115666666667">
Most event extraction systems are trained
with supervised learning and rely on a col-
lection of annotated documents. Due to
the domain-specificity of this task, event
extraction systems must be retrained with
new annotated data for each domain. In
this paper, we propose a bootstrapping so-
lution for event role filler extraction that re-
quires minimal human supervision. We aim
to rapidly train a state-of-the-art event ex-
traction system using a small set of “seed
nouns” for each event role, a collection
of relevant (in-domain) and irrelevant (out-
of-domain) texts, and a semantic dictio-
nary. The experimental results show that
the bootstrapped system outperforms previ-
ous weakly supervised event extraction sys-
tems on the MUC-4 data set, and achieves
performance levels comparable to super-
vised training with 700 manually annotated
documents.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999887019607843">
Event extraction systems process stories about
domain-relevant events and identify the role fillers
of each event. A key challenge for event extrac-
tion is that recognizing role fillers is inherently
contextual. For example, a PERSON can be a
perpetrator or a victim in different contexts (e.g.,
“John Smith assassinated the mayor” vs. “John
Smith was assassinated”). Similarly, any COM-
PANY can be an acquirer or an acquiree depending
on the context.
Many supervised learning techniques have
been used to create event extraction systems us-
ing gold standard “answer key” event templates
for training (e.g., (Freitag, 1998a; Chieu and Ng,
2002; Maslennikov and Chua, 2007)). How-
ever, manually generating answer keys for event
extraction is time-consuming and tedious. And
more importantly, event extraction annotations
are highly domain-specific, so new annotations
must be obtained for each domain.
The goal of our research is to use bootstrap-
ping techniques to automatically train a state-of-
the-art event extraction system without human-
generated answer key templates. The focus of our
work is the TIER event extraction model, which
is a multi-layered architecture for event extrac-
tion (Huang and Riloff, 2011). TIER’s innova-
tion over previous techniques is the use of four
different classifiers that analyze a document at in-
creasing levels of granularity. TIER progressively
zooms in on event information using a pipeline
of classifiers that perform document-level classi-
fication, sentence classification, and noun phrase
classification. TIER outperformed previous event
extraction systems on the MUC-4 data set, but re-
lied heavily on a large collection of 1,300 docu-
ments coupled with answer key templates to train
its four classifiers.
In this paper, we present a bootstrapping solu-
tion that exploits a large unannotated corpus for
training by using role-identifying nouns (Phillips
and Riloff, 2007) as seed terms. Phillips and
Riloff observed that some nouns, by definition,
refer to entities or objects that play a specific role
in an event. For example, “assassin”, “sniper”,
and “hitman” refer to people who play the role
of PERPETRATOR in a criminal event. Similarly,
“victim”, “casualty”, and “fatality” refer to peo-
ple who play the role of VICTIM, by virtue of
their lexical semantics. Phillips and Riloff called
these words role-identifying nouns and used them
</bodyText>
<page confidence="0.971323">
286
</page>
<note confidence="0.9763885">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 286–295,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999903290322581">
to learn extraction patterns. Our research also
uses role-identifying nouns to learn extraction pat-
terns, but the role-identifying nouns and patterns
are then used to create training data for event ex-
traction classifiers. Each classifier is then self-
trained in a bootstrapping loop.
Our weakly supervised training procedure re-
quires a small set of “seed nouns” for each event
role, and a collection of relevant (in-domain) and
irrelevant (out-of-domain) texts. No answer key
templates or annotated texts are needed. The seed
nouns are used to automatically generate a set
of role-identifying patterns, and then the nouns,
patterns, and a semantic dictionary are used to
label training instances. We also propagate the
event role labels across coreferent noun phrases
within a document to produce additional train-
ing instances. The automatically labeled texts are
used to train three components of TIER: its two
types of sentence classifiers and its noun phrase
classifiers. To create TIER’s fourth component,
its document genre classifier, we apply heuristics
to the output of the sentence classifiers.
We present experimental results on the MUC-
4 data set, which is a standard benchmark for
event extraction research. Our results show that
the bootstrapped system, TIERlit, outperforms
previous weakly supervised event extraction sys-
tems and achieves performance levels comparable
to supervised training with 700 manually anno-
tated documents.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999820912280702">
Event extraction techniques have largely focused
on detecting event “triggers” with their arguments
for extracting role fillers. Classical methods are
either pattern-based (Kim and Moldovan, 1993;
Riloff, 1993; Soderland et al., 1995; Huffman,
1996; Freitag, 1998b; Ciravegna, 2001; Califf and
Mooney, 2003; Riloff, 1996; Riloff and Jones,
1999; Yangarber et al., 2000; Sudo et al., 2003;
Stevenson and Greenwood, 2005) or classifier-
based (e.g., (Freitag, 1998a; Chieu and Ng, 2002;
Finn and Kushmerick, 2004; Li et al., 2005; Yu et
al., 2005)).
Recently, several approaches have been pro-
posed to address the insufficiency of using only
local context to identify role fillers. Some ap-
proaches look at the broader sentential context
around a potential role filler when making a de-
cision (e.g., (Gu and Cercone, 2006; Patwardhan
and Riloff, 2009)). Other systems take a more
global view and consider discourse properties of
the document as a whole to improve performance
(e.g., (Maslennikov and Chua, 2007; Ji and Gr-
ishman, 2008; Liao and Grishman, 2010; Huang
and Riloff, 2011)). Currently, the learning-based
event extraction systems that perform best all use
supervised learning techniques that require a large
number of texts coupled with manually-generated
annotations or answer key templates.
A variety of techniques have been explored
for weakly supervised training of event extrac-
tion systems, primarily in the realm of pattern or
rule-based approaches (e.g., (Riloff, 1996; Riloff
and Jones, 1999; Yangarber et al., 2000; Sudo et
al., 2003; Stevenson and Greenwood, 2005)). In
some of these approaches, a human must man-
ually review and “clean” the learned patterns to
obtain good performance. Research has also been
done to learn extraction patterns in an unsuper-
vised way (e.g., (Shinyama and Sekine, 2006;
Sekine, 2006)). But these efforts target open do-
main information extraction. To extract domain-
specific event information, domain experts are
needed to select the pattern subsets to use.
There have also been weakly supervised ap-
proaches that use more than just local context.
(Patwardhan and Riloff, 2007) uses a semantic
affinity measure to learn primary and secondary
patterns, and the secondary patterns are applied
only to event sentences. The event sentence clas-
sifier is self-trained using seed patterns. Most
recently, (Chambers and Jurafsky, 2011) acquire
event words from an external resource, group the
event words to form event scenarios, and group
extraction patterns for different event roles. How-
ever, these weakly supervised systems produce
substantially lower performance than the best su-
pervised systems.
</bodyText>
<sectionHeader confidence="0.919708" genericHeader="method">
3 Overview of TIER
</sectionHeader>
<bodyText confidence="0.9997808">
The goal of our research is to develop a weakly
supervised training process that can successfully
train a state-of-the-art event extraction system for
a new domain with minimal human input. We de-
cided to focus our efforts on the TIER event ex-
traction model because it recently produced bet-
ter performance on the MUC-4 data set than prior
learning-based event extraction systems (Huang
and Riloff, 2011). In this section, we briefly give
an overview of TIER’s architecture and its com-
</bodyText>
<page confidence="0.997464">
287
</page>
<figureCaption confidence="0.999712">
Figure 1: TIER Overview
</figureCaption>
<bodyText confidence="0.99924546">
ponents.
TIER is a multi-layered architecture for event
extraction, as shown in Figure 1. Documents pass
through a pipeline where they are analyzed at dif-
ferent levels of granularity, which enables the sys-
tem to gradually “zoom in” on relevant facts. The
pipeline consists of a document genre classifier,
two types of sentence classifiers, and a set of noun
phrase (role filler) classifiers.
The lower pathway in Figure 1 shows that all
documents pass through an event sentence clas-
sifier. Sentences labeled as event descriptions
then proceed to the noun phrase classifiers, which
are responsible for identifying the role fillers in
each sentence. The upper pathway in Figure 1 in-
volves a document genre classifier to determine
whether a document is an “event narrative” story
(i.e., an article that primarily discusses the details
of a domain-relevant event). Documents that are
classified as event narratives warrant additional
scrutiny because they most likely contain a lot of
event information. Event narrative stories are pro-
cessed by an additional set of role-specific sen-
tence classifiers that look for role-specific con-
texts that will not necessarily mention the event.
For example, a victim may be mentioned in a sen-
tence that describes the aftermath of a crime, such
as transportation to a hospital or the identifica-
tion of a body. Sentences that are determined to
have “role-specific” contexts are passed along to
the noun phrase classifiers for role filler extrac-
tion. Consequently, event narrative documents
pass through both the lower pathway and the up-
per pathway. This approach creates an event ex-
traction system that can discover role fillers in a
variety of different contexts by considering the
type of document being processed.
TIER was originally trained with supervised
learning using 1,300 texts and their corresponding
answer key templates from the MUC-4 data set
(MUC-4 Proceedings, 1992). Human-generated
answer key templates are expensive to produce
because the annotation process is both difficult
and time-consuming. Furthermore, answer key
templates for one domain are virtually never
reusable for different domains, so a new set of
answer keys must be produced from scratch for
each domain. In the next section, we present our
weakly supervised approach for training TIER’s
event extraction classifiers.
</bodyText>
<sectionHeader confidence="0.7860655" genericHeader="method">
4 Bootstrapped Training of Event
Extraction Classifiers
</sectionHeader>
<bodyText confidence="0.999995090909091">
We adopt a two-phase approach to train TIER’s
event extraction modules using minimal human-
generated resources. The goal of the first phase
is to automatically generate positive training ex-
amples using role-identifying seed nouns as input.
The seed nouns are used to automatically gener-
ate a set of role-identifying patterns for each event
role. Each set of patterns is then assigned a set
of semantic constraints (selectional restrictions)
that are appropriate for that event role. The se-
mantic constraints consist of the role-identifying
seed nouns as well as general semantic classes
that constrain the event role (e.g., a victim must
be a HUMAN). A noun phrase will satisfy the se-
mantic constraints if its head noun is in the seed
noun list or if it has the appropriate semantic type
(based on dictionary lookup). Each pattern is then
matched against the unannotated texts, and if the
extracted noun phrase satisfies its semantic con-
straints, then the noun phrase is automatically la-
beled as a role filler.
The second phase involves bootstrapped train-
ing of TIER’s classifiers. Using the labeled in-
stances generated in the first phase, we iteratively
train three of TIER’s components: the two types
of sentential classifiers and the noun phrase clas-
sifiers. For the fourth component, the document
classifier, we apply heuristics to the output of the
sentence classifiers to assess the density of rel-
evant sentences in a document and label high-
density stories as event narratives. In the fol-
lowing sections, we present the details of each of
these steps.
</bodyText>
<subsectionHeader confidence="0.999004">
4.1 Automatically Labeling Training Data
</subsectionHeader>
<bodyText confidence="0.9999482">
Finding seeding instances of high precision and
reasonable coverage is important in bootstrap-
ping. However, this is especially challenging
for event extraction task because identifying role
fillers is inherently contextual. Furthermore, role
</bodyText>
<page confidence="0.995457">
288
</page>
<figureCaption confidence="0.9729875">
Figure 2: Using Basilisk to Induce Role-Identifying
Patterns
</figureCaption>
<bodyText confidence="0.999525636363636">
fillers occur sparsely in text and in diverse con-
texts.
In this section, we explain how we gener-
ate role-identifying patterns automatically using
seed nouns, and we discuss why we add seman-
tic constraints to the patterns when producing la-
beled instances for training. Then, we discuss the
coreference-based label propagation that we used
to obtain additional training instances. Finally, we
give examples to illustrate how we create training
instances.
</bodyText>
<subsectionHeader confidence="0.835085">
4.1.1 Inducing Role-Identifying Patterns
</subsectionHeader>
<bodyText confidence="0.9999964375">
The input to our system is a small set of
manually-defined seed nouns for each event role.
Specifically, the user is required to provide
10 role-identifying nouns for each event role.
(Phillips and Riloff, 2007) defined a noun as be-
ing “role-identifying” if its lexical semantics re-
veal the role of the entity/object in an event. For
example, the words “assassin” and “sniper” are
people who participate in a violent event as a PER-
PETRATOR. Therefore, the entities referred to by
role-identifying nouns are probable role fillers.
However, treating every context surrounding a
role-identifying noun as a role-identifying pattern
is risky. The reason is that many instances of role-
identifying nouns appear in contexts that do not
describe the event. But, if one pattern has been
seen to extract many role-identifying nouns and
seldomly seen to extract other nouns, then the pat-
tern likely represents an event context.
As (Phillips and Riloff, 2007) did, we use
Basilisk to learn patterns for each event role.
Basilisk was originally designed for semantic
class learning (e.g., to learn nouns belonging to
semantic categories, such as building or human).
As shown in Figure 2, beginning with a small set
of seed nouns for each semantic class, Basilisk
learns additional nouns belonging to the same se-
mantic class. Internally, Basilisk uses extraction
patterns automatically generated from unanno-
tated texts to assess the similarity of nouns. First,
Basilisk assigns a score to each pattern based on
the number of seed words that co-occur with it.
Basilisk then collects the noun phrases extracted
by the highest-scoring patterns. Next, the head
noun of each noun phrase is assigned a score
based on the set of patterns that it co-occurred
with. Finally, Basilisk selects the highest-scoring
nouns, automatically labels them with the seman-
tic class of the seeds, adds these nouns to the lex-
icon, and restarts the learning process in a boot-
strapping fashion.
For our work, we give Basilisk role-identifying
seed nouns for each event role. We run the boot-
strapping process for 20 iterations and then har-
vest the 40 best patterns that Basilisk identifies
for each event role. We also tried using the addi-
tional role-identifying nouns learned by Basilisk,
but found that these nouns were too noisy.
</bodyText>
<subsectionHeader confidence="0.957035">
4.1.2 Using the Patterns to Label NPs
</subsectionHeader>
<bodyText confidence="0.999809275862069">
The induced role-identifying patterns can be
matched against the unannotated texts to produce
labeled instances. However, relying solely on the
pattern contexts can be misleading. For example,
the pattern context &lt;subject&gt; caused damage
will extract some noun phrases that are weapons
(e.g., the bomb) but some noun phrases that are
not (e.g., the tsunami).
Based on this observation, we add selectional
restrictions to each pattern that requires a noun
phrase to satisfy certain semantic constraints in
order to be extracted and labeled as a positive
instances for an event role. The selectional re-
strictions are satisfied if the head noun is among
the role-identifying seed nouns or if the semantic
class of the head noun is compatible with the cor-
responding event role. In the previous example,
tsunami will not be extracted as a weapon because
it has an incompatible semantic class (EVENT),
but bomb will be extracted because it has a com-
patible semantic class (WEAPON).
We use the semantic class labels assigned by
the Sundance parser (Riloff and Phillips, 2004) in
our experiments. Sundance looks up each noun
in a semantic dictionary to assign the semantic
class labels. As an alternative, general resources
(e.g., WordNet (Miller, 1990)) or a semantic tag-
ger (e.g., (Huang and Riloff, 2010)) could be
used.
</bodyText>
<page confidence="0.996378">
289
</page>
<figureCaption confidence="0.998503">
Figure 3: Automatic Training Data Creation
</figureCaption>
<subsubsectionHeader confidence="0.581485">
4.1.3 Propagating Labels with Coreference
</subsubsectionHeader>
<bodyText confidence="0.999994294117647">
To enrich the automatically labeled training in-
stances, we also propagate the event role labels
across coreferent noun phrases within a docu-
ment. The observation is that once a noun phrase
has been identified as a role filler, its corefer-
ent mentions in the same document likely fill the
same event role since they are referring to the
same real world entity.
To leverage these coreferential contexts, we
employ a simple head noun matching heuristic to
identify coreferent noun phrases. This heuristic
assumes that two noun phrases that have the same
head noun are coreferential. We considered us-
ing an off-the-shelf coreference resolver, but de-
cided that the head noun matching heuristic would
likely produce higher precision results, which is
important to produce high-quality labeled data.
</bodyText>
<subsectionHeader confidence="0.5559535">
4.1.4 Examples of Training Instance
Creation
</subsectionHeader>
<bodyText confidence="0.9985236">
Figure 3 illustrates how we label training in-
stances automatically. The text example shows
three noun phrases that are automatically labeled
as perpetrators. Noun phrases #1 and #2 oc-
cur in role-identifying pattern contexts (was killed
by &lt;np&gt; and &lt;subject&gt; attacked) and satisfy
the semantic constraints for perpetrators because
“men” has a compatible semantic type and “assas-
sins” is a role-identifying noun for perpetrators.
Noun phrase #3 (“the unidentified men”) does
not occur in a pattern context, but it is deemed
to be coreferent with “two armed men” because
they have the same head noun. Consequently, we
propagate the perpetrator label from noun phrase
#1 to noun phrase #3.
</bodyText>
<subsectionHeader confidence="0.987711">
4.2 Creating TIERlite with Bootstrapping
</subsectionHeader>
<bodyText confidence="0.999351851851852">
In this section, we explain how the labeled in-
stances are used to train TIER’s classifiers with
bootstrapping. In addition to the automatically
labeled instances, the training process depends
on a text corpus that consists of both relevant
(in-domain) and irrelevant (out-of-domain) doc-
uments. Positive instances are generated from
the relevant documents and negative instances are
generated by randomly sampling from the irrele-
vant documents.
The classifiers are all support vector machines
(SVMs), implemented using the SVMlin software
(Keerthi and DeCoste, 2005). When applying the
classifiers during bootstrapping, we use a sliding
confidence threshold to determine which labels
are reliable based on the values produced by the
SVM. Initially, we set the threshold to be 2.0 to
identify highly confident predictions. But if fewer
than k instances pass the threshold, then we slide
the threshold down in decrements of 0.1 until we
obtain at least k labeled instances or the thresh-
old drops below 0, in which case bootstrapping
ends. We used k=10 for both sentence classifiers
and k=30 for the noun phrase classifiers.
The following sections present the details of the
bootstrapped training process for each of TIER’s
components.
</bodyText>
<figureCaption confidence="0.998908">
Figure 4: The Bootstrapping Process
</figureCaption>
<subsubsectionHeader confidence="0.458757">
4.2.1 Noun Phrase Classifiers
</subsubsectionHeader>
<bodyText confidence="0.999756571428572">
The mission of the noun phrase classifiers is to
determine whether a noun phrase is a plausible
event role filler based on the local features sur-
rounding the noun phrase (NP). A set of classifiers
is needed, one for each event role.
As shown in Figure 4, to seed the classifier
training, the positive noun phrase instances are
</bodyText>
<figure confidence="0.997554214285714">
building = Object
men = Human
. . .
terrorists
assassins
snipers
. . .
was killed by &lt;np&gt;
&lt;subject&gt; attacked
&lt;subject&gt; fired shots
. . .
Role−Identifying
Patterns
Role−Identifying
Noun
Constraints Constraints
Semantic
Dictionary
John Smith was killed by
in broad daylight this morning.
The assassins attacked the mayor as he
2
left his house to go to work about 8:00 am.
Police arrested the unidentified men
3
an hour later.
two armed men
1
</figure>
<page confidence="0.981089">
290
</page>
<bodyText confidence="0.999954695652174">
generated from the relevant documents follow-
ing Section 4.1. The negative noun phrase in-
stances are drawn randomly from the irrelevant
documents. Considering the sparsity of role fillers
in texts, we set the negative:positive ratio to be
10:1. Once the classifier is trained, it is applied to
the unlabeled noun phrases in the relevant docu-
ments. Noun phrases that are assigned role filler
labels by the classifier with high confidence (us-
ing the sliding threshold) are added to the set of
positive instances. New negative instances are
drawn randomly from the irrelevant documents to
maintain the 10:1 (negative:positive) ratio.
We extract features from each noun phrase
(NP) and its surrounding context. The features
include the NP head noun and its premodifiers.
We also use the Stanford NER tagger (Finkel et
al., 2005) to identify Named Entities within the
NP. The context features include four words to the
left of the NP, four words to the right of the NP,
and the lexico-syntactic patterns generated by Au-
toSlog to capture expressions around the NP (see
(Riloff, 1993) for details).
</bodyText>
<subsectionHeader confidence="0.516597">
4.2.2 Event Sentence Classifier
</subsectionHeader>
<bodyText confidence="0.999990470588235">
The event sentence classifier is responsible
for identifying sentences that describe a relevant
event. Similar to the noun phrase classifier train-
ing, positive training instances are selected from
the relevant documents and negative instances are
drawn from the irrelevant documents. All sen-
tences in the relevant documents that contain one
or more labeled noun phrases (belonging to any
event role) are labeled as positive training in-
stances. We randomly sample sentences from the
irrelevant documents to obtain a negative:positive
training instance ratio of 10:1. The bootstrapping
process is then identical to that of the noun phrase
classifiers. The feature set for this classifier con-
sists of unigrams, bigrams and AutoSlog’s lexico-
syntactic patterns surrounding all noun phrases in
the sentence.
</bodyText>
<subsectionHeader confidence="0.806163">
4.2.3 Role-Specific Sentence Classifiers
</subsectionHeader>
<bodyText confidence="0.999979136363637">
The role-specific sentence classifiers are
trained to identify the contexts specific to each
event role. All sentences in the relevant doc-
uments that contain at least one labeled noun
phrase for the appropriate event role are used
as positive instances. Negative instances are
randomly sampled from the irrelevant documents
to maintain the negative:positive ratio of 10:1.
The bootstrapping process and feature set are the
same as for the event sentence classifier.
The difference between the two types of sen-
tence classifiers is that the event sentence classi-
fier uses positive instances from all event roles,
while each role-specific sentence classifiers only
uses the positive instances for one particular event
role. The rationale is similar as in the super-
vised setting (Huang and Riloff, 2011); the event
sentence classifier is expected to generalize over
all event roles to identify event mention contexts,
while the role-specific sentence classifiers are ex-
pected to learn to identify contexts specific to in-
dividual roles.
</bodyText>
<subsubsectionHeader confidence="0.631135">
4.2.4 Event Narrative Document Classifier
</subsubsectionHeader>
<bodyText confidence="0.999989083333333">
TIER also uses an event narrative document
classifier and only extracts information from role-
specific sentences within event narrative docu-
ments. In the supervised setting, TIER uses
heuristic rules derived from answer key templates
to identify the event narrative documents in the
training set, which are used to train an event nar-
rative document classifier. The heuristic rules re-
quire that an event narrative should have a high
density of relevant information and tend to men-
tion the relevant information within the first sev-
eral sentences.
In our weakly supervised setting, we use the
information density heuristic directly instead of
training an event narrative classifier. We approxi-
mate the relevant information density heuristic by
computing the ratio of relevant sentences (both
event sentences and role-specific sentences) out of
all the sentences in a document. Thus, the event
narrative labeller only relies on the output of the
two sentence classifiers. Specifically, we label a
document as an event narrative if &gt; 50% of the
sentences in the document are relevant (i.e., la-
beled positively by either sentence classifier).
</bodyText>
<sectionHeader confidence="0.999241" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999972375">
In this section, we evaluate our bootstrapped sys-
tem, TIERlite, on the MUC-4 event extraction
data set. First, we describe the IE task, the data
set, and the weakly supervised baseline systems
that we use for comparison. Then we present the
results of our fully bootstrapped system TIERlite,
the weakly supervised baseline systems, and two
fully supervised event extraction systems, TIER
</bodyText>
<page confidence="0.99406">
291
</page>
<bodyText confidence="0.999613">
and GLACIER. In addition, we analyze the per-
formance of TIERlite using different configura-
tions to assess the impact of its components.
</bodyText>
<subsectionHeader confidence="0.990794">
5.1 IE Task and Data
</subsectionHeader>
<bodyText confidence="0.9992173">
We evaluated the performance of our systems on
the MUC-4 terrorism IE task (MUC-4 Proceed-
ings, 1992) about Latin American terrorist events.
We used 1,300 texts (DEV) as our training set and
200 texts (TST3+TST4) as the test set. All the
documents have answer key templates. For the
training set, we used the answer keys to separate
the documents into relevant and irrelevant sub-
sets. Any document containing at least one rel-
evant event was considered to be relevant.
</bodyText>
<table confidence="0.9834565">
PerpInd PerpOrg Target Victim Weapon
129 74 126 201 58
</table>
<tableCaption confidence="0.99973">
Table 1: # of Role Fillers in the MUC-4 Test Set
</tableCaption>
<bodyText confidence="0.999905789473684">
Following previous studies, we evaluate our
system on five MUC-4 string event roles: perpe-
trator individuals (PerpInd), perpetrator organi-
zations (PerpOrg), physical targets, victims, and
weapons. Table 1 shows the distribution of role
fillers in the MUC-4 test set. The complete IE task
involves the creation of answer key templates, one
template per event1. Our work focuses on extract-
ing individual role fillers and not template genera-
tion, so we evaluate the accuracy of the role fillers
irrespective of which template they occur in.
We used the same head noun scoring scheme
as previous systems, where an extraction is cor-
rect if its head noun matches the head noun in the
answer key2. Pronouns were discarded from both
the system responses and the answer keys since
no coreference resolution is done. Duplicate ex-
tractions were conflated before being scored, so
they count as just one hit or one miss.
</bodyText>
<subsectionHeader confidence="0.999859">
5.2 Weakly Supervised Baselines
</subsectionHeader>
<bodyText confidence="0.99995775">
We compared the performance of our system with
three previous weakly supervised event extraction
systems.
AutoSlog-TS (Riloff, 1996) generates lexico-
syntactic patterns exhaustively from unannotated
texts and ranks them based on their frequency and
probability of occurring in relevant documents.
A human expert then examines the patterns and
</bodyText>
<footnote confidence="0.999685">
1Documents may contain multiple events per article.
2For example, “armed men” will match “5 armed men”.
</footnote>
<bodyText confidence="0.999856615384615">
manually selects the best patterns for each event
role. During testing, the patterns are matched
against unseen texts to extract event role fillers.
PIPER (Patwardhan and Riloff, 2007; Patward-
han, 2010) learns extraction patterns using a se-
mantic affinity measure, and it distinguishes be-
tween primary and secondary patterns and ap-
plies them selectively. (Chambers and Jurafsky,
2011) (C+J) created an event extraction system
by acquiring event words from WordNet (Miller,
1990), clustering the event words into different
event scenarios, and grouping extraction patterns
for different event roles.
</bodyText>
<subsectionHeader confidence="0.999658">
5.3 Performance of TIERlite
</subsectionHeader>
<bodyText confidence="0.998595125">
Table 2 shows the seed nouns that we used in our
experiments, which were generated by sorting the
nouns in the corpus by frequency and manually
identifying the first 10 role-identifying nouns for
each event role.3 Table 3 shows the number of
training instances (noun phrases) that were auto-
matically labeled for each event role using our
training data creation approach (Section 4.1).
</bodyText>
<table confidence="0.999280785714286">
Event Role Seed Nouns
Perpetrator terrorists assassins criminals rebels
Individual murderers death squads guerrillas
member members individuals
Perpetrator FMLN ELN FARC MRTA M-19 Front
Organization Shining Path Medellin Cartel
The Extraditables
Army of National Liberation
Target houses residence building home homes
offices pipeline hotel car vehicles
Victim victims civilians children jesuits Galan
priests students women peasants Romero
Weapon weapons bomb bombs explosives rifles
dynamite grenades device car bomb
</table>
<tableCaption confidence="0.968006">
Table 2: Role-Identifying Seed Nouns
</tableCaption>
<table confidence="0.998061">
PerpInd PerpOrg Target Victim Weapon
296 157 522 798 248
</table>
<tableCaption confidence="0.999854">
Table 3: # of Automatically Labeled NPs
</tableCaption>
<bodyText confidence="0.979645666666667">
Table 4 shows how our bootstrapped system
TIERlite compares with previous weakly super-
vised systems and two supervised systems, its su-
pervised counterpart TIER (Huang and Riloff,
2011) and a model that jointly considers local
and sentential contexts, GLACIER (Patwardhan
</bodyText>
<footnote confidence="0.684236">
3We only found 9 weapon terms among the high-
frequency terms.
</footnote>
<page confidence="0.984517">
292
</page>
<table confidence="0.9997879">
Weakly Supervised Baselines
PerpInd PerpOrg Target Victim Weapon Average
AUTOSLOG-TS (1996) 33/49/40 52/33/41 54/59/56 49/54/51 38/44/41 45/48/46
PIPERBe3t (2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/46/45
C+J (2011) - - - - - 44/36/40
Supervised Models
GLACIER (2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52
TIER (2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56
Weakly Supervised Models
TIER,ite 47/51/49 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50
</table>
<tableCaption confidence="0.999868">
Table 4: Performance of the Bootstrapped Event Extraction System (Precision/Recall/F-score)
</tableCaption>
<figure confidence="0.987269111111111">
60
55
IE performance(F1) 50
45
40
35
30
0 200 400 600 800 1000 1200 1400
# of training documents
</figure>
<figureCaption confidence="0.999964">
Figure 5: The Learning Curve of Supervised TIER
</figureCaption>
<subsectionHeader confidence="0.997513">
5.4 Analysis
</subsectionHeader>
<bodyText confidence="0.9998787">
Table 6 shows the effect of the coreference prop-
agation step described in Section 4.1.3 as part of
training data creation. Without this step, the per-
formance of the bootstrapped system yields an F
score of 41. With the benefit of the additional
training instances produced by coreference prop-
agation, the system yields an F score of 53. The
new instances produced by coreference propaga-
tion seem to substantially enrich the diversity of
the set of labeled instances.
</bodyText>
<table confidence="0.923568666666667">
Seeding P/R/F
wo/Coref 45/38/41
w/Coref 47/53/50
</table>
<tableCaption confidence="0.996376">
Table 6: Effects of Coreference Propagation
</tableCaption>
<bodyText confidence="0.999905434782609">
and Riloff, 2009). We see that TIERlite outper-
forms all three weakly supervised systems, with
slightly higher precision and substantially more
recall. When compared to the supervised sys-
tems, the performance of TIERlite is similar to
GLACIER, with comparable precision but slightly
lower recall. But the supervised TIER system,
which was trained with 1,300 annotated docu-
ments, is still superior, especially in recall.
Figure 5 shows the learning curve for TIER
when it is trained with fewer documents, rang-
ing from 100 to 1,300 in increments of 100. Each
data point represents five experiments where we
randomly selected k documents from the train-
ing set and averaged the results. The bars show
the range of results across the five runs. Figure 5
shows that TIER’s performance increases from an
F score of 34 when trained on just 100 documents
up to an F score of 56 when training on 1,300 doc-
uments. The circle shows the performance of our
bootstrapped system, TIERlite, which achieves an
F score comparable to supervised training with
about 700 manually annotated documents.
In the evaluation section, we saw that the su-
pervised event extraction systems achieve higher
recall than the weakly supervised systems. Al-
though our bootstrapped event extraction sys-
tem TIERlite produces higher recall than previ-
ous weakly supervised systems, a substantial re-
call gap still exists.
Considering the pipeline structure of the event
extraction system, as shown in Figure 1, the noun
phrase extractors are responsible for identifying
all candidate role fillers. The sentential classifiers
and the document classifier effectively serve as
filters to rule out candidates from irrelevant con-
texts. Consequently, there is no way to recover
missing recall (role fillers) if the noun phrase ex-
tractors fail to identify them.
Since the noun phrase classifiers are so central
to the performance of the system, we compared
the performance of the bootstrapped noun phrase
classifiers directly with their supervised conter-
parts. The results are shown in Table 5. Both sets
of classifiers produce low precision when used in
isolation, but their precision levels are compara-
</bodyText>
<page confidence="0.995365">
293
</page>
<table confidence="0.997766666666667">
PerpInd PerpOrg Target Victim Weapon Average
Supervised Classifier 25/67/36 26/78/39 34/83/49 32/72/45 30/75/43 30/75/42
Bootstrapped Classifier 30/54/39 37/53/44 30/71/42 28/63/39 36/57/44 32/60/42
</table>
<tableCaption confidence="0.999879">
Table 5: Evaluation of Bootstrapped Noun Phrase Classifiers (Precision/Recall/F-score)
</tableCaption>
<bodyText confidence="0.999948230769231">
ble. The TIER pipeline architecture is successful
at eliminating many of the false hits. However,
the recall of the bootstrapped classifiers is consis-
tently lower than the recall of the supervised clas-
sifiers. Specifically, the recall is about 10 points
lower for three event roles (PerpInd, Target and
Victim) and 20 points lower for the other two event
roles (PerpOrg and Weapon). These results sug-
gest that our bootstrapping approach to training
instance creation does not fully capture the diver-
sity of role filler contexts that are available in the
supervised training set of 1,300 documents. This
issue is an interesting direction for future work.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999970428571428">
We have presented a bootstrapping approach for
training a multi-layered event extraction model
using a small set of “seed nouns” for each event
role, a collection of relevant (in-domain) and ir-
relevant (out-of-domain) texts and a semantic dic-
tionary. The experimental results show that the
bootstrapped system, TIERlit, outperforms pre-
vious weakly supervised event extraction sys-
tems on a standard event extraction data set, and
achieves performance levels comparable to super-
vised training with 700 manually annotated docu-
ments. The minimal supervision required to train
such a model increases the portability of event ex-
traction systems.
</bodyText>
<sectionHeader confidence="0.998963" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999881272727273">
We gratefully acknowledge the support of the
National Science Foundation under grant IIS-
1018314 and the Defense Advanced Research
Projects Agency (DARPA) Machine Reading
Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0172.
Any opinions, findings, and conclusions or rec-
ommendations expressed in this material are those
of the authors and do not necessarily reflect the
view of the DARPA, AFRL, or the U.S. govern-
ment.
</bodyText>
<sectionHeader confidence="0.99601" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99942852">
M.E. Califf and R. Mooney. 2003. Bottom-up Re-
lational Learning of Pattern Matching rules for In-
formation Extraction. Journal ofMachine Learning
Research, 4:177–210.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-Based Information Extraction without the
Templates. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-11).
H.L. Chieu and H.T. Ng. 2002. A Maximum Entropy
Approach to Information Extraction from Semi-
Structured and Free Text. In Proceedings of the
18th National Conference on Artificial Intelligence.
F. Ciravegna. 2001. Adaptive Information Extraction
from Text by Rule Induction and Generalisation. In
Proceedings of the 17th International Joint Confer-
ence on Artificial Intelligence.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Information
Extraction Systems by Gibbs Sampling. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 363–370,
Ann Arbor, MI, June.
A. Finn and N. Kushmerick. 2004. Multi-level
Boundary Classification for Information Extraction.
In In Proceedings of the 15th European Conference
on Machine Learning, pages 111–122, Pisa, Italy,
September.
Dayne Freitag. 1998a. Multistrategy Learning for
Information Extraction. In Proceedings of the Fif-
teenth International Conference on Machine Learn-
ing. Morgan Kaufmann Publishers.
Dayne Freitag. 1998b. Toward General-Purpose
Learning for Information Extraction. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics.
Z. Gu and N. Cercone. 2006. Segment-Based Hidden
Markov Models for Information Extraction. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 481–488, Sydney, Australia, July.
Ruihong Huang and Ellen Riloff. 2010. Inducing
Domain-specific Semantic Class Taggers from (Al-
most) Nothing. In Proceedings of The 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2010).
Ruihong Huang and Ellen Riloff. 2011. Peeling Back
the Layers: Detecting Event Role Fillers in Sec-
ondary Contexts. In Proceedings ofthe 49th Annual
</reference>
<page confidence="0.976973">
294
</page>
<reference confidence="0.999805947368421">
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-11).
S. Huffman. 1996. Learning Information Extraction
Patterns from Examples. In Stefan Wermter, Ellen
Riloff, and Gabriele Scheler, editors, Connectionist,
Statistical, and Symbolic Approaches to Learning
for Natural Language Processing, pages 246–260.
Springer-Verlag, Berlin.
H. Ji and R. Grishman. 2008. Refining Event Extrac-
tion through Cross-Document Inference. In Pro-
ceedings ofACL-08: HLT, pages 254–262, Colum-
bus, OH, June.
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale
Linear SVMs. Journal of Machine Learning Re-
search.
J. Kim and D. Moldovan. 1993. Acquisition of
Semantic Patterns for Information Extraction from
Corpora. In Proceedings of the Ninth IEEE Con-
ference on Artificial Intelligence for Applications,
pages 171–176, Los Alamitos, CA. IEEE Computer
Society Press.
Y. Li, K. Bontcheva, and H. Cunningham. 2005. Us-
ing Uneven Margins SVM and Perceptron for Infor-
mation Extraction. In Proceedings ofNinth Confer-
ence on Computational Natural Language Learn-
ing, pages 72–79, Ann Arbor, MI, June.
Shasha Liao and Ralph Grishman. 2010. Using Docu-
ment Level Cross-Event Inference to Improve Event
Extraction. In Proceedings of the 48st Annual
Meeting on Association for Computational Linguis-
tics (ACL-10).
M. Maslennikov and T. Chua. 2007. A Multi-
Resolution Framework for Information Extraction
from Free Text. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics.
G. Miller. 1990. Wordnet: An On-line Lexical
Database. International Journal of Lexicography,
3(4).
MUC-4 Proceedings. 1992. Proceedings of the
Fourth Message Understanding Conference (MUC-
4). Morgan Kaufmann.
S. Patwardhan and E. Riloff. 2007. Effective Informa-
tion Extraction with Semantic Affinity Patterns and
Relevant Regions. In Proceedings of 2007 the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2007).
S. Patwardhan and E. Riloff. 2009. A Unified Model
of Phrasal and Sentential Evidence for Information
Extraction. In Proceedings of 2009 the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2009).
S. Patwardhan. 2010. Widening the Field of View
of Information Extraction through Sentential Event
Recognition. Ph.D. thesis, University of Utah.
W. Phillips and E. Riloff. 2007. Exploiting Role-
Identifying Nouns and Expressions for Information
Extraction. In Proceedings of the 2007 Interna-
tional Conference on Recent Advances in Natural
Language Processing (RANLP-07), pages 468–473.
E. Riloff and R. Jones. 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level Boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff. 1993. Automatically Constructing a Dictio-
nary for Information Extraction Tasks. In Proceed-
ings of the 11th National Conference on Artificial
Intelligence.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intel-
ligence, pages 1044–1049.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of Joint Conference of the In-
ternational Committee on Computational Linguis-
tics and the Association for Computational Linguis-
tics (COLING/ACL-06.
Y. Shinyama and S. Sekine. 2006. Preemptive In-
formation Extraction using Unrestricted Relation
Discovery. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 304–311, New York City, NY,
June.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a conceptual dictio-
nary. In Proc. of the Fourteenth International Joint
Conference on Artificial Intelligence, pages 1314–
1319.
M. Stevenson and M. Greenwood. 2005. A Seman-
tic Approach to IE Pattern Induction. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 379–386, Ann
Arbor, MI, June.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for
Automatic IE Pattern Acquisition. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL-03).
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the Eighteenth International Conference on
Computational Linguistics (COLING 2000).
K. Yu, G. Guan, and M. Zhou. 2005. Resum´e In-
formation Extraction with Cascaded Hybrid Model.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics, pages
499–506, Ann Arbor, MI, June.
</reference>
<page confidence="0.99857">
295
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.700871">
<title confidence="0.998507">Bootstrapped Training of Event Extraction Classifiers</title>
<author confidence="0.999122">Ruihong Huang</author>
<author confidence="0.999122">Ellen</author>
<affiliation confidence="0.998266">School of University of</affiliation>
<address confidence="0.806506">Salt Lake City, UT</address>
<abstract confidence="0.992927954545455">Most event extraction systems are trained with supervised learning and rely on a collection of annotated documents. Due to the domain-specificity of this task, event extraction systems must be retrained with new annotated data for each domain. In this paper, we propose a bootstrapping solution for event role filler extraction that requires minimal human supervision. We aim to rapidly train a state-of-the-art event extraction system using a small set of “seed nouns” for each event role, a collection of relevant (in-domain) and irrelevant (outof-domain) texts, and a semantic dictionary. The experimental results show that the bootstrapped system outperforms previous weakly supervised event extraction systems on the MUC-4 data set, and achieves performance levels comparable to supervised training with 700 manually annotated documents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M E Califf</author>
<author>R Mooney</author>
</authors>
<title>Bottom-up Relational Learning of Pattern Matching rules for Information Extraction.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>4--177</pages>
<contexts>
<context position="5394" citStr="Califf and Mooney, 2003" startWordPosition="811" endWordPosition="814"> MUC4 data set, which is a standard benchmark for event extraction research. Our results show that the bootstrapped system, TIERlit, outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discours</context>
</contexts>
<marker>Califf, Mooney, 2003</marker>
<rawString>M.E. Califf and R. Mooney. 2003. Bottom-up Relational Learning of Pattern Matching rules for Information Extraction. Journal ofMachine Learning Research, 4:177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Template-Based Information Extraction without the Templates.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-11).</booktitle>
<contexts>
<context position="7457" citStr="Chambers and Jurafsky, 2011" startWordPosition="1133" endWordPosition="1136">action patterns in an unsupervised way (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). But these efforts target open domain information extraction. To extract domainspecific event information, domain experts are needed to select the pattern subsets to use. There have also been weakly supervised approaches that use more than just local context. (Patwardhan and Riloff, 2007) uses a semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences. The event sentence classifier is self-trained using seed patterns. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resource, group the event words to form event scenarios, and group extraction patterns for different event roles. However, these weakly supervised systems produce substantially lower performance than the best supervised systems. 3 Overview of TIER The goal of our research is to develop a weakly supervised training process that can successfully train a state-of-the-art event extraction system for a new domain with minimal human input. We decided to focus our efforts on the TIER event extraction model because it recently produced better performance on the MU</context>
<context position="27492" citStr="Chambers and Jurafsky, 2011" startWordPosition="4305" endWordPosition="4308">s and ranks them based on their frequency and probability of occurring in relevant documents. A human expert then examines the patterns and 1Documents may contain multiple events per article. 2For example, “armed men” will match “5 armed men”. manually selects the best patterns for each event role. During testing, the patterns are matched against unseen texts to extract event role fillers. PIPER (Patwardhan and Riloff, 2007; Patwardhan, 2010) learns extraction patterns using a semantic affinity measure, and it distinguishes between primary and secondary patterns and applies them selectively. (Chambers and Jurafsky, 2011) (C+J) created an event extraction system by acquiring event words from WordNet (Miller, 1990), clustering the event words into different event scenarios, and grouping extraction patterns for different event roles. 5.3 Performance of TIERlite Table 2 shows the seed nouns that we used in our experiments, which were generated by sorting the nouns in the corpus by frequency and manually identifying the first 10 role-identifying nouns for each event role.3 Table 3 shows the number of training instances (noun phrases) that were automatically labeled for each event role using our training data creat</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Template-Based Information Extraction without the Templates. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H L Chieu</author>
<author>H T Ng</author>
</authors>
<title>A Maximum Entropy Approach to Information Extraction from SemiStructured and Free Text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 18th National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1685" citStr="Chieu and Ng, 2002" startWordPosition="253" endWordPosition="256">ms process stories about domain-relevant events and identify the role fillers of each event. A key challenge for event extraction is that recognizing role fillers is inherently contextual. For example, a PERSON can be a perpetrator or a victim in different contexts (e.g., “John Smith assassinated the mayor” vs. “John Smith was assassinated”). Similarly, any COMPANY can be an acquirer or an acquiree depending on the context. Many supervised learning techniques have been used to create event extraction systems using gold standard “answer key” event templates for training (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Maslennikov and Chua, 2007)). However, manually generating answer keys for event extraction is time-consuming and tedious. And more importantly, event extraction annotations are highly domain-specific, so new annotations must be obtained for each domain. The goal of our research is to use bootstrapping techniques to automatically train a state-ofthe-art event extraction system without humangenerated answer key templates. The focus of our work is the TIER event extraction model, which is a multi-layered architecture for event extraction (Huang and Riloff, 2011). TIER’s innovation over previou</context>
<context position="5569" citStr="Chieu and Ng, 2002" startWordPosition="839" endWordPosition="842">traction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Cu</context>
</contexts>
<marker>Chieu, Ng, 2002</marker>
<rawString>H.L. Chieu and H.T. Ng. 2002. A Maximum Entropy Approach to Information Extraction from SemiStructured and Free Text. In Proceedings of the 18th National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ciravegna</author>
</authors>
<title>Adaptive Information Extraction from Text by Rule Induction and Generalisation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 17th International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="5369" citStr="Ciravegna, 2001" startWordPosition="809" endWordPosition="810">al results on the MUC4 data set, which is a standard benchmark for event extraction research. Our results show that the bootstrapped system, TIERlit, outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global v</context>
</contexts>
<marker>Ciravegna, 2001</marker>
<rawString>F. Ciravegna. 2001. Adaptive Information Extraction from Text by Rule Induction and Generalisation. In Proceedings of the 17th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="21236" citStr="Finkel et al., 2005" startWordPosition="3324" endWordPosition="3327"> the negative:positive ratio to be 10:1. Once the classifier is trained, it is applied to the unlabeled noun phrases in the relevant documents. Noun phrases that are assigned role filler labels by the classifier with high confidence (using the sliding threshold) are added to the set of positive instances. New negative instances are drawn randomly from the irrelevant documents to maintain the 10:1 (negative:positive) ratio. We extract features from each noun phrase (NP) and its surrounding context. The features include the NP head noun and its premodifiers. We also use the Stanford NER tagger (Finkel et al., 2005) to identify Named Entities within the NP. The context features include four words to the left of the NP, four words to the right of the NP, and the lexico-syntactic patterns generated by AutoSlog to capture expressions around the NP (see (Riloff, 1993) for details). 4.2.2 Event Sentence Classifier The event sentence classifier is responsible for identifying sentences that describe a relevant event. Similar to the noun phrase classifier training, positive training instances are selected from the relevant documents and negative instances are drawn from the irrelevant documents. All sentences in</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 363–370, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Finn</author>
<author>N Kushmerick</author>
</authors>
<title>Multi-level Boundary Classification for Information Extraction. In</title>
<date>2004</date>
<booktitle>In Proceedings of the 15th European Conference on Machine Learning,</booktitle>
<pages>111--122</pages>
<location>Pisa, Italy,</location>
<contexts>
<context position="5596" citStr="Finn and Kushmerick, 2004" startWordPosition="843" endWordPosition="846"> achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based</context>
</contexts>
<marker>Finn, Kushmerick, 2004</marker>
<rawString>A. Finn and N. Kushmerick. 2004. Multi-level Boundary Classification for Information Extraction. In In Proceedings of the 15th European Conference on Machine Learning, pages 111–122, Pisa, Italy, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Multistrategy Learning for Information Extraction.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning.</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="1664" citStr="Freitag, 1998" startWordPosition="251" endWordPosition="252">extraction systems process stories about domain-relevant events and identify the role fillers of each event. A key challenge for event extraction is that recognizing role fillers is inherently contextual. For example, a PERSON can be a perpetrator or a victim in different contexts (e.g., “John Smith assassinated the mayor” vs. “John Smith was assassinated”). Similarly, any COMPANY can be an acquirer or an acquiree depending on the context. Many supervised learning techniques have been used to create event extraction systems using gold standard “answer key” event templates for training (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Maslennikov and Chua, 2007)). However, manually generating answer keys for event extraction is time-consuming and tedious. And more importantly, event extraction annotations are highly domain-specific, so new annotations must be obtained for each domain. The goal of our research is to use bootstrapping techniques to automatically train a state-ofthe-art event extraction system without humangenerated answer key templates. The focus of our work is the TIER event extraction model, which is a multi-layered architecture for event extraction (Huang and Riloff, 2011). TIER’s in</context>
<context position="5351" citStr="Freitag, 1998" startWordPosition="807" endWordPosition="808">esent experimental results on the MUC4 data set, which is a standard benchmark for event extraction research. Our results show that the bootstrapped system, TIERlit, outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems ta</context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>Dayne Freitag. 1998a. Multistrategy Learning for Information Extraction. In Proceedings of the Fifteenth International Conference on Machine Learning. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Toward General-Purpose Learning for Information Extraction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1664" citStr="Freitag, 1998" startWordPosition="251" endWordPosition="252">extraction systems process stories about domain-relevant events and identify the role fillers of each event. A key challenge for event extraction is that recognizing role fillers is inherently contextual. For example, a PERSON can be a perpetrator or a victim in different contexts (e.g., “John Smith assassinated the mayor” vs. “John Smith was assassinated”). Similarly, any COMPANY can be an acquirer or an acquiree depending on the context. Many supervised learning techniques have been used to create event extraction systems using gold standard “answer key” event templates for training (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Maslennikov and Chua, 2007)). However, manually generating answer keys for event extraction is time-consuming and tedious. And more importantly, event extraction annotations are highly domain-specific, so new annotations must be obtained for each domain. The goal of our research is to use bootstrapping techniques to automatically train a state-ofthe-art event extraction system without humangenerated answer key templates. The focus of our work is the TIER event extraction model, which is a multi-layered architecture for event extraction (Huang and Riloff, 2011). TIER’s in</context>
<context position="5351" citStr="Freitag, 1998" startWordPosition="807" endWordPosition="808">esent experimental results on the MUC4 data set, which is a standard benchmark for event extraction research. Our results show that the bootstrapped system, TIERlit, outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems ta</context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>Dayne Freitag. 1998b. Toward General-Purpose Learning for Information Extraction. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Gu</author>
<author>N Cercone</author>
</authors>
<title>Segment-Based Hidden Markov Models for Information Extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>481--488</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="5902" citStr="Gu and Cercone, 2006" startWordPosition="895" endWordPosition="898">3; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in t</context>
</contexts>
<marker>Gu, Cercone, 2006</marker>
<rawString>Z. Gu and N. Cercone. 2006. Segment-Based Hidden Markov Models for Information Extraction. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 481–488, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Inducing Domain-specific Semantic Class Taggers from (Almost) Nothing.</title>
<date>2010</date>
<booktitle>In Proceedings of The 48th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="16672" citStr="Huang and Riloff, 2010" startWordPosition="2597" endWordPosition="2600"> nouns or if the semantic class of the head noun is compatible with the corresponding event role. In the previous example, tsunami will not be extracted as a weapon because it has an incompatible semantic class (EVENT), but bomb will be extracted because it has a compatible semantic class (WEAPON). We use the semantic class labels assigned by the Sundance parser (Riloff and Phillips, 2004) in our experiments. Sundance looks up each noun in a semantic dictionary to assign the semantic class labels. As an alternative, general resources (e.g., WordNet (Miller, 1990)) or a semantic tagger (e.g., (Huang and Riloff, 2010)) could be used. 289 Figure 3: Automatic Training Data Creation 4.1.3 Propagating Labels with Coreference To enrich the automatically labeled training instances, we also propagate the event role labels across coreferent noun phrases within a document. The observation is that once a noun phrase has been identified as a role filler, its coreferent mentions in the same document likely fill the same event role since they are referring to the same real world entity. To leverage these coreferential contexts, we employ a simple head noun matching heuristic to identify coreferent noun phrases. This he</context>
</contexts>
<marker>Huang, Riloff, 2010</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2010. Inducing Domain-specific Semantic Class Taggers from (Almost) Nothing. In Proceedings of The 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts.</title>
<date>2011</date>
<booktitle>In Proceedings ofthe 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-11).</booktitle>
<contexts>
<context position="2253" citStr="Huang and Riloff, 2011" startWordPosition="338" endWordPosition="341">for training (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Maslennikov and Chua, 2007)). However, manually generating answer keys for event extraction is time-consuming and tedious. And more importantly, event extraction annotations are highly domain-specific, so new annotations must be obtained for each domain. The goal of our research is to use bootstrapping techniques to automatically train a state-ofthe-art event extraction system without humangenerated answer key templates. The focus of our work is the TIER event extraction model, which is a multi-layered architecture for event extraction (Huang and Riloff, 2011). TIER’s innovation over previous techniques is the use of four different classifiers that analyze a document at increasing levels of granularity. TIER progressively zooms in on event information using a pipeline of classifiers that perform document-level classification, sentence classification, and noun phrase classification. TIER outperformed previous event extraction systems on the MUC-4 data set, but relied heavily on a large collection of 1,300 documents coupled with answer key templates to train its four classifiers. In this paper, we present a bootstrapping solution that exploits a larg</context>
<context position="6164" citStr="Huang and Riloff, 2011" startWordPosition="937" endWordPosition="940">g, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must manually review and “clean” the learned patterns to o</context>
<context position="8145" citStr="Huang and Riloff, 2011" startWordPosition="1242" endWordPosition="1245">ords to form event scenarios, and group extraction patterns for different event roles. However, these weakly supervised systems produce substantially lower performance than the best supervised systems. 3 Overview of TIER The goal of our research is to develop a weakly supervised training process that can successfully train a state-of-the-art event extraction system for a new domain with minimal human input. We decided to focus our efforts on the TIER event extraction model because it recently produced better performance on the MUC-4 data set than prior learning-based event extraction systems (Huang and Riloff, 2011). In this section, we briefly give an overview of TIER’s architecture and its com287 Figure 1: TIER Overview ponents. TIER is a multi-layered architecture for event extraction, as shown in Figure 1. Documents pass through a pipeline where they are analyzed at different levels of granularity, which enables the system to gradually “zoom in” on relevant facts. The pipeline consists of a document genre classifier, two types of sentence classifiers, and a set of noun phrase (role filler) classifiers. The lower pathway in Figure 1 shows that all documents pass through an event sentence classifier. S</context>
<context position="23178" citStr="Huang and Riloff, 2011" startWordPosition="3622" endWordPosition="3625">ed noun phrase for the appropriate event role are used as positive instances. Negative instances are randomly sampled from the irrelevant documents to maintain the negative:positive ratio of 10:1. The bootstrapping process and feature set are the same as for the event sentence classifier. The difference between the two types of sentence classifiers is that the event sentence classifier uses positive instances from all event roles, while each role-specific sentence classifiers only uses the positive instances for one particular event role. The rationale is similar as in the supervised setting (Huang and Riloff, 2011); the event sentence classifier is expected to generalize over all event roles to identify event mention contexts, while the role-specific sentence classifiers are expected to learn to identify contexts specific to individual roles. 4.2.4 Event Narrative Document Classifier TIER also uses an event narrative document classifier and only extracts information from rolespecific sentences within event narrative documents. In the supervised setting, TIER uses heuristic rules derived from answer key templates to identify the event narrative documents in the training set, which are used to train an ev</context>
<context position="28957" citStr="Huang and Riloff, 2011" startWordPosition="4520" endWordPosition="4523">dellin Cartel The Extraditables Army of National Liberation Target houses residence building home homes offices pipeline hotel car vehicles Victim victims civilians children jesuits Galan priests students women peasants Romero Weapon weapons bomb bombs explosives rifles dynamite grenades device car bomb Table 2: Role-Identifying Seed Nouns PerpInd PerpOrg Target Victim Weapon 296 157 522 798 248 Table 3: # of Automatically Labeled NPs Table 4 shows how our bootstrapped system TIERlite compares with previous weakly supervised systems and two supervised systems, its supervised counterpart TIER (Huang and Riloff, 2011) and a model that jointly considers local and sentential contexts, GLACIER (Patwardhan 3We only found 9 weapon terms among the highfrequency terms. 292 Weakly Supervised Baselines PerpInd PerpOrg Target Victim Weapon Average AUTOSLOG-TS (1996) 33/49/40 52/33/41 54/59/56 49/54/51 38/44/41 45/48/46 PIPERBe3t (2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/46/45 C+J (2011) - - - - - 44/36/40 Supervised Models GLACIER (2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52 TIER (2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56 Weakly Supervised Models TIER,ite 47/51/49 60/39</context>
</contexts>
<marker>Huang, Riloff, 2011</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2011. Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts. In Proceedings ofthe 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Huffman</author>
</authors>
<title>Learning Information Extraction Patterns from Examples.</title>
<date>1996</date>
<booktitle>Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing,</booktitle>
<pages>246--260</pages>
<editor>In Stefan Wermter, Ellen Riloff, and Gabriele Scheler, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="5336" citStr="Huffman, 1996" startWordPosition="805" endWordPosition="806">ssifiers. We present experimental results on the MUC4 data set, which is a standard benchmark for event extraction research. Our results show that the bootstrapped system, TIERlit, outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). O</context>
</contexts>
<marker>Huffman, 1996</marker>
<rawString>S. Huffman. 1996. Learning Information Extraction Patterns from Examples. In Stefan Wermter, Ellen Riloff, and Gabriele Scheler, editors, Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing, pages 246–260. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
</authors>
<title>Refining Event Extraction through Cross-Document Inference.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>254--262</pages>
<location>Columbus, OH,</location>
<contexts>
<context position="6114" citStr="Ji and Grishman, 2008" startWordPosition="928" endWordPosition="932">eenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must man</context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>H. Ji and R. Grishman. 2008. Refining Event Extraction through Cross-Document Inference. In Proceedings ofACL-08: HLT, pages 254–262, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Keerthi</author>
<author>D DeCoste</author>
</authors>
<title>A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="18908" citStr="Keerthi and DeCoste, 2005" startWordPosition="2940" endWordPosition="2943"> to noun phrase #3. 4.2 Creating TIERlite with Bootstrapping In this section, we explain how the labeled instances are used to train TIER’s classifiers with bootstrapping. In addition to the automatically labeled instances, the training process depends on a text corpus that consists of both relevant (in-domain) and irrelevant (out-of-domain) documents. Positive instances are generated from the relevant documents and negative instances are generated by randomly sampling from the irrelevant documents. The classifiers are all support vector machines (SVMs), implemented using the SVMlin software (Keerthi and DeCoste, 2005). When applying the classifiers during bootstrapping, we use a sliding confidence threshold to determine which labels are reliable based on the values produced by the SVM. Initially, we set the threshold to be 2.0 to identify highly confident predictions. But if fewer than k instances pass the threshold, then we slide the threshold down in decrements of 0.1 until we obtain at least k labeled instances or the threshold drops below 0, in which case bootstrapping ends. We used k=10 for both sentence classifiers and k=30 for the noun phrase classifiers. The following sections present the details o</context>
</contexts>
<marker>Keerthi, DeCoste, 2005</marker>
<rawString>S. Keerthi and D. DeCoste. 2005. A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>D Moldovan</author>
</authors>
<title>Acquisition of Semantic Patterns for Information Extraction from Corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the Ninth IEEE Conference on Artificial Intelligence for Applications,</booktitle>
<pages>171--176</pages>
<publisher>IEEE Computer Society Press.</publisher>
<location>Los Alamitos, CA.</location>
<contexts>
<context position="5283" citStr="Kim and Moldovan, 1993" startWordPosition="795" endWordPosition="798">ssifier, we apply heuristics to the output of the sentence classifiers. We present experimental results on the MUC4 data set, which is a standard benchmark for event extraction research. Our results show that the bootstrapped system, TIERlit, outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (G</context>
</contexts>
<marker>Kim, Moldovan, 1993</marker>
<rawString>J. Kim and D. Moldovan. 1993. Acquisition of Semantic Patterns for Information Extraction from Corpora. In Proceedings of the Ninth IEEE Conference on Artificial Intelligence for Applications, pages 171–176, Los Alamitos, CA. IEEE Computer Society Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>K Bontcheva</author>
<author>H Cunningham</author>
</authors>
<title>Using Uneven Margins SVM and Perceptron for Information Extraction.</title>
<date>2005</date>
<booktitle>In Proceedings ofNinth Conference on Computational Natural Language Learning,</booktitle>
<pages>72--79</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="5613" citStr="Li et al., 2005" startWordPosition="847" endWordPosition="850">s comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction</context>
</contexts>
<marker>Li, Bontcheva, Cunningham, 2005</marker>
<rawString>Y. Li, K. Bontcheva, and H. Cunningham. 2005. Using Uneven Margins SVM and Perceptron for Information Extraction. In Proceedings ofNinth Conference on Computational Natural Language Learning, pages 72–79, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Liao</author>
<author>Ralph Grishman</author>
</authors>
<title>Using Document Level Cross-Event Inference to Improve Event Extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48st Annual Meeting on Association for Computational Linguistics (ACL-10).</booktitle>
<contexts>
<context position="6139" citStr="Liao and Grishman, 2010" startWordPosition="933" endWordPosition="936">ifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must manually review and “clean” </context>
</contexts>
<marker>Liao, Grishman, 2010</marker>
<rawString>Shasha Liao and Ralph Grishman. 2010. Using Document Level Cross-Event Inference to Improve Event Extraction. In Proceedings of the 48st Annual Meeting on Association for Computational Linguistics (ACL-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maslennikov</author>
<author>T Chua</author>
</authors>
<title>A MultiResolution Framework for Information Extraction from Free Text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1714" citStr="Maslennikov and Chua, 2007" startWordPosition="257" endWordPosition="260">bout domain-relevant events and identify the role fillers of each event. A key challenge for event extraction is that recognizing role fillers is inherently contextual. For example, a PERSON can be a perpetrator or a victim in different contexts (e.g., “John Smith assassinated the mayor” vs. “John Smith was assassinated”). Similarly, any COMPANY can be an acquirer or an acquiree depending on the context. Many supervised learning techniques have been used to create event extraction systems using gold standard “answer key” event templates for training (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Maslennikov and Chua, 2007)). However, manually generating answer keys for event extraction is time-consuming and tedious. And more importantly, event extraction annotations are highly domain-specific, so new annotations must be obtained for each domain. The goal of our research is to use bootstrapping techniques to automatically train a state-ofthe-art event extraction system without humangenerated answer key templates. The focus of our work is the TIER event extraction model, which is a multi-layered architecture for event extraction (Huang and Riloff, 2011). TIER’s innovation over previous techniques is the use of fo</context>
<context position="6091" citStr="Maslennikov and Chua, 2007" startWordPosition="924" endWordPosition="927"> al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these appro</context>
</contexts>
<marker>Maslennikov, Chua, 2007</marker>
<rawString>M. Maslennikov and T. Chua. 2007. A MultiResolution Framework for Information Extraction from Free Text. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: An On-line Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="16618" citStr="Miller, 1990" startWordPosition="2589" endWordPosition="2590">head noun is among the role-identifying seed nouns or if the semantic class of the head noun is compatible with the corresponding event role. In the previous example, tsunami will not be extracted as a weapon because it has an incompatible semantic class (EVENT), but bomb will be extracted because it has a compatible semantic class (WEAPON). We use the semantic class labels assigned by the Sundance parser (Riloff and Phillips, 2004) in our experiments. Sundance looks up each noun in a semantic dictionary to assign the semantic class labels. As an alternative, general resources (e.g., WordNet (Miller, 1990)) or a semantic tagger (e.g., (Huang and Riloff, 2010)) could be used. 289 Figure 3: Automatic Training Data Creation 4.1.3 Propagating Labels with Coreference To enrich the automatically labeled training instances, we also propagate the event role labels across coreferent noun phrases within a document. The observation is that once a noun phrase has been identified as a role filler, its coreferent mentions in the same document likely fill the same event role since they are referring to the same real world entity. To leverage these coreferential contexts, we employ a simple head noun matching </context>
<context position="27586" citStr="Miller, 1990" startWordPosition="4321" endWordPosition="4322">then examines the patterns and 1Documents may contain multiple events per article. 2For example, “armed men” will match “5 armed men”. manually selects the best patterns for each event role. During testing, the patterns are matched against unseen texts to extract event role fillers. PIPER (Patwardhan and Riloff, 2007; Patwardhan, 2010) learns extraction patterns using a semantic affinity measure, and it distinguishes between primary and secondary patterns and applies them selectively. (Chambers and Jurafsky, 2011) (C+J) created an event extraction system by acquiring event words from WordNet (Miller, 1990), clustering the event words into different event scenarios, and grouping extraction patterns for different event roles. 5.3 Performance of TIERlite Table 2 shows the seed nouns that we used in our experiments, which were generated by sorting the nouns in the corpus by frequency and manually identifying the first 10 role-identifying nouns for each event role.3 Table 3 shows the number of training instances (noun phrases) that were automatically labeled for each event role using our training data creation approach (Section 4.1). Event Role Seed Nouns Perpetrator terrorists assassins criminals r</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. Miller. 1990. Wordnet: An On-line Lexical Database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-4 Proceedings</author>
</authors>
<date>1992</date>
<booktitle>Proceedings of the Fourth Message Understanding Conference (MUC4).</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="10168" citStr="Proceedings, 1992" startWordPosition="1566" endWordPosition="1567">to a hospital or the identification of a body. Sentences that are determined to have “role-specific” contexts are passed along to the noun phrase classifiers for role filler extraction. Consequently, event narrative documents pass through both the lower pathway and the upper pathway. This approach creates an event extraction system that can discover role fillers in a variety of different contexts by considering the type of document being processed. TIER was originally trained with supervised learning using 1,300 texts and their corresponding answer key templates from the MUC-4 data set (MUC-4 Proceedings, 1992). Human-generated answer key templates are expensive to produce because the annotation process is both difficult and time-consuming. Furthermore, answer key templates for one domain are virtually never reusable for different domains, so a new set of answer keys must be produced from scratch for each domain. In the next section, we present our weakly supervised approach for training TIER’s event extraction classifiers. 4 Bootstrapped Training of Event Extraction Classifiers We adopt a two-phase approach to train TIER’s event extraction modules using minimal humangenerated resources. The goal of</context>
<context position="25251" citStr="Proceedings, 1992" startWordPosition="3948" endWordPosition="3950">r bootstrapped system, TIERlite, on the MUC-4 event extraction data set. First, we describe the IE task, the data set, and the weakly supervised baseline systems that we use for comparison. Then we present the results of our fully bootstrapped system TIERlite, the weakly supervised baseline systems, and two fully supervised event extraction systems, TIER 291 and GLACIER. In addition, we analyze the performance of TIERlite using different configurations to assess the impact of its components. 5.1 IE Task and Data We evaluated the performance of our systems on the MUC-4 terrorism IE task (MUC-4 Proceedings, 1992) about Latin American terrorist events. We used 1,300 texts (DEV) as our training set and 200 texts (TST3+TST4) as the test set. All the documents have answer key templates. For the training set, we used the answer keys to separate the documents into relevant and irrelevant subsets. Any document containing at least one relevant event was considered to be relevant. PerpInd PerpOrg Target Victim Weapon 129 74 126 201 58 Table 1: # of Role Fillers in the MUC-4 Test Set Following previous studies, we evaluate our system on five MUC-4 string event roles: perpetrator individuals (PerpInd), perpetrat</context>
</contexts>
<marker>Proceedings, 1992</marker>
<rawString>MUC-4 Proceedings. 1992. Proceedings of the Fourth Message Understanding Conference (MUC4). Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>E Riloff</author>
</authors>
<title>Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions.</title>
<date>2007</date>
<booktitle>In Proceedings of 2007 the Conference on Empirical Methods in Natural Language Processing (EMNLP-2007).</booktitle>
<contexts>
<context position="7207" citStr="Patwardhan and Riloff, 2007" startWordPosition="1096" endWordPosition="1099"> Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must manually review and “clean” the learned patterns to obtain good performance. Research has also been done to learn extraction patterns in an unsupervised way (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). But these efforts target open domain information extraction. To extract domainspecific event information, domain experts are needed to select the pattern subsets to use. There have also been weakly supervised approaches that use more than just local context. (Patwardhan and Riloff, 2007) uses a semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences. The event sentence classifier is self-trained using seed patterns. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resource, group the event words to form event scenarios, and group extraction patterns for different event roles. However, these weakly supervised systems produce substantially lower performance than the best supervised systems. 3 Overview of TIER The goal of our research is to develop a weakly supervised train</context>
<context position="27291" citStr="Patwardhan and Riloff, 2007" startWordPosition="4275" endWordPosition="4278"> compared the performance of our system with three previous weakly supervised event extraction systems. AutoSlog-TS (Riloff, 1996) generates lexicosyntactic patterns exhaustively from unannotated texts and ranks them based on their frequency and probability of occurring in relevant documents. A human expert then examines the patterns and 1Documents may contain multiple events per article. 2For example, “armed men” will match “5 armed men”. manually selects the best patterns for each event role. During testing, the patterns are matched against unseen texts to extract event role fillers. PIPER (Patwardhan and Riloff, 2007; Patwardhan, 2010) learns extraction patterns using a semantic affinity measure, and it distinguishes between primary and secondary patterns and applies them selectively. (Chambers and Jurafsky, 2011) (C+J) created an event extraction system by acquiring event words from WordNet (Miller, 1990), clustering the event words into different event scenarios, and grouping extraction patterns for different event roles. 5.3 Performance of TIERlite Table 2 shows the seed nouns that we used in our experiments, which were generated by sorting the nouns in the corpus by frequency and manually identifying </context>
</contexts>
<marker>Patwardhan, Riloff, 2007</marker>
<rawString>S. Patwardhan and E. Riloff. 2007. Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions. In Proceedings of 2007 the Conference on Empirical Methods in Natural Language Processing (EMNLP-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>E Riloff</author>
</authors>
<title>A Unified Model of Phrasal and Sentential Evidence for Information Extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of 2009 the Conference on Empirical Methods in Natural Language Processing (EMNLP-2009).</booktitle>
<contexts>
<context position="5932" citStr="Patwardhan and Riloff, 2009" startWordPosition="899" endWordPosition="902">land et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-ba</context>
</contexts>
<marker>Patwardhan, Riloff, 2009</marker>
<rawString>S. Patwardhan and E. Riloff. 2009. A Unified Model of Phrasal and Sentential Evidence for Information Extraction. In Proceedings of 2009 the Conference on Empirical Methods in Natural Language Processing (EMNLP-2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
</authors>
<title>Widening the Field of View of Information Extraction through Sentential Event Recognition.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Utah.</institution>
<contexts>
<context position="27310" citStr="Patwardhan, 2010" startWordPosition="4279" endWordPosition="4281">our system with three previous weakly supervised event extraction systems. AutoSlog-TS (Riloff, 1996) generates lexicosyntactic patterns exhaustively from unannotated texts and ranks them based on their frequency and probability of occurring in relevant documents. A human expert then examines the patterns and 1Documents may contain multiple events per article. 2For example, “armed men” will match “5 armed men”. manually selects the best patterns for each event role. During testing, the patterns are matched against unseen texts to extract event role fillers. PIPER (Patwardhan and Riloff, 2007; Patwardhan, 2010) learns extraction patterns using a semantic affinity measure, and it distinguishes between primary and secondary patterns and applies them selectively. (Chambers and Jurafsky, 2011) (C+J) created an event extraction system by acquiring event words from WordNet (Miller, 1990), clustering the event words into different event scenarios, and grouping extraction patterns for different event roles. 5.3 Performance of TIERlite Table 2 shows the seed nouns that we used in our experiments, which were generated by sorting the nouns in the corpus by frequency and manually identifying the first 10 role-i</context>
</contexts>
<marker>Patwardhan, 2010</marker>
<rawString>S. Patwardhan. 2010. Widening the Field of View of Information Extraction through Sentential Event Recognition. Ph.D. thesis, University of Utah.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Phillips</author>
<author>E Riloff</author>
</authors>
<title>Exploiting RoleIdentifying Nouns and Expressions for Information Extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 International Conference on Recent Advances in Natural Language Processing (RANLP-07),</booktitle>
<pages>468--473</pages>
<contexts>
<context position="2946" citStr="Phillips and Riloff, 2007" startWordPosition="442" endWordPosition="445">ferent classifiers that analyze a document at increasing levels of granularity. TIER progressively zooms in on event information using a pipeline of classifiers that perform document-level classification, sentence classification, and noun phrase classification. TIER outperformed previous event extraction systems on the MUC-4 data set, but relied heavily on a large collection of 1,300 documents coupled with answer key templates to train its four classifiers. In this paper, we present a bootstrapping solution that exploits a large unannotated corpus for training by using role-identifying nouns (Phillips and Riloff, 2007) as seed terms. Phillips and Riloff observed that some nouns, by definition, refer to entities or objects that play a specific role in an event. For example, “assassin”, “sniper”, and “hitman” refer to people who play the role of PERPETRATOR in a criminal event. Similarly, “victim”, “casualty”, and “fatality” refer to people who play the role of VICTIM, by virtue of their lexical semantics. Phillips and Riloff called these words role-identifying nouns and used them 286 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 286–295, Av</context>
<context position="13256" citStr="Phillips and Riloff, 2007" startWordPosition="2043" endWordPosition="2046">lain how we generate role-identifying patterns automatically using seed nouns, and we discuss why we add semantic constraints to the patterns when producing labeled instances for training. Then, we discuss the coreference-based label propagation that we used to obtain additional training instances. Finally, we give examples to illustrate how we create training instances. 4.1.1 Inducing Role-Identifying Patterns The input to our system is a small set of manually-defined seed nouns for each event role. Specifically, the user is required to provide 10 role-identifying nouns for each event role. (Phillips and Riloff, 2007) defined a noun as being “role-identifying” if its lexical semantics reveal the role of the entity/object in an event. For example, the words “assassin” and “sniper” are people who participate in a violent event as a PERPETRATOR. Therefore, the entities referred to by role-identifying nouns are probable role fillers. However, treating every context surrounding a role-identifying noun as a role-identifying pattern is risky. The reason is that many instances of roleidentifying nouns appear in contexts that do not describe the event. But, if one pattern has been seen to extract many role-identify</context>
</contexts>
<marker>Phillips, Riloff, 2007</marker>
<rawString>W. Phillips and E. Riloff. 2007. Exploiting RoleIdentifying Nouns and Expressions for Information Extraction. In Proceedings of the 2007 International Conference on Recent Advances in Natural Language Processing (RANLP-07), pages 468–473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="5432" citStr="Riloff and Jones, 1999" startWordPosition="817" endWordPosition="820">chmark for event extraction research. Our results show that the bootstrapped system, TIERlit, outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whol</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>W Phillips</author>
</authors>
<title>An Introduction to the Sundance and AutoSlog Systems.</title>
<date>2004</date>
<tech>Technical Report UUCS-04-015,</tech>
<institution>School of Computing, University of Utah.</institution>
<contexts>
<context position="16441" citStr="Riloff and Phillips, 2004" startWordPosition="2560" endWordPosition="2563">uires a noun phrase to satisfy certain semantic constraints in order to be extracted and labeled as a positive instances for an event role. The selectional restrictions are satisfied if the head noun is among the role-identifying seed nouns or if the semantic class of the head noun is compatible with the corresponding event role. In the previous example, tsunami will not be extracted as a weapon because it has an incompatible semantic class (EVENT), but bomb will be extracted because it has a compatible semantic class (WEAPON). We use the semantic class labels assigned by the Sundance parser (Riloff and Phillips, 2004) in our experiments. Sundance looks up each noun in a semantic dictionary to assign the semantic class labels. As an alternative, general resources (e.g., WordNet (Miller, 1990)) or a semantic tagger (e.g., (Huang and Riloff, 2010)) could be used. 289 Figure 3: Automatic Training Data Creation 4.1.3 Propagating Labels with Coreference To enrich the automatically labeled training instances, we also propagate the event role labels across coreferent noun phrases within a document. The observation is that once a noun phrase has been identified as a role filler, its coreferent mentions in the same </context>
</contexts>
<marker>Riloff, Phillips, 2004</marker>
<rawString>E. Riloff and W. Phillips. 2004. An Introduction to the Sundance and AutoSlog Systems. Technical Report UUCS-04-015, School of Computing, University of Utah.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically Constructing a Dictionary for Information Extraction Tasks.</title>
<date>1993</date>
<booktitle>In Proceedings of the 11th National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="5297" citStr="Riloff, 1993" startWordPosition="799" endWordPosition="800">tics to the output of the sentence classifiers. We present experimental results on the MUC4 data set, which is a standard benchmark for event extraction research. Our results show that the bootstrapped system, TIERlit, outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone,</context>
<context position="21489" citStr="Riloff, 1993" startWordPosition="3370" endWordPosition="3371">d) are added to the set of positive instances. New negative instances are drawn randomly from the irrelevant documents to maintain the 10:1 (negative:positive) ratio. We extract features from each noun phrase (NP) and its surrounding context. The features include the NP head noun and its premodifiers. We also use the Stanford NER tagger (Finkel et al., 2005) to identify Named Entities within the NP. The context features include four words to the left of the NP, four words to the right of the NP, and the lexico-syntactic patterns generated by AutoSlog to capture expressions around the NP (see (Riloff, 1993) for details). 4.2.2 Event Sentence Classifier The event sentence classifier is responsible for identifying sentences that describe a relevant event. Similar to the noun phrase classifier training, positive training instances are selected from the relevant documents and negative instances are drawn from the irrelevant documents. All sentences in the relevant documents that contain one or more labeled noun phrases (belonging to any event role) are labeled as positive training instances. We randomly sample sentences from the irrelevant documents to obtain a negative:positive training instance ra</context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>E. Riloff. 1993. Automatically Constructing a Dictionary for Information Extraction Tasks. In Proceedings of the 11th National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically Generating Extraction Patterns from Untagged Text.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1044--1049</pages>
<contexts>
<context position="5408" citStr="Riloff, 1996" startWordPosition="815" endWordPosition="816">a standard benchmark for event extraction research. Our results show that the bootstrapped system, TIERlit, outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties o</context>
<context position="26794" citStr="Riloff, 1996" startWordPosition="4203" endWordPosition="4204">the accuracy of the role fillers irrespective of which template they occur in. We used the same head noun scoring scheme as previous systems, where an extraction is correct if its head noun matches the head noun in the answer key2. Pronouns were discarded from both the system responses and the answer keys since no coreference resolution is done. Duplicate extractions were conflated before being scored, so they count as just one hit or one miss. 5.2 Weakly Supervised Baselines We compared the performance of our system with three previous weakly supervised event extraction systems. AutoSlog-TS (Riloff, 1996) generates lexicosyntactic patterns exhaustively from unannotated texts and ranks them based on their frequency and probability of occurring in relevant documents. A human expert then examines the patterns and 1Documents may contain multiple events per article. 2For example, “armed men” will match “5 armed men”. manually selects the best patterns for each event role. During testing, the patterns are matched against unseen texts to extract event role fillers. PIPER (Patwardhan and Riloff, 2007; Patwardhan, 2010) learns extraction patterns using a semantic affinity measure, and it distinguishes </context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1044–1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>On-demand information extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL-06.</booktitle>
<contexts>
<context position="6901" citStr="Sekine, 2006" startWordPosition="1051" endWordPosition="1052">re a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must manually review and “clean” the learned patterns to obtain good performance. Research has also been done to learn extraction patterns in an unsupervised way (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). But these efforts target open domain information extraction. To extract domainspecific event information, domain experts are needed to select the pattern subsets to use. There have also been weakly supervised approaches that use more than just local context. (Patwardhan and Riloff, 2007) uses a semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences. The event sentence classifier is self-trained using seed patterns. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resour</context>
</contexts>
<marker>Sekine, 2006</marker>
<rawString>Satoshi Sekine. 2006. On-demand information extraction. In Proceedings of Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Shinyama</author>
<author>S Sekine</author>
</authors>
<title>Preemptive Information Extraction using Unrestricted Relation Discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>304--311</pages>
<location>New York City, NY,</location>
<contexts>
<context position="6901" citStr="Shinyama and Sekine, 2006" startWordPosition="1049" endWordPosition="1052">es that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must manually review and “clean” the learned patterns to obtain good performance. Research has also been done to learn extraction patterns in an unsupervised way (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). But these efforts target open domain information extraction. To extract domainspecific event information, domain experts are needed to select the pattern subsets to use. There have also been weakly supervised approaches that use more than just local context. (Patwardhan and Riloff, 2007) uses a semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences. The event sentence classifier is self-trained using seed patterns. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resour</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Y. Shinyama and S. Sekine. 2006. Preemptive Information Extraction using Unrestricted Relation Discovery. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 304–311, New York City, NY, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soderland</author>
<author>D Fisher</author>
<author>J Aseltine</author>
<author>W Lehnert</author>
</authors>
<title>CRYSTAL: Inducing a conceptual dictionary.</title>
<date>1995</date>
<booktitle>In Proc. of the Fourteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1314--1319</pages>
<contexts>
<context position="5321" citStr="Soderland et al., 1995" startWordPosition="801" endWordPosition="804">tput of the sentence classifiers. We present experimental results on the MUC4 data set, which is a standard benchmark for event extraction research. Our results show that the bootstrapped system, TIERlit, outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Ri</context>
</contexts>
<marker>Soderland, Fisher, Aseltine, Lehnert, 1995</marker>
<rawString>S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995. CRYSTAL: Inducing a conceptual dictionary. In Proc. of the Fourteenth International Joint Conference on Artificial Intelligence, pages 1314– 1319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>M Greenwood</author>
</authors>
<title>A Semantic Approach to IE Pattern Induction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>379--386</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="5507" citStr="Stevenson and Greenwood, 2005" startWordPosition="829" endWordPosition="832">tstrapped system, TIERlit, outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishma</context>
</contexts>
<marker>Stevenson, Greenwood, 2005</marker>
<rawString>M. Stevenson and M. Greenwood. 2005. A Semantic Approach to IE Pattern Induction. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 379–386, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sudo</author>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03).</booktitle>
<contexts>
<context position="5475" citStr="Sudo et al., 2003" startWordPosition="825" endWordPosition="828">s show that the bootstrapped system, TIERlit, outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslenniko</context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2003</marker>
<rawString>K. Sudo, S. Sekine, and R. Grishman. 2003. An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
<author>R Grishman</author>
<author>P Tapanainen</author>
<author>S Huttunen</author>
</authors>
<title>Automatic Acquisition of Domain Knowledge for Information Extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="5456" citStr="Yangarber et al., 2000" startWordPosition="821" endWordPosition="824">ion research. Our results show that the bootstrapped system, TIERlit, outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen. 2000. Automatic Acquisition of Domain Knowledge for Information Extraction. In Proceedings of the Eighteenth International Conference on Computational Linguistics (COLING 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yu</author>
<author>G Guan</author>
<author>M Zhou</author>
</authors>
<title>Resum´e Information Extraction with Cascaded Hybrid Model.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>499--506</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="5631" citStr="Yu et al., 2005" startWordPosition="851" endWordPosition="854">upervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perf</context>
</contexts>
<marker>Yu, Guan, Zhou, 2005</marker>
<rawString>K. Yu, G. Guan, and M. Zhou. 2005. Resum´e Information Extraction with Cascaded Hybrid Model. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 499–506, Ann Arbor, MI, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>