<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996299">
Text Alignment for Real-Time Crowd Captioning
</title>
<author confidence="0.999114">
Iftekhar Naim, Daniel Gildea, Walter Lasecki and Jeffrey P. Bigham
</author>
<affiliation confidence="0.997784">
Department of Computer Science
University of Rochester
</affiliation>
<address confidence="0.289573">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.973155" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956913043478">
The primary way of providing real-time cap-
tioning for deaf and hard of hearing people
is to employ expensive professional stenogra-
phers who can type as fast as natural speak-
ing rates. Recent work has shown that a
feasible alternative is to combine the partial
captions of ordinary typists, each of whom
types part of what they hear. In this paper,
we describe an improved method for combin-
ing partial captions into a final output based
on weighted A* search and multiple sequence
alignment (MSA). In contrast to prior work,
our method allows the tradeoff between accu-
racy and speed to be tuned, and provides for-
mal error bounds. Our method outperforms
the current state-of-the-art on Word Error Rate
(WER) (29.6%), BLEU Score (41.4%), and
F-measure (36.9%). The end goal is for
these captions to be used by people, and so
we also compare how these metrics correlate
with the judgments of 50 study participants,
which may assist others looking to make fur-
ther progress on this problem.
</bodyText>
<sectionHeader confidence="0.998104" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999366733333333">
Real-time captioning provides deaf or hard of hear-
ing people access to speech in mainstream class-
rooms, at public events, and on live television. To
maintain consistency between the captions being
read and other visual cues, the latency between when
a word was said and when it is displayed must be
under five seconds. The most common approach to
real-time captioning is to recruit a trained stenogra-
pher with a special purpose phonetic keyboard, who
transcribes the speech to text within approximately 5
seconds. Unfortunately, professional captionists are
quite expensive ($150 per hour), must be recruited in
blocks of an hour or more, and are difficult to sched-
ule on short notice. Automatic speech recognition
(ASR) (Saraclar et al., 2002) attempts to solve this
</bodyText>
<note confidence="0.4704555">
Merging Incomplete Captions
the quick brown fox jumped over the lazy dog
</note>
<figureCaption confidence="0.801360666666667">
Figure 1: General layout of crowd captioning systems.
Captionists (C1, C2, C3) submit partial captions that are
automatically combined into a high-quality output.
</figureCaption>
<bodyText confidence="0.9998416">
problem by converting speech to text completely au-
tomatically. However, the accuracy of ASR quickly
plummets to below 30% when used on an untrained
speaker’s voice, in a new environment, or in the ab-
sence of a high quality microphone (Wald, 2006b).
An alternative approach is to combine the efforts
of multiple non-expert captionists (anyone who can
type) (Lasecki et al., 2012; Lasecki and Bigham,
2012; Lasecki et al., 2013). In this approach, mul-
tiple non-expert human workers transcribe an audio
stream containing speech in real-time, and their par-
tial input is combined to produce a final transcript
(see Figure 1). This approach has been shown to
dramatically outperform ASR in terms of both accu-
racy and Word Error Rate (WER), even when us-
ing captionists drawn from Amazon’s Mechanical
Turk. Furthermore, recall approached and even ex-
ceeded that of a trained expert stenographer with
seven workers contributing, suggesting that the in-
formation is present to meet the performance of a
stenographer. However, combining these captions
involves real-time alignment of partial captions that
may be incomplete and that often have spelling er-
rors and inconsistent timestamps. In this paper,
we present a more accurate combiner that leverages
</bodyText>
<figure confidence="0.967567222222222">
Final Caption
the brown fox jumped
quick fox lazy dog
fox jumped over the lazy
Combiner
Cl
Cs
C3
201
</figure>
<note confidence="0.4203535">
Proceedings of NAACL-HLT 2013, pages 201–210,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.990409416666667">
Multiple Sequence Alignment (MSA) and Natural
Language Processing to improve performance.
Gauging the quality of captions is not easy. Al-
though word error rate (WER) is commonly used in
speech recognition, it considers accuracy and com-
pleteness, not readability. As a result, a lower WER
does not always result in better understanding (Wang
et al., 2003). We compare WER with two other com-
monly used metrics: BLEU (Papineni et al., 2002)
and F-measure (Melamed et al., 2003), and report
their correlation with that of 50 human evaluators.
The key contributions of this paper are as follows:
</bodyText>
<listItem confidence="0.86918675">
• We have implemented an A∗-search based Mul-
tiple Sequence Alignment algorithm (Lermen
and Reinert, 2000) that can trade-off speed and
accuracy by varying the heuristic weight and
chunk-size parameters. We show that it outper-
forms previous approaches in terms of WER,
BLEU score, and F-measure.
• We propose a beam-search based technique us-
ing the timing information of the captions that
helps to restrict the search space and scales ef-
fectively to align longer sequences efficiently.
• We evaluate the correlation of WER, BLEU,
</listItem>
<bodyText confidence="0.9492868">
and F-measure with 50 human ratings of cap-
tion readability, and found that WER was more
highly correlated than BLEU score (Papineni
et al., 2002), implying it may be a more useful
metric overall when evaluating captions.
</bodyText>
<sectionHeader confidence="0.999768" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997031625">
Most of the previous research on real-time caption-
ing has focused on Automated Speech Recognition
(ASR) (Saraclar et al., 2002; Cooke et al., 2001;
Praˇz´ak et al., 2012). However, experiments show
that ASR systems are not robust enough to be ap-
plied for arbitrary speakers and in noisy environ-
ments (Wald, 2006b; Wald, 2006a; Bain et al., 2005;
Bain et al., 2012; Cooke et al., 2001).
</bodyText>
<subsectionHeader confidence="0.976596">
2.1 Crowd Captioning
</subsectionHeader>
<bodyText confidence="0.999968">
To address these limitations of ASR-based tech-
niques, the Scribe system collects partial captions
from the crowd and then uses a graph-based in-
cremental algorithm to combine them on the fly
(Lasecki et al., 2012). The system incrementally
builds a chain graph, where each node represents a
set of equivalent words entered by the workers and
the link between nodes are adjusted according to the
order of the input words. A greedy search is per-
formed to identify the path with the highest confi-
dence, based on worker input and an n-gram lan-
guage model. The algorithm is designed to be used
online, and hence has high speed and low latency.
However, due to the incremental nature of the algo-
rithm and due to the lack of a principled objective
function, it is not guaranteed to find the globally op-
timal alignment for the captions.
</bodyText>
<subsectionHeader confidence="0.998947">
2.2 Multiple Sequence Alignment
</subsectionHeader>
<bodyText confidence="0.999970411764706">
The problem of aligning and combining multiple
transcripts can be mapped to the well-studied Mul-
tiple Sequence Alignment (MSA) problem (Edgar
and Batzoglou, 2006). MSA is an important prob-
lem in computational biology (Durbin et al., 1998).
The goal is to find an optimal alignment from a
given set of biological sequences. The pairwise
alignment problem can be solved efficiently using
dynamic programming in O(N2) time and space,
where N is the sequence length. The complexity of
the MSA problem grows exponentially as the num-
ber of sequences grows, and has been shown to be
NP-complete (Wang and Jiang, 1994). Therefore,
it is important to apply some heuristic to perform
MSA in a reasonable amount of time.
Most MSA algorithms for biological sequences
follow a progressive alignment strategy that first per-
forms pairwise alignment among the sequences, and
then builds a guide tree based on the pairwise simi-
larity between these sequences (Edgar, 2004; Do et
al., 2005; Thompson et al., 1994). Finally, the input
sequences are aligned according to the order spec-
ified by the guide tree. While not commonly used
for biological sequences, MSA with A∗-style search
has been applied to these problems by Horton (1997)
and Lermen and Reinert (2000).
Lasecki et al. explored MSA in the context of
merging partial captions by using the off-the-shelf
MSA tool MUSCLE (Edgar, 2004), replacing the nu-
cleotide characters by English characters (Lasecki
et al., 2012). The substitution cost for nucleotides
was replaced by the ‘keyboard distance’ between
English characters, learned from the physical lay-
out of a keyboard and based on common spelling
</bodyText>
<page confidence="0.834907">
202
</page>
<bodyText confidence="0.993598833333333">
errors. However, MUSCLE relies on a progressive
alignment strategy and may result in suboptimal so-
lutions. Moreover, it uses characters as atomic sym-
bols instead of words. Our approach operates on a
per-word basis and is able to arrive at a solution that
is within a selectable error-bound of optimal.
</bodyText>
<sectionHeader confidence="0.988426" genericHeader="method">
3 Multiple Sequence Alignment
</sectionHeader>
<bodyText confidence="0.9999148">
We start with an overview of the MSA problem us-
ing standard notations as described by Lermen and
Reinert (2000). Let S1, ... , SK, K &gt; 2, be the K
sequences over an alphabet E, and having length
N1, ... , NK. The special gap symbol is denoted by
‘−’ and does not belong to E. Let A = (aij) be a
K x Nf matrix, where aij E E U 1−1, and the ith
row has exactly (Nf − Ni) gaps and is identical to
Si if we ignore the gaps. Every column of A must
have at least one non-gap symbol. Therefore, the jth
column of A indicates an alignment state for the jth
position, where the state can have one of the 2K − 1
possible combinations. Our goal is to find the op-
timum alignment matrix AOPT that minimizes the
sum of pairs (SOP) cost function:
</bodyText>
<equation confidence="0.857004833333333">
c(A) = � c(Aij) (1)
1&lt;i&lt;j&lt;K
where c(Aij) is the cost of the pairwise alignment
between Si and Sj according to A. Formally,
c(Aij) = ���
l=1 sub(ail, ajl), where sub(ail, ajl)
</equation>
<bodyText confidence="0.9990624">
denotes the cost of substituting ajl for ail. If ail
and ajl are identical, the substitution cost is usu-
ally zero. For the caption alignment task, we treat
each individual word as a symbol in our alphabet
E. The substitution cost for two words is estimated
based on the edit distance between two words. The
exact solution to the SOP optimization problem is
NP-Complete, but many methods solve it approxi-
mately. In this paper, we adapt weighted A* search
for approximately solving the MSA problem.
</bodyText>
<subsectionHeader confidence="0.99837">
3.1 A* Search for MSA
</subsectionHeader>
<bodyText confidence="0.8795215">
The problem of minimizing the SOP cost func-
tion for K sequences is equivalent to estimating the
shortest path between a single source and single sink
node in a K-dimensional lattice. The total num-
ber of nodes in the lattice is (N1 + 1) x (N2 +
Algorithm 1 MSA-A* Algorithm
</bodyText>
<listItem confidence="0.918297909090909">
Require: K input sequences S = {Sl, ... , SK1 having
length Nl, ... , NK, heuristic weight w, beam size b
1: start +— 0K, goal +— [Nl, ... , NK]
2: g(start) +— 0, f(start) +— w x h(start).
3: Q +— {start1
4: while Q =� 0 do
5: n +— EXTRACT-MIN(Q)
6: for all s E {0, 11K − {0K1 do
7: nz +— n + s
8: if nz = goal then
9: Return the alignment matrix for the reconstructed
path from start to nz
10: else if nz E� Beam(b) then
11: continue;
12: else
13: g(nz) +— g(n) + c(n, nz)
14: f(nz) +— g(nz) + w x h(nz)
15: INSERT-ITEM(Q, nz, f(nz))
16: end if
17: end for
18: end while
1) x · · · x (NK + 1), each corresponding to a dis-
</listItem>
<bodyText confidence="0.967119884615385">
tinct position in K sequences. The source node is
[0, ... , 0] and the sink node is [N1, ... , NK]. The
dynamic programming algorithm for estimating the
shortest path from source to sink treats each node
position [n1, ... , nK] as a state and calculates a ma-
trix that has one entry for each node. Assuming the
sequences have roughly same length N, the size of
the dynamic programming matrix is O(NK). At
each vertex, we need to minimize the cost over all
its 2K − 1 predecessor nodes, and, for each such
transition, we need to estimate the SOP objective
function that requires O(K2) operations. Therefore,
the dynamic programming algorithm has time com-
plexity of O(K22KNK) and space complexity of
O(NK), which is infeasible for most practical prob-
lem instances. However, we can efficiently solve it
via heuristic A* search (Lermen and Reinert, 2000).
We use A* search based MSA (shown in Algo-
rithm 1, illustrated in Figure 2) that uses a prior-
ity queue Q to store dynamic programming states
corresponding to node positions in the K dimen-
sional lattice. Let n = [n1, ... , nK] be any node
in the lattice, s be the source, and t be the sink. The
A* search can find the shortest path using a greedy
Best First Search according to an evaluation func-
tion f(n), which is the summation of the cost func-
</bodyText>
<figure confidence="0.973378956521739">
203
� � �
2 -1
k
���
6
�� r OD]\ r ��
nL
���
jumped Br r
n
jumped
� � �
7
Caption] the brownfox jumped the ���� quick fox jumped dog lazy the ���� quick fox jumped lazy dog lazy the ���� brown fox fox BBB �� BBB dog
quick fox lazy dog �� ���� brown fox lazy over the �� ���� brown fox jumped over the �� quick ���� fox jumped BBB �� lazy �� ��
fox jumped over the lazy �� ���� fox jumped �� ���� fox �� ���� ���� ����� over the lazy
���� jumped
Caption2
Caption3
����
�����
��
</figure>
<figureCaption confidence="0.847701">
Figure 2: A* MSA search algorithm. Each branch is one of 2K − 1 possible alignments for the current input. The
</figureCaption>
<figure confidence="0.374222">
�1
branch with minimum sum of the current alignment cost and the expected heuristic value hpair (precomputed).
�
</figure>
<bodyText confidence="0.9998058">
tions g(n) and the heuristic function h(n) for node
n. The cost function g(n) denotes the cost of the
shortest path from the source s to the current node
n. The heuristic function h(n) is the approximate
estimated cost of the shortest path from n to the des-
tination t. At each step of the A* search algorithm,
we extract the node with the smallest f(n) value
from the priority queue Q and expand it by one edge.
The heuristic function h(n) is admissible if it never
overestimates the cost of the cheapest solution from
n to the destination. An admissible heuristic func-
tion guarantees that A* will explore the minimum
number of nodes and will always find the optimal
solution. One commonly used admissible heuristic
function is hpair(n):
</bodyText>
<equation confidence="0.9958435">
�hpair(n) = L(n → t) = c(A*p(σni ,σn j ))
1&lt;i&lt;j&lt;K
</equation>
<bodyText confidence="0.998082029411765">
(2)
where L(n → t) denotes the lower bound on the
cost of the shortest path from n to destination t, A*p
is the optimal pairwise alignment, and σni is the suf-
fix of node n in the i-th sequence. A* search using
the pairwise heuristic function hpair significantly re-
duces the search space and also guarantees finding
the optimal solution. We must be able to estimate
hpair(n) efficiently. It may appear that we need to
estimate the optimal pairwise alignment for all the
pairs of suffix sequences at every node. However,
we can precompute the dynamic programming ma-
trix over all the pair of sequences (Si, Sj) once from
the backward direction, and then reuse these values
at each node. This simple trick significantly speeds
up the computation of hpair(n).
Despite the significant reduction in the search
space, the A* search may still need to explore a
large number of nodes, and may become too slow
for real-time captioning. However, we can further
improve the speed by following the idea of weighted
A* search (Pohl, 1970). We modify the evaluation
function f(n) = g(n)+hpair(n) to a weighted eval-
uation function f′(n) = g(n) + whpair(n), where
w ≥ 1 is a weight parameter. By setting the value
of w to be greater than 1, we increase the relative
weight of the estimated cost to reach the destina-
tion. Therefore, the search prefers the nodes that are
closer to the destination, and thus reaches the goal
faster. Weighted A* search can significantly reduce
the number of nodes to be examined, but it also loses
the optimality guarantee of the admissible heuristic
function. We can trade-off between accuracy and
speed by tuning the weight parameter w.
</bodyText>
<subsectionHeader confidence="0.999309">
3.2 Beam Search using Time-stamps
</subsectionHeader>
<bodyText confidence="0.992914205882353">
The computational cost of the A* search algorithm
grows exponentially with increase in the number of
sequences. However, in order to keep the crowd-
sourced captioning system cost-effective, only a
small number of workers are generally recruited at
a time (typically K ≤ 10). We, therefore, are more
concerned about the growth in computational cost as
the sequence length increases.
In practice, we break down the sequences into
smaller chunks by maintaining a window of a given
time interval, and we apply MSA only to the smaller
chunks of captions entered by the workers during
that time window. As the window size increases,
the accuracy of our MSA based combining system
increases, but so does the computational cost and la-
tency. Therefore, it is important to apply MSA with
a relatively small window size for real-time caption-
ing applications. Another interesting application can
be the offline captioning, for example, captioning an
entire lecture and uploading the captions later.
For the offline captioning problem, we can fo-
cus less on latency and more on accuracy by align-
ing longer sequences. To restrict the search space
from exploding with sequence length (N), we apply
a beam constraint on our search space using the time
stamps of each captioned words. For example, if we
204
so now what i want to do is introduce some of the
what i wanna do is introduce some of the aspects of the class
so now what i want to do is is introduce some of the aspects of the class
so now what i want to do is introduce
so now what i want to do is introduce some of the operational of the class
so i want to introduce some of the operational aspects of the clas
C. so now what i want to do is introduce some of the operational aspects of the class
</bodyText>
<figureCaption confidence="0.9912705">
Figure 3: An example of applying MSA-A* (threshold t„ = 2) to combine 6 partial captions (first 6 lines) by human
workers to obtain the final output caption (C).
</figureCaption>
<bodyText confidence="0.975567722222222">
set the beam size to be 20 seconds, then we ignore
any state in our search space that aligns two words
having more than 20 seconds time lag. Given a fixed
beam size b, we can restrict the number of priority
queue removals by the A* algorithm to O(NbK).
The maximum size of the priority queue is O(NbK).
For each node in the priority queue, for each of the
O(2K) successor states, the objective function and
heuristic estimation requires O(K2) operations and
each priority queue insertion requires O(log(NbK))
i.e. O(log N + K log b) operations. Therefore,
the overall worst case computational complexity is
O (NbK2K(K2 + log N + K log b)). Note that for
fixed beam size b and number of sequences K, the
computational cost grows as O(N log N) with the
increase in N. However, in practice, weighted A*
search explores much smaller number of states com-
pared to this beam-restricted space.
</bodyText>
<subsectionHeader confidence="0.998708">
3.3 Majority Voting after Alignment
</subsectionHeader>
<bodyText confidence="0.999957529411765">
After aligning the captions by multiple workers in a
given chunk, we need to combine them to obtain the
final caption. We do that via majority voting at each
position of the alignment matrix containing a non-
gap symbol. In case of tie, we apply the language
model to choose the most likely word.
Often workers type in nonstandard symbols, ab-
breviations, or misspelled words that do not match
with any other workers’ input and end up as a sin-
gle word aligned to gaps in all the other sequences.
To filter out such spurious words, we apply a vot-
ing threshold (t„) during majority voting and filter
out words having less than t„ votes. Typically we
set t„ = 2 (see the example in Figure 3). While ap-
plying the voting threshold improves the word error
rate and readability, it runs the risk of loosing correct
words if they are covered by only a single worker.
</bodyText>
<subsectionHeader confidence="0.988128">
3.4 Incorporating an N-gram Language Model
</subsectionHeader>
<bodyText confidence="0.99988985">
We also experimented with a version of our system
designed to incorporate the score from an n-gram
language model into the search. For this purpose,
we modified the alignment algorithm to produce a
hypothesized output string as it moves through the
input strings, as opposed to using voting to produce
the final string as a post-processing step. The states
for our dynamic programming are extended to in-
clude not only the current position in each input
string, but also the last two words of the hypothesis
string (i.e. [n1,... , nK, wi−1, wi−2]) for use in com-
puting the next trigram language model probability.
We replace our sum-of-all-pairs objective function
with the sum of the alignment cost of each input with
the hypothesis string, to which we add the log of the
language model probability and a feature for the to-
tal number of words in the hypothesis. Mathemati-
cally, we consider the hypothesis string to be the 0th
row of the alignment matrix, making our objective
function:
</bodyText>
<equation confidence="0.958595833333333">
Nf
c(A) = E c(A0,i) + wlen I(a0,l =� −)
1&lt;i&lt;K l=1
Nf
+ wlm log P(a0,l|a0,l−2, a0,l−1)
l=1
</equation>
<bodyText confidence="0.966900363636364">
where wlm and wlen are negative constants indicat-
ing the relative weights of the language model prob-
ability and the length penalty.
Extending states with two previous words results
in a larger computational complexity. Given K se-
quences of length N each, we can have O(NK) dis-
tinct words. Therefore, the number distinct states
is O(NbK(NK)2) i.e. O(N3K2bK). Each state
can have O(K2K) successors, giving an overall
computational complexity of O(N3K3bK2K(K2 +
log N + log K + K log b)). Alternatively, if the vo-
</bodyText>
<page confidence="0.620755">
205
</page>
<bodyText confidence="0.944489">
cabulary size IV I is smaller than NK, the number of
distinct states is bounded by O(NbK V I2).
</bodyText>
<subsectionHeader confidence="0.601345">
3.5 Evaluation Metric for Speech to Text
Captioning
</subsectionHeader>
<bodyText confidence="0.999947477272727">
Automated evaluation of speech to text captioning is
known to be a challenging task (Wang et al., 2003).
Word Error Rate (WER) is the most commonly used
metric that finds the best pairwise alignment be-
tween the candidate caption and the ground truth
reference sentence. WER is estimated as S+I+D
N ,
where S, I, and D is the number of incorrect word
substitutions, insertions, and deletions required to
match the candidate sentence with reference, and N
is the total number of words in the reference. WER
has several nice properties such as: 1) it is easy
to estimate, and 2) it tries to preserve word order-
ing. However, WER does not account for the overall
‘readability’ of text and thus does not always corre-
late well with human evaluation (Wang et al., 2003;
He et al., 2011).
The widely-used BLEU metric has been shown
to agree well with human judgment for evaluating
translation quality (Papineni et al., 2002). However,
unlike WER, BLEU imposes no explicit constraints
on the word ordering. BLEU has been criticized as
an ‘under-constrained’ measure (Callison-Burch et
al., 2006) for allowing too much variation in word
ordering. Moreover, BLEU does not directly esti-
mate recall, and instead relies on the brevity penalty.
Melamed et al. (2003) suggest that a better approach
is to explicitly measure both precision and recall and
combine them via F-measure.
Our application is similar to automatic speech
recognition in that there is a single correct output,
as opposed to machine translation where many out-
puts can be equally correct. On the other hand, un-
like with ASR, out-of-order output is frequently pro-
duced by our alignment system when there is not
enough overlap between the partial captions to de-
rive the correct ordering for all words. It may be
the case that even such out-of-order output can be
of value to the user, and should receive some sort of
partial credit that is not possible using WER. For
this reason, we wished to systematically compare
BLEU, F-measure, and WER as metrics for our task.
We performed a study to evaluate the agreement
of the three metrics with human judgment. We ran-
</bodyText>
<table confidence="0.99976875">
Metric Spearman Corr. Pearson Corr.
1-WER 0.5258 0.6282
BLEU 0.3137 0.6181
F-measure 0.4389 0.6240
</table>
<tableCaption confidence="0.9949595">
Table 1: The correlation of average human judgment with
three automated metrics: 1-WER, BLEU, and F-measure.
</tableCaption>
<bodyText confidence="0.99997772">
domly extracted one-minute long audio clips from
four MIT OpenCourseWare lectures. Each clip was
transcribed by 7 human workers, and then aligned
and combined using four different systems: the
graph-based system, and three different versions of
our weighted A∗ algorithm with different values of
tuning parameters. Fifty people participated in the
study and were split in two equal sized groups. Each
group was assigned two of the four audio clips,
and each person evaluated all four captions for both
clips. Each participant assigned a score between 1
to 10 to these captions, based on two criteria: 1) the
overall estimated agreement of the captions with the
ground truth text, and 2) the readability and under-
standability of the captions.
Finally, we estimated the correlation coefficients
(both Spearman and Pearson) for the three metrics
discussed above with respect to the average score
assigned by the human participants. The results
are presented in Table 1. Among the three metrics,
WER had the highest agreement with the human par-
ticipants. This indicates that reconstructing the cor-
rect word order is in fact important to the users, and
that, in this aspect, our task has more of the flavor of
speech recognition than of machine translation.
</bodyText>
<sectionHeader confidence="0.997216" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999995583333333">
We experiment with the MSA-A∗ algorithm for cap-
tioning different audio clips, and compare the results
with two existing techniques. Our experimental set
up is similar to the experiments by Lasecki et al.
(2012). Our dataset consists of four 5-minute long
audio clips extracted from lectures available on MIT
OpenCourseWare. The audio clips contain speech
from electrical engineering and chemistry lectures.
Each audio clip is transcribed by ten non-expert hu-
man workers in real-time. We then combine these
inputs using our MSA-A∗ algorithm, and also com-
pare with the existing graph-based system and mul-
</bodyText>
<page confidence="0.845671">
206
</page>
<figureCaption confidence="0.99681575">
Figure 4: Evaluation of different systems on using three
different automated metrics for measuring transcription
quality: 1- Word Error Rate (WER), BLEU, and F-
measure on the four audio clips.
</figureCaption>
<bodyText confidence="0.999095787878788">
tiple sequence alignment using MUSCLE.
As explained earlier, we vary the four key pa-
rameters of the algorithm: the chunk size (c), the
heuristic weight (w), the voting threshold (tv), and
the beam size (b). The heuristic weight and chunk
size parameters help us to trade-off between speed
versus accuracy; the voting threshold tv helps im-
prove precision by pruning words having less than
tv votes, and beam size reduces the search space by
restricting states to be inside a time window/beam.
We use affine gap penalty (Edgar, 2004) with dif-
ferent gap opening and gap extension penalty. We
set gap opening penalty to 0.125 and gap extension
penalty to 0.05. We evaluate the performance using
the three standard metrics: Word Error Rate (WER),
BLEU, and F-measure. The performance in terms of
these metrics using different systems is presented in
Figure 4.
Out of the five systems in Figure 4, the first three
are different versions of our A* search based MSA
algorithm with different parameter settings: 1) A*-
10-t system (c = 10 seconds, tv = 2), 2) A*-15-t (c =
15 seconds, tv = 2), and 3) A*-15 (c = 15 seconds, tv
= 1 i.e. no pruning while voting). For all three sys-
tems, the heuristic weight parameter w is set to 2.5
and beam size b = 20 seconds. The other two sys-
tems are the existing graph-based system and mul-
tiple sequence alignment using MUSCLE. Among
the three A* based algorithms, both A*-15-t and A*-
10-t produce better quality transcripts and outper-
form the existing algorithms. Both systems apply
the voting threshold that improves precision. The
system A*-15 applies no threshold and ends up pro-
ducing many spurious words having poor agreement
among the workers, and hence it scores worse in all
the three metrics. The A*-15-t achieves 57.4% aver-
age accuracy in terms of (1-WER), providing 29.6%
improvement with respect to the graph-based sys-
tem (average accuracy 42.6%), and 35.4% improve-
ment with respect to the MUSCLE-based MSA sys-
tem (average accuracy 41.9%). On the same set of
audio clips, Lasecki et al. (2012) reported 36.6% ac-
curacy using ASR (Dragon Naturally Speaking, ver-
sion 11.5 for Windows), which is worse than all the
crowd-based based systems used in this experiment.
To measure the statistical significance of this im-
provement, we performed a t-test at both the dataset
level (n = 4 clips) and the word level (n = 2862
words). The improvement over the graph-based
model was statistically significant with dataset level
p-value 0.001 and word level p-value smaller than
0.0001. The average time to align each 15 second
chunk with 10 input captions is ∼400 milliseconds.
We have also experimented with a trigram lan-
guage model, trained on the British National Cor-
pus (Burnard, 1995) having ∼122 million words.
The language-model-integrated A* search provided
a negligible 0.21% improvement in WER over the
A*-15-t system on average. The task of combin-
ing captions does not require recognizing words; it
only requires aligning them in the correct order. This
could explain why language model did not improve
accuracy, as it does for speech recognition. Since
the standard MSA-A* algorithm (without language
model) produced comparable accuracy and faster
running time, we used that version in the rest of the
</bodyText>
<figure confidence="0.940954775956283">
(1.0-WER) BLEU Score F-Measure
0.7
0.7 0.62 0.64
0.7
0.58 0.60
0.6
0.6
0.6
.55
0.5
0.53 0.55
0.53
0.47
0.47
0.5
0.5
0.5
0.44
Data Set 1
0.39 0.3
0.4
0.4
0.4
0.36
0.3
0.3
0.3
0.2
0.2
0.2
0.1
0.1
0.1
0.0
0.0
0.0
0.7
0.7
0.7 0.63 0.63
0.60 0.6
0.6
0.6
0.6
.5
0.4
Data Set 2
0.5
0.5
0.5
0.45 0.44
0.41
0.40
0.4
0.4
0.4
0.3
0.3
0.3
0.2
0.2
0.2
0.1
0.1
0.1
0.0
0.0
0.0
0.56 0.5
0.45
0.3
0.19
0.6
0.6 0.53 0.5
0.6
0.52 0.54
0.4
0.5
0.5
0.5
0.45 0.4
0.43 0.44 0.41
0.3
0.43 0.39
0.4
0.4
0.4
Data Set 3
0.35
0.3
0.3
0.3
0.23
0.2
0.2
0.2
0.1
0.1
0.1
0.0
0.0
0.0
0.6
0.6
0.6 0.53 0.56
0.49 0.5
0.5
0.5
0.5
.46
0.38
Data Set 4
0.36
0.4
0.4
0.4
0.35
0.30
0.26
0.3
0.3
0.3
0.2
0.2
0.2
0.1
0.1
0.1
0.0
0.0
0.0
0.43 0.46
0.36
0.29
0.09
MUSCLE
Graph-
based
A*-15
(c=15 sec, no threshold)
A*-10-t
(c=10 sec, threshold=2)
A*-15-t
(c=15 sec, threshold=2)
207
0.58
0.56
0.54
0.52
0.5
0.48
0.46
0.44
0.42
2 3 4 5 6 7 8
0.58
0.56
0.54
w = 1.8
w = 2
w = 2.5
w = 3
w = 4
w = 6
w = 8
0.42
2 3 4 5 6 7 8
1−WER
c = 5
c = 10
c = 15
c = 20
c = 40
c = 60
1−WER
0.52
0.5
0.48
0.46
0.44
Avg Running Time (in Seconds)
(a) Varying heuristic weights for fixed chunk sizes (c)
Avg Running Time (in Seconds)
(b) Varying chunk size for fixed heuristic weight (w)
</figure>
<figureCaption confidence="0.999977">
Figure 5: The trade-off between speed and accuracy for different heuristic weights and chunk size parameters.
</figureCaption>
<bodyText confidence="0.999510333333333">
experiments.
Next, we look at the critical speed versus accuracy
trade-off for different values of the heuristic weight
(w) and the chunk size (c) parameters. Since WER
has been shown to correlate most with human judg-
ment, we show the next results only with respect to
WER. First, we fix the chunk size at different val-
ues, and then vary the heuristic weight parameter:
w = 1.8, 2, 2.5, 3, 4, 6, and 8. The results are
shown in Figure 5(a), where each curve represents
how time and accuracy changed over the range of
values of w and a fixed value of c. We observe that
for smaller values of w, the algorithm is more accu-
rate, but comparatively slower. As w increases, the
search reaches the goal faster, but the quality of the
solution degrades as well. Next, we fix w and vary
chunk size c = 5, 10, 15, 20, 40, 60 second. We re-
peat this experiment for a range of values of w and
the results are shown in Figure 5(b). We can see that
the accuracy improves steeply up to c = 20 seconds,
and does not improve much beyond c = 40 seconds.
For all these benchmarks, we set the beam size (b)
to 20 seconds and voting threshold (t„) to 2.
In our tests, the beam size parameter (b) did not
play a significant role in performance, and setting it
to any reasonably large value (usually ≥ 15 seconds)
resulted in similar accuracy and running time. This
is because the A∗ search with hpai, heuristic already
reduces the the search space significantly, and usu-
ally reaches the goal in a number of steps smaller
than the state space size after the beam restriction.
Finally, we investigate how the accuracy of our
algorithm varies with the number of inputs/workers.
We start with a pool of 10 input captions for one of
the audio clips. We vary the number of input cap-
tions (K) to the MSA-A∗ algorithm from 2 up to 10.
The quality of input captions differs greatly among
the workers. Therefore, for each value of K, we re-
peat the experiment min (20, (K)) times; each time
we randomly select K input captions out of the total
pool of 10. Figure 6 shows that accuracy steeply
increases as the number of inputs increases to 7,
and after that adding more workers does not pro-
vide much improvement in accuracy, but increases
running time.
</bodyText>
<sectionHeader confidence="0.997758" genericHeader="evaluation">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999896866666667">
In this paper, we show that the A∗ search based
MSA algorithm performs better than existing algo-
rithms for combining multiple captions. The exist-
ing graph-based model has low latency, but it usually
can not find a near optimal alignment because of its
incremental alignment. Weighted A∗ search on the
other hand performs joint multiple sequence align-
ment, and is guaranteed to produce a solution hav-
ing cost no more than (1 + E) times the cost of the
optimal solution, given a heuristic weight of (1 + E).
Moreover, A∗ search allows for straightforward in-
tegration of an n-gram language model during the
search.
Another key advantage of the proposed algorithm
is the ease with which we can trade-off between
</bodyText>
<figure confidence="0.972990888888889">
208
0.6
0.5
0.4
0.3
0.2
0.1
0 2 4 6 8 10
Average Running Time (in sec)
</figure>
<figureCaption confidence="0.996474">
Figure 6: Experiments showing how the accuracy of the
</figureCaption>
<bodyText confidence="0.982469978723404">
final caption by MSA-A* algorithm varies with the num-
ber of inputs from 2 to 10.
speed and accuracy. The algorithm can be tailored
to real-time by using a larger heuristic weight. On
the other hand, we can produce better transcripts for
offline tasks by choosing a smaller weight.
It is interesting to compare our results with those
achieved using the MUSCLE MSA tool of Edgar
(2004). One difference is that our system takes a hi-
erarchical approach in that it aligns at the word level,
but also uses string edit distance at the letter level
as a substitution cost for words. Thus, it is able to
take advantage of the fact that individual transcrip-
tions do not generally contain arbitrary fragments of
words. More fundamentally, it is interesting to note
that MUSCLE and most other commonly used MSA
tools for biological sequences make use of a guide
tree formed by a hierarchical clustering of the in-
put sequences. The guide tree produced by the algo-
rithms may or may not match the evolutionary tree
of the organisms whose genomes are being aligned,
but, nevertheless, in the biological application, such
an underlying evolutionary tree generally exists. In
aligning transcriptions, there is no particular reason
to expect individual pairs of transcriptions to be es-
pecially similar to one another, which may make the
guide tree approach less appropriate.
In order to get competitive results, the A∗ search
based algorithm aligns sequences that are at least 7-
10 seconds long. The delay for collecting the cap-
tions within a chunk can introduce latency, however,
each alignment usually takes less than 300 millisec-
onds, allowing us to repeatedly align the stream of
words, even before the window is filled. This pro-
vides less accurate but immediate response to users.
Finally, when we have all the words entered in a
chunk, we perform the final alignment and show the
caption to users for the entire chunk.
After aligning the input sequences, we obtain the
final transcript by majority voting at each alignment
position, which treats each worker equally and does
not take individual quality into account. Recently,
some work has been done for automatically estimat-
ing individual worker’s quality for crowd-based data
labeling tasks (Karger et al., 2011; Liu et al., 2012).
Extending these methods for crowd-based text cap-
tioning could be an interesting future direction.
</bodyText>
<sectionHeader confidence="0.999143" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998127214285714">
In this paper, we have introduced a new A∗ search
based MSA algorithm for aligning partial captions
into a final output stream in real-time. This method
has advantages over prior approaches both in for-
mal guarantees of optimality and the ability to trade
off speed and accuracy. Our experiments on real
captioning data show that it outperforms prior ap-
proaches based on a dependency graph model and a
standard MSA implementation (MUSCLE). An ex-
periment with 50 participants explored whether ex-
iting automatic metrics of quality matched human
evaluations of readability, showing WER did best.
Acknowledgments Funded by NSF awards IIS-
1218209 and IIS-0910611.
</bodyText>
<sectionHeader confidence="0.998254" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999544769230769">
Keith Bain, Sara Basson, A Faisman, and D Kanevsky.
2005. Accessibility, transcription, and access every-
where. IBM Systems Journal, 44(3):589–603.
Keith Bain, Eunice Lund-Lucas, and Janice Stevens.
2012. 22. transcribe your class: Using speech recogni-
tion to improve access for at-risk students. Collected
Essays on Learning and Teaching, 5.
Lou Burnard. 1995. Users Reference Guide British Na-
tional Corpus Version 1.0.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of bleu in ma-
chine translation research. In Proceedings of EACL,
volume 2006, pages 249–256.
</reference>
<figure confidence="0.908143">
Average (1−WER)
0
209
</figure>
<reference confidence="0.999865742574258">
Martin Cooke, Phil Green, Ljubomir Josifovski, and As-
cension Vizinho. 2001. Robust automatic speech
recognition with missing and unreliable acoustic data.
Speech Communication, 34(3):267–285.
Chuong B Do, Mahathi SP Mahabhashyam, Michael
Brudno, and Serafim Batzoglou. 2005. Prob-
cons: Probabilistic consistency-based multiple se-
quence alignment. Genome Research, 15(2):330–340.
Richard Durbin, Sean R Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological sequence analy-
sis: probabilistic models ofproteins and nucleic acids.
Cambridge university press.
Robert C Edgar and Serafim Batzoglou. 2006. Multi-
ple sequence alignment. Current opinion in structural
biology, 16(3):368–373.
Robert C Edgar. 2004. MUSCLE: multiple sequence
alignment with high accuracy and high throughput.
Nucleic Acids Research, 32(5):1792–1797.
Xiaodong He, Li Deng, and Alex Acero. 2011. Why
word error rate is not a good metric for speech rec-
ognizer training for the speech translation task? In
IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), 2011, pages 5632–
5635. IEEE.
Phillip B Horton. 1997. Strings, algorithms, and ma-
chine learning applications for computational biology.
Ph.D. thesis, University of California, Berkeley.
David R Karger, Sewoong Oh, and Devavrat Shah. 2011.
Iterative learning for reliable crowdsourcing systems.
In Proceedings of Advances in Neural Information
Processing Systems (NIPS), volume 24, pages 1953–
1961.
Walter Lasecki and Jeffrey Bigham. 2012. Online qual-
ity control for real-time crowd captioning. In Pro-
ceedings of the 14th international ACM SIGACCESS
conference on Computers and accessibility (ASSETS
2012), pages 143–150. ACM.
Walter Lasecki, Christopher Miller, Adam Sadilek, An-
drew Abumoussa, Donato Borrello, Raja Kushalnagar,
and Jeffrey Bigham. 2012. Real-time captioning by
groups of non-experts. In Proceedings of the 25rd an-
nual ACM symposium on User interface software and
technology, UIST ’12.
Walter Lasecki, Christopher Miller, and Jeffrey Bigham.
2013. Warping time for more effective real-time
crowdsourcing. In Proceedings of the ACM confer-
ence on Human Factors in Computing Systems, CHI
’13, page To Appear, New York, NY, USA. ACM.
Martin Lermen and Knut Reinert. 2000. The prac-
tical use of the A* algorithm for exact multiple se-
quence alignment. Journal of Computational Biology,
7(5):655–671.
Qiang Liu, Jian Peng, and Alex Ihler. 2012. Varia-
tional inference for crowdsourcing. In Proceedings of
Advances in Neural Information Processing Systems
(NIPS), volume 25, pages 701–709.
Dan Melamed, Ryan Green, and Joseph P Turian. 2003.
Precision and recall of machine translation. In Pro-
ceedings HLT-NAACL 2003, volume 2, pages 61–63.
Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting ofAssociation for Computational
Linguistics, pages 311–318. Association for Computa-
tional Linguistics.
Ira Pohl. 1970. Heuristic search viewed as path finding
in a graph. Artificial Intelligence, 1(3):193–204.
Aleˇs Praˇz´ak, Zdenˇek Loose, Jan Trmal, Josef V Psutka,
and Josef Psutka. 2012. Captioning of Live
TV Programs through Speech Recognition and Re-
speaking. In Text, Speech and Dialogue, pages 513–
519. Springer.
Murat Saraclar, Michael Riley, Enrico Bocchieri, and
Vincent Goffin. 2002. Towards automatic closed cap-
tioning: Low latency real time broadcast news tran-
scription. In Proceedings of the International Confer-
ence on Spoken Language Processing (ICSLP), pages
1741–1744.
Julie D Thompson, Desmond G Higgins, and Toby J
Gibson. 1994. Clustal w: improving the sensitivity
of progressive multiple sequence alignment through
sequence weighting, position-specific gap penalties
and weight matrix choice. Nucleic Acids Research,
22(22):4673–4680.
Mike Wald. 2006a. Captioning for deaf and hard of
hearing people by editing automatic speech recogni-
tion in real time. Computers Helping People with Spe-
cial Needs, pages 683–690.
Mike Wald. 2006b. Creating accessible educational mul-
timedia through editing automatic speech recognition
captioning in real time. Interactive Technology and
Smart Education, 3(2):131–141.
Lusheng Wang and Tao Jiang. 1994. On the complexity
of multiple sequence alignment. Journal of Computa-
tional Biology, 1(4):337–348.
Ye-Yi Wang, Alex Acero, and Ciprian Chelba. 2003. Is
word error rate a good indicator for spoken language
understanding accuracy. In IEEE Workshop on Auto-
matic Speech Recognition and Understanding, 2003.
ASRU’03. 2003, pages 577–582. IEEE.
</reference>
<page confidence="0.96132">
210
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959120">
<title confidence="0.999168">Text Alignment for Real-Time Crowd Captioning</title>
<author confidence="0.99924">P Lasecki</author>
<affiliation confidence="0.99981">Department of Computer University of</affiliation>
<address confidence="0.999061">Rochester, NY 14627</address>
<abstract confidence="0.998386166666667">The primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates. Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom types part of what they hear. In this paper, we describe an improved method for combining partial captions into a final output based weighted search and multiple sequence alignment (MSA). In contrast to prior work, our method allows the tradeoff between accuracy and speed to be tuned, and provides formal error bounds. Our method outperforms the current state-of-the-art on Word Error Rate (WER) (29.6%), BLEU Score (41.4%), and F-measure (36.9%). The end goal is for these captions to be used by people, and so we also compare how these metrics correlate with the judgments of 50 study participants, which may assist others looking to make further progress on this problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Keith Bain</author>
<author>Sara Basson</author>
<author>A Faisman</author>
<author>D Kanevsky</author>
</authors>
<title>Accessibility, transcription, and access everywhere.</title>
<date>2005</date>
<journal>IBM Systems Journal,</journal>
<volume>44</volume>
<issue>3</issue>
<contexts>
<context position="5373" citStr="Bain et al., 2005" startWordPosition="864" endWordPosition="867">aluate the correlation of WER, BLEU, and F-measure with 50 human ratings of caption readability, and found that WER was more highly correlated than BLEU score (Papineni et al., 2002), implying it may be a more useful metric overall when evaluating captions. 2 Related Work Most of the previous research on real-time captioning has focused on Automated Speech Recognition (ASR) (Saraclar et al., 2002; Cooke et al., 2001; Praˇz´ak et al., 2012). However, experiments show that ASR systems are not robust enough to be applied for arbitrary speakers and in noisy environments (Wald, 2006b; Wald, 2006a; Bain et al., 2005; Bain et al., 2012; Cooke et al., 2001). 2.1 Crowd Captioning To address these limitations of ASR-based techniques, the Scribe system collects partial captions from the crowd and then uses a graph-based incremental algorithm to combine them on the fly (Lasecki et al., 2012). The system incrementally builds a chain graph, where each node represents a set of equivalent words entered by the workers and the link between nodes are adjusted according to the order of the input words. A greedy search is performed to identify the path with the highest confidence, based on worker input and an n-gram la</context>
</contexts>
<marker>Bain, Basson, Faisman, Kanevsky, 2005</marker>
<rawString>Keith Bain, Sara Basson, A Faisman, and D Kanevsky. 2005. Accessibility, transcription, and access everywhere. IBM Systems Journal, 44(3):589–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Bain</author>
<author>Eunice Lund-Lucas</author>
<author>Janice Stevens</author>
</authors>
<title>22. transcribe your class: Using speech recognition to improve access for at-risk students.</title>
<date>2012</date>
<booktitle>Collected Essays on Learning and Teaching,</booktitle>
<volume>5</volume>
<contexts>
<context position="5392" citStr="Bain et al., 2012" startWordPosition="868" endWordPosition="871">ion of WER, BLEU, and F-measure with 50 human ratings of caption readability, and found that WER was more highly correlated than BLEU score (Papineni et al., 2002), implying it may be a more useful metric overall when evaluating captions. 2 Related Work Most of the previous research on real-time captioning has focused on Automated Speech Recognition (ASR) (Saraclar et al., 2002; Cooke et al., 2001; Praˇz´ak et al., 2012). However, experiments show that ASR systems are not robust enough to be applied for arbitrary speakers and in noisy environments (Wald, 2006b; Wald, 2006a; Bain et al., 2005; Bain et al., 2012; Cooke et al., 2001). 2.1 Crowd Captioning To address these limitations of ASR-based techniques, the Scribe system collects partial captions from the crowd and then uses a graph-based incremental algorithm to combine them on the fly (Lasecki et al., 2012). The system incrementally builds a chain graph, where each node represents a set of equivalent words entered by the workers and the link between nodes are adjusted according to the order of the input words. A greedy search is performed to identify the path with the highest confidence, based on worker input and an n-gram language model. The a</context>
</contexts>
<marker>Bain, Lund-Lucas, Stevens, 2012</marker>
<rawString>Keith Bain, Eunice Lund-Lucas, and Janice Stevens. 2012. 22. transcribe your class: Using speech recognition to improve access for at-risk students. Collected Essays on Learning and Teaching, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<date>1995</date>
<journal>Users Reference Guide British National Corpus Version</journal>
<volume>1</volume>
<contexts>
<context position="27639" citStr="Burnard, 1995" startWordPosition="4769" endWordPosition="4770">on 11.5 for Windows), which is worse than all the crowd-based based systems used in this experiment. To measure the statistical significance of this improvement, we performed a t-test at both the dataset level (n = 4 clips) and the word level (n = 2862 words). The improvement over the graph-based model was statistically significant with dataset level p-value 0.001 and word level p-value smaller than 0.0001. The average time to align each 15 second chunk with 10 input captions is ∼400 milliseconds. We have also experimented with a trigram language model, trained on the British National Corpus (Burnard, 1995) having ∼122 million words. The language-model-integrated A* search provided a negligible 0.21% improvement in WER over the A*-15-t system on average. The task of combining captions does not require recognizing words; it only requires aligning them in the correct order. This could explain why language model did not improve accuracy, as it does for speech recognition. Since the standard MSA-A* algorithm (without language model) produced comparable accuracy and faster running time, we used that version in the rest of the (1.0-WER) BLEU Score F-Measure 0.7 0.7 0.62 0.64 0.7 0.58 0.60 0.6 0.6 0.6 </context>
</contexts>
<marker>Burnard, 1995</marker>
<rawString>Lou Burnard. 1995. Users Reference Guide British National Corpus Version 1.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of bleu in machine translation research.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<volume>volume</volume>
<pages>249--256</pages>
<contexts>
<context position="21610" citStr="Callison-Burch et al., 2006" startWordPosition="3768" endWordPosition="3771">the total number of words in the reference. WER has several nice properties such as: 1) it is easy to estimate, and 2) it tries to preserve word ordering. However, WER does not account for the overall ‘readability’ of text and thus does not always correlate well with human evaluation (Wang et al., 2003; He et al., 2011). The widely-used BLEU metric has been shown to agree well with human judgment for evaluating translation quality (Papineni et al., 2002). However, unlike WER, BLEU imposes no explicit constraints on the word ordering. BLEU has been criticized as an ‘under-constrained’ measure (Callison-Burch et al., 2006) for allowing too much variation in word ordering. Moreover, BLEU does not directly estimate recall, and instead relies on the brevity penalty. Melamed et al. (2003) suggest that a better approach is to explicitly measure both precision and recall and combine them via F-measure. Our application is similar to automatic speech recognition in that there is a single correct output, as opposed to machine translation where many outputs can be equally correct. On the other hand, unlike with ASR, out-of-order output is frequently produced by our alignment system when there is not enough overlap betwee</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the role of bleu in machine translation research. In Proceedings of EACL, volume 2006, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Cooke</author>
<author>Phil Green</author>
<author>Ljubomir Josifovski</author>
<author>Ascension Vizinho</author>
</authors>
<title>Robust automatic speech recognition with missing and unreliable acoustic data.</title>
<date>2001</date>
<journal>Speech Communication,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="5175" citStr="Cooke et al., 2001" startWordPosition="830" endWordPosition="833">. • We propose a beam-search based technique using the timing information of the captions that helps to restrict the search space and scales effectively to align longer sequences efficiently. • We evaluate the correlation of WER, BLEU, and F-measure with 50 human ratings of caption readability, and found that WER was more highly correlated than BLEU score (Papineni et al., 2002), implying it may be a more useful metric overall when evaluating captions. 2 Related Work Most of the previous research on real-time captioning has focused on Automated Speech Recognition (ASR) (Saraclar et al., 2002; Cooke et al., 2001; Praˇz´ak et al., 2012). However, experiments show that ASR systems are not robust enough to be applied for arbitrary speakers and in noisy environments (Wald, 2006b; Wald, 2006a; Bain et al., 2005; Bain et al., 2012; Cooke et al., 2001). 2.1 Crowd Captioning To address these limitations of ASR-based techniques, the Scribe system collects partial captions from the crowd and then uses a graph-based incremental algorithm to combine them on the fly (Lasecki et al., 2012). The system incrementally builds a chain graph, where each node represents a set of equivalent words entered by the workers an</context>
</contexts>
<marker>Cooke, Green, Josifovski, Vizinho, 2001</marker>
<rawString>Martin Cooke, Phil Green, Ljubomir Josifovski, and Ascension Vizinho. 2001. Robust automatic speech recognition with missing and unreliable acoustic data. Speech Communication, 34(3):267–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuong B Do</author>
<author>Mahathi SP Mahabhashyam</author>
<author>Michael Brudno</author>
<author>Serafim Batzoglou</author>
</authors>
<title>Probcons: Probabilistic consistency-based multiple sequence alignment.</title>
<date>2005</date>
<journal>Genome Research,</journal>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="7266" citStr="Do et al., 2005" startWordPosition="1182" endWordPosition="1185">an be solved efficiently using dynamic programming in O(N2) time and space, where N is the sequence length. The complexity of the MSA problem grows exponentially as the number of sequences grows, and has been shown to be NP-complete (Wang and Jiang, 1994). Therefore, it is important to apply some heuristic to perform MSA in a reasonable amount of time. Most MSA algorithms for biological sequences follow a progressive alignment strategy that first performs pairwise alignment among the sequences, and then builds a guide tree based on the pairwise similarity between these sequences (Edgar, 2004; Do et al., 2005; Thompson et al., 1994). Finally, the input sequences are aligned according to the order specified by the guide tree. While not commonly used for biological sequences, MSA with A∗-style search has been applied to these problems by Horton (1997) and Lermen and Reinert (2000). Lasecki et al. explored MSA in the context of merging partial captions by using the off-the-shelf MSA tool MUSCLE (Edgar, 2004), replacing the nucleotide characters by English characters (Lasecki et al., 2012). The substitution cost for nucleotides was replaced by the ‘keyboard distance’ between English characters, learne</context>
</contexts>
<marker>Do, Mahabhashyam, Brudno, Batzoglou, 2005</marker>
<rawString>Chuong B Do, Mahathi SP Mahabhashyam, Michael Brudno, and Serafim Batzoglou. 2005. Probcons: Probabilistic consistency-based multiple sequence alignment. Genome Research, 15(2):330–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Durbin</author>
<author>Sean R Eddy</author>
<author>Anders Krogh</author>
<author>Graeme Mitchison</author>
</authors>
<title>Biological sequence analysis: probabilistic models ofproteins and nucleic acids. Cambridge university press.</title>
<date>1998</date>
<contexts>
<context position="6534" citStr="Durbin et al., 1998" startWordPosition="1061" endWordPosition="1064"> highest confidence, based on worker input and an n-gram language model. The algorithm is designed to be used online, and hence has high speed and low latency. However, due to the incremental nature of the algorithm and due to the lack of a principled objective function, it is not guaranteed to find the globally optimal alignment for the captions. 2.2 Multiple Sequence Alignment The problem of aligning and combining multiple transcripts can be mapped to the well-studied Multiple Sequence Alignment (MSA) problem (Edgar and Batzoglou, 2006). MSA is an important problem in computational biology (Durbin et al., 1998). The goal is to find an optimal alignment from a given set of biological sequences. The pairwise alignment problem can be solved efficiently using dynamic programming in O(N2) time and space, where N is the sequence length. The complexity of the MSA problem grows exponentially as the number of sequences grows, and has been shown to be NP-complete (Wang and Jiang, 1994). Therefore, it is important to apply some heuristic to perform MSA in a reasonable amount of time. Most MSA algorithms for biological sequences follow a progressive alignment strategy that first performs pairwise alignment amon</context>
</contexts>
<marker>Durbin, Eddy, Krogh, Mitchison, 1998</marker>
<rawString>Richard Durbin, Sean R Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological sequence analysis: probabilistic models ofproteins and nucleic acids. Cambridge university press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Edgar</author>
<author>Serafim Batzoglou</author>
</authors>
<title>Multiple sequence alignment. Current opinion in structural biology,</title>
<date>2006</date>
<pages>16--3</pages>
<contexts>
<context position="6458" citStr="Edgar and Batzoglou, 2006" startWordPosition="1048" endWordPosition="1051">der of the input words. A greedy search is performed to identify the path with the highest confidence, based on worker input and an n-gram language model. The algorithm is designed to be used online, and hence has high speed and low latency. However, due to the incremental nature of the algorithm and due to the lack of a principled objective function, it is not guaranteed to find the globally optimal alignment for the captions. 2.2 Multiple Sequence Alignment The problem of aligning and combining multiple transcripts can be mapped to the well-studied Multiple Sequence Alignment (MSA) problem (Edgar and Batzoglou, 2006). MSA is an important problem in computational biology (Durbin et al., 1998). The goal is to find an optimal alignment from a given set of biological sequences. The pairwise alignment problem can be solved efficiently using dynamic programming in O(N2) time and space, where N is the sequence length. The complexity of the MSA problem grows exponentially as the number of sequences grows, and has been shown to be NP-complete (Wang and Jiang, 1994). Therefore, it is important to apply some heuristic to perform MSA in a reasonable amount of time. Most MSA algorithms for biological sequences follow </context>
</contexts>
<marker>Edgar, Batzoglou, 2006</marker>
<rawString>Robert C Edgar and Serafim Batzoglou. 2006. Multiple sequence alignment. Current opinion in structural biology, 16(3):368–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Edgar</author>
</authors>
<title>MUSCLE: multiple sequence alignment with high accuracy and high throughput.</title>
<date>2004</date>
<journal>Nucleic Acids Research,</journal>
<volume>32</volume>
<issue>5</issue>
<contexts>
<context position="7249" citStr="Edgar, 2004" startWordPosition="1180" endWordPosition="1181">ent problem can be solved efficiently using dynamic programming in O(N2) time and space, where N is the sequence length. The complexity of the MSA problem grows exponentially as the number of sequences grows, and has been shown to be NP-complete (Wang and Jiang, 1994). Therefore, it is important to apply some heuristic to perform MSA in a reasonable amount of time. Most MSA algorithms for biological sequences follow a progressive alignment strategy that first performs pairwise alignment among the sequences, and then builds a guide tree based on the pairwise similarity between these sequences (Edgar, 2004; Do et al., 2005; Thompson et al., 1994). Finally, the input sequences are aligned according to the order specified by the guide tree. While not commonly used for biological sequences, MSA with A∗-style search has been applied to these problems by Horton (1997) and Lermen and Reinert (2000). Lasecki et al. explored MSA in the context of merging partial captions by using the off-the-shelf MSA tool MUSCLE (Edgar, 2004), replacing the nucleotide characters by English characters (Lasecki et al., 2012). The substitution cost for nucleotides was replaced by the ‘keyboard distance’ between English c</context>
<context position="25452" citStr="Edgar, 2004" startWordPosition="4395" endWordPosition="4396">n quality: 1- Word Error Rate (WER), BLEU, and Fmeasure on the four audio clips. tiple sequence alignment using MUSCLE. As explained earlier, we vary the four key parameters of the algorithm: the chunk size (c), the heuristic weight (w), the voting threshold (tv), and the beam size (b). The heuristic weight and chunk size parameters help us to trade-off between speed versus accuracy; the voting threshold tv helps improve precision by pruning words having less than tv votes, and beam size reduces the search space by restricting states to be inside a time window/beam. We use affine gap penalty (Edgar, 2004) with different gap opening and gap extension penalty. We set gap opening penalty to 0.125 and gap extension penalty to 0.05. We evaluate the performance using the three standard metrics: Word Error Rate (WER), BLEU, and F-measure. The performance in terms of these metrics using different systems is presented in Figure 4. Out of the five systems in Figure 4, the first three are different versions of our A* search based MSA algorithm with different parameter settings: 1) A*- 10-t system (c = 10 seconds, tv = 2), 2) A*-15-t (c = 15 seconds, tv = 2), and 3) A*-15 (c = 15 seconds, tv = 1 i.e. no p</context>
<context position="32960" citStr="Edgar (2004)" startWordPosition="5781" endWordPosition="5782">search. Another key advantage of the proposed algorithm is the ease with which we can trade-off between 208 0.6 0.5 0.4 0.3 0.2 0.1 0 2 4 6 8 10 Average Running Time (in sec) Figure 6: Experiments showing how the accuracy of the final caption by MSA-A* algorithm varies with the number of inputs from 2 to 10. speed and accuracy. The algorithm can be tailored to real-time by using a larger heuristic weight. On the other hand, we can produce better transcripts for offline tasks by choosing a smaller weight. It is interesting to compare our results with those achieved using the MUSCLE MSA tool of Edgar (2004). One difference is that our system takes a hierarchical approach in that it aligns at the word level, but also uses string edit distance at the letter level as a substitution cost for words. Thus, it is able to take advantage of the fact that individual transcriptions do not generally contain arbitrary fragments of words. More fundamentally, it is interesting to note that MUSCLE and most other commonly used MSA tools for biological sequences make use of a guide tree formed by a hierarchical clustering of the input sequences. The guide tree produced by the algorithms may or may not match the e</context>
</contexts>
<marker>Edgar, 2004</marker>
<rawString>Robert C Edgar. 2004. MUSCLE: multiple sequence alignment with high accuracy and high throughput. Nucleic Acids Research, 32(5):1792–1797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Li Deng</author>
<author>Alex Acero</author>
</authors>
<title>Why word error rate is not a good metric for speech recognizer training for the speech translation task?</title>
<date>2011</date>
<booktitle>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>5632--5635</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="21303" citStr="He et al., 2011" startWordPosition="3723" endWordPosition="3726">at finds the best pairwise alignment between the candidate caption and the ground truth reference sentence. WER is estimated as S+I+D N , where S, I, and D is the number of incorrect word substitutions, insertions, and deletions required to match the candidate sentence with reference, and N is the total number of words in the reference. WER has several nice properties such as: 1) it is easy to estimate, and 2) it tries to preserve word ordering. However, WER does not account for the overall ‘readability’ of text and thus does not always correlate well with human evaluation (Wang et al., 2003; He et al., 2011). The widely-used BLEU metric has been shown to agree well with human judgment for evaluating translation quality (Papineni et al., 2002). However, unlike WER, BLEU imposes no explicit constraints on the word ordering. BLEU has been criticized as an ‘under-constrained’ measure (Callison-Burch et al., 2006) for allowing too much variation in word ordering. Moreover, BLEU does not directly estimate recall, and instead relies on the brevity penalty. Melamed et al. (2003) suggest that a better approach is to explicitly measure both precision and recall and combine them via F-measure. Our applicati</context>
</contexts>
<marker>He, Deng, Acero, 2011</marker>
<rawString>Xiaodong He, Li Deng, and Alex Acero. 2011. Why word error rate is not a good metric for speech recognizer training for the speech translation task? In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2011, pages 5632– 5635. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip B Horton</author>
</authors>
<title>Strings, algorithms, and machine learning applications for computational biology.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="7511" citStr="Horton (1997)" startWordPosition="1224" endWordPosition="1225">g, 1994). Therefore, it is important to apply some heuristic to perform MSA in a reasonable amount of time. Most MSA algorithms for biological sequences follow a progressive alignment strategy that first performs pairwise alignment among the sequences, and then builds a guide tree based on the pairwise similarity between these sequences (Edgar, 2004; Do et al., 2005; Thompson et al., 1994). Finally, the input sequences are aligned according to the order specified by the guide tree. While not commonly used for biological sequences, MSA with A∗-style search has been applied to these problems by Horton (1997) and Lermen and Reinert (2000). Lasecki et al. explored MSA in the context of merging partial captions by using the off-the-shelf MSA tool MUSCLE (Edgar, 2004), replacing the nucleotide characters by English characters (Lasecki et al., 2012). The substitution cost for nucleotides was replaced by the ‘keyboard distance’ between English characters, learned from the physical layout of a keyboard and based on common spelling 202 errors. However, MUSCLE relies on a progressive alignment strategy and may result in suboptimal solutions. Moreover, it uses characters as atomic symbols instead of words.</context>
</contexts>
<marker>Horton, 1997</marker>
<rawString>Phillip B Horton. 1997. Strings, algorithms, and machine learning applications for computational biology. Ph.D. thesis, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Karger</author>
<author>Sewoong Oh</author>
<author>Devavrat Shah</author>
</authors>
<title>Iterative learning for reliable crowdsourcing systems.</title>
<date>2011</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems (NIPS),</booktitle>
<volume>24</volume>
<pages>pages</pages>
<contexts>
<context position="34829" citStr="Karger et al., 2011" startWordPosition="6086" endWordPosition="6089">o repeatedly align the stream of words, even before the window is filled. This provides less accurate but immediate response to users. Finally, when we have all the words entered in a chunk, we perform the final alignment and show the caption to users for the entire chunk. After aligning the input sequences, we obtain the final transcript by majority voting at each alignment position, which treats each worker equally and does not take individual quality into account. Recently, some work has been done for automatically estimating individual worker’s quality for crowd-based data labeling tasks (Karger et al., 2011; Liu et al., 2012). Extending these methods for crowd-based text captioning could be an interesting future direction. 6 Conclusion In this paper, we have introduced a new A∗ search based MSA algorithm for aligning partial captions into a final output stream in real-time. This method has advantages over prior approaches both in formal guarantees of optimality and the ability to trade off speed and accuracy. Our experiments on real captioning data show that it outperforms prior approaches based on a dependency graph model and a standard MSA implementation (MUSCLE). An experiment with 50 partici</context>
</contexts>
<marker>Karger, Oh, Shah, 2011</marker>
<rawString>David R Karger, Sewoong Oh, and Devavrat Shah. 2011. Iterative learning for reliable crowdsourcing systems. In Proceedings of Advances in Neural Information Processing Systems (NIPS), volume 24, pages 1953– 1961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Lasecki</author>
<author>Jeffrey Bigham</author>
</authors>
<title>Online quality control for real-time crowd captioning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 14th international ACM SIGACCESS conference on Computers and accessibility (ASSETS 2012),</booktitle>
<pages>143--150</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2604" citStr="Lasecki and Bigham, 2012" startWordPosition="418" endWordPosition="421">ete Captions the quick brown fox jumped over the lazy dog Figure 1: General layout of crowd captioning systems. Captionists (C1, C2, C3) submit partial captions that are automatically combined into a high-quality output. problem by converting speech to text completely automatically. However, the accuracy of ASR quickly plummets to below 30% when used on an untrained speaker’s voice, in a new environment, or in the absence of a high quality microphone (Wald, 2006b). An alternative approach is to combine the efforts of multiple non-expert captionists (anyone who can type) (Lasecki et al., 2012; Lasecki and Bigham, 2012; Lasecki et al., 2013). In this approach, multiple non-expert human workers transcribe an audio stream containing speech in real-time, and their partial input is combined to produce a final transcript (see Figure 1). This approach has been shown to dramatically outperform ASR in terms of both accuracy and Word Error Rate (WER), even when using captionists drawn from Amazon’s Mechanical Turk. Furthermore, recall approached and even exceeded that of a trained expert stenographer with seven workers contributing, suggesting that the information is present to meet the performance of a stenographer</context>
</contexts>
<marker>Lasecki, Bigham, 2012</marker>
<rawString>Walter Lasecki and Jeffrey Bigham. 2012. Online quality control for real-time crowd captioning. In Proceedings of the 14th international ACM SIGACCESS conference on Computers and accessibility (ASSETS 2012), pages 143–150. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Lasecki</author>
<author>Christopher Miller</author>
<author>Adam Sadilek</author>
<author>Andrew Abumoussa</author>
<author>Donato Borrello</author>
<author>Raja Kushalnagar</author>
<author>Jeffrey Bigham</author>
</authors>
<title>Real-time captioning by groups of non-experts.</title>
<date>2012</date>
<booktitle>In Proceedings of the 25rd annual ACM symposium on User interface software and technology, UIST ’12.</booktitle>
<contexts>
<context position="2578" citStr="Lasecki et al., 2012" startWordPosition="414" endWordPosition="417">e this Merging Incomplete Captions the quick brown fox jumped over the lazy dog Figure 1: General layout of crowd captioning systems. Captionists (C1, C2, C3) submit partial captions that are automatically combined into a high-quality output. problem by converting speech to text completely automatically. However, the accuracy of ASR quickly plummets to below 30% when used on an untrained speaker’s voice, in a new environment, or in the absence of a high quality microphone (Wald, 2006b). An alternative approach is to combine the efforts of multiple non-expert captionists (anyone who can type) (Lasecki et al., 2012; Lasecki and Bigham, 2012; Lasecki et al., 2013). In this approach, multiple non-expert human workers transcribe an audio stream containing speech in real-time, and their partial input is combined to produce a final transcript (see Figure 1). This approach has been shown to dramatically outperform ASR in terms of both accuracy and Word Error Rate (WER), even when using captionists drawn from Amazon’s Mechanical Turk. Furthermore, recall approached and even exceeded that of a trained expert stenographer with seven workers contributing, suggesting that the information is present to meet the per</context>
<context position="5648" citStr="Lasecki et al., 2012" startWordPosition="910" endWordPosition="913">k Most of the previous research on real-time captioning has focused on Automated Speech Recognition (ASR) (Saraclar et al., 2002; Cooke et al., 2001; Praˇz´ak et al., 2012). However, experiments show that ASR systems are not robust enough to be applied for arbitrary speakers and in noisy environments (Wald, 2006b; Wald, 2006a; Bain et al., 2005; Bain et al., 2012; Cooke et al., 2001). 2.1 Crowd Captioning To address these limitations of ASR-based techniques, the Scribe system collects partial captions from the crowd and then uses a graph-based incremental algorithm to combine them on the fly (Lasecki et al., 2012). The system incrementally builds a chain graph, where each node represents a set of equivalent words entered by the workers and the link between nodes are adjusted according to the order of the input words. A greedy search is performed to identify the path with the highest confidence, based on worker input and an n-gram language model. The algorithm is designed to be used online, and hence has high speed and low latency. However, due to the incremental nature of the algorithm and due to the lack of a principled objective function, it is not guaranteed to find the globally optimal alignment fo</context>
<context position="7752" citStr="Lasecki et al., 2012" startWordPosition="1260" endWordPosition="1263">mong the sequences, and then builds a guide tree based on the pairwise similarity between these sequences (Edgar, 2004; Do et al., 2005; Thompson et al., 1994). Finally, the input sequences are aligned according to the order specified by the guide tree. While not commonly used for biological sequences, MSA with A∗-style search has been applied to these problems by Horton (1997) and Lermen and Reinert (2000). Lasecki et al. explored MSA in the context of merging partial captions by using the off-the-shelf MSA tool MUSCLE (Edgar, 2004), replacing the nucleotide characters by English characters (Lasecki et al., 2012). The substitution cost for nucleotides was replaced by the ‘keyboard distance’ between English characters, learned from the physical layout of a keyboard and based on common spelling 202 errors. However, MUSCLE relies on a progressive alignment strategy and may result in suboptimal solutions. Moreover, it uses characters as atomic symbols instead of words. Our approach operates on a per-word basis and is able to arrive at a solution that is within a selectable error-bound of optimal. 3 Multiple Sequence Alignment We start with an overview of the MSA problem using standard notations as describ</context>
<context position="24333" citStr="Lasecki et al. (2012)" startWordPosition="4213" endWordPosition="4216">espect to the average score assigned by the human participants. The results are presented in Table 1. Among the three metrics, WER had the highest agreement with the human participants. This indicates that reconstructing the correct word order is in fact important to the users, and that, in this aspect, our task has more of the flavor of speech recognition than of machine translation. 4 Experimental Results We experiment with the MSA-A∗ algorithm for captioning different audio clips, and compare the results with two existing techniques. Our experimental set up is similar to the experiments by Lasecki et al. (2012). Our dataset consists of four 5-minute long audio clips extracted from lectures available on MIT OpenCourseWare. The audio clips contain speech from electrical engineering and chemistry lectures. Each audio clip is transcribed by ten non-expert human workers in real-time. We then combine these inputs using our MSA-A∗ algorithm, and also compare with the existing graph-based system and mul206 Figure 4: Evaluation of different systems on using three different automated metrics for measuring transcription quality: 1- Word Error Rate (WER), BLEU, and Fmeasure on the four audio clips. tiple sequen</context>
<context position="26957" citStr="Lasecki et al. (2012)" startWordPosition="4654" endWordPosition="4657">10-t produce better quality transcripts and outperform the existing algorithms. Both systems apply the voting threshold that improves precision. The system A*-15 applies no threshold and ends up producing many spurious words having poor agreement among the workers, and hence it scores worse in all the three metrics. The A*-15-t achieves 57.4% average accuracy in terms of (1-WER), providing 29.6% improvement with respect to the graph-based system (average accuracy 42.6%), and 35.4% improvement with respect to the MUSCLE-based MSA system (average accuracy 41.9%). On the same set of audio clips, Lasecki et al. (2012) reported 36.6% accuracy using ASR (Dragon Naturally Speaking, version 11.5 for Windows), which is worse than all the crowd-based based systems used in this experiment. To measure the statistical significance of this improvement, we performed a t-test at both the dataset level (n = 4 clips) and the word level (n = 2862 words). The improvement over the graph-based model was statistically significant with dataset level p-value 0.001 and word level p-value smaller than 0.0001. The average time to align each 15 second chunk with 10 input captions is ∼400 milliseconds. We have also experimented wit</context>
</contexts>
<marker>Lasecki, Miller, Sadilek, Abumoussa, Borrello, Kushalnagar, Bigham, 2012</marker>
<rawString>Walter Lasecki, Christopher Miller, Adam Sadilek, Andrew Abumoussa, Donato Borrello, Raja Kushalnagar, and Jeffrey Bigham. 2012. Real-time captioning by groups of non-experts. In Proceedings of the 25rd annual ACM symposium on User interface software and technology, UIST ’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Lasecki</author>
<author>Christopher Miller</author>
<author>Jeffrey Bigham</author>
</authors>
<title>Warping time for more effective real-time crowdsourcing.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACM conference on Human Factors in Computing Systems, CHI ’13,</booktitle>
<pages>page</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<note>To Appear,</note>
<contexts>
<context position="2627" citStr="Lasecki et al., 2013" startWordPosition="422" endWordPosition="425">wn fox jumped over the lazy dog Figure 1: General layout of crowd captioning systems. Captionists (C1, C2, C3) submit partial captions that are automatically combined into a high-quality output. problem by converting speech to text completely automatically. However, the accuracy of ASR quickly plummets to below 30% when used on an untrained speaker’s voice, in a new environment, or in the absence of a high quality microphone (Wald, 2006b). An alternative approach is to combine the efforts of multiple non-expert captionists (anyone who can type) (Lasecki et al., 2012; Lasecki and Bigham, 2012; Lasecki et al., 2013). In this approach, multiple non-expert human workers transcribe an audio stream containing speech in real-time, and their partial input is combined to produce a final transcript (see Figure 1). This approach has been shown to dramatically outperform ASR in terms of both accuracy and Word Error Rate (WER), even when using captionists drawn from Amazon’s Mechanical Turk. Furthermore, recall approached and even exceeded that of a trained expert stenographer with seven workers contributing, suggesting that the information is present to meet the performance of a stenographer. However, combining th</context>
</contexts>
<marker>Lasecki, Miller, Bigham, 2013</marker>
<rawString>Walter Lasecki, Christopher Miller, and Jeffrey Bigham. 2013. Warping time for more effective real-time crowdsourcing. In Proceedings of the ACM conference on Human Factors in Computing Systems, CHI ’13, page To Appear, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Lermen</author>
<author>Knut Reinert</author>
</authors>
<title>The practical use of the A* algorithm for exact multiple sequence alignment.</title>
<date>2000</date>
<journal>Journal of Computational Biology,</journal>
<volume>7</volume>
<issue>5</issue>
<contexts>
<context position="4369" citStr="Lermen and Reinert, 2000" startWordPosition="697" endWordPosition="700">prove performance. Gauging the quality of captions is not easy. Although word error rate (WER) is commonly used in speech recognition, it considers accuracy and completeness, not readability. As a result, a lower WER does not always result in better understanding (Wang et al., 2003). We compare WER with two other commonly used metrics: BLEU (Papineni et al., 2002) and F-measure (Melamed et al., 2003), and report their correlation with that of 50 human evaluators. The key contributions of this paper are as follows: • We have implemented an A∗-search based Multiple Sequence Alignment algorithm (Lermen and Reinert, 2000) that can trade-off speed and accuracy by varying the heuristic weight and chunk-size parameters. We show that it outperforms previous approaches in terms of WER, BLEU score, and F-measure. • We propose a beam-search based technique using the timing information of the captions that helps to restrict the search space and scales effectively to align longer sequences efficiently. • We evaluate the correlation of WER, BLEU, and F-measure with 50 human ratings of caption readability, and found that WER was more highly correlated than BLEU score (Papineni et al., 2002), implying it may be a more use</context>
<context position="7541" citStr="Lermen and Reinert (2000)" startWordPosition="1227" endWordPosition="1230">e, it is important to apply some heuristic to perform MSA in a reasonable amount of time. Most MSA algorithms for biological sequences follow a progressive alignment strategy that first performs pairwise alignment among the sequences, and then builds a guide tree based on the pairwise similarity between these sequences (Edgar, 2004; Do et al., 2005; Thompson et al., 1994). Finally, the input sequences are aligned according to the order specified by the guide tree. While not commonly used for biological sequences, MSA with A∗-style search has been applied to these problems by Horton (1997) and Lermen and Reinert (2000). Lasecki et al. explored MSA in the context of merging partial captions by using the off-the-shelf MSA tool MUSCLE (Edgar, 2004), replacing the nucleotide characters by English characters (Lasecki et al., 2012). The substitution cost for nucleotides was replaced by the ‘keyboard distance’ between English characters, learned from the physical layout of a keyboard and based on common spelling 202 errors. However, MUSCLE relies on a progressive alignment strategy and may result in suboptimal solutions. Moreover, it uses characters as atomic symbols instead of words. Our approach operates on a pe</context>
<context position="11442" citStr="Lermen and Reinert, 2000" startWordPosition="1958" endWordPosition="1961">nK] as a state and calculates a matrix that has one entry for each node. Assuming the sequences have roughly same length N, the size of the dynamic programming matrix is O(NK). At each vertex, we need to minimize the cost over all its 2K − 1 predecessor nodes, and, for each such transition, we need to estimate the SOP objective function that requires O(K2) operations. Therefore, the dynamic programming algorithm has time complexity of O(K22KNK) and space complexity of O(NK), which is infeasible for most practical problem instances. However, we can efficiently solve it via heuristic A* search (Lermen and Reinert, 2000). We use A* search based MSA (shown in Algorithm 1, illustrated in Figure 2) that uses a priority queue Q to store dynamic programming states corresponding to node positions in the K dimensional lattice. Let n = [n1, ... , nK] be any node in the lattice, s be the source, and t be the sink. The A* search can find the shortest path using a greedy Best First Search according to an evaluation function f(n), which is the summation of the cost func203 � � � 2 -1 k ��� 6 �� r OD]\ r �� nL ��� jumped Br r n jumped � � � 7 Caption] the brownfox jumped the ���� quick fox jumped dog lazy the ���� quick f</context>
</contexts>
<marker>Lermen, Reinert, 2000</marker>
<rawString>Martin Lermen and Knut Reinert. 2000. The practical use of the A* algorithm for exact multiple sequence alignment. Journal of Computational Biology, 7(5):655–671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Liu</author>
<author>Jian Peng</author>
<author>Alex Ihler</author>
</authors>
<title>Variational inference for crowdsourcing.</title>
<date>2012</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems (NIPS),</booktitle>
<volume>25</volume>
<pages>701--709</pages>
<contexts>
<context position="34848" citStr="Liu et al., 2012" startWordPosition="6090" endWordPosition="6093">e stream of words, even before the window is filled. This provides less accurate but immediate response to users. Finally, when we have all the words entered in a chunk, we perform the final alignment and show the caption to users for the entire chunk. After aligning the input sequences, we obtain the final transcript by majority voting at each alignment position, which treats each worker equally and does not take individual quality into account. Recently, some work has been done for automatically estimating individual worker’s quality for crowd-based data labeling tasks (Karger et al., 2011; Liu et al., 2012). Extending these methods for crowd-based text captioning could be an interesting future direction. 6 Conclusion In this paper, we have introduced a new A∗ search based MSA algorithm for aligning partial captions into a final output stream in real-time. This method has advantages over prior approaches both in formal guarantees of optimality and the ability to trade off speed and accuracy. Our experiments on real captioning data show that it outperforms prior approaches based on a dependency graph model and a standard MSA implementation (MUSCLE). An experiment with 50 participants explored whet</context>
</contexts>
<marker>Liu, Peng, Ihler, 2012</marker>
<rawString>Qiang Liu, Jian Peng, and Alex Ihler. 2012. Variational inference for crowdsourcing. In Proceedings of Advances in Neural Information Processing Systems (NIPS), volume 25, pages 701–709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph P Turian</author>
</authors>
<title>Precision and recall of machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings HLT-NAACL 2003,</booktitle>
<volume>2</volume>
<pages>61--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4147" citStr="Melamed et al., 2003" startWordPosition="662" endWordPosition="665">iner Cl Cs C3 201 Proceedings of NAACL-HLT 2013, pages 201–210, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Multiple Sequence Alignment (MSA) and Natural Language Processing to improve performance. Gauging the quality of captions is not easy. Although word error rate (WER) is commonly used in speech recognition, it considers accuracy and completeness, not readability. As a result, a lower WER does not always result in better understanding (Wang et al., 2003). We compare WER with two other commonly used metrics: BLEU (Papineni et al., 2002) and F-measure (Melamed et al., 2003), and report their correlation with that of 50 human evaluators. The key contributions of this paper are as follows: • We have implemented an A∗-search based Multiple Sequence Alignment algorithm (Lermen and Reinert, 2000) that can trade-off speed and accuracy by varying the heuristic weight and chunk-size parameters. We show that it outperforms previous approaches in terms of WER, BLEU score, and F-measure. • We propose a beam-search based technique using the timing information of the captions that helps to restrict the search space and scales effectively to align longer sequences efficiently</context>
<context position="21775" citStr="Melamed et al. (2003)" startWordPosition="3795" endWordPosition="3798"> not account for the overall ‘readability’ of text and thus does not always correlate well with human evaluation (Wang et al., 2003; He et al., 2011). The widely-used BLEU metric has been shown to agree well with human judgment for evaluating translation quality (Papineni et al., 2002). However, unlike WER, BLEU imposes no explicit constraints on the word ordering. BLEU has been criticized as an ‘under-constrained’ measure (Callison-Burch et al., 2006) for allowing too much variation in word ordering. Moreover, BLEU does not directly estimate recall, and instead relies on the brevity penalty. Melamed et al. (2003) suggest that a better approach is to explicitly measure both precision and recall and combine them via F-measure. Our application is similar to automatic speech recognition in that there is a single correct output, as opposed to machine translation where many outputs can be equally correct. On the other hand, unlike with ASR, out-of-order output is frequently produced by our alignment system when there is not enough overlap between the partial captions to derive the correct ordering for all words. It may be the case that even such out-of-order output can be of value to the user, and should re</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>Dan Melamed, Ryan Green, and Joseph P Turian. 2003. Precision and recall of machine translation. In Proceedings HLT-NAACL 2003, volume 2, pages 61–63. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting ofAssociation for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4110" citStr="Papineni et al., 2002" startWordPosition="656" endWordPosition="659">lazy dog fox jumped over the lazy Combiner Cl Cs C3 201 Proceedings of NAACL-HLT 2013, pages 201–210, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Multiple Sequence Alignment (MSA) and Natural Language Processing to improve performance. Gauging the quality of captions is not easy. Although word error rate (WER) is commonly used in speech recognition, it considers accuracy and completeness, not readability. As a result, a lower WER does not always result in better understanding (Wang et al., 2003). We compare WER with two other commonly used metrics: BLEU (Papineni et al., 2002) and F-measure (Melamed et al., 2003), and report their correlation with that of 50 human evaluators. The key contributions of this paper are as follows: • We have implemented an A∗-search based Multiple Sequence Alignment algorithm (Lermen and Reinert, 2000) that can trade-off speed and accuracy by varying the heuristic weight and chunk-size parameters. We show that it outperforms previous approaches in terms of WER, BLEU score, and F-measure. • We propose a beam-search based technique using the timing information of the captions that helps to restrict the search space and scales effectively </context>
<context position="21440" citStr="Papineni et al., 2002" startWordPosition="3744" endWordPosition="3747">+D N , where S, I, and D is the number of incorrect word substitutions, insertions, and deletions required to match the candidate sentence with reference, and N is the total number of words in the reference. WER has several nice properties such as: 1) it is easy to estimate, and 2) it tries to preserve word ordering. However, WER does not account for the overall ‘readability’ of text and thus does not always correlate well with human evaluation (Wang et al., 2003; He et al., 2011). The widely-used BLEU metric has been shown to agree well with human judgment for evaluating translation quality (Papineni et al., 2002). However, unlike WER, BLEU imposes no explicit constraints on the word ordering. BLEU has been criticized as an ‘under-constrained’ measure (Callison-Burch et al., 2006) for allowing too much variation in word ordering. Moreover, BLEU does not directly estimate recall, and instead relies on the brevity penalty. Melamed et al. (2003) suggest that a better approach is to explicitly measure both precision and recall and combine them via F-measure. Our application is similar to automatic speech recognition in that there is a single correct output, as opposed to machine translation where many outp</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting ofAssociation for Computational Linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ira Pohl</author>
</authors>
<title>Heuristic search viewed as path finding in a graph.</title>
<date>1970</date>
<journal>Artificial Intelligence,</journal>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="14400" citStr="Pohl, 1970" startWordPosition="2511" endWordPosition="2512">d to estimate the optimal pairwise alignment for all the pairs of suffix sequences at every node. However, we can precompute the dynamic programming matrix over all the pair of sequences (Si, Sj) once from the backward direction, and then reuse these values at each node. This simple trick significantly speeds up the computation of hpair(n). Despite the significant reduction in the search space, the A* search may still need to explore a large number of nodes, and may become too slow for real-time captioning. However, we can further improve the speed by following the idea of weighted A* search (Pohl, 1970). We modify the evaluation function f(n) = g(n)+hpair(n) to a weighted evaluation function f′(n) = g(n) + whpair(n), where w ≥ 1 is a weight parameter. By setting the value of w to be greater than 1, we increase the relative weight of the estimated cost to reach the destination. Therefore, the search prefers the nodes that are closer to the destination, and thus reaches the goal faster. Weighted A* search can significantly reduce the number of nodes to be examined, but it also loses the optimality guarantee of the admissible heuristic function. We can trade-off between accuracy and speed by tu</context>
</contexts>
<marker>Pohl, 1970</marker>
<rawString>Ira Pohl. 1970. Heuristic search viewed as path finding in a graph. Artificial Intelligence, 1(3):193–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aleˇs Praˇz´ak</author>
<author>Zdenˇek Loose</author>
<author>Jan Trmal</author>
<author>Josef V Psutka</author>
<author>Josef Psutka</author>
</authors>
<title>Captioning of Live TV Programs through Speech Recognition and Respeaking.</title>
<date>2012</date>
<booktitle>In Text, Speech and Dialogue,</booktitle>
<pages>513--519</pages>
<publisher>Springer.</publisher>
<marker>Praˇz´ak, Loose, Trmal, Psutka, Psutka, 2012</marker>
<rawString>Aleˇs Praˇz´ak, Zdenˇek Loose, Jan Trmal, Josef V Psutka, and Josef Psutka. 2012. Captioning of Live TV Programs through Speech Recognition and Respeaking. In Text, Speech and Dialogue, pages 513– 519. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murat Saraclar</author>
<author>Michael Riley</author>
<author>Enrico Bocchieri</author>
<author>Vincent Goffin</author>
</authors>
<title>Towards automatic closed captioning: Low latency real time broadcast news transcription.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>1741--1744</pages>
<contexts>
<context position="1941" citStr="Saraclar et al., 2002" startWordPosition="313" endWordPosition="316">and on live television. To maintain consistency between the captions being read and other visual cues, the latency between when a word was said and when it is displayed must be under five seconds. The most common approach to real-time captioning is to recruit a trained stenographer with a special purpose phonetic keyboard, who transcribes the speech to text within approximately 5 seconds. Unfortunately, professional captionists are quite expensive ($150 per hour), must be recruited in blocks of an hour or more, and are difficult to schedule on short notice. Automatic speech recognition (ASR) (Saraclar et al., 2002) attempts to solve this Merging Incomplete Captions the quick brown fox jumped over the lazy dog Figure 1: General layout of crowd captioning systems. Captionists (C1, C2, C3) submit partial captions that are automatically combined into a high-quality output. problem by converting speech to text completely automatically. However, the accuracy of ASR quickly plummets to below 30% when used on an untrained speaker’s voice, in a new environment, or in the absence of a high quality microphone (Wald, 2006b). An alternative approach is to combine the efforts of multiple non-expert captionists (anyon</context>
<context position="5155" citStr="Saraclar et al., 2002" startWordPosition="826" endWordPosition="829">EU score, and F-measure. • We propose a beam-search based technique using the timing information of the captions that helps to restrict the search space and scales effectively to align longer sequences efficiently. • We evaluate the correlation of WER, BLEU, and F-measure with 50 human ratings of caption readability, and found that WER was more highly correlated than BLEU score (Papineni et al., 2002), implying it may be a more useful metric overall when evaluating captions. 2 Related Work Most of the previous research on real-time captioning has focused on Automated Speech Recognition (ASR) (Saraclar et al., 2002; Cooke et al., 2001; Praˇz´ak et al., 2012). However, experiments show that ASR systems are not robust enough to be applied for arbitrary speakers and in noisy environments (Wald, 2006b; Wald, 2006a; Bain et al., 2005; Bain et al., 2012; Cooke et al., 2001). 2.1 Crowd Captioning To address these limitations of ASR-based techniques, the Scribe system collects partial captions from the crowd and then uses a graph-based incremental algorithm to combine them on the fly (Lasecki et al., 2012). The system incrementally builds a chain graph, where each node represents a set of equivalent words enter</context>
</contexts>
<marker>Saraclar, Riley, Bocchieri, Goffin, 2002</marker>
<rawString>Murat Saraclar, Michael Riley, Enrico Bocchieri, and Vincent Goffin. 2002. Towards automatic closed captioning: Low latency real time broadcast news transcription. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), pages 1741–1744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie D Thompson</author>
<author>Desmond G Higgins</author>
<author>Toby J Gibson</author>
</authors>
<title>Clustal w: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties and weight matrix choice.</title>
<date>1994</date>
<journal>Nucleic Acids Research,</journal>
<volume>22</volume>
<issue>22</issue>
<contexts>
<context position="7290" citStr="Thompson et al., 1994" startWordPosition="1186" endWordPosition="1189">ciently using dynamic programming in O(N2) time and space, where N is the sequence length. The complexity of the MSA problem grows exponentially as the number of sequences grows, and has been shown to be NP-complete (Wang and Jiang, 1994). Therefore, it is important to apply some heuristic to perform MSA in a reasonable amount of time. Most MSA algorithms for biological sequences follow a progressive alignment strategy that first performs pairwise alignment among the sequences, and then builds a guide tree based on the pairwise similarity between these sequences (Edgar, 2004; Do et al., 2005; Thompson et al., 1994). Finally, the input sequences are aligned according to the order specified by the guide tree. While not commonly used for biological sequences, MSA with A∗-style search has been applied to these problems by Horton (1997) and Lermen and Reinert (2000). Lasecki et al. explored MSA in the context of merging partial captions by using the off-the-shelf MSA tool MUSCLE (Edgar, 2004), replacing the nucleotide characters by English characters (Lasecki et al., 2012). The substitution cost for nucleotides was replaced by the ‘keyboard distance’ between English characters, learned from the physical layo</context>
</contexts>
<marker>Thompson, Higgins, Gibson, 1994</marker>
<rawString>Julie D Thompson, Desmond G Higgins, and Toby J Gibson. 1994. Clustal w: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties and weight matrix choice. Nucleic Acids Research, 22(22):4673–4680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Wald</author>
</authors>
<title>Captioning for deaf and hard of hearing people by editing automatic speech recognition in real time. Computers Helping People with Special Needs,</title>
<date>2006</date>
<pages>683--690</pages>
<contexts>
<context position="2446" citStr="Wald, 2006" startWordPosition="396" endWordPosition="397">and are difficult to schedule on short notice. Automatic speech recognition (ASR) (Saraclar et al., 2002) attempts to solve this Merging Incomplete Captions the quick brown fox jumped over the lazy dog Figure 1: General layout of crowd captioning systems. Captionists (C1, C2, C3) submit partial captions that are automatically combined into a high-quality output. problem by converting speech to text completely automatically. However, the accuracy of ASR quickly plummets to below 30% when used on an untrained speaker’s voice, in a new environment, or in the absence of a high quality microphone (Wald, 2006b). An alternative approach is to combine the efforts of multiple non-expert captionists (anyone who can type) (Lasecki et al., 2012; Lasecki and Bigham, 2012; Lasecki et al., 2013). In this approach, multiple non-expert human workers transcribe an audio stream containing speech in real-time, and their partial input is combined to produce a final transcript (see Figure 1). This approach has been shown to dramatically outperform ASR in terms of both accuracy and Word Error Rate (WER), even when using captionists drawn from Amazon’s Mechanical Turk. Furthermore, recall approached and even exceed</context>
<context position="5340" citStr="Wald, 2006" startWordPosition="860" endWordPosition="861">ences efficiently. • We evaluate the correlation of WER, BLEU, and F-measure with 50 human ratings of caption readability, and found that WER was more highly correlated than BLEU score (Papineni et al., 2002), implying it may be a more useful metric overall when evaluating captions. 2 Related Work Most of the previous research on real-time captioning has focused on Automated Speech Recognition (ASR) (Saraclar et al., 2002; Cooke et al., 2001; Praˇz´ak et al., 2012). However, experiments show that ASR systems are not robust enough to be applied for arbitrary speakers and in noisy environments (Wald, 2006b; Wald, 2006a; Bain et al., 2005; Bain et al., 2012; Cooke et al., 2001). 2.1 Crowd Captioning To address these limitations of ASR-based techniques, the Scribe system collects partial captions from the crowd and then uses a graph-based incremental algorithm to combine them on the fly (Lasecki et al., 2012). The system incrementally builds a chain graph, where each node represents a set of equivalent words entered by the workers and the link between nodes are adjusted according to the order of the input words. A greedy search is performed to identify the path with the highest confidence, based</context>
</contexts>
<marker>Wald, 2006</marker>
<rawString>Mike Wald. 2006a. Captioning for deaf and hard of hearing people by editing automatic speech recognition in real time. Computers Helping People with Special Needs, pages 683–690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Wald</author>
</authors>
<title>Creating accessible educational multimedia through editing automatic speech recognition captioning in real time.</title>
<date>2006</date>
<booktitle>Interactive Technology and Smart Education,</booktitle>
<pages>3--2</pages>
<contexts>
<context position="2446" citStr="Wald, 2006" startWordPosition="396" endWordPosition="397">and are difficult to schedule on short notice. Automatic speech recognition (ASR) (Saraclar et al., 2002) attempts to solve this Merging Incomplete Captions the quick brown fox jumped over the lazy dog Figure 1: General layout of crowd captioning systems. Captionists (C1, C2, C3) submit partial captions that are automatically combined into a high-quality output. problem by converting speech to text completely automatically. However, the accuracy of ASR quickly plummets to below 30% when used on an untrained speaker’s voice, in a new environment, or in the absence of a high quality microphone (Wald, 2006b). An alternative approach is to combine the efforts of multiple non-expert captionists (anyone who can type) (Lasecki et al., 2012; Lasecki and Bigham, 2012; Lasecki et al., 2013). In this approach, multiple non-expert human workers transcribe an audio stream containing speech in real-time, and their partial input is combined to produce a final transcript (see Figure 1). This approach has been shown to dramatically outperform ASR in terms of both accuracy and Word Error Rate (WER), even when using captionists drawn from Amazon’s Mechanical Turk. Furthermore, recall approached and even exceed</context>
<context position="5340" citStr="Wald, 2006" startWordPosition="860" endWordPosition="861">ences efficiently. • We evaluate the correlation of WER, BLEU, and F-measure with 50 human ratings of caption readability, and found that WER was more highly correlated than BLEU score (Papineni et al., 2002), implying it may be a more useful metric overall when evaluating captions. 2 Related Work Most of the previous research on real-time captioning has focused on Automated Speech Recognition (ASR) (Saraclar et al., 2002; Cooke et al., 2001; Praˇz´ak et al., 2012). However, experiments show that ASR systems are not robust enough to be applied for arbitrary speakers and in noisy environments (Wald, 2006b; Wald, 2006a; Bain et al., 2005; Bain et al., 2012; Cooke et al., 2001). 2.1 Crowd Captioning To address these limitations of ASR-based techniques, the Scribe system collects partial captions from the crowd and then uses a graph-based incremental algorithm to combine them on the fly (Lasecki et al., 2012). The system incrementally builds a chain graph, where each node represents a set of equivalent words entered by the workers and the link between nodes are adjusted according to the order of the input words. A greedy search is performed to identify the path with the highest confidence, based</context>
</contexts>
<marker>Wald, 2006</marker>
<rawString>Mike Wald. 2006b. Creating accessible educational multimedia through editing automatic speech recognition captioning in real time. Interactive Technology and Smart Education, 3(2):131–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lusheng Wang</author>
<author>Tao Jiang</author>
</authors>
<title>On the complexity of multiple sequence alignment.</title>
<date>1994</date>
<journal>Journal of Computational Biology,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="6906" citStr="Wang and Jiang, 1994" startWordPosition="1124" endWordPosition="1127"> Alignment The problem of aligning and combining multiple transcripts can be mapped to the well-studied Multiple Sequence Alignment (MSA) problem (Edgar and Batzoglou, 2006). MSA is an important problem in computational biology (Durbin et al., 1998). The goal is to find an optimal alignment from a given set of biological sequences. The pairwise alignment problem can be solved efficiently using dynamic programming in O(N2) time and space, where N is the sequence length. The complexity of the MSA problem grows exponentially as the number of sequences grows, and has been shown to be NP-complete (Wang and Jiang, 1994). Therefore, it is important to apply some heuristic to perform MSA in a reasonable amount of time. Most MSA algorithms for biological sequences follow a progressive alignment strategy that first performs pairwise alignment among the sequences, and then builds a guide tree based on the pairwise similarity between these sequences (Edgar, 2004; Do et al., 2005; Thompson et al., 1994). Finally, the input sequences are aligned according to the order specified by the guide tree. While not commonly used for biological sequences, MSA with A∗-style search has been applied to these problems by Horton (</context>
</contexts>
<marker>Wang, Jiang, 1994</marker>
<rawString>Lusheng Wang and Tao Jiang. 1994. On the complexity of multiple sequence alignment. Journal of Computational Biology, 1(4):337–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Alex Acero</author>
<author>Ciprian Chelba</author>
</authors>
<title>Is word error rate a good indicator for spoken language understanding accuracy.</title>
<date>2003</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding, 2003. ASRU’03.</booktitle>
<pages>577--582</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4027" citStr="Wang et al., 2003" startWordPosition="641" endWordPosition="644"> accurate combiner that leverages Final Caption the brown fox jumped quick fox lazy dog fox jumped over the lazy Combiner Cl Cs C3 201 Proceedings of NAACL-HLT 2013, pages 201–210, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Multiple Sequence Alignment (MSA) and Natural Language Processing to improve performance. Gauging the quality of captions is not easy. Although word error rate (WER) is commonly used in speech recognition, it considers accuracy and completeness, not readability. As a result, a lower WER does not always result in better understanding (Wang et al., 2003). We compare WER with two other commonly used metrics: BLEU (Papineni et al., 2002) and F-measure (Melamed et al., 2003), and report their correlation with that of 50 human evaluators. The key contributions of this paper are as follows: • We have implemented an A∗-search based Multiple Sequence Alignment algorithm (Lermen and Reinert, 2000) that can trade-off speed and accuracy by varying the heuristic weight and chunk-size parameters. We show that it outperforms previous approaches in terms of WER, BLEU score, and F-measure. • We propose a beam-search based technique using the timing informat</context>
<context position="20628" citStr="Wang et al., 2003" startWordPosition="3603" endWordPosition="3606">s with two previous words results in a larger computational complexity. Given K sequences of length N each, we can have O(NK) distinct words. Therefore, the number distinct states is O(NbK(NK)2) i.e. O(N3K2bK). Each state can have O(K2K) successors, giving an overall computational complexity of O(N3K3bK2K(K2 + log N + log K + K log b)). Alternatively, if the vo205 cabulary size IV I is smaller than NK, the number of distinct states is bounded by O(NbK V I2). 3.5 Evaluation Metric for Speech to Text Captioning Automated evaluation of speech to text captioning is known to be a challenging task (Wang et al., 2003). Word Error Rate (WER) is the most commonly used metric that finds the best pairwise alignment between the candidate caption and the ground truth reference sentence. WER is estimated as S+I+D N , where S, I, and D is the number of incorrect word substitutions, insertions, and deletions required to match the candidate sentence with reference, and N is the total number of words in the reference. WER has several nice properties such as: 1) it is easy to estimate, and 2) it tries to preserve word ordering. However, WER does not account for the overall ‘readability’ of text and thus does not alway</context>
</contexts>
<marker>Wang, Acero, Chelba, 2003</marker>
<rawString>Ye-Yi Wang, Alex Acero, and Ciprian Chelba. 2003. Is word error rate a good indicator for spoken language understanding accuracy. In IEEE Workshop on Automatic Speech Recognition and Understanding, 2003. ASRU’03. 2003, pages 577–582. IEEE.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>