<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008014">
<title confidence="0.994998">
A Hybrid Approach to Biomedical Named Entity Recognition and
Semantic Role Labeling
</title>
<author confidence="0.997395">
Richard Tzong-Han Tsai
</author>
<affiliation confidence="0.9967265">
Department of Computer Science and Information Engineering
National Taiwan University
</affiliation>
<address confidence="0.82308">
Nankang, Taipei, Taiwan, 115
</address>
<email confidence="0.998421">
thtsai@iis.sinica.edu.tw
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999800051282051">
In this paper, we describe our hybrid ap-
proach to two key NLP technologies:
biomedical named entity recognition
(Bio-NER) and (Bio-SRL). In Bio-NER,
our system successfully integrates linguis-
tic features into the CRF framework. In
addition, we employ web lexicons and
template-based post-processing to further
boost its performance. Through these
broad linguistic features and the nature of
CRF, our system outperforms state-of-
the-art machine-learning-based systems,
especially in the recognition of protein
names (F=78.5%). In Bio-SRL, first, we
construct a proposition bank on top of the
popular biomedical GENIA treebank fol-
lowing the PropBank annotation scheme.
We only annotate the predicate-argument
structures (PAS’s) of thirty frequently
used biomedical verbs (predicates) and
their corresponding arguments. Second,
we use our proposition bank to train a
biomedical SRL system, which uses a
maximum entropy (ME) machine-
learning model. Thirdly, we automatically
generate argument-type templates, which
can be used to improve classification of
biomedical argument roles. Our experi-
mental results show that a newswire Eng-
lish SRL system that achieves an F-score
of 86.29% in the newswire English do-
main can maintain an F-score of 64.64%
when ported to the biomedical domain.
By using our annotated biomedical corpus,
we can increase that F-score by 22.9%.
Adding automatically generated template
features further increases overall F-score
by 0.47% and adjunct (AM) F-score by
1.57%, respectively.
</bodyText>
<sectionHeader confidence="0.943065" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999285">
The volume of biomedical literature available on
the Web has experienced unprecedented growth in
recent years, and demand for efficient methods to
process this material has increased accordingly.
Lately, there has been a surge of interest in mining
biomedical literature. To this end, more and more
information extraction (IE) systems using natural
language processing (NLP) technologies have been
developed for use in the biomedical field. Key
biomedical IE tasks include named entity (NE)
recognition (NER), such as the recognition of pro-
tein and gene names; and relation extraction, such
as the extraction of protein-protein and gene-gene
interactions.
NER identifies named entities from natural lan-
guage texts and classifies them into specific classes
according to a defined ontology or classification.
In general, biomedical NEs do not follow any no-
menclature and may comprise long compound
words and short abbreviations. Some NEs contain
various symbols and other spelling variations. On
average, an NE has five synonyms (Tsai et al.,
2006a), and it may belong to multiple categories
intrinsically. Since biomedical language and vo-
</bodyText>
<page confidence="0.996116">
243
</page>
<note confidence="0.9135755">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 243–246,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999824315789474">
cabulary are highly complex and evolving rapidly,
Bio-NER is a very challenging problem, which
raises a number of difficulties.
The other main focus of Bio-IE is relation ex-
traction. Most systems only extract the relation
targets (e.g., proteins, genes) and the verbs repre-
senting those relations, overlooking the many ad-
verbial and prepositional phrases and words that
describe location, manner, timing, condition, and
extent. However, the information in such phrases
may be important for precise definition and clarifi-
cation of complex biological relations.
This problem can be tackled by using semantic
role labeling (SRL) because it not only recognizes
main roles, such as agents and objects, but also
extracts adjunct roles such as location, manner,
timing, condition, and extent. (Morarescu et al.,
2005) has demonstrated that full-parsing and SRL
can improve the performance of relation extraction,
resulting in an F-score increase of 15% (from 67%
to 82%). This significant result leads us to surmise
that SRL may also have potential for relation ex-
traction in the biomedical domain. Unfortunately,
no SRL system for the biomedical domain exists.
In this paper, we tackle the problems of both
biomedical SRL and NER. Our contributions are (1)
employing web lexicons and template-based post-
processing to boost the performance of Bio-NER;
(2) constructing a proposition bank on top of the
popular biomedical GENIA treebank following the
PropBank annotation scheme and developing a
Biomedical SRL system. We adapt an SRL system
trained the World Street Journal (WSJ) corpus to
the biomedical domain. On adjunct arguments,
especially those relevant to the biomedical domain,
the performance is unsatisfactory. We, therefore,
develop automatically generated templates for
identifying these arguments.
</bodyText>
<sectionHeader confidence="0.94214" genericHeader="method">
2 Biomedical Named Entity Recognition
</sectionHeader>
<bodyText confidence="0.998648">
Our Bio-NER system uses the CRF model
(Lafferty et al., 2001), which has proven its effec-
tiveness in several sequence tagging tasks.
</bodyText>
<subsectionHeader confidence="0.895615">
2.1 Features and Post-Processing
Orthographical Features
</subsectionHeader>
<bodyText confidence="0.999952666666667">
In our experience, ALLCAPS, CAPSMIX, and
INITCAP are more useful than others. The details
are listed in (Tsai et al., 2006a).
</bodyText>
<subsectionHeader confidence="0.551841">
Context Features
</subsectionHeader>
<bodyText confidence="0.999952666666667">
Words preceding or following the target word may
be useful for determining its category. In our ex-
perience, a suitable window size is five.
</bodyText>
<subsectionHeader confidence="0.429181">
Part-of-speech Features
</subsectionHeader>
<bodyText confidence="0.999956875">
Part-of-speech information is quite useful for iden-
tifying NEs. Verbs and prepositions usually indi-
cate an NE’s boundaries, whereas nouns not found
in the dictionary are usually good candidates for
named entities. Our experience indicates that five
is also a suitable window size. The MBT POS tag-
ger is used to provide POS information. We trained
it on GENIA 3.02p and achieved 97.85% accuracy.
</bodyText>
<subsectionHeader confidence="0.898303">
Word Shape Features
</subsectionHeader>
<bodyText confidence="0.999991333333333">
As NEs in the same category may look similar
(e.g., IL-2 and IL-4), we have to find a simple way
to normalize all similar words. According to our
method, capitalized characters are all replaced by
‘A’, digits are all replaced by ‘0’, non-English
characters are replaced by ‘_’ (underscore), and
non-capitalized characters are replaced by ‘a’. To
further normalize these words, we reduce consecu-
tive strings of identical characters to one character.
</bodyText>
<sectionHeader confidence="0.886674" genericHeader="method">
Affix Features
</sectionHeader>
<bodyText confidence="0.999974333333333">
Some affixes can provide good clues for classify-
ing named entities (e.g., “ase”). In our experience,
an acceptable affix length is 3-5 characters.
</bodyText>
<sectionHeader confidence="0.62377" genericHeader="method">
Lexicon Features
</sectionHeader>
<bodyText confidence="0.976009944444444">
Depending on the quality of a given dictionary, our
system uses one of two different lexicon features to
estimate the possibility of a token in a biomedical
named entity. The first feature determines whether
a token is part of a multi-word NE in the dictionary,
while the second feature calculates the minimum
distance between the given token and a dictionary.
In our experience, the first feature is effective for a
dictionary containing high-quality items, for ex-
ample, human-curated protein dictionaries. The
second feature is effective for a dictionary that has
a large number of items that are not very accurate,
for example, web or database lexicons. Details can
be found in (Tsai et al., 2006a).
Post-Processing
We count the number of occurrences of a word x
appearing in the rightmost position of all NEs in
each category. Let the maximum occurrence be n,
</bodyText>
<page confidence="0.990855">
244
</page>
<bodyText confidence="0.999991545454545">
and the corresponding category be c. The total
number of occurrences of x in the rightmost posi-
tion of an NE is T; c/T is the consistency rate of x.
According to our analysis of the training set of the
JNLPBA 2004 data, 75% of words have a consis-
tency rate of over 95%. We record this 75% of
words and their associated categories in a table.
After testing, we crosscheck all the rightmost
words of NEs found by our system against this ta-
ble. If they match, we overwrite the NE categories
with those from the table.
</bodyText>
<subsectionHeader confidence="0.985328">
2.2 Experiments and Summary
</subsectionHeader>
<bodyText confidence="0.999818625">
We perform 10-fold cross validation on the
GENIA V3.02 corpus (Kim et al., 2003) to com-
pare our CRF-based system with other biomedical
NER systems. The experimental results are re-
ported in Table 1. Our system outperforms other
systems in protein names by an F-score of at least
2.6%. For DNA names, our performance is very
close to that of the best system.
</bodyText>
<table confidence="0.584508">
BioNER System Protein DNA
Our System (Tsai et al., 2006a)
HMM (Zhou et al., 2004)
Two Phase SVM (Lee et al., 2003)
</table>
<tableCaption confidence="0.870504">
Table 1. Performance of protein and DNA name
</tableCaption>
<bodyText confidence="0.987038121212121">
recognition on the GENIA V3.02 corpus
We have made every effort to implement a vari-
ety of linguistic features in our system’s CRF
framework. Thanks to these features and the nature
of CRF, our system outperforms state-of-the-art
machine-learning-based systems, especially in the
recognition of protein names.
Our system still has difficulty recognizing long,
complicated NEs and coordinated NEs and distin-
guishing between overlapping NE classes, e.g.,
cell-line and cell-type. This is because biomedical
texts have complicated sentence structures and in-
volve more expert knowledge than texts from the
general newswire domain. Since pure machine
learning approaches cannot model long contextual
phenomena well due to context window size limi-
tations and data sparseness, we believe that tem-
plate-based methods, which exploit long templates
containing different levels of linguistic information,
may be of help. Certain errors, such as incorrect
boundary identification, are more tolerable if the
main purpose is to discover relations between NEs
(Tsai et al., 2006c). We shall exploit more linguis-
tic features, such as composite features and exter-
nal features, in the future. However, machine
leaning approaches suffer from a serious problem
of annotation inconsistency, which confuses ma-
chine learning models and makes evaluation diffi-
cult. In order to reduce human annotation effort
and alleviate the scarcity of available annotated
corpora, we shall learn from web corpora to de-
velop machine learning techniques in different
biomedical domains.
</bodyText>
<sectionHeader confidence="0.993472" genericHeader="method">
3 Biomedical Semantic Role Labeling
</sectionHeader>
<bodyText confidence="0.999922">
In this section, we describe the main steps in build-
ing a biomedical SRL system: (1) create semantic
roles for each biomedical verb; (2) construct a
biomedical corpus, annotated with verbs and their
corresponding semantic roles; (3) build an auto-
matic semantic interpretation model, using the an-
notated text as a training corpus for machine
learning. However, on adjunct arguments, espe-
cially on those highly relevant to the biomedical
domain, such as AM-LOC (location), the perform-
ance is not satisfactory. We therefore develop a
template generation method to create templates
that are used as features for identifying these ar-
gument types.
</bodyText>
<subsectionHeader confidence="0.999355">
3.1 Biomedical Proposition Bank -- BioProp
</subsectionHeader>
<bodyText confidence="0.999902470588235">
Our biomedical proposition bank, BioProp, is
based on the GENIA Treebank (Yuka et al., 2005),
which is a 491-abstract corpus annotated with syn-
tactic structures. The semantic annotation in Bio-
Prop is added to the proper constituents in a
syntactic tree.
Basically, we adopt the definitions in PropBank
(Palmer et al., 2005). For the verbs not in Prop-
Bank, such as “phosphorylate”, we define their
framesets. Since the annotation is time-consuming,
we adopt a semi-automatic approach. We adapt an
SRL system trained on PropBank (Wall Street
Journal corpus) to the biomedical domain. We first
use this SRL system to automatically annotate our
corpus, and then human annotators to double check
the system’s results. Therefore, human effort is
greatly reduced.
</bodyText>
<subsectionHeader confidence="0.994933">
3.2 Biomedical SRL System -- SEROW
</subsectionHeader>
<figure confidence="0.875543666666667">
78.4 66.3
75.8 63.3
70.6 66.4
</figure>
<page confidence="0.996583">
245
</page>
<bodyText confidence="0.999839166666667">
Following (Punyakanok et al., 2004), we formulate
SRL as a constituent-by-constituent (C-by-C) tag-
ging problem. We use BioProp to train our bio-
medical SRL system, SEROW (Tsai et al., 2006b),
which uses a maximum entropy (ME) machine-
learning model. We use the basic features de-
scribed in (Xue &amp; Palmer, 2004). In addition, we
automatically generate templates which can be
used to improve classification of biomedical argu-
ment types. The details of SEROW system are de-
scribed in (Tsai et al., 2005) and (Tsai et al.,
2006b).
</bodyText>
<subsectionHeader confidence="0.995223">
3.3 Experiment and Summary
</subsectionHeader>
<bodyText confidence="0.99984925">
Our experimental results show that a newswire
English SRL system that achieves an F-score of
86.29% can maintain an F-score of 64.64% when
ported to the biomedical domain. By using SE-
ROW, we can increase that F-score by 22.9%.
Adding automatically generated template features
further increases overall F-score by 0.47% and ad-
junct (AM) F-score by 1.57%, respectively.
</bodyText>
<sectionHeader confidence="0.999291" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999927708333333">
NER and SRL are two key topics in biomedical
NLP. For NER, we find broad linguistic features
and integrate them into our CRF framework. Our
system outperforms most machine learning-based
systems, especially in the recognition of protein
names (78.4% of F-score). In the future, templates
that can match long contextual relations and coor-
dinated NEs may be applied to NER post-
processing. Web corpora may also be used to en-
hance unknown NE detection. In Bio-SRL, our
contribution is threefold. First, we construct a bio-
medical proposition bank, BioProp, on top of the
popular biomedical GENIA treebank following the
PropBank annotation scheme. We employ semi-
automatic annotation using an SRL system trained
on PropBank thereby significantly reducing anno-
tation effort. Second, we construct SEROW, which
uses BioProp as its training corpus. Thirdly, we
develop a method to automatically generate tem-
plates that can boost overall performance, espe-
cially on location, manner, adverb, and temporal
arguments. In the future, we will expand BioProp
to include more biomedical verbs and will also
integrate a parser into SEROW.
</bodyText>
<sectionHeader confidence="0.990261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833803921569">
Kim, J.-D., Ohta, T., Teteisi, Y., &amp; Tsujii, J. i. (2003).
Genia corpus - a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(suppl. 1).
Lafferty, J., McCallum, A., &amp; Pereira, F. (2001). Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. Paper presented
at the ICML-01.
Lee, K.-J., Hwang, Y.-S., &amp; Rim, H.-C. (2003). Two
phase biomedical ne recognition based on svms. Pa-
per presented at the ACL-03 Workshop on Natural
Language Processing in Biomedicine.
Morarescu, P., Bejan, C., &amp; Harabagiu, S. (2005). Shal-
low semantics for relation extraction. Paper presented
at the IJCAI-05.
Palmer, M., Gildea, D., &amp; Kingsbury, P. (2005). The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
Punyakanok, V., Roth, D., Yih, W., &amp; Zimak, D. (2004).
Semantic role labeling via integer linear program-
ming inference. Paper presented at the 20th Interna-
tional Conference on Computational Linguistics
(COLING-04).
Tsai, R. T.-H., Chou, W.-C., Wu, S.-H., Sung, T.-Y.,
Hsiang, J., &amp; Hsu, W.-L. (2006a). Integrating lin-
guistic knowledge into a conditional random field
framework to identify biomedical named entities.
Expert Systems with Applications, 30(1), 117-128.
Tsai, R. T.-H., Lin, W.-C. C. Y.-C., Ku, W., Su, Y.-S.,
Sung, T.-Y., &amp; Hsu, W.-L. (2006b). Serow: Adapting
semantic role labeling for biomedical verbs: An ex-
ponential model coupled with adapting semantic role
labeling for biomedical verbs: An exponential model
coupled with automatically generated template fea-
tures. To appear in BioNLP-2006.
Tsai, R. T.-H., Wu, C.-W., Lin, Y.-C., &amp; Hsu, W.-L.
(2005). Exploiting full parsing information to label
semantic roles using an ensemble of me and svm via
integer linear programming. Paper presented at the
CoNLL-2005.
Tsai, R. T.-H., Wu, S.-H., Chou, W.-C., Lin, Y.-C., He,
D., Hsiang, J., et al. (2006c). Various criteria in the
evaluation of biomedical named entity recognition.
BMC Bioinformatics, 7(92).
Xue, N., &amp; Palmer, M. (2004). Calibrating features for
semantic role labeling. Paper presented at the
EMNLP 2004.
Yuka, T., Yakushiji, A., Ohta, T., &amp; Tsujii, J. (2005).
Syntax annotation for the genia corpus.
Zhou, G., Zhang, J., Su, J., Shen, D., &amp; Tan, C. (2004).
Recognizing names in biomedical texts: A machine
learning approach. Bioinformatics, 20, 1178-1190.
</reference>
<page confidence="0.998726">
246
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.599145">
<title confidence="0.999281">A Hybrid Approach to Biomedical Named Entity Recognition and Semantic Role Labeling</title>
<author confidence="0.999958">Richard Tzong-Han Tsai</author>
<affiliation confidence="0.998742">Department of Computer Science and Information National Taiwan</affiliation>
<address confidence="0.98455">Nankang, Taipei, Taiwan,</address>
<email confidence="0.995305">thtsai@iis.sinica.edu.tw</email>
<abstract confidence="0.985669275">In this paper, we describe our hybrid approach to two key NLP technologies: biomedical named entity recognition (Bio-NER) and (Bio-SRL). In Bio-NER, our system successfully integrates linguistic features into the CRF framework. In addition, we employ web lexicons and template-based post-processing to further boost its performance. Through these broad linguistic features and the nature of CRF, our system outperforms state-ofthe-art machine-learning-based systems, especially in the recognition of protein names (F=78.5%). In Bio-SRL, first, we construct a proposition bank on top of the popular biomedical GENIA treebank following the PropBank annotation scheme. We only annotate the predicate-argument structures (PAS’s) of thirty frequently used biomedical verbs (predicates) and their corresponding arguments. Second, we use our proposition bank to train a biomedical SRL system, which uses a maximum entropy (ME) machinelearning model. Thirdly, we automatically generate argument-type templates, which can be used to improve classification of biomedical argument roles. Our experimental results show that a newswire English SRL system that achieves an F-score of 86.29% in the newswire English domain can maintain an F-score of 64.64% when ported to the biomedical domain. By using our annotated biomedical corpus, we can increase that F-score by 22.9%. Adding automatically generated template features further increases overall F-score by 0.47% and adjunct (AM) F-score by 1.57%, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>T Ohta</author>
<author>Y Teteisi</author>
<author>J i Tsujii</author>
</authors>
<title>Genia corpus - a semantically annotated corpus for bio-textmining.</title>
<date>2003</date>
<journal>Bioinformatics,</journal>
<volume>19</volume>
<contexts>
<context position="7970" citStr="Kim et al., 2003" startWordPosition="1224" endWordPosition="1227">e corresponding category be c. The total number of occurrences of x in the rightmost position of an NE is T; c/T is the consistency rate of x. According to our analysis of the training set of the JNLPBA 2004 data, 75% of words have a consistency rate of over 95%. We record this 75% of words and their associated categories in a table. After testing, we crosscheck all the rightmost words of NEs found by our system against this table. If they match, we overwrite the NE categories with those from the table. 2.2 Experiments and Summary We perform 10-fold cross validation on the GENIA V3.02 corpus (Kim et al., 2003) to compare our CRF-based system with other biomedical NER systems. The experimental results are reported in Table 1. Our system outperforms other systems in protein names by an F-score of at least 2.6%. For DNA names, our performance is very close to that of the best system. BioNER System Protein DNA Our System (Tsai et al., 2006a) HMM (Zhou et al., 2004) Two Phase SVM (Lee et al., 2003) Table 1. Performance of protein and DNA name recognition on the GENIA V3.02 corpus We have made every effort to implement a variety of linguistic features in our system’s CRF framework. Thanks to these featur</context>
</contexts>
<marker>Kim, Ohta, Teteisi, Tsujii, 2003</marker>
<rawString>Kim, J.-D., Ohta, T., Teteisi, Y., &amp; Tsujii, J. i. (2003). Genia corpus - a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl. 1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. Paper presented at the ICML-01.</title>
<date>2001</date>
<contexts>
<context position="5000" citStr="Lafferty et al., 2001" startWordPosition="732" endWordPosition="735">plate-based postprocessing to boost the performance of Bio-NER; (2) constructing a proposition bank on top of the popular biomedical GENIA treebank following the PropBank annotation scheme and developing a Biomedical SRL system. We adapt an SRL system trained the World Street Journal (WSJ) corpus to the biomedical domain. On adjunct arguments, especially those relevant to the biomedical domain, the performance is unsatisfactory. We, therefore, develop automatically generated templates for identifying these arguments. 2 Biomedical Named Entity Recognition Our Bio-NER system uses the CRF model (Lafferty et al., 2001), which has proven its effectiveness in several sequence tagging tasks. 2.1 Features and Post-Processing Orthographical Features In our experience, ALLCAPS, CAPSMIX, and INITCAP are more useful than others. The details are listed in (Tsai et al., 2006a). Context Features Words preceding or following the target word may be useful for determining its category. In our experience, a suitable window size is five. Part-of-speech Features Part-of-speech information is quite useful for identifying NEs. Verbs and prepositions usually indicate an NE’s boundaries, whereas nouns not found in the dictionar</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, J., McCallum, A., &amp; Pereira, F. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. Paper presented at the ICML-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-J Lee</author>
<author>Y-S Hwang</author>
<author>H-C Rim</author>
</authors>
<title>Two phase biomedical ne recognition based on svms.</title>
<date>2003</date>
<booktitle>Paper presented at the ACL-03 Workshop on Natural Language Processing in Biomedicine.</booktitle>
<contexts>
<context position="8361" citStr="Lee et al., 2003" startWordPosition="1295" endWordPosition="1298"> NEs found by our system against this table. If they match, we overwrite the NE categories with those from the table. 2.2 Experiments and Summary We perform 10-fold cross validation on the GENIA V3.02 corpus (Kim et al., 2003) to compare our CRF-based system with other biomedical NER systems. The experimental results are reported in Table 1. Our system outperforms other systems in protein names by an F-score of at least 2.6%. For DNA names, our performance is very close to that of the best system. BioNER System Protein DNA Our System (Tsai et al., 2006a) HMM (Zhou et al., 2004) Two Phase SVM (Lee et al., 2003) Table 1. Performance of protein and DNA name recognition on the GENIA V3.02 corpus We have made every effort to implement a variety of linguistic features in our system’s CRF framework. Thanks to these features and the nature of CRF, our system outperforms state-of-the-art machine-learning-based systems, especially in the recognition of protein names. Our system still has difficulty recognizing long, complicated NEs and coordinated NEs and distinguishing between overlapping NE classes, e.g., cell-line and cell-type. This is because biomedical texts have complicated sentence structures and inv</context>
</contexts>
<marker>Lee, Hwang, Rim, 2003</marker>
<rawString>Lee, K.-J., Hwang, Y.-S., &amp; Rim, H.-C. (2003). Two phase biomedical ne recognition based on svms. Paper presented at the ACL-03 Workshop on Natural Language Processing in Biomedicine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Morarescu</author>
<author>C Bejan</author>
<author>S Harabagiu</author>
</authors>
<title>Shallow semantics for relation extraction. Paper presented at the IJCAI-05.</title>
<date>2005</date>
<contexts>
<context position="3907" citStr="Morarescu et al., 2005" startWordPosition="568" endWordPosition="571"> systems only extract the relation targets (e.g., proteins, genes) and the verbs representing those relations, overlooking the many adverbial and prepositional phrases and words that describe location, manner, timing, condition, and extent. However, the information in such phrases may be important for precise definition and clarification of complex biological relations. This problem can be tackled by using semantic role labeling (SRL) because it not only recognizes main roles, such as agents and objects, but also extracts adjunct roles such as location, manner, timing, condition, and extent. (Morarescu et al., 2005) has demonstrated that full-parsing and SRL can improve the performance of relation extraction, resulting in an F-score increase of 15% (from 67% to 82%). This significant result leads us to surmise that SRL may also have potential for relation extraction in the biomedical domain. Unfortunately, no SRL system for the biomedical domain exists. In this paper, we tackle the problems of both biomedical SRL and NER. Our contributions are (1) employing web lexicons and template-based postprocessing to boost the performance of Bio-NER; (2) constructing a proposition bank on top of the popular biomedi</context>
</contexts>
<marker>Morarescu, Bejan, Harabagiu, 2005</marker>
<rawString>Morarescu, P., Bejan, C., &amp; Harabagiu, S. (2005). Shallow semantics for relation extraction. Paper presented at the IJCAI-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="10997" citStr="Palmer et al., 2005" startWordPosition="1695" endWordPosition="1698">ally on those highly relevant to the biomedical domain, such as AM-LOC (location), the performance is not satisfactory. We therefore develop a template generation method to create templates that are used as features for identifying these argument types. 3.1 Biomedical Proposition Bank -- BioProp Our biomedical proposition bank, BioProp, is based on the GENIA Treebank (Yuka et al., 2005), which is a 491-abstract corpus annotated with syntactic structures. The semantic annotation in BioProp is added to the proper constituents in a syntactic tree. Basically, we adopt the definitions in PropBank (Palmer et al., 2005). For the verbs not in PropBank, such as “phosphorylate”, we define their framesets. Since the annotation is time-consuming, we adopt a semi-automatic approach. We adapt an SRL system trained on PropBank (Wall Street Journal corpus) to the biomedical domain. We first use this SRL system to automatically annotate our corpus, and then human annotators to double check the system’s results. Therefore, human effort is greatly reduced. 3.2 Biomedical SRL System -- SEROW 78.4 66.3 75.8 63.3 70.6 66.4 245 Following (Punyakanok et al., 2004), we formulate SRL as a constituent-by-constituent (C-by-C) ta</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., Gildea, D., &amp; Kingsbury, P. (2005). The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<booktitle>Paper presented at the 20th International Conference on Computational Linguistics (COLING-04).</booktitle>
<contexts>
<context position="11535" citStr="Punyakanok et al., 2004" startWordPosition="1780" endWordPosition="1783"> syntactic tree. Basically, we adopt the definitions in PropBank (Palmer et al., 2005). For the verbs not in PropBank, such as “phosphorylate”, we define their framesets. Since the annotation is time-consuming, we adopt a semi-automatic approach. We adapt an SRL system trained on PropBank (Wall Street Journal corpus) to the biomedical domain. We first use this SRL system to automatically annotate our corpus, and then human annotators to double check the system’s results. Therefore, human effort is greatly reduced. 3.2 Biomedical SRL System -- SEROW 78.4 66.3 75.8 63.3 70.6 66.4 245 Following (Punyakanok et al., 2004), we formulate SRL as a constituent-by-constituent (C-by-C) tagging problem. We use BioProp to train our biomedical SRL system, SEROW (Tsai et al., 2006b), which uses a maximum entropy (ME) machinelearning model. We use the basic features described in (Xue &amp; Palmer, 2004). In addition, we automatically generate templates which can be used to improve classification of biomedical argument types. The details of SEROW system are described in (Tsai et al., 2005) and (Tsai et al., 2006b). 3.3 Experiment and Summary Our experimental results show that a newswire English SRL system that achieves an F-s</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>Punyakanok, V., Roth, D., Yih, W., &amp; Zimak, D. (2004). Semantic role labeling via integer linear programming inference. Paper presented at the 20th International Conference on Computational Linguistics (COLING-04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T-H Tsai</author>
<author>W-C Chou</author>
<author>S-H Wu</author>
<author>T-Y Sung</author>
<author>J Hsiang</author>
<author>W-L Hsu</author>
</authors>
<title>Integrating linguistic knowledge into a conditional random field framework to identify biomedical named entities. Expert Systems with Applications,</title>
<date>2006</date>
<volume>30</volume>
<issue>1</issue>
<pages>117--128</pages>
<contexts>
<context position="2818" citStr="Tsai et al., 2006" startWordPosition="405" endWordPosition="408">dical field. Key biomedical IE tasks include named entity (NE) recognition (NER), such as the recognition of protein and gene names; and relation extraction, such as the extraction of protein-protein and gene-gene interactions. NER identifies named entities from natural language texts and classifies them into specific classes according to a defined ontology or classification. In general, biomedical NEs do not follow any nomenclature and may comprise long compound words and short abbreviations. Some NEs contain various symbols and other spelling variations. On average, an NE has five synonyms (Tsai et al., 2006a), and it may belong to multiple categories intrinsically. Since biomedical language and vo243 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 243–246, New York, June 2006. c�2006 Association for Computational Linguistics cabulary are highly complex and evolving rapidly, Bio-NER is a very challenging problem, which raises a number of difficulties. The other main focus of Bio-IE is relation extraction. Most systems only extract the relation targets (e.g., proteins, genes) and the verbs representing those relations, overlooking the many ad</context>
<context position="5251" citStr="Tsai et al., 2006" startWordPosition="770" endWordPosition="773">ned the World Street Journal (WSJ) corpus to the biomedical domain. On adjunct arguments, especially those relevant to the biomedical domain, the performance is unsatisfactory. We, therefore, develop automatically generated templates for identifying these arguments. 2 Biomedical Named Entity Recognition Our Bio-NER system uses the CRF model (Lafferty et al., 2001), which has proven its effectiveness in several sequence tagging tasks. 2.1 Features and Post-Processing Orthographical Features In our experience, ALLCAPS, CAPSMIX, and INITCAP are more useful than others. The details are listed in (Tsai et al., 2006a). Context Features Words preceding or following the target word may be useful for determining its category. In our experience, a suitable window size is five. Part-of-speech Features Part-of-speech information is quite useful for identifying NEs. Verbs and prepositions usually indicate an NE’s boundaries, whereas nouns not found in the dictionary are usually good candidates for named entities. Our experience indicates that five is also a suitable window size. The MBT POS tagger is used to provide POS information. We trained it on GENIA 3.02p and achieved 97.85% accuracy. Word Shape Features </context>
<context position="7178" citStr="Tsai et al., 2006" startWordPosition="1079" endWordPosition="1082">nt lexicon features to estimate the possibility of a token in a biomedical named entity. The first feature determines whether a token is part of a multi-word NE in the dictionary, while the second feature calculates the minimum distance between the given token and a dictionary. In our experience, the first feature is effective for a dictionary containing high-quality items, for example, human-curated protein dictionaries. The second feature is effective for a dictionary that has a large number of items that are not very accurate, for example, web or database lexicons. Details can be found in (Tsai et al., 2006a). Post-Processing We count the number of occurrences of a word x appearing in the rightmost position of all NEs in each category. Let the maximum occurrence be n, 244 and the corresponding category be c. The total number of occurrences of x in the rightmost position of an NE is T; c/T is the consistency rate of x. According to our analysis of the training set of the JNLPBA 2004 data, 75% of words have a consistency rate of over 95%. We record this 75% of words and their associated categories in a table. After testing, we crosscheck all the rightmost words of NEs found by our system against t</context>
<context position="9469" citStr="Tsai et al., 2006" startWordPosition="1459" endWordPosition="1462"> e.g., cell-line and cell-type. This is because biomedical texts have complicated sentence structures and involve more expert knowledge than texts from the general newswire domain. Since pure machine learning approaches cannot model long contextual phenomena well due to context window size limitations and data sparseness, we believe that template-based methods, which exploit long templates containing different levels of linguistic information, may be of help. Certain errors, such as incorrect boundary identification, are more tolerable if the main purpose is to discover relations between NEs (Tsai et al., 2006c). We shall exploit more linguistic features, such as composite features and external features, in the future. However, machine leaning approaches suffer from a serious problem of annotation inconsistency, which confuses machine learning models and makes evaluation difficult. In order to reduce human annotation effort and alleviate the scarcity of available annotated corpora, we shall learn from web corpora to develop machine learning techniques in different biomedical domains. 3 Biomedical Semantic Role Labeling In this section, we describe the main steps in building a biomedical SRL system:</context>
<context position="11687" citStr="Tsai et al., 2006" startWordPosition="1805" endWordPosition="1808"> framesets. Since the annotation is time-consuming, we adopt a semi-automatic approach. We adapt an SRL system trained on PropBank (Wall Street Journal corpus) to the biomedical domain. We first use this SRL system to automatically annotate our corpus, and then human annotators to double check the system’s results. Therefore, human effort is greatly reduced. 3.2 Biomedical SRL System -- SEROW 78.4 66.3 75.8 63.3 70.6 66.4 245 Following (Punyakanok et al., 2004), we formulate SRL as a constituent-by-constituent (C-by-C) tagging problem. We use BioProp to train our biomedical SRL system, SEROW (Tsai et al., 2006b), which uses a maximum entropy (ME) machinelearning model. We use the basic features described in (Xue &amp; Palmer, 2004). In addition, we automatically generate templates which can be used to improve classification of biomedical argument types. The details of SEROW system are described in (Tsai et al., 2005) and (Tsai et al., 2006b). 3.3 Experiment and Summary Our experimental results show that a newswire English SRL system that achieves an F-score of 86.29% can maintain an F-score of 64.64% when ported to the biomedical domain. By using SEROW, we can increase that F-score by 22.9%. Adding aut</context>
</contexts>
<marker>Tsai, Chou, Wu, Sung, Hsiang, Hsu, 2006</marker>
<rawString>Tsai, R. T.-H., Chou, W.-C., Wu, S.-H., Sung, T.-Y., Hsiang, J., &amp; Hsu, W.-L. (2006a). Integrating linguistic knowledge into a conditional random field framework to identify biomedical named entities. Expert Systems with Applications, 30(1), 117-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T-H Tsai</author>
<author>W-C C Y-C Lin</author>
<author>W Ku</author>
<author>Y-S Su</author>
<author>T-Y Sung</author>
<author>W-L Hsu</author>
</authors>
<title>Serow: Adapting semantic role labeling for biomedical verbs: An exponential model coupled with adapting semantic role labeling for biomedical verbs: An exponential model coupled with automatically generated template features.</title>
<date>2006</date>
<note>To appear in BioNLP-2006.</note>
<contexts>
<context position="2818" citStr="Tsai et al., 2006" startWordPosition="405" endWordPosition="408">dical field. Key biomedical IE tasks include named entity (NE) recognition (NER), such as the recognition of protein and gene names; and relation extraction, such as the extraction of protein-protein and gene-gene interactions. NER identifies named entities from natural language texts and classifies them into specific classes according to a defined ontology or classification. In general, biomedical NEs do not follow any nomenclature and may comprise long compound words and short abbreviations. Some NEs contain various symbols and other spelling variations. On average, an NE has five synonyms (Tsai et al., 2006a), and it may belong to multiple categories intrinsically. Since biomedical language and vo243 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 243–246, New York, June 2006. c�2006 Association for Computational Linguistics cabulary are highly complex and evolving rapidly, Bio-NER is a very challenging problem, which raises a number of difficulties. The other main focus of Bio-IE is relation extraction. Most systems only extract the relation targets (e.g., proteins, genes) and the verbs representing those relations, overlooking the many ad</context>
<context position="5251" citStr="Tsai et al., 2006" startWordPosition="770" endWordPosition="773">ned the World Street Journal (WSJ) corpus to the biomedical domain. On adjunct arguments, especially those relevant to the biomedical domain, the performance is unsatisfactory. We, therefore, develop automatically generated templates for identifying these arguments. 2 Biomedical Named Entity Recognition Our Bio-NER system uses the CRF model (Lafferty et al., 2001), which has proven its effectiveness in several sequence tagging tasks. 2.1 Features and Post-Processing Orthographical Features In our experience, ALLCAPS, CAPSMIX, and INITCAP are more useful than others. The details are listed in (Tsai et al., 2006a). Context Features Words preceding or following the target word may be useful for determining its category. In our experience, a suitable window size is five. Part-of-speech Features Part-of-speech information is quite useful for identifying NEs. Verbs and prepositions usually indicate an NE’s boundaries, whereas nouns not found in the dictionary are usually good candidates for named entities. Our experience indicates that five is also a suitable window size. The MBT POS tagger is used to provide POS information. We trained it on GENIA 3.02p and achieved 97.85% accuracy. Word Shape Features </context>
<context position="7178" citStr="Tsai et al., 2006" startWordPosition="1079" endWordPosition="1082">nt lexicon features to estimate the possibility of a token in a biomedical named entity. The first feature determines whether a token is part of a multi-word NE in the dictionary, while the second feature calculates the minimum distance between the given token and a dictionary. In our experience, the first feature is effective for a dictionary containing high-quality items, for example, human-curated protein dictionaries. The second feature is effective for a dictionary that has a large number of items that are not very accurate, for example, web or database lexicons. Details can be found in (Tsai et al., 2006a). Post-Processing We count the number of occurrences of a word x appearing in the rightmost position of all NEs in each category. Let the maximum occurrence be n, 244 and the corresponding category be c. The total number of occurrences of x in the rightmost position of an NE is T; c/T is the consistency rate of x. According to our analysis of the training set of the JNLPBA 2004 data, 75% of words have a consistency rate of over 95%. We record this 75% of words and their associated categories in a table. After testing, we crosscheck all the rightmost words of NEs found by our system against t</context>
<context position="9469" citStr="Tsai et al., 2006" startWordPosition="1459" endWordPosition="1462"> e.g., cell-line and cell-type. This is because biomedical texts have complicated sentence structures and involve more expert knowledge than texts from the general newswire domain. Since pure machine learning approaches cannot model long contextual phenomena well due to context window size limitations and data sparseness, we believe that template-based methods, which exploit long templates containing different levels of linguistic information, may be of help. Certain errors, such as incorrect boundary identification, are more tolerable if the main purpose is to discover relations between NEs (Tsai et al., 2006c). We shall exploit more linguistic features, such as composite features and external features, in the future. However, machine leaning approaches suffer from a serious problem of annotation inconsistency, which confuses machine learning models and makes evaluation difficult. In order to reduce human annotation effort and alleviate the scarcity of available annotated corpora, we shall learn from web corpora to develop machine learning techniques in different biomedical domains. 3 Biomedical Semantic Role Labeling In this section, we describe the main steps in building a biomedical SRL system:</context>
<context position="11687" citStr="Tsai et al., 2006" startWordPosition="1805" endWordPosition="1808"> framesets. Since the annotation is time-consuming, we adopt a semi-automatic approach. We adapt an SRL system trained on PropBank (Wall Street Journal corpus) to the biomedical domain. We first use this SRL system to automatically annotate our corpus, and then human annotators to double check the system’s results. Therefore, human effort is greatly reduced. 3.2 Biomedical SRL System -- SEROW 78.4 66.3 75.8 63.3 70.6 66.4 245 Following (Punyakanok et al., 2004), we formulate SRL as a constituent-by-constituent (C-by-C) tagging problem. We use BioProp to train our biomedical SRL system, SEROW (Tsai et al., 2006b), which uses a maximum entropy (ME) machinelearning model. We use the basic features described in (Xue &amp; Palmer, 2004). In addition, we automatically generate templates which can be used to improve classification of biomedical argument types. The details of SEROW system are described in (Tsai et al., 2005) and (Tsai et al., 2006b). 3.3 Experiment and Summary Our experimental results show that a newswire English SRL system that achieves an F-score of 86.29% can maintain an F-score of 64.64% when ported to the biomedical domain. By using SEROW, we can increase that F-score by 22.9%. Adding aut</context>
</contexts>
<marker>Tsai, Lin, Ku, Su, Sung, Hsu, 2006</marker>
<rawString>Tsai, R. T.-H., Lin, W.-C. C. Y.-C., Ku, W., Su, Y.-S., Sung, T.-Y., &amp; Hsu, W.-L. (2006b). Serow: Adapting semantic role labeling for biomedical verbs: An exponential model coupled with adapting semantic role labeling for biomedical verbs: An exponential model coupled with automatically generated template features. To appear in BioNLP-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T-H Tsai</author>
<author>C-W Wu</author>
<author>Y-C Lin</author>
<author>W-L Hsu</author>
</authors>
<title>Exploiting full parsing information to label semantic roles using an ensemble of me and svm via integer linear programming. Paper presented at the CoNLL-2005.</title>
<date>2005</date>
<contexts>
<context position="11996" citStr="Tsai et al., 2005" startWordPosition="1857" endWordPosition="1860">results. Therefore, human effort is greatly reduced. 3.2 Biomedical SRL System -- SEROW 78.4 66.3 75.8 63.3 70.6 66.4 245 Following (Punyakanok et al., 2004), we formulate SRL as a constituent-by-constituent (C-by-C) tagging problem. We use BioProp to train our biomedical SRL system, SEROW (Tsai et al., 2006b), which uses a maximum entropy (ME) machinelearning model. We use the basic features described in (Xue &amp; Palmer, 2004). In addition, we automatically generate templates which can be used to improve classification of biomedical argument types. The details of SEROW system are described in (Tsai et al., 2005) and (Tsai et al., 2006b). 3.3 Experiment and Summary Our experimental results show that a newswire English SRL system that achieves an F-score of 86.29% can maintain an F-score of 64.64% when ported to the biomedical domain. By using SEROW, we can increase that F-score by 22.9%. Adding automatically generated template features further increases overall F-score by 0.47% and adjunct (AM) F-score by 1.57%, respectively. 4 Conclusion NER and SRL are two key topics in biomedical NLP. For NER, we find broad linguistic features and integrate them into our CRF framework. Our system outperforms most m</context>
</contexts>
<marker>Tsai, Wu, Lin, Hsu, 2005</marker>
<rawString>Tsai, R. T.-H., Wu, C.-W., Lin, Y.-C., &amp; Hsu, W.-L. (2005). Exploiting full parsing information to label semantic roles using an ensemble of me and svm via integer linear programming. Paper presented at the CoNLL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T-H Tsai</author>
<author>S-H Wu</author>
<author>W-C Chou</author>
<author>Y-C Lin</author>
<author>D He</author>
<author>J Hsiang</author>
</authors>
<title>Various criteria in the evaluation of biomedical named entity recognition.</title>
<date>2006</date>
<journal>BMC Bioinformatics,</journal>
<volume>7</volume>
<issue>92</issue>
<contexts>
<context position="2818" citStr="Tsai et al., 2006" startWordPosition="405" endWordPosition="408">dical field. Key biomedical IE tasks include named entity (NE) recognition (NER), such as the recognition of protein and gene names; and relation extraction, such as the extraction of protein-protein and gene-gene interactions. NER identifies named entities from natural language texts and classifies them into specific classes according to a defined ontology or classification. In general, biomedical NEs do not follow any nomenclature and may comprise long compound words and short abbreviations. Some NEs contain various symbols and other spelling variations. On average, an NE has five synonyms (Tsai et al., 2006a), and it may belong to multiple categories intrinsically. Since biomedical language and vo243 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 243–246, New York, June 2006. c�2006 Association for Computational Linguistics cabulary are highly complex and evolving rapidly, Bio-NER is a very challenging problem, which raises a number of difficulties. The other main focus of Bio-IE is relation extraction. Most systems only extract the relation targets (e.g., proteins, genes) and the verbs representing those relations, overlooking the many ad</context>
<context position="5251" citStr="Tsai et al., 2006" startWordPosition="770" endWordPosition="773">ned the World Street Journal (WSJ) corpus to the biomedical domain. On adjunct arguments, especially those relevant to the biomedical domain, the performance is unsatisfactory. We, therefore, develop automatically generated templates for identifying these arguments. 2 Biomedical Named Entity Recognition Our Bio-NER system uses the CRF model (Lafferty et al., 2001), which has proven its effectiveness in several sequence tagging tasks. 2.1 Features and Post-Processing Orthographical Features In our experience, ALLCAPS, CAPSMIX, and INITCAP are more useful than others. The details are listed in (Tsai et al., 2006a). Context Features Words preceding or following the target word may be useful for determining its category. In our experience, a suitable window size is five. Part-of-speech Features Part-of-speech information is quite useful for identifying NEs. Verbs and prepositions usually indicate an NE’s boundaries, whereas nouns not found in the dictionary are usually good candidates for named entities. Our experience indicates that five is also a suitable window size. The MBT POS tagger is used to provide POS information. We trained it on GENIA 3.02p and achieved 97.85% accuracy. Word Shape Features </context>
<context position="7178" citStr="Tsai et al., 2006" startWordPosition="1079" endWordPosition="1082">nt lexicon features to estimate the possibility of a token in a biomedical named entity. The first feature determines whether a token is part of a multi-word NE in the dictionary, while the second feature calculates the minimum distance between the given token and a dictionary. In our experience, the first feature is effective for a dictionary containing high-quality items, for example, human-curated protein dictionaries. The second feature is effective for a dictionary that has a large number of items that are not very accurate, for example, web or database lexicons. Details can be found in (Tsai et al., 2006a). Post-Processing We count the number of occurrences of a word x appearing in the rightmost position of all NEs in each category. Let the maximum occurrence be n, 244 and the corresponding category be c. The total number of occurrences of x in the rightmost position of an NE is T; c/T is the consistency rate of x. According to our analysis of the training set of the JNLPBA 2004 data, 75% of words have a consistency rate of over 95%. We record this 75% of words and their associated categories in a table. After testing, we crosscheck all the rightmost words of NEs found by our system against t</context>
<context position="9469" citStr="Tsai et al., 2006" startWordPosition="1459" endWordPosition="1462"> e.g., cell-line and cell-type. This is because biomedical texts have complicated sentence structures and involve more expert knowledge than texts from the general newswire domain. Since pure machine learning approaches cannot model long contextual phenomena well due to context window size limitations and data sparseness, we believe that template-based methods, which exploit long templates containing different levels of linguistic information, may be of help. Certain errors, such as incorrect boundary identification, are more tolerable if the main purpose is to discover relations between NEs (Tsai et al., 2006c). We shall exploit more linguistic features, such as composite features and external features, in the future. However, machine leaning approaches suffer from a serious problem of annotation inconsistency, which confuses machine learning models and makes evaluation difficult. In order to reduce human annotation effort and alleviate the scarcity of available annotated corpora, we shall learn from web corpora to develop machine learning techniques in different biomedical domains. 3 Biomedical Semantic Role Labeling In this section, we describe the main steps in building a biomedical SRL system:</context>
<context position="11687" citStr="Tsai et al., 2006" startWordPosition="1805" endWordPosition="1808"> framesets. Since the annotation is time-consuming, we adopt a semi-automatic approach. We adapt an SRL system trained on PropBank (Wall Street Journal corpus) to the biomedical domain. We first use this SRL system to automatically annotate our corpus, and then human annotators to double check the system’s results. Therefore, human effort is greatly reduced. 3.2 Biomedical SRL System -- SEROW 78.4 66.3 75.8 63.3 70.6 66.4 245 Following (Punyakanok et al., 2004), we formulate SRL as a constituent-by-constituent (C-by-C) tagging problem. We use BioProp to train our biomedical SRL system, SEROW (Tsai et al., 2006b), which uses a maximum entropy (ME) machinelearning model. We use the basic features described in (Xue &amp; Palmer, 2004). In addition, we automatically generate templates which can be used to improve classification of biomedical argument types. The details of SEROW system are described in (Tsai et al., 2005) and (Tsai et al., 2006b). 3.3 Experiment and Summary Our experimental results show that a newswire English SRL system that achieves an F-score of 86.29% can maintain an F-score of 64.64% when ported to the biomedical domain. By using SEROW, we can increase that F-score by 22.9%. Adding aut</context>
</contexts>
<marker>Tsai, Wu, Chou, Lin, He, Hsiang, 2006</marker>
<rawString>Tsai, R. T.-H., Wu, S.-H., Chou, W.-C., Lin, Y.-C., He, D., Hsiang, J., et al. (2006c). Various criteria in the evaluation of biomedical named entity recognition. BMC Bioinformatics, 7(92).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>M Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling. Paper presented at the EMNLP</title>
<date>2004</date>
<contexts>
<context position="11807" citStr="Xue &amp; Palmer, 2004" startWordPosition="1826" endWordPosition="1829"> on PropBank (Wall Street Journal corpus) to the biomedical domain. We first use this SRL system to automatically annotate our corpus, and then human annotators to double check the system’s results. Therefore, human effort is greatly reduced. 3.2 Biomedical SRL System -- SEROW 78.4 66.3 75.8 63.3 70.6 66.4 245 Following (Punyakanok et al., 2004), we formulate SRL as a constituent-by-constituent (C-by-C) tagging problem. We use BioProp to train our biomedical SRL system, SEROW (Tsai et al., 2006b), which uses a maximum entropy (ME) machinelearning model. We use the basic features described in (Xue &amp; Palmer, 2004). In addition, we automatically generate templates which can be used to improve classification of biomedical argument types. The details of SEROW system are described in (Tsai et al., 2005) and (Tsai et al., 2006b). 3.3 Experiment and Summary Our experimental results show that a newswire English SRL system that achieves an F-score of 86.29% can maintain an F-score of 64.64% when ported to the biomedical domain. By using SEROW, we can increase that F-score by 22.9%. Adding automatically generated template features further increases overall F-score by 0.47% and adjunct (AM) F-score by 1.57%, res</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Xue, N., &amp; Palmer, M. (2004). Calibrating features for semantic role labeling. Paper presented at the EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Yuka</author>
<author>A Yakushiji</author>
<author>T Ohta</author>
<author>J Tsujii</author>
</authors>
<title>Syntax annotation for the genia corpus.</title>
<date>2005</date>
<contexts>
<context position="10766" citStr="Yuka et al., 2005" startWordPosition="1658" endWordPosition="1661"> corpus, annotated with verbs and their corresponding semantic roles; (3) build an automatic semantic interpretation model, using the annotated text as a training corpus for machine learning. However, on adjunct arguments, especially on those highly relevant to the biomedical domain, such as AM-LOC (location), the performance is not satisfactory. We therefore develop a template generation method to create templates that are used as features for identifying these argument types. 3.1 Biomedical Proposition Bank -- BioProp Our biomedical proposition bank, BioProp, is based on the GENIA Treebank (Yuka et al., 2005), which is a 491-abstract corpus annotated with syntactic structures. The semantic annotation in BioProp is added to the proper constituents in a syntactic tree. Basically, we adopt the definitions in PropBank (Palmer et al., 2005). For the verbs not in PropBank, such as “phosphorylate”, we define their framesets. Since the annotation is time-consuming, we adopt a semi-automatic approach. We adapt an SRL system trained on PropBank (Wall Street Journal corpus) to the biomedical domain. We first use this SRL system to automatically annotate our corpus, and then human annotators to double check t</context>
</contexts>
<marker>Yuka, Yakushiji, Ohta, Tsujii, 2005</marker>
<rawString>Yuka, T., Yakushiji, A., Ohta, T., &amp; Tsujii, J. (2005). Syntax annotation for the genia corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Zhang</author>
<author>J Su</author>
<author>D Shen</author>
<author>C Tan</author>
</authors>
<title>Recognizing names in biomedical texts: A machine learning approach.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<pages>1178--1190</pages>
<contexts>
<context position="8328" citStr="Zhou et al., 2004" startWordPosition="1288" endWordPosition="1291">sscheck all the rightmost words of NEs found by our system against this table. If they match, we overwrite the NE categories with those from the table. 2.2 Experiments and Summary We perform 10-fold cross validation on the GENIA V3.02 corpus (Kim et al., 2003) to compare our CRF-based system with other biomedical NER systems. The experimental results are reported in Table 1. Our system outperforms other systems in protein names by an F-score of at least 2.6%. For DNA names, our performance is very close to that of the best system. BioNER System Protein DNA Our System (Tsai et al., 2006a) HMM (Zhou et al., 2004) Two Phase SVM (Lee et al., 2003) Table 1. Performance of protein and DNA name recognition on the GENIA V3.02 corpus We have made every effort to implement a variety of linguistic features in our system’s CRF framework. Thanks to these features and the nature of CRF, our system outperforms state-of-the-art machine-learning-based systems, especially in the recognition of protein names. Our system still has difficulty recognizing long, complicated NEs and coordinated NEs and distinguishing between overlapping NE classes, e.g., cell-line and cell-type. This is because biomedical texts have compli</context>
</contexts>
<marker>Zhou, Zhang, Su, Shen, Tan, 2004</marker>
<rawString>Zhou, G., Zhang, J., Su, J., Shen, D., &amp; Tan, C. (2004). Recognizing names in biomedical texts: A machine learning approach. Bioinformatics, 20, 1178-1190.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>