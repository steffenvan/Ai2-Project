<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9814315">
A Markov Model of Machine Translation using
Non-parametric Bayesian Inference
</title>
<author confidence="0.996624">
Yang Feng and Trevor Cohn
</author>
<affiliation confidence="0.879158">
Department of Computer Science
The University of Sheffield
Sheffield, United Kingdom
</affiliation>
<email confidence="0.977729">
yangfeng145@gmail.com and t.cohn@sheffield.ac.uk
</email>
<sectionHeader confidence="0.993408" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940869565217">
Most modern machine translation systems
use phrase pairs as translation units, al-
lowing for accurate modelling of phrase-
internal translation and reordering. How-
ever phrase-based approaches are much
less able to model sentence level effects
between different phrase-pairs. We pro-
pose a new model to address this im-
balance, based on a word-based Markov
model of translation which generates tar-
get translations left-to-right. Our model
encodes word and phrase level phenom-
ena by conditioning translation decisions
on previous decisions and uses a hierar-
chical Pitman-Yor Process prior to pro-
vide dynamic adaptive smoothing. This
mechanism implicitly supports not only
traditional phrase pairs, but also gapping
phrases which are non-consecutive in the
source. Our experiments on Chinese to
English and Arabic to English translation
show consistent improvements over com-
petitive baselines, of up to +3.4 BLEU.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999818369565218">
Recent years have witnessed burgeoning develop-
ment of statistical machine translation research,
notably phrase-based (Koehn et al., 2003) and
syntax-based approaches (Chiang, 2005; Galley
et al., 2006; Liu et al., 2006). These approaches
model sentence translation as a sequence of sim-
ple translation decisions, such as the application
of a phrase translation in phrase-based methods
or a grammar rule in syntax-based approaches.
In order to simplify modelling, most MT mod-
els make an independence assumption, stating that
the translation decisions in a derivation are in-
dependent of one another. This conflicts with
the intuition behind phrase-based MT, namely that
translation decisions should be dependent on con-
text. On one hand, the use of phrases can mem-
orize local context and hence helps to generate
better translation compared to word-based models
(Brown et al., 1993; Och and Ney, 2003). On the
other hand, this mechanism requires each phrase
to be matched strictly and to be used as a whole,
which precludes the use of discontinuous phrases
and leads to poor generalisation to unseen data
(where large phrases tend not to match).
In this paper we propose a new model to drop
the independence assumption, by instead mod-
elling correlations between translation decisions,
which we use to induce translation derivations
from aligned sentences (akin to word alignment).
We develop a Markov model over translation de-
cisions, in which each decision is conditioned on
previous n most recent decisions. Our approach
employs a sophisticated Bayesian non-parametric
prior, namely the hierarchical Pitman-Yor Process
(Teh, 2006; Teh et al., 2006) to represent back-
off from larger to smaller contexts. As a result,
we need only use very simple translation units
– primarily single words, but can still describe
complex multi-word units through correlations be-
tween their component translation decisions. We
further decompose the process of generating each
target word into component factors: finishing the
translating, jumping elsewhere in the source, emit-
ting a target word and deciding the fertility of the
source words.
Overall our model has the following features:
</bodyText>
<listItem confidence="0.991236272727273">
1. enabling model parameters to be shared be-
tween similar translation decisions, thereby
obtaining more reliable statistics and gener-
alizing better from small training sets.
2. learning a much richer set of transla-
tion fragments, such as gapping phrases,
e.g., the translation for the German werde
... ankommen in English is will arrive ....
3. providing a unifying framework spanning
word-based and phrase-based model of trans-
lation, while incorporating explicit transla-
</listItem>
<page confidence="0.989539">
333
</page>
<note confidence="0.9171325">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 333–342,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.985042625">
tion, insertion, deletion and reordering com-
ponents.
We demonstrate our model on Chinese-English
and Arabic-English translation datasets. The
model produces uniformly better translations than
those of a competitive phrase-based baseline,
amounting to an improvement of up to 3.4 BLEU
points absolute.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999365625">
Word based models have a long history in machine
translation, starting with the venerable IBM trans-
lation models (Brown et al., 1993) and the hid-
den Markov model (Vogel et al., 1996). These
models are still in wide-spread use today, albeit
only as a preprocessing step for inferring word
level alignments from sentence-aligned parallel
corpora. They combine a number of factors, in-
cluding distortion and fertility, which have been
shown to improve word-alignment and translation
performance over simpler models. Our approach
is similar to these works, as we also develop a
word-based model, and explicitly consider simi-
lar translation decisions, alignment jumps and fer-
tility. We extend these works in two important
respects: 1) while they assume a simple parame-
terisation by making iid assumptions about each
translation factor, we instead allow for rich cor-
relations by modelling sequences of translation
decisions; and 2) we develop our model in the
Bayesian framework, using a hierarchical Pitman-
Yor Process prior with rich backoff semantics be-
tween high and lower order sequences of transla-
tion decisions. Together this results in a model
with rich expressiveness but can still generalize
well to unseen data.
More recently, a number of authors have pro-
posed Markov models for machine translation.
Vaswani et al. (2011) propose a rule Markov
model for a tree-to-string model which models
correlations between pairs of mininal rules, and
use Kneser-Ney smoothing to alleviate the prob-
lems of data sparsity. Similarly, Crego et al.
(2011) develop a bilingual language model which
incorporates words in the source and target lan-
guages to predict the next unit, which they use as
a feature in a translation system. This line of work
was extended by Le et al. (2012) who develop a
novel estimation algorithm based around discrimi-
native projection into continuous spaces. Also rel-
evant is Durrani et al. (2011), who present a se-
quence model of translation including reordering.
Our work also uses bilingual information, using
the source words as part of the conditioning con-
text. In contrast to these approaches which pri-
marily address the decoding problem, we focus on
the learning problem of inferring alignments from
parallel sentences. Additionally, we develop a full
generative model using a Bayesian prior, and in-
corporate additional factors besides lexical items,
namely jumps in the source and word fertility.
Another aspect of this paper is the implicit sup-
port for phrase-pairs that are discontinous in the
source language. This idea has been developed
explicitly in a number of previous approaches, in
grammar based (Chiang, 2005) and phrase-based
systems (Galley and Manning, 2010). The latter is
most similar to this paper, and shows that discon-
tinuous phrases compliment standard contiguous
phrases, improving expressiveness and translation
performance. Unlike their work, here we develop
a complimentary approach by constructing a gen-
erative model which can induce these rich rules
directly from sentence-aligned corpora.
</bodyText>
<sectionHeader confidence="0.990481" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999951777777778">
Given a source sentence, our model infers a la-
tent derivation which produces a target translation
and meanwhile gives a word alignment between
the source and the target. We consider a pro-
cess in which the target string is generated using
a left-to-right order, similar to the decoding strat-
egy used by phrase-based machine translation sys-
tems (Koehn et al., 2003). During this process we
maintain a position in the source sentence, which
can jump around to allow for different sentence
ordering in the target vs. source languages. In
contrast to phrase-based models, we use words as
our basic translation unit, rather than multi-word
phrases. Furthermore, we decompose the deci-
sions involved in generating each target word to
a number of separate factors, where each factor is
modelled separately and conditioned on a rich his-
tory of recent translation decisions.
</bodyText>
<subsectionHeader confidence="0.998936">
3.1 Markov Translation
</subsectionHeader>
<bodyText confidence="0.935627125">
Our model generates target translation left-to-
right word by word. The generative process
employs the following recursive procedure to
construct the target sentence conditioned on the
source:
i ← 1
while Not finished do
Decide whether to finish the translation,ξi
</bodyText>
<page confidence="0.998305">
334
</page>
<table confidence="0.804878875">
Step Source sentence Translation finish jump emission
0 Je le prends
1 Je le prends I no monotone Je I
2 Je le prends I ’ll no insert null ’ll
3 Je le prends I ’ll take no forward prends take
4 Je le prends I ’ll take that no backward le that
5 Je le prends I ’ll take that one no stay le one
6 Je le prends I ’ll take that one yes
</table>
<figureCaption confidence="0.998041">
Figure 1: Translation agenda of Je le prends -+ I ’ll take that one.
</figureCaption>
<bodyText confidence="0.927623">
if ξi = false then
</bodyText>
<subsectionHeader confidence="0.368935">
Select a source word to jump to
Emit a target word for the source word
end if
</subsectionHeader>
<bodyText confidence="0.927802">
i i i + 1
end while
In the generation of each target word, our model
includes three separate factors: the binary finish
decision, a jump decision to move to a different
source word, and emission which translates or oth-
erwise inserts a word in the target string. This gen-
erative process resembles the sequence of transla-
tion decisions considered by a standard MT de-
coder (Koehn et al., 2003), but note that our ap-
proach differs in that there is no constraint that all
words are translated exactly once. Instead source
words can be skipped or repeatedly translated.
This makes the approach more suitable for learn-
ing alignments, e.g., to account for word fertilities
(see §3.3), while also permitting inference using
Gibbs sampling (§4).
More formally, we can express our probabilistic
model as
</bodyText>
<equation confidence="0.997623">
p(τi|fi−1
ai−n, ei−1
i−n)
p(ei|τi, fiai−n, ei−1
i−n) (1)
</equation>
<bodyText confidence="0.9997093125">
where ξi is the finish decision for target posi-
tion i, τi is the jump decision to source word fai
and fiai−n is the source words for target positions
i − n, i − n + 1, ..., i. Each of the three distribu-
tions (finish, jump and emission) is drawn respec-
tive from hierarchical Pitman-Yor Process priors,
as described in Section 3.2.
The jump decision τi in Equation 1 demands
further explanation. Instead of modelling jump
distances explicitly, which poses problems for
generalizing between different lengths of sen-
tences and general parameter explosion, we con-
sider a small handful of types of jump based on
the distance between the current source word ai
and the previous source word ai−1, i.e., di =
ai − ai−1.1 We bin jumps into five types:
</bodyText>
<listItem confidence="0.9821484">
a) insert;
b) backward, if di &lt; 0;
c) stay, if di = 0;
d) monotone, if di = 1;
e) forward, if di &gt; 1.
</listItem>
<bodyText confidence="0.999955785714286">
The special jump type insert handles null align-
ments, denoted ai = 0 which licence spurious in-
sertions in the target string.
To illustrate this translation process, Figure 1
shows the example translation &lt;Je le prends, I ’ll
take that one&gt;. Initially we set the source position
before the first source word Je. Then in step 1,
we decide not to finish (finish=no), jump to source
word Je and translate it as I. Next, we again de-
cide not to finish, jump to the null source word
and insert ’ll. The process continues until in step
6 we elect to finish (finish=yes), at which point the
translation is complete, with target string I ’ll take
that one.
</bodyText>
<subsectionHeader confidence="0.999935">
3.2 Hierarchical Pitman-Yor Process
</subsectionHeader>
<bodyText confidence="0.9494359375">
The Markov assumption limits the context of each
distribution to the n most recent translation deci-
sions, which limits the number of model param-
eters. However for any non-trivial value n &gt;
0, overfitting is a serious concern. We counter
the problem of a large parameter space using a
Bayesian non-parametric prior, namely the hier-
archical Pitman-Yor Process (PYP). The PYP de-
scribes distributions over possibly infinite event
spaces that follow a power law, with few events
taking the majority of the probability mass and a
long tail of less frequent events. We consider a hi-
erarchical PYP, where a sequence of chained PYP
1For a target position aligned to null, we denote its source
word as null and set its aligned source position as that of the
previous target word that is aligned to non-null.
</bodyText>
<equation confidence="0.999697666666667">
pbs(eI 1, aI 1|fJ 1 ) =
p(ξi|fi−1
ai−n,ei−1
i−n)
I+1
i=1
I
X
i=1
I
X
i=1
</equation>
<page confidence="0.977471">
335
</page>
<bodyText confidence="0.9998992">
priors allow backoff from larger to smaller con-
texts such that our model can learn rich contextual
models for known (large) contexts while also still
being able to generalize well to unseen contexts
(using smaller histories).
</bodyText>
<subsubsectionHeader confidence="0.676181">
3.2.1 Pitman-Yor Process
</subsubsectionHeader>
<bodyText confidence="0.995654380952381">
A PYP (Pitman and Yor, 1997) is defined by its
discount parameter 0 &lt; a &lt; 1, strength parameter
b &gt; −a and base distribution G0. For a distri-
bution drawn from a PYP, G — PYP(a, b, G0),
marginalising out G leads to a simple distribution
which can be described using a variant of the Chi-
nese Restaurant Process (CRP). In this analogy we
imagine a restaurant has an infinite number of ta-
bles and each table can accommodate an infinite
number of customers. Each customer (a sample
from G) walks in one at a time and seats them-
selves at a table. Finally each table is served a
communal dish (a draw from G0), which is served
to each customer seated at the table. The assign-
ment of customers to tables is such that popular
tables are more likely to be chosen, and this rich-
get-richer dynamic produces power-law distribu-
tions with few events (the dishes at popular tables)
dominating the distribution.
More formally, at time n a customer enters and
selects a table k which is either a table having been
</bodyText>
<equation confidence="0.8468436">
seated (1 &lt; k &lt; K−) or an empty table (k =
K− + 1) by
� _ ctk—a 1 &lt; k &lt; K−
p(tn = k |t−n) — n− 1 +b
n−1+b k = K− + 1
</equation>
<bodyText confidence="0.999955">
where tn is the table selected by the customer n,
t−n is the seating arrangement of previous n − 1
customers, c−tk is the number of customers seated
at table k in t−n and K− = K(t−n) is the number
of tables in t−n.
If the customer sits at an empty table, a dish h
is served to his table by the probability of G0(h),
otherwise, he can only share with others the dish
having been served to his table.2 Overall, the prob-
ability of the customer being served a dish h is
</bodyText>
<equation confidence="0.971812333333333">
c−
oh
p(on = h|t−n, o−n) = n − 1 + b
(aK− + b)
+ G0 (h)
n − 1 + b
</equation>
<bodyText confidence="0.996946">
where on is the dish served to the customer n, o−n
is the dish accommodation of previous n − 1 cus-
tomers, c−oh is the number of customers who are
</bodyText>
<footnote confidence="0.913119">
2We also say the customer is served with this dish.
</footnote>
<bodyText confidence="0.999866727272727">
served with the dish h in o−n and K−h is the num-
ber of tables served with the dish h in t−n.
The hierarchical PYP (hPYP; Teh (2006)) is an
extension of the PYP in which the base distribu-
tion G0 is itself a PYP distribution. This parent
(base) distribution can itself have a PYP as a base
distribution, giving rise to hierarchies of arbitrary
depth. Like the PYP, inference under the hPYP
can be also described in terms of CRP whereby
each table in one restaurant corresponds to a dish
in the next deeper level, and is said to share the
same dish. Whenever an empty table is seated in
one level, a customer must enter the restaurant in
the next deeper level and find a table to sit. This
process continues until the customer is assigned a
shared table or the deepest level of the hierarchy
is reached. A similar process occurs when a cus-
tomer leaves, where newly emptied tables must be
propagated up the hierarchy in the form of depart-
ing customers. There is not space for a complete
treatment of the hPYP and the particulars of infer-
ence; we refer the interested reader to Teh (2006).
</bodyText>
<subsectionHeader confidence="0.953055">
3.2.2 A Hierarchical PYP Translation Model
</subsectionHeader>
<bodyText confidence="0.999985517241379">
We draw the distributions for the various transla-
tion factors from respective hierarchical PYP pri-
ors, as shown in Figure 2 for the finish, jump and
emission factors. For the emission factor (Fig-
ure 2c), we draw the target word ei from a distribu-
tion conditioned on the last two source and target
words, as well as the current source word, fai and
the current jump type τi. Here the draw of a tar-
get word corresponds to a customer entering and
which target word to emit corresponds to which
dish to be served to the customer in the CRP. The
hierarchical prior encodes a backoff path in which
the jump type is dropped first, followed by pairs of
source and target words from least recent to most
recent. The final backoff stages drop the current
source word, terminating with the uniform base
distribution over the target vocabulary V .
The distributions over the other two factors in
Figure 2 follow a similar pattern. Note however
that these distributions don’t condition on the cur-
rent source word, and consequently have fewer
levels of backoff. The terminating base distribu-
tion for the finish factor is a uniform distribution
with equal probability for finishing versus contin-
uing. The jump factor has an additional condition-
ing variable t which encodes whether the previous
alignment is near the start or end of the source sen-
tence. This information affects which of the jump
values are legal from the current position, such
</bodyText>
<equation confidence="0.953598191489362">
− aK−h
336
ξi|fi−1
ai−2, ei−1
i−2 ∼ Gξ fi−1
ai−2,ei−1
i−2
Gξ 3, Gξ
fi−1
ai−2,ei−1
i−2 ∼ PYP(aξ 3, bξ fai−1,ei−1)
Gξfai−1,ei−1 ∼ PYP(aξ2, bξ2, Gξ)
Gξ ∼ PYP(aξ1, bξ1, Gξ0)
Gξ 0 ∼ U(1 2)
(a) Finishfactor
τi|fi−1
ai−2, ei−1
i−2, t ∼ Gτ fi−1
ai−2,ei−1
i−2,t
Gτ 3, Gτ
fi−1
ai−2,ei−1
i−2,t ∼ PYP(aτ 3, bτ fai−1,ei−1,t)
Gτfai−1,ei−1,t ∼ PYP(aτ2, bτ2, Gτt )
Gτt ∼ PYP(aτ1, bτ1, Gτ0,t)
Gτ0,t ∼ U
(b) Jump factor
ei|τi, fi ai−2, ei−1
i−2 ∼Ge τi,fi ai−2,ei−1
i−2
Ge 5, Ge
τi,fi ai−2,ei−1
i−2 ∼ PYP(ae 5, be fi ai−2,ei−1
i−2)
4, Ge
fi ai−2,ei−1
i−2 ∼ PYP(ae 4, be fi ai−1,ei−1)
Ge ∼ PYP(ae 3, be 3, Ge
fi fai)
ai−1,ei−1
Gefai ∼ PYP(ae2, be2, Ge)
Ge ∼ PYP(ae1, be1, Ge0)
Ge 0 ∼ U( 1
|V |)
(c) Emission factor
Ge
</equation>
<figureCaption confidence="0.999341">
Figure 2: Distributions over the translation factors and their hierarchical priors.
</figureCaption>
<bodyText confidence="0.998512">
that a jump could not go outside the bounds of the
source sentence. Accordingly we maintain sepa-
rate distributions for each setting, and each has a
different uniform base distribution parameterized
according to the number of possible jump types.
</bodyText>
<subsectionHeader confidence="0.969376">
3.3 Fertility
</subsectionHeader>
<bodyText confidence="0.999786727272727">
For each target position, our Markov model may
select a source word which has been covered,
which means a source word may be linked to sev-
eral target positions. Therefore, we introduce fer-
tility to denote the number of target positions a
source word is linked to in a sentence pair. Brown
et al. (1993) have demonstrated the usefulness of
fertility in probability estimation: IBM models 3–
5 exhibit large improvements over models 1–2. On
these grounds, we include fertility to produce our
advanced model,
</bodyText>
<equation confidence="0.984711">
J
pad(eI1,aI1|fJ1 )=pbs(eI1,aI1|fJ1 ) H p(φj|fjj−n) (2)
j=1
</equation>
<bodyText confidence="0.999393125">
where φj is the fertility of source word fj in the
sentence pair &lt; fJ1 , eI1 &gt; and pbs is the basic
model defined in Eq. 1. In order to avoid prob-
lems of data sparsity, we bin fertility into three
types, a) zero, if φ = 0; b) single, if φ = 1;
and c) multiple, if φ &gt; 1.
We draw the fertility variables from a hierarchi-
cal PYP distribution, using three levels of backoff,
</bodyText>
<equation confidence="0.99421075">
φj|fjj−1 ∼ Gφfj
j−1
Gφ ∼ PYP(aφ 3, bφ 3, Gφ fj)
fj
j−1
Gφfj ∼ PYP(aφ2, bφ2, Gφ)
Gφ ∼ PYP(aφ1, bφ1, Gφ0)
Gφ 0 ∼ U(13)
</equation>
<bodyText confidence="0.99989375">
where we condition the fertility of each word to-
ken on the token to its left, which we drop during
the first stage of backoff to simple word-based fer-
tility. The last level of backoff further generalises
to a shared fertility across all words. In this way
we gain the benefits of local context on fertility,
while including more general levels to allow wider
applicability.
</bodyText>
<sectionHeader confidence="0.978823" genericHeader="method">
4 Gibbs Sampling
</sectionHeader>
<bodyText confidence="0.994646482758621">
To train the model, we use Gibbs sampling, a
Markov Chain Monte Carlo (MCMC) technique
for posterior inference. Specifically we seek to
infer the latent sequence of translation decisions
given a corpus of sentence pairs. Given the struc-
ture of our model, a word alignment uniquely
specifies the translation decisions and the se-
quence follows the order of the target sentence left
to right. Our Gibbs sampler operates by sampling
an update to the alignment of each target word
in the corpus. It visits each sentence pair in the
corpus in a random order and resamples the align-
ments for each target position as follows. First we
discard the alignment to the current target word
and decrement the counts of all factors affected
by this alignment in their top level distributions
(which will percolate down to the lower restau-
rants). Next we calculate posterior probabilities
for all possible alignment to this target word based
on the table occupancies in the hPYP. Finally we
draw an alignment and increment the table counts
for the translation decisions affected by the new
alignment.
More specifically, we consider sampling from
Equation 2 with n = 2. When changing the align-
ment to a target word ei from j0 to j, the fin-
ish, jump and emission for three target positions
i, i + 1, i + 2 and fertility for two source positions
j, j0 may be affected. This leads to the following
</bodyText>
<page confidence="0.948501">
337
</page>
<equation confidence="0.968152363636364">
decrement increment
ξ(no  |null, ’ll, Je, I) ξ(no  |null, ’ll, Je, I)
ξ(no  |p..s, take, null, ’ll) ξ(no  |Je, take, null, ’ll)
ξ(no  |le, that, p..s, take) ξ(no  |le, that, Je, take)
τ(f  |null, ’ll, Je, I) τ(s |null, ’ll, Je, I)
τ(b  |p..s, take, null, ’ll) τ(m |Je, take, null, ’ll)
τ(s  |le, that, p..s, take) τ(s |le, that, Je, take)
e(take |f, p..s, null, ’ll, Je, I) e(take |s, Je, null, ’ll, Je, I)
e(that |b, le, p..s, take, null, ’ll) e(that |m, le, Je, take, null, ’ll)
e(one |s, le, le, that, p..s, take) e(one |s, le, le, that, Je, take)
φ(single  |p..s, le) φ(multiple  |Je, &lt;s&gt;)
</equation>
<figureCaption confidence="0.83915825">
Table 1: The count update when changing the
aligned source word of take from prends to Je in
Figure 1. Key: f–forward s–stay b–backward m–
monotone p..s–prends.
</figureCaption>
<bodyText confidence="0.446152">
posterior probability
</bodyText>
<equation confidence="0.9916564">
p(ai = j|t−i, o−i) oC i+2ri p(ξl)p(τl)p(el)
l=i
p(φj + 1)p(φj&apos; � 1)
X (3)
p(φj)p(φj&apos;)
</equation>
<bodyText confidence="0.999993875">
where φj, φj&apos; are the fertilities before changing the
link and for brevity we omit the conditioning con-
texts. For example, in Figure 1, we sample for
target word take and change the aligned source
word from prends to Je, then the items for which
we need to decrement and increment the counts by
one are shown in Table 1 and the posterior prob-
ability corresponding to the new alignment is the
product of the hierarchical PYP probabilities of all
increment items divided by the probability of the
fertility of prends being single.
Maintaining the current state of the hPYP as
events are incremented and decremented is non-
trivial and the naive approach requires significant
book-keeping and has poor runtime behaviour. For
this we adopt the approach of Blunsom et al.
(2009b), who present a method for maintaining
table counts without needing to record the table
assignments for each translation decision. Briefly,
this algorithm samples the table assignment during
the increment and decrement operations, which is
then used to maintain aggregate table statistics.
This can be done efficiently and without the need
for explicit table assignment tracking.
</bodyText>
<subsectionHeader confidence="0.982074">
4.1 Hyperparameter Inference
</subsectionHeader>
<bodyText confidence="0.999942846153846">
In our model, we treat all hyper-parameters
{(ax, bx), x E (ξ, τ, e, φ)} as latent random vari-
ables rather than fixed parameters. This means our
model is parameter free, and requires no user inter-
vention when adapting to different data sets. For
the discount parameter, we employ a uniform Beta
distribution ax — Beta(1,1) while for the strength
parameter, we employ a vague Gamma distribu-
tion bx — Gamma(10, 0.1). All restaurants in
the same level share the same hyper-prior and the
hyper-parameters for all levels are resampled us-
ing slice sampling (Johnson and Goldwater, 2009)
every 10 iterations.
</bodyText>
<subsectionHeader confidence="0.981556">
4.2 Parallel Implementation
</subsectionHeader>
<bodyText confidence="0.99996195">
As mentioned above, the hierarchical PYP takes
into consideration a rich history to evaluate the
probabilities of translation decisions. But this
leads to difficulties when applying the model to
large data sets, particularly in terms of tracking
the table and customer counts. We apply the tech-
nique from Blunsom et al. (2009a) of using multi-
ple processors to perform approximate Gibbs sam-
pling which they showed achieved equivalent per-
formance to the exact Gibbs sampler. Each pro-
cess performs sampling on a subset of the corpus
using local counts, and communicates changes to
these counts after each full iteration. All the count
deltas are then aggregated by each process to re-
fresh the counts at the end of each iteration. In
this way each process uses slightly “out-of-date”
counts, but can process the data independently of
the other processes. We found that this approxi-
mation improved the runtime significantly with no
noticeable effect on accuracy.
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999876705882353">
In principle our model could be directly used as a
MT decoder or as a feature in a decoder. However
in this paper we limit our focus to inducing word
alignments, i.e., by using the model to infer align-
ments which are then used in a standard phrase-
based translation pipeline. We leave full decod-
ing for later work, which we anticipate would fur-
ther improve performance by exploiting gapping
phrases and other phenomena that implicitly form
part of our model but are not represented in the
phrase-based decoder. Decoding under our model
would be straight-forward in principle, as the gen-
erative process was designed to closely parallel the
search procedure in the phrase-based model.3
Three data sets were used in the experi-
ments: two Chinese to English data sets on small
(IWSLT) and larger corpora (FBIS), and Arabic
</bodyText>
<footnote confidence="0.996033666666667">
3However the reverse translation probability would be in-
tractable, as this does not decompose following a left-to-right
generation order in the target language.
</footnote>
<page confidence="0.997101">
338
</page>
<bodyText confidence="0.999835142857143">
to English translation. Our experiments seek to
test how the model compares to a GIZA++ base-
line, quantifies the effect of each factor in the
probabilistic model (i.e., jump, fertility), and the
effect of different initialisations of the sampler.
We present results on translation quality and word
alignment.
</bodyText>
<subsectionHeader confidence="0.997281">
5.1 Data Setup
</subsectionHeader>
<bodyText confidence="0.999930454545454">
The Markov order of our model in all experiments
was set to n = 2, as shown in Equation 2. For each
data set, Gibbs sampling was performed on the
training set in each direction (source-to-target and
target-to-source), initialized using GIZA++.4 We
used the grow heuristic to combine the GIZA++
alignments in both directions (Koehn et al., 2003),
which we then intersect with the predictions of
GIZA++ in the relevant translation direction. This
initialisation setup gave the best results (we com-
pare other initialisations in §5.2). The two Gibbs
samplers were “burned in” for the first 1000 it-
erations, after which we ran a further 500 itera-
tions selecting every 50th sample. A phrase ta-
ble was constructed using these 10 sets of multi-
ple alignments after combining each pair of direc-
tional alignments using the grow-diag-final heuris-
tic. Using multiple samples in this way constitutes
Monte Carlo averaging, which provides a better
estimate of uncertainty cf. using a single sample.5
The alignment used for the baseline results was
produced by combining bidirectional GIZA++
alignments using the grow-diag-final heuristic.
We used the Moses machine translation decoder
(Koehn et al., 2007), using the default features
and decoding settings. We compared the perfor-
mance of Moses using the alignment produced by
our model and the baseline alignment, evaluating
translation quality using BLEU (Papineni et al.,
2002) with case-insensitive n-gram matching with
n = 4. We used minimum error rate training (Och,
2003) to tune the feature weights to maximise the
BLEU score on the development set.
</bodyText>
<subsectionHeader confidence="0.975055">
5.2 IWSLT Corpus
</subsectionHeader>
<bodyText confidence="0.9999848">
The first experiments are on the IWSLT data set
for Chinese-English translation. The training data
consists of 44k sentences from the tourism and
travel domain. For the development set we use
both ASR devset 1 and 2 from IWSLT 2005, and
</bodyText>
<footnote confidence="0.9931475">
4All GIZA++ alignments used in our experiments were
produced by IBM model4.
5The effect on translation scores is modest, roughly
amounting to +0.2 BLEU versus using a single sample.
</footnote>
<table confidence="0.9996402">
System Dev IWSLT05
baseline 45.78 49.98
Markov+fs+e 49.13 51.54
Markov+fs+e+j 49.68 52.55
Markov+fs+e+j+ft 51.32 53.41
</table>
<tableCaption confidence="0.938847">
Table 2: Impact of adding factors to our Markov
model, showing BLEU scores on IWSLT. Key: fs–
finish e–emission j–jump ft–fertility.
</tableCaption>
<bodyText confidence="0.999942690476191">
for the test set we use the IWSLT 2005 test set.
The language model is a 3-gram language model
trained using the SRILM toolkit (Stolcke, 2002)
on the English side of the training data. Because
the data set is small, we performed Gibbs sampling
on a single processor.
First we check the effect of the model factors
jump and fertility. Both emission and finish fac-
tors are indispensable to the generative translation
process, and consequently these two factors are in-
cluded in all runs. Table 2 shows translation result
for various models, including a baseline and our
Markov model with different combinations of fac-
tors. Note that even the simplest Markov model far
outperforms the GIZA++ baseline (+1.5 BLEU)
despite the baseline (IBM model 4) including a
number of advanced features (e.g., jump, fertility)
that are not present in the basic Markov model.
This improvement is a result of the Markov model
making use of rich bilingual contextual informa-
tion coupled with sophisticated backoff, as op-
posed to GIZA++ which considers much more lo-
cal events, with nothing larger than word-class bi-
grams. Our model shows large improvements as
the extra factors are included. Jump yields an im-
provement of +1 BLEU by capturing consistent re-
ordering patterns. Adding fertility results in a fur-
ther +1 BLEU point improvement. Like the IBM
models, our approach allows each source word to
produce any number of target words. This capac-
ity allows for many non-sensical alignments such
as dropping many source words, or aligning sin-
gle source words to several target words. Explic-
itly modelling fertility allows for more consistent
alignments, especially for special words such as
punctuation which usually have a fertility of one.
Next we check the stability of our model with
different initialisations. We compare different
combination techniques for merging the GIZA++
alignments: grow-diag-final (denoted as gdf), in-
tersection and grow. Table 3 shows that the dif-
ferent initialisations have only a small effect on
</bodyText>
<page confidence="0.996753">
339
</page>
<table confidence="0.996948666666667">
system gdf intersection grow
baseline 49.98 48.44 50.11
our model 52.96 52.79 53.41
</table>
<tableCaption confidence="0.994804">
Table 3: Machine translation performance in
</tableCaption>
<bodyText confidence="0.979413780487805">
BLEU % on the IWSLT 2005 Chinese-English test
set. The Gibbs samplers were initialized with three
different alignments, shown as columns.
the results of our model. While the baseline re-
sults vary by up to 1.7 BLEU points for the differ-
ent alignments, our Markov model provided more
stable results with the biggest difference of 0.6.
Among the three initialisations, we get the best
result with the initialisation of grow. Gdf of-
ten introduces alignment links involving function
words which should instead be aligned to null. In-
tersection includes many fewer alignments, typi-
cally only between content words, and the sparsity
means that words can only have a fertility of ei-
ther 0 or 1. This leads to the initialisation being a
strong mode which is difficult to escape from dur-
ing sampling. Despite this problem, it has only
a mild negative effect on the performance of our
model, which is probably due to improvements
in the alignments for words that truly should be
dropped or aligned only to one word. Grow pro-
vides a good compromise between gdf and inter-
section, and we use this initialisation in all our
subsequent experiments.
Figure 3 shows an example comparing align-
ments produced by our model and the GIZA++
baseline, in both cases after combining the two di-
rectional models. Note that GIZA++ has linked
many function words which should be left un-
aligned, by using rare English terms as garbage
collectors. Consequently this only allows for the
extraction of few large phrase-pairs (e.g. &lt;在
找, ’m looking for&gt;) and prevents the extraction
of some good phrases (e.g. &lt;烧 X 类 型 的,
grill-type&gt;, for “家” and “点 的” are wrongly
aligned to “grill-type”). In contrast, our model
better aligns the function words, such that many
more useful phrase pairs can be extracted, i.e.,
&lt;在, ’m&gt;, &lt;找, looking for&gt;, &lt;烧X 类型, grill-
type&gt; and their combinations with neighbouring
phrase pairs.
</bodyText>
<subsectionHeader confidence="0.670938">
5.3 FBIS Corpus
</subsectionHeader>
<bodyText confidence="0.999728333333333">
Theoretically, Bayesian models should out-
perform maximum likelihood approaches on small
data sets, due to their improved modelling of un-
</bodyText>
<figure confidence="0.999396461538462">
(a) GIZA++ baseline
i
&apos;m
looking
for
a
nice
,
quiet
grill-type
restaurant
.
(b) our model
</figure>
<figureCaption confidence="0.9905515">
Figure 3: Comparison of an alignment inferred by
the baseline vs. our approach.
</figureCaption>
<bodyText confidence="0.999955">
certainty. For larger datasets, however, the dif-
ference between the two techniques should nar-
row. Hence one might expect that upon moving
to larger translation datasets our gains might evap-
orate. This chain of reasoning ignores the fact that
our model is considerably richer than the baseline
IBM models, in that we model rich contextual cor-
relations between translation decisions, and con-
sequently our approach has a lower inductive bias.
For this reason our model should continue to im-
prove with more data, by inferring better estimates
of translation decision n-grams. A caveat though
is that inference by sampling becomes less effi-
cient on larger data sets due to stronger modes,
requiring more iterations for convergence.
To test whether our improvements carry over to
larger datasets, we assess the performance of our
model on the FBIS Chinese-English data set. Here
the training data consists of the non-UN portions
and non-HK Hansards portions of the NIST train-
ing corpora distributed by the LDC, totalling 303k
sentence pairs with 8m and 9.4m words of Chi-
nese and English, respectively. For the develop-
ment set we use the NIST 2002 test set, and eval-
uate performance on the test sets from NIST 2003
</bodyText>
<page confidence="0.990059">
340
</page>
<table confidence="0.998633">
NIST02 NIST03 NIST05
baseline 33.31 30.09 29.01
our model 33.83 31.02 30.23
</table>
<tableCaption confidence="0.982529333333333">
Table 4: Translation performance on Chinese to
English translation, showing BLEU% for models
trained on the FBIS data set.
</tableCaption>
<table confidence="0.999242666666667">
F1% NIST02 NIST03 NIST05
baseline 64.9 57.00 48.75 48.93
our model 65.7 57.14 49.49 48.96
</table>
<tableCaption confidence="0.870319666666667">
Table 5: Translation performance on Arabic to
English translation, showing BLEU%. Also shown
is word-alignment alignment accuracy.
</tableCaption>
<bodyText confidence="0.999322">
and 2005. The language model is a 3-gram LM
trained on Xinhua portion of the Gigaword corpus
using the SRILM toolkit with modified Kneser-
Ney smoothing. As the FBIS data set is large, we
employed 3-processor MPI for each Gibbs sam-
pler, which ran in half the time compared to using
a single processor.
Table 4 shows the results on the FBIS data set.
Our model outperforms the baseline on both test
sets by about 1 BLEU. This provides evidence that
our model performs well in the large data setting,
with our rich modelling of context still proving
useful. The non-parametric nature of the model al-
lows for rich dynamic backoff behaviour such that
it can learn accurate models in both high and low
data scenarios.
</bodyText>
<subsectionHeader confidence="0.975658">
5.4 Arabic English translation
</subsectionHeader>
<bodyText confidence="0.999926875">
Translation between Chinese and English is very
difficult, particularly due to word order differences
which are not handled well by phrase-based ap-
proaches. In contrast Arabic to English translation
needs less reordering, and phrase-based models
produce better translations. This translation task
is a good test for the generality of our approach.
Our Ar-En training data comprises several LDC
corpora,6 using the same experimental setup as in
Blunsom et al. (2009a). Overall there are 276k
sentence pairs and 8.21m and 8.97m words in Ara-
bic and English, respectively. We evaluate on the
NIST test sets from 2003 and 2005, and the 2002
test set was used for MERT training.
Table 5 shows the results. On all test sets our
approach outperforms the baseline, and for the
NIST03 test set the improvement is substantial,
with a +0.74 BLEU improvement. In general
the improvements are more modest than for the
Chinese-English results above. We suggest that
this is due to the structure of Arabic-English trans-
lation better suiting the modelling assumptions be-
hind IBM model 4, particularly its bias towards
monotone translations. Consequently the addi-
</bodyText>
<page confidence="0.367592">
6LDC2004E72, LDC2004T17, LDC2004T18,
LDC2006T02
</page>
<bodyText confidence="0.999823142857143">
tional context provided by our model is less im-
portant. Table 5 also reports alignment results on
manually aligned Ar-En sentence pairs,7 measur-
ing the F1 score for the GIZA++ baseline align-
ments and the alignment from the final sample
with our model.8 Our model outperforms the base-
line, although the improvement is modest.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999991684210526">
This paper proposes a word-based Markov model
of translation which correlates translation deci-
sions by conditioning on recent decisions, and
incorporates a hierarchical Pitman-Yor process
prior permitting elaborate backoff behaviour. The
model can learn sequences of translation deci-
sions, akin to phrases in standard phrase-based
models, while simultaneously learning word level
phenomena. This mechanism generalises the
concept of phrases in phrase-based MT, while
also capturing richer phenomena such as gapping
phrases in the source. Experiments show that our
model performs well both on the small and large
datasets for two different translation tasks, con-
sistently outperforming a competitive baseline. In
this paper the model was only used to infer word
alignments; in future work we intend to develop
a decoding algorithm for directly translating with
the model.
</bodyText>
<sectionHeader confidence="0.996525" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9988955">
This work was supported by the EPSRC (grant
EP/I034750/1).
</bodyText>
<sectionHeader confidence="0.999246" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99960125">
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles
Osborne. 2009a. A Gibbs sampler for phrasal
synchronous grammar induction. In Proc. of ACL-
IJCNLP, pages 782–790.
</reference>
<footnote confidence="0.861442333333333">
7LDC2012T16
8Directional alignments are intersected using the grow-
diag-final heuristic.
</footnote>
<page confidence="0.99556">
341
</page>
<reference confidence="0.999940833333333">
Phil Blunsom, Trevor Cohn, Sharon Goldwater, and
Mark Johnson. 2009b. A note on the implemen-
tation of hierarchical dirichlet processes. In Proc. of
ACL-IJCNLP, pages 337–340.
Peter E. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19:263–331.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of ACL, pages 263–270.
Josep Maria Crego, Franc¸ois Yvon, and Jos´e B.
Mari˜no. 2011. Ncode: an open source bilingual n-
gram SMT toolkit. Prague Bull. Math. Linguistics,
96:49–58.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proc. of ACL:HLT, pages
1045–1054.
Michel Galley and Christopher D. Manning. 2010.
Accurate non-hierarchical phrase-based translation.
In Proc. of NAACL, pages 966–974.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL, pages 961–968.
Mark Johnson and Sharon Goldwater. 2009. Improv-
ing nonparameteric bayesian inference: experiments
on unsupervised word segmentation with adaptor
grammars. In Proc. of HLT-NAACL, pages 317–325.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. ofHLT-
NAACL, pages 127–133.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of ACL.
Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous space translation models with
neural networks. In Proc. of NAACL, pages 39–48.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of COLING-ACL, pages 609–
616, July.
Frans J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19–51.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311–318.
Jim Pitman and Marc Yor. 1997. The two-parameter
poisson-dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25(2):855–
900.
Andreas Stolcke. 2002. SRILM: An extensible lan-
guage modeling toolkit. In Proc. of ICSLP.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M.
Blei. 2006. Hierarchical Dirichlet processes.
Journal of the American Statistical Association,
101(476):1566–1581.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proc. of ACL, pages 985–992.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proc. of ACL, pages 856–864.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proc. of COLING, pages 836–841.
</reference>
<page confidence="0.998263">
342
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.140458">
<title confidence="0.7506405">A Markov Model of Machine Translation Non-parametric Bayesian Inference</title>
<author confidence="0.977103">Feng</author>
<affiliation confidence="0.9833135">Department of Computer The University of</affiliation>
<address confidence="0.426151">Sheffield, United</address>
<abstract confidence="0.9860895">Most modern machine translation systems use phrase pairs as translation units, allowing for accurate modelling of phraseinternal translation and reordering. However phrase-based approaches are much less able to model sentence level effects between different phrase-pairs. We propose a new model to address this imbalance, based on a word-based Markov model of translation which generates target translations left-to-right. Our model encodes word and phrase level phenomena by conditioning translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A Gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. of ACLIJCNLP,</booktitle>
<pages>782--790</pages>
<contexts>
<context position="22404" citStr="Blunsom et al. (2009" startWordPosition="3847" endWordPosition="3850">target word take and change the aligned source word from prends to Je, then the items for which we need to decrement and increment the counts by one are shown in Table 1 and the posterior probability corresponding to the new alignment is the product of the hierarchical PYP probabilities of all increment items divided by the probability of the fertility of prends being single. Maintaining the current state of the hPYP as events are incremented and decremented is nontrivial and the naive approach requires significant book-keeping and has poor runtime behaviour. For this we adopt the approach of Blunsom et al. (2009b), who present a method for maintaining table counts without needing to record the table assignments for each translation decision. Briefly, this algorithm samples the table assignment during the increment and decrement operations, which is then used to maintain aggregate table statistics. This can be done efficiently and without the need for explicit table assignment tracking. 4.1 Hyperparameter Inference In our model, we treat all hyper-parameters {(ax, bx), x E (ξ, τ, e, φ)} as latent random variables rather than fixed parameters. This means our model is parameter free, and requires no use</context>
<context position="23770" citStr="Blunsom et al. (2009" startWordPosition="4061" endWordPosition="4064">e strength parameter, we employ a vague Gamma distribution bx — Gamma(10, 0.1). All restaurants in the same level share the same hyper-prior and the hyper-parameters for all levels are resampled using slice sampling (Johnson and Goldwater, 2009) every 10 iterations. 4.2 Parallel Implementation As mentioned above, the hierarchical PYP takes into consideration a rich history to evaluate the probabilities of translation decisions. But this leads to difficulties when applying the model to large data sets, particularly in terms of tracking the table and customer counts. We apply the technique from Blunsom et al. (2009a) of using multiple processors to perform approximate Gibbs sampling which they showed achieved equivalent performance to the exact Gibbs sampler. Each process performs sampling on a subset of the corpus using local counts, and communicates changes to these counts after each full iteration. All the count deltas are then aggregated by each process to refresh the counts at the end of each iteration. In this way each process uses slightly “out-of-date” counts, but can process the data independently of the other processes. We found that this approximation improved the runtime significantly with n</context>
<context position="35177" citStr="Blunsom et al. (2009" startWordPosition="5924" endWordPosition="5927"> model allows for rich dynamic backoff behaviour such that it can learn accurate models in both high and low data scenarios. 5.4 Arabic English translation Translation between Chinese and English is very difficult, particularly due to word order differences which are not handled well by phrase-based approaches. In contrast Arabic to English translation needs less reordering, and phrase-based models produce better translations. This translation task is a good test for the generality of our approach. Our Ar-En training data comprises several LDC corpora,6 using the same experimental setup as in Blunsom et al. (2009a). Overall there are 276k sentence pairs and 8.21m and 8.97m words in Arabic and English, respectively. We evaluate on the NIST test sets from 2003 and 2005, and the 2002 test set was used for MERT training. Table 5 shows the results. On all test sets our approach outperforms the baseline, and for the NIST03 test set the improvement is substantial, with a +0.74 BLEU improvement. In general the improvements are more modest than for the Chinese-English results above. We suggest that this is due to the structure of Arabic-English translation better suiting the modelling assumptions behind IBM mo</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009a. A Gibbs sampler for phrasal synchronous grammar induction. In Proc. of ACLIJCNLP, pages 782–790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>A note on the implementation of hierarchical dirichlet processes.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP,</booktitle>
<pages>337--340</pages>
<contexts>
<context position="22404" citStr="Blunsom et al. (2009" startWordPosition="3847" endWordPosition="3850">target word take and change the aligned source word from prends to Je, then the items for which we need to decrement and increment the counts by one are shown in Table 1 and the posterior probability corresponding to the new alignment is the product of the hierarchical PYP probabilities of all increment items divided by the probability of the fertility of prends being single. Maintaining the current state of the hPYP as events are incremented and decremented is nontrivial and the naive approach requires significant book-keeping and has poor runtime behaviour. For this we adopt the approach of Blunsom et al. (2009b), who present a method for maintaining table counts without needing to record the table assignments for each translation decision. Briefly, this algorithm samples the table assignment during the increment and decrement operations, which is then used to maintain aggregate table statistics. This can be done efficiently and without the need for explicit table assignment tracking. 4.1 Hyperparameter Inference In our model, we treat all hyper-parameters {(ax, bx), x E (ξ, τ, e, φ)} as latent random variables rather than fixed parameters. This means our model is parameter free, and requires no use</context>
<context position="23770" citStr="Blunsom et al. (2009" startWordPosition="4061" endWordPosition="4064">e strength parameter, we employ a vague Gamma distribution bx — Gamma(10, 0.1). All restaurants in the same level share the same hyper-prior and the hyper-parameters for all levels are resampled using slice sampling (Johnson and Goldwater, 2009) every 10 iterations. 4.2 Parallel Implementation As mentioned above, the hierarchical PYP takes into consideration a rich history to evaluate the probabilities of translation decisions. But this leads to difficulties when applying the model to large data sets, particularly in terms of tracking the table and customer counts. We apply the technique from Blunsom et al. (2009a) of using multiple processors to perform approximate Gibbs sampling which they showed achieved equivalent performance to the exact Gibbs sampler. Each process performs sampling on a subset of the corpus using local counts, and communicates changes to these counts after each full iteration. All the count deltas are then aggregated by each process to refresh the counts at the end of each iteration. In this way each process uses slightly “out-of-date” counts, but can process the data independently of the other processes. We found that this approximation improved the runtime significantly with n</context>
<context position="35177" citStr="Blunsom et al. (2009" startWordPosition="5924" endWordPosition="5927"> model allows for rich dynamic backoff behaviour such that it can learn accurate models in both high and low data scenarios. 5.4 Arabic English translation Translation between Chinese and English is very difficult, particularly due to word order differences which are not handled well by phrase-based approaches. In contrast Arabic to English translation needs less reordering, and phrase-based models produce better translations. This translation task is a good test for the generality of our approach. Our Ar-En training data comprises several LDC corpora,6 using the same experimental setup as in Blunsom et al. (2009a). Overall there are 276k sentence pairs and 8.21m and 8.97m words in Arabic and English, respectively. We evaluate on the NIST test sets from 2003 and 2005, and the 2002 test set was used for MERT training. Table 5 shows the results. On all test sets our approach outperforms the baseline, and for the NIST03 test set the improvement is substantial, with a +0.74 BLEU improvement. In general the improvements are more modest than for the Chinese-English results above. We suggest that this is due to the structure of Arabic-English translation better suiting the modelling assumptions behind IBM mo</context>
</contexts>
<marker>Blunsom, Cohn, Goldwater, Johnson, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Sharon Goldwater, and Mark Johnson. 2009b. A note on the implementation of hierarchical dirichlet processes. In Proc. of ACL-IJCNLP, pages 337–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter E Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2044" citStr="Brown et al., 1993" startWordPosition="297" endWordPosition="300">lation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on context. On one hand, the use of phrases can memorize local context and hence helps to generate better translation compared to word-based models (Brown et al., 1993; Och and Ney, 2003). On the other hand, this mechanism requires each phrase to be matched strictly and to be used as a whole, which precludes the use of discontinuous phrases and leads to poor generalisation to unseen data (where large phrases tend not to match). In this paper we propose a new model to drop the independence assumption, by instead modelling correlations between translation decisions, which we use to induce translation derivations from aligned sentences (akin to word alignment). We develop a Markov model over translation decisions, in which each decision is conditioned on previ</context>
<context position="4446" citStr="Brown et al., 1993" startWordPosition="660" endWordPosition="663">e 51st Annual Meeting of the Association for Computational Linguistics, pages 333–342, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tion, insertion, deletion and reordering components. We demonstrate our model on Chinese-English and Arabic-English translation datasets. The model produces uniformly better translations than those of a competitive phrase-based baseline, amounting to an improvement of up to 3.4 BLEU points absolute. 2 Related Work Word based models have a long history in machine translation, starting with the venerable IBM translation models (Brown et al., 1993) and the hidden Markov model (Vogel et al., 1996). These models are still in wide-spread use today, albeit only as a preprocessing step for inferring word level alignments from sentence-aligned parallel corpora. They combine a number of factors, including distortion and fertility, which have been shown to improve word-alignment and translation performance over simpler models. Our approach is similar to these works, as we also develop a word-based model, and explicitly consider similar translation decisions, alignment jumps and fertility. We extend these works in two important respects: 1) whil</context>
<context position="18257" citStr="Brown et al. (1993)" startWordPosition="3108" endWordPosition="3111">r the translation factors and their hierarchical priors. that a jump could not go outside the bounds of the source sentence. Accordingly we maintain separate distributions for each setting, and each has a different uniform base distribution parameterized according to the number of possible jump types. 3.3 Fertility For each target position, our Markov model may select a source word which has been covered, which means a source word may be linked to several target positions. Therefore, we introduce fertility to denote the number of target positions a source word is linked to in a sentence pair. Brown et al. (1993) have demonstrated the usefulness of fertility in probability estimation: IBM models 3– 5 exhibit large improvements over models 1–2. On these grounds, we include fertility to produce our advanced model, J pad(eI1,aI1|fJ1 )=pbs(eI1,aI1|fJ1 ) H p(φj|fjj−n) (2) j=1 where φj is the fertility of source word fj in the sentence pair &lt; fJ1 , eI1 &gt; and pbs is the basic model defined in Eq. 1. In order to avoid problems of data sparsity, we bin fertility into three types, a) zero, if φ = 0; b) single, if φ = 1; and c) multiple, if φ &gt; 1. We draw the fertility variables from a hierarchical PYP distribut</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter E. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1347" citStr="Chiang, 2005" startWordPosition="188" endWordPosition="189"> translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 1 Introduction Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based (Koehn et al., 2003) and syntax-based approaches (Chiang, 2005; Galley et al., 2006; Liu et al., 2006). These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on context. On one hand, the use of phrases can memorize local context</context>
<context position="6953" citStr="Chiang, 2005" startWordPosition="1060" endWordPosition="1061">e source words as part of the conditioning context. In contrast to these approaches which primarily address the decoding problem, we focus on the learning problem of inferring alignments from parallel sentences. Additionally, we develop a full generative model using a Bayesian prior, and incorporate additional factors besides lexical items, namely jumps in the source and word fertility. Another aspect of this paper is the implicit support for phrase-pairs that are discontinous in the source language. This idea has been developed explicitly in a number of previous approaches, in grammar based (Chiang, 2005) and phrase-based systems (Galley and Manning, 2010). The latter is most similar to this paper, and shows that discontinuous phrases compliment standard contiguous phrases, improving expressiveness and translation performance. Unlike their work, here we develop a complimentary approach by constructing a generative model which can induce these rich rules directly from sentence-aligned corpora. 3 Model Given a source sentence, our model infers a latent derivation which produces a target translation and meanwhile gives a word alignment between the source and the target. We consider a process in w</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of ACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep Maria Crego</author>
<author>Franc¸ois Yvon</author>
<author>Jos´e B Mari˜no</author>
</authors>
<title>Ncode: an open source bilingual ngram SMT toolkit.</title>
<date>2011</date>
<journal>Prague Bull. Math. Linguistics,</journal>
<pages>96--49</pages>
<marker>Crego, Yvon, Mari˜no, 2011</marker>
<rawString>Josep Maria Crego, Franc¸ois Yvon, and Jos´e B. Mari˜no. 2011. Ncode: an open source bilingual ngram SMT toolkit. Prague Bull. Math. Linguistics, 96:49–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
</authors>
<title>A joint sequence translation model with integrated reordering.</title>
<date>2011</date>
<booktitle>In Proc. of ACL:HLT,</booktitle>
<pages>1045--1054</pages>
<contexts>
<context position="6222" citStr="Durrani et al. (2011)" startWordPosition="944" endWordPosition="947">e translation. Vaswani et al. (2011) propose a rule Markov model for a tree-to-string model which models correlations between pairs of mininal rules, and use Kneser-Ney smoothing to alleviate the problems of data sparsity. Similarly, Crego et al. (2011) develop a bilingual language model which incorporates words in the source and target languages to predict the next unit, which they use as a feature in a translation system. This line of work was extended by Le et al. (2012) who develop a novel estimation algorithm based around discriminative projection into continuous spaces. Also relevant is Durrani et al. (2011), who present a sequence model of translation including reordering. Our work also uses bilingual information, using the source words as part of the conditioning context. In contrast to these approaches which primarily address the decoding problem, we focus on the learning problem of inferring alignments from parallel sentences. Additionally, we develop a full generative model using a Bayesian prior, and incorporate additional factors besides lexical items, namely jumps in the source and word fertility. Another aspect of this paper is the implicit support for phrase-pairs that are discontinous </context>
</contexts>
<marker>Durrani, Schmid, Fraser, 2011</marker>
<rawString>Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrated reordering. In Proc. of ACL:HLT, pages 1045–1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate non-hierarchical phrase-based translation.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>966--974</pages>
<contexts>
<context position="7005" citStr="Galley and Manning, 2010" startWordPosition="1065" endWordPosition="1068">ng context. In contrast to these approaches which primarily address the decoding problem, we focus on the learning problem of inferring alignments from parallel sentences. Additionally, we develop a full generative model using a Bayesian prior, and incorporate additional factors besides lexical items, namely jumps in the source and word fertility. Another aspect of this paper is the implicit support for phrase-pairs that are discontinous in the source language. This idea has been developed explicitly in a number of previous approaches, in grammar based (Chiang, 2005) and phrase-based systems (Galley and Manning, 2010). The latter is most similar to this paper, and shows that discontinuous phrases compliment standard contiguous phrases, improving expressiveness and translation performance. Unlike their work, here we develop a complimentary approach by constructing a generative model which can induce these rich rules directly from sentence-aligned corpora. 3 Model Given a source sentence, our model infers a latent derivation which produces a target translation and meanwhile gives a word alignment between the source and the target. We consider a process in which the target string is generated using a left-to-</context>
</contexts>
<marker>Galley, Manning, 2010</marker>
<rawString>Michel Galley and Christopher D. Manning. 2010. Accurate non-hierarchical phrase-based translation. In Proc. of NAACL, pages 966–974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="1368" citStr="Galley et al., 2006" startWordPosition="190" endWordPosition="193">ecisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 1 Introduction Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based (Koehn et al., 2003) and syntax-based approaches (Chiang, 2005; Galley et al., 2006; Liu et al., 2006). These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on context. On one hand, the use of phrases can memorize local context and hence helps to g</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. of ACL, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>317--325</pages>
<contexts>
<context position="23395" citStr="Johnson and Goldwater, 2009" startWordPosition="4003" endWordPosition="4006">signment tracking. 4.1 Hyperparameter Inference In our model, we treat all hyper-parameters {(ax, bx), x E (ξ, τ, e, φ)} as latent random variables rather than fixed parameters. This means our model is parameter free, and requires no user intervention when adapting to different data sets. For the discount parameter, we employ a uniform Beta distribution ax — Beta(1,1) while for the strength parameter, we employ a vague Gamma distribution bx — Gamma(10, 0.1). All restaurants in the same level share the same hyper-prior and the hyper-parameters for all levels are resampled using slice sampling (Johnson and Goldwater, 2009) every 10 iterations. 4.2 Parallel Implementation As mentioned above, the hierarchical PYP takes into consideration a rich history to evaluate the probabilities of translation decisions. But this leads to difficulties when applying the model to large data sets, particularly in terms of tracking the table and customer counts. We apply the technique from Blunsom et al. (2009a) of using multiple processors to perform approximate Gibbs sampling which they showed achieved equivalent performance to the exact Gibbs sampler. Each process performs sampling on a subset of the corpus using local counts, </context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proc. of HLT-NAACL, pages 317–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. ofHLTNAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1305" citStr="Koehn et al., 2003" startWordPosition="181" endWordPosition="184">s word and phrase level phenomena by conditioning translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 1 Introduction Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based (Koehn et al., 2003) and syntax-based approaches (Chiang, 2005; Galley et al., 2006; Liu et al., 2006). These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on context. On one hand, the</context>
<context position="7720" citStr="Koehn et al., 2003" startWordPosition="1176" endWordPosition="1179">dard contiguous phrases, improving expressiveness and translation performance. Unlike their work, here we develop a complimentary approach by constructing a generative model which can induce these rich rules directly from sentence-aligned corpora. 3 Model Given a source sentence, our model infers a latent derivation which produces a target translation and meanwhile gives a word alignment between the source and the target. We consider a process in which the target string is generated using a left-to-right order, similar to the decoding strategy used by phrase-based machine translation systems (Koehn et al., 2003). During this process we maintain a position in the source sentence, which can jump around to allow for different sentence ordering in the target vs. source languages. In contrast to phrase-based models, we use words as our basic translation unit, rather than multi-word phrases. Furthermore, we decompose the decisions involved in generating each target word to a number of separate factors, where each factor is modelled separately and conditioned on a rich history of recent translation decisions. 3.1 Markov Translation Our model generates target translation left-toright word by word. The genera</context>
<context position="9399" citStr="Koehn et al., 2003" startWordPosition="1481" endWordPosition="1484">that one no stay le one 6 Je le prends I ’ll take that one yes Figure 1: Translation agenda of Je le prends -+ I ’ll take that one. if ξi = false then Select a source word to jump to Emit a target word for the source word end if i i i + 1 end while In the generation of each target word, our model includes three separate factors: the binary finish decision, a jump decision to move to a different source word, and emission which translates or otherwise inserts a word in the target string. This generative process resembles the sequence of translation decisions considered by a standard MT decoder (Koehn et al., 2003), but note that our approach differs in that there is no constraint that all words are translated exactly once. Instead source words can be skipped or repeatedly translated. This makes the approach more suitable for learning alignments, e.g., to account for word fertilities (see §3.3), while also permitting inference using Gibbs sampling (§4). More formally, we can express our probabilistic model as p(τi|fi−1 ai−n, ei−1 i−n) p(ei|τi, fiai−n, ei−1 i−n) (1) where ξi is the finish decision for target position i, τi is the jump decision to source word fai and fiai−n is the source words for target </context>
<context position="26067" citStr="Koehn et al., 2003" startWordPosition="4438" endWordPosition="4441">est how the model compares to a GIZA++ baseline, quantifies the effect of each factor in the probabilistic model (i.e., jump, fertility), and the effect of different initialisations of the sampler. We present results on translation quality and word alignment. 5.1 Data Setup The Markov order of our model in all experiments was set to n = 2, as shown in Equation 2. For each data set, Gibbs sampling was performed on the training set in each direction (source-to-target and target-to-source), initialized using GIZA++.4 We used the grow heuristic to combine the GIZA++ alignments in both directions (Koehn et al., 2003), which we then intersect with the predictions of GIZA++ in the relevant translation direction. This initialisation setup gave the best results (we compare other initialisations in §5.2). The two Gibbs samplers were “burned in” for the first 1000 iterations, after which we ran a further 500 iterations selecting every 50th sample. A phrase table was constructed using these 10 sets of multiple alignments after combining each pair of directional alignments using the grow-diag-final heuristic. Using multiple samples in this way constitutes Monte Carlo averaging, which provides a better estimate of</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. ofHLTNAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch Mayne</author>
<author>Christopher Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="26913" citStr="Koehn et al., 2007" startWordPosition="4570" endWordPosition="4573"> for the first 1000 iterations, after which we ran a further 500 iterations selecting every 50th sample. A phrase table was constructed using these 10 sets of multiple alignments after combining each pair of directional alignments using the grow-diag-final heuristic. Using multiple samples in this way constitutes Monte Carlo averaging, which provides a better estimate of uncertainty cf. using a single sample.5 The alignment used for the baseline results was produced by combining bidirectional GIZA++ alignments using the grow-diag-final heuristic. We used the Moses machine translation decoder (Koehn et al., 2007), using the default features and decoding settings. We compared the performance of Moses using the alignment produced by our model and the baseline alignment, evaluating translation quality using BLEU (Papineni et al., 2002) with case-insensitive n-gram matching with n = 4. We used minimum error rate training (Och, 2003) to tune the feature weights to maximise the BLEU score on the development set. 5.2 IWSLT Corpus The first experiments are on the IWSLT data set for Chinese-English translation. The training data consists of 44k sentences from the tourism and travel domain. For the development </context>
</contexts>
<marker>Koehn, Hoang, Mayne, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne, Christopher Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>39--48</pages>
<contexts>
<context position="6079" citStr="Le et al. (2012)" startWordPosition="922" endWordPosition="925">ich expressiveness but can still generalize well to unseen data. More recently, a number of authors have proposed Markov models for machine translation. Vaswani et al. (2011) propose a rule Markov model for a tree-to-string model which models correlations between pairs of mininal rules, and use Kneser-Ney smoothing to alleviate the problems of data sparsity. Similarly, Crego et al. (2011) develop a bilingual language model which incorporates words in the source and target languages to predict the next unit, which they use as a feature in a translation system. This line of work was extended by Le et al. (2012) who develop a novel estimation algorithm based around discriminative projection into continuous spaces. Also relevant is Durrani et al. (2011), who present a sequence model of translation including reordering. Our work also uses bilingual information, using the source words as part of the conditioning context. In contrast to these approaches which primarily address the decoding problem, we focus on the learning problem of inferring alignments from parallel sentences. Additionally, we develop a full generative model using a Bayesian prior, and incorporate additional factors besides lexical ite</context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous space translation models with neural networks. In Proc. of NAACL, pages 39–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1387" citStr="Liu et al., 2006" startWordPosition="194" endWordPosition="197">decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 1 Introduction Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based (Koehn et al., 2003) and syntax-based approaches (Chiang, 2005; Galley et al., 2006; Liu et al., 2006). These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on context. On one hand, the use of phrases can memorize local context and hence helps to generate better tran</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proc. of COLING-ACL, pages 609– 616, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="2064" citStr="Och and Ney, 2003" startWordPosition="301" endWordPosition="304"> of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on context. On one hand, the use of phrases can memorize local context and hence helps to generate better translation compared to word-based models (Brown et al., 1993; Och and Ney, 2003). On the other hand, this mechanism requires each phrase to be matched strictly and to be used as a whole, which precludes the use of discontinuous phrases and leads to poor generalisation to unseen data (where large phrases tend not to match). In this paper we propose a new model to drop the independence assumption, by instead modelling correlations between translation decisions, which we use to induce translation derivations from aligned sentences (akin to word alignment). We develop a Markov model over translation decisions, in which each decision is conditioned on previous n most recent de</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Frans J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="27235" citStr="Och, 2003" startWordPosition="4622" endWordPosition="4623">aging, which provides a better estimate of uncertainty cf. using a single sample.5 The alignment used for the baseline results was produced by combining bidirectional GIZA++ alignments using the grow-diag-final heuristic. We used the Moses machine translation decoder (Koehn et al., 2007), using the default features and decoding settings. We compared the performance of Moses using the alignment produced by our model and the baseline alignment, evaluating translation quality using BLEU (Papineni et al., 2002) with case-insensitive n-gram matching with n = 4. We used minimum error rate training (Och, 2003) to tune the feature weights to maximise the BLEU score on the development set. 5.2 IWSLT Corpus The first experiments are on the IWSLT data set for Chinese-English translation. The training data consists of 44k sentences from the tourism and travel domain. For the development set we use both ASR devset 1 and 2 from IWSLT 2005, and 4All GIZA++ alignments used in our experiments were produced by IBM model4. 5The effect on translation scores is modest, roughly amounting to +0.2 BLEU versus using a single sample. System Dev IWSLT05 baseline 45.78 49.98 Markov+fs+e 49.13 51.54 Markov+fs+e+j 49.68 </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Frans J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="27137" citStr="Papineni et al., 2002" startWordPosition="4604" endWordPosition="4607">lignments using the grow-diag-final heuristic. Using multiple samples in this way constitutes Monte Carlo averaging, which provides a better estimate of uncertainty cf. using a single sample.5 The alignment used for the baseline results was produced by combining bidirectional GIZA++ alignments using the grow-diag-final heuristic. We used the Moses machine translation decoder (Koehn et al., 2007), using the default features and decoding settings. We compared the performance of Moses using the alignment produced by our model and the baseline alignment, evaluating translation quality using BLEU (Papineni et al., 2002) with case-insensitive n-gram matching with n = 4. We used minimum error rate training (Och, 2003) to tune the feature weights to maximise the BLEU score on the development set. 5.2 IWSLT Corpus The first experiments are on the IWSLT data set for Chinese-English translation. The training data consists of 44k sentences from the tourism and travel domain. For the development set we use both ASR devset 1 and 2 from IWSLT 2005, and 4All GIZA++ alignments used in our experiments were produced by IBM model4. 5The effect on translation scores is modest, roughly amounting to +0.2 BLEU versus using a s</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proc. of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
<author>Marc Yor</author>
</authors>
<title>The two-parameter poisson-dirichlet distribution derived from a stable subordinator. The Annals of Probability,</title>
<date>1997</date>
<volume>25</volume>
<issue>2</issue>
<pages>900</pages>
<contexts>
<context position="12540" citStr="Pitman and Yor, 1997" startWordPosition="2033" endWordPosition="2036"> of less frequent events. We consider a hierarchical PYP, where a sequence of chained PYP 1For a target position aligned to null, we denote its source word as null and set its aligned source position as that of the previous target word that is aligned to non-null. pbs(eI 1, aI 1|fJ 1 ) = p(ξi|fi−1 ai−n,ei−1 i−n) I+1 i=1 I X i=1 I X i=1 335 priors allow backoff from larger to smaller contexts such that our model can learn rich contextual models for known (large) contexts while also still being able to generalize well to unseen contexts (using smaller histories). 3.2.1 Pitman-Yor Process A PYP (Pitman and Yor, 1997) is defined by its discount parameter 0 &lt; a &lt; 1, strength parameter b &gt; −a and base distribution G0. For a distribution drawn from a PYP, G — PYP(a, b, G0), marginalising out G leads to a simple distribution which can be described using a variant of the Chinese Restaurant Process (CRP). In this analogy we imagine a restaurant has an infinite number of tables and each table can accommodate an infinite number of customers. Each customer (a sample from G) walks in one at a time and seats themselves at a table. Finally each table is served a communal dish (a draw from G0), which is served to each </context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>Jim Pitman and Marc Yor. 1997. The two-parameter poisson-dirichlet distribution derived from a stable subordinator. The Annals of Probability, 25(2):855– 900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM: An extensible language modeling toolkit.</title>
<date>2002</date>
<journal>Journal of the American Statistical Association,</journal>
<booktitle>In Proc. of</booktitle>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="28145" citStr="Stolcke, 2002" startWordPosition="4771" endWordPosition="4772">R devset 1 and 2 from IWSLT 2005, and 4All GIZA++ alignments used in our experiments were produced by IBM model4. 5The effect on translation scores is modest, roughly amounting to +0.2 BLEU versus using a single sample. System Dev IWSLT05 baseline 45.78 49.98 Markov+fs+e 49.13 51.54 Markov+fs+e+j 49.68 52.55 Markov+fs+e+j+ft 51.32 53.41 Table 2: Impact of adding factors to our Markov model, showing BLEU scores on IWSLT. Key: fs– finish e–emission j–jump ft–fertility. for the test set we use the IWSLT 2005 test set. The language model is a 3-gram language model trained using the SRILM toolkit (Stolcke, 2002) on the English side of the training data. Because the data set is small, we performed Gibbs sampling on a single processor. First we check the effect of the model factors jump and fertility. Both emission and finish factors are indispensable to the generative translation process, and consequently these two factors are included in all runs. Table 2 shows translation result for various models, including a baseline and our Markov model with different combinations of factors. Note that even the simplest Markov model far outperforms the GIZA++ baseline (+1.5 BLEU) despite the baseline (IBM model 4</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM: An extensible language modeling toolkit. In Proc. of ICSLP. Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>985--992</pages>
<contexts>
<context position="2794" citStr="Teh, 2006" startWordPosition="416" endWordPosition="417">s the use of discontinuous phrases and leads to poor generalisation to unseen data (where large phrases tend not to match). In this paper we propose a new model to drop the independence assumption, by instead modelling correlations between translation decisions, which we use to induce translation derivations from aligned sentences (akin to word alignment). We develop a Markov model over translation decisions, in which each decision is conditioned on previous n most recent decisions. Our approach employs a sophisticated Bayesian non-parametric prior, namely the hierarchical Pitman-Yor Process (Teh, 2006; Teh et al., 2006) to represent backoff from larger to smaller contexts. As a result, we need only use very simple translation units – primarily single words, but can still describe complex multi-word units through correlations between their component translation decisions. We further decompose the process of generating each target word into component factors: finishing the translating, jumping elsewhere in the source, emitting a target word and deciding the fertility of the source words. Overall our model has the following features: 1. enabling model parameters to be shared between similar t</context>
<context position="14486" citStr="Teh (2006)" startWordPosition="2435" endWordPosition="2436">le, a dish h is served to his table by the probability of G0(h), otherwise, he can only share with others the dish having been served to his table.2 Overall, the probability of the customer being served a dish h is c− oh p(on = h|t−n, o−n) = n − 1 + b (aK− + b) + G0 (h) n − 1 + b where on is the dish served to the customer n, o−n is the dish accommodation of previous n − 1 customers, c−oh is the number of customers who are 2We also say the customer is served with this dish. served with the dish h in o−n and K−h is the number of tables served with the dish h in t−n. The hierarchical PYP (hPYP; Teh (2006)) is an extension of the PYP in which the base distribution G0 is itself a PYP distribution. This parent (base) distribution can itself have a PYP as a base distribution, giving rise to hierarchies of arbitrary depth. Like the PYP, inference under the hPYP can be also described in terms of CRP whereby each table in one restaurant corresponds to a dish in the next deeper level, and is said to share the same dish. Whenever an empty table is seated in one level, a customer must enter the restaurant in the next deeper level and find a table to sit. This process continues until the customer is assi</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proc. of ACL, pages 985–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Rule markov models for fast tree-tostring translation.</title>
<date>2011</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>856--864</pages>
<contexts>
<context position="5637" citStr="Vaswani et al. (2011)" startWordPosition="847" endWordPosition="850">wo important respects: 1) while they assume a simple parameterisation by making iid assumptions about each translation factor, we instead allow for rich correlations by modelling sequences of translation decisions; and 2) we develop our model in the Bayesian framework, using a hierarchical PitmanYor Process prior with rich backoff semantics between high and lower order sequences of translation decisions. Together this results in a model with rich expressiveness but can still generalize well to unseen data. More recently, a number of authors have proposed Markov models for machine translation. Vaswani et al. (2011) propose a rule Markov model for a tree-to-string model which models correlations between pairs of mininal rules, and use Kneser-Ney smoothing to alleviate the problems of data sparsity. Similarly, Crego et al. (2011) develop a bilingual language model which incorporates words in the source and target languages to predict the next unit, which they use as a feature in a translation system. This line of work was extended by Le et al. (2012) who develop a novel estimation algorithm based around discriminative projection into continuous spaces. Also relevant is Durrani et al. (2011), who present a</context>
</contexts>
<marker>Vaswani, Mi, Huang, Chiang, 2011</marker>
<rawString>Ashish Vaswani, Haitao Mi, Liang Huang, and David Chiang. 2011. Rule markov models for fast tree-tostring translation. In Proc. of ACL, pages 856–864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="4495" citStr="Vogel et al., 1996" startWordPosition="670" endWordPosition="673">utational Linguistics, pages 333–342, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tion, insertion, deletion and reordering components. We demonstrate our model on Chinese-English and Arabic-English translation datasets. The model produces uniformly better translations than those of a competitive phrase-based baseline, amounting to an improvement of up to 3.4 BLEU points absolute. 2 Related Work Word based models have a long history in machine translation, starting with the venerable IBM translation models (Brown et al., 1993) and the hidden Markov model (Vogel et al., 1996). These models are still in wide-spread use today, albeit only as a preprocessing step for inferring word level alignments from sentence-aligned parallel corpora. They combine a number of factors, including distortion and fertility, which have been shown to improve word-alignment and translation performance over simpler models. Our approach is similar to these works, as we also develop a word-based model, and explicitly consider similar translation decisions, alignment jumps and fertility. We extend these works in two important respects: 1) while they assume a simple parameterisation by making</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proc. of COLING, pages 836–841.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>