<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.049577">
<title confidence="0.9595765">
Correlation between ROUGE and Human Evaluation of Extractive Meeting
Summaries
</title>
<author confidence="0.998747">
Feifan Liu, Yang Liu
</author>
<affiliation confidence="0.997396">
The University of Texas at Dallas
</affiliation>
<address confidence="0.588403">
Richardson, TX 75080, USA
</address>
<email confidence="0.998781">
ffliu,yangl@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.99563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999162066666667">
Automatic summarization evaluation is critical to
the development of summarization systems. While
ROUGE has been shown to correlate well with hu-
man evaluation for content match in text summa-
rization, there are many characteristics in multiparty
meeting domain, which may pose potential prob-
lems to ROUGE. In this paper, we carefully exam-
ine how well the ROUGE scores correlate with hu-
man evaluation for extractive meeting summariza-
tion. Our experiments show that generally the cor-
relation is rather low, but a significantly better cor-
relation can be obtained by accounting for several
unique meeting characteristics, such as disfluencies
and speaker information, especially when evaluating
system-generated summaries.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913222222222">
Meeting summarization has drawn an increasing atten-
tion recently; therefore a study on the automatic evalu-
ation metrics for this task is timely. Automatic evalua-
tion helps to advance system development and avoids the
labor-intensive and potentially inconsistent human eval-
uation. ROUGE (Lin, 2004) has been widely used for
summarization evaluation. In the news article domain,
ROUGE scores have been shown to be generally highly
correlated with human evaluation in content match (Lin,
2004). However, there are many differences between
written texts (e.g., news wire) and spoken documents, es-
pecially in the meeting domain, for example, the pres-
ence of disfluencies and multiple speakers, and the lack
of structure in spontaneous utterances. The question of
whether ROUGE is a good metric for meeting summa-
rization is unclear. (Murray et al., 2005) have reported
that ROUGE-1 (unigram match) scores have low correla-
tion with human evaluation in meetings.
In this paper we investigate the correlation between
ROUGE and human evaluation of extractive meeting
summaries and focus on two issues specific to the meet-
ing domain: disfluencies and multiple speakers. Both
human and system generated summaries are used. Our
analysis shows that by integrating meeting characteristics
into ROUGE settings, better correlation can be achieved
between the ROUGE scores and human evaluation based
on Spearman’s rho in the meeting domain.
</bodyText>
<sectionHeader confidence="0.99966" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999797">
Automatic summarization evaluation can be broadly clas-
sified into two categories (Jones and Galliers, 1996): in-
trinsic and extrinsic evaluation. Intrinsic evaluation, such
as relative utility based metric proposed in (Radev et al.,
2004), assesses a summarization system in itself (for ex-
ample, informativeness, redundancy, and coherence). Ex-
trinsic evaluation (Mani et al., 1998) tests the effective-
ness of a summarization system on other tasks. In this
study, we concentrate on the automatic intrinsic summa-
rization evaluation. It has been extensively studied in
text summarization. Different approaches have been pro-
posed to measure matches using words or more mean-
ingful semantic units, for example, ROUGE (Lin, 2004),
factoid analysis (Teufel and Halteren, 2004), pyramid
method (Nenkova and Passonneau, 2004), and Basic El-
ement (BE) (Hovy et al., 2006).
With the increasing recent research of summarization
moving into speech, especially meeting recordings, is-
sues related to spoken language are yet to be explored
for their impact on the evaluation metrics. Inspired by
automatic speech recognition (ASR) evaluation, (Hori et
al., 2003) proposed the summarization accuracy metric
(SumACCY) based on a word network created by merg-
ing manual summaries. However (Zhu and Penn, 2005)
found a statistically significant difference between the
ASR-inspired metrics and those taken from text summa-
rization (e.g., RU, ROUGE) on a subset of the Switch-
board data. ROUGE has been used in meeting summa-
rization evaluation (Murray et al., 2005; Galley, 2006),
yet the question remained whether ROUGE is a good
metric for the meeting domain. (Murray et al., 2005)
showed low correlation of ROUGE and human evalua-
tion in meeting summarization evaluation; however, they
</bodyText>
<page confidence="0.995589">
201
</page>
<reference confidence="0.2240955">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 201–204,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.998831428571429">
simply used ROUGE as is and did not take into account
the meeting characteristics during evaluation.
In this paper, we ask the question of whether ROUGE
correlates with human evaluation of extractive meeting
summaries and whether we can modify ROUGE to ac-
count for the meeting style for a better correlation with
human evaluation.
</bodyText>
<sectionHeader confidence="0.994216" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.993399">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999949838709678">
We used the ICSI meeting data (Janin et al., 2003) that
contains naturally-occurring research meetings. All the
meetings have been transcribed and annotated with dialog
acts (DA) (Shriberg et al., 2004), topics, and extractive
summaries (Murray et al., 2005).
For this study, we used the same 6 test meetings as in
(Murray et al., 2005; Galley, 2006). Each meeting al-
ready has 3 human summaries from 3 common annota-
tors. We recruited another 3 human subjects to generate
3 more human summaries, in order to create more data
points for a reliable analysis. The Kappa statistics for
those 6 different annotators varies from 0.11 to 0.35 for
different meetings. The human summaries have different
length, containing around 6.5% of the selected DAs and
13.5% of the words respectively. We used four different
system summaries for each of the 6 meetings: one based
on the MMR method in MEAD (Carbonell and Gold-
stein, 1998; et al., 2003), the other three are the system
output from (Galley, 2006; Murray et al., 2005; Xie and
Liu, 2008). All the system generated summaries contain
around 5% of the DAs and 16% of the words of the entire
meeting. Thus, in total we have 36 human summaries and
24 system summaries on the 6 test meetings, on which
the correlation between ROUGE and human evaluation
is calculated and investigated.
All the experiments in this paper are based on human
transcriptions, with a central interest on whether some
characteristics of the meeting recordings affect the corre-
lation between ROUGE and human evaluations, without
the effect from speech recognition or automatic sentence
segmentation errors.
</bodyText>
<subsectionHeader confidence="0.998317">
3.2 Automatic ROUGE Evaluation
</subsectionHeader>
<bodyText confidence="0.999954833333333">
ROUGE (Lin, 2004) measures the n-gram match between
system generated summaries and human summaries. In
most of this study, we used the same options in ROUGE
as in the DUC summarization evaluation (NIST, 2007),
and modify the input to ROUGE to account for the fol-
lowing two phenomena.
</bodyText>
<listItem confidence="0.962748">
• Disfluencies
</listItem>
<bodyText confidence="0.999629421052632">
Meetings contain spontaneous speech with many
disfluencies, such as filled pauses (uh, um), dis-
course markers (e.g., I mean, you know), repetitions,
corrections, and incomplete sentences. There have
been efforts on the study of the impact of disfluen-
cies on summarization techniques (Liu et al., 2007;
Zhu and Penn, 2006) and human readability (Jones
et al., 2003). However, it is not clear whether dis-
fluencies impact automatic evaluation of extractive
meeting summarization.
Since we use extractive summarization, summary
sentences may contain difluencies. We hand anno-
tated the transcripts for the 6 meetings and marked
the disfluencies such that we can remove them to
obtain cleaned up sentences for those selected sum-
mary sentences. To study the impact of disfluencies,
we run ROUGE using two different inputs: sum-
maries based on the original transcription, and the
summaries with disfluencies removed.
</bodyText>
<listItem confidence="0.991529">
• Speaker information
</listItem>
<bodyText confidence="0.9998214">
The existence of multiple speakers in meetings
raises questions about the evaluation method. (Gal-
ley, 2006) considered some location constrains in
meeting summarization evaluation, which utilizes
speaker information to some extent. In this study
we use the data in separate channels for each speaker
and thus have the speaker information available for
each sentence. We associate the speaker ID with
each word, treat them together as a new ‘word’ in
the input to ROUGE.
</bodyText>
<subsectionHeader confidence="0.998311">
3.3 Human Evaluation
</subsectionHeader>
<bodyText confidence="0.999908333333333">
Five human subjects (all undergraduate students in Com-
puter Science) participated in human evaluation. In to-
tal, there are 20 different summaries for each of the 6
test meetings: 6 human-generated, 4 system-generated,
and their corresponding ones with disfluencies removed.
We assigned 4 summaries with different configurations to
each human subject: human vs. system generated sum-
maries, with or without disfluencies. Each human evalu-
ated 24 summaries in total, for the 6 test meetings.
For each summary, the human subjects were asked to
rate the following statements using a scale of 1-5 accord-
ing to the extent of their agreement with them.
</bodyText>
<listItem confidence="0.998703368421053">
• S1: The summary reflects the discussion flow in the meet-
ing very well.
• S2: Almost all the important topic points of the meeting
are represented.
• S3: Most of the sentences in the summary are relevant to
the original meeting.
• S4: The information in the summary is not redundant.
• S5: The relationship between the importance of each topic
in the meeting and the amount of summary space given to
that topic seems appropriate.
• S6: The relationship between the role of each speaker and
the amount of summary speech selected for that speaker
seems appropriate.
• S7: Some sentences in the summary convey the same
meaning.
• S8: Some sentences are not necessary (e.g., in terms of
importance) to be included in the summary.
• S9: The summary is helpful to someone who wants to
know what are discussed in the meeting.
</listItem>
<page confidence="0.994402">
202
</page>
<bodyText confidence="0.999640833333333">
These statements are an extension of those used in
(Murray et al., 2005) for human evaluation of meeting
summaries. The additional ones we added were designed
to account for the discussion flow in the meetings. Some
of the statements above are used to measure similar as-
pects, but from different perspectives, such as S5 and S6,
S4 and S7. This may reduce some accidental noise in hu-
man evaluation. We grouped these statements into 4 cat-
egories: Informative Structure (IS): S1, S5 and S6; Infor-
mative Coverage (IC): S2 and S9; Informative Relevance
(IRV): S3 and S8; and Informative Redundancy (IRD):
S4 and S7.
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.995216">
4.1 Correlation between Human Evaluation and
Original ROUGE Score
</subsectionHeader>
<bodyText confidence="0.999793142857143">
Similar to (Murray et al., 2005), we also use Spearman’s
rank coefficient (rho) to investigate the correlation be-
tween ROUGE and human evaluation. We have 36 hu-
man summaries and 24 system summaries for the 6 meet-
ings in our study. For each of the human summaries,
the ROUGE scores are generated using the other 5 hu-
man summaries as references. For system generated sum-
maries, we calculate the ROUGE score using 5 human
references, and then obtain the average from 6 such se-
tups. The correlation results are presented in Table 1.
In addition to the overall average for human evaluation
(H AVG), we calculated the average score for each evalu-
ation category (see Section 3.3). For ROUGE evaluation,
we chose the F-measure for R-1 (unigram) and R-SU4
(skip-bigram with maximum gap length of 4), which is
based on our observation that other scores in ROUGE are
always highly correlated (rho&gt;0.9) to either of them for
this task. We compute the correlation separately for the
human and system summaries in order to avoid the im-
pact due to the inherent difference between the two dif-
ferent summaries.
</bodyText>
<table confidence="0.999889">
Correlation on Human Summaries
H AVG HIS HIC H IRV H IRD
R-1 0.09 0.22 0.21 0.03 -0.20
R-SU4 0.18 0.33 0.38 0.04 -0.30
Correlation on System Summaries
R-1 -0.07 -0.02 -0.17 -0.27 -0.02
R-SU4 0.08 0.05 0.01 -0.15 0.14
</table>
<tableCaption confidence="0.959941">
Table 1: Spearman’s rho between human evaluation (H) and
ROUGE (R) with basic setting.
</tableCaption>
<bodyText confidence="0.999947214285714">
We can see that R-SU4 obtains a higher correlation
with human evaluation than R-1 on the whole, but still
very low, which is consistent with the previous conclu-
sion from (Murray et al., 2005). Among the four cat-
egories, better correlation is achieved for information
structure (IS) and information coverage (IC) compared
to the other two categories. This is consistent with what
ROUGE is designed for, “recall oriented understudy gist-
ing evaluation” — we expect it to model IS and IC well
by ngram and skip-bigram matching but not relevancy
(IRV) and redundancy (IRD) effectively. In addition, we
found low correlation on system generated summaries,
suggesting it is more challenging to evaluate those sum-
maries both by humans and the automatic metrics.
</bodyText>
<subsectionHeader confidence="0.99959">
4.2 Impacts of Disfluencies on Correlation
</subsectionHeader>
<bodyText confidence="0.9999698125">
Table 2 shows the correlation results between ROUGE
(R-SU4) and human evaluation on the original and
cleaned up summaries respectively. For human sum-
maries, after removing disfluencies, the correlation be-
tween ROUGE and human evaluation improves on the
whole, but degrades on information structure (IS) and in-
formation coverage (IC) categories. However, for sys-
tem summaries, there is a significant gain of correlation
on those two evaluation categories, even though no im-
provement on the overall average score. Our hypothesis
for this is that removing disfluencies helps remove the
noise in the system generated summaries and make them
more easily to be evaluated by human and machines. In
contrast, the human created summaries have better qual-
ity in terms of the information content and may not suffer
as much from the disfluencies contained in the summary.
</bodyText>
<table confidence="0.999802">
Correlation on Human Summaries
H AVG HIS HIC H IRV H IRD
Original 0.18 0.33 0.38 0.04 -0.30
Disfluencies 0.21 0.21 0.31 0.19 -0.16
removed
Correlation on System Summaries
Original 0.08 0.05 0.01 -0.15 0.14)
Disfluencies 0.08 0.22 0.19 -0.02 -0.07
removed
</table>
<tableCaption confidence="0.9981565">
Table 2: Effect of disfluencies on the correlation between R-
SU4 and human evaluation.
</tableCaption>
<subsectionHeader confidence="0.999892">
4.3 Incorporating Speaker Information
</subsectionHeader>
<bodyText confidence="0.999972235294118">
We further incorporated speaker information in ROUGE
setting using the summaries with disfluencies removed.
Table 3 presents the resulting correlation values between
ROUGE SU4 score and human evaluation. For human
summaries, adding speaker information slightly degraded
the correlation, but it is still better compared to using
the original transcripts (results in Table 1). For the sys-
tem summaries, the overall correlation is significantly im-
proved, with some significant improvement in the infor-
mation redundancy (IRD) category. This suggests that
by leveraging speaker information, ROUGE can assign
better credits or penalties to system generated summaries
(same words from different speakers will not be counted
as a match), and thus yield better correlation with human
evaluation; whereas for human summaries, this may not
happen often. For similar sentences from different speak-
ers, human annotators are more likely to agree with each
</bodyText>
<page confidence="0.997985">
203
</page>
<table confidence="0.923114222222222">
other in their selection compared to automatic summa-
rization.
Correlation on Human Summaries
Speaker Info. H AVG H IS H IC H IRV H IRD
NO 0.21 0.21 0.31 0.19 -0.16
YES 0.20 0.20 0.27 0.12 -0.09
Correlation on System Summaries
NO 0.08 0.22 0.19 -0.02 -0.07
YES 0.14 0.20 0.16 0.02 0.21
</table>
<tableCaption confidence="0.9693975">
Table 3: Effect of speaker information on the correlation be-
tween R-SU4 and human evaluation.
</tableCaption>
<sectionHeader confidence="0.988111" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999970772727273">
In this paper, we have made a first attempt to system-
atically investigate the correlation of automatic ROUGE
scores with human evaluation for meeting summariza-
tion. Adaptations on ROUGE setting based on meeting
characteristics are proposed and evaluated using Spear-
man’s rank coefficient. Our experimental results show
that in general the correlation between ROUGE scores
and human evaluation is low, with ROUGE SU4 score
showing better correlation than ROUGE-1 score. There
is significant improvement in correlation when disfluen-
cies are removed and speaker information is leveraged,
especially for evaluating system-generated summaries. In
addition, we observe that the correlation is affected differ-
ently by those factors for human summaries and system-
generated summaries.
In our future work we will examine the correlation be-
tween each statement and ROUGE scores to better rep-
resent human evaluation results instead of using simply
the average over all the statements. Further studies are
also needed using a larger data set. Finally, we plan to in-
vestigate meeting summarization evaluation using speech
recognition output.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999827125">
The authors thank University of Edinburgh for providing the an-
notated ICSI meeting corpus and Michel Galley for sharing his
tool to process the annotated data. We also thank Gabriel Mur-
ray and Michel Galley for letting us use their automatic summa-
rization system output for this study. This work is supported by
NSF grant IIS-0714132. Any opinions expressed in this work
are those of the authors and do not necessarily reflect the views
of NSF.
</bodyText>
<sectionHeader confidence="0.998549" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998488935483871">
J. Carbonell and J. Goldstein. 1998. The use of mmr, diversity-
based reranking for reordering documents and producing
summaries. In SIGIR, pages 335–336.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In EMNLP,
pages 364–372.
C. Hori, T. Hori, and S. Furui. 2003. Evaluation methods for
automatic speech summarization. In EUROSPEECH, pages
2825–2828.
E. Hovy, C. Lin, L. Zhou, and J. Fukumoto. 2006. Automated
summarization evaluation with basic elements. In LREC.
A. Janin, D. Baron, J. Edwards, D. Ellis, G. Gelbart, N. Norgan,
B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters.
2003. The icsi meeting corpus. In ICASSP.
K. S. Jones and J. Galliers. 1996. Evaluating natural language
processing systems: An analysis and review. Lecture Notes
in Artificial Intelligence.
D. Jones, F. Wlof, E. Gilbson, E. Williams, E. Fedorenko,
D. Reynolds, and M. Zissman. 2003. Measuring the
readability of automatic speech-to-text transcripts. In EU-
ROSPEECH, pages 1585–1588.
C. Lin. 2004. Rouge: A package for automatic evaluation of
summaries. In Workshop on Text Summarization Branches
Out at ACL, pages 74–81.
Y. Liu, F. Liu, B. Li, and S. Xie. 2007. Do disfluencies af-
fect meeting summarization? a pilot study on the impact of
disfluencies. In MLMI Workshop, Poster Session.
I. Mani, T. Firmin, D. House, M. Chrzanowski, G. Klein,
L. Hirschman, B. Sundheim, and L. Obrst. 1998. The tipster
summac text summarization evaluation: Final report. Tech-
nical report, The MITRE Corporation.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. Eval-
uating automatic summaries of meeting recordings. In ACL
2005 MTSE Workshop, pages 33–40.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method. In
HLT/NAACL.
NIST. 2007. Document understanding conference (DUC).
http://duc.nist.gov/.
D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer, A. C¸elebi,
E. Drabek, W. Lam, D. Liu, H. Qi, H. Saggion, S. Teufel,
M. Topper, and A. Winkel. 2003. The MEAD Multidocu-
ment Summarizer. http://www.summarization.com/mead/.
D. R. Radev, H. Jing, M. Stys, and T. Daniel. 2004. Centroid-
based summarization of multiple documents. Information
Processing and Management, 40:919–938.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004.
The icsi meeting recorder dialog act (mrda) corpus. In SIG-
DAL Workshop, pages 97–100.
S. Teufel and H. Halteren. 2004. Evaluating information con-
tent by factoid analysis: Human annotation and stability. In
EMNLP.
S. Xie and Y. Liu. 2008. Using corpus and knowledge-based
similarity measure in maximum marginal relevance for meet-
ing summarization. In ICASSP.
X. Zhu and G. Penn. 2005. Evaluation of sentence selection for
speech summarization. In ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or Summariza-
tion.
X. Zhu and G. Penn. 2006. Comparing the roles of tex-
tual, acoustic and spoken-language features on spontaneous-
conversation summarization. In HLT/NAACL.
</reference>
<page confidence="0.998895">
204
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.979766">
<title confidence="0.9993325">Correlation between ROUGE and Human Evaluation of Extractive Meeting Summaries</title>
<author confidence="0.997271">Feifan Liu</author>
<author confidence="0.997271">Yang Liu</author>
<affiliation confidence="0.999949">The University of Texas at Dallas</affiliation>
<address confidence="0.990453">Richardson, TX 75080, USA</address>
<email confidence="0.999922">ffliu,yangl@hlt.utdallas.edu</email>
<abstract confidence="0.9995789375">Automatic summarization evaluation is critical to the development of summarization systems. While ROUGE has been shown to correlate well with human evaluation for content match in text summarization, there are many characteristics in multiparty meeting domain, which may pose potential problems to ROUGE. In this paper, we carefully examine how well the ROUGE scores correlate with human evaluation for extractive meeting summarization. Our experiments show that generally the correlation is rather low, but a significantly better correlation can be obtained by accounting for several unique meeting characteristics, such as disfluencies and speaker information, especially when evaluating system-generated summaries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>201--204</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 201–204,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>J Goldstein</author>
</authors>
<title>The use of mmr, diversitybased reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In SIGIR,</booktitle>
<pages>335--336</pages>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>J. Carbonell and J. Goldstein. 1998. The use of mmr, diversitybased reranking for reordering documents and producing summaries. In SIGIR, pages 335–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
</authors>
<title>A skip-chain conditional random field for ranking meeting utterances by importance.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<pages>364--372</pages>
<marker>Galley, 2006</marker>
<rawString>M. Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In EMNLP, pages 364–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hori</author>
<author>T Hori</author>
<author>S Furui</author>
</authors>
<title>Evaluation methods for automatic speech summarization.</title>
<date>2003</date>
<booktitle>In EUROSPEECH,</booktitle>
<pages>2825--2828</pages>
<contexts>
<context position="3512" citStr="Hori et al., 2003" startWordPosition="520" endWordPosition="523">ion. It has been extensively studied in text summarization. Different approaches have been proposed to measure matches using words or more meaningful semantic units, for example, ROUGE (Lin, 2004), factoid analysis (Teufel and Halteren, 2004), pyramid method (Nenkova and Passonneau, 2004), and Basic Element (BE) (Hovy et al., 2006). With the increasing recent research of summarization moving into speech, especially meeting recordings, issues related to spoken language are yet to be explored for their impact on the evaluation metrics. Inspired by automatic speech recognition (ASR) evaluation, (Hori et al., 2003) proposed the summarization accuracy metric (SumACCY) based on a word network created by merging manual summaries. However (Zhu and Penn, 2005) found a statistically significant difference between the ASR-inspired metrics and those taken from text summarization (e.g., RU, ROUGE) on a subset of the Switchboard data. ROUGE has been used in meeting summarization evaluation (Murray et al., 2005; Galley, 2006), yet the question remained whether ROUGE is a good metric for the meeting domain. (Murray et al., 2005) showed low correlation of ROUGE and human evaluation in meeting summarization evaluatio</context>
</contexts>
<marker>Hori, Hori, Furui, 2003</marker>
<rawString>C. Hori, T. Hori, and S. Furui. 2003. Evaluation methods for automatic speech summarization. In EUROSPEECH, pages 2825–2828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C Lin</author>
<author>L Zhou</author>
<author>J Fukumoto</author>
</authors>
<title>Automated summarization evaluation with basic elements.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="3227" citStr="Hovy et al., 2006" startWordPosition="478" endWordPosition="481">ssesses a summarization system in itself (for example, informativeness, redundancy, and coherence). Extrinsic evaluation (Mani et al., 1998) tests the effectiveness of a summarization system on other tasks. In this study, we concentrate on the automatic intrinsic summarization evaluation. It has been extensively studied in text summarization. Different approaches have been proposed to measure matches using words or more meaningful semantic units, for example, ROUGE (Lin, 2004), factoid analysis (Teufel and Halteren, 2004), pyramid method (Nenkova and Passonneau, 2004), and Basic Element (BE) (Hovy et al., 2006). With the increasing recent research of summarization moving into speech, especially meeting recordings, issues related to spoken language are yet to be explored for their impact on the evaluation metrics. Inspired by automatic speech recognition (ASR) evaluation, (Hori et al., 2003) proposed the summarization accuracy metric (SumACCY) based on a word network created by merging manual summaries. However (Zhu and Penn, 2005) found a statistically significant difference between the ASR-inspired metrics and those taken from text summarization (e.g., RU, ROUGE) on a subset of the Switchboard data</context>
</contexts>
<marker>Hovy, Lin, Zhou, Fukumoto, 2006</marker>
<rawString>E. Hovy, C. Lin, L. Zhou, and J. Fukumoto. 2006. Automated summarization evaluation with basic elements. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>G Gelbart</author>
<author>N Norgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The icsi meeting corpus.</title>
<date>2003</date>
<booktitle>In ICASSP.</booktitle>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Norgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, G. Gelbart, N. Norgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The icsi meeting corpus. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Jones</author>
<author>J Galliers</author>
</authors>
<title>Evaluating natural language processing systems: An analysis and review.</title>
<date>1996</date>
<journal>Lecture Notes in Artificial Intelligence.</journal>
<contexts>
<context position="2476" citStr="Jones and Galliers, 1996" startWordPosition="365" endWordPosition="368">with human evaluation in meetings. In this paper we investigate the correlation between ROUGE and human evaluation of extractive meeting summaries and focus on two issues specific to the meeting domain: disfluencies and multiple speakers. Both human and system generated summaries are used. Our analysis shows that by integrating meeting characteristics into ROUGE settings, better correlation can be achieved between the ROUGE scores and human evaluation based on Spearman’s rho in the meeting domain. 2 Related work Automatic summarization evaluation can be broadly classified into two categories (Jones and Galliers, 1996): intrinsic and extrinsic evaluation. Intrinsic evaluation, such as relative utility based metric proposed in (Radev et al., 2004), assesses a summarization system in itself (for example, informativeness, redundancy, and coherence). Extrinsic evaluation (Mani et al., 1998) tests the effectiveness of a summarization system on other tasks. In this study, we concentrate on the automatic intrinsic summarization evaluation. It has been extensively studied in text summarization. Different approaches have been proposed to measure matches using words or more meaningful semantic units, for example, ROU</context>
</contexts>
<marker>Jones, Galliers, 1996</marker>
<rawString>K. S. Jones and J. Galliers. 1996. Evaluating natural language processing systems: An analysis and review. Lecture Notes in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jones</author>
<author>F Wlof</author>
<author>E Gilbson</author>
<author>E Williams</author>
<author>E Fedorenko</author>
<author>D Reynolds</author>
<author>M Zissman</author>
</authors>
<title>Measuring the readability of automatic speech-to-text transcripts.</title>
<date>2003</date>
<booktitle>In EUROSPEECH,</booktitle>
<pages>1585--1588</pages>
<marker>Jones, Wlof, Gilbson, Williams, Fedorenko, Reynolds, Zissman, 2003</marker>
<rawString>D. Jones, F. Wlof, E. Gilbson, E. Williams, E. Fedorenko, D. Reynolds, and M. Zissman. 2003. Measuring the readability of automatic speech-to-text transcripts. In EUROSPEECH, pages 1585–1588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Workshop on Text Summarization Branches Out at ACL,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="1228" citStr="Lin, 2004" startWordPosition="176" endWordPosition="177">g summarization. Our experiments show that generally the correlation is rather low, but a significantly better correlation can be obtained by accounting for several unique meeting characteristics, such as disfluencies and speaker information, especially when evaluating system-generated summaries. 1 Introduction Meeting summarization has drawn an increasing attention recently; therefore a study on the automatic evaluation metrics for this task is timely. Automatic evaluation helps to advance system development and avoids the labor-intensive and potentially inconsistent human evaluation. ROUGE (Lin, 2004) has been widely used for summarization evaluation. In the news article domain, ROUGE scores have been shown to be generally highly correlated with human evaluation in content match (Lin, 2004). However, there are many differences between written texts (e.g., news wire) and spoken documents, especially in the meeting domain, for example, the presence of disfluencies and multiple speakers, and the lack of structure in spontaneous utterances. The question of whether ROUGE is a good metric for meeting summarization is unclear. (Murray et al., 2005) have reported that ROUGE-1 (unigram match) score</context>
<context position="3090" citStr="Lin, 2004" startWordPosition="459" endWordPosition="460">trinsic and extrinsic evaluation. Intrinsic evaluation, such as relative utility based metric proposed in (Radev et al., 2004), assesses a summarization system in itself (for example, informativeness, redundancy, and coherence). Extrinsic evaluation (Mani et al., 1998) tests the effectiveness of a summarization system on other tasks. In this study, we concentrate on the automatic intrinsic summarization evaluation. It has been extensively studied in text summarization. Different approaches have been proposed to measure matches using words or more meaningful semantic units, for example, ROUGE (Lin, 2004), factoid analysis (Teufel and Halteren, 2004), pyramid method (Nenkova and Passonneau, 2004), and Basic Element (BE) (Hovy et al., 2006). With the increasing recent research of summarization moving into speech, especially meeting recordings, issues related to spoken language are yet to be explored for their impact on the evaluation metrics. Inspired by automatic speech recognition (ASR) evaluation, (Hori et al., 2003) proposed the summarization accuracy metric (SumACCY) based on a word network created by merging manual summaries. However (Zhu and Penn, 2005) found a statistically significant </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C. Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Workshop on Text Summarization Branches Out at ACL, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>F Liu</author>
<author>B Li</author>
<author>S Xie</author>
</authors>
<title>Do disfluencies affect meeting summarization? a pilot study on the impact of disfluencies.</title>
<date>2007</date>
<booktitle>In MLMI Workshop,</booktitle>
<location>Poster Session.</location>
<marker>Liu, Liu, Li, Xie, 2007</marker>
<rawString>Y. Liu, F. Liu, B. Li, and S. Xie. 2007. Do disfluencies affect meeting summarization? a pilot study on the impact of disfluencies. In MLMI Workshop, Poster Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>T Firmin</author>
<author>D House</author>
<author>M Chrzanowski</author>
<author>G Klein</author>
<author>L Hirschman</author>
<author>B Sundheim</author>
<author>L Obrst</author>
</authors>
<title>The tipster summac text summarization evaluation: Final report.</title>
<date>1998</date>
<tech>Technical report, The MITRE</tech>
<publisher>Corporation.</publisher>
<contexts>
<context position="2749" citStr="Mani et al., 1998" startWordPosition="404" endWordPosition="407"> are used. Our analysis shows that by integrating meeting characteristics into ROUGE settings, better correlation can be achieved between the ROUGE scores and human evaluation based on Spearman’s rho in the meeting domain. 2 Related work Automatic summarization evaluation can be broadly classified into two categories (Jones and Galliers, 1996): intrinsic and extrinsic evaluation. Intrinsic evaluation, such as relative utility based metric proposed in (Radev et al., 2004), assesses a summarization system in itself (for example, informativeness, redundancy, and coherence). Extrinsic evaluation (Mani et al., 1998) tests the effectiveness of a summarization system on other tasks. In this study, we concentrate on the automatic intrinsic summarization evaluation. It has been extensively studied in text summarization. Different approaches have been proposed to measure matches using words or more meaningful semantic units, for example, ROUGE (Lin, 2004), factoid analysis (Teufel and Halteren, 2004), pyramid method (Nenkova and Passonneau, 2004), and Basic Element (BE) (Hovy et al., 2006). With the increasing recent research of summarization moving into speech, especially meeting recordings, issues related t</context>
</contexts>
<marker>Mani, Firmin, House, Chrzanowski, Klein, Hirschman, Sundheim, Obrst, 1998</marker>
<rawString>I. Mani, T. Firmin, D. House, M. Chrzanowski, G. Klein, L. Hirschman, B. Sundheim, and L. Obrst. 1998. The tipster summac text summarization evaluation: Final report. Technical report, The MITRE Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Carletta</author>
<author>J Moore</author>
</authors>
<title>Evaluating automatic summaries of meeting recordings.</title>
<date>2005</date>
<booktitle>In ACL 2005 MTSE Workshop,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="1779" citStr="Murray et al., 2005" startWordPosition="261" endWordPosition="264">sive and potentially inconsistent human evaluation. ROUGE (Lin, 2004) has been widely used for summarization evaluation. In the news article domain, ROUGE scores have been shown to be generally highly correlated with human evaluation in content match (Lin, 2004). However, there are many differences between written texts (e.g., news wire) and spoken documents, especially in the meeting domain, for example, the presence of disfluencies and multiple speakers, and the lack of structure in spontaneous utterances. The question of whether ROUGE is a good metric for meeting summarization is unclear. (Murray et al., 2005) have reported that ROUGE-1 (unigram match) scores have low correlation with human evaluation in meetings. In this paper we investigate the correlation between ROUGE and human evaluation of extractive meeting summaries and focus on two issues specific to the meeting domain: disfluencies and multiple speakers. Both human and system generated summaries are used. Our analysis shows that by integrating meeting characteristics into ROUGE settings, better correlation can be achieved between the ROUGE scores and human evaluation based on Spearman’s rho in the meeting domain. 2 Related work Automatic </context>
</contexts>
<marker>Murray, Renals, Carletta, Moore, 2005</marker>
<rawString>G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. Evaluating automatic summaries of meeting recordings. In ACL 2005 MTSE Workshop, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: the pyramid method.</title>
<date>2004</date>
<booktitle>In HLT/NAACL.</booktitle>
<contexts>
<context position="3183" citStr="Nenkova and Passonneau, 2004" startWordPosition="469" endWordPosition="472">tility based metric proposed in (Radev et al., 2004), assesses a summarization system in itself (for example, informativeness, redundancy, and coherence). Extrinsic evaluation (Mani et al., 1998) tests the effectiveness of a summarization system on other tasks. In this study, we concentrate on the automatic intrinsic summarization evaluation. It has been extensively studied in text summarization. Different approaches have been proposed to measure matches using words or more meaningful semantic units, for example, ROUGE (Lin, 2004), factoid analysis (Teufel and Halteren, 2004), pyramid method (Nenkova and Passonneau, 2004), and Basic Element (BE) (Hovy et al., 2006). With the increasing recent research of summarization moving into speech, especially meeting recordings, issues related to spoken language are yet to be explored for their impact on the evaluation metrics. Inspired by automatic speech recognition (ASR) evaluation, (Hori et al., 2003) proposed the summarization accuracy metric (SumACCY) based on a word network created by merging manual summaries. However (Zhu and Penn, 2005) found a statistically significant difference between the ASR-inspired metrics and those taken from text summarization (e.g., RU</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>A. Nenkova and R. Passonneau. 2004. Evaluating content selection in summarization: the pyramid method. In HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<date>2007</date>
<note>Document understanding conference (DUC). http://duc.nist.gov/.</note>
<marker>NIST, 2007</marker>
<rawString>NIST. 2007. Document understanding conference (DUC). http://duc.nist.gov/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>T Allison</author>
<author>S Blair-Goldensohn</author>
<author>J Blitzer</author>
<author>A C¸elebi</author>
<author>E Drabek</author>
<author>W Lam</author>
<author>D Liu</author>
<author>H Qi</author>
<author>H Saggion</author>
<author>S Teufel</author>
<author>M Topper</author>
<author>A Winkel</author>
</authors>
<title>The MEAD Multidocument Summarizer.</title>
<date>2003</date>
<note>http://www.summarization.com/mead/.</note>
<marker>Radev, Allison, Blair-Goldensohn, Blitzer, C¸elebi, Drabek, Lam, Liu, Qi, Saggion, Teufel, Topper, Winkel, 2003</marker>
<rawString>D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer, A. C¸elebi, E. Drabek, W. Lam, D. Liu, H. Qi, H. Saggion, S. Teufel, M. Topper, and A. Winkel. 2003. The MEAD Multidocument Summarizer. http://www.summarization.com/mead/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>H Jing</author>
<author>M Stys</author>
<author>T Daniel</author>
</authors>
<title>Centroidbased summarization of multiple documents.</title>
<date>2004</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>40--919</pages>
<contexts>
<context position="2606" citStr="Radev et al., 2004" startWordPosition="384" endWordPosition="387"> summaries and focus on two issues specific to the meeting domain: disfluencies and multiple speakers. Both human and system generated summaries are used. Our analysis shows that by integrating meeting characteristics into ROUGE settings, better correlation can be achieved between the ROUGE scores and human evaluation based on Spearman’s rho in the meeting domain. 2 Related work Automatic summarization evaluation can be broadly classified into two categories (Jones and Galliers, 1996): intrinsic and extrinsic evaluation. Intrinsic evaluation, such as relative utility based metric proposed in (Radev et al., 2004), assesses a summarization system in itself (for example, informativeness, redundancy, and coherence). Extrinsic evaluation (Mani et al., 1998) tests the effectiveness of a summarization system on other tasks. In this study, we concentrate on the automatic intrinsic summarization evaluation. It has been extensively studied in text summarization. Different approaches have been proposed to measure matches using words or more meaningful semantic units, for example, ROUGE (Lin, 2004), factoid analysis (Teufel and Halteren, 2004), pyramid method (Nenkova and Passonneau, 2004), and Basic Element (BE</context>
</contexts>
<marker>Radev, Jing, Stys, Daniel, 2004</marker>
<rawString>D. R. Radev, H. Jing, M. Stys, and T. Daniel. 2004. Centroidbased summarization of multiple documents. Information Processing and Management, 40:919–938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>R Dhillon</author>
<author>S Bhagat</author>
<author>J Ang</author>
<author>H Carvey</author>
</authors>
<title>The icsi meeting recorder dialog act (mrda) corpus.</title>
<date>2004</date>
<booktitle>In SIGDAL Workshop,</booktitle>
<pages>97--100</pages>
<marker>Shriberg, Dhillon, Bhagat, Ang, Carvey, 2004</marker>
<rawString>E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004. The icsi meeting recorder dialog act (mrda) corpus. In SIGDAL Workshop, pages 97–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>H Halteren</author>
</authors>
<title>Evaluating information content by factoid analysis: Human annotation and stability.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="3136" citStr="Teufel and Halteren, 2004" startWordPosition="463" endWordPosition="466">on. Intrinsic evaluation, such as relative utility based metric proposed in (Radev et al., 2004), assesses a summarization system in itself (for example, informativeness, redundancy, and coherence). Extrinsic evaluation (Mani et al., 1998) tests the effectiveness of a summarization system on other tasks. In this study, we concentrate on the automatic intrinsic summarization evaluation. It has been extensively studied in text summarization. Different approaches have been proposed to measure matches using words or more meaningful semantic units, for example, ROUGE (Lin, 2004), factoid analysis (Teufel and Halteren, 2004), pyramid method (Nenkova and Passonneau, 2004), and Basic Element (BE) (Hovy et al., 2006). With the increasing recent research of summarization moving into speech, especially meeting recordings, issues related to spoken language are yet to be explored for their impact on the evaluation metrics. Inspired by automatic speech recognition (ASR) evaluation, (Hori et al., 2003) proposed the summarization accuracy metric (SumACCY) based on a word network created by merging manual summaries. However (Zhu and Penn, 2005) found a statistically significant difference between the ASR-inspired metrics an</context>
</contexts>
<marker>Teufel, Halteren, 2004</marker>
<rawString>S. Teufel and H. Halteren. 2004. Evaluating information content by factoid analysis: Human annotation and stability. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Xie</author>
<author>Y Liu</author>
</authors>
<title>Using corpus and knowledge-based similarity measure in maximum marginal relevance for meeting summarization.</title>
<date>2008</date>
<booktitle>In ICASSP.</booktitle>
<marker>Xie, Liu, 2008</marker>
<rawString>S. Xie and Y. Liu. 2008. Using corpus and knowledge-based similarity measure in maximum marginal relevance for meeting summarization. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>G Penn</author>
</authors>
<title>Evaluation of sentence selection for speech summarization.</title>
<date>2005</date>
<booktitle>In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</booktitle>
<marker>Zhu, Penn, 2005</marker>
<rawString>X. Zhu and G. Penn. 2005. Evaluation of sentence selection for speech summarization. In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>G Penn</author>
</authors>
<title>Comparing the roles of textual, acoustic and spoken-language features on spontaneousconversation summarization.</title>
<date>2006</date>
<booktitle>In HLT/NAACL.</booktitle>
<marker>Zhu, Penn, 2006</marker>
<rawString>X. Zhu and G. Penn. 2006. Comparing the roles of textual, acoustic and spoken-language features on spontaneousconversation summarization. In HLT/NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>