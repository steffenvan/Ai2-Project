<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.092363">
<title confidence="0.636313833333333">
SmartNotes: Implicit Labeling of Meeting Data
through User Note–Taking and Browsing
Satanjeev Banerjee
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
</title>
<email confidence="0.99307">
banerjee@cs.cmu.edu
</email>
<author confidence="0.905799">
Alexander I. Rudnicky
</author>
<affiliation confidence="0.96179">
School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.748344">
Pittsburgh, PA 15213
</address>
<email confidence="0.999036">
air@cs.cmu.edu
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977533333334">
We have implemented SmartNotes, a sys-
tem that automatically acquires labeled
meeting data as users take notes during
meetings and browse the notes afterwards.
Such data can enable meeting understand-
ing components such as topic and ac-
tion item detectors to automatically im-
prove their performance over a sequence
of meetings. The SmartNotes system con-
sists of a laptop based note taking appli-
cation, and a web based note retrieval sys-
tem. We shall demonstrate the functional-
ities of this system, and will also demon-
strate the labeled data obtained during typ-
ical meetings and browsing sessions.
</bodyText>
<sectionHeader confidence="0.724784" genericHeader="method">
1 Goals of the SmartNotes System
</sectionHeader>
<bodyText confidence="0.999807434782609">
Most institutions hold a large number of meetings
every day. Several of these meetings are important,
and meeting participants need to recall the details
of the discussions at a future date. In a previous
survey (Banerjee et al., 2005) of busy professors at
Carnegie Mellon University we showed that meeting
participants needed to recall details of past meetings
on average about twice a month. Performing such
retrieval is not an easy task. It is time consuming;
in our study participants took on average between
15 minutes to an hour to recall the information they
were seeking. Further, the quality of the retrieval
is dependent on whether or not the participants had
access to the notes at the meeting. On a scale of 0
to 5, with 5 denoting complete satisfaction with re-
trieval results, participants reported a satisfaction of
3.4 when they did not have notes, and 4.0 when they
did.
Despite the prevalence of important meetings and
the importance of notes, there is a relative paucity of
technology to help meeting participants take notes
easily at meetings. Some commercial applications
allow users to take notes (e.g. OneNote1) and even
record audio/video (e.g. Quindi2), but no product at-
tempts to automatically take notes. Our long term
goal is to create a system that makes note–taking
easier by performing tasks such as automatically
highlighting portions of the meeting that are likely
to be important to the user, automatically detecting
“note–worthy” phrases spoken during the meeting,
etc.
To perform such note taking, the system needs to
form an understanding of the meeting. Our short
term goal is to create a system that can detect the
topics of discussion, the action items being dis-
cussed, and the roles of the meeting participants.
Additionally, these components must adapt to spe-
cific users and groups of users since different people
will likely take different notes at the same meeting.
Thus we wish to implement the note taking system
in such a way that the user’s interactions with the
system result in labeled meeting data that can then
be used to adapt and improve the meeting under-
standing components.
Towards these goals, we have built SmartNotes
which helps users easily record and retrieve notes.
</bodyText>
<footnote confidence="0.9999705">
1http://office.microsoft.com/onenote
2http://www.quindi.com
</footnote>
<page confidence="0.92828">
261
</page>
<subsectionHeader confidence="0.759659">
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 261–264,
New York City, June 2006. c�2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999862090909091">
The system also records the user interactions to form
labeled meeting data that can later be used to auto-
matically improve the meeting understanding com-
ponents. In the next section we describe the meeting
understanding components in more detail. Next we
describe SmartNotes itself, and show how it is cur-
rently helping users take and retrieve notes, while
acquiring labeled data to aid each of the meeting un-
derstanding components. Finally we end with a dis-
cussion of what functionality we plan to demonstrate
at the conference.
</bodyText>
<sectionHeader confidence="0.938651" genericHeader="method">
2 Automatic Meeting Understanding
</sectionHeader>
<bodyText confidence="0.999972808510638">
Topic detection and segmentation: We are at-
tempting to automatically detect the topics being
discussed at meetings. This task consists of two sub-
tasks: discovering the points in a meeting when the
topic changes, and then associating a descriptive la-
bel to the segment between two topic shifts. Our cur-
rent strategy for topic shift detection (Banerjee and
Rudnicky, 2006a) is to perform an edge detection
using such features as speech activity (who spoke
when and for how long), the words that each per-
son spoke, etc. For labeling, we are currently sim-
ply associating the agenda item names recorded in
the notes with the segments they are most relevant
to, as decided by a tf.idf matching technique. Topic
detection is particularly useful during meeting infor-
mation retrieval; (Banerjee et al., 2005) showed that
when users wish to retrieve information from past
meetings, they are typically interested in a specific
discussion topic, as opposed to an entire meeting.
Action item detection: An obvious application
of meeting understanding is the automatic discovery
and recording of action items as they are discussed
during a meeting. Arguably one of the most impor-
tant outcomes of a meeting are the action items de-
cided upon, and automatically recording them could
be a huge benefit especially to those participants that
are likely to not note them down and consequently
forget about them later on.
Meeting participant role detection: Each meet-
ing participant plays a variety of roles in an insti-
tution. These roles can be based on their function
in the institution (managers, assistants, professors,
students, etc), or based on their expertise (speech
recognition experts, facilities experts, etc). Our cur-
rent strategy for role detection (Banerjee and Rud-
nicky, 2006b) is to train detectors on hand labeled
data. Our next step is to perform discovery of new
roles through clustering techniques. Detecting such
roles has several benefits. First, it allows us to build
prior expectations of a meeting between a group of
participants. For example, if we know person A is
a speech recognition expert and person B a speech
synthesis expert, a reasonable expectation is that
when they meet they are likely to talk about tech-
nologies related speech processing. Consequently,
we can use this expectation to aid the action item
detection and the topic detection in that meeting.
</bodyText>
<sectionHeader confidence="0.997093" genericHeader="method">
3 SmartNotes: System Description
</sectionHeader>
<bodyText confidence="0.999967428571429">
We have implemented SmartNotes to help users
take multi–media notes during meetings, and re-
trieve them later on. SmartNotes consists of two ma-
jor components: The note taking application which
meeting participants use to take notes during the
meeting, and the note retrieval application which
users use to retrieve notes at a later point.
</bodyText>
<subsectionHeader confidence="0.995074">
3.1 SmartNotes Note Taking Application
</subsectionHeader>
<bodyText confidence="0.999786739130435">
The note taking application is a stand–alone system,
that runs on each meeting participant’s laptop, and
allows him to take notes during the meeting. In ad-
dition to recording the text notes, it also records the
participant’s speech, and video, if a video camera is
connected to the laptop. This system is an extension
of the Carnegie Mellon Meeting Recorder (Banerjee
et al., 2004).
Figure 1 shows a screen–shot of this application.
It is a server–client application, and each participant
logs into a central server at the beginning of each
meeting. Thus, the system knows the precise iden-
tity of each note taker as well as each speaker in
the meeting. This allows us to avoid the onerous
problem of automatically detecting who is speaking
at any time during the meeting. Further, after log-
ging on, each client automatically synchronizes it-
self with a central NTP time server. Thus the time
stamps that each client associates with its recordings
are all synchronized, to facilitate merging and play
back of audio/video during browsing (described in
the next sub–section).
Once logged in, each participant’s note taking
</bodyText>
<page confidence="0.996439">
262
</page>
<figureCaption confidence="0.998366">
Figure 1: Screen shot of the SmartNotes note–taking client
</figureCaption>
<bodyText confidence="0.999959080645161">
area is split into two sections: a shared note taking
area, and a private note taking area. Notes written
in the shared area are viewable by all meeting par-
ticipants. This allows meeting participants to share
the task of taking notes during a meeting: As long as
one participant has recorded an important point dur-
ing a meeting, the other participants do not need to,
thus making the note taking task easier for the group
as a whole. Private notes that a participant does not
wish to share with all participants can be taken in the
private note taking area.
The interface has a mechanism to allow meeting
participants to insert an agenda into the shared area.
Once inserted, the shared area is split into as many
boxes as there are agenda items. Participants can
then take notes during the discussion of an agenda
item in the corresponding agenda item box. This
is useful to the participants because it organizes the
notes as they are being taken, and, additionally, the
notes can later be retrieved agenda item by agenda
item. Thus, the user can access all notes he has taken
in different meetings regarding “buying a printer”,
without having to see the notes taken for the other
agenda items in each such meeting.
In addition to being useful to the user, this act of
inserting an agenda and then taking notes within the
relevant agenda item box results in generating (un-
beknownst to the participant) labeled data for the
topic detection component. Specifically, if we de-
fine each agenda item as being a separate “topic”,
and make the assumption that notes are taken ap-
proximately concurrent with the discussion of the
contents of the notes, then we can conclude that
there is a shift in the topic of discussion at some
point between the time stamp on the last note in
an agenda item box, and the time stamp on the first
note of the next agenda item box. This information
can then be used to improve the performance of the
topic shift detector. The accuracy of the topic shift
data thus acquired depends on the length of time be-
tween the two time points. Since this length is easy
to calculate automatically, this information can be
factored into the topic detector trainer.
The interface also allows participants to enter ac-
tion items through a dedicated action item form.
Again the advantage of such a form to the partici-
pants is that the action items (and thus the notes) are
better organized: After the meeting, they can per-
form retrieval on specific fields of the action items.
For example, they can ask to retrieve all the action
items assigned to a particular participant, or that are
due a particular day, etc.
In addition to being beneficial to the participant,
the action item form filling action results in gener-
ating labeled data for the action item detector.
Specifically, if we make the assumption that an ac-
tion item form filling action is preceded by a discus-
sion of the action item, then the system can couple
the contents of the form with all the speech within
a window of time before the form filling action, and
use this pair as a data point to retrain its action item
detector.
</bodyText>
<subsectionHeader confidence="0.99935">
3.2 SmartNotes Note Retrieval Website
</subsectionHeader>
<bodyText confidence="0.9987836">
As notes and audio/video are recorded on each indi-
vidual participant’s laptop, they also get transferred
over the internet to a central meeting server. This
transfer occurs in the background without any in-
tervention from the user, utilizes only the left–over
bandwidth beyond the user’s current bandwidth us-
age, and is robust to system shut–downs, crashes,
etc. This process is described in more detail in
(Banerjee et al., 2004).
Once the meeting is over and all the data has been
transferred to the central server, meeting participants
can use the SmartNotes multi–media notes retrieval
system to view the notes and access the recorded
audio/video. This is a web–based application that
uses the same login process as the stand–along note
</bodyText>
<page confidence="0.998156">
263
</page>
<figureCaption confidence="0.987028">
Figure 2: Screen shot ofthe SmartNotes website
</figureCaption>
<bodyText confidence="0.999979352941176">
taking system. Users can view a list of meetings
they have recorded using the SmartNotes applica-
tion in the past, and then for each meeting, they can
view the shared notes taken at the meeting. Figure
2 shows a screen shot of such a notes browsing ses-
sion. Additionally, participants can view their own
private notes taken during the meeting.
In addition to viewing the notes, they can also ac-
cess all recorded audio/video, indexed by the notes.
That is, they can access the audio/video recorded
around the time that the note was entered. Further
they can specify how many minutes before and af-
ter the note they wish to access. Since the server
has the audio from each meeting participant’s audio
channel, the viewer of the notes can choose to listen
to any one person’s channel, or a combination of the
audio channels. The merging of channels is done in
real time and is achievable because their time stamps
have been synchronized during recording.
In the immediate future we plan to implement a
simple key–word based search on the notes recorded
in all the recorded meetings (or in one specific meet-
ing). This search will return notes that match the
search using a standard tf.idf approach. The user
will also be provided the option of rating the qual-
ity of the search retrieval on a one bit satisfied/not–
satisfied scale. If the user chooses to provide this
rating, it can be used as a feedback to improve the
search. Additionally, which parts of the meeting the
user chooses to access the audio/video from can be
used to form a model of the parts of the meetings
most relevant to the user. This information can help
the system tailor its retrieval to individual prefer-
ences.
</bodyText>
<sectionHeader confidence="0.997085" genericHeader="method">
4 The Demonstration
</sectionHeader>
<bodyText confidence="0.999995882352941">
We shall demonstrate both the SmartNotes note tak-
ing client as well as the SmartNotes note–retrieval
website. Specifically we will perform 2 minute long
mock meetings between 2 or 3 demonstrators. We
will show how notes can be taken, how agendas can
be created and action items noted. We will then
show how the notes and the audio/video from the 2
minute meeting can be accessed through the Smart-
Notes note retrieval website. We shall also show the
automatically labeled data that gets created both dur-
ing the mock meeting, as well as during the brows-
ing session. Finally, if time permits, we shall show
results on how much we can improve the meeting
understanding components’ capabilities through la-
beled meeting data automatically acquired through
participants’ use of SmartNotes at CMU and other
institutions that are currently using the system.
</bodyText>
<sectionHeader confidence="0.99906" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999281238095238">
S. Banerjee and A. I. Rudnicky. 2006a. A texttiling
based approach to topic boundary detection in multi–
participant conversations. Submitted for publication.
S. Banerjee and A. I. Rudnicky. 2006b. You are what
you say: Using meeting participants’ speech to detect
their roles and expertise. In Analyzing Conversations
in Text and Speech Workshop at HLT–NAACL 2006,
New York City, USA, June.
S. Banerjee, J. Cohen, T. Quisel, A. Chan, Y. Pato-
dia, Z. Al-Bawab, R. Zhang, P. Rybski, M. Veloso,
A. Black, R. Stern, R. Rosenfeld, and A. I. Rudnicky.
2004. Creating multi-modal, user–centric records of
meetings with the Carnegie Mellon meeting recorder
architecture. In Proceedings of the ICASSP Meeting
Recognition Workshop, Montreal, Canada.
S. Banerjee, C. Rose, and A. I. Rudnicky. 2005. The
necessity of a meeting recording and playback system,
and the benefit of topic–level annotations to meeting
browsing. In Proceedings of the Tenth International
Conference on Human-Computer Interaction, Rome,
Italy, September.
</reference>
<page confidence="0.998212">
264
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.016889">
<title confidence="0.89356325">SmartNotes: Implicit Labeling of Meeting through User Note–Taking and Browsing Satanjeev Language Technologies</title>
<affiliation confidence="0.928404">Carnegie Mellon</affiliation>
<address confidence="0.791782">Pittsburgh, PA</address>
<email confidence="0.999333">banerjee@cs.cmu.edu</email>
<author confidence="0.997021">I Alexander</author>
<affiliation confidence="0.999731">School of Computer Carnegie Mellon</affiliation>
<address confidence="0.827835">Pittsburgh, PA</address>
<email confidence="0.999131">air@cs.cmu.edu</email>
<abstract confidence="0.998027707070708">We have implemented SmartNotes, a system that automatically acquires labeled meeting data as users take notes during meetings and browse the notes afterwards. Such data can enable meeting understanding components such as topic and action item detectors to automatically improve their performance over a sequence of meetings. The SmartNotes system consists of a laptop based note taking application, and a web based note retrieval system. We shall demonstrate the functionalities of this system, and will also demonstrate the labeled data obtained during typical meetings and browsing sessions. Goals of the Most institutions hold a large number of meetings every day. Several of these meetings are important, and meeting participants need to recall the details of the discussions at a future date. In a previous survey (Banerjee et al., 2005) of busy professors at Carnegie Mellon University we showed that meeting participants needed to recall details of past meetings on average about twice a month. Performing such retrieval is not an easy task. It is time consuming; in our study participants took on average between 15 minutes to an hour to recall the information they were seeking. Further, the quality of the retrieval is dependent on whether or not the participants had access to the notes at the meeting. On a scale of 0 to 5, with 5 denoting complete satisfaction with retrieval results, participants reported a satisfaction of 3.4 when they did not have notes, and 4.0 when they did. Despite the prevalence of important meetings and the importance of notes, there is a relative paucity of technology to help meeting participants take notes easily at meetings. Some commercial applications users to take notes (e.g. and even audio/video (e.g. but no product attempts to automatically take notes. Our long term goal is to create a system that makes note–taking easier by performing tasks such as automatically highlighting portions of the meeting that are likely to be important to the user, automatically detecting “note–worthy” phrases spoken during the meeting, etc. To perform such note taking, the system needs to form an understanding of the meeting. Our short term goal is to create a system that can detect the topics of discussion, the action items being discussed, and the roles of the meeting participants. Additionally, these components must adapt to specific users and groups of users since different people will likely take different notes at the same meeting. Thus we wish to implement the note taking system in such a way that the user’s interactions with the result in meeting data can then be used to adapt and improve the meeting understanding components. these goals, we have built which helps users easily record and retrieve notes. 261 of the Human Language Technology Conference of the NAACL, Companion pages York City, June 2006. Association for Computational Linguistics The system also records the user interactions to form labeled meeting data that can later be used to automatically improve the meeting understanding components. In the next section we describe the meeting understanding components in more detail. Next we describe SmartNotes itself, and show how it is currently helping users take and retrieve notes, while acquiring labeled data to aid each of the meeting understanding components. Finally we end with a discussion of what functionality we plan to demonstrate at the conference. 2 Automatic Meeting Understanding Topic detection and segmentation: We are attempting to automatically detect the topics being discussed at meetings. This task consists of two subtasks: discovering the points in a meeting when the changes, and then associating a descriptive lathe segment between two topic shifts. Our current strategy for topic shift detection (Banerjee and Rudnicky, 2006a) is to perform an edge detection using such features as speech activity (who spoke when and for how long), the words that each person spoke, etc. For labeling, we are currently simply associating the agenda item names recorded in the notes with the segments they are most relevant to, as decided by a tf.idf matching technique. Topic detection is particularly useful during meeting information retrieval; (Banerjee et al., 2005) showed that when users wish to retrieve information from past meetings, they are typically interested in a specific discussion topic, as opposed to an entire meeting. Action item detection: An obvious application of meeting understanding is the automatic discovery and recording of action items as they are discussed during a meeting. Arguably one of the most important outcomes of a meeting are the action items decided upon, and automatically recording them could be a huge benefit especially to those participants that are likely to not note them down and consequently forget about them later on. Meeting participant role detection: Each meeting participant plays a variety of roles in an institution. These roles can be based on their function in the institution (managers, assistants, professors, students, etc), or based on their expertise (speech recognition experts, facilities experts, etc). Our current strategy for role detection (Banerjee and Rudnicky, 2006b) is to train detectors on hand labeled data. Our next step is to perform discovery of new roles through clustering techniques. Detecting such roles has several benefits. First, it allows us to build prior expectations of a meeting between a group of participants. For example, if we know person A is a speech recognition expert and person B a speech synthesis expert, a reasonable expectation is that when they meet they are likely to talk about technologies related speech processing. Consequently, we can use this expectation to aid the action item detection and the topic detection in that meeting. 3 SmartNotes: System Description We have implemented SmartNotes to help users notes meetings, and retrieve them later on. SmartNotes consists of two major components: The note taking application which meeting participants use to take notes during the meeting, and the note retrieval application which users use to retrieve notes at a later point. 3.1 SmartNotes Note Taking Application The note taking application is a stand–alone system, that runs on each meeting participant’s laptop, and allows him to take notes during the meeting. In addition to recording the text notes, it also records the participant’s speech, and video, if a video camera is connected to the laptop. This system is an extension of the Carnegie Mellon Meeting Recorder (Banerjee et al., 2004). Figure 1 shows a screen–shot of this application. It is a server–client application, and each participant logs into a central server at the beginning of each meeting. Thus, the system knows the precise identity of each note taker as well as each speaker in the meeting. This allows us to avoid the onerous problem of automatically detecting who is speaking at any time during the meeting. Further, after logging on, each client automatically synchronizes itself with a central NTP time server. Thus the time stamps that each client associates with its recordings are all synchronized, to facilitate merging and play back of audio/video during browsing (described in the next sub–section). Once logged in, each participant’s note taking 262 1: shot of the SmartNotes note–taking client is split into two sections: a taking and a taking area. Notes written in the shared area are viewable by all meeting participants. This allows meeting participants to share the task of taking notes during a meeting: As long as one participant has recorded an important point during a meeting, the other participants do not need to, thus making the note taking task easier for the group as a whole. Private notes that a participant does not wish to share with all participants can be taken in the private note taking area. The interface has a mechanism to allow meeting participants to insert an agenda into the shared area. Once inserted, the shared area is split into as many boxes as there are agenda items. Participants can then take notes during the discussion of an agenda item in the corresponding agenda item box. This is useful to the participants because it organizes the notes as they are being taken, and, additionally, the notes can later be retrieved agenda item by agenda item. Thus, the user can access all notes he has taken in different meetings regarding “buying a printer”, without having to see the notes taken for the other agenda items in each such meeting. In addition to being useful to the user, this act of inserting an agenda and then taking notes within the relevant agenda item box results in generating (unbeknownst to the participant) labeled data for the topic detection component. Specifically, if we define each agenda item as being a separate “topic”, and make the assumption that notes are taken approximately concurrent with the discussion of the contents of the notes, then we can conclude that there is a shift in the topic of discussion at some point between the time stamp on the last note in an agenda item box, and the time stamp on the first note of the next agenda item box. This information can then be used to improve the performance of the topic shift detector. The accuracy of the topic shift data thus acquired depends on the length of time between the two time points. Since this length is easy to calculate automatically, this information can be factored into the topic detector trainer. The interface also allows participants to enter action items through a dedicated action item form. Again the advantage of such a form to the participants is that the action items (and thus the notes) are better organized: After the meeting, they can perform retrieval on specific fields of the action items. For example, they can ask to retrieve all the action items assigned to a particular participant, or that are due a particular day, etc. In addition to being beneficial to the participant, the action item form filling action results in generating labeled data for the action item detector. Specifically, if we make the assumption that an action item form filling action is preceded by a discussion of the action item, then the system can couple the contents of the form with all the speech within a window of time before the form filling action, and use this pair as a data point to retrain its action item detector. 3.2 SmartNotes Note Retrieval Website As notes and audio/video are recorded on each individual participant’s laptop, they also get transferred over the internet to a central meeting server. This transfer occurs in the background without any intervention from the user, utilizes only the left–over bandwidth beyond the user’s current bandwidth usage, and is robust to system shut–downs, crashes, etc. This process is described in more detail in (Banerjee et al., 2004). Once the meeting is over and all the data has been transferred to the central server, meeting participants can use the SmartNotes multi–media notes retrieval system to view the notes and access the recorded audio/video. This is a web–based application that uses the same login process as the stand–along note 263 2: shot ofthe SmartNotes website taking system. Users can view a list of meetings they have recorded using the SmartNotes application in the past, and then for each meeting, they can view the shared notes taken at the meeting. Figure 2 shows a screen shot of such a notes browsing session. Additionally, participants can view their own private notes taken during the meeting. In addition to viewing the notes, they can also access all recorded audio/video, indexed by the notes. That is, they can access the audio/video recorded around the time that the note was entered. Further they can specify how many minutes before and after the note they wish to access. Since the server has the audio from each meeting participant’s audio channel, the viewer of the notes can choose to listen to any one person’s channel, or a combination of the audio channels. The merging of channels is done in real time and is achievable because their time stamps have been synchronized during recording. In the immediate future we plan to implement a simple key–word based search on the notes recorded in all the recorded meetings (or in one specific meeting). This search will return notes that match the search using a standard tf.idf approach. The user will also be provided the option of rating the quality of the search retrieval on a one bit satisfied/not– satisfied scale. If the user chooses to provide this rating, it can be used as a feedback to improve the search. Additionally, which parts of the meeting the user chooses to access the audio/video from can be used to form a model of the parts of the meetings most relevant to the user. This information can help the system tailor its retrieval to individual preferences. 4 The Demonstration We shall demonstrate both the SmartNotes note taking client as well as the SmartNotes note–retrieval website. Specifically we will perform 2 minute long mock meetings between 2 or 3 demonstrators. We will show how notes can be taken, how agendas can be created and action items noted. We will then show how the notes and the audio/video from the 2 minute meeting can be accessed through the Smart- Notes note retrieval website. We shall also show the automatically labeled data that gets created both during the mock meeting, as well as during the browsing session. Finally, if time permits, we shall show results on how much we can improve the meeting understanding components’ capabilities through labeled meeting data automatically acquired through participants’ use of SmartNotes at CMU and other institutions that are currently using the system. References S. Banerjee and A. I. Rudnicky. 2006a. A texttiling based approach to topic boundary detection in multi– participant conversations. Submitted for publication. S. Banerjee and A. I. Rudnicky. 2006b. You are what you say: Using meeting participants’ speech to detect</abstract>
<title confidence="0.866174">roles and expertise. In Conversations Text and Speech Workshop at HLT–NAACL</title>
<author confidence="0.927521666666667">S Banerjee</author>
<author confidence="0.927521666666667">J Cohen</author>
<author confidence="0.927521666666667">T Quisel</author>
<author confidence="0.927521666666667">A Chan</author>
<author confidence="0.927521666666667">Y Patodia</author>
<author confidence="0.927521666666667">Z Al-Bawab</author>
<author confidence="0.927521666666667">R Zhang</author>
<author confidence="0.927521666666667">P Rybski</author>
<author confidence="0.927521666666667">M Veloso</author>
<abstract confidence="0.738833666666667">A. Black, R. Stern, R. Rosenfeld, and A. I. Rudnicky. 2004. Creating multi-modal, user–centric records of meetings with the Carnegie Mellon meeting recorder</abstract>
<affiliation confidence="0.574021">In of the ICASSP Meeting</affiliation>
<address confidence="0.870794">Montreal, Canada.</address>
<author confidence="0.597005">The</author>
<affiliation confidence="0.595272">necessity of a meeting recording and playback system, and the benefit of topic–level annotations to meeting In of the Tenth International on Human-Computer Rome,</affiliation>
<address confidence="0.874845">Italy, September. 264</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A I Rudnicky</author>
</authors>
<title>A texttiling based approach to topic boundary detection in multi– participant conversations.</title>
<date>2006</date>
<note>Submitted for publication.</note>
<contexts>
<context position="4353" citStr="Banerjee and Rudnicky, 2006" startWordPosition="683" endWordPosition="686">ently helping users take and retrieve notes, while acquiring labeled data to aid each of the meeting understanding components. Finally we end with a discussion of what functionality we plan to demonstrate at the conference. 2 Automatic Meeting Understanding Topic detection and segmentation: We are attempting to automatically detect the topics being discussed at meetings. This task consists of two subtasks: discovering the points in a meeting when the topic changes, and then associating a descriptive label to the segment between two topic shifts. Our current strategy for topic shift detection (Banerjee and Rudnicky, 2006a) is to perform an edge detection using such features as speech activity (who spoke when and for how long), the words that each person spoke, etc. For labeling, we are currently simply associating the agenda item names recorded in the notes with the segments they are most relevant to, as decided by a tf.idf matching technique. Topic detection is particularly useful during meeting information retrieval; (Banerjee et al., 2005) showed that when users wish to retrieve information from past meetings, they are typically interested in a specific discussion topic, as opposed to an entire meeting. Ac</context>
<context position="5753" citStr="Banerjee and Rudnicky, 2006" startWordPosition="907" endWordPosition="911">y one of the most important outcomes of a meeting are the action items decided upon, and automatically recording them could be a huge benefit especially to those participants that are likely to not note them down and consequently forget about them later on. Meeting participant role detection: Each meeting participant plays a variety of roles in an institution. These roles can be based on their function in the institution (managers, assistants, professors, students, etc), or based on their expertise (speech recognition experts, facilities experts, etc). Our current strategy for role detection (Banerjee and Rudnicky, 2006b) is to train detectors on hand labeled data. Our next step is to perform discovery of new roles through clustering techniques. Detecting such roles has several benefits. First, it allows us to build prior expectations of a meeting between a group of participants. For example, if we know person A is a speech recognition expert and person B a speech synthesis expert, a reasonable expectation is that when they meet they are likely to talk about technologies related speech processing. Consequently, we can use this expectation to aid the action item detection and the topic detection in that meeti</context>
</contexts>
<marker>Banerjee, Rudnicky, 2006</marker>
<rawString>S. Banerjee and A. I. Rudnicky. 2006a. A texttiling based approach to topic boundary detection in multi– participant conversations. Submitted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A I Rudnicky</author>
</authors>
<title>You are what you say: Using meeting participants’ speech to detect their roles and expertise.</title>
<date>2006</date>
<booktitle>In Analyzing Conversations in Text and Speech Workshop at HLT–NAACL 2006,</booktitle>
<location>New York City, USA,</location>
<contexts>
<context position="4353" citStr="Banerjee and Rudnicky, 2006" startWordPosition="683" endWordPosition="686">ently helping users take and retrieve notes, while acquiring labeled data to aid each of the meeting understanding components. Finally we end with a discussion of what functionality we plan to demonstrate at the conference. 2 Automatic Meeting Understanding Topic detection and segmentation: We are attempting to automatically detect the topics being discussed at meetings. This task consists of two subtasks: discovering the points in a meeting when the topic changes, and then associating a descriptive label to the segment between two topic shifts. Our current strategy for topic shift detection (Banerjee and Rudnicky, 2006a) is to perform an edge detection using such features as speech activity (who spoke when and for how long), the words that each person spoke, etc. For labeling, we are currently simply associating the agenda item names recorded in the notes with the segments they are most relevant to, as decided by a tf.idf matching technique. Topic detection is particularly useful during meeting information retrieval; (Banerjee et al., 2005) showed that when users wish to retrieve information from past meetings, they are typically interested in a specific discussion topic, as opposed to an entire meeting. Ac</context>
<context position="5753" citStr="Banerjee and Rudnicky, 2006" startWordPosition="907" endWordPosition="911">y one of the most important outcomes of a meeting are the action items decided upon, and automatically recording them could be a huge benefit especially to those participants that are likely to not note them down and consequently forget about them later on. Meeting participant role detection: Each meeting participant plays a variety of roles in an institution. These roles can be based on their function in the institution (managers, assistants, professors, students, etc), or based on their expertise (speech recognition experts, facilities experts, etc). Our current strategy for role detection (Banerjee and Rudnicky, 2006b) is to train detectors on hand labeled data. Our next step is to perform discovery of new roles through clustering techniques. Detecting such roles has several benefits. First, it allows us to build prior expectations of a meeting between a group of participants. For example, if we know person A is a speech recognition expert and person B a speech synthesis expert, a reasonable expectation is that when they meet they are likely to talk about technologies related speech processing. Consequently, we can use this expectation to aid the action item detection and the topic detection in that meeti</context>
</contexts>
<marker>Banerjee, Rudnicky, 2006</marker>
<rawString>S. Banerjee and A. I. Rudnicky. 2006b. You are what you say: Using meeting participants’ speech to detect their roles and expertise. In Analyzing Conversations in Text and Speech Workshop at HLT–NAACL 2006, New York City, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>J Cohen</author>
<author>T Quisel</author>
<author>A Chan</author>
<author>Y Patodia</author>
<author>Z Al-Bawab</author>
<author>R Zhang</author>
<author>P Rybski</author>
<author>M Veloso</author>
<author>A Black</author>
<author>R Stern</author>
<author>R Rosenfeld</author>
<author>A I Rudnicky</author>
</authors>
<title>Creating multi-modal, user–centric records of meetings with the Carnegie Mellon meeting recorder architecture.</title>
<date>2004</date>
<booktitle>In Proceedings of the ICASSP Meeting Recognition Workshop,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="7148" citStr="Banerjee et al., 2004" startWordPosition="1135" endWordPosition="1138">two major components: The note taking application which meeting participants use to take notes during the meeting, and the note retrieval application which users use to retrieve notes at a later point. 3.1 SmartNotes Note Taking Application The note taking application is a stand–alone system, that runs on each meeting participant’s laptop, and allows him to take notes during the meeting. In addition to recording the text notes, it also records the participant’s speech, and video, if a video camera is connected to the laptop. This system is an extension of the Carnegie Mellon Meeting Recorder (Banerjee et al., 2004). Figure 1 shows a screen–shot of this application. It is a server–client application, and each participant logs into a central server at the beginning of each meeting. Thus, the system knows the precise identity of each note taker as well as each speaker in the meeting. This allows us to avoid the onerous problem of automatically detecting who is speaking at any time during the meeting. Further, after logging on, each client automatically synchronizes itself with a central NTP time server. Thus the time stamps that each client associates with its recordings are all synchronized, to facilitate</context>
<context position="11516" citStr="Banerjee et al., 2004" startWordPosition="1887" endWordPosition="1890">he form with all the speech within a window of time before the form filling action, and use this pair as a data point to retrain its action item detector. 3.2 SmartNotes Note Retrieval Website As notes and audio/video are recorded on each individual participant’s laptop, they also get transferred over the internet to a central meeting server. This transfer occurs in the background without any intervention from the user, utilizes only the left–over bandwidth beyond the user’s current bandwidth usage, and is robust to system shut–downs, crashes, etc. This process is described in more detail in (Banerjee et al., 2004). Once the meeting is over and all the data has been transferred to the central server, meeting participants can use the SmartNotes multi–media notes retrieval system to view the notes and access the recorded audio/video. This is a web–based application that uses the same login process as the stand–along note 263 Figure 2: Screen shot ofthe SmartNotes website taking system. Users can view a list of meetings they have recorded using the SmartNotes application in the past, and then for each meeting, they can view the shared notes taken at the meeting. Figure 2 shows a screen shot of such a notes</context>
</contexts>
<marker>Banerjee, Cohen, Quisel, Chan, Patodia, Al-Bawab, Zhang, Rybski, Veloso, Black, Stern, Rosenfeld, Rudnicky, 2004</marker>
<rawString>S. Banerjee, J. Cohen, T. Quisel, A. Chan, Y. Patodia, Z. Al-Bawab, R. Zhang, P. Rybski, M. Veloso, A. Black, R. Stern, R. Rosenfeld, and A. I. Rudnicky. 2004. Creating multi-modal, user–centric records of meetings with the Carnegie Mellon meeting recorder architecture. In Proceedings of the ICASSP Meeting Recognition Workshop, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>C Rose</author>
<author>A I Rudnicky</author>
</authors>
<title>The necessity of a meeting recording and playback system, and the benefit of topic–level annotations to meeting browsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth International Conference on Human-Computer Interaction,</booktitle>
<location>Rome, Italy,</location>
<contexts>
<context position="1186" citStr="Banerjee et al., 2005" startWordPosition="176" endWordPosition="179">em detectors to automatically improve their performance over a sequence of meetings. The SmartNotes system consists of a laptop based note taking application, and a web based note retrieval system. We shall demonstrate the functionalities of this system, and will also demonstrate the labeled data obtained during typical meetings and browsing sessions. 1 Goals of the SmartNotes System Most institutions hold a large number of meetings every day. Several of these meetings are important, and meeting participants need to recall the details of the discussions at a future date. In a previous survey (Banerjee et al., 2005) of busy professors at Carnegie Mellon University we showed that meeting participants needed to recall details of past meetings on average about twice a month. Performing such retrieval is not an easy task. It is time consuming; in our study participants took on average between 15 minutes to an hour to recall the information they were seeking. Further, the quality of the retrieval is dependent on whether or not the participants had access to the notes at the meeting. On a scale of 0 to 5, with 5 denoting complete satisfaction with retrieval results, participants reported a satisfaction of 3.4 </context>
<context position="4783" citStr="Banerjee et al., 2005" startWordPosition="755" endWordPosition="758"> a meeting when the topic changes, and then associating a descriptive label to the segment between two topic shifts. Our current strategy for topic shift detection (Banerjee and Rudnicky, 2006a) is to perform an edge detection using such features as speech activity (who spoke when and for how long), the words that each person spoke, etc. For labeling, we are currently simply associating the agenda item names recorded in the notes with the segments they are most relevant to, as decided by a tf.idf matching technique. Topic detection is particularly useful during meeting information retrieval; (Banerjee et al., 2005) showed that when users wish to retrieve information from past meetings, they are typically interested in a specific discussion topic, as opposed to an entire meeting. Action item detection: An obvious application of meeting understanding is the automatic discovery and recording of action items as they are discussed during a meeting. Arguably one of the most important outcomes of a meeting are the action items decided upon, and automatically recording them could be a huge benefit especially to those participants that are likely to not note them down and consequently forget about them later on.</context>
</contexts>
<marker>Banerjee, Rose, Rudnicky, 2005</marker>
<rawString>S. Banerjee, C. Rose, and A. I. Rudnicky. 2005. The necessity of a meeting recording and playback system, and the benefit of topic–level annotations to meeting browsing. In Proceedings of the Tenth International Conference on Human-Computer Interaction, Rome, Italy, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>