<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.996654">
Evaluating Dependency Parsing:
Robust and Heuristics-Free Cross- nnotation Evaluation
</title>
<author confidence="0.973482">
Reut Tsarfaty Joakim Nivre Evelina ndersson
</author>
<affiliation confidence="0.9663245">
Uppsala University Uppsala University Uppsala University
Sweden Sweden Sweden
</affiliation>
<sectionHeader confidence="0.881436" genericHeader="abstract">
bstract
</sectionHeader>
<bodyText confidence="0.992786588235294">
Methods for evaluating dependency parsing
using attachment scores are highly sensitive
to representational variation between depen-
dency treebanks, making cross-experimental
evaluation opaque. This paper develops a ro-
bust procedure for cross-experimental eval-
uation, based on deterministic unification-
based operations for harmonizing different
representations and a refined notion of tree
edit distance for evaluating parse hypothe-
ses relative to multiple gold standards. We
demonstrate that, for different conversions of
the Penn Treebank into dependencies, perfor-
mance trends that are observed for parsing
results in isolation change or dissolve com-
pletely when parse hypotheses are normalized
and brought into the same common ground.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976229166667">
Data-driven dependency parsing has seen a consid-
erable surge of interest in recent years. Dependency
parsers have been tested on parsing sentences in En-
glish (Yamada and Matsumoto, 2003; Nivre and
Scholz, 2004; McDonald et al., 2005) as well as
many other languages (Nivre et al., 2007a). The
evaluation metric traditionally associated with de-
pendency parsing is based on scoring labeled or
unlabeled attachment decisions, whereby each cor-
rectly identified pair of head-dependent words is
counted towards the success of the parser (Buchholz
and Marsi, 2006). As it turns out, however, such
evaluation procedures are sensitive to the annotation
choices in the data on which the parser was trained.
Different annotation schemes often make differ-
ent assumptions with respect to how linguistic con-
tent is represented in a treebank (Rambow, 2010).
The consequence of such annotation discrepancies is
that when we compare parsing results across differ-
ent experiments, even ones that use the same parser
and the same set of sentences, the gap between re-
sults in different experiments may not reflect a true
gap in performance, but rather a difference in the an-
notation decisions made in the respective treebanks.
Different methods have been proposed for making
dependency parsing results comparable across ex-
periments. These methods include picking a single
gold standard for all experiments to which the parser
output should be converted (Carroll et al., 1998; Cer
et al., 2010), evaluating parsers by comparing their
performance in an embedding task (Miyao et al.,
2008; Buyko and Hahn, 2010), or neutralizing the
arc direction in the native representation of depen-
dency trees (Schwartz et al., 2011).
Each of these methods has its own drawbacks.
Picking a single gold standard skews the results in
favor of parsers which were trained on it. Trans-
forming dependency trees to a set of pre-defined la-
beled dependencies, or into task-based features, re-
quires the use of heuristic rules that run the risk of
distorting correct information and introducing noise
of their own. Neutralizing the direction of arcs is
limited to unlabeled evaluation and local context,
and thus may not cover all possible discrepancies.
This paper proposes a new three-step protocol for
cross-experiment parser evaluation, and in particu-
lar for comparing parsing results across data sets
that adhere to different annotation schemes. In the
</bodyText>
<page confidence="0.985034">
385
</page>
<note confidence="0.95795">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 385–396,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999856925925926">
first step all structures are brought into a single for-
mal space of events that neutralizes representation
peculiarities (for instance, arc directionality). The
second step formally computes, for each sentence
in the data, the common denominator of the differ-
ent gold standards, containing all and only linguistic
content that is shared between the different schemes.
The last step computes the normalized distance from
this common denominator to parse hypotheses, mi-
nus the cost of distances that reflect mere annotation
idiosyncrasies. The procedure that implements this
protocol is fully deterministic and heuristics-free.
We use the proposed procedure to compare de-
pendency parsing results trained on Penn Treebank
trees converted into dependency trees according to
five different sets of linguistic assumptions. We
show that when starting off with the same set of
sentences and the same parser, training on differ-
ent conversion schemes yields apparently significant
performance gaps. When results across schemes are
normalized and compared against the shared linguis-
tic content, these performance gaps decrease or dis-
solve completely. This effect is robust across parsing
algorithms. We conclude that it is imperative that
cross-experiment parse evaluation be a well thought-
through endeavor, and suggest ways to extend the
protocol to additional evaluation scenarios.
</bodyText>
<sectionHeader confidence="0.991905" genericHeader="method">
2 The Challenge: Treebank Theories
</sectionHeader>
<bodyText confidence="0.99509575">
Dependency treebanks contain information about
the grammatically meaningful elements in the utter-
ance and the grammatical relations between them.
Even if the formal representation in a dependency
treebank is well-defined according to current stan-
dards (K¨ubler et al., 2009), there are different ways
in which the trees can be used to express syntactic
content (Rambow, 2010). Consider, for instance, al-
gorithms for converting the phrase-structure trees in
the Penn Treebank (Marcus et al., 1993) into depen-
dency structures. Different conversion algorithms
implicitly make different assumptions about how to
represent linguistic content in the data. When mul-
tiple conversion algorithms are applied to the same
data, we end up with different dependency trees for
the same sentences (Johansson and Nugues, 2007;
Choi and Palmer, 2010; de Marneffe et al., 2006).
Some common cases of discrepancies are as follows.
Lexical vs. Functional Head Choice. In linguis-
tics, there is a distinction between lexical heads and
functional heads. A lexical head carries the seman-
tic gist of a phrase while a functional one marks its
relation to other parts of the sentence. The two kinds
of heads may or may not coincide in a single word
form (Zwicky, 1993). Common examples refer to
prepositional phrases, such as the phrase “on Sun-
day”. This phrase has two possible analyses, one se-
lects a lexical head (1a) and the other selects a func-
tional one (1b), as depicted below.
Similar choices are found in phrases which contain
functional elements such as determiners, coordina-
tion markers, subordinating elements, and so on.
Multi-Headed Constructions. Some phrases are
considered to have multiple lexical heads, for in-
stance, coordinated structures. Since dependency-
based formalisms require us to represent all con-
tent as binary relations, there are different ways we
could represent such constructions. Let us consider
the coordination of nominals below. We can choose
between a functional head (1a) and a lexical head
(2b, 2c). We can further choose between a flat rep-
resentation in which the first conjunct is a single
head (2b), or a nested structure where each con-
junct/marker is the head of the following element
(2c). All three alternatives empirically exist. Exam-
ple (2a) reflects the structures in the CoNLL 2007
shared task data (Nivre et al., 2007a). Johansson
and Nugues (2007) use structures like (2b). Exam-
ple (2c) reflects the analysis of Mel’ˇcuk (1988).
Periphrastic Marking. When a phrase includes
periphrastic marking — such as the tense and modal
marking in the phrase “would have worked” below
— there are different ways to consider its division
into phrases. One way to analyze this phrase would
be to choose auxiliaries as heads, as in (3a). Another
alternative would be to choose the final verb as the
</bodyText>
<figure confidence="0.958689823529412">
(2a) and
(2b) earth
(2c) earth
conj
fire
coord
wind
coord
and
earth wind fire
conj cc
wind and fire
(1b) on
(1a) Sunday
prep pobj
on
Sunday
</figure>
<page confidence="0.844997">
386
</page>
<table confidence="0.711252727272727">
Experiment Gold Parse
#1 arrive arrive
tmod tmod
on on
pobj pobj
Sunday Sunday
#2 arrive arrive
tmod tmod
Sunday Sunday
prep prep
on on
</table>
<figureCaption confidence="0.996758">
Figure 1: Calculating cross-experiment LAS results
</figureCaption>
<bodyText confidence="0.999940690909091">
main head, and let the auxiliaries create a verb chain
with different levels of projection. Each annotation
decision dictates a different direction of the arcs and
imposes its own internal division into phrases.
In standard settings, an experiment that uses
a data set which adheres to a certain annotation
scheme reports results that are compared against the
annotation standard that the parser was trained on.
But if parsers were trained on different annotation
standards, the empirical results are not comparable
across experiments. Consider, for instance, the ex-
ample in Figure 1. If parse1 and parse2 are com-
pared against gold2 using labeled attachment scores
(LAS), then parse1 results are lower than the results
of parse2, even though both parsers produced lin-
guistically correct and perfectly useful output.
Existing methods for making parsing results com-
parable across experiments include heuristics for
converting outputs into dependency trees of a prede-
fined standard (Briscoe et al., 2002; Cer et al., 2010)
or evaluating the performance of a parser within an
embedding task (Miyao et al., 2008; Buyko and
Hahn, 2010). However, heuristic rules for cross-
annotation conversion are typically hand written and
error prone, and may not cover all possible discrep-
ancies. Task-based evaluation may be sensitive to
the particular implementation of the embedding task
and the procedures that extract specific task-related
features from the different parses. Beyond that,
conversion heuristics and task-based procedures are
currently developed almost exclusively for English.
Other languages typically lack such resources.
A recent study by Schwartz et al. (2011) takes
a different approach towards cross-annotation eval-
uation. They consider different directions of
head-dependent relations (such as on-+Sunday
and Sunday-+on) and different parent-child and
grandparent-child relations in a chain (such as
arrive-+on and arrive-+sunday in “arrive on sun-
day”) as equivalent. They then score arcs that fall
within corresponding equivalence sets. Using these
new scores Schwartz et al. (2011) neutralize certain
annotation discrepancies that distort parse compar-
ison. However, their treatment is limited to local
context and does not treat structures larger than two
sequential arcs. Additionally, since arcs in differ-
ent directions are typically labeled differently, this
method only applies for unlabeled dependencies.
What we need is a fully deterministic and for-
mally precise procedure for comparing any set of la-
beled or unlabeled dependency trees, by consolidat-
ing the shared linguistic content of the complete de-
pendency trees in different annotation schemes, and
comparing parse hypotheses through sound metrics
that can take into account multiple gold standards.
</bodyText>
<sectionHeader confidence="0.765713" genericHeader="method">
3 The Proposal: Cross- nnotation
Evaluation in Three Simple Steps
</sectionHeader>
<bodyText confidence="0.999995904761905">
We propose a new protocol for cross-experiment
parse evaluation, consisting of three fundamental
components: (i) abstracting away from annotation
peculiarities, (ii) generalizing theory-specific struc-
tures into a single linguistically coherent gold stan-
dard that contains all and only consistent informa-
tion from all sources, and (iii) defining a sound met-
ric that takes into account the different gold stan-
dards that are being considered in the experiments.
In this section we first define functional trees as
the common space of formal objects and define a de-
terministic conversion procedure from dependency
trees to functional trees. Next we define a set of for-
mal operations on functional trees that compute, for
every pair of corresponding trees of the same yield, a
single gold tree that resolves inconsistencies among
gold standard alternatives and combines the infor-
mation that they share. Finally, we define scores
based on tree edit distance, refined to consider the
distance from parses to the overall gold tree as well
as the different annotation alternatives.
</bodyText>
<figure confidence="0.998316684210526">
Parse
#1
#2
Gold:
#1
0.0
1.0
# 2
0.0
1.0
(3a) would
(3b) worked
have
vg vg
have
vg
vg
would
worked
</figure>
<page confidence="0.986421">
387
</page>
<bodyText confidence="0.996580081632653">
Preliminaries. Let T be a finite set of terminal
symbols and let L be a set of grammatical relation
labels. A dependency graph d is a directed graph
which consists of nodes Vd and arcs Ad ⊆ Vd × Vd.
We assume that all nodes in Vd are labeled by ter-
minal symbols via a function labely : Vd → T. A
well-formed dependency graph d = (Vd, Ad) for a
sentence S = t1, t2, ..., to is any dependency graph
that is a directed tree originating out of a node v0
labeled t0 = ROOT, and spans all terminals in
the sentence, that is, for every ti ∈ S there exists
vj ∈ Vd labeled labely (vj) = ti. For simplicity we
assume that every node vj is indexed according to
the position of the terminal label, i.e., that for each
ti labeling vj, i always equals j. In a labeled de-
pendency tree, arcs in Ad are labeled by elements
of L via a function labelA : Ad → L that encodes
the grammatical relation between the terminals la-
beling the connected nodes. We define two auxiliary
functions on nodes in dependency trees. The func-
tion subtree : Vd → P(Vd) assigns to every node
v ∈ Vd the set of nodes accessible by it through
the reflexive transitive closure of the arc relation Ad.
The function span : Vd → P(T) assigns to every
node v ∈ Vd a set of terminals such that span(v) =
{t ∈ T |t = labely(u) and u ∈ subtree(v)}.1
Step 1: Functional Representation Our first goal
is to define a representation format that keeps all
functional relationships that are represented in the
dependency trees intact, but remains neutral with
respect to the directionality of the head-dependent
relations. To do so we define functional trees
— linearly-ordered labeled trees which, instead of
head-to-head binary relations, represent the com-
plete functional structure of a sentence. Assuming
the same sets of terminal symbols T and grammat-
ical relation labels L, and assuming extended sets
of nodes V and arcs A ⊆ V × V , a functional tree
7r = (V, A) is a directed tree originating from a sin-
gle root v0 ∈ V where all non-terminal nodes in
7r are labeled with grammatical relation labels that
signify the grammatical function of the chunk they
dominate inside the tree via labelNT : V → L. All
terminal nodes in 7r are labeled with terminal sym-
bols via a labelT : V → T function. The function
span : V → P(V ) now picks out the set of ter-
minal labels of the terminal nodes accessible by a
node v ∈ V via A. We obtain functional trees from
dependency trees using the following procedure:
</bodyText>
<listItem confidence="0.993018">
• Initialize the set of nodes and arcs in the tree.
</listItem>
<equation confidence="0.9548425">
V := Vd
A := Ad
</equation>
<listItem confidence="0.875615833333333">
• Label each node v ∈ V with the label of its
incoming arc.
labelNT(v) = labelA(u, v)
• In case |span(v) |&gt; 1 add a new node u as a
daughter designating the lexical head, labeled
with the wildcard symbol *:
</listItem>
<equation confidence="0.999596">
V := V ∪ {u}
A := A ∪ {(v, u)}
labelNT(u) = ∗
</equation>
<listItem confidence="0.998256333333333">
• For each node v such that |span(v) |= 1, add a
new node u as a daughter, labeled with its own
terminal:
</listItem>
<equation confidence="0.9986905">
V := V ∪ {u}
A := A ∪ {(v, u)}
if (labelNT(v) =� ∗)
labelT(u) := labely(v)
else
labelT(u) := labely (parent(v))
</equation>
<bodyText confidence="0.999892416666667">
That is to say, we label all nodes with spans
greater than 1 with the grammatical function of their
head, and for each node we add a new daughter u
designating the head word, labeled with its gram-
matical function. Wildcard labels are compatible
with any, more specific, grammatical function of the
word inside the phrase. This gives us a constituency-
like representation of dependency trees labeled with
functional information, which retains the linguis-
tic assumptions reflected in the dependency trees.
When applying this procedure, examples (1)–(3) get
transformed into (4)–(6) respectively.
</bodyText>
<equation confidence="0.530533">
(4b) ...
</equation>
<footnote confidence="0.6845665">
* pobj
on Sunday
1If a dependency tree d is projective, than for all v E Vd the
terminals in span(v) form a contiguous segment of S. The cur-
rent discussion assumes that all trees are projective. We com-
ment on non-projective dependencies in Section 4.
</footnote>
<equation confidence="0.3632502">
(4a) ...
prep
on
*
Sunday
</equation>
<page confidence="0.982898">
388
</page>
<bodyText confidence="0.990037536231884">
Considering the functional trees resulting from
our procedure, it is easy to see that for tree pairs
(4a)–(4b) and (5a)–(5b) the respective functional
trees are identical modulo wildcards, while tree pairs
(5b)–(5c) and (6a)–(6b) end up with different tree
structures that realize different assumptions con-
cerning the internal structure of the tree. In order
to compare, combine or detect inconsistencies in the
information inherent in different functional trees, we
define a set of formal operations that are inspired by
familiar notions from unification-based formalisms
(Shieber (1986) and references therein).
Step 2: Formal Operations on Trees The intu-
ition behind the formal operations we define is sim-
ple. A completely flat tree over a span is the most
general structural description that can be given to it.
The more nodes dominate a span, the more linguis-
tic assumptions are made with respect to its struc-
ture. If an arc structure in one tree merely elaborates
an existing flat span in another tree, the theories un-
derlying the schemes are compatible, and their in-
formation can be combined. Otherwise, there exists
a conflict in the linguistic assumptions, and we need
to relax some of the assumptions, i.e., remove func-
tional nodes, in order to obtain a coherent structure
that contains the information on which they agree.
Let 7r1, 7r2 be functional trees over the same yield
t1, .., tr,,. Let the function span(v) pick out the ter-
minals labeling terminal nodes that are accessible
via a node v ∈ V in the functional tree through the
relation A. We define first the tree subsumption re-
lation for comparing the amount of information in-
herent in the arc-structure of two trees.2
T-Subsumption, denoted Et, is a relation be-
tween trees which indicates that a tree 7r1 is
consistent with and more general than tree
7r2. Formally: 7r1 Ct 7r2 iff for every node
n ∈ 7r1 there exists a node m ∈ 7r2 such
that span(n) = span(m) and label(n) =
label(m).
Looking at the functional trees of (4a)–(4b) we
see that their unlabeled skeletons mutually subsume
each other. In their labeled versions, however, each
tree contains labeling information that is lacking in
the other. In the functional trees (5b)–(5c) a flat
structure over a span in (5b) is more elaborated in
(5c). In order to combine information in trees with
compatible arc structures, we define tree unification.
T-Unification, denoted Lit, is the operation that
returns the most general tree structure 7r3 that
is subsumed by both 7r1, 7r2 if such exists, and
fails otherwise. Formally: 7r1 Ut 7r2 = 7r3 iff
7r1 Ct 7r3 and 7r2 Ct 7r3, and for all 7r4 such that
7r1 Ct 7r4 and 7r2 Ct 7r4 it holds that 7r3 Ct 7r4.
Tree unification collects the information from two
trees into a single result if they are consistent, and
detects an inconsistency otherwise. In case of an
inconsistency, as is the case in the functional trees
(6a) and (6b), we cannot unify the structures due
to a conflict concerning the internal division of an
expression into phrases. However, we still want to
generalize these two trees into one tree that contains
all and only the information that they share. For that
we define the tree generalization operation.
T-Generalization, denoted fit, is the operation
that returns the most specific tree that is more
general than both trees. Formally, 7r1 flt 7r2 =
7r3 iff 7r3 Ct 7r1 and 7r3 Ct 7r2, and for every 7r4
such that 7r4 Ct 7r1 and 7r4 Ct 7r2 it holds that
</bodyText>
<footnote confidence="0.874503166666667">
7r4 _Et 7r3.
2Note that the wildcard symbol * is equal to any other sym-
bol. In case the node labels consist of complex feature structures
made of attribute-value lists, we replace label(n) = label(m)
in the subsumption definition with label(n) C label(m) in the
sense of (Shieber, 1986).
</footnote>
<figure confidence="0.999727302325581">
(5a)
...
(5b) ...
(5c) ...
(6a) ...
(6b) ...
conj
earth
conj
wind
conj
and fire
*
conj
earth wind
conj
and fire
*
coord
*
cc
*
earth
coord
*
v9
v9
*
*
would
v9
worked
*
have
worked
v9
would
have
*
and
wind
conj
fire
</figure>
<page confidence="0.773281">
389
</page>
<figureCaption confidence="0.97649092">
Unlike unification, generalization can never fail. Our formalization follows closely the formulation
For every pair of trees there exists a tree that is more of the T-Dice measure of Emms (2008), building on
general than both: in the extreme case, pick the com- his thorough investigation of the formal and empir-
pletely flat structure over the yield, which is more ical differences between TED-based measures and
general than any other structure. For (6a)–(6b), for Parseval. We first define for any ordered and labeled
instance, we get that (6a)nt(6b) is a flat tree over tree π the following operations.
pre-terminals where “would” and “have” are labeled relabel-node change the label of node v in π
with ‘vg’ and “worked” is the head, labeled with ‘*’. delete-node delete a non-root node v in π with
The generalization of two functional trees pro- parent u, making the children of v the children
vides us with one structure that reflects the common of u, inserted in the place of v as a subsequence
and consistent content of the two trees. These struc- in the left-to-right order of the children of u.
tures thus provide us with a formally well-defined insert-node insert a node v as a child of u in
gold standard for cross-treebank evaluation. π making it the parent of a consecutive subse-
Step 3: Measuring Distances. Our functional quence of the children of u.
trees superficially look like constituency-based An edit script ES(π1, π2) = {e0, e1....ekI between
trees, so a simple proposal would be to use Parse- π1 and π2 is a set of edit operations required for turn-
val measures (Black et al., 1991) for comparing the ing π1 into π2. Now, assume that we are given a cost
parsed trees against the new generalized gold trees. function defined for each edit operation. The cost of
Parseval scores, however, have two significant draw- ES(π1, π2) is the sum of the costs of the operations
backs. First, they are known to be too restrictive in the script. An optimal edit script is an edit script
with respect to some errors and too permissive with between π1 and π2 of minimum cost.
respect to others (Carroll et al., 1998; K¨ubler and �ES*(π1, π2) = argminES(1r1,1r2) cost(e)
Telljohann, 2002; Roark, 2002; Rehbein and van eEES(1r1,1r2)
Genabith, 2007). Secondly, F1 scores would still The tree edit distance problem is defined to be the
penalize structures that are correct with respect to problem of finding the optimal edit script and com-
</figureCaption>
<bodyText confidence="0.748735666666667">
the original gold, but are not there in the generalized puting the corresponding distance (Bille, 2005).
structure. Here we propose to adopt measures that A simple way to calculate the error δ of a parse
are based on tree edit distance (TED) instead. TED- would be to define it as the edit distance between
based measures are, in fact, an extension of attach- the parse hypothesis π1 and the gold standard π2.
ment scores for dependency trees. Consider, for in- δ(π1,π2) = cost(ES*(π1,π2))
stance, the following operations on dependency arcs. However, in such cases the parser may still get pe-
reattach-arc remove arc (u, v) E Ad and add nalized for recovering nodes that are lacking in the
an arc Ad U {(w, v)I. generalization. To solve this, we refine the distance
relabel-arc relabel arc l1(u, v) as l2(u, v) between a parse tree and the generalized gold tree
Assuming that each operation is assigned a cost, to discard edit operations on nodes that are there in
the attachment score of comparing two dependency the native gold tree but are eliminated through gen-
trees is simply the cost of all edit operations that are eralization. We compute the intersection of the edit
required to turn a parse tree into its gold standard, script turning the parse tree into the generalize gold
normalized with respect to the overall size of the de- with the edit script turning the native gold tree into
pendency tree and subtracted from a unity.3 Here the generalized gold, and discard its cost. That is, if
we apply the idea of defining scores by TED costs parse1 and parse2 are compared against gold1 and
normalized relative to the size of the tree and sub- gold2 respectively, and if we set gold3 to be the re-
stracted from a unity, and extend it from fixed-size sult of gold1ntgold2, then δnew is defined as:
dependency trees to ordered trees of arbitrary size.
3The size of a dependency tree, either parse or gold, is al-
ways fixed by the number of terminals.
</bodyText>
<figure confidence="0.980454235294117">
390
TB2
parse1.dep
gold1.dep
parse2.dep
gold2.dep
parse1
gold1
gold2
parse2
gold3
TB1
parse
parse
parse transform
generalize
parse
</figure>
<figureCaption confidence="0.976281333333333">
Figure 2: The evaluation pipeline. Different versions of the treebank go into different experiments, resulting in
different parse and gold files. All trees are transformed into functional trees. All gold files enter generalization to
yield a new gold. The different S arcs represent the different tree distances used for calculating the TED-based scores.
</figureCaption>
<equation confidence="0.996375333333333">
snew(parse1, gold1,gold3) =
S(parse1,gold3)
−cost(ES∗(parse1,gold3)∩ES∗(gold1,gold3))
</equation>
<bodyText confidence="0.995641666666667">
Now, if gold1 and gold3 are identi-
cal, then ES∗(gold1,gold3)=∅ and we fall
back on the simple tree edit distance score
snew(parse1,gold1,gold3)=6(parse1, gold3).
When parse1 and gold1 are identical,
i.e., the parser produced perfect out-
put with respect to its own scheme, then
snew(parse1,gold1,gold3)=Snew(gold1,gold1,gold3)
=S(gold1,gold3) − cost(ES∗(gold1,gold3))=0, and
the parser does not get penalized for recovering a
correct structure in gold1 that is lacking in gold3.
In order to turn distances into accuracy measures
we have to normalize distances relative to the maxi-
mal number of operations that is conceivable. In the
worst case, we would have to remove all the internal
nodes in the parse tree and add all the internal nodes
of the generalized gold, so our normalization factor
t is defined as follows, where |7r |is the size4 of 7r.
</bodyText>
<equation confidence="0.9619156">
t(parse1,gold3) = |parse1 |+ |gold3|
We now define the score of parse1 as follows:5
1 −
Jnew (parse 1,gold1,gold3)
t(parse1,gold3)
</equation>
<bodyText confidence="0.997978666666667">
Figure 2 summarizes the steps in the evalu-
ation procedure we defined so far. We start
off with two versions of the treebank, TB1 and
TB2, which are parsed separately and provide their
own gold standards and parse hypotheses in a la-
beled dependencies format. All dependency trees
</bodyText>
<footnote confidence="0.995201666666667">
4Following common practice, we equate size |ir |with the
number of nodes in ir, discarding the terminals and root node.
5If the trees have only root and leaves, t = 0, score := 1.
</footnote>
<bodyText confidence="0.999756363636364">
are then converted into functional trees, and we
compute the generalization of each pair of gold
trees for each sentence in the data. This pro-
vides the generalized gold standard for all exper-
iments, here marked as gold3.6 We finally com-
pute the distances snew(parse1,gold1,gold3) and
snew(parse2,gold2,gold3) using the different tree
edit distances that are now available, and we repeat
the procedure for each sentence in the test set.
To normalize the scores for an entire test set of
size n we can take the arithmetic mean of the scores.
</bodyText>
<equation confidence="0.4999035">
�|test-set |i=1score(parse1i,gold1i,gold3i)
|test-set|
</equation>
<bodyText confidence="0.999623666666667">
Alternatively we can globally average of all edit dis-
tance costs, normalized by the maximally possible
edits on parse trees turned into generalized trees.
</bodyText>
<equation confidence="0.98948975">
E Z= ltesit-set |snew (parse1 i,gold1 i,gold3i)
1 −
E|test-set |t (parse 1i,gold3i)
i=1
</equation>
<bodyText confidence="0.7499565">
The latter score, global averaging over the entire test
set, is the metric we use in our evaluation procedure.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="conclusions">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999885555555556">
We demonstrate the application of our procedure to
comparing dependency parsing results on different
versions of the Penn Treebank (Marcus et al., 1993).
The Data We use data from the PTB, converted
into dependency structures using the LTH soft-
ware, a general purpose tool for constituency-to-
dependency conversion (Johansson and Nugues,
2007). We use LTH to implement the five different
annotation standards detailed in Table 3.
</bodyText>
<footnote confidence="0.9936995">
6Generalization is an associative and commutative opera-
tion, so it can be extended for n experiments in any order.
</footnote>
<page confidence="0.99287">
391
</page>
<table confidence="0.999942568181818">
Train Default Old LTH CoNLL07
Gold
Default UAS 0.9142 0.6077 0.7772
LAS 0.8820 0.4801 0.6454
U-TED 0.9488 0.8926 0.9237
L-TED 0.9241 0.7811 0.8441
Old LTH UAS 0.6053 0.8955 0.6508
LAS 0.4816 0.8644 0.5771
U-TED 0.8931 0.9564 0.9092
L-TED 0.7811 0.9317 0.8197
CoNLL07 UAS 0.7734 0.6474 0.8917
LAS 0.6479 0.5722 0.8736
U-TED 0.9260 0.9097 0.9474
L-TED 0.8480 0.8204 0.9233
Default-OldLTH U-TED 0.9500 0.9543
L-TED 0.9278 0.9324
Default-CoNLL07 U-TED 0.9444† 0.9453†
L-TED 0.9266† 0.9260†
oldLTH-CoNLL07 U-TED 0.9519 0.9490
L-TED 0.9323 0.9283
default-oldLTH-CoNLL U-TED 0.9464† 0.9515 0.9471†
L-TED 0.9281† 0.9336 0.9280†
Train CoNLL07 Functional Lexical
Gold
CoNLL07 UAS 0.8917 0.8054 0.6986
LAS 0.8736 0.7895 0.6831
U-TED 0.9474 0.9357 0.9237
L-TED 0.9233 0.8960 0.8606
Functional UAS 0.8040 0.8970 0.6110
LAS 0.7873 0.8793 0.5977
U-TED 0.9347 0.9466 0.9107
L-TED 0.8948 0.9239 0.8316
Lexical UAS 0.7013 0.6138 0.8823
LAS 0.6875 0.6022 0.8635
U-TED 0.9252 0.9132 0.9500
L-TED 0.8623 0.8345 0.9266
CoNLL07-Functional U-TED 0.9473† 0.9473†
L-TED 0.9233 0.9247
CoNLL07-Lexical U-TED 0.9490† 0.9500†
L-TED 0.9253† 0.9266†
Functional-Lexical U-TED 0.9489† 0.9501†
L-TED 0.9266† 0.9267†
CoNLL07-Functional-Lexical U-TED 0.9489† 0.9489† 0.9501†
L-TED 0.9254† 0.9266† 0.9267†
</table>
<tableCaption confidence="0.991960666666667">
Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan-
dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported
in the same row. The † sign marks pairwise results where the difference is not statistically significant.
</tableCaption>
<table confidence="0.999971909090909">
Train Default Old LTH CoNLL07
Gold
Default UAS 0.9173 0.6085 0.7709
LAS 0.8833 0.4780 0.6414
U-TED 0.9513 0.8903 0.9236
L-TED 0.9249 0.7727 0.8424
Old LTH UAS 0.6078 0.8952 0.6415
LAS 0.4809 0.8471 0.5669
U-TED 0.8960 0.9550 0.9096
L-TED 0.7823 0.9224 0.8170
CoNLL07 UAS 0.7767 0.6517 0.8991
LAS 0.6504 0.5725 0.8709
U-TED 0.9289 0.9087 0.9479
L-TED 0.8502 0.8159 0.9208
Default-oldLTH U-TED 0.9533 0.9515
L-TED 0.9289 0.9224
Default-CoNLL U-TED 0.9474† 0.9460†
L-TED 0.9281 0.9238
OldLTH-CoNLL U-TED 0.9479 0.9493
L-TED 0.9234 0.9258
Default-OldLTH-CoNLL U-TED 0.9492† 0.9461 0.9480†
L-TED 0.9298 0.9241† 0.9258†
Train CoNLL07 Functional Lexical
Gold
CoNLL07 UAS 0.8991 0.8077 0.7018
LAS 0.8709 0.7902 0.6804
U-TED 0.9479 0.9373 0.9221
L-TED 0.9208 0.8955 0.8505
Functional UAS 0.8083 0.8978 0.6150
LAS 0.7895 0.8782 0.5975
U-TED 0.9356 0.9476 0.9092
L-TED 0.8929 0.9226 0.8218
Lexical UAS 0.6997 0.6161 0.8826
LAS 0.6835 0.6034 0.8491
U-TED 0.9259 0.9152 0.9483
L-TED 0.8593 0.8340 0.9160
CoNLL-Functional U-TED 0.9479† 0.9487†
L-TED 0.9209 0.9237
CoNLL-Lexical U-TED 0.9497 0.9483
L-TED 0.9228 0.9161
Functional-Lexical U-TED 0.9504 0.9483
L-TED 0.9258 0.9161
CoNLL-Functional-Lexical U-TED 0.9498 0.9504† 0.9483†
L-TED 0.9229 0.9258 0.9161
</table>
<tableCaption confidence="0.983387666666667">
Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We
report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results
reported in the same row. The † sign marks pairwise results where the difference is not statistically significant.
</tableCaption>
<table confidence="0.995439333333333">
ID Description
Default The LTH conversion default settings
OldLTH The conversion used in Johansson and Nugues (2007)
CoNLL07
Lexical
Functional
The conversion used in the CoNLL shared task (Nivre et al., 2007a)
Same as CoNLL, but selecting only lexical heads when a choice exists
Same as CoNLL, but selecting only functional heads when a choice exists
</table>
<tableCaption confidence="0.9993525">
Table 3: LTH conversion schemes used in the experiments. The LTH conversion settings in terms of the complete
feature-value pairs associated with the LTH parameters in different schemes are detailed in the supplementary material.
</tableCaption>
<page confidence="0.996805">
392
</page>
<bodyText confidence="0.955677860465116">
The Default, OldLTH and CoNLL schemes
mainly differ in their coordination structure, and the
Functional and Lexical schemes differ in their selec-
tion of a functional and a lexical head, respectively.
All schemes use the same inventory of labels.7 The
LTH parameter settings for the different schemes are
elaborated in the supplementary material.
The Setup We use two different parsers: (i) Malt-
Parser (Nivre et al., 2007b) with the arc eager algo-
rithm as optimized for English in (Nivre et al., 2010)
and (ii) MSTParser with the second-order projec-
tive model of McDonald and Pereira (2006). Both
parsers were trained on the different instances of
sections 2-21 of the PTB obeying the different an-
notation schemes in Table 3. Each trained model
was used to parse section 23. All non-projective de-
pendencies in the training and gold sets were projec-
tivized prior to training and parsing using the algo-
rithm of Nivre and Nilsson (2005). A more princi-
pled treatment of non-projective dependency trees is
an important topic for future research. We evaluated
the parses using labeled and unlabeled attachment
scores, and using our TEDEVAL software package.
Evaluation Our TEDEVAL software package im-
plements the pipeline described in Section 3. We
convert all parse and gold trees into functional
trees using the algorithm defined in Section 3, and
for each pair of parsing experiments we calculate
a shared gold standard using generalization deter-
mined through a chart-based greedy algorithm.8 Our
scoring procedure uses the TED algorithm defined
by Zhang and Shasha (1989).9 The unlabeled score
is obtained by assigning cost(e) = 0 for every e re-
labeling operation. To calculate pairwise statistical
significance we use a shuffling test with 10,000 it-
erations (Cohen, 1995). A sample of all files in the
evaluation pipeline for a subset of 10 PTB sentences
is available in the supplementary materials.10
7In case the labels are not taken from the same inventory,
e.g., subjects in one scheme are marked as SUB and in the other
marked as SBJ, it is possible define a a set of zero-cost operation
types — in such case, to the operation relabel(SUB,SBJ) — in
order not to penalize string label discrepancies.
</bodyText>
<footnote confidence="0.9952562">
8Our algorithm has space and runtime complexity of O(n2).
9Available via http://web.science.mq.edu. u/
-sw n/howtos/treedist nce/
10The TEDEVAL software package is available via http:
//stp.lingfil.uu.se/-ts rf ty/unip r
</footnote>
<bodyText confidence="0.999881541666667">
Results Table 1 reports the results for the inter-
and cross-experiment evaluation of parses produced
by MaltParser. The left hand side of the table
presents the parsing results for a set of experiments
in which we compare parsing results trained on the
Default, OldLTH and CoNLL07 schemes. In a sec-
ond set of experiments we compare the CoNLL07,
Lexical and Functional schemes. Table 2 reports the
evaluation of the parses produced by MSTParser for
the same experimental setup. Our goal here is not to
compare the parsers, but to verify that the effects of
switching from LAS to TEDEVAL are robust across
parsing algorithms.
In each of the tables, the top three groups of four
rows compare results of parsed dependency trees
trained on a particular scheme against gold trees of
the same and the other schemes. The next three
groups of two rows report the results for compar-
ing pairwise sets of experiments against a general-
ized gold using our proposed procedure. In the last
group of two rows we compare all parsing results
against a single gold obtained through a three-way
generalization.
As expected, every parser appears to perform at
its best when evaluated against the scheme it was
trained on. This is the case for both LAS and TEDE-
VAL measures and the performance gaps are statis-
tically significant. When moving to pairwise evalu-
ation against a single generalized gold, for instance,
when comparing CoNLL07 to the Default settings,
there is still a gap in performance, e.g., between
OldLTH and CoNLL07, and between OldLTH and
Default. This gap is however a lot smaller and is not
always statistically significant. In fact, when evalu-
ating the effect of linguistically disparate annotation
variations such as Lexical and Functional on the per-
formance of MaltParser, Table 1 shows that when
using TEDEVAL and a generalized gold the perfor-
mance gaps are small and statistically insignificant.
Moreover, observed performance trends when
evaluating individual experiments on their original
training scheme may change when compared against
a generalized gold. The Default scheme, for Malt-
Parser, appears better than OldLTH when both are
evaluated against their training schemes. But look-
ing at the pairwise-evaluated experiments, it is the
other way round (the difference is smaller, but statis-
tically significant). In evaluating against a three-way
</bodyText>
<page confidence="0.977403">
393
</page>
<bodyText confidence="0.996350085106383">
generalization, all the results obtained for different is straightforward to define: simply replace the node
training schemes are on a par with one another, with labels with the grammatical function of their domi-
minor gaps in performance, rarely statistically sig- nating arc – and the rest of the pipeline follows.
nificant. This suggests that apparent performance A pre-condition for cross-framework evaluation
trends between experiments when evaluating with is that all representations encode the same set of
respect to the training schemes may be misleading. grammatical relations by, e.g., annotating arcs in de-
These observations are robust across parsing algo- pendency trees or decorating nodes in constituency
rithms. In each of the tables, results obtained against trees. For some treebanks this is already the case
the training schemes show significant differences (Nivre and Megyesi, 2007; Skut et al., 1997; Hin-
whereas applying our cross-experimental procedure richs et al., 2004) while for others this is still lack-
shows small to no gaps in performance across dif- ing. Recent studies (Briscoe et al., 2002; de Marn-
ferent schemes. Annotation variants which seem to effe et al., 2006) suggest that evaluation through a
have crucial effects have a relatively small influence single set of grammatical relations as the common
when parsed structures are brought into the same denominator is a linguistically sound and practically
formal and theoretical common ground for compar- useful way to go. To guarantee extensions for cross-
ison. Of course, it may be the case that one parser is framework evaluation it would be fruitful to make
better trained on one scheme while the other utilizes sure that resources use the same set of grammatical
better another scheme, but objective performance relation labels across different formal representation
gaps can only be observed when they are compared types. Moreover, we further aim to inquire whether
against shared linguistic content. we can find a single set of grammatical relation la-
5 Discussion and Extensions bels that can be used across treebanks for multiple
This paper addresses the problem of cross- languages. This would then pave the way for the de-
experiment evaluation. As it turns out, this prob- velopment of cross-language evaluation procedures.
lem arises in NLP in different shapes and forms; 6 Conclusion
when evaluating a parser against different annota- We propose an end-to-end procedure for compar-
tion schemes, when evaluating parsing performance ing dependency parsing results across experiments
across parsers and different formalisms, and when based on three steps: (i) converting dependency trees
comparing parser performance across languages. to functional trees, (ii) generalizing functional trees
We consider our contribution successful if after to harmonize information from different sources,
reading it the reader develops a healthy suspicion to and (iii) using distance-based metrics that take the
blunt comparison of numbers across experiments, or different sources into account. When applied to
better yet, across different papers. Cross-experiment parsing results of different dependency schemes,
comparison should be a careful and well thought- dramatic gaps observed when comparing parsing re-
through endeavor, in which we retain as much infor- sults obtained in isolation decrease or dissolve com-
mation as we can from the parsed structures, avoid pletely when using our proposed pipeline.
lossy conversions, and focus on an object of evalua- cknowledgments We thank the developers of
tion which is agreed upon by all variants. the LTH and TED software who made their code
Our proposal introduces one way of doing so in available for our use. We thank Richard Johansson
a streamlined, efficient and formally worked out for providing us with the LTH parameter settings of
way. While individual components may be further existing dependency schemes. We thank Ari Rap-
refined or improved, the proposed setup and imple- poport, Omri Abend, Roy Schwartz and members of
mentation can be straightforwardly applied to cross- the NLP lab at the Hebrew University of Jerusalem
parser and cross-framework evaluation. In the fu- for stimulating discussion. We finally thank three
ture we plan to use this procedure for comparing anonymous reviewers for useful comments on an
constituency and dependency parsers. A conversion earlier draft. The research reported in the paper was
from constituency-based trees into functional trees partially funded by the Swedish Research Council.
394
</bodyText>
<sectionHeader confidence="0.996059" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999876865384615">
Philip Bille. 2005. A survey on tree edit distance
and related. problems. Theoretical Computer Science,
337:217–239.
Ezra Black, Steven P. Abney, D. Flickenger, Claudia
Gdaniec, Ralph Grishman, P. Harrison, Donald Hin-
dle, Robert Ingria, Frederick Jelinek, Judith L. Kla-
vans, Mark Liberman, Mitchell P. Marcus, Salim
Roukos, Beatrice Santorini, and Tomek Strzalkowski.
1991. Procedure for quantitatively comparing the syn-
tactic coverage of English grammars. In E. Black, ed-
itor, Proceedings of the workshop on Speech and Nat-
ural Language, HLT, pages 306–311. Association for
Computational Linguistics.
Ted Briscoe, John Carroll, Jonathan Graham, and Ann
Copestake. 2002. Relational evaluation schemes.
In Proceedings of LREC Workshop“Beyond Parseval
– Towards improved evaluation measures for parsing
systems”.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149–164.
Ekaterina Buyko and Udo Hahn. 2010. Evaluating
the impact of alternative dependency graph encodings
on solving event extraction tasks. In Proceedings of
EMNLP, pages 982–992.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal. In
Proceedings of LREC, pages 447–454.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
stanford dependencies: Trade-offs between speed and
accuracy. In Proceedings of LREC.
Jinho D. Choi and Martha Palmer. 2010. Robust
constituent-to-dependency conversion for English. In
Proceedings of TLT.
Paul Cohen. 1995. Empirical Methods for rtificial In-
telligence. The MIT Press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, pages 449–454.
Martin Emms. 2008. Tree-distance and some other vari-
ants of evalb. In Proceedings of LREC.
Erhard Hinrichs, Sandra K¨ubler, Karin Naumann, Heike
Telljohan, and Julia Trushkina. 2004. Recent develop-
ment in linguistic annotations of the T¨uBa-D/Z Tree-
bank. In Proceedings of TLT.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NOD LID .
Sandra K¨ubler and Heike Telljohann. 2002. Towards
a dependency-oriented evaluation for partial parsing.
In Proceedings of LREC Workshop“Beyond Parseval
– Towards improved evaluation measures for parsing
systems”.
Sandra K¨ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Morgan
&amp; Claypool Publishers.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313–330.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of E CL, pages 81–88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of CL, pages 91–98.
Igor Mel’ˇcuk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Yusuke Miyao, Rune Sætre, Kenji Sagae, Takuya Mat-
suzaki, and Jun’ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
Proceedings of CL, pages 46–54.
Joakim Nivre and Beata Megyesi. 2007. Bootstrapping
a Swedish Treebank using cross-corpus harmonization
and annotation projection. In Proceedings of TLT.
Joakim Nivre and Jens Nilsson. 2005. Pseudo projective
dependency parsing. In Proceeding of CL, pages 99–
106.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings of
COLING, pages 64–70.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915–932.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov,
and Erwin Marsi. 2007b. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(1):1–41.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos
G´omez-Rodr´ıguez. 2010. Evaluation of dependency
parsers on unbounded dependencies. pages 813–821.
Owen Rambow. 2010. The Simple Truth about Depen-
dency and Phrase Structure Representations: An Opin-
ion Piece. In Proceedings of HLT- CL, pages 337–
340.
Ines Rehbein and Josef van Genabith. 2007. Why is it so
difficult to compare treebanks? Tiger and T¨uBa-D/Z
revisited. In Proceedings of TLT, pages 115–126.
</reference>
<page confidence="0.986814">
395
</page>
<reference confidence="0.999789481481482">
Brian Roark. 2002. Evaluating parser accuracy us-
ing edit distance. In Proceedings of LREC Work-
shop“Beyond Parseval – Towards improved evaluation
measures for parsing systems”.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-
poport. 2011. Neutralizing linguistically problematic
annotations in unsupervised dependency parsing eval-
uation. In Proceedings of CL, pages 663–672.
Stuart M. Shieber. 1986. n Introduction to Unification-
Based Grammars. Center for the Study of Language
and Information.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for free
word-order languages. In Proceedings of the fifth con-
ference on pplied natural language processing, pages
88–95.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceeding of IWPT, pages 195–206.
Kaizhong Zhang and Dennis Shasha. 1989. Simple fast
algorithms for the editing distance between trees and
related problems. In SI MJournal of Computing, vol-
ume 18, pages 1245–1262.
Arnold M. Zwicky. 1993. Heads, bases, and functors.
In G.G. Corbett, N. Fraser, and S. McGlashan, editors,
Heads in Grammatical Theory, pages 292–315. Cam-
bridge University Press.
</reference>
<page confidence="0.999089">
396
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.322607">
<title confidence="0.999513">Evaluating Dependency Parsing: Robust and Heuristics-Free Crossnnotation Evaluation</title>
<author confidence="0.969617">Reut Tsarfaty Joakim Nivre Evelina ndersson</author>
<affiliation confidence="0.999926">Uppsala University Uppsala University Uppsala University</affiliation>
<author confidence="0.446099">Sweden Sweden Sweden</author>
<abstract confidence="0.987434833333333">bstract Methods for evaluating dependency parsing using attachment scores are highly sensitive to representational variation between dependency treebanks, making cross-experimental evaluation opaque. This paper develops a robust procedure for cross-experimental evaluation, based on deterministic unificationbased operations for harmonizing different representations and a refined notion of tree edit distance for evaluating parse hypotheses relative to multiple gold standards. We demonstrate that, for different conversions of the Penn Treebank into dependencies, performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philip Bille</author>
</authors>
<title>A survey on tree edit distance and related. problems.</title>
<date>2005</date>
<journal>Theoretical Computer Science,</journal>
<pages>337--217</pages>
<contexts>
<context position="22503" citStr="Bille, 2005" startWordPosition="3712" endWordPosition="3713">ictive in the script. An optimal edit script is an edit script with respect to some errors and too permissive with between π1 and π2 of minimum cost. respect to others (Carroll et al., 1998; K¨ubler and �ES*(π1, π2) = argminES(1r1,1r2) cost(e) Telljohann, 2002; Roark, 2002; Rehbein and van eEES(1r1,1r2) Genabith, 2007). Secondly, F1 scores would still The tree edit distance problem is defined to be the penalize structures that are correct with respect to problem of finding the optimal edit script and comthe original gold, but are not there in the generalized puting the corresponding distance (Bille, 2005). structure. Here we propose to adopt measures that A simple way to calculate the error δ of a parse are based on tree edit distance (TED) instead. TED- would be to define it as the edit distance between based measures are, in fact, an extension of attach- the parse hypothesis π1 and the gold standard π2. ment scores for dependency trees. Consider, for in- δ(π1,π2) = cost(ES*(π1,π2)) stance, the following operations on dependency arcs. However, in such cases the parser may still get pereattach-arc remove arc (u, v) E Ad and add nalized for recovering nodes that are lacking in the an arc Ad U {</context>
</contexts>
<marker>Bille, 2005</marker>
<rawString>Philip Bille. 2005. A survey on tree edit distance and related. problems. Theoretical Computer Science, 337:217–239.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ezra Black</author>
<author>Steven P Abney</author>
<author>D Flickenger</author>
<author>Claudia Gdaniec</author>
<author>Ralph Grishman</author>
<author>P Harrison</author>
</authors>
<title>Procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>Proceedings of the workshop on Speech and Natural Language, HLT,</booktitle>
<pages>306--311</pages>
<editor>Donald Hindle, Robert Ingria, Frederick Jelinek, Judith L. Klavans, Mark Liberman, Mitchell P. Marcus, Salim Roukos, Beatrice Santorini, and Tomek</editor>
<publisher>Association for Computational Linguistics.</publisher>
<contexts>
<context position="21562" citStr="Black et al., 1991" startWordPosition="3552" endWordPosition="3555">s a subsequence and consistent content of the two trees. These struc- in the left-to-right order of the children of u. tures thus provide us with a formally well-defined insert-node insert a node v as a child of u in gold standard for cross-treebank evaluation. π making it the parent of a consecutive subseStep 3: Measuring Distances. Our functional quence of the children of u. trees superficially look like constituency-based An edit script ES(π1, π2) = {e0, e1....ekI between trees, so a simple proposal would be to use Parse- π1 and π2 is a set of edit operations required for turnval measures (Black et al., 1991) for comparing the ing π1 into π2. Now, assume that we are given a cost parsed trees against the new generalized gold trees. function defined for each edit operation. The cost of Parseval scores, however, have two significant draw- ES(π1, π2) is the sum of the costs of the operations backs. First, they are known to be too restrictive in the script. An optimal edit script is an edit script with respect to some errors and too permissive with between π1 and π2 of minimum cost. respect to others (Carroll et al., 1998; K¨ubler and �ES*(π1, π2) = argminES(1r1,1r2) cost(e) Telljohann, 2002; Roark, 20</context>
</contexts>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, 1991</marker>
<rawString>Ezra Black, Steven P. Abney, D. Flickenger, Claudia Gdaniec, Ralph Grishman, P. Harrison, Donald Hindle, Robert Ingria, Frederick Jelinek, Judith L. Klavans, Mark Liberman, Mitchell P. Marcus, Salim Roukos, Beatrice Santorini, and Tomek Strzalkowski. 1991. Procedure for quantitatively comparing the syntactic coverage of English grammars. In E. Black, editor, Proceedings of the workshop on Speech and Natural Language, HLT, pages 306–311. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Jonathan Graham</author>
<author>Ann Copestake</author>
</authors>
<title>Relational evaluation schemes.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC Workshop“Beyond Parseval – Towards</booktitle>
<contexts>
<context position="9122" citStr="Briscoe et al., 2002" startWordPosition="1402" endWordPosition="1405">that the parser was trained on. But if parsers were trained on different annotation standards, the empirical results are not comparable across experiments. Consider, for instance, the example in Figure 1. If parse1 and parse2 are compared against gold2 using labeled attachment scores (LAS), then parse1 results are lower than the results of parse2, even though both parsers produced linguistically correct and perfectly useful output. Existing methods for making parsing results comparable across experiments include heuristics for converting outputs into dependency trees of a predefined standard (Briscoe et al., 2002; Cer et al., 2010) or evaluating the performance of a parser within an embedding task (Miyao et al., 2008; Buyko and Hahn, 2010). However, heuristic rules for crossannotation conversion are typically hand written and error prone, and may not cover all possible discrepancies. Task-based evaluation may be sensitive to the particular implementation of the embedding task and the procedures that extract specific task-related features from the different parses. Beyond that, conversion heuristics and task-based procedures are currently developed almost exclusively for English. Other languages typica</context>
<context position="37540" citStr="Briscoe et al., 2002" startWordPosition="6064" endWordPosition="6067"> same set of respect to the training schemes may be misleading. grammatical relations by, e.g., annotating arcs in deThese observations are robust across parsing algo- pendency trees or decorating nodes in constituency rithms. In each of the tables, results obtained against trees. For some treebanks this is already the case the training schemes show significant differences (Nivre and Megyesi, 2007; Skut et al., 1997; Hinwhereas applying our cross-experimental procedure richs et al., 2004) while for others this is still lackshows small to no gaps in performance across dif- ing. Recent studies (Briscoe et al., 2002; de Marnferent schemes. Annotation variants which seem to effe et al., 2006) suggest that evaluation through a have crucial effects have a relatively small influence single set of grammatical relations as the common when parsed structures are brought into the same denominator is a linguistically sound and practically formal and theoretical common ground for compar- useful way to go. To guarantee extensions for crossison. Of course, it may be the case that one parser is framework evaluation it would be fruitful to make better trained on one scheme while the other utilizes sure that resources u</context>
</contexts>
<marker>Briscoe, Carroll, Graham, Copestake, 2002</marker>
<rawString>Ted Briscoe, John Carroll, Jonathan Graham, and Ann Copestake. 2002. Relational evaluation schemes. In Proceedings of LREC Workshop“Beyond Parseval – Towards improved evaluation measures for parsing systems”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X,</booktitle>
<pages>149--164</pages>
<contexts>
<context position="1524" citStr="Buchholz and Marsi, 2006" startWordPosition="210" endWordPosition="213"> normalized and brought into the same common ground. 1 Introduction Data-driven dependency parsing has seen a considerable surge of interest in recent years. Dependency parsers have been tested on parsing sentences in English (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005) as well as many other languages (Nivre et al., 2007a). The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). As it turns out, however, such evaluation procedures are sensitive to the annotation choices in the data on which the parser was trained. Different annotation schemes often make different assumptions with respect to how linguistic content is represented in a treebank (Rambow, 2010). The consequence of such annotation discrepancies is that when we compare parsing results across different experiments, even ones that use the same parser and the same set of sentences, the gap between results in different experiments may not reflect a true gap in performance, but rather a difference in the annota</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL-X, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Buyko</author>
<author>Udo Hahn</author>
</authors>
<title>Evaluating the impact of alternative dependency graph encodings on solving event extraction tasks.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>982--992</pages>
<contexts>
<context position="2554" citStr="Buyko and Hahn, 2010" startWordPosition="374" endWordPosition="377"> ones that use the same parser and the same set of sentences, the gap between results in different experiments may not reflect a true gap in performance, but rather a difference in the annotation decisions made in the respective treebanks. Different methods have been proposed for making dependency parsing results comparable across experiments. These methods include picking a single gold standard for all experiments to which the parser output should be converted (Carroll et al., 1998; Cer et al., 2010), evaluating parsers by comparing their performance in an embedding task (Miyao et al., 2008; Buyko and Hahn, 2010), or neutralizing the arc direction in the native representation of dependency trees (Schwartz et al., 2011). Each of these methods has its own drawbacks. Picking a single gold standard skews the results in favor of parsers which were trained on it. Transforming dependency trees to a set of pre-defined labeled dependencies, or into task-based features, requires the use of heuristic rules that run the risk of distorting correct information and introducing noise of their own. Neutralizing the direction of arcs is limited to unlabeled evaluation and local context, and thus may not cover all possi</context>
<context position="9251" citStr="Buyko and Hahn, 2010" startWordPosition="1425" endWordPosition="1428">parable across experiments. Consider, for instance, the example in Figure 1. If parse1 and parse2 are compared against gold2 using labeled attachment scores (LAS), then parse1 results are lower than the results of parse2, even though both parsers produced linguistically correct and perfectly useful output. Existing methods for making parsing results comparable across experiments include heuristics for converting outputs into dependency trees of a predefined standard (Briscoe et al., 2002; Cer et al., 2010) or evaluating the performance of a parser within an embedding task (Miyao et al., 2008; Buyko and Hahn, 2010). However, heuristic rules for crossannotation conversion are typically hand written and error prone, and may not cover all possible discrepancies. Task-based evaluation may be sensitive to the particular implementation of the embedding task and the procedures that extract specific task-related features from the different parses. Beyond that, conversion heuristics and task-based procedures are currently developed almost exclusively for English. Other languages typically lack such resources. A recent study by Schwartz et al. (2011) takes a different approach towards cross-annotation evaluation.</context>
</contexts>
<marker>Buyko, Hahn, 2010</marker>
<rawString>Ekaterina Buyko and Udo Hahn. 2010. Evaluating the impact of alternative dependency graph encodings on solving event extraction tasks. In Proceedings of EMNLP, pages 982–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ted Briscoe</author>
<author>Antonio Sanfilippo</author>
</authors>
<title>Parser evaluation: a survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>447--454</pages>
<contexts>
<context position="2420" citStr="Carroll et al., 1998" startWordPosition="352" endWordPosition="355">w, 2010). The consequence of such annotation discrepancies is that when we compare parsing results across different experiments, even ones that use the same parser and the same set of sentences, the gap between results in different experiments may not reflect a true gap in performance, but rather a difference in the annotation decisions made in the respective treebanks. Different methods have been proposed for making dependency parsing results comparable across experiments. These methods include picking a single gold standard for all experiments to which the parser output should be converted (Carroll et al., 1998; Cer et al., 2010), evaluating parsers by comparing their performance in an embedding task (Miyao et al., 2008; Buyko and Hahn, 2010), or neutralizing the arc direction in the native representation of dependency trees (Schwartz et al., 2011). Each of these methods has its own drawbacks. Picking a single gold standard skews the results in favor of parsers which were trained on it. Transforming dependency trees to a set of pre-defined labeled dependencies, or into task-based features, requires the use of heuristic rules that run the risk of distorting correct information and introducing noise o</context>
<context position="22080" citStr="Carroll et al., 1998" startWordPosition="3645" endWordPosition="3648">to use Parse- π1 and π2 is a set of edit operations required for turnval measures (Black et al., 1991) for comparing the ing π1 into π2. Now, assume that we are given a cost parsed trees against the new generalized gold trees. function defined for each edit operation. The cost of Parseval scores, however, have two significant draw- ES(π1, π2) is the sum of the costs of the operations backs. First, they are known to be too restrictive in the script. An optimal edit script is an edit script with respect to some errors and too permissive with between π1 and π2 of minimum cost. respect to others (Carroll et al., 1998; K¨ubler and �ES*(π1, π2) = argminES(1r1,1r2) cost(e) Telljohann, 2002; Roark, 2002; Rehbein and van eEES(1r1,1r2) Genabith, 2007). Secondly, F1 scores would still The tree edit distance problem is defined to be the penalize structures that are correct with respect to problem of finding the optimal edit script and comthe original gold, but are not there in the generalized puting the corresponding distance (Bille, 2005). structure. Here we propose to adopt measures that A simple way to calculate the error δ of a parse are based on tree edit distance (TED) instead. TED- would be to define it as</context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998. Parser evaluation: a survey and a new proposal. In Proceedings of LREC, pages 447–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing to stanford dependencies: Trade-offs between speed and accuracy.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>Cer, de Marneffe, Jurafsky, Manning, 2010</marker>
<rawString>Daniel Cer, Marie-Catherine de Marneffe, Daniel Jurafsky, and Christopher D. Manning. 2010. Parsing to stanford dependencies: Trade-offs between speed and accuracy. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Martha Palmer</author>
</authors>
<title>Robust constituent-to-dependency conversion for English.</title>
<date>2010</date>
<booktitle>In Proceedings of TLT.</booktitle>
<contexts>
<context position="5814" citStr="Choi and Palmer, 2010" startWordPosition="861" endWordPosition="864">ll-defined according to current standards (K¨ubler et al., 2009), there are different ways in which the trees can be used to express syntactic content (Rambow, 2010). Consider, for instance, algorithms for converting the phrase-structure trees in the Penn Treebank (Marcus et al., 1993) into dependency structures. Different conversion algorithms implicitly make different assumptions about how to represent linguistic content in the data. When multiple conversion algorithms are applied to the same data, we end up with different dependency trees for the same sentences (Johansson and Nugues, 2007; Choi and Palmer, 2010; de Marneffe et al., 2006). Some common cases of discrepancies are as follows. Lexical vs. Functional Head Choice. In linguistics, there is a distinction between lexical heads and functional heads. A lexical head carries the semantic gist of a phrase while a functional one marks its relation to other parts of the sentence. The two kinds of heads may or may not coincide in a single word form (Zwicky, 1993). Common examples refer to prepositional phrases, such as the phrase “on Sunday”. This phrase has two possible analyses, one selects a lexical head (1a) and the other selects a functional one</context>
</contexts>
<marker>Choi, Palmer, 2010</marker>
<rawString>Jinho D. Choi and Martha Palmer. 2010. Robust constituent-to-dependency conversion for English. In Proceedings of TLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cohen</author>
</authors>
<title>Empirical Methods for rtificial Intelligence.</title>
<date>1995</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="33413" citStr="Cohen, 1995" startWordPosition="5409" endWordPosition="5410">tion Our TEDEVAL software package implements the pipeline described in Section 3. We convert all parse and gold trees into functional trees using the algorithm defined in Section 3, and for each pair of parsing experiments we calculate a shared gold standard using generalization determined through a chart-based greedy algorithm.8 Our scoring procedure uses the TED algorithm defined by Zhang and Shasha (1989).9 The unlabeled score is obtained by assigning cost(e) = 0 for every e relabeling operation. To calculate pairwise statistical significance we use a shuffling test with 10,000 iterations (Cohen, 1995). A sample of all files in the evaluation pipeline for a subset of 10 PTB sentences is available in the supplementary materials.10 7In case the labels are not taken from the same inventory, e.g., subjects in one scheme are marked as SUB and in the other marked as SBJ, it is possible define a a set of zero-cost operation types — in such case, to the operation relabel(SUB,SBJ) — in order not to penalize string label discrepancies. 8Our algorithm has space and runtime complexity of O(n2). 9Available via http://web.science.mq.edu. u/ -sw n/howtos/treedist nce/ 10The TEDEVAL software package is ava</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Paul Cohen. 1995. Empirical Methods for rtificial Intelligence. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Emms</author>
</authors>
<title>Tree-distance and some other variants of evalb.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="20151" citStr="Emms (2008)" startWordPosition="3311" endWordPosition="3312">e labels consist of complex feature structures made of attribute-value lists, we replace label(n) = label(m) in the subsumption definition with label(n) C label(m) in the sense of (Shieber, 1986). (5a) ... (5b) ... (5c) ... (6a) ... (6b) ... conj earth conj wind conj and fire * conj earth wind conj and fire * coord * cc * earth coord * v9 v9 * * would v9 worked * have worked v9 would have * and wind conj fire 389 Unlike unification, generalization can never fail. Our formalization follows closely the formulation For every pair of trees there exists a tree that is more of the T-Dice measure of Emms (2008), building on general than both: in the extreme case, pick the com- his thorough investigation of the formal and empirpletely flat structure over the yield, which is more ical differences between TED-based measures and general than any other structure. For (6a)–(6b), for Parseval. We first define for any ordered and labeled instance, we get that (6a)nt(6b) is a flat tree over tree π the following operations. pre-terminals where “would” and “have” are labeled relabel-node change the label of node v in π with ‘vg’ and “worked” is the head, labeled with ‘*’. delete-node delete a non-root node v i</context>
</contexts>
<marker>Emms, 2008</marker>
<rawString>Martin Emms. 2008. Tree-distance and some other variants of evalb. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erhard Hinrichs</author>
<author>Sandra K¨ubler</author>
<author>Karin Naumann</author>
<author>Heike Telljohan</author>
<author>Julia Trushkina</author>
</authors>
<title>Recent development in linguistic annotations of the T¨uBa-D/Z Treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of TLT.</booktitle>
<marker>Hinrichs, K¨ubler, Naumann, Telljohan, Trushkina, 2004</marker>
<rawString>Erhard Hinrichs, Sandra K¨ubler, Karin Naumann, Heike Telljohan, and Julia Trushkina. 2004. Recent development in linguistic annotations of the T¨uBa-D/Z Treebank. In Proceedings of TLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings of NOD LID .</booktitle>
<contexts>
<context position="5791" citStr="Johansson and Nugues, 2007" startWordPosition="857" endWordPosition="860"> a dependency treebank is well-defined according to current standards (K¨ubler et al., 2009), there are different ways in which the trees can be used to express syntactic content (Rambow, 2010). Consider, for instance, algorithms for converting the phrase-structure trees in the Penn Treebank (Marcus et al., 1993) into dependency structures. Different conversion algorithms implicitly make different assumptions about how to represent linguistic content in the data. When multiple conversion algorithms are applied to the same data, we end up with different dependency trees for the same sentences (Johansson and Nugues, 2007; Choi and Palmer, 2010; de Marneffe et al., 2006). Some common cases of discrepancies are as follows. Lexical vs. Functional Head Choice. In linguistics, there is a distinction between lexical heads and functional heads. A lexical head carries the semantic gist of a phrase while a functional one marks its relation to other parts of the sentence. The two kinds of heads may or may not coincide in a single word form (Zwicky, 1993). Common examples refer to prepositional phrases, such as the phrase “on Sunday”. This phrase has two possible analyses, one selects a lexical head (1a) and the other s</context>
<context position="7354" citStr="Johansson and Nugues (2007)" startWordPosition="1114" endWordPosition="1117">encybased formalisms require us to represent all content as binary relations, there are different ways we could represent such constructions. Let us consider the coordination of nominals below. We can choose between a functional head (1a) and a lexical head (2b, 2c). We can further choose between a flat representation in which the first conjunct is a single head (2b), or a nested structure where each conjunct/marker is the head of the following element (2c). All three alternatives empirically exist. Example (2a) reflects the structures in the CoNLL 2007 shared task data (Nivre et al., 2007a). Johansson and Nugues (2007) use structures like (2b). Example (2c) reflects the analysis of Mel’ˇcuk (1988). Periphrastic Marking. When a phrase includes periphrastic marking — such as the tense and modal marking in the phrase “would have worked” below — there are different ways to consider its division into phrases. One way to analyze this phrase would be to choose auxiliaries as heads, as in (3a). Another alternative would be to choose the final verb as the (2a) and (2b) earth (2c) earth conj fire coord wind coord and earth wind fire conj cc wind and fire (1b) on (1a) Sunday prep pobj on Sunday 386 Experiment Gold Par</context>
<context position="27670" citStr="Johansson and Nugues, 2007" startWordPosition="4551" endWordPosition="4554"> maximally possible edits on parse trees turned into generalized trees. E Z= ltesit-set |snew (parse1 i,gold1 i,gold3i) 1 − E|test-set |t (parse 1i,gold3i) i=1 The latter score, global averaging over the entire test set, is the metric we use in our evaluation procedure. 4 Experiments We demonstrate the application of our procedure to comparing dependency parsing results on different versions of the Penn Treebank (Marcus et al., 1993). The Data We use data from the PTB, converted into dependency structures using the LTH software, a general purpose tool for constituency-todependency conversion (Johansson and Nugues, 2007). We use LTH to implement the five different annotation standards detailed in Table 3. 6Generalization is an associative and commutative operation, so it can be extended for n experiments in any order. 391 Train Default Old LTH CoNLL07 Gold Default UAS 0.9142 0.6077 0.7772 LAS 0.8820 0.4801 0.6454 U-TED 0.9488 0.8926 0.9237 L-TED 0.9241 0.7811 0.8441 Old LTH UAS 0.6053 0.8955 0.6508 LAS 0.4816 0.8644 0.5771 U-TED 0.8931 0.9564 0.9092 L-TED 0.7811 0.9317 0.8197 CoNLL07 UAS 0.7734 0.6474 0.8917 LAS 0.6479 0.5722 0.8736 U-TED 0.9260 0.9097 0.9474 L-TED 0.8480 0.8204 0.9233 Default-OldLTH U-TED 0.</context>
<context position="31174" citStr="Johansson and Nugues (2007)" startWordPosition="5049" endWordPosition="5052">0.9497 0.9483 L-TED 0.9228 0.9161 Functional-Lexical U-TED 0.9504 0.9483 L-TED 0.9258 0.9161 CoNLL-Functional-Lexical U-TED 0.9498 0.9504† 0.9483† L-TED 0.9229 0.9258 0.9161 Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported in the same row. The † sign marks pairwise results where the difference is not statistically significant. ID Description Default The LTH conversion default settings OldLTH The conversion used in Johansson and Nugues (2007) CoNLL07 Lexical Functional The conversion used in the CoNLL shared task (Nivre et al., 2007a) Same as CoNLL, but selecting only lexical heads when a choice exists Same as CoNLL, but selecting only functional heads when a choice exists Table 3: LTH conversion schemes used in the experiments. The LTH conversion settings in terms of the complete feature-value pairs associated with the LTH parameters in different schemes are detailed in the supplementary material. 392 The Default, OldLTH and CoNLL schemes mainly differ in their coordination structure, and the Functional and Lexical schemes differ</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proceedings of NOD LID .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Heike Telljohann</author>
</authors>
<title>Towards a dependency-oriented evaluation for partial parsing.</title>
<date>2002</date>
<marker>K¨ubler, Telljohann, 2002</marker>
<rawString>Sandra K¨ubler and Heike Telljohann. 2002. Towards a dependency-oriented evaluation for partial parsing.</rawString>
</citation>
<citation valid="false">
<title>improved evaluation measures for parsing systems”.</title>
<booktitle>In Proceedings of LREC Workshop“Beyond Parseval – Towards</booktitle>
<marker></marker>
<rawString>In Proceedings of LREC Workshop“Beyond Parseval – Towards improved evaluation measures for parsing systems”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Parsing.</title>
<date>2009</date>
<journal>Number</journal>
<booktitle>in Synthesis Lectures on Human Language Technologies.</booktitle>
<volume>2</volume>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra K¨ubler, Ryan McDonald, and Joakim Nivre. 2009. Dependency Parsing. Number 2 in Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5479" citStr="Marcus et al., 1993" startWordPosition="811" endWordPosition="814">gh endeavor, and suggest ways to extend the protocol to additional evaluation scenarios. 2 The Challenge: Treebank Theories Dependency treebanks contain information about the grammatically meaningful elements in the utterance and the grammatical relations between them. Even if the formal representation in a dependency treebank is well-defined according to current standards (K¨ubler et al., 2009), there are different ways in which the trees can be used to express syntactic content (Rambow, 2010). Consider, for instance, algorithms for converting the phrase-structure trees in the Penn Treebank (Marcus et al., 1993) into dependency structures. Different conversion algorithms implicitly make different assumptions about how to represent linguistic content in the data. When multiple conversion algorithms are applied to the same data, we end up with different dependency trees for the same sentences (Johansson and Nugues, 2007; Choi and Palmer, 2010; de Marneffe et al., 2006). Some common cases of discrepancies are as follows. Lexical vs. Functional Head Choice. In linguistics, there is a distinction between lexical heads and functional heads. A lexical head carries the semantic gist of a phrase while a funct</context>
<context position="27480" citStr="Marcus et al., 1993" startWordPosition="4522" endWordPosition="4525"> can take the arithmetic mean of the scores. �|test-set |i=1score(parse1i,gold1i,gold3i) |test-set| Alternatively we can globally average of all edit distance costs, normalized by the maximally possible edits on parse trees turned into generalized trees. E Z= ltesit-set |snew (parse1 i,gold1 i,gold3i) 1 − E|test-set |t (parse 1i,gold3i) i=1 The latter score, global averaging over the entire test set, is the metric we use in our evaluation procedure. 4 Experiments We demonstrate the application of our procedure to comparing dependency parsing results on different versions of the Penn Treebank (Marcus et al., 1993). The Data We use data from the PTB, converted into dependency structures using the LTH software, a general purpose tool for constituency-todependency conversion (Johansson and Nugues, 2007). We use LTH to implement the five different annotation standards detailed in Table 3. 6Generalization is an associative and commutative operation, so it can be extended for n experiments in any order. 391 Train Default Old LTH CoNLL07 Gold Default UAS 0.9142 0.6077 0.7772 LAS 0.8820 0.4801 0.6454 U-TED 0.9488 0.8926 0.9237 L-TED 0.9241 0.7811 0.8441 Old LTH UAS 0.6053 0.8955 0.6508 LAS 0.4816 0.8644 0.5771</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of E CL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="32233" citStr="McDonald and Pereira (2006)" startWordPosition="5219" endWordPosition="5222">ed in the supplementary material. 392 The Default, OldLTH and CoNLL schemes mainly differ in their coordination structure, and the Functional and Lexical schemes differ in their selection of a functional and a lexical head, respectively. All schemes use the same inventory of labels.7 The LTH parameter settings for the different schemes are elaborated in the supplementary material. The Setup We use two different parsers: (i) MaltParser (Nivre et al., 2007b) with the arc eager algorithm as optimized for English in (Nivre et al., 2010) and (ii) MSTParser with the second-order projective model of McDonald and Pereira (2006). Both parsers were trained on the different instances of sections 2-21 of the PTB obeying the different annotation schemes in Table 3. Each trained model was used to parse section 23. All non-projective dependencies in the training and gold sets were projectivized prior to training and parsing using the algorithm of Nivre and Nilsson (2005). A more principled treatment of non-projective dependency trees is an important topic for future research. We evaluated the parses using labeled and unlabeled attachment scores, and using our TEDEVAL software package. Evaluation Our TEDEVAL software packag</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of E CL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of CL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="1200" citStr="McDonald et al., 2005" startWordPosition="161" endWordPosition="164"> a refined notion of tree edit distance for evaluating parse hypotheses relative to multiple gold standards. We demonstrate that, for different conversions of the Penn Treebank into dependencies, performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground. 1 Introduction Data-driven dependency parsing has seen a considerable surge of interest in recent years. Dependency parsers have been tested on parsing sentences in English (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005) as well as many other languages (Nivre et al., 2007a). The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). As it turns out, however, such evaluation procedures are sensitive to the annotation choices in the data on which the parser was trained. Different annotation schemes often make different assumptions with respect to how linguistic content is represented in a treebank (Rambo</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of CL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press.</publisher>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>Igor Mel’ˇcuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Rune Sætre</author>
<author>Kenji Sagae</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Task-oriented evaluation of syntactic parsers and their representations.</title>
<date>2008</date>
<booktitle>In Proceedings of CL,</booktitle>
<pages>46--54</pages>
<contexts>
<context position="2531" citStr="Miyao et al., 2008" startWordPosition="370" endWordPosition="373">nt experiments, even ones that use the same parser and the same set of sentences, the gap between results in different experiments may not reflect a true gap in performance, but rather a difference in the annotation decisions made in the respective treebanks. Different methods have been proposed for making dependency parsing results comparable across experiments. These methods include picking a single gold standard for all experiments to which the parser output should be converted (Carroll et al., 1998; Cer et al., 2010), evaluating parsers by comparing their performance in an embedding task (Miyao et al., 2008; Buyko and Hahn, 2010), or neutralizing the arc direction in the native representation of dependency trees (Schwartz et al., 2011). Each of these methods has its own drawbacks. Picking a single gold standard skews the results in favor of parsers which were trained on it. Transforming dependency trees to a set of pre-defined labeled dependencies, or into task-based features, requires the use of heuristic rules that run the risk of distorting correct information and introducing noise of their own. Neutralizing the direction of arcs is limited to unlabeled evaluation and local context, and thus </context>
<context position="9228" citStr="Miyao et al., 2008" startWordPosition="1421" endWordPosition="1424"> results are not comparable across experiments. Consider, for instance, the example in Figure 1. If parse1 and parse2 are compared against gold2 using labeled attachment scores (LAS), then parse1 results are lower than the results of parse2, even though both parsers produced linguistically correct and perfectly useful output. Existing methods for making parsing results comparable across experiments include heuristics for converting outputs into dependency trees of a predefined standard (Briscoe et al., 2002; Cer et al., 2010) or evaluating the performance of a parser within an embedding task (Miyao et al., 2008; Buyko and Hahn, 2010). However, heuristic rules for crossannotation conversion are typically hand written and error prone, and may not cover all possible discrepancies. Task-based evaluation may be sensitive to the particular implementation of the embedding task and the procedures that extract specific task-related features from the different parses. Beyond that, conversion heuristics and task-based procedures are currently developed almost exclusively for English. Other languages typically lack such resources. A recent study by Schwartz et al. (2011) takes a different approach towards cross</context>
</contexts>
<marker>Miyao, Sætre, Sagae, Matsuzaki, Tsujii, 2008</marker>
<rawString>Yusuke Miyao, Rune Sætre, Kenji Sagae, Takuya Matsuzaki, and Jun’ichi Tsujii. 2008. Task-oriented evaluation of syntactic parsers and their representations. In Proceedings of CL, pages 46–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Beata Megyesi</author>
</authors>
<title>Bootstrapping a Swedish Treebank using cross-corpus harmonization and annotation projection.</title>
<date>2007</date>
<booktitle>In Proceedings of TLT.</booktitle>
<contexts>
<context position="37320" citStr="Nivre and Megyesi, 2007" startWordPosition="6027" endWordPosition="6030">– and the rest of the pipeline follows. nificant. This suggests that apparent performance A pre-condition for cross-framework evaluation trends between experiments when evaluating with is that all representations encode the same set of respect to the training schemes may be misleading. grammatical relations by, e.g., annotating arcs in deThese observations are robust across parsing algo- pendency trees or decorating nodes in constituency rithms. In each of the tables, results obtained against trees. For some treebanks this is already the case the training schemes show significant differences (Nivre and Megyesi, 2007; Skut et al., 1997; Hinwhereas applying our cross-experimental procedure richs et al., 2004) while for others this is still lackshows small to no gaps in performance across dif- ing. Recent studies (Briscoe et al., 2002; de Marnferent schemes. Annotation variants which seem to effe et al., 2006) suggest that evaluation through a have crucial effects have a relatively small influence single set of grammatical relations as the common when parsed structures are brought into the same denominator is a linguistically sound and practically formal and theoretical common ground for compar- useful way </context>
</contexts>
<marker>Nivre, Megyesi, 2007</marker>
<rawString>Joakim Nivre and Beata Megyesi. 2007. Bootstrapping a Swedish Treebank using cross-corpus harmonization and annotation projection. In Proceedings of TLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudo projective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceeding of CL,</booktitle>
<pages>99--106</pages>
<contexts>
<context position="32576" citStr="Nivre and Nilsson (2005)" startWordPosition="5278" endWordPosition="5281">orated in the supplementary material. The Setup We use two different parsers: (i) MaltParser (Nivre et al., 2007b) with the arc eager algorithm as optimized for English in (Nivre et al., 2010) and (ii) MSTParser with the second-order projective model of McDonald and Pereira (2006). Both parsers were trained on the different instances of sections 2-21 of the PTB obeying the different annotation schemes in Table 3. Each trained model was used to parse section 23. All non-projective dependencies in the training and gold sets were projectivized prior to training and parsing using the algorithm of Nivre and Nilsson (2005). A more principled treatment of non-projective dependency trees is an important topic for future research. We evaluated the parses using labeled and unlabeled attachment scores, and using our TEDEVAL software package. Evaluation Our TEDEVAL software package implements the pipeline described in Section 3. We convert all parse and gold trees into functional trees using the algorithm defined in Section 3, and for each pair of parsing experiments we calculate a shared gold standard using generalization determined through a chart-based greedy algorithm.8 Our scoring procedure uses the TED algorith</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudo projective dependency parsing. In Proceeding of CL, pages 99– 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of English text.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>64--70</pages>
<contexts>
<context position="1176" citStr="Nivre and Scholz, 2004" startWordPosition="157" endWordPosition="160">rent representations and a refined notion of tree edit distance for evaluating parse hypotheses relative to multiple gold standards. We demonstrate that, for different conversions of the Penn Treebank into dependencies, performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground. 1 Introduction Data-driven dependency parsing has seen a considerable surge of interest in recent years. Dependency parsers have been tested on parsing sentences in English (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005) as well as many other languages (Nivre et al., 2007a). The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). As it turns out, however, such evaluation procedures are sensitive to the annotation choices in the data on which the parser was trained. Different annotation schemes often make different assumptions with respect to how linguistic content is represen</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of English text. In Proceedings of COLING, pages 64–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL</booktitle>
<pages>915--932</pages>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007a. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
<author>Johan Hall</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="1252" citStr="Nivre et al., 2007" startWordPosition="171" endWordPosition="174">arse hypotheses relative to multiple gold standards. We demonstrate that, for different conversions of the Penn Treebank into dependencies, performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground. 1 Introduction Data-driven dependency parsing has seen a considerable surge of interest in recent years. Dependency parsers have been tested on parsing sentences in English (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005) as well as many other languages (Nivre et al., 2007a). The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). As it turns out, however, such evaluation procedures are sensitive to the annotation choices in the data on which the parser was trained. Different annotation schemes often make different assumptions with respect to how linguistic content is represented in a treebank (Rambow, 2010). The consequence of such annotation discrep</context>
<context position="7323" citStr="Nivre et al., 2007" startWordPosition="1110" endWordPosition="1113">ructures. Since dependencybased formalisms require us to represent all content as binary relations, there are different ways we could represent such constructions. Let us consider the coordination of nominals below. We can choose between a functional head (1a) and a lexical head (2b, 2c). We can further choose between a flat representation in which the first conjunct is a single head (2b), or a nested structure where each conjunct/marker is the head of the following element (2c). All three alternatives empirically exist. Example (2a) reflects the structures in the CoNLL 2007 shared task data (Nivre et al., 2007a). Johansson and Nugues (2007) use structures like (2b). Example (2c) reflects the analysis of Mel’ˇcuk (1988). Periphrastic Marking. When a phrase includes periphrastic marking — such as the tense and modal marking in the phrase “would have worked” below — there are different ways to consider its division into phrases. One way to analyze this phrase would be to choose auxiliaries as heads, as in (3a). Another alternative would be to choose the final verb as the (2a) and (2b) earth (2c) earth conj fire coord wind coord and earth wind fire conj cc wind and fire (1b) on (1a) Sunday prep pobj on</context>
<context position="31266" citStr="Nivre et al., 2007" startWordPosition="5064" endWordPosition="5067">nctional-Lexical U-TED 0.9498 0.9504† 0.9483† L-TED 0.9229 0.9258 0.9161 Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported in the same row. The † sign marks pairwise results where the difference is not statistically significant. ID Description Default The LTH conversion default settings OldLTH The conversion used in Johansson and Nugues (2007) CoNLL07 Lexical Functional The conversion used in the CoNLL shared task (Nivre et al., 2007a) Same as CoNLL, but selecting only lexical heads when a choice exists Same as CoNLL, but selecting only functional heads when a choice exists Table 3: LTH conversion schemes used in the experiments. The LTH conversion settings in terms of the complete feature-value pairs associated with the LTH parameters in different schemes are detailed in the supplementary material. 392 The Default, OldLTH and CoNLL schemes mainly differ in their coordination structure, and the Functional and Lexical schemes differ in their selection of a functional and a lexical head, respectively. All schemes use the sa</context>
</contexts>
<marker>Nivre, Nilsson, Hall, 2007</marker>
<rawString>Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007b. Maltparser: A languageindependent system for data-driven dependency parsing. Natural Language Engineering, 13(1):1–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Laura Rimell</author>
<author>Ryan McDonald</author>
<author>Carlos G´omez-Rodr´ıguez</author>
</authors>
<title>Evaluation of dependency parsers on unbounded dependencies.</title>
<date>2010</date>
<pages>813--821</pages>
<marker>Nivre, Rimell, McDonald, G´omez-Rodr´ıguez, 2010</marker>
<rawString>Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos G´omez-Rodr´ıguez. 2010. Evaluation of dependency parsers on unbounded dependencies. pages 813–821.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
</authors>
<title>The Simple Truth about Dependency and Phrase Structure Representations: An Opinion Piece.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT- CL,</booktitle>
<pages>337--340</pages>
<contexts>
<context position="1808" citStr="Rambow, 2010" startWordPosition="257" endWordPosition="258">2005) as well as many other languages (Nivre et al., 2007a). The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). As it turns out, however, such evaluation procedures are sensitive to the annotation choices in the data on which the parser was trained. Different annotation schemes often make different assumptions with respect to how linguistic content is represented in a treebank (Rambow, 2010). The consequence of such annotation discrepancies is that when we compare parsing results across different experiments, even ones that use the same parser and the same set of sentences, the gap between results in different experiments may not reflect a true gap in performance, but rather a difference in the annotation decisions made in the respective treebanks. Different methods have been proposed for making dependency parsing results comparable across experiments. These methods include picking a single gold standard for all experiments to which the parser output should be converted (Carroll </context>
<context position="5358" citStr="Rambow, 2010" startWordPosition="795" endWordPosition="796">arsing algorithms. We conclude that it is imperative that cross-experiment parse evaluation be a well thoughtthrough endeavor, and suggest ways to extend the protocol to additional evaluation scenarios. 2 The Challenge: Treebank Theories Dependency treebanks contain information about the grammatically meaningful elements in the utterance and the grammatical relations between them. Even if the formal representation in a dependency treebank is well-defined according to current standards (K¨ubler et al., 2009), there are different ways in which the trees can be used to express syntactic content (Rambow, 2010). Consider, for instance, algorithms for converting the phrase-structure trees in the Penn Treebank (Marcus et al., 1993) into dependency structures. Different conversion algorithms implicitly make different assumptions about how to represent linguistic content in the data. When multiple conversion algorithms are applied to the same data, we end up with different dependency trees for the same sentences (Johansson and Nugues, 2007; Choi and Palmer, 2010; de Marneffe et al., 2006). Some common cases of discrepancies are as follows. Lexical vs. Functional Head Choice. In linguistics, there is a d</context>
</contexts>
<marker>Rambow, 2010</marker>
<rawString>Owen Rambow. 2010. The Simple Truth about Dependency and Phrase Structure Representations: An Opinion Piece. In Proceedings of HLT- CL, pages 337– 340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef van Genabith</author>
</authors>
<title>Why is it so difficult to compare treebanks? Tiger and T¨uBa-D/Z revisited.</title>
<date>2007</date>
<booktitle>In Proceedings of TLT,</booktitle>
<pages>115--126</pages>
<marker>Rehbein, van Genabith, 2007</marker>
<rawString>Ines Rehbein and Josef van Genabith. 2007. Why is it so difficult to compare treebanks? Tiger and T¨uBa-D/Z revisited. In Proceedings of TLT, pages 115–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Evaluating parser accuracy using edit distance.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC Workshop“Beyond Parseval – Towards</booktitle>
<contexts>
<context position="22164" citStr="Roark, 2002" startWordPosition="3658" endWordPosition="3659">l., 1991) for comparing the ing π1 into π2. Now, assume that we are given a cost parsed trees against the new generalized gold trees. function defined for each edit operation. The cost of Parseval scores, however, have two significant draw- ES(π1, π2) is the sum of the costs of the operations backs. First, they are known to be too restrictive in the script. An optimal edit script is an edit script with respect to some errors and too permissive with between π1 and π2 of minimum cost. respect to others (Carroll et al., 1998; K¨ubler and �ES*(π1, π2) = argminES(1r1,1r2) cost(e) Telljohann, 2002; Roark, 2002; Rehbein and van eEES(1r1,1r2) Genabith, 2007). Secondly, F1 scores would still The tree edit distance problem is defined to be the penalize structures that are correct with respect to problem of finding the optimal edit script and comthe original gold, but are not there in the generalized puting the corresponding distance (Bille, 2005). structure. Here we propose to adopt measures that A simple way to calculate the error δ of a parse are based on tree edit distance (TED) instead. TED- would be to define it as the edit distance between based measures are, in fact, an extension of attach- the </context>
</contexts>
<marker>Roark, 2002</marker>
<rawString>Brian Roark. 2002. Evaluating parser accuracy using edit distance. In Proceedings of LREC Workshop“Beyond Parseval – Towards improved evaluation measures for parsing systems”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Schwartz</author>
<author>Omri Abend</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of CL,</booktitle>
<pages>663--672</pages>
<contexts>
<context position="2662" citStr="Schwartz et al., 2011" startWordPosition="391" endWordPosition="394">nts may not reflect a true gap in performance, but rather a difference in the annotation decisions made in the respective treebanks. Different methods have been proposed for making dependency parsing results comparable across experiments. These methods include picking a single gold standard for all experiments to which the parser output should be converted (Carroll et al., 1998; Cer et al., 2010), evaluating parsers by comparing their performance in an embedding task (Miyao et al., 2008; Buyko and Hahn, 2010), or neutralizing the arc direction in the native representation of dependency trees (Schwartz et al., 2011). Each of these methods has its own drawbacks. Picking a single gold standard skews the results in favor of parsers which were trained on it. Transforming dependency trees to a set of pre-defined labeled dependencies, or into task-based features, requires the use of heuristic rules that run the risk of distorting correct information and introducing noise of their own. Neutralizing the direction of arcs is limited to unlabeled evaluation and local context, and thus may not cover all possible discrepancies. This paper proposes a new three-step protocol for cross-experiment parser evaluation, and</context>
<context position="9787" citStr="Schwartz et al. (2011)" startWordPosition="1500" endWordPosition="1503">rmance of a parser within an embedding task (Miyao et al., 2008; Buyko and Hahn, 2010). However, heuristic rules for crossannotation conversion are typically hand written and error prone, and may not cover all possible discrepancies. Task-based evaluation may be sensitive to the particular implementation of the embedding task and the procedures that extract specific task-related features from the different parses. Beyond that, conversion heuristics and task-based procedures are currently developed almost exclusively for English. Other languages typically lack such resources. A recent study by Schwartz et al. (2011) takes a different approach towards cross-annotation evaluation. They consider different directions of head-dependent relations (such as on-+Sunday and Sunday-+on) and different parent-child and grandparent-child relations in a chain (such as arrive-+on and arrive-+sunday in “arrive on sunday”) as equivalent. They then score arcs that fall within corresponding equivalence sets. Using these new scores Schwartz et al. (2011) neutralize certain annotation discrepancies that distort parse comparison. However, their treatment is limited to local context and does not treat structures larger than two</context>
</contexts>
<marker>Schwartz, Abend, Reichart, Rappoport, 2011</marker>
<rawString>Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rappoport. 2011. Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation. In Proceedings of CL, pages 663–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>n Introduction to UnificationBased Grammars. Center for the Study of Language and Information.</title>
<date>1986</date>
<contexts>
<context position="16595" citStr="Shieber (1986)" startWordPosition="2677" endWordPosition="2678"> ... prep on * Sunday 388 Considering the functional trees resulting from our procedure, it is easy to see that for tree pairs (4a)–(4b) and (5a)–(5b) the respective functional trees are identical modulo wildcards, while tree pairs (5b)–(5c) and (6a)–(6b) end up with different tree structures that realize different assumptions concerning the internal structure of the tree. In order to compare, combine or detect inconsistencies in the information inherent in different functional trees, we define a set of formal operations that are inspired by familiar notions from unification-based formalisms (Shieber (1986) and references therein). Step 2: Formal Operations on Trees The intuition behind the formal operations we define is simple. A completely flat tree over a span is the most general structural description that can be given to it. The more nodes dominate a span, the more linguistic assumptions are made with respect to its structure. If an arc structure in one tree merely elaborates an existing flat span in another tree, the theories underlying the schemes are compatible, and their information can be combined. Otherwise, there exists a conflict in the linguistic assumptions, and we need to relax s</context>
<context position="19735" citStr="Shieber, 1986" startWordPosition="3229" endWordPosition="3230">rmation that they share. For that we define the tree generalization operation. T-Generalization, denoted fit, is the operation that returns the most specific tree that is more general than both trees. Formally, 7r1 flt 7r2 = 7r3 iff 7r3 Ct 7r1 and 7r3 Ct 7r2, and for every 7r4 such that 7r4 Ct 7r1 and 7r4 Ct 7r2 it holds that 7r4 _Et 7r3. 2Note that the wildcard symbol * is equal to any other symbol. In case the node labels consist of complex feature structures made of attribute-value lists, we replace label(n) = label(m) in the subsumption definition with label(n) C label(m) in the sense of (Shieber, 1986). (5a) ... (5b) ... (5c) ... (6a) ... (6b) ... conj earth conj wind conj and fire * conj earth wind conj and fire * coord * cc * earth coord * v9 v9 * * would v9 worked * have worked v9 would have * and wind conj fire 389 Unlike unification, generalization can never fail. Our formalization follows closely the formulation For every pair of trees there exists a tree that is more of the T-Dice measure of Emms (2008), building on general than both: in the extreme case, pick the com- his thorough investigation of the formal and empirpletely flat structure over the yield, which is more ical differen</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Stuart M. Shieber. 1986. n Introduction to UnificationBased Grammars. Center for the Study of Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Skut</author>
<author>Brigitte Krenn</author>
<author>Thorsten Brants</author>
<author>Hans Uszkoreit</author>
</authors>
<title>An annotation scheme for free word-order languages.</title>
<date>1997</date>
<booktitle>In Proceedings of the fifth conference on pplied natural language processing,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="37339" citStr="Skut et al., 1997" startWordPosition="6031" endWordPosition="6034">eline follows. nificant. This suggests that apparent performance A pre-condition for cross-framework evaluation trends between experiments when evaluating with is that all representations encode the same set of respect to the training schemes may be misleading. grammatical relations by, e.g., annotating arcs in deThese observations are robust across parsing algo- pendency trees or decorating nodes in constituency rithms. In each of the tables, results obtained against trees. For some treebanks this is already the case the training schemes show significant differences (Nivre and Megyesi, 2007; Skut et al., 1997; Hinwhereas applying our cross-experimental procedure richs et al., 2004) while for others this is still lackshows small to no gaps in performance across dif- ing. Recent studies (Briscoe et al., 2002; de Marnferent schemes. Annotation variants which seem to effe et al., 2006) suggest that evaluation through a have crucial effects have a relatively small influence single set of grammatical relations as the common when parsed structures are brought into the same denominator is a linguistically sound and practically formal and theoretical common ground for compar- useful way to go. To guarantee</context>
</contexts>
<marker>Skut, Krenn, Brants, Uszkoreit, 1997</marker>
<rawString>Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans Uszkoreit. 1997. An annotation scheme for free word-order languages. In Proceedings of the fifth conference on pplied natural language processing, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceeding of IWPT,</booktitle>
<pages>195--206</pages>
<contexts>
<context position="1152" citStr="Yamada and Matsumoto, 2003" startWordPosition="153" endWordPosition="156">ations for harmonizing different representations and a refined notion of tree edit distance for evaluating parse hypotheses relative to multiple gold standards. We demonstrate that, for different conversions of the Penn Treebank into dependencies, performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground. 1 Introduction Data-driven dependency parsing has seen a considerable surge of interest in recent years. Dependency parsers have been tested on parsing sentences in English (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005) as well as many other languages (Nivre et al., 2007a). The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). As it turns out, however, such evaluation procedures are sensitive to the annotation choices in the data on which the parser was trained. Different annotation schemes often make different assumptions with respect to how lingui</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceeding of IWPT, pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaizhong Zhang</author>
<author>Dennis Shasha</author>
</authors>
<title>Simple fast algorithms for the editing distance between trees and related problems.</title>
<date>1989</date>
<booktitle>In SI MJournal of Computing,</booktitle>
<volume>18</volume>
<pages>1245--1262</pages>
<contexts>
<context position="33212" citStr="Zhang and Shasha (1989)" startWordPosition="5375" endWordPosition="5378">cipled treatment of non-projective dependency trees is an important topic for future research. We evaluated the parses using labeled and unlabeled attachment scores, and using our TEDEVAL software package. Evaluation Our TEDEVAL software package implements the pipeline described in Section 3. We convert all parse and gold trees into functional trees using the algorithm defined in Section 3, and for each pair of parsing experiments we calculate a shared gold standard using generalization determined through a chart-based greedy algorithm.8 Our scoring procedure uses the TED algorithm defined by Zhang and Shasha (1989).9 The unlabeled score is obtained by assigning cost(e) = 0 for every e relabeling operation. To calculate pairwise statistical significance we use a shuffling test with 10,000 iterations (Cohen, 1995). A sample of all files in the evaluation pipeline for a subset of 10 PTB sentences is available in the supplementary materials.10 7In case the labels are not taken from the same inventory, e.g., subjects in one scheme are marked as SUB and in the other marked as SBJ, it is possible define a a set of zero-cost operation types — in such case, to the operation relabel(SUB,SBJ) — in order not to pen</context>
</contexts>
<marker>Zhang, Shasha, 1989</marker>
<rawString>Kaizhong Zhang and Dennis Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. In SI MJournal of Computing, volume 18, pages 1245–1262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnold M Zwicky</author>
</authors>
<title>Heads, bases, and functors.</title>
<date>1993</date>
<booktitle>Heads in Grammatical Theory,</booktitle>
<pages>292--315</pages>
<editor>In G.G. Corbett, N. Fraser, and S. McGlashan, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6223" citStr="Zwicky, 1993" startWordPosition="935" endWordPosition="936">c content in the data. When multiple conversion algorithms are applied to the same data, we end up with different dependency trees for the same sentences (Johansson and Nugues, 2007; Choi and Palmer, 2010; de Marneffe et al., 2006). Some common cases of discrepancies are as follows. Lexical vs. Functional Head Choice. In linguistics, there is a distinction between lexical heads and functional heads. A lexical head carries the semantic gist of a phrase while a functional one marks its relation to other parts of the sentence. The two kinds of heads may or may not coincide in a single word form (Zwicky, 1993). Common examples refer to prepositional phrases, such as the phrase “on Sunday”. This phrase has two possible analyses, one selects a lexical head (1a) and the other selects a functional one (1b), as depicted below. Similar choices are found in phrases which contain functional elements such as determiners, coordination markers, subordinating elements, and so on. Multi-Headed Constructions. Some phrases are considered to have multiple lexical heads, for instance, coordinated structures. Since dependencybased formalisms require us to represent all content as binary relations, there are differen</context>
</contexts>
<marker>Zwicky, 1993</marker>
<rawString>Arnold M. Zwicky. 1993. Heads, bases, and functors. In G.G. Corbett, N. Fraser, and S. McGlashan, editors, Heads in Grammatical Theory, pages 292–315. Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>