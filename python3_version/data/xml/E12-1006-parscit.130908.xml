<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000103">
<title confidence="0.99469">
Cross-Framework Evaluation for Statistical Parsing
</title>
<author confidence="0.935571">
Reut Tsarfaty Joakim Nivre Evelina Andersson
</author>
<affiliation confidence="0.953532">
Uppsala University, Box 635, 75126 Uppsala, Sweden
</affiliation>
<email confidence="0.99198">
tsarfaty@stp.lingfil.uu.se,{joakim.nivre,evelina.andersson}@lingfil.uu.se
</email>
<sectionHeader confidence="0.998542" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999761117647059">
A serious bottleneck of comparative parser
evaluation is the fact that different parsers
subscribe to different formal frameworks
and theoretical assumptions. Converting
outputs from one framework to another is
less than optimal as it easily introduces
noise into the process. Here we present a
principled protocol for evaluating parsing
results across frameworks based on func-
tion trees, tree generalization and edit dis-
tance metrics. This extends a previously
proposed framework for cross-theory eval-
uation and allows us to compare a wider
class of parsers. We demonstrate the useful-
ness and language independence of our pro-
cedure by evaluating constituency and de-
pendency parsers on English and Swedish.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990637037037">
The goal of statistical parsers is to recover a for-
mal representation of the grammatical relations
that constitute the argument structure of natural
language sentences. The argument structure en-
compasses grammatical relationships between el-
ements such as subject, predicate, object, etc.,
which are useful for further (e.g., semantic) pro-
cessing. The parses yielded by different parsing
frameworks typically obey different formal and
theoretical assumptions concerning how to rep-
resent the grammatical relationships in the data
(Rambow, 2010). For example, grammatical rela-
tions may be encoded on top of dependency arcs
in a dependency tree (Mel’ˇcuk, 1988), they may
decorate nodes in a phrase-structure tree (Marcus
et al., 1993; Maamouri et al., 2004; Sima’an et
al., 2001), or they may be read off of positions in
a phrase-structure tree using hard-coded conver-
sion procedures (de Marneffe et al., 2006). This
diversity poses a challenge to cross-experimental
parser evaluation, namely: How can we evaluate
the performance of these different parsers relative
to one another?
Current evaluation practices assume a set of
correctly annotated test data (or gold standard)
for evaluation. Typically, every parser is eval-
uated with respect to its own formal representa-
tion type and the underlying theory which it was
trained to recover. Therefore, numerical scores
of parses across experiments are incomparable.
When comparing parses that belong to different
formal frameworks, the notion of a single gold
standard becomes problematic, and there are two
different questions we have to answer. First, what
is an appropriate gold standard for cross-parser
evaluation? And secondly, how can we alle-
viate the differences between formal representa-
tion types and theoretical assumptions in order to
make our comparison sound – that is, to make sure
that we are not comparing apples and oranges?
A popular way to address this has been to
pick one of the frameworks and convert all
parser outputs to its formal type. When com-
paring constituency-based and dependency-based
parsers, for instance, the output of constituency
parsers has often been converted to dependency
structures prior to evaluation (Cer et al., 2010;
Nivre et al., 2010). This solution has vari-
ous drawbacks. First, it demands a conversion
script that maps one representation type to another
when some theoretical assumptions in one frame-
work may be incompatible with the other one.
In the constituency-to-dependency case, some
constituency-based structures (e.g., coordination
</bodyText>
<page confidence="0.992009">
44
</page>
<note confidence="0.976411">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 44–54,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999926549019608">
and ellipsis) do not comply with the single head
assumption of dependency treebanks. Secondly,
these scripts may be labor intensive to create, and
are available mostly for English. So the evalua-
tion protocol becomes language-dependent.
In Tsarfaty et al. (2011) we proposed a gen-
eral protocol for handling annotation discrepan-
cies when comparing parses across different de-
pendency theories. The protocol consists of three
phases: converting all structures into function
trees, for each sentence, generalizing the different
gold standard function trees to get their common
denominator, and employing an evaluation mea-
sure based on tree edit distance (TED) which dis-
cards edit operations that recover theory-specific
structures. Although the protocol is potentially
applicable to a wide class of syntactic represen-
tation types, formal restrictions in the procedures
effectively limit its applicability only to represen-
tations that are isomorphic to dependency trees.
The present paper breaks new ground in the
ability to soundly compare the accuracy of differ-
ent parsers relative to one another given that they
employ different formal representation types and
obey different theoretical assumptions. Our solu-
tion generally confines with the protocol proposed
in Tsarfaty et al. (2011) but is re-formalized to
allow for arbitrary linearly ordered labeled trees,
thus encompassing constituency-based as well as
dependency-based representations. The frame-
work in Tsarfaty et al. (2011) assumes structures
that are isomorphic to dependency trees, bypass-
ing the problem of arbitrary branching. Here we
lift this restriction, and define a protocol which
is based on generalization and TED measures to
soundly compare the output of different parsers.
We demonstrate the utility of this protocol by
comparing the performance of different parsers
for English and Swedish. For English, our
parser evaluation across representation types al-
lows us to analyze and precisely quantify previ-
ously encountered performance tendencies. For
Swedish we show the first ever evaluation be-
tween dependency-based and constituency-based
parsing models, all trained on the Swedish tree-
bank data. All in all we show that our ex-
tended protocol, which can handle linearly-
ordered labeled trees with arbitrary branch-
ing, can soundly compare parsing results across
frameworks in a representation-independent and
language-independent fashion.
</bodyText>
<sectionHeader confidence="0.701128" genericHeader="method">
2 Preliminaries: Relational Schemes for
Cross-Framework Parse Evaluation
</sectionHeader>
<bodyText confidence="0.997270625000001">
Traditionally, different statistical parsers have
been evaluated using specially designated evalu-
ation measures that are designed to fit their repre-
sentation types. Dependency trees are evaluated
using attachment scores (Buchholz and Marsi,
2006), phrase-structure trees are evaluated using
ParsEval (Black et al., 1991), LFG-based parsers
postulate an evaluation procedure based on f-
structures (Cahill et al., 2008), and so on. From a
downstream application point of view, there is no
significance as to which formalism was used for
generating the representation and which learning
methods have been utilized. The bottom line is
simply which parsing framework most accurately
recovers a useful representation that helps to un-
ravel the human-perceived interpretation.
Relational schemes, that is, schemes that en-
code the set of grammatical relations that con-
stitute the predicate-argument structures of sen-
tences, provide an interface to semantic interpre-
tation. They are more intuitively understood than,
say, phrase-structure trees, and thus they are also
more useful for practical applications. For these
reasons, relational schemes have been repeatedly
singled out as an appropriate level of representa-
tion for the evaluation of statistical parsers (Lin,
1995; Carroll et al., 1998; Cer et al., 2010).
The annotated data which statistical parsers are
trained on encode these grammatical relationships
in different ways. Dependency treebanks provide
a ready-made representation of grammatical rela-
tions on top of arcs connecting the words in the
sentence (K¨ubler et al., 2009). The Penn Tree-
bank and phrase-structure annotated resources en-
code partial information about grammatical rela-
tions as dash-features decorating phrase structure
nodes (Marcus et al., 1993). Treebanks like Tiger
for German (Brants et al., 2002) and Talbanken
for Swedish (Nivre and Megyesi, 2007) explic-
itly map phrase structures onto grammatical rela-
tions using dedicated edge labels. The Relational-
Realizational structures of Tsarfaty and Sima’an
(2008) encode relational networks (sets of rela-
tions) projected and realized by syntactic cate-
gories on top of ordinary phrase-structure nodes.
Function trees, as defined in Tsarfaty et al.
(2011), are linearly ordered labeled trees in which
every node is labeled with the grammatical func-
</bodyText>
<page confidence="0.99747">
45
</page>
<figure confidence="0.799623">
root
</figure>
<figureCaption confidence="0.865450125">
Figure 1: Deterministic conversion into function trees.
The algorithm for extracting a function tree from a de-
pendency tree as in (a) is provided in Tsarfaty et al.
(2011). For a phrase-structure tree as in (b) we can re-
place each node label with its function (dash-feature).
In a relational-realizational structure like (c) we can re-
move the projection nodes (sets) and realization nodes
(phrase labels), which leaves the function nodes intact.
</figureCaption>
<bodyText confidence="0.999987764705883">
tion of the dominated span. Function trees ben-
efit from the same advantages as other relational
schemes, namely that they are intuitive to under-
stand, they provide the interface for semantic in-
terpretation, and thus may be useful for down-
stream applications. Yet they do not suffer from
formal restrictions inherent in dependency struc-
tures, for instance, the single head assumption.
For many formal representation types there ex-
ists a fully deterministic, heuristics-free, proce-
dure mapping them to function trees. In Figure 1
we illustrate some such procedures for a simple
transitive sentence. Now, while all the structures
at the right hand side of Figure 1 are of the same
formal type (function trees), they have different
tree structures due to different theoretical assump-
tions underlying the original formal frameworks.
</bodyText>
<equation confidence="0.983282">
(t2) root (t3) root
f2 {f1,f2}
f2 f1
w w
</equation>
<figureCaption confidence="0.944002">
Figure 2: Unary chains in function trees
</figureCaption>
<bodyText confidence="0.999714357142857">
Once we have converted framework-specific
representations into function trees, the problem of
cross-framework evaluation can potentially be re-
duced to a cross-theory evaluation following Tsar-
faty et al. (2011). The main idea is that once
all structures have been converted into function
trees, one can perform a formal operation called
generalization in order to harmonize the differ-
ences between theories, and measure accurately
the distance of parse hypotheses from the gener-
alized gold. The generalization operation defined
in Tsarfaty et al. (2011), however, cannot handle
trees that may contain unary chains, and therefore
cannot be used for arbitrary function trees.
Consider for instance (t1) and (t2) in Figure 2.
According to the definition of subsumption in
Tsarfaty et al. (2011), (t1) is subsumed by (t2)
and vice versa, so the two trees should be identi-
cal – but they are not. The interpretation we wish
to give to a function tree such as (t1) is that the
word w has both the grammatical function f1 and
the grammatical function f2. This can be graphi-
cally represented as a set of labels dominating w,
as in (t3). We call structures such as (t3) multi-
function trees. In the next section we formally de-
fine multi-function trees, and then use them to de-
velop our protocol for cross-framework and cross-
theory evaluation.
</bodyText>
<sectionHeader confidence="0.9395125" genericHeader="method">
3 The Proposal: Cross-Framework
Evaluation with Multi-Function Trees
</sectionHeader>
<bodyText confidence="0.999875454545454">
Our proposal is a three-phase evaluation proto-
col in the spirit of Tsarfaty et al. (2011). First,
we obtain a formal common ground for all frame-
works in terms of multi-function trees. Then we
obtain a theoretical common ground by means
of tree-generalization on gold trees. Finally, we
calculate TED-based scores that discard the cost
of annotation-specific edits. In this section, we
define multi-function trees and update the tree-
generalization and TED-based metrics to handle
multi-function trees that reflect different theories.
</bodyText>
<figure confidence="0.999498068965518">
NN
John
NN
Mary
Mary
Mary
sbj obj
(a) -ROOT- John loves Mary
� root
loves
hd
sbj
John
obj
Mary
(b) S-root
prd
VP-prd
hd
V-hd
obj
hd
loves
John
loves
NP-obj
NN-hd
� root
sbj
hd
NP-sbj
NN-hd
John
(c) S
{sbj,prd,obj}
sbj
NP
{hd}
hd
sbj
hd
prd
loves
obj
hd
prd
V
obj
NP
{hd}
hd
loves
John
Mary
� root
(t1) root
f1
w
</figure>
<page confidence="0.929509">
46
</page>
<figureCaption confidence="0.991088333333333">
Figure 3: The Evaluation Protocol. Different formal frameworks yield different parse and gold formal types.
All types are transformed into multi-function trees. All gold trees enter generalization to yield a new gold for
each sentence. The different δ arcs represent the different edit scripts used for calculating the TED-based scores.
</figureCaption>
<subsectionHeader confidence="0.999681">
3.1 Defining Multi-Function Trees
</subsectionHeader>
<bodyText confidence="0.982031230769231">
An ordinary function tree is a linearly ordered tree
T = (V, A) with yield w1, ..., wn, where internal
nodes are labeled with grammatical function la-
bels drawn from some set L. We use span(v)
and label(v) to denote the yield and label, respec-
tively, of an internal node v. A multi-function tree
is a linearly ordered tree T = (V, A) with yield
w1, ..., wn, where internal nodes are labeled with
sets of grammatical function labels drawn from L
and where v =6 v0 implies span(v) =6 span(v0)
for all internal nodes v, v0. We use labels(v) to
denote the label set of an internal node v.
We interpret multi-function trees as encoding
sets of functional constraints over spans in func-
tion trees. Each node v in a multi-function tree
represents a constraint of the form: for each
l ∈ labels(v), there should be a node v0 in the
function tree such that span(v) = span(v0) and
label(v0) = l. Whenever we have a conversion for
function trees, we can efficiently collapse them
into multi-function trees with no unary produc-
tions, and with label sets labeling their nodes.
Thus, trees (t1) and (t2) in Figure 2 would both
be mapped to tree (t3), which encodes the func-
tional constraints encoded in either of them.
For dependency trees, we assume the conver-
sion to function trees defined in Tsarfaty et al.
(2011), where head daughters always get the la-
bel ‘hd’. For PTB style phrase-structure trees, we
replace the phrase-structure labels with functional
dash-features. In relational-realization structures
we remove projection and realization nodes. De-
terministic conversions exist also for Tiger style
treebanks and frameworks such as LFG, but we
do not discuss them here.1
1All the conversions we use are deterministic and are
defined in graph-theoretic and language-independent terms.
We make them available at http://stp.lingfil.uu.
se/˜tsarfaty/unipar/index.html.
</bodyText>
<subsectionHeader confidence="0.999793">
3.2 Generalizing Multi-Function Trees
</subsectionHeader>
<bodyText confidence="0.999977694444445">
Once we obtain multi-function trees for all the
different gold standard representations in the sys-
tem, we feed them to a generalization operation
as shown in Figure 3. The goal of this opera-
tion is to provide a consensus gold standard that
captures the linguistic structure that the different
gold theories agree on. The generalization struc-
tures are later used as the basis for the TED-based
evaluation. Generalization is defined by means of
subsumption. A multi-function tree subsumes an-
other one if and only if all the constraints defined
by the first tree are also defined by the second tree.
So, instead of demanding equality of labels as in
Tsarfaty et al. (2011), we demand set inclusion:
T-Subsumption, denoted vt, is a relation
between multi-function trees that indicates
that a tree π1 is consistent with and more
general than tree π2. Formally: π1 vt π2
iff for every node n ∈ π1 there exists a node
m ∈ π2 such that span(n) = span(m) and
labels(n) ⊆ labels(m).
T-Unification, denoted tt, is an operation
that returns the most general tree structure
that contains the information from both input
trees, and fails if such a tree does not exist.
Formally: π1 tt π2 = π3 iff π1 vt π3 and
π2 vt π3, and for all π4 such that π1 vt π4
and π2 vt π4 it holds that π3 vt π4.
T-Generalization, denoted ut, is an opera-
tion that returns the most specific tree that
is more general than both trees. Formally,
π1utπ2 = π3 iff π3 vt π1 and π3 vt π2, and
for every π4 such that π4 vt π1 and π4 vt π2
it holds that π4 vt π3.
The generalization tree contains all nodes that ex-
ist in both trees, and for each node it is labeled by
</bodyText>
<page confidence="0.996041">
47
</page>
<bodyText confidence="0.997289756756757">
the intersection of the label sets dominating the
same span in both trees. The unification tree con-
tains nodes that exist in one tree or another, and
for each span it is labeled by the union of all label
sets for this span in either tree. If we generalize
two trees and one tree has no specification for la-
bels over a span, it does not share anything with
the label set dominating the same span in the other
tree, and the label set dominating this span in the
generalized tree is empty. If the trees do not agree
on any label for a particular span, the respective
node is similarly labeled with an empty set. When
we wish to unify theories, then an empty set over
a span is unified with any other set dominating the
same span in the other tree, without altering it.
Digression: Using Unification to Merge Infor-
mation From Different Treebanks In Tsarfaty
et al. (2011), only the generalization operation
was used, providing the common denominator of
all the gold structures and serving as a common
ground for evaluation. The unification operation
is useful for other NLP tasks, for instance, com-
bining information from two different annotation
schemes or enriching one annotation scheme with
information from a different one. In particular,
we can take advantage of the new framework to
enrich the node structure reflected in one theory
with grammatical functions reflected in an anno-
tation scheme that follows a different theory. To
do so, we define the Tree-Labeling-Unification
operation on multi-function trees.
TL-Unification, denoted Litl, is an opera-
tion that returns a tree that retains the struc-
ture of the first tree and adds labels that ex-
ist over its spans in the second tree. For-
mally: 7r1 Utl 7r2 = 7r3 iff for every node
n E 7r1 there exists a node m E 7r3 such
</bodyText>
<equation confidence="0.658843">
that span(m) = span(n) and labels(m) =
labels(n) U labels(7r2, span(n)).
</equation>
<bodyText confidence="0.99707025">
Where labels(7r2, span(n)) is the set of labels of
the node with yield span(n) in 7r2 if such a node
exists and 0 otherwise. We further discuss the TL-
Unification and its use for data preparation in §4.
</bodyText>
<subsectionHeader confidence="0.989458">
3.3 TED Measures for Multi-Function Trees
</subsectionHeader>
<bodyText confidence="0.999962027027027">
The result of the generalization operation pro-
vides us with multi-function trees for each of the
sentences in the test set representing sets of con-
straints on which the different gold theories agree.
We would now like to use distance-based met-
rics in order to measure the gap between the gold
and predicted theories. The idea behind distance-
based evaluation in Tsarfaty et al. (2011) is that
recording the edit operations between the native
gold and the generalized gold allows one to dis-
card their cost when computing the cost of a parse
hypothesis turned into the generalized gold. This
makes sure that different parsers do not get penal-
ized, or favored, due to annotation specific deci-
sions that are not shared by other frameworks.
The problem is now that TED is undefined with
respect to multi-function trees because it cannot
handle complex labels. To overcome this, we
convert multi-function trees into sorted function
trees, which are simply function trees in which
any label set is represented as a unary chain of
single-labeled nodes, and the nodes are sorted ac-
cording to the canonical order of their labels.2 In
case of an empty set, a 0-length chain is created,
that is, no node is created over this span. Sorted
function trees prevent reordering nodes in a chain
in one tree to fit the order in another tree, since it
would violate the idea that the set of constraints
over a span in a multi-function tree is unordered.
The edit operations we assume are add-
node(l, i, j) and delete-node(l, i, j) where l E L
is a grammatical function label and i &lt; j define
the span of a node in the tree. Insertion into a
unary chain must confine with the canonical order
of the labels. Every operation is assigned a cost.
An edit script is a sequence of edit operations that
turns a function tree 7r1 into 7r2, that is:
</bodyText>
<equation confidence="0.849532">
ES(7r1, 7r2) = (e1, ... , ek)
</equation>
<bodyText confidence="0.999497666666667">
Since all operations are anchored in spans, the se-
quence can be determined to have a unique order
of traversing the tree (say, DFS). Different edit
scripts then only differ in their set of operations
on spans. The edit distance problem is finding the
minimal cost script, that is, one needs to solve:
</bodyText>
<equation confidence="0.995743333333333">
�ES�(7r1, 7r2) = min
ES(7r1,1C2)
eEES(7r1,-7r2)
</equation>
<bodyText confidence="0.999782">
In the current setting, when using only add and
delete operations on spans, there is only one edit
script that corresponds to the minimal edit cost.
So, finding the minimal edit script entails finding
a single set of operations turning 7r1 into 7r2.
</bodyText>
<footnote confidence="0.799546">
2The ordering can be alphabetic, thematic, etc.
</footnote>
<equation confidence="0.392597">
cost(e)
</equation>
<page confidence="0.987172">
48
</page>
<bodyText confidence="0.999943444444444">
We can now define 6 for the ith framework, as
the error of parsei relative to its native gold stan-
dard goldi and to the generalized gold gen. This
is the edit cost minus the cost of the script turning
parsei into gen intersected with the script turning
goldi into gen. The underlying intuition is that
if an operation that was used to turn parsei into
gen is used to discard theory-specific information
from goldi, its cost should not be counted as error.
</bodyText>
<equation confidence="0.532399">
6(parsei, goldi, gen) = cost(E5∗(parsei, gen))
−cost(E5∗(parsei, gen) n E5∗(goldi, gen))
</equation>
<bodyText confidence="0.9992346">
In order to turn distance measures into parse-
scores we now normalize the error relative to the
size of the trees and subtract it from a unity. So
the Sentence Score for parsing with framework i
is:
</bodyText>
<equation confidence="0.8897291">
score(parsei, goldi, gen) =
6(parsei, goldi,gen)
1 −
|parsei |+ |gen|
Finally, Test-Set Average is defined by macro-
avaraging over all sentences in the test-set:
arseij, goldij, genj)
1 −
�|testset|
j=1 |parseij |+ |genj|
</equation>
<bodyText confidence="0.999565785714286">
This last formula represents the TEDEVAL metric
that we use in our experiments.
A Note on System Complexity Conversion of
a dependency or a constituency tree into a func-
tion tree is linear in the size of the tree. Our
implementation of the generalization and unifica-
tion operation is an exact, greedy, chart-based al-
gorithm that runs in polynomial time (O(n2) in
n the number of terminals). The TED software
that we utilize builds on the TED efficient algo-
rithm of Zhang and Shasha (1989) which runs in
O(|T1||T2 |min(d1, n1) min(d2, n2)) time where
di is the tree degree (depth) and ni is the number
of terminals in the respective tree (Bille, 2005).
</bodyText>
<sectionHeader confidence="0.99982" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9998424">
We validate our cross-framework evaluation pro-
cedure on two languages, English and Swedish.
For English, we compare the performance of
two dependency parsers, MaltParser (Nivre et al.,
2006) and MSTParser (McDonald et al., 2005),
and two constituency-based parsers, the Berkeley
parser (Petrov et al., 2006) and the Brown parser
(Charniak and Johnson, 2005). All experiments
use Penn Treebank (PTB) data. For Swedish,
we compare MaltParser and MSTParser with two
variants of the Berkeley parser, one trained on
phrase structure trees, and one trained on a vari-
ant of the Relational-Realizational representation
of Tsarfaty and Sima’an (2008). All experiments
use the Talbanken Swedish Treebank (STB) data.
</bodyText>
<subsectionHeader confidence="0.967564">
4.1 English Cross-Framework Evaluation
</subsectionHeader>
<bodyText confidence="0.9944486">
We use sections 02–21 of the WSJ Penn Tree-
bank for training and section 00 for evaluation and
analysis. We use two different native gold stan-
dards subscribing to different theories of encoding
grammatical relations in tree structures:
</bodyText>
<listItem confidence="0.817604333333333">
o THE DEPENDENCY-BASED THEORY is the
theory encoded in the basic Stanford Depen-
dencies (SD) scheme. We obtain the set of
basic stanford dependency trees using the
software of de Marneffe et al. (2006) and
train the dependency parsers directly on it.
</listItem>
<bodyText confidence="0.924018428571429">
o THE CONSTITUENCY-BASED THEORY is
the theory reflected in the phrase-structure
representation of the PTB (Marcus et al.,
1993) enriched with function labels compat-
ible with the Stanford Dependencies (SD)
scheme. We obtain trees that reflect this
theory by TL-Unification of the PTB multi-
function trees with the SD multi-function
trees (PTBUtlSD) as illustrated in Figure 4.
The theory encoded in the multi-function trees
corresponding to SD is different from the one
obtained by our TL-Unification, as may be seen
from the difference between the flat SD multi-
function tree and the result of the PTBUtlSD in
Figure 4. Another difference concerns coordina-
tion structures, encoded as binary branching trees
in SD and as flat productions in the PTBUtlSD.
Such differences are not only observable but also
quantifiable, and using our redefined TED metric
the cross-theory overlap is 0.8571.
The two dependency parsers were trained using
the same settings as in Tsarfaty et al. (2011), using
SVMTool (Gim´enez and M`arquez, 2004) to pre-
dict part-of-speech tags at parsing time. The two
constituency parsers were used with default set-
tings and were allowed to predict their own part-
of-speech tags. We report three different evalua-
tion metrics for the different experiments:
</bodyText>
<equation confidence="0.915446">
�|testset |6(p
j=1
</equation>
<page confidence="0.973878">
49
</page>
<figure confidence="0.80118">
root
</figure>
<figureCaption confidence="0.930621666666667">
Figure 4: Conversion of PTB and SD tree to multi-
function trees, followed by TL-Unification of the trees.
Note that some PTB nodes remain without an SD label.
</figureCaption>
<bodyText confidence="0.94180075">
o LAS/UAS (Buchholz and Marsi, 2006)
o PARSEVAL (Black et al., 1991)
o TEDEVAL as defined in Section 3
We use LAS/UAS for dependency parsers that
were trained on the same dependency theory. We
use ParseEval to evaluate phrase-structure parsers
that were trained on PTB trees in which dash-
features and empty traces are removed. We
use our implementation of TEDEVAL to evaluate
parsing results across all frameworks under two
different scenarios:3 TEDEVAL SINGLE evalu-
ates against the native gold multi-function trees.
TEDEVAL MULTIPLE evaluates against the gen-
eralized (cross-theory) multi-function trees. Un-
labeled TEDEVAL scores are obtained by sim-
ply removing all labels from the multi-function
nodes, and using unlabeled edit operations. We
calculate pairwise statistical significance using a
shuffling test with 10K iterations (Cohen, 1995).
Tables 1 and 2 present the results of our cross-
framework evaluation for English Parsing. In the
left column of Table 1 we report ParsEval scores
for constituency-based parsers. As expected, F-
Scores for the Brown parser are higher than the
F-Scores of the Berkeley parser. F-Scores are
however not applicable across frameworks. In
the rightmost column of Table 1 we report the
LAS/UAS results for all parsers. If a parser yields
</bodyText>
<footnote confidence="0.957830666666667">
3Our TedEval software can be downloaded at
http://stp.lingfil.uu.se/˜tsarfaty/
unipar/download.html.
</footnote>
<bodyText confidence="0.999914567567568">
a constituency tree, it is converted to and evalu-
ated on SD. Here we see that MST outperforms
Malt, though the differences for labeled depen-
dencies are insignificant. We also observe here a
familiar pattern from Cer et al. (2010) and others,
where the constituency parsers significantly out-
perform the dependency parsers after conversion
of their output into dependencies.
The conversion to SD allows one to compare
results across formal frameworks, but not with-
out a cost. The conversion introduces a set of an-
notation specific decisions which may introduce
a bias into the evaluation. In the middle column
of Table 1 we report the TEDEVAL metrics mea-
sured against the generalized gold standard for all
parsing frameworks. We can now confirm that
the constituency-based parsers significantly out-
perform the dependency parsers, and that this is
not due to specific theoretical decisions which are
seen to affect LAS/UAS metrics (Schwartz et al.,
2011). For the dependency parsers we now see
that Malt outperforms MST on labeled dependen-
cies slightly, but the difference is insignificant.
The fact that the discrepancy in theoretical as-
sumptions between different frameworks indeed
affects the conversion-based evaluation procedure
is reflected in the results we report in Table 2.
Here the leftmost and rightmost columns report
TEDEVAL scores against the own native gold
(SINGLE) and the middle column against the gen-
eralized gold (MULTIPLE). Had the theories
for SD and PTBUtlSD been identical, TEDEVAL
SINGLE and TEDEVAL MULTIPLE would have
been equal in each line. Because of theoretical
discrepancies, we see small gaps in parser perfor-
mance between these cases. Our protocol ensures
that such discrepancies do not bias the results.
</bodyText>
<subsectionHeader confidence="0.999295">
4.2 Cross-Framework Swedish Parsing
</subsectionHeader>
<bodyText confidence="0.9955795">
We use the standard training and test sets of the
Swedish Treebank (Nivre and Megyesi, 2007)
with two gold standards presupposing different
theories:
</bodyText>
<listItem confidence="0.987315857142857">
• THE DEPENDENCY-BASED THEORY is the
dependency version of the Swedish Tree-
bank. All trees are projectivized (STB-Dep).
• THE CONSTITUENCY-BASED THEORY is
the standard Swedish Treebank with gram-
matical function labels on the edges of con-
stituency structures (STB).
</listItem>
<figure confidence="0.996089534883721">
�
0
VP
V
NP
0
Mary
NN
loves
John
�0
0
John 0
loves
(PTB) S
NP
NN
Mary
Mary
(PTB) Utl (SD) = {root}
{sbj}
John
{hd}
loves
{obj}
Mary
0
� root
sbj
John
obj
Mary
{sbj}
John
{hd}
loves
{obj}
Mary
loves
hd
sbj obj
(SD) -ROOT- John loves Mary
� {root}
</figure>
<page confidence="0.903874">
50
</page>
<table confidence="0.999268">
Formalism PS Trees MF Trees Dep Trees
Theory PTB tlt SD (PTB tlt SD) SD
ut SD
Metrics PARSEVAL TEDEVAL ATTSCORES
MALT N/A U: 0.9525 U: 0.8962
L: 0.9088 L: 0.8772
MST N/A U: 0.9549 U: 0.9059
L: 0.9049 L: 0.8795
BERKELEY F-Scores U: 0.9677 U: 0.9254
0.9096 L: 0.9227 L: 0.9031
BROWN F-Scores U: 0.9702 U: 0.9289
0.9129 L: 0.9264 L: 0.9057
</table>
<tableCaption confidence="0.9447115">
Table 1: English cross-framework evaluation: Three
measures as applicable to the different schemes. Bold-
face scores are highest in their column. Italic scores
are the highest for dependency parsers in their column.
</tableCaption>
<table confidence="0.999921923076923">
Formalism PS Trees MF Trees Dep Trees
Theory PTB tlt SD (PTB tlt SD) SD
ut SD
Metrics TEDEVAL TEDEVAL TEDEVAL
SINGLE MULTIPLE SINGLE
MALT N/A U: 0.9525 U: 0.9524
L: 0.9088 L: 0.9186
MST N/A U: 0.9549 U: 0.9548
L: 0.9049 L: 0.9149
BERKELEY U: 0.9645 U: 0.9677 U: 0.9649
L: 0.9271 L: 0.9227 L: 0.9324
BROWN U: 0.9667 U: 9702 U: 0.9679
L: 0.9301 L: 9264 L: 0.9362
</table>
<tableCaption confidence="0.99641975">
Table 2: English cross-framework evaluation: TEDE-
VAL scores against gold and generalized gold. Bold-
face scores are highest in their column. Italic scores
are highest for dependency parsers in their column.
</tableCaption>
<bodyText confidence="0.9999504">
Because there are no parsers that can out-
put the complete STB representation including
edge labels, we experiment with two variants of
this theory, one which is obtained by simply re-
moving the edge labels and keeping only the
phrase-structure labels (STB-PS) and one which
is loosely based on the Relational-Realizational
scheme of Tsarfaty and Sima’an (2008) but ex-
cludes the projection set nodes (STB-RR). RR
trees only add function nodes to PS trees, and
it holds that STB-PSutSTB-RR=STB-PS. The
overlap between the theories expressed in multi-
function trees originating from STB-Dep and
STB-RR is 0.7559. Our evaluation protocol takes
into account such discrepancies while avoiding
biases that may be caused due to these differences.
We evaluate MaltParser, MSTParser and two
versions of the Berkeley parser, one trained on
STB-PS and one trained on STB-RR. We use
predicted part-of-speech tags for the dependency
</bodyText>
<table confidence="0.999316583333333">
Formalism PS Trees MF Trees Dep Trees
Theory STB STB ut Dep Dep
Metrics PARSEVAL TEDEVAL ATTSCORE
MALT N/A U: 0.9266 U: 0.8298
L: 0.8225 L: 0.7782
MST N/A U: 0.9275 U: 0.8438
L: 0.8121 L: 0.7824
BKLY/STB-RR U: 0.9281 N/A
F-Score L: 0.7861
0.7914
BKLY/STB-PS F-Score N/A N/A
0.7855
</table>
<tableCaption confidence="0.995817">
Table 3: Swedish cross-framework evaluation: Three
measures as applicable to the different schemes. Bold-
face scores are the highest in their column.
</tableCaption>
<table confidence="0.9998722">
Formalism PS Trees MF Trees Dep Trees
Theory STB STB ut Dep Dep
Metrics TEDEVAL TEDEVAL TEDEVAL
SINGLE MULTIPLE SINGLE
MALT N/A U: 0.9266 U: 0.9264
L: 0.8225 L: 0.8372
MST N/A U: 0.9275 U: 0.9272
L: 0.8121 L: 0.8275
BKLY-STB-RR U: 0.9239 U: 0.9281 N/A
L: 0.7946 L: 0.7861
</table>
<tableCaption confidence="0.934412666666667">
Table 4: Swedish cross-framework evaluation: TEDE-
VAL scores against the native gold and the generalized
gold. Boldface scores are the highest in their column.
</tableCaption>
<bodyText confidence="0.9999755">
parsers, using the HunPoS tagger (Megyesi,
2009), but let the Berkeley parser predict its own
tags. We use the same evaluation metrics and pro-
cedures as before. Prior to evaluating RR trees
using ParsEval we strip off the added function
nodes. Prior to evaluating them using TedEval we
strip off the phrase-structure nodes.
Tables 3 and 4 summarize the parsing results
for the different Swedish parsers. In the leftmost
column of table 3 we present the constituency-
based evaluation measures. Interestingly, the
Berkeley RR instantiation performs better than
when training the Berkeley parser on PS trees.
These constituency-based scores however have a
limited applicability, and we cannot use them to
compare constituency and dependency parsers. In
the rightmost column of Table 3 we report the
LAS/UAS results for the two dependency parsers.
Here we see higher performance demonstrated by
MST on both labeled and unlabeled dependen-
cies, but the differences on labeled dependencies
are insignificant. Since there is no automatic pro-
cedure for converting bare-bone phrase-structure
Swedish trees to dependency trees, we cannot use
</bodyText>
<page confidence="0.994833">
51
</page>
<bodyText confidence="0.998596">
LAS/UAS to compare across frameworks, and we
use TEDEVAL for cross-framework evaluation.
Training the Berkeley parser on RR trees which
encode a mapping of PS nodes to grammatical
functions allows us to compare parse results for
trees belonging to the STB theory with trees obey-
ing the STB-Dep theory. For unlabeled TEDE-
VAL scores, the dependency parsers perform at the
same level as the constituency parser, though the
difference is insignificant. For labeled TEDEVAL
the dependency parsers significantly outperform
the constituency parser. When considering only
the dependency parsers, there is a small advantage
for Malt on labeled dependencies, and an advan-
tage for MST on unlabeled dependencies, but the
latter is insignificant. This effect is replicated in
Table 4 where we evaluate dependency parsers us-
ing TEDEVAL against their own gold theories. Ta-
ble 4 further confirms that there is a gap between
the STB and the STB-Dep theories, reflected in
the scores against the native and generalized gold.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999991053571429">
We presented a formal protocol for evaluating
parsers across frameworks and used it to soundly
compare parsing results for English and Swedish.
Our approach follows the three-phase protocol of
Tsarfaty et al. (2011), namely: (i) obtaining a for-
mal common ground for the different representa-
tion types, (ii) computing the theoretical common
ground for each test sentence, and (iii) counting
only what counts, that is, measuring the distance
between the common ground and the parse tree
while discarding annotation-specific edits.
A pre-condition for applying our protocol is the
availability of a relational interpretation of trees in
the different frameworks. For dependency frame-
works this is straightforward, as these relations
are encoded on top of dependency arcs. For con-
stituency trees with an inherent mapping of nodes
onto grammatical relations (Merlo and Musillo,
2005; Gabbard et al., 2006; Tsarfaty and Sima’an,
2008), a procedure for reading relational schemes
off of the trees is trivial to implement.
For parsers that are trained on and parse into
bare-bones phrase-structure trees this is not so.
Reading off the relational structure may be more
costly and require interjection of additional theo-
retical assumptions via manually written scripts.
Scripts that read off grammatical relations based
on tree positions work well for configurational
languages such as English (de Marneffe et al.,
2006) but since grammatical relations are re-
flected differently in different languages (Postal
and Perlmutter, 1977; Bresnan, 2000), a proce-
dure to read off these relations in a language-
independent fashion from phrase-structure trees
does not, and should not, exist (Rambow, 2010).
The crucial point is that even when using ex-
ternal scripts for recovering a relational scheme
for phrase-structure trees, our protocol has a clear
advantage over simply scoring converted trees.
Manually created conversion scripts alter the the-
oretical assumptions inherent in the trees and thus
may bias the results. Our generalization operation
and three-way TED make sure that theory-specific
idiosyncrasies injected through such scripts do
not lead to over-penalizing or over-crediting
theory-specific structural variations.
Certain linguistic structures cannot yet be eval-
uated with our protocol because of the strict as-
sumption that the labeled spans in a parse form a
tree. In the future we plan to extend the protocol
for evaluating structures that go beyond linearly-
ordered trees in order to allow for non-projective
trees and directed acyclic graphs. In addition, we
plan to lift the restriction that the parse yield is
known in advance, in order to allow for evalua-
tion of joint parse-segmentation hypotheses.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99918825">
We developed a protocol for comparing parsing
results across different theories and representa-
tion types which is framework-independent in the
sense that it can accommodate any formal syntac-
tic framework that encodes grammatical relations,
and it is language-independent in the sense that
there is no language specific knowledge encoded
in the procedure. As such, this protocol is ad-
equate for parser evaluation in cross-framework
and cross-language tasks and parsing competi-
tions, and using it across the board is expected
to open new horizons in our understanding of the
strengths and weaknesses of different parsers in
the face of different theories and different data.
Acknowledgments We thank David McClosky,
Marco Khulmann, Yoav Goldberg and three
anonymous reviewers for useful comments. We
further thank Jennifer Foster for the Brown parses
and parameter files. This research is partly funded
by the Swedish National Science Foundation.
</bodyText>
<page confidence="0.997814">
52
</page>
<sectionHeader confidence="0.998347" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99995490990991">
Philip Bille. 2005. A survey on tree edit distance and
related. problems. Theoretical Computer Science,
337:217–239.
Ezra Black, Steven P. Abney, D. Flickenger, Clau-
dia Gdaniec, Ralph Grishman, P. Harrison, Don-
ald Hindle, Robert Ingria, Frederick Jelinek, Ju-
dith L. Klavans, Mark Liberman, Mitchell P. Mar-
cus, Salim Roukos, Beatrice Santorini, and Tomek
Strzalkowski. 1991. A procedure for quantitatively
comparing the syntactic coverage of English gram-
mars. In Proceedings of the DARPA Workshop on
Speech and Natural Language, pages 306–311.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The Tiger
treebank. In Proceedings of TLT.
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149–164.
Aoife Cahill, Michael Burke, Ruth O’Donovan, Stefan
Riezler, Josef van Genabith, and Andy Way. 2008.
Wide-coverage deep statistical parsing using auto-
matic dependency structure annotation. Computa-
tional Linguistics, 34(1):81–124.
John Carroll, Edward Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: A survey and a new pro-
posal. In Proceedings of LREC, pages 447–454.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Ju-
rafsky, and Christopher D. Manning. 2010. Pars-
ing to Stanford Dependencies: Trade-offs between
speed and accuracy. In Proceedings of LREC.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL.
Paul Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, pages 449–454.
Ryan Gabbard, Mitchell Marcus, and Seth Kulick.
2006. Fully parsing the Penn treebank. In Proceed-
ing of HLT-NAACL, pages 184–191.
Jes´us Gim´enez and Lluis M`arquez. 2004. SVMTool:
A general POS tagger generator based on support
vector machines. In Proceedings of LREC.
Sandra K¨ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Mor-
gan &amp; Claypool Publishers.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of IJCAI-95, pages 1420–1425.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR International Conference
on Arabic Language Resources and Tools.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313–330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In HLT ’05:
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 523–530, Morristown, NJ,
USA. Association for Computational Linguistics.
Beata Megyesi. 2009. The open source tagger Hun-
PoS for Swedish. In Proceedings of the 17th Nordic
Conference of Computational Linguistics (NODAL-
IDA), pages 239–241.
Igor Mel’ˇcuk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Paola Merlo and Gabriele Musillo. 2005. Accurate
function parsing. In Proceedings of EMNLP, pages
620–627.
Joakim Nivre and Beata Megyesi. 2007. Bootstrap-
ping a Swedish Treebank using cross-corpus har-
monization and annotation projection. In Proceed-
ings of TLT.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, pages
2216–2219.
Joakim Nivre, Laura Rimell, Ryan McDonald, and
Carlos G´omez-Rodriguez. 2010. Evaluation of de-
pendency parsers on unbounded dependencies. In
Proceedings of COLING, pages 813–821.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL.
Paul M. Postal and David M. Perlmutter. 1977. To-
ward a universal characterization of passivization.
In Proceedings of the 3rd Annual Meeting of the
Berkeley Linguistics Society, pages 394–417.
Owen Rambow. 2010. The simple truth about de-
pendency and phrase structure representations: An
opinion piece. In Proceedings of HLT-ACL, pages
337–340.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In Proceedings of ACL, pages
663–672.
Khalil Sima’an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique
des Langues.
</reference>
<page confidence="0.986858">
53
</page>
<reference confidence="0.9900306">
Reut Tsarfaty and Khalil Sima’an. 2008. Relational-
Realizational parsing. In Proceedings of CoLing.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP.
Kaizhong Zhang and Dennis Shasha. 1989. Sim-
ple fast algorithms for the editing distance between
trees and related problems. In SIAM Journal of
Computing, volume 18, pages 1245–1262.
</reference>
<page confidence="0.999001">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.514402">
<title confidence="0.996281">Cross-Framework Evaluation for Statistical Parsing</title>
<author confidence="0.885413">Reut Tsarfaty Joakim Nivre Evelina Andersson</author>
<affiliation confidence="0.717015">Uppsala University, Box 635, 75126 Uppsala, Sweden</affiliation>
<abstract confidence="0.983908833333333">A serious bottleneck of comparative parser evaluation is the fact that different parsers subscribe to different formal frameworks and theoretical assumptions. Converting outputs from one framework to another is less than optimal as it easily introduces noise into the process. Here we present a principled protocol for evaluating parsing results across frameworks based on function trees, tree generalization and edit distance metrics. This extends a previously proposed framework for cross-theory evaluation and allows us to compare a wider class of parsers. We demonstrate the usefulness and language independence of our procedure by evaluating constituency and dependency parsers on English and Swedish.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philip Bille</author>
</authors>
<title>A survey on tree edit distance and related. problems.</title>
<date>2005</date>
<journal>Theoretical Computer Science,</journal>
<pages>337--217</pages>
<contexts>
<context position="22216" citStr="Bille, 2005" startWordPosition="3601" endWordPosition="3602">AL metric that we use in our experiments. A Note on System Complexity Conversion of a dependency or a constituency tree into a function tree is linear in the size of the tree. Our implementation of the generalization and unification operation is an exact, greedy, chart-based algorithm that runs in polynomial time (O(n2) in n the number of terminals). The TED software that we utilize builds on the TED efficient algorithm of Zhang and Shasha (1989) which runs in O(|T1||T2 |min(d1, n1) min(d2, n2)) time where di is the tree degree (depth) and ni is the number of terminals in the respective tree (Bille, 2005). 4 Experiments We validate our cross-framework evaluation procedure on two languages, English and Swedish. For English, we compare the performance of two dependency parsers, MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and two constituency-based parsers, the Berkeley parser (Petrov et al., 2006) and the Brown parser (Charniak and Johnson, 2005). All experiments use Penn Treebank (PTB) data. For Swedish, we compare MaltParser and MSTParser with two variants of the Berkeley parser, one trained on phrase structure trees, and one trained on a variant of the Relational-Re</context>
</contexts>
<marker>Bille, 2005</marker>
<rawString>Philip Bille. 2005. A survey on tree edit distance and related. problems. Theoretical Computer Science, 337:217–239.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ezra Black</author>
<author>Steven P Abney</author>
<author>D Flickenger</author>
<author>Claudia Gdaniec</author>
<author>Ralph Grishman</author>
<author>P Harrison</author>
<author>Donald Hindle</author>
<author>Robert Ingria</author>
<author>Frederick Jelinek</author>
<author>Judith L Klavans</author>
<author>Mark Liberman</author>
<author>Mitchell P Marcus</author>
<author>Salim Roukos</author>
<author>Beatrice Santorini</author>
<author>Tomek Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Workshop on Speech and Natural Language,</booktitle>
<pages>306--311</pages>
<contexts>
<context position="6500" citStr="Black et al., 1991" startWordPosition="945" endWordPosition="948">show that our extended protocol, which can handle linearlyordered labeled trees with arbitrary branching, can soundly compare parsing results across frameworks in a representation-independent and language-independent fashion. 2 Preliminaries: Relational Schemes for Cross-Framework Parse Evaluation Traditionally, different statistical parsers have been evaluated using specially designated evaluation measures that are designed to fit their representation types. Dependency trees are evaluated using attachment scores (Buchholz and Marsi, 2006), phrase-structure trees are evaluated using ParsEval (Black et al., 1991), LFG-based parsers postulate an evaluation procedure based on fstructures (Cahill et al., 2008), and so on. From a downstream application point of view, there is no significance as to which formalism was used for generating the representation and which learning methods have been utilized. The bottom line is simply which parsing framework most accurately recovers a useful representation that helps to unravel the human-perceived interpretation. Relational schemes, that is, schemes that encode the set of grammatical relations that constitute the predicate-argument structures of sentences, provid</context>
<context position="24983" citStr="Black et al., 1991" startWordPosition="4035" endWordPosition="4038">ncy parsers were trained using the same settings as in Tsarfaty et al. (2011), using SVMTool (Gim´enez and M`arquez, 2004) to predict part-of-speech tags at parsing time. The two constituency parsers were used with default settings and were allowed to predict their own partof-speech tags. We report three different evaluation metrics for the different experiments: �|testset |6(p j=1 49 root Figure 4: Conversion of PTB and SD tree to multifunction trees, followed by TL-Unification of the trees. Note that some PTB nodes remain without an SD label. o LAS/UAS (Buchholz and Marsi, 2006) o PARSEVAL (Black et al., 1991) o TEDEVAL as defined in Section 3 We use LAS/UAS for dependency parsers that were trained on the same dependency theory. We use ParseEval to evaluate phrase-structure parsers that were trained on PTB trees in which dashfeatures and empty traces are removed. We use our implementation of TEDEVAL to evaluate parsing results across all frameworks under two different scenarios:3 TEDEVAL SINGLE evaluates against the native gold multi-function trees. TEDEVAL MULTIPLE evaluates against the generalized (cross-theory) multi-function trees. Unlabeled TEDEVAL scores are obtained by simply removing all la</context>
</contexts>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>Ezra Black, Steven P. Abney, D. Flickenger, Claudia Gdaniec, Ralph Grishman, P. Harrison, Donald Hindle, Robert Ingria, Frederick Jelinek, Judith L. Klavans, Mark Liberman, Mitchell P. Marcus, Salim Roukos, Beatrice Santorini, and Tomek Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proceedings of the DARPA Workshop on Speech and Natural Language, pages 306–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The Tiger treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of TLT.</booktitle>
<contexts>
<context position="8003" citStr="Brants et al., 2002" startWordPosition="1168" endWordPosition="1171">ntation for the evaluation of statistical parsers (Lin, 1995; Carroll et al., 1998; Cer et al., 2010). The annotated data which statistical parsers are trained on encode these grammatical relationships in different ways. Dependency treebanks provide a ready-made representation of grammatical relations on top of arcs connecting the words in the sentence (K¨ubler et al., 2009). The Penn Treebank and phrase-structure annotated resources encode partial information about grammatical relations as dash-features decorating phrase structure nodes (Marcus et al., 1993). Treebanks like Tiger for German (Brants et al., 2002) and Talbanken for Swedish (Nivre and Megyesi, 2007) explicitly map phrase structures onto grammatical relations using dedicated edge labels. The RelationalRealizational structures of Tsarfaty and Sima’an (2008) encode relational networks (sets of relations) projected and realized by syntactic categories on top of ordinary phrase-structure nodes. Function trees, as defined in Tsarfaty et al. (2011), are linearly ordered labeled trees in which every node is labeled with the grammatical func45 root Figure 1: Deterministic conversion into function trees. The algorithm for extracting a function tr</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The Tiger treebank. In Proceedings of TLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>Lexical-Functional Syntax.</title>
<date>2000</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="35314" citStr="Bresnan, 2000" startWordPosition="5684" endWordPosition="5685">2008), a procedure for reading relational schemes off of the trees is trivial to implement. For parsers that are trained on and parse into bare-bones phrase-structure trees this is not so. Reading off the relational structure may be more costly and require interjection of additional theoretical assumptions via manually written scripts. Scripts that read off grammatical relations based on tree positions work well for configurational languages such as English (de Marneffe et al., 2006) but since grammatical relations are reflected differently in different languages (Postal and Perlmutter, 1977; Bresnan, 2000), a procedure to read off these relations in a languageindependent fashion from phrase-structure trees does not, and should not, exist (Rambow, 2010). The crucial point is that even when using external scripts for recovering a relational scheme for phrase-structure trees, our protocol has a clear advantage over simply scoring converted trees. Manually created conversion scripts alter the theoretical assumptions inherent in the trees and thus may bias the results. Our generalization operation and three-way TED make sure that theory-specific idiosyncrasies injected through such scripts do not le</context>
</contexts>
<marker>Bresnan, 2000</marker>
<rawString>Joan Bresnan. 2000. Lexical-Functional Syntax. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X,</booktitle>
<pages>149--164</pages>
<contexts>
<context position="6426" citStr="Buchholz and Marsi, 2006" startWordPosition="935" endWordPosition="938">y-based parsing models, all trained on the Swedish treebank data. All in all we show that our extended protocol, which can handle linearlyordered labeled trees with arbitrary branching, can soundly compare parsing results across frameworks in a representation-independent and language-independent fashion. 2 Preliminaries: Relational Schemes for Cross-Framework Parse Evaluation Traditionally, different statistical parsers have been evaluated using specially designated evaluation measures that are designed to fit their representation types. Dependency trees are evaluated using attachment scores (Buchholz and Marsi, 2006), phrase-structure trees are evaluated using ParsEval (Black et al., 1991), LFG-based parsers postulate an evaluation procedure based on fstructures (Cahill et al., 2008), and so on. From a downstream application point of view, there is no significance as to which formalism was used for generating the representation and which learning methods have been utilized. The bottom line is simply which parsing framework most accurately recovers a useful representation that helps to unravel the human-perceived interpretation. Relational schemes, that is, schemes that encode the set of grammatical relati</context>
<context position="24951" citStr="Buchholz and Marsi, 2006" startWordPosition="4029" endWordPosition="4032">ory overlap is 0.8571. The two dependency parsers were trained using the same settings as in Tsarfaty et al. (2011), using SVMTool (Gim´enez and M`arquez, 2004) to predict part-of-speech tags at parsing time. The two constituency parsers were used with default settings and were allowed to predict their own partof-speech tags. We report three different evaluation metrics for the different experiments: �|testset |6(p j=1 49 root Figure 4: Conversion of PTB and SD tree to multifunction trees, followed by TL-Unification of the trees. Note that some PTB nodes remain without an SD label. o LAS/UAS (Buchholz and Marsi, 2006) o PARSEVAL (Black et al., 1991) o TEDEVAL as defined in Section 3 We use LAS/UAS for dependency parsers that were trained on the same dependency theory. We use ParseEval to evaluate phrase-structure parsers that were trained on PTB trees in which dashfeatures and empty traces are removed. We use our implementation of TEDEVAL to evaluate parsing results across all frameworks under two different scenarios:3 TEDEVAL SINGLE evaluates against the native gold multi-function trees. TEDEVAL MULTIPLE evaluates against the generalized (cross-theory) multi-function trees. Unlabeled TEDEVAL scores are ob</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL-X, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Michael Burke</author>
<author>Ruth O’Donovan</author>
<author>Stefan Riezler</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Wide-coverage deep statistical parsing using automatic dependency structure annotation.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<marker>Cahill, Burke, O’Donovan, Riezler, van Genabith, Way, 2008</marker>
<rawString>Aoife Cahill, Michael Burke, Ruth O’Donovan, Stefan Riezler, Josef van Genabith, and Andy Way. 2008. Wide-coverage deep statistical parsing using automatic dependency structure annotation. Computational Linguistics, 34(1):81–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Edward Briscoe</author>
<author>Antonio Sanfilippo</author>
</authors>
<title>Parser evaluation: A survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>447--454</pages>
<contexts>
<context position="7465" citStr="Carroll et al., 1998" startWordPosition="1089" endWordPosition="1092"> accurately recovers a useful representation that helps to unravel the human-perceived interpretation. Relational schemes, that is, schemes that encode the set of grammatical relations that constitute the predicate-argument structures of sentences, provide an interface to semantic interpretation. They are more intuitively understood than, say, phrase-structure trees, and thus they are also more useful for practical applications. For these reasons, relational schemes have been repeatedly singled out as an appropriate level of representation for the evaluation of statistical parsers (Lin, 1995; Carroll et al., 1998; Cer et al., 2010). The annotated data which statistical parsers are trained on encode these grammatical relationships in different ways. Dependency treebanks provide a ready-made representation of grammatical relations on top of arcs connecting the words in the sentence (K¨ubler et al., 2009). The Penn Treebank and phrase-structure annotated resources encode partial information about grammatical relations as dash-features decorating phrase structure nodes (Marcus et al., 1993). Treebanks like Tiger for German (Brants et al., 2002) and Talbanken for Swedish (Nivre and Megyesi, 2007) explicitl</context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>John Carroll, Edward Briscoe, and Antonio Sanfilippo. 1998. Parser evaluation: A survey and a new proposal. In Proceedings of LREC, pages 447–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing to Stanford Dependencies: Trade-offs between speed and accuracy.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>Cer, de Marneffe, Jurafsky, Manning, 2010</marker>
<rawString>Daniel Cer, Marie-Catherine de Marneffe, Daniel Jurafsky, and Christopher D. Manning. 2010. Parsing to Stanford Dependencies: Trade-offs between speed and accuracy. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="22589" citStr="Charniak and Johnson, 2005" startWordPosition="3654" endWordPosition="3657">ED software that we utilize builds on the TED efficient algorithm of Zhang and Shasha (1989) which runs in O(|T1||T2 |min(d1, n1) min(d2, n2)) time where di is the tree degree (depth) and ni is the number of terminals in the respective tree (Bille, 2005). 4 Experiments We validate our cross-framework evaluation procedure on two languages, English and Swedish. For English, we compare the performance of two dependency parsers, MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and two constituency-based parsers, the Berkeley parser (Petrov et al., 2006) and the Brown parser (Charniak and Johnson, 2005). All experiments use Penn Treebank (PTB) data. For Swedish, we compare MaltParser and MSTParser with two variants of the Berkeley parser, one trained on phrase structure trees, and one trained on a variant of the Relational-Realizational representation of Tsarfaty and Sima’an (2008). All experiments use the Talbanken Swedish Treebank (STB) data. 4.1 English Cross-Framework Evaluation We use sections 02–21 of the WSJ Penn Treebank for training and section 00 for evaluation and analysis. We use two different native gold standards subscribing to different theories of encoding grammatical relatio</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cohen</author>
</authors>
<title>Empirical Methods for Artificial Intelligence.</title>
<date>1995</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="25759" citStr="Cohen, 1995" startWordPosition="4152" endWordPosition="4153">ure parsers that were trained on PTB trees in which dashfeatures and empty traces are removed. We use our implementation of TEDEVAL to evaluate parsing results across all frameworks under two different scenarios:3 TEDEVAL SINGLE evaluates against the native gold multi-function trees. TEDEVAL MULTIPLE evaluates against the generalized (cross-theory) multi-function trees. Unlabeled TEDEVAL scores are obtained by simply removing all labels from the multi-function nodes, and using unlabeled edit operations. We calculate pairwise statistical significance using a shuffling test with 10K iterations (Cohen, 1995). Tables 1 and 2 present the results of our crossframework evaluation for English Parsing. In the left column of Table 1 we report ParsEval scores for constituency-based parsers. As expected, FScores for the Brown parser are higher than the F-Scores of the Berkeley parser. F-Scores are however not applicable across frameworks. In the rightmost column of Table 1 we report the LAS/UAS results for all parsers. If a parser yields 3Our TedEval software can be downloaded at http://stp.lingfil.uu.se/˜tsarfaty/ unipar/download.html. a constituency tree, it is converted to and evaluated on SD. Here we </context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Paul Cohen. 1995. Empirical Methods for Artificial Intelligence. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Gabbard</author>
<author>Mitchell Marcus</author>
<author>Seth Kulick</author>
</authors>
<title>Fully parsing the Penn treebank.</title>
<date>2006</date>
<booktitle>In Proceeding of HLT-NAACL,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="34676" citStr="Gabbard et al., 2006" startWordPosition="5587" endWordPosition="5590"> representation types, (ii) computing the theoretical common ground for each test sentence, and (iii) counting only what counts, that is, measuring the distance between the common ground and the parse tree while discarding annotation-specific edits. A pre-condition for applying our protocol is the availability of a relational interpretation of trees in the different frameworks. For dependency frameworks this is straightforward, as these relations are encoded on top of dependency arcs. For constituency trees with an inherent mapping of nodes onto grammatical relations (Merlo and Musillo, 2005; Gabbard et al., 2006; Tsarfaty and Sima’an, 2008), a procedure for reading relational schemes off of the trees is trivial to implement. For parsers that are trained on and parse into bare-bones phrase-structure trees this is not so. Reading off the relational structure may be more costly and require interjection of additional theoretical assumptions via manually written scripts. Scripts that read off grammatical relations based on tree positions work well for configurational languages such as English (de Marneffe et al., 2006) but since grammatical relations are reflected differently in different languages (Posta</context>
</contexts>
<marker>Gabbard, Marcus, Kulick, 2006</marker>
<rawString>Ryan Gabbard, Mitchell Marcus, and Seth Kulick. 2006. Fully parsing the Penn treebank. In Proceeding of HLT-NAACL, pages 184–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>SVMTool: A general POS tagger generator based on support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>Gim´enez, M`arquez, 2004</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2004. SVMTool: A general POS tagger generator based on support vector machines. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Parsing.</title>
<date>2009</date>
<journal>Number</journal>
<booktitle>in Synthesis Lectures on Human Language Technologies.</booktitle>
<volume>2</volume>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra K¨ubler, Ryan McDonald, and Joakim Nivre. 2009. Dependency Parsing. Number 2 in Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A dependency-based method for evaluating broad-coverage parsers.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI-95,</booktitle>
<pages>1420--1425</pages>
<contexts>
<context position="7443" citStr="Lin, 1995" startWordPosition="1087" endWordPosition="1088">mework most accurately recovers a useful representation that helps to unravel the human-perceived interpretation. Relational schemes, that is, schemes that encode the set of grammatical relations that constitute the predicate-argument structures of sentences, provide an interface to semantic interpretation. They are more intuitively understood than, say, phrase-structure trees, and thus they are also more useful for practical applications. For these reasons, relational schemes have been repeatedly singled out as an appropriate level of representation for the evaluation of statistical parsers (Lin, 1995; Carroll et al., 1998; Cer et al., 2010). The annotated data which statistical parsers are trained on encode these grammatical relationships in different ways. Dependency treebanks provide a ready-made representation of grammatical relations on top of arcs connecting the words in the sentence (K¨ubler et al., 2009). The Penn Treebank and phrase-structure annotated resources encode partial information about grammatical relations as dash-features decorating phrase structure nodes (Marcus et al., 1993). Treebanks like Tiger for German (Brants et al., 2002) and Talbanken for Swedish (Nivre and Me</context>
</contexts>
<marker>Lin, 1995</marker>
<rawString>Dekang Lin. 1995. A dependency-based method for evaluating broad-coverage parsers. In Proceedings of IJCAI-95, pages 1420–1425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Tim Buckwalter</author>
<author>Wigdan Mekki</author>
</authors>
<title>The Penn Arabic treebank: Building a large-scale annotated Arabic corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of NEMLAR International Conference on Arabic Language Resources and Tools.</booktitle>
<contexts>
<context position="1705" citStr="Maamouri et al., 2004" startWordPosition="237" endWordPosition="240">cture of natural language sentences. The argument structure encompasses grammatical relationships between elements such as subject, predicate, object, etc., which are useful for further (e.g., semantic) processing. The parses yielded by different parsing frameworks typically obey different formal and theoretical assumptions concerning how to represent the grammatical relationships in the data (Rambow, 2010). For example, grammatical relations may be encoded on top of dependency arcs in a dependency tree (Mel’ˇcuk, 1988), they may decorate nodes in a phrase-structure tree (Marcus et al., 1993; Maamouri et al., 2004; Sima’an et al., 2001), or they may be read off of positions in a phrase-structure tree using hard-coded conversion procedures (de Marneffe et al., 2006). This diversity poses a challenge to cross-experimental parser evaluation, namely: How can we evaluate the performance of these different parsers relative to one another? Current evaluation practices assume a set of correctly annotated test data (or gold standard) for evaluation. Typically, every parser is evaluated with respect to its own formal representation type and the underlying theory which it was trained to recover. Therefore, numeri</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, Mekki, 2004</marker>
<rawString>Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Wigdan Mekki. 2004. The Penn Arabic treebank: Building a large-scale annotated Arabic corpus. In Proceedings of NEMLAR International Conference on Arabic Language Resources and Tools.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1682" citStr="Marcus et al., 1993" startWordPosition="233" endWordPosition="236">ute the argument structure of natural language sentences. The argument structure encompasses grammatical relationships between elements such as subject, predicate, object, etc., which are useful for further (e.g., semantic) processing. The parses yielded by different parsing frameworks typically obey different formal and theoretical assumptions concerning how to represent the grammatical relationships in the data (Rambow, 2010). For example, grammatical relations may be encoded on top of dependency arcs in a dependency tree (Mel’ˇcuk, 1988), they may decorate nodes in a phrase-structure tree (Marcus et al., 1993; Maamouri et al., 2004; Sima’an et al., 2001), or they may be read off of positions in a phrase-structure tree using hard-coded conversion procedures (de Marneffe et al., 2006). This diversity poses a challenge to cross-experimental parser evaluation, namely: How can we evaluate the performance of these different parsers relative to one another? Current evaluation practices assume a set of correctly annotated test data (or gold standard) for evaluation. Typically, every parser is evaluated with respect to its own formal representation type and the underlying theory which it was trained to rec</context>
<context position="7948" citStr="Marcus et al., 1993" startWordPosition="1159" endWordPosition="1162">peatedly singled out as an appropriate level of representation for the evaluation of statistical parsers (Lin, 1995; Carroll et al., 1998; Cer et al., 2010). The annotated data which statistical parsers are trained on encode these grammatical relationships in different ways. Dependency treebanks provide a ready-made representation of grammatical relations on top of arcs connecting the words in the sentence (K¨ubler et al., 2009). The Penn Treebank and phrase-structure annotated resources encode partial information about grammatical relations as dash-features decorating phrase structure nodes (Marcus et al., 1993). Treebanks like Tiger for German (Brants et al., 2002) and Talbanken for Swedish (Nivre and Megyesi, 2007) explicitly map phrase structures onto grammatical relations using dedicated edge labels. The RelationalRealizational structures of Tsarfaty and Sima’an (2008) encode relational networks (sets of relations) projected and realized by syntactic categories on top of ordinary phrase-structure nodes. Function trees, as defined in Tsarfaty et al. (2011), are linearly ordered labeled trees in which every node is labeled with the grammatical func45 root Figure 1: Deterministic conversion into fun</context>
<context position="23589" citStr="Marcus et al., 1993" startWordPosition="3809" endWordPosition="3812">luation We use sections 02–21 of the WSJ Penn Treebank for training and section 00 for evaluation and analysis. We use two different native gold standards subscribing to different theories of encoding grammatical relations in tree structures: o THE DEPENDENCY-BASED THEORY is the theory encoded in the basic Stanford Dependencies (SD) scheme. We obtain the set of basic stanford dependency trees using the software of de Marneffe et al. (2006) and train the dependency parsers directly on it. o THE CONSTITUENCY-BASED THEORY is the theory reflected in the phrase-structure representation of the PTB (Marcus et al., 1993) enriched with function labels compatible with the Stanford Dependencies (SD) scheme. We obtain trees that reflect this theory by TL-Unification of the PTB multifunction trees with the SD multi-function trees (PTBUtlSD) as illustrated in Figure 4. The theory encoded in the multi-function trees corresponding to SD is different from the one obtained by our TL-Unification, as may be seen from the difference between the flat SD multifunction tree and the result of the PTBUtlSD in Figure 4. Another difference concerns coordination structures, encoded as binary branching trees in SD and as flat prod</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 523–530, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Megyesi</author>
</authors>
<title>The open source tagger HunPoS for Swedish.</title>
<date>2009</date>
<booktitle>In Proceedings of the 17th Nordic Conference of Computational Linguistics (NODALIDA),</booktitle>
<pages>239--241</pages>
<contexts>
<context position="31672" citStr="Megyesi, 2009" startWordPosition="5123" endWordPosition="5124">sh cross-framework evaluation: Three measures as applicable to the different schemes. Boldface scores are the highest in their column. Formalism PS Trees MF Trees Dep Trees Theory STB STB ut Dep Dep Metrics TEDEVAL TEDEVAL TEDEVAL SINGLE MULTIPLE SINGLE MALT N/A U: 0.9266 U: 0.9264 L: 0.8225 L: 0.8372 MST N/A U: 0.9275 U: 0.9272 L: 0.8121 L: 0.8275 BKLY-STB-RR U: 0.9239 U: 0.9281 N/A L: 0.7946 L: 0.7861 Table 4: Swedish cross-framework evaluation: TEDEVAL scores against the native gold and the generalized gold. Boldface scores are the highest in their column. parsers, using the HunPoS tagger (Megyesi, 2009), but let the Berkeley parser predict its own tags. We use the same evaluation metrics and procedures as before. Prior to evaluating RR trees using ParsEval we strip off the added function nodes. Prior to evaluating them using TedEval we strip off the phrase-structure nodes. Tables 3 and 4 summarize the parsing results for the different Swedish parsers. In the leftmost column of table 3 we present the constituencybased evaluation measures. Interestingly, the Berkeley RR instantiation performs better than when training the Berkeley parser on PS trees. These constituency-based scores however hav</context>
</contexts>
<marker>Megyesi, 2009</marker>
<rawString>Beata Megyesi. 2009. The open source tagger HunPoS for Swedish. In Proceedings of the 17th Nordic Conference of Computational Linguistics (NODALIDA), pages 239–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press.</publisher>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>Igor Mel’ˇcuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
</authors>
<title>Accurate function parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>620--627</pages>
<contexts>
<context position="34654" citStr="Merlo and Musillo, 2005" startWordPosition="5583" endWordPosition="5586"> ground for the different representation types, (ii) computing the theoretical common ground for each test sentence, and (iii) counting only what counts, that is, measuring the distance between the common ground and the parse tree while discarding annotation-specific edits. A pre-condition for applying our protocol is the availability of a relational interpretation of trees in the different frameworks. For dependency frameworks this is straightforward, as these relations are encoded on top of dependency arcs. For constituency trees with an inherent mapping of nodes onto grammatical relations (Merlo and Musillo, 2005; Gabbard et al., 2006; Tsarfaty and Sima’an, 2008), a procedure for reading relational schemes off of the trees is trivial to implement. For parsers that are trained on and parse into bare-bones phrase-structure trees this is not so. Reading off the relational structure may be more costly and require interjection of additional theoretical assumptions via manually written scripts. Scripts that read off grammatical relations based on tree positions work well for configurational languages such as English (de Marneffe et al., 2006) but since grammatical relations are reflected differently in diff</context>
</contexts>
<marker>Merlo, Musillo, 2005</marker>
<rawString>Paola Merlo and Gabriele Musillo. 2005. Accurate function parsing. In Proceedings of EMNLP, pages 620–627.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Beata Megyesi</author>
</authors>
<title>Bootstrapping a Swedish Treebank using cross-corpus harmonization and annotation projection.</title>
<date>2007</date>
<booktitle>In Proceedings of TLT.</booktitle>
<contexts>
<context position="8055" citStr="Nivre and Megyesi, 2007" startWordPosition="1176" endWordPosition="1179">s (Lin, 1995; Carroll et al., 1998; Cer et al., 2010). The annotated data which statistical parsers are trained on encode these grammatical relationships in different ways. Dependency treebanks provide a ready-made representation of grammatical relations on top of arcs connecting the words in the sentence (K¨ubler et al., 2009). The Penn Treebank and phrase-structure annotated resources encode partial information about grammatical relations as dash-features decorating phrase structure nodes (Marcus et al., 1993). Treebanks like Tiger for German (Brants et al., 2002) and Talbanken for Swedish (Nivre and Megyesi, 2007) explicitly map phrase structures onto grammatical relations using dedicated edge labels. The RelationalRealizational structures of Tsarfaty and Sima’an (2008) encode relational networks (sets of relations) projected and realized by syntactic categories on top of ordinary phrase-structure nodes. Function trees, as defined in Tsarfaty et al. (2011), are linearly ordered labeled trees in which every node is labeled with the grammatical func45 root Figure 1: Deterministic conversion into function trees. The algorithm for extracting a function tree from a dependency tree as in (a) is provided in T</context>
<context position="28155" citStr="Nivre and Megyesi, 2007" startWordPosition="4525" endWordPosition="4528">ed in the results we report in Table 2. Here the leftmost and rightmost columns report TEDEVAL scores against the own native gold (SINGLE) and the middle column against the generalized gold (MULTIPLE). Had the theories for SD and PTBUtlSD been identical, TEDEVAL SINGLE and TEDEVAL MULTIPLE would have been equal in each line. Because of theoretical discrepancies, we see small gaps in parser performance between these cases. Our protocol ensures that such discrepancies do not bias the results. 4.2 Cross-Framework Swedish Parsing We use the standard training and test sets of the Swedish Treebank (Nivre and Megyesi, 2007) with two gold standards presupposing different theories: • THE DEPENDENCY-BASED THEORY is the dependency version of the Swedish Treebank. All trees are projectivized (STB-Dep). • THE CONSTITUENCY-BASED THEORY is the standard Swedish Treebank with grammatical function labels on the edges of constituency structures (STB). � 0 VP V NP 0 Mary NN loves John �0 0 John 0 loves (PTB) S NP NN Mary Mary (PTB) Utl (SD) = {root} {sbj} John {hd} loves {obj} Mary 0 � root sbj John obj Mary {sbj} John {hd} loves {obj} Mary loves hd sbj obj (SD) -ROOT- John loves Mary � {root} 50 Formalism PS Trees MF Trees </context>
</contexts>
<marker>Nivre, Megyesi, 2007</marker>
<rawString>Joakim Nivre and Beata Megyesi. 2007. Bootstrapping a Swedish Treebank using cross-corpus harmonization and annotation projection. In Proceedings of TLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>2216--2219</pages>
<contexts>
<context position="22422" citStr="Nivre et al., 2006" startWordPosition="3629" endWordPosition="3632">he generalization and unification operation is an exact, greedy, chart-based algorithm that runs in polynomial time (O(n2) in n the number of terminals). The TED software that we utilize builds on the TED efficient algorithm of Zhang and Shasha (1989) which runs in O(|T1||T2 |min(d1, n1) min(d2, n2)) time where di is the tree degree (depth) and ni is the number of terminals in the respective tree (Bille, 2005). 4 Experiments We validate our cross-framework evaluation procedure on two languages, English and Swedish. For English, we compare the performance of two dependency parsers, MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and two constituency-based parsers, the Berkeley parser (Petrov et al., 2006) and the Brown parser (Charniak and Johnson, 2005). All experiments use Penn Treebank (PTB) data. For Swedish, we compare MaltParser and MSTParser with two variants of the Berkeley parser, one trained on phrase structure trees, and one trained on a variant of the Relational-Realizational representation of Tsarfaty and Sima’an (2008). All experiments use the Talbanken Swedish Treebank (STB) data. 4.1 English Cross-Framework Evaluation We use sections 02–21 of the WSJ Penn Treeban</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of LREC, pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Laura Rimell</author>
<author>Ryan McDonald</author>
<author>Carlos G´omez-Rodriguez</author>
</authors>
<title>Evaluation of dependency parsers on unbounded dependencies.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>813--821</pages>
<marker>Nivre, Rimell, McDonald, G´omez-Rodriguez, 2010</marker>
<rawString>Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos G´omez-Rodriguez. 2010. Evaluation of dependency parsers on unbounded dependencies. In Proceedings of COLING, pages 813–821.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="22539" citStr="Petrov et al., 2006" startWordPosition="3646" endWordPosition="3649">(O(n2) in n the number of terminals). The TED software that we utilize builds on the TED efficient algorithm of Zhang and Shasha (1989) which runs in O(|T1||T2 |min(d1, n1) min(d2, n2)) time where di is the tree degree (depth) and ni is the number of terminals in the respective tree (Bille, 2005). 4 Experiments We validate our cross-framework evaluation procedure on two languages, English and Swedish. For English, we compare the performance of two dependency parsers, MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and two constituency-based parsers, the Berkeley parser (Petrov et al., 2006) and the Brown parser (Charniak and Johnson, 2005). All experiments use Penn Treebank (PTB) data. For Swedish, we compare MaltParser and MSTParser with two variants of the Berkeley parser, one trained on phrase structure trees, and one trained on a variant of the Relational-Realizational representation of Tsarfaty and Sima’an (2008). All experiments use the Talbanken Swedish Treebank (STB) data. 4.1 English Cross-Framework Evaluation We use sections 02–21 of the WSJ Penn Treebank for training and section 00 for evaluation and analysis. We use two different native gold standards subscribing to </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul M Postal</author>
<author>David M Perlmutter</author>
</authors>
<title>Toward a universal characterization of passivization.</title>
<date>1977</date>
<booktitle>In Proceedings of the 3rd Annual Meeting of the Berkeley Linguistics Society,</booktitle>
<pages>394--417</pages>
<contexts>
<context position="35298" citStr="Postal and Perlmutter, 1977" startWordPosition="5680" endWordPosition="5683"> 2006; Tsarfaty and Sima’an, 2008), a procedure for reading relational schemes off of the trees is trivial to implement. For parsers that are trained on and parse into bare-bones phrase-structure trees this is not so. Reading off the relational structure may be more costly and require interjection of additional theoretical assumptions via manually written scripts. Scripts that read off grammatical relations based on tree positions work well for configurational languages such as English (de Marneffe et al., 2006) but since grammatical relations are reflected differently in different languages (Postal and Perlmutter, 1977; Bresnan, 2000), a procedure to read off these relations in a languageindependent fashion from phrase-structure trees does not, and should not, exist (Rambow, 2010). The crucial point is that even when using external scripts for recovering a relational scheme for phrase-structure trees, our protocol has a clear advantage over simply scoring converted trees. Manually created conversion scripts alter the theoretical assumptions inherent in the trees and thus may bias the results. Our generalization operation and three-way TED make sure that theory-specific idiosyncrasies injected through such s</context>
</contexts>
<marker>Postal, Perlmutter, 1977</marker>
<rawString>Paul M. Postal and David M. Perlmutter. 1977. Toward a universal characterization of passivization. In Proceedings of the 3rd Annual Meeting of the Berkeley Linguistics Society, pages 394–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
</authors>
<title>The simple truth about dependency and phrase structure representations: An opinion piece.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-ACL,</booktitle>
<pages>337--340</pages>
<contexts>
<context position="1494" citStr="Rambow, 2010" startWordPosition="204" endWordPosition="205">tituency and dependency parsers on English and Swedish. 1 Introduction The goal of statistical parsers is to recover a formal representation of the grammatical relations that constitute the argument structure of natural language sentences. The argument structure encompasses grammatical relationships between elements such as subject, predicate, object, etc., which are useful for further (e.g., semantic) processing. The parses yielded by different parsing frameworks typically obey different formal and theoretical assumptions concerning how to represent the grammatical relationships in the data (Rambow, 2010). For example, grammatical relations may be encoded on top of dependency arcs in a dependency tree (Mel’ˇcuk, 1988), they may decorate nodes in a phrase-structure tree (Marcus et al., 1993; Maamouri et al., 2004; Sima’an et al., 2001), or they may be read off of positions in a phrase-structure tree using hard-coded conversion procedures (de Marneffe et al., 2006). This diversity poses a challenge to cross-experimental parser evaluation, namely: How can we evaluate the performance of these different parsers relative to one another? Current evaluation practices assume a set of correctly annotate</context>
<context position="35463" citStr="Rambow, 2010" startWordPosition="5708" endWordPosition="5709">phrase-structure trees this is not so. Reading off the relational structure may be more costly and require interjection of additional theoretical assumptions via manually written scripts. Scripts that read off grammatical relations based on tree positions work well for configurational languages such as English (de Marneffe et al., 2006) but since grammatical relations are reflected differently in different languages (Postal and Perlmutter, 1977; Bresnan, 2000), a procedure to read off these relations in a languageindependent fashion from phrase-structure trees does not, and should not, exist (Rambow, 2010). The crucial point is that even when using external scripts for recovering a relational scheme for phrase-structure trees, our protocol has a clear advantage over simply scoring converted trees. Manually created conversion scripts alter the theoretical assumptions inherent in the trees and thus may bias the results. Our generalization operation and three-way TED make sure that theory-specific idiosyncrasies injected through such scripts do not lead to over-penalizing or over-crediting theory-specific structural variations. Certain linguistic structures cannot yet be evaluated with our protoco</context>
</contexts>
<marker>Rambow, 2010</marker>
<rawString>Owen Rambow. 2010. The simple truth about dependency and phrase structure representations: An opinion piece. In Proceedings of HLT-ACL, pages 337–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Schwartz</author>
<author>Omri Abend</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>663--672</pages>
<contexts>
<context position="27241" citStr="Schwartz et al., 2011" startWordPosition="4384" endWordPosition="4387">nversion of their output into dependencies. The conversion to SD allows one to compare results across formal frameworks, but not without a cost. The conversion introduces a set of annotation specific decisions which may introduce a bias into the evaluation. In the middle column of Table 1 we report the TEDEVAL metrics measured against the generalized gold standard for all parsing frameworks. We can now confirm that the constituency-based parsers significantly outperform the dependency parsers, and that this is not due to specific theoretical decisions which are seen to affect LAS/UAS metrics (Schwartz et al., 2011). For the dependency parsers we now see that Malt outperforms MST on labeled dependencies slightly, but the difference is insignificant. The fact that the discrepancy in theoretical assumptions between different frameworks indeed affects the conversion-based evaluation procedure is reflected in the results we report in Table 2. Here the leftmost and rightmost columns report TEDEVAL scores against the own native gold (SINGLE) and the middle column against the generalized gold (MULTIPLE). Had the theories for SD and PTBUtlSD been identical, TEDEVAL SINGLE and TEDEVAL MULTIPLE would have been equ</context>
</contexts>
<marker>Schwartz, Abend, Reichart, Rappoport, 2011</marker>
<rawString>Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rappoport. 2011. Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation. In Proceedings of ACL, pages 663–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
<author>Alon Itai</author>
<author>Yoad Winter</author>
<author>Alon Altman</author>
<author>Noa Nativ</author>
</authors>
<title>Building a Tree-Bank for Modern Hebrew Text. In Traitement Automatique des Langues.</title>
<date>2001</date>
<marker>Sima’an, Itai, Winter, Altman, Nativ, 2001</marker>
<rawString>Khalil Sima’an, Alon Itai, Yoad Winter, Alon Altman, and Noa Nativ. 2001. Building a Tree-Bank for Modern Hebrew Text. In Traitement Automatique des Langues.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Khalil Sima’an</author>
</authors>
<title>RelationalRealizational parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of CoLing.</booktitle>
<marker>Tsarfaty, Sima’an, 2008</marker>
<rawString>Reut Tsarfaty and Khalil Sima’an. 2008. RelationalRealizational parsing. In Proceedings of CoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Joakim Nivre</author>
<author>Evelina Andersson</author>
</authors>
<title>Evaluating dependency parsing: Robust and heuristics-free cross-framework evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3961" citStr="Tsarfaty et al. (2011)" startWordPosition="581" endWordPosition="584"> one framework may be incompatible with the other one. In the constituency-to-dependency case, some constituency-based structures (e.g., coordination 44 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 44–54, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics and ellipsis) do not comply with the single head assumption of dependency treebanks. Secondly, these scripts may be labor intensive to create, and are available mostly for English. So the evaluation protocol becomes language-dependent. In Tsarfaty et al. (2011) we proposed a general protocol for handling annotation discrepancies when comparing parses across different dependency theories. The protocol consists of three phases: converting all structures into function trees, for each sentence, generalizing the different gold standard function trees to get their common denominator, and employing an evaluation measure based on tree edit distance (TED) which discards edit operations that recover theory-specific structures. Although the protocol is potentially applicable to a wide class of syntactic representation types, formal restrictions in the procedur</context>
<context position="8404" citStr="Tsarfaty et al. (2011)" startWordPosition="1227" endWordPosition="1230">d phrase-structure annotated resources encode partial information about grammatical relations as dash-features decorating phrase structure nodes (Marcus et al., 1993). Treebanks like Tiger for German (Brants et al., 2002) and Talbanken for Swedish (Nivre and Megyesi, 2007) explicitly map phrase structures onto grammatical relations using dedicated edge labels. The RelationalRealizational structures of Tsarfaty and Sima’an (2008) encode relational networks (sets of relations) projected and realized by syntactic categories on top of ordinary phrase-structure nodes. Function trees, as defined in Tsarfaty et al. (2011), are linearly ordered labeled trees in which every node is labeled with the grammatical func45 root Figure 1: Deterministic conversion into function trees. The algorithm for extracting a function tree from a dependency tree as in (a) is provided in Tsarfaty et al. (2011). For a phrase-structure tree as in (b) we can replace each node label with its function (dash-feature). In a relational-realizational structure like (c) we can remove the projection nodes (sets) and realization nodes (phrase labels), which leaves the function nodes intact. tion of the dominated span. Function trees benefit fr</context>
<context position="10070" citStr="Tsarfaty et al. (2011)" startWordPosition="1491" endWordPosition="1495">to function trees. In Figure 1 we illustrate some such procedures for a simple transitive sentence. Now, while all the structures at the right hand side of Figure 1 are of the same formal type (function trees), they have different tree structures due to different theoretical assumptions underlying the original formal frameworks. (t2) root (t3) root f2 {f1,f2} f2 f1 w w Figure 2: Unary chains in function trees Once we have converted framework-specific representations into function trees, the problem of cross-framework evaluation can potentially be reduced to a cross-theory evaluation following Tsarfaty et al. (2011). The main idea is that once all structures have been converted into function trees, one can perform a formal operation called generalization in order to harmonize the differences between theories, and measure accurately the distance of parse hypotheses from the generalized gold. The generalization operation defined in Tsarfaty et al. (2011), however, cannot handle trees that may contain unary chains, and therefore cannot be used for arbitrary function trees. Consider for instance (t1) and (t2) in Figure 2. According to the definition of subsumption in Tsarfaty et al. (2011), (t1) is subsumed </context>
<context position="11350" citStr="Tsarfaty et al. (2011)" startWordPosition="1707" endWordPosition="1710">al – but they are not. The interpretation we wish to give to a function tree such as (t1) is that the word w has both the grammatical function f1 and the grammatical function f2. This can be graphically represented as a set of labels dominating w, as in (t3). We call structures such as (t3) multifunction trees. In the next section we formally define multi-function trees, and then use them to develop our protocol for cross-framework and crosstheory evaluation. 3 The Proposal: Cross-Framework Evaluation with Multi-Function Trees Our proposal is a three-phase evaluation protocol in the spirit of Tsarfaty et al. (2011). First, we obtain a formal common ground for all frameworks in terms of multi-function trees. Then we obtain a theoretical common ground by means of tree-generalization on gold trees. Finally, we calculate TED-based scores that discard the cost of annotation-specific edits. In this section, we define multi-function trees and update the treegeneralization and TED-based metrics to handle multi-function trees that reflect different theories. NN John NN Mary Mary Mary sbj obj (a) -ROOT- John loves Mary � root loves hd sbj John obj Mary (b) S-root prd VP-prd hd V-hd obj hd loves John loves NP-obj </context>
<context position="13781" citStr="Tsarfaty et al. (2011)" startWordPosition="2130" endWordPosition="2133">tion trees. Each node v in a multi-function tree represents a constraint of the form: for each l ∈ labels(v), there should be a node v0 in the function tree such that span(v) = span(v0) and label(v0) = l. Whenever we have a conversion for function trees, we can efficiently collapse them into multi-function trees with no unary productions, and with label sets labeling their nodes. Thus, trees (t1) and (t2) in Figure 2 would both be mapped to tree (t3), which encodes the functional constraints encoded in either of them. For dependency trees, we assume the conversion to function trees defined in Tsarfaty et al. (2011), where head daughters always get the label ‘hd’. For PTB style phrase-structure trees, we replace the phrase-structure labels with functional dash-features. In relational-realization structures we remove projection and realization nodes. Deterministic conversions exist also for Tiger style treebanks and frameworks such as LFG, but we do not discuss them here.1 1All the conversions we use are deterministic and are defined in graph-theoretic and language-independent terms. We make them available at http://stp.lingfil.uu. se/˜tsarfaty/unipar/index.html. 3.2 Generalizing Multi-Function Trees Once</context>
<context position="15046" citStr="Tsarfaty et al. (2011)" startWordPosition="2321" endWordPosition="2324"> different gold standard representations in the system, we feed them to a generalization operation as shown in Figure 3. The goal of this operation is to provide a consensus gold standard that captures the linguistic structure that the different gold theories agree on. The generalization structures are later used as the basis for the TED-based evaluation. Generalization is defined by means of subsumption. A multi-function tree subsumes another one if and only if all the constraints defined by the first tree are also defined by the second tree. So, instead of demanding equality of labels as in Tsarfaty et al. (2011), we demand set inclusion: T-Subsumption, denoted vt, is a relation between multi-function trees that indicates that a tree π1 is consistent with and more general than tree π2. Formally: π1 vt π2 iff for every node n ∈ π1 there exists a node m ∈ π2 such that span(n) = span(m) and labels(n) ⊆ labels(m). T-Unification, denoted tt, is an operation that returns the most general tree structure that contains the information from both input trees, and fails if such a tree does not exist. Formally: π1 tt π2 = π3 iff π1 vt π3 and π2 vt π3, and for all π4 such that π1 vt π4 and π2 vt π4 it holds that π3</context>
<context position="16870" citStr="Tsarfaty et al. (2011)" startWordPosition="2672" endWordPosition="2675">generalize two trees and one tree has no specification for labels over a span, it does not share anything with the label set dominating the same span in the other tree, and the label set dominating this span in the generalized tree is empty. If the trees do not agree on any label for a particular span, the respective node is similarly labeled with an empty set. When we wish to unify theories, then an empty set over a span is unified with any other set dominating the same span in the other tree, without altering it. Digression: Using Unification to Merge Information From Different Treebanks In Tsarfaty et al. (2011), only the generalization operation was used, providing the common denominator of all the gold structures and serving as a common ground for evaluation. The unification operation is useful for other NLP tasks, for instance, combining information from two different annotation schemes or enriching one annotation scheme with information from a different one. In particular, we can take advantage of the new framework to enrich the node structure reflected in one theory with grammatical functions reflected in an annotation scheme that follows a different theory. To do so, we define the Tree-Labeling</context>
<context position="18479" citStr="Tsarfaty et al. (2011)" startWordPosition="2944" endWordPosition="2947"> labels(7r2, span(n)) is the set of labels of the node with yield span(n) in 7r2 if such a node exists and 0 otherwise. We further discuss the TLUnification and its use for data preparation in §4. 3.3 TED Measures for Multi-Function Trees The result of the generalization operation provides us with multi-function trees for each of the sentences in the test set representing sets of constraints on which the different gold theories agree. We would now like to use distance-based metrics in order to measure the gap between the gold and predicted theories. The idea behind distancebased evaluation in Tsarfaty et al. (2011) is that recording the edit operations between the native gold and the generalized gold allows one to discard their cost when computing the cost of a parse hypothesis turned into the generalized gold. This makes sure that different parsers do not get penalized, or favored, due to annotation specific decisions that are not shared by other frameworks. The problem is now that TED is undefined with respect to multi-function trees because it cannot handle complex labels. To overcome this, we convert multi-function trees into sorted function trees, which are simply function trees in which any label </context>
<context position="24441" citStr="Tsarfaty et al. (2011)" startWordPosition="3945" endWordPosition="3948">ated in Figure 4. The theory encoded in the multi-function trees corresponding to SD is different from the one obtained by our TL-Unification, as may be seen from the difference between the flat SD multifunction tree and the result of the PTBUtlSD in Figure 4. Another difference concerns coordination structures, encoded as binary branching trees in SD and as flat productions in the PTBUtlSD. Such differences are not only observable but also quantifiable, and using our redefined TED metric the cross-theory overlap is 0.8571. The two dependency parsers were trained using the same settings as in Tsarfaty et al. (2011), using SVMTool (Gim´enez and M`arquez, 2004) to predict part-of-speech tags at parsing time. The two constituency parsers were used with default settings and were allowed to predict their own partof-speech tags. We report three different evaluation metrics for the different experiments: �|testset |6(p j=1 49 root Figure 4: Conversion of PTB and SD tree to multifunction trees, followed by TL-Unification of the trees. Note that some PTB nodes remain without an SD label. o LAS/UAS (Buchholz and Marsi, 2006) o PARSEVAL (Black et al., 1991) o TEDEVAL as defined in Section 3 We use LAS/UAS for depe</context>
<context position="33992" citStr="Tsarfaty et al. (2011)" startWordPosition="5483" endWordPosition="5486">e for Malt on labeled dependencies, and an advantage for MST on unlabeled dependencies, but the latter is insignificant. This effect is replicated in Table 4 where we evaluate dependency parsers using TEDEVAL against their own gold theories. Table 4 further confirms that there is a gap between the STB and the STB-Dep theories, reflected in the scores against the native and generalized gold. 5 Discussion We presented a formal protocol for evaluating parsers across frameworks and used it to soundly compare parsing results for English and Swedish. Our approach follows the three-phase protocol of Tsarfaty et al. (2011), namely: (i) obtaining a formal common ground for the different representation types, (ii) computing the theoretical common ground for each test sentence, and (iii) counting only what counts, that is, measuring the distance between the common ground and the parse tree while discarding annotation-specific edits. A pre-condition for applying our protocol is the availability of a relational interpretation of trees in the different frameworks. For dependency frameworks this is straightforward, as these relations are encoded on top of dependency arcs. For constituency trees with an inherent mappin</context>
</contexts>
<marker>Tsarfaty, Nivre, Andersson, 2011</marker>
<rawString>Reut Tsarfaty, Joakim Nivre, and Evelina Andersson. 2011. Evaluating dependency parsing: Robust and heuristics-free cross-framework evaluation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaizhong Zhang</author>
<author>Dennis Shasha</author>
</authors>
<title>Simple fast algorithms for the editing distance between trees and related problems.</title>
<date>1989</date>
<journal>In SIAM Journal of Computing,</journal>
<volume>18</volume>
<pages>1245--1262</pages>
<marker>Zhang, Shasha, 1989</marker>
<rawString>Kaizhong Zhang and Dennis Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. In SIAM Journal of Computing, volume 18, pages 1245–1262.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>