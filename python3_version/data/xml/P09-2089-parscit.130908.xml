<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001158">
<title confidence="0.929492">
Updating a Name Tagger Using Contemporary Unlabeled Data
</title>
<author confidence="0.548792">
Cristina Mota
</author>
<note confidence="0.847669666666667">
L2F (INESC-ID) &amp; IST &amp; NYU
Rua Alves Redol 9
1000-029 Lisboa Portugal
</note>
<email confidence="0.980766">
cmota@ist.utl.pt
</email>
<author confidence="0.59504">
Ralph Grishman
</author>
<affiliation confidence="0.524146">
New York University
Computer Science Department
</affiliation>
<address confidence="0.704827">
New York NY 10003 USA
</address>
<email confidence="0.998862">
grishman@cs.nyu.edu
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999884941176471">
For many NLP tasks, including named en-
tity tagging, semi-supervised learning has
been proposed as a reasonable alternative
to methods that require annotating large
amounts of training data. In this paper,
we address the problem of analyzing new
data given a semi-supervised NE tagger
trained on data from an earlier time pe-
riod. We will show that updating the unla-
beled data is sufficient to maintain quality
over time, and outperforms updating the
labeled data. Furthermore, we will also
show that augmenting the unlabeled data
with older data in most cases does not re-
sult in better performance than simply us-
ing a smaller amount of current unlabeled
data.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949652173913">
Brill (2003) observed large gains in performance
for different NLP tasks solely by increasing the
size of unlabeled data, but stressed that for other
NLP tasks, such as named entity recognition
(NER), we still need to focus on developing tools
that help to increase the size of annotated data.
This problem is particularly crucial when pro-
cessing languages, such as Portuguese, for which
the labeled data is scarce. For instance, in the first
NER evaluation for Portuguese, HAREM (San-
tos and Cardoso, 2007), only two out of the nine
participants presented systems based on machine
learning, and they both argued they could have
achieved significantly better results if they had
larger training sets.
Semi-supervised methods are commonly cho-
sen as an alternative to overcome the lack of an-
notated resources, because they present a good
trade-off between amount of labeled data needed
and performance achieved. Co-training is one of
those methods, and has been extensively studied in
NLP (Nigam and Ghani, 2000; Pierce and Cardie,
2001; Ng and Cardie, 2003; Mota and Grishman,
2008). In particular, we showed that the perfor-
mance of a name tagger based on co-training de-
cays as the time gap between training data (seeds
and unlabeled data) and test data increases (Mota
and Grishman, 2008). Compared to the original
classifier of Collins and Singer (1999) that uses
seven seeds, we used substantially larger seed sets
(more than 1000), which raises the question of
which of the parameters (seeds or unlabeled data)
are causing the performance deterioration.
In the present study, we investigated two main
questions, from the point of view of a developer
who wants to analyze a new data set, given an NE
tagger trained with older data. First, we studied
whether it was better to update the seeds or the
unlabeled data; then, we analyzed whether using
a smaller amount of current unlabeled data could
be better than increasing the amount of unlabeled
data drawn from older sources. The experiments
show that using contemporary unlabeled data is
the best choice, outperforming most experiments
with larger amounts of older unlabeled data and
all experiments with contemporary seeds.
</bodyText>
<sectionHeader confidence="0.782174" genericHeader="method">
2 Contemporary labeled data in NLP
</sectionHeader>
<bodyText confidence="0.999961769230769">
The speech community has been defending for
some time now the idea of having similar tem-
poral data for training and testing automatic
speech recognition systems for broadcast news.
Most works focus on improving out-of-vocabulary
(OOV) rates, to which new names contribute
significantly. For instance, Palmer and Osten-
dorf (2005) aiming at reducing the error rate due
to OOV names propose to generate offline name
lists from diverse sources, including temporally
relevant news texts; Federico and Bertoldi (2004),
and Martins et al. (2006) propose to daily adapt
the statistical language model of a broadcast
</bodyText>
<page confidence="0.997629">
353
</page>
<note confidence="0.9261115">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 353–356,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999888789473684">
news transcription system, exploiting contempo-
rary newswire texts available on the web; Auzanne
et al. (2000) proposed a time-adaptive language
model, studying its impact over a period of five
months on the reduction of OOV rate, word error
rate and retrieval accuracy on a spoken document
retrieval system.
Concerning variations over longer periods of
time, we observed that the performance of a semi-
supervised name tagger decays over a period of
eight years, which seems to be directly related
with the fact that the texts used to train and test the
tagger also show a tendency to become less simi-
lar over time (Mota and Grishman, 2008); Batista
et al. (2008) also observed a decaying tendency in
the performance of a system for recovering capi-
talization over a period of six years, proposing to
retrain a MaxEnt model using additional contem-
porary written texts.
</bodyText>
<sectionHeader confidence="0.979353" genericHeader="method">
3 Name tagger overview
</sectionHeader>
<bodyText confidence="0.979220166666667">
We assessed the name tagger described in Mota
and Grishman (2008) to recognize names of peo-
ple, organizations and locations. The tagger is
based on the co-training NE classifier proposed
by Collins and Singer (1999), and is comprised
of several components organized sequentially (cf.
</bodyText>
<figureCaption confidence="0.8603995">
Figure 1).
Figure 1: NE tagger architecture
</figureCaption>
<sectionHeader confidence="0.983367" genericHeader="method">
4 Data sets
</sectionHeader>
<bodyText confidence="0.999963333333333">
CETEMP´ublico (Rocha and Santos, 2000) is a
Portuguese journalistic corpus with 180 million
words that spans eight years of news, from 1991
to 1998. The minimum size of epoch (time span
of data set) available for analysis is a six-month
period, corresponding either to the first half of the
year or the second.
The data sets were created using the first 8256
extracts1 within each six-month period of the pol-
itics section of the corpus: the first 192 are used to
collect seeds, the next 208 extracts are used as test
sets and the remaining 7856 are used to collect the
unlabeled examples. The seeds correspond to the
first 1150 names occurring in those extracts. From
the list of unlabeled examples obtained after the
NE identification stage, only the first 41226 exam-
ples of each epoch were used to bootstrap in the
classification stage.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999816818181818">
We denote by S, U and T, respectively, the seed,
unlabeled and test texts, and by (Si, Uj, Tk) a
training-test configuration, where 91a &lt; i, j, k &lt;
98b, i.e., epochs i, j and k vary between the first
half of 1991 (91a) and the second half of 1998
(98b). For instance, the training-test configuration
(Si=91a...98b, Ui=91a...98b, Tj=98b) represents the
training-test configuration where the test set was
drawn from epoch 98b, and the tagger was trained
in turn with seeds and unlabeled data drawn from
the same epoch i that varied from 91a to 98b.
</bodyText>
<subsectionHeader confidence="0.974785">
5.1 Do we need contemporary labeled data?
</subsectionHeader>
<bodyText confidence="0.999794105263158">
In order to understand whether it is better to label
examples falling within the epoch of the test set
or to keep using old labeled data while bootstrap-
ping with contemporary unlabeled data, we fixed
the test set to be within the last epoch of the inter-
val (98b), and performed backward experiments,
i.e., we varied the epoch of either the seeds or the
unlabeled data backwards. The choice of fixing
the test within the last epoch of the interval is the
one that most approximates a real situation where
one has a tagger trained with old data and wants to
process a more recent text.
Figure 2 shows the results for both experiments,
where (Sj=98b, Ui=91a...98b, Tj=98b) represents the
experiment where the test was within the same
epoch as the seeds and the unlabeled data were
drawn from a single, variable, epoch in turn, and
(Si=91a...98b, Uj=98b, Tj=98b) represents the exper-
iment where the test was within the epoch of the
</bodyText>
<figure confidence="0.847428157894737">
1Extracts are typically two paragraphs.
Seeds
Training
Spelling+
c0ntextual rules
C0-training
Pairs (spelling features,
c0ntextual features)
Pairs (NE,c0ntext)
Feature extracti0n
Identificati0n
Text with classified NE
Classificati0n
Labeled Pairs
Pr0pagati0n
Text with unclassified NE
Testing
Unlabeled text
Testtext
</figure>
<page confidence="0.97982">
354
</page>
<bodyText confidence="0.9696405">
rary unlabeled data contributes to its correct clas-
sification in the test set.
</bodyText>
<subsectionHeader confidence="0.556813">
5.2 Is more older unlabeled data better?
</subsectionHeader>
<bodyText confidence="0.8058775">
unlabeled data and the seeds were drawn in turn
from each of the epochs; the graphic also shows
the baseline backward training (varying the epoch
of both the seeds and the unlabeled data together).
</bodyText>
<subsectionHeader confidence="0.430769">
Training epoch
</subsectionHeader>
<figureCaption confidence="0.9541475">
Figure 2: F-measure over time for test set 98b with
configurations: (Si=91a...98b, Ui=91a...98b, Tj=98b),
(Sj=98b, Ui=91a...98b, Tj=98b), and (Si=91a...98b,
Uj=98b, Tj=98b)
</figureCaption>
<bodyText confidence="0.972777104166667">
As can be seen, there is a small gain in perfor-
mance by using seeds within the epoch of the test
set, but the decay is still observable as we increase
the time gap between the unlabeled data and the
test set. On the contrary, if we use unlabeled data
within the epoch of the test set, we hardly see
a degradation trend as the time gap between the
epochs of seeds and test set is increased.
An examination of the results shows that, for
instance, Sendero Luminoso received the correct
classification of organization when the tagger is
trained with unlabeled data drawn from the same
epoch, but is incorretly classified as person when
trained with data that is not contemporary with the
test set. Even though that name is not a seed in any
of the cases, it occurs twice in good contexts for
organization in unlabeled data contemporary with
the test set (lider do Sendero Luminoso/leader of
the Shining Path and acc¸˜oes do Sendero Lumi-
noso/actions of the Shining Path), while it does
not occur in the unlabeled data that is not contem-
porary. Given that both the name spelling and the
context in the test set, o messianismo do peruano
Sendero Luminoso/the messianism of the Peruvian
Shining Path, are insufficient to assign a correct la-
bel, the occurrence of the name in the contempo-
The second question we addressed was whether
having more older unlabeled data could result in
better performance than less data but within the
epoch of the test set. In this case, we conducted
two backward experiments, augmenting the un-
labeled data backwards with older data than the
test set (98b), starting in the previous epoch (98a):
in the first experiment, the seeds were within the
same epoch as the test set, and in the second ex-
periment the seeds were within the same epoch as
the unlabeled set being added. This corresponds to
configurations (Sj=98b, U0i=91a...98a, Tj=98b) and
(Si=91a...98a, U0i=91a...98a, Tj=98b), respectively,
where U0i = U98a
k=i Uk.
In Figure 3, we show the result of these con-
figurations together with the result of the back-
ward experiment corresponding to configuration
(Si=91a...98b, Uj=98b, Tj=98b), also represented in
Figure 2. We note that, in the case of the former
experiments, the size of the unlabeled examples is
increasing in the direction 98a to 91a.
</bodyText>
<subsectionHeader confidence="0.802185">
Training epoch
</subsectionHeader>
<figureCaption confidence="0.921048666666667">
Figure 3: F-measure for test set 98b with
configurations (Si=91a...98b, Uj=98b, Tj=98b),
(Sj=98b, U0i=91a...98a, Tj=98b0 and (Si=91a ... 98a,
</figureCaption>
<bodyText confidence="0.880725142857143">
U0i=91a ... 98a, Tj=98b), where Ui = �98a
k=i Uk
As can be observed, increasing the size of the
unlabeled data does not necessarily result in bet-
ter performance: for both choices of seeds, perfor-
mance sometimes improves, sometimes worsens,
as the unlabeled data grows (following the curves
</bodyText>
<figure confidence="0.999244738095238">
91a
91b
92a
92b
93a
93b
94a
94b
95a
95b
96a
96b
97a
97b
98a
98b
F−measure
0.74 0.76 0.78 0.80 0.82
(i,i,98b)
(98b,i,98b)
(i,98b,98b)
91a
91b
92a
92b
93a
93b
94a
94b
95a
95b
96a
96b
97a
97b
98a
98b
F−measure
0.77 0.78 0.79 0.80 0.81 0.82 0.83
(i,98b,98b)
(i,u[i,...,98a],98b)
(98b,u[i,...,98a],98b)
</figure>
<page confidence="0.988526">
355
</page>
<bodyText confidence="0.9691978">
from right to left).
Furthermore, the tagger trained with more unla-
beled data in most cases did not outperform the
tagger trained with less unlabeled data selected
from the epoch of the test set.
</bodyText>
<sectionHeader confidence="0.99611" genericHeader="conclusions">
6 Discussion and future directions
</sectionHeader>
<bodyText confidence="0.99998265">
We conducted experiments varying the epoch of
seeds and unlabeled data of a named entity tagger
based on co-training. We observed that the per-
formance decay resulting from increasing the time
gap between training data (seeds and unlabeled ex-
amples) and the test set can be slightly attenuated
by using the seeds contemporary with the test set.
The gain is larger if one uses older seeds and con-
temporary unlabeled data, a strategy that, in most
of the experiments, results in better performance
than using increasing sizes of older unlabeled data.
These results suggest that we may not need to
label new data nor train our tagger with increasing
sizes of data, as long as we are able to train it with
unlabeled data time compatible with the test set.
In the future, one issue that needs clarification is
why bootstraping from contemporary labeled data
had so little influence on the performance of co-
training, and if other semi-supervised approches
are also sensitive to this question.
</bodyText>
<sectionHeader confidence="0.952408" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999753333333333">
The first author’s research work was funded by
Fundac¸˜ao para a Ciˆencia e a Tecnologia through a
doctoral scholarship (ref.: SFRH/BD/3237/2000).
</bodyText>
<sectionHeader confidence="0.999147" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999387118644068">
C´edric Auzanne, John S. Garofolo, Jonathan G. Fiscus,
and William M. Fisher. 2000. Automatic language
model adaptation for spoken document retrieval. In
Proceedings of RIAO 2000 Conference on Content-
Based Multimedia Information Access.
Fernando Batista, Nuno Mamede, and Isabel Trancoso.
2008. Language dynamics and capitalization using
maximum entropy. In Proceedings ofACL-08: HLT,
Short Papers, pages 1–4, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Eric Brill. 2003. Processing natural language with-
out natural language processing. In CICLing, pages
360–369.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
Proceedings of the Joint SIGDAT Conference on
EMNLP.
Marcello Federico and Nicola Bertoldi. 2004. Broad-
cast news lm adaptation over time. Computer
Speech &amp; Language, 18(4):417–435.
Ciro Martins, Ant´onio Teixeira, and Jo˜ao Neto. 2006.
Dynamic vocabulary adaptation for a daily and
real-time broadcast news transcription system. In
IEEE/ACL Workshop on Spoken Language Technol-
ogy, Aruba.
Cristina Mota and Ralph Grishman. 2008. Is this NE
tagger getting old? In Proceedings of the Sixth
International Language Resources and Evaluation
(LREC’08), Marrakech, Morocco, may.
Vincente Ng and Claire Cardie. 2003. Weakly super-
vised natural language learning without redundant
views. In NAACL’03: Proceedings ofthe 2003 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology, pages 94–101, Morristown,
NJ, USA. ACL.
Kamal Nigam and Rayid Ghani. 2000. Analyzing
the effectiveness and applicability of co-training. In
Proceedings of CIKM, pages 86–93.
David D. Palmer and Mari Ostendorf. 2005. Improv-
ing out-of-vocabulary name resolution. Computer
Speech &amp; Language, 19(1):107–128.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2001).
Paulo Rocha and Diana Santos. 2000. Cetemp´ublico:
Um corpus de grandes dimens˜oes de linguagem
jornalistica portuguesa. In Maria das Grac¸as
Volpe Nunes, editor, Actas do V Encontro para o
processamento computacional da lingua portuguesa
escrita e falada PROPOR 2000, pages 131–140, At-
ibaia, S˜ao Paulo, Brasil.
Diana Santos and Nuno Cardoso, editors. 2007. Re-
conhecimento de entidades mencionadas em por-
tuguˆes: Documentac¸˜ao e actas do HAREM, a
primeira avaliac¸˜ao conjunta na ´area. Linguateca,
12 de Novembro.
</reference>
<page confidence="0.998934">
356
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.633750">
<title confidence="0.999969">Updating a Name Tagger Using Contemporary Unlabeled Data</title>
<author confidence="0.999049">Cristina Mota</author>
<affiliation confidence="0.850923">L2F (INESC-ID) &amp; IST &amp; NYU</affiliation>
<address confidence="0.9278065">Rua Alves Redol 9 1000-029 Lisboa Portugal</address>
<email confidence="0.988959">cmota@ist.utl.pt</email>
<author confidence="0.999849">Ralph Grishman</author>
<affiliation confidence="0.994954">New York University Computer Science Department</affiliation>
<address confidence="0.883751">New York NY 10003 USA</address>
<email confidence="0.999817">grishman@cs.nyu.edu</email>
<abstract confidence="0.996195611111111">For many NLP tasks, including named entity tagging, semi-supervised learning has been proposed as a reasonable alternative to methods that require annotating large amounts of training data. In this paper, we address the problem of analyzing new data given a semi-supervised NE tagger trained on data from an earlier time period. We will show that updating the unlabeled data is sufficient to maintain quality over time, and outperforms updating the labeled data. Furthermore, we will also show that augmenting the unlabeled data with older data in most cases does not result in better performance than simply using a smaller amount of current unlabeled data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C´edric Auzanne</author>
<author>John S Garofolo</author>
<author>Jonathan G Fiscus</author>
<author>William M Fisher</author>
</authors>
<title>Automatic language model adaptation for spoken document retrieval.</title>
<date>2000</date>
<booktitle>In Proceedings of RIAO 2000 Conference on ContentBased Multimedia Information Access.</booktitle>
<contexts>
<context position="4010" citStr="Auzanne et al. (2000)" startWordPosition="635" endWordPosition="638">OOV) rates, to which new names contribute significantly. For instance, Palmer and Ostendorf (2005) aiming at reducing the error rate due to OOV names propose to generate offline name lists from diverse sources, including temporally relevant news texts; Federico and Bertoldi (2004), and Martins et al. (2006) propose to daily adapt the statistical language model of a broadcast 353 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 353–356, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP news transcription system, exploiting contemporary newswire texts available on the web; Auzanne et al. (2000) proposed a time-adaptive language model, studying its impact over a period of five months on the reduction of OOV rate, word error rate and retrieval accuracy on a spoken document retrieval system. Concerning variations over longer periods of time, we observed that the performance of a semisupervised name tagger decays over a period of eight years, which seems to be directly related with the fact that the texts used to train and test the tagger also show a tendency to become less similar over time (Mota and Grishman, 2008); Batista et al. (2008) also observed a decaying tendency in the perfor</context>
</contexts>
<marker>Auzanne, Garofolo, Fiscus, Fisher, 2000</marker>
<rawString>C´edric Auzanne, John S. Garofolo, Jonathan G. Fiscus, and William M. Fisher. 2000. Automatic language model adaptation for spoken document retrieval. In Proceedings of RIAO 2000 Conference on ContentBased Multimedia Information Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Batista</author>
<author>Nuno Mamede</author>
<author>Isabel Trancoso</author>
</authors>
<title>Language dynamics and capitalization using maximum entropy.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT, Short Papers,</booktitle>
<pages>1--4</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4562" citStr="Batista et al. (2008)" startWordPosition="730" endWordPosition="733">ntemporary newswire texts available on the web; Auzanne et al. (2000) proposed a time-adaptive language model, studying its impact over a period of five months on the reduction of OOV rate, word error rate and retrieval accuracy on a spoken document retrieval system. Concerning variations over longer periods of time, we observed that the performance of a semisupervised name tagger decays over a period of eight years, which seems to be directly related with the fact that the texts used to train and test the tagger also show a tendency to become less similar over time (Mota and Grishman, 2008); Batista et al. (2008) also observed a decaying tendency in the performance of a system for recovering capitalization over a period of six years, proposing to retrain a MaxEnt model using additional contemporary written texts. 3 Name tagger overview We assessed the name tagger described in Mota and Grishman (2008) to recognize names of people, organizations and locations. The tagger is based on the co-training NE classifier proposed by Collins and Singer (1999), and is comprised of several components organized sequentially (cf. Figure 1). Figure 1: NE tagger architecture 4 Data sets CETEMP´ublico (Rocha and Santos,</context>
</contexts>
<marker>Batista, Mamede, Trancoso, 2008</marker>
<rawString>Fernando Batista, Nuno Mamede, and Isabel Trancoso. 2008. Language dynamics and capitalization using maximum entropy. In Proceedings ofACL-08: HLT, Short Papers, pages 1–4, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Processing natural language without natural language processing.</title>
<date>2003</date>
<booktitle>In CICLing,</booktitle>
<pages>360--369</pages>
<contexts>
<context position="958" citStr="Brill (2003)" startWordPosition="150" endWordPosition="151">ng has been proposed as a reasonable alternative to methods that require annotating large amounts of training data. In this paper, we address the problem of analyzing new data given a semi-supervised NE tagger trained on data from an earlier time period. We will show that updating the unlabeled data is sufficient to maintain quality over time, and outperforms updating the labeled data. Furthermore, we will also show that augmenting the unlabeled data with older data in most cases does not result in better performance than simply using a smaller amount of current unlabeled data. 1 Introduction Brill (2003) observed large gains in performance for different NLP tasks solely by increasing the size of unlabeled data, but stressed that for other NLP tasks, such as named entity recognition (NER), we still need to focus on developing tools that help to increase the size of annotated data. This problem is particularly crucial when processing languages, such as Portuguese, for which the labeled data is scarce. For instance, in the first NER evaluation for Portuguese, HAREM (Santos and Cardoso, 2007), only two out of the nine participants presented systems based on machine learning, and they both argued </context>
</contexts>
<marker>Brill, 2003</marker>
<rawString>Eric Brill. 2003. Processing natural language without natural language processing. In CICLing, pages 360–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on EMNLP.</booktitle>
<contexts>
<context position="2299" citStr="Collins and Singer (1999)" startWordPosition="366" endWordPosition="369">commonly chosen as an alternative to overcome the lack of annotated resources, because they present a good trade-off between amount of labeled data needed and performance achieved. Co-training is one of those methods, and has been extensively studied in NLP (Nigam and Ghani, 2000; Pierce and Cardie, 2001; Ng and Cardie, 2003; Mota and Grishman, 2008). In particular, we showed that the performance of a name tagger based on co-training decays as the time gap between training data (seeds and unlabeled data) and test data increases (Mota and Grishman, 2008). Compared to the original classifier of Collins and Singer (1999) that uses seven seeds, we used substantially larger seed sets (more than 1000), which raises the question of which of the parameters (seeds or unlabeled data) are causing the performance deterioration. In the present study, we investigated two main questions, from the point of view of a developer who wants to analyze a new data set, given an NE tagger trained with older data. First, we studied whether it was better to update the seeds or the unlabeled data; then, we analyzed whether using a smaller amount of current unlabeled data could be better than increasing the amount of unlabeled data d</context>
<context position="5005" citStr="Collins and Singer (1999)" startWordPosition="802" endWordPosition="805">directly related with the fact that the texts used to train and test the tagger also show a tendency to become less similar over time (Mota and Grishman, 2008); Batista et al. (2008) also observed a decaying tendency in the performance of a system for recovering capitalization over a period of six years, proposing to retrain a MaxEnt model using additional contemporary written texts. 3 Name tagger overview We assessed the name tagger described in Mota and Grishman (2008) to recognize names of people, organizations and locations. The tagger is based on the co-training NE classifier proposed by Collins and Singer (1999), and is comprised of several components organized sequentially (cf. Figure 1). Figure 1: NE tagger architecture 4 Data sets CETEMP´ublico (Rocha and Santos, 2000) is a Portuguese journalistic corpus with 180 million words that spans eight years of news, from 1991 to 1998. The minimum size of epoch (time span of data set) available for analysis is a six-month period, corresponding either to the first half of the year or the second. The data sets were created using the first 8256 extracts1 within each six-month period of the politics section of the corpus: the first 192 are used to collect seed</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Joint SIGDAT Conference on EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
</authors>
<title>Broadcast news lm adaptation over time.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="3670" citStr="Federico and Bertoldi (2004)" startWordPosition="583" endWordPosition="586">rger amounts of older unlabeled data and all experiments with contemporary seeds. 2 Contemporary labeled data in NLP The speech community has been defending for some time now the idea of having similar temporal data for training and testing automatic speech recognition systems for broadcast news. Most works focus on improving out-of-vocabulary (OOV) rates, to which new names contribute significantly. For instance, Palmer and Ostendorf (2005) aiming at reducing the error rate due to OOV names propose to generate offline name lists from diverse sources, including temporally relevant news texts; Federico and Bertoldi (2004), and Martins et al. (2006) propose to daily adapt the statistical language model of a broadcast 353 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 353–356, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP news transcription system, exploiting contemporary newswire texts available on the web; Auzanne et al. (2000) proposed a time-adaptive language model, studying its impact over a period of five months on the reduction of OOV rate, word error rate and retrieval accuracy on a spoken document retrieval system. Concerning variations over longer periods of time, we observe</context>
</contexts>
<marker>Federico, Bertoldi, 2004</marker>
<rawString>Marcello Federico and Nicola Bertoldi. 2004. Broadcast news lm adaptation over time. Computer Speech &amp; Language, 18(4):417–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciro Martins</author>
<author>Ant´onio Teixeira</author>
<author>Jo˜ao Neto</author>
</authors>
<title>Dynamic vocabulary adaptation for a daily and real-time broadcast news transcription system.</title>
<date>2006</date>
<booktitle>In IEEE/ACL Workshop on Spoken Language Technology,</booktitle>
<location>Aruba.</location>
<contexts>
<context position="3697" citStr="Martins et al. (2006)" startWordPosition="588" endWordPosition="591">ta and all experiments with contemporary seeds. 2 Contemporary labeled data in NLP The speech community has been defending for some time now the idea of having similar temporal data for training and testing automatic speech recognition systems for broadcast news. Most works focus on improving out-of-vocabulary (OOV) rates, to which new names contribute significantly. For instance, Palmer and Ostendorf (2005) aiming at reducing the error rate due to OOV names propose to generate offline name lists from diverse sources, including temporally relevant news texts; Federico and Bertoldi (2004), and Martins et al. (2006) propose to daily adapt the statistical language model of a broadcast 353 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 353–356, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP news transcription system, exploiting contemporary newswire texts available on the web; Auzanne et al. (2000) proposed a time-adaptive language model, studying its impact over a period of five months on the reduction of OOV rate, word error rate and retrieval accuracy on a spoken document retrieval system. Concerning variations over longer periods of time, we observed that the performance of a</context>
</contexts>
<marker>Martins, Teixeira, Neto, 2006</marker>
<rawString>Ciro Martins, Ant´onio Teixeira, and Jo˜ao Neto. 2006. Dynamic vocabulary adaptation for a daily and real-time broadcast news transcription system. In IEEE/ACL Workshop on Spoken Language Technology, Aruba.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Mota</author>
<author>Ralph Grishman</author>
</authors>
<title>Is this NE tagger getting old?</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="2026" citStr="Mota and Grishman, 2008" startWordPosition="320" endWordPosition="323">n for Portuguese, HAREM (Santos and Cardoso, 2007), only two out of the nine participants presented systems based on machine learning, and they both argued they could have achieved significantly better results if they had larger training sets. Semi-supervised methods are commonly chosen as an alternative to overcome the lack of annotated resources, because they present a good trade-off between amount of labeled data needed and performance achieved. Co-training is one of those methods, and has been extensively studied in NLP (Nigam and Ghani, 2000; Pierce and Cardie, 2001; Ng and Cardie, 2003; Mota and Grishman, 2008). In particular, we showed that the performance of a name tagger based on co-training decays as the time gap between training data (seeds and unlabeled data) and test data increases (Mota and Grishman, 2008). Compared to the original classifier of Collins and Singer (1999) that uses seven seeds, we used substantially larger seed sets (more than 1000), which raises the question of which of the parameters (seeds or unlabeled data) are causing the performance deterioration. In the present study, we investigated two main questions, from the point of view of a developer who wants to analyze a new d</context>
<context position="4539" citStr="Mota and Grishman, 2008" startWordPosition="726" endWordPosition="729">tion system, exploiting contemporary newswire texts available on the web; Auzanne et al. (2000) proposed a time-adaptive language model, studying its impact over a period of five months on the reduction of OOV rate, word error rate and retrieval accuracy on a spoken document retrieval system. Concerning variations over longer periods of time, we observed that the performance of a semisupervised name tagger decays over a period of eight years, which seems to be directly related with the fact that the texts used to train and test the tagger also show a tendency to become less similar over time (Mota and Grishman, 2008); Batista et al. (2008) also observed a decaying tendency in the performance of a system for recovering capitalization over a period of six years, proposing to retrain a MaxEnt model using additional contemporary written texts. 3 Name tagger overview We assessed the name tagger described in Mota and Grishman (2008) to recognize names of people, organizations and locations. The tagger is based on the co-training NE classifier proposed by Collins and Singer (1999), and is comprised of several components organized sequentially (cf. Figure 1). Figure 1: NE tagger architecture 4 Data sets CETEMP´ub</context>
</contexts>
<marker>Mota, Grishman, 2008</marker>
<rawString>Cristina Mota and Ralph Grishman. 2008. Is this NE tagger getting old? In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco, may.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincente Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Weakly supervised natural language learning without redundant views.</title>
<date>2003</date>
<booktitle>In NAACL’03: Proceedings ofthe 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>94--101</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2000" citStr="Ng and Cardie, 2003" startWordPosition="316" endWordPosition="319">e first NER evaluation for Portuguese, HAREM (Santos and Cardoso, 2007), only two out of the nine participants presented systems based on machine learning, and they both argued they could have achieved significantly better results if they had larger training sets. Semi-supervised methods are commonly chosen as an alternative to overcome the lack of annotated resources, because they present a good trade-off between amount of labeled data needed and performance achieved. Co-training is one of those methods, and has been extensively studied in NLP (Nigam and Ghani, 2000; Pierce and Cardie, 2001; Ng and Cardie, 2003; Mota and Grishman, 2008). In particular, we showed that the performance of a name tagger based on co-training decays as the time gap between training data (seeds and unlabeled data) and test data increases (Mota and Grishman, 2008). Compared to the original classifier of Collins and Singer (1999) that uses seven seeds, we used substantially larger seed sets (more than 1000), which raises the question of which of the parameters (seeds or unlabeled data) are causing the performance deterioration. In the present study, we investigated two main questions, from the point of view of a developer wh</context>
</contexts>
<marker>Ng, Cardie, 2003</marker>
<rawString>Vincente Ng and Claire Cardie. 2003. Weakly supervised natural language learning without redundant views. In NAACL’03: Proceedings ofthe 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 94–101, Morristown, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Rayid Ghani</author>
</authors>
<title>Analyzing the effectiveness and applicability of co-training.</title>
<date>2000</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>86--93</pages>
<contexts>
<context position="1954" citStr="Nigam and Ghani, 2000" startWordPosition="308" endWordPosition="311"> the labeled data is scarce. For instance, in the first NER evaluation for Portuguese, HAREM (Santos and Cardoso, 2007), only two out of the nine participants presented systems based on machine learning, and they both argued they could have achieved significantly better results if they had larger training sets. Semi-supervised methods are commonly chosen as an alternative to overcome the lack of annotated resources, because they present a good trade-off between amount of labeled data needed and performance achieved. Co-training is one of those methods, and has been extensively studied in NLP (Nigam and Ghani, 2000; Pierce and Cardie, 2001; Ng and Cardie, 2003; Mota and Grishman, 2008). In particular, we showed that the performance of a name tagger based on co-training decays as the time gap between training data (seeds and unlabeled data) and test data increases (Mota and Grishman, 2008). Compared to the original classifier of Collins and Singer (1999) that uses seven seeds, we used substantially larger seed sets (more than 1000), which raises the question of which of the parameters (seeds or unlabeled data) are causing the performance deterioration. In the present study, we investigated two main quest</context>
</contexts>
<marker>Nigam, Ghani, 2000</marker>
<rawString>Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In Proceedings of CIKM, pages 86–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
<author>Mari Ostendorf</author>
</authors>
<title>Improving out-of-vocabulary name resolution.</title>
<date>2005</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="3487" citStr="Palmer and Ostendorf (2005)" startWordPosition="554" endWordPosition="558">creasing the amount of unlabeled data drawn from older sources. The experiments show that using contemporary unlabeled data is the best choice, outperforming most experiments with larger amounts of older unlabeled data and all experiments with contemporary seeds. 2 Contemporary labeled data in NLP The speech community has been defending for some time now the idea of having similar temporal data for training and testing automatic speech recognition systems for broadcast news. Most works focus on improving out-of-vocabulary (OOV) rates, to which new names contribute significantly. For instance, Palmer and Ostendorf (2005) aiming at reducing the error rate due to OOV names propose to generate offline name lists from diverse sources, including temporally relevant news texts; Federico and Bertoldi (2004), and Martins et al. (2006) propose to daily adapt the statistical language model of a broadcast 353 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 353–356, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP news transcription system, exploiting contemporary newswire texts available on the web; Auzanne et al. (2000) proposed a time-adaptive language model, studying its impact over a period o</context>
</contexts>
<marker>Palmer, Ostendorf, 2005</marker>
<rawString>David D. Palmer and Mari Ostendorf. 2005. Improving out-of-vocabulary name resolution. Computer Speech &amp; Language, 19(1):107–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pierce</author>
<author>Claire Cardie</author>
</authors>
<title>Limitations of co-training for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-2001).</booktitle>
<contexts>
<context position="1979" citStr="Pierce and Cardie, 2001" startWordPosition="312" endWordPosition="315">arce. For instance, in the first NER evaluation for Portuguese, HAREM (Santos and Cardoso, 2007), only two out of the nine participants presented systems based on machine learning, and they both argued they could have achieved significantly better results if they had larger training sets. Semi-supervised methods are commonly chosen as an alternative to overcome the lack of annotated resources, because they present a good trade-off between amount of labeled data needed and performance achieved. Co-training is one of those methods, and has been extensively studied in NLP (Nigam and Ghani, 2000; Pierce and Cardie, 2001; Ng and Cardie, 2003; Mota and Grishman, 2008). In particular, we showed that the performance of a name tagger based on co-training decays as the time gap between training data (seeds and unlabeled data) and test data increases (Mota and Grishman, 2008). Compared to the original classifier of Collins and Singer (1999) that uses seven seeds, we used substantially larger seed sets (more than 1000), which raises the question of which of the parameters (seeds or unlabeled data) are causing the performance deterioration. In the present study, we investigated two main questions, from the point of v</context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>David Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large datasets. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paulo Rocha</author>
<author>Diana Santos</author>
</authors>
<title>Cetemp´ublico: Um corpus de grandes dimens˜oes de linguagem jornalistica portuguesa.</title>
<date>2000</date>
<booktitle>In Maria das Grac¸as Volpe Nunes, editor, Actas do V Encontro para o processamento computacional da lingua portuguesa escrita e falada PROPOR 2000,</booktitle>
<pages>131--140</pages>
<location>Atibaia, S˜ao Paulo, Brasil.</location>
<contexts>
<context position="5168" citStr="Rocha and Santos, 2000" startWordPosition="826" endWordPosition="829">sta et al. (2008) also observed a decaying tendency in the performance of a system for recovering capitalization over a period of six years, proposing to retrain a MaxEnt model using additional contemporary written texts. 3 Name tagger overview We assessed the name tagger described in Mota and Grishman (2008) to recognize names of people, organizations and locations. The tagger is based on the co-training NE classifier proposed by Collins and Singer (1999), and is comprised of several components organized sequentially (cf. Figure 1). Figure 1: NE tagger architecture 4 Data sets CETEMP´ublico (Rocha and Santos, 2000) is a Portuguese journalistic corpus with 180 million words that spans eight years of news, from 1991 to 1998. The minimum size of epoch (time span of data set) available for analysis is a six-month period, corresponding either to the first half of the year or the second. The data sets were created using the first 8256 extracts1 within each six-month period of the politics section of the corpus: the first 192 are used to collect seeds, the next 208 extracts are used as test sets and the remaining 7856 are used to collect the unlabeled examples. The seeds correspond to the first 1150 names occu</context>
</contexts>
<marker>Rocha, Santos, 2000</marker>
<rawString>Paulo Rocha and Diana Santos. 2000. Cetemp´ublico: Um corpus de grandes dimens˜oes de linguagem jornalistica portuguesa. In Maria das Grac¸as Volpe Nunes, editor, Actas do V Encontro para o processamento computacional da lingua portuguesa escrita e falada PROPOR 2000, pages 131–140, Atibaia, S˜ao Paulo, Brasil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Santos</author>
<author>Nuno Cardoso</author>
<author>editors</author>
</authors>
<title>Reconhecimento de entidades mencionadas em portuguˆes: Documentac¸˜ao e actas do HAREM, a primeira avaliac¸˜ao conjunta na ´area.</title>
<date>2007</date>
<journal>Linguateca,</journal>
<volume>12</volume>
<note>de Novembro.</note>
<marker>Santos, Cardoso, editors, 2007</marker>
<rawString>Diana Santos and Nuno Cardoso, editors. 2007. Reconhecimento de entidades mencionadas em portuguˆes: Documentac¸˜ao e actas do HAREM, a primeira avaliac¸˜ao conjunta na ´area. Linguateca, 12 de Novembro.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>