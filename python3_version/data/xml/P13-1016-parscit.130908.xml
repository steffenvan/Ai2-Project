<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.836399">
Distortion Model Considering Rich Context
for Statistical Machine Translation
</title>
<note confidence="0.4996202">
Isao Goto†,‡ Masao Utiyama† Eiichiro Sumita†
Akihiro Tamura† Sadao Kurohashi‡†National Institute of Information and Communications Technology
‡Kyoto University
goto.i-es@nhk.or.jp
{mutiyama, eiichiro.sumita, akihiro.tamura}@nict.go.jp
</note>
<email confidence="0.9384">
kuro@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.98225" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997880952381">
This paper proposes new distortion mod-
els for phrase-based SMT. In decoding, a
distortion model estimates the source word
position to be translated next (NP) given
the last translated source word position
(CP). We propose a distortion model that
can consider the word at the CP, a word
at an NP candidate, and the context of the
CP and the NP candidate simultaneously.
Moreover, we propose a further improved
model that considers richer context by dis-
criminating label sequences that specify
spans from the CP to NP candidates. It
enables our model to learn the effect of
relative word order among NP candidates
as well as to learn the effect of distances
from the training data. In our experiments,
our model improved 2.9 BLEU points for
Japanese-English and 2.6 BLEU points for
Chinese-English translation compared to
the lexical reordering models.
</bodyText>
<sectionHeader confidence="0.992538" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.943396625">
Estimating appropriate word order in a target lan-
guage is one of the most difficult problems for
statistical machine translation (SMT). This is par-
ticularly true when translating between languages
with widely different word orders.
To address this problem, there has been a lot
of research done into word reordering: lexical
reordering model (Tillman, 2004), which is one
of the distortion models, reordering constraint
(Zens et al., 2004), pre-ordering (Xia and Mc-
Cord, 2004), hierarchical phrase-based SMT (Chi-
ang, 2007), and syntax-based SMT (Yamada and
Knight, 2001).
In general, source language syntax is useful for
handling long distance word reordering. However,
obtaining syntax requires a syntactic parser, which
is not available for many languages. Phrase-based
SMT (Koehn et al., 2007) is a widely used SMT
method that does not use a parser.
Phrase-based SMT mainly1 estimates word re-
ordering using distortion models2. Therefore, dis-
tortion models are one of the most important com-
ponents for phrase-based SMT. On the other hand,
there are methods other than distortion models for
improving word reordering for phrase-based SMT,
such as pre-ordering or reordering constraints.
However, these methods also use distortion mod-
els when translating by phrase-based SMT. There-
fore, distortion models do not compete against
these methods and are commonly used with them.
If there is a good distortion model, it will improve
the translation quality of phrase-based SMT and
benefit to the methods using distortion models.
In this paper, we propose two distortion mod-
els for phrase-based SMT. In decoding, a distor-
tion model estimates the source word position to
be translated next (NP) given the last translated
source word position (CP). The proposed models
are the pair model and the sequence model. The
pair model utilizes the word at the CP, a word at
an NP candidate site, and the words surrounding
the CP and the NP candidates (context) simultane-
ously. In addition, the sequence model, which is
the further improved model, considers richer con-
text by identifying the label sequence that spec-
ify the span from the CP to the NP. It enables
our model to learn the effect of relative word or-
der among NP candidates as well as to learn the
effect of distances from the training data. Our
model learns the preference relations among NP
1A language model also supports the estimation.
2In this paper, reordering models for phrase-based SMT,
which are intended to estimate the source word position to
be translated next in decoding, are called distortion models.
This estimation is used to produce a hypothesis in the target
language word order sequentially from left to right.
</bodyText>
<page confidence="0.502943">
155
</page>
<note confidence="0.97725">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 155–165,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.995500363636364">
()
nou1 ae2 wa3 pa4 5 on6 wo7 8
he bought books in Paris yesterday
W
P
W
Target:
books
he bought
in Paris
yesterday
</figure>
<figureCaption confidence="0.99917">
Figure 1: An example of left-to-right translation
</figureCaption>
<bodyText confidence="0.909986111111111">
for Japanese-English. Boxes represent phrases
and arrows indicate the translation order of the
phrases.
candidates. Our model consists of one probabilis-
tic model and does not require a parser. Exper-
iments confirmed the effectiveness of our method
for Japanese-English and Chinese-English transla-
tion, using NTCIR-9 Patent Machine Translation
Task data sets (Goto et al., 2011).
</bodyText>
<sectionHeader confidence="0.937706" genericHeader="method">
2 Distortion Model for Phrase-Based
SMT
</sectionHeader>
<bodyText confidence="0.99979992">
A Moses-style phrase-based SMT generates target
hypotheses sequentially from left to right. There-
fore, the role of the distortion model is to esti-
mate the source phrase position to be translated
next whose target side phrase will be located im-
mediately to the right of the already generated hy-
potheses. An example is shown in Figure 1. In
Figure 1, we assume that only the kare wa (En-
glish side: “he”) has been translated. The target
word to be generated next will be “bought” and the
source word to be selected next will be its corre-
sponding Japanese word katta. Thus, a distortion
model should estimate phrases including katta as
a source phrase position to be translated next.
To explain the distortion model task in more de-
tail, we need to redefine more precisely two terms,
the current position (CP) and next position (NP) in
the source sentence. CP is the source sentence po-
sition corresponding to the rightmost aligned tar-
get word in the generated target word sequence.
NP is the source sentence position corresponding
to the leftmost aligned target word in the target
phrase to be generated next. The task of the distor-
tion model is to estimate the NP3 from NP candi-
dates (NPCs) for each CP in the source sentence.4
</bodyText>
<footnote confidence="0.935411">
3NP is not always one position, because there may be mul-
tiple correct hypotheses.
4This definition is slightly different from that of existing
methods such as Moses and (Green et al., 2010). In existing
methods, CP is the rightmost position of the last translated
source phrase and NP is the leftmost position of the source
phrase to be translated next. Note that existing methods do
</footnote>
<equation confidence="0.7557476">
kinou1 kare2 wa3 pari4 de5 ni6 satsu7 hon8 wo9 katt10
he bought two books in Paris yesterday
kinou1 kare2 wa3 hon4 wo5 karita6 ga7 kanojo8 wa9 ktt10
he borrowed books yesterday but she bought
(d)��
�
kinou1 kare2 wa3 hon4 wo5 katta6 ga7 kanojo8 wa9 kit10
he bought books yesterday but she borrd
(e)
kinou1 kare2 wa3 kanojo4 ga5 katta6 hon7 wo8 karit9
</equation>
<bodyText confidence="0.937335357142857">
yesterday he borrowed the books that she bouht
Figure 2: Examples of CP and NP for Japanese-
English translation. The upper sentence is the
source sentence and the sentence underneath is a
target hypothesis for each example. The NP is in
bold, and the CP is in bold italics. The point of an
arrow with a x mark indicates a wrong NP candi-
date.
Estimating NP is a difficult task. Figure 2 shows
some examples. The superscript numbers indicate
the word position in the source sentence.
In Figure 2 (a), the NP is 8. However, in Fig-
ure 2 (b), the word (kare) at the CP is the same as
(a), but the NP is different (the NP is 10). From
these examples, we see that distance is not the es-
sential factor in deciding an NP. And it also turns
out that the word at the CP alone is not enough to
estimate the NP. Thus, not only the word at the CP
but also the word at a NP candidate (NPC) should
be considered simultaneously.
In (c) and (d) in Figure 2, the word (kare) at the
CP is the same and karita (borrowed) and katta
(bought) are at the NPCs. Karita is the word at
the NP and katta is not the word at the NP for
(c), while katta is the word at the NP and karita
is not the word at the NP for (d). From these ex-
amples, considering what the word is at the NP
not consider word-level correspondences.
</bodyText>
<page confidence="0.88149">
156
</page>
<bodyText confidence="0.999746040816326">
is not enough to estimate the NP. One of the rea-
sons for this difference is the relative word order
between words. Thus, considering relative word
order is important.
In (d) and (e) in Figure 2, the word (kare) at the
CP and the word order between katta and karita
are the same. However, the word at the NP for
(d) and the word at the NP for (e) are different.
From these examples, we can see that selecting
a nearby word is not always correct. The differ-
ence is caused by the words surrounding the NPCs
(context), the CP context, and the words between
the CP and the NPC. Thus, these should be con-
sidered when estimating the NP.
In summary, in order to estimate the NP, the fol-
lowing should be considered simultaneously: the
word at the NP, the word at the CP, the relative
word order among the NPCs, the words surround-
ing NP and CP (context), and the words between
the CP and the NPC.
There are distortion models that do not require
a parser for phrase-based SMT. The linear dis-
tortion cost model used in Moses (Koehn et al.,
2007), whose costs are linearly proportional to
the reordering distance, always gives a high cost
to long distance reordering, even if the reorder-
ing is correct. The MSD lexical reordering model
(Tillman, 2004; Koehn et al., 2005; Galley and
Manning, 2008) only calculates probabilities for
the three kinds of phrase reorderings (monotone,
swap, and discontinuous), and does not consider
relative word order or words between the CP and
the NPC. Thus, these models are not sufficient for
long distance word reordering.
Al-Onaizan and Papineni (2006) proposed a
distortion model that used the word at the CP and
the word at an NPC. However, their model did not
use context, relative word order, or words between
the CP and the NPC.
Ni et al. (2009) proposed a method that adjusts
the linear distortion cost using the word at the CP
and its context. Their model does not simultane-
ously consider both the word specified at the CP
and the word specified at the NPCs.
Green et al. (2010) proposed distortion mod-
els that used context. Their model (the outbound
model) estimates how far the NP should be from
the CP using the word at the CP and its con-
text.5 Their model does not simultaneously con-
</bodyText>
<subsectionHeader confidence="0.505596">
5They also proposed another model (the inbound model)
</subsectionHeader>
<bodyText confidence="0.9998845">
sider both the word specified at the CP and the
word specified at an NPC. For example, the out-
bound model considers the word specified at the
CP, but does not consider the word specified at an
NPC. Their models also do not consider relative
word order.
In contrast, our distortion model solves the
aforementioned problems. Our distortion models
utilize the word specified at the CP, the word spec-
ified at an NPC, and also the context of the CP
and the NPC simultaneously. Furthermore, our se-
quence model considers richer context including
the relative word order among NPCs and also in-
cluding all the words between the CP and the NPC.
In addition, unlike previous methods, our models
learn the preference relations among NPCs.
</bodyText>
<sectionHeader confidence="0.983846" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.999961">
In this section, we first define our distortion model
and explain our learning strategy. Then, we de-
scribe two proposed models: the pair model and
the sequence model that is the further improved
model.
</bodyText>
<subsectionHeader confidence="0.998254">
3.1 Distortion Model and Learning Strategy
</subsectionHeader>
<bodyText confidence="0.9992735">
First, we define our distortion model. Let i be a
CP, j be an NPC, 5 be a source sentence, and X be
the random variable of the NP. In this paper, dis-
tortion probability is defined as P(X = j|i, 5),
which is the probability of an NPC j being the NP.
Our distortion model is defined as the model cal-
culating the distortion probability.
Next, we explain the learning strategy for our
distortion model. We train this model as a dis-
criminative model that discriminates the NP from
NPCs. Let J be a set of word positions in 5 other
than i. We train the distortion model subject to
</bodyText>
<equation confidence="0.9837445">
� P(X = j|i, 5) = 1.
jEJ
</equation>
<bodyText confidence="0.999506777777778">
The model parameters are learned to maximize the
distortion probability of the NP among all of the
NPCs J in each source sentence. This learning
strategy is a kind of preference relation learning
(Evgniou and Pontil, 2002). In this learning, the
that estimates reverse direction distance. Each NPC is re-
garded as an NP, and the inbound model estimates how far
the corresponding CP should be from the NP using the word
at the NP and its context.
</bodyText>
<page confidence="0.583591">
157
</page>
<bodyText confidence="0.999985071428572">
distortion probability of the actual NP will be rel-
atively higher than those of all the other NPCs J.
This learning strategy is different from that of
(Al-Onaizan and Papineni, 2006; Green et al.,
2010). For example, Green et al. (2010) trained
their outbound model subject to &amp;EC P(Y =
c|i, S) = 1, where C is the set of the nine distor-
tion classes6 and Y is the random variable of the
correct distortion class that the correct distortion is
classified into. Distortion is defined as j − i − 1.
Namely, the model probabilities that they learned
were the probabilities of distortion classes in all
of the training data, not the relative preferences
among the NPCs in each source sentence.
</bodyText>
<subsectionHeader confidence="0.994692">
3.2 Pair Model
</subsectionHeader>
<bodyText confidence="0.988020684210527">
The pair model utilizes the word at the CP, the
word at an NPC, and the context of the CP and the
NPC simultaneously to estimate the NP. This can
be done by our distortion model definition and the
learning strategy described in the previous section.
In this work, we use the maximum entropy
method (Berger et al., 1996) as a discriminative
machine learning method. The reason for this
is that a model based on the maximum entropy
method can calculate probabilities. However, if
we use scores as an approximation of the distor-
tion probabilities, various discriminative machine
learning methods can be applied to build the dis-
tortion model.
Let s be a source word and sn1 = s1s2...sn be
a source sentence. We add a beginning of sen-
tence (BOS) marker to the head of the source sen-
tence and an end of sentence (EOS) marker to the
end, so the source sentence S is expressed as sn+1
</bodyText>
<equation confidence="0.910314">
0
(s0 = BOS, sn+1 = EOS). Our distortion model
calculates the distortion probability for an NPC
j E {j|1 &lt; j &lt; n + 1 n j =7� i} for each CP
i E {i|0 &lt; i &lt; n}
1exp (wTf (i, j, S, o, d))
P(X = j|i, S) =
Zi
(1)
</equation>
<bodyText confidence="0.755118">
where
</bodyText>
<equation confidence="0.9606845">
0 (i &lt; j) d = �� 0 (|j − i |= 1) 5),
o =1 (i &gt; j) , �� 1 (2 ≤ |j − i |≤
{ 2 (6 ≤ |j − i|)
�
</equation>
<bodyText confidence="0.9504966">
6(−∞, −8], [−7, −5], [−4, −3], −2, 0, 1, [2, 3], [4, 6],
and [7, ∞). In (Green et al., 2010), −1 was used as one of
distortion classes. However, −1 represents the CP in our def-
inition, and CP is not an NPC. Thus, we shifted all of the
distortion classes for negative distortions by −1.
</bodyText>
<equation confidence="0.826441">
Template
⟨o⟩, ⟨o, sp⟩1, ⟨o, ti⟩, ⟨o, tj⟩, ⟨o, d⟩, ⟨o, sp, sq⟩2,
⟨o, ti, tj⟩, ⟨o, ti−1, ti, tj⟩, ⟨o, ti, ti+1, tj⟩,
⟨o,ti,
tj−1, tj⟩, ⟨o, ti, tj, tj+1⟩, ⟨o, si, ti, tj⟩,
⟨o, sj, ti, tj⟩
1 p ∈ {p|i − 2 ≤ p ≤ i + 2 ∨ j − 2 ≤ p ≤ j + 2}
2 (p, q) ∈ {(p, q)|i − 2 ≤ p ≤ i + 2 ∧ j − 2 ≤ q ≤
j + 2 ∧ (|p − i |≤ 1 ∨ |q − j |≤ 1)}
Table 1: Feature templates. t is the part of speech
of s.
w is a weight parameter vector, each element
of f(·) is a binary feature function, and Zi =
E
</equation>
<bodyText confidence="0.994333117647059">
jE{jJ1&lt;j&lt;n+1 n jai}(numerator of Equation 1)
is a normalization factor. o is an orientation of i to
j and d is a distance class.
The binary feature function that constitutes an
element of f(·) returns 1 when its feature is
matched and if else, returns 0. Table 1 shows the
feature templates used to produce the features. A
feature is an instance of a feature template.
In Equation 1, i, j, and S are used by the feature
functions. Thus, Equation 1 can utilize features
consisting of both si, which is the word specified
at i, and sj, which is the word specified at j, or
both the context of i and the context of j simulta-
neously. Distance is considered using the distance
class d. Distortion is represented by distance and
orientation. The pair model considers distortion
using six joint classes of d and o.
</bodyText>
<subsectionHeader confidence="0.998179">
3.3 Sequence Model
</subsectionHeader>
<bodyText confidence="0.9998986">
The pair model does not consider relative word or-
der among NPCs or all the words between the CP
and an NPC. In this section, we propose a further
improved model, the sequence model, which con-
siders richer context including relative word order
among NPCs and also including all the words be-
tween the CP and an NPC.
In (c) and (d) in Figure 2, karita (borrowed) and
katta (bought) occur in the source sentences. The
pair model considers the effect of distances using
only the distance class d. If these positions are
in the same distance class, the pair model cannot
consider the differences in distances. In this case,
these are conflict instances during training and it
is difficult to distinguish the NP for translation.
Now to explain how to consider the relative
word order by the sequence model. The sequence
model considers the relative word order by dis-
criminating the label sequence corresponding to
the NP from the label sequences corresponding to
</bodyText>
<page confidence="0.857031">
158
</page>
<tableCaption confidence="0.987461">
Table 2: The “C, I, and N” label set.
</tableCaption>
<figure confidence="0.618067">
Source sentence
</figure>
<figureCaption confidence="0.99571375">
Figure 3: Example of label sequences that specify
spans from the CP to each NPC for the case of
Figure 2 (c). The labels (C, I, and N) in the boxes
are the label sequences.
</figureCaption>
<bodyText confidence="0.997530891891892">
each NPC in each sentence. Each label sequence
corresponds to one NPC. Therefore, if we identify
the label sequence that corresponds to the NP, we
can obtain the NP. The label sequences specify the
spans from the CP to each NPC using three kinds
of labels indicating the type of word positions in
the spans. The three kinds of labels, “C, I, and N,”
are shown in Table 2. Figure 3 shows examples
of the label sequences for the case of Figure 2 (c).
In Figure 3, the label sequences are represented by
boxes and the elements of the sequences are labels.
The NPC is used as the label sequence ID for each
label sequence.
The label sequence can treat relative word or-
der. For example, the label sequence ID of 10 in
Figure 3 knows that karita exists to the left of the
NPC of 10. This is because karita6 carries a la-
bel I while katta1° carries a label N, and a position
with label I is defined as relatively closer to the CP
than a position with label N. By utilizing the label
sequence and corresponding words, the model can
reflect the effect of karita existing between the CP
and the NPC of 10 on the probability.
For the sequence model, karita (borrowed) and
katta (bought) in (c) and (d) in Figure 2 are not
conflict instances in training, whereas they are
conflict instances in training for the pair model.
The reason is as follows. In order to make the
probability of the NPC of 10 smaller than the NPC
of 6, instead of making the weight parameters for
the features with respect to the word at the position
of 10 with label N smaller than the weight param-
eters for the features with respect to the word at
the position of 6 with label N, the sequence model
can give negative weight parameters for the fea-
tures with respect to the word at the position of 6
with label I.
We use a sequence discrimination technique
based on CRF (Lafferty et al., 2001) to identify the
label sequence that corresponds to the NP. There
are two differences between our task and the CRF
task. One difference is that CRF discriminates la-
bel sequences that consist of labels from all of the
label candidates, whereas we constrain the label
sequences to sequences where the label at the CP
is C, the label at an NPC is N, and the labels be-
tween the CP and the NPC are I. The other dif-
ference is that CRF is designed for discriminat-
ing label sequences corresponding to the same ob-
ject sequence, whereas we do not assign labels to
words outside the spans from the CP to each NPC.
However, when we assume that another label such
as E has been assigned to the words outside the
spans and there are no features involving label E,
CRF with our label constraints can be applied to
our task. In this paper, the method designed to
discriminate label sequences corresponding to the
different word sequence lengths is called partial
CRF.
The sequence model based on partial CRF is de-
rived by extending the pair model. We introduce
the label l and extend the pair model to discrimi-
nating the label sequences. There are two exten-
sions to the pair model. One extension uses la-
bels. We suppose that label sequences specify the
spans from the CP to each NPC. We conjoined all
the feature templates in Table 1 with an additional
feature template (li, lj) to include the labels into
features where li is the label corresponding to the
position of i. The other extension uses sequence.
In the pair model, the position pair of (i, j) is used
to derive features. In contrast, to descriminate la-
bel sequences in the sequence model, the position
pairs of (i,k), k E {k|i &lt; k &lt; j V j &lt; k &lt; i}
</bodyText>
<figure confidence="0.990986295454545">
Label
Description
A position is the CP.
A position is a position between the CP
and the NPC.
A position is the NPC.
C
I
N
1 N C
Label sequence ID
10
11
4
7
3
5
6
8
9
C N
C I N
C I I N
C I I I N
C I I I I N
C I I I I I N
C I I I I I I N
C I I I I I I I N
C I I I I I I I I N
BOS0
kinoul
kare2
wa3
hon4
wo
karitas
gal
kanojo8
wag
kattal0
EOS&amp;quot;
(yesterday) (book) (borrowed) (she) (bought)
(he)
159
</figure>
<bodyText confidence="0.994779222222222">
and (k,j), k E {k Ii &lt; k &lt; j V j &lt; k &lt; i}
are used to derive features. Note that in the feature
templates in Table 1, i and j are used to specify
two positions. When features are used for the se-
quence model, one of the positions is regarded as
k.
The distortion probability for an NPC j being
the NP given a CP i and a source sentence S is
calculated as:
</bodyText>
<equation confidence="0.991874">
P(X = j Ii, S) =
</equation>
<bodyText confidence="0.727983">
where
</bodyText>
<equation confidence="0.8332665">
M _ r{m|i &lt; m &lt; j} (i &lt; j)
Sl{m|j &lt; m &lt; i} (i &gt; j)
</equation>
<bodyText confidence="0.978221428571428">
and Zi = I:jE{j|1&lt;j&lt;n+1 n j̸�i}(numerator of
Equation 2) is a normalization factor. Since j is
used as the label sequence ID, discriminating j
also means discriminating label sequence IDs.
The first term in exp(·) in Equation 2 considers
all of the word pairs located at i and other posi-
tions in the sequence, and also their context. The
second term in exp(·) in Equation 2 considers all
of the word pairs located at j and other positions
in the sequence, and also their context.
By designing our model to discriminate among
different length label sequences, our model can
naturally handle the effect of distances. Many fea-
tures are derived from a long label sequence be-
cause it will contain many labels between the CP
and the NPC. On the other hand, fewer features
are derived from a short label sequence because a
short label sequence will contain fewer labels be-
tween the CP and the NPC. The bias from these
differences provides important clues for learning
the effect of distances.7
7Note that the sequence model does not only consider
larger context than the pair model, but that it also considers
labels. The pair model does not discriminate labels, whereas
the sequence model uses label N and label I for the positions
except for the CP, depending on each situation. For example,
in Figure 3, at position 6, label N is used in the label sequence
ID of 6, but label I is used in the label sequence IDs of 7 to
11. Namely, even if they are at the same position, the labels
in the label sequences are different. The sequence model dis-
criminates the label differences.
Figure 4: Examples of supervised training data.
The lines represent word alignments. The English
side arrows point to the nearest word aligned on
the right.
</bodyText>
<subsectionHeader confidence="0.983134">
3.4 Training Data for Discriminative
Distortion Model
</subsectionHeader>
<bodyText confidence="0.999993363636364">
To train our discriminative distortion model, su-
pervised training data is needed. The training data
is built from a parallel corpus and word alignments
between corresponding source words and target
words. Figure 4 shows examples of training data.
We select the target words aligned to the source
words sequentially from left to right (target side
arrows). Then, the order of the source words in
the target word order is decided (source side ar-
rows). The source sentence and the source side
arrows are the training data.
</bodyText>
<sectionHeader confidence="0.997837" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999902">
In order to confirm the effects of our distortion
model, we conducted a series of Japanese to En-
glish (JE) and Chinese to English (CE) translation
experiments.8
</bodyText>
<subsectionHeader confidence="0.997692">
4.1 Common Settings
</subsectionHeader>
<bodyText confidence="0.930753375">
We used the patent data for the Japanese to En-
glish and Chinese to English translation subtasks
from the NTCIR-9 Patent Machine Translation
Task (Goto et al., 2011). There were 2,000 sen-
tences for the test data and 2,000 sentences for the
development data.
Mecab9 was used for the Japanese morpholog-
ical analysis. The Stanford segmenter10 and tag-
ger11 were used for Chinese segmentation and
POS tagging. The translation model was trained
using sentences of 40 words or less from the train-
ing data. So approximately 2.05 million sen-
tence pairs consisting of approximately 54 million
8We conducted JE and CE translation as examples of
language pairs with different word orders and of languages
where there is a great need for translation into English.
</bodyText>
<figure confidence="0.8837095">
9http://mecab.sourceforge.net/
10http://nlp.stanford.edu/software/segmenter.shtml
11http://nlp.stanford.edu/software/tagger.shtml
Source:
BOS kare wa pari de hon w ktt
Target:
BOS he bought books in Paris ES
1
exp 1 2_.,wTf (i, k, S, o, d, li, lk)
� �wTf (k, j, S, o, d, lk, lj)
+ (2)
kEMU{i}
Zi \kEMU{j}
160
</figure>
<bodyText confidence="0.997082136363636">
Japanese tokens whose lexicon size was 134k and
50 million English tokens whose lexicon size was
213k were used for JE. And approximately 0.49
million sentence pairs consisting of 14.9 million
Chinese tokens whose lexicon size was 169k and
16.3 million English tokens whose lexicon size
was 240k were used for CE. GIZA++ and grow-
diag-final-and heuristics were used to obtain word
alignments. In order to reduce word alignment er-
rors, we removed articles {a, an, the} in English
and particles {ga, wo, wa} in Japanese before per-
forming word alignments because these function
words do not correspond to any words in the other
languages. After word alignment, we restored the
removed words and shifted the word alignment po-
sitions to the original word positions. We used 5-
gram language models that were trained using the
English side of each set of bilingual training data.
We used an in-house standard phrase-based
SMT system compatible with the Moses decoder
(Koehn et al., 2007). The SMT weighting param-
eters were tuned by MERT (Och, 2003) using the
development data. To stabilize the MERT results,
we tuned three times by MERT using the first half
of the development data and we selected the SMT
weighting parameter set that performed the best on
the second half of the development data based on
the BLEU scores from the three SMT weighting
parameter sets.
We compared systems that used a common
SMT feature set from standard SMT features and
different distortion model features. The com-
mon SMT feature set consists of: four translation
model features, phrase penalty, word penalty, and
a language model feature. The compared different
distortion model features are: the linear distortion
cost model feature (LINEAR), the linear distortion
cost model feature and the six MSD bidirectional
lexical distortion model (Koehn et al., 2005) fea-
tures (LINEAR+LEX), the outbound and inbound
distortion model features discriminating nine dis-
tortion classes (Green et al., 2010) (9-CLASS), the
proposed pair model feature (PAIR), and the pro-
posed sequence model feature (SEQUENCE).
</bodyText>
<subsectionHeader confidence="0.898946">
4.2 Training for the Proposed Models
</subsectionHeader>
<bodyText confidence="0.9999099375">
Our distortion model was trained as follows: We
used 0.2 million sentence pairs and their word
alignments from the data used to build the trans-
lation model as the training data for our distortion
models. The features that were selected and used
were the ones that had been counted12, using the
feature templates in Table 1, at least four times
for all of the (i, j) position pairs in the training
sentences. We conjoined the features with three
types of label pairs (C, I), (I, N), or (C, N) as in-
stances of the feature template (li, lj) to produce
features for SEQUENCE. The L-BFGS method
(Liu and Nocedal, 1989) was used to estimate the
weight parameters of maximum entropy models.
The Gaussian prior (Chen and Rosenfeld, 1999)
was used for smoothing.
</bodyText>
<subsectionHeader confidence="0.915733">
4.3 Training for the Compared Models
</subsectionHeader>
<bodyText confidence="0.999982142857143">
For 9-CLASS, we used the same training data as
for our distortion models. Let ti be the part of
speech of si. We used the following feature tem-
plates to produce features for the outbound model:
(si−2), (si−1), (si), (si+1), (si+2), (ti), (ti−1, ti),
(ti, ti+1), and (si, ti). These feature templates corre-
spond to the components of the feature templates
of our distortion models. In addition to these fea-
tures, we used a feature consisting of the relative
source sentence position as the feature used by
(Green et al., 2010). The relative source sentence
position is discretized into five bins, one for each
quintile of the sentence. For the inbound model13,
i of the feature templates was changed to j. Fea-
tures occurring four or more times in the train-
ing sentences were used. The maximum entropy
method with Gaussian prior smoothing was used
to estimate the model parameters.
The MSD bidirectional lexical distortion model
was built using all of the data used to build the
translation model.
</bodyText>
<subsectionHeader confidence="0.965797">
4.4 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.990914357142857">
We evaluated translation quality based on the case-
insensitive automatic evaluation score BLEU-4
(Papineni et al., 2002). We used distortion lim-
its of 10, 20, 30, and unlimited (oo), which limited
the number of words for word reordering to a max-
imum number. Table 3 presents our main results.
The proposed SEQUENCE outperformed the base-
lines for both Japanese to English and Chinese to
English translation. This demonstrates the effec-
tiveness of the proposed SEQUENCE. The scores
of the proposed SEQUENCE were higher than those
12When we counted features for selection, we only counted
features that were from the feature templates of (si, sj),
(ti, tj), (si, ti, tj), and (sj, ti, tj) in Table 1 when j was not
</bodyText>
<table confidence="0.9163824">
the NP, in order to avoid increasing the number of features.
13The inbound model is explained in footnote 5.
161
Distortion limit 10 Japanese-English 00 10 Chinese-English 00
20 30 20 30
LINEAR 27.98 27.74 27.75 27.30 29.18 28.74 28.31 28.33
LINEAR+LEX 30.25 30.37 30.17 29.98 30.81 30.24 30.16 30.13
9-CLASS 30.74 30.98 30.92 30.75 31.80 31.56 31.31 30.84
PAIR 31.62 32.36 31.96 32.03 32.51 32.30 32.25 32.32
SEQUENCE 32.02 32.96 33.29 32.81 33.41 33.44 33.35 33.41
</table>
<tableCaption confidence="0.9898785">
Table 3: Evaluation results for each method. The values are case-insensitive BLEU scores. Bold numbers
indicate no significant difference from the best result in each language pair using the bootstrap resampling
</tableCaption>
<table confidence="0.601014666666667">
test at a significance level a = 0.01 (Koehn, 2004).
Japanese-English Chinese-English
HIER 30.47 32.66
</table>
<tableCaption confidence="0.9338125">
Table 4: Evaluation results for hierarchical phrase-
based SMT.
</tableCaption>
<bodyText confidence="0.999573666666666">
of the proposed PAIR. This confirms the effective-
ness for considering relative word order and words
between the CP and an NPC. The proposed PAIR
outperformed 9-CLASS, confirming that consider-
ing both the word specified at the CP and the word
specified at the NPC simultaneously was more ef-
fective than that of 9-CLASS.
For translating between languages with widely
different word orders such as Japanese and En-
glish, a small distortion limit is undesirable be-
cause there are cases where correct translations
cannot be produced with a small distortion limit,
since the distortion limit prunes the search space
that does not meet the constraint. Therefore,
a large distortion limit is required to translate
correctly. For JE translation, our SEQUENCE
achieved significantly better results at distortion
limits of 20 and 30 than that at a distortion limit
of 10, while the baseline systems of LINEAR,
LINEAR+LEX, and 9-CLASS did not achieve this.
This indicate that SEQUENCE could treat long
distance reordering candidates more appropriately
than the compared methods.
We also tested hierarchical phrase-based SMT
(Chiang, 2007) (HIER) using the Moses imple-
mentation. The common data was used to train
HIER. We used unlimited max-chart-span for the
system setting. Results are given in Table 4. Our
SEQUENCE outperformed HIER. The gain for JE
was large but the gain for CE was modest. Since
phrase-based SMT is generally faster in decod-
ing speed than hierarchical phrase-based SMT,
achieving better or comparable scores is worth-
</bodyText>
<figure confidence="0.412555">
Distortion
</figure>
<figureCaption confidence="0.81867">
Figure 5: Average probabilities for large distortion
for Japanese-English translation.
</figureCaption>
<bodyText confidence="0.997918227272727">
while.
To investigate the tolerance for sparsity of the
training data, we reduced the training data for
the sequence model to 20,000 sentences for JE
translation.14 SEQUENCE using this model with
a distortion limit of 30 achieved a BLEU score
of 32.22.15 Although the score is lower than the
score of SEQUENCE with a distortion limit of 30
in Table 3, the score was still higher than those
of LINEAR, LINEAR+LEX, and 9-CLASS for JE
in Table 3. This indicates that the sequence model
also works even when the training data is not large.
This is because the sequence model considers not
only the word at the CP and the word at an NPC
but also rich context, and rich context would be ef-
fective even for a smaller set of training data.
14We did not conduct experiments using larger training
data because there would have been a very high computa-
tional cost to build models using the L-BFGS method.
15To avoid effects from differences in the SMT weighting
parameters, we used the same SMT weighting parameters for
SEQUENCE, with a distortion limit of 30, in Table 3.
</bodyText>
<page confidence="0.675092">
162
</page>
<bodyText confidence="0.999982777777778">
To investigate how well SEQUENCE learns the
effect of distance, we checked the average distor-
tion probabilities for large distortions of j − i − 1.
Figure 5 shows three kinds of probabilities for dis-
tortions from 3 to 20 for Japanese-English transla-
tion. One is the average distortion probabilities
in the Japanese test sentences for each distortion
for SEQUENCE, and another is this for PAIR. The
third (CORPUS) is the probabilities for the actual
distortions in the training data that were obtained
from the word alignments used to build the trans-
lation model. The probability for a distortion for
CORPUS was calculated by the number of the dis-
tortion divided by the total number of distortions
in the training data.
Figure 5 shows that when a distance class fea-
ture used in the model was the same (e.g., distor-
tions from 5 to 20 were the same distance class
feature), PAIR produced average distortion prob-
abilities that were almost the same. In contrast,
the average distortion probabilities for SEQUENCE
decreased when the lengths of the distortions in-
creased, even if the distance class feature was
the same, and this behavior was the same as that
of CORPUS. This confirms that the proposed
SEQUENCE could learn the effect of distances ap-
propriately from the training data.16
</bodyText>
<sectionHeader confidence="0.999659" genericHeader="method">
5 Related Works
</sectionHeader>
<bodyText confidence="0.999594">
We discuss related works other than discussed in
Section 2. Xiong et al. (2012) proposed a model
predicting the orientation of an argument with re-
spect to its verb using a parser. Syntactic struc-
tures and predicate-argument structures are useful
for reordering. However, orientations do not han-
dle distances. Thus, our distortion model does not
compete against the methods predicting orienta-
tions using a parser and would assist them if used
16We also checked the average distortion probabilities for
the 9-CLASS outbound model in the Japanese test sentences
for Japanese-English translation. We averaged the average
probabilities for distortions in a distortion span of [4, 6] and
also averaged those in a distortion span of [7, 20], where the
distortions in each span are in the same distortion class. The
average probability for [4, 6] was 0.058 and that for [7, 20]
was 0.165. From CORPUS, the average probabilities in the
training data for each distortion in [4, 6] were higher than
those for each distortion in [7, 20]. However, the converse
was true for the comparison between the two average prob-
abilities for the outbound model. This is because the sum
of probabilities for distortions from 7 and above was larger
than the sum of probabilities for distortions from 4 to 6 in the
training data. This comparison indicates that the 9-CLASS
outbound model could not appropriately learn the effects of
large distances for JE translation.
together.
There are word reordering constraint methods
using ITG (Wu, 1997) for phrase-based SMT
(Zens et al., 2004; Yamamoto et al., 2008; Feng et
al., 2010). These methods consider sentence level
consistency with respect to ITG. The ITG con-
straint does not consider distances of reordering
and was used with other distortion models. Our
distortion model does not consider sentence level
consistency, so our distortion model and ITG con-
straint methods are thought to be complementary.
There are tree-based SMT methods (Chiang,
2007; Galley et al., 2004; Liu et al., 2006). In
many cases, tree-based SMT methods do not use
the distortion models that consider reordering dis-
tance apart from translation rules because it is not
trivial to use distortion scores considering the dis-
tances for decoders that do not generate hypothe-
ses from left to right. If it could be applied to these
methods, our distortion model might contribute to
tree-based SMT methods. Investigating the effects
will be for future work.
</bodyText>
<sectionHeader confidence="0.998211" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999511">
This paper described our distortion models for
phrase-based SMT. Our sequence model simply
consists of only one probabilistic model, but it can
consider rich context. Experiments indicate that
our models achieved better performance and the
sequence model could learn the effect of distances
appropriately. Since our models do not require a
parser, they can be applied to many languages. Fu-
ture work includes application to other language
pairs, incorporation into ITG constraint methods
and other reordering methods, and application to
tree-based SMT methods.
</bodyText>
<sectionHeader confidence="0.979384" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.975640428571429">
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 529–536, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput.
Linguist., 22(1):39–71, March.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical report.
</reference>
<page confidence="0.827998">
163
</page>
<reference confidence="0.999796580357143">
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Theodoros Evgniou and Massimiliano Pontil. 2002.
Learning preference relations from data. Neural
Nets Lecture Notes in Computer Science, 2486:23–
32.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010.
An efficient shift-reduce decoding algorithm for
phrased-based machine translation. In Coling 2010:
Posters, pages 285–293, Beijing, China, August.
Coling 2010 Organizing Committee.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848–856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation
rule? In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 273–280, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR-9, pages 559–578.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 867–875, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of 18th International Conference on Machine Learn-
ing, pages 282–289.
D.C. Liu and J. Nocedal. 1989. On the limited memory
method for large scale optimization. Mathematical
Programming B, 45(3):503–528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609–616, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Yizhao Ni, Craig Saunders, Sandor Szedmak, and Ma-
hesan Niranjan. 2009. Handling phrase reorder-
ings for machine translation. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
241–244, Suntec, Singapore, August. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Short Papers, pages 101–
104, Boston, Massachusetts, USA, May 2 - May 7.
Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of Coling 2004, pages 508–
514, Geneva, Switzerland, Aug 23–Aug 27. COL-
ING.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 902–911, Jeju
Island, Korea, July. Association for Computational
Linguistics.
</reference>
<page confidence="0.616328">
164
</page>
<reference confidence="0.999488777777778">
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of 39th Annual Meeting of the Association for Com-
putational Linguistics, pages 523–530, Toulouse,
France, July. Association for Computational Lin-
guistics.
Hirofumi Yamamoto, Hideo Okuma, and Eiichiro
Sumita. 2008. Imposing constraints from the source
tree on ITG constraints for SMT. In Proceedings of
the ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
1–9, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering constraints for
phrase-based statistical machine translation. In Pro-
ceedings of Coling 2004, pages 205–211, Geneva,
Switzerland, Aug 23–Aug 27. COLING.
</reference>
<page confidence="0.944763">
165
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.576085">
<title confidence="0.901311">Distortion Model Considering Rich for Statistical Machine Translation Institute of Information and Communications</title>
<email confidence="0.8575905">eiichiro.sumita,kuro@i.kyoto-u.ac.jp</email>
<abstract confidence="0.998211181818182">This paper proposes new distortion models for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>529--536</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="9491" citStr="Al-Onaizan and Papineni (2006)" startWordPosition="1598" endWordPosition="1601">r phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP and its context. Their model does not simultaneously consider both the word specified at the CP and the word specified at the NPCs. Green et al. (2010) proposed distortion models that used context. Their model (the outbound model) estimates how far the NP should be from the CP using the word at the CP and its context</context>
<context position="12407" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="2123" endWordPosition="2126">he model parameters are learned to maximize the distortion probability of the NP among all of the NPCs J in each source sentence. This learning strategy is a kind of preference relation learning (Evgniou and Pontil, 2002). In this learning, the that estimates reverse direction distance. Each NPC is regarded as an NP, and the inbound model estimates how far the corresponding CP should be from the NP using the word at the NP and its context. 157 distortion probability of the actual NP will be relatively higher than those of all the other NPCs J. This learning strategy is different from that of (Al-Onaizan and Papineni, 2006; Green et al., 2010). For example, Green et al. (2010) trained their outbound model subject to &amp;EC P(Y = c|i, S) = 1, where C is the set of the nine distortion classes6 and Y is the random variable of the correct distortion class that the correct distortion is classified into. Distortion is defined as j − i − 1. Namely, the model probabilities that they learned were the probabilities of distortion classes in all of the training data, not the relative preferences among the NPCs in each source sentence. 3.2 Pair Model The pair model utilizes the word at the CP, the word at an NPC, and the conte</context>
</contexts>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 529–536, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Comput. Linguist.,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="13249" citStr="Berger et al., 1996" startWordPosition="2276" endWordPosition="2279">on class that the correct distortion is classified into. Distortion is defined as j − i − 1. Namely, the model probabilities that they learned were the probabilities of distortion classes in all of the training data, not the relative preferences among the NPCs in each source sentence. 3.2 Pair Model The pair model utilizes the word at the CP, the word at an NPC, and the context of the CP and the NPC simultaneously to estimate the NP. This can be done by our distortion model definition and the learning strategy described in the previous section. In this work, we use the maximum entropy method (Berger et al., 1996) as a discriminative machine learning method. The reason for this is that a model based on the maximum entropy method can calculate probabilities. However, if we use scores as an approximation of the distortion probabilities, various discriminative machine learning methods can be applied to build the distortion model. Let s be a source word and sn1 = s1s2...sn be a source sentence. We add a beginning of sentence (BOS) marker to the head of the source sentence and an end of sentence (EOS) marker to the end, so the source sentence S is expressed as sn+1 0 (s0 = BOS, sn+1 = EOS). Our distortion m</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Comput. Linguist., 22(1):39–71, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report.</tech>
<contexts>
<context position="27654" citStr="Chen and Rosenfeld, 1999" startWordPosition="4955" endWordPosition="4958">s from the data used to build the translation model as the training data for our distortion models. The features that were selected and used were the ones that had been counted12, using the feature templates in Table 1, at least four times for all of the (i, j) position pairs in the training sentences. We conjoined the features with three types of label pairs (C, I), (I, N), or (C, N) as instances of the feature template (li, lj) to produce features for SEQUENCE. The L-BFGS method (Liu and Nocedal, 1989) was used to estimate the weight parameters of maximum entropy models. The Gaussian prior (Chen and Rosenfeld, 1999) was used for smoothing. 4.3 Training for the Compared Models For 9-CLASS, we used the same training data as for our distortion models. Let ti be the part of speech of si. We used the following feature templates to produce features for the outbound model: (si−2), (si−1), (si), (si+1), (si+2), (ti), (ti−1, ti), (ti, ti+1), and (si, ti). These feature templates correspond to the components of the feature templates of our distortion models. In addition to these features, we used a feature consisting of the relative source sentence position as the feature used by (Green et al., 2010). The relative</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1731" citStr="Chiang, 2007" startWordPosition="252" endWordPosition="254"> for Chinese-English translation compared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, obtaining syntax requires a syntactic parser, which is not available for many languages. Phrase-based SMT (Koehn et al., 2007) is a widely used SMT method that does not use a parser. Phrase-based SMT mainly1 estimates word reordering using distortion models2. Therefore, distortion models are one of the most important components for phrase-based SMT. On the other hand, there are methods other than distortion models for improving word reordering for </context>
<context position="31418" citStr="Chiang, 2007" startWordPosition="5568" endWordPosition="5569">ot be produced with a small distortion limit, since the distortion limit prunes the search space that does not meet the constraint. Therefore, a large distortion limit is required to translate correctly. For JE translation, our SEQUENCE achieved significantly better results at distortion limits of 20 and 30 than that at a distortion limit of 10, while the baseline systems of LINEAR, LINEAR+LEX, and 9-CLASS did not achieve this. This indicate that SEQUENCE could treat long distance reordering candidates more appropriately than the compared methods. We also tested hierarchical phrase-based SMT (Chiang, 2007) (HIER) using the Moses implementation. The common data was used to train HIER. We used unlimited max-chart-span for the system setting. Results are given in Table 4. Our SEQUENCE outperformed HIER. The gain for JE was large but the gain for CE was modest. Since phrase-based SMT is generally faster in decoding speed than hierarchical phrase-based SMT, achieving better or comparable scores is worthDistortion Figure 5: Average probabilities for large distortion for Japanese-English translation. while. To investigate the tolerance for sparsity of the training data, we reduced the training data fo</context>
<context position="36253" citStr="Chiang, 2007" startWordPosition="6372" endWordPosition="6373">ould not appropriately learn the effects of large distances for JE translation. together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to use distortion scores considering the distances for decoders that do not generate hypotheses from left to right. If it could be applied to these methods, our distortion model might contribute to tree-based SMT methods. Investigating the effects will be for future work. 6 Conclusion This paper described our distortion models for phrase-based SMT. Our sequence model simply consists of only one </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theodoros Evgniou</author>
<author>Massimiliano Pontil</author>
</authors>
<title>Learning preference relations from data.</title>
<date>2002</date>
<journal>Neural Nets Lecture Notes in Computer Science,</journal>
<volume>2486</volume>
<pages>32</pages>
<contexts>
<context position="11999" citStr="Evgniou and Pontil, 2002" startWordPosition="2050" endWordPosition="2053"> which is the probability of an NPC j being the NP. Our distortion model is defined as the model calculating the distortion probability. Next, we explain the learning strategy for our distortion model. We train this model as a discriminative model that discriminates the NP from NPCs. Let J be a set of word positions in 5 other than i. We train the distortion model subject to � P(X = j|i, 5) = 1. jEJ The model parameters are learned to maximize the distortion probability of the NP among all of the NPCs J in each source sentence. This learning strategy is a kind of preference relation learning (Evgniou and Pontil, 2002). In this learning, the that estimates reverse direction distance. Each NPC is regarded as an NP, and the inbound model estimates how far the corresponding CP should be from the NP using the word at the NP and its context. 157 distortion probability of the actual NP will be relatively higher than those of all the other NPCs J. This learning strategy is different from that of (Al-Onaizan and Papineni, 2006; Green et al., 2010). For example, Green et al. (2010) trained their outbound model subject to &amp;EC P(Y = c|i, S) = 1, where C is the set of the nine distortion classes6 and Y is the random va</context>
</contexts>
<marker>Evgniou, Pontil, 2002</marker>
<rawString>Theodoros Evgniou and Massimiliano Pontil. 2002. Learning preference relations from data. Neural Nets Lecture Notes in Computer Science, 2486:23– 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Feng</author>
<author>Haitao Mi</author>
<author>Yang Liu</author>
<author>Qun Liu</author>
</authors>
<title>An efficient shift-reduce decoding algorithm for phrased-based machine translation.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>285--293</pages>
<location>Beijing, China,</location>
<contexts>
<context position="35879" citStr="Feng et al., 2010" startWordPosition="6313" endWordPosition="6316"> than those for each distortion in [7, 20]. However, the converse was true for the comparison between the two average probabilities for the outbound model. This is because the sum of probabilities for distortions from 7 and above was larger than the sum of probabilities for distortions from 4 to 6 in the training data. This comparison indicates that the 9-CLASS outbound model could not appropriately learn the effects of large distances for JE translation. together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to use distortion scores</context>
</contexts>
<marker>Feng, Mi, Liu, Liu, 2010</marker>
<rawString>Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An efficient shift-reduce decoding algorithm for phrased-based machine translation. In Coling 2010: Posters, pages 285–293, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>848--856</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="9199" citStr="Galley and Manning, 2008" startWordPosition="1554" endWordPosition="1557">ate the NP, the following should be considered simultaneously: the word at the NP, the word at the CP, the relative word order among the NPCs, the words surrounding NP and CP (context), and the words between the CP and the NPC. There are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP and its context. Their mod</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848–856, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>273--280</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="36274" citStr="Galley et al., 2004" startWordPosition="6374" endWordPosition="6377">priately learn the effects of large distances for JE translation. together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to use distortion scores considering the distances for decoders that do not generate hypotheses from left to right. If it could be applied to these methods, our distortion model might contribute to tree-based SMT methods. Investigating the effects will be for future work. 6 Conclusion This paper described our distortion models for phrase-based SMT. Our sequence model simply consists of only one probabilistic model, </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 273–280, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Bin Lu</author>
<author>Ka Po Chow</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Overview of the patent machine translation task at the NTCIR-9 workshop.</title>
<date>2011</date>
<booktitle>In Proceedings of NTCIR-9,</booktitle>
<pages>559--578</pages>
<contexts>
<context position="4620" citStr="Goto et al., 2011" startWordPosition="714" endWordPosition="717">Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics () nou1 ae2 wa3 pa4 5 on6 wo7 8 he bought books in Paris yesterday W P W Target: books he bought in Paris yesterday Figure 1: An example of left-to-right translation for Japanese-English. Boxes represent phrases and arrows indicate the translation order of the phrases. candidates. Our model consists of one probabilistic model and does not require a parser. Experiments confirmed the effectiveness of our method for Japanese-English and Chinese-English translation, using NTCIR-9 Patent Machine Translation Task data sets (Goto et al., 2011). 2 Distortion Model for Phrase-Based SMT A Moses-style phrase-based SMT generates target hypotheses sequentially from left to right. Therefore, the role of the distortion model is to estimate the source phrase position to be translated next whose target side phrase will be located immediately to the right of the already generated hypotheses. An example is shown in Figure 1. In Figure 1, we assume that only the kare wa (English side: “he”) has been translated. The target word to be generated next will be “bought” and the source word to be selected next will be its corresponding Japanese word k</context>
<context position="23918" citStr="Goto et al., 2011" startWordPosition="4345" endWordPosition="4348"> target words aligned to the source words sequentially from left to right (target side arrows). Then, the order of the source words in the target word order is decided (source side arrows). The source sentence and the source side arrows are the training data. 4 Experiment In order to confirm the effects of our distortion model, we conducted a series of Japanese to English (JE) and Chinese to English (CE) translation experiments.8 4.1 Common Settings We used the patent data for the Japanese to English and Chinese to English translation subtasks from the NTCIR-9 Patent Machine Translation Task (Goto et al., 2011). There were 2,000 sentences for the test data and 2,000 sentences for the development data. Mecab9 was used for the Japanese morphological analysis. The Stanford segmenter10 and tagger11 were used for Chinese segmentation and POS tagging. The translation model was trained using sentences of 40 words or less from the training data. So approximately 2.05 million sentence pairs consisting of approximately 54 million 8We conducted JE and CE translation as examples of language pairs with different word orders and of languages where there is a great need for translation into English. 9http://mecab.</context>
</contexts>
<marker>Goto, Lu, Chow, Sumita, Tsou, 2011</marker>
<rawString>Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K. Tsou. 2011. Overview of the patent machine translation task at the NTCIR-9 workshop. In Proceedings of NTCIR-9, pages 559–578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved models of distortion cost for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>867--875</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="6075" citStr="Green et al., 2010" startWordPosition="966" endWordPosition="969">ition (CP) and next position (NP) in the source sentence. CP is the source sentence position corresponding to the rightmost aligned target word in the generated target word sequence. NP is the source sentence position corresponding to the leftmost aligned target word in the target phrase to be generated next. The task of the distortion model is to estimate the NP3 from NP candidates (NPCs) for each CP in the source sentence.4 3NP is not always one position, because there may be multiple correct hypotheses. 4This definition is slightly different from that of existing methods such as Moses and (Green et al., 2010). In existing methods, CP is the rightmost position of the last translated source phrase and NP is the leftmost position of the source phrase to be translated next. Note that existing methods do kinou1 kare2 wa3 pari4 de5 ni6 satsu7 hon8 wo9 katt10 he bought two books in Paris yesterday kinou1 kare2 wa3 hon4 wo5 karita6 ga7 kanojo8 wa9 ktt10 he borrowed books yesterday but she bought (d)�� � kinou1 kare2 wa3 hon4 wo5 katta6 ga7 kanojo8 wa9 kit10 he bought books yesterday but she borrd (e) kinou1 kare2 wa3 kanojo4 ga5 katta6 hon7 wo8 karit9 yesterday he borrowed the books that she bouht Figure </context>
<context position="9924" citStr="Green et al. (2010)" startWordPosition="1680" endWordPosition="1683">s), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP and its context. Their model does not simultaneously consider both the word specified at the CP and the word specified at the NPCs. Green et al. (2010) proposed distortion models that used context. Their model (the outbound model) estimates how far the NP should be from the CP using the word at the CP and its context.5 Their model does not simultaneously con5They also proposed another model (the inbound model) sider both the word specified at the CP and the word specified at an NPC. For example, the outbound model considers the word specified at the CP, but does not consider the word specified at an NPC. Their models also do not consider relative word order. In contrast, our distortion model solves the aforementioned problems. Our distortion</context>
<context position="12428" citStr="Green et al., 2010" startWordPosition="2127" endWordPosition="2130"> to maximize the distortion probability of the NP among all of the NPCs J in each source sentence. This learning strategy is a kind of preference relation learning (Evgniou and Pontil, 2002). In this learning, the that estimates reverse direction distance. Each NPC is regarded as an NP, and the inbound model estimates how far the corresponding CP should be from the NP using the word at the NP and its context. 157 distortion probability of the actual NP will be relatively higher than those of all the other NPCs J. This learning strategy is different from that of (Al-Onaizan and Papineni, 2006; Green et al., 2010). For example, Green et al. (2010) trained their outbound model subject to &amp;EC P(Y = c|i, S) = 1, where C is the set of the nine distortion classes6 and Y is the random variable of the correct distortion class that the correct distortion is classified into. Distortion is defined as j − i − 1. Namely, the model probabilities that they learned were the probabilities of distortion classes in all of the training data, not the relative preferences among the NPCs in each source sentence. 3.2 Pair Model The pair model utilizes the word at the CP, the word at an NPC, and the context of the CP and the </context>
<context position="14205" citStr="Green et al., 2010" startWordPosition="2488" endWordPosition="2491"> word and sn1 = s1s2...sn be a source sentence. We add a beginning of sentence (BOS) marker to the head of the source sentence and an end of sentence (EOS) marker to the end, so the source sentence S is expressed as sn+1 0 (s0 = BOS, sn+1 = EOS). Our distortion model calculates the distortion probability for an NPC j E {j|1 &lt; j &lt; n + 1 n j =7� i} for each CP i E {i|0 &lt; i &lt; n} 1exp (wTf (i, j, S, o, d)) P(X = j|i, S) = Zi (1) where 0 (i &lt; j) d = �� 0 (|j − i |= 1) 5), o =1 (i &gt; j) , �� 1 (2 ≤ |j − i |≤ { 2 (6 ≤ |j − i|) � 6(−∞, −8], [−7, −5], [−4, −3], −2, 0, 1, [2, 3], [4, 6], and [7, ∞). In (Green et al., 2010), −1 was used as one of distortion classes. However, −1 represents the CP in our definition, and CP is not an NPC. Thus, we shifted all of the distortion classes for negative distortions by −1. Template ⟨o⟩, ⟨o, sp⟩1, ⟨o, ti⟩, ⟨o, tj⟩, ⟨o, d⟩, ⟨o, sp, sq⟩2, ⟨o, ti, tj⟩, ⟨o, ti−1, ti, tj⟩, ⟨o, ti, ti+1, tj⟩, ⟨o,ti, tj−1, tj⟩, ⟨o, ti, tj, tj+1⟩, ⟨o, si, ti, tj⟩, ⟨o, sj, ti, tj⟩ 1 p ∈ {p|i − 2 ≤ p ≤ i + 2 ∨ j − 2 ≤ p ≤ j + 2} 2 (p, q) ∈ {(p, q)|i − 2 ≤ p ≤ i + 2 ∧ j − 2 ≤ q ≤ j + 2 ∧ (|p − i |≤ 1 ∨ |q − j |≤ 1)} Table 1: Feature templates. t is the part of speech of s. w is a weight parameter vec</context>
<context position="26784" citStr="Green et al., 2010" startWordPosition="4806" endWordPosition="4809">r sets. We compared systems that used a common SMT feature set from standard SMT features and different distortion model features. The common SMT feature set consists of: four translation model features, phrase penalty, word penalty, and a language model feature. The compared different distortion model features are: the linear distortion cost model feature (LINEAR), the linear distortion cost model feature and the six MSD bidirectional lexical distortion model (Koehn et al., 2005) features (LINEAR+LEX), the outbound and inbound distortion model features discriminating nine distortion classes (Green et al., 2010) (9-CLASS), the proposed pair model feature (PAIR), and the proposed sequence model feature (SEQUENCE). 4.2 Training for the Proposed Models Our distortion model was trained as follows: We used 0.2 million sentence pairs and their word alignments from the data used to build the translation model as the training data for our distortion models. The features that were selected and used were the ones that had been counted12, using the feature templates in Table 1, at least four times for all of the (i, j) position pairs in the training sentences. We conjoined the features with three types of label</context>
<context position="28240" citStr="Green et al., 2010" startWordPosition="5056" endWordPosition="5059"> prior (Chen and Rosenfeld, 1999) was used for smoothing. 4.3 Training for the Compared Models For 9-CLASS, we used the same training data as for our distortion models. Let ti be the part of speech of si. We used the following feature templates to produce features for the outbound model: (si−2), (si−1), (si), (si+1), (si+2), (ti), (ti−1, ti), (ti, ti+1), and (si, ti). These feature templates correspond to the components of the feature templates of our distortion models. In addition to these features, we used a feature consisting of the relative source sentence position as the feature used by (Green et al., 2010). The relative source sentence position is discretized into five bins, one for each quintile of the sentence. For the inbound model13, i of the feature templates was changed to j. Features occurring four or more times in the training sentences were used. The maximum entropy method with Gaussian prior smoothing was used to estimate the model parameters. The MSD bidirectional lexical distortion model was built using all of the data used to build the translation model. 4.4 Results and Discussion We evaluated translation quality based on the caseinsensitive automatic evaluation score BLEU-4 (Papin</context>
</contexts>
<marker>Green, Galley, Manning, 2010</marker>
<rawString>Spence Green, Michel Galley, and Christopher D. Manning. 2010. Improved models of distortion cost for statistical machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 867–875, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="9172" citStr="Koehn et al., 2005" startWordPosition="1550" endWordPosition="1553">y, in order to estimate the NP, the following should be considered simultaneously: the word at the NP, the word at the CP, the relative word order among the NPCs, the words surrounding NP and CP (context), and the words between the CP and the NPC. There are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP</context>
<context position="26650" citStr="Koehn et al., 2005" startWordPosition="4787" endWordPosition="4790"> set that performed the best on the second half of the development data based on the BLEU scores from the three SMT weighting parameter sets. We compared systems that used a common SMT feature set from standard SMT features and different distortion model features. The common SMT feature set consists of: four translation model features, phrase penalty, word penalty, and a language model feature. The compared different distortion model features are: the linear distortion cost model feature (LINEAR), the linear distortion cost model feature and the six MSD bidirectional lexical distortion model (Koehn et al., 2005) features (LINEAR+LEX), the outbound and inbound distortion model features discriminating nine distortion classes (Green et al., 2010) (9-CLASS), the proposed pair model feature (PAIR), and the proposed sequence model feature (SEQUENCE). 4.2 Training for the Proposed Models Our distortion model was trained as follows: We used 0.2 million sentence pairs and their word alignments from the data used to build the translation model as the training data for our distortion models. The features that were selected and used were the ones that had been counted12, using the feature templates in Table 1, a</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of the International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2005" citStr="Koehn et al., 2007" startWordPosition="291" endWordPosition="294">g between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, obtaining syntax requires a syntactic parser, which is not available for many languages. Phrase-based SMT (Koehn et al., 2007) is a widely used SMT method that does not use a parser. Phrase-based SMT mainly1 estimates word reordering using distortion models2. Therefore, distortion models are one of the most important components for phrase-based SMT. On the other hand, there are methods other than distortion models for improving word reordering for phrase-based SMT, such as pre-ordering or reordering constraints. However, these methods also use distortion models when translating by phrase-based SMT. Therefore, distortion models do not compete against these methods and are commonly used with them. If there is a good di</context>
<context position="8948" citStr="Koehn et al., 2007" startWordPosition="1514" endWordPosition="1517">by word is not always correct. The difference is caused by the words surrounding the NPCs (context), the CP context, and the words between the CP and the NPC. Thus, these should be considered when estimating the NP. In summary, in order to estimate the NP, the following should be considered simultaneously: the word at the NP, the word at the CP, the relative word order among the NPCs, the words surrounding NP and CP (context), and the words between the CP and the NPC. There are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP</context>
<context position="25793" citStr="Koehn et al., 2007" startWordPosition="4649" endWordPosition="4652">sed to obtain word alignments. In order to reduce word alignment errors, we removed articles {a, an, the} in English and particles {ga, wo, wa} in Japanese before performing word alignments because these function words do not correspond to any words in the other languages. After word alignment, we restored the removed words and shifted the word alignment positions to the original word positions. We used 5- gram language models that were trained using the English side of each set of bilingual training data. We used an in-house standard phrase-based SMT system compatible with the Moses decoder (Koehn et al., 2007). The SMT weighting parameters were tuned by MERT (Och, 2003) using the development data. To stabilize the MERT results, we tuned three times by MERT using the first half of the development data and we selected the SMT weighting parameter set that performed the best on the second half of the development data based on the BLEU scores from the three SMT weighting parameter sets. We compared systems that used a common SMT feature set from standard SMT features and different distortion model features. The common SMT feature set consists of: four translation model features, phrase penalty, word pen</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="30179" citStr="Koehn, 2004" startWordPosition="5378" endWordPosition="5379">rtion limit 10 Japanese-English 00 10 Chinese-English 00 20 30 20 30 LINEAR 27.98 27.74 27.75 27.30 29.18 28.74 28.31 28.33 LINEAR+LEX 30.25 30.37 30.17 29.98 30.81 30.24 30.16 30.13 9-CLASS 30.74 30.98 30.92 30.75 31.80 31.56 31.31 30.84 PAIR 31.62 32.36 31.96 32.03 32.51 32.30 32.25 32.32 SEQUENCE 32.02 32.96 33.29 32.81 33.41 33.44 33.35 33.41 Table 3: Evaluation results for each method. The values are case-insensitive BLEU scores. Bold numbers indicate no significant difference from the best result in each language pair using the bootstrap resampling test at a significance level a = 0.01 (Koehn, 2004). Japanese-English Chinese-English HIER 30.47 32.66 Table 4: Evaluation results for hierarchical phrasebased SMT. of the proposed PAIR. This confirms the effectiveness for considering relative word order and words between the CP and an NPC. The proposed PAIR outperformed 9-CLASS, confirming that considering both the word specified at the CP and the word specified at the NPC simultaneously was more effective than that of 9-CLASS. For translating between languages with widely different word orders such as Japanese and English, a small distortion limit is undesirable because there are cases where</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of 18th International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="18739" citStr="Lafferty et al., 2001" startWordPosition="3359" endWordPosition="3362"> training, whereas they are conflict instances in training for the pair model. The reason is as follows. In order to make the probability of the NPC of 10 smaller than the NPC of 6, instead of making the weight parameters for the features with respect to the word at the position of 10 with label N smaller than the weight parameters for the features with respect to the word at the position of 6 with label N, the sequence model can give negative weight parameters for the features with respect to the word at the position of 6 with label I. We use a sequence discrimination technique based on CRF (Lafferty et al., 2001) to identify the label sequence that corresponds to the NP. There are two differences between our task and the CRF task. One difference is that CRF discriminates label sequences that consist of labels from all of the label candidates, whereas we constrain the label sequences to sequences where the label at the CP is C, the label at an NPC is N, and the labels between the CP and the NPC are I. The other difference is that CRF is designed for discriminating label sequences corresponding to the same object sequence, whereas we do not assign labels to words outside the spans from the CP to each NP</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of 18th International Conference on Machine Learning, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming B,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="27538" citStr="Liu and Nocedal, 1989" startWordPosition="4937" endWordPosition="4940">d Models Our distortion model was trained as follows: We used 0.2 million sentence pairs and their word alignments from the data used to build the translation model as the training data for our distortion models. The features that were selected and used were the ones that had been counted12, using the feature templates in Table 1, at least four times for all of the (i, j) position pairs in the training sentences. We conjoined the features with three types of label pairs (C, I), (I, N), or (C, N) as instances of the feature template (li, lj) to produce features for SEQUENCE. The L-BFGS method (Liu and Nocedal, 1989) was used to estimate the weight parameters of maximum entropy models. The Gaussian prior (Chen and Rosenfeld, 1999) was used for smoothing. 4.3 Training for the Compared Models For 9-CLASS, we used the same training data as for our distortion models. Let ti be the part of speech of si. We used the following feature templates to produce features for the outbound model: (si−2), (si−1), (si), (si+1), (si+2), (ti), (ti−1, ti), (ti, ti+1), and (si, ti). These feature templates correspond to the components of the feature templates of our distortion models. In addition to these features, we used a f</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D.C. Liu and J. Nocedal. 1989. On the limited memory method for large scale optimization. Mathematical Programming B, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>609--616</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="36293" citStr="Liu et al., 2006" startWordPosition="6378" endWordPosition="6381">fects of large distances for JE translation. together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to use distortion scores considering the distances for decoders that do not generate hypotheses from left to right. If it could be applied to these methods, our distortion model might contribute to tree-based SMT methods. Investigating the effects will be for future work. 6 Conclusion This paper described our distortion models for phrase-based SMT. Our sequence model simply consists of only one probabilistic model, but it can consider</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 609–616, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yizhao Ni</author>
<author>Craig Saunders</author>
<author>Sandor Szedmak</author>
<author>Mahesan Niranjan</author>
</authors>
<title>Handling phrase reorderings for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>241--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="9689" citStr="Ni et al. (2009)" startWordPosition="1637" endWordPosition="1640">n if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP and its context. Their model does not simultaneously consider both the word specified at the CP and the word specified at the NPCs. Green et al. (2010) proposed distortion models that used context. Their model (the outbound model) estimates how far the NP should be from the CP using the word at the CP and its context.5 Their model does not simultaneously con5They also proposed another model (the inbound model) sider both the word specified at the CP and the word specified at an NPC. For example, the outbound mo</context>
</contexts>
<marker>Ni, Saunders, Szedmak, Niranjan, 2009</marker>
<rawString>Yizhao Ni, Craig Saunders, Sandor Szedmak, and Mahesan Niranjan. 2009. Handling phrase reorderings for machine translation. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 241–244, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="25854" citStr="Och, 2003" startWordPosition="4662" endWordPosition="4663">s, we removed articles {a, an, the} in English and particles {ga, wo, wa} in Japanese before performing word alignments because these function words do not correspond to any words in the other languages. After word alignment, we restored the removed words and shifted the word alignment positions to the original word positions. We used 5- gram language models that were trained using the English side of each set of bilingual training data. We used an in-house standard phrase-based SMT system compatible with the Moses decoder (Koehn et al., 2007). The SMT weighting parameters were tuned by MERT (Och, 2003) using the development data. To stabilize the MERT results, we tuned three times by MERT using the first half of the development data and we selected the SMT weighting parameter set that performed the best on the second half of the development data based on the BLEU scores from the three SMT weighting parameter sets. We compared systems that used a common SMT feature set from standard SMT features and different distortion model features. The common SMT feature set consists of: four translation model features, phrase penalty, word penalty, and a language model feature. The compared different di</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="28857" citStr="Papineni et al., 2002" startWordPosition="5155" endWordPosition="5158">2010). The relative source sentence position is discretized into five bins, one for each quintile of the sentence. For the inbound model13, i of the feature templates was changed to j. Features occurring four or more times in the training sentences were used. The maximum entropy method with Gaussian prior smoothing was used to estimate the model parameters. The MSD bidirectional lexical distortion model was built using all of the data used to build the translation model. 4.4 Results and Discussion We evaluated translation quality based on the caseinsensitive automatic evaluation score BLEU-4 (Papineni et al., 2002). We used distortion limits of 10, 20, 30, and unlimited (oo), which limited the number of words for word reordering to a maximum number. Table 3 presents our main results. The proposed SEQUENCE outperformed the baselines for both Japanese to English and Chinese to English translation. This demonstrates the effectiveness of the proposed SEQUENCE. The scores of the proposed SEQUENCE were higher than those 12When we counted features for selection, we only counted features that were from the feature templates of (si, sj), (ti, tj), (si, ti, tj), and (sj, ti, tj) in Table 1 when j was not the NP, </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004: Short Papers,</booktitle>
<volume>2</volume>
<pages>101--104</pages>
<editor>In Daniel Marcu Susan Dumais and Salim Roukos, editors,</editor>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="1566" citStr="Tillman, 2004" startWordPosition="228" endWordPosition="229">es as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, obtaining syntax requires a syntactic parser, which is not available for many languages. Phrase-based SMT (Koehn et al., 2007) is a widely used SMT method that does not use a parser. Phrase-based SMT mainly1 estimates word reordering using distortion models2. Therefore, distortion model</context>
<context position="9152" citStr="Tillman, 2004" startWordPosition="1548" endWordPosition="1549">e NP. In summary, in order to estimate the NP, the following should be considered simultaneously: the word at the NP, the word at the CP, the relative word order among the NPCs, the words surrounding NP and CP (context), and the words between the CP and the NPC. There are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost usin</context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Short Papers, pages 101– 104, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="35796" citStr="Wu, 1997" startWordPosition="6300" endWordPosition="6301">obabilities in the training data for each distortion in [4, 6] were higher than those for each distortion in [7, 20]. However, the converse was true for the comparison between the two average probabilities for the outbound model. This is because the sum of probabilities for distortions from 7 and above was larger than the sum of probabilities for distortions from 4 to 6 in the training data. This comparison indicates that the 9-CLASS outbound model could not appropriately learn the effects of large distances for JE translation. together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering dista</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical mt system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>508--514</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="1685" citStr="Xia and McCord, 2004" startWordPosition="244" endWordPosition="248">9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, obtaining syntax requires a syntactic parser, which is not available for many languages. Phrase-based SMT (Koehn et al., 2007) is a widely used SMT method that does not use a parser. Phrase-based SMT mainly1 estimates word reordering using distortion models2. Therefore, distortion models are one of the most important components for phrase-based SMT. On the other hand, there are methods other than distor</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical mt system with automatically learned rewrite patterns. In Proceedings of Coling 2004, pages 508– 514, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Modeling the translation of predicate-argument structure for smt.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>902--911</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="34358" citStr="Xiong et al. (2012)" startWordPosition="6065" endWordPosition="6068">ass feature used in the model was the same (e.g., distortions from 5 to 20 were the same distance class feature), PAIR produced average distortion probabilities that were almost the same. In contrast, the average distortion probabilities for SEQUENCE decreased when the lengths of the distortions increased, even if the distance class feature was the same, and this behavior was the same as that of CORPUS. This confirms that the proposed SEQUENCE could learn the effect of distances appropriately from the training data.16 5 Related Works We discuss related works other than discussed in Section 2. Xiong et al. (2012) proposed a model predicting the orientation of an argument with respect to its verb using a parser. Syntactic structures and predicate-argument structures are useful for reordering. However, orientations do not handle distances. Thus, our distortion model does not compete against the methods predicting orientations using a parser and would assist them if used 16We also checked the average distortion probabilities for the 9-CLASS outbound model in the Japanese test sentences for Japanese-English translation. We averaged the average probabilities for distortions in a distortion span of [4, 6] a</context>
</contexts>
<marker>Xiong, Zhang, Li, 2012</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Modeling the translation of predicate-argument structure for smt. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 902–911, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Toulouse, France,</location>
<contexts>
<context position="1779" citStr="Yamada and Knight, 2001" startWordPosition="258" endWordPosition="261">ared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, obtaining syntax requires a syntactic parser, which is not available for many languages. Phrase-based SMT (Koehn et al., 2007) is a widely used SMT method that does not use a parser. Phrase-based SMT mainly1 estimates word reordering using distortion models2. Therefore, distortion models are one of the most important components for phrase-based SMT. On the other hand, there are methods other than distortion models for improving word reordering for phrase-based SMT, such as pre-ordering or reorde</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 523–530, Toulouse, France, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirofumi Yamamoto</author>
<author>Hideo Okuma</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Imposing constraints from the source tree on ITG constraints for SMT.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2),</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="35859" citStr="Yamamoto et al., 2008" startWordPosition="6309" endWordPosition="6312">n in [4, 6] were higher than those for each distortion in [7, 20]. However, the converse was true for the comparison between the two average probabilities for the outbound model. This is because the sum of probabilities for distortions from 7 and above was larger than the sum of probabilities for distortions from 4 to 6 in the training data. This comparison indicates that the 9-CLASS outbound model could not appropriately learn the effects of large distances for JE translation. together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to u</context>
</contexts>
<marker>Yamamoto, Okuma, Sumita, 2008</marker>
<rawString>Hirofumi Yamamoto, Hideo Okuma, and Eiichiro Sumita. 2008. Imposing constraints from the source tree on ITG constraints for SMT. In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 1–9, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Reordering constraints for phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>205--211</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="1648" citStr="Zens et al., 2004" startWordPosition="239" endWordPosition="242">experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, obtaining syntax requires a syntactic parser, which is not available for many languages. Phrase-based SMT (Koehn et al., 2007) is a widely used SMT method that does not use a parser. Phrase-based SMT mainly1 estimates word reordering using distortion models2. Therefore, distortion models are one of the most important components for phrase-based SMT. On the other hand</context>
<context position="35836" citStr="Zens et al., 2004" startWordPosition="6305" endWordPosition="6308"> for each distortion in [4, 6] were higher than those for each distortion in [7, 20]. However, the converse was true for the comparison between the two average probabilities for the outbound model. This is because the sum of probabilities for distortions from 7 and above was larger than the sum of probabilities for distortions from 4 to 6 in the training data. This comparison indicates that the 9-CLASS outbound model could not appropriately learn the effects of large distances for JE translation. together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because</context>
</contexts>
<marker>Zens, Ney, Watanabe, Sumita, 2004</marker>
<rawString>Richard Zens, Hermann Ney, Taro Watanabe, and Eiichiro Sumita. 2004. Reordering constraints for phrase-based statistical machine translation. In Proceedings of Coling 2004, pages 205–211, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>