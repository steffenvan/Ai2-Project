<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000577">
<title confidence="0.989292">
Collective Classification of Congressional Floor-Debate Transcripts
</title>
<author confidence="0.998817">
Clinton Burfoot, Steven Bird and Timothy Baldwin
</author>
<affiliation confidence="0.99956">
Department of Computer Science and Software Engineering
University of Melbourne, VIC 3010, Australia
</affiliation>
<email confidence="0.996897">
{cburfoot, sb, tim}@csse.unimelb.edu.au
</email>
<sectionHeader confidence="0.995631" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999626692307692">
This paper explores approaches to sentiment
classification of U.S. Congressional floor-
debate transcripts. Collective classification
techniques are used to take advantage of the
informal citation structure present in the de-
bates. We use a range of methods based on
local and global formulations and introduce
novel approaches for incorporating the outputs
of machine learners into collective classifica-
tion algorithms. Our experimental evaluation
shows that the mean-field algorithm obtains
the best results for the task, significantly out-
performing the benchmark technique.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925630434783">
Supervised document classification is a well-studied
task. Research has been performed across many
document types with a variety of classification tasks.
Examples are topic classification of newswire ar-
ticles (Yang and Liu, 1999), sentiment classifica-
tion of movie reviews (Pang et al., 2002), and satire
classification of news articles (Burfoot and Baldwin,
2009). This and other work has established the use-
fulness of document classifiers as stand-alone sys-
tems and as components of broader NLP systems.
This paper deals with methods relevant to super-
vised document classification in domains with net-
work structures, where collective classification can
yield better performance than approaches that con-
sider documents in isolation. Simply put, a network
structure is any set of relationships between docu-
ments that can be used to assist the document clas-
sification process. Web encyclopedias and scholarly
publications are two examples of document domains
where network structures have been used to assist
classification (Gantner and Schmidt-Thieme, 2009;
Cao and Gao, 2005).
The contribution of this research is in four parts:
(1) we introduce an approach that gives better than
state of the art performance for collective classifica-
tion on the ConVote corpus of congressional debate
transcripts (Thomas et al., 2006); (2) we provide a
comparative overview of collective document classi-
fication techniques to assist researchers in choosing
an algorithm for collective document classification
tasks; (3) we demonstrate effective novel approaches
for incorporating the outputs of SVM classifiers into
collective classifiers; and (4) we demonstrate effec-
tive novel feature models for iterative local classifi-
cation of debate transcript data.
In the next section (Section 2) we provide a for-
mal definition of collective classification and de-
scribe the ConVote corpus that is the basis for our
experimental evaluation. Subsequently, we describe
and critique the established benchmark approach for
congressional floor-debate transcript classification,
before describing approaches based on three alterna-
tive collective classification algorithms (Section 3).
We then present an experimental evaluation (Sec-
tion 4). Finally, we describe related work (Section 5)
and offer analysis and conclusions (Section 6).
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="method">
2 Task Definition
</sectionHeader>
<subsectionHeader confidence="0.982739">
2.1 Collective Classification
</subsectionHeader>
<bodyText confidence="0.997817">
Given a network and an object o in the network,
there are three types of correlations that can be used
</bodyText>
<page confidence="0.926342">
1506
</page>
<note confidence="0.9791815">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1506–1515,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.992064288888889">
to infer a label for o: (1) the correlations between 3 Collective Classification Techniques
the label of o and its observed attributes; (2) the cor- In this section we describe techniques for perform-
relations between the label of o and the observed at- ing collective classification on the ConVote cor-
tributes and labels of nodes connected to o; and (3) pus. We differentiate between dual-classifier and
the correlations between the label of o and the un- iterative-classifier approaches.
observed labels of objects connected to o (Sen et al., Dual-classifier approach: This approach uses
2008). a collective classification algorithm that takes inputs
Standard approaches to classification generally from two classifiers: (1) a content-only classifier that
ignore any network information and only take into determines the likelihood of a y or n label for an in-
account the correlations in (1). Each object is clas- stance given its text content; and (2) a citation clas-
sified as an individual instance with features derived sifier that determines, based on citation information,
from its observed attributes. Collective classification whether a given pair of instances are “same class” or
takes advantage of the network by using all three “different class”.
sources. Instances may have features derived from Let T denote a set of functions representing the
their source objects or from other objects. Classifi- classification preferences produced by the content-
cation proceeds in a joint fashion so that the label only and citation classifiers:
given to each instance takes into account the labels • For each Vi E V, Oi E T is a function Oi: L �
given to all of the other instances. R+ U {0}.
Formally, collective classification takes a graph, • For each (Vi, Vj) E E, Oij E T is a function
made up of nodes V = {V1, ... , Vn} and edges 0ij: L x L —* R+ U {0}.
E. The task is to label the nodes Vi E V from Later in this section we will describe three collec-
a label set L = {L1, ... , Lq}, making use of the tive classification algorithms capable of performing
graph in the form of a neighborhood function N = overall classification based on these inputs: (1) the
{N1,. . . , Nn}, where Ni C V \ {Vi}. minimum-cut approach, which is the benchmark for
2.2 The ConVote Corpus collective classification with ConVote, established
ConVote, compiled by Thomas et al. (2006), is a by Thomas et al.; (2) loopy belief propagation; and
corpus of U.S. congressional debate transcripts. It (3) mean-field. We will show that these latter two
consists of 3,857 speeches organized into 53 debates techniques, which are both approximate solutions
on specific pieces of legislation. Each speech is for Markov random fields, are superior to minimum-
tagged with the identity of the speaker and a “for” cut for the task.
or “against” label derived from congressional voting Figure 2 gives a visual overview of the dual-
records. In addition, places where one speaker cites classifier approach.
another have been annotated, as shown in Figure 1. Iterative-classifier approach: This approach
We apply collective classification to ConVote de- incorporates content-only and citation features into
bates by letting V refer to the individual speakers in a a single local classifier that works on the assump-
debate and populating N using the citation graph be- tion that correct neighbor labels are already known.
tween speakers. We set L = {y, n}, corresponding This approach represents a marked deviation from
to “for” and “against” votes respectively. The text the dual-classifier approach and offers unique ad-
of each instance is the concatenation of the speeches vantages. It is fully described in Section 3.4.
by a speaker within a debate. This results in a corpus Figure 3 gives a visual overview of the iterative-
of 1,699 instances with a roughly even class distri- classifier approach.
bution. Approximately 70% of these are connected, For a detailed introduction to collective classifica-
i.e. they are the source or target of one or more cita- tion see Sen et al. (2008).
tions. The remainder are isolated.
1507
</bodyText>
<figure confidence="0.7770485">
Debate 006
Speaker 400378 [against]
Mr. Speaker, ... all over Washington and in the country, people are talking today about the
majority’s last-minute decision to abandon ...
. . .
Speaker 400115 [for]
. . .
Mr. Speaker, ... I just want to say to the gentlewoman from New York that every single member
of this institution ...
. . .
</figure>
<figureCaption confidence="0.9997185">
Figure 1: Sample speech fragments from the ConVote corpus. The phrase gentlewoman from New York by speaker
400115 is annotated as a reference to speaker 400378.
</figureCaption>
<figure confidence="0.951377714285714">
Debate content
Content-only and
citation scores
MF/LBP/Mincut
Content-only classifications
Extract features Extract features
Content-only vectors
SVM
Normalise
Citation classifications
Citation vectors
Normalise
SVM
in the debate.
</figure>
<bodyText confidence="0.969854333333333">
The decision plane distance computed by the
content-only SVM is normalized to a positive real
number and stripped of outliers:
</bodyText>
<equation confidence="0.94586025">
φi(y) = { 1 di &gt; 2σi;
0 di &lt; −2σi
(1 + iii) /2 |di |≤ 2σi;
Overall classifications
</equation>
<figureCaption confidence="0.9999375">
Figure 2: Dual-classifier approach.
Figure 3: Iterative-classifier approach.
</figureCaption>
<subsectionHeader confidence="0.9974265">
3.1 Dual-classifier Approach with
Minimum-cut
</subsectionHeader>
<bodyText confidence="0.946156391304348">
Thomas et al. use linear kernel SVMs as their base
classifiers. The content-only classifier is trained to
predict y or n based on the unigram presence fea-
tures found in speeches. The citation classifier is
trained to predict “same class” or “different class”
labels based on the unigram presence features found
in the context windows (30 tokens before, 20 tokens
after) surrounding citations for each pair of speakers
where σi is the standard deviation of the decision
plane distance, di, over all of the instances in the
debate and φi(n) = 1−φi(y). The citation classifier
output is processed similarly:1
{ 0 dij &lt; θ;
α · dij/4σij θ ≤ dij ≤ 4σij;
α dij &gt; 4σij
where σij is the standard deviation of the decision
plane distance, dij over all of the citations in the de-
bate and ψij(n, n) = ψij(y, y). The α and θ vari-
ables are free parameters.
A given class assignment v is assigned a cost that
is the sum of per-instance and per-pair class costs
derived from the content-only and citation classifiers
respectively:
</bodyText>
<equation confidence="0.9927405">
Ec(v) = φi(¯vi) + E ψij(vi, vi)
ViEV (Vi,Vj)EE:vi7�vj
</equation>
<bodyText confidence="0.919487">
where vi is the label of node Vi and ¯vi denotes the
complement class of vi.
</bodyText>
<footnote confidence="0.996996">
1Thomas et al. classify each citation context window sep-
arately, so their 0 values are actually calculated in a slightly
more complicated way. We adopted the present approach for
conceptual simplicity and because it gave superior performance
in preliminary experiments.
</footnote>
<figure confidence="0.977727416666667">
Debate content Extract features
SVM
Content-only vectors
Content-only classifications Combine content-only
and citation features
Update citation features
Local classifications
Terminate iteration
Local vectors
SVM
Overall classifications
ψij(y, y) =
</figure>
<page confidence="0.98089">
1508
</page>
<bodyText confidence="0.999976396551724">
The cost function is modeled in a flow graph
where extra source and sink nodes represent the y
and n labels respectively. Each node in V is con-
nected to the source and sink with capacities φi(y)
and φi(n) respectively. Pairs classified in the “same
class” class are linked with capacities defined by ψ.
An exact optimum and corresponding overall
classification is efficiently computed by finding the
minimum-cut of the flow graph (Blum and Chawla,
2001). The free parameters are tuned on a set of
held-out data.
Thomas et al. demonstrate improvements over
content-only classification, without attempting to
show that the approach does better than any alter-
natives; the main appeal is the simplicity of the flow
graph model. There are a number of theoretical lim-
itations to the approach, which we now discuss.
As Thomas et al. point out, the model has no way
of representing the “different class” output from the
citation classifier and these citations must be dis-
carded. This, to us, is the most significant problem
with the model. Inspection of the corpus shows that
approximately 80% of citations indicate agreement,
meaning that for the present task the impact of dis-
carding this information may not be large. However,
the primary utility in collective approaches lies in
their ability to fill in gaps in information not picked
up by content-only classification. All available link
information should be applied to this end, so we
need models capable of accepting both positive and
negative information.
The normalization techniques used for converting
SVM outputs to graph weights are somewhat arbi-
trary. The use of standard deviations appears prob-
lematic as, intuitively, the strength of a classification
should be independent of its variance. As a case in
point, consider a set of instances in a debate all clas-
sified as similarly weak positives by the SVM. Use
of ψi as defined above would lead to these being er-
roneously assigned the maximum score because of
their low variance.
The minimum-cut approach places instances in
either the positive or negative class depending on
which side of the cut they fall on. This means
that no measure of classification confidence is avail-
able. This extra information is useful at the very
least to give a human user an idea of how much to
trust the classification. A measure of classification
confidence may also be necessary for incorporation
into a broader system, e.g., a meta-classifier (An-
dreevskaia and Bergler, 2008; Li and Zong, 2008).
Tuning the α and θ parameters is likely to become
a source of inaccuracy in cases where the tuning and
test debates have dissimilar link structures. For ex-
ample, if the tuning debates tend to have fewer, more
accurate links the α parameter will be higher. This
will not produce good results if the test debates have
more frequent, less accurate links.
</bodyText>
<subsectionHeader confidence="0.993914">
3.2 Heuristics for Improving Minimum-cut
</subsectionHeader>
<bodyText confidence="0.999965823529412">
Bansal et al. (2008) offer preliminary work describ-
ing additions to the Thomas et al. minimum-cut ap-
proach to incorporate “different class” citation clas-
sifications. They use post hoc adjustments of graph
capacities based on simple heuristics. Two of the
three approaches they trial appear to offer perfor-
mance improvements:
The SetTo heuristic: This heuristic works
through E in order and tries to force Vi and Vj into
different classes for every “different class” (dij &lt; 0)
citation classifier output where i &lt; j. It does this by
altering the four relevant content-only preferences,
φi(y), φi(n), φj(y), and φj(n). Assume without
loss of generality that the largest of these values is
φi(y). If this preference is respected, it follows that
Vj should be put into class n. Bansal et al. instanti-
ate this chain of reasoning by setting:
</bodyText>
<listItem confidence="0.9953005">
• φ0i(y) = max(β, φi(y))
• φ0j(n) = max(β, φj(n))
</listItem>
<bodyText confidence="0.99488375">
where φ0 is the replacement content-only function,
β is a free parameter E (.5,1], φ0 i(n) = 1 − φ0 i(y),
and φ0j(y) = 1 − φ0j(y).
The IncBy heuristic: This heuristic is a more
conservative version of the SetTo heuristic. Instead
of replacing the content-only preferences with fixed
constants, it increments and decrements the previous
values so they are somewhat preserved:
</bodyText>
<equation confidence="0.7625855">
• φ0i(y) = min(1,φi(y) + β)
•φ0j(n) = min(1, φj(n) + β)
</equation>
<bodyText confidence="0.999983333333333">
There are theoretical shortcomings with these ap-
proaches. The most obvious problem is the arbitrary
nature of the manipulations, which produce a flow
</bodyText>
<page confidence="0.977211">
1509
</page>
<bodyText confidence="0.9999801">
graph that has an indistinct relationship to the out-
puts of the two classifiers.
Bensal et al. trial a range of β values, with vary-
ing impacts on performance. No attempt is made to
demonstrate a method for choosing a good β value.
It is not clear that the tuning approach used to set α
and θ would be successful here. In any case, having
a third parameter to tune would make the process
more time-consuming and increase the risks of in-
correct tuning, described above.
As Bansal et al. point out, proceeding through E
in order means that earlier changes may be undone
for speakers who have multiple “different class” ci-
tations.
Finally, we note that the confidence of the cita-
tion classifier is not embodied in the graph structure.
The most marginal “different class” citation, classi-
fied just on the negative side of the decision plane, is
treated identically to the most confident one furthest
from the decision plane.
</bodyText>
<subsectionHeader confidence="0.875521">
3.3 Dual-classifier Approach with Markov
Random Field Approximations
</subsectionHeader>
<bodyText confidence="0.9999615">
A pairwise Markov random field (Taskar et al.,
2002) is given by the pair (G, IF), where G and IF
are as previously defined, IF being re-termed as a set
of clique potentials. Given an assignment v to the
nodes V, the pairwise Markov random field is asso-
ciated with the probability distribution:
</bodyText>
<equation confidence="0.986081">
1 Y
P (v) = Z
Vi∈V
where:
XZ = Y φi(v0 i) Y ψij(v0i, v0j)
v/ Vi∈V (Vi,Vj)∈E
</equation>
<bodyText confidence="0.999764533333333">
and v0 i denotes the label of Vi for an alternative as-
signment in v0.
In general, exact inference over a pairwise
Markov random field is known to be NP-hard. There
are certain conditions under which exact inference
is tractable, but real-world data is not guaranteed to
satisfy these. A class of approximate inference al-
gorithms known as variational methods (Jordan et
al., 1999) solve this problem by substituting a sim-
pler “trial” distribution which is fitted to the Markov
random field distribution.
Loopy Belief Propagation: Applied to a pair-
wise Markov random field, loopy belief propagation
is a message passing algorithm that can be concisely
expressed as the following set of equations:
</bodyText>
<equation confidence="0.998386333333333">
Xmi→j(vj) = α lψij(vi, vj)φi(vi)
vi∈L
Y mk→i(vi),bvj E Ll
Vk∈Ni∩V\Vj
Ybi(vi) = αφi(vi) mj→i(vi),bvi E L
Vj∈Ni∩V
</equation>
<bodyText confidence="0.998332888888889">
where mi→j is a message sent by Vi to Vj and α is
a normalization constant that ensures that each mes-
sage and each set of marginal probabilities sum to 1.
The algorithm proceeds by making each node com-
municate with its neighbors until the messages sta-
bilize. The marginal probability is then derived by
calculating bi(vi).
Mean-Field: The basic mean-field algorithm can
be described with the equation:
</bodyText>
<equation confidence="0.995316666666667">
Y Y ψbi(vi)
bj(vj) = αφj(vj) ij (vi, vj), vj E L
Vi∈Nj∩V vi∈L
</equation>
<bodyText confidence="0.966598294117647">
where α is a normalization constant that ensures
Pv. bj(vj) = 1. The algorithm computes the fixed
point equation for every node and continues to do so
until the marginal probabilities bj(vj) stabilize.
Mean-field can be shown to be a variational
method in the same way as loopy belief propagation,
using a simpler trial distribution. For details see Sen
et al. (2008).
Probabilistic SVM Normalisation: Unlike
minimum-cut, the Markov random field approaches
have inherent support for the “different class” out-
put of the citation classifier. This allows us to ap-
ply a more principled SVM normalisation technique.
Platt (1999) describes a technique for converting the
output of an SVM classifier to a calibrated posterior
probability. Platt finds that the posterior can be fit
using a parametric form of a sigmoid:
</bodyText>
<equation confidence="0.9965415">
1
P(y = 1|d) = 1 + exp(Ad + B)
</equation>
<bodyText confidence="0.941045666666667">
This is equivalent to assuming that the output of
the SVM is proportional to the log odds of a positive
example. Experimental analysis shows error rate is
</bodyText>
<equation confidence="0.718955">
Yφi(vi) ψij(vi, vj)
(Vi,Vj)∈E
</equation>
<page confidence="0.638642">
1510
</page>
<bodyText confidence="0.809384677419354">
improved over a plain linear SVM and probabilities Algorithm 1 Iterative Classification Algorithm
are of comparable quality to those produced using a
regularized likelihood kernel method.
By applying this technique to the base classifiers,
we can produce new, simpler Ψ functions, OZ(y) =
PZ and OZj(y, y) = PZj where PZ is the probabilis-
tic normalized output of the content-only classifier
and PZj is the probabilistic normalized output of the
citation classifier.
This approach addresses the problems with the
Thomas et al. method where the use of standard
deviations can produce skewed normalizations (see
Section 3.1). By using probabilities we also open
up the possibility of replacing the SVM classifiers
with any other model than can be made to produce
a probability. Note also that there are no parameters
to tune.
3.4 Iterative Classifier Approach
The dual-classifier approaches described above rep-
resent global attempts to solve the collective classifi-
cation problem. We can choose to narrow our focus
to the local level, in which we aim to produce the
best classification for a single instance with the as-
sumption that all other parts of the problem (i.e. the
correct labeling of the other instances) are solved.
The Iterative Classification Algorithm (Bilgic et
al., 2007), defined in Algorithm 1, is a simple tech-
nique for performing collective classification using
such a local classifier. After bootstrapping with a
content-only classifier, it repeatedly generates new
estimates for vZ based on its current knowledge of
NZ. The algorithm terminates when the predictions
stabilize or a fixed number of iterations is com-
pleted. Each iteration is completed using a newly
generated ordering O, over the instances V.
We propose three feature models for the local
classifier.
Citation presence and Citation count: Given
that the majority of citations represent the “same
class” relationship (see Section 3.1), we can an-
ticipate that content-only classification performance
will be improved if we add features to represent the
presence of neighbours of each class.
We define the function c(i, l) = EvjENinV Svj,l
giving the number of neighbors for node U with la-
bel l, where S is the Kronecker delta. We incorporate
these citation count values, one for the supporting
1511
for each node VZ E V do {bootstrapping}
compute dZ using only local attributes of node
vZ +— f(��Z)
end for
repeat {iterative classification}
randomly generate ordering O over nodes in V
for each node U E O do
{compute new estimate of vZ}
compute dZ using current assignments to ,NZ
vZ +— f(��Z)
end for
until labels have stabilized or maximum iterations
reached
class and one for the opposing class, obtaining a new
feature vector (u1Z, u2Z , ... , ujZ, c(i, y), c(i, n)) where
u1Z, u2Z,..., ujZ are the elements of iiZ, the binary un-
igram feature vector used by the content-only clas-
sifier to represent instance i.
Alternatively, we can represent neighbor labels
using binary citation presence values where any
non-zero count becomes a 1 in the feature vector.
Context window: We can adopt a more nu-
anced model for citation information if we incor-
porate the citation context window features into the
feature vector. This is, in effect, a synthesis of
the content-only and citation feature models. Con-
text window features come from the product space
L x C, where C is the set of unigrams used in ci-
tation context windows and cZ denotes the context
window features for instance i. The new feature vec-
tor becomes: (u1Z, u2Z,... , ujZ , c1 Z , c2Z , ... , ck ). This
approach implements the intuition that speakers in-
dicate their voting intentions by the words they use
to refer to speakers whose vote is known. Because
neighbor relations are bi-directional the reverse is
also true: Speakers indicate other speakers’ voting
intentions by the words they use to refer to them.
As an example, consider the context window fea-
ture AGREE-FOR, indicating the presence of the
agree unigram in the citation window I agree with
the gentleman from Louisiana, where the label for
the gentleman from Louisiana instance is y. This
feature will be correctly correlated with the y label.
Similarly, if the unigram were disagree the feature
would be correlated with the n label.
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999976842105263">
In this section we compare the performance of our
dual-classifier and iterative-classifier approaches.
We also evaluate the performance of the three fea-
ture models for local classification.
All accuracies are given as the percentages of
instances correctly classified. Results are macro-
averaged using 10 x 10-fold cross validation, i.e.
10 runs of 10-fold cross validation using different
randomly assigned data splits.
Where quoted, statistical significance has been
calculated using a two-tailed paired t-test measured
over all 100 pairs with 10 degrees of freedom. See
Bouckaert (2003) for an experimental justification
for this approach.
Note that the results presented in this section
are not directly comparable with those reported by
Thomas et al. and Bansal et al. because their exper-
iments do not use cross-validation. See Section 4.3
for further discussion of experimental configuration.
</bodyText>
<subsectionHeader confidence="0.990282">
4.1 Local Classification
</subsectionHeader>
<bodyText confidence="0.99991305">
We evaluate three models for local classification: ci-
tation presence features, citation count features and
context window features. In each case the SVM
classifier is given feature vectors with both content-
only and citation information, as described in Sec-
tion 3.4.
Table 1 shows that context window performs the
best with 89.66% accuracy, approximately 1.5%
ahead of citation count and 3.5% ahead of citation
presence. All three classifiers significantly improve
on the content-only classifier.
These relative scores seem reasonable. Knowing
the words used in citations of each class is better
than knowing the number of citations in each class,
and better still than only knowing which classes of
citations exist.
These results represent an upper-bound for the
performance of the iterative classifier, which re-
lies on iteration to produce the reliable information
about citations given here by oracle.
</bodyText>
<subsectionHeader confidence="0.989787">
4.2 Collective Classification
</subsectionHeader>
<bodyText confidence="0.991892666666667">
Table 2 shows overall results for the three collective
classification algorithms. The iterative classifier was
run separately with citation count and context win-
</bodyText>
<table confidence="0.998964833333333">
Method Accuracy (%)
Majority 52.46
Content-only 75.29
Citation presence 85.01
Citation count 88.18
Context window 89.66
</table>
<tableCaption confidence="0.979246666666667">
Table 1: Local classifier accuracy. All three local
classifiers are significant over the in-isolation classifier
(p &lt; .001).
</tableCaption>
<bodyText confidence="0.999937277777778">
dow citation features, the two best performing local
classification methods, both with a threshold of 30
iterations.
Results are shown for connected instances, iso-
lated instances, and all instances. Collective clas-
sification techniques can only have an impact on
connected instances, so these figures are most im-
portant. The figures for all instances show the per-
formance of the classifiers in our real-world task,
where both connected and isolated instances need to
be classified and the end-user may not distinguish
between the two types.
Each of the four collective classifiers outperform
the minimum-cut benchmark over connected in-
stances, with the iterative classifier (context win-
dow) (79.05%) producing the smallest gain of less
than 1% and mean-field doing best with a nearly
6% gain (84.13%). All show a statistically signif-
icant improvement over the content-only classifier.
Mean-field shows a statistically significant improve-
ment over minimum-cut.
The dual-classifier approaches based on loopy
belief propagation and mean-field do better than
the iterative-classifier approaches by an average of
about 3%.
Iterative classification performs slightly better
with citation count features than with context win-
dow features, despite the fact that the context win-
dow model performs better in the local classifier
evaluation. We speculate that this may be due to ci-
tation count performing better when given incorrect
neighbor labels. This is an aspect of local classi-
fier performance we do not otherwise measure, so a
clear conclusion is not possible. Given the closeness
of the results it is also possible that natural statistical
variation is the cause of the difference.
</bodyText>
<page confidence="0.988715">
1512
</page>
<bodyText confidence="0.999921833333333">
The performance of the minimum-cut method is
not reliably enhanced by either the SetTo or IncBy
heuristics. Only IncBy(.15) gives a very small im-
provement (0.14%) over plain minimum-cut. All
of the other combinations tried diminished perfor-
mance slightly.
</bodyText>
<subsectionHeader confidence="0.999737">
4.3 A Note on Error Propagation and
Experimental Configuration
</subsectionHeader>
<bodyText confidence="0.999990925925926">
Early in our experimental work we noticed that per-
formance often varied greatly depending on the de-
bates that were allocated to training, tuning and test-
ing. This observation is supported by the per-fold
scores that are the basis for the macro-average per-
formance figures reported in Table 2, which tend
to have large standard deviations. The absolute
standard deviations over the 100 evaluations for the
minimum-cut and mean-field methods were 11.19%
and 8.94% respectively. These were significantly
larger than the standard deviation for the content-
only baseline, which was 7.34%. This leads us to
conclude that the performance of collective classifi-
cation methods is highly variable.
Bilgic and Getoor (2008) offer a possible expla-
nation for this. They note that the cost of incor-
rectly classifying a given instance can be magnified
in collective classification, because errors are prop-
agated throughout the network. The extent to which
this happens may depend on the random interaction
between base classification accuracy and network
structure. There is scope for further work to more
fully explain this phenomenon.
From these statistical and theoretical factors we
infer that more reliable conclusions can be drawn
from collective classification experiments that use
cross-validation instead of a single, fixed data split.
</bodyText>
<sectionHeader confidence="0.999869" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.99997850877193">
Somasundaran et al. (2009) use ICA to improve sen-
timent polarity classification of dialogue acts in a
corpus of multi-party meeting transcripts. Link fea-
tures are derived from annotations giving frame re-
lations and target relations. Respectively, these re-
late dialogue acts based on the sentiment expressed
and the object towards which the sentiment is ex-
pressed. Somasundaran et al. provides another ar-
gument for the usefulness of collective classification
(specifically ICA), in this case as applied at a dia-
logue act level and relying on a complex system of
annotations for link information.
Somasundaran and Wiebe (2009) propose an un-
supervised method for classifying the stance of each
contribution to an online debate concerning the mer-
its of competing products. Concessions to other
stances are modeled, but there are no overt citations
in the data that could be used to induce the network
structure required for collective classification.
Pang and Lee (2005) use metric labeling to per-
form multi-class collective classification of movie
reviews. Metric labeling is a multi-class equiva-
lent of the minimum-cut technique in which opti-
mization is done over a cost function incorporat-
ing content-only and citation scores. Links are con-
structed between test instances and a set of k near-
est neighbors drawn only from the training set. Re-
stricting the links in this way means the optimization
problem is simple. A similarity metric is used to find
nearest neighbors.
The Pang and Lee method is an instance of im-
plicit link construction, an approach which is be-
yond the scope of this paper but nevertheless an im-
portant area for future research. A similar technique
is used in a variation on the Thomas et al. experi-
ment where additional links between speeches are
inferred via a similarity metric (Burfoot, 2008). In
cases where both citation and similarity links are
present, the overall link score is taken as the sum of
the two scores. This seems counter-intuitive, given
that the two links are unlikely to be independent. In
the framework of this research, the approach would
be to train a link meta-classifier to take scores from
both link classifiers and output an overall link prob-
ability.
Within NLP, the use of LBP has not been re-
stricted to document classification. Examples of
other applications are dependency parsing (Smith
and Eisner, 2008) and alignment (Cromires and
Kurohashi, 2009). Conditional random fields
(CRFs) are an approach based on Markov random
fields that have been popular for segmenting and
labeling sequence data (Lafferty et al., 2001). We
rejected linear-chain CRFs as a candidate approach
for our evaluation on the grounds that the arbitrar-
ily connected graphs used in collective classification
can not be fully represented in graphical format, i.e.
</bodyText>
<page confidence="0.898979">
1513
</page>
<table confidence="0.999616285714286">
Connected Isolated All
Majority 52.46 46.29 50.51
Content only 75.31 78.90 76.28
Minimum-cut 78.31 78.90 78.40
Minimum-cut (SetTo(.6)) 78.22 78.90 78.32
Minimum-cut (SetTo(.8)) 78.01 78.90 78.14
Minimum-cut (SetTo(1)) 77.71 78.90 77.93
Minimum-cut (IncBy(.05)) 78.14 78.90 78.25
Minimum-cut (IncBy(.15)) 78.45 78.90 78.46
Minimum-cut (IncBy(.25)) 78.02 78.90 78.15
Iterative-classifier (citation count) 80.07? 78.90 79.69?
Iterative-classifier (context window) 79.05 78.90 78.93
Loopy Belief Propagation 83.37† 78.90 81.93†
Mean-Field 84.12† 78.90 82.45†
</table>
<tableCaption confidence="0.900677666666667">
Table 2: Speaker classification accuracies (%) over connected, isolated and all instances. The marked results are
statistically significant over the content only benchmark (*p &lt; .01, † p &lt; .001). The mean-field results are statistically
significant over minimum-cut (p &lt; .05).
</tableCaption>
<bodyText confidence="0.795646">
linear-chain CRFs do not scale to the complexity of
graphs used in this research.
</bodyText>
<sectionHeader confidence="0.995294" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999986764705883">
By applying alternative models, we have demon-
strated the best recorded performance for collective
classification of ConVote using bag-of-words fea-
tures, beating the previous benchmark by nearly 6%.
Moreover, each of the three alternative approaches
trialed are theoretically superior to the minimum-cut
approach approach for three main reasons: (1) they
support multi-class classification; (2) they support
negative and positive citations; (3) they require no
parameter tuning.
The superior performance of the dual-classifier
approach with loopy belief propagation and mean-
field suggests that either algorithm could be consid-
ered as a first choice for collective document classi-
fication. Their advantage is increased by their abil-
ity to output classification confidences as probabili-
ties, while minimum-cut and the local formulations
only give absolute class assignments. We do not dis-
miss the iterative-classifier approach entirely. The
most compelling point in its favor is its ability to
unify content only and citation features in a single
classifier. Conceptually speaking, such an approach
should allow the two types of features to inter-relate
in more nuanced ways. A case in point comes from
our use of a fixed size context window to build a
citation classifier. Future approaches may be able
to do away with this arbitrary separation of features
by training a local classifier to consider all words in
terms of their impact on content-only classification
and their relations to neighbors.
Probabilistic SVM normalization offers a conve-
nient, principled way of incorporating the outputs of
an SVM classifier into a collective classifier. An op-
portunity for future work is to consider normaliza-
tion approaches for other classifiers. For example,
confidence-weighted linear classifiers (Dredze et al.,
2008) have been shown to give superior performance
to SVMs on a range of tasks and may therefore be a
better choice for collective document classification.
Of the three models trialled for local classifiers,
context window features did best when measured in
an oracle experiment, but citation count features did
better when used in a collective classifier. We con-
clude that context window features are a more nu-
anced and powerful approach that is also more likely
to suffer from data sparseness. Citation count fea-
tures would have been the less effective in a scenario
where the fact of the citation existing was less infor-
mative, for example, if a citation was 50% likely to
indicate agreement rather than 80% likely. There is
much scope for further research in this area.
</bodyText>
<page confidence="0.992442">
1514
</page>
<sectionHeader confidence="0.995862" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999862011111112">
Alina Andreevskaia and Sabine Bergler. 2008. When
specialists and generalists work together: Overcom-
ing domain dependence in sentiment tagging. In ACL,
pages 290–298.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The
power of negative thinking: Exploiting label disagree-
ment in the min-cut classification framework. In COL-
ING, pages 15–18.
Mustafa Bilgic and Lise Getoor. 2008. Effective label
acquisition for collective classification. In KDD, pages
43–51.
Mustafa Bilgic, Galileo Namata, and Lise Getoor. 2007.
Combining collective classification and link predic-
tion. In ICDM Workshops, pages 381–386. IEEE
Computer Society.
Avrim Blum and Shuchi Chawla. 2001. Learning from
labeled and unlabeled data using graph mincuts. In
ICML, pages 19–26.
Remco R. Bouckaert. 2003. Choosing between two
learning algorithms based on calibrated tests. In
ICML, pages 51–58.
Clint Burfoot and Timothy Baldwin. 2009. Automatic
satire detection: Are you having a laugh? In ACL-
IJCNLP Short Papers, pages 161–164.
Clint Burfoot. 2008. Using multiple sources of agree-
ment information for sentiment classification of polit-
ical transcripts. In Australasian Language Technology
Association Workshop 2008, pages 11–18. ALTA.
Minh Duc Cao and Xiaoying Gao. 2005. Combining
contents and citations for scientific document classifi-
cation. In 18th Australian Joint Conference on Artifi-
cial Intelligence, pages 143–152.
Fabien Cromires and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In EACL, pages
166–174.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML, pages 264–271.
Zeno Gantner and Lars Schmidt-Thieme. 2009. Auto-
matic content-based categorization of Wikipedia ar-
ticles. In 2009 Workshop on The People’s Web
Meets NLP: Collaboratively Constructed Semantic
Resources, pages 32–37.
Michael Jordan, Zoubin Ghahramani, Tommi Jaakkola,
Lawrence Saul, and David Heckerman. 1999. An in-
troduction to variational methods for graphical mod-
els. Machine Learning, 37:183–233.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282–289.
Shoushan Li and Chengqing Zong. 2008. Multi-domain
sentiment classification. In ACL, pages 257–260.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In ACL, pages 115–124.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using ma-
chine learning techniques. In EMNLP, pages 79–86.
John C. Platt. 1999. Probabilistic outputs for support
vector machines and comparisons to regularized likeli-
hood methods. In A. Smola, P. Bartlett, B. Scholkopf,
and D. Schuurmans, editors, Advances in Large Mar-
gin Classifiers, pages 61–74. MIT Press.
Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic,
Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.
2008. Collective classification in network data. AI
Magazine, 29:93–106.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP, pages 145–
156.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In ACL-IJCNLP,
pages 226–234.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In EMNLP, pages
170–179.
Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In UAI.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In EMNLP, pages
327–335.
Yiming Yang and Xin Liu. 1999. A re-examination of
text categorization methods. In Proceedings ACM SI-
GIR, pages 42–49.
</reference>
<page confidence="0.992479">
1515
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.720375">
<title confidence="0.999748">Collective Classification of Congressional Floor-Debate Transcripts</title>
<author confidence="0.99963">Steven Bird Burfoot</author>
<affiliation confidence="0.8808015">Department of Computer Science and Software University of Melbourne, VIC 3010,</affiliation>
<email confidence="0.964729">sb,</email>
<abstract confidence="0.9991095">This paper explores approaches to sentiment classification of U.S. Congressional floordebate transcripts. Collective classification techniques are used to take advantage of the informal citation structure present in the debates. We use a range of methods based on local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alina Andreevskaia</author>
<author>Sabine Bergler</author>
</authors>
<title>When specialists and generalists work together: Overcoming domain dependence in sentiment tagging.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>290--298</pages>
<contexts>
<context position="12790" citStr="Andreevskaia and Bergler, 2008" startWordPosition="2022" endWordPosition="2026">weak positives by the SVM. Use of ψi as defined above would lead to these being erroneously assigned the maximum score because of their low variance. The minimum-cut approach places instances in either the positive or negative class depending on which side of the cut they fall on. This means that no measure of classification confidence is available. This extra information is useful at the very least to give a human user an idea of how much to trust the classification. A measure of classification confidence may also be necessary for incorporation into a broader system, e.g., a meta-classifier (Andreevskaia and Bergler, 2008; Li and Zong, 2008). Tuning the α and θ parameters is likely to become a source of inaccuracy in cases where the tuning and test debates have dissimilar link structures. For example, if the tuning debates tend to have fewer, more accurate links the α parameter will be higher. This will not produce good results if the test debates have more frequent, less accurate links. 3.2 Heuristics for Improving Minimum-cut Bansal et al. (2008) offer preliminary work describing additions to the Thomas et al. minimum-cut approach to incorporate “different class” citation classifications. They use post hoc a</context>
</contexts>
<marker>Andreevskaia, Bergler, 2008</marker>
<rawString>Alina Andreevskaia and Sabine Bergler. 2008. When specialists and generalists work together: Overcoming domain dependence in sentiment tagging. In ACL, pages 290–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Claire Cardie</author>
<author>Lillian Lee</author>
</authors>
<title>The power of negative thinking: Exploiting label disagreement in the min-cut classification framework.</title>
<date>2008</date>
<booktitle>In COLING,</booktitle>
<pages>15--18</pages>
<contexts>
<context position="13225" citStr="Bansal et al. (2008)" startWordPosition="2098" endWordPosition="2101">o trust the classification. A measure of classification confidence may also be necessary for incorporation into a broader system, e.g., a meta-classifier (Andreevskaia and Bergler, 2008; Li and Zong, 2008). Tuning the α and θ parameters is likely to become a source of inaccuracy in cases where the tuning and test debates have dissimilar link structures. For example, if the tuning debates tend to have fewer, more accurate links the α parameter will be higher. This will not produce good results if the test debates have more frequent, less accurate links. 3.2 Heuristics for Improving Minimum-cut Bansal et al. (2008) offer preliminary work describing additions to the Thomas et al. minimum-cut approach to incorporate “different class” citation classifications. They use post hoc adjustments of graph capacities based on simple heuristics. Two of the three approaches they trial appear to offer performance improvements: The SetTo heuristic: This heuristic works through E in order and tries to force Vi and Vj into different classes for every “different class” (dij &lt; 0) citation classifier output where i &lt; j. It does this by altering the four relevant content-only preferences, φi(y), φi(n), φj(y), and φj(n). Ass</context>
</contexts>
<marker>Bansal, Cardie, Lee, 2008</marker>
<rawString>Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The power of negative thinking: Exploiting label disagreement in the min-cut classification framework. In COLING, pages 15–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mustafa Bilgic</author>
<author>Lise Getoor</author>
</authors>
<title>Effective label acquisition for collective classification. In</title>
<date>2008</date>
<booktitle>KDD,</booktitle>
<pages>43--51</pages>
<contexts>
<context position="27574" citStr="Bilgic and Getoor (2008)" startWordPosition="4421" endWordPosition="4424">ng on the debates that were allocated to training, tuning and testing. This observation is supported by the per-fold scores that are the basis for the macro-average performance figures reported in Table 2, which tend to have large standard deviations. The absolute standard deviations over the 100 evaluations for the minimum-cut and mean-field methods were 11.19% and 8.94% respectively. These were significantly larger than the standard deviation for the contentonly baseline, which was 7.34%. This leads us to conclude that the performance of collective classification methods is highly variable. Bilgic and Getoor (2008) offer a possible explanation for this. They note that the cost of incorrectly classifying a given instance can be magnified in collective classification, because errors are propagated throughout the network. The extent to which this happens may depend on the random interaction between base classification accuracy and network structure. There is scope for further work to more fully explain this phenomenon. From these statistical and theoretical factors we infer that more reliable conclusions can be drawn from collective classification experiments that use cross-validation instead of a single, </context>
</contexts>
<marker>Bilgic, Getoor, 2008</marker>
<rawString>Mustafa Bilgic and Lise Getoor. 2008. Effective label acquisition for collective classification. In KDD, pages 43–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mustafa Bilgic</author>
<author>Galileo Namata</author>
<author>Lise Getoor</author>
</authors>
<title>Combining collective classification and link prediction.</title>
<date>2007</date>
<booktitle>In ICDM Workshops,</booktitle>
<pages>381--386</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="19625" citStr="Bilgic et al., 2007" startWordPosition="3170" endWordPosition="3173">e possibility of replacing the SVM classifiers with any other model than can be made to produce a probability. Note also that there are no parameters to tune. 3.4 Iterative Classifier Approach The dual-classifier approaches described above represent global attempts to solve the collective classification problem. We can choose to narrow our focus to the local level, in which we aim to produce the best classification for a single instance with the assumption that all other parts of the problem (i.e. the correct labeling of the other instances) are solved. The Iterative Classification Algorithm (Bilgic et al., 2007), defined in Algorithm 1, is a simple technique for performing collective classification using such a local classifier. After bootstrapping with a content-only classifier, it repeatedly generates new estimates for vZ based on its current knowledge of NZ. The algorithm terminates when the predictions stabilize or a fixed number of iterations is completed. Each iteration is completed using a newly generated ordering O, over the instances V. We propose three feature models for the local classifier. Citation presence and Citation count: Given that the majority of citations represent the “same clas</context>
</contexts>
<marker>Bilgic, Namata, Getoor, 2007</marker>
<rawString>Mustafa Bilgic, Galileo Namata, and Lise Getoor. 2007. Combining collective classification and link prediction. In ICDM Workshops, pages 381–386. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Shuchi Chawla</author>
</authors>
<title>Learning from labeled and unlabeled data using graph mincuts.</title>
<date>2001</date>
<booktitle>In ICML,</booktitle>
<pages>pages</pages>
<contexts>
<context position="10776" citStr="Blum and Chawla, 2001" startWordPosition="1692" endWordPosition="1695">Combine content-only and citation features Update citation features Local classifications Terminate iteration Local vectors SVM Overall classifications ψij(y, y) = 1508 The cost function is modeled in a flow graph where extra source and sink nodes represent the y and n labels respectively. Each node in V is connected to the source and sink with capacities φi(y) and φi(n) respectively. Pairs classified in the “same class” class are linked with capacities defined by ψ. An exact optimum and corresponding overall classification is efficiently computed by finding the minimum-cut of the flow graph (Blum and Chawla, 2001). The free parameters are tuned on a set of held-out data. Thomas et al. demonstrate improvements over content-only classification, without attempting to show that the approach does better than any alternatives; the main appeal is the simplicity of the flow graph model. There are a number of theoretical limitations to the approach, which we now discuss. As Thomas et al. point out, the model has no way of representing the “different class” output from the citation classifier and these citations must be discarded. This, to us, is the most significant problem with the model. Inspection of the cor</context>
</contexts>
<marker>Blum, Chawla, 2001</marker>
<rawString>Avrim Blum and Shuchi Chawla. 2001. Learning from labeled and unlabeled data using graph mincuts. In ICML, pages 19–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remco R Bouckaert</author>
</authors>
<title>Choosing between two learning algorithms based on calibrated tests.</title>
<date>2003</date>
<booktitle>In ICML,</booktitle>
<pages>51--58</pages>
<contexts>
<context position="23185" citStr="Bouckaert (2003)" startWordPosition="3759" endWordPosition="3760"> the n label. 4 Experiments In this section we compare the performance of our dual-classifier and iterative-classifier approaches. We also evaluate the performance of the three feature models for local classification. All accuracies are given as the percentages of instances correctly classified. Results are macroaveraged using 10 x 10-fold cross validation, i.e. 10 runs of 10-fold cross validation using different randomly assigned data splits. Where quoted, statistical significance has been calculated using a two-tailed paired t-test measured over all 100 pairs with 10 degrees of freedom. See Bouckaert (2003) for an experimental justification for this approach. Note that the results presented in this section are not directly comparable with those reported by Thomas et al. and Bansal et al. because their experiments do not use cross-validation. See Section 4.3 for further discussion of experimental configuration. 4.1 Local Classification We evaluate three models for local classification: citation presence features, citation count features and context window features. In each case the SVM classifier is given feature vectors with both contentonly and citation information, as described in Section 3.4.</context>
</contexts>
<marker>Bouckaert, 2003</marker>
<rawString>Remco R. Bouckaert. 2003. Choosing between two learning algorithms based on calibrated tests. In ICML, pages 51–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clint Burfoot</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic satire detection: Are you having a laugh?</title>
<date>2009</date>
<booktitle>In ACLIJCNLP Short Papers,</booktitle>
<pages>161--164</pages>
<contexts>
<context position="1220" citStr="Burfoot and Baldwin, 2009" startWordPosition="161" endWordPosition="164">r incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique. 1 Introduction Supervised document classification is a well-studied task. Research has been performed across many document types with a variety of classification tasks. Examples are topic classification of newswire articles (Yang and Liu, 1999), sentiment classification of movie reviews (Pang et al., 2002), and satire classification of news articles (Burfoot and Baldwin, 2009). This and other work has established the usefulness of document classifiers as stand-alone systems and as components of broader NLP systems. This paper deals with methods relevant to supervised document classification in domains with network structures, where collective classification can yield better performance than approaches that consider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process. Web encyclopedias and scholarly publications are two examples of document domains where </context>
</contexts>
<marker>Burfoot, Baldwin, 2009</marker>
<rawString>Clint Burfoot and Timothy Baldwin. 2009. Automatic satire detection: Are you having a laugh? In ACLIJCNLP Short Papers, pages 161–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clint Burfoot</author>
</authors>
<title>Using multiple sources of agreement information for sentiment classification of political transcripts.</title>
<date>2008</date>
<booktitle>In Australasian Language Technology Association Workshop</booktitle>
<pages>11--18</pages>
<publisher>ALTA.</publisher>
<contexts>
<context position="30021" citStr="Burfoot, 2008" startWordPosition="4815" endWordPosition="4816">only and citation scores. Links are constructed between test instances and a set of k nearest neighbors drawn only from the training set. Restricting the links in this way means the optimization problem is simple. A similarity metric is used to find nearest neighbors. The Pang and Lee method is an instance of implicit link construction, an approach which is beyond the scope of this paper but nevertheless an important area for future research. A similar technique is used in a variation on the Thomas et al. experiment where additional links between speeches are inferred via a similarity metric (Burfoot, 2008). In cases where both citation and similarity links are present, the overall link score is taken as the sum of the two scores. This seems counter-intuitive, given that the two links are unlikely to be independent. In the framework of this research, the approach would be to train a link meta-classifier to take scores from both link classifiers and output an overall link probability. Within NLP, the use of LBP has not been restricted to document classification. Examples of other applications are dependency parsing (Smith and Eisner, 2008) and alignment (Cromires and Kurohashi, 2009). Conditional</context>
</contexts>
<marker>Burfoot, 2008</marker>
<rawString>Clint Burfoot. 2008. Using multiple sources of agreement information for sentiment classification of political transcripts. In Australasian Language Technology Association Workshop 2008, pages 11–18. ALTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh Duc Cao</author>
<author>Xiaoying Gao</author>
</authors>
<title>Combining contents and citations for scientific document classification.</title>
<date>2005</date>
<booktitle>In 18th Australian Joint Conference on Artificial Intelligence,</booktitle>
<pages>143--152</pages>
<contexts>
<context position="1932" citStr="Cao and Gao, 2005" startWordPosition="268" endWordPosition="271">stems and as components of broader NLP systems. This paper deals with methods relevant to supervised document classification in domains with network structures, where collective classification can yield better performance than approaches that consider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process. Web encyclopedias and scholarly publications are two examples of document domains where network structures have been used to assist classification (Gantner and Schmidt-Thieme, 2009; Cao and Gao, 2005). The contribution of this research is in four parts: (1) we introduce an approach that gives better than state of the art performance for collective classification on the ConVote corpus of congressional debate transcripts (Thomas et al., 2006); (2) we provide a comparative overview of collective document classification techniques to assist researchers in choosing an algorithm for collective document classification tasks; (3) we demonstrate effective novel approaches for incorporating the outputs of SVM classifiers into collective classifiers; and (4) we demonstrate effective novel feature mod</context>
</contexts>
<marker>Cao, Gao, 2005</marker>
<rawString>Minh Duc Cao and Xiaoying Gao. 2005. Combining contents and citations for scientific document classification. In 18th Australian Joint Conference on Artificial Intelligence, pages 143–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabien Cromires</author>
<author>Sadao Kurohashi</author>
</authors>
<title>An alignment algorithm using belief propagation and a structure-based distortion model.</title>
<date>2009</date>
<booktitle>In EACL,</booktitle>
<pages>166--174</pages>
<contexts>
<context position="30608" citStr="Cromires and Kurohashi, 2009" startWordPosition="4909" endWordPosition="4912">ed via a similarity metric (Burfoot, 2008). In cases where both citation and similarity links are present, the overall link score is taken as the sum of the two scores. This seems counter-intuitive, given that the two links are unlikely to be independent. In the framework of this research, the approach would be to train a link meta-classifier to take scores from both link classifiers and output an overall link probability. Within NLP, the use of LBP has not been restricted to document classification. Examples of other applications are dependency parsing (Smith and Eisner, 2008) and alignment (Cromires and Kurohashi, 2009). Conditional random fields (CRFs) are an approach based on Markov random fields that have been popular for segmenting and labeling sequence data (Lafferty et al., 2001). We rejected linear-chain CRFs as a candidate approach for our evaluation on the grounds that the arbitrarily connected graphs used in collective classification can not be fully represented in graphical format, i.e. 1513 Connected Isolated All Majority 52.46 46.29 50.51 Content only 75.31 78.90 76.28 Minimum-cut 78.31 78.90 78.40 Minimum-cut (SetTo(.6)) 78.22 78.90 78.32 Minimum-cut (SetTo(.8)) 78.01 78.90 78.14 Minimum-cut (S</context>
</contexts>
<marker>Cromires, Kurohashi, 2009</marker>
<rawString>Fabien Cromires and Sadao Kurohashi. 2009. An alignment algorithm using belief propagation and a structure-based distortion model. In EACL, pages 166–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Confidence-weighted linear classification.</title>
<date>2008</date>
<booktitle>In ICML,</booktitle>
<pages>264--271</pages>
<contexts>
<context position="33756" citStr="Dredze et al., 2008" startWordPosition="5368" endWordPosition="5371">s from our use of a fixed size context window to build a citation classifier. Future approaches may be able to do away with this arbitrary separation of features by training a local classifier to consider all words in terms of their impact on content-only classification and their relations to neighbors. Probabilistic SVM normalization offers a convenient, principled way of incorporating the outputs of an SVM classifier into a collective classifier. An opportunity for future work is to consider normalization approaches for other classifiers. For example, confidence-weighted linear classifiers (Dredze et al., 2008) have been shown to give superior performance to SVMs on a range of tasks and may therefore be a better choice for collective document classification. Of the three models trialled for local classifiers, context window features did best when measured in an oracle experiment, but citation count features did better when used in a collective classifier. We conclude that context window features are a more nuanced and powerful approach that is also more likely to suffer from data sparseness. Citation count features would have been the less effective in a scenario where the fact of the citation exist</context>
</contexts>
<marker>Dredze, Crammer, Pereira, 2008</marker>
<rawString>Mark Dredze, Koby Crammer, and Fernando Pereira. 2008. Confidence-weighted linear classification. In ICML, pages 264–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeno Gantner</author>
<author>Lars Schmidt-Thieme</author>
</authors>
<title>Automatic content-based categorization of Wikipedia articles.</title>
<date>2009</date>
<booktitle>In 2009 Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources,</booktitle>
<pages>32--37</pages>
<contexts>
<context position="1912" citStr="Gantner and Schmidt-Thieme, 2009" startWordPosition="264" endWordPosition="267">ment classifiers as stand-alone systems and as components of broader NLP systems. This paper deals with methods relevant to supervised document classification in domains with network structures, where collective classification can yield better performance than approaches that consider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process. Web encyclopedias and scholarly publications are two examples of document domains where network structures have been used to assist classification (Gantner and Schmidt-Thieme, 2009; Cao and Gao, 2005). The contribution of this research is in four parts: (1) we introduce an approach that gives better than state of the art performance for collective classification on the ConVote corpus of congressional debate transcripts (Thomas et al., 2006); (2) we provide a comparative overview of collective document classification techniques to assist researchers in choosing an algorithm for collective document classification tasks; (3) we demonstrate effective novel approaches for incorporating the outputs of SVM classifiers into collective classifiers; and (4) we demonstrate effecti</context>
</contexts>
<marker>Gantner, Schmidt-Thieme, 2009</marker>
<rawString>Zeno Gantner and Lars Schmidt-Thieme. 2009. Automatic content-based categorization of Wikipedia articles. In 2009 Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources, pages 32–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Jordan</author>
<author>Zoubin Ghahramani</author>
<author>Tommi Jaakkola</author>
<author>Lawrence Saul</author>
<author>David Heckerman</author>
</authors>
<title>An introduction to variational methods for graphical models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--183</pages>
<contexts>
<context position="16416" citStr="Jordan et al., 1999" startWordPosition="2645" endWordPosition="2648">ing re-termed as a set of clique potentials. Given an assignment v to the nodes V, the pairwise Markov random field is associated with the probability distribution: 1 Y P (v) = Z Vi∈V where: XZ = Y φi(v0 i) Y ψij(v0i, v0j) v/ Vi∈V (Vi,Vj)∈E and v0 i denotes the label of Vi for an alternative assignment in v0. In general, exact inference over a pairwise Markov random field is known to be NP-hard. There are certain conditions under which exact inference is tractable, but real-world data is not guaranteed to satisfy these. A class of approximate inference algorithms known as variational methods (Jordan et al., 1999) solve this problem by substituting a simpler “trial” distribution which is fitted to the Markov random field distribution. Loopy Belief Propagation: Applied to a pairwise Markov random field, loopy belief propagation is a message passing algorithm that can be concisely expressed as the following set of equations: Xmi→j(vj) = α lψij(vi, vj)φi(vi) vi∈L Y mk→i(vi),bvj E Ll Vk∈Ni∩V\Vj Ybi(vi) = αφi(vi) mj→i(vi),bvi E L Vj∈Ni∩V where mi→j is a message sent by Vi to Vj and α is a normalization constant that ensures that each message and each set of marginal probabilities sum to 1. The algorithm pro</context>
</contexts>
<marker>Jordan, Ghahramani, Jaakkola, Saul, Heckerman, 1999</marker>
<rawString>Michael Jordan, Zoubin Ghahramani, Tommi Jaakkola, Lawrence Saul, and David Heckerman. 1999. An introduction to variational methods for graphical models. Machine Learning, 37:183–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="30777" citStr="Lafferty et al., 2001" startWordPosition="4935" endWordPosition="4938">ems counter-intuitive, given that the two links are unlikely to be independent. In the framework of this research, the approach would be to train a link meta-classifier to take scores from both link classifiers and output an overall link probability. Within NLP, the use of LBP has not been restricted to document classification. Examples of other applications are dependency parsing (Smith and Eisner, 2008) and alignment (Cromires and Kurohashi, 2009). Conditional random fields (CRFs) are an approach based on Markov random fields that have been popular for segmenting and labeling sequence data (Lafferty et al., 2001). We rejected linear-chain CRFs as a candidate approach for our evaluation on the grounds that the arbitrarily connected graphs used in collective classification can not be fully represented in graphical format, i.e. 1513 Connected Isolated All Majority 52.46 46.29 50.51 Content only 75.31 78.90 76.28 Minimum-cut 78.31 78.90 78.40 Minimum-cut (SetTo(.6)) 78.22 78.90 78.32 Minimum-cut (SetTo(.8)) 78.01 78.90 78.14 Minimum-cut (SetTo(1)) 77.71 78.90 77.93 Minimum-cut (IncBy(.05)) 78.14 78.90 78.25 Minimum-cut (IncBy(.15)) 78.45 78.90 78.46 Minimum-cut (IncBy(.25)) 78.02 78.90 78.15 Iterative-cla</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shoushan Li</author>
<author>Chengqing Zong</author>
</authors>
<title>Multi-domain sentiment classification.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>257--260</pages>
<contexts>
<context position="12810" citStr="Li and Zong, 2008" startWordPosition="2027" endWordPosition="2030">f ψi as defined above would lead to these being erroneously assigned the maximum score because of their low variance. The minimum-cut approach places instances in either the positive or negative class depending on which side of the cut they fall on. This means that no measure of classification confidence is available. This extra information is useful at the very least to give a human user an idea of how much to trust the classification. A measure of classification confidence may also be necessary for incorporation into a broader system, e.g., a meta-classifier (Andreevskaia and Bergler, 2008; Li and Zong, 2008). Tuning the α and θ parameters is likely to become a source of inaccuracy in cases where the tuning and test debates have dissimilar link structures. For example, if the tuning debates tend to have fewer, more accurate links the α parameter will be higher. This will not produce good results if the test debates have more frequent, less accurate links. 3.2 Heuristics for Improving Minimum-cut Bansal et al. (2008) offer preliminary work describing additions to the Thomas et al. minimum-cut approach to incorporate “different class” citation classifications. They use post hoc adjustments of graph </context>
</contexts>
<marker>Li, Zong, 2008</marker>
<rawString>Shoushan Li and Chengqing Zong. 2008. Multi-domain sentiment classification. In ACL, pages 257–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="29173" citStr="Pang and Lee (2005)" startWordPosition="4670" endWordPosition="4673">nt is expressed. Somasundaran et al. provides another argument for the usefulness of collective classification (specifically ICA), in this case as applied at a dialogue act level and relying on a complex system of annotations for link information. Somasundaran and Wiebe (2009) propose an unsupervised method for classifying the stance of each contribution to an online debate concerning the merits of competing products. Concessions to other stances are modeled, but there are no overt citations in the data that could be used to induce the network structure required for collective classification. Pang and Lee (2005) use metric labeling to perform multi-class collective classification of movie reviews. Metric labeling is a multi-class equivalent of the minimum-cut technique in which optimization is done over a cost function incorporating content-only and citation scores. Links are constructed between test instances and a set of k nearest neighbors drawn only from the training set. Restricting the links in this way means the optimization problem is simple. A similarity metric is used to find nearest neighbors. The Pang and Lee method is an instance of implicit link construction, an approach which is beyond</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1148" citStr="Pang et al., 2002" startWordPosition="151" endWordPosition="154"> local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique. 1 Introduction Supervised document classification is a well-studied task. Research has been performed across many document types with a variety of classification tasks. Examples are topic classification of newswire articles (Yang and Liu, 1999), sentiment classification of movie reviews (Pang et al., 2002), and satire classification of news articles (Burfoot and Baldwin, 2009). This and other work has established the usefulness of document classifiers as stand-alone systems and as components of broader NLP systems. This paper deals with methods relevant to supervised document classification in domains with network structures, where collective classification can yield better performance than approaches that consider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process. Web encyclopedia</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment classification using machine learning techniques. In EMNLP, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<pages>61--74</pages>
<editor>In A. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="17931" citStr="Platt (1999)" startWordPosition="2895" endWordPosition="2896">a normalization constant that ensures Pv. bj(vj) = 1. The algorithm computes the fixed point equation for every node and continues to do so until the marginal probabilities bj(vj) stabilize. Mean-field can be shown to be a variational method in the same way as loopy belief propagation, using a simpler trial distribution. For details see Sen et al. (2008). Probabilistic SVM Normalisation: Unlike minimum-cut, the Markov random field approaches have inherent support for the “different class” output of the citation classifier. This allows us to apply a more principled SVM normalisation technique. Platt (1999) describes a technique for converting the output of an SVM classifier to a calibrated posterior probability. Platt finds that the posterior can be fit using a parametric form of a sigmoid: 1 P(y = 1|d) = 1 + exp(Ad + B) This is equivalent to assuming that the output of the SVM is proportional to the log odds of a positive example. Experimental analysis shows error rate is Yφi(vi) ψij(vi, vj) (Vi,Vj)∈E 1510 improved over a plain linear SVM and probabilities Algorithm 1 Iterative Classification Algorithm are of comparable quality to those produced using a regularized likelihood kernel method. By</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In A. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 61–74. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prithviraj Sen</author>
<author>Galileo Mark Namata</author>
<author>Mustafa Bilgic</author>
<author>Lise Getoor</author>
<author>Brian Gallagher</author>
<author>Tina Eliassi-Rad</author>
</authors>
<title>Collective classification in network data.</title>
<date>2008</date>
<journal>AI Magazine,</journal>
<pages>29--93</pages>
<contexts>
<context position="7553" citStr="Sen et al. (2008)" startWordPosition="1170" endWordPosition="1173">ng This approach represents a marked deviation from to “for” and “against” votes respectively. The text the dual-classifier approach and offers unique adof each instance is the concatenation of the speeches vantages. It is fully described in Section 3.4. by a speaker within a debate. This results in a corpus Figure 3 gives a visual overview of the iterativeof 1,699 instances with a roughly even class distri- classifier approach. bution. Approximately 70% of these are connected, For a detailed introduction to collective classificai.e. they are the source or target of one or more cita- tion see Sen et al. (2008). tions. The remainder are isolated. 1507 Debate 006 Speaker 400378 [against] Mr. Speaker, ... all over Washington and in the country, people are talking today about the majority’s last-minute decision to abandon ... . . . Speaker 400115 [for] . . . Mr. Speaker, ... I just want to say to the gentlewoman from New York that every single member of this institution ... . . . Figure 1: Sample speech fragments from the ConVote corpus. The phrase gentlewoman from New York by speaker 400115 is annotated as a reference to speaker 400378. Debate content Content-only and citation scores MF/LBP/Mincut Con</context>
<context position="17675" citStr="Sen et al. (2008)" startWordPosition="2856" endWordPosition="2859"> its neighbors until the messages stabilize. The marginal probability is then derived by calculating bi(vi). Mean-Field: The basic mean-field algorithm can be described with the equation: Y Y ψbi(vi) bj(vj) = αφj(vj) ij (vi, vj), vj E L Vi∈Nj∩V vi∈L where α is a normalization constant that ensures Pv. bj(vj) = 1. The algorithm computes the fixed point equation for every node and continues to do so until the marginal probabilities bj(vj) stabilize. Mean-field can be shown to be a variational method in the same way as loopy belief propagation, using a simpler trial distribution. For details see Sen et al. (2008). Probabilistic SVM Normalisation: Unlike minimum-cut, the Markov random field approaches have inherent support for the “different class” output of the citation classifier. This allows us to apply a more principled SVM normalisation technique. Platt (1999) describes a technique for converting the output of an SVM classifier to a calibrated posterior probability. Platt finds that the posterior can be fit using a parametric form of a sigmoid: 1 P(y = 1|d) = 1 + exp(Ad + B) This is equivalent to assuming that the output of the SVM is proportional to the log odds of a positive example. Experimenta</context>
</contexts>
<marker>Sen, Namata, Bilgic, Getoor, Gallagher, Eliassi-Rad, 2008</marker>
<rawString>Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad. 2008. Collective classification in network data. AI Magazine, 29:93–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>145--156</pages>
<contexts>
<context position="30563" citStr="Smith and Eisner, 2008" startWordPosition="4903" endWordPosition="4906">ional links between speeches are inferred via a similarity metric (Burfoot, 2008). In cases where both citation and similarity links are present, the overall link score is taken as the sum of the two scores. This seems counter-intuitive, given that the two links are unlikely to be independent. In the framework of this research, the approach would be to train a link meta-classifier to take scores from both link classifiers and output an overall link probability. Within NLP, the use of LBP has not been restricted to document classification. Examples of other applications are dependency parsing (Smith and Eisner, 2008) and alignment (Cromires and Kurohashi, 2009). Conditional random fields (CRFs) are an approach based on Markov random fields that have been popular for segmenting and labeling sequence data (Lafferty et al., 2001). We rejected linear-chain CRFs as a candidate approach for our evaluation on the grounds that the arbitrarily connected graphs used in collective classification can not be fully represented in graphical format, i.e. 1513 Connected Isolated All Majority 52.46 46.29 50.51 Content only 75.31 78.90 76.28 Minimum-cut 78.31 78.90 78.40 Minimum-cut (SetTo(.6)) 78.22 78.90 78.32 Minimum-cut</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>David A. Smith and Jason Eisner. 2008. Dependency parsing by belief propagation. In EMNLP, pages 145– 156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP,</booktitle>
<pages>226--234</pages>
<contexts>
<context position="28831" citStr="Somasundaran and Wiebe (2009)" startWordPosition="4615" endWordPosition="4618">work Somasundaran et al. (2009) use ICA to improve sentiment polarity classification of dialogue acts in a corpus of multi-party meeting transcripts. Link features are derived from annotations giving frame relations and target relations. Respectively, these relate dialogue acts based on the sentiment expressed and the object towards which the sentiment is expressed. Somasundaran et al. provides another argument for the usefulness of collective classification (specifically ICA), in this case as applied at a dialogue act level and relying on a complex system of annotations for link information. Somasundaran and Wiebe (2009) propose an unsupervised method for classifying the stance of each contribution to an online debate concerning the merits of competing products. Concessions to other stances are modeled, but there are no overt citations in the data that could be used to induce the network structure required for collective classification. Pang and Lee (2005) use metric labeling to perform multi-class collective classification of movie reviews. Metric labeling is a multi-class equivalent of the minimum-cut technique in which optimization is done over a cost function incorporating content-only and citation scores</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In ACL-IJCNLP, pages 226–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Galileo Namata</author>
<author>Janyce Wiebe</author>
<author>Lise Getoor</author>
</authors>
<title>Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>170--179</pages>
<contexts>
<context position="28233" citStr="Somasundaran et al. (2009)" startWordPosition="4521" endWordPosition="4524"> this. They note that the cost of incorrectly classifying a given instance can be magnified in collective classification, because errors are propagated throughout the network. The extent to which this happens may depend on the random interaction between base classification accuracy and network structure. There is scope for further work to more fully explain this phenomenon. From these statistical and theoretical factors we infer that more reliable conclusions can be drawn from collective classification experiments that use cross-validation instead of a single, fixed data split. 5 Related work Somasundaran et al. (2009) use ICA to improve sentiment polarity classification of dialogue acts in a corpus of multi-party meeting transcripts. Link features are derived from annotations giving frame relations and target relations. Respectively, these relate dialogue acts based on the sentiment expressed and the object towards which the sentiment is expressed. Somasundaran et al. provides another argument for the usefulness of collective classification (specifically ICA), in this case as applied at a dialogue act level and relying on a complex system of annotations for link information. Somasundaran and Wiebe (2009) p</context>
</contexts>
<marker>Somasundaran, Namata, Wiebe, Getoor, 2009</marker>
<rawString>Swapna Somasundaran, Galileo Namata, Janyce Wiebe, and Lise Getoor. 2009. Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. In EMNLP, pages 170–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Pieter Abbeel</author>
<author>Daphne Koller</author>
</authors>
<title>Discriminative probabilistic models for relational data. In UAI.</title>
<date>2002</date>
<contexts>
<context position="15718" citStr="Taskar et al., 2002" startWordPosition="2518" endWordPosition="2521">he risks of incorrect tuning, described above. As Bansal et al. point out, proceeding through E in order means that earlier changes may be undone for speakers who have multiple “different class” citations. Finally, we note that the confidence of the citation classifier is not embodied in the graph structure. The most marginal “different class” citation, classified just on the negative side of the decision plane, is treated identically to the most confident one furthest from the decision plane. 3.3 Dual-classifier Approach with Markov Random Field Approximations A pairwise Markov random field (Taskar et al., 2002) is given by the pair (G, IF), where G and IF are as previously defined, IF being re-termed as a set of clique potentials. Given an assignment v to the nodes V, the pairwise Markov random field is associated with the probability distribution: 1 Y P (v) = Z Vi∈V where: XZ = Y φi(v0 i) Y ψij(v0i, v0j) v/ Vi∈V (Vi,Vj)∈E and v0 i denotes the label of Vi for an alternative assignment in v0. In general, exact inference over a pairwise Markov random field is known to be NP-hard. There are certain conditions under which exact inference is tractable, but real-world data is not guaranteed to satisfy the</context>
</contexts>
<marker>Taskar, Abbeel, Koller, 2002</marker>
<rawString>Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002. Discriminative probabilistic models for relational data. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<pages>327--335</pages>
<contexts>
<context position="2176" citStr="Thomas et al., 2006" startWordPosition="307" endWordPosition="310"> consider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process. Web encyclopedias and scholarly publications are two examples of document domains where network structures have been used to assist classification (Gantner and Schmidt-Thieme, 2009; Cao and Gao, 2005). The contribution of this research is in four parts: (1) we introduce an approach that gives better than state of the art performance for collective classification on the ConVote corpus of congressional debate transcripts (Thomas et al., 2006); (2) we provide a comparative overview of collective document classification techniques to assist researchers in choosing an algorithm for collective document classification tasks; (3) we demonstrate effective novel approaches for incorporating the outputs of SVM classifiers into collective classifiers; and (4) we demonstrate effective novel feature models for iterative local classification of debate transcript data. In the next section (Section 2) we provide a formal definition of collective classification and describe the ConVote corpus that is the basis for our experimental evaluation. Sub</context>
<context position="5874" citStr="Thomas et al. (2006)" startWordPosition="899" endWordPosition="902">or each (Vi, Vj) E E, Oij E T is a function made up of nodes V = {V1, ... , Vn} and edges 0ij: L x L —* R+ U {0}. E. The task is to label the nodes Vi E V from Later in this section we will describe three colleca label set L = {L1, ... , Lq}, making use of the tive classification algorithms capable of performing graph in the form of a neighborhood function N = overall classification based on these inputs: (1) the {N1,. . . , Nn}, where Ni C V \ {Vi}. minimum-cut approach, which is the benchmark for 2.2 The ConVote Corpus collective classification with ConVote, established ConVote, compiled by Thomas et al. (2006), is a by Thomas et al.; (2) loopy belief propagation; and corpus of U.S. congressional debate transcripts. It (3) mean-field. We will show that these latter two consists of 3,857 speeches organized into 53 debates techniques, which are both approximate solutions on specific pieces of legislation. Each speech is for Markov random fields, are superior to minimumtagged with the identity of the speaker and a “for” cut for the task. or “against” label derived from congressional voting Figure 2 gives a visual overview of the dualrecords. In addition, places where one speaker cites classifier approa</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In EMNLP, pages 327–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Xin Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>In Proceedings ACM SIGIR,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="1085" citStr="Yang and Liu, 1999" startWordPosition="141" endWordPosition="144">cture present in the debates. We use a range of methods based on local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique. 1 Introduction Supervised document classification is a well-studied task. Research has been performed across many document types with a variety of classification tasks. Examples are topic classification of newswire articles (Yang and Liu, 1999), sentiment classification of movie reviews (Pang et al., 2002), and satire classification of news articles (Burfoot and Baldwin, 2009). This and other work has established the usefulness of document classifiers as stand-alone systems and as components of broader NLP systems. This paper deals with methods relevant to supervised document classification in domains with network structures, where collective classification can yield better performance than approaches that consider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used </context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Yiming Yang and Xin Liu. 1999. A re-examination of text categorization methods. In Proceedings ACM SIGIR, pages 42–49.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>