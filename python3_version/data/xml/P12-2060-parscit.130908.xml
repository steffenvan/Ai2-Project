<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.044340">
<title confidence="0.989447">
Improving the IBM Alignment Models Using Variational Bayes
</title>
<author confidence="0.995076">
Darcey Riley and Daniel Gildea
</author>
<affiliation confidence="0.9967">
Computer Science Dept.
University of Rochester
</affiliation>
<address confidence="0.337756">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.976684" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999238">
Bayesian approaches have been shown to re-
duce the amount of overfitting that occurs
when running the EM algorithm, by placing
prior probabilities on the model parameters.
We apply one such Bayesian technique, vari-
ational Bayes, to the IBM models of word
alignment for statistical machine translation.
We show that using variational Bayes im-
proves the performance of the widely used
GIZA++ software, as well as improving the
overall performance of the Moses machine
translation system in terms of BLEU score.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999720719298246">
The IBM Models of word alignment (Brown et
al., 1993), along with the Hidden Markov Model
(HMM) (Vogel et al., 1996), serve as the starting
point for most current state-of-the-art machine trans-
lation systems, both phrase-based and syntax-based
(Koehn et al., 2007; Chiang, 2005; Galley et al.,
2004).
Both the IBM Models and the HMM are
trained using the EM algorithm (Dempster et al.,
1977). Recently, Bayesian techniques have become
widespread in applications of EM to natural lan-
guage processing tasks, as a very general method of
controlling overfitting. For instance, Johnson (2007)
showed the benefits of such techniques when ap-
plied to HMMs for unsupervised part of speech tag-
ging. In machine translation, Blunsom et al. (2008)
and DeNero et al. (2008) use Bayesian techniques to
learn bilingual phrase pairs. In this setting, which in-
volves finding a segmentation of the input sentences
into phrasal units, it is particularly important to con-
trol the tendency of EM to choose longer phrases,
which explain the training data well but are unlikely
to generalize.
However, most state-of-the-art machine transla-
tion systems today are built on the basis of word-
level alignments of the type generated by GIZA++
from the IBM Models and the HMM. Overfitting is
also a problem in this context, and improving these
word alignment systems could be of broad utility in
machine translation research.
Moore (2004) discusses details of how EM over-
fits the data when training IBM Model 1. He dis-
covers that the EM algorithm is particularly suscep-
tible to overfitting in the case of rare words, due to
the “garbage collection” phenomenon. Suppose a
sentence contains an English word e1 that occurs
nowhere else in the data, and its French transla-
tion f1. Suppose that same sentence also contains a
word e2 which occurs frequently in the overall data
but whose translation in this sentence, f2, co-occurs
with it infrequently. If the translation t(f2|e2) oc-
curs with probability 0.1, then the sentence will have
a higher probability if EM assigns the rare word and
its actual translation a probability of t(f1|e1) = 0.5,
and assigns the rare word’s translation to f2 a prob-
ability of t(f2Ie1) = 0.5, than if it assigns a proba-
bility of 1 to the correct translation t(f1|e1). Moore
suggests a number of solutions to this issue, includ-
ing add-n smoothing and initializing the probabili-
ties based on a heuristic rather than choosing uni-
form probabilities. When combined, his solutions
cause a significant decrease in alignment error rate
(AER). More recently, Mermer and Saraclar (2011)
have added a Bayesian prior to IBM Model 1 us-
ing Gibbs sampling for inference, showing improve-
ments in BLEU scores.
In this paper, we describe the results of incorpo-
</bodyText>
<page confidence="0.991798">
306
</page>
<note confidence="0.684426">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 306–310,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9986375">
rating variational Bayes (VB) into the widely used
GIZA++ software for word alignment. We use VB
both because it converges more quickly than Gibbs
sampling, and because it can be applied in a fairly
straightforward manner to all of the models imple-
mented by GIZA++. In Section 2, we describe VB
in more detail. In Section 3, we present results for
VB for the various models, in terms of perplexity of
held-out test data, alignment error rate (AER), and
the BLEU scores which result from using our ver-
sion of GIZA++ in the end-to-end phrase-based ma-
chine translation system Moses.
</bodyText>
<sectionHeader confidence="0.828856" genericHeader="method">
2 Variational Bayes and GIZA++
</sectionHeader>
<bodyText confidence="0.998580142857143">
Beal (2003) gives a detailed derivation of a varia-
tional Bayesian algorithm for HMMs. The result is
a very slight change to the M step of the original
EM algorithm. During the M step of the original al-
gorithm, the expected counts collected in the E step
are normalized to give the new values of the param-
eters:
</bodyText>
<equation confidence="0.990871">
E[c(xi|y)]
θx�|y = � (1)
j E[c(xj|y)]
</equation>
<bodyText confidence="0.999885285714286">
The variational Bayesian M step performs an inexact
normalization, where the resulting parameters will
add up to less than one. It does this by passing
the expected counts collected in the E step through
the function f(v) = exp(ψ(v)), where ψ is the
digamma function, and α is the hyperparameter of
the Dirichlet prior (Johnson, 2007):
</bodyText>
<equation confidence="0.696004666666667">
θ c
xi 11 — f ( (E [ (I yI y)] +a )) (2)
�
</equation>
<bodyText confidence="0.999771642857143">
This modified M step can be applied to any model
which uses a multinomial distribution; for this rea-
son, it works for the IBM Models as well as HMMs,
and is thus what we use for GIZA++.
In practice, the digamma function has the effect
of subtracting 0.5 from its argument. When α is
set to a low value, this results in “anti-smoothing”.
For the translation probabilities, because about 0.5
is subtracted from the expected counts, small counts
corresponding to rare co-occurrences of words will
be penalized heavily, while larger counts will not be
affected very much. Thus, low values of α cause
the algorithm to favor words which co-occur fre-
quently and to distrust words that co-occur rarely.
</bodyText>
<table confidence="0.991060428571429">
Sentence pair count
e2 9
f3
e2 2
f2
e1 e2 1
f1 f2
</table>
<tableCaption confidence="0.999961">
Table 1: An example of data with rare words.
</tableCaption>
<bodyText confidence="0.999983037037037">
In this way, VB controls the overfitting that would
otherwise occur with rare words. On the other hand,
higher values of α can be chosen if smoothing is de-
sired, for instance in the case of the alignment prob-
abilities, which state how likely a word in position i
of the English sentence is to align to a word in po-
sition j of the French sentence. For these probabili-
ties, smoothing is important because we do not want
to rule out any alignment altogether, no matter how
infrequently it occurs in the data.
We implemented VB for the translation probabil-
ities as well as for the position alignment probabili-
ties of IBM Model 2. We discovered that adding VB
for the translation probabilities improved the perfor-
mance of the system. However, including VB for
the alignment probabilities had relatively little ef-
fect, because the alignment table in its original form
does some smoothing during normalization by inter-
polating the counts with a uniform distribution. Be-
cause VB can itself be a form of smoothing, the two
versions of the code behave similarly. We did not
experiment with VB for the distortion probabilities
of the HMM or Models 3 and 4, as these distribu-
tions have fewer parameters and are likely to have
reliable counts during EM. Thus, in Section 3, we
present the results of using VB for the translation
probabilities only.
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.996072777777778">
First, we ran our modified version of GIZA++ on a
simple test case designed to be similar to the exam-
ple from Moore (2004) discussed in Section 1. Our
test case, shown in Table 1, had three different sen-
tence pairs; we included nine instances of the first,
two instances of the second, and one of the third.
Human intuition tells us that f2 should translate to
e2 and f1 should translate to e1. However, the EM
algorithm without VB prefers e1 as the translation
</bodyText>
<page confidence="0.960345">
307
</page>
<figure confidence="0.998945666666667">
5
0.50
0.45
0.40
0.35
0.30
0.25
0.20
0.15
0.10
1e-10
1600
1400
1200
1000
800
600
400
</figure>
<figureCaption confidence="0.990794416666667">
Figure 1: Determining the best value of α for the transla-
tion probabilities. Training data is 10,000 sentence pairs
from each language pair. VB is used for Model 1 only.
This table shows the AER for different values of α af-
ter training is complete (five iterations each of Models 1,
HMM, 3, and 4).
Figure 2: Effect of variational Bayes on overfitting for
Model 1. Training data is 10,000 sentence pairs. This
table contrasts the test perplexities of Model 1 with vari-
ational Bayes and Model 1 without variational Bayes af-
ter different numbers of training iterations. Variational
Bayes successfully controls overfitting.
</figureCaption>
<bodyText confidence="0.999672962962963">
of f2, due to the “garbage collection” phenomenon
described above. The EM algorithm with VB does
not overfit this data and prefers e2 as f2’s translation.
For our experiments with bilingual data, we used
three language pairs: French and English, Chi-
nese and English, and German and English. We
used Canadian Hansard data for French-English,
Europarl data for German-English, and newswire
data for Chinese-English. For measuring align-
ment error rate, we used 447 French-English sen-
tences provided by Hermann Ney and Franz Och
containing both sure and possible alignments, while
for German-English we used 220 sentences pro-
vided by Chris Callison-Burch with sure alignments
only, and for Chinese-English we used the first 400
sentences of the data provided by Yang Liu, also
with sure alignments only. For computing BLEU
scores, we used single reference datasets for French-
English and German-English, and four references
for Chinese-English. For minimum error rate train-
ing, we used 1000 sentences for French-English,
2000 sentences for German-English, and 1274 sen-
tences for Chinese-English. Our test sets con-
tained 1000 sentences each for French-English and
German-English, and 686 sentences for Chinese-
English. For scoring the Viterbi alignments of each
system against gold-standard annotated alignments,
we use the alignment error rate (AER) of Och and
Ney (2000), which measures agreement at the level
of pairs of words.
We ran our code on ten thousand sentence pairs
to determine the best value of α for the transla-
tion probabilities t(f|e). For our training, we ran
GIZA++ for five iterations each of Model 1, the
HMM, Model 3, and Model 4. Variational Bayes
was only used for Model 1. Figure 1 shows how VB,
and different values of α in particular, affect the per-
formance of GIZA++ in terms of AER. We discover
that, after all training is complete, VB improves the
performance of the overall system, lowering AER
(Figure 1) for all three language pairs. We find that
low values of α cause the most consistent improve-
ments, and so we use α = 0 for the translation prob-
abilities in the remaining experiments. Note that,
while a value of α = 0 does not define a proba-
bilistically valid Dirichlet prior, it does not cause any
practical problems in the update equation for VB.
Figure 2 shows the test perplexity after GIZA++
has been run for twenty-five iterations of Model 1:
without VB, the test perplexity increases as training
continues, but it remains stable when VB is used.
Thus, VB eliminates the need for the early stopping
that is often employed with GIZA++.
After choosing 0 as the best value of α for the
</bodyText>
<page confidence="0.989439">
308
</page>
<figure confidence="0.999450363636364">
0.50
0.45
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
10000
</figure>
<figureCaption confidence="0.91422">
Figure 3: Performance of GIZA++ on different amounts
of test data. Variational Bayes is used for Model 1 only.
</figureCaption>
<tableCaption confidence="0.823594">
Table shows AER after all the training has completed
(five iterations each of Models 1, HMM, 3, and 4).
</tableCaption>
<table confidence="0.999825875">
French AER German
Chinese
Baseline 0.14 0.42 0.43
M1 Only 0.12 0.39 0.41
HMM Only 0.14 0.42 0.42
M3 Only 0.14 0.42 0.43
M4 Only 0.14 0.42 0.43
All Models 0.19 0.44 0.45
</table>
<tableCaption confidence="0.911691">
Table 2: Effect of Adding Variational Bayes to Specific
Models
</tableCaption>
<bodyText confidence="0.999946105263158">
translation probabilities, we reran the test above
(five iterations each of Models 1, HMM, 3, and
4, with VB turned on for Model 1) on different
amounts of data. We found that the results for larger
data sizes were comparable to the results for ten
thousand sentence pairs, both with and without VB
(Figure 3).
We then tested whether VB should be used for the
later models. In all of these experiments, we ran
Models 1, HMM, 3, and 4 for five iterations each,
training on the same ten thousand sentence pairs that
we used in the previous experiments. In Table 2, we
show the performance of the system when no VB is
used, when it is used for each of the four models in-
dividually, and when it is used for all four models
simultaneously. We saw the most overall improve-
ment when VB was used only for Model 1; using VB
for all four models simultaneously caused the most
improvement to the test perplexity, but at the cost of
</bodyText>
<table confidence="0.9984838">
BLEU Score
French Chinese German
Baseline 26.34 21.03 21.14
M1 Only 26.54 21.58 21.73
All Models 26.46 22.08 21.96
</table>
<tableCaption confidence="0.995852">
Table 3: BLEU Scores
</tableCaption>
<bodyText confidence="0.998412791666667">
the AER.
For the MT experiments, we ran GIZA++ through
Moses, training Model 1, the HMM, and Model 4 on
100,000 sentence pairs from each language pair. We
ran three experiments, one with VB turned on for all
models, one with VB turned on for Model 1 only,
and one (the baseline) with VB turned off for all
models. When VB was turned on, we ran GIZA++
for five iterations per model as in our earlier tests,
but when VB was turned off, we ran GIZA++ for
only four iterations per model, having determined
that this was the optimal number of iterations for
baseline system. VB was used for the translation
probabilities only, with α set to 0.
As can be seen in Table 3, using VB increases
the BLEU score for all three language pairs. For
French, the best results were achieved when VB was
used for Model 1 only; for Chinese and German, on
the other hand, using VB for all models caused the
most improvements. For French, the BLEU score
increased by 0.20; for German, it increased by 0.82;
for Chinese, it increased by 1.05. Overall, VB seems
to have the greatest impact on the language pairs that
are most difficult to align and translate to begin with.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999992071428571">
We find that applying variational Bayes with a
Dirichlet prior to the translation models imple-
mented in GIZA++ improves alignments, both in
terms of AER and the BLEU score of an end-to-end
translation system. Variational Bayes is especially
beneficial for IBM Model 1, because its lack of fer-
tility and position information makes it particularly
susceptible to the garbage collection phenomenon.
Applying VB to Model 1 alone tends to improve
the performance of later models in the training se-
quence. Model 1 is an essential stepping stone in
avoiding local minima when training the following
models, and improvements to Model 1 lead to im-
provements in the end-to-end system.
</bodyText>
<page confidence="0.998681">
309
</page>
<sectionHeader confidence="0.996261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999927296296296">
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Univer-
sity College London.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Neural
Information Processing Systems (NIPS).
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL-05, pages 263–270, Ann Arbor, MI.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1–21.
John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 314–323, Honolulu, Hawaii, October.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings of NAACL-04, pages 273–280, Boston.
Mark Johnson. 2007. Why doesn’t EM find good
HMM POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296–305,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, Demonstration Session, pages 177–180.
Coskun Mermer and Murat Saraclar. 2011. Bayesian
word alignment for statistical machine translation. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-11), pages
182–187.
Robert C. Moore. 2004. Improving IBM word alignment
Model 1. In Proceedings of the 42nd Meeting of the
Association for Computational Linguistics (ACL’04),
Main Volume, pages 518–525, Barcelona, Spain, July.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL-
00, pages 440–447, Hong Kong, October.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING-96, pages 836–841.
</reference>
<page confidence="0.998613">
310
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985902">
<title confidence="0.99984">Improving the IBM Alignment Models Using Variational Bayes</title>
<author confidence="0.996787">Riley</author>
<affiliation confidence="0.9998285">Computer Science University of</affiliation>
<address confidence="0.999065">Rochester, NY 14627</address>
<abstract confidence="0.999239384615385">Bayesian approaches have been shown to reduce the amount of overfitting that occurs when running the EM algorithm, by placing prior probabilities on the model parameters. We apply one such Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew J Beal</author>
</authors>
<title>Variational Algorithms for Approximate Bayesian Inference.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University College London.</institution>
<contexts>
<context position="4248" citStr="Beal (2003)" startWordPosition="692" endWordPosition="693">ational Bayes (VB) into the widely used GIZA++ software for word alignment. We use VB both because it converges more quickly than Gibbs sampling, and because it can be applied in a fairly straightforward manner to all of the models implemented by GIZA++. In Section 2, we describe VB in more detail. In Section 3, we present results for VB for the various models, in terms of perplexity of held-out test data, alignment error rate (AER), and the BLEU scores which result from using our version of GIZA++ in the end-to-end phrase-based machine translation system Moses. 2 Variational Bayes and GIZA++ Beal (2003) gives a detailed derivation of a variational Bayesian algorithm for HMMs. The result is a very slight change to the M step of the original EM algorithm. During the M step of the original algorithm, the expected counts collected in the E step are normalized to give the new values of the parameters: E[c(xi|y)] θx�|y = � (1) j E[c(xj|y)] The variational Bayesian M step performs an inexact normalization, where the resulting parameters will add up to less than one. It does this by passing the expected counts collected in the E step through the function f(v) = exp(ψ(v)), where ψ is the digamma func</context>
</contexts>
<marker>Beal, 2003</marker>
<rawString>Matthew J. Beal. 2003. Variational Algorithms for Approximate Bayesian Inference. Ph.D. thesis, University College London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>Bayesian synchronous grammar induction.</title>
<date>2008</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="1423" citStr="Blunsom et al. (2008)" startWordPosition="220" endWordPosition="223">96), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for unsupervised part of speech tagging. In machine translation, Blunsom et al. (2008) and DeNero et al. (2008) use Bayesian techniques to learn bilingual phrase pairs. In this setting, which involves finding a segmentation of the input sentences into phrasal units, it is particularly important to control the tendency of EM to choose longer phrases, which explain the training data well but are unlikely to generalize. However, most state-of-the-art machine translation systems today are built on the basis of wordlevel alignments of the type generated by GIZA++ from the IBM Models and the HMM. Overfitting is also a problem in this context, and improving these word alignment system</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. Bayesian synchronous grammar induction. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="742" citStr="Brown et al., 1993" startWordPosition="112" endWordPosition="115"> of Rochester Rochester, NY 14627 Abstract Bayesian approaches have been shown to reduce the amount of overfitting that occurs when running the EM algorithm, by placing prior probabilities on the model parameters. We apply one such Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score. 1 Introduction The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for u</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="966" citStr="Chiang, 2005" startWordPosition="148" endWordPosition="149"> Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score. 1 Introduction The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for unsupervised part of speech tagging. In machine translation, Blunsom et al. (2008) and DeNero et al. (2008) use Bayesian techniques to learn bilingual phrase pairs. In this setting, which involves finding a segmentation of th</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL-05, pages 263–270, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="1080" citStr="Dempster et al., 1977" startWordPosition="167" endWordPosition="170">slation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score. 1 Introduction The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for unsupervised part of speech tagging. In machine translation, Blunsom et al. (2008) and DeNero et al. (2008) use Bayesian techniques to learn bilingual phrase pairs. In this setting, which involves finding a segmentation of the input sentences into phrasal units, it is particularly important to control the tendency of EM to choose longer </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>314--323</pages>
<location>Honolulu, Hawaii,</location>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314–323, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-04,</booktitle>
<pages>273--280</pages>
<location>Boston.</location>
<contexts>
<context position="988" citStr="Galley et al., 2004" startWordPosition="150" endWordPosition="153">nique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score. 1 Introduction The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for unsupervised part of speech tagging. In machine translation, Blunsom et al. (2008) and DeNero et al. (2008) use Bayesian techniques to learn bilingual phrase pairs. In this setting, which involves finding a segmentation of the input sentences into</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of NAACL-04, pages 273–280, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>296--305</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1276" citStr="Johnson (2007)" startWordPosition="197" endWordPosition="198">BLEU score. 1 Introduction The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for unsupervised part of speech tagging. In machine translation, Blunsom et al. (2008) and DeNero et al. (2008) use Bayesian techniques to learn bilingual phrase pairs. In this setting, which involves finding a segmentation of the input sentences into phrasal units, it is particularly important to control the tendency of EM to choose longer phrases, which explain the training data well but are unlikely to generalize. However, most state-of-the-art machine translation systems today are built on the basis of wordlevel alignments of the</context>
<context position="4920" citStr="Johnson, 2007" startWordPosition="811" endWordPosition="812">orithm for HMMs. The result is a very slight change to the M step of the original EM algorithm. During the M step of the original algorithm, the expected counts collected in the E step are normalized to give the new values of the parameters: E[c(xi|y)] θx�|y = � (1) j E[c(xj|y)] The variational Bayesian M step performs an inexact normalization, where the resulting parameters will add up to less than one. It does this by passing the expected counts collected in the E step through the function f(v) = exp(ψ(v)), where ψ is the digamma function, and α is the hyperparameter of the Dirichlet prior (Johnson, 2007): θ c xi 11 — f ( (E [ (I yI y)] +a )) (2) � This modified M step can be applied to any model which uses a multinomial distribution; for this reason, it works for the IBM Models as well as HMMs, and is thus what we use for GIZA++. In practice, the digamma function has the effect of subtracting 0.5 from its argument. When α is set to a low value, this results in “anti-smoothing”. For the translation probabilities, because about 0.5 is subtracted from the expected counts, small counts corresponding to rare co-occurrences of words will be penalized heavily, while larger counts will not be affecte</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 296–305, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL, Demonstration Session,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="952" citStr="Koehn et al., 2007" startWordPosition="144" endWordPosition="147">s. We apply one such Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score. 1 Introduction The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for unsupervised part of speech tagging. In machine translation, Blunsom et al. (2008) and DeNero et al. (2008) use Bayesian techniques to learn bilingual phrase pairs. In this setting, which involves finding a segm</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL, Demonstration Session, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Coskun Mermer</author>
<author>Murat Saraclar</author>
</authors>
<title>Bayesian word alignment for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-11),</booktitle>
<pages>182--187</pages>
<contexts>
<context position="3262" citStr="Mermer and Saraclar (2011)" startWordPosition="526" endWordPosition="529">curs with probability 0.1, then the sentence will have a higher probability if EM assigns the rare word and its actual translation a probability of t(f1|e1) = 0.5, and assigns the rare word’s translation to f2 a probability of t(f2Ie1) = 0.5, than if it assigns a probability of 1 to the correct translation t(f1|e1). Moore suggests a number of solutions to this issue, including add-n smoothing and initializing the probabilities based on a heuristic rather than choosing uniform probabilities. When combined, his solutions cause a significant decrease in alignment error rate (AER). More recently, Mermer and Saraclar (2011) have added a Bayesian prior to IBM Model 1 using Gibbs sampling for inference, showing improvements in BLEU scores. In this paper, we describe the results of incorpo306 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 306–310, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics rating variational Bayes (VB) into the widely used GIZA++ software for word alignment. We use VB both because it converges more quickly than Gibbs sampling, and because it can be applied in a fairly straightforward manner to all of the </context>
</contexts>
<marker>Mermer, Saraclar, 2011</marker>
<rawString>Coskun Mermer and Murat Saraclar. 2011. Bayesian word alignment for statistical machine translation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-11), pages 182–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Improving IBM word alignment Model 1.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>518--525</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2096" citStr="Moore (2004)" startWordPosition="332" endWordPosition="333">lingual phrase pairs. In this setting, which involves finding a segmentation of the input sentences into phrasal units, it is particularly important to control the tendency of EM to choose longer phrases, which explain the training data well but are unlikely to generalize. However, most state-of-the-art machine translation systems today are built on the basis of wordlevel alignments of the type generated by GIZA++ from the IBM Models and the HMM. Overfitting is also a problem in this context, and improving these word alignment systems could be of broad utility in machine translation research. Moore (2004) discusses details of how EM overfits the data when training IBM Model 1. He discovers that the EM algorithm is particularly susceptible to overfitting in the case of rare words, due to the “garbage collection” phenomenon. Suppose a sentence contains an English word e1 that occurs nowhere else in the data, and its French translation f1. Suppose that same sentence also contains a word e2 which occurs frequently in the overall data but whose translation in this sentence, f2, co-occurs with it infrequently. If the translation t(f2|e2) occurs with probability 0.1, then the sentence will have a hig</context>
<context position="7223" citStr="Moore (2004)" startWordPosition="1228" endWordPosition="1229"> form does some smoothing during normalization by interpolating the counts with a uniform distribution. Because VB can itself be a form of smoothing, the two versions of the code behave similarly. We did not experiment with VB for the distortion probabilities of the HMM or Models 3 and 4, as these distributions have fewer parameters and are likely to have reliable counts during EM. Thus, in Section 3, we present the results of using VB for the translation probabilities only. 3 Results First, we ran our modified version of GIZA++ on a simple test case designed to be similar to the example from Moore (2004) discussed in Section 1. Our test case, shown in Table 1, had three different sentence pairs; we included nine instances of the first, two instances of the second, and one of the third. Human intuition tells us that f2 should translate to e2 and f1 should translate to e1. However, the EM algorithm without VB prefers e1 as the translation 307 5 0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 1e-10 1600 1400 1200 1000 800 600 400 Figure 1: Determining the best value of α for the translation probabilities. Training data is 10,000 sentence pairs from each language pair. VB is used for Model 1 only. T</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Robert C. Moore. 2004. Improving IBM word alignment Model 1. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 518–525, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL00,</booktitle>
<pages>440--447</pages>
<location>Hong Kong,</location>
<contexts>
<context position="9638" citStr="Och and Ney (2000)" startWordPosition="1618" endWordPosition="1621">rovided by Yang Liu, also with sure alignments only. For computing BLEU scores, we used single reference datasets for FrenchEnglish and German-English, and four references for Chinese-English. For minimum error rate training, we used 1000 sentences for French-English, 2000 sentences for German-English, and 1274 sentences for Chinese-English. Our test sets contained 1000 sentences each for French-English and German-English, and 686 sentences for ChineseEnglish. For scoring the Viterbi alignments of each system against gold-standard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words. We ran our code on ten thousand sentence pairs to determine the best value of α for the translation probabilities t(f|e). For our training, we ran GIZA++ for five iterations each of Model 1, the HMM, Model 3, and Model 4. Variational Bayes was only used for Model 1. Figure 1 shows how VB, and different values of α in particular, affect the performance of GIZA++ in terms of AER. We discover that, after all training is complete, VB improves the performance of the overall system, lowering AER (Figure 1) for all three language pairs. We fi</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL00, pages 440–447, Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING-96,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="805" citStr="Vogel et al., 1996" startWordPosition="123" endWordPosition="126">have been shown to reduce the amount of overfitting that occurs when running the EM algorithm, by placing prior probabilities on the model parameters. We apply one such Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score. 1 Introduction The IBM Models of word alignment (Brown et al., 1993), along with the Hidden Markov Model (HMM) (Vogel et al., 1996), serve as the starting point for most current state-of-the-art machine translation systems, both phrase-based and syntax-based (Koehn et al., 2007; Chiang, 2005; Galley et al., 2004). Both the IBM Models and the HMM are trained using the EM algorithm (Dempster et al., 1977). Recently, Bayesian techniques have become widespread in applications of EM to natural language processing tasks, as a very general method of controlling overfitting. For instance, Johnson (2007) showed the benefits of such techniques when applied to HMMs for unsupervised part of speech tagging. In machine translation, Blu</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In COLING-96, pages 836–841.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>