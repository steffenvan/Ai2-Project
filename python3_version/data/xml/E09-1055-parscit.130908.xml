<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004484">
<title confidence="0.996695">
Treebank Grammar Techniques for Non-Projective Dependency Parsing
</title>
<author confidence="0.999363">
Marco Kuhlmann Giorgio Satta
</author>
<affiliation confidence="0.961926">
Uppsala University University of Padua
Uppsala, Sweden Padova, Italy
</affiliation>
<email confidence="0.995745">
marco.kuhlmann@lingfil.uu.se satta@dei.unipd.it
</email>
<sectionHeader confidence="0.997337" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999885">
An open problem in dependency parsing
is the accurate and efficient treatment of
non-projective structures. We propose to
attack this problem using chart-parsing
algorithms developed for mildly context-
sensitive grammar formalisms. In this pa-
per, we provide two key tools for this ap-
proach. First, we show how to reduce non-
projective dependency parsing to parsing
with Linear Context-Free Rewriting Sys-
tems (LCFRS), by presenting a technique
for extracting LCFRS from dependency
treebanks. For efficient parsing, the ex-
tracted grammars need to be transformed
in order to minimize the number of nonter-
minal symbols per production. Our second
contribution is an algorithm that computes
this transformation for a large, empirically
relevant class of grammars.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901949152543">
Dependency parsing is the task of predicting the
most probable dependency structure for a given
sentence. One of the key choices in dependency
parsing is about the class of candidate structures
for this prediction. Many parsers are confined to
projective structures, in which the yield of a syn-
tactic head is required to be continuous. A major
benefit of this choice is computational efficiency:
an exhaustive search over all projective structures
can be done in cubic, greedy parsing in linear time
(Eisner, 1996; Nivre, 2003). A major drawback of
the restriction to projective dependency structures
is a potential loss in accuracy. For example, around
23% of the analyses in the Prague Dependency
Treebank of Czech (Hajiˇc et al., 2001) are non-
projective, and for German and Dutch treebanks,
the proportion of non-projective structures is even
higher (Havelka, 2007).
The problem of non-projective dependency pars-
ing under the joint requirement of accuracy and
efficiency has only recently been addressed in the
literature. Some authors propose to solve it by tech-
niques for recovering non-projectivity from the out-
put of a projective parser in a post-processing step
(Hall and Novák, 2005; Nivre and Nilsson, 2005),
others extend projective parsers by heuristics that
allow at least certain non-projective constructions
to be parsed (Attardi, 2006; Nivre, 2007). McDon-
ald et al. (2005) formulate dependency parsing as
the search for the most probable spanning tree over
the full set of all possible dependencies. However,
this approach is limited to probability models with
strong independence assumptions. Exhaustive non-
projective dependency parsing with more powerful
models is intractable (McDonald and Satta, 2007),
and one has to resort to approximation algorithms
(McDonald and Pereira, 2006).
In this paper, we propose to attack non-project-
ive dependency parsing in a principled way, us-
ing polynomial chart-parsing algorithms developed
for mildly context-sensitive grammar formalisms.
This proposal is motivated by the observation that
most dependency structures required for the ana-
lysis of natural language are very nearly projective,
differing only minimally from the best projective
approximation (Kuhlmann and Nivre, 2006), and
by the close link between such ‘mildly non-project-
ive’ dependency structures on the one hand, and
grammar formalisms with mildly context-sensitive
generative capacity on the other (Kuhlmann and
Möhl, 2007). Furthermore, as pointed out by Mc-
Donald and Satta (2007), chart-parsing algorithms
are amenable to augmentation by non-local inform-
ation such as arity constraints and Markovization,
and therefore should allow for more predictive stat-
istical models than those used by current systems
for non-projective dependency parsing. Hence,
mildly non-projective dependency parsing prom-
ises to be both efficient and accurate.
</bodyText>
<note confidence="0.9231295">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 478–486,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998484">
478
</page>
<bodyText confidence="0.999900243243243">
Contributions In this paper, we contribute two
key tools for making the mildly context-sensitive
approach to accurate and efficient non-projective
dependency parsing work.
First, we extend the standard technique for ex-
tracting context-free grammars from phrase-struc-
ture treebanks (Charniak, 1996) to mildly con-
text-sensitive grammars and dependency treebanks.
More specifically, we show how to extract, from
a given dependency treebank, a lexicalized Linear
Context-Free Rewriting System (LCFRS) whose
derivations capture the dependency analyses in the
treebank in the same way as the derivations of
a context-free treebank grammar capture phrase-
structure analyses. Our technique works for arbit-
rary, even non-projective dependency treebanks,
and essentially reduces non-projective dependency
to parsing with LCFRS. This problem can be solved
using standard chart-parsing techniques.
Our extraction technique yields a grammar
whose parsing complexity is polynomial in the
length of the sentence, but exponential in both a
measure of the non-projectivity of the treebank and
the maximal number of dependents per word, re-
flected as the rank of the extracted LCFRS. While
the number of highly non-projective dependency
structures is negligible for practical applications
(Kuhlmann and Nivre, 2006), the rank cannot eas-
ily be bounded. Therefore, we present an algorithm
that transforms the extracted grammar into a nor-
mal form that has rank 2, and thus can be parsed
more efficiently. This contribution is important
even independently of the extraction procedure:
While it is known that a rank-2 normal form of
LCFRS does not exist in the general case (Rambow
and Satta, 1999), our algorithm succeeds for a large
and empirically relevant class of grammars.
</bodyText>
<sectionHeader confidence="0.992323" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.9997102">
We start by introducing dependency trees and
Linear Context-Free Rewriting Systems (LCFRS).
Throughout the paper, for positive integers i and j,
we write [i, j ] for the interval { k I i &lt; k &lt; j },
and use [n] as a shorthand for [1, n].
</bodyText>
<subsectionHeader confidence="0.974937">
2.1 Dependency Trees
</subsectionHeader>
<bodyText confidence="0.999540032258064">
Dependency parsing is the task to assign depend-
ency structures to a given sentence w. For the
purposes of this paper, dependency structures are
edge-labelled trees. More formally, let w be a sen-
tence, understood as a sequence of tokens over
some given alphabet T , and let L be an alphabet
of edge labels. A dependency tree for w is a con-
struct D = (w, E, k), where E forms a rooted tree
(in the standard graph-theoretic sense) on the set
[IwI], and k is a total function that assigns every
edge in E a label in L. Each node of D represents
a (position of a) token in w.
Example 1 Figure 2 shows a dependency tree for
the sentence A hearing is scheduled on the issue
today, which consists of 8 tokens and the edges
{ (2, 1), (2, 5), (3, 2), (3, 4), (4, 8), (5, 7), (7, 6) }.
The edges are labelled with syntactic functions
such as sbj for ‘subject’. The root node is marked
by a dotted line. ❑
Let u be a node of a dependency tree D. A node u&apos;
is a descendant of u, if there is a (possibly empty)
path from u to u&apos;. A block of u is a maximal
interval of descendants of u. The number of blocks
of u is called the block-degree of u. The block-
degree of a dependency tree is the maximum among
the block-degrees of its nodes. A dependency tree
is projective, if its block-degree is 1.
Example 2 The tree shown in Figure 2 is not
projective: both node 2 (hearing) and node 4
(scheduled) have block-degree 2. Their blocks are
{ 2 }, {5, 6, 7 } and { 4 }, { 8 }, respectively.
</bodyText>
<subsectionHeader confidence="0.932295">
2.2 LCFRS
</subsectionHeader>
<bodyText confidence="0.991333272727273">
Linear Context-Free Rewriting Systems (LCFRS)
have been introduced as a generalization of sev-
eral mildly context-sensitive grammar formalisms.
Here we use the standard definition of LCFRS
(Vijay-Shanker et al., 1987) and only fix our nota-
tion; for a more thorough discussion of this formal-
ism, we refer to the literature.
Let G be an LCFRS. Recall that each nonter-
minal symbol A of G comes with a positive integer
called the fan-out of A, and that a production p
of G has the form
</bodyText>
<equation confidence="0.502025">
A --* g(A1, ... , Ar) ; g(�x1, ... , �xr) = a�,
</equation>
<bodyText confidence="0.9999656">
where A, A1, ... , Ar are nonterminals with fan-out
f, f1, ... , fr, respectively, g is a function symbol,
and the equation to the right of the semicolon spe-
cifies the semantics of g. For each i E [r], xi is
an fi-tuple of variables, and a� = (a1, ... , af ) is a
tuple of strings over the variables on the left-hand
side of the equation and the alphabet of terminal
symbols in which each variable appears exactly
once. The production p is said to have rank r,
fan-out f , and length Ia1I + • • • + Iaf I + (f —1).
</bodyText>
<page confidence="0.999501">
479
</page>
<sectionHeader confidence="0.996189" genericHeader="method">
3 Grammar Extraction
</sectionHeader>
<bodyText confidence="0.999974">
We now explain how to extract an LCFRS from a
dependency treebank, in very much the same way
as a context-free grammar can be extracted from a
phrase-structure treebank (Charniak, 1996).
</bodyText>
<subsectionHeader confidence="0.998669">
3.1 Dependency Treebank Grammars
</subsectionHeader>
<bodyText confidence="0.999992891891892">
A simple way to induce a context-free grammar
from a phrase-structure treebank is to read off the
productions of the grammar from the trees. We will
specify a procedure for extracting, from a given
dependency treebank, a lexicalized LCFRS G that
is adequate in the sense that for every analysis D
of a sentence w in the treebank, there is a derivation
tree of G that is isomorphic to D, meaning that
it becomes equal to D after a suitable renaming
and relabelling of nodes, and has w as its derived
string. Here, a derivation tree of an LCFRS G is
an ordered tree such that each node u is labelled
with a production p of G, the number of children
of u equals the rank r of p, and for each i E [r],
the ith child of u is labelled with a production that
has as its left-hand side the ith nonterminal on the
right-hand side of p.
The basic idea behind our extraction procedure
is that, in order to represent the compositional struc-
ture of a possibly non-projective dependency tree,
one needs to represent the decomposition and relat-
ive order not of subtrees, but of blocks of subtrees
(Kuhlmann and Möhl, 2007). We introduce some
terminology. A component of a node u in a de-
pendency tree is either a block B of some child u&apos;
of u, or the singleton interval that contains u; this
interval will represent the position in the string that
is occupied by the lexical item corresponding to u.
We say that u&apos; contributes B, and that u contrib-
utes [u, u] to u. Notice that the number of com-
ponents that u&apos; contributes to its parent u equals
the block-degree of u&apos;. Our goal is to construct
for u a production of an LCFRS that specifies how
each block of u decomposes into components, and
how these components are ordered relative to one
another. These productions will make an adequate
LCFRS, in the sense defined above.
</bodyText>
<subsectionHeader confidence="0.999903">
3.2 Annotating the Components
</subsectionHeader>
<bodyText confidence="0.9990532">
The core of our extraction procedure is an efficient
algorithm that annotates each node u of a given de-
pendency tree with the list of its components, sor-
ted by their left endpoints. It is helpful to think of
this algorithm as of two independent parts, one that
</bodyText>
<listItem confidence="0.998656">
1: Function ANNOTATE-L.D/
2: for each u of D, from left to right do
3: if u is the first node of D then
4: b := the root node of D
5: else
6: b := the lca of u and its predecessor
7: for each u&apos; on the path b . . . u do
8: left[u&apos;] := left[u&apos;] . u
</listItem>
<figureCaption confidence="0.992404">
Figure 1: Annotation with components
</figureCaption>
<bodyText confidence="0.997917763157895">
annotates each node u with the list of the left end-
points of its components (ANNOTATE-L) and one
that annotates the corresponding right endpoints
(ANNOTATE-R). The list of components can then
be obtained by zipping the two lists of endpoints
together in linear time.
Figure 1 shows pseudocode for ANNOTATE-L;
the pseudocode for ANNOTATE-R is symmetric. We
do a single left-to-right sweep over the nodes of the
input tree D. In each step, we annotate all nodes u&apos;
that have the current node u as the left endpoint of
one of their components. Since the sweep is from
left to right, this will get us the left endpoints of u&apos;
in the desired order. The nodes that we annotate are
the nodes u&apos; on the path between u and the least
common ancestor (lca) b of u and its predecessor,
or the path from the root node to u, in case that u
is the leftmost node of D.
Example 3 For the dependency tree in Figure 2,
ANNOTATE-L constructs the following lists left[u]
of left endpoints, for u = 1, ... , 8:
1, 1 . 2 . 5, 1 . 3 . 4 . 5 . 8, 4 . 8, 5 . 6, 6, 6 . 7, 8
The following Lemma establishes the correctness
of the algorithm:
Lemma 1 Let D be a dependency tree, and let u
and u&apos; be nodes of D. Let b be the least common
ancestor of u and its predecessor, or the root node
in case that u is the leftmost node of D. Then u is
the left endpoint of a component of u&apos; if and only
if u&apos; lies on the path from b to u. ❑
PROOF It is clear that u&apos; must be an ancestor of u.
If u is the leftmost node of D, then u is the left
endpoint of the leftmost component of all of its
ancestors. Now suppose that u is not the leftmost
node of D, and let u� be the predecessor of u. Dis-
tinguish three cases: If u&apos; is not an ancestor of u,
then u� does not belong to any component of u&apos;;
therefore, u is the left endpoint of a component
</bodyText>
<page confidence="0.986293">
480
</page>
<bodyText confidence="0.999982192307692">
of u&apos;. If u&apos; is an ancestor of u� but u&apos; # b, then u�
and u belong to the same component of u&apos;; there-
fore, u is not the left endpoint of this component.
Finally, if u&apos; = b, then u� and u belong to different
components of u&apos;; therefore, u is the left endpoint
of the component it belongs to. 0
We now turn to an analysis of the runtime of the
algorithm. Let n be the number of components
of D. It is not hard to imagine an algorithm that
performs the annotation task in time O.n log n/:
such an algorithm could construct the components
for a given node u by essentially merging the list of
components of the children of u into a new sorted
list. In contrast, our algorithm takes time O.n/.
The crucial part of the analysis is the assignment
in line 6, which computes the least common an-
cestor of u and its predecessor. Using markers for
the path from the root node to u, it is straightfor-
ward to implement this assignment in time O.17r1/,
where 7r is the path b • • • u. Now notice that, by our
correctness argument, line 8 of the algorithm is ex-
ecuted exactly n times. Therefore, the sum over the
lengths of all the paths 7r, and hence the amortized
time of computing all the least common ancest-
ors in line 6, is O.n/. This runtime complexity is
optimal for the task we are solving.
</bodyText>
<subsectionHeader confidence="0.998347">
3.3 Extraction Procedure
</subsectionHeader>
<bodyText confidence="0.999981">
We now describe how to extend the annotation al-
gorithm into a procedure that extracts an LCFRS
from a given dependency tree D. The basic idea is
to transform the list of components of each node u
of D into a production p. This transformation will
only rename and relabel nodes, and therefore yield
an adequate derivation tree. For the construction
of the production, we actually need an extended
version of the annotation algorithm, in which each
component is annotated with the node that contrib-
uted it. This extension is straightforward, and does
not affect the linear runtime complexity.
Let D be a dependency tree for a sentence w.
Consider a single node u of D, and assume that u
has r children, and that the block-degree of u is f .
We construct for u a production p with rank r
and fan-out f . For convenience, let us order the
children of u, say by their leftmost descendants,
and let us write ui for the ith child of u according
to this order, and fi for the block-degree of ui,
</bodyText>
<equation confidence="0.6967675">
i E Œr•. The production p has the form
L --* g.L1;:::;Lr/ ; g.�x1;:::; 1r/ = ˛�;
</equation>
<bodyText confidence="0.997661956521739">
where L is the label of the incoming edge of u
(or the special label root in case that u is the root
node of D) and for each i E Œr•: Li is the label of
the incoming edge of ui; xi is a fi-tuple of vari-
ables of the form xi,j , where j E Œfi•; and ˛� is
an f -tuple that is constructed in a single left-to-
right sweep over the list of components computed
for u as follows. Let k E Œfi• be a pointer to a cur-
rent segment of a; initially, k = 1. If the current
component is not adjacent (as an interval) to the
previous component, we increase k by one. If the
current component is contributed by the child ui,
i E Œr•, we add the variable xi,j to ˛k, where j
is the number of times we have seen a component
contributed by ui during the sweep. Notice that
j E Œfi•. If the current component is the (unique)
component contributed by u, we add the token cor-
responding to u to ˛k. In this way, we obtain a
complete specification of how the blocks of u (rep-
resented by the segments of the tuple a) decompose
into the components of u, and of the relative order
of the components. As an example, Figure 2 shows
the productions extracted from the tree above.
</bodyText>
<subsectionHeader confidence="0.990548">
3.4 Parsing the Extracted Grammar
</subsectionHeader>
<bodyText confidence="0.99994012">
Once we have extracted the grammar for a depend-
ency treebank, we can apply any parsing algorithm
for LCFRS to non-projective dependency parsing.
The generic chart-parsing algorithm for LCFRS
runs in time O.1P 1•1w1f.r+1//, where P is the set
of productions of the input grammar G, w is the in-
put string, r is the maximal rank, and f is the max-
imal fan-out of a production in G (Seki et al., 1991).
For a grammar G extracted by our technique, the
number f equals the maximal block-degree per
node. Hence, without any further modification, we
obtain a parsing algorithm that is polynomial in the
length of the sentence, but exponential in both the
block-degree and the rank. This is clearly unaccept-
able in practical systems. The relative frequency
of analyses with a block-degree &gt; 2 is almost neg-
ligible (Havelka, 2007); the bigger obstacle in ap-
plying the treebank grammar is the rank of the res-
ulting LCFRS. Therefore, in the remainder of the
paper, we present an algorithm that can transform
the productions of the input grammar G into an
equivalent set of productions with rank at most 2,
while preserving the fan-out. This transformation,
if it succeeds, yields a parsing algorithm that runs
in time O.1P 1 • r • 1w13f /.
</bodyText>
<page confidence="0.861684">
481
</page>
<equation confidence="0.991533333333333">
1A 2hearing 3is 4scheduled5on 6the 7issue 8today
nmod ! g1 g1 D hAi
sbj ! g2(nmod, pp) g2(hx1;1i, hx2;1i) D hx1;1 hearing, x2;1i
root ! g3(sbj,vc) g3(hx1;1,x1;2i, hx2;1, x2;2i) D hx1;1 is x2;1 x1;2 x2;2i
vc ! g4(tmp) g4(hx1;1i) D hscheduled, x1;1i
pp ! g5(np) g5(hx1;1i) D hon x1;1i
nmod ! g6 g6 D hthei
np ! g7(nmod) g7(hx1;1i) D hx1;1 issuei
tmp ! g8 g8 D htodayi
</equation>
<figureCaption confidence="0.997525">
Figure 2: A dependency tree, and the LCFRS extracted for it
</figureCaption>
<figure confidence="0.9937915">
tmp
np
nmod
root node
sbj
nmod
pp
vc
</figure>
<sectionHeader confidence="0.975738" genericHeader="method">
4 Adjacency
</sectionHeader>
<bodyText confidence="0.9999836">
In this section we discuss a method for factorizing
an LCFRS into productions of rank 2. Before start-
ing, we get rid of the ‘easy’ cases. A production p
is connected if any two strings ai, aj in p’s defini-
tion share at least one variable referring to the same
nonterminal. It is not difficult to see that, when p is
not connected, we can always split it into new pro-
ductions of lower rank. Therefore, throughout this
section we assume that LCFRS only have connec-
ted productions. We can split p into its connected
components using standard methods for finding the
strongly connected components of an undirected
graph. This can be implemented in time O(r • f ),
where r and f are the rank and the fan-out of p,
respectively.
</bodyText>
<subsectionHeader confidence="0.995447">
4.1 Adjacency Graphs
</subsectionHeader>
<bodyText confidence="0.986535788461539">
Let p be a production with length n and fan-out f ,
associated with function a g. The set of positions
of p is the set [n]. Informally, each position rep-
resents a variable or a lexical element in one of the
components of the definition of g, or else a ‘gap’
between two of these components. (Recall that n
also accounts for the f - 1 gaps in the body of g.)
Example 4 The set of positions of the production
for hearing in Figure 2 is [4]: 1 for variable x1, 2
for hearing, 3 for the gap, and 4 for y1. ❑
Let i1, j1, i2, j2 2 [n]. An interval [i1, j1] is ad-
jacent to an interval [i2, j2] if either j1 D i2 - 1
(left-adjacent) or i1 D j2 C 1 (right-adjacent). A
multi-interval, or m-interval for short, is a set v of
pairwise disjoint intervals such that no interval in v
is adjacent to any other interval in v. The fan-out
of v, written f (v), is defined as jvj.
We use m-intervals to represent the nonterminals
and the lexical element heading p. The ith nonter-
minal on the right-hand side of p is represented by
the m-interval obtained by collecting all the pos-
itions of p that represent a variable from the ith
argument of g. The head of p is represented by the
m-interval containing the associated position. Note
that all these m-intervals are pairwise disjoint.
Example 5 Consider the production for is in
Figure 2. The set of positions is [5]. The
first nonterminal is represented by the m-inter-
val f [1, 1], [4, 4] g, the second nonterminal by
f [3, 3], [5, 5] g, and the lexical head by f [2, 2] g. ❑
For disjoint m-intervals v1, v2, we say that v1 is
adjacent to v2, denoted by v1 ! v2, if for every
interval I1 2 v1, there is an interval I2 2 v2 such
that I1 is adjacent to I2. Adjacency is not symmet-
ric: if v1 D f [1, 1], [4, 4] g and v2 D f [2, 2] g, then
v2 ! v1, but not vice versa.
Let V be some collection of pairwise disjoint
m-intervals representing p as above. The ad-
jacency graph associated with p is the graph
G D (V, !G) whose vertices are the m-intervals
in V , and whose edges !G are defined by restrict-
ing the adjacency relation ! to the set V .
For m-intervals v1, v2 2 V , the merger of v1
and v2, denoted by v1 ˚ v2, is the (uniquely
determined) m-interval whose span is the union
of the spans of v1 and v2. As an example, if
v1 D f[1, 1], [3, 3] g and v2 D f [2, 2] g, then
v1 ˚ v2 D f [1, 3] g. Notice that the way in which
we defined m-intervals ensures that a merging oper-
ation collapses all adjacent intervals. The proof of
the following lemma is straightforward and omitted
for space reasons:
</bodyText>
<page confidence="0.995055">
482
</page>
<listItem confidence="0.985338090909091">
1: Function FACTORIZE(G D (V, !G))
2: R WD ;;
3: while !G ¤ ; do
4: choose (v1, v2) 2 !G;
5: R WD R [ f (v1, v2) g;
6: V WD V - fv1,v2 g [ fv1 ˚ v2 g;
7: !G WD f (v, v&apos;) j v, v&apos; 2 V, v ! v&apos; g;
8: if jV j D 1 then
9: output R and accept;
10: else
11: reject;
</listItem>
<figureCaption confidence="0.999054">
Figure 3: Factorization algorithm
</figureCaption>
<bodyText confidence="0.324819">
Lemma 2 If v1 ! v2, then f (v1 ˚ v2) &lt; f(v2).
</bodyText>
<subsectionHeader confidence="0.991158">
4.2 The Adjacency Algorithm
</subsectionHeader>
<bodyText confidence="0.999664177777778">
Let G D (V, !G) be some adjacency graph, and
let v1 !G v2. We can derive a new adjacency
graph from G by merging v1 and v2. The resulting
graph G&apos; has vertices V &apos; D V — f v1, v2 g [ f v1 ˚
v2 g and set of edges !G0 obtained by restricting
the adjacency relation ! to V &apos;. We denote the
derive relation as G ).v1;v2/ G&apos;.
Informally, if G represents some LCFRS produc-
tion p and v1, v2 represent nonterminals A1, A2,
then G&apos; represents a production p&apos; obtained from p
by replacing A1, A2 with a fresh nonterminal A. A
new production p&apos;&apos; can also be constructed, expand-
ing A into A1, A2, so that p&apos;, p&apos;&apos; together will be
equivalent to p. Furthermore, p&apos; has a rank smaller
than the rank of p and, from Lemma 2, A does not
increase the overall fan-out of the grammar.
In order to simplify the notation, we adopt the
following convention. Let G ).v1;v2/ G&apos; and
let v !G v1, v ¤ v2. If v !G0 v1 ˚ v2, then
edges (v, v1) and (v, v1 ˚ v2) will be identified,
and we say that G&apos; inherits (v, v1 ˚ v2) from G.
If v 6!G0 v1 ˚v2, then we say that (v, v1) does not
survive the derive step. This convention is used for
all edges incident upon v1 or v2.
Our factorization algorithm is reported in Fig-
ure 3. We start from an adjacency graph repres-
enting some LCFRS production that needs to be
factorized. We arbitrarily choose an edge e of the
graph, and push it into a set R, in order to keep
a record of the candidate factorization. We then
merge the two m-intervals incident to e, and we
recompute the adjacency relation for the new set
of vertices. We iterate until the resulting graph has
an empty edge set. If the final graph has one one
vertex, then we have managed to factorize our pro-
duction into a set of productions with rank at most
two that can be computed from R.
Example 6 Let V D f v1, v2, v3 g with v1 D
f [4, 4] g, v2 D f [1, 1], [3, 3] g, and v3 D
f [2, 2], [5, 5] g. Then !G D f (v1, v2) g. After
merging v1, v2 we have a new graph G with V D
f v1 ˚ v2, v3 g and !G D f (v1 ˚ v2, v3) g. We
finally merge v1 ˚ v2, v3 resulting in a new graph
Gwith V D fv1 ˚ v2 ˚ v3 g and !G D ;. We
then accept and stop. ❑
</bodyText>
<subsectionHeader confidence="0.998008">
4.3 Mathematical Properties
</subsectionHeader>
<bodyText confidence="0.991510243243243">
We have already argued that, if the algorithm ac-
cepts, then a binary factorization that does not
increase the fan-out of the grammar can be built
from R. We still need to prove that the algorithm
answers consistently on a given input, despite of
possibly different choices of edges at line 4. We do
this through several intermediate results.
A derivation for an adjacency graph G is a se-
quence of edges d D he1, ... , eni, n &gt; 1, such
that G D G0 and Gi_1 )ei Gi for every i with
1 &lt; i &lt; n. For short, we write G0 )d Gn.
Two derivations for G are competing if one is a
permutation of the other.
Lemma 3 If G )d1 G1 and G )d2 G2 with d1
and d2 competing derivations, then G1 D G2.
PROOF We claim that the statement of the lemma
holds for jd1j D 2. To see this, let G )e1
G&apos;1 )e2 G1 and G )e2 G&apos;2 )e1 G2 be valid
derivations. We observe that G1 and G2 have the
same set of vertices. Since the edges of G1 and G2
are defined by restricting the adjacency relation to
their set of vertices, our claim immediately follows.
The statement of the lemma then follows from
the above claim and from the fact that we can al-
ways obtain the sequence d2 starting from d1 by
repeatedly switching consecutive edges. 0
We now consider derivations for the same adja-
cency graph that are not competing, and show that
they always lead to isomorphic adjacency graphs.
Two graphs are isomorphic if they become equal
after some suitable renaming of the vertices.
Lemma 4 The out-degree of G is bounded by 2.
PROOF Assume v !G v1 and v !G v2, with v1 ¤
v2, and let I 2 v. I must be adjacent to some in-
terval I1 2 v1. Without loss of generality, assume
that I is left-adjacent to I1. I must also be adja-
cent to some interval I2 2 v2. Since v1 and v2
</bodyText>
<page confidence="0.998534">
483
</page>
<bodyText confidence="0.9316274">
are disjoint, I must be right-adjacent to I2. This
implies that I cannot be adjacent to an interval in
any other m-interval v0 of G. 0
A vertex v of G such that v --*G v1 and v --*G v2
is called a bifurcation.
</bodyText>
<figureCaption confidence="0.479262">
Example 7 Assume v = { Œ2; 2• }, v1 =
{ Œ3; 3•; Œ5; 5• }, v2 = {Œ1; 1• } with v --*G v1 and
v --*G v2. The m-interval v ® v1 = { Œ2; 3•; Œ5; 5• }
</figureCaption>
<bodyText confidence="0.998401301075269">
is no longer adjacent to v2. ❑
The example above shows that, when choosing one
of the two outgoing edges in a bifurcation for mer-
ging, the other edge might not survive. Thus, such
a choice might lead to distinguishable derivations
that are not competing (one derivation has an edge
that is not present in the other). As we will see (in
the proof of Theorem 1), bifurcations are the only
cases in which edges might not survive a merging.
Lemma 5 Let v be a bifurcation of G with outgo-
ing edges e1; e2, and let G me1 G1, G =:�e2 G2.
Then G1 and G2 are isomorphic.
PROOF (SKETCH) Assume e1 has the form
v --*G v1 and e2 has the form v --*G v2. Let
also VS be the set of vertices shared by G1 and
G2. We show that the statement holds under the
isomorphism mapping v ® v1 and v2 in G1 to v1
and v ® v2 in G2, respectively.
When restricted to VS, the graphs G1 and G2
are equal. Let us then consider edges from G1 and
G2 involving exactly one vertex in VS. We show
that, for v0 E VS, v0 --*G1 v ® v1 if and only if
v0 --*G2 v1. Consider an arbitrary interval I0 E v0.
If v0 --*G1 v ®v1, then I0 must be adjacent to some
interval I1 E v ® v1. If I1 E v1 we are done.
Otherwise, I1 must be the concatenation of two
intervals I1v and I1v1 with I1v E v and I1v1 E
v1. Since v --*G2 v2, I1v is also adjacent to some
interval in v2. However, v0 and v2 are disjoint.
Thus I0 must be adjacent to I1v1 E v1. Conversely,
if v0 --*G2 v1, then I0 must be adjacent to some
interval I1 E v1. Because v0 and v are disjoint, I0
must also be adjacent to some interval in v ® v1.
Using very similar arguments, we can conclude
that G1 and G2 are isomorphic when restricted to
edges with at most one vertex in VS.
Finally, we need to consider edges from G1 and
G2 that are not incident upon vertices in VS. We
show that v ® v1 --*G1 v2 only if v1 --*G2 v ® v2;
a similar argument can be used to prove the con-
verse. Consider an arbitrary interval I1 E v®v1. If
v ® v1 --*G1 v2, then I1 must be adjacent to some
interval I2 E v2. If I1 E v1 we are done. Other-
wise, I1 must be the concatenation of two adjacent
intervals I1v and I1v1 with I1v E v and I1v1 E v1.
Since I1v is also adjacent to some interval I20 E v2
(here I02 might as well be I2), we conclude that
I1v1 E v1 is adjacent to the concatenation of I1v
and I02, which is indeed an interval in v ® v2. Note
that our case distinction is exhaustive. We thus
conclude that v1 --*G2 v ® v2.
A symmetrical argument can be used to show
that v2 --*G1 v ® v1 if and only if v ® v2 --*G2 v1,
which concludes our proof. 0
Theorem 1 Let d1 and d2 be derivations for G,
describing two different computations c1 and c2 of
the algorithm of Figure 3 on input G. Computation
c1 is accepting if and only if c2 is accepting.
PROOF First, we prove the claim that if e is not an
edge outgoing from a bifurcation vertex, then in the
derive relation G =:�e G0 all of the edges of G but
e and its reverse are inherited by G0. Let us write
e in the form v1 --*G v2. Obviously, any edge of
G not incident upon v1 or v2 will be inherited by
G0. If v --*G v2 for some m-interval v # v1, then
every interval I E v is adjacent to some interval
in v2. Since v and v1 are disjoint, I will also be
adjacent to some interval in v1 ® v2. Thus we have
v --*Gl v1 ® v2. A similar argument shows that
v --*G v1 implies v --*Gl v1 ® v2.
If v2 --*G v for some v # v1, then every in-
terval I E v2 is adjacent to some interval in v.
From v1 --*G v2 we also have that each interval
I12 E v1 ® v2 is either an interval in v2 or else
the concatenation of exactly two intervals I1 E v1
and I2 E v2. (The interval I2 cannot be adjacent
to more than an interval in v1, because v2 --*G v).
In both cases I12 is adjacent to some interval in
v, and hence v1 ® v2 --*Gl v. This concludes the
proof of our claim.
Let d1, d2 be as in the statement of the the-
orem, with G =:�d1 G1 and G =:�d2 G2. If d1
and d2 are competing, then the theorem follows
from Lemma 3. Otherwise, assume that d1 and d2
are not competing. From our claim above, some
bifurcation vertices must appear in these deriva-
tions. Let us reorder the edges in d1 in such a way
that edges outgoing from a bifurcation vertex are
processed last and in some canonical order. The
resulting derivation has the form dd01, where d01
involves the processing of all bifurcation vertices.
We can also reorder edges in d2 to obtain dd02,
where d02 involves the processing of all bifurcation
</bodyText>
<page confidence="0.996212">
484
</page>
<table confidence="0.997354333333333">
not context-free 102 687 100.00%
not binarizable 24 0.02%
not well-nested 622 0.61%
</table>
<tableCaption confidence="0.8414235">
Table 1: Properties of productions extracted from
the CoNLL 2006 data (3 794 605 productions)
</tableCaption>
<bodyText confidence="0.998511696969697">
vertices in exactly the same order as in di, but with
possibly different choices for the outgoing edges.
Let G Ed Gd Edi Gi and G Ed Gd Edi
G?. Derivations ddi and d1 are competing. Thus,
by Lemma 3, we have Gi = G1. Similarly, we can
conclude that G? = G2. Since bifurcation vertices
in di and in d2 are processed in the same canonical
order, from repeated applications of Lemma 5 we
have that Gi and G? are isomorphic. We then con-
clude that G1 and G2 are isomorphic as well. The
statement of the theorem follows immediately. 0
We now turn to a computational analysis of the
algorithm of Figure 3. Let G be the representation
of an LCFRS production p with rank r. G has
r vertices and, following Lemma 4, O(r) edges.
Let v be an m-interval of G with fan-out fv. The
incoming and outgoing edges for v can be detected
in time O(fv) by inspecting the 2 • fv endpoints of
v. Thus we can compute G in time O(IpI).
The number of iterations of the while cycle in the
algorithm is bounded by r, since at each iteration
one vertex of G is removed. Consider now an
iteration in which m-intervals v1 and v2 have been
chosen for merging, with v1 --*G v2. (These m-
intervals might be associated with nonterminals
in the right-hand side of p, or else might have
been obtained as the result of previous merging
operations.) Again, we can compute the incoming
and outgoing edges of v1 ® v2 in time proportional
to the number of endpoints of such an m-interval.
By Lemma 2, this number is bounded by O(f ), f
the fan-out of the grammar. We thus conclude that
a run of the algorithm on G takes time O(r • f ).
</bodyText>
<sectionHeader confidence="0.999861" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999948473684211">
We have shown how to extract mildly context-
sensitive grammars from dependency treebanks,
and presented an efficient algorithm that attempts
to convert these grammars into an efficiently par-
seable binary form. Due to previous results (Ram-
bow and Satta, 1999), we know that this is not
always possible. However, our algorithm may fail
even in cases where a binarization exists—our no-
tion of adjacency is not strong enough to capture
all binarizable cases. This raises the question about
the practical relevance of our technique.
In order to get at least a preliminary answer to
this question, we extracted LCFRS productions
from the data used in the 2006 CoNLL shared task
on data-driven dependency parsing (Buchholz and
Marsi, 2006), and evaluated how large a portion
of these productions could be binarized using our
algorithm. The results are given in Table 1. Since it
is easy to see that our algorithm always succeeds on
context-free productions (productions where each
nonterminal has fan-out 1), we evaluated our al-
gorithm on the 102 687 productions with a higher
fan-out. Out of these, only 24 (0.02%) could not be
binarized using our technique. We take this number
as an indicator for the usefulness of our result.
It is interesting to compare our approach
with techniques for well-nested dependency trees
(Kuhlmann and Nivre, 2006). Well-nestedness is
a property that implies the binarizability of the
extracted grammar; however, the classes of well-
nested trees and those whose corresponding pro-
ductions can be binarized using our algorithm are
incomparable—in particular, there are well-nested
productions that cannot be binarized in our frame-
work. Nevertheless, the coverage of our technique
is actually higher than that of an approach that
relies on well-nestedness, at least on the CoNLL
2006 data (see again Table 1).
We see our results as promising first steps in a
thorough exploration of the connections between
non-projective and mildly context-sensitive pars-
ing. The obvious next step is the evaluation of our
technique in the context of an actual parser.
As a final remark, we would like to point out
that an alternative technique for efficient non-pro-
jective dependency parsing, developed by Gómez
Rodríguez et al. independently of this work, is
presented elsewhere in this volume.
Acknowledgements We would like to thank
Ryan McDonald, Joakim Nivre, and the anonym-
ous reviewers for useful comments on drafts of this
paper, and Carlos Gómez Rodríguez and David J.
Weir for making a preliminary version of their pa-
per available to us. The work of the first author
was funded by the Swedish Research Council. The
second author was partially supported by MIUR
under project PRIN No. 2007TJNZRE_002.
</bodyText>
<page confidence="0.998892">
485
</page>
<sectionHeader confidence="0.998341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999736537634408">
Giuseppe Attardi. 2006. Experiments with a mul-
tilanguage non-projective dependency parser. In
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 166–170, New
York, USA.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency pars-
ing. In Tenth Conference on Computational Natural
Language Learning (CoNLL), pages 149–164, New
York, USA.
Eugene Charniak. 1996. Tree-bank grammars. In 13th
National Conference on Artificial Intelligence, pages
1031–1036, Portland, Oregon, USA.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In 16th In-
ternational Conference on Computational Linguist-
ics (COLING), pages 340–345, Copenhagen, Den-
mark.
Carlos Gómez-Rodríguez, David J. Weir, and John
Carroll. 2009. Parsing mildly non-projective de-
pendency structures. In Twelfth Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL), Athens, Greece.
Jan Hajiˇc, Barbora Vidova Hladka, Jarmila Panevová,
Eva Hajiˇcová, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. Linguistic Data
Consortium, 2001T10.
Keith Hall and Václav Novák. 2005. Corrective mod-
elling for non-projective dependency grammar. In
Ninth International Workshop on Parsing Technolo-
gies (IWPT), pages 42–52, Vancouver, Canada.
Jiˇrí Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In 45th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 608–615, Prague, Czech Republic.
Marco Kuhlmann and Mathias Möhl. 2007. Mildly
context-sensitive dependency languages. In 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 160–167, Prague, Czech
Republic.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In 21st In-
ternational Conference on Computational Linguist-
ics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), Main
Conference Poster Sessions, pages 507–514, Sydney,
Australia.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Eleventh Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 81–88, Trento, Italy.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Tenth International Conference on Pars-
ing Technologies (IWPT), pages 121–132, Prague,
Czech Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Human Lan-
guage Technology Conference (HLT) and Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 523–530, Vancouver,
Canada.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 99–106, Ann Arbor, USA.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Eighth International
Workshop on Parsing Technologies (IWPT), pages
149–160, Nancy, France.
Joakim Nivre. 2007. Incremental non-projective
dependency parsing. In Human Language Tech-
nologies: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 396–403, Rochester,
NY, USA.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theoretical Computer Science, 223(1–2):87–
120.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On Multiple Context-
Free Grammars. Theoretical Computer Science,
88(2):191–229.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In 25th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 104–111, Stanford,
CA, USA.
</reference>
<page confidence="0.999037">
486
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.792334">
<title confidence="0.99965">Treebank Grammar Techniques for Non-Projective Dependency Parsing</title>
<author confidence="0.999904">Marco Kuhlmann Giorgio Satta</author>
<affiliation confidence="0.999889">Uppsala University University of Padua</affiliation>
<address confidence="0.873888">Uppsala, Sweden Padova, Italy</address>
<email confidence="0.908443">marco.kuhlmann@lingfil.uu.sesatta@dei.unipd.it</email>
<abstract confidence="0.99919815">An open problem in dependency parsing is the accurate and efficient treatment of non-projective structures. We propose to attack this problem using chart-parsing algorithms developed for mildly contextsensitive grammar formalisms. In this paper, we provide two key tools for this approach. First, we show how to reduce nonprojective dependency parsing to parsing with Linear Context-Free Rewriting Systems (LCFRS), by presenting a technique for extracting LCFRS from dependency treebanks. For efficient parsing, the extracted grammars need to be transformed in order to minimize the number of nonterminal symbols per production. Our second contribution is an algorithm that computes this transformation for a large, empirically relevant class of grammars.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
</authors>
<title>Experiments with a multilanguage non-projective dependency parser.</title>
<date>2006</date>
<booktitle>In Tenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>166--170</pages>
<location>New York, USA.</location>
<contexts>
<context position="2342" citStr="Attardi, 2006" startWordPosition="348" endWordPosition="349">) are nonprojective, and for German and Dutch treebanks, the proportion of non-projective structures is even higher (Havelka, 2007). The problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in the literature. Some authors propose to solve it by techniques for recovering non-projectivity from the output of a projective parser in a post-processing step (Hall and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). In this paper, we propose to attack non-projective dependency parsing in a principled way, using polynomial chart-parsing algorithms developed for </context>
</contexts>
<marker>Attardi, 2006</marker>
<rawString>Giuseppe Attardi. 2006. Experiments with a multilanguage non-projective dependency parser. In Tenth Conference on Computational Natural Language Learning (CoNLL), pages 166–170, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLLX shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Tenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>149--164</pages>
<location>New York, USA.</location>
<contexts>
<context position="33088" citStr="Buchholz and Marsi, 2006" startWordPosition="6275" endWordPosition="6278">efficient algorithm that attempts to convert these grammars into an efficiently parseable binary form. Due to previous results (Rambow and Satta, 1999), we know that this is not always possible. However, our algorithm may fail even in cases where a binarization exists—our notion of adjacency is not strong enough to capture all binarizable cases. This raises the question about the practical relevance of our technique. In order to get at least a preliminary answer to this question, we extracted LCFRS productions from the data used in the 2006 CoNLL shared task on data-driven dependency parsing (Buchholz and Marsi, 2006), and evaluated how large a portion of these productions could be binarized using our algorithm. The results are given in Table 1. Since it is easy to see that our algorithm always succeeds on context-free productions (productions where each nonterminal has fan-out 1), we evaluated our algorithm on the 102 687 productions with a higher fan-out. Out of these, only 24 (0.02%) could not be binarized using our technique. We take this number as an indicator for the usefulness of our result. It is interesting to compare our approach with techniques for well-nested dependency trees (Kuhlmann and Nivr</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLLX shared task on multilingual dependency parsing. In Tenth Conference on Computational Natural Language Learning (CoNLL), pages 149–164, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In 13th National Conference on Artificial Intelligence,</booktitle>
<pages>1031--1036</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="4333" citStr="Charniak, 1996" startWordPosition="633" endWordPosition="634">ystems for non-projective dependency parsing. Hence, mildly non-projective dependency parsing promises to be both efficient and accurate. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 478–486, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 478 Contributions In this paper, we contribute two key tools for making the mildly context-sensitive approach to accurate and efficient non-projective dependency parsing work. First, we extend the standard technique for extracting context-free grammars from phrase-structure treebanks (Charniak, 1996) to mildly context-sensitive grammars and dependency treebanks. More specifically, we show how to extract, from a given dependency treebank, a lexicalized Linear Context-Free Rewriting System (LCFRS) whose derivations capture the dependency analyses in the treebank in the same way as the derivations of a context-free treebank grammar capture phrasestructure analyses. Our technique works for arbitrary, even non-projective dependency treebanks, and essentially reduces non-projective dependency to parsing with LCFRS. This problem can be solved using standard chart-parsing techniques. Our extracti</context>
<context position="8797" citStr="Charniak, 1996" startWordPosition="1443" endWordPosition="1444"> the equation to the right of the semicolon specifies the semantics of g. For each i E [r], xi is an fi-tuple of variables, and a� = (a1, ... , af ) is a tuple of strings over the variables on the left-hand side of the equation and the alphabet of terminal symbols in which each variable appears exactly once. The production p is said to have rank r, fan-out f , and length Ia1I + • • • + Iaf I + (f —1). 479 3 Grammar Extraction We now explain how to extract an LCFRS from a dependency treebank, in very much the same way as a context-free grammar can be extracted from a phrase-structure treebank (Charniak, 1996). 3.1 Dependency Treebank Grammars A simple way to induce a context-free grammar from a phrase-structure treebank is to read off the productions of the grammar from the trees. We will specify a procedure for extracting, from a given dependency treebank, a lexicalized LCFRS G that is adequate in the sense that for every analysis D of a sentence w in the treebank, there is a derivation tree of G that is isomorphic to D, meaning that it becomes equal to D after a suitable renaming and relabelling of nodes, and has w as its derived string. Here, a derivation tree of an LCFRS G is an ordered tree s</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>Eugene Charniak. 1996. Tree-bank grammars. In 13th National Conference on Artificial Intelligence, pages 1031–1036, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In 16th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>340--345</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1505" citStr="Eisner, 1996" startWordPosition="220" endWordPosition="221">t computes this transformation for a large, empirically relevant class of grammars. 1 Introduction Dependency parsing is the task of predicting the most probable dependency structure for a given sentence. One of the key choices in dependency parsing is about the class of candidate structures for this prediction. Many parsers are confined to projective structures, in which the yield of a syntactic head is required to be continuous. A major benefit of this choice is computational efficiency: an exhaustive search over all projective structures can be done in cubic, greedy parsing in linear time (Eisner, 1996; Nivre, 2003). A major drawback of the restriction to projective dependency structures is a potential loss in accuracy. For example, around 23% of the analyses in the Prague Dependency Treebank of Czech (Hajiˇc et al., 2001) are nonprojective, and for German and Dutch treebanks, the proportion of non-projective structures is even higher (Havelka, 2007). The problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in the literature. Some authors propose to solve it by techniques for recovering non-projectivity from the</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In 16th International Conference on Computational Linguistics (COLING), pages 340–345, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Gómez-Rodríguez</author>
<author>David J Weir</author>
<author>John Carroll</author>
</authors>
<title>Parsing mildly non-projective dependency structures.</title>
<date>2009</date>
<booktitle>In Twelfth Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<location>Athens, Greece.</location>
<marker>Gómez-Rodríguez, Weir, Carroll, 2009</marker>
<rawString>Carlos Gómez-Rodríguez, David J. Weir, and John Carroll. 2009. Parsing mildly non-projective dependency structures. In Twelfth Conference of the European Chapter of the Association for Computational Linguistics (EACL), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Barbora Vidova Hladka, Jarmila Panevová, Eva Hajiˇcová, Petr Sgall, and Petr Pajas.</title>
<date>2001</date>
<marker>Hajiˇc, 2001</marker>
<rawString>Jan Hajiˇc, Barbora Vidova Hladka, Jarmila Panevová, Eva Hajiˇcová, Petr Sgall, and Petr Pajas. 2001. Prague Dependency Treebank 1.0. Linguistic Data Consortium, 2001T10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Václav Novák</author>
</authors>
<title>Corrective modelling for non-projective dependency grammar.</title>
<date>2005</date>
<booktitle>In Ninth International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>42--52</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="2183" citStr="Hall and Novák, 2005" startWordPosition="324" endWordPosition="327">jective dependency structures is a potential loss in accuracy. For example, around 23% of the analyses in the Prague Dependency Treebank of Czech (Hajiˇc et al., 2001) are nonprojective, and for German and Dutch treebanks, the proportion of non-projective structures is even higher (Havelka, 2007). The problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in the literature. Some authors propose to solve it by techniques for recovering non-projectivity from the output of a projective parser in a post-processing step (Hall and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pere</context>
</contexts>
<marker>Hall, Novák, 2005</marker>
<rawString>Keith Hall and Václav Novák. 2005. Corrective modelling for non-projective dependency grammar. In Ninth International Workshop on Parsing Technologies (IWPT), pages 42–52, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiˇrí Havelka</author>
</authors>
<title>Beyond projectivity: Multilingual evaluation of constraints and measures on nonprojective structures.</title>
<date>2007</date>
<booktitle>In 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>608--615</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1860" citStr="Havelka, 2007" startWordPosition="275" endWordPosition="276"> structures, in which the yield of a syntactic head is required to be continuous. A major benefit of this choice is computational efficiency: an exhaustive search over all projective structures can be done in cubic, greedy parsing in linear time (Eisner, 1996; Nivre, 2003). A major drawback of the restriction to projective dependency structures is a potential loss in accuracy. For example, around 23% of the analyses in the Prague Dependency Treebank of Czech (Hajiˇc et al., 2001) are nonprojective, and for German and Dutch treebanks, the proportion of non-projective structures is even higher (Havelka, 2007). The problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in the literature. Some authors propose to solve it by techniques for recovering non-projectivity from the output of a projective parser in a post-processing step (Hall and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree </context>
<context position="17402" citStr="Havelka, 2007" startWordPosition="3110" endWordPosition="3111">.1P 1•1w1f.r+1//, where P is the set of productions of the input grammar G, w is the input string, r is the maximal rank, and f is the maximal fan-out of a production in G (Seki et al., 1991). For a grammar G extracted by our technique, the number f equals the maximal block-degree per node. Hence, without any further modification, we obtain a parsing algorithm that is polynomial in the length of the sentence, but exponential in both the block-degree and the rank. This is clearly unacceptable in practical systems. The relative frequency of analyses with a block-degree &gt; 2 is almost negligible (Havelka, 2007); the bigger obstacle in applying the treebank grammar is the rank of the resulting LCFRS. Therefore, in the remainder of the paper, we present an algorithm that can transform the productions of the input grammar G into an equivalent set of productions with rank at most 2, while preserving the fan-out. This transformation, if it succeeds, yields a parsing algorithm that runs in time O.1P 1 • r • 1w13f /. 481 1A 2hearing 3is 4scheduled5on 6the 7issue 8today nmod ! g1 g1 D hAi sbj ! g2(nmod, pp) g2(hx1;1i, hx2;1i) D hx1;1 hearing, x2;1i root ! g3(sbj,vc) g3(hx1;1,x1;2i, hx2;1, x2;2i) D hx1;1 is </context>
</contexts>
<marker>Havelka, 2007</marker>
<rawString>Jiˇrí Havelka. 2007. Beyond projectivity: Multilingual evaluation of constraints and measures on nonprojective structures. In 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 608–615, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Mathias Möhl</author>
</authors>
<title>Mildly context-sensitive dependency languages.</title>
<date>2007</date>
<booktitle>In 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3440" citStr="Kuhlmann and Möhl, 2007" startWordPosition="504" endWordPosition="507">pose to attack non-projective dependency parsing in a principled way, using polynomial chart-parsing algorithms developed for mildly context-sensitive grammar formalisms. This proposal is motivated by the observation that most dependency structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Kuhlmann and Nivre, 2006), and by the close link between such ‘mildly non-projective’ dependency structures on the one hand, and grammar formalisms with mildly context-sensitive generative capacity on the other (Kuhlmann and Möhl, 2007). Furthermore, as pointed out by McDonald and Satta (2007), chart-parsing algorithms are amenable to augmentation by non-local information such as arity constraints and Markovization, and therefore should allow for more predictive statistical models than those used by current systems for non-projective dependency parsing. Hence, mildly non-projective dependency parsing promises to be both efficient and accurate. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 478–486, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 478 Cont</context>
<context position="9939" citStr="Kuhlmann and Möhl, 2007" startWordPosition="1650" endWordPosition="1653"> as its derived string. Here, a derivation tree of an LCFRS G is an ordered tree such that each node u is labelled with a production p of G, the number of children of u equals the rank r of p, and for each i E [r], the ith child of u is labelled with a production that has as its left-hand side the ith nonterminal on the right-hand side of p. The basic idea behind our extraction procedure is that, in order to represent the compositional structure of a possibly non-projective dependency tree, one needs to represent the decomposition and relative order not of subtrees, but of blocks of subtrees (Kuhlmann and Möhl, 2007). We introduce some terminology. A component of a node u in a dependency tree is either a block B of some child u&apos; of u, or the singleton interval that contains u; this interval will represent the position in the string that is occupied by the lexical item corresponding to u. We say that u&apos; contributes B, and that u contributes [u, u] to u. Notice that the number of components that u&apos; contributes to its parent u equals the block-degree of u&apos;. Our goal is to construct for u a production of an LCFRS that specifies how each block of u decomposes into components, and how these components are order</context>
</contexts>
<marker>Kuhlmann, Möhl, 2007</marker>
<rawString>Marco Kuhlmann and Mathias Möhl. 2007. Mildly context-sensitive dependency languages. In 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Joakim Nivre</author>
</authors>
<title>Mildly non-projective dependency structures.</title>
<date>2006</date>
<booktitle>In 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), Main Conference Poster Sessions,</booktitle>
<pages>507--514</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3229" citStr="Kuhlmann and Nivre, 2006" startWordPosition="473" endWordPosition="476">haustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). In this paper, we propose to attack non-projective dependency parsing in a principled way, using polynomial chart-parsing algorithms developed for mildly context-sensitive grammar formalisms. This proposal is motivated by the observation that most dependency structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Kuhlmann and Nivre, 2006), and by the close link between such ‘mildly non-projective’ dependency structures on the one hand, and grammar formalisms with mildly context-sensitive generative capacity on the other (Kuhlmann and Möhl, 2007). Furthermore, as pointed out by McDonald and Satta (2007), chart-parsing algorithms are amenable to augmentation by non-local information such as arity constraints and Markovization, and therefore should allow for more predictive statistical models than those used by current systems for non-projective dependency parsing. Hence, mildly non-projective dependency parsing promises to be bo</context>
<context position="5331" citStr="Kuhlmann and Nivre, 2006" startWordPosition="774" endWordPosition="777"> technique works for arbitrary, even non-projective dependency treebanks, and essentially reduces non-projective dependency to parsing with LCFRS. This problem can be solved using standard chart-parsing techniques. Our extraction technique yields a grammar whose parsing complexity is polynomial in the length of the sentence, but exponential in both a measure of the non-projectivity of the treebank and the maximal number of dependents per word, reflected as the rank of the extracted LCFRS. While the number of highly non-projective dependency structures is negligible for practical applications (Kuhlmann and Nivre, 2006), the rank cannot easily be bounded. Therefore, we present an algorithm that transforms the extracted grammar into a normal form that has rank 2, and thus can be parsed more efficiently. This contribution is important even independently of the extraction procedure: While it is known that a rank-2 normal form of LCFRS does not exist in the general case (Rambow and Satta, 1999), our algorithm succeeds for a large and empirically relevant class of grammars. 2 Preliminaries We start by introducing dependency trees and Linear Context-Free Rewriting Systems (LCFRS). Throughout the paper, for positiv</context>
<context position="33696" citStr="Kuhlmann and Nivre, 2006" startWordPosition="6375" endWordPosition="6378"> and Marsi, 2006), and evaluated how large a portion of these productions could be binarized using our algorithm. The results are given in Table 1. Since it is easy to see that our algorithm always succeeds on context-free productions (productions where each nonterminal has fan-out 1), we evaluated our algorithm on the 102 687 productions with a higher fan-out. Out of these, only 24 (0.02%) could not be binarized using our technique. We take this number as an indicator for the usefulness of our result. It is interesting to compare our approach with techniques for well-nested dependency trees (Kuhlmann and Nivre, 2006). Well-nestedness is a property that implies the binarizability of the extracted grammar; however, the classes of wellnested trees and those whose corresponding productions can be binarized using our algorithm are incomparable—in particular, there are well-nested productions that cannot be binarized in our framework. Nevertheless, the coverage of our technique is actually higher than that of an approach that relies on well-nestedness, at least on the CoNLL 2006 data (see again Table 1). We see our results as promising first steps in a thorough exploration of the connections between non-project</context>
</contexts>
<marker>Kuhlmann, Nivre, 2006</marker>
<rawString>Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-projective dependency structures. In 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), Main Conference Poster Sessions, pages 507–514, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>81--88</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="2793" citStr="McDonald and Pereira, 2006" startWordPosition="412" endWordPosition="415">l and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). In this paper, we propose to attack non-projective dependency parsing in a principled way, using polynomial chart-parsing algorithms developed for mildly context-sensitive grammar formalisms. This proposal is motivated by the observation that most dependency structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Kuhlmann and Nivre, 2006), and by the close link between such ‘mildly non-projective’ dependency structures on the one hand, and grammar formalisms with mildly context-sensitive generative </context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 81–88, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Giorgio Satta</author>
</authors>
<title>On the complexity of non-projective data-driven dependency parsing.</title>
<date>2007</date>
<booktitle>In Tenth International Conference on Parsing Technologies (IWPT),</booktitle>
<pages>121--132</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2713" citStr="McDonald and Satta, 2007" startWordPosition="400" endWordPosition="403">ectivity from the output of a projective parser in a post-processing step (Hall and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). In this paper, we propose to attack non-projective dependency parsing in a principled way, using polynomial chart-parsing algorithms developed for mildly context-sensitive grammar formalisms. This proposal is motivated by the observation that most dependency structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Kuhlmann and Nivre, 2006), and by the close link between such ‘mildly non-projective’ dependency structures o</context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>Ryan McDonald and Giorgio Satta. 2007. On the complexity of non-projective data-driven dependency parsing. In Tenth International Conference on Parsing Technologies (IWPT), pages 121–132, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Human Language Technology Conference (HLT) and Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>523--530</pages>
<location>Vancouver, Canada.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Human Language Technology Conference (HLT) and Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–530, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudoprojective dependency parsing.</title>
<date>2005</date>
<booktitle>In 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>99--106</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="2209" citStr="Nivre and Nilsson, 2005" startWordPosition="328" endWordPosition="331">uctures is a potential loss in accuracy. For example, around 23% of the analyses in the Prague Dependency Treebank of Czech (Hajiˇc et al., 2001) are nonprojective, and for German and Dutch treebanks, the proportion of non-projective structures is even higher (Havelka, 2007). The problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in the literature. Some authors propose to solve it by techniques for recovering non-projectivity from the output of a projective parser in a post-processing step (Hall and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). In this paper,</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudoprojective dependency parsing. In 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 99–106, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Eighth International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>149--160</pages>
<location>Nancy, France.</location>
<contexts>
<context position="1519" citStr="Nivre, 2003" startWordPosition="222" endWordPosition="223">s transformation for a large, empirically relevant class of grammars. 1 Introduction Dependency parsing is the task of predicting the most probable dependency structure for a given sentence. One of the key choices in dependency parsing is about the class of candidate structures for this prediction. Many parsers are confined to projective structures, in which the yield of a syntactic head is required to be continuous. A major benefit of this choice is computational efficiency: an exhaustive search over all projective structures can be done in cubic, greedy parsing in linear time (Eisner, 1996; Nivre, 2003). A major drawback of the restriction to projective dependency structures is a potential loss in accuracy. For example, around 23% of the analyses in the Prague Dependency Treebank of Czech (Hajiˇc et al., 2001) are nonprojective, and for German and Dutch treebanks, the proportion of non-projective structures is even higher (Havelka, 2007). The problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in the literature. Some authors propose to solve it by techniques for recovering non-projectivity from the output of a p</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Eighth International Workshop on Parsing Technologies (IWPT), pages 149–160, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incremental non-projective dependency parsing.</title>
<date>2007</date>
<booktitle>In Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>396--403</pages>
<location>Rochester, NY, USA.</location>
<contexts>
<context position="2356" citStr="Nivre, 2007" startWordPosition="350" endWordPosition="351">tive, and for German and Dutch treebanks, the proportion of non-projective structures is even higher (Havelka, 2007). The problem of non-projective dependency parsing under the joint requirement of accuracy and efficiency has only recently been addressed in the literature. Some authors propose to solve it by techniques for recovering non-projectivity from the output of a projective parser in a post-processing step (Hall and Novák, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). McDonald et al. (2005) formulate dependency parsing as the search for the most probable spanning tree over the full set of all possible dependencies. However, this approach is limited to probability models with strong independence assumptions. Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). In this paper, we propose to attack non-projective dependency parsing in a principled way, using polynomial chart-parsing algorithms developed for mildly context</context>
</contexts>
<marker>Nivre, 2007</marker>
<rawString>Joakim Nivre. 2007. Incremental non-projective dependency parsing. In Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 396–403, Rochester, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>Giorgio Satta</author>
</authors>
<title>Independent parallelism in finite copying parallel rewriting systems.</title>
<date>1999</date>
<journal>Theoretical Computer Science,</journal>
<volume>223</volume>
<issue>1</issue>
<pages>120</pages>
<contexts>
<context position="5709" citStr="Rambow and Satta, 1999" startWordPosition="839" endWordPosition="842">vity of the treebank and the maximal number of dependents per word, reflected as the rank of the extracted LCFRS. While the number of highly non-projective dependency structures is negligible for practical applications (Kuhlmann and Nivre, 2006), the rank cannot easily be bounded. Therefore, we present an algorithm that transforms the extracted grammar into a normal form that has rank 2, and thus can be parsed more efficiently. This contribution is important even independently of the extraction procedure: While it is known that a rank-2 normal form of LCFRS does not exist in the general case (Rambow and Satta, 1999), our algorithm succeeds for a large and empirically relevant class of grammars. 2 Preliminaries We start by introducing dependency trees and Linear Context-Free Rewriting Systems (LCFRS). Throughout the paper, for positive integers i and j, we write [i, j ] for the interval { k I i &lt; k &lt; j }, and use [n] as a shorthand for [1, n]. 2.1 Dependency Trees Dependency parsing is the task to assign dependency structures to a given sentence w. For the purposes of this paper, dependency structures are edge-labelled trees. More formally, let w be a sentence, understood as a sequence of tokens over some</context>
<context position="32614" citStr="Rambow and Satta, 1999" startWordPosition="6196" endWordPosition="6200">ght have been obtained as the result of previous merging operations.) Again, we can compute the incoming and outgoing edges of v1 ® v2 in time proportional to the number of endpoints of such an m-interval. By Lemma 2, this number is bounded by O(f ), f the fan-out of the grammar. We thus conclude that a run of the algorithm on G takes time O(r • f ). 5 Discussion We have shown how to extract mildly contextsensitive grammars from dependency treebanks, and presented an efficient algorithm that attempts to convert these grammars into an efficiently parseable binary form. Due to previous results (Rambow and Satta, 1999), we know that this is not always possible. However, our algorithm may fail even in cases where a binarization exists—our notion of adjacency is not strong enough to capture all binarizable cases. This raises the question about the practical relevance of our technique. In order to get at least a preliminary answer to this question, we extracted LCFRS productions from the data used in the 2006 CoNLL shared task on data-driven dependency parsing (Buchholz and Marsi, 2006), and evaluated how large a portion of these productions could be binarized using our algorithm. The results are given in Tabl</context>
</contexts>
<marker>Rambow, Satta, 1999</marker>
<rawString>Owen Rambow and Giorgio Satta. 1999. Independent parallelism in finite copying parallel rewriting systems. Theoretical Computer Science, 223(1–2):87– 120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Seki</author>
<author>Takashi Matsumura</author>
<author>Mamoru Fujii</author>
<author>Tadao Kasami</author>
</authors>
<title>On Multiple ContextFree Grammars.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<volume>88</volume>
<issue>2</issue>
<contexts>
<context position="16979" citStr="Seki et al., 1991" startWordPosition="3039" endWordPosition="3042"> the segments of the tuple a) decompose into the components of u, and of the relative order of the components. As an example, Figure 2 shows the productions extracted from the tree above. 3.4 Parsing the Extracted Grammar Once we have extracted the grammar for a dependency treebank, we can apply any parsing algorithm for LCFRS to non-projective dependency parsing. The generic chart-parsing algorithm for LCFRS runs in time O.1P 1•1w1f.r+1//, where P is the set of productions of the input grammar G, w is the input string, r is the maximal rank, and f is the maximal fan-out of a production in G (Seki et al., 1991). For a grammar G extracted by our technique, the number f equals the maximal block-degree per node. Hence, without any further modification, we obtain a parsing algorithm that is polynomial in the length of the sentence, but exponential in both the block-degree and the rank. This is clearly unacceptable in practical systems. The relative frequency of analyses with a block-degree &gt; 2 is almost negligible (Havelka, 2007); the bigger obstacle in applying the treebank grammar is the rank of the resulting LCFRS. Therefore, in the remainder of the paper, we present an algorithm that can transform t</context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and Tadao Kasami. 1991. On Multiple ContextFree Grammars. Theoretical Computer Science, 88(2):191–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
<author>Aravind K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In 25th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>104--111</pages>
<location>Stanford, CA, USA.</location>
<contexts>
<context position="7759" citStr="Vijay-Shanker et al., 1987" startWordPosition="1231" endWordPosition="1234"> u. The number of blocks of u is called the block-degree of u. The blockdegree of a dependency tree is the maximum among the block-degrees of its nodes. A dependency tree is projective, if its block-degree is 1. Example 2 The tree shown in Figure 2 is not projective: both node 2 (hearing) and node 4 (scheduled) have block-degree 2. Their blocks are { 2 }, {5, 6, 7 } and { 4 }, { 8 }, respectively. 2.2 LCFRS Linear Context-Free Rewriting Systems (LCFRS) have been introduced as a generalization of several mildly context-sensitive grammar formalisms. Here we use the standard definition of LCFRS (Vijay-Shanker et al., 1987) and only fix our notation; for a more thorough discussion of this formalism, we refer to the literature. Let G be an LCFRS. Recall that each nonterminal symbol A of G comes with a positive integer called the fan-out of A, and that a production p of G has the form A --* g(A1, ... , Ar) ; g(�x1, ... , �xr) = a�, where A, A1, ... , Ar are nonterminals with fan-out f, f1, ... , fr, respectively, g is a function symbol, and the equation to the right of the semicolon specifies the semantics of g. For each i E [r], xi is an fi-tuple of variables, and a� = (a1, ... , af ) is a tuple of strings over t</context>
</contexts>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In 25th Annual Meeting of the Association for Computational Linguistics (ACL), pages 104–111, Stanford, CA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>