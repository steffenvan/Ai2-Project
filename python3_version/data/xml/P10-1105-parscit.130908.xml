<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000195">
<title confidence="0.994634">
Finding Cognate Groups using Phylogenies
</title>
<author confidence="0.996115">
David Hall and Dan Klein
</author>
<affiliation confidence="0.998432">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.994992">
{dlwh,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.994693" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999806857142857">
A central problem in historical linguistics
is the identification of historically related
cognate words. We present a generative
phylogenetic model for automatically in-
ducing cognate group structure from un-
aligned word lists. Our model represents
the process of transformation and trans-
mission from ancestor word to daughter
word, as well as the alignment between
the words lists of the observed languages.
We also present a novel method for sim-
plifying complex weighted automata cre-
ated during inference to counteract the
otherwise exponential growth of message
sizes. On the task of identifying cognates
in a dataset of Romance words, our model
significantly outperforms a baseline ap-
proach, increasing accuracy by as much as
80%. Finally, we demonstrate that our au-
tomatically induced groups can be used to
successfully reconstruct ancestral words.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999352403508772">
A crowning achievement of historical linguistics
is the comparative method (Ohala, 1993), wherein
linguists use word similarity to elucidate the hid-
den phonological and morphological processes
which govern historical descent. The comparative
method requires reasoning about three important
hidden variables: the overall phylogenetic guide
tree among languages, the evolutionary parame-
ters of the ambient changes at each branch, and
the cognate group structure that specifies which
words share common ancestors.
All three of these variables interact and inform
each other, and so historical linguists often con-
sider them jointly. However, linguists are cur-
rently required to make qualitative judgments re-
garding the relative likelihood of certain sound
changes, cognate groups, and so on. Several re-
cent statistical methods have been introduced to
provide increased quantitative backing to the com-
parative method (Oakes, 2000; Bouchard-Cˆot´e et
al., 2007; Bouchard-Cˆot´e et al., 2009); others have
modeled the spread of language changes and spe-
ciation (Ringe et al., 2002; Daum´e III and Camp-
bell, 2007; Daum´e III, 2009; Nerbonne, 2010).
These automated methods, while providing ro-
bustness and scale in the induction of ancestral
word forms and evolutionary parameters, assume
that cognate groups are already known. In this
work, we address this limitation, presenting a
model in which cognate groups can be discovered
automatically.
Finding cognate groups is not an easy task,
because underlying morphological and phonolog-
ical changes can obscure relationships between
words, especially for distant cognates, where sim-
ple string overlap is an inadequate measure of sim-
ilarity. Indeed, a standard string similarity met-
ric like Levenshtein distance can lead to false
positives. Consider the often cited example of
Greek /ma:ti/ and Malay /mata/, both meaning
“eye” (Bloomfield, 1938). If we were to rely on
Levenshtein distance, these words would seem to
be a highly attractive match as cognates: they are
nearly identical, essentially differing in only a sin-
gle character. However, no linguist would posit
that these two words are related. To correctly learn
that they are not related, linguists typically rely
on two kinds of evidence. First, because sound
change is largely regular, we would need to com-
monly see /i/ in Greek wherever we see /a/ in
Malay (Ross, 1950). Second, we should look at
languages closely related to Greek and Malay, to
see if similar patterns hold there, too.
Some authors have attempted to automatically
detect cognate words (Mann and Yarowsky, 2001;
Lowe and Mazaudon, 1994; Oakes, 2000; Kon-
drak, 2001; Mulloni, 2007), but these methods
</bodyText>
<page confidence="0.940013">
1030
</page>
<note confidence="0.9421285">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1030–1039,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999870948717949">
typically work on language pairs rather than on
larger language families. To fully automate the
comparative method, it is necessary to consider
multiple languages, and to do so in a model which
couples cognate detection with similarity learning.
In this paper, we present a new generative model
for the automatic induction of cognate groups
given only (1) a known family tree of languages
and (2) word lists from those languages. A prior
on word survival generates a number of cognate
groups and decides which groups are attested in
each modern language. An evolutionary model
captures how each word is generated from its par-
ent word. Finally, an alignment model maps the
flat word lists to cognate groups. Inference re-
quires a combination of message-passing in the
evolutionary model and iterative bipartite graph
matching in the alignment model.
In the message-passing phase, our model en-
codes distributions over strings as weighted finite
state automata (Mohri, 2009). Weighted automata
have been successfully applied to speech process-
ing (Mohri et al., 1996) and more recently to mor-
phology (Dreyer and Eisner, 2009). Here, we
present a new method for automatically compress-
ing our message automata in a way that can take
into account prior information about the expected
outcome of inference.
In this paper, we focus on a transcribed word
list of 583 cognate sets from three Romance lan-
guages (Portuguese, Italian and Spanish), as well
as their common ancestor Latin (Bouchard-Cˆot´e
et al., 2007). We consider both the case where
we know that all cognate groups have a surface
form in all languages, and where we do not know
that. On the former, easier task we achieve iden-
tification accuracies of 90.6%. On the latter task,
we achieve F1 scores of 73.6%. Both substantially
beat baseline performance.
</bodyText>
<sectionHeader confidence="0.991663" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.999956571428572">
In this section, we describe a new generative
model for vocabulary lists in multiple related lan-
guages given the phylogenetic relationship be-
tween the languages (their family tree). The gener-
ative process factors into three subprocesses: sur-
vival, evolution, and alignment, as shown in Fig-
ure 1(a). Survival dictates, for each cognate group,
which languages have words in that group. Evo-
lution describes the process by which daughter
words are transformed from their parent word. Fi-
nally, alignment describes the “scrambling” of the
word lists into a flat order that hides their lineage.
We present each subprocess in detail in the follow-
ing subsections.
</bodyText>
<subsectionHeader confidence="0.988348">
2.1 Survival
</subsectionHeader>
<bodyText confidence="0.999919461538462">
First, we choose a number G of ancestral cognate
groups from a geometric distribution. For each
cognate group g, our generative process walks
down the tree. At each branch, the word may ei-
ther survive or die. This process is modeled in a
“death tree” with a Bernoulli random variable S4
for each language E and cognate group g specify-
ing whether or not the word died before reaching
that language. Death at any node in the tree causes
all of that node’s descendants to also be dead. This
process captures the intuition that cognate words
are more likely to be found clustered in sibling lan-
guages than scattered across unrelated languages.
</bodyText>
<subsectionHeader confidence="0.99566">
2.2 Evolution
</subsectionHeader>
<bodyText confidence="0.999978962962963">
Once we know which languages will have an at-
tested word and which will not, we generate the
actual word forms. The evolution component of
the model generates words according to a branch-
specific transformation from a node’s immediate
ancestor. Figure 1(a) graphically describes our
generative model for three Romance languages:
Italian, Portuguese, and Spanish.1 In each cog-
nate group, each word Wt is generated from its
parent according to a conditional distribution with
parameter cot, which is specific to that edge in the
tree, but shared between all cognate groups.
In this paper, each cot takes the form of a pa-
rameterized edit distance similar to the standard
Levenshtein distance. Richer models – such as the
ones in Bouchard-Cˆot´e et al. (2007) – could in-
stead be used, although with an increased infer-
ential cost. The edit transducers are represented
schematically in Figure 1(b). Characters x and
y are arbitrary phonemes, and u(x, y) represents
the cost of substituting x with y. a represents the
empty phoneme and is used as shorthand for inser-
tion and deletion, which have parameters q and 6,
respectively.
As an example, see the illustration in Fig-
ure 1(c). Here, the Italian word /fwOko/ (“fire”) is
generated from its parent form /fokus/ (“hearth”)
</bodyText>
<footnote confidence="0.99938675">
1Though we have data for Latin, we treat it as unobserved
to represent the more common case where the ancestral lan-
guage is unattested; we also evaluate our system using the
Latin data.
</footnote>
<page confidence="0.994621">
1031
</page>
<figureCaption confidence="0.998688">
Figure 1: (a) The process by which cognate words are generated. Here, we show the derivation of Romance language words
</figureCaption>
<bodyText confidence="0.9797408">
We from their respective Latin ancestor, parameterized by transformations We and survival variables Se. Languages shown
are Latin (LA), Vulgar Latin (VL), Proto-Iberian (PI), Italian (IT), Portuguese (PT), and Spanish (ES). Note that only modern
language words are observed (shaded). (b) The class of parameterized edit distances used in this paper. Each pair of phonemes
has a weight v for deletion, and each phoneme has weights rl and S for insertion and deletion respectively. (c) A possible
alignment produced by an edit distance between the Latin word focus (“hearth”) and the Italian word fuoco (“fire”).
</bodyText>
<figure confidence="0.9973985625">
Survival
SIT SPT SES
SLA
SVL
(a)
SPI
Alignment
WIT
Q)
IT w
ITwITwITwITwITwIT
WLA
WVL
Q)
Q)
7r
Q)
Q) Q)
WPI
Evolution
G
LL
L
f
f w
ɔ
o
k
k
u
o
s
</figure>
<bodyText confidence="0.999087142857143">
by a series of edits: two matches, two substitu-
tions (/u/—* /o/, and /o/—*/O/), one insertion (w)
and one deletion (/s/). The probability of each
individual edit is determined by co. Note that the
marginal probability of a specific Italian word con-
ditioned on its Vulgar Latin parent is the sum over
all possible derivations that generate it.
</bodyText>
<subsectionHeader confidence="0.999305">
2.3 Alignment
</subsectionHeader>
<bodyText confidence="0.99654865">
Finally, at the leaves of the trees are the observed
words. (We take non-leaf nodes to be unobserved.)
Here, we make the simplifying assumption that in
any language there is at most one word per lan-
guage per cognate group. Because the assign-
ments of words to cognates is unknown, we spec-
ify an unknown alignment parameter 7r` for each
modern language which is an alignment of cognate
groups to entries in the word list. In the case that
every cognate group has a word in each language,
each 7r` is a permutation. In the more general case
that some cognate groups do not have words from
all languages, this mapping is injective from words
to cognate groups. From a generative perspective,
7r` generates observed positions of the words in
some vocabulary list.
In this paper, our task is primarily to learn the
alignment variables 7r`. All other hidden variables
are auxiliary and are to be marginalized to the
greatest extent possible.
</bodyText>
<sectionHeader confidence="0.990167" genericHeader="method">
3 Inference of Cognate Assignments
</sectionHeader>
<bodyText confidence="0.9999388">
In this section, we discuss the inference method
for determining cognate assignments under fixed
parameters co. We are given a set of languages and
a list of words in each language, and our objec-
tive is to determine which words are cognate with
each other. Because the parameters 7r` are either
permutations or injections, the inference task is re-
duced to finding an alignment 7r of the respective
word lists to maximize the log probability of the
observed words.
</bodyText>
<equation confidence="0.54897">
log p(w(`,πe(g))|co, 7r, W_`)
</equation>
<bodyText confidence="0.9977555">
w(`,πe(g)) is the word in language E that 7r` has
assigned to cognate group g. Maximizing this
quantity directly is intractable, and so instead we
use a coordinate ascent algorithm to iteratively
</bodyText>
<equation confidence="0.77943">
7r* = arg max
π
�
g
</equation>
<page confidence="0.939127">
1032
</page>
<bodyText confidence="0.955089555555556">
maximize the alignment corresponding to a
single language ` while holding the others fixed:
log p(w(`,πe(g))|ϕ, π−`, π`, w−`)
Each iteration is then actually an instance of
bipartite graph matching, with the words in one
language one set of nodes, and the current cognate
groups in the other languages the other set of
nodes. The edge affinities aff between these
nodes are the conditional probabilities of each
word w` belonging to each cognate group g:
aff(w`, g) = p(w`|w−`,π−e(g), ϕ, π−`)
To compute these affinities, we perform in-
ference in each tree to calculate the marginal
distribution of the words from the language `.
For the marginals, we use an analog of the for-
ward/backward algorithm. In the upward pass, we
send messages from the leaves of the tree toward
the root. For observed leaf nodes Wd, we have:
</bodyText>
<equation confidence="0.8534098">
µd→a(wa) = p(Wd = wd|wa, ϕd)
and for interior nodes Wi:
Eµi→a(wa) = Hp(wi|wa, ϕi) µd→i(wi)
w; d∈child(w;)
(1)
</equation>
<bodyText confidence="0.9967485">
In the downward pass (toward the lan-
guage `), we sum over ancestral words Wa:
</bodyText>
<equation confidence="0.97694625">
µa→d(wd)
E= Hp(wd|wa, ϕd)µa0→a(wa) µd0→a(wa)
wa d0∈child(wa)
d06�d
</equation>
<bodyText confidence="0.999904714285714">
where a0 is the ancestor of a. Computing these
messages gives a posterior marginal distribution
µ`(w`) = p(w`|w−`,π−e(g), ϕ,π−`), which is pre-
cisely the affinity score we need for the bipartite
matching. We then use the Hungarian algorithm
(Kuhn, 1955) to find the optimal assignment for
the bipartite matching problem.
One important final note is initialization. In our
early experiments we found that choosing a ran-
dom starting configuration unsurprisingly led to
rather poor local optima. Instead, we started with
empty trees, and added in one language per itera-
tion until all languages were added, and then con-
tinued iterations on the full tree.
</bodyText>
<sectionHeader confidence="0.992889" genericHeader="method">
4 Learning
</sectionHeader>
<bodyText confidence="0.999885214285715">
So far we have only addressed searching for
Viterbi alignments π under fixed parameters. In
practice, it is important to estimate better para-
metric edit distances ϕ` and survival variables
S`. To motivate the need for good transducers,
consider the example of English “day” /deI/ and
Latin “di¯es” /dIe:s/, both with the same mean-
ing. Surprisingly, these words are in no way re-
lated, with English “day” probably coming from a
verb meaning “to burn” (OED, 1989). However,
a naively constructed edit distance, which for ex-
ample might penalize vowel substitutions lightly,
would fail to learn that Latin words that are bor-
rowed into English would not undergo the sound
change /I/→/eI/. Therefore, our model must learn
not only which sound changes are plausible (e.g.
vowels turning into other vowels is more common
than vowels turning into consonants), but which
changes are appropriate for a given language.2
At a high level, our learning algorithm is much
like Expectation Maximization with hard assign-
ments: after we update the alignment variables π
and thus form new potential cognate sets, we re-
estimate our model’s parameters to maximize the
likelihood of those assignments.3 The parameters
can be learned through standard maximum likeli-
hood estimation, which we detail in this section.
Because we enforce that a word in language d
must be dead if its parent word in language a is
dead, we just need to learn the conditional prob-
abilities p(Sd = dead|Sa = alive). Given fixed
assignments π, the maximum likelihood estimate
can be found by counting the number of “deaths”
that occurred between a child and a live parent,
applying smoothing – we found adding 0.5 to be
reasonable – and dividing by the total number of
live parents.
For the transducers ϕ, we learn parameterized
edit distances that model the probabilities of dif-
ferent sound changes. For each ϕ` we fit a non-
uniform substitution, insertion, and deletion ma-
trix σ(x, y). These edit distances define a condi-
2We note two further difficulties: our model does not han-
dle “borrowings,” which would be necessary to capture a
significant portion of English vocabulary; nor can it seam-
lessly handle words that are inherited later in the evolution of
language than others. For instance, French borrowed words
from its parent language Latin during the Renaissance and
the Enlightenment that have not undergone the same changes
as words that evolved “naturally” from Latin. See Bloom-
field (1938). Handling these cases is a direction for future
research.
3Strictly, we can cast this problem in a variational frame-
work similar to mean field where we iteratively maximize pa-
rameters to minimize a KL-divergence. We omit details for
clarity.
</bodyText>
<equation confidence="0.9176135">
π∗` = arg max
πe
E
g
</equation>
<page confidence="0.839576">
1033
</page>
<bodyText confidence="0.934331666666667">
tional exponential family distribution when condi-
tioned on an ancestral word. That is, for any fixed
wa:
</bodyText>
<equation confidence="0.997656333333333">
�(wd|wa, σ) = � score(z;σ)
wd z∈
align(wa,wd)
ri σ(x, y) = 1
align(wa,wd)
z∈ (x,y)Ez
</equation>
<bodyText confidence="0.99998">
where align(wa, wd) is the set of possible align-
ments between the phonemes in words wa and wd.
We are seeking the maximum likelihood esti-
mate of each ϕ, given fixed alignments π:
</bodyText>
<equation confidence="0.842283">
ˆϕ` = arg max p(W|ϕ, π)
ϕe
</equation>
<bodyText confidence="0.998744142857143">
To find this maximizer for any given π`, we
need to find a marginal distribution over the
edges connecting any two languages a and
d. With this distribution, we calculate the
expected “alignment unigrams.” That is, for
each pair of phonemes x and y (or empty
phoneme ε), we need to find the quantity:
</bodyText>
<equation confidence="0.995364">
Ep(wa,wd)[#(x, y; z)] =
E
wa,wd align(wa,wd)
z∈
</equation>
<bodyText confidence="0.9999892">
where we denote #(x, y; z) to be the num-
ber of times the pair of phonemes (x, y) are
aligned in alignment z. The exact method for
computing these counts is to use an expectation
semiring (Eisner, 2001).
Given the expected counts, we now need to nor-
malize them to ensure that the transducer repre-
sents a conditional probability distribution (Eis-
ner, 2002; Oncina and Sebban, 2006). We have
that, for each phoneme x in the ancestor language:
</bodyText>
<equation confidence="0.999505666666667">
ηy = E[#(ε, y; z)]
E[#(·, ·;z)]
E[#(x, y; z)]
ηy0) E[#(x, ·; z)]
E[#(x, ε; z)]
ηy0) E[#(x,·; z)]
</equation>
<bodyText confidence="0.989453714285714">
Here, we have #(·, ·; z) = Ex,y #(x, y; z) and
#(x, ·; z) = Ey #(x, y; z). The (1 − Ey0 ηy0)
term ensure that for any ancestral phoneme x,
Ey ηy+Ey σ(x, y)+δx = 1. These equations en-
sure that the three transition types (insertion, sub-
stitution/match, deletion) are normalized for each
ancestral phoneme.
</bodyText>
<sectionHeader confidence="0.964395" genericHeader="method">
5 Transducers and Automata
</sectionHeader>
<bodyText confidence="0.999977666666667">
In our model, it is not just the edit distances
that are finite state machines. Indeed, the words
themselves are string-valued random variables that
have, in principle, an infinite domain. To represent
distributions and messages over these variables,
we chose weighted finite state automata, which
can compactly represent functions over strings.
Unfortunately, while initially compact, these au-
tomata become unwieldy during inference, and so
approximations must be used (Dreyer and Eisner,
2009). In this section, we summarize the standard
algorithms and representations used for weighted
finite state transducers. For more detailed treat-
ment of the general transducer operations, we di-
rect readers to Mohri (2009).
A weighted automaton (resp. transducer) en-
codes a function over strings (resp. pairs of
strings) as weighted paths through a directed
graph. Each edge in the graph has a real-valued
weight4 and a label, which is a single phoneme
in some alphabet Σ or the empty phoneme ε (resp.
pair of labels in some alphabet ΣxΔ). The weight
of a string is then the sum of all paths through the
graph that accept that string.
For our purposes, we are concerned with three
fundamental operations on weighted transducers.
The first is computing the sum of all paths through
a transducer, which corresponds to computing the
partition function of a distribution over strings.
This operation can be performed in worst-case
cubic time (using a generalization of the Floyd-
Warshall algorithm). For acyclic or feed-forward
transducers, this time can be improved dramati-
cally by using a generalization of Djisktra’s algo-
rithm or other related algorithms (Mohri, 2009).
The second operation is the composition of two
transducers. Intuitively, composition creates a new
transducer that takes the output from the first trans-
ducer, processes it through the second transducer,
and then returns the output of the second trans-
ducer. That is, consider two transducers T1 and
T2. T1 has input alphabet Σ and output alpha-
bet Δ, while T2 has input alphabet Δ and out-
put alphabet Ω. The composition T1 o T2 returns
a new transducer over Σ and Ω such that (T1 o
T2)(x, y) = Eu T1(x, u) · T2(u, y). In this paper,
we use composition for marginalization and fac-
tor products. Given a factor f1(x, u; T1) and an-
</bodyText>
<footnote confidence="0.8818665">
4The weights can be anything that form a semiring, but for
the sake of exposition we specialize to real-valued weights.
</footnote>
<equation confidence="0.963124818181818">
�
p
wd
�=
wd
�
E #(x,y; z)p(z|wa, wd)p(wa, wd)
σ(x, y) = (1 − �
y0
δx = (1 − �
y0
</equation>
<page confidence="0.899788">
1034
</page>
<bodyText confidence="0.999411882352941">
other factor f2(u, y; T2), composition corresponds
to the operation ψ(x, y) = Eu f1(x, u)f2(u, y).
For two messages µ1(w) and µ2(w), the same al-
gorithm can be used to find the product µ(w) =
µ1(w)µ2(w).
The third operation is transducer minimization.
Transducer composition produces O(nm) states,
where n and m are the number of states in each
transducer. Repeated compositions compound the
problem: iterated composition of k transducers
produces O(nk) states. Minimization alleviates
this problem by collapsing indistinguishable states
into a single state. Unfortunately, minimization
does not always collapse enough states. In the next
section we discuss approaches to “lossy” mini-
mization that produce automata that are not ex-
actly the same but are much smaller.
</bodyText>
<sectionHeader confidence="0.992846" genericHeader="method">
6 Message Approximation
</sectionHeader>
<bodyText confidence="0.99995459375">
Recall that in inference, when summing out in-
terior nodes wi we calculated the product over
incoming messages µd,i(wi) (Equation 1), and
that these products are calculated using transducer
composition. Unfortunately, the maximal number
of states in a message is exponential in the num-
ber of words in the cognate group. Minimization
can only help so much: in order for two states to
be collapsed, the distribution over transitions from
those states must be indistinguishable. In practice,
for the automata generated in our model, mini-
mization removes at most half the states, which is
not sufficient to counteract the exponential growth.
Thus, we need to find a way to approximate a mes-
sage µ(w) using a simpler automata µ(w; θ) taken
from a restricted class parameterized by θ.
In the context of transducers, previous authors
have focused on a combination of n-best lists
and unigram back-off models (Dreyer and Eis-
ner, 2009), a schematic diagram of which is in
Figure 2(d). For their problem, n-best lists are
sensible: their nodes’ local potentials already fo-
cus messages on a small number of hypotheses.
In our setting, however, n-best lists are problem-
atic; early experiments showed that a 10,000-best
list for a typical message only accounts for 50%
of message log perplexity. That is, the posterior
marginals in our model are (at least initially) fairly
flat.
An alternative approach might be to simply
treat messages as unnormalized probability distri-
butions, and to minimize the KL divergence be-
</bodyText>
<figureCaption confidence="0.9976978">
Figure 2: Various topologies for approximating topologies:
(a) a unigram model, (b) a bigram model, (c) the anchored
unigram model, and (d) the n-best plus backoff model used in
Dreyer and Eisner (2009). In (c) and (d), the relative height
of arcs is meant to convey approximate probabilities.
</figureCaption>
<bodyText confidence="0.99943125">
tween some approximating message µ(w) and the
true message µ(w). However, messages are not
always probability distributions and – because the
number of possible strings is in principle infinite –
they need not sum to a finite number.5 Instead, we
propose to minimize the KL divergence between
the “expected” marginal distribution and the ap-
proximated “expected” marginal distribution:
</bodyText>
<equation confidence="0.989057857142857">
Dxz,(τ(w)µ(w)||τ(w)A(w; θ))
τ(w)µ(w)
τ(w)µ(w)log
τ(w)˜µ(w;θ)
τ(w)µ(w) log µ(w)
�µ(w; θ)
(2)
</equation>
<bodyText confidence="0.952286928571429">
where τ is a term acting as a surrogate for the pos-
terior distribution over w without the information
from µ. That is, we seek to approximate µ not on
its own, but as it functions in an environment rep-
resenting its final context. For example, if µ(w) is
a backward message, τ could be a stand-in for a
forward probability.6
In this paper, µ(w) is a complex automaton with
potentially many states, µ(w; θ) is a simple para-
metric automaton with forms that we discuss be-
low, and τ(w) is an arbitrary (but hopefully fairly
simple) automaton. The actual method we use is
5As an extreme example, suppose we have observed that
Wd = wd and that P(Wd = wd|wa) = 1 for all ancestral
</bodyText>
<equation confidence="0.636759333333333">
words wa. Then, clearly Ewd µ(wd) = � � P(Wd =
wd
wd|wa) = oo whenever there are an infinite number of pos-
</equation>
<footnote confidence="0.964845333333333">
sible ancestral strings wa.
6This approach is reminiscent of Expectation Propaga-
tion (Minka, 2001).
</footnote>
<figure confidence="0.999804266666666">
u
u
2 3
g
o
f
f
u
(a)
(c)
0 e 1
g
o
e
o
f
g
e
g
o
e
u
f
4
u
f
o
g
e
5
g
u e
g o
e e
g
g
ug
u e
g o
(b)
e u f
u
o
o f
f
o
e u
o
e
g
o
e
ff
g
o
f
u
u
(d)
f
f
f
f
u
o f f u e o
g
e e
e u g
eg
θ� = arg min
B
�= arg min
B w
�= arg min
B w
</figure>
<page confidence="0.988601">
1035
</page>
<bodyText confidence="0.999569977272727">
as follows. Given a deterministic prior automa-
ton T, and a deterministic automaton topology µ*,
we create the composed unweighted automaton
Toµ*, and calculate arc transitions weights to min-
imize the KL divergence between that composed
transducer and T o µ. The procedure for calcu-
lating these statistics is described in Li and Eis-
ner (2009), which amounts to using an expectation
semiring (Eisner, 2001) to compute expected tran-
sitions in T o µ* under the probability distribution
Toµ.
From there, we need to create the automaton
T−1 o T o A. That is, we need to divide out the
influence of T(w). Since we know the topology
and arc weights for T ahead of time, this is often
as simple as dividing arc weights in T o µ� by the
corresponding arc weight in T(w). For example,
if T encodes a geometric distribution over word
lengths and a uniform distribution over phonemes
(that is, T(w) a p|w|), then computing µ� is as sim-
ple as dividing each arc in T o µ� by p.7
There are a number of choices for T. One is a
hard maximum on the length of words. Another is
to choose T(w) to be a unigram language model
over the language in question with a geometric
probability over lengths. In our experiments, we
find that T(w) can be a geometric distribution over
lengths with a uniform distribution over phonemes
and still give reasonable results. This distribution
captures the importance of shorter strings while
still maintaining a relatively weak prior.
What remains is the selection of the topologies
for the approximating message A. We consider
three possible approximations, illustrated in Fig-
ure 2. The first is a plain unigram model, the
second is a bigram model, and the third is an an-
chored unigram topology: a position-specific un-
igram model for each position up to some maxi-
mum length.
The first we consider is a standard unigram
model, which is illustrated in Figure 2(a). It
has |E |+ 2 parameters: one weight Qa for each
phoneme a E E, a starting weight A, and a stop-
ping probability p. µ� then has the form:
</bodyText>
<equation confidence="0.9512365">
�µ(w) = A� � Qwi
i&lt;|w|
</equation>
<bodyText confidence="0.962429">
Estimating this model involves only computing
the expected count of each phoneme, along with
7Also, we must be sure to divide each final weight in the
transducer by (1 − |E|p), which is the stopping probability
for a geometric transducer.
the expected length of a word, E[|w|]. We then
normalize the counts according to the maximum
likelihood estimate, with arc weights set as:
</bodyText>
<subsectionHeader confidence="0.874907">
Qa a E[#(a)]
</subsectionHeader>
<bodyText confidence="0.999319166666667">
Recall that these expectations can be computed us-
ing an expectation semiring.
Finally, A can be computed by ensuring that the
approximate and exact expected marginals have
the same partition function. That is, with the other
parameters fixed, solve:
</bodyText>
<equation confidence="0.985567666666667">
E�
T(w)˜µ(w) =
w w
</equation>
<bodyText confidence="0.99988804">
which amounts to rescaling µ� by some constant.
The second topology we consider is the bigram
topology, illustrated in Figure 2(b). It is similar
to the unigram topology except that, instead of
a single state, we have a state for each phoneme
in E, along with a special start state. Each state
a has transitions with weights Qb|a = p(b|a) a
E[#(b|a)]. Normalization is similar to the un-
igram case, except that we normalize the transi-
tions from each state.
The final topology we consider is the positional
unigram model in Figure 2(c). This topology takes
positional information into account. Namely, for
each position (up to some maximum position), we
have a unigram model over phonemes emitted at
that position, along with the probability of stop-
ping at that position (i.e. a “sausage lattice”). Es-
timating the parameters of this model is similar,
except that the expected counts for the phonemes
in the alphabet are conditioned on their position in
the string. With the expected counts for each posi-
tion, we normalize each state’s final and outgoing
weights. In our experiments, we set the maximum
length to seven more than the length of the longest
observed string.
</bodyText>
<sectionHeader confidence="0.997187" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.9986948">
We conduct three experiments. The first is a “com-
plete data” experiment, in which we reconstitute
the cognate groups from the Romance data set,
where all cognate groups have words in all three
languages. This task highlights the evolution and
alignment models. The second is a much harder
“partial data” experiment, in which we randomly
prune 20% of the branches from the dataset ac-
cording to the survival process described in Sec-
tion 2.1. Here, only a fraction of words appear
</bodyText>
<equation confidence="0.591376">
T(w)µ(w)
</equation>
<page confidence="0.964551">
1036
</page>
<bodyText confidence="0.9999692">
in any cognate group, so this task crucially in-
volves the survival model. The ultimate purpose
of the induced cognate groups is to feed richer
evolutionary models, such as full reconstruction
models. Therefore, we also consider a proto-word
reconstruction experiment. For this experiment,
using the system of Bouchard-Cˆot´e et al. (2009),
we compare the reconstructions produced from
our automatic groups to those produced from gold
cognate groups.
</bodyText>
<subsectionHeader confidence="0.994055">
7.1 Baseline
</subsectionHeader>
<bodyText confidence="0.9915472">
As a novel but heuristic baseline for cognate group
detection, we use an iterative bipartite matching
algorithm where instead of conditional likelihoods
for affinities we use Dice’s coefficient, defined for
sets X and Y as:
</bodyText>
<equation confidence="0.999356666666667">
2|X ∩ Y |
Dice(X, Y ) = (3)
|X |+ |Y |
</equation>
<bodyText confidence="0.999773428571428">
Dice’s coefficients are commonly used in bilingual
detection of cognates (Kondrak, 2001; Kondrak et
al., 2003). We follow prior work and use sets of
bigrams within words. In our case, during bipar-
tite matching the set X is the set of bigrams in the
language being re-permuted, and Y is the union of
bigrams in the other languages.
</bodyText>
<subsectionHeader confidence="0.990908">
7.2 Experiment 1: Complete Data
</subsectionHeader>
<bodyText confidence="0.999654">
In this experiment, we know precisely how many
cognate groups there are and that every cognate
group has a word in each language. While this
scenario does not include all of the features of the
real-world task, it represents a good test case of
how well these models can perform without the
non-parametric task of deciding how many clus-
ters to use.
We scrambled the 583 cognate groups in the
Romance dataset and ran each method to conver-
gence. Besides the heuristic baseline, we tried our
model-based approach using Unigrams, Bigrams
and Anchored Unigrams, with and without learn-
ing the parametric edit distances. When we did not
use learning, we set the parameters of the edit dis-
tance to (0, -3, -4) for matches, substitutions, and
deletions/insertions, respectively. With learning
enabled, transducers were initialized with those
parameters.
For evaluation, we report two metrics. The first
is pairwise accuracy for each pair of languages,
averaged across pairs of words. The other is accu-
</bodyText>
<table confidence="0.999642583333333">
Pairwise Exact
Acc. Match
Heuristic
Baseline 48.1 35.4
Model
Transducers Messages
Levenshtein Unigrams 37.2 26.2
Levenshtein Bigrams 43.0 26.5
Levenshtein Anch. Unigrams 68.6 56.8
Learned Unigrams 0.1 0.0
Learned Bigrams 38.7 11.3
Learned Anch. Unigrams 90.3 86.6
</table>
<tableCaption confidence="0.994892857142857">
Table 1: Accuracies for reconstructing cognate groups. Lev-
enshtein refers to fixed parameter edit distance transducer.
Learned refers to automatically learned edit distances. Pair-
wise Accuracy means averaged on each word pair; Exact
Match refers to percentage of completely and accurately re-
constructed groups. For a description of the baseline, see Sec-
tion 7.1.
</tableCaption>
<table confidence="0.999909714285714">
Prec. Recall F1
Heuristic
Baseline 49.0 43.5 46.1
Model
Transducers Messages
Levenshtein Anch. Unigrams 86.5 36.1 50.9
Learned Anch. Unigrams 66.9 82.0 73.6
</table>
<tableCaption confidence="0.993527666666667">
Table 2: Accuracies for reconstructing incomplete groups.
Scores reported are precision, recall, and F1, averaged over
all word pairs.
</tableCaption>
<bodyText confidence="0.9993193">
racy measured in terms of the number of correctly,
completely reconstructed cognate groups.
Table 1 shows the results under various config-
urations. As can be seen, the kind of approxima-
tion used matters immensely. In this application,
positional information is important, more so than
the context of the previous phoneme. Both Un-
igrams and Bigrams significantly under-perform
the baseline, while Anchored Unigrams easily out-
performs it both with and without learning.
An initially surprising result is that learning ac-
tually harms performance under the unanchored
approximations. The explanation is that these
topologies are not sensitive enough to context, and
that the learning procedure ends up flattening the
distributions. In the case of unigrams – which have
the least context – learning degrades performance
to chance. However, in the case of positional uni-
grams, learning reduces the error rate by more than
two-thirds.
</bodyText>
<subsectionHeader confidence="0.990537">
7.3 Experiment 2: Incomplete Data
</subsectionHeader>
<bodyText confidence="0.999494">
As a more realistic scenario, we consider the case
where we do not know that all cognate groups have
words in all languages. To test our model, we ran-
</bodyText>
<page confidence="0.986492">
1037
</page>
<bodyText confidence="0.99997588372093">
domly pruned 20% of the branches according the
survival process of our model.8
Because only Anchored Unigrams performed
well in Experiment 1, we consider only it and the
Dice’s coefficient baseline. The baseline needs to
be augmented to support the fact that some words
may not appear in all cognate groups. To do this,
we thresholded the bipartite matching process so
that if the coefficient fell below some value, we
started a new group for that word. We experi-
mented on 10 values in the range (0,1) for the
baseline’s threshold and report on the one (0.2)
that gives the best pairwise F1.
The results are in Table 2. Here again, we see
that the positional unigrams perform much better
than the baseline system. The learned transduc-
ers seem to sacrifice precision for the sake of in-
creased recall. This makes sense because the de-
fault edit distance parameter settings strongly fa-
vor exact matches, while the learned transducers
learn more realistic substitution and deletion ma-
trices, at the expense of making more mistakes.
For example, the learned transducers enable
our model to correctly infer that Portuguese
/d1femdu/, Spanish /defiendo/, and Italian
/difEndo/ are all derived from Latin /de:fendo:/
“defend.” Using the simple Levenshtein transduc-
ers, on the other hand, our model keeps all three
separated, because the transducers cannot know –
among other things – that Portuguese /1/, Span-
ish /e/, and Italian /i/ are commonly substituted
for one another. Unfortunately, because the trans-
ducers used cannot learn contextual rules, cer-
tain transformations can be over-applied. For in-
stance, Spanish /nombRar/ “name” is grouped to-
gether with Portuguese /num1RaR/ “number” and
Italian /numerare/ “number,” largely because the
rule Portuguese /u/ —* Spanish /o/ is applied out-
side of its normal context. This sound change oc-
curs primarily with final vowels, and does not usu-
ally occur word medially. Thus, more sophisti-
cated transducers could learn better sound laws,
which could translate into improved accuracy.
</bodyText>
<subsectionHeader confidence="0.978347">
7.4 Experiment 3: Reconstructions
</subsectionHeader>
<bodyText confidence="0.956931785714286">
As a final trial, we wanted to see how each au-
tomatically found cognate group faired as com-
pared to the “true groups” for actual reconstruc-
tion of proto-words. Our model is not optimized
8This dataset will be made available at
http://nlp.cs.berkeley.edu/Main.html#Historical
for faithful reconstruction, and so we used the An-
cestry Resampling system of Bouchard-Cˆot´e et al.
(2009). To evaluate, we matched each Latin word
with the best possible cognate group for that word.
The process for the matching was as follows. If
two or three of the words in an constructed cognate
group agreed, we assigned the Latin word associ-
ated with the true group to it. With the remainder,
we executed a bipartite matching based on bigram
overlap.
For evaluation, we examined the Levenshtein
distance between the reconstructed word and the
chosen Latin word. As a kind of “skyline,”
we compare to the edit distances reported in
Bouchard-Cˆot´e et al. (2009), which was based on
complete knowledge of the cognate groups. On
this task, our reconstructed cognate groups had
an average edit distance of 3.8 from the assigned
Latin word. This compares favorably to the edit
distances reported in Bouchard-Cˆot´e et al. (2009),
who using oracle cognate assignments achieved an
average Levenshtein distance of 3.0.9
</bodyText>
<sectionHeader confidence="0.998218" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999994666666667">
We presented a new generative model of word
lists that automatically finds cognate groups from
scrambled vocabulary lists. This model jointly
models the origin, propagation, and evolution of
cognate groups from a common root word. We
also introduced a novel technique for approximat-
ing automata. Using these approximations, our
model can reduce the error rate by 80% over a
baseline approach. Finally, we demonstrate that
these automatically generated cognate groups can
be used to automatically reconstruct proto-words
faithfully, with a small increase in error.
</bodyText>
<sectionHeader confidence="0.998238" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99977975">
Thanks to Alexandre Bouchard-Cˆot´e for the many
insights. This project is funded in part by the NSF
under grant 0915265 and an NSF graduate fellow-
ship to the first author.
</bodyText>
<sectionHeader confidence="0.998337" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.933637">
Leonard Bloomfield. 1938. Language. Holt, New
York.
</reference>
<footnote confidence="0.829119">
9Morphological noise and transcription errors contribute
to the absolute error rate for this data set.
</footnote>
<page confidence="0.963317">
1038
</page>
<reference confidence="0.999841782051282">
Alexandre Bouchard-Cˆot´e, Percy Liang, Thomas Grif-
fiths, and Dan Klein. 2007. A probabilistic ap-
proach to diachronic phonology. In EMNLP.
Alexandre Bouchard-Cˆot´e, Thomas L. Griffiths, and
Dan Klein. 2009. Improved reconstruction of pro-
tolanguage word forms. In NAACL, pages 65–73.
Hal Daum´e III and Lyle Campbell. 2007. A Bayesian
model for discovering typological implications. In
Conference of the Association for Computational
Linguistics (ACL).
Hal Daum´e III. 2009. Non-parametric Bayesian model
areal linguistics. In NAACL.
Markus Dreyer and Jason Eisner. 2009. Graphical
models over multiple strings. In EMNLP, Singa-
pore, August.
Jason Eisner. 2001. Expectation semirings: Flexible
EM for finite-state transducers. In Gertjan van No-
ord, editor, FSMNLP.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Grzegorz Kondrak, Daniel Marcu, and Keven Knight.
2003. Cognates can improve statistical translation
models. In NAACL.
Grzegorz Kondrak. 2001. Identifying cognates by
phonetic and semantic similarity. In NAACL.
Harold W. Kuhn. 1955. The Hungarian method for
the assignment problem. Naval Research Logistics
Quarterly, 2:83–97.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP.
John B. Lowe and Martine Mazaudon. 1994. The re-
construction engine: a computer implementation of
the comparative method. Computational Linguis-
tics, 20(3):381–417.
Gideon S. Mann and David Yarowsky. 2001. Mul-
tipath translation lexicon induction via bridge lan-
guages. In NAACL, pages 1–8. Association for
Computational Linguistics.
Thomas P. Minka. 2001. Expectation propagation for
approximate bayesian inference. In UAI, pages 362–
369.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech pro-
cessing. In ECAI-96 Workshop. John Wiley and
Sons.
Mehryar Mohri, 2009. Handbook of Weighted Au-
tomata, chapter Weighted Automata Algorithms.
Springer.
Andrea Mulloni. 2007. Automatic prediction of cog-
nate orthography using support vector machines. In
ACL, pages 25–30.
John Nerbonne. 2010. Measuring the diffusion of lin-
guistic change. Philosophical Transactions of the
Royal Society B: Biological Sciences.
Michael P. Oakes. 2000. Computer estimation of
vocabulary in a protolanguage from word lists in
four daughter languages. Quantitative Linguistics,
7(3):233–243.
OED. 1989. “day, n.”. In The Oxford English Dictio-
nary online. Oxford University Press.
John Ohala, 1993. Historical linguistics: Problems
and perspectives, chapter The phonetics of sound
change, pages 237–238. Longman.
Jose Oncina and Marc Sebban. 2006. Learning
stochastic edit distance: Application in handwritten
character recognition. Pattern Recognition, 39(9).
Don Ringe, Tandy Warnow, and Ann Taylor. 2002.
Indo-european and computational cladistics. Trans-
actions of the Philological Society, 100(1):59–129.
Alan S.C. Ross. 1950. Philological probability prob-
lems. Journal of the Royal Statistical Society Series
B.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2000. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In NAACL.
</reference>
<page confidence="0.995958">
1039
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.980559">
<title confidence="0.999994">Finding Cognate Groups using Phylogenies</title>
<author confidence="0.99975">Hall Klein</author>
<affiliation confidence="0.996011">Computer Science Division University of California, Berkeley</affiliation>
<abstract confidence="0.999475454545455">A central problem in historical linguistics is the identification of historically related We present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word lists. Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages. We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes. On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline approach, increasing accuracy by as much as 80%. Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leonard Bloomfield</author>
</authors>
<date>1938</date>
<location>Language. Holt, New York.</location>
<contexts>
<context position="2916" citStr="Bloomfield, 1938" startWordPosition="427" endWordPosition="428">ers, assume that cognate groups are already known. In this work, we address this limitation, presenting a model in which cognate groups can be discovered automatically. Finding cognate groups is not an easy task, because underlying morphological and phonological changes can obscure relationships between words, especially for distant cognates, where simple string overlap is an inadequate measure of similarity. Indeed, a standard string similarity metric like Levenshtein distance can lead to false positives. Consider the often cited example of Greek /ma:ti/ and Malay /mata/, both meaning “eye” (Bloomfield, 1938). If we were to rely on Levenshtein distance, these words would seem to be a highly attractive match as cognates: they are nearly identical, essentially differing in only a single character. However, no linguist would posit that these two words are related. To correctly learn that they are not related, linguists typically rely on two kinds of evidence. First, because sound change is largely regular, we would need to commonly see /i/ in Greek wherever we see /a/ in Malay (Ross, 1950). Second, we should look at languages closely related to Greek and Malay, to see if similar patterns hold there, </context>
<context position="15590" citStr="Bloomfield (1938)" startWordPosition="2535" endWordPosition="2537">nt sound changes. For each ϕ` we fit a nonuniform substitution, insertion, and deletion matrix σ(x, y). These edit distances define a condi2We note two further difficulties: our model does not handle “borrowings,” which would be necessary to capture a significant portion of English vocabulary; nor can it seamlessly handle words that are inherited later in the evolution of language than others. For instance, French borrowed words from its parent language Latin during the Renaissance and the Enlightenment that have not undergone the same changes as words that evolved “naturally” from Latin. See Bloomfield (1938). Handling these cases is a direction for future research. 3Strictly, we can cast this problem in a variational framework similar to mean field where we iteratively maximize parameters to minimize a KL-divergence. We omit details for clarity. π∗` = arg max πe E g 1033 tional exponential family distribution when conditioned on an ancestral word. That is, for any fixed wa: �(wd|wa, σ) = � score(z;σ) wd z∈ align(wa,wd) ri σ(x, y) = 1 align(wa,wd) z∈ (x,y)Ez where align(wa, wd) is the set of possible alignments between the phonemes in words wa and wd. We are seeking the maximum likelihood estimate</context>
</contexts>
<marker>Bloomfield, 1938</marker>
<rawString>Leonard Bloomfield. 1938. Language. Holt, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Percy Liang</author>
<author>Thomas Griffiths</author>
<author>Dan Klein</author>
</authors>
<title>A probabilistic approach to diachronic phonology.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<marker>Bouchard-Cˆot´e, Liang, Griffiths, Klein, 2007</marker>
<rawString>Alexandre Bouchard-Cˆot´e, Percy Liang, Thomas Griffiths, and Dan Klein. 2007. A probabilistic approach to diachronic phonology. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Thomas L Griffiths</author>
<author>Dan Klein</author>
</authors>
<title>Improved reconstruction of protolanguage word forms.</title>
<date>2009</date>
<booktitle>In NAACL,</booktitle>
<pages>65--73</pages>
<marker>Bouchard-Cˆot´e, Griffiths, Klein, 2009</marker>
<rawString>Alexandre Bouchard-Cˆot´e, Thomas L. Griffiths, and Dan Klein. 2009. Improved reconstruction of protolanguage word forms. In NAACL, pages 65–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Lyle Campbell</author>
</authors>
<title>A Bayesian model for discovering typological implications.</title>
<date>2007</date>
<booktitle>In Conference of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Daum´e, Campbell, 2007</marker>
<rawString>Hal Daum´e III and Lyle Campbell. 2007. A Bayesian model for discovering typological implications. In Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Non-parametric Bayesian model areal linguistics.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<marker>Daum´e, 2009</marker>
<rawString>Hal Daum´e III. 2009. Non-parametric Bayesian model areal linguistics. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Graphical models over multiple strings.</title>
<date>2009</date>
<booktitle>In EMNLP, Singapore,</booktitle>
<contexts>
<context position="5014" citStr="Dreyer and Eisner, 2009" startWordPosition="760" endWordPosition="763">decides which groups are attested in each modern language. An evolutionary model captures how each word is generated from its parent word. Finally, an alignment model maps the flat word lists to cognate groups. Inference requires a combination of message-passing in the evolutionary model and iterative bipartite graph matching in the alignment model. In the message-passing phase, our model encodes distributions over strings as weighted finite state automata (Mohri, 2009). Weighted automata have been successfully applied to speech processing (Mohri et al., 1996) and more recently to morphology (Dreyer and Eisner, 2009). Here, we present a new method for automatically compressing our message automata in a way that can take into account prior information about the expected outcome of inference. In this paper, we focus on a transcribed word list of 583 cognate sets from three Romance languages (Portuguese, Italian and Spanish), as well as their common ancestor Latin (Bouchard-Cˆot´e et al., 2007). We consider both the case where we know that all cognate groups have a surface form in all languages, and where we do not know that. On the former, easier task we achieve identification accuracies of 90.6%. On the la</context>
<context position="17967" citStr="Dreyer and Eisner, 2009" startWordPosition="2949" endWordPosition="2952">transition types (insertion, substitution/match, deletion) are normalized for each ancestral phoneme. 5 Transducers and Automata In our model, it is not just the edit distances that are finite state machines. Indeed, the words themselves are string-valued random variables that have, in principle, an infinite domain. To represent distributions and messages over these variables, we chose weighted finite state automata, which can compactly represent functions over strings. Unfortunately, while initially compact, these automata become unwieldy during inference, and so approximations must be used (Dreyer and Eisner, 2009). In this section, we summarize the standard algorithms and representations used for weighted finite state transducers. For more detailed treatment of the general transducer operations, we direct readers to Mohri (2009). A weighted automaton (resp. transducer) encodes a function over strings (resp. pairs of strings) as weighted paths through a directed graph. Each edge in the graph has a real-valued weight4 and a label, which is a single phoneme in some alphabet Σ or the empty phoneme ε (resp. pair of labels in some alphabet ΣxΔ). The weight of a string is then the sum of all paths through the</context>
<context position="21679" citStr="Dreyer and Eisner, 2009" startWordPosition="3569" endWordPosition="3573">he cognate group. Minimization can only help so much: in order for two states to be collapsed, the distribution over transitions from those states must be indistinguishable. In practice, for the automata generated in our model, minimization removes at most half the states, which is not sufficient to counteract the exponential growth. Thus, we need to find a way to approximate a message µ(w) using a simpler automata µ(w; θ) taken from a restricted class parameterized by θ. In the context of transducers, previous authors have focused on a combination of n-best lists and unigram back-off models (Dreyer and Eisner, 2009), a schematic diagram of which is in Figure 2(d). For their problem, n-best lists are sensible: their nodes’ local potentials already focus messages on a small number of hypotheses. In our setting, however, n-best lists are problematic; early experiments showed that a 10,000-best list for a typical message only accounts for 50% of message log perplexity. That is, the posterior marginals in our model are (at least initially) fairly flat. An alternative approach might be to simply treat messages as unnormalized probability distributions, and to minimize the KL divergence beFigure 2: Various topo</context>
</contexts>
<marker>Dreyer, Eisner, 2009</marker>
<rawString>Markus Dreyer and Jason Eisner. 2009. Graphical models over multiple strings. In EMNLP, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Expectation semirings: Flexible EM for finite-state transducers.</title>
<date>2001</date>
<editor>In Gertjan van Noord, editor, FSMNLP.</editor>
<contexts>
<context position="16805" citStr="Eisner, 2001" startWordPosition="2757" endWordPosition="2758">f each ϕ, given fixed alignments π: ˆϕ` = arg max p(W|ϕ, π) ϕe To find this maximizer for any given π`, we need to find a marginal distribution over the edges connecting any two languages a and d. With this distribution, we calculate the expected “alignment unigrams.” That is, for each pair of phonemes x and y (or empty phoneme ε), we need to find the quantity: Ep(wa,wd)[#(x, y; z)] = E wa,wd align(wa,wd) z∈ where we denote #(x, y; z) to be the number of times the pair of phonemes (x, y) are aligned in alignment z. The exact method for computing these counts is to use an expectation semiring (Eisner, 2001). Given the expected counts, we now need to normalize them to ensure that the transducer represents a conditional probability distribution (Eisner, 2002; Oncina and Sebban, 2006). We have that, for each phoneme x in the ancestor language: ηy = E[#(ε, y; z)] E[#(·, ·;z)] E[#(x, y; z)] ηy0) E[#(x, ·; z)] E[#(x, ε; z)] ηy0) E[#(x,·; z)] Here, we have #(·, ·; z) = Ex,y #(x, y; z) and #(x, ·; z) = Ey #(x, y; z). The (1 − Ey0 ηy0) term ensure that for any ancestral phoneme x, Ey ηy+Ey σ(x, y)+δx = 1. These equations ensure that the three transition types (insertion, substitution/match, deletion) are</context>
<context position="24550" citStr="Eisner, 2001" startWordPosition="4115" endWordPosition="4116">1 g o e o f g e g o e u f 4 u f o g e 5 g u e g o e e g g ug u e g o (b) e u f u o o f f o e u o e g o e ff g o f u u (d) f f f f u o f f u e o g e e e u g eg θ� = arg min B �= arg min B w �= arg min B w 1035 as follows. Given a deterministic prior automaton T, and a deterministic automaton topology µ*, we create the composed unweighted automaton Toµ*, and calculate arc transitions weights to minimize the KL divergence between that composed transducer and T o µ. The procedure for calculating these statistics is described in Li and Eisner (2009), which amounts to using an expectation semiring (Eisner, 2001) to compute expected transitions in T o µ* under the probability distribution Toµ. From there, we need to create the automaton T−1 o T o A. That is, we need to divide out the influence of T(w). Since we know the topology and arc weights for T ahead of time, this is often as simple as dividing arc weights in T o µ� by the corresponding arc weight in T(w). For example, if T encodes a geometric distribution over word lengths and a uniform distribution over phonemes (that is, T(w) a p|w|), then computing µ� is as simple as dividing each arc in T o µ� by p.7 There are a number of choices for T. One</context>
</contexts>
<marker>Eisner, 2001</marker>
<rawString>Jason Eisner. 2001. Expectation semirings: Flexible EM for finite-state transducers. In Gertjan van Noord, editor, FSMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Parameter estimation for probabilistic finite-state transducers.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="16957" citStr="Eisner, 2002" startWordPosition="2781" endWordPosition="2783">edges connecting any two languages a and d. With this distribution, we calculate the expected “alignment unigrams.” That is, for each pair of phonemes x and y (or empty phoneme ε), we need to find the quantity: Ep(wa,wd)[#(x, y; z)] = E wa,wd align(wa,wd) z∈ where we denote #(x, y; z) to be the number of times the pair of phonemes (x, y) are aligned in alignment z. The exact method for computing these counts is to use an expectation semiring (Eisner, 2001). Given the expected counts, we now need to normalize them to ensure that the transducer represents a conditional probability distribution (Eisner, 2002; Oncina and Sebban, 2006). We have that, for each phoneme x in the ancestor language: ηy = E[#(ε, y; z)] E[#(·, ·;z)] E[#(x, y; z)] ηy0) E[#(x, ·; z)] E[#(x, ε; z)] ηy0) E[#(x,·; z)] Here, we have #(·, ·; z) = Ex,y #(x, y; z) and #(x, ·; z) = Ey #(x, y; z). The (1 − Ey0 ηy0) term ensure that for any ancestral phoneme x, Ey ηy+Ey σ(x, y)+δx = 1. These equations ensure that the three transition types (insertion, substitution/match, deletion) are normalized for each ancestral phoneme. 5 Transducers and Automata In our model, it is not just the edit distances that are finite state machines. Indee</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002. Parameter estimation for probabilistic finite-state transducers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
<author>Daniel Marcu</author>
<author>Keven Knight</author>
</authors>
<title>Cognates can improve statistical translation models.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="29354" citStr="Kondrak et al., 2003" startWordPosition="4938" endWordPosition="4941">lso consider a proto-word reconstruction experiment. For this experiment, using the system of Bouchard-Cˆot´e et al. (2009), we compare the reconstructions produced from our automatic groups to those produced from gold cognate groups. 7.1 Baseline As a novel but heuristic baseline for cognate group detection, we use an iterative bipartite matching algorithm where instead of conditional likelihoods for affinities we use Dice’s coefficient, defined for sets X and Y as: 2|X ∩ Y | Dice(X, Y ) = (3) |X |+ |Y | Dice’s coefficients are commonly used in bilingual detection of cognates (Kondrak, 2001; Kondrak et al., 2003). We follow prior work and use sets of bigrams within words. In our case, during bipartite matching the set X is the set of bigrams in the language being re-permuted, and Y is the union of bigrams in the other languages. 7.2 Experiment 1: Complete Data In this experiment, we know precisely how many cognate groups there are and that every cognate group has a word in each language. While this scenario does not include all of the features of the real-world task, it represents a good test case of how well these models can perform without the non-parametric task of deciding how many clusters to use</context>
</contexts>
<marker>Kondrak, Marcu, Knight, 2003</marker>
<rawString>Grzegorz Kondrak, Daniel Marcu, and Keven Knight. 2003. Cognates can improve statistical translation models. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
</authors>
<title>Identifying cognates by phonetic and semantic similarity.</title>
<date>2001</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="3664" citStr="Kondrak, 2001" startWordPosition="552" endWordPosition="554">ical, essentially differing in only a single character. However, no linguist would posit that these two words are related. To correctly learn that they are not related, linguists typically rely on two kinds of evidence. First, because sound change is largely regular, we would need to commonly see /i/ in Greek wherever we see /a/ in Malay (Ross, 1950). Second, we should look at languages closely related to Greek and Malay, to see if similar patterns hold there, too. Some authors have attempted to automatically detect cognate words (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), but these methods 1030 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1030–1039, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics typically work on language pairs rather than on larger language families. To fully automate the comparative method, it is necessary to consider multiple languages, and to do so in a model which couples cognate detection with similarity learning. In this paper, we present a new generative model for the automatic induction of cognate groups given only (1) a known family</context>
<context position="29331" citStr="Kondrak, 2001" startWordPosition="4936" endWordPosition="4937">Therefore, we also consider a proto-word reconstruction experiment. For this experiment, using the system of Bouchard-Cˆot´e et al. (2009), we compare the reconstructions produced from our automatic groups to those produced from gold cognate groups. 7.1 Baseline As a novel but heuristic baseline for cognate group detection, we use an iterative bipartite matching algorithm where instead of conditional likelihoods for affinities we use Dice’s coefficient, defined for sets X and Y as: 2|X ∩ Y | Dice(X, Y ) = (3) |X |+ |Y | Dice’s coefficients are commonly used in bilingual detection of cognates (Kondrak, 2001; Kondrak et al., 2003). We follow prior work and use sets of bigrams within words. In our case, during bipartite matching the set X is the set of bigrams in the language being re-permuted, and Y is the union of bigrams in the other languages. 7.2 Experiment 1: Complete Data In this experiment, we know precisely how many cognate groups there are and that every cognate group has a word in each language. While this scenario does not include all of the features of the real-world task, it represents a good test case of how well these models can perform without the non-parametric task of deciding h</context>
</contexts>
<marker>Kondrak, 2001</marker>
<rawString>Grzegorz Kondrak. 2001. Identifying cognates by phonetic and semantic similarity. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The Hungarian method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistics Quarterly,</journal>
<pages>2--83</pages>
<contexts>
<context position="12731" citStr="Kuhn, 1955" startWordPosition="2066" endWordPosition="2067">ass, we send messages from the leaves of the tree toward the root. For observed leaf nodes Wd, we have: µd→a(wa) = p(Wd = wd|wa, ϕd) and for interior nodes Wi: Eµi→a(wa) = Hp(wi|wa, ϕi) µd→i(wi) w; d∈child(w;) (1) In the downward pass (toward the language `), we sum over ancestral words Wa: µa→d(wd) E= Hp(wd|wa, ϕd)µa0→a(wa) µd0→a(wa) wa d0∈child(wa) d06�d where a0 is the ancestor of a. Computing these messages gives a posterior marginal distribution µ`(w`) = p(w`|w−`,π−e(g), ϕ,π−`), which is precisely the affinity score we need for the bipartite matching. We then use the Hungarian algorithm (Kuhn, 1955) to find the optimal assignment for the bipartite matching problem. One important final note is initialization. In our early experiments we found that choosing a random starting configuration unsurprisingly led to rather poor local optima. Instead, we started with empty trees, and added in one language per iteration until all languages were added, and then continued iterations on the full tree. 4 Learning So far we have only addressed searching for Viterbi alignments π under fixed parameters. In practice, it is important to estimate better parametric edit distances ϕ` and survival variables S`</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>Harold W. Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2:83–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and secondorder expectation semirings with applications to minimum-risk training on translation forests.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="24487" citStr="Li and Eisner (2009)" startWordPosition="4103" endWordPosition="4107"> Expectation Propagation (Minka, 2001). u u 2 3 g o f f u (a) (c) 0 e 1 g o e o f g e g o e u f 4 u f o g e 5 g u e g o e e g g ug u e g o (b) e u f u o o f f o e u o e g o e ff g o f u u (d) f f f f u o f f u e o g e e e u g eg θ� = arg min B �= arg min B w �= arg min B w 1035 as follows. Given a deterministic prior automaton T, and a deterministic automaton topology µ*, we create the composed unweighted automaton Toµ*, and calculate arc transitions weights to minimize the KL divergence between that composed transducer and T o µ. The procedure for calculating these statistics is described in Li and Eisner (2009), which amounts to using an expectation semiring (Eisner, 2001) to compute expected transitions in T o µ* under the probability distribution Toµ. From there, we need to create the automaton T−1 o T o A. That is, we need to divide out the influence of T(w). Since we know the topology and arc weights for T ahead of time, this is often as simple as dividing arc weights in T o µ� by the corresponding arc weight in T(w). For example, if T encodes a geometric distribution over word lengths and a uniform distribution over phonemes (that is, T(w) a p|w|), then computing µ� is as simple as dividing eac</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and secondorder expectation semirings with applications to minimum-risk training on translation forests. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John B Lowe</author>
<author>Martine Mazaudon</author>
</authors>
<title>The reconstruction engine: a computer implementation of the comparative method.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="3636" citStr="Lowe and Mazaudon, 1994" startWordPosition="546" endWordPosition="549">tch as cognates: they are nearly identical, essentially differing in only a single character. However, no linguist would posit that these two words are related. To correctly learn that they are not related, linguists typically rely on two kinds of evidence. First, because sound change is largely regular, we would need to commonly see /i/ in Greek wherever we see /a/ in Malay (Ross, 1950). Second, we should look at languages closely related to Greek and Malay, to see if similar patterns hold there, too. Some authors have attempted to automatically detect cognate words (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), but these methods 1030 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1030–1039, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics typically work on language pairs rather than on larger language families. To fully automate the comparative method, it is necessary to consider multiple languages, and to do so in a model which couples cognate detection with similarity learning. In this paper, we present a new generative model for the automatic induction of cognate groups g</context>
</contexts>
<marker>Lowe, Mazaudon, 1994</marker>
<rawString>John B. Lowe and Martine Mazaudon. 1994. The reconstruction engine: a computer implementation of the comparative method. Computational Linguistics, 20(3):381–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>David Yarowsky</author>
</authors>
<title>Multipath translation lexicon induction via bridge languages. In</title>
<date>2001</date>
<booktitle>NAACL,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3611" citStr="Mann and Yarowsky, 2001" startWordPosition="542" endWordPosition="545">be a highly attractive match as cognates: they are nearly identical, essentially differing in only a single character. However, no linguist would posit that these two words are related. To correctly learn that they are not related, linguists typically rely on two kinds of evidence. First, because sound change is largely regular, we would need to commonly see /i/ in Greek wherever we see /a/ in Malay (Ross, 1950). Second, we should look at languages closely related to Greek and Malay, to see if similar patterns hold there, too. Some authors have attempted to automatically detect cognate words (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), but these methods 1030 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1030–1039, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics typically work on language pairs rather than on larger language families. To fully automate the comparative method, it is necessary to consider multiple languages, and to do so in a model which couples cognate detection with similarity learning. In this paper, we present a new generative model for the automatic indu</context>
</contexts>
<marker>Mann, Yarowsky, 2001</marker>
<rawString>Gideon S. Mann and David Yarowsky. 2001. Multipath translation lexicon induction via bridge languages. In NAACL, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas P Minka</author>
</authors>
<title>Expectation propagation for approximate bayesian inference.</title>
<date>2001</date>
<booktitle>In UAI,</booktitle>
<pages>362--369</pages>
<contexts>
<context position="23905" citStr="Minka, 2001" startWordPosition="3945" endWordPosition="3946">message, τ could be a stand-in for a forward probability.6 In this paper, µ(w) is a complex automaton with potentially many states, µ(w; θ) is a simple parametric automaton with forms that we discuss below, and τ(w) is an arbitrary (but hopefully fairly simple) automaton. The actual method we use is 5As an extreme example, suppose we have observed that Wd = wd and that P(Wd = wd|wa) = 1 for all ancestral words wa. Then, clearly Ewd µ(wd) = � � P(Wd = wd wd|wa) = oo whenever there are an infinite number of possible ancestral strings wa. 6This approach is reminiscent of Expectation Propagation (Minka, 2001). u u 2 3 g o f f u (a) (c) 0 e 1 g o e o f g e g o e u f 4 u f o g e 5 g u e g o e e g g ug u e g o (b) e u f u o o f f o e u o e g o e ff g o f u u (d) f f f f u o f f u e o g e e e u g eg θ� = arg min B �= arg min B w �= arg min B w 1035 as follows. Given a deterministic prior automaton T, and a deterministic automaton topology µ*, we create the composed unweighted automaton Toµ*, and calculate arc transitions weights to minimize the KL divergence between that composed transducer and T o µ. The procedure for calculating these statistics is described in Li and Eisner (2009), which amounts to</context>
</contexts>
<marker>Minka, 2001</marker>
<rawString>Thomas P. Minka. 2001. Expectation propagation for approximate bayesian inference. In UAI, pages 362– 369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted automata in text and speech processing.</title>
<date>1996</date>
<booktitle>In ECAI-96 Workshop.</booktitle>
<publisher>John Wiley and Sons.</publisher>
<contexts>
<context position="4956" citStr="Mohri et al., 1996" startWordPosition="750" endWordPosition="753">rd survival generates a number of cognate groups and decides which groups are attested in each modern language. An evolutionary model captures how each word is generated from its parent word. Finally, an alignment model maps the flat word lists to cognate groups. Inference requires a combination of message-passing in the evolutionary model and iterative bipartite graph matching in the alignment model. In the message-passing phase, our model encodes distributions over strings as weighted finite state automata (Mohri, 2009). Weighted automata have been successfully applied to speech processing (Mohri et al., 1996) and more recently to morphology (Dreyer and Eisner, 2009). Here, we present a new method for automatically compressing our message automata in a way that can take into account prior information about the expected outcome of inference. In this paper, we focus on a transcribed word list of 583 cognate sets from three Romance languages (Portuguese, Italian and Spanish), as well as their common ancestor Latin (Bouchard-Cˆot´e et al., 2007). We consider both the case where we know that all cognate groups have a surface form in all languages, and where we do not know that. On the former, easier tas</context>
</contexts>
<marker>Mohri, Pereira, Riley, 1996</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 1996. Weighted automata in text and speech processing. In ECAI-96 Workshop. John Wiley and Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Handbook of Weighted Automata, chapter Weighted Automata Algorithms.</title>
<date>2009</date>
<publisher>Springer.</publisher>
<contexts>
<context position="4864" citStr="Mohri, 2009" startWordPosition="738" endWordPosition="739">known family tree of languages and (2) word lists from those languages. A prior on word survival generates a number of cognate groups and decides which groups are attested in each modern language. An evolutionary model captures how each word is generated from its parent word. Finally, an alignment model maps the flat word lists to cognate groups. Inference requires a combination of message-passing in the evolutionary model and iterative bipartite graph matching in the alignment model. In the message-passing phase, our model encodes distributions over strings as weighted finite state automata (Mohri, 2009). Weighted automata have been successfully applied to speech processing (Mohri et al., 1996) and more recently to morphology (Dreyer and Eisner, 2009). Here, we present a new method for automatically compressing our message automata in a way that can take into account prior information about the expected outcome of inference. In this paper, we focus on a transcribed word list of 583 cognate sets from three Romance languages (Portuguese, Italian and Spanish), as well as their common ancestor Latin (Bouchard-Cˆot´e et al., 2007). We consider both the case where we know that all cognate groups ha</context>
<context position="18186" citStr="Mohri (2009)" startWordPosition="2984" endWordPosition="2985">selves are string-valued random variables that have, in principle, an infinite domain. To represent distributions and messages over these variables, we chose weighted finite state automata, which can compactly represent functions over strings. Unfortunately, while initially compact, these automata become unwieldy during inference, and so approximations must be used (Dreyer and Eisner, 2009). In this section, we summarize the standard algorithms and representations used for weighted finite state transducers. For more detailed treatment of the general transducer operations, we direct readers to Mohri (2009). A weighted automaton (resp. transducer) encodes a function over strings (resp. pairs of strings) as weighted paths through a directed graph. Each edge in the graph has a real-valued weight4 and a label, which is a single phoneme in some alphabet Σ or the empty phoneme ε (resp. pair of labels in some alphabet ΣxΔ). The weight of a string is then the sum of all paths through the graph that accept that string. For our purposes, we are concerned with three fundamental operations on weighted transducers. The first is computing the sum of all paths through a transducer, which corresponds to comput</context>
</contexts>
<marker>Mohri, 2009</marker>
<rawString>Mehryar Mohri, 2009. Handbook of Weighted Automata, chapter Weighted Automata Algorithms. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Mulloni</author>
</authors>
<title>Automatic prediction of cognate orthography using support vector machines.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>25--30</pages>
<contexts>
<context position="3680" citStr="Mulloni, 2007" startWordPosition="555" endWordPosition="556">ly differing in only a single character. However, no linguist would posit that these two words are related. To correctly learn that they are not related, linguists typically rely on two kinds of evidence. First, because sound change is largely regular, we would need to commonly see /i/ in Greek wherever we see /a/ in Malay (Ross, 1950). Second, we should look at languages closely related to Greek and Malay, to see if similar patterns hold there, too. Some authors have attempted to automatically detect cognate words (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), but these methods 1030 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1030–1039, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics typically work on language pairs rather than on larger language families. To fully automate the comparative method, it is necessary to consider multiple languages, and to do so in a model which couples cognate detection with similarity learning. In this paper, we present a new generative model for the automatic induction of cognate groups given only (1) a known family tree of languag</context>
</contexts>
<marker>Mulloni, 2007</marker>
<rawString>Andrea Mulloni. 2007. Automatic prediction of cognate orthography using support vector machines. In ACL, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Nerbonne</author>
</authors>
<title>Measuring the diffusion of linguistic change.</title>
<date>2010</date>
<journal>Philosophical Transactions of the Royal Society B: Biological Sciences.</journal>
<contexts>
<context position="2170" citStr="Nerbonne, 2010" startWordPosition="316" endWordPosition="317"> these variables interact and inform each other, and so historical linguists often consider them jointly. However, linguists are currently required to make qualitative judgments regarding the relative likelihood of certain sound changes, cognate groups, and so on. Several recent statistical methods have been introduced to provide increased quantitative backing to the comparative method (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009); others have modeled the spread of language changes and speciation (Ringe et al., 2002; Daum´e III and Campbell, 2007; Daum´e III, 2009; Nerbonne, 2010). These automated methods, while providing robustness and scale in the induction of ancestral word forms and evolutionary parameters, assume that cognate groups are already known. In this work, we address this limitation, presenting a model in which cognate groups can be discovered automatically. Finding cognate groups is not an easy task, because underlying morphological and phonological changes can obscure relationships between words, especially for distant cognates, where simple string overlap is an inadequate measure of similarity. Indeed, a standard string similarity metric like Levenshte</context>
</contexts>
<marker>Nerbonne, 2010</marker>
<rawString>John Nerbonne. 2010. Measuring the diffusion of linguistic change. Philosophical Transactions of the Royal Society B: Biological Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael P Oakes</author>
</authors>
<title>Computer estimation of vocabulary in a protolanguage from word lists in four daughter languages.</title>
<date>2000</date>
<journal>Quantitative Linguistics,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="1956" citStr="Oakes, 2000" startWordPosition="282" endWordPosition="283">verall phylogenetic guide tree among languages, the evolutionary parameters of the ambient changes at each branch, and the cognate group structure that specifies which words share common ancestors. All three of these variables interact and inform each other, and so historical linguists often consider them jointly. However, linguists are currently required to make qualitative judgments regarding the relative likelihood of certain sound changes, cognate groups, and so on. Several recent statistical methods have been introduced to provide increased quantitative backing to the comparative method (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009); others have modeled the spread of language changes and speciation (Ringe et al., 2002; Daum´e III and Campbell, 2007; Daum´e III, 2009; Nerbonne, 2010). These automated methods, while providing robustness and scale in the induction of ancestral word forms and evolutionary parameters, assume that cognate groups are already known. In this work, we address this limitation, presenting a model in which cognate groups can be discovered automatically. Finding cognate groups is not an easy task, because underlying morphological and phonolo</context>
<context position="3649" citStr="Oakes, 2000" startWordPosition="550" endWordPosition="551"> nearly identical, essentially differing in only a single character. However, no linguist would posit that these two words are related. To correctly learn that they are not related, linguists typically rely on two kinds of evidence. First, because sound change is largely regular, we would need to commonly see /i/ in Greek wherever we see /a/ in Malay (Ross, 1950). Second, we should look at languages closely related to Greek and Malay, to see if similar patterns hold there, too. Some authors have attempted to automatically detect cognate words (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), but these methods 1030 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1030–1039, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics typically work on language pairs rather than on larger language families. To fully automate the comparative method, it is necessary to consider multiple languages, and to do so in a model which couples cognate detection with similarity learning. In this paper, we present a new generative model for the automatic induction of cognate groups given only (1)</context>
</contexts>
<marker>Oakes, 2000</marker>
<rawString>Michael P. Oakes. 2000. Computer estimation of vocabulary in a protolanguage from word lists in four daughter languages. Quantitative Linguistics, 7(3):233–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>OED</author>
</authors>
<title>day, n.”.</title>
<date>1989</date>
<booktitle>In The Oxford English Dictionary online.</booktitle>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="13600" citStr="OED, 1989" startWordPosition="2209" endWordPosition="2210"> with empty trees, and added in one language per iteration until all languages were added, and then continued iterations on the full tree. 4 Learning So far we have only addressed searching for Viterbi alignments π under fixed parameters. In practice, it is important to estimate better parametric edit distances ϕ` and survival variables S`. To motivate the need for good transducers, consider the example of English “day” /deI/ and Latin “di¯es” /dIe:s/, both with the same meaning. Surprisingly, these words are in no way related, with English “day” probably coming from a verb meaning “to burn” (OED, 1989). However, a naively constructed edit distance, which for example might penalize vowel substitutions lightly, would fail to learn that Latin words that are borrowed into English would not undergo the sound change /I/→/eI/. Therefore, our model must learn not only which sound changes are plausible (e.g. vowels turning into other vowels is more common than vowels turning into consonants), but which changes are appropriate for a given language.2 At a high level, our learning algorithm is much like Expectation Maximization with hard assignments: after we update the alignment variables π and thus f</context>
</contexts>
<marker>OED, 1989</marker>
<rawString>OED. 1989. “day, n.”. In The Oxford English Dictionary online. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Ohala</author>
</authors>
<title>Historical linguistics: Problems and perspectives, chapter The phonetics of sound change,</title>
<date>1993</date>
<pages>237--238</pages>
<publisher>Longman.</publisher>
<contexts>
<context position="1120" citStr="Ohala, 1993" startWordPosition="161" endWordPosition="162">he alignment between the words lists of the observed languages. We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes. On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline approach, increasing accuracy by as much as 80%. Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words. 1 Introduction A crowning achievement of historical linguistics is the comparative method (Ohala, 1993), wherein linguists use word similarity to elucidate the hidden phonological and morphological processes which govern historical descent. The comparative method requires reasoning about three important hidden variables: the overall phylogenetic guide tree among languages, the evolutionary parameters of the ambient changes at each branch, and the cognate group structure that specifies which words share common ancestors. All three of these variables interact and inform each other, and so historical linguists often consider them jointly. However, linguists are currently required to make qualitati</context>
</contexts>
<marker>Ohala, 1993</marker>
<rawString>John Ohala, 1993. Historical linguistics: Problems and perspectives, chapter The phonetics of sound change, pages 237–238. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jose Oncina</author>
<author>Marc Sebban</author>
</authors>
<title>Learning stochastic edit distance: Application in handwritten character recognition.</title>
<date>2006</date>
<journal>Pattern Recognition,</journal>
<volume>39</volume>
<issue>9</issue>
<contexts>
<context position="16983" citStr="Oncina and Sebban, 2006" startWordPosition="2784" endWordPosition="2787">ng any two languages a and d. With this distribution, we calculate the expected “alignment unigrams.” That is, for each pair of phonemes x and y (or empty phoneme ε), we need to find the quantity: Ep(wa,wd)[#(x, y; z)] = E wa,wd align(wa,wd) z∈ where we denote #(x, y; z) to be the number of times the pair of phonemes (x, y) are aligned in alignment z. The exact method for computing these counts is to use an expectation semiring (Eisner, 2001). Given the expected counts, we now need to normalize them to ensure that the transducer represents a conditional probability distribution (Eisner, 2002; Oncina and Sebban, 2006). We have that, for each phoneme x in the ancestor language: ηy = E[#(ε, y; z)] E[#(·, ·;z)] E[#(x, y; z)] ηy0) E[#(x, ·; z)] E[#(x, ε; z)] ηy0) E[#(x,·; z)] Here, we have #(·, ·; z) = Ex,y #(x, y; z) and #(x, ·; z) = Ey #(x, y; z). The (1 − Ey0 ηy0) term ensure that for any ancestral phoneme x, Ey ηy+Ey σ(x, y)+δx = 1. These equations ensure that the three transition types (insertion, substitution/match, deletion) are normalized for each ancestral phoneme. 5 Transducers and Automata In our model, it is not just the edit distances that are finite state machines. Indeed, the words themselves ar</context>
</contexts>
<marker>Oncina, Sebban, 2006</marker>
<rawString>Jose Oncina and Marc Sebban. 2006. Learning stochastic edit distance: Application in handwritten character recognition. Pattern Recognition, 39(9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don Ringe</author>
<author>Tandy Warnow</author>
<author>Ann Taylor</author>
</authors>
<title>Indo-european and computational cladistics.</title>
<date>2002</date>
<journal>Transactions of the Philological Society,</journal>
<volume>100</volume>
<issue>1</issue>
<contexts>
<context position="2104" citStr="Ringe et al., 2002" startWordPosition="303" endWordPosition="306">cture that specifies which words share common ancestors. All three of these variables interact and inform each other, and so historical linguists often consider them jointly. However, linguists are currently required to make qualitative judgments regarding the relative likelihood of certain sound changes, cognate groups, and so on. Several recent statistical methods have been introduced to provide increased quantitative backing to the comparative method (Oakes, 2000; Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2009); others have modeled the spread of language changes and speciation (Ringe et al., 2002; Daum´e III and Campbell, 2007; Daum´e III, 2009; Nerbonne, 2010). These automated methods, while providing robustness and scale in the induction of ancestral word forms and evolutionary parameters, assume that cognate groups are already known. In this work, we address this limitation, presenting a model in which cognate groups can be discovered automatically. Finding cognate groups is not an easy task, because underlying morphological and phonological changes can obscure relationships between words, especially for distant cognates, where simple string overlap is an inadequate measure of simi</context>
</contexts>
<marker>Ringe, Warnow, Taylor, 2002</marker>
<rawString>Don Ringe, Tandy Warnow, and Ann Taylor. 2002. Indo-european and computational cladistics. Transactions of the Philological Society, 100(1):59–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan S C Ross</author>
</authors>
<title>Philological probability problems.</title>
<date>1950</date>
<journal>Journal of the Royal Statistical Society Series B.</journal>
<contexts>
<context position="3403" citStr="Ross, 1950" startWordPosition="511" endWordPosition="512">o false positives. Consider the often cited example of Greek /ma:ti/ and Malay /mata/, both meaning “eye” (Bloomfield, 1938). If we were to rely on Levenshtein distance, these words would seem to be a highly attractive match as cognates: they are nearly identical, essentially differing in only a single character. However, no linguist would posit that these two words are related. To correctly learn that they are not related, linguists typically rely on two kinds of evidence. First, because sound change is largely regular, we would need to commonly see /i/ in Greek wherever we see /a/ in Malay (Ross, 1950). Second, we should look at languages closely related to Greek and Malay, to see if similar patterns hold there, too. Some authors have attempted to automatically detect cognate words (Mann and Yarowsky, 2001; Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak, 2001; Mulloni, 2007), but these methods 1030 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1030–1039, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics typically work on language pairs rather than on larger language families. To fully automate the comparative m</context>
</contexts>
<marker>Ross, 1950</marker>
<rawString>Alan S.C. Ross. 1950. Philological probability problems. Journal of the Royal Statistical Society Series B.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora. In NAACL.</title>
<date>2000</date>
<marker>Yarowsky, Ngai, Wicentowski, 2000</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2000. Inducing multilingual text analysis tools via robust projection across aligned corpora. In NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>