<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.066655">
<title confidence="0.875718">
G2P Conversion of Proper Names Using Word Origin Information
</title>
<author confidence="0.987846">
Sonjia Waxmonsky and Sravana Reddy
</author>
<affiliation confidence="0.9988075">
Department of Computer Science
The University of Chicago
</affiliation>
<address confidence="0.664599">
Chicago, IL 60637
</address>
<email confidence="0.997596">
{wax, sravana}@cs.uchicago.edu
</email>
<sectionHeader confidence="0.993836" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999723615384615">
Motivated by the fact that the pronuncia-
tion of a name may be influenced by its
language of origin, we present methods to
improve pronunciation prediction of proper
names using word origin information. We
train grapheme-to-phoneme (G2P) models on
language-specific data sets and interpolate the
outputs. We perform experiments on US sur-
names, a data set where word origin variation
occurs naturally. Our methods can be used
with any G2P algorithm that outputs poste-
rior probabilities of phoneme sequences for a
given word.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999509576923077">
Speakers can often associate proper names with their
language of origin, even when the words have not
been seen before. For example, many English speak-
ers will recognize that Makowski and Masiello are
Polish and Italian respectively, without prior knowl-
edge of either name. Such recognition is important
for language processing tasks since the pronuncia-
tions of out-of-vocabulary (OOV) words may de-
pend on the language of origin. For example, as
noted by Llitj´os (2001), ‘sch’ is likely to be pro-
nounced as /sh/ for German-origin names (Schoe-
nenberg) and /sk/ for Italian-origin words (Schi-
avone).
In this work, we apply word origin recognition
to grapheme-to-phoneme (G2P) conversion, the task
of predicting the phonemic representation of a word
given its written form. We specifically study G2P
conversion for personal surnames, a domain where
OOVs are common and expected.
Our goal is to show how word origin information
can be used to train language-specific G2P models,
and how output from these models can be combined
to improve prediction of the best pronunciation of a
name. We deal with data sparsity in rare language
classes by re-weighting the output of the language-
specific and language-independent models.
</bodyText>
<sectionHeader confidence="0.993597" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999925615384615">
Llitj´os (2001) applies word origin information to
pronunciation modeling for speech synthesis. Here,
a CART decision tree system is presented for G2P
conversion that maps letters to phonemes using local
context. Experiments use a data set of US surnames
that naturally draws from a diverse set of origin lan-
guages, and show that the inclusion of word origin
features in the model improves pronunciation accu-
racy. We use similar data, as described in §4.1.
Some works on lexical modeling for speech
recognition also make use of word origin. Here,
the focus is on expanding the vocabulary of an ASR
system rather than choosing a single best pronunci-
ation. Maison et al. (2003) train language-specific
G2P models for eight languages and output pronun-
ciations to augment a baseline lexicon. This aug-
mented lexicon outperforms a handcrafted lexicon
in ASR experiments; error reduction is highest for
foreign names spoken by native speakers of the ori-
gin language. Cremelie and ten Bosch (2001) carry
out a similar lexicon augmentation, and make use of
penalty weighting, with different penalties for pro-
nunciations generated by the language-specific and
language-independent G2P models.
The problem of machine transliteration is closely
related to grapheme-to-phoneme conversion. Many
</bodyText>
<page confidence="0.905448333333333">
367
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 367–371,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999890909090909">
transliteration systems (Khapra and Bhattacharyya,
2009; Bose and Sarkar, 2009; Bhargava and Kon-
drak, 2010) use word origin information. The
method described by Hagiwara and Sekine (2011)
is similar to our work, except that (a) we use a data
set where multiple languages of origin occur nat-
urally, rather than creating language-specific lists
and merging them into a single set, and (b) we
consider methods of smoothing against a language-
independent model to overcome the problems of
data sparsity and errors in word origin recognition.
</bodyText>
<sectionHeader confidence="0.928006" genericHeader="method">
3 Language-Aware G2P
</sectionHeader>
<bodyText confidence="0.9999175">
Our methods are designed to be used with any
statistical G2P system that produces the posterior
probability Pr( φ|¯g) of a phoneme sequence φ for
a word (grapheme sequence) g¯ (or a score that
can be normalized to give a probability). The
most likely pronunciation of a word is taken to be
arg maxi Pr( φ|¯g).
Our baseline is a single G2P model that is trained
on all available training data. We train additional
models on language-specific training subsets and in-
corporate the output of these models to re-estimate
Pr(¯φ|¯g), which involves the following steps:
</bodyText>
<listItem confidence="0.998864666666667">
1. Train a supervised word origin classifier to pre-
dict Pr(l|w) for all l ∈ L, the set of languages
in our hand-labeled word origin training set.
2. Train G2P models for each l ∈ L. Each model
ml is trained on words with Pr(l|w) greater
than some threshold α. Here, we use α = 0.7.
3. For each word w in the test set, generate can-
didate transcriptions from model ml for each
language with nonzero Pr(l|w). Re-estimate
Pr(¯φ|¯g) by interpolating the outputs of the
language-specific models. We may also use the
output of the language-independent model.
</listItem>
<bodyText confidence="0.927042">
We elaborate on our approaches to Steps 1 and 3.
</bodyText>
<subsectionHeader confidence="0.996015">
3.1 Step 1: Word origin modeling
</subsectionHeader>
<bodyText confidence="0.964183294117647">
We apply a sequential conditional model to predict
Pr(l|w), the probability of a language class given
the word. A similar Maximum Entropy model is
presented by Chen and Maison (2003), where fea-
tures are the presence or absence of a given charac-
ter n-gram in w. In our approach, feature functions
are defined at character positions rather than over the
entire word. Specifically, for word wj composed of
character sequence c1 ... cm of length m (including
start and end symbols), binary features test for the
presence or absence of an n-gram context at each
position m. A context is the presence of a charac-
ter n-gram starting or ending at position m. Model
features are represented as:
fi(w, m,lk) = { 1, if lang(w) = lk and context
0, otherwise
i is present at position m
</bodyText>
<equation confidence="0.995984333333333">
(1)
Then, for wj = ci ... cm:
Pr(lk|wj) = exp Em Ej ifi(cm, lk)(2)
</equation>
<bodyText confidence="0.999974333333333">
where Z = Ej exp Em Ei λi fi(cm, lk) is a nor-
malization factor. In practice, we can implement this
model as a CRF, where a language label is applied
at each character position rather than for the word.
While all the language labels in a sequence need
not be the same, we find only a handful of words
where a transition occurs from one language label to
another within a word. For these cases, we take the
label of the last character in the word as the language
of origin. Experiments comparing this sequential
Maximum Entropy method with other word origin
classifiers are described by Waxmonsky (2011).
</bodyText>
<subsectionHeader confidence="0.998619">
3.2 Step 3: Re-weighting of G2P output
</subsectionHeader>
<bodyText confidence="0.987589">
We test two methods of re-weighting Pr(¯φ|¯g) us-
ing the word origin estimation and the output of
language-specific G2P models.
</bodyText>
<equation confidence="0.983851">
Method A uses only language-specific models:
˜Pr( ¯φ|¯g) = � Pr(φ|¯g, l) Pr(l|g) (3)
lEL
</equation>
<bodyText confidence="0.9902024">
where Pr(φ|¯g, l) is estimated by model ml.
Method B With the previous method, names from
infrequent classes suffer from data sparsity. We
therefore smooth with the output PI of the baseline
language-independent model.
</bodyText>
<equation confidence="0.989832333333333">
˜Pr(¯φ|¯g) = σ Pr( � Pr(φ|¯g, l) Pr(l|g)
I ¯φ|¯g)+(1−σ) (4)
lEL
</equation>
<bodyText confidence="0.856566">
The factor σ is tuned on a development set.
</bodyText>
<page confidence="0.982704">
368
</page>
<table confidence="0.99990275">
Language Train Test Base (A) (B)
Class Count Count -line
British 16.1k 2111 71.8 73.1 73.9
German 8360 1109 75.8 74.2 78.2
Italian 3358 447 61.7 66.2 65.1
Slavic 1658 232 50.9 49.6 51.7
Spanish 1460 246 44.7 41.5 48.0
French 1143 177 42.9 42.4 45.2
Dutch 468 82 70.7 52.4 68.3
Scandin. 393 61 77.1 60.7 72.1
Japanese 116 23 73.9 52.2 78.3
Arabic 68 18 33.3 11.1 38.9
Portug. 34 4 25.0 25.0 50.0
Hungarian 28 3 100.0 66.7 100.0
Other 431 72 55.6 54.2 59.7
All 67.8 67.4 70.0
</table>
<tableCaption confidence="0.9902215">
Table 1: G2P word accuracy for various weighting meth-
ods using a character-based word origin model.
</tableCaption>
<sectionHeader confidence="0.998569" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.941185">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999993">
We assemble a data set of surnames that occur fre-
quently in the United States. Since surnames are
often “Americanized” in their written and phone-
mic forms, our goal is to model how a name is
most likely to be pronounced in standard US English
rather than in its language of origin.
We consider the 50,000 most frequent surnames
in the 1990 census1, and extract those entries that
also appear in the CMU Pronouncing Dictionary2,
giving us a set of 45,841 surnames with their
phoneme representations transcribed in the Arpabet
symbol set. We divide this data 80/10/10 into train,
test, and development sets.
To build a word origin classification training set,
we randomly select 3,000 surnames from the same
census lists, and label by hand the most likely lan-
guage of origin of each name when it occurs in the
US. Labeling was done primarily using the Dictio-
nary of American Family Names (Hanks, 2003) and
Ellis Island immigration records.3 We find that, in
many cases, a surname cannot be attributed to a sin-
gle language but can be assigned to a set of lan-
</bodyText>
<footnote confidence="0.999704">
1http://www.census.gov/genealogy/names/
2http://www.speech.cs.cmu.edu/cgi-bin/
cmudict
3http://www.ellisisland.org
</footnote>
<bodyText confidence="0.99996355">
guages related by geography and language family.
For example, we discovered several surnames that
could be ambiguously labeled as English, Scottish,
or Irish in origin. For languages that are frequently
confusable, we create a single language group to be
used as a class label. Here, we use groups for British
Isles, Slavic, and Scandinavian languages. Names
of undetermined origin are removed, leaving a final
training set of 2,795 labeled surnames and 33 dif-
ferent language classes. We have made this anno-
tated word origin data publicly available for future
research.4
In these experiments, we use surnames from the
12 language classes that contain at least 10 hand-
labeled words, and merge the remaining languages
into an “Other” class. Table 1 shows the final lan-
guage classes used. Unlike the training sets, we do
not remove names with ambiguous or unknown ori-
gin from the test set, so our G2P system is also eval-
uated on the ambiguous names.
</bodyText>
<sectionHeader confidence="0.572762" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.996474826086957">
The Sequitur G2P algorithm (Bisani and Ney, 2008)
is used for all our experiments.
We use the CMU Dictionary as the gold stan-
dard, with the assumption that it contains the stan-
dard pronunciations in US English. While surnames
may have multiple valid pronunciations, we make
the simplifying assumption that a name has one best
pronunciation. Evaluation is done on the test set of
4,585 names from the CMU Dictionary.
Table 1 shows G2P accuracy for the baseline sys-
tem and Methods A and B. Test data is partitioned
by the most likely language of origin.
We see that Method A, which uses only language-
specific G2P models, has lower overall accuracy
than the baseline. We attribute this to data spar-
sity introduced by dividing the training set by lan-
guage. With the exception of British and German,
language-specific training set sizes are less than 10%
the size of the baseline training set of 37k names.
Another cause of the lowered performance is likely
due to errors made by our word origin model.
Examining results for individual language classes
for Method A, we see that Italian and British are
</bodyText>
<footnote confidence="0.9933405">
4The data may be downloaded from http://people.
cs.uchicago.edu/˜wax/wordorigin/.
</footnote>
<page confidence="0.992917">
369
</page>
<table confidence="0.999639083333333">
Language Surname Baseline Method B
Italian Carcione K AA R S IY OW N IY K AA R CH OWN IY
Cuttino K AH T IY N OW K UW T IY N OW
Lubrano L AH B R AA N OW L UW B R AA N OW
Pesola P EH S AH L AH P EH S OW L AH
Slavic Kotula K OW T UW L AH K IY K AH T UW L AH IY
Jaworowski JH AH W ER AO F S Y AH W ER AO F S K
Lisak L IY S AH K L IH S AH K
Wasik W AA S IH K V AA S IH K
Spanish Bencivenga B EH N S IH V IH N G AH B EH N CH IY V EH NG G AH
Vivona V IH V OW N AH L V IY V OW N AH
Zavadil Z AA V AA D AH Z AA V AA D IY L
</table>
<tableCaption confidence="0.895294666666667">
Table 2: Sample G2P output from the Baseline (language-independent) and Method B systems. Language labels
shown here are the arg max, P(l|w) using the character-based word origin model. Phoneme symbols are from an
Arpabet-based alphabet, as used in the CMU Pronouncing Dictionary.
</tableCaption>
<bodyText confidence="0.999988953488372">
the only language classes where accuracy improves.
For Italian, we attribute this to two factors: high
divergence in pronunciation from US English, and
the availability of enough training data to build a
successful language-specific model. In the case of
British, a language-specific model removes foreign
words but leaves enough training data to model the
language sufficiently.
Method B shows accuracy gains of 2.2%, with
gains for almost all language classes except Dutch
and Scandinavian. This is probably because names
in these two classes have almost standard US En-
glish pronunciations, and are already well-modeled
by a language-independent model.
We next look at some sample outputs from our
G2P systems. Table 2 shows names where Method
B generated the gold standard pronunciation and the
baseline system did not. For the Italian and Span-
ish sets, we see that the letter-to-phoneme mappings
produced by Method B are indicative of the lan-
guage of origin: (c → /CH/) in Carcione, (u →
/UW/) in Cuttino, (o → /OW/) in Pesola, and (i →
/IY/) in Zavadil and Vivona. Interestingly, the name
Bencivenga is categorized as Spanish but appears
with the letter-to-phoneme mapping (c → /CH/),
which corresponds to Italian as the language of ori-
gin. We found other examples of the (c → /CH/)
mappings, indicating that Italian-origin names have
been folded into Spanish data. This is not surprising
since Spanish and Italian names have high confusion
with each other. Effectively, our word origin model
produced a noisy Spanish G2P training set, but the
re-weighted G2P system is robust to these errors.
We see examples in the Slavic set where the gold
standard dictionary pronunciation is partially but not
completely Americanized. In Jaworowski, we have
the mappings (j → /Y/) and (w → /F/), both of which
are derived from the original Polish pronunciation.
But for the same name, we also have (w → /W/)
rather than (w → /V/), although the latter is truer to
the original Polish. This illustrates one of the goals
of our project, which is is to capture these patterns
of Americanization as they occur in the data.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999984388888889">
We apply word origin modeling to grapheme-
to-phoneme conversion, interpolating between
language-independent and language-specific proba-
bilistic grapheme-to-phoneme models. We find that
our system outperforms the baseline in predicting
Americanized surname pronunciations and captures
several letter-to-phoneme features that are specific
to the language of origin.
Our method operates as a wrapper around G2P
output without modifying the underlying algorithm,
and therefore can be applied to any state-of-the-art
G2P system that outputs posterior probabilities of
phoneme sequences for a word.
Future work will consider unsupervised or semi-
supervised approaches to word origin recognition
for this task, and methods to tune the smoothing
weights a at the language rather than the global
level.
</bodyText>
<page confidence="0.996163">
370
</page>
<sectionHeader confidence="0.995868" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997738305555555">
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proceed-
ings of NAACL.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication.
Dipankar Bose and Sudeshna Sarkar. 2009. Learning
multi character alignment rules and classification of
training data for transliteration. In Proceedings of the
ACL Named Entities Workshop.
Stanley F. Chen and Benoit Maison. 2003. Using place
name data to train language identification models. In
Proceedings of Eurospeech.
Nick Cremelie and Louis ten Bosch. 2001. Improv-
ing the recognition of foreign names and non-native
speech by combining multiple grapheme-to-phoneme
converters. In Proceedings of ITRW on Adaptation
Methods for Speech Recognition.
Masato Hagiwara and Satoshi Sekine. 2011. Latent class
transliteration based on source language origin. In
Proceedings of ACL.
Patrick Hanks. 2003. Dictionary of American family
names. New York: Oxford University Press.
Mitesh M. Khapra and Pushpak Bhattacharyya. 2009.
Improving transliteration accuracy using word-origin
detection and lexicon lookup. In Proceedings of the
ACL Named Entities Workshop.
Ariadna Font Llitj´os. 2001. Improving pronunciation
accuracy of proper names with language origin classes.
Master’s thesis, Carnegie Mellon University.
Benoit Maison, Stanley F. Chen, and Paul S. Cohen.
2003. Pronunciation modeling for names of foreign
origin. In Proceedings of ASRU.
Sonjia Waxmonsky. 2011. Natural language process-
ing for named entities with word-internal information.
Ph.D. thesis, University of Chicago.
</reference>
<page confidence="0.998772">
371
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.747481">
<title confidence="0.99469">G2P Conversion of Proper Names Using Word Origin Information</title>
<author confidence="0.952156">Waxmonsky</author>
<affiliation confidence="0.996654">Department of Computer The University of</affiliation>
<address confidence="0.802902">Chicago, IL</address>
<abstract confidence="0.998560142857143">Motivated by the fact that the pronunciation of a name may be influenced by its language of origin, we present methods to improve pronunciation prediction of proper names using word origin information. We train grapheme-to-phoneme (G2P) models on language-specific data sets and interpolate the outputs. We perform experiments on US surnames, a data set where word origin variation occurs naturally. Our methods can be used with any G2P algorithm that outputs posterior probabilities of phoneme sequences for a given word.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aditya Bhargava</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Language identification of names with SVMs.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="3601" citStr="Bhargava and Kondrak, 2010" startWordPosition="547" endWordPosition="551"> carry out a similar lexicon augmentation, and make use of penalty weighting, with different penalties for pronunciations generated by the language-specific and language-independent G2P models. The problem of machine transliteration is closely related to grapheme-to-phoneme conversion. Many 367 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 367–371, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics transliteration systems (Khapra and Bhattacharyya, 2009; Bose and Sarkar, 2009; Bhargava and Kondrak, 2010) use word origin information. The method described by Hagiwara and Sekine (2011) is similar to our work, except that (a) we use a data set where multiple languages of origin occur naturally, rather than creating language-specific lists and merging them into a single set, and (b) we consider methods of smoothing against a languageindependent model to overcome the problems of data sparsity and errors in word origin recognition. 3 Language-Aware G2P Our methods are designed to be used with any statistical G2P system that produces the posterior probability Pr( φ|¯g) of a phoneme sequence φ for a w</context>
</contexts>
<marker>Bhargava, Kondrak, 2010</marker>
<rawString>Aditya Bhargava and Grzegorz Kondrak. 2010. Language identification of names with SVMs. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Bisani</author>
<author>Hermann Ney</author>
</authors>
<title>Jointsequence models for grapheme-to-phoneme conversion. Speech Communication.</title>
<date>2008</date>
<contexts>
<context position="10053" citStr="Bisani and Ney, 2008" startWordPosition="1658" endWordPosition="1661">emoved, leaving a final training set of 2,795 labeled surnames and 33 different language classes. We have made this annotated word origin data publicly available for future research.4 In these experiments, we use surnames from the 12 language classes that contain at least 10 handlabeled words, and merge the remaining languages into an “Other” class. Table 1 shows the final language classes used. Unlike the training sets, we do not remove names with ambiguous or unknown origin from the test set, so our G2P system is also evaluated on the ambiguous names. 4.2 Results The Sequitur G2P algorithm (Bisani and Ney, 2008) is used for all our experiments. We use the CMU Dictionary as the gold standard, with the assumption that it contains the standard pronunciations in US English. While surnames may have multiple valid pronunciations, we make the simplifying assumption that a name has one best pronunciation. Evaluation is done on the test set of 4,585 names from the CMU Dictionary. Table 1 shows G2P accuracy for the baseline system and Methods A and B. Test data is partitioned by the most likely language of origin. We see that Method A, which uses only languagespecific G2P models, has lower overall accuracy tha</context>
</contexts>
<marker>Bisani, Ney, 2008</marker>
<rawString>Maximilian Bisani and Hermann Ney. 2008. Jointsequence models for grapheme-to-phoneme conversion. Speech Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipankar Bose</author>
<author>Sudeshna Sarkar</author>
</authors>
<title>Learning multi character alignment rules and classification of training data for transliteration.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL Named Entities Workshop.</booktitle>
<contexts>
<context position="3572" citStr="Bose and Sarkar, 2009" startWordPosition="543" endWordPosition="546">ie and ten Bosch (2001) carry out a similar lexicon augmentation, and make use of penalty weighting, with different penalties for pronunciations generated by the language-specific and language-independent G2P models. The problem of machine transliteration is closely related to grapheme-to-phoneme conversion. Many 367 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 367–371, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics transliteration systems (Khapra and Bhattacharyya, 2009; Bose and Sarkar, 2009; Bhargava and Kondrak, 2010) use word origin information. The method described by Hagiwara and Sekine (2011) is similar to our work, except that (a) we use a data set where multiple languages of origin occur naturally, rather than creating language-specific lists and merging them into a single set, and (b) we consider methods of smoothing against a languageindependent model to overcome the problems of data sparsity and errors in word origin recognition. 3 Language-Aware G2P Our methods are designed to be used with any statistical G2P system that produces the posterior probability Pr( φ|¯g) of</context>
</contexts>
<marker>Bose, Sarkar, 2009</marker>
<rawString>Dipankar Bose and Sudeshna Sarkar. 2009. Learning multi character alignment rules and classification of training data for transliteration. In Proceedings of the ACL Named Entities Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Benoit Maison</author>
</authors>
<title>Using place name data to train language identification models.</title>
<date>2003</date>
<booktitle>In Proceedings of Eurospeech.</booktitle>
<contexts>
<context position="5431" citStr="Chen and Maison (2003)" startWordPosition="860" endWordPosition="863">l is trained on words with Pr(l|w) greater than some threshold α. Here, we use α = 0.7. 3. For each word w in the test set, generate candidate transcriptions from model ml for each language with nonzero Pr(l|w). Re-estimate Pr(¯φ|¯g) by interpolating the outputs of the language-specific models. We may also use the output of the language-independent model. We elaborate on our approaches to Steps 1 and 3. 3.1 Step 1: Word origin modeling We apply a sequential conditional model to predict Pr(l|w), the probability of a language class given the word. A similar Maximum Entropy model is presented by Chen and Maison (2003), where features are the presence or absence of a given character n-gram in w. In our approach, feature functions are defined at character positions rather than over the entire word. Specifically, for word wj composed of character sequence c1 ... cm of length m (including start and end symbols), binary features test for the presence or absence of an n-gram context at each position m. A context is the presence of a character n-gram starting or ending at position m. Model features are represented as: fi(w, m,lk) = { 1, if lang(w) = lk and context 0, otherwise i is present at position m (1) Then,</context>
</contexts>
<marker>Chen, Maison, 2003</marker>
<rawString>Stanley F. Chen and Benoit Maison. 2003. Using place name data to train language identification models. In Proceedings of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Cremelie</author>
<author>Louis ten Bosch</author>
</authors>
<title>Improving the recognition of foreign names and non-native speech by combining multiple grapheme-to-phoneme converters.</title>
<date>2001</date>
<booktitle>In Proceedings of ITRW on Adaptation Methods for Speech Recognition.</booktitle>
<marker>Cremelie, Bosch, 2001</marker>
<rawString>Nick Cremelie and Louis ten Bosch. 2001. Improving the recognition of foreign names and non-native speech by combining multiple grapheme-to-phoneme converters. In Proceedings of ITRW on Adaptation Methods for Speech Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masato Hagiwara</author>
<author>Satoshi Sekine</author>
</authors>
<title>Latent class transliteration based on source language origin.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3681" citStr="Hagiwara and Sekine (2011)" startWordPosition="560" endWordPosition="563">h different penalties for pronunciations generated by the language-specific and language-independent G2P models. The problem of machine transliteration is closely related to grapheme-to-phoneme conversion. Many 367 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 367–371, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics transliteration systems (Khapra and Bhattacharyya, 2009; Bose and Sarkar, 2009; Bhargava and Kondrak, 2010) use word origin information. The method described by Hagiwara and Sekine (2011) is similar to our work, except that (a) we use a data set where multiple languages of origin occur naturally, rather than creating language-specific lists and merging them into a single set, and (b) we consider methods of smoothing against a languageindependent model to overcome the problems of data sparsity and errors in word origin recognition. 3 Language-Aware G2P Our methods are designed to be used with any statistical G2P system that produces the posterior probability Pr( φ|¯g) of a phoneme sequence φ for a word (grapheme sequence) g¯ (or a score that can be normalized to give a probabil</context>
</contexts>
<marker>Hagiwara, Sekine, 2011</marker>
<rawString>Masato Hagiwara and Satoshi Sekine. 2011. Latent class transliteration based on source language origin. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hanks</author>
</authors>
<title>Dictionary of American family names.</title>
<date>2003</date>
<publisher>University Press.</publisher>
<location>New York: Oxford</location>
<contexts>
<context position="8774" citStr="Hanks, 2003" startWordPosition="1458" endWordPosition="1459">nsider the 50,000 most frequent surnames in the 1990 census1, and extract those entries that also appear in the CMU Pronouncing Dictionary2, giving us a set of 45,841 surnames with their phoneme representations transcribed in the Arpabet symbol set. We divide this data 80/10/10 into train, test, and development sets. To build a word origin classification training set, we randomly select 3,000 surnames from the same census lists, and label by hand the most likely language of origin of each name when it occurs in the US. Labeling was done primarily using the Dictionary of American Family Names (Hanks, 2003) and Ellis Island immigration records.3 We find that, in many cases, a surname cannot be attributed to a single language but can be assigned to a set of lan1http://www.census.gov/genealogy/names/ 2http://www.speech.cs.cmu.edu/cgi-bin/ cmudict 3http://www.ellisisland.org guages related by geography and language family. For example, we discovered several surnames that could be ambiguously labeled as English, Scottish, or Irish in origin. For languages that are frequently confusable, we create a single language group to be used as a class label. Here, we use groups for British Isles, Slavic, and </context>
</contexts>
<marker>Hanks, 2003</marker>
<rawString>Patrick Hanks. 2003. Dictionary of American family names. New York: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh M Khapra</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Improving transliteration accuracy using word-origin detection and lexicon lookup.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL Named Entities Workshop.</booktitle>
<contexts>
<context position="3549" citStr="Khapra and Bhattacharyya, 2009" startWordPosition="539" endWordPosition="542">s of the origin language. Cremelie and ten Bosch (2001) carry out a similar lexicon augmentation, and make use of penalty weighting, with different penalties for pronunciations generated by the language-specific and language-independent G2P models. The problem of machine transliteration is closely related to grapheme-to-phoneme conversion. Many 367 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 367–371, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics transliteration systems (Khapra and Bhattacharyya, 2009; Bose and Sarkar, 2009; Bhargava and Kondrak, 2010) use word origin information. The method described by Hagiwara and Sekine (2011) is similar to our work, except that (a) we use a data set where multiple languages of origin occur naturally, rather than creating language-specific lists and merging them into a single set, and (b) we consider methods of smoothing against a languageindependent model to overcome the problems of data sparsity and errors in word origin recognition. 3 Language-Aware G2P Our methods are designed to be used with any statistical G2P system that produces the posterior p</context>
</contexts>
<marker>Khapra, Bhattacharyya, 2009</marker>
<rawString>Mitesh M. Khapra and Pushpak Bhattacharyya. 2009. Improving transliteration accuracy using word-origin detection and lexicon lookup. In Proceedings of the ACL Named Entities Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadna Font Llitj´os</author>
</authors>
<title>Improving pronunciation accuracy of proper names with language origin classes. Master’s thesis,</title>
<date>2001</date>
<institution>Carnegie Mellon University.</institution>
<marker>Llitj´os, 2001</marker>
<rawString>Ariadna Font Llitj´os. 2001. Improving pronunciation accuracy of proper names with language origin classes. Master’s thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Maison</author>
<author>Stanley F Chen</author>
<author>Paul S Cohen</author>
</authors>
<title>Pronunciation modeling for names of foreign origin.</title>
<date>2003</date>
<booktitle>In Proceedings of ASRU.</booktitle>
<contexts>
<context position="2660" citStr="Maison et al. (2003)" startWordPosition="418" endWordPosition="421">unciation modeling for speech synthesis. Here, a CART decision tree system is presented for G2P conversion that maps letters to phonemes using local context. Experiments use a data set of US surnames that naturally draws from a diverse set of origin languages, and show that the inclusion of word origin features in the model improves pronunciation accuracy. We use similar data, as described in §4.1. Some works on lexical modeling for speech recognition also make use of word origin. Here, the focus is on expanding the vocabulary of an ASR system rather than choosing a single best pronunciation. Maison et al. (2003) train language-specific G2P models for eight languages and output pronunciations to augment a baseline lexicon. This augmented lexicon outperforms a handcrafted lexicon in ASR experiments; error reduction is highest for foreign names spoken by native speakers of the origin language. Cremelie and ten Bosch (2001) carry out a similar lexicon augmentation, and make use of penalty weighting, with different penalties for pronunciations generated by the language-specific and language-independent G2P models. The problem of machine transliteration is closely related to grapheme-to-phoneme conversion.</context>
</contexts>
<marker>Maison, Chen, Cohen, 2003</marker>
<rawString>Benoit Maison, Stanley F. Chen, and Paul S. Cohen. 2003. Pronunciation modeling for names of foreign origin. In Proceedings of ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonjia Waxmonsky</author>
</authors>
<title>Natural language processing for named entities with word-internal information.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Chicago.</institution>
<contexts>
<context position="6690" citStr="Waxmonsky (2011)" startWordPosition="1093" endWordPosition="1094">ifi(cm, lk)(2) where Z = Ej exp Em Ei λi fi(cm, lk) is a normalization factor. In practice, we can implement this model as a CRF, where a language label is applied at each character position rather than for the word. While all the language labels in a sequence need not be the same, we find only a handful of words where a transition occurs from one language label to another within a word. For these cases, we take the label of the last character in the word as the language of origin. Experiments comparing this sequential Maximum Entropy method with other word origin classifiers are described by Waxmonsky (2011). 3.2 Step 3: Re-weighting of G2P output We test two methods of re-weighting Pr(¯φ|¯g) using the word origin estimation and the output of language-specific G2P models. Method A uses only language-specific models: ˜Pr( ¯φ|¯g) = � Pr(φ|¯g, l) Pr(l|g) (3) lEL where Pr(φ|¯g, l) is estimated by model ml. Method B With the previous method, names from infrequent classes suffer from data sparsity. We therefore smooth with the output PI of the baseline language-independent model. ˜Pr(¯φ|¯g) = σ Pr( � Pr(φ|¯g, l) Pr(l|g) I ¯φ|¯g)+(1−σ) (4) lEL The factor σ is tuned on a development set. 368 Language Tra</context>
</contexts>
<marker>Waxmonsky, 2011</marker>
<rawString>Sonjia Waxmonsky. 2011. Natural language processing for named entities with word-internal information. Ph.D. thesis, University of Chicago.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>