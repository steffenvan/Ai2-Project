<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011456">
<title confidence="0.994697">
XML-Based Data Preparation for Robust Deep Parsing
</title>
<author confidence="0.993235">
Claire Grover and Alex Lascarides
</author>
<affiliation confidence="0.998718">
Division of Informatics
The University of Edinburgh
</affiliation>
<address confidence="0.8395295">
2 Buccleuch Place
Edinburgh EH8 9LW, UK
</address>
<email confidence="0.997793">
C.Grover, A.Lascarides @ed.ac.uk
</email>
<sectionHeader confidence="0.995614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964">
We describe the use of XML tokenisa-
tion, tagging and mark-up tools to pre-
pare a corpus for parsing. Our tech-
niques are generally applicable but here
we focus on parsing Medline abstracts
with the ANLT wide-coverage grammar.
Hand-crafted grammars inevitably lack
coverage but many coverage failures
are due to inadequacies of their lexi-
cons. We describe a method of gain-
ing a degree of robustness by interfac-
ing POS tag information with the exist-
ing lexicon. We also show that XML
tools provide a sophisticated approach
to pre-processing, helping to ameliorate
the ‘messiness’ in real language data
and improve parse performance.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999561369565217">
The field of parsing technology currently has two
distinct strands of research with few points of
contact between them. On the one hand, there
is thriving research on shallow parsing, chunk-
ing and induction of statistical syntactic analysers
from treebanks; and on the other hand, there are
systems which use hand-crafted grammars which
provide both syntactic and semantic coverage.
‘Shallow’ approaches have good coverage on cor-
pus data, but extensions to semantic analysis are
still in a relative infancy. The ‘deep’ strand of
research has two main problems: inadequate cov-
erage, and a lack of reliable techniques to select
the correct parse. In this paper we describe on-
going research which uses hybrid technologies to
address the problem of inadequate coverage of a
‘deep’ parsing system. In Section 2 we describe
how we have modified an existing hand-crafted
grammar’s look-up procedure to utilise part-of-
speech (POS) tag information, thereby ameliorat-
ing the lexical information shortfall. In Section 3
we describe how we combine a variety of existing
NLP tools to pre-process real data up to the point
where a hand-crafted grammar can start to be use-
ful. The work described in both sections is en-
abled by the use of an XML processing paradigm
whereby the corpus is converted to XML with
analysis results encoded as XML annotations. In
Section 4 we report on an experiment with a ran-
dom sample of 200 sentences which gives an ap-
proximate measure of the increase in performance
we have gained.
The work we describe here is part of a project
which aims to combine statistical and symbolic
processing techniques to compute lexical seman-
tic relationships, e.g. the semantic relations be-
tween nouns in complex nominals. We have cho-
sen the medical domain because the field of med-
ical informatics provides a relative abundance
of pre-existing knowledge bases and ontologies.
Our efforts so far have focused on the OHSUMED
corpus (Hersh et al., 1994) which is a collection
of Medline abstracts of medical journal papers.1
While the focus of the project is on seman-
tic issues, a prerequisite is a large, reliably an-
notated corpus and a level of syntactic process-
</bodyText>
<footnote confidence="0.793583">
1Sager et al. (1994) describe the Linguistic String
Project’s approach to parsing medical texts.
</footnote>
<bodyText confidence="0.999973222222222">
ing that supports the computation of semantics.
The computation of ‘grammatical relations’ from
shallow parsers or chunkers is still at an early
stage (Buchholz et al., 1999, Carroll et al., 1998)
and there are few other robust semantic pro-
cessors, and none in the medical domain. We
have therefore chosen to re-use an existing hand-
crafted grammar which produces compositionally
derived underspecified logical forms, namely the
wide-coverage grammar, morphological analyser
and lexicon provided by the Alvey Natural Lan-
guage Tools (ANLT) system (Carroll et al. 1991,
Grover et al. 1993). Our immediate aim is to
increase coverage up to a reasonable level and
thereafter to experiment with ranking the parses,
e.g. using Briscoe and Carroll’s (1993) proba-
bilistic extension of the ANLT software.
We use XML as the preprocessing mark-up
technology, specifically the LT TTT and LT XML
tools (Grover et al., 2000; Thompson et al., 1997).
In the initial stages of the project we converted
the OHSUMED corpus into XML annotated format
with mark-up that encodes word tokens, POS tags,
lemmatisation information etc. The research re-
ported here builds on that mark-up in a further
stage of pre-processing prior to parsing. The XML
paradigm has proved invaluable throughout.
</bodyText>
<sectionHeader confidence="0.9036" genericHeader="method">
2 Improving the Lexical Component
</sectionHeader>
<subsectionHeader confidence="0.930527">
2.1 Strategy
</subsectionHeader>
<bodyText confidence="0.999939009174312">
The ANLT grammar is a unification grammar
based on the GPSG formalism (Gazdar et al.,
1985), which is a precursor of more recent ‘lex-
icalist’ grammar formalisms such as HPSG (Pol-
lard and Sag, 1994). In these frameworks lexical
entries carry a significant amount of information
including subcategorisation information. Thus
the practical parse success of a grammar is sig-
nificantly dependent on the quality of the lexicon.
The ANLT grammar is distributed with a large
lexicon which was derived semi-automatically
from a machine-readable dictionary (Carroll and
Grover, 1988). This lexicon is of varying quality:
function words such as complementizers, prepo-
sitions, determiners and quantifiers are all reli-
ably hand-coded but content words are less reli-
able. Verbs are generally coded to a high stan-
dard but the noun and adjective lexicons are full
of redundancies and duplications. Since these du-
plications can lead to huge increases in the num-
ber of spurious parses, an obvious first step was
to remove all duplications from the existing lex-
icons and to collapse certain ambiguities such as
the count/mass distinction into single underspeci-
fied entries. A second critical step was to increase
the character set that the spelling rules in the mor-
phological analyser handle, so as to accept capi-
talised and non-alphabetic characters in the input.
Once these ANLT-internal problems are over-
come, the main problem of inadequate lexi-
cal coverage still remains: if we try to parse
OHSUMED sentences using the ANLT lexicon and
no other resources, we achieve very poor results
because most of the medical domain words are
simply not in the lexicon and there is no ‘robust-
ness’ strategy built into ANLT. One solution to
this problem would be to find domain specific lex-
ical resources from elsewhere and to merge the
new resources with the existing lexicon. How-
ever, the resulting merged lexicon may still not
have sufficient coverage and a means of achieving
robustness in the face of unknown words would
still be required. Furthermore, every move to a
new domain would depend on domain-specific
lexical resources being available. Because of
these disadvantages, we have pursued an alter-
native solution which allows parsing to proceed
without the need for extra lexical resources and
with robustness built into the strategy. This alter-
native strategy does not preclude the use of do-
main specific lexical resources but it does pro-
vide a basic level of performance which further
resources can be used to improve upon.
The strategy we have adopted relies first on
sophisticated XML-based tokenisation (see Sec-
tion 3) and second on the combination of POS
tag information with the existing ANLT lexical re-
sources. Our view is that POS tag information for
content words (nouns, verbs, adjectives, adverbs)
is usually reliable and informative, while tag-
ging of function words (complementizers, deter-
miners, particles, conjunctions, auxiliaries, pro-
nouns, etc.) can be erratic and provides less in-
formation than the hand-written entries for func-
tion words that are typically developed side-by-
side with wide coverage grammars. Furthermore,
unknown words are far more likely to be con-
tent words than function words, so knowledge of
the POS tag will most often be needed for con-
tent words. Our idea, then, is to tag the input but
to retain only the content word POS tags and use
them during lexical look-up in one of two ways.
If the word exists in the lexicon then the POS tag
is used to access only those entries of the same
basic category. If, on the other hand, the word is
not in the lexicon then a basic underspecified en-
try for the POS tag is used as the lexical entry for
the word. In the first case, the POS tag is used as
a filter, accessing only entries of the appropriate
category and cutting down on the parser’s search
space. In the second case, the basic category of
the unknown word is supplied and this enables
parsing to proceed. For example, if the following
partially tagged sentence is input to the parser, it
is successfully parsed.2
We have developed VBN a variable JJ
suction NN system NN for irrigation NN ,
aspiration NN and vitrectomy NN
Without the tags there would be no parse since
the words irrigation and vitrectomy are not in the
ANLT lexicon. Furthermore, tagging variable as
an adjective ensures that the noun entry for vari-
able is not accessed, thus cutting down on parse
numbers (3 versus 6 in this case).
The two cases interact where a lexical entry is
present in the ANLT lexicon but not with the rele-
vant category. For example, monitoring is present
in the ANLT lexicon as a verb but not as a noun:
We studied VBD the value NN of
transcutaneous JJ carbon NN dioxide NN
monitoring NN during transport NN
Look up of the word tag pair monitoring NN
fails and the basic entry for the tag NN is used in-
stead. Without the tag, the verb entry for monitor-
ing would be accessed and the parse would fail.
In the following example the adjectives dimin-
ished and stabilized exist only as verb entries:
with the JJ tag the parse succeeds but without it,
the verb entries are accessed and the parse fails.
</bodyText>
<footnote confidence="0.874871166666667">
There was radiographic JJ evidence NN of
diminished JJ or stabilized JJ pleural JJ
effusion NN
2The LT TTT tagger uses the Penn Treebank tagset (Mar-
cus et al., 1994): JJ labels adjectives, NN labels nouns and
VB labels verbs.
</footnote>
<bodyText confidence="0.9999358">
Note that cases such as these would be problem-
atic for a strategy where tagging was used only
when lexical look-up failed, since here lexical
look-up doesn’t fail, it just provides an incom-
plete set of entries. It is of course possible to aug-
ment the grammar and/or lexicon with rules to in-
fer noun entries from verb+ing entries and adjec-
tive entries from verb+ed entries. However, this
will increase lexical ambiguity quite considerably
and lead to higher numbers of spurious parses.
</bodyText>
<subsectionHeader confidence="0.996845">
2.2 Implementation
</subsectionHeader>
<bodyText confidence="0.9999763125">
We expect the technique outlined above to be ap-
plicable across a range of parsing systems. In this
section we describe how we have implemented it
within ANLT.
The version of the ANLT system described
in Carroll et al. (1991) and Grover et al. (1993)
does not allow tagged input but work by Briscoe
and Carroll (1993) on statistical parsing uses an
adapted version of the system which is able to
process tagged input, ignoring the words in order
to parse sequences of tags. We use this version of
the system, running in a mode where ‘words’ are
looked up according to three distinct cases:
word look-up: the word has no tag and must
be looked up in the lexicon (and if look-up
fails, the parse fails)
tag look-up: the word has a tag, look-up of
the word tag pair fails, but the tag has a spe-
cial hand-written entry which is used instead
word tag look-up: the word has a tag and
look-up of the word tag pair succeeds.
The resources provided by the system already ad-
equately deal with the first two cases but the third
case had to be implemented. The existing mor-
phological analysis software was relatively easily
adapted to give the performance we required. The
ANLT morphological analyser performs regular
inflectional morphology using a unification gram-
mar for combining morphemes and rules govern-
ing spelling changes when morphemes are con-
catenated. Thus a plural noun such as patients is
composed of the morphemes patient and +s with
the features on the top node being inherited par-
tially from the noun and partially from the inflec-
tional affix:
In dealing with word tag pairs, we have used
the word grammar to treat the tag as a novel kind
of affix which constrains the category of the lex-
ical entry it attaches to. We have defined mor-
pheme entries for content word tags so they can
be used by special word grammar rules and at-
tached to words of the appropriate category. Thus
patient NN is analysed using the noun entry
for patient but not the adjective entry. Tag mor-
phemes can be attached to inflected as well as to
base forms, so the string patients NNS has the
following internal structure:
In defining the rules for word tag pairs, we
were careful to ensure that the resulting category
would have exactly the same feature specification
as the word itself. Thus the tag morpheme is spec-
ified only for basic category features which the
word grammar requires to be shared by word and
tag. All other feature specifications on the cov-
ering node are inherited from the word, not the
tag. This method of combining POS tag infor-
mation with lexical entries preserves all informa-
tion in the lexical entries, including inflectional
and subcategorisation information. The preserva-
tion of subcategorisation information is particu-
larly necessary since the ANLT lexicon makes so-
phisticated distinctions between different subcat-
egorisation frames which are critical for obtaining
the correct parse and associated logical form.
</bodyText>
<sectionHeader confidence="0.988084" genericHeader="method">
3 XML Tools for Pre-Processing
</sectionHeader>
<bodyText confidence="0.999994279069767">
The techniques described in this section, and
those in the previous section, are made possi-
ble by our use of an XML processing paradigm
throughout. We use the LT TTT and LT XML tools
in pipelines where they add, modify or remove
pieces of XML mark-up. Different combinations
of the tools can be used for different processing
tasks. Some of the XML programs are rule-based
while others use maximum entropy modelling.
We have developed a pipeline which converts
OHSUMED data into XML format and adds lin-
guistic annotations. The early stages of the
pipeline segment character strings first into words
and then into sentences while subsequent stages
perform POS tagging and lemmatisation. A sam-
ple part of the output of this basic pipeline is
shown in Figure 1. The initial conversion to XML
and the identification of words is achieved us-
ing the core LT TTT program fsgmatch, a gen-
eral purpose transducer which processes an in-
put stream and rewrites it using rules provided
in a grammar file. The identification of sentence
boundaries, mark-up of sentence elements and
POS tagging is done by the statistical program lt-
pos (Mikheev, 1997). Words are marked up as
W elements with further information encoded as
values of attributes on the W elements. In the ex-
ample, the P attribute’s value is a POS tag and
the LM attribute’s is a lemma (only on nouns and
verbs). The lemmatisation is performed by Min-
nen et al.’s (2000) morpha program which is not
an XML processor. In such cases we pass data out
of the pipeline in the format required by the tool
and merge its output back into the XML mark-up.
Typically we use McKelvie’s (1999) xmlperl pro-
gram to convert out of and back into XML: for
ANLT this involves putting each sentence on one
line, converting some W elements into word tag
pairs and stripping out all other XML mark-up to
provide input to the parser in the form it requires.
We are currently experimenting with bringing the
labelled bracketing of the parse result back into
the XML as ‘stand-off’ mark up.
</bodyText>
<subsectionHeader confidence="0.972873">
3.1 Pre-Processing for Parsing
</subsectionHeader>
<bodyText confidence="0.912949">
In Section 2 we showed how POS tag mark-
up could be used to add to existing lexical re-
sources. In this section we demonstrate how the
</bodyText>
<table confidence="0.86929803125">
patient +s
N , V , PLU
N , V , PLU
PLU , STEMPLU
N , V , PLU
N , V , PLU
N , V
NNS
patient +s
N , V , PLU
PLU , STEMPLU
RECORD
ID 395 /ID
MEDLINE-ID 87052477 /MEDLINE-ID
SOURCE Clin Pediatr (Phila) 8703; 25(12):617-9 /SOURCE
MESH
Adolescence; Alcoholic Intoxication/BL/*EP; Blood Glucose/AN; Canada; Child; Child, Preschool; Electrolytes/BL; Female;
Human; Hypoglycemia/ET; Infant; Male; Retrospective Studies.
/MESH
TITLE Ethyl alcohol ingestion in children. A 15-year review. /TITLE
PTYPE JOURNAL ARTICLE. /PTYPE
ABSTRACT
SENTENCE W P=’DT’ A /W W P=’JJ’ retrospective /W
W P=’NN’ LM=’study’ study /W W P=’VBD’ LM=’be’ was /W
W P=’VBN’ LM=’conduct’ conducted /W W P=’IN’ by /W W P=’NN’ LM=’chart’ chart /W
W P=’NNS’ LM=’review’ reviews /W W P=’IN’ of✍/W W P=’CD’ 27 /W
W P=’NNS’ LM=’patient’ patients /W W P=’IN’ with /W W P=’JJ’ documented /W W P=’NN’
LM=’ethanol’ ethanol /W W P=’NN’ LM=’ingestion’ ingestion /W W P=’.’ . /W
/SENTENCE SENTENCE /SENTENCE SENTENCE /SENTENCE
/ABSTRACT
AUTHOR Leung AK. /AUTHOR
/RECORD
</table>
<figureCaption confidence="0.999676">
Figure 1: A sample from the XML-marked-up OHSUMED corpus
</figureCaption>
<bodyText confidence="0.9841195">
XML approach allows for flexibility in the way
data is converted from marked-up corpus mate-
rial to parser input. This method enables ‘messy’
linguistic data to be rendered innocuous prior to
parsing, thereby avoiding the need to make hand-
written low-level additions to the grammar itself.
</bodyText>
<subsectionHeader confidence="0.7916">
3.1.1 Changing POS tag labels
</subsectionHeader>
<bodyText confidence="0.999998304347826">
One of the failings of the ANLT lexicon is in the
subcategorisation of nouns: each noun has a zero
subcategorisation entry but many nouns which
optionally subcategorise a complement lack the
appropriate entry. For example, the nouns use
and management do not have entries with an of-PP
subcategorisation frame so that in contexts where
an of-PP is present, the correct parse will not be
found. The case of of-PPs is a special one since
we can assume that whenever of follows a noun it
marks that noun’s complement. We can encode
this assumption in the layer of processing that
converts the XML mark-up to the format required
by the parser: an fsgmatch rule changes the value
of the P attribute of a noun from NN to NNOF or
from NNS to NNSOF whenever it is followed by
of. By not adding morpheme entries for NNOF
and NNSOF we ensure that word tag look-up will
fail and the system will fall back on tag look-up
using special entries for NNOF and NNSOF which
have only an of-PP subcategorisation frame. In
this way the parser will be forced to attach of-PPs
following nouns as their complements.
</bodyText>
<subsubsectionHeader confidence="0.48">
3.1.2 Numbers, formulae, etc.
</subsubsectionHeader>
<bodyText confidence="0.999900916666666">
Although we have stated that we only retain
content word tags, in practice we also retain cer-
tain other tags for which we provide no mor-
pheme entry in the morphological system so as
to achieve tag rather than word tag look-up. For
example, we retain the CD tag assigned to numer-
als and provide a general purpose entry for it so
that sentences containing numerals can be parsed
without needing lexical entries for them. We also
use a pre-existing tokenisation component which
recognises spelled out numbers to which the CD
tag is also assigned:
</bodyText>
<equation confidence="0.860054333333333">
W P=’CD’ thirty-five /W thirty-five CD
W P=’CD’ Twenty one /W Twenty one CD
W P=’CD’ 176 /W 176 CD
</equation>
<bodyText confidence="0.999939166666667">
The program fsgmatch can be used to group
words together into larger units using handwritten
rules and small lexicons of ‘multi-word’ words.
For the purposes of parsing, these larger units can
be treated as words, so the grammar does not need
to contain special rules for ‘multi-word’ words:
</bodyText>
<equation confidence="0.868578666666667">
W P=’IN’ In order to /W In order to IN
W P=’IN’ in relation to /W in relation to IN
W P=’JJ’ in vitro /W in vitro JJ
</equation>
<bodyText confidence="0.999524142857143">
The same technique can be used to pack-
age up a wide variety of formulaic expressions
which would cause severe problems to most hand-
crafted grammars. Thus all of the following
‘words’ have been identified using fsgmatch rules
and can be passed to the parser as unanalysable
chunks.3 The classification of the examples be-
low as nouns reflects a working hypothesis that
they can slot into the correct parse as noun phrases
but there is room for experimentation since the
conversion to parser input format can rewrite the
tag in any way. It may turn out that they should
be given a more general tag which corresponds to
several major category types.
</bodyText>
<equation confidence="0.8084054">
W P=’NN’ P less than 0.001 /W
W P=’NN’ 166 +/- 77 mg/dl /W
W P=’NN’ 2 to 5 cc/day /W
W P=’NN’ 9.1 v. 5.1 ml /W
W P=’NN’ 2.5 mg i.v. /W
</equation>
<bodyText confidence="0.999973461538461">
It is important to note that our method of divid-
ing the labour between pre-processing and pars-
ing allows for experimentation to get the best pos-
sible balance. We are still developing our for-
mula recognition subcomponent which has so far
been entirely hand-coded using fsgmatch rules.
We believe that it is more appropriate to do this
hand-coding at the pre-processing stage rather
than with the relatively unwieldy formalism of
the ANLT grammar. Moreover, use of the XML
paradigm might allow us to build a component
that can induce rules for regular formulaic expres-
sions thus reducing the need for hand-coding.
</bodyText>
<subsectionHeader confidence="0.994665">
3.1.3 Dealing with tagger errors
</subsectionHeader>
<bodyText confidence="0.9999595">
The tagger we use, ltpos, has a reported per-
formance comparable to other state-of-the-art tag-
gers. However, all taggers make errors, especially
when used on data different from their training
data. With the strategy outlined in this paper,
where we only retain a subset of tags, many tag-
ging errors will be harmless. However, con-
tent word tagging errors will be detrimental since
the basic noun/verb/adjective/adverb distinction
drives lexical look-up and only entries of the same
category as the tag will be accessed. If we find
that the tagger consistently makes the same er-
ror in a particular context, for example mistag-
ging +ing nominalisations as verbs (VBG), then
</bodyText>
<footnote confidence="0.787444">
3Futrelle et al. (1991) discuss tokenisation issues in bio-
logical texts.
</footnote>
<bodyText confidence="0.999429555555556">
we can use fsgmatch rules to replace the tag in just
those contexts. The new tag can be given a defi-
nition which is ambiguous between NN and VBG,
thereby ensuring that a parse can be achieved.
A second strategy that we are exploring in-
volves using more than one tagger. Our cur-
rent pipeline includes a call to Elworthy’s (1994)
CLAWS2 tagger. We encode the tags from this
tagger as values of the attribute C2 on words:
</bodyText>
<equation confidence="0.958334">
W P=’NNS’ C2=’NN2’ LM=’case’ cases /W
W P=’VBN’ C2=’VVN’ LM=’find’ found /W
</equation>
<bodyText confidence="0.99952008">
Many mistaggings can be found by searching
for words where the two taggers disagree and they
can be corrected in the mapping from XML for-
mat to parser input by assigning a new tag which
is ambiguous between the two possibilities. For
example, ltpos incorrectly tags the word bound in
the following example as a noun but the CLAWS2
tagger correctly categorises it as a verb.
a large JJ body NNOF of hemoglobin NN
bound NNVVN to the ghost NN membrane NN
We use xmlperl rules to map from XML to ANLT
input and reassign these cases to the ‘compos-
ite’ tag NNVVN, which is given both a noun
and a verb entry. This allows the correct parse
to be found whichever tagger is correct. An
alternative approach to the mistagging problem
would be to use just one tagger which returns
multiple tags and to use the relative probabil-
ity of the tags to determine cases where a com-
posite tag could be created in the mapping to
parser input. Charniak et al. (forthcoming) reject
a multiple tag approach when using a probabilis-
tic context-free-grammar parser, but it is unclear
whether their result is relevant to a hand-crafted
grammar.
</bodyText>
<subsectionHeader confidence="0.999856">
3.2 An XML corpus
</subsectionHeader>
<bodyText confidence="0.999987695652174">
There are numerous advantages to working with
XML tools. One general advantage is that we can
add linguistic annotations in an entirely automatic
and incremental fashion, so as to produce a heav-
ily annotated corpus which may well prove useful
to a number of researchers for a number of lin-
guistic activities. In the work described here we
have not used any domain specific information.
However, it would clearly be possible to add do-
main specific information as further annotations
using such resources as UMLS (UMLS, 2000). In-
deed, we have begun to utilise UMLS and hope to
improve the accuracy of the existing mark-up by
incorporating lexical and semantic information.
Since the annotations we describe are computed
entirely automatically, it would be a simple mat-
ter to use our system to mark up new Medline data
to increase the size of our corpus considerably.
A heavily annoted corpus quickly becomes un-
readable but if it is an XML annotated corpus then
there are several tools to help visualise the data.
For example, we use xmlperl to convert from XML
to HTML to view the corpus in a browser.
</bodyText>
<sectionHeader confidence="0.989359" genericHeader="evaluation">
4 Evaluation and Future Research
</sectionHeader>
<bodyText confidence="0.997272698924731">
With a corpus such as OHSUMED where there
is no gold-standard tagged or hand-parsed sub-
part, it is hard to reliably evaluate our system.
However, we did an experiment on 200 sentences
taken at random from the corpus (average sen-
tence length: 21 words). We ran three versions of
our pre-processor over the 200 sentences to pro-
duce three different input files for the parser and
for each input we counted the sentences which
were assigned at least one parse. All three ver-
sions started from the same basic XML annotated
data, where words were tagged by both taggers
and parenthesised material was removed. Ver-
sion 1 converted from this format to ANLT input
simply by discarding the mark-up and separating
off punctuation. Version 2 was the same except
that content word POS tags were retained. Ver-
sion 3 was put through our full pipeline which
recognises formulae, numbers etc. and which cor-
rects some tagging errors. The following table
shows numbers of sentences successfully parsed
with each of the three different inputs:
Version 1 Version 2 Version 3
Parses 4 (2%) 32 (16%) 79 (39.5%)
The extremely low success rate of Version 1 is a
reflection of the fact that the ANLT lexicon does
not contain any specialist lexical items. In fact, of
the 200 sentences, 188 contained words that were
not in the lexicon, and of the 12 that remained, 4
were successfully parsed. The figure for Version 2
gives a crude measure of the contribution of our
use of tags in lexical look-up and the figure for
Version 3 shows further gains when further pre-
processing techniques are used.
Although we have achieved an encouraging
overall improvement in performance, the total of
39.5% for Version 3 is not a precise reflection of
accuracy of the parser. In order to determine ac-
curacy, we hand-examined the parser output for
the 79 sentences that were parsed and recorded
whether or not the correct parse was among the
parses found. Of these 79 sentences, 61 (77.2%)
were parsed correctly while 18 (22.8%) were not,
giving a total accuracy measure of 30.5% for Ver-
sion 3. While this figure is rather low for a practi-
cal application, it is worth reiterating that this still
means that nearly one in three sentences are not
only correctly parsed but they are also assigned
a logical form. We are confident that the further
work outlined below will achieve an improvement
in performance which will lead to a useful seman-
tic analysis of a significant proportion of the cor-
pus. Furthermore, in the case of the 18 sentences
which were parsed incorrectly, it is important to
note that the ‘wrong’ parses may sometimes be
capable of yielding useful semantic information.
For example, the grammar’s compounding rules
do not yet include the possibility of coordinations
within compounds so that the NP the MS and di-
rect blood pressure methods can only be wrongly
parsed as a coordination of two NPs. However,
the rest of the sentence in which the NP occurs is
correctly parsed.
An analysis of the 18 sentences which were
parsed incorrectly reveals that the reasons for fail-
ure are distributed evenly across three causes: a
word was mistagged and not corrected during pre-
processing (6); the segmentation into tokens was
inadequate (5); and the grammar lacked coverage
(7). A casual inspection of a random sample of
10 of the sentences which failed to parse at all re-
veals a similar pattern although for several there
were multiple reasons for failure. Lack of gram-
matical coverage was more in evidence, perhaps
not surprisingly since work on tuning the gram-
mar to the domain has not yet been done.
Although we are only able to parse between
30 and 40 percent of the corpus, we will be able
to improve on that figure quite considerably in
the future through continued development of the
pre-processing component. Moreover, we have
not yet incorporated any domain specific lexical
knowledge from, e.g., UMLS but we would expect
this to contribute to improved performance. Fur-
thermore, our current level of success has been
achieved without significant changes to the origi-
nal grammar and, once we start to tailor the gram-
mar to the domain, we will gain further significant
increases in performance. As a final stage, we
may find it useful to follow Kasper et al. (1999)
and have a ‘fallback’ strategy for failed parses
where the best partial analyses are assembled in
a robust processing phase.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999939159090909">
T. Briscoe and J. Carroll. 1993. Generalised prob-
abilistic LR parsing of natural language (corpora)
with unification grammars. Computational Lin-
guistics, 19(1):25–60.
S. Buchholz, J. Veenstra, and W. Daelemans. 1999.
Cascaded grammatical relation assignment. In
EMNLP ’99, pp 239–246, Maryland.
J. Carroll and C. Grover. 1988. The derivation
of a large computational lexicon of English from
LDOCE. In B. Boguraev and E. J. Briscoe, editors,
Computational Lexicographyfor Natural Language
Processing. Longman, London.
J. Carroll, T. Briscoe, and C. Grover. 1991. A de-
velopment environment for large natural language
grammars. Technical Report 233, Computer Labo-
ratory, University of Cambridge.
J. Carroll, T. Briscoe, and G. Minnen. 1998. Can sub-
categorisation probabilities help a statistical parser?
In Proceedings of the 6th ACL/SIGDAT Workshop
on Very Large Corpora, pp 118–126, Montreal.
ACL/SIGDAT.
E. Charniak, G. Carroll, J. Adcock, A. Cassandra,
Y. Gotoh, J. Katz, M. Littman, and J. McCann.
forthcoming. Taggers for parsers. Artificial Intel-
ligence.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Proceedings of the 4th ACL con-
ference on Applied Natural Language Processing,
pp 53–58, Stuttgart, Germany.
R. Futrelle, C. Dunn, D. Ellis, and M. Pescitelli. 1991.
Preprocessing and lexicon design for parsing tech-
nical text. In 2nd International Workshop on Pars-
ing Technologies (IWPT-91), pp 31–40, Morris-
town, New Jersey.
G. Gazdar, E. Klein, G. Pullum, and I. Sag. 1985.
Generalized Phrase Structure Grammar. Basil
Blackwell, London.
C. Grover, J. Carroll, and T. Briscoe. 1993. The
Alvey Natural Language Tools grammar (4th re-
lease). Technical Report 284, Computer Labora-
tory, University of Cambridge.
C. Grover, C. Matheson, A. Mikheev, and M. Moens.
2000. LT TTT—a flexible tokenisation tool. In
LREC 2000—Proceedings of the Second Interna-
tional Conference on Language Resources and
Evaluation, Athens, pp 1147–1154.
W. Hersh, C. Buckley, TJ Leone, and D. Hickam.
1994. OHSUMED: an interactive retrieval evalu-
ation and new large test collection for research. In
W. Bruce Croft and C. J. van Rijsbergen, editors,
Proceedings of the 17th Annual International Con-
ference on Research and Development in Informa-
tion Retrieval, pp 192–201, Dublin, Ireland.
W. Kasper, B. Kiefer, H.-U. Krieger, C.J. Rupp, and
K. Worm. 1999. Charting the depths of robust
speech parsing. In Proceedings of the 37th Annual
Meeting of the Association for Computational Lin-
guistics, pp 405–412, Maryland.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn treebank: annotating predicate ar-
gument structure. In ARPA Human Language Tech-
nologies Workshop.
D. McKelvie. 1999. XMLPERL 1.0.4. XML process-
ing software. http://www.cogsci.ed.ac.
uk/˜dmck/xmlperl.
A. Mikheev. 1997. Automatic rule induction for un-
known word guessing. Computational Linguistics,
23(3):405–423.
G. Minnen, J. Carroll, and D. Pearce. 2000. Robust,
applied morphological generation. In Proceedings
of 1st International Natural Language Conference
(INLG ’2000), Mitzpe Ramon, Israel.
C. Pollard and I. Sag. 1994. Head-Driven Phrase
Structure Grammar. CSLI and University of
Chicago Press, Stanford, Ca. and Chicago, Ill.
N. Sager, M. Lyman, C. Bucknall, N. Nhan, and
L. J. Tick. 1994. Natural language processing
and the representation of clinical data. Journal
of the American Medical Informatics Association,
1(2):142–160.
H. Thompson, R. Tobin, D. McKelvie, and C. Brew.
1997. LT XML. Software API and toolkit for
XML processing. http://www.ltg.ed.ac.
uk/software/.
UMLS. 2000. Unified Medical Language System
(UMLS) Knowledge Sources. National Library of
Medicine, Bethesda (MD), 11th edition.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.743326">
<title confidence="0.999186">XML-Based Data Preparation for Robust Deep Parsing</title>
<author confidence="0.972747">Grover Lascarides</author>
<affiliation confidence="0.938588666666667">Division of Informatics The University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.996062">Edinburgh EH8 9LW, UK</address>
<email confidence="0.948817">C.Grover,A.Lascarides@ed.ac.uk</email>
<abstract confidence="0.999385944444444">describe the use of tokenisation, tagging and mark-up tools to prepare a corpus for parsing. Our techniques are generally applicable but here we focus on parsing Medline abstracts the grammar. Hand-crafted grammars inevitably lack coverage but many coverage failures are due to inadequacies of their lexicons. We describe a method of gaining a degree of robustness by interfacinformation with the existlexicon. We also show that tools provide a sophisticated approach to pre-processing, helping to ameliorate the ‘messiness’ in real language data and improve parse performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalised probabilistic LR parsing of natural language (corpora) with unification grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="10589" citStr="Briscoe and Carroll (1993)" startWordPosition="1760" endWordPosition="1763"> set of entries. It is of course possible to augment the grammar and/or lexicon with rules to infer noun entries from verb+ing entries and adjective entries from verb+ed entries. However, this will increase lexical ambiguity quite considerably and lead to higher numbers of spurious parses. 2.2 Implementation We expect the technique outlined above to be applicable across a range of parsing systems. In this section we describe how we have implemented it within ANLT. The version of the ANLT system described in Carroll et al. (1991) and Grover et al. (1993) does not allow tagged input but work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. We use this version of the system, running in a mode where ‘words’ are looked up according to three distinct cases: word look-up: the word has no tag and must be looked up in the lexicon (and if look-up fails, the parse fails) tag look-up: the word has a tag, look-up of the word tag pair fails, but the tag has a special hand-written entry which is used instead word tag look-up: the word has a tag and look-up of the word tag pair succeeds. </context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>T. Briscoe and J. Carroll. 1993. Generalised probabilistic LR parsing of natural language (corpora) with unification grammars. Computational Linguistics, 19(1):25–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>J Veenstra</author>
<author>W Daelemans</author>
</authors>
<title>Cascaded grammatical relation assignment.</title>
<date>1999</date>
<booktitle>In EMNLP ’99,</booktitle>
<pages>239--246</pages>
<location>Maryland.</location>
<contexts>
<context position="3292" citStr="Buchholz et al., 1999" startWordPosition="529" endWordPosition="532">elative abundance of pre-existing knowledge bases and ontologies. Our efforts so far have focused on the OHSUMED corpus (Hersh et al., 1994) which is a collection of Medline abstracts of medical journal papers.1 While the focus of the project is on semantic issues, a prerequisite is a large, reliably annotated corpus and a level of syntactic process1Sager et al. (1994) describe the Linguistic String Project’s approach to parsing medical texts. ing that supports the computation of semantics. The computation of ‘grammatical relations’ from shallow parsers or chunkers is still at an early stage (Buchholz et al., 1999, Carroll et al., 1998) and there are few other robust semantic processors, and none in the medical domain. We have therefore chosen to re-use an existing handcrafted grammar which produces compositionally derived underspecified logical forms, namely the wide-coverage grammar, morphological analyser and lexicon provided by the Alvey Natural Language Tools (ANLT) system (Carroll et al. 1991, Grover et al. 1993). Our immediate aim is to increase coverage up to a reasonable level and thereafter to experiment with ranking the parses, e.g. using Briscoe and Carroll’s (1993) probabilistic extension </context>
</contexts>
<marker>Buchholz, Veenstra, Daelemans, 1999</marker>
<rawString>S. Buchholz, J. Veenstra, and W. Daelemans. 1999. Cascaded grammatical relation assignment. In EMNLP ’99, pp 239–246, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>C Grover</author>
</authors>
<title>The derivation of a large computational lexicon of English from LDOCE.</title>
<date>1988</date>
<booktitle>Computational Lexicographyfor Natural Language Processing.</booktitle>
<editor>In B. Boguraev and E. J. Briscoe, editors,</editor>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="5004" citStr="Carroll and Grover, 1988" startWordPosition="796" endWordPosition="799">out. 2 Improving the Lexical Component 2.1 Strategy The ANLT grammar is a unification grammar based on the GPSG formalism (Gazdar et al., 1985), which is a precursor of more recent ‘lexicalist’ grammar formalisms such as HPSG (Pollard and Sag, 1994). In these frameworks lexical entries carry a significant amount of information including subcategorisation information. Thus the practical parse success of a grammar is significantly dependent on the quality of the lexicon. The ANLT grammar is distributed with a large lexicon which was derived semi-automatically from a machine-readable dictionary (Carroll and Grover, 1988). This lexicon is of varying quality: function words such as complementizers, prepositions, determiners and quantifiers are all reliably hand-coded but content words are less reliable. Verbs are generally coded to a high standard but the noun and adjective lexicons are full of redundancies and duplications. Since these duplications can lead to huge increases in the number of spurious parses, an obvious first step was to remove all duplications from the existing lexicons and to collapse certain ambiguities such as the count/mass distinction into single underspecified entries. A second critical </context>
</contexts>
<marker>Carroll, Grover, 1988</marker>
<rawString>J. Carroll and C. Grover. 1988. The derivation of a large computational lexicon of English from LDOCE. In B. Boguraev and E. J. Briscoe, editors, Computational Lexicographyfor Natural Language Processing. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>T Briscoe</author>
<author>C Grover</author>
</authors>
<title>A development environment for large natural language grammars.</title>
<date>1991</date>
<tech>Technical Report 233,</tech>
<institution>Computer Laboratory, University of Cambridge.</institution>
<contexts>
<context position="3684" citStr="Carroll et al. 1991" startWordPosition="588" endWordPosition="591">c String Project’s approach to parsing medical texts. ing that supports the computation of semantics. The computation of ‘grammatical relations’ from shallow parsers or chunkers is still at an early stage (Buchholz et al., 1999, Carroll et al., 1998) and there are few other robust semantic processors, and none in the medical domain. We have therefore chosen to re-use an existing handcrafted grammar which produces compositionally derived underspecified logical forms, namely the wide-coverage grammar, morphological analyser and lexicon provided by the Alvey Natural Language Tools (ANLT) system (Carroll et al. 1991, Grover et al. 1993). Our immediate aim is to increase coverage up to a reasonable level and thereafter to experiment with ranking the parses, e.g. using Briscoe and Carroll’s (1993) probabilistic extension of the ANLT software. We use XML as the preprocessing mark-up technology, specifically the LT TTT and LT XML tools (Grover et al., 2000; Thompson et al., 1997). In the initial stages of the project we converted the OHSUMED corpus into XML annotated format with mark-up that encodes word tokens, POS tags, lemmatisation information etc. The research reported here builds on that mark-up in a f</context>
<context position="10497" citStr="Carroll et al. (1991)" startWordPosition="1743" endWordPosition="1746">look-up failed, since here lexical look-up doesn’t fail, it just provides an incomplete set of entries. It is of course possible to augment the grammar and/or lexicon with rules to infer noun entries from verb+ing entries and adjective entries from verb+ed entries. However, this will increase lexical ambiguity quite considerably and lead to higher numbers of spurious parses. 2.2 Implementation We expect the technique outlined above to be applicable across a range of parsing systems. In this section we describe how we have implemented it within ANLT. The version of the ANLT system described in Carroll et al. (1991) and Grover et al. (1993) does not allow tagged input but work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. We use this version of the system, running in a mode where ‘words’ are looked up according to three distinct cases: word look-up: the word has no tag and must be looked up in the lexicon (and if look-up fails, the parse fails) tag look-up: the word has a tag, look-up of the word tag pair fails, but the tag has a special hand-written entry which is u</context>
</contexts>
<marker>Carroll, Briscoe, Grover, 1991</marker>
<rawString>J. Carroll, T. Briscoe, and C. Grover. 1991. A development environment for large natural language grammars. Technical Report 233, Computer Laboratory, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>T Briscoe</author>
<author>G Minnen</author>
</authors>
<title>Can subcategorisation probabilities help a statistical parser?</title>
<date>1998</date>
<booktitle>In Proceedings of the 6th ACL/SIGDAT Workshop on Very Large Corpora,</booktitle>
<pages>118--126</pages>
<publisher>ACL/SIGDAT.</publisher>
<location>Montreal.</location>
<contexts>
<context position="3315" citStr="Carroll et al., 1998" startWordPosition="533" endWordPosition="536">e-existing knowledge bases and ontologies. Our efforts so far have focused on the OHSUMED corpus (Hersh et al., 1994) which is a collection of Medline abstracts of medical journal papers.1 While the focus of the project is on semantic issues, a prerequisite is a large, reliably annotated corpus and a level of syntactic process1Sager et al. (1994) describe the Linguistic String Project’s approach to parsing medical texts. ing that supports the computation of semantics. The computation of ‘grammatical relations’ from shallow parsers or chunkers is still at an early stage (Buchholz et al., 1999, Carroll et al., 1998) and there are few other robust semantic processors, and none in the medical domain. We have therefore chosen to re-use an existing handcrafted grammar which produces compositionally derived underspecified logical forms, namely the wide-coverage grammar, morphological analyser and lexicon provided by the Alvey Natural Language Tools (ANLT) system (Carroll et al. 1991, Grover et al. 1993). Our immediate aim is to increase coverage up to a reasonable level and thereafter to experiment with ranking the parses, e.g. using Briscoe and Carroll’s (1993) probabilistic extension of the ANLT software. W</context>
</contexts>
<marker>Carroll, Briscoe, Minnen, 1998</marker>
<rawString>J. Carroll, T. Briscoe, and G. Minnen. 1998. Can subcategorisation probabilities help a statistical parser? In Proceedings of the 6th ACL/SIGDAT Workshop on Very Large Corpora, pp 118–126, Montreal. ACL/SIGDAT.</rawString>
</citation>
<citation valid="false">
<authors>
<author>forthcoming</author>
</authors>
<title>Taggers for parsers.</title>
<journal>Artificial Intelligence.</journal>
<marker>forthcoming, </marker>
<rawString>E. Charniak, G. Carroll, J. Adcock, A. Cassandra, Y. Gotoh, J. Katz, M. Littman, and J. McCann. forthcoming. Taggers for parsers. Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch re-estimation help taggers?</title>
<date>1994</date>
<booktitle>In Proceedings of the 4th ACL conference on Applied Natural Language Processing,</booktitle>
<pages>53--58</pages>
<location>Stuttgart, Germany.</location>
<marker>Elworthy, 1994</marker>
<rawString>D. Elworthy. 1994. Does Baum-Welch re-estimation help taggers? In Proceedings of the 4th ACL conference on Applied Natural Language Processing, pp 53–58, Stuttgart, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Futrelle</author>
<author>C Dunn</author>
<author>D Ellis</author>
<author>M Pescitelli</author>
</authors>
<title>Preprocessing and lexicon design for parsing technical text.</title>
<date>1991</date>
<booktitle>In 2nd International Workshop on Parsing Technologies (IWPT-91),</booktitle>
<pages>31--40</pages>
<location>Morristown, New Jersey.</location>
<contexts>
<context position="21089" citStr="Futrelle et al. (1991)" startWordPosition="3577" endWordPosition="3580">r state-of-the-art taggers. However, all taggers make errors, especially when used on data different from their training data. With the strategy outlined in this paper, where we only retain a subset of tags, many tagging errors will be harmless. However, content word tagging errors will be detrimental since the basic noun/verb/adjective/adverb distinction drives lexical look-up and only entries of the same category as the tag will be accessed. If we find that the tagger consistently makes the same error in a particular context, for example mistagging +ing nominalisations as verbs (VBG), then 3Futrelle et al. (1991) discuss tokenisation issues in biological texts. we can use fsgmatch rules to replace the tag in just those contexts. The new tag can be given a definition which is ambiguous between NN and VBG, thereby ensuring that a parse can be achieved. A second strategy that we are exploring involves using more than one tagger. Our current pipeline includes a call to Elworthy’s (1994) CLAWS2 tagger. We encode the tags from this tagger as values of the attribute C2 on words: W P=’NNS’ C2=’NN2’ LM=’case’ cases /W W P=’VBN’ C2=’VVN’ LM=’find’ found /W Many mistaggings can be found by searching for words wh</context>
</contexts>
<marker>Futrelle, Dunn, Ellis, Pescitelli, 1991</marker>
<rawString>R. Futrelle, C. Dunn, D. Ellis, and M. Pescitelli. 1991. Preprocessing and lexicon design for parsing technical text. In 2nd International Workshop on Parsing Technologies (IWPT-91), pp 31–40, Morristown, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Basil Blackwell,</publisher>
<location>London.</location>
<contexts>
<context position="4522" citStr="Gazdar et al., 1985" startWordPosition="724" endWordPosition="727">T software. We use XML as the preprocessing mark-up technology, specifically the LT TTT and LT XML tools (Grover et al., 2000; Thompson et al., 1997). In the initial stages of the project we converted the OHSUMED corpus into XML annotated format with mark-up that encodes word tokens, POS tags, lemmatisation information etc. The research reported here builds on that mark-up in a further stage of pre-processing prior to parsing. The XML paradigm has proved invaluable throughout. 2 Improving the Lexical Component 2.1 Strategy The ANLT grammar is a unification grammar based on the GPSG formalism (Gazdar et al., 1985), which is a precursor of more recent ‘lexicalist’ grammar formalisms such as HPSG (Pollard and Sag, 1994). In these frameworks lexical entries carry a significant amount of information including subcategorisation information. Thus the practical parse success of a grammar is significantly dependent on the quality of the lexicon. The ANLT grammar is distributed with a large lexicon which was derived semi-automatically from a machine-readable dictionary (Carroll and Grover, 1988). This lexicon is of varying quality: function words such as complementizers, prepositions, determiners and quantifier</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>G. Gazdar, E. Klein, G. Pullum, and I. Sag. 1985. Generalized Phrase Structure Grammar. Basil Blackwell, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Grover</author>
<author>J Carroll</author>
<author>T Briscoe</author>
</authors>
<title>The Alvey Natural Language Tools grammar (4th release).</title>
<date>1993</date>
<tech>Technical Report 284,</tech>
<institution>Computer Laboratory, University of Cambridge.</institution>
<contexts>
<context position="3705" citStr="Grover et al. 1993" startWordPosition="592" endWordPosition="595">proach to parsing medical texts. ing that supports the computation of semantics. The computation of ‘grammatical relations’ from shallow parsers or chunkers is still at an early stage (Buchholz et al., 1999, Carroll et al., 1998) and there are few other robust semantic processors, and none in the medical domain. We have therefore chosen to re-use an existing handcrafted grammar which produces compositionally derived underspecified logical forms, namely the wide-coverage grammar, morphological analyser and lexicon provided by the Alvey Natural Language Tools (ANLT) system (Carroll et al. 1991, Grover et al. 1993). Our immediate aim is to increase coverage up to a reasonable level and thereafter to experiment with ranking the parses, e.g. using Briscoe and Carroll’s (1993) probabilistic extension of the ANLT software. We use XML as the preprocessing mark-up technology, specifically the LT TTT and LT XML tools (Grover et al., 2000; Thompson et al., 1997). In the initial stages of the project we converted the OHSUMED corpus into XML annotated format with mark-up that encodes word tokens, POS tags, lemmatisation information etc. The research reported here builds on that mark-up in a further stage of pre-p</context>
<context position="10522" citStr="Grover et al. (1993)" startWordPosition="1748" endWordPosition="1751"> lexical look-up doesn’t fail, it just provides an incomplete set of entries. It is of course possible to augment the grammar and/or lexicon with rules to infer noun entries from verb+ing entries and adjective entries from verb+ed entries. However, this will increase lexical ambiguity quite considerably and lead to higher numbers of spurious parses. 2.2 Implementation We expect the technique outlined above to be applicable across a range of parsing systems. In this section we describe how we have implemented it within ANLT. The version of the ANLT system described in Carroll et al. (1991) and Grover et al. (1993) does not allow tagged input but work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. We use this version of the system, running in a mode where ‘words’ are looked up according to three distinct cases: word look-up: the word has no tag and must be looked up in the lexicon (and if look-up fails, the parse fails) tag look-up: the word has a tag, look-up of the word tag pair fails, but the tag has a special hand-written entry which is used instead word tag look</context>
</contexts>
<marker>Grover, Carroll, Briscoe, 1993</marker>
<rawString>C. Grover, J. Carroll, and T. Briscoe. 1993. The Alvey Natural Language Tools grammar (4th release). Technical Report 284, Computer Laboratory, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Grover</author>
<author>C Matheson</author>
<author>A Mikheev</author>
<author>M Moens</author>
</authors>
<title>LT TTT—a flexible tokenisation tool.</title>
<date>2000</date>
<booktitle>In LREC 2000—Proceedings of the Second International Conference on Language Resources and Evaluation,</booktitle>
<pages>1147--1154</pages>
<location>Athens,</location>
<contexts>
<context position="4027" citStr="Grover et al., 2000" startWordPosition="645" endWordPosition="648">therefore chosen to re-use an existing handcrafted grammar which produces compositionally derived underspecified logical forms, namely the wide-coverage grammar, morphological analyser and lexicon provided by the Alvey Natural Language Tools (ANLT) system (Carroll et al. 1991, Grover et al. 1993). Our immediate aim is to increase coverage up to a reasonable level and thereafter to experiment with ranking the parses, e.g. using Briscoe and Carroll’s (1993) probabilistic extension of the ANLT software. We use XML as the preprocessing mark-up technology, specifically the LT TTT and LT XML tools (Grover et al., 2000; Thompson et al., 1997). In the initial stages of the project we converted the OHSUMED corpus into XML annotated format with mark-up that encodes word tokens, POS tags, lemmatisation information etc. The research reported here builds on that mark-up in a further stage of pre-processing prior to parsing. The XML paradigm has proved invaluable throughout. 2 Improving the Lexical Component 2.1 Strategy The ANLT grammar is a unification grammar based on the GPSG formalism (Gazdar et al., 1985), which is a precursor of more recent ‘lexicalist’ grammar formalisms such as HPSG (Pollard and Sag, 1994</context>
</contexts>
<marker>Grover, Matheson, Mikheev, Moens, 2000</marker>
<rawString>C. Grover, C. Matheson, A. Mikheev, and M. Moens. 2000. LT TTT—a flexible tokenisation tool. In LREC 2000—Proceedings of the Second International Conference on Language Resources and Evaluation, Athens, pp 1147–1154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hersh</author>
<author>C Buckley</author>
<author>TJ Leone</author>
<author>D Hickam</author>
</authors>
<title>OHSUMED: an interactive retrieval evaluation and new large test collection for research.</title>
<date>1994</date>
<booktitle>Proceedings of the 17th Annual International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>192--201</pages>
<editor>In W. Bruce Croft and C. J. van Rijsbergen, editors,</editor>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2811" citStr="Hersh et al., 1994" startWordPosition="451" endWordPosition="454">otations. In Section 4 we report on an experiment with a random sample of 200 sentences which gives an approximate measure of the increase in performance we have gained. The work we describe here is part of a project which aims to combine statistical and symbolic processing techniques to compute lexical semantic relationships, e.g. the semantic relations between nouns in complex nominals. We have chosen the medical domain because the field of medical informatics provides a relative abundance of pre-existing knowledge bases and ontologies. Our efforts so far have focused on the OHSUMED corpus (Hersh et al., 1994) which is a collection of Medline abstracts of medical journal papers.1 While the focus of the project is on semantic issues, a prerequisite is a large, reliably annotated corpus and a level of syntactic process1Sager et al. (1994) describe the Linguistic String Project’s approach to parsing medical texts. ing that supports the computation of semantics. The computation of ‘grammatical relations’ from shallow parsers or chunkers is still at an early stage (Buchholz et al., 1999, Carroll et al., 1998) and there are few other robust semantic processors, and none in the medical domain. We have the</context>
</contexts>
<marker>Hersh, Buckley, Leone, Hickam, 1994</marker>
<rawString>W. Hersh, C. Buckley, TJ Leone, and D. Hickam. 1994. OHSUMED: an interactive retrieval evaluation and new large test collection for research. In W. Bruce Croft and C. J. van Rijsbergen, editors, Proceedings of the 17th Annual International Conference on Research and Development in Information Retrieval, pp 192–201, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kasper</author>
<author>B Kiefer</author>
<author>H-U Krieger</author>
<author>C J Rupp</author>
<author>K Worm</author>
</authors>
<title>Charting the depths of robust speech parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>405--412</pages>
<location>Maryland.</location>
<marker>Kasper, Kiefer, Krieger, Rupp, Worm, 1999</marker>
<rawString>W. Kasper, B. Kiefer, H.-U. Krieger, C.J. Rupp, and K. Worm. 1999. Charting the depths of robust speech parsing. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pp 405–412, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>G Kim</author>
<author>M Marcinkiewicz</author>
<author>R MacIntyre</author>
<author>A Bies</author>
<author>M Ferguson</author>
<author>K Katz</author>
<author>B Schasberger</author>
</authors>
<title>The Penn treebank: annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In ARPA Human Language Technologies Workshop.</booktitle>
<contexts>
<context position="9708" citStr="Marcus et al., 1994" startWordPosition="1608" endWordPosition="1612">utaneous JJ carbon NN dioxide NN monitoring NN during transport NN Look up of the word tag pair monitoring NN fails and the basic entry for the tag NN is used instead. Without the tag, the verb entry for monitoring would be accessed and the parse would fail. In the following example the adjectives diminished and stabilized exist only as verb entries: with the JJ tag the parse succeeds but without it, the verb entries are accessed and the parse fails. There was radiographic JJ evidence NN of diminished JJ or stabilized JJ pleural JJ effusion NN 2The LT TTT tagger uses the Penn Treebank tagset (Marcus et al., 1994): JJ labels adjectives, NN labels nouns and VB labels verbs. Note that cases such as these would be problematic for a strategy where tagging was used only when lexical look-up failed, since here lexical look-up doesn’t fail, it just provides an incomplete set of entries. It is of course possible to augment the grammar and/or lexicon with rules to infer noun entries from verb+ing entries and adjective entries from verb+ed entries. However, this will increase lexical ambiguity quite considerably and lead to higher numbers of spurious parses. 2.2 Implementation We expect the technique outlined ab</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn treebank: annotating predicate argument structure. In ARPA Human Language Technologies Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McKelvie</author>
</authors>
<date>1999</date>
<marker>McKelvie, 1999</marker>
<rawString>D. McKelvie. 1999. XMLPERL 1.0.4. XML processing software. http://www.cogsci.ed.ac. uk/˜dmck/xmlperl.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
</authors>
<title>Automatic rule induction for unknown word guessing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="14357" citStr="Mikheev, 1997" startWordPosition="2404" endWordPosition="2405">s. The early stages of the pipeline segment character strings first into words and then into sentences while subsequent stages perform POS tagging and lemmatisation. A sample part of the output of this basic pipeline is shown in Figure 1. The initial conversion to XML and the identification of words is achieved using the core LT TTT program fsgmatch, a general purpose transducer which processes an input stream and rewrites it using rules provided in a grammar file. The identification of sentence boundaries, mark-up of sentence elements and POS tagging is done by the statistical program ltpos (Mikheev, 1997). Words are marked up as W elements with further information encoded as values of attributes on the W elements. In the example, the P attribute’s value is a POS tag and the LM attribute’s is a lemma (only on nouns and verbs). The lemmatisation is performed by Minnen et al.’s (2000) morpha program which is not an XML processor. In such cases we pass data out of the pipeline in the format required by the tool and merge its output back into the XML mark-up. Typically we use McKelvie’s (1999) xmlperl program to convert out of and back into XML: for ANLT this involves putting each sentence on one l</context>
</contexts>
<marker>Mikheev, 1997</marker>
<rawString>A. Mikheev. 1997. Automatic rule induction for unknown word guessing. Computational Linguistics, 23(3):405–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>J Carroll</author>
<author>D Pearce</author>
</authors>
<title>Robust, applied morphological generation.</title>
<date>2000</date>
<booktitle>In Proceedings of 1st International Natural Language Conference (INLG ’2000),</booktitle>
<location>Mitzpe Ramon,</location>
<marker>Minnen, Carroll, Pearce, 2000</marker>
<rawString>G. Minnen, J. Carroll, and D. Pearce. 2000. Robust, applied morphological generation. In Proceedings of 1st International Natural Language Conference (INLG ’2000), Mitzpe Ramon, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<date>1994</date>
<booktitle>Head-Driven Phrase Structure Grammar. CSLI</booktitle>
<institution>and University of Chicago Press,</institution>
<location>Stanford, Ca. and Chicago, Ill.</location>
<contexts>
<context position="4628" citStr="Pollard and Sag, 1994" startWordPosition="742" endWordPosition="746"> (Grover et al., 2000; Thompson et al., 1997). In the initial stages of the project we converted the OHSUMED corpus into XML annotated format with mark-up that encodes word tokens, POS tags, lemmatisation information etc. The research reported here builds on that mark-up in a further stage of pre-processing prior to parsing. The XML paradigm has proved invaluable throughout. 2 Improving the Lexical Component 2.1 Strategy The ANLT grammar is a unification grammar based on the GPSG formalism (Gazdar et al., 1985), which is a precursor of more recent ‘lexicalist’ grammar formalisms such as HPSG (Pollard and Sag, 1994). In these frameworks lexical entries carry a significant amount of information including subcategorisation information. Thus the practical parse success of a grammar is significantly dependent on the quality of the lexicon. The ANLT grammar is distributed with a large lexicon which was derived semi-automatically from a machine-readable dictionary (Carroll and Grover, 1988). This lexicon is of varying quality: function words such as complementizers, prepositions, determiners and quantifiers are all reliably hand-coded but content words are less reliable. Verbs are generally coded to a high sta</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C. Pollard and I. Sag. 1994. Head-Driven Phrase Structure Grammar. CSLI and University of Chicago Press, Stanford, Ca. and Chicago, Ill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Sager</author>
<author>M Lyman</author>
<author>C Bucknall</author>
<author>N Nhan</author>
<author>L J Tick</author>
</authors>
<title>Natural language processing and the representation of clinical data.</title>
<date>1994</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="3042" citStr="Sager et al. (1994)" startWordPosition="491" endWordPosition="495">combine statistical and symbolic processing techniques to compute lexical semantic relationships, e.g. the semantic relations between nouns in complex nominals. We have chosen the medical domain because the field of medical informatics provides a relative abundance of pre-existing knowledge bases and ontologies. Our efforts so far have focused on the OHSUMED corpus (Hersh et al., 1994) which is a collection of Medline abstracts of medical journal papers.1 While the focus of the project is on semantic issues, a prerequisite is a large, reliably annotated corpus and a level of syntactic process1Sager et al. (1994) describe the Linguistic String Project’s approach to parsing medical texts. ing that supports the computation of semantics. The computation of ‘grammatical relations’ from shallow parsers or chunkers is still at an early stage (Buchholz et al., 1999, Carroll et al., 1998) and there are few other robust semantic processors, and none in the medical domain. We have therefore chosen to re-use an existing handcrafted grammar which produces compositionally derived underspecified logical forms, namely the wide-coverage grammar, morphological analyser and lexicon provided by the Alvey Natural Languag</context>
</contexts>
<marker>Sager, Lyman, Bucknall, Nhan, Tick, 1994</marker>
<rawString>N. Sager, M. Lyman, C. Bucknall, N. Nhan, and L. J. Tick. 1994. Natural language processing and the representation of clinical data. Journal of the American Medical Informatics Association, 1(2):142–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Thompson</author>
<author>R Tobin</author>
<author>D McKelvie</author>
<author>C Brew</author>
</authors>
<date>1997</date>
<booktitle>LT XML. Software API and toolkit for XML processing. http://www.ltg.ed.ac. uk/software/.</booktitle>
<contexts>
<context position="4051" citStr="Thompson et al., 1997" startWordPosition="649" endWordPosition="652">e-use an existing handcrafted grammar which produces compositionally derived underspecified logical forms, namely the wide-coverage grammar, morphological analyser and lexicon provided by the Alvey Natural Language Tools (ANLT) system (Carroll et al. 1991, Grover et al. 1993). Our immediate aim is to increase coverage up to a reasonable level and thereafter to experiment with ranking the parses, e.g. using Briscoe and Carroll’s (1993) probabilistic extension of the ANLT software. We use XML as the preprocessing mark-up technology, specifically the LT TTT and LT XML tools (Grover et al., 2000; Thompson et al., 1997). In the initial stages of the project we converted the OHSUMED corpus into XML annotated format with mark-up that encodes word tokens, POS tags, lemmatisation information etc. The research reported here builds on that mark-up in a further stage of pre-processing prior to parsing. The XML paradigm has proved invaluable throughout. 2 Improving the Lexical Component 2.1 Strategy The ANLT grammar is a unification grammar based on the GPSG formalism (Gazdar et al., 1985), which is a precursor of more recent ‘lexicalist’ grammar formalisms such as HPSG (Pollard and Sag, 1994). In these frameworks l</context>
</contexts>
<marker>Thompson, Tobin, McKelvie, Brew, 1997</marker>
<rawString>H. Thompson, R. Tobin, D. McKelvie, and C. Brew. 1997. LT XML. Software API and toolkit for XML processing. http://www.ltg.ed.ac. uk/software/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>UMLS</author>
</authors>
<title>Unified Medical Language System (UMLS) Knowledge Sources.</title>
<date>2000</date>
<booktitle>National Library of Medicine, Bethesda (MD), 11th edition.</booktitle>
<contexts>
<context position="23292" citStr="UMLS, 2000" startWordPosition="3966" endWordPosition="3967">s unclear whether their result is relevant to a hand-crafted grammar. 3.2 An XML corpus There are numerous advantages to working with XML tools. One general advantage is that we can add linguistic annotations in an entirely automatic and incremental fashion, so as to produce a heavily annotated corpus which may well prove useful to a number of researchers for a number of linguistic activities. In the work described here we have not used any domain specific information. However, it would clearly be possible to add domain specific information as further annotations using such resources as UMLS (UMLS, 2000). Indeed, we have begun to utilise UMLS and hope to improve the accuracy of the existing mark-up by incorporating lexical and semantic information. Since the annotations we describe are computed entirely automatically, it would be a simple matter to use our system to mark up new Medline data to increase the size of our corpus considerably. A heavily annoted corpus quickly becomes unreadable but if it is an XML annotated corpus then there are several tools to help visualise the data. For example, we use xmlperl to convert from XML to HTML to view the corpus in a browser. 4 Evaluation and Future</context>
</contexts>
<marker>UMLS, 2000</marker>
<rawString>UMLS. 2000. Unified Medical Language System (UMLS) Knowledge Sources. National Library of Medicine, Bethesda (MD), 11th edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>