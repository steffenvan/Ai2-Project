<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000448">
<title confidence="0.956202">
Scaling Semi-supervised Naive Bayes with Feature Marginals
</title>
<author confidence="0.984922">
Michael R. Lucas and Doug Downey
</author>
<affiliation confidence="0.99069">
Northwestern University
</affiliation>
<address confidence="0.9669615">
2133 Sheridan Road
Evanston, IL 60208
</address>
<email confidence="0.9797325">
mlucas@u.northwestern.edu
ddowney@eecs.northwestern.edu
</email>
<bodyText confidence="0.997879658536585">
(e.g., (Nigam et al., 2000; Mann and McCallum,
2010)).
However, current SSL techniques have scal-
ability limitations. Typically, for each target
concept to be learned, a semi-supervised classi-
fier is trained using iterative techniques that exe-
cute multiple passes over the unlabeled data (e.g.,
Expectation-Maximization (Nigam et al., 2000) or
Label Propagation (Zhu and Ghahramani, 2002)).
This is problematic for text classification over
large unlabeled corpora like the Web: new tar-
get concepts (new tasks and new topics of interest)
arise frequently, and performing even a single pass
over a large corpus for each new target concept is
intractable.
In this paper, we present a new SSL text classi-
fication approach that scales to large corpora. In-
stead of utilizing unlabeled examples directly for
each given target concept, our approach is to pre-
compute a small set of statistics over the unlabeled
data in advance. Then, for a given target class and
labeled data set, we utilize the statistics to improve
a classifier.
Specifically, we introduce a method that ex-
tends Multinomial Naive Bayes (MNB) to lever-
age marginal probability statistics P(w) of each
word w, computed over the unlabeled data. The
marginal statistics are used as a constraint to im-
prove the class-conditional probability estimates
P(w|+) and P(w|−) for the positive and negative
classes, which are often noisy when estimated over
sparse labeled data sets. We refer to the technique
as MNB with Frequency Marginals (MNB-FM).
In experiments with large unlabeled data sets
and sparse labeled data, we find that MNB-
FM is both faster and more accurate on aver-
age than standard SSL methods from previous
work, including Label Propagation, MNB with
Expectation-Maximization,, and the recent Semi-
supervised Frequency Estimate (SFE) algorithm
3(43 u et al., 2011). We also analyze how MNB-
</bodyText>
<sectionHeader confidence="0.659533" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999570041666667">
Semi-supervised learning (SSL) methods
augment standard machine learning (ML)
techniques to leverage unlabeled data.
SSL techniques are often effective in text
classification, where labeled data is scarce
but large unlabeled corpora are readily
available. However, existing SSL tech-
niques typically require multiple passes
over the entirety of the unlabeled data,
meaning the techniques are not applicable
to large corpora being produced today.
In this paper, we show that improving
marginal word frequency estimates using
unlabeled data can enable semi-supervised
text classification that scales to massive
unlabeled data sets. We present a novel
learning algorithm, which optimizes a
Naive Bayes model to accord with statis-
tics calculated from the unlabeled corpus.
In experiments with text topic classifica-
tion and sentiment analysis, we show that
our method is both more scalable and more
accurate than SSL techniques from previ-
ous work.
</bodyText>
<sectionHeader confidence="0.982152" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998147923076923">
Semi-supervised Learning (SSL) is a Machine
Learning (ML) approach that utilizes large
amounts of unlabeled data, combined with a
smaller amount of labeled data, to learn a tar-
get function (Zhu, 2006; Chapelle et al., 2006).
SSL is motivated by a simple reality: the amount
of available machine-readable data is exploding,
while human capacity for hand-labeling data for
any given ML task remains relatively constant.
Experiments in text classification and other do-
mains have demonstrated that by leveraging un-
labeled data, SSL techniques improve machine
learning performance when human input is limited
</bodyText>
<note confidence="0.8506985">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 343–351,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9968502">
FM improves accuracy, and find that surprisingly
MNB-FM is especially useful for improving class-
conditional probability estimates for words that
never occur in the training set.
The paper proceeds as follows. We formally de-
fine the task in Section 2. Our algorithm is defined
in Section 3. We present experimental results in
Section 4, and analysis in Section 5. We discuss
related work in Section 6 and conclude in Section
7 with a discussion of future work.
</bodyText>
<sectionHeader confidence="0.99241" genericHeader="introduction">
2 Problem Definition
</sectionHeader>
<bodyText confidence="0.9999247">
We consider a semi-supervised classification task,
in which the goal is to produce a mapping
from an instance space X consisting of T-tuples
of non-negative integer-valued features w =
(w1, ... , wT), to a binary output space Y =
{−, +}. In particular, our experiments will fo-
cus on the case in which the wz’s represent word
counts in a given document, in a corpus of vocab-
ulary size T.
We assume the following inputs:
</bodyText>
<listItem confidence="0.9997086">
• A set of zero or more labeled documents
DL = {(wd, yd)|d = 1, ... , n}, drawn i.i.d.
from a distribution P(w, y) for w ∈ X and
y ∈ Y.
• A large set of unlabeled documents DU =
</listItem>
<equation confidence="0.858523333333333">
{(wd)|d = n+1, ... , n+u} drawn from the
�marginal distribution P(w) = P(w, y).
y
</equation>
<bodyText confidence="0.99980725">
The goal of the task is to output a classifer
f : X → Y that performs well in predicting the
classes of given unlabeled documents. The met-
rics of evaluation we focus on in our experiments
are detailed in Section 4.
Our semi-supervised technique utilizes statis-
tics computed over the labeled corpus, denoted as
follows. We use N+w to denote the sum of the
occurrences of word w over all documents in the
positive class in the labeled data DL. Also, let
N+ = En w∈DL N+w be the sum value of all word
counts in the labeled positive documents. The
count of the remaining words in the positive doc-
uments is represented as N+¬w = N+ − N+w . The
quantities N−, N−w , and N−¬w are defined similarly
for the negative class.
</bodyText>
<sectionHeader confidence="0.984616" genericHeader="method">
3 MNB with Feature Marginals
</sectionHeader>
<bodyText confidence="0.9997666">
We now introduce our algorithm, which scalably
utilizes large unlabeled data stores for classifica-
tion tasks. The technique builds upon the multino-
mial Naive Bayes model, and is denoted as MNB
with Feature Marginals (MNB-FM).
</bodyText>
<subsectionHeader confidence="0.989211">
3.1 MNB-FM Method
</subsectionHeader>
<bodyText confidence="0.999970454545455">
In the text classification setting, each feature value
wd represents count of observations of word w in
document d. MNB makes the simplifying assump-
tion that word occurrences are conditionally inde-
pendent of each other given the class (+ or −) of
the example. Formally, let the probability P(w|+)
of the w in the positive class be denoted as B+w. Let
P(+) denote the prior probability that a document
is of the positive class, and P(−) = 1 − P(+) the
prior for the negative class. Then MNB represents
the class probability of an example as:
</bodyText>
<equation confidence="0.9989935">
ri (B+w)wdP(+)
P(+|d) = w∈d
</equation>
<bodyText confidence="0.998551333333333">
MNB estimates the parameters B+w from the
corresponding counts in the training set. The
maximum-likelihood estimate of B+w is N+w /N+,
and to prevent zero-probability estimates we em-
ploy “add-1” smoothing (typical in MNB) to ob-
tain the estimate:
</bodyText>
<equation confidence="0.997301">
N+ w + 1
B+ w =
N+ + |T|.
</equation>
<bodyText confidence="0.993052857142857">
After MNB calculates B+w and B−w from the train-
ing set for each feature in the feature space, it can
then classify test examples using Equation 1.
MNB-FM attempts to improve MNB’s esti-
mates of B+w and B−w, using statistics computed over
the unlabeled data. Formally, MNB-FM leverages
the equality:
</bodyText>
<equation confidence="0.999923">
P(w) = B+wPt(+) + B−wPt(−) (2)
</equation>
<bodyText confidence="0.999230545454545">
The left-hand-side of Equation 2, P(w), repre-
sents the probability that a given randomly drawn
token from the unlabeled data happens to be the
word w. We write Pt(+) to denote the probabil-
ity that a randomly drawn token (i.e. a word oc-
currence) from the corpus comes from the posi-
tive class. Note that Pt(+) can differ from P(+),
the prior probability that a document is positive,
due to variations in document length. Pt(−) is de-
fined similarly for the negative class. MNB-FM is
motivated by the insight that the left-hand-side of
</bodyText>
<page confidence="0.902747">
344
</page>
<equation confidence="0.9724775">
ri (B− w)wdP(−) + ri (B+w)wdP(+)
w∈d w∈d
(1)
N,a, ln(K − Lθ+w) + N��w ln(1 − K + Lθ+w)
</equation>
<bodyText confidence="0.7195975">
The optimal values for θ+ w are thus located at the
solutions of:
</bodyText>
<page confidence="0.990698">
345
</page>
<bodyText confidence="0.9998412">
Equation 2 can be estimated in advance, without
knowledge of the target class, simply by counting
the number of tokens of each word in the unla-
beled data.
MNB-FM then uses this improved estimate of
P(w) as a constraint to improve the MNB param-
eters on the right-hand-side of Equation 2. We
note that Pt(+) and Pt(−), even for a small train-
ing set, can typically be estimated reliably— ev-
ery token in the training data serves as an obser-
vation of these quantities. However, for large and
sparse feature spaces common in settings like text
classification, many features occur in only a small
fraction of examples—meaning θ+w and θw must
be estimated from only a handful of observations.
MNB-FM attempts to improve the noisy estimates
θ+w and θw utilizing the robust estimate for P(w)
computed over unlabeled data.
Specifically, MNB-FM proceeds by assuming
the MLEs for P(w) (computed over unlabeled
data), Pt(+), and Pt(−) are correct, and re-
estimates θ+w and θw under the constraint in Equa-
tion 2.
First, the maximum likelihood estimates of θ+ w
and θw given the training data DL are:
</bodyText>
<equation confidence="0.978072166666667">
arg max P (DL|θ+w, θ�w)
θ1,θ−w
= arg maxθ+(N� w )
w (1 − θ+w)(Nw)
θ1,θ−w /
θW(Nw )(1 − θw)\N¬w)
= arg maxN+w ln(θ+w) + N+�w ln(1 − θ+w)+
θ1,θ−w
N,a, ln(θw) + N��w ln(1 − θ�w)
(3)
We can rewrite the constraint in Equation 2 as:
θw = K − θ+wL
</equation>
<bodyText confidence="0.97429">
where for compactness we represent:
</bodyText>
<equation confidence="0.9995545">
K = P(w) ; L = Pt(+)
Pt(−) Pt(−)
</equation>
<bodyText confidence="0.999978">
Substituting the constraint into Equation 3
shows that we wish to choose θ+ w as:
</bodyText>
<equation confidence="0.9795175">
arg maxN+w ln(θ+w) + N+�w ln(1 − θ+w)+
θw
LN��w
Lθ+w − K + 1
</equation>
<bodyText confidence="0.999731">
Both θ+w and θw are constrained to valid prob-
abilities in [0,1] when θ+w E [0, KL ]. If N+�w and
N,,, have non-zero counts, vertical asymptotes ex-
ist at 0 and KL and guarantee a solution in this
range. Otherwise, a valid solution may not ex-
ist. In that case, we default to the add-1 Smooth-
ing estimates used by MNB. Finally, after optimiz-
ing the values θ+w and θw for each word w as de-
scribed above, we normalize the estimates to ob-
tain valid conditional probability distributions, i.e.
with Ew θ+w = Ew θw = 1
</bodyText>
<subsectionHeader confidence="0.99552">
3.2 MNB-FM Example
</subsectionHeader>
<bodyText confidence="0.988143642857143">
The following concrete example illustrates how
MNB-FM can improve MNB parameters using the
statistic P(w) computed over unlabeled data. The
example comes from the Reuters Aptemod text
classification task addressed in Section 4, using
bag-of-words features for the Earnings class. In
one experiment with 10 labeled training examples,
we observed 5 positive and 5 negative examples,
with the word “resources” occurring three times
in the set (once in the positive class, twice in the
negative class).
MNB uses add-1 smoothing to estimate the con-
ditional probability of the word “resources” in
each class as θ+w = 1+1
</bodyText>
<equation confidence="0.9784948">
216+33504 = 5.93e-5, and
θ�w = 2+1
547+33504 = 8.81e-5. Thus, θ� w = 0.673
w
θ−
</equation>
<bodyText confidence="0.9999755">
implying that “resources” is a negative indicator
of the Earnings class. However, this estimate is
inaccurate. In fact, over the full dataset, the pa-
rameter values we observe are θ+w = 168549
</bodyText>
<equation confidence="0.922084">
5.70e-4 and θw = 593
64717 = 4.65e-4, with a ratio
of θ�w
θ−w
</equation>
<bodyText confidence="0.999769764705882">
sources” is a mild positive indicator of the Earn-
ings class. Yet because MNB estimates its param-
eters from only the sparse training data, it can be
inaccurate.
The optimization in MNB-FM seeks to accord
its parameter estimates with the feature frequency,
computed from unlabeled data, of P(w) = 4.89e-
4. We see that compared with P(w), the θ+ w and
θw that MNB estimates from the training data are
both too low by almost an order of magnitude.
Further, the maximum likelihood estimate for θw
(based on an occurrence count of 2 out of 547 ob-
servations) is somewhat more reliable than that for
θ+w (1 of 216 observations). As a result, θ+ w is ad-
justed upward relatively more than θw via MNB-
FM’s constrained ML estimation. MNB-FM re-
turns θ+w = 6.52e-5 and θw = 6.04e-5. The ratio
</bodyText>
<equation confidence="0.717592333333333">
.
0 = N+θ+ w N+ LN,,,
w − 1 + Lθw − K +
</equation>
<page confidence="0.488305">
�w
θ+
</page>
<bodyText confidence="0.974970545454546">
= 1.223. Thus, in actuality, the word “re-
Bw NM-FM 1.079, meaning M-FM correctly identi-
fies the word “resources” as an indicator of the
positive class.
The above example illustrates how MNB-FM
can leverage frequency marginal statistics com-
puted over unlabeled data to improve MNB’s
conditional probability estimates. We analyze
how frequently MNB-FM succeeds in improving
MNB’s estimates in practice, and the resulting im-
pact on classification accuracy, below.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9996882">
In this section, we describe our experiments quan-
tifying the accuracy and scalability of our pro-
posed technique. Across multiple domains, we
find that MNB-FM outperforms a variety of ap-
proaches from previous work.
</bodyText>
<subsectionHeader confidence="0.992286">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.9999935">
We evaluate on two text classification tasks: topic
classification, and sentiment detection. In topic
classification, the task is to determine whether a
test document belongs to a specified topic. We
train a classifier separately (i.e., in a binary clas-
sification setting) for each topic and measure clas-
sification performance for each class individually.
The sentiment detection task is to determine
whether a document is written with a positive or
negative sentiment. In our case, the goal is to de-
termine if the given text belongs to a positive re-
view of a product.
</bodyText>
<subsectionHeader confidence="0.600424">
4.1.1 RCV1
</subsectionHeader>
<bodyText confidence="0.99997">
The Reuters RCV1 corpus is a standard large cor-
pus used for topic classification evaluations (Lewis
et al., 2004). It includes 804,414 documents with
several nested target classes. We consider the 5
largest base classes after punctuation and stop-
words were removed. The vocabulary consisted
of 288,062 unique words, and the total number of
tokens in the data set was 99,702,278. Details of
the classes can be found in Table 1.
</bodyText>
<subsubsectionHeader confidence="0.783662">
4.1.2 Reuters Aptemod
</subsubsectionHeader>
<bodyText confidence="0.9970175">
While MNB-FM is designed to improve the scala-
bility of SSL to large corpora, some of the com-
parison methods from previous work were not
tractable on the large topic classification data set
RCV1. To evaluate these methods, we also exper-
imented with the Reuters ApteMod dataset (Yang
and Liu, 1999), consisting of 10,788 documents
belonging to 90 classes. We consider the 10 most
</bodyText>
<table confidence="0.989387833333333">
Class # Positive
CCAT 381327 (47.40%)
GCAT 239267 (29.74%)
MCAT 204820 (25.46%)
ECAT 119920 (14.91%)
GPOL 56878 (7.07%)
</table>
<tableCaption confidence="0.969015">
Table 1: RCV1 dataset details
</tableCaption>
<table confidence="0.999895363636364">
Class # Positive
Earnings 3964 (36.7%)
Acquisitions 2369 (22.0%)
Foreign 717 (6.6%)
Grain 582 (5.4%)
Crude 578 (5.4%)
Trade 485 (4.5%)
Interest 478 (4.4%)
Shipping 286 (2.7%)
Wheat 283 (2.6%)
Corn 237 (2.2%)
</table>
<tableCaption confidence="0.998593">
Table 2: Aptemod dataset details
</tableCaption>
<bodyText confidence="0.995063833333333">
frequent classes, with varying degrees of posi-
tive/negative skew. Punctuation and stopwords
were removed during preprocessing. The Apte-
mod data set contained 33,504 unique words and
a total of 733,266 word tokens. Details of the
classes can be found in Table 2.
</bodyText>
<subsectionHeader confidence="0.747789">
4.1.3 Sentiment Classification Data
</subsectionHeader>
<bodyText confidence="0.999841846153846">
In the domain of Sentiment Classification, we
tested on the Amazon dataset from (Blitzer et al.,
2007). Stopwords listed in an included file were
ignored for our experiments and we only the con-
sidered unigram features. Unlike the two Reuters
data sets, each category had a unique set of doc-
uments of varying size. For our experiments, we
only used the 10 largest categories. Details of the
categories can be found in Table 3.
In the Amazon Sentiment Classification data
set, the task is to determine whether a review is
positive or negative based solely on the reviewer’s
submitted text. As such, the positive and negative
</bodyText>
<table confidence="0.999534363636363">
Class # Instances # Positive Vocabulary
Music 124362 113997 (91.67%) 419936
Books 54337 47767 (87.91%) 220275
Dvd 46088 39563 (85.84%) 217744
Electronics 20393 15918 (78.06%) 65535
Kitchen 18466 14595 (79.04%) 47180
Video 17389 15017 (86.36%) 106467
Toys 12636 10151 (80.33%) 37939
Apparel 8940 7642 (85.48%) 22326
Health 6507 5124 (78.75%) 24380
Sports 5358 4352 (81.22%) 24237
</table>
<tableCaption confidence="0.999668">
Table 3: Amazon dataset details
</tableCaption>
<page confidence="0.993433">
346
</page>
<bodyText confidence="0.9999596">
labels are equally relevant. For our metrics, we
calculate the scores for both the positive and neg-
ative class and report the average of the two (in
contrast to the Reuters data sets, in which we only
report the scores for the positive class).
</bodyText>
<subsectionHeader confidence="0.809974">
4.2 Comparison Methods
</subsectionHeader>
<bodyText confidence="0.999915">
In addition to Multinomial Naive Bayes (discussed
in Section 3), we evaluate against a variety of
supervised and semi-supervised techniques from
previous work, which provide a representation of
the state of the art. Below, we detail the compar-
ison methods that we re-implemented for our ex-
periments.
</bodyText>
<subsubsectionHeader confidence="0.664813">
4.2.1 NB + EM
</subsubsectionHeader>
<bodyText confidence="0.99998825">
We implemented a semi-supervised version of
Naive Bayes with Expectation Maximization,
based on (Nigam et al., 2000). We found that 15
iterations of EM was sufficient to ensure approxi-
mate convergence of the parameters.
We also experimented with different weighting
factors to assign to the unlabeled data. While per-
forming per-data-split cross-validation was com-
putationally prohibitive for NB+EM, we per-
formed experiments on one class from each data
set that revealed weighting unlabeled examples at
1/5 the weight of a labeled example performed
best. We found that our re-implementation of
NB+EM slightly outperformed published results
on a separate data set (Mann and McCallum,
2010), validating our design choices.
</bodyText>
<subsubsectionHeader confidence="0.866746">
4.2.2 Logistic Regression
</subsubsectionHeader>
<bodyText confidence="0.999985642857143">
We implemented Logistic Regression using L2-
Normalization, finding this to outperform L1-
Normalized and non-normalized versions. The
strength of the normalization was selected for each
training data set of each size utilized in our exper-
iments.
The strength of the normalization in the logis-
tic regression required cross-validation, which we
limited to 20 values logarithmically spaced be-
tween 10−4 and 104. The optimal value was se-
lected based upon the best average F1 score over
the 10 folds. We selected a normalization param-
eter separately for each subset of the training data
during experimentation.
</bodyText>
<subsubsectionHeader confidence="0.842581">
4.2.3 Label Propagation
</subsubsectionHeader>
<bodyText confidence="0.9999542">
For our large unlabeled data set sizes, we found
that a standard Label Propogation (LP) approach,
which considers propagating information between
all pairs of unlabeled examples, was not tractable.
We instead implemented a constrained version of
LP for comparison.
In our implementation, we limit the number of
edges in the propagation graph. Each node prop-
agates to only to its 10 nearest neighbors, where
distance is calculated as the cosine distance be-
tween the tf-idf representation of two documents.
We found the tf-idf weighting to improve perfor-
mance over that of simple cosine distance. Propa-
gation was run for 100 iterations or until the en-
tropy dropped below a predetermined threshold,
whichever occurred first. Even with these aggres-
sive constraints, Label Propagation was intractable
to execute on some of the larger data sets, so we
do not report LP results for the RCV1 dataset or
for the 5 largest Amazon categories.
</bodyText>
<sectionHeader confidence="0.40682" genericHeader="method">
4.2.4 SFE
</sectionHeader>
<bodyText confidence="0.998661961538462">
We also re-implemented a version of the recent
Semi-supervised Frequency Estimate approach
(Su et al., 2011). SFE was found to outperform
MNB and NB+EM in previous work. Consis-
tent with our MNB implementation, we use Add-
1 Smoothing in our SFE calculations although its
use is not specifically mentioned in (Su et al.,
2011).
SFE also augments multinomial Naive Bayes
with the frequency information P(w), although in
a manner distinct from MNB-FM. In particular,
SFE uses the equality P(+|w) = P(+, w)/P(w)
and estimates the rhs using P(w) computed over
all the unlabeled data, rather than using only la-
beled data as in standard MNB. The primary dis-
tinction between MNB-FM and SFE is that SFE
adjusts sparse estimates P(+, w) in the same way
as non-sparse estimates, whereas MNB-FM is de-
signed to adjust sparse estimates more than non-
sparse ones. Further, it can be shown that as P(w)
of a word w in the unlabeled data becomes larger
than that in the labeled data, SFE’s estimate of the
ratio P(w|+)/P(w|−) approaches one. Depend-
ing on the labeled data, such an estimate can be ar-
bitrarily inaccurate. MNB-FM does not have this
limitation.
</bodyText>
<subsectionHeader confidence="0.876566">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999902">
For each data set, we evaluate on 50 randomly
drawn training splits, each comprised of 1,000 ran-
domly selected documents. Each set included at
least one positive and one negative document. We
</bodyText>
<page confidence="0.994806">
347
</page>
<table confidence="0.9999357">
Data Set MNB-FM SFE MNB NBEM LProp Logist.
Apte (10) 0.306 0.271 0.336 0.306 0.245 0.208
Apte (100) 0.554 0.389 0.222 0.203 0.263 0.330
Apte (1k) 0.729 0.614 0.452 0.321 0.267 0.702
Amzn (10) 0.542 0.524 0.508 0.475 0.470* 0.499
Amzn (100) 0.587 0.559 0.456 0.456 0.498* 0.542
Amzn (1k) 0.687 0.611 0.465 0.455 0.539* 0.713
RCV1 (10) 0.494 0.477 0.387 0.485 - 0.272
RCV1 (100) 0.677 0.613 0.337 0.470 - 0.518
RCV1 (1k) 0.772 0.735 0.408 0.491 - 0.774
</table>
<tableCaption confidence="0.851724">
* Limited to 5 of 10 Amazon categories
Table 4: F1, training size in parentheses
</tableCaption>
<bodyText confidence="0.999556608695652">
respected the order of the training splits such that
each sample was a strict subset of any larger train-
ing sample of the same split.
We evaluate on the standard metric of F1 with
respect to the target class. For Amazon, in which
both the “positive” and “negative” classes are po-
tential target classes, we evaluate using macro-
averaged scores.
The primary results of our experiments are
shown in Table 4. The results show that MNB-FM
improves upon the MNB classifier substantially,
and also tends to outperform the other SSL and
supervised learning methods we evaluated. MNB-
FM is the best performing method over all data
sets when the labeled data is limited to 10 and 100
documents, except for training sets of size 10 in
Aptemod, where MNB has a slight edge.
Tables 5 and 6 present detailed results of the
experiments on the RCV1 data set. These exper-
iments are limited to the 5 largest base classes
and show the F1 performance of MNB-FM and
the various comparison methods, excluding Label
Propagation which was intractable on this data set.
</bodyText>
<table confidence="0.998971714285714">
Class MNB-FM SFE MNB NBEM Logist.
CCAT 0.641 0.643 0.580 0.639 0.532
GCAT 0.639 0.686 0.531 0.732 0.466
MCAT 0.572 0.505 0.393 0.504 0.225
ECAT 0.306 0.267 0.198 0.224 0.096
GPOL 0.313 0.283 0.233 0.326 0.043
Average 0.494 0.477 0.387 0.485 0.272
</table>
<tableCaption confidence="0.996667">
Table 5: RCV1: F1, IDLI= 10
</tableCaption>
<table confidence="0.999715857142857">
Class MNB-FM SFE MNB NBEM Logist.
CCAT 0.797 0.793 0.624 0.713 0.754
GCAT 0.849 0.848 0.731 0.837 0.831
MCAT 0.776 0.737 0.313 0.516 0.689
ECAT 0.463 0.317 0.017 0.193 0.203
GPOL 0.499 0.370 0.002 0.089 0.114
Average 0.677 0.613 0.337 0.470 0.518
</table>
<tableCaption confidence="0.994378">
Table 6: RCV1: F1, IDLI= 100
</tableCaption>
<table confidence="0.9997992">
Method 1000 5000 10k 50k 100k
MNB-FM 1.44 1.61 1.69 2.47 5.50
NB+EM 2.95 3.43 4.93 10.07 16.90
MNB 1.15 1.260 1.40 2.20 3.61
Labelprop 0.26 4.17 10.62 67.58 -
</table>
<tableCaption confidence="0.999403">
Table 7: Runtimes of SSL methods (sec.)
</tableCaption>
<bodyText confidence="0.999733571428571">
The runtimes of our methods can be seen in Ta-
ble 7. The results show the runtimes of the SSL
methods discussed in this paper as the size of the
unlabeled dataset grows. As expected, we find that
MNB-FM has runtime similar to MNB, and scales
much better than methods that take multiple passes
over the unlabeled data.
</bodyText>
<sectionHeader confidence="0.988595" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999992954545454">
From our experiments, it is clear that the perfor-
mance of MNB-FM improves on MNB, and in
many cases outperforms all existing SSL algo-
rithms we evaluated. MNB-FM improves the con-
ditional probability estimates in MNB and, sur-
prisingly, we found that it can often improve these
estimates for words that do not even occur in the
training set.
Tables 8 and 9 show the details of the improve-
ments MNB-FM makes on the feature marginal
estimates. We ran MNB-FM and MNB on the
RCV1 class MCAT and stored the computed fea-
ture marginals for direct comparison. For each
word in the vocabulary, we compared each clas-
sifier’s conditional probability ratios, i.e. 0+/0−,
to the true value over the entire data set. We com-
puted which classifier was closer to the correct ra-
tio for each word. These results were averaged
over 5 iterations. From the data, we can see that
MNB-FM improves the estimates for many words
not seen in the training set as well as the most com-
mon words, even with small training sets.
</bodyText>
<subsectionHeader confidence="0.999602">
5.1 Ranking Performance
</subsectionHeader>
<bodyText confidence="0.999402454545454">
We also analyzed how well the different meth-
ods rank, rather than classify, the test documents.
We evaluated ranking using the R-precision met-
ric, equal to the precision (i.e. fraction of positive
documents classified correctly) of the R highest-
ranked test documents, where R is the total num-
ber of positive test documents.
Logistic Regression performed particularly well
on the R-Precision Metric, as can be seen in Tables
10, 11, and 12. Logistic Regression performed
less well in the F1 metric. We find that NB+EM
</bodyText>
<page confidence="0.991483">
348
</page>
<table confidence="0.999262428571429">
Fraction Improved vs MNB Avg Improvement vs MNB Probability Mass
Word Freq. Known Half Known Unknown Known Half Known Unknown Known Half Known Unknown
0-10−6 - 0.165 0.847 - -0.805 0.349 - 0.02% 7.69%
10−6-10−5 0.200 0.303 0.674 0.229 -0.539 0.131 0.00% 0.54% 14.77%
10−5-10−4 0.322 0.348 0.592 -0.597 -0.424 0.025 0.74% 10.57% 32.42%
10−4-10−3 0.533 0.564 0.433 0.014 0.083 -0.155 7.94% 17.93% 7.39%
&gt; 10−3 - - - - - - - - -
</table>
<tableCaption confidence="0.9973998">
Table 8: Analysis of Feature Marginal Improvement of MNB-FM over MNB (IDLI = 10). “Known”
indicates words occurring in both positive and negative training examples, “Half Known” indicates words
occurring in only positive or negative training examples, while “Unknown” indicates words that never
occur in labelled examples. Data is for the RCV1 MCAT category. MNB-FM improves estimates by a
substantial amount for unknown words and also the most common known and half-known words.
</tableCaption>
<table confidence="0.999656571428572">
Fraction Improved vs MNB Avg Improvement vs MNB Probability Mass
Word Freq. Known Half Known Unknown Known Half Known Unknown Known Half Known Unknown
0-10−6 0.567 0.243 0.853 0.085 -0.347 0.143 0.00% 0.22% 7.49%
10−6-10−5 0.375 0.310 0.719 -0.213 -0.260 0.087 0.38% 4.43% 10.50%
10−5-10−4 0.493 0.426 0.672 -0.071 -0.139 0.067 18.68% 20.37% 4.67%
10−4-10−3 0.728 0.669 - 0.233 0.018 - 31.70% 1.56% -
&gt; 10−3 - - - - - - - - -
</table>
<tableCaption confidence="0.989228">
Table 9: Analysis of Feature Marginal Improvement of MNB-FM over MNB (|DL |= 100). Data is
</tableCaption>
<bodyText confidence="0.998434052631579">
for the RCV1 MCAT category (see Table 8). MNB-FM improves estimates by a substantial amount for
unknown words and also the most common known and half-known words.
performs particularly well on the R-precision met-
ric on ApteMod, suggesting that its modelling as-
sumptions are more accurate for that particular
data set (NB+EM performs significantly worse on
the other data sets, however). MNB-FM performs
essentially equivalently well, on average, to the
best competing method (Logistic Regression) on
the large RCV1 data set. However, these experi-
ments show that MNB-FM offers more advantages
in document classification than in document rank-
ing.
The ranking results show that LR may be pre-
ferred when ranking is important. However, LR
underperforms in classification tasks (in terms of
F1, Tables 4-6). The reason for this is that LR’s
learned classification threshold becomes less accu-
rate when datasets are small and classes are highly
</bodyText>
<table confidence="0.9979418">
Class MNB-FM SFE MNB NBEM LProp Logist.
Apte (10) 0.353 0.304 0.359 0.631 0.490 0.416
Apte (100) 0.555 0.421 0.343 0.881 0.630 0.609
Apte (1k) 0.723 0.652 0.532 0.829 0.754 0.795
Amzn (10) 0.536 0.527 0.516 0.481 0.535* 0.544
Amzn (100) 0.614 0.562 0.517 0.480 0.573* 0.639
Amzn (1k) 0.717 0.650 0.562 0.483 0.639* 0.757
RCV1 (10) 0.505 0.480 0.421 0.450 - 0.512
RCV1 (100) 0.683 0.614 0.474 0.422 - 0.689
RCV1 (1k) 0.781 0.748 0.535 0.454 - 0.802
</table>
<tableCaption confidence="0.8950945">
* Limited to 5 of 10 Amazon categories
Table 10: R-Precision, training size in parentheses
</tableCaption>
<bodyText confidence="0.999802428571429">
skewed. In these cases, LR classifies too fre-
quently in favor of the larger class which is detri-
mental to its performance. This effect is visible
in Tables 5 and 6, where LR’s performance sig-
nificantly drops for the ECAT and GPOL classes.
ECAT and GPOL represent only 14.91% and
7.07% of the RCV1 dataset, respectively.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.988176">
To our knowledge, MNB-FM is the first approach
that utilizes a small set of statistics computed over
</bodyText>
<table confidence="0.999602857142857">
Data SetMNB-FM SFE MNB NBEM Logist.
CCAT 0.637 0.631 0.620 0.498 0.653
GCAT 0.663 0.711 0.600 0.792 0.671
MCAT 0.580 0.492 0.477 0.510 0.596
ECAT 0.291 0.217 0.214 0.111 0.297
GPOL 0.354 0.352 0.193 0.341 0.341
Average 0.505 0.480 0.421 0.450 0.512
</table>
<tableCaption confidence="0.994314">
Table 11: RCV1: R-Precision, DL= 10
</tableCaption>
<table confidence="0.999648571428571">
Class MNB-FM SFE MNB NBEM Logist.
CCAT 0.805 0.797 0.765 0.533 0.809
GCAT 0.849 0.858 0.780 0.869 0.843
MCAT 0.782 0.753 0.579 0.533 0.774
ECAT 0.471 0.293 0.203 0.119 0.498
GPOL 0.509 0.370 0.042 0.056 0.520
Average 0.683 0.614 0.474 0.422 0.689
</table>
<tableCaption confidence="0.999759">
Table 12: RCV1: R-Precision, DL= 100
</tableCaption>
<page confidence="0.998367">
349
</page>
<bodyText confidence="0.999773714285715">
a large unlabeled data set as constraints to im-
prove a semi-supervised classifier. Our exper-
iments demonstrate that MNB-FM outperforms
previous approaches across multiple text classi-
fication techniques including topic classification
and sentiment analysis. Further, the MNB-FM ap-
proach offers scalability advantages over most ex-
isting semi-supervised approaches.
Current popular Semi-Supervised Learning ap-
proaches include using Expectation-Maximization
on probabilistic models (e.g. (Nigam et al.,
2000)); Transductive Support Vector Machines
(Joachims, 1999); and graph-based methods such
as Label Propagation (LP) (Zhu and Ghahramani,
2002) and their more recent, more scalable vari-
ants (e.g. identifying a small number of represen-
tative unlabeled examples (Liu et al., 2010)). In
general, these techniques require passes over the
entirety of the unlabeled data for each new learn-
ing task, intractable for massive unlabeled data
sets. Naive implementations of LP cannot scale
to large unlabeled data sets, as they have time
complexity that increases quadratically with the
number of unlabeled examples. Recent LP tech-
niques have achieved greater scalability through
the use of parallel processing and heuristics such
as Approximate-Nearest Neighbor (Subramanya
and Bilmes, 2009), or by decomposing the sim-
ilarity matrix (Lin and Cohen, 2011). Our ap-
proach, by contrast, is to pre-compute a small
set of marginal statistics over the unlabeled data,
which eliminates the need to scan unlabeled data
for each new task. Instead, the complexity of
MNB-FM is proportional only to the number of
unique words in the labeled data set.
In recent work, Su et al. propose the Semi-
supervised Frequency Estimate (SFE), which like
MNB-FM utilizes the marginal probabilities of
features computed from unlabeled data to im-
prove the Multinomial Naive Bayes (MNB) clas-
sifier (Su et al., 2011). SFE has the same scal-
ability advantages as MNB-FM. However, unlike
our approach, SFE does not compute maximum-
likelihood estimates using the marginal statistics
as a constraint. Our experiments show that MNB-
FM substantially outperforms SFE.
A distinct method for pre-processing unlabeled
data in order to help scale semi-supervised learn-
ing techniques involves dimensionality reduction
or manifold learning (Belkin and Niyogi, 2004),
and for NLP tasks, identifying word representa-
tions from unlabeled data (Turian et al., 2010). In
contrast to these approaches, MNB-FM preserves
the original feature set and is more scalable (the
marginal statistics can be computed in a single
pass over the unlabeled data set).
</bodyText>
<sectionHeader confidence="0.999067" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999977611111111">
We presented a novel algorithm for efficiently
leveraging large unlabeled data sets for semi-
supervised learning. Our MNB-FM technique op-
timizes a Multinomial Naive Bayes model to ac-
cord with statistics of the unlabeled corpus. In ex-
periments across topic classification and sentiment
analysis, MNB-FM was found to be more accu-
rate and more scalable than several supervised and
semi-supervised baselines from previous work.
In future work, we plan to explore utilizing
richer statistics from the unlabeled data, beyond
word marginals. Further, we plan to experiment
with techniques for unlabeled data sets that also
include continuous-valued features. Lastly, we
also wish to explore ensemble approaches that
combine the best supervised classifiers with the
improved class-conditional estimates provided by
MNB-FM.
</bodyText>
<sectionHeader confidence="0.999061" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9896515">
This work was supported in part by DARPA con-
tract D11AP00268.
</bodyText>
<sectionHeader confidence="0.999458" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998041">
Mikhail Belkin and Partha Niyogi. 2004. Semi-
supervised learning on riemannian manifolds. Ma-
chine Learning, 56(1):209–239.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Association for Computational Linguis-
tics, Prague, Czech Republic.
O. Chapelle, B. Sch¨olkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the Sixteenth International Confer-
ence on Machine Learning, ICML ’99, pages 200–
209, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. Rcv1: A new benchmark collection for
text categorization research. The Journal of Ma-
350 chine Learning Research, 5:361–397.
Frank Lin and William W Cohen. 2011. Adaptation
of graph-based semi-supervised methods to large-
scale text data. In The 9th Workshop on Mining and
Learning with Graphs.
Wei Liu, Junfeng He, and Shih-Fu Chang. 2010. Large
graph construction for scalable semi-supervised
learning. In ICML, pages 679–686.
Gideon S. Mann and Andrew McCallum. 2010.
Generalized expectation criteria for semi-supervised
learning with weakly labeled data. J. Mach. Learn.
Res., 11:955–984, March.
Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classifica-
tion from labeled and unlabeled documents using
em. Mach. Learn., 39(2-3):103–134, May.
Jiang Su, Jelber Sayyad Shirab, and Stan Matwin.
2011. Large scale text classification using semisu-
pervised multinomial naive bayes. In Lise Getoor
and Tobias Scheffer, editors, ICML, pages 97–104.
Omnipress.
Amar Subramanya and Jeff A. Bilmes. 2009. En-
tropic graph regularization in non-parametric semi-
supervised classification. In Neural Information
Processing Society (NIPS), Vancouver, Canada, De-
cember.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. Urbana, 51:61801.
Yiming Yang and Xin Liu. 1999. A re-examination
of text categorization methods. In Proceedings of
the 22nd annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 42–49. ACM.
X. Zhu and Z. Ghahramani. 2002. Learning from
labeled and unlabeled data with label propagation.
Technical report, Technical Report CMU-CALD-
02-107, Carnegie Mellon University.
Xiaojin Zhu. 2006. Semi-supervised learning litera-
ture survey.
</reference>
<page confidence="0.998668">
351
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.555903">
<title confidence="0.999979">Scaling Semi-supervised Naive Bayes with Feature Marginals</title>
<author confidence="0.99849">R Lucas</author>
<affiliation confidence="0.946278">Northwestern</affiliation>
<address confidence="0.8646765">2133 Sheridan Evanston, IL</address>
<email confidence="0.987608">ddowney@eecs.northwestern.edu</email>
<abstract confidence="0.996100666666667">e.g., (Nigam et al., 2000; Mann and McCallum, 2010)). However, current SSL techniques have scalability limitations. Typically, for each target concept to be learned, a semi-supervised classifier is trained using iterative techniques that execute multiple passes over the unlabeled data (e.g., Expectation-Maximization (Nigam et al., 2000) or Label Propagation (Zhu and Ghahramani, 2002)). This is problematic for text classification over large unlabeled corpora like the Web: new target concepts (new tasks and new topics of interest) arise frequently, and performing even a single pass over a large corpus for each new target concept is intractable. In this paper, we present a new SSL text classification approach that scales to large corpora. Instead of utilizing unlabeled examples directly for each given target concept, our approach is to prea small set of the unlabeled data in advance. Then, for a given target class and labeled data set, we utilize the statistics to improve a classifier. Specifically, we introduce a method that extends Multinomial Naive Bayes (MNB) to levermarginal probability statistics each computed over the unlabeled data. The marginal statistics are used as a constraint to improve the class-conditional probability estimates the positive and negative classes, which are often noisy when estimated over sparse labeled data sets. We refer to the technique as MNB with Frequency Marginals (MNB-FM). In experiments with large unlabeled data sets and sparse labeled data, we find that MNB- FM is both faster and more accurate on average than standard SSL methods from previous work, including Label Propagation, MNB with Expectation-Maximization,, and the recent Semisupervised Frequency Estimate (SFE) algorithm u et al., 2011). We also analyze how MNB- Abstract Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. However, existing SSL techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets. We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus. In experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previous work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mikhail Belkin</author>
<author>Partha Niyogi</author>
</authors>
<title>Semisupervised learning on riemannian manifolds.</title>
<date>2004</date>
<booktitle>Machine Learning,</booktitle>
<volume>56</volume>
<issue>1</issue>
<contexts>
<context position="30161" citStr="Belkin and Niyogi, 2004" startWordPosition="5033" endWordPosition="5036">quency Estimate (SFE), which like MNB-FM utilizes the marginal probabilities of features computed from unlabeled data to improve the Multinomial Naive Bayes (MNB) classifier (Su et al., 2011). SFE has the same scalability advantages as MNB-FM. However, unlike our approach, SFE does not compute maximumlikelihood estimates using the marginal statistics as a constraint. Our experiments show that MNBFM substantially outperforms SFE. A distinct method for pre-processing unlabeled data in order to help scale semi-supervised learning techniques involves dimensionality reduction or manifold learning (Belkin and Niyogi, 2004), and for NLP tasks, identifying word representations from unlabeled data (Turian et al., 2010). In contrast to these approaches, MNB-FM preserves the original feature set and is more scalable (the marginal statistics can be computed in a single pass over the unlabeled data set). 7 Conclusion We presented a novel algorithm for efficiently leveraging large unlabeled data sets for semisupervised learning. Our MNB-FM technique optimizes a Multinomial Naive Bayes model to accord with statistics of the unlabeled corpus. In experiments across topic classification and sentiment analysis, MNB-FM was f</context>
</contexts>
<marker>Belkin, Niyogi, 2004</marker>
<rawString>Mikhail Belkin and Partha Niyogi. 2004. Semisupervised learning on riemannian manifolds. Machine Learning, 56(1):209–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="14570" citStr="Blitzer et al., 2007" startWordPosition="2465" endWordPosition="2468">nings 3964 (36.7%) Acquisitions 2369 (22.0%) Foreign 717 (6.6%) Grain 582 (5.4%) Crude 578 (5.4%) Trade 485 (4.5%) Interest 478 (4.4%) Shipping 286 (2.7%) Wheat 283 (2.6%) Corn 237 (2.2%) Table 2: Aptemod dataset details frequent classes, with varying degrees of positive/negative skew. Punctuation and stopwords were removed during preprocessing. The Aptemod data set contained 33,504 unique words and a total of 733,266 word tokens. Details of the classes can be found in Table 2. 4.1.3 Sentiment Classification Data In the domain of Sentiment Classification, we tested on the Amazon dataset from (Blitzer et al., 2007). Stopwords listed in an included file were ignored for our experiments and we only the considered unigram features. Unlike the two Reuters data sets, each category had a unique set of documents of varying size. For our experiments, we only used the 10 largest categories. Details of the categories can be found in Table 3. In the Amazon Sentiment Classification data set, the task is to determine whether a review is positive or negative based solely on the reviewer’s submitted text. As such, the positive and negative Class # Instances # Positive Vocabulary Music 124362 113997 (91.67%) 419936 Boo</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Chapelle</author>
<author>B Sch¨olkopf</author>
<author>A Zien</author>
<author>editors</author>
</authors>
<title>Semi-Supervised Learning.</title>
<date>2006</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Chapelle, Sch¨olkopf, Zien, editors, 2006</marker>
<rawString>O. Chapelle, B. Sch¨olkopf, and A. Zien, editors. 2006. Semi-Supervised Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Transductive inference for text classification using support vector machines.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth International Conference on Machine Learning, ICML ’99,</booktitle>
<pages>200--209</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="28412" citStr="Joachims, 1999" startWordPosition="4765" endWordPosition="4766">4 0.422 0.689 Table 12: RCV1: R-Precision, DL= 100 349 a large unlabeled data set as constraints to improve a semi-supervised classifier. Our experiments demonstrate that MNB-FM outperforms previous approaches across multiple text classification techniques including topic classification and sentiment analysis. Further, the MNB-FM approach offers scalability advantages over most existing semi-supervised approaches. Current popular Semi-Supervised Learning approaches include using Expectation-Maximization on probabilistic models (e.g. (Nigam et al., 2000)); Transductive Support Vector Machines (Joachims, 1999); and graph-based methods such as Label Propagation (LP) (Zhu and Ghahramani, 2002) and their more recent, more scalable variants (e.g. identifying a small number of representative unlabeled examples (Liu et al., 2010)). In general, these techniques require passes over the entirety of the unlabeled data for each new learning task, intractable for massive unlabeled data sets. Naive implementations of LP cannot scale to large unlabeled data sets, as they have time complexity that increases quadratically with the number of unlabeled examples. Recent LP techniques have achieved greater scalability</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Transductive inference for text classification using support vector machines. In Proceedings of the Sixteenth International Conference on Machine Learning, ICML ’99, pages 200– 209, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Yiming Yang</author>
<author>Tony G Rose</author>
<author>Fan Li</author>
</authors>
<title>Rcv1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<booktitle>The Journal of Ma350 chine Learning Research,</booktitle>
<pages>5--361</pages>
<contexts>
<context position="13065" citStr="Lewis et al., 2004" startWordPosition="2222" endWordPosition="2225">ntiment detection. In topic classification, the task is to determine whether a test document belongs to a specified topic. We train a classifier separately (i.e., in a binary classification setting) for each topic and measure classification performance for each class individually. The sentiment detection task is to determine whether a document is written with a positive or negative sentiment. In our case, the goal is to determine if the given text belongs to a positive review of a product. 4.1.1 RCV1 The Reuters RCV1 corpus is a standard large corpus used for topic classification evaluations (Lewis et al., 2004). It includes 804,414 documents with several nested target classes. We consider the 5 largest base classes after punctuation and stopwords were removed. The vocabulary consisted of 288,062 unique words, and the total number of tokens in the data set was 99,702,278. Details of the classes can be found in Table 1. 4.1.2 Reuters Aptemod While MNB-FM is designed to improve the scalability of SSL to large corpora, some of the comparison methods from previous work were not tractable on the large topic classification data set RCV1. To evaluate these methods, we also experimented with the Reuters Apte</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. 2004. Rcv1: A new benchmark collection for text categorization research. The Journal of Ma350 chine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Lin</author>
<author>William W Cohen</author>
</authors>
<title>Adaptation of graph-based semi-supervised methods to largescale text data.</title>
<date>2011</date>
<booktitle>In The 9th Workshop on Mining and Learning with Graphs.</booktitle>
<contexts>
<context position="29196" citStr="Lin and Cohen, 2011" startWordPosition="4882" endWordPosition="4885">f representative unlabeled examples (Liu et al., 2010)). In general, these techniques require passes over the entirety of the unlabeled data for each new learning task, intractable for massive unlabeled data sets. Naive implementations of LP cannot scale to large unlabeled data sets, as they have time complexity that increases quadratically with the number of unlabeled examples. Recent LP techniques have achieved greater scalability through the use of parallel processing and heuristics such as Approximate-Nearest Neighbor (Subramanya and Bilmes, 2009), or by decomposing the similarity matrix (Lin and Cohen, 2011). Our approach, by contrast, is to pre-compute a small set of marginal statistics over the unlabeled data, which eliminates the need to scan unlabeled data for each new task. Instead, the complexity of MNB-FM is proportional only to the number of unique words in the labeled data set. In recent work, Su et al. propose the Semisupervised Frequency Estimate (SFE), which like MNB-FM utilizes the marginal probabilities of features computed from unlabeled data to improve the Multinomial Naive Bayes (MNB) classifier (Su et al., 2011). SFE has the same scalability advantages as MNB-FM. However, unlike</context>
</contexts>
<marker>Lin, Cohen, 2011</marker>
<rawString>Frank Lin and William W Cohen. 2011. Adaptation of graph-based semi-supervised methods to largescale text data. In The 9th Workshop on Mining and Learning with Graphs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Liu</author>
<author>Junfeng He</author>
<author>Shih-Fu Chang</author>
</authors>
<title>Large graph construction for scalable semi-supervised learning.</title>
<date>2010</date>
<booktitle>In ICML,</booktitle>
<pages>679--686</pages>
<contexts>
<context position="28630" citStr="Liu et al., 2010" startWordPosition="4797" endWordPosition="4800"> multiple text classification techniques including topic classification and sentiment analysis. Further, the MNB-FM approach offers scalability advantages over most existing semi-supervised approaches. Current popular Semi-Supervised Learning approaches include using Expectation-Maximization on probabilistic models (e.g. (Nigam et al., 2000)); Transductive Support Vector Machines (Joachims, 1999); and graph-based methods such as Label Propagation (LP) (Zhu and Ghahramani, 2002) and their more recent, more scalable variants (e.g. identifying a small number of representative unlabeled examples (Liu et al., 2010)). In general, these techniques require passes over the entirety of the unlabeled data for each new learning task, intractable for massive unlabeled data sets. Naive implementations of LP cannot scale to large unlabeled data sets, as they have time complexity that increases quadratically with the number of unlabeled examples. Recent LP techniques have achieved greater scalability through the use of parallel processing and heuristics such as Approximate-Nearest Neighbor (Subramanya and Bilmes, 2009), or by decomposing the similarity matrix (Lin and Cohen, 2011). Our approach, by contrast, is to</context>
</contexts>
<marker>Liu, He, Chang, 2010</marker>
<rawString>Wei Liu, Junfeng He, and Shih-Fu Chang. 2010. Large graph construction for scalable semi-supervised learning. In ICML, pages 679–686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning with weakly labeled data.</title>
<date>2010</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>11--955</pages>
<contexts>
<context position="16774" citStr="Mann and McCallum, 2010" startWordPosition="2818" endWordPosition="2821">pectation Maximization, based on (Nigam et al., 2000). We found that 15 iterations of EM was sufficient to ensure approximate convergence of the parameters. We also experimented with different weighting factors to assign to the unlabeled data. While performing per-data-split cross-validation was computationally prohibitive for NB+EM, we performed experiments on one class from each data set that revealed weighting unlabeled examples at 1/5 the weight of a labeled example performed best. We found that our re-implementation of NB+EM slightly outperformed published results on a separate data set (Mann and McCallum, 2010), validating our design choices. 4.2.2 Logistic Regression We implemented Logistic Regression using L2- Normalization, finding this to outperform L1- Normalized and non-normalized versions. The strength of the normalization was selected for each training data set of each size utilized in our experiments. The strength of the normalization in the logistic regression required cross-validation, which we limited to 20 values logarithmically spaced between 10−4 and 104. The optimal value was selected based upon the best average F1 score over the 10 folds. We selected a normalization parameter separa</context>
</contexts>
<marker>Mann, McCallum, 2010</marker>
<rawString>Gideon S. Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. J. Mach. Learn. Res., 11:955–984, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew Kachites McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using em.</title>
<date>2000</date>
<pages>39--2</pages>
<location>Mach. Learn.,</location>
<contexts>
<context position="16203" citStr="Nigam et al., 2000" startWordPosition="2731" endWordPosition="2734"> the positive and negative class and report the average of the two (in contrast to the Reuters data sets, in which we only report the scores for the positive class). 4.2 Comparison Methods In addition to Multinomial Naive Bayes (discussed in Section 3), we evaluate against a variety of supervised and semi-supervised techniques from previous work, which provide a representation of the state of the art. Below, we detail the comparison methods that we re-implemented for our experiments. 4.2.1 NB + EM We implemented a semi-supervised version of Naive Bayes with Expectation Maximization, based on (Nigam et al., 2000). We found that 15 iterations of EM was sufficient to ensure approximate convergence of the parameters. We also experimented with different weighting factors to assign to the unlabeled data. While performing per-data-split cross-validation was computationally prohibitive for NB+EM, we performed experiments on one class from each data set that revealed weighting unlabeled examples at 1/5 the weight of a labeled example performed best. We found that our re-implementation of NB+EM slightly outperformed published results on a separate data set (Mann and McCallum, 2010), validating our design choic</context>
<context position="28356" citStr="Nigam et al., 2000" startWordPosition="4757" endWordPosition="4760"> GPOL 0.509 0.370 0.042 0.056 0.520 Average 0.683 0.614 0.474 0.422 0.689 Table 12: RCV1: R-Precision, DL= 100 349 a large unlabeled data set as constraints to improve a semi-supervised classifier. Our experiments demonstrate that MNB-FM outperforms previous approaches across multiple text classification techniques including topic classification and sentiment analysis. Further, the MNB-FM approach offers scalability advantages over most existing semi-supervised approaches. Current popular Semi-Supervised Learning approaches include using Expectation-Maximization on probabilistic models (e.g. (Nigam et al., 2000)); Transductive Support Vector Machines (Joachims, 1999); and graph-based methods such as Label Propagation (LP) (Zhu and Ghahramani, 2002) and their more recent, more scalable variants (e.g. identifying a small number of representative unlabeled examples (Liu et al., 2010)). In general, these techniques require passes over the entirety of the unlabeled data for each new learning task, intractable for massive unlabeled data sets. Naive implementations of LP cannot scale to large unlabeled data sets, as they have time complexity that increases quadratically with the number of unlabeled examples</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom Mitchell. 2000. Text classification from labeled and unlabeled documents using em. Mach. Learn., 39(2-3):103–134, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Su</author>
<author>Jelber Sayyad Shirab</author>
<author>Stan Matwin</author>
</authors>
<title>Large scale text classification using semisupervised multinomial naive bayes.</title>
<date>2011</date>
<booktitle>In Lise Getoor and</booktitle>
<pages>97--104</pages>
<editor>Tobias Scheffer, editors, ICML,</editor>
<publisher>Omnipress.</publisher>
<contexts>
<context position="18514" citStr="Su et al., 2011" startWordPosition="3091" endWordPosition="3094">he cosine distance between the tf-idf representation of two documents. We found the tf-idf weighting to improve performance over that of simple cosine distance. Propagation was run for 100 iterations or until the entropy dropped below a predetermined threshold, whichever occurred first. Even with these aggressive constraints, Label Propagation was intractable to execute on some of the larger data sets, so we do not report LP results for the RCV1 dataset or for the 5 largest Amazon categories. 4.2.4 SFE We also re-implemented a version of the recent Semi-supervised Frequency Estimate approach (Su et al., 2011). SFE was found to outperform MNB and NB+EM in previous work. Consistent with our MNB implementation, we use Add1 Smoothing in our SFE calculations although its use is not specifically mentioned in (Su et al., 2011). SFE also augments multinomial Naive Bayes with the frequency information P(w), although in a manner distinct from MNB-FM. In particular, SFE uses the equality P(+|w) = P(+, w)/P(w) and estimates the rhs using P(w) computed over all the unlabeled data, rather than using only labeled data as in standard MNB. The primary distinction between MNB-FM and SFE is that SFE adjusts sparse e</context>
<context position="29728" citStr="Su et al., 2011" startWordPosition="4971" endWordPosition="4974">nya and Bilmes, 2009), or by decomposing the similarity matrix (Lin and Cohen, 2011). Our approach, by contrast, is to pre-compute a small set of marginal statistics over the unlabeled data, which eliminates the need to scan unlabeled data for each new task. Instead, the complexity of MNB-FM is proportional only to the number of unique words in the labeled data set. In recent work, Su et al. propose the Semisupervised Frequency Estimate (SFE), which like MNB-FM utilizes the marginal probabilities of features computed from unlabeled data to improve the Multinomial Naive Bayes (MNB) classifier (Su et al., 2011). SFE has the same scalability advantages as MNB-FM. However, unlike our approach, SFE does not compute maximumlikelihood estimates using the marginal statistics as a constraint. Our experiments show that MNBFM substantially outperforms SFE. A distinct method for pre-processing unlabeled data in order to help scale semi-supervised learning techniques involves dimensionality reduction or manifold learning (Belkin and Niyogi, 2004), and for NLP tasks, identifying word representations from unlabeled data (Turian et al., 2010). In contrast to these approaches, MNB-FM preserves the original feature</context>
</contexts>
<marker>Su, Shirab, Matwin, 2011</marker>
<rawString>Jiang Su, Jelber Sayyad Shirab, and Stan Matwin. 2011. Large scale text classification using semisupervised multinomial naive bayes. In Lise Getoor and Tobias Scheffer, editors, ICML, pages 97–104. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amar Subramanya</author>
<author>Jeff A Bilmes</author>
</authors>
<title>Entropic graph regularization in non-parametric semisupervised classification.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Society (NIPS),</booktitle>
<location>Vancouver, Canada,</location>
<contexts>
<context position="29133" citStr="Subramanya and Bilmes, 2009" startWordPosition="4871" endWordPosition="4874"> more recent, more scalable variants (e.g. identifying a small number of representative unlabeled examples (Liu et al., 2010)). In general, these techniques require passes over the entirety of the unlabeled data for each new learning task, intractable for massive unlabeled data sets. Naive implementations of LP cannot scale to large unlabeled data sets, as they have time complexity that increases quadratically with the number of unlabeled examples. Recent LP techniques have achieved greater scalability through the use of parallel processing and heuristics such as Approximate-Nearest Neighbor (Subramanya and Bilmes, 2009), or by decomposing the similarity matrix (Lin and Cohen, 2011). Our approach, by contrast, is to pre-compute a small set of marginal statistics over the unlabeled data, which eliminates the need to scan unlabeled data for each new task. Instead, the complexity of MNB-FM is proportional only to the number of unique words in the labeled data set. In recent work, Su et al. propose the Semisupervised Frequency Estimate (SFE), which like MNB-FM utilizes the marginal probabilities of features computed from unlabeled data to improve the Multinomial Naive Bayes (MNB) classifier (Su et al., 2011). SFE</context>
</contexts>
<marker>Subramanya, Bilmes, 2009</marker>
<rawString>Amar Subramanya and Jeff A. Bilmes. 2009. Entropic graph regularization in non-parametric semisupervised classification. In Neural Information Processing Society (NIPS), Vancouver, Canada, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<pages>51--61801</pages>
<location>Urbana,</location>
<contexts>
<context position="30256" citStr="Turian et al., 2010" startWordPosition="5048" endWordPosition="5051">rom unlabeled data to improve the Multinomial Naive Bayes (MNB) classifier (Su et al., 2011). SFE has the same scalability advantages as MNB-FM. However, unlike our approach, SFE does not compute maximumlikelihood estimates using the marginal statistics as a constraint. Our experiments show that MNBFM substantially outperforms SFE. A distinct method for pre-processing unlabeled data in order to help scale semi-supervised learning techniques involves dimensionality reduction or manifold learning (Belkin and Niyogi, 2004), and for NLP tasks, identifying word representations from unlabeled data (Turian et al., 2010). In contrast to these approaches, MNB-FM preserves the original feature set and is more scalable (the marginal statistics can be computed in a single pass over the unlabeled data set). 7 Conclusion We presented a novel algorithm for efficiently leveraging large unlabeled data sets for semisupervised learning. Our MNB-FM technique optimizes a Multinomial Naive Bayes model to accord with statistics of the unlabeled corpus. In experiments across topic classification and sentiment analysis, MNB-FM was found to be more accurate and more scalable than several supervised and semi-supervised baseline</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. Urbana, 51:61801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Xin Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>42--49</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13697" citStr="Yang and Liu, 1999" startWordPosition="2328" endWordPosition="2331"> 804,414 documents with several nested target classes. We consider the 5 largest base classes after punctuation and stopwords were removed. The vocabulary consisted of 288,062 unique words, and the total number of tokens in the data set was 99,702,278. Details of the classes can be found in Table 1. 4.1.2 Reuters Aptemod While MNB-FM is designed to improve the scalability of SSL to large corpora, some of the comparison methods from previous work were not tractable on the large topic classification data set RCV1. To evaluate these methods, we also experimented with the Reuters ApteMod dataset (Yang and Liu, 1999), consisting of 10,788 documents belonging to 90 classes. We consider the 10 most Class # Positive CCAT 381327 (47.40%) GCAT 239267 (29.74%) MCAT 204820 (25.46%) ECAT 119920 (14.91%) GPOL 56878 (7.07%) Table 1: RCV1 dataset details Class # Positive Earnings 3964 (36.7%) Acquisitions 2369 (22.0%) Foreign 717 (6.6%) Grain 582 (5.4%) Crude 578 (5.4%) Trade 485 (4.5%) Interest 478 (4.4%) Shipping 286 (2.7%) Wheat 283 (2.6%) Corn 237 (2.2%) Table 2: Aptemod dataset details frequent classes, with varying degrees of positive/negative skew. Punctuation and stopwords were removed during preprocessing. </context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Yiming Yang and Xin Liu. 1999. A re-examination of text categorization methods. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 42–49. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical report, Technical Report CMU-CALD02-107,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="28495" citStr="Zhu and Ghahramani, 2002" startWordPosition="4775" endWordPosition="4778">data set as constraints to improve a semi-supervised classifier. Our experiments demonstrate that MNB-FM outperforms previous approaches across multiple text classification techniques including topic classification and sentiment analysis. Further, the MNB-FM approach offers scalability advantages over most existing semi-supervised approaches. Current popular Semi-Supervised Learning approaches include using Expectation-Maximization on probabilistic models (e.g. (Nigam et al., 2000)); Transductive Support Vector Machines (Joachims, 1999); and graph-based methods such as Label Propagation (LP) (Zhu and Ghahramani, 2002) and their more recent, more scalable variants (e.g. identifying a small number of representative unlabeled examples (Liu et al., 2010)). In general, these techniques require passes over the entirety of the unlabeled data for each new learning task, intractable for massive unlabeled data sets. Naive implementations of LP cannot scale to large unlabeled data sets, as they have time complexity that increases quadratically with the number of unlabeled examples. Recent LP techniques have achieved greater scalability through the use of parallel processing and heuristics such as Approximate-Nearest </context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>X. Zhu and Z. Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical report, Technical Report CMU-CALD02-107, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
</authors>
<title>Semi-supervised learning literature survey.</title>
<date>2006</date>
<contexts>
<context position="3228" citStr="Zhu, 2006" startWordPosition="489" endWordPosition="490">e semi-supervised text classification that scales to massive unlabeled data sets. We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus. In experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previous work. 1 Introduction Semi-supervised Learning (SSL) is a Machine Learning (ML) approach that utilizes large amounts of unlabeled data, combined with a smaller amount of labeled data, to learn a target function (Zhu, 2006; Chapelle et al., 2006). SSL is motivated by a simple reality: the amount of available machine-readable data is exploding, while human capacity for hand-labeling data for any given ML task remains relatively constant. Experiments in text classification and other domains have demonstrated that by leveraging unlabeled data, SSL techniques improve machine learning performance when human input is limited Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 343–351, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics FM impro</context>
</contexts>
<marker>Zhu, 2006</marker>
<rawString>Xiaojin Zhu. 2006. Semi-supervised learning literature survey.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>