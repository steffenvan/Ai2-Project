<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000614">
<title confidence="0.9762045">
Detecting short passages of similar text in large document
collections
</title>
<author confidence="0.988795">
Caroline Lyon, James Malcolm and Bob Dickerson
</author>
<affiliation confidence="0.998736">
Department of Computer Science, University of Hertfordshire,
</affiliation>
<address confidence="0.983216">
Hatfield, Hertfordshire, AL10 9A13, UK
</address>
<email confidence="0.99924">
fc.m.lyon,j.a.malcolm,r.g.dickersonl@herts.ac.uk
</email>
<sectionHeader confidence="0.995657" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951066666667">
This paper presents a statistical method for finger-
printing text. In a large collection of independently
written documents each text is associated with a fin-
gerprint which should be different from all the oth-
ers. If fingerprints are too close then it is suspected
that passages of copied or similar text occur in two
documents. Our method exploits the characteris-
tic distribution of word trigrams, and measures to
determine similarity are based on set theoretic prin-
ciples. The system was developed using a corpus
of broadcast news reports and has been successfully
used to detect plagiarism in students&apos; work. It can
find small sections that are similar as well as those
that are identical. The method is very simple and
effective, but seems not to have been used before
</bodyText>
<sectionHeader confidence="0.998508" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999868880952381">
Lexical patterns in text can be automatically cap-
tured and used to give a fingerprint to a piece of
writing. Given a set of documents we can detect
similar passages in any two documents by compar-
ing their fingerprints and seeing if they are too close.
We introduce a simple but novel system that extends
the scope of textual pattern analysis, and applies it
to copy detection tasks.
This tool has been used to detect plagiarism in
scripts submitted by large classes of students. How-
ever, the method could be applied to other purposes,
such as finding relationships between large numbers
of documents in order to detect copyright infringe-
ments.
This work was developed with a corpus of 335 TV
news reports. This is a good corpus to use since
it contains examples of varying degrees of similar-
ity, when the same issues are addressed in consec-
utive reports. This corpus is just under 1 million
words long — see Table 3. We also make use of a
smaller corpus, the Federalist Papers, of 85 texts,
about 183,000 words, which has been investigated
extensively (Hamilton et al., 1787 1788), to estab-
lish benchmarks. We describe how the plagiarism
detector works on students&apos; work, when similar but
usually not identical passages a few sentences long
can be detected.
Our tool will flag pairs of documents that contain
similar passages. However, it can be the case that
a student correctly quotes and cites source material.
This can only be determined by inspection.
The rest of this paper is organised in the following
way. Section 2 introduces the principles underlying
our method, and discusses why word trigrams are
suitable features to extract. Section 3 looks at the
context of our work from the theoretical point of
view, and gives an outline of the metrics that will
be employed. Section 4 looks at related work in this
field and Section 5 describes the domain in which we
develop our prototype. Sections 6 and 7 describe the
experiments undertaken and their results. Section 8
concludes the paper.
</bodyText>
<sectionHeader confidence="0.83144" genericHeader="method">
2 Principle of fingerprint extraction
</sectionHeader>
<bodyText confidence="0.993641181818182">
A fundamental issue in language modelling for Auto-
mated Speech Recognition is the sparse data prob-
lem (Ney et al., 1997, page 176), (Gibbon et al.,
1997, page 248). However, this phenomenon can be
turned on its head and put to good use to fingerprint
text.
The principle underlying our system is that the
identifying fingerprint associated with a piece of text
is based on a large number of small, easily extracted
lexical features: word trigrams. Each text is con-
verted into the set of overlapping 3 word sequences
of which it is composed.
When we consider separate texts on the same sub-
ject there will be certain common words, bigrams
and trigrams. Compound noun phrases provide typ-
ical examples. However, the phenomenon we exploit
is the fact that common trigrams constitute a very
small proportion of all the trigrams derived from in-
dependent texts — see the examples in Table 1 and
Table 2.
Consider the well documented distribution of sin-
gle words in English and other languages (Shannon,
1951; Manning and Schutze, 1999). We find a char-
acteristic Zipfian distribution in which a small num-
ber of words are used very often, but a significant
number are used rarely. In the Brown corpus of
A
Classrooms have become inoculation centres as health workers try to stop
the spread of the disease. More than 1,700 pupils and staff were injected today
to combat what&apos;s been described as a public health emergency.
This morning children were queuing for injections not lessons at the school at the centre
of the outbreak. Health teams have begun immunising 1,700 pupils and staff in an attempt
to stop any further cases of meningitis and bring this public health emergency under control.
</bodyText>
<tableCaption confidence="0.660390666666667">
Table 1: Examples of news text, from independent reports on the same subject. Though the semantic
content is similar there are only 3 matching trigrams out of 33 in A and 43 in B. (Trigrams overlap, so 4
consecutive matching words produce 2 matching trigrams).
</tableCaption>
<bodyText confidence="0.9652584">
There&apos;s a lot of pressure put on people in their varioms capacities and if yon
suddenly find their are pressures coming on you that make it impossible to do your job .. .
There&apos;s a lot of pressure on people in varioms capacities, and if you
find their are pressures that make it impossible to do your job ...
Table 2: Examples of similar news reports where a common source is suspected. Here there are 15 matching
trigrams out of 29 in C, and 23 in D. If we take 4-grams there are only 10 matches, and for 5-grams only 6
matches.
about 1 million words 40% of the word forms oc-
cur only once (Kupiec, 1992). This distribution of
words is an empirical observation, but can also be
understood on theoretical grounds given certain as-
sumptions on English language production (Bell et
al., 1990, chapter 4).
This distinctive distribution of words is more
pronounced for word bigrams and even more pro-
nounced for trigrams. If the probability of a word
occuring is low, the probabilty of that word occur-
ring in conjunction with others is lower still. Thus, it
is usually the case that most trigrams turning up in
new texts never occurred in large training corpora,
even when documents are on the same subject and
sometimes by the same author.
Table 4 shows the high percentage of trigrams that
occur only once in the TV News corpus. Gibbon
et al. give figures for large corpora from the Wall
Street Journal (Gibbon et al., 1997). This corpus is
in a well defined, limited domain so we might expect
recurrent lexical features to become quite common
as the corpus size increased. However, as the table
shows, even after 38 million words, 77% of trigrams
have only occurred once. In any particular article,
the majority of trigrams will probably belong to that
article alone. The set of trigrams derived from any
one article is a distinguishing feature set.
We investigated the use of n-grams as lexical fea-
tures for various it. Single words and word pairs had
inadequte distinguishing power, whereas trigrams
are effective, as demonstrated later.
For ri &gt; 3 we reduce the sensitivity of the tool,
its ability to detect similar as well as exactly copied
text. This is illustrated in the sample texts shown
in Table 2: there are 15 matching trigrams (52% of
29 trigrams in text C), 10 matching 4-grams (36% of
28 4-grams in text C) and 6 matching 5-grams (22%
of 27 5-grams in text C).
</bodyText>
<sectionHeader confidence="0.4889705" genericHeader="method">
3 The context of statistical pattern
recognition
</sectionHeader>
<bodyText confidence="0.6327764">
Recent overviews of work in this vast field include
the special issue of the journal &amp;quot;Pattern Analysis
and Machine Intelligence&amp;quot; , January 2000 (Jain et
al., 2000). A dominant approach to pattern recog-
nition is to take the data that is being analysed and
</bodyText>
<table confidence="0.998341875">
TV News corpus
Number of texts 335
Total number of words 985,316
Maximum file size in words 5090
Number of files with &lt; 1000 words 6
Average file size in words 2941
Average number of distinct trigrams per file 2818
Average % of singleton trigrams within each file 96%
</table>
<tableCaption confidence="0.99787">
Table 3: Statistics from the TV News corpus used in this work
</tableCaption>
<table confidence="0.995582571428571">
Source Corpus size Distinct trigrams Singleton trigrams % of trigrams that
in words are singletons
TV News 985,316 718,953 614,172 85%
Federalist Papers 183,372 135,830 118,842 87%
WSJ 972,868 648,482 556,185 86%
4,513,716 2,420,168 1,990,507 82%
38, 532,517 14,096,109 10,907,373 77%
</table>
<tableCaption confidence="0.8759225">
Table 4: Statistics from the TV News corpus, the Federalist Papers and from the Wall Street Journal corpora
(Gibbon et al., 1997, page 258)
</tableCaption>
<bodyText confidence="0.992556807692308">
abstract significant features from it. The resultant
features are lined up in a feature vector that charac-
terises the data. Feature vectors are then processed
to achieve a given objective, such as a classifying
task. Associated with this approach is the need to
abstract appropriate features, get their associated
weights, and find the most effective methods of pro-
cessing them. The method has been successfully
used in many sound and image processing tasks.
However, this approach only works within certain
limits. There are relationships between the number
of elements of the feature vector, and the amount of
data that needs to be used to set the parameters of
the system: a large number of features need a very
large amount of training data. For instance, in the
neural network branch of statistical pattern recogni-
tion, guidelines on the number of training examples
needed for a certain size of feature vector typically
quote a minimum ratio of 10 to 1 (Jain et al., 2000,
page 11), (Bishop, 1995, page 380).
In language processing some tasks can be ad-
dressed within the limitations of the main stream
statistical pattern recognition approach. For in-
stance, in parsing, an indefinite number of words
may be mapped onto a limited number of parts-of-
speech. (Lyon and Frank, 1997). In detecting se-
mantic similarities in texts linguistic indicators can
be extracted for a feature vector (Hatzivassiloglou
et al., 1999). However, there are other tasks where
we cannot abstract out a reduced number of signif-
icant features without losing necessary information.
When we need to use lexical information the num-
ber of words in unrestricted natural language will
usually be prohibitive. The standard pattern recog-
nition approach of processing linear feature vectors
cannot be used, and document processing with large
vocabulary texts is cited as a problem remaining to
be addressed (Jain et al., 2000, page 58).
For copy detection we need to use lexical informa-
tion. Most trigrams are prima facie of comparable
value, and we have not abstracted out a reduced set
before we started processing. Therefore, we take the
different approach of classifying text using using set
theoretic concepts. Instead of lining up features in
a linear vector, large numbers of mini-features are
grouped together in a set. Using this approach we
do not weight different mini-features, nor model de-
pendencies between them. However, we can use a
much greater range of lexical information.
In spite of the advances made in statistical pattern
recognition in the last 25 years comparatively little
attention has been paid to this approach.
3.1 The comparison of documents based on
set theoretic concepts
Our objective is to compare two documents and clas-
sify them as either &amp;quot;independent&amp;quot; or else &amp;quot;similar&amp;quot; .
The comparison can take place in two modes: first,
we may compare items of comparable length, and in
this case we will be looking for &amp;quot;resemblance&amp;quot; be-
tween texts. Secondly, a piece of text can be com-
pared with a large body of material which could have
been used as a source. In this case the texts will be
of unequal size, and we will be looking for &amp;quot;contain-
ment&amp;quot; . If a significant portion of the smaller text
is contained within the larger, then it indicates that
material has been lifted from the suspected source.
Using a method based on set theoretic concepts,
we first transform each piece of text to a set of tri-
grams. Then, for the two documents being consid-
ered, the sets of trigrams are compared.
The measures we use come from work by Broder
(Broder, 1998). The concept of resemblance, infor-
mally, is the number of matches between the (4e-
merits of two sets of trigrams, scaled by joint set
size.
Let S(A) and S(B) be the set of trigrams from
documents A and B respectively. Let R(A,B) be the
resemblance between A and
</bodyText>
<equation confidence="0.994546666666667">
IS (A) n S (B)1
R— (1)
IS (A) U S (B)1
</equation>
<bodyText confidence="0.9989153">
For the concmt of containment. C(A,B), suppose
we are measuring the extent to which set B is on-
tallied in set A. Set A might be derived from on-
catenated potential source material from the web,
a large set. Set B might be derived from a single
student essay, a small set. Informally, containment
is the number of matches between the elements of
trigram sets from A and B. scaled by the size of set
B; in other words, the proportion of trigrams in B
that are also in A.
</bodyText>
<footnote confidence="0.9204215">
1-also known as the Jaccard coefficient, and used in feature
vector analysis (Manning and Schutze, 1999, page 299).
</footnote>
<table confidence="0.867066333333333">
IS (A) n S(B)I (2)
c =
Is(B)1
</table>
<sectionHeader confidence="0.988447" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.999747">
Lexical statistics have been widely used in speech
and language processing for many years (Kuki(h,
1992; Gibbon et al., 1997). However, we can find
no record of their use in the way we propose.
</bodyText>
<subsectionHeader confidence="0.977756">
4.1 Copy detection in text
</subsectionHeader>
<bodyText confidence="0.999969206896552">
Other approaches to the copy detection task in-
clude methods based on the concmt of searching
for matching strings, typically much longer than tri-
Jams.
A well known system is SCAM (Stanford Copy
Analysis Mechanism) (Shivakumar and Gar(ia-
Molina, 1996), which has two web based objectives.
The first is to check the internet for copyright in-
fringements, the second to filter out duplicates and
near duplicates in information retrieval. To do this
they process &amp;quot;several tens of millions of web pages&amp;quot;.
On this large scale, evaluation inevitably presents
problems, and is very limited.
Though the scale of their current work puts their
objectives into a different class from our own, their
work is based on a similar concept: fingerprint-
ing text by extracting sets of &amp;quot;chunks&amp;quot;. A chunk
is a string of words, based on non-overlapping se-
quences of various length and distribution. A sam-
pling mechanism is chasm to reduce the number of
fingerprints stored. Using non-overlapping chunks
reduces storage requirements, but loses much useful
information. If two chunks match in all except one
word they will not match at all. Problems can arise
when similar chunks get out of phase and appear not
to match.
Broder (Broder, 1998) has addressed the task of
clustering 30,000,000 documents into groups of those
that closely resembled each other. His approach was
based on the concmt of matching &amp;quot;shingles&amp;quot; , where
a shingle is a sequmce of words. The length of the
shingle has to be determined To reduce storage de-
mands, only some shingles were selected, by a sam-
pling process. The similarity measures: resemblance
and containment were used as defined above.
A similar approach is employed by Heintze
(N.Heintze, 1996), who has developed a system for
analysing up to 1 million documents. He uses char-
acter strings rather than word strings as the basis
for a fingerprint, and quotes effective lengths of 30 -
45 characters. His work focuses on methods of selec-
tion to produce a reduced size fingerprint from the
full set. A web-based prototype is available.
These systems address copy detection on a very
large scale, from 1 million to tens of millions of doc-
ummts. As with all problems on this scale, (walu-
ation presents difficulties. For instance, the SCAM
system was evaluated just by classifying a sample
of 50 texts on a subjective basis (Shivakumar and
Garcia-Molina, 1996, section 4.4).
In all these methods longer chunks, shingles or
character strings are a less sensitive tool for detect-
ing similar texts rather than exact copies, as illus-
trated in Table 2. The insertion, deletion or substi-
tution of a small number of words can undermine the
matching process. Using our trigram method under-
lying similarities can be better detected, and cannot
be camouflaged by superficial variations in the text.
</bodyText>
<subsectionHeader confidence="0.941361">
4.2 Other related work
</subsectionHeader>
<bodyText confidence="0.999918888888889">
Hatzivassiloglou et al. (Hatzivassiloglou et al., 1999)
have developed a method for detecting semantically
similar texts by extracting a heterogenous collection
of morphological, syntactic and semantic features
and then processing the resultant feature vector with
a rule induction system. This is an example of the
traditional feature vector approach described in sec-
tion 3.
This approach has also been used for detecting
similar sections of code, or clones, in very large pro-
grams. A successful example is a neural processor
that detects clones in 26 million lines of code, the
product of a telecommunications company (Barton
et al., 1995). In this case the feature vector is a
heterogenous collection of items, derived from infor-
mation on keyword frequency counts, numbers of pa-
rameters, formatting and other factors. The limited
number of features makes a neural approach appro-
priate.
Phelps and Wilensky (Phelps and Wilensky, 2000)
have shown that texts can be identified by very small
signatures, no more than 5 words long. This is of use
in producing robust URLs, so when a web page is
moved it can still be traced if the URL is augmented
with a content based signature. This however is a
different task to copy detection: documents with dif-
ferent signatures can still contain copied material.
</bodyText>
<sectionHeader confidence="0.980241" genericHeader="method">
5 The TV News corpus
</sectionHeader>
<bodyText confidence="0.9999878">
The principal corpus we used to develop our proto-
type was a set of 335 TV news reports, taken over
periods from 1999 to 2000. Some statistics on the
corpus are given in Table 3.
There were usually texts from 3 news programmes
(mai day in our corpus. Often the same semantic
content was described in different words, as illus-
trated in Table 1. Sometimes the language used was
very similar, as illustrated in Table 2. Sometimes
there was verbatim copying. As we ran our diag-
nostic tests over this corpus more examples than ex-
pected of copied text came to light.
Though there are clear instances of independent or
similar text, there is no definitive objective bound-
ary for borderline cases. Eventually, the classifica-
tion of documents as similar or independent becomes
a subjective judgement.
Where the boundary is set also depmds on the
purpose for which the tool is being used. More pos-
sible copies can be flagged if some false positives are
tolerated. To avoid this, the threshold can be set
higher so that there are no false positives, but some
short similar passages may go undetected.
We decided that texts like those in Table 2 should
count as sufficiently close to be flagged.
</bodyText>
<sectionHeader confidence="0.972419" genericHeader="evaluation">
6 Experiments and results
</sectionHeader>
<subsectionHeader confidence="0.463149">
Preprocessing
</subsectionHeader>
<bodyText confidence="0.999371125">
In all our experiments, some pre-processing of the
text was done first. For words starting sentences the
initial letter was decapitalised, providing it was not
a proper noun. Punctuation was removed. Num-
bers were replaced with a symbol, so that typical
word patterns like &amp;quot;inflation rose x percent&amp;quot; were
preserved. Next, (mai text was transformed into its
associated set of trigrams.
</bodyText>
<subsectionHeader confidence="0.983996">
6.1 Experiments on Resemblance
</subsectionHeader>
<bodyText confidence="0.99551024">
We first compared each pair of files in the original
corpus. All except 6 scores for resemblance were
below 0.1. There was one aberrant result discussed
below, where R = 0.16
The number of matches for programmes not on
the same day was typically in the range 20-60, giving
R &lt; 0.01
The scores above 0.1 were all for news programmes
for the same day. On inspection we found that these
texts included sections copied verbatim, or very sim-
ilar copies, varying from 150 to 250 words in length.
In some cases several sections were copied.
The exceptional as where R = 0.16 indicated
that news on 10.3.99 resembled a programme on
27.4.99. On inspection it turned out that an item
440 words long from the first date was re-broadcast
verbatim in a programme 3080 words long on the
second date, about 147€ copied. This item, on (mvi-
ronmmtal pollution, was not particularly topical.
On closer inspection we found that for pairs of
texts where R &gt; 0.06 there were copied sections
of varying lengths, and varying degrees of similar-
ity. This was taken as a preliminary threshold above
which copying was indicated, but was subsequently
revised downwards to R &gt; 0.03.
</bodyText>
<subsectionHeader confidence="0.993311">
6.2 Identifying known copying
</subsectionHeader>
<bodyText confidence="0.999673166666667">
A blind test on identifying known copying was done
with 10 files, which were put aside by one experi-
menter. For each of them, half their contents were
used to replace the text in another file in the main
body of the corpus, then they were put back in the
main corpus. The other experimenter then ran the
</bodyText>
<table confidence="0.998866">
Number of files compared 335
Original files: R-max = 0.16
For 6 pairs out of 55945 R &gt; 0.1
Ilaximum R
Doctored files: R-min &gt; 0.30
Mininorm, R between doctored files and counterpart
files from which passages had been copied
</table>
<tableCaption confidence="0.641015">
Table 5: Scores for resemblance, R, on original TV News corpus files compared to scores for doctored files.
</tableCaption>
<table confidence="0.946489">
Number of files compared 10
Ilaxinorm containment C between undoctored files and rest-of-corpus 0.33
11/RniTrorm, containment C between doctored files and rest-of-corpus &gt; 0.49
</table>
<tableCaption confidence="0.700875">
Table 6: Scores for containment, C, between 10 files from original TV News corpus and rest-of-corpus
</tableCaption>
<bodyText confidence="0.9780914375">
compared to scores for 10 doctored texts.
fingerprinting programs over the whole corpus, in-
cluding the doctored files. They were very easily
identified, see Table 5.
Similar tests were carried out using the contain-
ment metric. We compared single files to a large
body of material that could be a source. The whole
body of texts was concatenated, apart from 10 that
were reserved to be assessed. These were taken one
at a time and compared to the rest-of-corpus. Re-
sults in Table 6 show that there is a clear distinction
beween the C scores for the doctored files, and the
exceptional case, compared with the normal ones.
However, these experiments with doctored files
only show that gross plagiarism can be very easily
detected.
</bodyText>
<subsectionHeader confidence="0.982519">
6.3 Establishing thresholds
The Federalist corpus
</subsectionHeader>
<bodyText confidence="0.976686">
The Federalist Papers are texts from a completely
different domain. They are the celebrated collection
of 85 papers, written in 1787-1788, on the proposed
American Constitution. They are easily accessible
and have been extensively studied (Mosteller and
Wallace, 1984). We considered 81 of the papers,
written by Madison or Hamilton, totalling 183,372
words. The average paper length is 2,300 words.
In this corpus the same subjects are addressed re-
peatedly. The papers are all written by one of the 2
authors, under the same pseudonym.
The threshold for R
When we ran our prototype over these papers the
maximum R score was 0.03. This confirmed our view
that R = 0.03 was a reasonable level above which
similar text would be found. If R is reduced, as
in some of our investigations into students&apos; work,
copying is likely to be found, but some false positives
may arise.
The threshold for C
We then investigated containment scores between
some individual files and the concatenated rest-of-
corpus. We took out 9 files spaced through the
corpus to assess. They averaged 2,300 words each,
and 162,655 words were left in the rest-of-corpus.
The resulting scores for C, containment, ranged from
0.23 &lt; C &lt; 0.37. All except one were below 0.34.
We conducted a similar experiment with files ex-
tracted from the News corpus, chosen so that no
other news programmes on the same day were in
the rest-of-corpus. Some of them were on consecu-
tive days. News reports on the same day frequently
treat the same subjects, and there are often copied
sections embedded in them. Taking reports on dif-
ferent days reduces, but does not eliminate, this risk.
The results in Table 7 were obtained, showing
0.25 &lt;C &lt; 0.31
</bodyText>
<table confidence="0.99161075">
Number of words in concatenated corpus 959772
Number of files to be assessed 9
Average number of words in each assessed file 2838
Range of C scores 0.25 &lt; C &lt; 0.31
</table>
<tableCaption confidence="0.977092">
Table 7: Results from experiment to establish base lines for thresholds when comparing one text to a large
potential source
</tableCaption>
<bodyText confidence="0.984806459016394">
From this we propose a provisional C threshold
of 0.31, but are aware that false positives may slip
through.
7 Detecting plagiarism in students&apos;
work
The plagiarism detector has been used on students&apos;
work submitted electronically. The vast majority of
scripts are clearly independently written or clearly
flagged as potential copies. Once the assignments
have been loaded the processing time is very quick:
for instance, less than 30 seconds for 280 files The
preprocessing has been simplified: all letters are con-
verted to lower case, and non-alphabetic characters
are ignored.
One example of its use was to investigate 124 re-
ports. 103 students submitted their reports in a sin-
gle text file, averaging 4000 words, 200 sentences.
21 submitted their reports in sets of smaller HTML
files. Each of these averaged 400 words.
This is a robust system and handles HTML as
well as ordinary text: non-alphabetic symbols are
ignored, and if parts of embedded HTML commands
are left as words this does not undermine the system.
14 cases of matching sections of text were found.
Two of these were significant, several paragraphs
long. The others were matching sentences sprinkled
through the scripts. It seems that some students
cut and paste fragments from the web and from each
other. Matching passages were mostly similar rather
than identical, with occasional words and phrases
deleted, substituted or inserted. There is of course
no way of telling the direction of the copying between
students.
Of the two significant cases one had R = 0.19 with
1400 matching trigrams in files of 4000 and 5000
words. There were 6 similar paragraphs. The other
had R = 0.19 with 805 matching trigrams in files
of 2000 and 3000 words. There were 8 similar para-
graphs.
Of the other cases with 4 to 10 matching sen-
tences, R ranged from 0.025 to 0.1. When the
threshold for R is reduced below 0.03 some false
positives arise. In this assignment there could be
a small number of matching trigrams without any
plagiarism - for instance from the table of contents.
However, taking the threshold R at 0.03 there were
no cases where we did not find matching passages in
scripts where R was above the threshold.
In another case 54 reports were analysed, averag-
ing 2,800 words each. The highest reading for R was
0.027. When we looked at 4 cases where R &gt; 0.023
we found the same 3 matching sentences each time:
the students had quoted the assignment brief.
An interface is being developed for this plagiarism
detection tool. It will display the names of pairs of
texts ranked in order, those most likely to include
similar text coming first. Then the contents of the
files will be displayed if required, with similar pas-
sages of text highlighted, side by side. At this point
a subjective decision can be made on whether work
has been copied, or is suspiciously similar.
</bodyText>
<sectionHeader confidence="0.990912" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999982829787234">
In this work we have introduced another way of us-
ing lexical information for language processing, and
produced an application. The method is very sim-
ple and effective, but seems not to have been used
before.
The mainstream of textual pattern analysis has
been restricted by the focus on linear feature vectors
as described in section 3, and in some examples in
section 4. We have used a different approach, based
on set theoretic principles.
Our system has been developed using material
with subtle similarities so the distinction between
independent and non-independent text was hard to
determine This makes it a good basis on which to
develop a robust tool.
The system has been evaluated to the extent that
it is of proven effectiveness in detecting copied ma-
terial in students&apos; work. Similar passages a few sen-
tences long have been found in files of several thou-
sand words. Passages that are similar but not iden-
tical have been identified, so slight editing does not
mask plagiarism.
Other work in this field (section 4) is much more
ambitious than ours, addressing the comparison of
many millions of web pages. However, evaluation
is difficult for these large scale tasks. Our smaller
system demonstrably works, and using our trigram
based fingerprint we are more likely to detect similar
passages that are not identical.
However, we need to compare our system with
other methods, and the whole issue of evaluation
of large scale language processing tools needs much
more attention. This will be integrated into future
work which will focus on scaling up our system, in
particular to use material from the web.
We also plan to investigate whether this approach
can be extended to comparing programs: in this case
non-alphanumeric symbols would be candidates for
fingerprinting. We will again start by investigating
data, in particular the distribution of symbol tuples.
Our work illustrates how empirical investigations
can lead to a new intepretation of data. We can
claim that we have carried out the well known in-
junction (Wittgenstein, 1945, pages 31,47) on ac-
quiring knowledge of language &amp;quot;don&apos;t think, but
look! ... problems are solved ... by arranging what
we have always known&amp;quot; .
</bodyText>
<sectionHeader confidence="0.998092" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999857557377049">
P Barton, N Davey, R Frank, and D Tansley. 1995.
Dynamic competitive learning applied to the clone
detection problem. In Inteiwational Workshop on
Applications of Neural Net to Telecommxini-
cations, Stockholm.
T C Bell, J G Cleary, and I H Witten. 1990. Text
Compression. Prentice Hall.
C M Bishop. 1995. Neural Networks for Pattern
Recognition. OUP.
A Z Broder. 1998. On the resemblance and contain-
ment of documents. In Compression and Com-
plexity of Sequences, IEEE Computer Society.
D Gibbon, R Moore, and R Winski. 1997. Handbook
of Standards and Resources for Spoken Language
Systems. Mouton de Gruyter.
A Hamilton, J Madison, and J Jay,
1787-1788. The Federalist Papers.
www.mes.net knautzr/fed/fedi.htm.
✓ Hatzivassiloglou, 1 Klavans, and E Eskin. 1999.
Detecting text similarity over short passages: Ex-
ploring linguistic feature combinations via ma-
chine learning. In Pim. of EMNLP.
A K Jain, R P W Duin, and J Mao. 2000. Statistical
pattern recognition: A review. IEEE Trans. on
Pattern Analysis and Machine Intelligence, 22, 1.
K Kukich. 1992. Techniques for automatically cor-
recting words in text. A CAT Computing Suilleys,
December.
Kupiec. 1992. Robust part-of-speech tagging us-
ing a hidden Markov model. Computer Speech and
Lanplage.
C Lyon and R Frank. 1997. Using Single Layer Net-
works for Discrete, Sequential Data: an Example
from Natural Language Processing. Neural Com-
puting Applications, 5 (4).
C D Manning and H Schutze. 1999. Foundations of
Statistical Natural Lanplage Pmcessing. MIT.
F Mosteller and D L Wallace. 1984. Applied
Bayesian and Classical Inference: The case of the
Federahst papers. Springer-Verlag.
H Ney, S Martini, and F Wessel. 1997. Statis-
tical language modelling using leaving-one-out.
In S Young and G Bloothooft, editors, Corpus
Based Methods in Language and Speech Process-
ing. Kluwer Academic Publishers.
N.Heintze, 1996. Scalable Document
Fingeiprinting. Bell Laboratories,
www.es.cmthedu/afs/es/user/nch/www/koala/.
Thomas Phelps and Robert Wilensky, 2000. Robust
Hypeilinks: Cheap, Everywhere, Now. Proc. of
Digital Documents and Electronic Publishing,
www.es.berkeley.edu/ phelps/Robust/papers.html.
C E Shannon. 1951. Prediction and Entropy of
Printed English. Bell System Technical Journal,
pages 50-64.
N Shivakumar and H Garcia-Molina. 1996. Build-
ing a scalable and accurate copy detection mecha-
nism. In Prue. of 3rd Inteiwational Conference on
Theory and Practice of Digital Libraries.
L Wittgenstein. 1945. Philosphical Investigations.
Blackwell, 1992.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.446146">
<title confidence="0.9785125">Detecting short passages of similar text in large document collections</title>
<author confidence="0.998454">Caroline Lyon</author>
<author confidence="0.998454">James Malcolm</author>
<author confidence="0.998454">Bob</author>
<affiliation confidence="0.999609">Department of Computer Science, University of</affiliation>
<address confidence="0.937282">Hatfield, Hertfordshire, AL10 9A13,</address>
<email confidence="0.999419">fc.m.lyon,j.a.malcolm,r.g.dickersonl@herts.ac.uk</email>
<abstract confidence="0.999905466666667">This paper presents a statistical method for fingerprinting text. In a large collection of independently written documents each text is associated with a fingerprint which should be different from all the others. If fingerprints are too close then it is suspected that passages of copied or similar text occur in two documents. Our method exploits the characteristic distribution of word trigrams, and measures to determine similarity are based on set theoretic principles. The system was developed using a corpus of broadcast news reports and has been successfully used to detect plagiarism in students&apos; work. It can find small sections that are similar as well as those that are identical. The method is very simple and</abstract>
<intro confidence="0.496159">effective, but seems not to have been used before</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Barton</author>
<author>N Davey</author>
<author>R Frank</author>
<author>D Tansley</author>
</authors>
<title>Dynamic competitive learning applied to the clone detection problem.</title>
<date>1995</date>
<booktitle>In Inteiwational Workshop on Applications of Neural Net to Telecommxinications,</booktitle>
<location>Stockholm.</location>
<contexts>
<context position="16627" citStr="Barton et al., 1995" startWordPosition="2814" endWordPosition="2817">l. (Hatzivassiloglou et al., 1999) have developed a method for detecting semantically similar texts by extracting a heterogenous collection of morphological, syntactic and semantic features and then processing the resultant feature vector with a rule induction system. This is an example of the traditional feature vector approach described in section 3. This approach has also been used for detecting similar sections of code, or clones, in very large programs. A successful example is a neural processor that detects clones in 26 million lines of code, the product of a telecommunications company (Barton et al., 1995). In this case the feature vector is a heterogenous collection of items, derived from information on keyword frequency counts, numbers of parameters, formatting and other factors. The limited number of features makes a neural approach appropriate. Phelps and Wilensky (Phelps and Wilensky, 2000) have shown that texts can be identified by very small signatures, no more than 5 words long. This is of use in producing robust URLs, so when a web page is moved it can still be traced if the URL is augmented with a content based signature. This however is a different task to copy detection: documents w</context>
</contexts>
<marker>Barton, Davey, Frank, Tansley, 1995</marker>
<rawString>P Barton, N Davey, R Frank, and D Tansley. 1995. Dynamic competitive learning applied to the clone detection problem. In Inteiwational Workshop on Applications of Neural Net to Telecommxinications, Stockholm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T C Bell</author>
<author>J G Cleary</author>
<author>I H Witten</author>
</authors>
<title>Text Compression.</title>
<date>1990</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="5830" citStr="Bell et al., 1990" startWordPosition="982" endWordPosition="985"> There&apos;s a lot of pressure on people in varioms capacities, and if you find their are pressures that make it impossible to do your job ... Table 2: Examples of similar news reports where a common source is suspected. Here there are 15 matching trigrams out of 29 in C, and 23 in D. If we take 4-grams there are only 10 matches, and for 5-grams only 6 matches. about 1 million words 40% of the word forms occur only once (Kupiec, 1992). This distribution of words is an empirical observation, but can also be understood on theoretical grounds given certain assumptions on English language production (Bell et al., 1990, chapter 4). This distinctive distribution of words is more pronounced for word bigrams and even more pronounced for trigrams. If the probability of a word occuring is low, the probabilty of that word occurring in conjunction with others is lower still. Thus, it is usually the case that most trigrams turning up in new texts never occurred in large training corpora, even when documents are on the same subject and sometimes by the same author. Table 4 shows the high percentage of trigrams that occur only once in the TV News corpus. Gibbon et al. give figures for large corpora from the Wall Stre</context>
</contexts>
<marker>Bell, Cleary, Witten, 1990</marker>
<rawString>T C Bell, J G Cleary, and I H Witten. 1990. Text Compression. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Bishop</author>
</authors>
<title>Neural Networks for Pattern Recognition.</title>
<date>1995</date>
<publisher>OUP.</publisher>
<contexts>
<context position="9476" citStr="Bishop, 1995" startWordPosition="1605" endWordPosition="1606"> has been successfully used in many sound and image processing tasks. However, this approach only works within certain limits. There are relationships between the number of elements of the feature vector, and the amount of data that needs to be used to set the parameters of the system: a large number of features need a very large amount of training data. For instance, in the neural network branch of statistical pattern recognition, guidelines on the number of training examples needed for a certain size of feature vector typically quote a minimum ratio of 10 to 1 (Jain et al., 2000, page 11), (Bishop, 1995, page 380). In language processing some tasks can be addressed within the limitations of the main stream statistical pattern recognition approach. For instance, in parsing, an indefinite number of words may be mapped onto a limited number of parts-ofspeech. (Lyon and Frank, 1997). In detecting semantic similarities in texts linguistic indicators can be extracted for a feature vector (Hatzivassiloglou et al., 1999). However, there are other tasks where we cannot abstract out a reduced number of significant features without losing necessary information. When we need to use lexical information t</context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>C M Bishop. 1995. Neural Networks for Pattern Recognition. OUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Z Broder</author>
</authors>
<title>On the resemblance and containment of documents.</title>
<date>1998</date>
<booktitle>In Compression and Complexity of Sequences, IEEE</booktitle>
<publisher>Computer Society.</publisher>
<contexts>
<context position="12020" citStr="Broder, 1998" startWordPosition="2032" endWordPosition="2033">en texts. Secondly, a piece of text can be compared with a large body of material which could have been used as a source. In this case the texts will be of unequal size, and we will be looking for &amp;quot;containment&amp;quot; . If a significant portion of the smaller text is contained within the larger, then it indicates that material has been lifted from the suspected source. Using a method based on set theoretic concepts, we first transform each piece of text to a set of trigrams. Then, for the two documents being considered, the sets of trigrams are compared. The measures we use come from work by Broder (Broder, 1998). The concept of resemblance, informally, is the number of matches between the (4emerits of two sets of trigrams, scaled by joint set size. Let S(A) and S(B) be the set of trigrams from documents A and B respectively. Let R(A,B) be the resemblance between A and IS (A) n S (B)1 R— (1) IS (A) U S (B)1 For the concmt of containment. C(A,B), suppose we are measuring the extent to which set B is ontallied in set A. Set A might be derived from oncatenated potential source material from the web, a large set. Set B might be derived from a single student essay, a small set. Informally, containment is t</context>
<context position="14387" citStr="Broder, 1998" startWordPosition="2450" endWordPosition="2451">eir current work puts their objectives into a different class from our own, their work is based on a similar concept: fingerprinting text by extracting sets of &amp;quot;chunks&amp;quot;. A chunk is a string of words, based on non-overlapping sequences of various length and distribution. A sampling mechanism is chasm to reduce the number of fingerprints stored. Using non-overlapping chunks reduces storage requirements, but loses much useful information. If two chunks match in all except one word they will not match at all. Problems can arise when similar chunks get out of phase and appear not to match. Broder (Broder, 1998) has addressed the task of clustering 30,000,000 documents into groups of those that closely resembled each other. His approach was based on the concmt of matching &amp;quot;shingles&amp;quot; , where a shingle is a sequmce of words. The length of the shingle has to be determined To reduce storage demands, only some shingles were selected, by a sampling process. The similarity measures: resemblance and containment were used as defined above. A similar approach is employed by Heintze (N.Heintze, 1996), who has developed a system for analysing up to 1 million documents. He uses character strings rather than word </context>
</contexts>
<marker>Broder, 1998</marker>
<rawString>A Z Broder. 1998. On the resemblance and containment of documents. In Compression and Complexity of Sequences, IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gibbon</author>
<author>R Moore</author>
<author>R Winski</author>
</authors>
<title>Handbook of Standards and Resources for Spoken Language Systems. Mouton de Gruyter.</title>
<date>1997</date>
<contexts>
<context position="3250" citStr="Gibbon et al., 1997" startWordPosition="532" endWordPosition="535">thod, and discusses why word trigrams are suitable features to extract. Section 3 looks at the context of our work from the theoretical point of view, and gives an outline of the metrics that will be employed. Section 4 looks at related work in this field and Section 5 describes the domain in which we develop our prototype. Sections 6 and 7 describe the experiments undertaken and their results. Section 8 concludes the paper. 2 Principle of fingerprint extraction A fundamental issue in language modelling for Automated Speech Recognition is the sparse data problem (Ney et al., 1997, page 176), (Gibbon et al., 1997, page 248). However, this phenomenon can be turned on its head and put to good use to fingerprint text. The principle underlying our system is that the identifying fingerprint associated with a piece of text is based on a large number of small, easily extracted lexical features: word trigrams. Each text is converted into the set of overlapping 3 word sequences of which it is composed. When we consider separate texts on the same subject there will be certain common words, bigrams and trigrams. Compound noun phrases provide typical examples. However, the phenomenon we exploit is the fact that c</context>
<context position="6462" citStr="Gibbon et al., 1997" startWordPosition="1093" endWordPosition="1096">. This distinctive distribution of words is more pronounced for word bigrams and even more pronounced for trigrams. If the probability of a word occuring is low, the probabilty of that word occurring in conjunction with others is lower still. Thus, it is usually the case that most trigrams turning up in new texts never occurred in large training corpora, even when documents are on the same subject and sometimes by the same author. Table 4 shows the high percentage of trigrams that occur only once in the TV News corpus. Gibbon et al. give figures for large corpora from the Wall Street Journal (Gibbon et al., 1997). This corpus is in a well defined, limited domain so we might expect recurrent lexical features to become quite common as the corpus size increased. However, as the table shows, even after 38 million words, 77% of trigrams have only occurred once. In any particular article, the majority of trigrams will probably belong to that article alone. The set of trigrams derived from any one article is a distinguishing feature set. We investigated the use of n-grams as lexical features for various it. Single words and word pairs had inadequte distinguishing power, whereas trigrams are effective, as dem</context>
<context position="8463" citStr="Gibbon et al., 1997" startWordPosition="1434" endWordPosition="1437"> words 6 Average file size in words 2941 Average number of distinct trigrams per file 2818 Average % of singleton trigrams within each file 96% Table 3: Statistics from the TV News corpus used in this work Source Corpus size Distinct trigrams Singleton trigrams % of trigrams that in words are singletons TV News 985,316 718,953 614,172 85% Federalist Papers 183,372 135,830 118,842 87% WSJ 972,868 648,482 556,185 86% 4,513,716 2,420,168 1,990,507 82% 38, 532,517 14,096,109 10,907,373 77% Table 4: Statistics from the TV News corpus, the Federalist Papers and from the Wall Street Journal corpora (Gibbon et al., 1997, page 258) abstract significant features from it. The resultant features are lined up in a feature vector that characterises the data. Feature vectors are then processed to achieve a given objective, such as a classifying task. Associated with this approach is the need to abstract appropriate features, get their associated weights, and find the most effective methods of processing them. The method has been successfully used in many sound and image processing tasks. However, this approach only works within certain limits. There are relationships between the number of elements of the feature ve</context>
<context position="13075" citStr="Gibbon et al., 1997" startWordPosition="2229" endWordPosition="2232">d from oncatenated potential source material from the web, a large set. Set B might be derived from a single student essay, a small set. Informally, containment is the number of matches between the elements of trigram sets from A and B. scaled by the size of set B; in other words, the proportion of trigrams in B that are also in A. 1-also known as the Jaccard coefficient, and used in feature vector analysis (Manning and Schutze, 1999, page 299). IS (A) n S(B)I (2) c = Is(B)1 4 Related work Lexical statistics have been widely used in speech and language processing for many years (Kuki(h, 1992; Gibbon et al., 1997). However, we can find no record of their use in the way we propose. 4.1 Copy detection in text Other approaches to the copy detection task include methods based on the concmt of searching for matching strings, typically much longer than triJams. A well known system is SCAM (Stanford Copy Analysis Mechanism) (Shivakumar and Gar(iaMolina, 1996), which has two web based objectives. The first is to check the internet for copyright infringements, the second to filter out duplicates and near duplicates in information retrieval. To do this they process &amp;quot;several tens of millions of web pages&amp;quot;. On thi</context>
</contexts>
<marker>Gibbon, Moore, Winski, 1997</marker>
<rawString>D Gibbon, R Moore, and R Winski. 1997. Handbook of Standards and Resources for Spoken Language Systems. Mouton de Gruyter.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Hamilton</author>
<author>J Madison</author>
<author>J Jay</author>
</authors>
<title>The Federalist Papers. www.mes.net knautzr/fed/fedi.htm.</title>
<pages>1787--1788</pages>
<marker>Hamilton, Madison, Jay, </marker>
<rawString>A Hamilton, J Madison, and J Jay, 1787-1788. The Federalist Papers. www.mes.net knautzr/fed/fedi.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klavans</author>
<author>E Eskin</author>
</authors>
<title>Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning.</title>
<date>1999</date>
<booktitle>In Pim. of EMNLP.</booktitle>
<marker>Klavans, Eskin, 1999</marker>
<rawString>✓ Hatzivassiloglou, 1 Klavans, and E Eskin. 1999. Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning. In Pim. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Jain</author>
<author>R P W Duin</author>
<author>J Mao</author>
</authors>
<title>Statistical pattern recognition: A review.</title>
<date>2000</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence,</journal>
<volume>22</volume>
<pages>1</pages>
<contexts>
<context position="7628" citStr="Jain et al., 2000" startWordPosition="1293" endWordPosition="1296">ishing power, whereas trigrams are effective, as demonstrated later. For ri &gt; 3 we reduce the sensitivity of the tool, its ability to detect similar as well as exactly copied text. This is illustrated in the sample texts shown in Table 2: there are 15 matching trigrams (52% of 29 trigrams in text C), 10 matching 4-grams (36% of 28 4-grams in text C) and 6 matching 5-grams (22% of 27 5-grams in text C). 3 The context of statistical pattern recognition Recent overviews of work in this vast field include the special issue of the journal &amp;quot;Pattern Analysis and Machine Intelligence&amp;quot; , January 2000 (Jain et al., 2000). A dominant approach to pattern recognition is to take the data that is being analysed and TV News corpus Number of texts 335 Total number of words 985,316 Maximum file size in words 5090 Number of files with &lt; 1000 words 6 Average file size in words 2941 Average number of distinct trigrams per file 2818 Average % of singleton trigrams within each file 96% Table 3: Statistics from the TV News corpus used in this work Source Corpus size Distinct trigrams Singleton trigrams % of trigrams that in words are singletons TV News 985,316 718,953 614,172 85% Federalist Papers 183,372 135,830 118,842 8</context>
<context position="9451" citStr="Jain et al., 2000" startWordPosition="1599" endWordPosition="1602">of processing them. The method has been successfully used in many sound and image processing tasks. However, this approach only works within certain limits. There are relationships between the number of elements of the feature vector, and the amount of data that needs to be used to set the parameters of the system: a large number of features need a very large amount of training data. For instance, in the neural network branch of statistical pattern recognition, guidelines on the number of training examples needed for a certain size of feature vector typically quote a minimum ratio of 10 to 1 (Jain et al., 2000, page 11), (Bishop, 1995, page 380). In language processing some tasks can be addressed within the limitations of the main stream statistical pattern recognition approach. For instance, in parsing, an indefinite number of words may be mapped onto a limited number of parts-ofspeech. (Lyon and Frank, 1997). In detecting semantic similarities in texts linguistic indicators can be extracted for a feature vector (Hatzivassiloglou et al., 1999). However, there are other tasks where we cannot abstract out a reduced number of significant features without losing necessary information. When we need to </context>
</contexts>
<marker>Jain, Duin, Mao, 2000</marker>
<rawString>A K Jain, R P W Duin, and J Mao. 2000. Statistical pattern recognition: A review. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>A CAT Computing Suilleys,</journal>
<marker>Kukich, 1992</marker>
<rawString>K Kukich. 1992. Techniques for automatically correcting words in text. A CAT Computing Suilleys, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Lanplage.</title>
<date>1992</date>
<contexts>
<context position="5647" citStr="Kupiec, 1992" startWordPosition="956" endWordPosition="957">igrams). There&apos;s a lot of pressure put on people in their varioms capacities and if yon suddenly find their are pressures coming on you that make it impossible to do your job .. . There&apos;s a lot of pressure on people in varioms capacities, and if you find their are pressures that make it impossible to do your job ... Table 2: Examples of similar news reports where a common source is suspected. Here there are 15 matching trigrams out of 29 in C, and 23 in D. If we take 4-grams there are only 10 matches, and for 5-grams only 6 matches. about 1 million words 40% of the word forms occur only once (Kupiec, 1992). This distribution of words is an empirical observation, but can also be understood on theoretical grounds given certain assumptions on English language production (Bell et al., 1990, chapter 4). This distinctive distribution of words is more pronounced for word bigrams and even more pronounced for trigrams. If the probability of a word occuring is low, the probabilty of that word occurring in conjunction with others is lower still. Thus, it is usually the case that most trigrams turning up in new texts never occurred in large training corpora, even when documents are on the same subject and </context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>Kupiec. 1992. Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Lanplage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lyon</author>
<author>R Frank</author>
</authors>
<title>Using Single Layer Networks for Discrete, Sequential Data: an Example from Natural Language Processing.</title>
<date>1997</date>
<journal>Neural Computing Applications,</journal>
<volume>5</volume>
<issue>4</issue>
<contexts>
<context position="9757" citStr="Lyon and Frank, 1997" startWordPosition="1649" endWordPosition="1652">of the system: a large number of features need a very large amount of training data. For instance, in the neural network branch of statistical pattern recognition, guidelines on the number of training examples needed for a certain size of feature vector typically quote a minimum ratio of 10 to 1 (Jain et al., 2000, page 11), (Bishop, 1995, page 380). In language processing some tasks can be addressed within the limitations of the main stream statistical pattern recognition approach. For instance, in parsing, an indefinite number of words may be mapped onto a limited number of parts-ofspeech. (Lyon and Frank, 1997). In detecting semantic similarities in texts linguistic indicators can be extracted for a feature vector (Hatzivassiloglou et al., 1999). However, there are other tasks where we cannot abstract out a reduced number of significant features without losing necessary information. When we need to use lexical information the number of words in unrestricted natural language will usually be prohibitive. The standard pattern recognition approach of processing linear feature vectors cannot be used, and document processing with large vocabulary texts is cited as a problem remaining to be addressed (Jain</context>
</contexts>
<marker>Lyon, Frank, 1997</marker>
<rawString>C Lyon and R Frank. 1997. Using Single Layer Networks for Discrete, Sequential Data: an Example from Natural Language Processing. Neural Computing Applications, 5 (4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schutze</author>
</authors>
<date>1999</date>
<journal>Foundations of Statistical Natural Lanplage Pmcessing. MIT.</journal>
<contexts>
<context position="4125" citStr="Manning and Schutze, 1999" startWordPosition="680" endWordPosition="683">ily extracted lexical features: word trigrams. Each text is converted into the set of overlapping 3 word sequences of which it is composed. When we consider separate texts on the same subject there will be certain common words, bigrams and trigrams. Compound noun phrases provide typical examples. However, the phenomenon we exploit is the fact that common trigrams constitute a very small proportion of all the trigrams derived from independent texts — see the examples in Table 1 and Table 2. Consider the well documented distribution of single words in English and other languages (Shannon, 1951; Manning and Schutze, 1999). We find a characteristic Zipfian distribution in which a small number of words are used very often, but a significant number are used rarely. In the Brown corpus of A Classrooms have become inoculation centres as health workers try to stop the spread of the disease. More than 1,700 pupils and staff were injected today to combat what&apos;s been described as a public health emergency. This morning children were queuing for injections not lessons at the school at the centre of the outbreak. Health teams have begun immunising 1,700 pupils and staff in an attempt to stop any further cases of meningit</context>
<context position="12892" citStr="Manning and Schutze, 1999" startWordPosition="2196" endWordPosition="2199"> between A and IS (A) n S (B)1 R— (1) IS (A) U S (B)1 For the concmt of containment. C(A,B), suppose we are measuring the extent to which set B is ontallied in set A. Set A might be derived from oncatenated potential source material from the web, a large set. Set B might be derived from a single student essay, a small set. Informally, containment is the number of matches between the elements of trigram sets from A and B. scaled by the size of set B; in other words, the proportion of trigrams in B that are also in A. 1-also known as the Jaccard coefficient, and used in feature vector analysis (Manning and Schutze, 1999, page 299). IS (A) n S(B)I (2) c = Is(B)1 4 Related work Lexical statistics have been widely used in speech and language processing for many years (Kuki(h, 1992; Gibbon et al., 1997). However, we can find no record of their use in the way we propose. 4.1 Copy detection in text Other approaches to the copy detection task include methods based on the concmt of searching for matching strings, typically much longer than triJams. A well known system is SCAM (Stanford Copy Analysis Mechanism) (Shivakumar and Gar(iaMolina, 1996), which has two web based objectives. The first is to check the internet</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>C D Manning and H Schutze. 1999. Foundations of Statistical Natural Lanplage Pmcessing. MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Mosteller</author>
<author>D L Wallace</author>
</authors>
<title>Applied Bayesian and Classical Inference: The case of the Federahst papers.</title>
<date>1984</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="22090" citStr="Mosteller and Wallace, 1984" startWordPosition="3739" endWordPosition="3742">a time and compared to the rest-of-corpus. Results in Table 6 show that there is a clear distinction beween the C scores for the doctored files, and the exceptional case, compared with the normal ones. However, these experiments with doctored files only show that gross plagiarism can be very easily detected. 6.3 Establishing thresholds The Federalist corpus The Federalist Papers are texts from a completely different domain. They are the celebrated collection of 85 papers, written in 1787-1788, on the proposed American Constitution. They are easily accessible and have been extensively studied (Mosteller and Wallace, 1984). We considered 81 of the papers, written by Madison or Hamilton, totalling 183,372 words. The average paper length is 2,300 words. In this corpus the same subjects are addressed repeatedly. The papers are all written by one of the 2 authors, under the same pseudonym. The threshold for R When we ran our prototype over these papers the maximum R score was 0.03. This confirmed our view that R = 0.03 was a reasonable level above which similar text would be found. If R is reduced, as in some of our investigations into students&apos; work, copying is likely to be found, but some false positives may aris</context>
</contexts>
<marker>Mosteller, Wallace, 1984</marker>
<rawString>F Mosteller and D L Wallace. 1984. Applied Bayesian and Classical Inference: The case of the Federahst papers. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>S Martini</author>
<author>F Wessel</author>
</authors>
<title>Statistical language modelling using leaving-one-out.</title>
<date>1997</date>
<booktitle>In S Young and G Bloothooft, editors, Corpus Based Methods in Language and Speech Processing.</booktitle>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="3217" citStr="Ney et al., 1997" startWordPosition="526" endWordPosition="529">e principles underlying our method, and discusses why word trigrams are suitable features to extract. Section 3 looks at the context of our work from the theoretical point of view, and gives an outline of the metrics that will be employed. Section 4 looks at related work in this field and Section 5 describes the domain in which we develop our prototype. Sections 6 and 7 describe the experiments undertaken and their results. Section 8 concludes the paper. 2 Principle of fingerprint extraction A fundamental issue in language modelling for Automated Speech Recognition is the sparse data problem (Ney et al., 1997, page 176), (Gibbon et al., 1997, page 248). However, this phenomenon can be turned on its head and put to good use to fingerprint text. The principle underlying our system is that the identifying fingerprint associated with a piece of text is based on a large number of small, easily extracted lexical features: word trigrams. Each text is converted into the set of overlapping 3 word sequences of which it is composed. When we consider separate texts on the same subject there will be certain common words, bigrams and trigrams. Compound noun phrases provide typical examples. However, the phenome</context>
</contexts>
<marker>Ney, Martini, Wessel, 1997</marker>
<rawString>H Ney, S Martini, and F Wessel. 1997. Statistical language modelling using leaving-one-out. In S Young and G Bloothooft, editors, Corpus Based Methods in Language and Speech Processing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Heintze</author>
</authors>
<title>Scalable Document Fingeiprinting. Bell Laboratories,</title>
<date>1996</date>
<location>www.es.cmthedu/afs/es/user/nch/www/koala/.</location>
<contexts>
<context position="14874" citStr="Heintze, 1996" startWordPosition="2530" endWordPosition="2531">y will not match at all. Problems can arise when similar chunks get out of phase and appear not to match. Broder (Broder, 1998) has addressed the task of clustering 30,000,000 documents into groups of those that closely resembled each other. His approach was based on the concmt of matching &amp;quot;shingles&amp;quot; , where a shingle is a sequmce of words. The length of the shingle has to be determined To reduce storage demands, only some shingles were selected, by a sampling process. The similarity measures: resemblance and containment were used as defined above. A similar approach is employed by Heintze (N.Heintze, 1996), who has developed a system for analysing up to 1 million documents. He uses character strings rather than word strings as the basis for a fingerprint, and quotes effective lengths of 30 - 45 characters. His work focuses on methods of selection to produce a reduced size fingerprint from the full set. A web-based prototype is available. These systems address copy detection on a very large scale, from 1 million to tens of millions of docummts. As with all problems on this scale, (waluation presents difficulties. For instance, the SCAM system was evaluated just by classifying a sample of 50 text</context>
</contexts>
<marker>Heintze, 1996</marker>
<rawString>N.Heintze, 1996. Scalable Document Fingeiprinting. Bell Laboratories, www.es.cmthedu/afs/es/user/nch/www/koala/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Phelps</author>
<author>Robert Wilensky</author>
</authors>
<title>Robust Hypeilinks: Cheap, Everywhere, Now.</title>
<date>2000</date>
<booktitle>Proc. of Digital Documents and Electronic Publishing, www.es.berkeley.edu/</booktitle>
<pages>phelps/Robust/papers.html.</pages>
<contexts>
<context position="16922" citStr="Phelps and Wilensky, 2000" startWordPosition="2860" endWordPosition="2863">f the traditional feature vector approach described in section 3. This approach has also been used for detecting similar sections of code, or clones, in very large programs. A successful example is a neural processor that detects clones in 26 million lines of code, the product of a telecommunications company (Barton et al., 1995). In this case the feature vector is a heterogenous collection of items, derived from information on keyword frequency counts, numbers of parameters, formatting and other factors. The limited number of features makes a neural approach appropriate. Phelps and Wilensky (Phelps and Wilensky, 2000) have shown that texts can be identified by very small signatures, no more than 5 words long. This is of use in producing robust URLs, so when a web page is moved it can still be traced if the URL is augmented with a content based signature. This however is a different task to copy detection: documents with different signatures can still contain copied material. 5 The TV News corpus The principal corpus we used to develop our prototype was a set of 335 TV news reports, taken over periods from 1999 to 2000. Some statistics on the corpus are given in Table 3. There were usually texts from 3 news</context>
</contexts>
<marker>Phelps, Wilensky, 2000</marker>
<rawString>Thomas Phelps and Robert Wilensky, 2000. Robust Hypeilinks: Cheap, Everywhere, Now. Proc. of Digital Documents and Electronic Publishing, www.es.berkeley.edu/ phelps/Robust/papers.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
</authors>
<title>Prediction and Entropy of Printed English.</title>
<date>1951</date>
<journal>Bell System Technical Journal,</journal>
<pages>50--64</pages>
<contexts>
<context position="4097" citStr="Shannon, 1951" startWordPosition="678" endWordPosition="679">r of small, easily extracted lexical features: word trigrams. Each text is converted into the set of overlapping 3 word sequences of which it is composed. When we consider separate texts on the same subject there will be certain common words, bigrams and trigrams. Compound noun phrases provide typical examples. However, the phenomenon we exploit is the fact that common trigrams constitute a very small proportion of all the trigrams derived from independent texts — see the examples in Table 1 and Table 2. Consider the well documented distribution of single words in English and other languages (Shannon, 1951; Manning and Schutze, 1999). We find a characteristic Zipfian distribution in which a small number of words are used very often, but a significant number are used rarely. In the Brown corpus of A Classrooms have become inoculation centres as health workers try to stop the spread of the disease. More than 1,700 pupils and staff were injected today to combat what&apos;s been described as a public health emergency. This morning children were queuing for injections not lessons at the school at the centre of the outbreak. Health teams have begun immunising 1,700 pupils and staff in an attempt to stop a</context>
</contexts>
<marker>Shannon, 1951</marker>
<rawString>C E Shannon. 1951. Prediction and Entropy of Printed English. Bell System Technical Journal, pages 50-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Shivakumar</author>
<author>H Garcia-Molina</author>
</authors>
<title>Building a scalable and accurate copy detection mechanism.</title>
<date>1996</date>
<booktitle>In Prue. of 3rd Inteiwational Conference on Theory and Practice of Digital Libraries. L Wittgenstein.</booktitle>
<publisher>Blackwell,</publisher>
<contexts>
<context position="15533" citStr="Shivakumar and Garcia-Molina, 1996" startWordPosition="2641" endWordPosition="2644">tem for analysing up to 1 million documents. He uses character strings rather than word strings as the basis for a fingerprint, and quotes effective lengths of 30 - 45 characters. His work focuses on methods of selection to produce a reduced size fingerprint from the full set. A web-based prototype is available. These systems address copy detection on a very large scale, from 1 million to tens of millions of docummts. As with all problems on this scale, (waluation presents difficulties. For instance, the SCAM system was evaluated just by classifying a sample of 50 texts on a subjective basis (Shivakumar and Garcia-Molina, 1996, section 4.4). In all these methods longer chunks, shingles or character strings are a less sensitive tool for detecting similar texts rather than exact copies, as illustrated in Table 2. The insertion, deletion or substitution of a small number of words can undermine the matching process. Using our trigram method underlying similarities can be better detected, and cannot be camouflaged by superficial variations in the text. 4.2 Other related work Hatzivassiloglou et al. (Hatzivassiloglou et al., 1999) have developed a method for detecting semantically similar texts by extracting a heterogeno</context>
</contexts>
<marker>Shivakumar, Garcia-Molina, 1996</marker>
<rawString>N Shivakumar and H Garcia-Molina. 1996. Building a scalable and accurate copy detection mechanism. In Prue. of 3rd Inteiwational Conference on Theory and Practice of Digital Libraries. L Wittgenstein. 1945. Philosphical Investigations. Blackwell, 1992.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>