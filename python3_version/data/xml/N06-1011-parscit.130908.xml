<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026287">
<title confidence="0.9724745">
Named Entity Transliteration and Discovery from Multilingual Comparable
Corpora
</title>
<author confidence="0.998686">
Alexandre Klementiev Dan Roth
</author>
<affiliation confidence="0.999041">
Department of Computer Science
University of Illinois
</affiliation>
<address confidence="0.81332">
Urbana, IL 61801
</address>
<email confidence="0.996985">
klementi,danr @uiuc.edu
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999882095238095">
Named Entity recognition (NER) is an im-
portant part of many natural language pro-
cessing tasks. Most current approaches
employ machine learning techniques and
require supervised data. However, many
languages lack such resources. This paper
presents an algorithm to automatically dis-
cover Named Entities (NEs) in a resource
free language, given a bilingual corpora
in which it is weakly temporally aligned
with a resource rich language. We ob-
serve that NEs have similar time distribu-
tions across such corpora, and that they
are often transliterated, and develop an al-
gorithm that exploits both iteratively. The
algorithm makes use of a new, frequency
based, metric for time distributions and a
resource free discriminative approach to
transliteration. We evaluate the algorithm
on an English-Russian corpus, and show
high level of NEs discovery in Russian.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957142857143">
Named Entity recognition has been getting much
attention in NLP research in recent years, since it
is seen as a significant component of higher level
NLP tasks such as information distillation and ques-
tion answering, and an enabling technology for bet-
ter information access. Most successful approaches
to NER employ machine learning techniques, which
require supervised training data. However, for many
languages, these resources do not exist. Moreover,
it is often difficult to find experts in these languages
both for the expensive annotation effort and even for
language specific clues. On the other hand, compa-
rable multilingual data (such as multilingual news
streams) are increasingly available (see section 4).
In this work, we make two independent observa-
tions about Named Entities encountered in such cor-
pora, and use them to develop an algorithm that ex-
tracts pairs of NEs across languages. Specifically,
given a bilingual corpora that is weakly temporally
aligned, and a capability to annotate the text in one
of the languages with NEs, our algorithm identifies
the corresponding NEs in the second language text,
and annotates them with the appropriate type, as in
the source text.
The first observation is that NEs in one language
in such corpora tend to co-occur with their coun-
terparts in the other. E.g., Figure 1 shows a his-
togram of the number of occurrences of the word
Hussein and its Russian transliteration in our bilin-
gual news corpus spanning years 2001 through late
2005. One can see several common peaks in the two
histograms, largest one being around the time of the
beginning of the war in Iraq. The word Russia, on
the other hand, has a distinctly different temporal
signature. We can exploit such weak synchronicity
of NEs across languages as a way to associate them.
In order to score a pair of entities across languages,
we compute the similarity of their time distributions.
The second observation is that NEs are often
transliterated or have a common etymological origin
across languages, and thus are phonetically similar.
Figure 2 shows an example list of NEs and their pos-
</bodyText>
<page confidence="0.987659">
82
</page>
<note confidence="0.924164">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 82–88,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<figure confidence="0.882984">
01/01/01 10/05/05
Time
</figure>
<figureCaption confidence="0.997938666666667">
Figure 1: Temporal histograms for Hussein (top),
its Russian transliteration (middle), and of the word
Russia (bottom).
</figureCaption>
<bodyText confidence="0.99120812">
sible Russian transliterations.
Approaches that attempt to use these two charac-
teristics separately to identify NEs across languages
would have significant shortcomings. Translitera-
tion based approaches require a good model, typi-
cally handcrafted or trained on a clean set of translit-
eration pairs. On the other hand, time sequence sim-
ilarity based approaches would incorrectly match
words which happen to have similar time signatures
(e.g. Taliban and Afghanistan in recent news).
We introduce an algorithm we call co-ranking
which exploits these observations simultaneously to
match NEs on one side of the bilingual corpus to
their counterparts on the other. We use a Discrete
Fourier Transform (Arfken, 1985) based metric for
computing similarity of time distributions, and we
score NEs similarity with a linear transliteration
model. For a given NE in one language, the translit-
eration model chooses a top ranked list of candidates
in another language. Time sequence scoring is then
used to re-rank the candidates and choose the one
best temporally aligned with the NE. That is, we at-
tempt to choose a candidate which is both a good
transliteration (according to the current model) and
is well aligned with the NE. Finally, pairs of NEs
</bodyText>
<figureCaption confidence="0.970556">
Figure 2: Example English NEs and their transliter-
ated Russian counterparts.
</figureCaption>
<bodyText confidence="0.988332117647059">
and the best candidates are used to iteratively train
the transliteration model.
A major challenge inherent in discovering
transliterated NEs is the fact that a single entity may
be represented by multiple transliteration strings.
One reason is language morphology. For example,
in Russian, depending on a case being used, the
same noun may appear with various endings. An-
other reason is the lack of transliteration standards.
Again, in Russian, several possible transliterations
of an English entity may be acceptable, as long as
they are phonetically similar to the source.
Thus, in order to rely on the time sequences we
obtain, we need to be able to group variants of
the same NE into an equivalence class, and col-
lect their aggregate mention counts. We would then
score time sequences of these equivalence classes.
For instance, we would like to count the aggregate
number of occurrences of Herzegovina, Hercegov-
ina on the English side in order to map it accu-
rately to the equivalence class of that NE’s vari-
ants we may see on the Russian side of our cor-
pus (e.g.
).
One of the objectives for this work was to use as
little of the knowledge of both languages as possible.
In order to effectively rely on the quality of time se-
quence scoring, we used a simple, knowledge poor
approach to group NE variants for Russian.
In the rest of the paper, whenever we refer to a
Named Entity, we imply an NE equivalence class.
Note that although we expect that better use of lan-
guage specific knowledge would improve the re-
sults, it would defeat one of the goals of this work.
</bodyText>
<figure confidence="0.9997811875">
’hussein’ (Russian)
’hussein’ (English)
’russia’ (English)
Number of Occurences 20
15
10
5
2 0
15
10
5
2 0
15
10
5
0
</figure>
<page confidence="0.99274">
83
</page>
<sectionHeader confidence="0.989098" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999987657142857">
There has been other work to automatically discover
NE with minimal supervision. Both (Cucerzan and
Yarowsky, 1999) and (Collins and Singer, 1999)
present algorithms to obtain NEs from untagged cor-
pora. However, they focus on the classification stage
of already segmented entities, and make use of con-
textual and morphological clues that require knowl-
edge of the language beyond the level we want to
assume with respect to the target language.
The use of similarity of time distributions for in-
formation extraction, in general, and NE extraction,
in particular, is not new. (Hetland, 2004) surveys
recent methods for scoring time sequences for sim-
ilarity. (Shinyama and Sekine, 2004) used the idea
to discover NEs, but in a single language, English,
across two news sources.
A large amount of previous work exists on
transliteration models. Most are generative and con-
sider the task of producing an appropriate translit-
eration for a given word, and thus require consid-
erable knowledge of the languages. For example,
(AbdulJaleel and Larkey, 2003; Jung et al., 2000)
train English-Arabic and English-Korean generative
transliteration models, respectively. (Knight and
Graehl, 1997) build a generative model for back-
ward transliteration from Japanese to English.
While generative models are often robust, they
tend to make independence assumptions that do not
hold in data. The discriminative learning framework
argued for in (Roth, 1998; Roth, 1999) as an alter-
native to generative models is now used widely in
NLP, even in the context of word alignment (Taskar
et al., 2005; Moore, 2005). We make use of it here
too, to learn a discriminative transliteration model
that requires little knowledge of the target language.
</bodyText>
<sectionHeader confidence="0.9983405" genericHeader="method">
3 Co-ranking: An Algorithm for NE
Discovery
</sectionHeader>
<bodyText confidence="0.999984538461538">
In essence, the algorithm we present uses temporal
alignment as a supervision signal to iteratively train
a discriminative transliteration model, which can be
viewed as a distance metric between and English NE
and a potential transliteration. On each iteration, it
selects a set of transliteration candidates for each NE
according to the current model (line 6). It then uses
temporal alignment (with thresholding) to select the
best transliteration candidate for the next round of
training (lines 8, and 9).
Once the training is complete, lines 4 through 10
are executed without thresholding for each NE in
to discover its counterpart in .
</bodyText>
<subsectionHeader confidence="0.999365">
3.1 Time Sequence Generation and Matching
</subsectionHeader>
<bodyText confidence="0.991111666666667">
In order to generate time sequence for a word, we
divide the corpus into a sequence of temporal bins,
and count the number of occurrences of the word in
each bin. We then normalize the sequence.
We use a method called the F-index (Hetland,
2004) to implement the similarity function
on line 8 of the algorithm. We first run a Discrete
Fourier Transform on a time sequence to extract its
Fourier expansion coefficients. The score of a pair of
time sequences is then computed as a Euclidian dis-
tance between their expansion coefficient vectors.
.
</bodyText>
<listItem confidence="0.6354245">
Algorithm 1: Co-ranking: an algorithm for it-
erative cross lingual NE discovery.
</listItem>
<subsectionHeader confidence="0.530295">
3.1.1 Equivalence Classes
</subsectionHeader>
<bodyText confidence="0.999976375">
As we mentioned earlier, an NE in one language
may map to multiple morphological variants and
transliterations in another. Identification of the en-
tity’s equivalence class of transliterations is impor-
tant for obtaining its accurate time sequence.
In order to keep to our objective of requiring as lit-
tle language knowledge as possible, we took a rather
simplistic approach to take into account morpholog-
</bodyText>
<figure confidence="0.9491825">
Input: Bilingual, comparable corpus ( ,☞), set of
named entities from , threshold
Output: Transliteration model
1 Initialize ;
3 repeat
4 ;
5 for each do
6 Use to collect a set of candidates
with high transliteration scores;
7 collect time distribution ;
8 Select candidate with the best
;
9 if exceeds , add tuple to ;
10 end
11 Use to train ;
12 until D stops changing between iterations ;
, collect time distribution ;
2
</figure>
<page confidence="0.990851">
84
</page>
<bodyText confidence="0.999745777777778">
ical ambiguities of NEs in Russian. Two words were
considered variants of the same NE if they share a
prefix of size five or longer. At this point, our al-
gorithm takes a simplistic approach also for the En-
glish side of the corpus – each unique word had its
own equivalence class although, in principle, we can
incorporate works such as (Li et al., 2004) into the
algorithm. A cumulative distribution was then col-
lected for such equivalence classes.
</bodyText>
<subsectionHeader confidence="0.979799">
3.2 Transliteration Model
</subsectionHeader>
<bodyText confidence="0.999699236842105">
Unlike most of the previous work to transliteration,
that consider generative transliteration models, we
take a discriminative approach. We train a linear
model to decide whether a word is a translit-
eration of an NE . The words in the pair
are partitioned into a set of substrings and
up to a particular length (including the empty string
). Couplings of the substrings from both
sets produce features we use for training. Note
that couplings with the empty string represent inser-
tions/omissions.
Consider the following example: ( ,)
We build a feature vector by coupling sub-
strings from the two sets:
We use the observation that transliteration tends
to preserve phonetic sequence to limit the number
of couplings. For example, we can disallow the
coupling of substrings whose starting positions are
too far apart: thus, we might not consider a pair-
ing in the above example. In our experi-
ments, we paired substrings if their positions in their
respective words differed by -1, 0, or 1.
We use the perceptron (Rosenblatt, 1958) algo-
rithm to train the model. The model activation pro-
vides the score we use to select best transliterations
on line 6. Our version of perceptron takes exam-
ples with a variable number of features; each ex-
ample is a set of all features seen so far that are
active in the input. As the iterative algorithm ob-
serves more data, it discovers and makes use of more
features. This model is called the infinite attribute
model (Blum, 1992) and it follows the perceptron
version in SNoW (Roth, 1998).
Positive examples used for iterative training are
pairs of NEs and their best temporally aligned
(thresholded) transliteration candidates. Negative
examples are English non-NEs paired with random
Russian words.
</bodyText>
<sectionHeader confidence="0.994889" genericHeader="method">
4 Experimental Study
</sectionHeader>
<bodyText confidence="0.99648545">
We ran experiments using a bilingual comparable
English-Russian news corpus we built by crawling
a Russian news web site (www.lenta.ru).
The site provides loose translations of (and
pointers to) the original English texts. We col-
lected pairs of articles spanning from 1/1/2001
through 12/24/2004. The corpus consists of
2,022 documents with 0-8 documents per day.
The corpus is available on our web page at
http://L2R.cs.uiuc.edu/ cogcomp/.
The English side was tagged with a publicly
available NER system based on the SNoW learning
architecture (Roth, 1998), that is available at the
same site. This set of English NEs was hand-pruned
to remove incorrectly classified words to obtain 978
single word NEs.
In order to reduce running time, some limited
preprocessing was done on the Russian side. All
classes, whose temporal distributions were close
to uniform (i.e. words with a similar likelihood
of occurrence throughout the corpus) were deemed
common and not considered as NE candidates.
Unique words were grouped into 15,594 equivalence
classes, and 1,605 of those classes were discarded
using this method.
Insertions/omissions features were not used in the
experiments as they provided no tangible benefit for
the languages of our corpus.
Unless mentioned otherwise, the transliteration
model was initialized with a subset of 254 pairs
of NEs and their transliteration equivalence classes.
Negative examples here and during the rest of the
training were pairs of randomly selected non-NE
English and Russian words.
In each iteration, we used the current transliter-
=
(powell, pauel). We build a feature vector from this
example in the following manner:
First, we split both words into all possible sub-
strings of up to size two:
</bodyText>
<page confidence="0.996047">
85
</page>
<figure confidence="0.9900905">
0 1 2 3 4 5 6 7 8 9
Iteration
</figure>
<figureCaption confidence="0.9992845">
Figure 3: Proportion of correctly discovered NE
pairs vs. iteration. Complete algorithm outperforms
both transliteration model and temporal sequence
matching when used on their own.
</figureCaption>
<figure confidence="0.987601">
0 1 2 3 4 5 6 7 8 9
Iteration
</figure>
<figureCaption confidence="0.996282">
Figure 5: Proportion of the correctly discovered NE
pairs for various initial example set sizes. Decreas-
ing the size does not have a significant effect of the
performance on later iterations.
</figureCaption>
<figure confidence="0.9969755">
40
20
80
70
60
50
30
Complete Algorithm
Transliteration Model Only
Sequence Only
40
70
60
50
30
254 examples
127 examples
85 examples
Accuracy (%)
Accuracy (%)
</figure>
<bodyText confidence="0.9991806875">
ation model to find a list of 30 best transliteration
equivalence classes for each NE. We then computed
time sequence similarity score between NE and each
class from its list to find the one with the best match-
ing time sequence. If its similarity score surpassed
a set threshold, it was added to the list of positive
examples for the next round of training. Positive ex-
amples were constructed by pairing each English NE
with each of the transliterations from the best equiv-
alence class that surpasses the threshold. We used
the same number of positive and negative examples.
For evaluation, random 727 of the total of 978 NE
pairs matched by the algorithm were selected and
checked by a language expert. Accuracy was com-
puted as the percentage of those NEs correctly dis-
covered by the algorithm.
</bodyText>
<subsectionHeader confidence="0.996425">
4.1 NE Discovery
</subsectionHeader>
<bodyText confidence="0.999919">
Figure 3 shows the proportion of correctly discov-
ered NE transliteration equivalence classes through-
out the run of the algorithm. The figure also shows
the accuracy if transliterations are selected accord-
ing to the current transliteration model (top scor-
ing candidate) and sequence matching alone. The
transliteration model alone achieves an accuracy of
about 47%, while the time sequence alone gets about
41%. The combined algorithm achieves about 66%,
giving a significant improvement.
In order to understand what happens to the
transliteration model as the algorithm proceeds, let
us consider the following example. Figure 4 shows
parts of transliteration lists for NE forsyth for two
iterations of the algorithm. The weak transliteration
model selects the correct transliteration (italicized)
as the 24th best transliteration in the first iteration.
Time sequence scoring function chooses it to be one
of the training examples for the next round of train-
ing of the model. By the eighth iteration, the model
has improved to select it as a best transliteration.
Not all correct transliterations make it to the top of
the candidates list (transliteration model by itself is
never as accurate as the complete algorithm on Fig-
ure 3). That is not required, however, as the model
only needs to be good enough to place the correct
transliteration anywhere in the candidate list.
Not surprisingly, some of the top transliteration
candidates start sounding like the NE itself, as train-
ing progresses. On Figure 4, candidates for forsyth
on iteration 7 include fross and fossett.
</bodyText>
<page confidence="0.996941">
86
</page>
<figureCaption confidence="0.9994105">
Figure 4: Transliteration lists for forsyth for two iterations of the algorithm ranked by the current transliter-
ation model. As the model improves, the correct transliteration moves up the list.
</figureCaption>
<subsectionHeader confidence="0.999214">
4.2 Rate of Improvement vs. Initial Example
Set Size
</subsectionHeader>
<bodyText confidence="0.999967363636364">
We ran a series of experiments to see how the size
of the initial training set affects the accuracy of the
model as training progresses (Figure 5). Although
the performance of the early iterations is signifi-
cantly affected by the size of the initial training ex-
ample set, the algorithm quickly improves its perfor-
mance. As we decrease the size from 254, to 127, to
85 examples, the accuracy of the first iteration drops
by roughly 10% each time. However, starting at the
6th iteration, the three are with 3% of one another.
These numbers suggest that we only need a few
initial positive examples to bootstrap the translitera-
tion model. The intuition is the following: the few
examples in the initial training set produce features
corresponding to substring pairs characteristic for
English-Russian transliterations. Model trained on
these (few) examples chooses other transliterations
containing these same substring pairs. In turn, the
chosen positive examples contain other characteris-
tic substring pairs, which will be used by the model
to select more positive examples on the next round,
and so on.
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="method">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9986435">
We have proposed a novel algorithm for cross lin-
gual NE discovery in a bilingual weakly temporally
aligned corpus. We have demonstrated that using
two independent sources of information (transliter-
ation and temporal similarity) together to guide NE
extraction gives better performance than using either
of them alone (see Figure 3).
We developed a linear discriminative translitera-
</bodyText>
<figureCaption confidence="0.9815665">
Figure 6: Example of correct transliterations discov-
ered by the algorithm.
</figureCaption>
<bodyText confidence="0.999817055555556">
tion model, and presented a method to automatically
generate features. For time sequence matching, we
used a scoring metric novel in this domain. As sup-
ported by our own experiments, this method outper-
forms other scoring metrics traditionally used (such
as cosine (Salton and McGill, 1986)) when corpora
are not well temporally aligned.
In keeping with our objective to provide as lit-
tle language knowledge as possible, we introduced
a simplistic approach to identifying transliteration
equivalence classes, which sometimes produced er-
roneous groupings (e.g. an equivalence class for
NE lincoln in Russian included both lincoln and lin-
colnshire on Figure 6). This approach is specific
to Russian morphology, and would have to be al-
tered for other languages. For example, for Arabic,
a small set of prefixes can be used to group most NE
variants. We expect that language specific knowl-
</bodyText>
<page confidence="0.996421">
87
</page>
<bodyText confidence="0.8470595">
edge used to discover accurate equivalence classes
would result in performance improvements.
Michael Collins and Yoram Singer. 1999. Unsupervised mod-
els for named entity classification. In Proc. of the Confer-
ence on Empirical Methods for Natural Language Process-
ing (EMNLP).
</bodyText>
<sectionHeader confidence="0.97986" genericHeader="discussions">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999985695652174">
In this work, we only consider single word Named
Entities. A subject of future work is to extend the
algorithm to the multi-word setting. Many of the
multi-word NEs are translated as well as transliter-
ated. For example, Mount in Mount Rainier will
probably be translated, and Rainier - transliterated.
If a dictionary exists for the two languages, it can be
consulted first, and, if a match is found, translitera-
tion model can be bypassed.
The algorithm can be naturally extended to com-
parable corpora of more than two languages. Pair-
wise time sequence scoring and transliteration mod-
els should give better confidence in NE matches.
It seems plausible to suppose that phonetic fea-
tures (if available) would help learning our translit-
eration model. We would like to verify if this is in-
deed the case.
The ultimate goal of this work is to automatically
tag NEs so that they can be used for training of an
NER system for a new language. To this end, we
would like to compare the performance of an NER
system trained on a corpus tagged using this ap-
proach to one trained on a hand-tagged corpus.
</bodyText>
<sectionHeader confidence="0.999308" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99926775">
We thank Richard Sproat, ChengXiang Zhai, and
Kevin Small for their useful feedback during this
work, and the anonymous referees for their help-
ful comments. This research is supported by
the Advanced Research and Development Activity
(ARDA)’s Advanced Question Answering for Intel-
ligence (AQUAINT) Program and a DOI grant under
the Reflex program.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999787884615384">
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statistical
transliteration for english-arabic cross language information
retrieval. In Proceedings of CIKM, pages 139–146, New
York, NY, USA.
George Arfken. 1985. Mathematical Methods for Physicists.
Academic Press.
Avrim Blum. 1992. Learning boolean functions in an infinite
attribute space. Machine Learning, 9(4):373–386.
Silviu Cucerzan and David Yarowsky. 1999. Language in-
dependent named entity recognition combining morpholog-
ical and contextual evidence. In Proc. of the Conference
on Empirical Methods for Natural Language Processing
(EMNLP).
Magnus Lie Hetland, 2004. Data Mining in Time Series
Databases, chapter A Survey of Recent Methods for Effi-
cient Retrieval of Similar Time Sequences. World Scientific.
Sung Young Jung, SungLim Hong, and Eunok Paek. 2000. An
english to korean transliteration model of extended markov
window. In Proc. the International Conference on Compu-
tational Linguistics (COLING), pages 383–389.
Kevin Knight and Jonathan Graehl. 1997. Machine translitera-
tion. In Proc. of the Meeting of the European Association of
Computational Linguistics, pages 128–135.
Xin Li, Paul Morie, and Dan Roth. 2004. Identification and
tracing of ambiguous names: Discriminative and generative
approaches. In Proceedings of the National Conference on
Artificial Intelligence (AAAI), pages 419–424.
Robert C. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In Proc. of the Conference on Empir-
ical Methods for Natural Language Processing (EMNLP),
pages 81–88.
Frank Rosenblatt. 1958. The perceptron: A probabilistic model
for information storage and organization in the brain. Psy-
chological Review, 65.
Dan Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proceedings of the Na-
tional Conference on Artificial Intelligence (AAAI), pages
806–813.
Dan Roth. 1999. Learning in natural language. In Proc. of
the International Joint Conference on Artificial Intelligence
(IJCAI), pages 898–904.
Gerard Salton and Michael J. McGill. 1986. Introduction to
Modern Information Retrieval. McGraw-Hill, Inc., New
York, NY, USA.
Yusuke Shinyama and Satoshi Sekine. 2004. Named entity dis-
covery using comparable news articles. In Proc. the Interna-
tional Conference on Computational Linguistics (COLING),
pages 848–853.
Ben Taskar, Simon Lacoste-Julien, and Michael Jordan. 2005.
Structured prediction via the extragradient method. In The
Conference on Advances in Neural Information Processing
Systems (NIPS). MIT Press.
</reference>
<page confidence="0.999406">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.893124">
<title confidence="0.96466">Named Entity Transliteration and Discovery from Multilingual Comparable Corpora</title>
<author confidence="0.999865">Alexandre Klementiev Dan Roth</author>
<affiliation confidence="0.999909">Department of Computer University of</affiliation>
<address confidence="0.977439">Urbana, IL</address>
<email confidence="0.994703">klementi,danr@uiuc.edu</email>
<abstract confidence="0.999355636363636">Named Entity recognition (NER) is an important part of many natural language processing tasks. Most current approaches employ machine learning techniques and require supervised data. However, many languages lack such resources. This paper presents an algorithm to automatically discover Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language. We observe that NEs have similar time distributions across such corpora, and that they are often transliterated, and develop an algorithm that exploits both iteratively. The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration. We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nasreen AbdulJaleel</author>
<author>Leah S Larkey</author>
</authors>
<title>Statistical transliteration for english-arabic cross language information retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>139--146</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="7588" citStr="AbdulJaleel and Larkey, 2003" startWordPosition="1227" endWordPosition="1230">ct to the target language. The use of similarity of time distributions for information extraction, in general, and NE extraction, in particular, is not new. (Hetland, 2004) surveys recent methods for scoring time sequences for similarity. (Shinyama and Sekine, 2004) used the idea to discover NEs, but in a single language, English, across two news sources. A large amount of previous work exists on transliteration models. Most are generative and consider the task of producing an appropriate transliteration for a given word, and thus require considerable knowledge of the languages. For example, (AbdulJaleel and Larkey, 2003; Jung et al., 2000) train English-Arabic and English-Korean generative transliteration models, respectively. (Knight and Graehl, 1997) build a generative model for backward transliteration from Japanese to English. While generative models are often robust, they tend to make independence assumptions that do not hold in data. The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005). We make use of it here too, to learn a discriminative tr</context>
</contexts>
<marker>AbdulJaleel, Larkey, 2003</marker>
<rawString>Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statistical transliteration for english-arabic cross language information retrieval. In Proceedings of CIKM, pages 139–146, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Arfken</author>
</authors>
<title>Mathematical Methods for Physicists.</title>
<date>1985</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="4230" citStr="Arfken, 1985" startWordPosition="658" endWordPosition="659">y to identify NEs across languages would have significant shortcomings. Transliteration based approaches require a good model, typically handcrafted or trained on a clean set of transliteration pairs. On the other hand, time sequence similarity based approaches would incorrectly match words which happen to have similar time signatures (e.g. Taliban and Afghanistan in recent news). We introduce an algorithm we call co-ranking which exploits these observations simultaneously to match NEs on one side of the bilingual corpus to their counterparts on the other. We use a Discrete Fourier Transform (Arfken, 1985) based metric for computing similarity of time distributions, and we score NEs similarity with a linear transliteration model. For a given NE in one language, the transliteration model chooses a top ranked list of candidates in another language. Time sequence scoring is then used to re-rank the candidates and choose the one best temporally aligned with the NE. That is, we attempt to choose a candidate which is both a good transliteration (according to the current model) and is well aligned with the NE. Finally, pairs of NEs Figure 2: Example English NEs and their transliterated Russian counter</context>
</contexts>
<marker>Arfken, 1985</marker>
<rawString>George Arfken. 1985. Mathematical Methods for Physicists. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
</authors>
<title>Learning boolean functions in an infinite attribute space.</title>
<date>1992</date>
<booktitle>Machine Learning,</booktitle>
<volume>9</volume>
<issue>4</issue>
<contexts>
<context position="12404" citStr="Blum, 1992" startWordPosition="2037" endWordPosition="2038">iring in the above example. In our experiments, we paired substrings if their positions in their respective words differed by -1, 0, or 1. We use the perceptron (Rosenblatt, 1958) algorithm to train the model. The model activation provides the score we use to select best transliterations on line 6. Our version of perceptron takes examples with a variable number of features; each example is a set of all features seen so far that are active in the input. As the iterative algorithm observes more data, it discovers and makes use of more features. This model is called the infinite attribute model (Blum, 1992) and it follows the perceptron version in SNoW (Roth, 1998). Positive examples used for iterative training are pairs of NEs and their best temporally aligned (thresholded) transliteration candidates. Negative examples are English non-NEs paired with random Russian words. 4 Experimental Study We ran experiments using a bilingual comparable English-Russian news corpus we built by crawling a Russian news web site (www.lenta.ru). The site provides loose translations of (and pointers to) the original English texts. We collected pairs of articles spanning from 1/1/2001 through 12/24/2004. The corpus</context>
</contexts>
<marker>Blum, 1992</marker>
<rawString>Avrim Blum. 1992. Learning boolean functions in an infinite attribute space. Machine Learning, 9(4):373–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>David Yarowsky</author>
</authors>
<title>Language independent named entity recognition combining morphological and contextual evidence.</title>
<date>1999</date>
<booktitle>In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="6658" citStr="Cucerzan and Yarowsky, 1999" startWordPosition="1077" endWordPosition="1080">ctively rely on the quality of time sequence scoring, we used a simple, knowledge poor approach to group NE variants for Russian. In the rest of the paper, whenever we refer to a Named Entity, we imply an NE equivalence class. Note that although we expect that better use of language specific knowledge would improve the results, it would defeat one of the goals of this work. ’hussein’ (Russian) ’hussein’ (English) ’russia’ (English) Number of Occurences 20 15 10 5 2 0 15 10 5 2 0 15 10 5 0 83 2 Previous Work There has been other work to automatically discover NE with minimal supervision. Both (Cucerzan and Yarowsky, 1999) and (Collins and Singer, 1999) present algorithms to obtain NEs from untagged corpora. However, they focus on the classification stage of already segmented entities, and make use of contextual and morphological clues that require knowledge of the language beyond the level we want to assume with respect to the target language. The use of similarity of time distributions for information extraction, in general, and NE extraction, in particular, is not new. (Hetland, 2004) surveys recent methods for scoring time sequences for similarity. (Shinyama and Sekine, 2004) used the idea to discover NEs, </context>
</contexts>
<marker>Cucerzan, Yarowsky, 1999</marker>
<rawString>Silviu Cucerzan and David Yarowsky. 1999. Language independent named entity recognition combining morphological and contextual evidence. In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Lie Hetland</author>
</authors>
<title>Data Mining in Time Series Databases, chapter A Survey of Recent Methods for Efficient Retrieval of Similar Time Sequences. World Scientific.</title>
<date>2004</date>
<contexts>
<context position="7132" citStr="Hetland, 2004" startWordPosition="1156" endWordPosition="1157">0 5 0 83 2 Previous Work There has been other work to automatically discover NE with minimal supervision. Both (Cucerzan and Yarowsky, 1999) and (Collins and Singer, 1999) present algorithms to obtain NEs from untagged corpora. However, they focus on the classification stage of already segmented entities, and make use of contextual and morphological clues that require knowledge of the language beyond the level we want to assume with respect to the target language. The use of similarity of time distributions for information extraction, in general, and NE extraction, in particular, is not new. (Hetland, 2004) surveys recent methods for scoring time sequences for similarity. (Shinyama and Sekine, 2004) used the idea to discover NEs, but in a single language, English, across two news sources. A large amount of previous work exists on transliteration models. Most are generative and consider the task of producing an appropriate transliteration for a given word, and thus require considerable knowledge of the languages. For example, (AbdulJaleel and Larkey, 2003; Jung et al., 2000) train English-Arabic and English-Korean generative transliteration models, respectively. (Knight and Graehl, 1997) build a </context>
<context position="9234" citStr="Hetland, 2004" startWordPosition="1493" endWordPosition="1494">each NE according to the current model (line 6). It then uses temporal alignment (with thresholding) to select the best transliteration candidate for the next round of training (lines 8, and 9). Once the training is complete, lines 4 through 10 are executed without thresholding for each NE in to discover its counterpart in . 3.1 Time Sequence Generation and Matching In order to generate time sequence for a word, we divide the corpus into a sequence of temporal bins, and count the number of occurrences of the word in each bin. We then normalize the sequence. We use a method called the F-index (Hetland, 2004) to implement the similarity function on line 8 of the algorithm. We first run a Discrete Fourier Transform on a time sequence to extract its Fourier expansion coefficients. The score of a pair of time sequences is then computed as a Euclidian distance between their expansion coefficient vectors. . Algorithm 1: Co-ranking: an algorithm for iterative cross lingual NE discovery. 3.1.1 Equivalence Classes As we mentioned earlier, an NE in one language may map to multiple morphological variants and transliterations in another. Identification of the entity’s equivalence class of transliterations is</context>
</contexts>
<marker>Hetland, 2004</marker>
<rawString>Magnus Lie Hetland, 2004. Data Mining in Time Series Databases, chapter A Survey of Recent Methods for Efficient Retrieval of Similar Time Sequences. World Scientific.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung Young Jung</author>
<author>SungLim Hong</author>
<author>Eunok Paek</author>
</authors>
<title>An english to korean transliteration model of extended markov window.</title>
<date>2000</date>
<booktitle>In Proc. the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>383--389</pages>
<contexts>
<context position="7608" citStr="Jung et al., 2000" startWordPosition="1231" endWordPosition="1234"> use of similarity of time distributions for information extraction, in general, and NE extraction, in particular, is not new. (Hetland, 2004) surveys recent methods for scoring time sequences for similarity. (Shinyama and Sekine, 2004) used the idea to discover NEs, but in a single language, English, across two news sources. A large amount of previous work exists on transliteration models. Most are generative and consider the task of producing an appropriate transliteration for a given word, and thus require considerable knowledge of the languages. For example, (AbdulJaleel and Larkey, 2003; Jung et al., 2000) train English-Arabic and English-Korean generative transliteration models, respectively. (Knight and Graehl, 1997) build a generative model for backward transliteration from Japanese to English. While generative models are often robust, they tend to make independence assumptions that do not hold in data. The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005). We make use of it here too, to learn a discriminative transliteration model </context>
</contexts>
<marker>Jung, Hong, Paek, 2000</marker>
<rawString>Sung Young Jung, SungLim Hong, and Eunok Paek. 2000. An english to korean transliteration model of extended markov window. In Proc. the International Conference on Computational Linguistics (COLING), pages 383–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<title>Machine transliteration.</title>
<date>1997</date>
<booktitle>In Proc. of the Meeting of the European Association of Computational Linguistics,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="7723" citStr="Knight and Graehl, 1997" startWordPosition="1243" endWordPosition="1246">ular, is not new. (Hetland, 2004) surveys recent methods for scoring time sequences for similarity. (Shinyama and Sekine, 2004) used the idea to discover NEs, but in a single language, English, across two news sources. A large amount of previous work exists on transliteration models. Most are generative and consider the task of producing an appropriate transliteration for a given word, and thus require considerable knowledge of the languages. For example, (AbdulJaleel and Larkey, 2003; Jung et al., 2000) train English-Arabic and English-Korean generative transliteration models, respectively. (Knight and Graehl, 1997) build a generative model for backward transliteration from Japanese to English. While generative models are often robust, they tend to make independence assumptions that do not hold in data. The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005). We make use of it here too, to learn a discriminative transliteration model that requires little knowledge of the target language. 3 Co-ranking: An Algorithm for NE Discovery In essence, the </context>
</contexts>
<marker>Knight, Graehl, 1997</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1997. Machine transliteration. In Proc. of the Meeting of the European Association of Computational Linguistics, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Paul Morie</author>
<author>Dan Roth</author>
</authors>
<title>Identification and tracing of ambiguous names: Discriminative and generative approaches.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>419--424</pages>
<contexts>
<context position="10823" citStr="Li et al., 2004" startWordPosition="1767" endWordPosition="1770">ollect a set of candidates with high transliteration scores; 7 collect time distribution ; 8 Select candidate with the best ; 9 if exceeds , add tuple to ; 10 end 11 Use to train ; 12 until D stops changing between iterations ; , collect time distribution ; 2 84 ical ambiguities of NEs in Russian. Two words were considered variants of the same NE if they share a prefix of size five or longer. At this point, our algorithm takes a simplistic approach also for the English side of the corpus – each unique word had its own equivalence class although, in principle, we can incorporate works such as (Li et al., 2004) into the algorithm. A cumulative distribution was then collected for such equivalence classes. 3.2 Transliteration Model Unlike most of the previous work to transliteration, that consider generative transliteration models, we take a discriminative approach. We train a linear model to decide whether a word is a transliteration of an NE . The words in the pair are partitioned into a set of substrings and up to a particular length (including the empty string ). Couplings of the substrings from both sets produce features we use for training. Note that couplings with the empty string represent ins</context>
</contexts>
<marker>Li, Morie, Roth, 2004</marker>
<rawString>Xin Li, Paul Morie, and Dan Roth. 2004. Identification and tracing of ambiguous names: Discriminative and generative approaches. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 419–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<booktitle>In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>81--88</pages>
<contexts>
<context position="8130" citStr="Moore, 2005" startWordPosition="1312" endWordPosition="1313">knowledge of the languages. For example, (AbdulJaleel and Larkey, 2003; Jung et al., 2000) train English-Arabic and English-Korean generative transliteration models, respectively. (Knight and Graehl, 1997) build a generative model for backward transliteration from Japanese to English. While generative models are often robust, they tend to make independence assumptions that do not hold in data. The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005). We make use of it here too, to learn a discriminative transliteration model that requires little knowledge of the target language. 3 Co-ranking: An Algorithm for NE Discovery In essence, the algorithm we present uses temporal alignment as a supervision signal to iteratively train a discriminative transliteration model, which can be viewed as a distance metric between and English NE and a potential transliteration. On each iteration, it selects a set of transliteration candidates for each NE according to the current model (line 6). It then uses temporal alignment (with thresholding) to select</context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert C. Moore. 2005. A discriminative framework for bilingual word alignment. In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Rosenblatt</author>
</authors>
<title>The perceptron: A probabilistic model for information storage and organization in the brain.</title>
<date>1958</date>
<journal>Psychological Review,</journal>
<volume>65</volume>
<contexts>
<context position="11972" citStr="Rosenblatt, 1958" startWordPosition="1958" endWordPosition="1959">or training. Note that couplings with the empty string represent insertions/omissions. Consider the following example: ( ,) We build a feature vector by coupling substrings from the two sets: We use the observation that transliteration tends to preserve phonetic sequence to limit the number of couplings. For example, we can disallow the coupling of substrings whose starting positions are too far apart: thus, we might not consider a pairing in the above example. In our experiments, we paired substrings if their positions in their respective words differed by -1, 0, or 1. We use the perceptron (Rosenblatt, 1958) algorithm to train the model. The model activation provides the score we use to select best transliterations on line 6. Our version of perceptron takes examples with a variable number of features; each example is a set of all features seen so far that are active in the input. As the iterative algorithm observes more data, it discovers and makes use of more features. This model is called the infinite attribute model (Blum, 1992) and it follows the perceptron version in SNoW (Roth, 1998). Positive examples used for iterative training are pairs of NEs and their best temporally aligned (threshold</context>
</contexts>
<marker>Rosenblatt, 1958</marker>
<rawString>Frank Rosenblatt. 1958. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
</authors>
<title>Learning to resolve natural language ambiguities: A unified approach.</title>
<date>1998</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>806--813</pages>
<contexts>
<context position="7978" citStr="Roth, 1998" startWordPosition="1284" endWordPosition="1285">eration models. Most are generative and consider the task of producing an appropriate transliteration for a given word, and thus require considerable knowledge of the languages. For example, (AbdulJaleel and Larkey, 2003; Jung et al., 2000) train English-Arabic and English-Korean generative transliteration models, respectively. (Knight and Graehl, 1997) build a generative model for backward transliteration from Japanese to English. While generative models are often robust, they tend to make independence assumptions that do not hold in data. The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005). We make use of it here too, to learn a discriminative transliteration model that requires little knowledge of the target language. 3 Co-ranking: An Algorithm for NE Discovery In essence, the algorithm we present uses temporal alignment as a supervision signal to iteratively train a discriminative transliteration model, which can be viewed as a distance metric between and English NE and a potential transliteration. On each iteration, it select</context>
<context position="12463" citStr="Roth, 1998" startWordPosition="2047" endWordPosition="2048">ubstrings if their positions in their respective words differed by -1, 0, or 1. We use the perceptron (Rosenblatt, 1958) algorithm to train the model. The model activation provides the score we use to select best transliterations on line 6. Our version of perceptron takes examples with a variable number of features; each example is a set of all features seen so far that are active in the input. As the iterative algorithm observes more data, it discovers and makes use of more features. This model is called the infinite attribute model (Blum, 1992) and it follows the perceptron version in SNoW (Roth, 1998). Positive examples used for iterative training are pairs of NEs and their best temporally aligned (thresholded) transliteration candidates. Negative examples are English non-NEs paired with random Russian words. 4 Experimental Study We ran experiments using a bilingual comparable English-Russian news corpus we built by crawling a Russian news web site (www.lenta.ru). The site provides loose translations of (and pointers to) the original English texts. We collected pairs of articles spanning from 1/1/2001 through 12/24/2004. The corpus consists of 2,022 documents with 0-8 documents per day. Th</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>Dan Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 806–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
</authors>
<title>Learning in natural language.</title>
<date>1999</date>
<booktitle>In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>898--904</pages>
<contexts>
<context position="7991" citStr="Roth, 1999" startWordPosition="1286" endWordPosition="1287">ls. Most are generative and consider the task of producing an appropriate transliteration for a given word, and thus require considerable knowledge of the languages. For example, (AbdulJaleel and Larkey, 2003; Jung et al., 2000) train English-Arabic and English-Korean generative transliteration models, respectively. (Knight and Graehl, 1997) build a generative model for backward transliteration from Japanese to English. While generative models are often robust, they tend to make independence assumptions that do not hold in data. The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005). We make use of it here too, to learn a discriminative transliteration model that requires little knowledge of the target language. 3 Co-ranking: An Algorithm for NE Discovery In essence, the algorithm we present uses temporal alignment as a supervision signal to iteratively train a discriminative transliteration model, which can be viewed as a distance metric between and English NE and a potential transliteration. On each iteration, it selects a set of tr</context>
</contexts>
<marker>Roth, 1999</marker>
<rawString>Dan Roth. 1999. Learning in natural language. In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI), pages 898–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1986</date>
<publisher>McGraw-Hill, Inc.,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="19525" citStr="Salton and McGill, 1986" startWordPosition="3180" endWordPosition="3183">e demonstrated that using two independent sources of information (transliteration and temporal similarity) together to guide NE extraction gives better performance than using either of them alone (see Figure 3). We developed a linear discriminative transliteraFigure 6: Example of correct transliterations discovered by the algorithm. tion model, and presented a method to automatically generate features. For time sequence matching, we used a scoring metric novel in this domain. As supported by our own experiments, this method outperforms other scoring metrics traditionally used (such as cosine (Salton and McGill, 1986)) when corpora are not well temporally aligned. In keeping with our objective to provide as little language knowledge as possible, we introduced a simplistic approach to identifying transliteration equivalence classes, which sometimes produced erroneous groupings (e.g. an equivalence class for NE lincoln in Russian included both lincoln and lincolnshire on Figure 6). This approach is specific to Russian morphology, and would have to be altered for other languages. For example, for Arabic, a small set of prefixes can be used to group most NE variants. We expect that language specific knowl87 ed</context>
</contexts>
<marker>Salton, McGill, 1986</marker>
<rawString>Gerard Salton and Michael J. McGill. 1986. Introduction to Modern Information Retrieval. McGraw-Hill, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Named entity discovery using comparable news articles.</title>
<date>2004</date>
<booktitle>In Proc. the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>848--853</pages>
<contexts>
<context position="7226" citStr="Shinyama and Sekine, 2004" startWordPosition="1168" endWordPosition="1171">th minimal supervision. Both (Cucerzan and Yarowsky, 1999) and (Collins and Singer, 1999) present algorithms to obtain NEs from untagged corpora. However, they focus on the classification stage of already segmented entities, and make use of contextual and morphological clues that require knowledge of the language beyond the level we want to assume with respect to the target language. The use of similarity of time distributions for information extraction, in general, and NE extraction, in particular, is not new. (Hetland, 2004) surveys recent methods for scoring time sequences for similarity. (Shinyama and Sekine, 2004) used the idea to discover NEs, but in a single language, English, across two news sources. A large amount of previous work exists on transliteration models. Most are generative and consider the task of producing an appropriate transliteration for a given word, and thus require considerable knowledge of the languages. For example, (AbdulJaleel and Larkey, 2003; Jung et al., 2000) train English-Arabic and English-Korean generative transliteration models, respectively. (Knight and Graehl, 1997) build a generative model for backward transliteration from Japanese to English. While generative model</context>
</contexts>
<marker>Shinyama, Sekine, 2004</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2004. Named entity discovery using comparable news articles. In Proc. the International Conference on Computational Linguistics (COLING), pages 848–853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Michael Jordan</author>
</authors>
<title>Structured prediction via the extragradient method.</title>
<date>2005</date>
<booktitle>In The Conference on Advances in Neural Information Processing Systems (NIPS).</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8116" citStr="Taskar et al., 2005" startWordPosition="1308" endWordPosition="1311">require considerable knowledge of the languages. For example, (AbdulJaleel and Larkey, 2003; Jung et al., 2000) train English-Arabic and English-Korean generative transliteration models, respectively. (Knight and Graehl, 1997) build a generative model for backward transliteration from Japanese to English. While generative models are often robust, they tend to make independence assumptions that do not hold in data. The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005). We make use of it here too, to learn a discriminative transliteration model that requires little knowledge of the target language. 3 Co-ranking: An Algorithm for NE Discovery In essence, the algorithm we present uses temporal alignment as a supervision signal to iteratively train a discriminative transliteration model, which can be viewed as a distance metric between and English NE and a potential transliteration. On each iteration, it selects a set of transliteration candidates for each NE according to the current model (line 6). It then uses temporal alignment (with threshold</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Jordan, 2005</marker>
<rawString>Ben Taskar, Simon Lacoste-Julien, and Michael Jordan. 2005. Structured prediction via the extragradient method. In The Conference on Advances in Neural Information Processing Systems (NIPS). MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>