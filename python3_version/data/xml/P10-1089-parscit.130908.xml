<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002403">
<title confidence="0.985891">
Creating Robust Supervised Classifiers via Web-Scale N-gram Data
</title>
<author confidence="0.99851">
Shane Bergsma Emily Pitler Dekang Lin
</author>
<affiliation confidence="0.999894">
University of Alberta University of Pennsylvania Google, Inc.
</affiliation>
<email confidence="0.986724">
sbergsma@ualberta.ca epitler@seas.upenn.edu lindek@google.com
</email>
<sectionHeader confidence="0.993681" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956882352941">
In this paper, we systematically assess the
value of using web-scale N-gram data in
state-of-the-art supervised NLP classifiers.
We compare classifiers that include or ex-
clude features for the counts of various
N-grams, where the counts are obtained
from a web-scale auxiliary corpus. We
show that including N-gram count features
can advance the state-of-the-art accuracy
on standard data sets for adjective order-
ing, spelling correction, noun compound
bracketing, and verb part-of-speech dis-
ambiguation. More importantly, when op-
erating on new domains, or when labeled
training data is not plentiful, we show that
using web-scale N-gram features is essen-
tial for achieving robust performance.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996853262295082">
Many NLP systems use web-scale N-gram counts
(Keller and Lapata, 2003; Nakov and Hearst,
2005; Brants et al., 2007). Lapata and Keller
(2005) demonstrate good performance on eight
tasks using unsupervised web-based models. They
show web counts are superior to counts from a
large corpus. Bergsma et al. (2009) propose un-
supervised and supervised systems that use counts
from Google’s N-gram corpus (Brants and Franz,
2006). Web-based models perform particularly
well on generation tasks, where systems choose
between competing sequences of output text (such
as different spellings), as opposed to analysis
tasks, where systems choose between abstract la-
bels (such as part-of-speech tags or parse trees).
In this work, we address two natural and related
questions which these previous studies leave open:
1. Is there a benefit in combining web-scale
counts with the features used in state-of-the-
art supervised approaches?
2. How well do web-based models perform on
new domains or when labeled data is scarce?
We address these questions on two generation
and two analysis tasks, using both existing N-gram
data and a novel web-scale N-gram corpus that
includes part-of-speech information (Section 2).
While previous work has combined web-scale fea-
tures with other features in specific classification
problems (Modjeska et al., 2003; Yang et al.,
2005; Vadas and Curran, 2007b), we provide a
multi-task, multi-domain comparison.
Some may question why supervised approaches
are needed at all for generation problems. Why
not solely rely on direct evidence from a giant cor-
pus? For example, for the task of prenominal ad-
jective ordering (Section 3), a system that needs
to describe a ball that is both big and red can sim-
ply check that big red is more common on the web
than red big, and order the adjectives accordingly.
It is, however, suboptimal to only use N-gram
data. For example, ordering adjectives by direct
web evidence performs 7% worse than our best
supervised system (Section 3.2). No matter how
large the web becomes, there will always be plau-
sible constructions that never occur. For example,
there are currently no pages indexed by Google
with the preferred adjective ordering for bedrag-
gled 56-year-old [professor]. Also, in a particu-
lar domain, words may have a non-standard usage.
Systems trained on labeled data can learn the do-
main usage and leverage other regularities, such as
suffixes and transitivity for adjective ordering.
With these benefits, systems trained on labeled
data have become the dominant technology in aca-
demic NLP. There is a growing recognition, how-
ever, that these systems are highly domain de-
pendent. For example, parsers trained on anno-
tated newspaper text perform poorly on other gen-
res (Gildea, 2001). While many approaches have
adapted NLP systems to specific domains (Tsu-
ruoka et al., 2005; McClosky et al., 2006; Blitzer
</bodyText>
<page confidence="0.993186">
865
</page>
<note confidence="0.9435095">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 865–874,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999903888888889">
et al., 2007; Daum´e III, 2007; Rimell and Clark,
2008), these techniques assume the system knows
on which domain it is being used, and that it has
access to representative data in that domain. These
assumptions are unrealistic in many real-world sit-
uations; for example, when automatically process-
ing a heterogeneous collection of web pages. How
well do supervised and unsupervised NLP systems
perform when used uncustomized, out-of-the-box
on new domains, and how can we best design our
systems for robust open-domain performance?
Our results show that using web-scale N-gram
data in supervised systems advances the state-of-
the-art performance on standard analysis and gen-
eration tasks. More importantly, when operating
out-of-domain, or when labeled data is not plen-
tiful, using web-scale N-gram data not only helps
achieve good performance – it is essential.
</bodyText>
<sectionHeader confidence="0.766825" genericHeader="introduction">
2 Experiments and Data
2.1 Experimental Design
</sectionHeader>
<bodyText confidence="0.999938857142857">
We evaluate the benefit of N-gram data on multi-
class classification problems. For each task, we
have some labeled data indicating the correct out-
put for each example. We evaluate with accuracy:
the percentage of examples correctly classified in
test data. We use one in-domain and two out-of-
domain test sets for each task. Statistical signifi-
cance is assessed with McNemar’s test, p&lt;0.01.
We provide results for unsupervised approaches
and the majority-class baseline for each task.
For our supervised approaches, we represent the
examples as feature vectors, and learn a classi-
fier on the training vectors. There are two fea-
ture classes: features that use N-grams (N-GM)
and those that do not (LEX). N-GM features are
real-valued features giving the log-count of a par-
ticular N-gram in the auxiliary web corpus. LEX
features are binary features that indicate the pres-
ence or absence of a particular string at a given po-
sition in the input. The name LEX emphasizes that
they identify specific lexical items. The instantia-
tions of both types of features depend on the task
and are described in the corresponding sections.
Each classifier is a linear Support Vector Ma-
chine (SVM), trained using LIBLINEAR (Fan et al.,
2008) on the standard domain. We use the one-vs-
all strategy when there are more than two classes
(in Section 4). We plot learning curves to mea-
sure the accuracy of the classifier when the num-
ber of labeled training examples varies. The size
of the N-gram data and its counts remain constant.
We always optimize the SVM’s (L2) regulariza-
tion parameter on the in-domain development set.
We present results with L2-SVM, but achieve sim-
ilar results with L1-SVM and logistic regression.
</bodyText>
<subsectionHeader confidence="0.998206">
2.2 Tasks and Labeled Data
</subsectionHeader>
<bodyText confidence="0.991337722222222">
We study two generation tasks: prenominal ad-
jective ordering (Section 3) and context-sensitive
spelling correction (Section 4), followed by two
analysis tasks: noun compound bracketing (Sec-
tion 5) and verb part-of-speech disambiguation
(Section 6). In each section, we provide refer-
ences to the origin of the labeled data. For the
out-of-domain Gutenberg and Medline data used
in Sections 3 and 4, we generate examples our-
selves.1 We chose Gutenberg and Medline in order
to provide challenging, distinct domains from our
training corpora. Our Gutenberg corpus consists
of out-of-copyright books, automatically down-
loaded from the Project Gutenberg website.2 The
Medline data consists of a large collection of on-
line biomedical abstracts. We describe how la-
beled adjective and spelling examples are created
from these corpora in the corresponding sections.
</bodyText>
<subsectionHeader confidence="0.999833">
2.3 Web-Scale Auxiliary Data
</subsectionHeader>
<bodyText confidence="0.998711411764706">
The most widely-used N-gram corpus is the
Google 5-gram Corpus (Brants and Franz, 2006).
For our tasks, we also use Google V2: a new
N-gram corpus (also with N-grams of length one-
to-five) that we created from the same one-trillion-
word snapshot of the web as the Google 5-gram
Corpus, but with several enhancements. These in-
clude: 1) Reducing noise by removing duplicate
sentences and sentences with a high proportion
of non-alphanumeric characters (together filtering
about 80% of the source data), 2) pre-converting
all digits to the 0 character to reduce sparsity for
numeric expressions, and 3) including the part-of-
speech (POS) tag distribution for each N-gram.
The source data was automatically tagged with
TnT (Brants, 2000), using the Penn Treebank tag
set. Lin et al. (2010) provide more details on the
</bodyText>
<footnote confidence="0.993405">
1http://webdocs.cs.ualberta.ca/—bergsma/Robust/
provides our Gutenberg corpus, a link to Medline, and also
the generated examples for both Gutenberg and Medline.
2www.gutenberg.org. All books just released in 2009 and
thus unlikely to occur in the source data for our N-gram cor-
pus (from 2006). Of course, with removal of sentence dupli-
cates and also N-gram thresholding, the possible presence of
a test sentence in the massive source data is unlikely to affect
results. Carlson et al. (2008) reach a similar conclusion.
</footnote>
<page confidence="0.998149">
866
</page>
<bodyText confidence="0.992427882352941">
N-gram data and N-gram search tools.
The third enhancement is especially relevant
here, as we can use the POS distribution to collect
counts for N-grams of mixed words and tags. For
example, we have developed an N-gram search en-
gine that can count how often the adjective un-
precedented precedes another adjective in our web
corpus (113K times) and how often it follows one
(11K times). Thus, even if we haven’t seen a par-
ticular adjective pair directly, we can use the posi-
tional preferences of each adjective to order them.
Early web-based models used search engines to
collect N-gram counts, and thus could not use cap-
italization, punctuation, and annotations such as
part-of-speech (Kilgarriff and Grefenstette, 2003).
Using a POS-tagged web corpus goes a long way
to addressing earlier criticisms of web-based NLP.
</bodyText>
<sectionHeader confidence="0.992986" genericHeader="method">
3 Prenominal Adjective Ordering
</sectionHeader>
<bodyText confidence="0.999804285714286">
Prenominal adjective ordering strongly affects text
readability. For example, while the unprecedented
statistical revolution is fluent, the statistical un-
precedented revolution is not. Many NLP systems
need to handle adjective ordering robustly. In ma-
chine translation, if a noun has two adjective mod-
ifiers, they must be ordered correctly in the tar-
get language. Adjective ordering is also needed
in Natural Language Generation systems that pro-
duce information from databases; for example, to
convey information (in sentences) about medical
patients (Shaw and Hatzivassiloglou, 1999).
We focus on the task of ordering a pair of adjec-
tives independently of the noun they modify and
achieve good performance in this setting. Follow-
ing the set-up of Malouf (2000), we experiment
on the 263K adjective pairs Malouf extracted from
the British National Corpus (BNC). We use 90%
of pairs for training, 5% for testing, and 5% for
development. This forms our in-domain data.3
We create out-of-domain examples by tokeniz-
ing Medline and Gutenberg (Section 2.2), then
POS-tagging them with CRFTagger (Phan, 2006).
We create examples from all sequences of two ad-
jectives followed by a noun. Like Malouf (2000),
we assume that edited text has adjectives ordered
fluently. We extract 13K and 9.1K out-of-domain
pairs from Gutenberg and Medline, respectively.4
</bodyText>
<footnote confidence="0.9805482">
3BNC is not a domain per se (rather a balanced corpus),
but has a style and vocabulary distinct from our OOD data.
4Like Malouf (2000), we convert our pairs to lower-case.
Since the N-gram data includes case, we merge counts from
the upper and lower case combinations.
</footnote>
<bodyText confidence="0.999957833333333">
The input to the system is a pair of adjectives,
(a1, a2), ordered alphabetically. The task is to
classify this order as correct (the positive class) or
incorrect (the negative class). Since both classes
are equally likely, the majority-class baseline is
around 50% on each of the three test sets.
</bodyText>
<subsectionHeader confidence="0.996134">
3.1 Supervised Adjective Ordering
3.1.1 LEX features
</subsectionHeader>
<bodyText confidence="0.999926615384615">
Our adjective ordering model with LEX features is
a novel contribution of this paper.
We begin with two features for each pair: an in-
dicator feature for a1, which gets a feature value of
+1, and an indicator feature for a2, which gets a
feature value of −1. The parameters of the model
are therefore weights on specific adjectives. The
higher the weight on an adjective, the more it is
preferred in the first position of a pair. If the alpha-
betic ordering is correct, the weight on a1 should
be higher than the weight on a2, so that the clas-
sifier returns a positive score. If the reverse order-
ing is preferred, a2 should receive a higher weight.
Training the model in this setting is a matter of as-
signing weights to all the observed adjectives such
that the training pairs are maximally ordered cor-
rectly. The feature weights thus implicitly produce
a linear ordering of all observed adjectives. The
examples can also be regarded as rank constraints
in a discriminative ranker (Joachims, 2002). Tran-
sitivity is achieved naturally in that if we correctly
order pairs a ≺ b and b ≺ c in the training set,
then a ≺ c by virtue of the weights on a and c.
While exploiting transitivity has been shown
to improve adjective ordering, there are many
conflicting pairs that make a strict linear order-
ing of adjectives impossible (Malouf, 2000). We
therefore provide an indicator feature for the pair
a1a2, so the classifier can memorize exceptions
to the linear ordering, breaking strict order tran-
sitivity. Our classifier thus operates along the lines
of rankers in the preference-based setting as de-
scribed in Ailon and Mohri (2008).
Finally, we also have features for all suffixes of
length 1-to-4 letters, as these encode useful infor-
mation about adjective class (Malouf, 2000). Like
the adjective features, the suffix features receive a
value of +1 for adjectives in the first position and
−1 for those in the second.
</bodyText>
<subsubsectionHeader confidence="0.563603">
3.1.2 N-GM features
</subsubsectionHeader>
<bodyText confidence="0.9874825">
Lapata and Keller (2005) propose a web-based
approach to adjective ordering: take the most-
</bodyText>
<page confidence="0.992473">
867
</page>
<table confidence="0.9997005">
System IN O1 O2
Malouf (2000) 91.5 65.6 71.6
web c(a1, a2) vs. c(a2, a1) 87.1 83.7 86.0
SVM with N-GM features 90.0 85.8 88.5
SVM with LEX features 93.0 70.0 73.9
SVM with N-GM + LEX 93.7 83.6 85.4
</table>
<tableCaption confidence="0.749184333333333">
Table 1: Adjective ordering accuracy (%). SVM
and Malouf (2000) trained on BNC, tested on
BNC (IN), Gutenberg (O1), and Medline (O2).
</tableCaption>
<bodyText confidence="0.9994895">
frequent order of the words on the web, c(a1, a2)
vs. c(a2, a1). We adopt this as our unsupervised
approach. We merge the counts for the adjectives
occurring contiguously and separated by a comma.
These are indubitably the most important N-GM
features; we include them but also other, tag-based
counts from Google V2. Raw counts include cases
where one of the adjectives is not used as a mod-
ifier: “the special present was” vs. “the present
special issue.” We include log-counts for the
following, more-targeted patterns:5 c(a1 a2 N.*),
c(a2 a1 N.*), c(DT a1 a2 N.*), c(DT a2 a1 N.*).
We also include features for the log-counts of
each adjective preceded or followed by a word
matching an adjective-tag: c(a1 J.*), c(J.* a1),
c(a2 J.*), c(J.* a2). These assess the positional
preferences of each adjective. Finally, we include
the log-frequency of each adjective. The more fre-
quent adjective occurs first 57% of the time.
As in all tasks, the counts are features in a clas-
sifier, so the importance of the different patterns is
weighted discriminatively during training.
</bodyText>
<subsectionHeader confidence="0.999461">
3.2 Adjective Ordering Results
</subsectionHeader>
<bodyText confidence="0.999927866666667">
In-domain, with both feature classes, we set a
strong new standard on this data: 93.7% accuracy
for the N-GM+LEX system (Table 1). We trained
and tested Malouf (2000)’s program on our data;
our LEX classifier, which also uses no auxiliary
corpus, makes 18% fewer errors than Malouf’s
system. Our web-based N-GM model is also su-
perior to the direct evidence web-based approach
of Lapata and Keller (2005), scoring 90.0% vs.
87.1% accuracy. These results show the benefit
of our new lexicalized and web-based features.
Figure 1 gives the in-domain learning curve.
With fewer training examples, the systems with
N-GM features strongly outperform the LEX-only
system. Note that with tens of thousands of test
</bodyText>
<footnote confidence="0.964659">
5In this notation, capital letters (and regular expressions)
are matched against tags while a1 and a2 match words.
</footnote>
<figure confidence="0.557686">
Number of training examples
</figure>
<figureCaption confidence="0.9970305">
Figure 1: In-domain learning curve of adjective
ordering classifiers on BNC.
</figureCaption>
<figure confidence="0.917021">
Number of training examples
</figure>
<figureCaption confidence="0.8635705">
Figure 2: Out-of-domain learning curve of adjec-
tive ordering classifiers on Gutenberg.
</figureCaption>
<bodyText confidence="0.987483142857143">
examples, all differences are highly significant.
Out-of-domain, LEX’s accuracy drops a shock-
ing 23% on Gutenberg and 19% on Medline (Ta-
ble 1). Malouf (2000)’s system fares even worse.
The overlap between training and test pairs helps
explain. While 59% of the BNC test pairs were
seen in the training corpus, only 25% of Gutenberg
and 18% of Medline pairs were seen in training.
While other ordering models have also achieved
“very poor results” out-of-domain (Mitchell,
2009), we expected our expanded set of LEX fea-
tures to provide good generalization on new data.
Instead, LEX is very unreliable on new domains.
N-GM features do not rely on specific pairs in
training data, and thus remain fairly robust cross-
domain. Across the three test sets, 84-89% of
examples had the correct ordering appear at least
once on the web. On new domains, the learned
N-GM system maintains an advantage over the un-
supervised c(a1, a2) vs. c(a2, a1), but the differ-
ence is reduced. Note that training with 10-fold
</bodyText>
<figure confidence="0.99845175">
100 1e3 1e4 1e5
100
95
90
85
80
75
70
65
60
N-GM+LEX
N-GM
LEX
100 1e3 1e4 1e5
Accuracy (%)
100
95
90
85
80
75
70
65
60
N-GM+LEX
N-GM
LEX
Accuracy (%)
</figure>
<page confidence="0.985647">
868
</page>
<bodyText confidence="0.999190769230769">
cross validation, the N-GM system can achieve up
to 87.5% on Gutenberg (90.0% for N-GM + LEX).
The learning curve showing performance on
Gutenberg (but still training on BNC) is particu-
larly instructive (Figure 2, performance on Med-
line is very similar). The LEX system performs
much worse than the web-based models across
all training sizes. For our top in-domain sys-
tem, N-GM + LEX, as you add more labeled ex-
amples, performance begins decreasing out-of-
domain. The system disregards the robust N-gram
counts as it is more and more confident in the LEX
features, and it suffers the consequences.
</bodyText>
<sectionHeader confidence="0.983897" genericHeader="method">
4 Context-Sensitive Spelling Correction
</sectionHeader>
<bodyText confidence="0.999908833333333">
We now turn to the generation problem of context-
sensitive spelling correction. For every occurrence
of a word in a pre-defined set of confusable words
(like peace and piece), the system must select the
most likely word from the set, flagging possible
usage errors when the predicted word disagrees
with the original. Contextual spell checkers are
one of the most widely used NLP technologies,
reaching millions of users via compressed N-gram
models in Microsoft Office (Church et al., 2007).
Our in-domain examples are from the New York
Times (NYT) portion of Gigaword, from Bergsma
et al. (2009). They include the 5 confusion sets
where accuracy was below 90% in Golding and
Roth (1999). There are 100K training, 10K devel-
opment, and 10K test examples for each confusion
set. Our results are averages across confusion sets.
Out-of-domain examples are again drawn from
Gutenberg and Medline. We extract all instances
of words that are in one of our confusion sets,
along with surrounding context. By assuming the
extracted instances represent correct usage, we la-
bel 7.8K and 56K out-of-domain test examples for
Gutenberg and Medline, respectively.
We test three unsupervised systems: 1) Lapata
and Keller (2005) use one token of context on the
left and one on the right, and output the candidate
from the confusion set that occurs most frequently
in this pattern. 2) Bergsma et al. (2009) measure
the frequency of the candidates in all the 3-to-5-
gram patterns that span the confusable word. For
each candidate, they sum the log-counts of all pat-
terns filled with the candidate, and output the can-
didate with the highest total. 3) The baseline pre-
dicts the most frequent member of each confusion
set, based on frequencies in the NYT training data.
</bodyText>
<table confidence="0.999831714285714">
System IN O1 O2
Baseline 66.9 44.6 60.6
Lapata and Keller (2005) 88.4 78.0 87.4
Bergsma et al. (2009) 94.8 87.7 94.2
SVM with N-GM features 95.7 92.1 93.9
SVM with LEX features 95.2 85.8 91.0
SVM with N-GM + LEX 96.5 91.9 94.8
</table>
<tableCaption confidence="0.979492333333333">
Table 2: Spelling correction accuracy (%). SVM
trained on NYT, tested on NYT (IN) and out-of-
domain Gutenberg (O1) and Medline (O2).
</tableCaption>
<figure confidence="0.913235">
Number of training examples
</figure>
<figureCaption confidence="0.997535">
Figure 3: In-domain learning curve of spelling
correction classifiers on NYT.
</figureCaption>
<subsectionHeader confidence="0.996246">
4.1 Supervised Spelling Correction
</subsectionHeader>
<bodyText confidence="0.999983666666667">
Our LEX features are typical disambiguation fea-
tures that flag specific aspects of the context. We
have features for the words at all positions in
a 9-word window (called collocation features by
Golding and Roth (1999)), plus indicators for a
particular word preceding or following the con-
fusable word. We also include indicators for all
N-grams, and their position, in a 9-word window.
For N-GM count features, we follow Bergsma
et al. (2009). We include the log-counts of all
N-grams that span the confusable word, with each
word in the confusion set filling the N-gram pat-
tern. These features do not use part-of-speech.
Following Bergsma et al. (2009), we get N-gram
counts using the original Google N-gram Corpus.
While neither our LEX nor N-GM features are
novel on their own, they have, perhaps surpris-
ingly, not yet been evaluated in a single model.
</bodyText>
<subsectionHeader confidence="0.992819">
4.2 Spelling Correction Results
</subsectionHeader>
<bodyText confidence="0.993575666666667">
The N-GM features outperform the LEX features,
95.7% vs. 95.2% (Table 2). Together, they
achieve a very strong 96.5% in-domain accuracy.
</bodyText>
<figure confidence="0.997943916666667">
100 1e3 1e4 1e5
Accuracy (%)
100
95
90
85
80
75
70
N-GM+LEX
N-GM
LEX
</figure>
<page confidence="0.996672">
869
</page>
<bodyText confidence="0.999970304347826">
This is 2% higher than the best unsupervised ap-
proach (Bergsma et al., 2009). Web-based models
again perform well across a range of training data
sizes (Figure 3).
The error rate of LEX nearly triples on Guten-
berg and almost doubles on Medline (Table 2). Re-
moving N-GM features from the N-GM + LEX sys-
tem, errors increase around 75% on both Guten-
berg and Medline. The LEX features provide no
help to the combined system on Gutenberg, while
they do help significantly on Medline. Note the
learning curves for N-GM+LEX on Gutenberg and
Medline (not shown) do not display the decrease
that we observed in adjective ordering (Figure 2).
Both the baseline and LEX perform poorly on
Gutenberg. The baseline predicts the majority
class from NYT, but it’s not always the majority
class in Gutenberg. For example, while in NYT
site occurs 87% of the time for the (cite, sight,
site) confusion set, sight occurs 90% of the time in
Gutenberg. The LEX classifier exploits this bias as
it is regularized toward a more economical model,
but the bias does not transfer to the new domain.
</bodyText>
<sectionHeader confidence="0.984878" genericHeader="method">
5 Noun Compound Bracketing
</sectionHeader>
<bodyText confidence="0.999922461538462">
About 70% of web queries are noun phrases (Barr
et al., 2008) and methods that can reliably parse
these phrases are of great interest in NLP. For
example, a web query for zebra hair straightener
should be bracketed as (zebra (hair straightener)),
a stylish hair straightener with zebra print, rather
than ((zebra hair) straightener), a useless product
since the fur of zebras is already quite straight.
The noun compound (NC) bracketing task is
usually cast as a decision whether a 3-word NC
has a left or right bracketing. Most approaches are
unsupervised, using a large corpus to compare the
statistical association between word pairs in the
NC. The adjacency model (Marcus, 1980) pro-
poses a left bracketing if the association between
words one and two is higher than between two
and three. The dependency model (Lauer, 1995a)
compares one-two vs. one-three. We include de-
pendency model results using PMI as the associ-
ation measure; results were lower with the adja-
cency model.
As in-domain data, we use Vadas and Curran
(2007a)’s Wall-Street Journal (WSJ) data, an ex-
tension of the Treebank (which originally left NPs
flat). We extract all sequences of three consec-
utive common nouns, generating 1983 examples
</bodyText>
<table confidence="0.999620333333333">
System IN O1 O2
Baseline 70.5 66.8 84.1
Dependency model 74.7 82.8 84.4
SVM with N-GM features 89.5 81.6 86.2
SVM with LEX features 81.1 70.9 79.0
SVM with N-GM + LEX 91.6 81.6 87.4
</table>
<tableCaption confidence="0.972890333333333">
Table 3: NC-bracketing accuracy (%). SVM
trained on WSJ, tested on WSJ (IN) and out-of-
domain Grolier (O1) and Medline (O2).
</tableCaption>
<figure confidence="0.901767">
Number of labeled examples
</figure>
<figureCaption confidence="0.999765">
Figure 4: In-domain NC-bracketer learning curve
</figureCaption>
<bodyText confidence="0.999734166666667">
from sections 0-22 of the Treebank as training, 72
from section 24 for development and 95 from sec-
tion 23 as a test set. As out-of-domain data, we
use 244 NCs from Grolier Encyclopedia (Lauer,
1995a) and 429 NCs from Medline (Nakov, 2007).
The majority class baseline is left-bracketing.
</bodyText>
<subsectionHeader confidence="0.972356">
5.1 Supervised Noun Bracketing
</subsectionHeader>
<bodyText confidence="0.999981933333334">
Our LEX features indicate the specific noun at
each position in the compound, plus the three pairs
of nouns and the full noun triple. We also add fea-
tures for the capitalization pattern of the sequence.
N-GM features give the log-count of all subsets
of the compound. Counts are from Google V2.
Following Nakov and Hearst (2005), we also in-
clude counts of noun pairs collapsed into a single
token; if a pair occurs often on the web as a single
unit, it strongly indicates the pair is a constituent.
Vadas and Curran (2007a) use simpler features,
e.g. they do not use collapsed pair counts. They
achieve 89.9% in-domain on WSJ and 80.7% on
Grolier. Vadas and Curran (2007b) use compara-
ble features to ours, but do not test out-of-domain.
</bodyText>
<subsectionHeader confidence="0.991491">
5.2 Noun Compound Bracketing Results
</subsectionHeader>
<bodyText confidence="0.996018">
N-GM systems perform much better on this task
(Table 3). N-GM+LEX is statistically significantly
</bodyText>
<figure confidence="0.998155142857143">
10 100 1e3
Accuracy (%)
100
95
90
85
80
75
70
65
60
N-GM+LEX
N-GM
LEX
</figure>
<page confidence="0.99282">
870
</page>
<bodyText confidence="0.999942105263158">
better than LEX on all sets. In-domain, errors
more than double without N-GM features. LEX
performs poorly here because there are far fewer
training examples. The learning curve (Figure 4)
looks much like earlier in-domain curves (Fig-
ures 1 and 3), but truncated before LEX becomes
competitive. The absence of a sufficient amount of
labeled data explains why NC-bracketing is gen-
erally regarded as a task where corpus counts are
crucial.
All web-based models (including the depen-
dency model) exceed 81.5% on Grolier, which
is the level of human agreement (Lauer, 1995b).
N-GM + LEX is highest on Medline, and close
to the 88% human agreement (Nakov and Hearst,
2005). Out-of-domain, the LEX approach per-
forms very poorly, close to or below the base-
line accuracy. With little training data and cross-
domain usage, N-gram features are essential.
</bodyText>
<sectionHeader confidence="0.977773" genericHeader="method">
6 Verb Part-of-Speech Disambiguation
</sectionHeader>
<bodyText confidence="0.999988740740741">
Our final task is POS-tagging. We focus on one
frequent and difficult tagging decision: the distinc-
tion between a past-tense verb (VBD) and a past
participle (VBN). For example, in the troops sta-
tioned in Iraq, the verb stationed is a VBN; troops
is the head of the phrase. On the other hand, for
the troops vacationed in Iraq, the verb vacationed
is a VBD and also the head. Some verbs make the
distinction explicit (eat has VBD ate, VBN eaten),
but most require context for resolution.
Conflating VBN/VBD is damaging because it af-
fects downstream parsers and semantic role la-
belers. The task is difficult because nearby POS
tags can be identical in both cases. When the
verb follows a noun, tag assignment can hinge on
world-knowledge, i.e., the global lexical relation
between the noun and verb (E.g., troops tends to
be the object of stationed but the subject of vaca-
tioned).6 Web-scale N-gram data might help im-
prove the VBN/VBD distinction by providing rela-
tional evidence, even if the verb, noun, or verb-
noun pair were not observed in training data.
We extract nouns followed by a VBN/VBD in the
WSJ portion of the Treebank (Marcus et al., 1993),
getting 23K training, 1091 development and 1130
test examples from sections 2-22, 24, and 23, re-
spectively. For out-of-domain data, we get 21K
</bodyText>
<footnote confidence="0.7638055">
6HMM-style taggers, like the fast TnT tagger used on our
web corpus, do not use bilexical features, and so perform es-
pecially poorly on these cases. One motivation for our work
was to develop a fast post-processor to fix VBN/VBD errors.
</footnote>
<bodyText confidence="0.99812">
examples from the Brown portion of the Treebank
and 6296 examples from tagged Medline abstracts
in the PennBioIE corpus (Kulick et al., 2004).
The majority class baseline is to choose VBD.
</bodyText>
<subsectionHeader confidence="0.998771">
6.1 Supervised Verb Disambiguation
</subsectionHeader>
<bodyText confidence="0.99982">
There are two orthogonal sources of information
for predicting VBN/VBD: 1) the noun-verb pair,
and 2) the context around the pair. Both N-GM
and LEX features encode both these sources.
</bodyText>
<subsectionHeader confidence="0.850539">
6.1.1 LEX features
</subsectionHeader>
<bodyText confidence="0.999967777777778">
For 1), we use indicators for the noun and verb,
the noun-verb pair, whether the verb is on an in-
house list of said-verb (like warned, announced,
etc.), whether the noun is capitalized and whether
it’s upper-case. Note that in training data, 97.3%
of capitalized nouns are followed by a VBD and
98.5% of said-verbs are VBDs. For 2), we provide
indicator features for the words before the noun
and after the verb.
</bodyText>
<sectionHeader confidence="0.354692" genericHeader="method">
6.1.2 N-GM features
</sectionHeader>
<bodyText confidence="0.999976344827586">
For 1), we characterize a noun-verb relation via
features for the pair’s distribution in Google V2.
Characterizing a word by its distribution has a
long history in NLP; we apply similar techniques
to relations, like Turney (2006), but with a larger
corpus and richer annotations. We extract the 20
most-frequent N-grams that contain both the noun
and the verb in the pair. For each of these, we con-
vert the tokens to POS-tags, except for tokens that
are among the most frequent 100 unigrams in our
corpus, which we include in word form. We mask
the noun of interest as N and the verb of interest
as V. This converted N-gram is the feature label.
The value is the pattern’s log-count. A high count
for patterns like (N that V), (N have V) suggests
the relation is a VBD, while patterns (N that were
V), (N V by), (V some N) indicate a VBN. As al-
ways, the classifier learns the association between
patterns and classes.
For 2), we use counts for the verb’s context co-
occurring with a VBD or VBN tag. E.g., we see
whether VBD cases like troops ate or VBN cases
like troops eaten are more frequent. Although our
corpus contains many VBN/VBD errors, we hope
the errors are random enough for aggregate counts
to be useful. The context is an N-gram spanning
the VBN/VBD. We have log-count features for all
five such N-grams in the (previous-word, noun,
verb, next-word) quadruple. The log-count is in-
</bodyText>
<page confidence="0.99448">
871
</page>
<table confidence="0.999747833333333">
System IN O1 O2
Baseline 89.2 85.2 79.6
ContextSum 92.5 91.1 90.4
SVM with N-GM features 96.1 93.4 93.8
SVM with LEX features 95.8 93.4 93.0
SVM with N-GM + LEX 96.4 93.5 94.0
</table>
<tableCaption confidence="0.985834666666667">
Table 4: Verb-POS-disambiguation accuracy (%)
trained on WSJ, tested on WSJ (IN) and out-of-
domain Brown (O1) and Medline (O2).
</tableCaption>
<figure confidence="0.88012">
Number of training examples
</figure>
<figureCaption confidence="0.9889165">
Figure 5: Out-of-domain learning curve of verb
disambiguation classifiers on Medline.
</figureCaption>
<bodyText confidence="0.998646222222222">
dexed by the position and length of the N-gram.
We include separate count features for contexts
matching the specific noun and for when the noun
token can match any word tagged as a noun.
ContextSum: We use these context counts in an
unsupervised system, ContextSum. Analogously
to Bergsma et al. (2009), we separately sum the
log-counts for all contexts filled with VBD and
then VBN, outputting the tag with the higher total.
</bodyText>
<subsectionHeader confidence="0.991055">
6.2 Verb POS Disambiguation Results
</subsectionHeader>
<bodyText confidence="0.999962375">
As in all tasks, N-GM+LEX has the best in-domain
accuracy (96.4%, Table 4). Out-of-domain, when
N-grams are excluded, errors only increase around
14% on Medline and 2% on Brown (the differ-
ences are not statistically significant). Why? Fig-
ure 5, the learning curve for performance on Med-
line, suggests some reasons. We omit N-GM+LEX
from Figure 5 as it closely follows N-GM.
Recall that we grouped the features into two
views: 1) noun-verb (N,V) and 2) context. If we
use just (N,V) features, we do see a large drop out-
of-domain: LEX (N,V) lags N-GM (N,V) even us-
ing all the training examples. The same is true us-
ing only context features (not shown). Using both
views, the results are closer: 93.8% for N-GM and
93.0% for LEX. With two views of an example,
LEX is more likely to have domain-neutral fea-
tures to draw on. Data sparsity is reduced.
Also, the Treebank provides an atypical num-
ber of labeled examples for analysis tasks. In a
more typical situation with less labeled examples,
N-GM strongly dominates LEX, even when two
views are used. E.g., with 2285 training exam-
ples, N-GM+LEX is statistically significantly bet-
ter than LEX on both out-of-domain sets.
All systems, however, perform log-linearly with
training size. In other tasks we only had a handful
of N-GM features; here there are 21K features for
the distributional patterns of N,V pairs. Reducing
this feature space by pruning or performing trans-
formations may improve accuracy in and out-of-
domain.
</bodyText>
<sectionHeader confidence="0.997227" genericHeader="evaluation">
7 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.99990846875">
Of all classifiers, LEX performs worst on all cross-
domain tasks. Clearly, many of the regularities
that a typical classifier exploits in one domain do
not transfer to new genres. N-GM features, how-
ever, do not depend directly on training examples,
and thus work better cross-domain. Of course, us-
ing web-scale N-grams is not the only way to cre-
ate robust classifiers. Counts from any large auxil-
iary corpus may also help, but web counts should
help more (Lapata and Keller, 2005). Section 6.2
suggests that another way to mitigate domain-
dependence is having multiple feature views.
Banko and Brill (2001) argue “a logical next
step for the research community would be to di-
rect efforts towards increasing the size of anno-
tated training collections.” Assuming we really do
want systems that operate beyond the specific do-
mains on which they are trained, the community
also needs to identify which systems behave as in
Figure 2, where the accuracy of the best in-domain
system actually decreases with more training ex-
amples. Our results suggest better features, such
as web pattern counts, may help more than ex-
panding training data. Also, systems using web-
scale unlabeled data will improve automatically as
the web expands, without annotation effort.
In some sense, using web counts as features
is a form of domain adaptation: adapting a web
model to the training domain. How do we ensure
these features are adapted well and not used in
domain-specific ways (especially with many fea-
tures to adapt, as in Section 6)? One option may
</bodyText>
<figure confidence="0.998159272727273">
100 1e3 1e4
Accuracy (%)
100
95
90
85
80
N-GM (N,V+context)
LEX (N,V+context)
N-GM (N,V)
LEX (N,V)
</figure>
<page confidence="0.992679">
872
</page>
<bodyText confidence="0.999936833333333">
be to regularize the classifier specifically for out-
of-domain accuracy. We found that adjusting the
SVM misclassification penalty (for more regular-
ization) can help or hurt out-of-domain. Other
regularizations are possible. In each task, there
are domain-neutral unsupervised approaches. We
could encode these systems as linear classifiers
with corresponding weights. Rather than a typical
SVM that minimizes the weight-norm    |(plus
the slacks), we could regularize toward domain-
neutral weights. This regularization could be opti-
mized on creative splits of the training data.
</bodyText>
<sectionHeader confidence="0.99747" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999986272727273">
We presented results on tasks spanning a range of
NLP research: generation, disambiguation, pars-
ing and tagging. Using web-scale N-gram data
improves accuracy on each task. When less train-
ing data is used, or when the system is used on a
different domain, N-gram features greatly improve
performance. Since most supervised NLP systems
do not use web-scale counts, further cross-domain
evaluation may reveal some very brittle systems.
Continued effort in new domains should be a pri-
ority for the community going forward.
</bodyText>
<sectionHeader confidence="0.997311" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999991">
We gratefully acknowledge the Center for Lan-
guage and Speech Processing at Johns Hopkins
University for hosting the workshop at which part
of this research was conducted.
</bodyText>
<sectionHeader confidence="0.998078" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999164308823529">
Nir Ailon and Mehryar Mohri. 2008. An efficient re-
duction of ranking to classification. In COLT.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In ACL.
Cory Barr, Rosie Jones, and Moira Regelson. 2008.
The linguistic structure of English web-search
queries. In EMNLP.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2009. Web-scale N-gram models for lexical disam-
biguation. In IJCAI.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
Thorsten Brants and Alex Franz. 2006. The Google
Web 1T 5-gram Corpus Version 1.1. LDC2006T13.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In EMNLP.
Thorsten Brants. 2000. TnT – a statistical part-of-
speech tagger. In ANLP.
Andrew Carlson, Tom M. Mitchell, and Ian Fette.
2008. Data analysis project: Leveraging massive
textual corpora using n-gram statistics. Technial Re-
port CMU-ML-08-107.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with Golomb
coding. In EMNLP-CoNLL.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9.
Dan Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP.
Andrew R. Golding and Dan Roth. 1999. A Winnow-
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107–130.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3):459–484.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the Web as corpus.
Computational Linguistics, 29(3):333–347.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein,
Lyle Ungar, Scott Winters, and Pete White. 2004.
Integrated annotation for biomedical information ex-
traction. In BioLINK 2004: Linking Biological Lit-
erature, Ontologies and Databases.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM
Transactions on Speech and Language Processing,
2(1):1–31.
Mark Lauer. 1995a. Corpus statistics meet the noun
compound: Some empirical results. In ACL.
Mark Lauer. 1995b. Designing Statistical Language
Learners: Experiments on Compound Nouns. Ph.D.
thesis, Macquarie University.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale N-grams. In LREC.
</reference>
<page confidence="0.990016">
873
</page>
<reference confidence="0.999550978723404">
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In ACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Mitchell P. Marcus. 1980. Theory of Syntactic Recog-
nition for Natural Languages. MIT Press, Cam-
bridge, MA, USA.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In COLING-ACL.
Margaret Mitchell. 2009. Class-based ordering of
prenominal modifiers. In 12th European Workshop
on Natural Language Generation.
Natalia N. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the Web in machine learning for
other-anaphora resolution. In EMNLP.
Preslav Nakov and Marti Hearst. 2005. Search engine
statistics beyond the n-gram: Application to noun
compound bracketing. In CoNLL.
Preslav Ivanov Nakov. 2007. Using the Web as an Im-
plicit Training Set: Application to Noun Compound
Syntax and Semantics. Ph.D. thesis, University of
California, Berkeley.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English
POS Tagger. crftagger.sourceforge.net.
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains.
In EMNLP.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In ACL.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun’ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379–416.
David Vadas and James R. Curran. 2007a. Adding
noun phrase structure to the Penn Treebank. In ACL.
David Vadas and James R. Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In
PACLING.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005.
Improving pronoun resolution using statistics-based
semantic compatibility information. In ACL.
</reference>
<page confidence="0.998766">
874
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.927766">
<title confidence="0.999576">Creating Robust Supervised Classifiers via Web-Scale N-gram Data</title>
<author confidence="0.999308">Shane Bergsma Emily Pitler Dekang Lin</author>
<affiliation confidence="0.99916">University of Alberta University of Pennsylvania Google, Inc.</affiliation>
<email confidence="0.998826">sbergsma@ualberta.caepitler@seas.upenn.edulindek@google.com</email>
<abstract confidence="0.996139222222222">In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classifiers. We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nir Ailon</author>
<author>Mehryar Mohri</author>
</authors>
<title>An efficient reduction of ranking to classification.</title>
<date>2008</date>
<booktitle>In COLT.</booktitle>
<contexts>
<context position="13302" citStr="Ailon and Mohri (2008)" startWordPosition="2113" endWordPosition="2116">vity is achieved naturally in that if we correctly order pairs a ≺ b and b ≺ c in the training set, then a ≺ c by virtue of the weights on a and c. While exploiting transitivity has been shown to improve adjective ordering, there are many conflicting pairs that make a strict linear ordering of adjectives impossible (Malouf, 2000). We therefore provide an indicator feature for the pair a1a2, so the classifier can memorize exceptions to the linear ordering, breaking strict order transitivity. Our classifier thus operates along the lines of rankers in the preference-based setting as described in Ailon and Mohri (2008). Finally, we also have features for all suffixes of length 1-to-4 letters, as these encode useful information about adjective class (Malouf, 2000). Like the adjective features, the suffix features receive a value of +1 for adjectives in the first position and −1 for those in the second. 3.1.2 N-GM features Lapata and Keller (2005) propose a web-based approach to adjective ordering: take the most867 System IN O1 O2 Malouf (2000) 91.5 65.6 71.6 web c(a1, a2) vs. c(a2, a1) 87.1 83.7 86.0 SVM with N-GM features 90.0 85.8 88.5 SVM with LEX features 93.0 70.0 73.9 SVM with N-GM + LEX 93.7 83.6 85.4</context>
</contexts>
<marker>Ailon, Mohri, 2008</marker>
<rawString>Nir Ailon and Mehryar Mohri. 2008. An efficient reduction of ranking to classification. In COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="32931" citStr="Banko and Brill (2001)" startWordPosition="5431" endWordPosition="5434">n and Future Work Of all classifiers, LEX performs worst on all crossdomain tasks. Clearly, many of the regularities that a typical classifier exploits in one domain do not transfer to new genres. N-GM features, however, do not depend directly on training examples, and thus work better cross-domain. Of course, using web-scale N-grams is not the only way to create robust classifiers. Counts from any large auxiliary corpus may also help, but web counts should help more (Lapata and Keller, 2005). Section 6.2 suggests that another way to mitigate domaindependence is having multiple feature views. Banko and Brill (2001) argue “a logical next step for the research community would be to direct efforts towards increasing the size of annotated training collections.” Assuming we really do want systems that operate beyond the specific domains on which they are trained, the community also needs to identify which systems behave as in Figure 2, where the accuracy of the best in-domain system actually decreases with more training examples. Our results suggest better features, such as web pattern counts, may help more than expanding training data. Also, systems using webscale unlabeled data will improve automatically a</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cory Barr</author>
<author>Rosie Jones</author>
<author>Moira Regelson</author>
</authors>
<title>The linguistic structure of English web-search queries.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="22479" citStr="Barr et al., 2008" startWordPosition="3656" endWordPosition="3659">not shown) do not display the decrease that we observed in adjective ordering (Figure 2). Both the baseline and LEX perform poorly on Gutenberg. The baseline predicts the majority class from NYT, but it’s not always the majority class in Gutenberg. For example, while in NYT site occurs 87% of the time for the (cite, sight, site) confusion set, sight occurs 90% of the time in Gutenberg. The LEX classifier exploits this bias as it is regularized toward a more economical model, but the bias does not transfer to the new domain. 5 Noun Compound Bracketing About 70% of web queries are noun phrases (Barr et al., 2008) and methods that can reliably parse these phrases are of great interest in NLP. For example, a web query for zebra hair straightener should be bracketed as (zebra (hair straightener)), a stylish hair straightener with zebra print, rather than ((zebra hair) straightener), a useless product since the fur of zebras is already quite straight. The noun compound (NC) bracketing task is usually cast as a decision whether a 3-word NC has a left or right bracketing. Most approaches are unsupervised, using a large corpus to compare the statistical association between word pairs in the NC. The adjacency</context>
</contexts>
<marker>Barr, Jones, Regelson, 2008</marker>
<rawString>Cory Barr, Rosie Jones, and Moira Regelson. 2008. The linguistic structure of English web-search queries. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Web-scale N-gram models for lexical disambiguation.</title>
<date>2009</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="1254" citStr="Bergsma et al. (2009)" startWordPosition="176" endWordPosition="179"> adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance. 1 Introduction Many NLP systems use web-scale N-gram counts (Keller and Lapata, 2003; Nakov and Hearst, 2005; Brants et al., 2007). Lapata and Keller (2005) demonstrate good performance on eight tasks using unsupervised web-based models. They show web counts are superior to counts from a large corpus. Bergsma et al. (2009) propose unsupervised and supervised systems that use counts from Google’s N-gram corpus (Brants and Franz, 2006). Web-based models perform particularly well on generation tasks, where systems choose between competing sequences of output text (such as different spellings), as opposed to analysis tasks, where systems choose between abstract labels (such as part-of-speech tags or parse trees). In this work, we address two natural and related questions which these previous studies leave open: 1. Is there a benefit in combining web-scale counts with the features used in state-of-theart supervised </context>
<context position="18565" citStr="Bergsma et al. (2009)" startWordPosition="2991" endWordPosition="2994">tive Spelling Correction We now turn to the generation problem of contextsensitive spelling correction. For every occurrence of a word in a pre-defined set of confusable words (like peace and piece), the system must select the most likely word from the set, flagging possible usage errors when the predicted word disagrees with the original. Contextual spell checkers are one of the most widely used NLP technologies, reaching millions of users via compressed N-gram models in Microsoft Office (Church et al., 2007). Our in-domain examples are from the New York Times (NYT) portion of Gigaword, from Bergsma et al. (2009). They include the 5 confusion sets where accuracy was below 90% in Golding and Roth (1999). There are 100K training, 10K development, and 10K test examples for each confusion set. Our results are averages across confusion sets. Out-of-domain examples are again drawn from Gutenberg and Medline. We extract all instances of words that are in one of our confusion sets, along with surrounding context. By assuming the extracted instances represent correct usage, we label 7.8K and 56K out-of-domain test examples for Gutenberg and Medline, respectively. We test three unsupervised systems: 1) Lapata a</context>
<context position="19820" citStr="Bergsma et al. (2009)" startWordPosition="3202" endWordPosition="3205">context on the left and one on the right, and output the candidate from the confusion set that occurs most frequently in this pattern. 2) Bergsma et al. (2009) measure the frequency of the candidates in all the 3-to-5- gram patterns that span the confusable word. For each candidate, they sum the log-counts of all patterns filled with the candidate, and output the candidate with the highest total. 3) The baseline predicts the most frequent member of each confusion set, based on frequencies in the NYT training data. System IN O1 O2 Baseline 66.9 44.6 60.6 Lapata and Keller (2005) 88.4 78.0 87.4 Bergsma et al. (2009) 94.8 87.7 94.2 SVM with N-GM features 95.7 92.1 93.9 SVM with LEX features 95.2 85.8 91.0 SVM with N-GM + LEX 96.5 91.9 94.8 Table 2: Spelling correction accuracy (%). SVM trained on NYT, tested on NYT (IN) and out-ofdomain Gutenberg (O1) and Medline (O2). Number of training examples Figure 3: In-domain learning curve of spelling correction classifiers on NYT. 4.1 Supervised Spelling Correction Our LEX features are typical disambiguation features that flag specific aspects of the context. We have features for the words at all positions in a 9-word window (called collocation features by Goldin</context>
<context position="21394" citStr="Bergsma et al., 2009" startWordPosition="3467" endWordPosition="3470">illing the N-gram pattern. These features do not use part-of-speech. Following Bergsma et al. (2009), we get N-gram counts using the original Google N-gram Corpus. While neither our LEX nor N-GM features are novel on their own, they have, perhaps surprisingly, not yet been evaluated in a single model. 4.2 Spelling Correction Results The N-GM features outperform the LEX features, 95.7% vs. 95.2% (Table 2). Together, they achieve a very strong 96.5% in-domain accuracy. 100 1e3 1e4 1e5 Accuracy (%) 100 95 90 85 80 75 70 N-GM+LEX N-GM LEX 869 This is 2% higher than the best unsupervised approach (Bergsma et al., 2009). Web-based models again perform well across a range of training data sizes (Figure 3). The error rate of LEX nearly triples on Gutenberg and almost doubles on Medline (Table 2). Removing N-GM features from the N-GM + LEX system, errors increase around 75% on both Gutenberg and Medline. The LEX features provide no help to the combined system on Gutenberg, while they do help significantly on Medline. Note the learning curves for N-GM+LEX on Gutenberg and Medline (not shown) do not display the decrease that we observed in adjective ordering (Figure 2). Both the baseline and LEX perform poorly on</context>
<context position="30668" citStr="Bergsma et al. (2009)" startWordPosition="5050" endWordPosition="5053">93.8 SVM with LEX features 95.8 93.4 93.0 SVM with N-GM + LEX 96.4 93.5 94.0 Table 4: Verb-POS-disambiguation accuracy (%) trained on WSJ, tested on WSJ (IN) and out-ofdomain Brown (O1) and Medline (O2). Number of training examples Figure 5: Out-of-domain learning curve of verb disambiguation classifiers on Medline. dexed by the position and length of the N-gram. We include separate count features for contexts matching the specific noun and for when the noun token can match any word tagged as a noun. ContextSum: We use these context counts in an unsupervised system, ContextSum. Analogously to Bergsma et al. (2009), we separately sum the log-counts for all contexts filled with VBD and then VBN, outputting the tag with the higher total. 6.2 Verb POS Disambiguation Results As in all tasks, N-GM+LEX has the best in-domain accuracy (96.4%, Table 4). Out-of-domain, when N-grams are excluded, errors only increase around 14% on Medline and 2% on Brown (the differences are not statistically significant). Why? Figure 5, the learning curve for performance on Medline, suggests some reasons. We omit N-GM+LEX from Figure 5 as it closely follows N-GM. Recall that we grouped the features into two views: 1) noun-verb (</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2009</marker>
<rawString>Shane Bergsma, Dekang Lin, and Randy Goebel. 2009. Web-scale N-gram models for lexical disambiguation. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<booktitle>The Google Web 1T 5-gram Corpus Version</booktitle>
<volume>1</volume>
<pages>2006--13</pages>
<contexts>
<context position="1367" citStr="Brants and Franz, 2006" startWordPosition="193" endWordPosition="196">e importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance. 1 Introduction Many NLP systems use web-scale N-gram counts (Keller and Lapata, 2003; Nakov and Hearst, 2005; Brants et al., 2007). Lapata and Keller (2005) demonstrate good performance on eight tasks using unsupervised web-based models. They show web counts are superior to counts from a large corpus. Bergsma et al. (2009) propose unsupervised and supervised systems that use counts from Google’s N-gram corpus (Brants and Franz, 2006). Web-based models perform particularly well on generation tasks, where systems choose between competing sequences of output text (such as different spellings), as opposed to analysis tasks, where systems choose between abstract labels (such as part-of-speech tags or parse trees). In this work, we address two natural and related questions which these previous studies leave open: 1. Is there a benefit in combining web-scale counts with the features used in state-of-theart supervised approaches? 2. How well do web-based models perform on new domains or when labeled data is scarce? We address the</context>
<context position="7609" citStr="Brants and Franz, 2006" startWordPosition="1182" endWordPosition="1185">nberg and Medline data used in Sections 3 and 4, we generate examples ourselves.1 We chose Gutenberg and Medline in order to provide challenging, distinct domains from our training corpora. Our Gutenberg corpus consists of out-of-copyright books, automatically downloaded from the Project Gutenberg website.2 The Medline data consists of a large collection of online biomedical abstracts. We describe how labeled adjective and spelling examples are created from these corpora in the corresponding sections. 2.3 Web-Scale Auxiliary Data The most widely-used N-gram corpus is the Google 5-gram Corpus (Brants and Franz, 2006). For our tasks, we also use Google V2: a new N-gram corpus (also with N-grams of length oneto-five) that we created from the same one-trillionword snapshot of the web as the Google 5-gram Corpus, but with several enhancements. These include: 1) Reducing noise by removing duplicate sentences and sentences with a high proportion of non-alphanumeric characters (together filtering about 80% of the source data), 2) pre-converting all digits to the 0 character to reduce sparsity for numeric expressions, and 3) including the part-ofspeech (POS) tag distribution for each N-gram. The source data was a</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. The Google Web 1T 5-gram Corpus Version 1.1. LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1060" citStr="Brants et al., 2007" startWordPosition="146" endWordPosition="149">arious N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance. 1 Introduction Many NLP systems use web-scale N-gram counts (Keller and Lapata, 2003; Nakov and Hearst, 2005; Brants et al., 2007). Lapata and Keller (2005) demonstrate good performance on eight tasks using unsupervised web-based models. They show web counts are superior to counts from a large corpus. Bergsma et al. (2009) propose unsupervised and supervised systems that use counts from Google’s N-gram corpus (Brants and Franz, 2006). Web-based models perform particularly well on generation tasks, where systems choose between competing sequences of output text (such as different spellings), as opposed to analysis tasks, where systems choose between abstract labels (such as part-of-speech tags or parse trees). In this wor</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT – a statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In ANLP.</booktitle>
<contexts>
<context position="8252" citStr="Brants, 2000" startWordPosition="1287" endWordPosition="1288">oogle V2: a new N-gram corpus (also with N-grams of length oneto-five) that we created from the same one-trillionword snapshot of the web as the Google 5-gram Corpus, but with several enhancements. These include: 1) Reducing noise by removing duplicate sentences and sentences with a high proportion of non-alphanumeric characters (together filtering about 80% of the source data), 2) pre-converting all digits to the 0 character to reduce sparsity for numeric expressions, and 3) including the part-ofspeech (POS) tag distribution for each N-gram. The source data was automatically tagged with TnT (Brants, 2000), using the Penn Treebank tag set. Lin et al. (2010) provide more details on the 1http://webdocs.cs.ualberta.ca/—bergsma/Robust/ provides our Gutenberg corpus, a link to Medline, and also the generated examples for both Gutenberg and Medline. 2www.gutenberg.org. All books just released in 2009 and thus unlikely to occur in the source data for our N-gram corpus (from 2006). Of course, with removal of sentence duplicates and also N-gram thresholding, the possible presence of a test sentence in the massive source data is unlikely to affect results. Carlson et al. (2008) reach a similar conclusion</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT – a statistical part-ofspeech tagger. In ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Tom M Mitchell</author>
<author>Ian Fette</author>
</authors>
<title>Data analysis project: Leveraging massive textual corpora using n-gram statistics.</title>
<date>2008</date>
<tech>Technial Report CMU-ML-08-107.</tech>
<contexts>
<context position="8825" citStr="Carlson et al. (2008)" startWordPosition="1375" endWordPosition="1378">was automatically tagged with TnT (Brants, 2000), using the Penn Treebank tag set. Lin et al. (2010) provide more details on the 1http://webdocs.cs.ualberta.ca/—bergsma/Robust/ provides our Gutenberg corpus, a link to Medline, and also the generated examples for both Gutenberg and Medline. 2www.gutenberg.org. All books just released in 2009 and thus unlikely to occur in the source data for our N-gram corpus (from 2006). Of course, with removal of sentence duplicates and also N-gram thresholding, the possible presence of a test sentence in the massive source data is unlikely to affect results. Carlson et al. (2008) reach a similar conclusion. 866 N-gram data and N-gram search tools. The third enhancement is especially relevant here, as we can use the POS distribution to collect counts for N-grams of mixed words and tags. For example, we have developed an N-gram search engine that can count how often the adjective unprecedented precedes another adjective in our web corpus (113K times) and how often it follows one (11K times). Thus, even if we haven’t seen a particular adjective pair directly, we can use the positional preferences of each adjective to order them. Early web-based models used search engines</context>
</contexts>
<marker>Carlson, Mitchell, Fette, 2008</marker>
<rawString>Andrew Carlson, Tom M. Mitchell, and Ian Fette. 2008. Data analysis project: Leveraging massive textual corpora using n-gram statistics. Technial Report CMU-ML-08-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Ted Hart</author>
<author>Jianfeng Gao</author>
</authors>
<title>Compressing trigram language models with Golomb coding.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="18459" citStr="Church et al., 2007" startWordPosition="2973" endWordPosition="2976">ts as it is more and more confident in the LEX features, and it suffers the consequences. 4 Context-Sensitive Spelling Correction We now turn to the generation problem of contextsensitive spelling correction. For every occurrence of a word in a pre-defined set of confusable words (like peace and piece), the system must select the most likely word from the set, flagging possible usage errors when the predicted word disagrees with the original. Contextual spell checkers are one of the most widely used NLP technologies, reaching millions of users via compressed N-gram models in Microsoft Office (Church et al., 2007). Our in-domain examples are from the New York Times (NYT) portion of Gigaword, from Bergsma et al. (2009). They include the 5 confusion sets where accuracy was below 90% in Golding and Roth (1999). There are 100K training, 10K development, and 10K test examples for each confusion set. Our results are averages across confusion sets. Out-of-domain examples are again drawn from Gutenberg and Medline. We extract all instances of words that are in one of our confusion sets, along with surrounding context. By assuming the extracted instances represent correct usage, we label 7.8K and 56K out-of-dom</context>
</contexts>
<marker>Church, Hart, Gao, 2007</marker>
<rawString>Kenneth Church, Ted Hart, and Jianfeng Gao. 2007. Compressing trigram language models with Golomb coding. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<contexts>
<context position="6133" citStr="Fan et al., 2008" startWordPosition="950" endWordPosition="953">rs. There are two feature classes: features that use N-grams (N-GM) and those that do not (LEX). N-GM features are real-valued features giving the log-count of a particular N-gram in the auxiliary web corpus. LEX features are binary features that indicate the presence or absence of a particular string at a given position in the input. The name LEX emphasizes that they identify specific lexical items. The instantiations of both types of features depend on the task and are described in the corresponding sections. Each classifier is a linear Support Vector Machine (SVM), trained using LIBLINEAR (Fan et al., 2008) on the standard domain. We use the one-vsall strategy when there are more than two classes (in Section 4). We plot learning curves to measure the accuracy of the classifier when the number of labeled training examples varies. The size of the N-gram data and its counts remain constant. We always optimize the SVM’s (L2) regularization parameter on the in-domain development set. We present results with L2-SVM, but achieve similar results with L1-SVM and logistic regression. 2.2 Tasks and Labeled Data We study two generation tasks: prenominal adjective ordering (Section 3) and context-sensitive s</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="3687" citStr="Gildea, 2001" startWordPosition="566" endWordPosition="567">es indexed by Google with the preferred adjective ordering for bedraggled 56-year-old [professor]. Also, in a particular domain, words may have a non-standard usage. Systems trained on labeled data can learn the domain usage and leverage other regularities, such as suffixes and transitivity for adjective ordering. With these benefits, systems trained on labeled data have become the dominant technology in academic NLP. There is a growing recognition, however, that these systems are highly domain dependent. For example, parsers trained on annotated newspaper text perform poorly on other genres (Gildea, 2001). While many approaches have adapted NLP systems to specific domains (Tsuruoka et al., 2005; McClosky et al., 2006; Blitzer 865 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 865–874, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics et al., 2007; Daum´e III, 2007; Rimell and Clark, 2008), these techniques assume the system knows on which domain it is being used, and that it has access to representative data in that domain. These assumptions are unrealistic in many real-world situations; for example, when automati</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Dan Gildea. 2001. Corpus variation and parser performance. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
<author>Dan Roth</author>
</authors>
<title>A Winnowbased approach to context-sensitive spelling correction.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="18656" citStr="Golding and Roth (1999)" startWordPosition="3007" endWordPosition="3010">ng correction. For every occurrence of a word in a pre-defined set of confusable words (like peace and piece), the system must select the most likely word from the set, flagging possible usage errors when the predicted word disagrees with the original. Contextual spell checkers are one of the most widely used NLP technologies, reaching millions of users via compressed N-gram models in Microsoft Office (Church et al., 2007). Our in-domain examples are from the New York Times (NYT) portion of Gigaword, from Bergsma et al. (2009). They include the 5 confusion sets where accuracy was below 90% in Golding and Roth (1999). There are 100K training, 10K development, and 10K test examples for each confusion set. Our results are averages across confusion sets. Out-of-domain examples are again drawn from Gutenberg and Medline. We extract all instances of words that are in one of our confusion sets, along with surrounding context. By assuming the extracted instances represent correct usage, we label 7.8K and 56K out-of-domain test examples for Gutenberg and Medline, respectively. We test three unsupervised systems: 1) Lapata and Keller (2005) use one token of context on the left and one on the right, and output the </context>
<context position="20437" citStr="Golding and Roth (1999)" startWordPosition="3304" endWordPosition="3307">(2009) 94.8 87.7 94.2 SVM with N-GM features 95.7 92.1 93.9 SVM with LEX features 95.2 85.8 91.0 SVM with N-GM + LEX 96.5 91.9 94.8 Table 2: Spelling correction accuracy (%). SVM trained on NYT, tested on NYT (IN) and out-ofdomain Gutenberg (O1) and Medline (O2). Number of training examples Figure 3: In-domain learning curve of spelling correction classifiers on NYT. 4.1 Supervised Spelling Correction Our LEX features are typical disambiguation features that flag specific aspects of the context. We have features for the words at all positions in a 9-word window (called collocation features by Golding and Roth (1999)), plus indicators for a particular word preceding or following the confusable word. We also include indicators for all N-grams, and their position, in a 9-word window. For N-GM count features, we follow Bergsma et al. (2009). We include the log-counts of all N-grams that span the confusable word, with each word in the confusion set filling the N-gram pattern. These features do not use part-of-speech. Following Bergsma et al. (2009), we get N-gram counts using the original Google N-gram Corpus. While neither our LEX nor N-GM features are novel on their own, they have, perhaps surprisingly, not</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>Andrew R. Golding and Dan Roth. 1999. A Winnowbased approach to context-sensitive spelling correction. Machine Learning, 34(1-3):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="12670" citStr="Joachims, 2002" startWordPosition="2006" endWordPosition="2007">it is preferred in the first position of a pair. If the alphabetic ordering is correct, the weight on a1 should be higher than the weight on a2, so that the classifier returns a positive score. If the reverse ordering is preferred, a2 should receive a higher weight. Training the model in this setting is a matter of assigning weights to all the observed adjectives such that the training pairs are maximally ordered correctly. The feature weights thus implicitly produce a linear ordering of all observed adjectives. The examples can also be regarded as rank constraints in a discriminative ranker (Joachims, 2002). Transitivity is achieved naturally in that if we correctly order pairs a ≺ b and b ≺ c in the training set, then a ≺ c by virtue of the weights on a and c. While exploiting transitivity has been shown to improve adjective ordering, there are many conflicting pairs that make a strict linear ordering of adjectives impossible (Malouf, 2000). We therefore provide an indicator feature for the pair a1a2, so the classifier can memorize exceptions to the linear ordering, breaking strict order transitivity. Our classifier thus operates along the lines of rankers in the preference-based setting as des</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Mirella Lapata</author>
</authors>
<title>Using the web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="1014" citStr="Keller and Lapata, 2003" startWordPosition="138" endWordPosition="141">t include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance. 1 Introduction Many NLP systems use web-scale N-gram counts (Keller and Lapata, 2003; Nakov and Hearst, 2005; Brants et al., 2007). Lapata and Keller (2005) demonstrate good performance on eight tasks using unsupervised web-based models. They show web counts are superior to counts from a large corpus. Bergsma et al. (2009) propose unsupervised and supervised systems that use counts from Google’s N-gram corpus (Brants and Franz, 2006). Web-based models perform particularly well on generation tasks, where systems choose between competing sequences of output text (such as different spellings), as opposed to analysis tasks, where systems choose between abstract labels (such as pa</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3):459–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Gregory Grefenstette</author>
</authors>
<title>Introduction to the special issue on the Web as corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="9578" citStr="Kilgarriff and Grefenstette, 2003" startWordPosition="1498" endWordPosition="1501">, as we can use the POS distribution to collect counts for N-grams of mixed words and tags. For example, we have developed an N-gram search engine that can count how often the adjective unprecedented precedes another adjective in our web corpus (113K times) and how often it follows one (11K times). Thus, even if we haven’t seen a particular adjective pair directly, we can use the positional preferences of each adjective to order them. Early web-based models used search engines to collect N-gram counts, and thus could not use capitalization, punctuation, and annotations such as part-of-speech (Kilgarriff and Grefenstette, 2003). Using a POS-tagged web corpus goes a long way to addressing earlier criticisms of web-based NLP. 3 Prenominal Adjective Ordering Prenominal adjective ordering strongly affects text readability. For example, while the unprecedented statistical revolution is fluent, the statistical unprecedented revolution is not. Many NLP systems need to handle adjective ordering robustly. In machine translation, if a noun has two adjective modifiers, they must be ordered correctly in the target language. Adjective ordering is also needed in Natural Language Generation systems that produce information from da</context>
</contexts>
<marker>Kilgarriff, Grefenstette, 2003</marker>
<rawString>Adam Kilgarriff and Gregory Grefenstette. 2003. Introduction to the special issue on the Web as corpus. Computational Linguistics, 29(3):333–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seth Kulick</author>
<author>Ann Bies</author>
<author>Mark Liberman</author>
<author>Mark Mandel</author>
<author>Ryan McDonald</author>
<author>Martha Palmer</author>
<author>Andrew Schein</author>
<author>Lyle Ungar</author>
<author>Scott Winters</author>
<author>Pete White</author>
</authors>
<title>Integrated annotation for biomedical information extraction.</title>
<date>2004</date>
<booktitle>In BioLINK</booktitle>
<contexts>
<context position="27831" citStr="Kulick et al., 2004" startWordPosition="4559" endWordPosition="4562">. We extract nouns followed by a VBN/VBD in the WSJ portion of the Treebank (Marcus et al., 1993), getting 23K training, 1091 development and 1130 test examples from sections 2-22, 24, and 23, respectively. For out-of-domain data, we get 21K 6HMM-style taggers, like the fast TnT tagger used on our web corpus, do not use bilexical features, and so perform especially poorly on these cases. One motivation for our work was to develop a fast post-processor to fix VBN/VBD errors. examples from the Brown portion of the Treebank and 6296 examples from tagged Medline abstracts in the PennBioIE corpus (Kulick et al., 2004). The majority class baseline is to choose VBD. 6.1 Supervised Verb Disambiguation There are two orthogonal sources of information for predicting VBN/VBD: 1) the noun-verb pair, and 2) the context around the pair. Both N-GM and LEX features encode both these sources. 6.1.1 LEX features For 1), we use indicators for the noun and verb, the noun-verb pair, whether the verb is on an inhouse list of said-verb (like warned, announced, etc.), whether the noun is capitalized and whether it’s upper-case. Note that in training data, 97.3% of capitalized nouns are followed by a VBD and 98.5% of said-verb</context>
</contexts>
<marker>Kulick, Bies, Liberman, Mandel, McDonald, Palmer, Schein, Ungar, Winters, White, 2004</marker>
<rawString>Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel, Ryan McDonald, Martha Palmer, Andrew Schein, Lyle Ungar, Scott Winters, and Pete White. 2004. Integrated annotation for biomedical information extraction. In BioLINK 2004: Linking Biological Literature, Ontologies and Databases.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="1086" citStr="Lapata and Keller (2005)" startWordPosition="150" endWordPosition="153">the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance. 1 Introduction Many NLP systems use web-scale N-gram counts (Keller and Lapata, 2003; Nakov and Hearst, 2005; Brants et al., 2007). Lapata and Keller (2005) demonstrate good performance on eight tasks using unsupervised web-based models. They show web counts are superior to counts from a large corpus. Bergsma et al. (2009) propose unsupervised and supervised systems that use counts from Google’s N-gram corpus (Brants and Franz, 2006). Web-based models perform particularly well on generation tasks, where systems choose between competing sequences of output text (such as different spellings), as opposed to analysis tasks, where systems choose between abstract labels (such as part-of-speech tags or parse trees). In this work, we address two natural </context>
<context position="13635" citStr="Lapata and Keller (2005)" startWordPosition="2168" endWordPosition="2171">. We therefore provide an indicator feature for the pair a1a2, so the classifier can memorize exceptions to the linear ordering, breaking strict order transitivity. Our classifier thus operates along the lines of rankers in the preference-based setting as described in Ailon and Mohri (2008). Finally, we also have features for all suffixes of length 1-to-4 letters, as these encode useful information about adjective class (Malouf, 2000). Like the adjective features, the suffix features receive a value of +1 for adjectives in the first position and −1 for those in the second. 3.1.2 N-GM features Lapata and Keller (2005) propose a web-based approach to adjective ordering: take the most867 System IN O1 O2 Malouf (2000) 91.5 65.6 71.6 web c(a1, a2) vs. c(a2, a1) 87.1 83.7 86.0 SVM with N-GM features 90.0 85.8 88.5 SVM with LEX features 93.0 70.0 73.9 SVM with N-GM + LEX 93.7 83.6 85.4 Table 1: Adjective ordering accuracy (%). SVM and Malouf (2000) trained on BNC, tested on BNC (IN), Gutenberg (O1), and Medline (O2). frequent order of the words on the web, c(a1, a2) vs. c(a2, a1). We adopt this as our unsupervised approach. We merge the counts for the adjectives occurring contiguously and separated by a comma. T</context>
<context position="15542" citStr="Lapata and Keller (2005)" startWordPosition="2490" endWordPosition="2493">t adjective occurs first 57% of the time. As in all tasks, the counts are features in a classifier, so the importance of the different patterns is weighted discriminatively during training. 3.2 Adjective Ordering Results In-domain, with both feature classes, we set a strong new standard on this data: 93.7% accuracy for the N-GM+LEX system (Table 1). We trained and tested Malouf (2000)’s program on our data; our LEX classifier, which also uses no auxiliary corpus, makes 18% fewer errors than Malouf’s system. Our web-based N-GM model is also superior to the direct evidence web-based approach of Lapata and Keller (2005), scoring 90.0% vs. 87.1% accuracy. These results show the benefit of our new lexicalized and web-based features. Figure 1 gives the in-domain learning curve. With fewer training examples, the systems with N-GM features strongly outperform the LEX-only system. Note that with tens of thousands of test 5In this notation, capital letters (and regular expressions) are matched against tags while a1 and a2 match words. Number of training examples Figure 1: In-domain learning curve of adjective ordering classifiers on BNC. Number of training examples Figure 2: Out-of-domain learning curve of adjectiv</context>
<context position="19181" citStr="Lapata and Keller (2005)" startWordPosition="3088" endWordPosition="3091">. (2009). They include the 5 confusion sets where accuracy was below 90% in Golding and Roth (1999). There are 100K training, 10K development, and 10K test examples for each confusion set. Our results are averages across confusion sets. Out-of-domain examples are again drawn from Gutenberg and Medline. We extract all instances of words that are in one of our confusion sets, along with surrounding context. By assuming the extracted instances represent correct usage, we label 7.8K and 56K out-of-domain test examples for Gutenberg and Medline, respectively. We test three unsupervised systems: 1) Lapata and Keller (2005) use one token of context on the left and one on the right, and output the candidate from the confusion set that occurs most frequently in this pattern. 2) Bergsma et al. (2009) measure the frequency of the candidates in all the 3-to-5- gram patterns that span the confusable word. For each candidate, they sum the log-counts of all patterns filled with the candidate, and output the candidate with the highest total. 3) The baseline predicts the most frequent member of each confusion set, based on frequencies in the NYT training data. System IN O1 O2 Baseline 66.9 44.6 60.6 Lapata and Keller (200</context>
<context position="32806" citStr="Lapata and Keller, 2005" startWordPosition="5412" endWordPosition="5415">irs. Reducing this feature space by pruning or performing transformations may improve accuracy in and out-ofdomain. 7 Discussion and Future Work Of all classifiers, LEX performs worst on all crossdomain tasks. Clearly, many of the regularities that a typical classifier exploits in one domain do not transfer to new genres. N-GM features, however, do not depend directly on training examples, and thus work better cross-domain. Of course, using web-scale N-grams is not the only way to create robust classifiers. Counts from any large auxiliary corpus may also help, but web counts should help more (Lapata and Keller, 2005). Section 6.2 suggests that another way to mitigate domaindependence is having multiple feature views. Banko and Brill (2001) argue “a logical next step for the research community would be to direct efforts towards increasing the size of annotated training collections.” Assuming we really do want systems that operate beyond the specific domains on which they are trained, the community also needs to identify which systems behave as in Figure 2, where the accuracy of the best in-domain system actually decreases with more training examples. Our results suggest better features, such as web pattern</context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>Mirella Lapata and Frank Keller. 2005. Web-based models for natural language processing. ACM Transactions on Speech and Language Processing, 2(1):1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Corpus statistics meet the noun compound: Some empirical results.</title>
<date>1995</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="23244" citStr="Lauer, 1995" startWordPosition="3783" endWordPosition="3784">as (zebra (hair straightener)), a stylish hair straightener with zebra print, rather than ((zebra hair) straightener), a useless product since the fur of zebras is already quite straight. The noun compound (NC) bracketing task is usually cast as a decision whether a 3-word NC has a left or right bracketing. Most approaches are unsupervised, using a large corpus to compare the statistical association between word pairs in the NC. The adjacency model (Marcus, 1980) proposes a left bracketing if the association between words one and two is higher than between two and three. The dependency model (Lauer, 1995a) compares one-two vs. one-three. We include dependency model results using PMI as the association measure; results were lower with the adjacency model. As in-domain data, we use Vadas and Curran (2007a)’s Wall-Street Journal (WSJ) data, an extension of the Treebank (which originally left NPs flat). We extract all sequences of three consecutive common nouns, generating 1983 examples System IN O1 O2 Baseline 70.5 66.8 84.1 Dependency model 74.7 82.8 84.4 SVM with N-GM features 89.5 81.6 86.2 SVM with LEX features 81.1 70.9 79.0 SVM with N-GM + LEX 91.6 81.6 87.4 Table 3: NC-bracketing accuracy</context>
<context position="25843" citStr="Lauer, 1995" startWordPosition="4223" endWordPosition="4224"> 90 85 80 75 70 65 60 N-GM+LEX N-GM LEX 870 better than LEX on all sets. In-domain, errors more than double without N-GM features. LEX performs poorly here because there are far fewer training examples. The learning curve (Figure 4) looks much like earlier in-domain curves (Figures 1 and 3), but truncated before LEX becomes competitive. The absence of a sufficient amount of labeled data explains why NC-bracketing is generally regarded as a task where corpus counts are crucial. All web-based models (including the dependency model) exceed 81.5% on Grolier, which is the level of human agreement (Lauer, 1995b). N-GM + LEX is highest on Medline, and close to the 88% human agreement (Nakov and Hearst, 2005). Out-of-domain, the LEX approach performs very poorly, close to or below the baseline accuracy. With little training data and crossdomain usage, N-gram features are essential. 6 Verb Part-of-Speech Disambiguation Our final task is POS-tagging. We focus on one frequent and difficult tagging decision: the distinction between a past-tense verb (VBD) and a past participle (VBN). For example, in the troops stationed in Iraq, the verb stationed is a VBN; troops is the head of the phrase. On the other </context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995a. Corpus statistics meet the noun compound: Some empirical results. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Designing Statistical Language Learners: Experiments on Compound Nouns.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Macquarie University.</institution>
<contexts>
<context position="23244" citStr="Lauer, 1995" startWordPosition="3783" endWordPosition="3784">as (zebra (hair straightener)), a stylish hair straightener with zebra print, rather than ((zebra hair) straightener), a useless product since the fur of zebras is already quite straight. The noun compound (NC) bracketing task is usually cast as a decision whether a 3-word NC has a left or right bracketing. Most approaches are unsupervised, using a large corpus to compare the statistical association between word pairs in the NC. The adjacency model (Marcus, 1980) proposes a left bracketing if the association between words one and two is higher than between two and three. The dependency model (Lauer, 1995a) compares one-two vs. one-three. We include dependency model results using PMI as the association measure; results were lower with the adjacency model. As in-domain data, we use Vadas and Curran (2007a)’s Wall-Street Journal (WSJ) data, an extension of the Treebank (which originally left NPs flat). We extract all sequences of three consecutive common nouns, generating 1983 examples System IN O1 O2 Baseline 70.5 66.8 84.1 Dependency model 74.7 82.8 84.4 SVM with N-GM features 89.5 81.6 86.2 SVM with LEX features 81.1 70.9 79.0 SVM with N-GM + LEX 91.6 81.6 87.4 Table 3: NC-bracketing accuracy</context>
<context position="25843" citStr="Lauer, 1995" startWordPosition="4223" endWordPosition="4224"> 90 85 80 75 70 65 60 N-GM+LEX N-GM LEX 870 better than LEX on all sets. In-domain, errors more than double without N-GM features. LEX performs poorly here because there are far fewer training examples. The learning curve (Figure 4) looks much like earlier in-domain curves (Figures 1 and 3), but truncated before LEX becomes competitive. The absence of a sufficient amount of labeled data explains why NC-bracketing is generally regarded as a task where corpus counts are crucial. All web-based models (including the dependency model) exceed 81.5% on Grolier, which is the level of human agreement (Lauer, 1995b). N-GM + LEX is highest on Medline, and close to the 88% human agreement (Nakov and Hearst, 2005). Out-of-domain, the LEX approach performs very poorly, close to or below the baseline accuracy. With little training data and crossdomain usage, N-gram features are essential. 6 Verb Part-of-Speech Disambiguation Our final task is POS-tagging. We focus on one frequent and difficult tagging decision: the distinction between a past-tense verb (VBD) and a past participle (VBN). For example, in the troops stationed in Iraq, the verb stationed is a VBN; troops is the head of the phrase. On the other </context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995b. Designing Statistical Language Learners: Experiments on Compound Nouns. Ph.D. thesis, Macquarie University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Kenneth Church</author>
<author>Heng Ji</author>
<author>Satoshi Sekine</author>
<author>David Yarowsky</author>
<author>Shane Bergsma</author>
</authors>
<title>New tools for web-scale N-grams. In LREC.</title>
<date>2010</date>
<institution>Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant</institution>
<contexts>
<context position="8304" citStr="Lin et al. (2010)" startWordPosition="1295" endWordPosition="1298"> of length oneto-five) that we created from the same one-trillionword snapshot of the web as the Google 5-gram Corpus, but with several enhancements. These include: 1) Reducing noise by removing duplicate sentences and sentences with a high proportion of non-alphanumeric characters (together filtering about 80% of the source data), 2) pre-converting all digits to the 0 character to reduce sparsity for numeric expressions, and 3) including the part-ofspeech (POS) tag distribution for each N-gram. The source data was automatically tagged with TnT (Brants, 2000), using the Penn Treebank tag set. Lin et al. (2010) provide more details on the 1http://webdocs.cs.ualberta.ca/—bergsma/Robust/ provides our Gutenberg corpus, a link to Medline, and also the generated examples for both Gutenberg and Medline. 2www.gutenberg.org. All books just released in 2009 and thus unlikely to occur in the source data for our N-gram corpus (from 2006). Of course, with removal of sentence duplicates and also N-gram thresholding, the possible presence of a test sentence in the massive source data is unlikely to affect results. Carlson et al. (2008) reach a similar conclusion. 866 N-gram data and N-gram search tools. The third</context>
</contexts>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, 2010</marker>
<rawString>Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale. 2010. New tools for web-scale N-grams. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>The order of prenominal adjectives in natural language generation.</title>
<date>2000</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="10470" citStr="Malouf (2000)" startWordPosition="1636" endWordPosition="1637">cal unprecedented revolution is not. Many NLP systems need to handle adjective ordering robustly. In machine translation, if a noun has two adjective modifiers, they must be ordered correctly in the target language. Adjective ordering is also needed in Natural Language Generation systems that produce information from databases; for example, to convey information (in sentences) about medical patients (Shaw and Hatzivassiloglou, 1999). We focus on the task of ordering a pair of adjectives independently of the noun they modify and achieve good performance in this setting. Following the set-up of Malouf (2000), we experiment on the 263K adjective pairs Malouf extracted from the British National Corpus (BNC). We use 90% of pairs for training, 5% for testing, and 5% for development. This forms our in-domain data.3 We create out-of-domain examples by tokenizing Medline and Gutenberg (Section 2.2), then POS-tagging them with CRFTagger (Phan, 2006). We create examples from all sequences of two adjectives followed by a noun. Like Malouf (2000), we assume that edited text has adjectives ordered fluently. We extract 13K and 9.1K out-of-domain pairs from Gutenberg and Medline, respectively.4 3BNC is not a d</context>
<context position="13011" citStr="Malouf, 2000" startWordPosition="2069" endWordPosition="2070">he observed adjectives such that the training pairs are maximally ordered correctly. The feature weights thus implicitly produce a linear ordering of all observed adjectives. The examples can also be regarded as rank constraints in a discriminative ranker (Joachims, 2002). Transitivity is achieved naturally in that if we correctly order pairs a ≺ b and b ≺ c in the training set, then a ≺ c by virtue of the weights on a and c. While exploiting transitivity has been shown to improve adjective ordering, there are many conflicting pairs that make a strict linear ordering of adjectives impossible (Malouf, 2000). We therefore provide an indicator feature for the pair a1a2, so the classifier can memorize exceptions to the linear ordering, breaking strict order transitivity. Our classifier thus operates along the lines of rankers in the preference-based setting as described in Ailon and Mohri (2008). Finally, we also have features for all suffixes of length 1-to-4 letters, as these encode useful information about adjective class (Malouf, 2000). Like the adjective features, the suffix features receive a value of +1 for adjectives in the first position and −1 for those in the second. 3.1.2 N-GM features </context>
<context position="15305" citStr="Malouf (2000)" startWordPosition="2453" endWordPosition="2454"> or followed by a word matching an adjective-tag: c(a1 J.*), c(J.* a1), c(a2 J.*), c(J.* a2). These assess the positional preferences of each adjective. Finally, we include the log-frequency of each adjective. The more frequent adjective occurs first 57% of the time. As in all tasks, the counts are features in a classifier, so the importance of the different patterns is weighted discriminatively during training. 3.2 Adjective Ordering Results In-domain, with both feature classes, we set a strong new standard on this data: 93.7% accuracy for the N-GM+LEX system (Table 1). We trained and tested Malouf (2000)’s program on our data; our LEX classifier, which also uses no auxiliary corpus, makes 18% fewer errors than Malouf’s system. Our web-based N-GM model is also superior to the direct evidence web-based approach of Lapata and Keller (2005), scoring 90.0% vs. 87.1% accuracy. These results show the benefit of our new lexicalized and web-based features. Figure 1 gives the in-domain learning curve. With fewer training examples, the systems with N-GM features strongly outperform the LEX-only system. Note that with tens of thousands of test 5In this notation, capital letters (and regular expressions) </context>
</contexts>
<marker>Malouf, 2000</marker>
<rawString>Robert Malouf. 2000. The order of prenominal adjectives in natural language generation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="27308" citStr="Marcus et al., 1993" startWordPosition="4472" endWordPosition="4475">fects downstream parsers and semantic role labelers. The task is difficult because nearby POS tags can be identical in both cases. When the verb follows a noun, tag assignment can hinge on world-knowledge, i.e., the global lexical relation between the noun and verb (E.g., troops tends to be the object of stationed but the subject of vacationed).6 Web-scale N-gram data might help improve the VBN/VBD distinction by providing relational evidence, even if the verb, noun, or verbnoun pair were not observed in training data. We extract nouns followed by a VBN/VBD in the WSJ portion of the Treebank (Marcus et al., 1993), getting 23K training, 1091 development and 1130 test examples from sections 2-22, 24, and 23, respectively. For out-of-domain data, we get 21K 6HMM-style taggers, like the fast TnT tagger used on our web corpus, do not use bilexical features, and so perform especially poorly on these cases. One motivation for our work was to develop a fast post-processor to fix VBN/VBD errors. examples from the Brown portion of the Treebank and 6296 examples from tagged Medline abstracts in the PennBioIE corpus (Kulick et al., 2004). The majority class baseline is to choose VBD. 6.1 Supervised Verb Disambigu</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
</authors>
<title>Theory of Syntactic Recognition for Natural Languages.</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="23100" citStr="Marcus, 1980" startWordPosition="3758" endWordPosition="3759">hods that can reliably parse these phrases are of great interest in NLP. For example, a web query for zebra hair straightener should be bracketed as (zebra (hair straightener)), a stylish hair straightener with zebra print, rather than ((zebra hair) straightener), a useless product since the fur of zebras is already quite straight. The noun compound (NC) bracketing task is usually cast as a decision whether a 3-word NC has a left or right bracketing. Most approaches are unsupervised, using a large corpus to compare the statistical association between word pairs in the NC. The adjacency model (Marcus, 1980) proposes a left bracketing if the association between words one and two is higher than between two and three. The dependency model (Lauer, 1995a) compares one-two vs. one-three. We include dependency model results using PMI as the association measure; results were lower with the adjacency model. As in-domain data, we use Vadas and Curran (2007a)’s Wall-Street Journal (WSJ) data, an extension of the Treebank (which originally left NPs flat). We extract all sequences of three consecutive common nouns, generating 1983 examples System IN O1 O2 Baseline 70.5 66.8 84.1 Dependency model 74.7 82.8 84</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Mitchell P. Marcus. 1980. Theory of Syntactic Recognition for Natural Languages. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In COLING-ACL.</booktitle>
<contexts>
<context position="3801" citStr="McClosky et al., 2006" startWordPosition="583" endWordPosition="586">n a particular domain, words may have a non-standard usage. Systems trained on labeled data can learn the domain usage and leverage other regularities, such as suffixes and transitivity for adjective ordering. With these benefits, systems trained on labeled data have become the dominant technology in academic NLP. There is a growing recognition, however, that these systems are highly domain dependent. For example, parsers trained on annotated newspaper text perform poorly on other genres (Gildea, 2001). While many approaches have adapted NLP systems to specific domains (Tsuruoka et al., 2005; McClosky et al., 2006; Blitzer 865 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 865–874, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics et al., 2007; Daum´e III, 2007; Rimell and Clark, 2008), these techniques assume the system knows on which domain it is being used, and that it has access to representative data in that domain. These assumptions are unrealistic in many real-world situations; for example, when automatically processing a heterogeneous collection of web pages. How well do supervised and unsupervised NLP systems perf</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
</authors>
<title>Class-based ordering of prenominal modifiers.</title>
<date>2009</date>
<booktitle>In 12th European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="16656" citStr="Mitchell, 2009" startWordPosition="2664" endWordPosition="2665">ng classifiers on BNC. Number of training examples Figure 2: Out-of-domain learning curve of adjective ordering classifiers on Gutenberg. examples, all differences are highly significant. Out-of-domain, LEX’s accuracy drops a shocking 23% on Gutenberg and 19% on Medline (Table 1). Malouf (2000)’s system fares even worse. The overlap between training and test pairs helps explain. While 59% of the BNC test pairs were seen in the training corpus, only 25% of Gutenberg and 18% of Medline pairs were seen in training. While other ordering models have also achieved “very poor results” out-of-domain (Mitchell, 2009), we expected our expanded set of LEX features to provide good generalization on new data. Instead, LEX is very unreliable on new domains. N-GM features do not rely on specific pairs in training data, and thus remain fairly robust crossdomain. Across the three test sets, 84-89% of examples had the correct ordering appear at least once on the web. On new domains, the learned N-GM system maintains an advantage over the unsupervised c(a1, a2) vs. c(a2, a1), but the difference is reduced. Note that training with 10-fold 100 1e3 1e4 1e5 100 95 90 85 80 75 70 65 60 N-GM+LEX N-GM LEX 100 1e3 1e4 1e5 </context>
</contexts>
<marker>Mitchell, 2009</marker>
<rawString>Margaret Mitchell. 2009. Class-based ordering of prenominal modifiers. In 12th European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalia N Modjeska</author>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Using the Web in machine learning for other-anaphora resolution.</title>
<date>2003</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2274" citStr="Modjeska et al., 2003" startWordPosition="330" endWordPosition="333">his work, we address two natural and related questions which these previous studies leave open: 1. Is there a benefit in combining web-scale counts with the features used in state-of-theart supervised approaches? 2. How well do web-based models perform on new domains or when labeled data is scarce? We address these questions on two generation and two analysis tasks, using both existing N-gram data and a novel web-scale N-gram corpus that includes part-of-speech information (Section 2). While previous work has combined web-scale features with other features in specific classification problems (Modjeska et al., 2003; Yang et al., 2005; Vadas and Curran, 2007b), we provide a multi-task, multi-domain comparison. Some may question why supervised approaches are needed at all for generation problems. Why not solely rely on direct evidence from a giant corpus? For example, for the task of prenominal adjective ordering (Section 3), a system that needs to describe a ball that is both big and red can simply check that big red is more common on the web than red big, and order the adjectives accordingly. It is, however, suboptimal to only use N-gram data. For example, ordering adjectives by direct web evidence perf</context>
</contexts>
<marker>Modjeska, Markert, Nissim, 2003</marker>
<rawString>Natalia N. Modjeska, Katja Markert, and Malvina Nissim. 2003. Using the Web in machine learning for other-anaphora resolution. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Search engine statistics beyond the n-gram: Application to noun compound bracketing.</title>
<date>2005</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="1038" citStr="Nakov and Hearst, 2005" startWordPosition="142" endWordPosition="145">ures for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance. 1 Introduction Many NLP systems use web-scale N-gram counts (Keller and Lapata, 2003; Nakov and Hearst, 2005; Brants et al., 2007). Lapata and Keller (2005) demonstrate good performance on eight tasks using unsupervised web-based models. They show web counts are superior to counts from a large corpus. Bergsma et al. (2009) propose unsupervised and supervised systems that use counts from Google’s N-gram corpus (Brants and Franz, 2006). Web-based models perform particularly well on generation tasks, where systems choose between competing sequences of output text (such as different spellings), as opposed to analysis tasks, where systems choose between abstract labels (such as part-of-speech tags or par</context>
<context position="24659" citStr="Nakov and Hearst (2005)" startWordPosition="4019" endWordPosition="4022">e Treebank as training, 72 from section 24 for development and 95 from section 23 as a test set. As out-of-domain data, we use 244 NCs from Grolier Encyclopedia (Lauer, 1995a) and 429 NCs from Medline (Nakov, 2007). The majority class baseline is left-bracketing. 5.1 Supervised Noun Bracketing Our LEX features indicate the specific noun at each position in the compound, plus the three pairs of nouns and the full noun triple. We also add features for the capitalization pattern of the sequence. N-GM features give the log-count of all subsets of the compound. Counts are from Google V2. Following Nakov and Hearst (2005), we also include counts of noun pairs collapsed into a single token; if a pair occurs often on the web as a single unit, it strongly indicates the pair is a constituent. Vadas and Curran (2007a) use simpler features, e.g. they do not use collapsed pair counts. They achieve 89.9% in-domain on WSJ and 80.7% on Grolier. Vadas and Curran (2007b) use comparable features to ours, but do not test out-of-domain. 5.2 Noun Compound Bracketing Results N-GM systems perform much better on this task (Table 3). N-GM+LEX is statistically significantly 10 100 1e3 Accuracy (%) 100 95 90 85 80 75 70 65 60 N-GM+</context>
<context position="25942" citStr="Nakov and Hearst, 2005" startWordPosition="4239" endWordPosition="4242">ors more than double without N-GM features. LEX performs poorly here because there are far fewer training examples. The learning curve (Figure 4) looks much like earlier in-domain curves (Figures 1 and 3), but truncated before LEX becomes competitive. The absence of a sufficient amount of labeled data explains why NC-bracketing is generally regarded as a task where corpus counts are crucial. All web-based models (including the dependency model) exceed 81.5% on Grolier, which is the level of human agreement (Lauer, 1995b). N-GM + LEX is highest on Medline, and close to the 88% human agreement (Nakov and Hearst, 2005). Out-of-domain, the LEX approach performs very poorly, close to or below the baseline accuracy. With little training data and crossdomain usage, N-gram features are essential. 6 Verb Part-of-Speech Disambiguation Our final task is POS-tagging. We focus on one frequent and difficult tagging decision: the distinction between a past-tense verb (VBD) and a past participle (VBN). For example, in the troops stationed in Iraq, the verb stationed is a VBN; troops is the head of the phrase. On the other hand, for the troops vacationed in Iraq, the verb vacationed is a VBD and also the head. Some verbs</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. Search engine statistics beyond the n-gram: Application to noun compound bracketing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Ivanov Nakov</author>
</authors>
<title>Using the Web as an Implicit Training Set: Application to Noun Compound Syntax and Semantics.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="24250" citStr="Nakov, 2007" startWordPosition="3954" endWordPosition="3955">IN O1 O2 Baseline 70.5 66.8 84.1 Dependency model 74.7 82.8 84.4 SVM with N-GM features 89.5 81.6 86.2 SVM with LEX features 81.1 70.9 79.0 SVM with N-GM + LEX 91.6 81.6 87.4 Table 3: NC-bracketing accuracy (%). SVM trained on WSJ, tested on WSJ (IN) and out-ofdomain Grolier (O1) and Medline (O2). Number of labeled examples Figure 4: In-domain NC-bracketer learning curve from sections 0-22 of the Treebank as training, 72 from section 24 for development and 95 from section 23 as a test set. As out-of-domain data, we use 244 NCs from Grolier Encyclopedia (Lauer, 1995a) and 429 NCs from Medline (Nakov, 2007). The majority class baseline is left-bracketing. 5.1 Supervised Noun Bracketing Our LEX features indicate the specific noun at each position in the compound, plus the three pairs of nouns and the full noun triple. We also add features for the capitalization pattern of the sequence. N-GM features give the log-count of all subsets of the compound. Counts are from Google V2. Following Nakov and Hearst (2005), we also include counts of noun pairs collapsed into a single token; if a pair occurs often on the web as a single unit, it strongly indicates the pair is a constituent. Vadas and Curran (20</context>
</contexts>
<marker>Nakov, 2007</marker>
<rawString>Preslav Ivanov Nakov. 2007. Using the Web as an Implicit Training Set: Application to Noun Compound Syntax and Semantics. Ph.D. thesis, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan-Hieu Phan</author>
</authors>
<date>2006</date>
<journal>CRFTagger: CRF English POS Tagger. crftagger.sourceforge.net.</journal>
<contexts>
<context position="10810" citStr="Phan, 2006" startWordPosition="1689" endWordPosition="1690"> to convey information (in sentences) about medical patients (Shaw and Hatzivassiloglou, 1999). We focus on the task of ordering a pair of adjectives independently of the noun they modify and achieve good performance in this setting. Following the set-up of Malouf (2000), we experiment on the 263K adjective pairs Malouf extracted from the British National Corpus (BNC). We use 90% of pairs for training, 5% for testing, and 5% for development. This forms our in-domain data.3 We create out-of-domain examples by tokenizing Medline and Gutenberg (Section 2.2), then POS-tagging them with CRFTagger (Phan, 2006). We create examples from all sequences of two adjectives followed by a noun. Like Malouf (2000), we assume that edited text has adjectives ordered fluently. We extract 13K and 9.1K out-of-domain pairs from Gutenberg and Medline, respectively.4 3BNC is not a domain per se (rather a balanced corpus), but has a style and vocabulary distinct from our OOD data. 4Like Malouf (2000), we convert our pairs to lower-case. Since the N-gram data includes case, we merge counts from the upper and lower case combinations. The input to the system is a pair of adjectives, (a1, a2), ordered alphabetically. The</context>
</contexts>
<marker>Phan, 2006</marker>
<rawString>Xuan-Hieu Phan. 2006. CRFTagger: CRF English POS Tagger. crftagger.sourceforge.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Adapting a lexicalized-grammar parser to contrasting domains.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="4057" citStr="Rimell and Clark, 2008" startWordPosition="619" endWordPosition="622">led data have become the dominant technology in academic NLP. There is a growing recognition, however, that these systems are highly domain dependent. For example, parsers trained on annotated newspaper text perform poorly on other genres (Gildea, 2001). While many approaches have adapted NLP systems to specific domains (Tsuruoka et al., 2005; McClosky et al., 2006; Blitzer 865 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 865–874, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics et al., 2007; Daum´e III, 2007; Rimell and Clark, 2008), these techniques assume the system knows on which domain it is being used, and that it has access to representative data in that domain. These assumptions are unrealistic in many real-world situations; for example, when automatically processing a heterogeneous collection of web pages. How well do supervised and unsupervised NLP systems perform when used uncustomized, out-of-the-box on new domains, and how can we best design our systems for robust open-domain performance? Our results show that using web-scale N-gram data in supervised systems advances the state-ofthe-art performance on standa</context>
</contexts>
<marker>Rimell, Clark, 2008</marker>
<rawString>Laura Rimell and Stephen Clark. 2008. Adapting a lexicalized-grammar parser to contrasting domains. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Shaw</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Ordering among premodifiers.</title>
<date>1999</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="10293" citStr="Shaw and Hatzivassiloglou, 1999" startWordPosition="1602" endWordPosition="1605"> web-based NLP. 3 Prenominal Adjective Ordering Prenominal adjective ordering strongly affects text readability. For example, while the unprecedented statistical revolution is fluent, the statistical unprecedented revolution is not. Many NLP systems need to handle adjective ordering robustly. In machine translation, if a noun has two adjective modifiers, they must be ordered correctly in the target language. Adjective ordering is also needed in Natural Language Generation systems that produce information from databases; for example, to convey information (in sentences) about medical patients (Shaw and Hatzivassiloglou, 1999). We focus on the task of ordering a pair of adjectives independently of the noun they modify and achieve good performance in this setting. Following the set-up of Malouf (2000), we experiment on the 263K adjective pairs Malouf extracted from the British National Corpus (BNC). We use 90% of pairs for training, 5% for testing, and 5% for development. This forms our in-domain data.3 We create out-of-domain examples by tokenizing Medline and Gutenberg (Section 2.2), then POS-tagging them with CRFTagger (Phan, 2006). We create examples from all sequences of two adjectives followed by a noun. Like </context>
</contexts>
<marker>Shaw, Hatzivassiloglou, 1999</marker>
<rawString>James Shaw and Vasileios Hatzivassiloglou. 1999. Ordering among premodifiers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Yuka Tateishi</author>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>John McNaught</author>
<author>Sophia Ananiadou</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Developing a robust partof-speech tagger for biomedical text.</title>
<date>2005</date>
<booktitle>In Advances in Informatics.</booktitle>
<contexts>
<context position="3778" citStr="Tsuruoka et al., 2005" startWordPosition="578" endWordPosition="582">ld [professor]. Also, in a particular domain, words may have a non-standard usage. Systems trained on labeled data can learn the domain usage and leverage other regularities, such as suffixes and transitivity for adjective ordering. With these benefits, systems trained on labeled data have become the dominant technology in academic NLP. There is a growing recognition, however, that these systems are highly domain dependent. For example, parsers trained on annotated newspaper text perform poorly on other genres (Gildea, 2001). While many approaches have adapted NLP systems to specific domains (Tsuruoka et al., 2005; McClosky et al., 2006; Blitzer 865 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 865–874, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics et al., 2007; Daum´e III, 2007; Rimell and Clark, 2008), these techniques assume the system knows on which domain it is being used, and that it has access to representative data in that domain. These assumptions are unrealistic in many real-world situations; for example, when automatically processing a heterogeneous collection of web pages. How well do supervised and unsupe</context>
</contexts>
<marker>Tsuruoka, Tateishi, Kim, Ohta, McNaught, Ananiadou, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, Tomoko Ohta, John McNaught, Sophia Ananiadou, and Jun’ichi Tsujii. 2005. Developing a robust partof-speech tagger for biomedical text. In Advances in Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="28780" citStr="Turney (2006)" startWordPosition="4719" endWordPosition="4720">the noun-verb pair, whether the verb is on an inhouse list of said-verb (like warned, announced, etc.), whether the noun is capitalized and whether it’s upper-case. Note that in training data, 97.3% of capitalized nouns are followed by a VBD and 98.5% of said-verbs are VBDs. For 2), we provide indicator features for the words before the noun and after the verb. 6.1.2 N-GM features For 1), we characterize a noun-verb relation via features for the pair’s distribution in Google V2. Characterizing a word by its distribution has a long history in NLP; we apply similar techniques to relations, like Turney (2006), but with a larger corpus and richer annotations. We extract the 20 most-frequent N-grams that contain both the noun and the verb in the pair. For each of these, we convert the tokens to POS-tags, except for tokens that are among the most frequent 100 unigrams in our corpus, which we include in word form. We mask the noun of interest as N and the verb of interest as V. This converted N-gram is the feature label. The value is the pattern’s log-count. A high count for patterns like (N that V), (N have V) suggests the relation is a VBD, while patterns (N that were V), (N V by), (V some N) indica</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Adding noun phrase structure to the Penn Treebank.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2317" citStr="Vadas and Curran, 2007" startWordPosition="338" endWordPosition="341">ed questions which these previous studies leave open: 1. Is there a benefit in combining web-scale counts with the features used in state-of-theart supervised approaches? 2. How well do web-based models perform on new domains or when labeled data is scarce? We address these questions on two generation and two analysis tasks, using both existing N-gram data and a novel web-scale N-gram corpus that includes part-of-speech information (Section 2). While previous work has combined web-scale features with other features in specific classification problems (Modjeska et al., 2003; Yang et al., 2005; Vadas and Curran, 2007b), we provide a multi-task, multi-domain comparison. Some may question why supervised approaches are needed at all for generation problems. Why not solely rely on direct evidence from a giant corpus? For example, for the task of prenominal adjective ordering (Section 3), a system that needs to describe a ball that is both big and red can simply check that big red is more common on the web than red big, and order the adjectives accordingly. It is, however, suboptimal to only use N-gram data. For example, ordering adjectives by direct web evidence performs 7% worse than our best supervised syst</context>
<context position="23446" citStr="Vadas and Curran (2007" startWordPosition="3815" endWordPosition="3818"> noun compound (NC) bracketing task is usually cast as a decision whether a 3-word NC has a left or right bracketing. Most approaches are unsupervised, using a large corpus to compare the statistical association between word pairs in the NC. The adjacency model (Marcus, 1980) proposes a left bracketing if the association between words one and two is higher than between two and three. The dependency model (Lauer, 1995a) compares one-two vs. one-three. We include dependency model results using PMI as the association measure; results were lower with the adjacency model. As in-domain data, we use Vadas and Curran (2007a)’s Wall-Street Journal (WSJ) data, an extension of the Treebank (which originally left NPs flat). We extract all sequences of three consecutive common nouns, generating 1983 examples System IN O1 O2 Baseline 70.5 66.8 84.1 Dependency model 74.7 82.8 84.4 SVM with N-GM features 89.5 81.6 86.2 SVM with LEX features 81.1 70.9 79.0 SVM with N-GM + LEX 91.6 81.6 87.4 Table 3: NC-bracketing accuracy (%). SVM trained on WSJ, tested on WSJ (IN) and out-ofdomain Grolier (O1) and Medline (O2). Number of labeled examples Figure 4: In-domain NC-bracketer learning curve from sections 0-22 of the Treebank</context>
<context position="24852" citStr="Vadas and Curran (2007" startWordPosition="4056" endWordPosition="4059">edline (Nakov, 2007). The majority class baseline is left-bracketing. 5.1 Supervised Noun Bracketing Our LEX features indicate the specific noun at each position in the compound, plus the three pairs of nouns and the full noun triple. We also add features for the capitalization pattern of the sequence. N-GM features give the log-count of all subsets of the compound. Counts are from Google V2. Following Nakov and Hearst (2005), we also include counts of noun pairs collapsed into a single token; if a pair occurs often on the web as a single unit, it strongly indicates the pair is a constituent. Vadas and Curran (2007a) use simpler features, e.g. they do not use collapsed pair counts. They achieve 89.9% in-domain on WSJ and 80.7% on Grolier. Vadas and Curran (2007b) use comparable features to ours, but do not test out-of-domain. 5.2 Noun Compound Bracketing Results N-GM systems perform much better on this task (Table 3). N-GM+LEX is statistically significantly 10 100 1e3 Accuracy (%) 100 95 90 85 80 75 70 65 60 N-GM+LEX N-GM LEX 870 better than LEX on all sets. In-domain, errors more than double without N-GM features. LEX performs poorly here because there are far fewer training examples. The learning curv</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James R. Curran. 2007a. Adding noun phrase structure to the Penn Treebank. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Large-scale supervised models for noun phrase bracketing.</title>
<date>2007</date>
<booktitle>In PACLING.</booktitle>
<contexts>
<context position="2317" citStr="Vadas and Curran, 2007" startWordPosition="338" endWordPosition="341">ed questions which these previous studies leave open: 1. Is there a benefit in combining web-scale counts with the features used in state-of-theart supervised approaches? 2. How well do web-based models perform on new domains or when labeled data is scarce? We address these questions on two generation and two analysis tasks, using both existing N-gram data and a novel web-scale N-gram corpus that includes part-of-speech information (Section 2). While previous work has combined web-scale features with other features in specific classification problems (Modjeska et al., 2003; Yang et al., 2005; Vadas and Curran, 2007b), we provide a multi-task, multi-domain comparison. Some may question why supervised approaches are needed at all for generation problems. Why not solely rely on direct evidence from a giant corpus? For example, for the task of prenominal adjective ordering (Section 3), a system that needs to describe a ball that is both big and red can simply check that big red is more common on the web than red big, and order the adjectives accordingly. It is, however, suboptimal to only use N-gram data. For example, ordering adjectives by direct web evidence performs 7% worse than our best supervised syst</context>
<context position="23446" citStr="Vadas and Curran (2007" startWordPosition="3815" endWordPosition="3818"> noun compound (NC) bracketing task is usually cast as a decision whether a 3-word NC has a left or right bracketing. Most approaches are unsupervised, using a large corpus to compare the statistical association between word pairs in the NC. The adjacency model (Marcus, 1980) proposes a left bracketing if the association between words one and two is higher than between two and three. The dependency model (Lauer, 1995a) compares one-two vs. one-three. We include dependency model results using PMI as the association measure; results were lower with the adjacency model. As in-domain data, we use Vadas and Curran (2007a)’s Wall-Street Journal (WSJ) data, an extension of the Treebank (which originally left NPs flat). We extract all sequences of three consecutive common nouns, generating 1983 examples System IN O1 O2 Baseline 70.5 66.8 84.1 Dependency model 74.7 82.8 84.4 SVM with N-GM features 89.5 81.6 86.2 SVM with LEX features 81.1 70.9 79.0 SVM with N-GM + LEX 91.6 81.6 87.4 Table 3: NC-bracketing accuracy (%). SVM trained on WSJ, tested on WSJ (IN) and out-ofdomain Grolier (O1) and Medline (O2). Number of labeled examples Figure 4: In-domain NC-bracketer learning curve from sections 0-22 of the Treebank</context>
<context position="24852" citStr="Vadas and Curran (2007" startWordPosition="4056" endWordPosition="4059">edline (Nakov, 2007). The majority class baseline is left-bracketing. 5.1 Supervised Noun Bracketing Our LEX features indicate the specific noun at each position in the compound, plus the three pairs of nouns and the full noun triple. We also add features for the capitalization pattern of the sequence. N-GM features give the log-count of all subsets of the compound. Counts are from Google V2. Following Nakov and Hearst (2005), we also include counts of noun pairs collapsed into a single token; if a pair occurs often on the web as a single unit, it strongly indicates the pair is a constituent. Vadas and Curran (2007a) use simpler features, e.g. they do not use collapsed pair counts. They achieve 89.9% in-domain on WSJ and 80.7% on Grolier. Vadas and Curran (2007b) use comparable features to ours, but do not test out-of-domain. 5.2 Noun Compound Bracketing Results N-GM systems perform much better on this task (Table 3). N-GM+LEX is statistically significantly 10 100 1e3 Accuracy (%) 100 95 90 85 80 75 70 65 60 N-GM+LEX N-GM LEX 870 better than LEX on all sets. In-domain, errors more than double without N-GM features. LEX performs poorly here because there are far fewer training examples. The learning curv</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James R. Curran. 2007b. Large-scale supervised models for noun phrase bracketing. In PACLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Improving pronoun resolution using statistics-based semantic compatibility information.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2293" citStr="Yang et al., 2005" startWordPosition="334" endWordPosition="337">o natural and related questions which these previous studies leave open: 1. Is there a benefit in combining web-scale counts with the features used in state-of-theart supervised approaches? 2. How well do web-based models perform on new domains or when labeled data is scarce? We address these questions on two generation and two analysis tasks, using both existing N-gram data and a novel web-scale N-gram corpus that includes part-of-speech information (Section 2). While previous work has combined web-scale features with other features in specific classification problems (Modjeska et al., 2003; Yang et al., 2005; Vadas and Curran, 2007b), we provide a multi-task, multi-domain comparison. Some may question why supervised approaches are needed at all for generation problems. Why not solely rely on direct evidence from a giant corpus? For example, for the task of prenominal adjective ordering (Section 3), a system that needs to describe a ball that is both big and red can simply check that big red is more common on the web than red big, and order the adjectives accordingly. It is, however, suboptimal to only use N-gram data. For example, ordering adjectives by direct web evidence performs 7% worse than </context>
</contexts>
<marker>Yang, Su, Tan, 2005</marker>
<rawString>Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Improving pronoun resolution using statistics-based semantic compatibility information. In ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>