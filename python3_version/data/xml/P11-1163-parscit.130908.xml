<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000621">
<title confidence="0.990277">
Event Extraction as Dependency Parsing
</title>
<author confidence="0.999315">
David McClosky, Mihai Surdeanu, and Christopher D. Manning
</author>
<affiliation confidence="0.987395">
Department of Computer Science
Stanford University
</affiliation>
<address confidence="0.942879">
Stanford, CA 94305
</address>
<email confidence="0.999789">
{mcclosky,mihais,manning}@stanford.edu
</email>
<sectionHeader confidence="0.996663" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999517333333334">
Nested event structures are a common occur-
rence in both open domain and domain spe-
cific extraction tasks, e.g., a “crime” event
can cause a “investigation” event, which can
lead to an “arrest” event. However, most cur-
rent approaches address event extraction with
highly local models that extract each event and
argument independently. We propose a simple
approach for the extraction of such structures
by taking the tree of event-argument relations
and using it directly as the representation in a
reranking dependency parser. This provides a
simple framework that captures global prop-
erties of both nested and flat event structures.
We explore a rich feature space that models
both the events to be parsed and context from
the original supporting text. Our approach ob-
tains competitive results in the extraction of
biomedical events from the BioNLP’09 shared
task with a F1 score of 53.5% in development
and 48.6% in testing.
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996857069767442">
Event structures in open domain texts are frequently
highly complex and nested: a “crime” event can
cause an “investigation” event, which can lead to an
“arrest” event (Chambers and Jurafsky, 2009). The
same observation holds in specific domains. For ex-
ample, the BioNLP’09 shared task (Kim et al., 2009)
focuses on the extraction of nested biomolecular
events, where, e.g., a REGULATION event causes a
TRANSCRIPTION event (see Figure 1a for a detailed
example). Despite this observation, many state-
of-the-art supervised event extraction models still
extract events and event arguments independently,
ignoring their underlying structure (Bj¨orne et al.,
2009; Miwa et al., 2010b).
In this paper, we propose a new approach for su-
pervised event extraction where we take the tree of
relations and their arguments and use it directly as
the representation in a dependency parser (rather
than conventional syntactic relations). Our approach
is conceptually simple: we first convert the origi-
nal representation of events and their arguments to
dependency trees by creating dependency arcs be-
tween event anchors (phrases that anchor events in
the supporting text) and their corresponding argu-
ments.1 Note that after conversion, only event an-
chors and entities remain. Figure 1 shows a sentence
and its converted form from the biomedical do-
main with four events: two POSITIVE REGULATION
events, anchored by the phrase “acts as a costim-
ulatory signal,” and two TRANSCRIPTION events,
both anchored on “gene transcription.” All events
take either protein entity mentions (PROT) or other
events as arguments. The latter is what allows for
nested event structures. Existing dependency pars-
ing models can be adapted to produce these seman-
tic structures instead of syntactic dependencies. We
built a global reranking parser model using multiple
decoders from MSTParser (McDonald et al., 2005;
McDonald et al., 2005b). The main contributions of
this paper are the following:
1. We demonstrate that parsing is an attractive ap-
proach for extracting events, both nested and
otherwise.
</bodyText>
<footnote confidence="0.974132">
1While our approach only works on trees, we show how we
can handle directed acyclic graphs in Section 5.
</footnote>
<page confidence="0.830852">
1626
</page>
<note confidence="0.9721205">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1626–1635,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.805837">
(a) Original sentence with nested events (b) After conversion to event dependencies
</figure>
<figureCaption confidence="0.990778">
Figure 1: Nested events in the text fragment: “... the HTLV-1 transactivator protein, tax, acts as a costim-
</figureCaption>
<bodyText confidence="0.6015365">
ulatory signal for GM-CSF and IL-2 gene transcription ... ” Throughout this paper, bold text indicates
instances of event anchors and italicized text denotes entities (PROTEINs in the BioNLP’09 domain). Note
that in (a) there are two copies of each type of event, which are merged to single nodes in the dependency
tree (Section 3.1).
</bodyText>
<listItem confidence="0.802231555555556">
2. We propose a wide range of features for event
extraction. Our analysis indicates that fea-
tures which model the global event structure
yield considerable performance improvements,
which proves that modeling event structure
jointly is beneficial.
3. We evaluate on the biomolecular event corpus
from the the BioNLP’09 shared task and show
that our approach obtains competitive results.
</listItem>
<sectionHeader confidence="0.999194" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999941981818182">
The pioneering work of Miller et al. (1997) was
the first, to our knowledge, to propose parsing as
a framework for information extraction. They ex-
tended the syntactic annotations of the Penn Tree-
bank corpus (Marcus et al., 1993) with entity and
relation mentions specific to the MUC-7 evalua-
tion (Chinchor et al., 1997) — e.g., EMPLOYEE OF
relations that hold between person and organization
named entities — and then trained a generative pars-
ing model over this combined syntactic and seman-
tic representation. In the same spirit, Finkel and
Manning (2009) merged the syntactic annotations
and the named entity annotations of the OntoNotes
corpus (Hovy et al., 2006) and trained a discrimina-
tive parsing model for the joint problem of syntac-
tic parsing and named entity recognition. However,
both these works require a unified annotation of syn-
tactic and semantic elements, which is not always
feasible, and focused only on named entities and bi-
nary relations. On the other hand, our approach fo-
cuses on event structures that are nested and have
an arbitrary number of arguments. We do not need
a unified syntactic and semantic representation (but
we can and do extract features from the underlying
syntactic structure of the text).
Finkel and Manning (2009b) also proposed a
parsing model for the extraction of nested named en-
tity mentions, which, like this work, parses just the
corresponding semantic annotations. In this work,
we focus on more complex structures (events instead
of named entities) and we explore more global fea-
tures through our reranking layer.
In the biomedical domain, two recent papers pro-
posed joint models for event extraction based on
Markov logic networks (MLN) (Riedel et al., 2009;
Poon and Vanderwende, 2010). Both works propose
elegant frameworks where event anchors and argu-
ments are jointly predicted for all events in the same
sentence. One disadvantage of MLN models is the
requirement that a human expert develop domain-
specific predicates and formulas, which can be a
cumbersome process because it requires thorough
domain understanding. On the other hand, our ap-
proach maintains the joint modeling advantage, but
our model is built over simple, domain-independent
features. We also propose and analyze a richer fea-
ture space that captures more information on the
global event structure in a sentence. Furthermore,
since our approach is agnostic to the parsing model
used, it could easily be tuned for various scenarios,
e.g., models with lower inference overhead such as
shift-reduce parsers.
Our work is conceptually close to the recent
CoNLL shared tasks on semantic role labeling,
where the predicate frames were converted to se-
</bodyText>
<page confidence="0.996796">
1627
</page>
<figureCaption confidence="0.7849296">
Figure 2: Overview of the approach. Rounded rect-
angles indicate domain-independent components;
regular rectangles mark domain-specific modules;
blocks in dashed lines surround components not nec-
essary for the domain presented in this paper.
</figureCaption>
<bodyText confidence="0.999933777777778">
mantic dependencies between predicates and their
arguments (Surdeanu et al., 2008; Hajic et al., 2009).
In this representation the dependency structure is a
directed acyclic graph (DAG), i.e., the same node
can be an argument to multiple predicates, and there
are no explicit dependencies between predicates.
Due to this representation, all joint models proposed
for semantic role labeling handle semantic frames
independently.
</bodyText>
<sectionHeader confidence="0.994345" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.9993071875">
Figure 2 summarizes our architecture. Our approach
converts the original event representation to depen-
dency trees containing both event anchors and entity
mentions, and trains a battery of parsers to recognize
these structures. The trees are built using event an-
chors predicted by a separate classifier. In this work,
we do not discuss entity recognition because in
the BioNLP’09 domain used for evaluation entities
(PROTEINs) are given (but including entity recog-
nition is an obvious extension of our model). Our
parsers are several instances of MSTParser2 (Mc-
Donald et al., 2005; McDonald et al., 2005b) con-
figured with different decoders. However, our ap-
proach is agnostic to the actual parsing models used
and could easily be adapted to other dependency
parsers. The output from the reranking parser is
</bodyText>
<footnote confidence="0.840144">
2http://sourceforge.net/projects/mstparser/
</footnote>
<bodyText confidence="0.999366857142857">
converted back to the original event representation
and passed to a reranker component (Collins, 2000;
Charniak and Johnson, 2005), tailored to optimize
the task-specific evaluation metric.
Note that although we use the biomedical event
domain from the BioNLP’09 shared task to illustrate
our work, the core of our approach is almost do-
main independent. Our only constraints are that each
event mention be activated by a phrase that serves as
an event anchor, and that the event-argument struc-
tures be mapped to a dependency tree. The conver-
sion between event and dependency structures and
the reranker metric are the only domain dependent
components in our approach.
</bodyText>
<subsectionHeader confidence="0.996096">
3.1 Converting between Event Structures and
Dependencies
</subsectionHeader>
<bodyText confidence="0.999875483870968">
As in previous work, we extract event structures at
sentence granularity, i.e., we ignore events which
span sentences (Bj¨orne et al., 2009; Riedel et al.,
2009; Poon and Vanderwende, 2010). These form
approximately 5% of the events in the BioNLP’09
corpus. For each sentence, we convert the
BioNLP’09 event representation to a graph (repre-
senting a labeled dependency tree) as follows. The
nodes in the graph are protein entity mentions, event
anchors, and a virtual ROOT node. Thus, the only
words in this dependency tree are those which par-
ticipate in events. We create edges in the graph in
the following way. For each event anchor, we cre-
ate one link to each of its arguments labeled with the
slot name of the argument (for example, connecting
gene transcription to IL-2 with the label THEME in
Figure 1b). We link the ROOT node to each entity
that does not participate in an event using the ROOT-
LABEL dependency label. Finally, we link the ROOT
node to each top-level event anchor, (those which do
not serve as arguments to other events) again using
the ROOT-LABEL label. We follow the convention
that the source of each dependency arc is the head
while the target is the modifier.
The output of this process is a directed graph,
since a phrase can easily play a role in two or more
events. Furthermore, the graph may contain self-
referential edges (self-loops) due to related events
sharing the same anchor (example below). To guar-
antee that the output of this process is a tree, we
must post-process the above graph with the follow-
</bodyText>
<figure confidence="0.999253533333333">
...
Parser 1
Parser k
Dependencies
to Events
Dependencies
to Events
Reranker
Events to
Dependencies
Event
Trigger
Recognizer
Entity
Recognizer
</figure>
<page confidence="0.938246">
1628
</page>
<bodyText confidence="0.996710166666667">
ing three heuristics:
Step 1: We remove self-referential edges. An exam-
ple of these can be seen in the text “the domain in-
teracted preferentially with underphosphorylated
TRAF2,” there are two events anchored by the same
underphosphorylated phrase, a NEGATIVE REGU-
LATION and a PHOSPHORYLATION event, and the
latter serves as a THEME argument for the former.
Due to the shared anchor, our conversion compo-
nent creates an self-referential THEME dependency.
By removing these edges, 1.5% of the events in the
training arguments are left without arguments, so we
remove them as well.
Step 2: We break structures where one argument par-
ticipates in multiple events, by keeping only the de-
pendency to the event that appears first in text. For
example, in the fragment “by enhancing its inactiva-
tion through binding to soluble TNF-alpha receptor
type II,” the protein TNF-alpha receptor type II is
an argument in both a BINDING event (binding) and
in a NEGATIVE REGULATION event (inactivation).
As a consequence of this step, 4.7% of the events in
training are removed.
Step 3: We unify events with the same types an-
chored on the same anchor phrase. For example,
for the fragment “Surface expression of intercellu-
lar adhesion molecule-1, P-selectin, and E-selectin,”
the BioNLP’09 annotation contains three distinct
GENE EXPRESSION events anchored on the same
phrase (expression), each having one of the proteins
as THEMEs. In such cases, we migrate all arguments
to one of the events, and remove the empty events.
21.5% of the events in training are removed in this
step (but no dependencies are lost).
Note that we do not guarantee that the resulting
tree is projective. In fact, our trees are more likely
to be non-projective than syntactic dependency trees
of English sentences, because in our representation
many nodes can be linked directly to the ROOT node.
Our analysis indicates that 2.9% of the dependencies
generated in the training corpus are non-projective
and 7.9% of the sentences contain at least one non-
projective dependency (for comparison, these num-
bers for the English Penn Treebank are 0.3% and
6.7%, respectively).
After parsing, we implement the inverse process,
i.e., we convert the generated dependency trees to
the BioNLP’09 representation. In addition to the
obvious conversions, this process implements the
heuristics proposed by Bj¨orne et al. (2009), which
reverse step 3 above, e.g., we duplicate GENE EX-
PRESSION events with multiple THEME arguments.
The heuristics are executed sequentially in the given
order:
</bodyText>
<listItem confidence="0.991346545454546">
1. Since all non-BINDING events can have at
most one THEME argument, we duplicate non-
BINDING events with multiple THEME argu-
ments by creating one separate event for each
THEME.
2. Similarly, since REGULATION events accepts
only one CAUSE argument, we duplicate REG-
ULATION events with multiple CAUSE argu-
ments, obtaining one event per CAUSE.
3. Lastly, we implement the heuristic of Bj¨orne et
al. (2009) to handle the splitting of BINDING
</listItem>
<bodyText confidence="0.987384222222222">
events with multiple THEME arguments. This is
more complex because these events can accept
one or more THEMEs. In such situations, we
first group THEME arguments by the label of the
first Stanford dependency (Marneffe and Man-
ning, 2008) from the head word of the anchor
to this argument. Then we create one event for
each combination of THEME arguments in dif-
ferent groups.
</bodyText>
<subsectionHeader confidence="0.999942">
3.2 Recognition of Event Anchors
</subsectionHeader>
<bodyText confidence="0.833446058823529">
For anchor detection, we used a multiclass classifier
that labels each token independently.3 Since over
92% of the anchor phrases in our evaluation domain
contain a single word, we simplify the task by re-
ducing all multi-word anchor phrases in the training
corpus to their syntactic head word (e.g., “acts” for
the anchor “acts as a costimulatory signal”).
We implemented this model using a logistic re-
gression classifier with L2 regularization over the
following features:
3We experimented with using conditional random fields as a
sequence labeler but did not see improvements in the biomed-
ical domain. We hypothesize that the sequence tagger fails to
capture potential dependencies between anchor labels – which
are its main advantage over an i.i.d. classifier – because anchor
words are typically far apart in text. This result is consistent
with observations in previous work (Bj¨orne et al., 2009).
</bodyText>
<page confidence="0.975777">
1629
</page>
<listItem confidence="0.998411391304348">
• Token-level: The form, lemma, and whether
the token is present in a gazetteer of known an-
chor words.4
• Surface context: The above token features ex-
tracted from a context of two words around the
current token. Additionally, we build token bi-
grams in this context window, and model them
with similar features.
• Syntactic context: We model all syntactic de-
pendency paths up to depth two starting from
the token to be classified. These paths are built
from Stanford syntactic dependencies (Marn-
effe and Manning, 2008). We extract token
features from the first and last token in these
paths. We also generate combination features
by concatenating: (a) the last token in each path
with the sequence of dependency labels along
the corresponding path; and (b) the word to be
classified, the last token in each path, and the
sequence of dependency labels in that path.
• Bag-of-word and entity count: Extracted
from (a) the entire sentence, and (b) a window
of five words around the token to be classified.
</listItem>
<subsectionHeader confidence="0.998832">
3.3 Parsing Event Structures
</subsectionHeader>
<bodyText confidence="0.9999471">
Given the entities and event anchors from the pre-
vious stages in the pipeline, the parser generates la-
beled dependency links between them. Many de-
pendency parsers are available and we chose MST-
Parser for its ability to produce non-projective and
n-best parses directly. MSTParser frames parsing
as a graph algorithm. To parse a sentence, MST-
Parser finds the tree covering all the words (nodes)
in the sentence (graph) with the largest sum of edge
weights, i.e., the maximum weighted spanning tree.
Each labeled, directed edge in the graph represents a
possible dependency between its two endpoints and
has an associated score (weight). Scores for edges
come from the dot product between the edge’s corre-
sponding feature vector and learned feature weights.
As a result, all features for MSTParser must be edge-
factored, i.e., functions of both endpoints and the la-
bel connecting them. McDonald et al. (2006) ex-
tends the basic model to include second-order de-
pendencies (i.e., two adjacent sibling nodes and their
</bodyText>
<footnote confidence="0.626598">
4These are automatically extracted from the training corpus.
</footnote>
<bodyText confidence="0.999452">
parent). Both first and second-order modes include
projective and non-projective decoders.
Our features for MSTParser use both the event
structures themselves as well as the surrounding
English sentences which include them. By map-
ping event anchors and entities back to the original
text, we can incorporate information from the orig-
inal English sentence as well its syntactic tree and
corresponding Stanford dependencies. Both forms
of context are valuable and complementary. MST-
Parser comes with a large number of features which,
in our setup, operate on the event structure level
(since this is the “sentence” from the parser’s point
of view). The majority of additional features that
we introduced take advantage of the original text as
context (primarily its associated Stanford dependen-
cies). Our system includes the following first-order
features:
</bodyText>
<listItem confidence="0.972773482758621">
• Path: Syntactic paths in the original sentence
between nodes in an event dependency (as in
previous work by Bj¨orne et al. (2009)). These
have many variations including using Stanford
dependencies (“collapsed” and “uncollapsed”)
or constituency trees as sources, optionally lex-
icalizing the path, and using words or relation
names along the path. Additionally, we include
the bucketed length of the paths.
• Original sentence words: Words from the full
English sentence surrounding and between the
nodes in event dependencies, and their buck-
eted distances. This additional context helps
compensate for how our anchor detection pro-
vides only the head word of each anchor, which
does not necessarily provide the full context for
event disambiguation.
• Graph: Parents, children, and siblings of
nodes in the Stanford dependencies graph
along with label of the edge. This provides ad-
ditional syntactic context.
• Consistency: Soft constraints on edges be-
tween anchors and their arguments (e.g., only
regulation events can have edges labeled with
CAUSE). These features fire if their constraints
are violated.
• Ontology: Generalized types of the end-
points of edges using a given type hierar-
chy (e.g., POSITIVE REGULATION is a COM-
</listItem>
<page confidence="0.969804">
1630
</page>
<bodyText confidence="0.999891054054054">
PLEX EVENT5 is an EVENT). Values of
this feature are coded with the types of each
of the endpoints on an edge, running over
the cross-product of types for each endpoint.
For instance, an edge between a BINDING
event anchor and a POSITIVE REGULATION
could cause this feature to fire with the val-
ues [head:EVENT, child:COMPLEX EVENT] or
[head:SIMPLE EVENT, child:EVENT].6 The lat-
ter feature can capture generalizations such as
“simple event anchors cannot take other events
as arguments.”
Both Consistency and Ontology feature classes in-
clude domain-specific information but can be used
on other domains under different constraints and
type hierarchies. When using second-order de-
pendencies, we use additional Path and Ontol-
ogy features. We include the syntactic paths be-
tween sibling nodes (adjacent arguments of the same
event anchor). These Path features are as above
but differentiated as paths between sibling nodes.
The second-order Ontology features use the type
hierarchy information on both sibling nodes and
their parent. For example, a POSITIVE REGULA-
TION anchor attached to a PROTEIN and a BINDING
event would produce an Ontology feature with the
value [parent:COMPLEX EVENT, child1:PROTEIN,
child2:SIMPLE EVENT] (among several other possi-
ble combinations).
To prune the number of features used, we employ
a simple entropy-based measure. Our intuition is
that good features should typically appear with only
one edge label.7 Given all edges enumerated during
training and their gold labels, we obtain a distribu-
tion over edge labels (df) for each feature f. Given
this distribution and the frequency of a feature, we
can score the feature with the following:
</bodyText>
<equation confidence="0.999179">
score(f) = α x lo92 Qreq(f)) − H(df)
</equation>
<bodyText confidence="0.99983425">
The α parameter adjusts the relative weight of the
two components. The log frequency component fa-
vors more frequent features while the entropy com-
ponent favors features with low entropy in their edge
</bodyText>
<footnote confidence="0.973564">
5We define complex events are those which can accept other
events are arguments. Simple events can only take PROTEINs.
6We omit listing the other two combinations.
7Labels include ROOT-LABEL, THEME, CAUSE, and NULL.
We assign the NULL label to edges which aren’t in the gold data.
</footnote>
<bodyText confidence="0.986797">
label distribution. Features are pruned by accepting
all features with a score above a certain threshold.
</bodyText>
<subsectionHeader confidence="0.994125">
3.4 Reranking Event Structures
</subsectionHeader>
<bodyText confidence="0.999978756756757">
When decoding, the parser finds the highest scoring
tree which incorporates global properties of the sen-
tence. However, its features are edge-factored and
thus unable to take into account larger contexts. To
incorporate arbitrary global features, we employ a
two-step reranking parser. For the first step, we ex-
tend our parser to output its n-best parses instead
of just its top scoring parse. In the second step, a
discriminative reranker rescores each parse and re-
orders the n-best list. Rerankers have been success-
fully used in syntactic parsing (Collins, 2000; Char-
niak and Johnson, 2005; Huang, 2008) and semantic
role labeling (Toutanova et al., 2008).
Rerankers provide additional advantages in our
case due to the mismatch between the dependency
structures that the parser operates on and their cor-
responding event structures. We convert the out-
put from the parser to event structures (Section 3.1)
before including them in the reranker. This al-
lows the reranker to capture features over the ac-
tual event structures rather than their original de-
pendency trees which may contain extraneous por-
tions.8 Furthermore, this lets the reranker optimize
the actual BioNLP F1 score. The parser, on the other
hand, attempts to optimize the Labeled Attachment
Score (LAS) between the dependency trees and con-
verted gold dependency trees. LAS is approximate
for two reasons. First, it is much more local than
the BioNLP metric.9 Second, the converted gold de-
pendency trees lose information that doesn’t transfer
to trees (specifically, that event structures are really
multi-DAGs and not trees).
We adapt the maximum entropy reranker from
Charniak and Johnson (2005) by creating a cus-
tomized feature extractor for event structures — in
all other ways, the reranker model is unchanged. We
use the following types of features in the reranker:
</bodyText>
<listItem confidence="0.996363">
• Source: Score and rank of the parse from the
</listItem>
<footnote confidence="0.999522833333333">
8For instance, event anchors with no arguments could be
proposed by the parser. These event anchors are automatically
dropped by the conversion process.
9As an example, getting an edge label between an anchor
and its argument correct is unimportant if the anchor is missing
other arguments.
</footnote>
<page confidence="0.946532">
1631
</page>
<table confidence="0.9908892">
Unreranked Reranked
Decoder(s) R P F1 R P F1
1P 65.6 76.7 70.7 68.0 77.6 72.5
2P 67.4 77.1 71.9 67.9 77.3 72.3
1N 67.5 76.7 71.8 — — —
2N 68.9 77.1 72.7 — — —
1P, 2P, 2N — — — 68.5 78.2 73.1
Unreranked Reranked
Decoder(s) R P F1 R P F1
1P 44.7 62.2 52.0 47.8 59.6 53.1
2P 45.9 61.8 52.7 48.4 57.5 52.5
1N 46.0 61.2 52.5 — — —
2N 38.6 66.6 48.8 — — —
1P, 2P, 2N — — — 48.7 59.3 53.5
(a) Gold event anchors (b) Predicted event anchors
</table>
<tableCaption confidence="0.941657">
Table 1: BioNLP recall, precision, and F1 scores of individual decoders and the best decoder combination
on development data with the impact of event anchor detection and reranking. Decoder names include the
features order (1 or 2) followed by the projectivity (P = projective, N = non-projective).
</tableCaption>
<bodyText confidence="0.5335535">
decoder; number of different decoders produc-
ing the parse (when using multiple decoders).
</bodyText>
<listItem confidence="0.988183">
• Event path: Path from each node in the event
tree up to the root. Unlike the Path features
in the parser, these paths are over event struc-
tures, not the syntactic dependency graphs from
the original English sentence. Variations of the
Event path features include whether to include
word forms (e.g., “binds”), types (BINDING),
and/or argument slot names (THEME). We also
include the path length as a feature.
• Event frames: Event anchors with all their ar-
guments and argument slot names.
• Consistency: Similar to the parser Consis-
tency features, but capable of capturing larger
classes of errors (e.g., incorrect number or
types of arguments). We include the number of
violations from four different classes of errors.
</listItem>
<bodyText confidence="0.978452461538461">
To improve performance and robustness, features
are pruned as in Charniak and Johnson (2005): se-
lected features must distinguish a parse with the
highest F1 score in a n-best list, from a parse with a
suboptimal F1 score at least five times.
Rerankers can also be used to perform model
combination (Toutanova et al., 2008; Zhang et al.,
2009; Johnson and Ural, 2010). While we use a sin-
gle parsing model, it has multiple decoders.10 When
combining multiple decoders, we concatenate their
n-best lists and extract the unique parses.
10We only have n-best versions of the projective decoders.
For the non-projective decoders, we use their 1-best parse.
</bodyText>
<sectionHeader confidence="0.994036" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999862366666667">
Our experiments use the BioNLP’09 shared task
corpus (Kim et al., 2009) which includes 800
biomedical abstracts (7,449 sentences, 8,597 events)
for training and 150 abstracts (1,450 sentences,
1,809 events) for development. The test set includes
260 abstracts, 2,447 sentences, and 3,182 events.
Throughout our experiments, we report BioNLP F1
scores with approximate span and recursive event
matching (as described in the shared task definition).
For preprocessing, we parsed all documents us-
ing the self-trained biomedical McClosky-Charniak-
Johnson reranking parser (McClosky, 2010). We
bias the anchor detector to favor recall, allowing the
parser and reranker to determine which event an-
chors will ultimately be used. When performing n-
best parsing, n = 50. For parser feature pruning,
α = 0.001.
Table 1a shows the performance of each of the de-
coders when using gold event anchors. In both cases
where n-best decoding is available, the reranker im-
proves performance over the 1-best parsers. We also
present the results from a reranker trained from mul-
tiple decoders which is our highest scoring model.11
In Table 1b, we present the output for the predicted
anchor scenario. In the case of the 2P decoder,
the reranker does not improve performance, though
the drop is minimal. This is because the reranker
chose an unfortunate regularization constant during
crossvalidation, most likely due to the small size of
the training data. In later experiments where more
</bodyText>
<footnote confidence="0.8704705">
11Including the 1N decoder as well provided no gains, possi-
bly because its outputs are mostly subsumed by the 2N decoder.
</footnote>
<page confidence="0.993891">
1632
</page>
<bodyText confidence="0.999850644444444">
data is available, the reranker consistently improves
accuracy (McClosky et al., 2011). As before, the
reranker trained from multiple decoders outperforms
unreranked models and reranked single decoders.
All in all, our best model in Table 1a scores 1 F1
point higher than the best system at the BioNLP’09
shared task, and the best model in Table 1b performs
similarly to the best shared task system (Bj¨orne et
al., 2009), which also scores 53.5% on development.
We show the effects of each system component
in Table 2. Note how our upper limit is 87.1%
due to our conversion process, which enforces the
tree constraint, drops events spanning sentences, and
performs approximate reconstruction of BINDING
events. Given that state-of-the-art systems on this
task currently perform in the 50-60% range, we are
not troubled by this number as it still allows for
plenty of potential.12 Bj¨orne et al. (2009) list 94.7%
as the upper limit for their system. Considering
this relatively large difference, we find the results
in the previous table very encouraging. As in other
BioNLP’09 systems, our performance drops when
switching from gold to predicted anchor informa-
tion. Our decrease is similar to the one seen in
Bj¨orne et al. (2009).
To show the potential of reranking, we provide or-
acle reranker scores in Table 3. An oracle reranker
picks the highest scoring parse from the available
parses. We limit the n-best lists to the top k parses
where k E 11, 2, 10, All}. For single decoders,
“All” uses the entire 50-best list. For multiple de-
coders, the n-best lists are concatenated together.
The oracle score with multiple decoders and gold
anchors is only 0.4% lower than our upper limit (see
Table 2). This indicates that parses which could have
achieved that limit were nearly always present. Im-
proving the features in the reranker as well as the
original parsers will help us move closer to the limit.
With predicated anchors, the oracle score is about
13% lower but still shows significant potential.
Our final results on the test set, broken down by
class, are shown in Table 4. As with other systems,
complex events (e.g., REGULATION) prove harder
than simple events. To get a complex event cor-
rect, one must correctly detect and parse all events in
</bodyText>
<footnote confidence="0.473590666666667">
12Additionally, improvements such as document-level pars-
ing and DAG parsing would eliminate the need for much of the
approximate and lossy portions of the conversion process.
</footnote>
<table confidence="0.999674166666667">
AD Parse RR Conv R P F1
✓ ✓ ✓ 45.9 61.8 52.7
✓ ✓ ✓ ✓ 48.7 59.3 53.5
G ✓ ✓ 68.9 77.1 72.7
G ✓ ✓ ✓ 68.5 78.2 73.1
G G G ✓ 81.6 93.4 87.1
</table>
<tableCaption confidence="0.843696375">
Table 2: Effect of each major component to the over-
all performance in the development corpus. Compo-
nents shown: AD — event anchor detection; Parse
— best individual parsing model; RR — reranking
multiple parsers; Conv — conversion between the
event and dependency representations. ‘G’ indicates
that gold data was used; ‘✓’ indicates that the actual
component was used.
</tableCaption>
<table confidence="0.999072">
n-best parses considered
Anchors Decoder(s) 1 2 10 All
1P 70.7 76.6 84.0 85.7
Gold 2P 71.8 77.5 84.8 86.2
1P, 2P, 2N — — — 86.7
1P 52.0 60.3 69.9 72.5
Predicted 2P 52.7 60.7 70.1 72.5
1P, 2P, 2N — — — 73.4
</table>
<tableCaption confidence="0.986839">
Table 3: Oracle reranker BioNLP F1 scores for
</tableCaption>
<bodyText confidence="0.9948921">
our n-best decoders and their combinations before
reranking on the development corpus.
the event subtree allowing small errors to have large
effects. Top systems on this task obtain F1 scores
of 52.0% at the shared task evaluation (Bj¨orne et
al., 2009) and 56.3% post evaluation (Miwa et al.,
2010a). However, both systems are tailored to the
biomedical domain (the latter uses multiple syntac-
tic parsers), whereas our system has a design that is
virtually domain independent.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999746333333333">
We believe that the potential of our approach is
higher than what the current experiments show. For
example, the reranker can be used to combine not
only several parsers but also multiple anchor rec-
ognizers. This passes the anchor selection decision
to the reranker, which uses global information not
available to the current anchor recognizer or parser.
Furthermore, our approach can be adapted to parse
event structures in entire documents (instead of in-
</bodyText>
<page confidence="0.894063">
1633
</page>
<table confidence="0.999660545454545">
Event Class Count R P F1
Gene Expression 722 68.6 75.8 72.0
Transcription 137 42.3 51.3 46.4
Protein Catabolism 14 64.3 75.0 69.2
Phosphorylation 135 80.0 82.4 81.2
Localization 174 44.8 78.8 57.1
Binding 347 42.9 51.7 46.9
Regulation 291 23.0 36.6 28.3
Positive Regulation 983 28.4 42.5 34.0
Negative Regulation 379 29.3 43.5 35.0
Total 3,182 42.6 56.6 48.6
</table>
<tableCaption confidence="0.998624">
Table 4: Results in the test set broken by event class;
</tableCaption>
<bodyText confidence="0.99380275">
scores generated with the main official metric of ap-
proximate span and recursive event matching.
dividual sentences) by using a representation with a
unique ROOT node for all event structures in a doc-
ument. This representation has the advantage that
it maintains cross-sentence events (which account
for 5% of BioNLP’09 events), and it allows for
document-level features that model discourse struc-
ture. We plan to explore these ideas in future work.
One current limitation of the proposed model is
that it constrains event structures to map to trees. In
the BioNLP’09 corpus this leads to the removal of
almost 5% of the events, which generate DAGs in-
stead of trees. Local event extraction models (Bj¨orne
et al., 2009) do not have this limitation, because
their local decisions are blind to (and hence not
limited by) the global event structure. However,
our approach is agnostic to the actual parsing mod-
els used, so we can easily incorporate models that
can parse DAGs (Sagae and Tsujii, 2008). Addi-
tionally, we are free to incorporate any new tech-
niques from dependency parsing. Parsing using
dual-decomposition (Rush et al., 2010) seems espe-
cially promising in this area.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99911475">
In this paper we proposed a simple approach for the
joint extraction of event structures: we converted
the representation of events and their arguments to
dependency trees with arcs between event anchors
and event arguments, and used a reranking parser to
parse these structures. Despite the fact that our ap-
proach has very little domain-specific engineering,
we obtain competitive results. Most importantly, we
showed that the joint modeling of event structures is
beneficial: our reranker outperforms parsing models
without reranking in five out of the six configura-
tions investigated.
</bodyText>
<sectionHeader confidence="0.995469" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999919833333333">
The authors would like to thank Mark Johnson for
helpful discussions on the reranker component and
the BioNLP shared task organizers, Sampo Pyysalo
and Jin-Dong Kim, for answering questions. We
gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-
C-0181. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of DARPA, AFRL, or the US government.
</bodyText>
<sectionHeader confidence="0.997077" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995614">
Jari Bj¨orne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting Complex Biological Events with Rich Graph-
Based Feature Sets. Proceedings of the Workshop on
BioNLP: Shared Task.
Nate Chambers and Dan Jurafsky. 2009. Unsupervised
Learning of Narrative Schemas and their Participants.
Proceedings of ACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 2005 Meeting of the As-
sociation for Computational Linguistics (ACL), pages
173–180
Nancy Chinchor. 1997. Overview of MUC-7. Pro-
ceedings of the Message Understanding Conference
(MUC-7).
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Machine Learning: Pro-
ceedings of the Seventeenth International Conference
(ICML 2000), pages 175–182.
Jenny R. Finkel and Christopher D. Manning. 2009.
Joint Parsing and Named Entity Recognition. Pro-
ceedings of NAACL.
Jenny R. Finkel and Christopher D. Manning. 2009b.
Nested Named Entity Recognition. Proceedings of
EMNLP.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria A. Marti, Lluis Marquez,
Adam Meyers, Joakim Nivre, Sebastian Pado, Jan
Stepanek, Pavel Stranak, Mihai Surdeanu, Nianwen
</reference>
<page confidence="0.866712">
1634
</page>
<reference confidence="0.999723083333333">
Xue, and Yi Zhang. 2009. The CoNLL-2009 Shared
Task: Syntactic and Semantic Dependencies in Multi-
ple Languages. Proceedings of CoNLL.
Eduard Hovy, Mitchell P. Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. Proceedings of the NAACL-HLT.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586–594, Association for Com-
putational Linguistics.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. Proceedings of
NAACL.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview
of the BioNLP’09 Shared Task on Event Extrac-
tion. Proceedings of the NAACL-HLT 2009 Work-
shop on Natural Language Processing in Biomedicine
(BioNLP’09).
Mitchell P. Marcus, Beatrice Santorini, and Marry Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford Typed Hierarchies Represen-
tation. Proceedings of the COLING Workshop on
Cross-Framework and Cross-Domain Parser Evalua-
tion.
David McClosky. 2010. Any Domain Parsing: Auto-
matic Domain Adaptation for Natural Language Pars-
ing. PhD thesis, Department of Computer Science,
Brown University.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency pars-
ing in BioNLP 2011. In BioNLP 2011 Shared Task
(submitted).
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. Proceedings of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005b. Non-projective Dependency Pars-
ing using Spanning Tree Algorithms. Proceedings of
HLT/EMNLP.
Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Algo-
rithms. Proceedings of EACL.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone, and
Ralph Weischedel. 1997. BBN: Description of the
SIFT System as Used for MUC-7. Proceedings of the
Message Understanding Conference (MUC-7).
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun’ichi Tsujii. 2010a. A Comparative Study of Syn-
tactic Parsers for Event Extraction. Proceedings of
the 2010 Workshop on Biomedical Natural Language
Processing.
Makoto Miwa, Rune Saetre, Jin-Dong Kim, and Jun’ichi
Tsujii. 2010b. Event Extraction with Complex Event
Classification Using Rich Features. Journal of Bioin-
formatics and Computational Biology, 8 (1).
Hoifung Poon and Lucy Vanderwende. 2010. Joint Infer-
ence for Knowledge Extraction from Biomedical Liter-
ature. Proceedings of NAACL.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun’ichi Tsujii. 2009. A Markov Logic Approach
to Bio-Molecular Event Extraction. Proceedings of the
Workshop on BioNLP: Shared Task.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. Proceedings of EMNLP.
Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-Reduce De-
pendency DAG Parsing. Proceedings of the COLING.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluis Marquez, and Joakim Nivre. 2008. The CoNLL-
2008 Shared Task on Joint Parsing of Syntactic and
Semantic Dependencies. Proceedings of CoNLL.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A Global Joint Model for Semantic
Role Labeling. Computational Linguistics 34(2).
Zhang, H. and Zhang, M. and Tan, C.L. and Li, H. 2009.
K-best combination of syntactic parsers. Proceedings
of Empirical Methods in Natural Language Process-
ing.
</reference>
<page confidence="0.99236">
1635
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.488911">
<title confidence="0.999973">Event Extraction as Dependency Parsing</title>
<author confidence="0.999005">David McClosky</author>
<author confidence="0.999005">Mihai Surdeanu</author>
<author confidence="0.999005">D Christopher</author>
<affiliation confidence="0.761336">Department of Computer Stanford</affiliation>
<address confidence="0.89127">Stanford, CA</address>
<abstract confidence="0.999255">Nested event structures are a common occurrence in both open domain and domain specific extraction tasks, e.g., a “crime” event can cause a “investigation” event, which can lead to an “arrest” event. However, most current approaches address event extraction with highly local models that extract each event and argument independently. We propose a simple approach for the extraction of such structures by taking the tree of event-argument relations and using it directly as the representation in a reranking dependency parser. This provides a simple framework that captures global properties of both nested and flat event structures. We explore a rich feature space that models both the events to be parsed and context from the original supporting text. Our approach obtains competitive results in the extraction of biomedical events from the BioNLP’09 shared task with a F1 score of 53.5% in development and 48.6% in testing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jari Bj¨orne</author>
<author>Juho Heimonen</author>
<author>Filip Ginter</author>
<author>Antti Airola</author>
<author>Tapio Pahikkala</author>
<author>Tapio Salakoski</author>
</authors>
<title>Extracting Complex Biological Events with Rich GraphBased Feature Sets.</title>
<date>2009</date>
<booktitle>Proceedings of the Workshop on BioNLP: Shared Task.</booktitle>
<marker>Bj¨orne, Heimonen, Ginter, Airola, Pahikkala, Salakoski, 2009</marker>
<rawString>Jari Bj¨orne, Juho Heimonen, Filip Ginter, Antti Airola, Tapio Pahikkala, and Tapio Salakoski. 2009. Extracting Complex Biological Events with Rich GraphBased Feature Sets. Proceedings of the Workshop on BioNLP: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nate Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised Learning of Narrative Schemas and their Participants.</title>
<date>2009</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="1355" citStr="Chambers and Jurafsky, 2009" startWordPosition="203" endWordPosition="206">endency parser. This provides a simple framework that captures global properties of both nested and flat event structures. We explore a rich feature space that models both the events to be parsed and context from the original supporting text. Our approach obtains competitive results in the extraction of biomedical events from the BioNLP’09 shared task with a F1 score of 53.5% in development and 48.6% in testing. 1 Introduction Event structures in open domain texts are frequently highly complex and nested: a “crime” event can cause an “investigation” event, which can lead to an “arrest” event (Chambers and Jurafsky, 2009). The same observation holds in specific domains. For example, the BioNLP’09 shared task (Kim et al., 2009) focuses on the extraction of nested biomolecular events, where, e.g., a REGULATION event causes a TRANSCRIPTION event (see Figure 1a for a detailed example). Despite this observation, many stateof-the-art supervised event extraction models still extract events and event arguments independently, ignoring their underlying structure (Bj¨orne et al., 2009; Miwa et al., 2010b). In this paper, we propose a new approach for supervised event extraction where we take the tree of relations and the</context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nate Chambers and Dan Jurafsky. 2009. Unsupervised Learning of Narrative Schemas and their Participants. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>173--180</pages>
<contexts>
<context position="8789" citStr="Charniak and Johnson, 2005" startWordPosition="1357" endWordPosition="1360"> because in the BioNLP’09 domain used for evaluation entities (PROTEINs) are given (but including entity recognition is an obvious extension of our model). Our parsers are several instances of MSTParser2 (McDonald et al., 2005; McDonald et al., 2005b) configured with different decoders. However, our approach is agnostic to the actual parsing models used and could easily be adapted to other dependency parsers. The output from the reranking parser is 2http://sourceforge.net/projects/mstparser/ converted back to the original event representation and passed to a reranker component (Collins, 2000; Charniak and Johnson, 2005), tailored to optimize the task-specific evaluation metric. Note that although we use the biomedical event domain from the BioNLP’09 shared task to illustrate our work, the core of our approach is almost domain independent. Our only constraints are that each event mention be activated by a phrase that serves as an event anchor, and that the event-argument structures be mapped to a dependency tree. The conversion between event and dependency structures and the reranker metric are the only domain dependent components in our approach. 3.1 Converting between Event Structures and Dependencies As in</context>
<context position="22467" citStr="Charniak and Johnson, 2005" startWordPosition="3562" endWordPosition="3566">ld. 3.4 Reranking Event Structures When decoding, the parser finds the highest scoring tree which incorporates global properties of the sentence. However, its features are edge-factored and thus unable to take into account larger contexts. To incorporate arbitrary global features, we employ a two-step reranking parser. For the first step, we extend our parser to output its n-best parses instead of just its top scoring parse. In the second step, a discriminative reranker rescores each parse and reorders the n-best list. Rerankers have been successfully used in syntactic parsing (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008) and semantic role labeling (Toutanova et al., 2008). Rerankers provide additional advantages in our case due to the mismatch between the dependency structures that the parser operates on and their corresponding event structures. We convert the output from the parser to event structures (Section 3.1) before including them in the reranker. This allows the reranker to capture features over the actual event structures rather than their original dependency trees which may contain extraneous portions.8 Furthermore, this lets the reranker optimize the actual BioNLP F1 score. The parser</context>
<context position="25692" citStr="Charniak and Johnson (2005)" startWordPosition="4111" endWordPosition="4114">the original English sentence. Variations of the Event path features include whether to include word forms (e.g., “binds”), types (BINDING), and/or argument slot names (THEME). We also include the path length as a feature. • Event frames: Event anchors with all their arguments and argument slot names. • Consistency: Similar to the parser Consistency features, but capable of capturing larger classes of errors (e.g., incorrect number or types of arguments). We include the number of violations from four different classes of errors. To improve performance and robustness, features are pruned as in Charniak and Johnson (2005): selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times. Rerankers can also be used to perform model combination (Toutanova et al., 2008; Zhang et al., 2009; Johnson and Ural, 2010). While we use a single parsing model, it has multiple decoders.10 When combining multiple decoders, we concatenate their n-best lists and extract the unique parses. 10We only have n-best versions of the projective decoders. For the non-projective decoders, we use their 1-best parse. 4 Experimental Results Our experiments us</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 2005 Meeting of the Association for Computational Linguistics (ACL), pages 173–180</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<title>Overview of MUC-7.</title>
<date>1997</date>
<booktitle>Proceedings of the Message Understanding Conference (MUC-7).</booktitle>
<marker>Chinchor, 1997</marker>
<rawString>Nancy Chinchor. 1997. Overview of MUC-7. Proceedings of the Message Understanding Conference (MUC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Machine Learning: Proceedings of the Seventeenth International Conference (ICML</booktitle>
<pages>175--182</pages>
<contexts>
<context position="8760" citStr="Collins, 2000" startWordPosition="1355" endWordPosition="1356">ity recognition because in the BioNLP’09 domain used for evaluation entities (PROTEINs) are given (but including entity recognition is an obvious extension of our model). Our parsers are several instances of MSTParser2 (McDonald et al., 2005; McDonald et al., 2005b) configured with different decoders. However, our approach is agnostic to the actual parsing models used and could easily be adapted to other dependency parsers. The output from the reranking parser is 2http://sourceforge.net/projects/mstparser/ converted back to the original event representation and passed to a reranker component (Collins, 2000; Charniak and Johnson, 2005), tailored to optimize the task-specific evaluation metric. Note that although we use the biomedical event domain from the BioNLP’09 shared task to illustrate our work, the core of our approach is almost domain independent. Our only constraints are that each event mention be activated by a phrase that serves as an event anchor, and that the event-argument structures be mapped to a dependency tree. The conversion between event and dependency structures and the reranker metric are the only domain dependent components in our approach. 3.1 Converting between Event Stru</context>
<context position="22439" citStr="Collins, 2000" startWordPosition="3560" endWordPosition="3561">certain threshold. 3.4 Reranking Event Structures When decoding, the parser finds the highest scoring tree which incorporates global properties of the sentence. However, its features are edge-factored and thus unable to take into account larger contexts. To incorporate arbitrary global features, we employ a two-step reranking parser. For the first step, we extend our parser to output its n-best parses instead of just its top scoring parse. In the second step, a discriminative reranker rescores each parse and reorders the n-best list. Rerankers have been successfully used in syntactic parsing (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008) and semantic role labeling (Toutanova et al., 2008). Rerankers provide additional advantages in our case due to the mismatch between the dependency structures that the parser operates on and their corresponding event structures. We convert the output from the parser to event structures (Section 3.1) before including them in the reranker. This allows the reranker to capture features over the actual event structures rather than their original dependency trees which may contain extraneous portions.8 Furthermore, this lets the reranker optimize the actual</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Machine Learning: Proceedings of the Seventeenth International Conference (ICML 2000), pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint Parsing and Named Entity Recognition.</title>
<date>2009</date>
<booktitle>Proceedings of NAACL.</booktitle>
<contexts>
<context position="5009" citStr="Finkel and Manning (2009)" startWordPosition="773" endWordPosition="776">d show that our approach obtains competitive results. 2 Related Work The pioneering work of Miller et al. (1997) was the first, to our knowledge, to propose parsing as a framework for information extraction. They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997) — e.g., EMPLOYEE OF relations that hold between person and organization named entities — and then trained a generative parsing model over this combined syntactic and semantic representation. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al., 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. However, both these works require a unified annotation of syntactic and semantic elements, which is not always feasible, and focused only on named entities and binary relations. On the other hand, our approach focuses on event structures that are nested and have an arbitrary number of arguments. We do not need a unified syntactic and semantic representation (but we can a</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny R. Finkel and Christopher D. Manning. 2009. Joint Parsing and Named Entity Recognition. Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Nested Named Entity Recognition.</title>
<date>2009</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5009" citStr="Finkel and Manning (2009)" startWordPosition="773" endWordPosition="776">d show that our approach obtains competitive results. 2 Related Work The pioneering work of Miller et al. (1997) was the first, to our knowledge, to propose parsing as a framework for information extraction. They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997) — e.g., EMPLOYEE OF relations that hold between person and organization named entities — and then trained a generative parsing model over this combined syntactic and semantic representation. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al., 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. However, both these works require a unified annotation of syntactic and semantic elements, which is not always feasible, and focused only on named entities and binary relations. On the other hand, our approach focuses on event structures that are nested and have an arbitrary number of arguments. We do not need a unified syntactic and semantic representation (but we can a</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny R. Finkel and Christopher D. Manning. 2009b. Nested Named Entity Recognition. Proceedings of EMNLP.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria A Marti</author>
</authors>
<location>Lluis Marquez, Adam Meyers, Joakim Nivre, Sebastian Pado, Jan Stepanek, Pavel Stranak, Mihai Surdeanu, Nianwen</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Marti, </marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria A. Marti, Lluis Marquez, Adam Meyers, Joakim Nivre, Sebastian Pado, Jan Stepanek, Pavel Stranak, Mihai Surdeanu, Nianwen</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xue</author>
<author>Yi Zhang</author>
</authors>
<date>2009</date>
<booktitle>The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. Proceedings of CoNLL.</booktitle>
<marker>Xue, Zhang, 2009</marker>
<rawString>Xue, and Yi Zhang. 2009. The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell P Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: The 90% Solution.</title>
<date>2006</date>
<booktitle>Proceedings of the NAACL-HLT.</booktitle>
<contexts>
<context position="5119" citStr="Hovy et al., 2006" startWordPosition="790" endWordPosition="793">the first, to our knowledge, to propose parsing as a framework for information extraction. They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997) — e.g., EMPLOYEE OF relations that hold between person and organization named entities — and then trained a generative parsing model over this combined syntactic and semantic representation. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al., 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. However, both these works require a unified annotation of syntactic and semantic elements, which is not always feasible, and focused only on named entities and binary relations. On the other hand, our approach focuses on event structures that are nested and have an arbitrary number of arguments. We do not need a unified syntactic and semantic representation (but we can and do extract features from the underlying syntactic structure of the text). Finkel and Manning (2009b) also p</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell P. Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The 90% Solution. Proceedings of the NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>586--594</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="22481" citStr="Huang, 2008" startWordPosition="3567" endWordPosition="3568">ctures When decoding, the parser finds the highest scoring tree which incorporates global properties of the sentence. However, its features are edge-factored and thus unable to take into account larger contexts. To incorporate arbitrary global features, we employ a two-step reranking parser. For the first step, we extend our parser to output its n-best parses instead of just its top scoring parse. In the second step, a discriminative reranker rescores each parse and reorders the n-best list. Rerankers have been successfully used in syntactic parsing (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008) and semantic role labeling (Toutanova et al., 2008). Rerankers provide additional advantages in our case due to the mismatch between the dependency structures that the parser operates on and their corresponding event structures. We convert the output from the parser to event structures (Section 3.1) before including them in the reranker. This allows the reranker to capture features over the actual event structures rather than their original dependency trees which may contain extraneous portions.8 Furthermore, this lets the reranker optimize the actual BioNLP F1 score. The parser, on the other</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL-08: HLT, pages 586–594, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Ahmet Engin Ural</author>
</authors>
<title>Reranking the Berkeley and Brown Parsers.</title>
<date>2010</date>
<booktitle>Proceedings of NAACL.</booktitle>
<contexts>
<context position="25966" citStr="Johnson and Ural, 2010" startWordPosition="4160" endWordPosition="4163">and argument slot names. • Consistency: Similar to the parser Consistency features, but capable of capturing larger classes of errors (e.g., incorrect number or types of arguments). We include the number of violations from four different classes of errors. To improve performance and robustness, features are pruned as in Charniak and Johnson (2005): selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times. Rerankers can also be used to perform model combination (Toutanova et al., 2008; Zhang et al., 2009; Johnson and Ural, 2010). While we use a single parsing model, it has multiple decoders.10 When combining multiple decoders, we concatenate their n-best lists and extract the unique parses. 10We only have n-best versions of the projective decoders. For the non-projective decoders, we use their 1-best parse. 4 Experimental Results Our experiments use the BioNLP’09 shared task corpus (Kim et al., 2009) which includes 800 biomedical abstracts (7,449 sentences, 8,597 events) for training and 150 abstracts (1,450 sentences, 1,809 events) for development. The test set includes 260 abstracts, 2,447 sentences, and 3,182 even</context>
</contexts>
<marker>Johnson, Ural, 2010</marker>
<rawString>Mark Johnson and Ahmet Engin Ural. 2010. Reranking the Berkeley and Brown Parsers. Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Jun’ichi Tsujii</author>
</authors>
<date>2009</date>
<booktitle>Overview of the BioNLP’09 Shared Task on Event Extraction. Proceedings of the NAACL-HLT 2009 Workshop on Natural Language Processing in Biomedicine (BioNLP’09).</booktitle>
<contexts>
<context position="1462" citStr="Kim et al., 2009" startWordPosition="221" endWordPosition="224">res. We explore a rich feature space that models both the events to be parsed and context from the original supporting text. Our approach obtains competitive results in the extraction of biomedical events from the BioNLP’09 shared task with a F1 score of 53.5% in development and 48.6% in testing. 1 Introduction Event structures in open domain texts are frequently highly complex and nested: a “crime” event can cause an “investigation” event, which can lead to an “arrest” event (Chambers and Jurafsky, 2009). The same observation holds in specific domains. For example, the BioNLP’09 shared task (Kim et al., 2009) focuses on the extraction of nested biomolecular events, where, e.g., a REGULATION event causes a TRANSCRIPTION event (see Figure 1a for a detailed example). Despite this observation, many stateof-the-art supervised event extraction models still extract events and event arguments independently, ignoring their underlying structure (Bj¨orne et al., 2009; Miwa et al., 2010b). In this paper, we propose a new approach for supervised event extraction where we take the tree of relations and their arguments and use it directly as the representation in a dependency parser (rather than conventional syn</context>
<context position="26345" citStr="Kim et al., 2009" startWordPosition="4219" endWordPosition="4222"> a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times. Rerankers can also be used to perform model combination (Toutanova et al., 2008; Zhang et al., 2009; Johnson and Ural, 2010). While we use a single parsing model, it has multiple decoders.10 When combining multiple decoders, we concatenate their n-best lists and extract the unique parses. 10We only have n-best versions of the projective decoders. For the non-projective decoders, we use their 1-best parse. 4 Experimental Results Our experiments use the BioNLP’09 shared task corpus (Kim et al., 2009) which includes 800 biomedical abstracts (7,449 sentences, 8,597 events) for training and 150 abstracts (1,450 sentences, 1,809 events) for development. The test set includes 260 abstracts, 2,447 sentences, and 3,182 events. Throughout our experiments, we report BioNLP F1 scores with approximate span and recursive event matching (as described in the shared task definition). For preprocessing, we parsed all documents using the self-trained biomedical McClosky-CharniakJohnson reranking parser (McClosky, 2010). We bias the anchor detector to favor recall, allowing the parser and reranker to deter</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun’ichi Tsujii. 2009. Overview of the BioNLP’09 Shared Task on Event Extraction. Proceedings of the NAACL-HLT 2009 Workshop on Natural Language Processing in Biomedicine (BioNLP’09).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Marry Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="4681" citStr="Marcus et al., 1993" startWordPosition="720" endWordPosition="723">ose a wide range of features for event extraction. Our analysis indicates that features which model the global event structure yield considerable performance improvements, which proves that modeling event structure jointly is beneficial. 3. We evaluate on the biomolecular event corpus from the the BioNLP’09 shared task and show that our approach obtains competitive results. 2 Related Work The pioneering work of Miller et al. (1997) was the first, to our knowledge, to propose parsing as a framework for information extraction. They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997) — e.g., EMPLOYEE OF relations that hold between person and organization named entities — and then trained a generative parsing model over this combined syntactic and semantic representation. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al., 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. However, both these works require a unified a</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Marry Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford Typed Hierarchies Representation.</title>
<date>2008</date>
<booktitle>Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation.</booktitle>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. Stanford Typed Hierarchies Representation. Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
</authors>
<title>Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing.</title>
<date>2010</date>
<tech>PhD thesis,</tech>
<institution>Department of Computer Science, Brown University.</institution>
<contexts>
<context position="26857" citStr="McClosky, 2010" startWordPosition="4291" endWordPosition="4292">t parse. 4 Experimental Results Our experiments use the BioNLP’09 shared task corpus (Kim et al., 2009) which includes 800 biomedical abstracts (7,449 sentences, 8,597 events) for training and 150 abstracts (1,450 sentences, 1,809 events) for development. The test set includes 260 abstracts, 2,447 sentences, and 3,182 events. Throughout our experiments, we report BioNLP F1 scores with approximate span and recursive event matching (as described in the shared task definition). For preprocessing, we parsed all documents using the self-trained biomedical McClosky-CharniakJohnson reranking parser (McClosky, 2010). We bias the anchor detector to favor recall, allowing the parser and reranker to determine which event anchors will ultimately be used. When performing nbest parsing, n = 50. For parser feature pruning, α = 0.001. Table 1a shows the performance of each of the decoders when using gold event anchors. In both cases where n-best decoding is available, the reranker improves performance over the 1-best parsers. We also present the results from a reranker trained from multiple decoders which is our highest scoring model.11 In Table 1b, we present the output for the predicted anchor scenario. In the</context>
</contexts>
<marker>McClosky, 2010</marker>
<rawString>David McClosky. 2010. Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing. PhD thesis, Department of Computer Science, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Mihai Surdeanu</author>
<author>Christopher D Manning</author>
</authors>
<title>Event extraction as dependency parsing in BioNLP</title>
<date>2011</date>
<booktitle>In BioNLP</booktitle>
<note>Shared Task (submitted).</note>
<contexts>
<context position="27952" citStr="McClosky et al., 2011" startWordPosition="4471" endWordPosition="4474">ple decoders which is our highest scoring model.11 In Table 1b, we present the output for the predicted anchor scenario. In the case of the 2P decoder, the reranker does not improve performance, though the drop is minimal. This is because the reranker chose an unfortunate regularization constant during crossvalidation, most likely due to the small size of the training data. In later experiments where more 11Including the 1N decoder as well provided no gains, possibly because its outputs are mostly subsumed by the 2N decoder. 1632 data is available, the reranker consistently improves accuracy (McClosky et al., 2011). As before, the reranker trained from multiple decoders outperforms unreranked models and reranked single decoders. All in all, our best model in Table 1a scores 1 F1 point higher than the best system at the BioNLP’09 shared task, and the best model in Table 1b performs similarly to the best shared task system (Bj¨orne et al., 2009), which also scores 53.5% on development. We show the effects of each system component in Table 2. Note how our upper limit is 87.1% due to our conversion process, which enforces the tree constraint, drops events spanning sentences, and performs approximate reconst</context>
</contexts>
<marker>McClosky, Surdeanu, Manning, 2011</marker>
<rawString>David McClosky, Mihai Surdeanu, and Christopher D. Manning. 2011. Event extraction as dependency parsing in BioNLP 2011. In BioNLP 2011 Shared Task (submitted).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Large-Margin Training of Dependency Parsers.</title>
<date>2005</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="3034" citStr="McDonald et al., 2005" startWordPosition="461" endWordPosition="464">main. Figure 1 shows a sentence and its converted form from the biomedical domain with four events: two POSITIVE REGULATION events, anchored by the phrase “acts as a costimulatory signal,” and two TRANSCRIPTION events, both anchored on “gene transcription.” All events take either protein entity mentions (PROT) or other events as arguments. The latter is what allows for nested event structures. Existing dependency parsing models can be adapted to produce these semantic structures instead of syntactic dependencies. We built a global reranking parser model using multiple decoders from MSTParser (McDonald et al., 2005; McDonald et al., 2005b). The main contributions of this paper are the following: 1. We demonstrate that parsing is an attractive approach for extracting events, both nested and otherwise. 1While our approach only works on trees, we show how we can handle directed acyclic graphs in Section 5. 1626 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1626–1635, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics (a) Original sentence with nested events (b) After conversion to event dependencies Figure 1: Nested events i</context>
<context position="8388" citStr="McDonald et al., 2005" startWordPosition="1299" endWordPosition="1303">emantic frames independently. 3 Approach Figure 2 summarizes our architecture. Our approach converts the original event representation to dependency trees containing both event anchors and entity mentions, and trains a battery of parsers to recognize these structures. The trees are built using event anchors predicted by a separate classifier. In this work, we do not discuss entity recognition because in the BioNLP’09 domain used for evaluation entities (PROTEINs) are given (but including entity recognition is an obvious extension of our model). Our parsers are several instances of MSTParser2 (McDonald et al., 2005; McDonald et al., 2005b) configured with different decoders. However, our approach is agnostic to the actual parsing models used and could easily be adapted to other dependency parsers. The output from the reranking parser is 2http://sourceforge.net/projects/mstparser/ converted back to the original event representation and passed to a reranker component (Collins, 2000; Charniak and Johnson, 2005), tailored to optimize the task-specific evaluation metric. Note that although we use the biomedical event domain from the BioNLP’09 shared task to illustrate our work, the core of our approach is al</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online Large-Margin Training of Dependency Parsers. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>Proceedings of HLT/EMNLP.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005b. Non-projective Dependency Parsing using Spanning Tree Algorithms. Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Learning of Approximate Dependency Parsing Algorithms.</title>
<date>2006</date>
<booktitle>Proceedings of EACL.</booktitle>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Michael Crystal</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
<author>Richard Schwartz</author>
<author>Rebecca Stone</author>
<author>Ralph Weischedel</author>
</authors>
<date>1997</date>
<booktitle>BBN: Description of the SIFT System as Used for MUC-7. Proceedings of the Message Understanding Conference (MUC-7).</booktitle>
<contexts>
<context position="4496" citStr="Miller et al. (1997)" startWordPosition="689" endWordPosition="692">tities (PROTEINs in the BioNLP’09 domain). Note that in (a) there are two copies of each type of event, which are merged to single nodes in the dependency tree (Section 3.1). 2. We propose a wide range of features for event extraction. Our analysis indicates that features which model the global event structure yield considerable performance improvements, which proves that modeling event structure jointly is beneficial. 3. We evaluate on the biomolecular event corpus from the the BioNLP’09 shared task and show that our approach obtains competitive results. 2 Related Work The pioneering work of Miller et al. (1997) was the first, to our knowledge, to propose parsing as a framework for information extraction. They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997) — e.g., EMPLOYEE OF relations that hold between person and organization named entities — and then trained a generative parsing model over this combined syntactic and semantic representation. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes cor</context>
</contexts>
<marker>Miller, Crystal, Fox, Ramshaw, Schwartz, Stone, Weischedel, 1997</marker>
<rawString>Scott Miller, Michael Crystal, Heidi Fox, Lance Ramshaw, Richard Schwartz, Rebecca Stone, and Ralph Weischedel. 1997. BBN: Description of the SIFT System as Used for MUC-7. Proceedings of the Message Understanding Conference (MUC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Sampo Pyysalo</author>
<author>Tadayoshi Hara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A Comparative Study of Syntactic Parsers for Event Extraction.</title>
<date>2010</date>
<booktitle>Proceedings of the 2010 Workshop on Biomedical Natural Language Processing.</booktitle>
<contexts>
<context position="1835" citStr="Miwa et al., 2010" startWordPosition="274" endWordPosition="277">complex and nested: a “crime” event can cause an “investigation” event, which can lead to an “arrest” event (Chambers and Jurafsky, 2009). The same observation holds in specific domains. For example, the BioNLP’09 shared task (Kim et al., 2009) focuses on the extraction of nested biomolecular events, where, e.g., a REGULATION event causes a TRANSCRIPTION event (see Figure 1a for a detailed example). Despite this observation, many stateof-the-art supervised event extraction models still extract events and event arguments independently, ignoring their underlying structure (Bj¨orne et al., 2009; Miwa et al., 2010b). In this paper, we propose a new approach for supervised event extraction where we take the tree of relations and their arguments and use it directly as the representation in a dependency parser (rather than conventional syntactic relations). Our approach is conceptually simple: we first convert the original representation of events and their arguments to dependency trees by creating dependency arcs between event anchors (phrases that anchor events in the supporting text) and their corresponding arguments.1 Note that after conversion, only event anchors and entities remain. Figure 1 shows a</context>
<context position="31353" citStr="Miwa et al., 2010" startWordPosition="5064" endWordPosition="5067">d data was used; ‘✓’ indicates that the actual component was used. n-best parses considered Anchors Decoder(s) 1 2 10 All 1P 70.7 76.6 84.0 85.7 Gold 2P 71.8 77.5 84.8 86.2 1P, 2P, 2N — — — 86.7 1P 52.0 60.3 69.9 72.5 Predicted 2P 52.7 60.7 70.1 72.5 1P, 2P, 2N — — — 73.4 Table 3: Oracle reranker BioNLP F1 scores for our n-best decoders and their combinations before reranking on the development corpus. the event subtree allowing small errors to have large effects. Top systems on this task obtain F1 scores of 52.0% at the shared task evaluation (Bj¨orne et al., 2009) and 56.3% post evaluation (Miwa et al., 2010a). However, both systems are tailored to the biomedical domain (the latter uses multiple syntactic parsers), whereas our system has a design that is virtually domain independent. 5 Discussion We believe that the potential of our approach is higher than what the current experiments show. For example, the reranker can be used to combine not only several parsers but also multiple anchor recognizers. This passes the anchor selection decision to the reranker, which uses global information not available to the current anchor recognizer or parser. Furthermore, our approach can be adapted to parse ev</context>
</contexts>
<marker>Miwa, Pyysalo, Hara, Tsujii, 2010</marker>
<rawString>Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and Jun’ichi Tsujii. 2010a. A Comparative Study of Syntactic Parsers for Event Extraction. Proceedings of the 2010 Workshop on Biomedical Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Rune Saetre</author>
<author>Jin-Dong Kim</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Event Extraction with Complex Event Classification Using Rich Features.</title>
<date>2010</date>
<journal>Journal of Bioinformatics and Computational Biology,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1835" citStr="Miwa et al., 2010" startWordPosition="274" endWordPosition="277">complex and nested: a “crime” event can cause an “investigation” event, which can lead to an “arrest” event (Chambers and Jurafsky, 2009). The same observation holds in specific domains. For example, the BioNLP’09 shared task (Kim et al., 2009) focuses on the extraction of nested biomolecular events, where, e.g., a REGULATION event causes a TRANSCRIPTION event (see Figure 1a for a detailed example). Despite this observation, many stateof-the-art supervised event extraction models still extract events and event arguments independently, ignoring their underlying structure (Bj¨orne et al., 2009; Miwa et al., 2010b). In this paper, we propose a new approach for supervised event extraction where we take the tree of relations and their arguments and use it directly as the representation in a dependency parser (rather than conventional syntactic relations). Our approach is conceptually simple: we first convert the original representation of events and their arguments to dependency trees by creating dependency arcs between event anchors (phrases that anchor events in the supporting text) and their corresponding arguments.1 Note that after conversion, only event anchors and entities remain. Figure 1 shows a</context>
<context position="31353" citStr="Miwa et al., 2010" startWordPosition="5064" endWordPosition="5067">d data was used; ‘✓’ indicates that the actual component was used. n-best parses considered Anchors Decoder(s) 1 2 10 All 1P 70.7 76.6 84.0 85.7 Gold 2P 71.8 77.5 84.8 86.2 1P, 2P, 2N — — — 86.7 1P 52.0 60.3 69.9 72.5 Predicted 2P 52.7 60.7 70.1 72.5 1P, 2P, 2N — — — 73.4 Table 3: Oracle reranker BioNLP F1 scores for our n-best decoders and their combinations before reranking on the development corpus. the event subtree allowing small errors to have large effects. Top systems on this task obtain F1 scores of 52.0% at the shared task evaluation (Bj¨orne et al., 2009) and 56.3% post evaluation (Miwa et al., 2010a). However, both systems are tailored to the biomedical domain (the latter uses multiple syntactic parsers), whereas our system has a design that is virtually domain independent. 5 Discussion We believe that the potential of our approach is higher than what the current experiments show. For example, the reranker can be used to combine not only several parsers but also multiple anchor recognizers. This passes the anchor selection decision to the reranker, which uses global information not available to the current anchor recognizer or parser. Furthermore, our approach can be adapted to parse ev</context>
</contexts>
<marker>Miwa, Saetre, Kim, Tsujii, 2010</marker>
<rawString>Makoto Miwa, Rune Saetre, Jin-Dong Kim, and Jun’ichi Tsujii. 2010b. Event Extraction with Complex Event Classification Using Rich Features. Journal of Bioinformatics and Computational Biology, 8 (1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Joint Inference for Knowledge Extraction from Biomedical Literature.</title>
<date>2010</date>
<booktitle>Proceedings of NAACL.</booktitle>
<contexts>
<context position="6193" citStr="Poon and Vanderwende, 2010" startWordPosition="965" endWordPosition="968">c and semantic representation (but we can and do extract features from the underlying syntactic structure of the text). Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. In this work, we focus on more complex structures (events instead of named entities) and we explore more global features through our reranking layer. In the biomedical domain, two recent papers proposed joint models for event extraction based on Markov logic networks (MLN) (Riedel et al., 2009; Poon and Vanderwende, 2010). Both works propose elegant frameworks where event anchors and arguments are jointly predicted for all events in the same sentence. One disadvantage of MLN models is the requirement that a human expert develop domainspecific predicates and formulas, which can be a cumbersome process because it requires thorough domain understanding. On the other hand, our approach maintains the joint modeling advantage, but our model is built over simple, domain-independent features. We also propose and analyze a richer feature space that captures more information on the global event structure in a sentence. </context>
<context position="9573" citStr="Poon and Vanderwende, 2010" startWordPosition="1481" endWordPosition="1484">e our work, the core of our approach is almost domain independent. Our only constraints are that each event mention be activated by a phrase that serves as an event anchor, and that the event-argument structures be mapped to a dependency tree. The conversion between event and dependency structures and the reranker metric are the only domain dependent components in our approach. 3.1 Converting between Event Structures and Dependencies As in previous work, we extract event structures at sentence granularity, i.e., we ignore events which span sentences (Bj¨orne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010). These form approximately 5% of the events in the BioNLP’09 corpus. For each sentence, we convert the BioNLP’09 event representation to a graph (representing a labeled dependency tree) as follows. The nodes in the graph are protein entity mentions, event anchors, and a virtual ROOT node. Thus, the only words in this dependency tree are those which participate in events. We create edges in the graph in the following way. For each event anchor, we create one link to each of its arguments labeled with the slot name of the argument (for example, connecting gene transcription to IL-2 with the labe</context>
</contexts>
<marker>Poon, Vanderwende, 2010</marker>
<rawString>Hoifung Poon and Lucy Vanderwende. 2010. Joint Inference for Knowledge Extraction from Biomedical Literature. Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Hong-Woo Chun</author>
<author>Toshihisa Takagi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A Markov Logic Approach to Bio-Molecular Event Extraction.</title>
<date>2009</date>
<booktitle>Proceedings of the Workshop on BioNLP: Shared Task.</booktitle>
<contexts>
<context position="6164" citStr="Riedel et al., 2009" startWordPosition="961" endWordPosition="964">ed a unified syntactic and semantic representation (but we can and do extract features from the underlying syntactic structure of the text). Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. In this work, we focus on more complex structures (events instead of named entities) and we explore more global features through our reranking layer. In the biomedical domain, two recent papers proposed joint models for event extraction based on Markov logic networks (MLN) (Riedel et al., 2009; Poon and Vanderwende, 2010). Both works propose elegant frameworks where event anchors and arguments are jointly predicted for all events in the same sentence. One disadvantage of MLN models is the requirement that a human expert develop domainspecific predicates and formulas, which can be a cumbersome process because it requires thorough domain understanding. On the other hand, our approach maintains the joint modeling advantage, but our model is built over simple, domain-independent features. We also propose and analyze a richer feature space that captures more information on the global ev</context>
<context position="9544" citStr="Riedel et al., 2009" startWordPosition="1477" endWordPosition="1480">red task to illustrate our work, the core of our approach is almost domain independent. Our only constraints are that each event mention be activated by a phrase that serves as an event anchor, and that the event-argument structures be mapped to a dependency tree. The conversion between event and dependency structures and the reranker metric are the only domain dependent components in our approach. 3.1 Converting between Event Structures and Dependencies As in previous work, we extract event structures at sentence granularity, i.e., we ignore events which span sentences (Bj¨orne et al., 2009; Riedel et al., 2009; Poon and Vanderwende, 2010). These form approximately 5% of the events in the BioNLP’09 corpus. For each sentence, we convert the BioNLP’09 event representation to a graph (representing a labeled dependency tree) as follows. The nodes in the graph are protein entity mentions, event anchors, and a virtual ROOT node. Thus, the only words in this dependency tree are those which participate in events. We create edges in the graph in the following way. For each event anchor, we create one link to each of its arguments labeled with the slot name of the argument (for example, connecting gene transc</context>
</contexts>
<marker>Riedel, Chun, Takagi, Tsujii, 2009</marker>
<rawString>Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi, and Jun’ichi Tsujii. 2009. A Markov Logic Approach to Bio-Molecular Event Extraction. Proceedings of the Workshop on BioNLP: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<contexts>
<context position="33557" citStr="Rush et al., 2010" startWordPosition="5424" endWordPosition="5427">event structures to map to trees. In the BioNLP’09 corpus this leads to the removal of almost 5% of the events, which generate DAGs instead of trees. Local event extraction models (Bj¨orne et al., 2009) do not have this limitation, because their local decisions are blind to (and hence not limited by) the global event structure. However, our approach is agnostic to the actual parsing models used, so we can easily incorporate models that can parse DAGs (Sagae and Tsujii, 2008). Additionally, we are free to incorporate any new techniques from dependency parsing. Parsing using dual-decomposition (Rush et al., 2010) seems especially promising in this area. 6 Conclusion In this paper we proposed a simple approach for the joint extraction of event structures: we converted the representation of events and their arguments to dependency trees with arcs between event anchors and event arguments, and used a reranking parser to parse these structures. Despite the fact that our approach has very little domain-specific engineering, we obtain competitive results. Most importantly, we showed that the joint modeling of event structures is beneficial: our reranker outperforms parsing models without reranking in five o</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<date>2008</date>
<booktitle>Shift-Reduce Dependency DAG Parsing. Proceedings of the COLING.</booktitle>
<contexts>
<context position="33418" citStr="Sagae and Tsujii, 2008" startWordPosition="5403" endWordPosition="5406">at model discourse structure. We plan to explore these ideas in future work. One current limitation of the proposed model is that it constrains event structures to map to trees. In the BioNLP’09 corpus this leads to the removal of almost 5% of the events, which generate DAGs instead of trees. Local event extraction models (Bj¨orne et al., 2009) do not have this limitation, because their local decisions are blind to (and hence not limited by) the global event structure. However, our approach is agnostic to the actual parsing models used, so we can easily incorporate models that can parse DAGs (Sagae and Tsujii, 2008). Additionally, we are free to incorporate any new techniques from dependency parsing. Parsing using dual-decomposition (Rush et al., 2010) seems especially promising in this area. 6 Conclusion In this paper we proposed a simple approach for the joint extraction of event structures: we converted the representation of events and their arguments to dependency trees with arcs between event anchors and event arguments, and used a reranking parser to parse these structures. Despite the fact that our approach has very little domain-specific engineering, we obtain competitive results. Most importantl</context>
</contexts>
<marker>Sagae, Tsujii, 2008</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-Reduce Dependency DAG Parsing. Proceedings of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluis Marquez</author>
<author>Joakim Nivre</author>
</authors>
<date>2008</date>
<booktitle>The CoNLL2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. Proceedings of CoNLL.</booktitle>
<contexts>
<context position="7450" citStr="Surdeanu et al., 2008" startWordPosition="1156" endWordPosition="1159">gnostic to the parsing model used, it could easily be tuned for various scenarios, e.g., models with lower inference overhead such as shift-reduce parsers. Our work is conceptually close to the recent CoNLL shared tasks on semantic role labeling, where the predicate frames were converted to se1627 Figure 2: Overview of the approach. Rounded rectangles indicate domain-independent components; regular rectangles mark domain-specific modules; blocks in dashed lines surround components not necessary for the domain presented in this paper. mantic dependencies between predicates and their arguments (Surdeanu et al., 2008; Hajic et al., 2009). In this representation the dependency structure is a directed acyclic graph (DAG), i.e., the same node can be an argument to multiple predicates, and there are no explicit dependencies between predicates. Due to this representation, all joint models proposed for semantic role labeling handle semantic frames independently. 3 Approach Figure 2 summarizes our architecture. Our approach converts the original event representation to dependency trees containing both event anchors and entity mentions, and trains a battery of parsers to recognize these structures. The trees are </context>
</contexts>
<marker>Surdeanu, Johansson, Meyers, Marquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluis Marquez, and Joakim Nivre. 2008. The CoNLL2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>A Global Joint Model for Semantic Role Labeling.</title>
<date>2008</date>
<journal>Computational Linguistics</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="22533" citStr="Toutanova et al., 2008" startWordPosition="3573" endWordPosition="3576">e highest scoring tree which incorporates global properties of the sentence. However, its features are edge-factored and thus unable to take into account larger contexts. To incorporate arbitrary global features, we employ a two-step reranking parser. For the first step, we extend our parser to output its n-best parses instead of just its top scoring parse. In the second step, a discriminative reranker rescores each parse and reorders the n-best list. Rerankers have been successfully used in syntactic parsing (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008) and semantic role labeling (Toutanova et al., 2008). Rerankers provide additional advantages in our case due to the mismatch between the dependency structures that the parser operates on and their corresponding event structures. We convert the output from the parser to event structures (Section 3.1) before including them in the reranker. This allows the reranker to capture features over the actual event structures rather than their original dependency trees which may contain extraneous portions.8 Furthermore, this lets the reranker optimize the actual BioNLP F1 score. The parser, on the other hand, attempts to optimize the Labeled Attachment S</context>
<context position="25921" citStr="Toutanova et al., 2008" startWordPosition="4152" endWordPosition="4155">mes: Event anchors with all their arguments and argument slot names. • Consistency: Similar to the parser Consistency features, but capable of capturing larger classes of errors (e.g., incorrect number or types of arguments). We include the number of violations from four different classes of errors. To improve performance and robustness, features are pruned as in Charniak and Johnson (2005): selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times. Rerankers can also be used to perform model combination (Toutanova et al., 2008; Zhang et al., 2009; Johnson and Ural, 2010). While we use a single parsing model, it has multiple decoders.10 When combining multiple decoders, we concatenate their n-best lists and extract the unique parses. 10We only have n-best versions of the projective decoders. For the non-projective decoders, we use their 1-best parse. 4 Experimental Results Our experiments use the BioNLP’09 shared task corpus (Kim et al., 2009) which includes 800 biomedical abstracts (7,449 sentences, 8,597 events) for training and 150 abstracts (1,450 sentences, 1,809 events) for development. The test set includes 2</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2008. A Global Joint Model for Semantic Role Labeling. Computational Linguistics 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>M Zhang</author>
<author>C L Tan</author>
<author>H Li</author>
</authors>
<title>K-best combination of syntactic parsers.</title>
<date>2009</date>
<booktitle>Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="25941" citStr="Zhang et al., 2009" startWordPosition="4156" endWordPosition="4159">all their arguments and argument slot names. • Consistency: Similar to the parser Consistency features, but capable of capturing larger classes of errors (e.g., incorrect number or types of arguments). We include the number of violations from four different classes of errors. To improve performance and robustness, features are pruned as in Charniak and Johnson (2005): selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times. Rerankers can also be used to perform model combination (Toutanova et al., 2008; Zhang et al., 2009; Johnson and Ural, 2010). While we use a single parsing model, it has multiple decoders.10 When combining multiple decoders, we concatenate their n-best lists and extract the unique parses. 10We only have n-best versions of the projective decoders. For the non-projective decoders, we use their 1-best parse. 4 Experimental Results Our experiments use the BioNLP’09 shared task corpus (Kim et al., 2009) which includes 800 biomedical abstracts (7,449 sentences, 8,597 events) for training and 150 abstracts (1,450 sentences, 1,809 events) for development. The test set includes 260 abstracts, 2,447 </context>
</contexts>
<marker>Zhang, Zhang, Tan, Li, 2009</marker>
<rawString>Zhang, H. and Zhang, M. and Tan, C.L. and Li, H. 2009. K-best combination of syntactic parsers. Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>