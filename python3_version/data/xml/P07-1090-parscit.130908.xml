<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.997113">
Ordering Phrases with Function Words
</title>
<author confidence="0.987655">
Hendra Setiawan and Min-Yen Kan
</author>
<affiliation confidence="0.998705">
School of Computing
National University of Singapore
</affiliation>
<address confidence="0.976281">
Singapore 117543
</address>
<email confidence="0.997499">
{hendrase,kanmy}@comp.nus.edu.sg
</email>
<author confidence="0.956973">
Haizhou Li
</author>
<affiliation confidence="0.954727">
Institute for Infocomm Research
</affiliation>
<address confidence="0.973087">
21 Heng Mui Keng Terrace
Singapore 119613
</address>
<email confidence="0.995474">
hli@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.996596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879684210526">
This paper presents a Function Word cen-
tered, Syntax-based (FWS) solution to ad-
dress phrase ordering in the context of
statistical machine translation (SMT). Mo-
tivated by the observation that function
words often encode grammatical relation-
ship among phrases within a sentence, we
propose a probabilistic synchronous gram-
mar to model the ordering of function words
and their left and right arguments. We im-
prove phrase ordering performance by lexi-
calizing the resulting rules in a small number
of cases corresponding to function words.
The experiments show that the FWS ap-
proach consistently outperforms the base-
line system in ordering function words’ ar-
guments and improving translation quality
in both perfect and noisy word alignment
scenarios.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951730769231">
The focus of this paper is on function words, a class
of words with little intrinsic meaning but is vital in
expressing grammatical relationships among words
within a sentence. Such encoded grammatical infor-
mation, often implicit, makes function words piv-
otal in modeling structural divergences, as project-
ing them in different languages often result in long-
range structural changes to the realized sentences.
Just as a foreign language learner often makes
mistakes in using function words, we observe that
current machine translation (MT) systems often per-
form poorly in ordering function words’ arguments;
lexically correct translations often end up reordered
incorrectly. Thus, we are interested in modeling
the structural divergence encoded by such function
words. A key finding of our work is that modeling
the ordering of the dependent arguments of function
words results in better translation quality.
Most current systems use statistical knowledge
obtained from corpora in favor of rich natural lan-
guage knowledge. Instead of using syntactic knowl-
edge to determine function words, we approximate
this by equating the most frequent words as func-
tion words. By explicitly modeling phrase ordering
around these frequent words, we aim to capture the
most important and prevalent ordering productions.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999972315789474">
A good translation should be both faithful with ade-
quate lexical choice to the source language and flu-
ent in its word ordering to the target language. In
pursuit of better translation, phrase-based models
(Och and Ney, 2004) have significantly improved the
quality over classical word-based models (Brown et
al., 1993). These multiword phrasal units contribute
to fluency by inherently capturing intra-phrase re-
ordering. However, despite this progress, inter-
phrase reordering (especially long distance ones)
still poses a great challenge to statistical machine
translation (SMT).
The basic phrase reordering model is a simple
unlexicalized, context-insensitive distortion penalty
model (Koehn et al., 2003). This model assumes
little or no structural divergence between language
pairs, preferring the original, translated order by pe-
nalizing reordering. This simple model works well
when properly coupled with a well-trained language
</bodyText>
<page confidence="0.946322">
712
</page>
<note confidence="0.925786">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 712–719,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999737111111111">
model, but is otherwise impoverished without any
lexical evidence to characterize the reordering.
To address this, lexicalized context-sensitive
models incorporate contextual evidence. The local
prediction model (Tillmann and Zhang, 2005) mod-
els structural divergence as the relative position be-
tween the translation of two neighboring phrases.
Other further generalizations of orientation include
the global prediction model (Nagata et al., 2006) and
distortion model (Al-Onaizan and Papineni, 2006).
However, these models are often fully lexicalized
and sensitive to individual phrases. As a result, they
are not robust to unseen phrases. A careful approx-
imation is vital to avoid data sparseness. Proposals
to alleviate this problem include utilizing bilingual
phrase cluster or words at the phrase boundary (Na-
gata et al., 2006) as the phrase identity.
The benefit of introducing lexical evidence with-
out being fully lexicalized has been demonstrated
by a recent state-of-the-art formally syntax-based
model1, Hiero (Chiang, 2005). Hiero performs
phrase ordering by using linked non-terminal sym-
bols in its synchronous CFG production rules cou-
pled with lexical evidence. However, since it is dif-
ficult to specify a well-defined rule, Hiero has to rely
on weak heuristics (i.e., length-based thresholds) to
extract rules. As a result, Hiero produces grammars
of enormous size. Watanabe et al. (2006) further
reduces the grammar’s size by enforcing all rules to
comply with Greibach Normal Form.
Taking the lexicalization an intuitive a step for-
ward, we propose a novel, finer-grained solution
which models the content and context information
encoded by function words - approximated by high
frequency words. Inspired by the success of syntax-
based approaches, we propose a synchronous gram-
mar that accommodates gapping production rules,
while focusing on the statistical modeling in rela-
tion to function words. We refer to our approach
as the Function Word-centered Syntax-based ap-
proach (FWS). Our FWS approach is different from
Hiero in two key aspects. First, we use only a
small set of high frequency lexical items to lexi-
calize non-terminals in the grammar. This results
in a much smaller set of rules compared to Hiero,
</bodyText>
<footnote confidence="0.89793">
1Chiang (2005) used the term “formal” to indicate the use of
synchronous grammar but without linguistic commitment
</footnote>
<note confidence="0.267849">
Rig VIA V
</note>
<equation confidence="0.991088857142857">
�� � �� � �� �� � �� �
����������������
�� �����������
� �� � � � �� ��
�
� � � � �
a form is a coll. of data entry fields on a page
</equation>
<figureCaption confidence="0.996927">
Figure 1: A Chinese-English sentence pair.
</figureCaption>
<bodyText confidence="0.999930428571429">
greatly reducing the computational overhead that
arises when moving from phrase-based to syntax-
based approach. Furthermore, by modeling only
high frequency words, we are able to obtain reliable
statistics even in small datasets. Second, as opposed
to Hiero, where phrase ordering is done implicitly
alongside phrase translation and lexical weighting,
we directly model the reordering process using ori-
entation statistics.
The FWS approach is also akin to (Xiong et al.,
2006) in using a synchronous grammar as a reorder-
ing constraint. Instead of using Inversion Transduc-
tion Grammar (ITG) (Wu, 1997) directly, we will
discuss an ITG extension to accommodate gapping.
</bodyText>
<sectionHeader confidence="0.9771125" genericHeader="method">
3 Phrase Ordering around Function
Words
</sectionHeader>
<bodyText confidence="0.999989">
We use the following Chinese (c) to English (e)
translation in Fig.1 as an illustration to conduct an
inquiry to the problem. Note that the sentence trans-
lation requires some translations of English words
to be ordered far from their original position in Chi-
nese. Recovering the correct English ordering re-
quires the inversion of the Chinese postpositional
phrase, followed by the inversion of the first smaller
noun phrase, and finally the inversion of the sec-
ond larger noun phrase. Nevertheless, the correct
ordering can be recovered if the position and the se-
mantic roles of the arguments of the boxed function
words were known. Such a function word centered
approach also hinges on knowing the correct phrase
boundaries for the function words’ arguments and
which reorderings are given precedence, in case of
conflicts.
We propose modeling these sources of knowl-
edge using a statistical formalism. It includes 1) a
model to capture bilingual orientations of the left
and right arguments of these function words; 2) a
model to approximate correct reordering sequence;
and 3) a model for finding constituent boundaries of
</bodyText>
<figure confidence="0.987152">
14 M_-N �
a
</figure>
<page confidence="0.995305">
713
</page>
<bodyText confidence="0.99984966">
the left and right arguments. Assuming that the most
frequent words in a language are function words,
we can apply orientation statistics associated with
these words to reorder their adjacent left and right
neighbors. We follow the notation in (Nagata et
al., 2006) and define the following bilingual ori-
entation values given two neighboring source (Chi-
nese) phrases: Monotone-Adjacent (MA); Reverse-
Adjacent (RA); Monotone-Gap (MG); and Reverse-
Gap (RG). The first clause (monotone, reverse) in-
dicates whether the target language translation order
follows the source order; the second (adjacent, gap)
indicates whether the source phrases are adjacent or
separated by an intervening phrase on the target side.
Table 1 shows the orientation statistics for several
function words. Note that we separate the statistics
for left and right arguments to account for differ-
ences in argument structures: some function words
take a single argument (e.g., prepositions), while
others take two or more (e.g., copulas). To han-
dle other reordering decisions not explicitly encoded
(i.e., lexicalized) in our FWS model, we introduce a
universal token U, to be used as a backoff statistic
when function words are absent.
For example, orientation statistics for 4 (to be)
overwhelmingly suggests that the English transla-
tion of its surrounding phrases is identical to its Chi-
nese ordering. This reflects the fact that the argu-
ments of copulas in both languages are realized in
the same order. The orientation statistics for post-
position Þ (on) suggests inversion which captures
the divergence between Chinese postposition to the
English preposition. Similarly, the dominant orien-
tation for particle { (of) suggests the noun-phrase
shift from modified-modifier to modifier-modified,
which is common when translating Chinese noun
phrases to English.
Taking all parts of the model, which we detail
later, together with the knowledge in Table 1, we
demonstrate the steps taken to translate the exam-
ple in Fig. 2. We highlight the function words with
boxed characters and encapsulate content words as
indexed symbols. As shown, orientation statistics
from function words alone are adequate to recover
the English ordering - in practice, content words also
influence the reordering through a language model.
One can think of the FWS approach as a foreign lan-
guage learner with limited knowledge about Chinese
grammar but fairly knowledgable about the role of
Chinese function words.
</bodyText>
<figure confidence="0.686283777777778">
�K\ 4 MJU { jig Qœ V { va
X1 4 X2 Þ { X3 { X4
✟✙ ❍❍❥ X2 ✘✘ ✘ ✘ ✏✮✏ ✏✏ ✏
✟ ❳❳❳❳❳③ ❄
✘✾ ✘ ❄
Þ
X3
{ X5 ❳❳❳❳❳❳❳③
X4 { X6
❄ ❄ ❄
X1 X7
4
X1 X4 { X3 { Þ X2
4
�K\ a { jIMIœV { Þ �-N
4
❄ ❄ ❄ ❄ ❄ ❄ ❄ ❄ ❄
a form is a coll. of data entry fields on a page
</figure>
<figureCaption confidence="0.968902">
Figure 2: In Step 1, function words (boxed char-
</figureCaption>
<bodyText confidence="0.931051166666667">
acters) and content words (indexed symbols) are
identified. Step 2 reorders phrases according to
knowledge embedded in function words. A new in-
dexed symbol is introduced to indicate previously
reordered phrases for conciseness. Step 3 finally
maps Chinese phrases to their English translation.
</bodyText>
<sectionHeader confidence="0.998286" genericHeader="method">
4 The FWS Model
</sectionHeader>
<bodyText confidence="0.999993">
We first discuss the extension of standard ITG to
accommodate gapping and then detail the statistical
components of the model later.
</bodyText>
<subsectionHeader confidence="0.998077">
4.1 Single Gap ITG (SG-ITG)
</subsectionHeader>
<bodyText confidence="0.999792142857143">
The FWS model employs a synchronous grammar
to describe the admissible orderings.
The utility of ITG as a reordering constraint for
most language pairs, is well-known both empirically
(Zens and Ney, 2003) and analytically (Wu, 1997),
however ITG’s straight (monotone) and inverted (re-
verse) rules exhibit strong cohesiveness, which is in-
adequate to express orientations that require gaps.
We propose SG-ITG that follows Wellington et al.
(2006)’s suggestion to model at most one gap.
We show the rules for SG-ITG below. Rules 1-
3 are identical to those defined in standard ITG, in
which monotone and reverse orderings are repre-
sented by square and angle brackets, respectively.
</bodyText>
<figure confidence="0.825429333333333">
#1
#2
#3
</figure>
<page confidence="0.94454">
714
</page>
<table confidence="0.998787">
Rank Word unigram MAL RAL MGL RGL MAR RAR MGR RGR
1 { 0.0580 0.45 0.52 0.01 0.02 0.44 0.52 0.01 0.03
2 0.0507 0.85 0.12 0.02 0.01 0.84 0.12 0.02 0.02
3 0.0550 0.99 0.01 0.00 0.00 0.92 0.08 0.00 0.00
4 0.0155 0.87 0.10 0.02 0.00 0.82 0.12 0.05 0.02
5 0.0153 0.84 0.11 0.01 0.04 0.88 0.11 0.01 0.01
6 �P 0.0138 0.95 0.02 0.01 0.01 0.97 0.02 0.01 0.00
7 &apos;f-1-* 0.0123 0.73 0.12 0.10 0.04 0.51 0.14 0.14 0.20
8 PT 0.0114 0.78 0.12 0.03 0.07 0.86 0.05 0.08 0.01
9 :P1 0.0099 0.95 0.02 0.02 0.01 0.96 0.01 0.02 0.01
10 0.0091 0.87 0.10 0.01 0.02 0.88 0.10 0.01 0.00
21 , 0.0056 0.85 0.11 0.02 0.02 0.85 0.04 0.09 0.02
37 0.0035 0.33 0.65 0.02 0.01 0.31 0.63 0.03 0.03
- U 0.0002 0.76 0.14 0.06 0.05 0.74 0.13 0.07 0.06
</table>
<tableCaption confidence="0.893597">
Table 1: Orientation statistics and unigram probability of selected frequent Chinese words in the HIT corpus.
Subscripts L/R refers to lexical unit’s orientation with respect to its left/right neighbor. U is the universal
token used in back-off for N = 128. Dominant orientations of each word are in bold.
</tableCaption>
<listItem confidence="0.84703875">
(1) X -* c/e
(2) X -* [XX] (3) X -* (XX)
(4) X°-* [X o X] (5) X°-* (X o X)
(6) X -* [X * X] (7) X -* (X * X)
</listItem>
<bodyText confidence="0.999561066666667">
SG-ITG introduces two new sets of rules: gap-
ping (Rules 4-5) and dovetailing (Rules 6-7) that
deal specifically with gaps. On the RHS of the gap-
ping rules, a diamond symbol (o) indicates a gap,
while on the LHS, it emits a superscripted symbol
X* to indicate a gapped phrase (plain Xs without
superscripts are thus contiguous phrases). Gaps in
X* are eventually filled by actual phrases via dove-
tailing (marked with an * on the RHS).
Fig.3 illustrates gapping and dovetailing rules
using an example where two Chinese adjectival
phrases are translated into a single English subordi-
nate clause. SG-ITG can generate the correct order-
ing by employing gapping followed by dovetailing,
as shown in the following simplified trace:
</bodyText>
<table confidence="0.967143571428571">
Xi -* ( 1997 { ‘---Ç, V.101997 )
X2 -* ( 1998 ‘__Ç, V.2 01998 )
{
X3 -* [X1 * X2]
-* [ 1997 { ‘---Ç fn 1998 { ‘-Ç,
V.1 * 1997 * V.2 * 1998 ]
� 1997 {‘---Ç�1998 {‘~Ç,
</table>
<bodyText confidence="0.942648">
V.1 and V.2 that were released in 1997 and 1998
where Xi and X2 each generate the translation of
their respective Chinese noun phrase using gapping
and X3 generates the English subclause by dovetail-
ing the two gapped phrases together.
Thus far, the grammar is unlexicalized, and does
</bodyText>
<equation confidence="0.937221857142857">
19974-I&apos;AT7 { ‘---Ç 3M 19984-I&apos;V7 {
�����
�� �� �� � �� � �� � �
�������������
PPP
� � �
V.1 and V.2 that were released in 1997 and 1998.
</equation>
<figureCaption confidence="0.783394">
Figure 3: An example of an alignment that can be
</figureCaption>
<bodyText confidence="0.99598015">
generated only by allowing gaps.
not incorporate any lexical evidence. Now we mod-
ify the grammar to introduce lexicalized function
words to SG-ITG. In practice, we introduce a new
set of lexicalized non-terminal symbols Y, i E
11...NJ, to represent the top N most-frequent words
in the vocabulary; the existing unlexicalized X is
now reserved for content words. This difference
does not inherently affect the structure of the gram-
mar, but rather lexicalizes the statistical model.
In this way, although different Ys follow the same
production rules, they are associated with different
statistics. This is reflected in Rules 8-9. Rule 8 emits
the function word; Rule 9 reorders the arguments
around the function word, resembling our orienta-
tion model (see Section 4.2) where a function word
influences the orientation of its left and right argu-
ments. For clarity, we omit notation that denotes
which rules have been applied (monotone, reverse;
gapping, dovetailing).
</bodyText>
<equation confidence="0.802099">
(8) Y-* c/e (9) X-* XYZX
</equation>
<bodyText confidence="0.9971065">
In practice, we replace Rule 9 with its equivalent
2-normal form set of rules (Rules 10-13). Finally,
we introduce rules to handle back-off (Rules 14-16)
and upgrade (Rule 17). These allow SG-ITG to re-
</bodyText>
<equation confidence="0.619069">
‘~Ç
</equation>
<page confidence="0.964474">
715
</page>
<bodyText confidence="0.843211">
vert function words to normal words and vice versa.
</bodyText>
<equation confidence="0.3608525">
(10) R—* YiX (11) L —* XYi
(12) X—* LX (13) X—* XR
(14) Yi—* X (15) R—* X
(16) L —* X (17) X—* YU
</equation>
<bodyText confidence="0.999085230769231">
Back-off rules are needed when the grammar has
to reorder two adjacent function words, where one
set of orientation statistics must take precedence
over the other. The example in Fig.1 illustrates such
a case where the orientation of ± (on) and In, (of)
compete for influence. In this case, the grammar
chooses to use I n,(of) and reverts the function word
� (on) to the unlexicalized form.
The upgrade rule is used for cases where there are
two adjacent phrases, both of which are not function
words. Upgrading allows either phrase to act as a
function word, making use of the universal word’s
orientation statistics to reorder its neighbor.
</bodyText>
<subsectionHeader confidence="0.974456">
4.2 Statistical model
</subsectionHeader>
<bodyText confidence="0.999762833333333">
We now formulate the FWS model as a statistical
framework. We replace the deterministic rules in our
SG-ITG grammar with probabilistic ones, elevating
it to a stochastic grammar. In particular, we develop
the three sub models (see Section 3) which influence
the choice of production rules for ordering decision.
These models operate on the 2-norm rules, where the
RHS contains one function word and its argument
(except in the case of the phrase boundary model).
We provide the intuition for these models next, but
their actual form will be discussed in the next section
on training.
</bodyText>
<listItem confidence="0.988690727272727">
1) Orientation Model ori(o|H,Yi): This model
captures the preference of a function word Yi to a
particular orientation o E {MA, RA, MG, RG} in
reordering its H E {left, right} argument X. The
parameter H determines which set of Yi’s statistics
to use (left or right); the model consults Yi’s left ori-
entation statistic for Rules 11 and 13 where X pre-
cedes Yi, otherwise Yi’s right orientation statistic is
used for Rules 10 and 12.
2) Preference Model pref(Yi): This model ar-
bitrates reordering in the cases where two function
</listItem>
<bodyText confidence="0.953281535714286">
words are adjacent and the backoff rules have to de-
cide which function word takes precedence, revert-
ing the other to the unlexicalized X form. This
model prefers the function word with higher uni-
gram probability to take the precedence.
3) Phrase Boundary Model pb(X): This model is
a penalty-based model, favoring the resulting align-
ment that conforms to the source constituent bound-
ary. It penalizes Rule 1 if the terminal rule X
emits a Chinese phrase that violates the boundary
(pb = e−1), otherwise it is inactive (pb = 1).
These three sub models act as features alongside
seven other standard SMT features in a log-linear
model, resulting in the following set of features
{f1, ... , f10}: f1) orientation ori(o|H,Yi); f2)
preference pref(Yi); f3) phrase boundary pb(X);
f4) language model lm(e); f5 − f6) phrase trans-
lation score O(e|c) and its inverse 0(c|e); f7 − f8)
lexical weight lex(e|c) and its inverse lex(c|e); f9)
word penalty wp; and f10) phrase penalty pp.
The translation is then obtained from the most
probable derivation of the stochastic SG-ITG. The
formula for a single derivation is shown in Eq. (18),
where X1, X2,..., XL is a sequence of rules with
w(Xl) being the weight of each particular rule Xl.
w(Xl) is estimated through a log-linear model, as
in Eq. (19), with all the abovementioned features
where Aj reflects the contribution of each feature fj.
</bodyText>
<equation confidence="0.944113666666667">
L
(18) P(X1, ..., XL) = 11l=1w(Xl)
(19) w(Xl) = Yj01fj(Xl)λ&apos;
</equation>
<sectionHeader confidence="0.941777" genericHeader="method">
5 Training
</sectionHeader>
<bodyText confidence="0.999960777777778">
We train the orientation and preference models from
statistics of a training corpus. To this end, we first
derive the event counts and then compute the rela-
tive frequency of each event. The remaining phrase
boundary model can be modeled by the output of a
standard text chunker, as in practice it is simply a
constituent boundary detection mechanism together
with a penalty scheme.
The events of interest to the orientation model are
(Yi, o) tuples, where o E {MA, RA, MG, RG} is
an orientation value of a particular function word
Yi. Note that these tuples are not directly observable
from training data. Hence, we need an algorithm to
derive (Yi, o) tuples from a parallel corpus. Since
both left and right statistics share identical training
steps, thus we omit references to them.
The algorithm to derive (Yi, o) involves several
steps. First, we estimate the bi-directional alignment
</bodyText>
<page confidence="0.965455">
716
</page>
<bodyText confidence="0.959865282608695">
by running GIZA++ and applying the “grow-diag- is the dominant one, suggests that the grammar in-
final” heuristic. Then, the algorithm enumerates all versely order In, (of)’s left argument; while in our
Yi and determines its orientation o with respect to illustration of backoff rules in Fig.1, the grammar
its argument X to derive (Yi, o). To determine o, chooses I , (of) to take precedence since pref(In, ) &gt;
the algorithm inspects the monotonicity (monotone pref(�) .
or reverse) and adjacency (adjacent or gap) between 6 Decoding
Yi’s and X’s alignments. We employ a bottom-up CKY parser with a beam
Monotonicity can be determined by looking at the to find the derivation of a Chinese sentence which
Yi’s alignment with respect to the most fine-grained maximizes Eq. (18). The English translation is then
level of X (i.e., word level alignment). However, obtained by post-processing the best parse.
such a heuristic may inaccurately suggest gap ori- We set the beam size to 30 in our experiment and
entation. Figure 1 illustrates this problem when de- further constrain reordering to occur within a win-
riving the orientation for the second n, (of). Look- dow of 10 words. Our decoder also prunes entries
ing only at the word alignment of its left argument that violate the following constraints: 1) each entry
:R (fields) incorrectly suggests a gapped orientation, contains at most one gap; 2) any gapped entries must
where the alignment of RIMIA (data entry) in- be dovetailed at the next level higher; 3) an entry
tervened. It is desirable to look at the alignment of spanning the whole sentence must not contain gaps.
IMIA (data entry fields) at the phrase level, The score of each newly-created entry is derived
which suggests the correct adjacent orientation in- from the scores of its parts accordingly. When scor-
stead. ing entries, we treat gapped entries as contiguous
To address this issue, the algorithm uses gap- phrases by ignoring the gap symbol and rely on the
ping conservatively by utilizing the consistency con- orientation model to penalize such entries. This al-
straint (Och and Ney, 2004) to suggest phrase level lows a fair score comparison between gapped and
alignment of X. The algorithm exhaustively grows contiguous entries.
consistent blocks containing the most fine-grained 7 Experiments
level of X not including Yi. Subsequently, it merges We would like to study how the FWS model affects
each hypothetical argument with the Yi’s alignment. 1) the ordering of phrases around function words; 2)
The algorithm decides that Yi has a gapped orienta- the overall translation quality. We achieve this by
tion only if all merged blocks violate the consistency evaluating the FWS model against a baseline system
constraint, concluding an adjacent orientation other- using two metrics, namely, orientation accuracy and
wise. BLEU respectively.
With the event counts C(Yi, o) of tuple (Yi, o), we We define the orientation accuracy of a (function)
estimate the orientation model for Yi and U using word as the accuracy of assigning correct orientation
Eqs. (20) and (21). We also estimate the prefer- values to both its left and right arguments. We report
ence model with word unigram counts C(Yi) using the aggregate for the top 1024 most frequent words;
Eqs. (22) and (23), where V indicates the vocabu- these words cover 90% of the test set.
lary size. We devise a series of experiments and run it in two
(20) ori(o|Yi) = C(Yi, o)/C(Yi, ·), i S N scenarios - manual and automatic alignment - to as-
(21) ori(o|U) = � C(Yi, o)/ � C(Yi, ·) sess the effects of using perfect or real-world input.
i&gt;N i&gt;N We utilize the HIT bilingual computer manual cor-
(22) pref(Yi) = C(Yi)/C(·), i S N pus, which has been manually aligned, to perform
(23) pref(U) = 1/(V − N) � C(Yi)/C(·) Chinese-to-English translation (see Table 2). Man-
i&gt;N ual alignment is essential as we need to measure ori-
Samples of these statistics are found in Table 1 entation accuracy with respect to a gold standard.
and have been used in the running examples. For
instance, the statistic ori(RAL|In, ) = 0.52, which
</bodyText>
<table confidence="0.9589665">
717
Chinese English
train words 145,731 135,032
(7K sentences) vocabulary 5,267 8,064
dev words 13,986 14,638
(1K sentences) untranslatable 486 (3.47%)
test words 27,732 28,490
(2K sentences) untranslatable 935 (3.37%)
</table>
<tableCaption confidence="0.987393">
Table 2: Statistics for the HIT corpus.
</tableCaption>
<bodyText confidence="0.999981285714286">
A language model is trained using the SRILM-
Toolkit, and a text chunker (Chen et al., 2006) is ap-
plied to the Chinese sentences in the test and dev
sets to extract the constituent boundaries necessary
for the phrase boundary model. We run minimum er-
ror rate training on dev set using Chiang’s toolkit to
find a set of parameters that optimizes BLEU score.
</bodyText>
<subsectionHeader confidence="0.990618">
7.1 Perfect Lexical Choice
</subsectionHeader>
<bodyText confidence="0.9999728">
Here, the task is simplified to recovering the correct
order of the English sentence from the scrambled
Chinese order. We trained the orientation model us-
ing manual alignment as input. The aforementioned
decoder is used with phrase translation, lexical map-
ping and penalty features turned off.
Table 4 compares orientation accuracy and BLEU
between our FWS model and the baseline. The
baseline (lm+d) employs a language model and
distortion penalty features, emulating the standard
Pharaoh model. We study the behavior of the
FWS model with different numbers of lexicalized
items N. We start with the language model alone
(N=0) and incrementally add the orientation (+ori),
preference (+ori+pref) and phrase boundary models
(+ori+pref+pb).
As shown, the language model alone is rela-
tively weak, assigning the correct orientation in only
62.28% of the cases. A closer inspection reveals that
the lm component aggressively promotes reverse re-
orderings. Including a distortion penalty model (the
baseline) improves the accuracy to 72.55%. This
trend is also apparent for the BLEU score.
When we incorporate the FSW model, including
just the most frequent word (Y1=n, ), we see im-
provement. This model promotes non-monotone re-
ordering conservatively around Y1 (where the dom-
inant statistic suggests reverse ordering). Increasing
the value of N leads to greater improvement. The
most effective improvement is obtained by increas-
</bodyText>
<table confidence="0.99962025">
pharaoh (dl=5) 22.44 f 0.94
+ori 23.80 f 0.98
+ori+pref 23.85 f 1.00
+ori+pref+pb 23.86 f 1.08
</table>
<tableCaption confidence="0.997842">
Table 3: BLEU score with the 95% confidence in-
</tableCaption>
<bodyText confidence="0.935323777777778">
tervals based on (Zhang and Vogel, 2004). All im-
provement over the baseline (row 1) are statistically
significant under paired bootstrap resampling.
ing N to 128. Additional (marginal) improvement
is obtained at the expense of modeling an additional
900+ lexical items. We see these results as validat-
ing our claim that modeling the top few most fre-
quent words captures most important and prevalent
ordering productions.
Lastly, we study the effect of the pref and pb fea-
tures. The inclusion of both sub models has little af-
fect on orientation accuracy, but it improves BLEU
consistently (although not significantly). This sug-
gests that both models correct the mistakes made by
the ori model while preserving the gain. They are
not as effective as the addition of the basic orienta-
tion model as they only play a role when two lexi-
calized entries are adjacent.
</bodyText>
<subsectionHeader confidence="0.966914">
7.2 Full SMT experiments
</subsectionHeader>
<bodyText confidence="0.99995545">
Here, all knowledge is automatically trained on the
train set, and as a result, the input word alignment
is noisy. As a baseline, we use the state-of-the-art
phrase-based Pharaoh decoder. For a fair compari-
son, we run minimum error rate training for different
distortion limits from 0 to 10 and report the best pa-
rameter (dl=5) as the baseline.
We use the phrase translation table from the base-
line and perform an identical set of experiments as
the perfect lexical choice scenario, except that we
only report the result for N=128, due to space con-
straint. Table 3 reports the resulting BLEU scores.
As shown, the FWS model improves BLEU score
significantly over the baseline. We observe the same
trend as the one in perfect lexical choice scenario
where top 128 most frequent words provides the ma-
jority of improvement. However, the pb features
yields no noticeable improvement unlike in prefect
lexical choice scenario; this is similar to the findings
in (Koehn et al., 2003).
</bodyText>
<page confidence="0.991104">
718
</page>
<table confidence="0.9998175">
N=0 N=1 N=4 N=16 N=64 N=128 N=256 N=1024
Orientation lm+d 72.55
Acc. (%)
+ori 62.28 76.52 76.58 77.38 77.54 78.17 77.76 78.38
+ori+pref 76.66 76.82 77.57 77.74 78.13 77.94 78.54
+ori+pref+pb 76.70 76.85 77.58 77.70 78.20 77.94 78.56
BLEU lm+d 75.13
+ori 66.54 77.54 77.57 78.22 78.48 78.76 78.58 79.20
+ori+pref 77.60 77.70 78.29 78.65 78.77 78.70 79.30
+ori+pref+pb 77.69 77.80 78.34 78.65 78.93 78.79 79.30
</table>
<tableCaption confidence="0.832715666666667">
Table 4: Results using perfect aligned input. Here, (lm+d) is the baseline; (+ori), (+ori+pref) and
(+ori+pref+pb) are different FWS configurations. The results of the model (where N is varied) that fea-
tures the largest gain are bold, whereas the highest score is italicized.
</tableCaption>
<sectionHeader confidence="0.997277" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999989913043478">
In this paper, we present a statistical model to cap-
ture the grammatical information encoded in func-
tion words. Formally, we develop the Function Word
Syntax-based (FWS) model, a probabilistic syn-
chronous grammar, to encode the orientation statis-
tics of arguments to function words. Our experimen-
tal results shows that the FWS model significantly
improves the state-of-the-art phrase-based model.
We have touched only the surface benefits of mod-
eling function words. In particular, our proposal is
limited to modeling function words in the source
language. We believe that conditioning on both
source and target pair would result in more fine-
grained, accurate orientation statistics.
From our error analysis, we observe that 1) re-
ordering may span several levels and the preference
model does not handle this phenomena well; 2) cor-
rectly reordered phrases with incorrect boundaries
severely affects BLEU score and the phrase bound-
ary model is inadequate to correct the boundaries es-
pecially for cases of long phrase. In future, we hope
to address these issues while maintaining the bene-
fits offered by modeling function words.
</bodyText>
<sectionHeader confidence="0.999274" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997001787234043">
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical Lower Bounds on
the Complexity of Translational Equivalence. In
ACL/COLING 2006, pp. 977–984.
Christoph Tillman and Tong Zhang. 2005. A Localized
Prediction Model for Statistical Machine Translation.
In ACL 2005, pp. 557–564.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In ACL
2005, pp. 263–270.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377–403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for Sta-
tistical Machine Translation. In ACL/COLING 2006,
pp. 521–528.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Translation.
Computational Linguistics, 30(4):417–449.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A Clustered Global
Phrase Reordering Model for Statistical Machine
Translation. In ACL/COLING 2006, pp. 713–720.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, Robert L. Mercer 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263–311.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In HLT-NAACL
2003, pp. 127–133.
Richard Zens and Hermann Ney. 2003. A Compara-
tive Study on Reordering Constraints in Statistical Ma-
chine Translation. In ACL 2003.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-Right Target Generation for Hierarchi-
cal Phrase-Based Translation. In ACL/COLING 2006,
pp. 777–784.
Wenliang Chen, Yujie Zhang and Hitoshi Isahara 2006.
An Empirical Study of Chinese Chunking In ACL
2006 Poster Sessions, pp. 97–104.
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion Models for Statistical Machine Translation. In
ACL/COLING 2006, pp. 529–536.
Ying Zhang and Stephan Vogel. 2004. Measuring Confi-
dence Intervals for the Machine Translation Evaluation
Metrics. In TMI 2004.
</reference>
<page confidence="0.998467">
719
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.340031">
<title confidence="0.999942">Ordering Phrases with Function Words</title>
<author confidence="0.999687">Setiawan Kan</author>
<affiliation confidence="0.999914">School of Computing National University of Singapore</affiliation>
<address confidence="0.895272">Singapore 117543</address>
<author confidence="0.532735">Haizhou Li</author>
<affiliation confidence="0.999926">Institute for Infocomm Research</affiliation>
<address confidence="0.906366">21 Heng Mui Keng Terrace Singapore 119613</address>
<email confidence="0.851493">hli@i2r.a-star.edu.sg</email>
<abstract confidence="0.99777715">This paper presents a Function Word centered, Syntax-based (FWS) solution to address phrase ordering in the context of statistical machine translation (SMT). Motivated by the observation that function words often encode grammatical relationship among phrases within a sentence, we propose a probabilistic synchronous grammar to model the ordering of function words and their left and right arguments. We improve phrase ordering performance by lexicalizing the resulting rules in a small number of cases corresponding to function words. The experiments show that the FWS approach consistently outperforms the baseline system in ordering function words’ arguments and improving translation quality in both perfect and noisy word alignment scenarios.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Benjamin Wellington</author>
<author>Sonjia Waxmonsky</author>
<author>I Dan Melamed</author>
</authors>
<date>2006</date>
<booktitle>Empirical Lower Bounds on the Complexity of Translational Equivalence. In ACL/COLING</booktitle>
<pages>977--984</pages>
<contexts>
<context position="11537" citStr="Wellington et al. (2006)" startWordPosition="1818" endWordPosition="1821">n. 4 The FWS Model We first discuss the extension of standard ITG to accommodate gapping and then detail the statistical components of the model later. 4.1 Single Gap ITG (SG-ITG) The FWS model employs a synchronous grammar to describe the admissible orderings. The utility of ITG as a reordering constraint for most language pairs, is well-known both empirically (Zens and Ney, 2003) and analytically (Wu, 1997), however ITG’s straight (monotone) and inverted (reverse) rules exhibit strong cohesiveness, which is inadequate to express orientations that require gaps. We propose SG-ITG that follows Wellington et al. (2006)’s suggestion to model at most one gap. We show the rules for SG-ITG below. Rules 1- 3 are identical to those defined in standard ITG, in which monotone and reverse orderings are represented by square and angle brackets, respectively. #1 #2 #3 714 Rank Word unigram MAL RAL MGL RGL MAR RAR MGR RGR 1 { 0.0580 0.45 0.52 0.01 0.02 0.44 0.52 0.01 0.03 2 0.0507 0.85 0.12 0.02 0.01 0.84 0.12 0.02 0.02 3 0.0550 0.99 0.01 0.00 0.00 0.92 0.08 0.00 0.00 4 0.0155 0.87 0.10 0.02 0.00 0.82 0.12 0.05 0.02 5 0.0153 0.84 0.11 0.01 0.04 0.88 0.11 0.01 0.01 6 �P 0.0138 0.95 0.02 0.01 0.01 0.97 0.02 0.01 0.00 7 &apos;</context>
</contexts>
<marker>Wellington, Waxmonsky, Melamed, 2006</marker>
<rawString>Benjamin Wellington, Sonjia Waxmonsky, and I. Dan Melamed. 2006. Empirical Lower Bounds on the Complexity of Translational Equivalence. In ACL/COLING 2006, pp. 977–984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
<author>Tong Zhang</author>
</authors>
<title>A Localized Prediction Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In ACL</booktitle>
<pages>557--564</pages>
<marker>Tillman, Zhang, 2005</marker>
<rawString>Christoph Tillman and Tong Zhang. 2005. A Localized Prediction Model for Statistical Machine Translation. In ACL 2005, pp. 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="4530" citStr="Chiang, 2005" startWordPosition="659" endWordPosition="660">odel (Nagata et al., 2006) and distortion model (Al-Onaizan and Papineni, 2006). However, these models are often fully lexicalized and sensitive to individual phrases. As a result, they are not robust to unseen phrases. A careful approximation is vital to avoid data sparseness. Proposals to alleviate this problem include utilizing bilingual phrase cluster or words at the phrase boundary (Nagata et al., 2006) as the phrase identity. The benefit of introducing lexical evidence without being fully lexicalized has been demonstrated by a recent state-of-the-art formally syntax-based model1, Hiero (Chiang, 2005). Hiero performs phrase ordering by using linked non-terminal symbols in its synchronous CFG production rules coupled with lexical evidence. However, since it is difficult to specify a well-defined rule, Hiero has to rely on weak heuristics (i.e., length-based thresholds) to extract rules. As a result, Hiero produces grammars of enormous size. Watanabe et al. (2006) further reduces the grammar’s size by enforcing all rules to comply with Greibach Normal Form. Taking the lexicalization an intuitive a step forward, we propose a novel, finer-grained solution which models the content and context i</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In ACL 2005, pp. 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="6624" citStr="Wu, 1997" startWordPosition="1004" endWordPosition="1005">nce pair. greatly reducing the computational overhead that arises when moving from phrase-based to syntaxbased approach. Furthermore, by modeling only high frequency words, we are able to obtain reliable statistics even in small datasets. Second, as opposed to Hiero, where phrase ordering is done implicitly alongside phrase translation and lexical weighting, we directly model the reordering process using orientation statistics. The FWS approach is also akin to (Xiong et al., 2006) in using a synchronous grammar as a reordering constraint. Instead of using Inversion Transduction Grammar (ITG) (Wu, 1997) directly, we will discuss an ITG extension to accommodate gapping. 3 Phrase Ordering around Function Words We use the following Chinese (c) to English (e) translation in Fig.1 as an illustration to conduct an inquiry to the problem. Note that the sentence translation requires some translations of English words to be ordered far from their original position in Chinese. Recovering the correct English ordering requires the inversion of the Chinese postpositional phrase, followed by the inversion of the first smaller noun phrase, and finally the inversion of the second larger noun phrase. Neverth</context>
<context position="11325" citStr="Wu, 1997" startWordPosition="1789" endWordPosition="1790">to knowledge embedded in function words. A new indexed symbol is introduced to indicate previously reordered phrases for conciseness. Step 3 finally maps Chinese phrases to their English translation. 4 The FWS Model We first discuss the extension of standard ITG to accommodate gapping and then detail the statistical components of the model later. 4.1 Single Gap ITG (SG-ITG) The FWS model employs a synchronous grammar to describe the admissible orderings. The utility of ITG as a reordering constraint for most language pairs, is well-known both empirically (Zens and Ney, 2003) and analytically (Wu, 1997), however ITG’s straight (monotone) and inverted (reverse) rules exhibit strong cohesiveness, which is inadequate to express orientations that require gaps. We propose SG-ITG that follows Wellington et al. (2006)’s suggestion to model at most one gap. We show the rules for SG-ITG below. Rules 1- 3 are identical to those defined in standard ITG, in which monotone and reverse orderings are represented by square and angle brackets, respectively. #1 #2 #3 714 Rank Word unigram MAL RAL MGL RGL MAR RAR MGR RGR 1 { 0.0580 0.45 0.52 0.01 0.02 0.44 0.52 0.01 0.03 2 0.0507 0.85 0.12 0.02 0.01 0.84 0.12 </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In ACL/COLING</booktitle>
<pages>521--528</pages>
<contexts>
<context position="6500" citStr="Xiong et al., 2006" startWordPosition="982" endWordPosition="985">������� �� ����������� � �� � � � �� �� � � � � � � a form is a coll. of data entry fields on a page Figure 1: A Chinese-English sentence pair. greatly reducing the computational overhead that arises when moving from phrase-based to syntaxbased approach. Furthermore, by modeling only high frequency words, we are able to obtain reliable statistics even in small datasets. Second, as opposed to Hiero, where phrase ordering is done implicitly alongside phrase translation and lexical weighting, we directly model the reordering process using orientation statistics. The FWS approach is also akin to (Xiong et al., 2006) in using a synchronous grammar as a reordering constraint. Instead of using Inversion Transduction Grammar (ITG) (Wu, 1997) directly, we will discuss an ITG extension to accommodate gapping. 3 Phrase Ordering around Function Words We use the following Chinese (c) to English (e) translation in Fig.1 as an illustration to conduct an inquiry to the problem. Note that the sentence translation requires some translations of English words to be ordered far from their original position in Chinese. Recovering the correct English ordering requires the inversion of the Chinese postpositional phrase, fol</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In ACL/COLING 2006, pp. 521–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="2595" citStr="Och and Ney, 2004" startWordPosition="388" endWordPosition="391">ems use statistical knowledge obtained from corpora in favor of rich natural language knowledge. Instead of using syntactic knowledge to determine function words, we approximate this by equating the most frequent words as function words. By explicitly modeling phrase ordering around these frequent words, we aim to capture the most important and prevalent ordering productions. 2 Related Work A good translation should be both faithful with adequate lexical choice to the source language and fluent in its word ordering to the target language. In pursuit of better translation, phrase-based models (Och and Ney, 2004) have significantly improved the quality over classical word-based models (Brown et al., 1993). These multiword phrasal units contribute to fluency by inherently capturing intra-phrase reordering. However, despite this progress, interphrase reordering (especially long distance ones) still poses a great challenge to statistical machine translation (SMT). The basic phrase reordering model is a simple unlexicalized, context-insensitive distortion penalty model (Koehn et al., 2003). This model assumes little or no structural divergence between language pairs, preferring the original, translated or</context>
<context position="21856" citStr="Och and Ney, 2004" startWordPosition="3621" endWordPosition="3624"> level higher; 3) an entry tervened. It is desirable to look at the alignment of spanning the whole sentence must not contain gaps. IMIA (data entry fields) at the phrase level, The score of each newly-created entry is derived which suggests the correct adjacent orientation in- from the scores of its parts accordingly. When scorstead. ing entries, we treat gapped entries as contiguous To address this issue, the algorithm uses gap- phrases by ignoring the gap symbol and rely on the ping conservatively by utilizing the consistency con- orientation model to penalize such entries. This alstraint (Och and Ney, 2004) to suggest phrase level lows a fair score comparison between gapped and alignment of X. The algorithm exhaustively grows contiguous entries. consistent blocks containing the most fine-grained 7 Experiments level of X not including Yi. Subsequently, it merges We would like to study how the FWS model affects each hypothetical argument with the Yi’s alignment. 1) the ordering of phrases around function words; 2) The algorithm decides that Yi has a gapped orienta- the overall translation quality. We achieve this by tion only if all merged blocks violate the consistency evaluating the FWS model ag</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
<author>Kuniko Saito</author>
<author>Kazuhide Yamamoto</author>
<author>Kazuteru Ohashi</author>
</authors>
<title>A Clustered Global Phrase Reordering Model for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In ACL/COLING</booktitle>
<pages>713--720</pages>
<contexts>
<context position="3943" citStr="Nagata et al., 2006" startWordPosition="569" endWordPosition="572"> the 45th Annual Meeting of the Association of Computational Linguistics, pages 712–719, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics model, but is otherwise impoverished without any lexical evidence to characterize the reordering. To address this, lexicalized context-sensitive models incorporate contextual evidence. The local prediction model (Tillmann and Zhang, 2005) models structural divergence as the relative position between the translation of two neighboring phrases. Other further generalizations of orientation include the global prediction model (Nagata et al., 2006) and distortion model (Al-Onaizan and Papineni, 2006). However, these models are often fully lexicalized and sensitive to individual phrases. As a result, they are not robust to unseen phrases. A careful approximation is vital to avoid data sparseness. Proposals to alleviate this problem include utilizing bilingual phrase cluster or words at the phrase boundary (Nagata et al., 2006) as the phrase identity. The benefit of introducing lexical evidence without being fully lexicalized has been demonstrated by a recent state-of-the-art formally syntax-based model1, Hiero (Chiang, 2005). Hiero perfo</context>
<context position="8138" citStr="Nagata et al., 2006" startWordPosition="1248" endWordPosition="1251">iven precedence, in case of conflicts. We propose modeling these sources of knowledge using a statistical formalism. It includes 1) a model to capture bilingual orientations of the left and right arguments of these function words; 2) a model to approximate correct reordering sequence; and 3) a model for finding constituent boundaries of 14 M_-N � a 713 the left and right arguments. Assuming that the most frequent words in a language are function words, we can apply orientation statistics associated with these words to reorder their adjacent left and right neighbors. We follow the notation in (Nagata et al., 2006) and define the following bilingual orientation values given two neighboring source (Chinese) phrases: Monotone-Adjacent (MA); ReverseAdjacent (RA); Monotone-Gap (MG); and ReverseGap (RG). The first clause (monotone, reverse) indicates whether the target language translation order follows the source order; the second (adjacent, gap) indicates whether the source phrases are adjacent or separated by an intervening phrase on the target side. Table 1 shows the orientation statistics for several function words. Note that we separate the statistics for left and right arguments to account for differe</context>
</contexts>
<marker>Nagata, Saito, Yamamoto, Ohashi, 2006</marker>
<rawString>Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto, and Kazuteru Ohashi. 2006. A Clustered Global Phrase Reordering Model for Statistical Machine Translation. In ACL/COLING 2006, pp. 713–720.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2689" citStr="Brown et al., 1993" startWordPosition="401" endWordPosition="404">ge. Instead of using syntactic knowledge to determine function words, we approximate this by equating the most frequent words as function words. By explicitly modeling phrase ordering around these frequent words, we aim to capture the most important and prevalent ordering productions. 2 Related Work A good translation should be both faithful with adequate lexical choice to the source language and fluent in its word ordering to the target language. In pursuit of better translation, phrase-based models (Och and Ney, 2004) have significantly improved the quality over classical word-based models (Brown et al., 1993). These multiword phrasal units contribute to fluency by inherently capturing intra-phrase reordering. However, despite this progress, interphrase reordering (especially long distance ones) still poses a great challenge to statistical machine translation (SMT). The basic phrase reordering model is a simple unlexicalized, context-insensitive distortion penalty model (Koehn et al., 2003). This model assumes little or no structural divergence between language pairs, preferring the original, translated order by penalizing reordering. This simple model works well when properly coupled with a well-t</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Robert L. Mercer 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL</booktitle>
<pages>127--133</pages>
<contexts>
<context position="3077" citStr="Koehn et al., 2003" startWordPosition="452" endWordPosition="455">anguage and fluent in its word ordering to the target language. In pursuit of better translation, phrase-based models (Och and Ney, 2004) have significantly improved the quality over classical word-based models (Brown et al., 1993). These multiword phrasal units contribute to fluency by inherently capturing intra-phrase reordering. However, despite this progress, interphrase reordering (especially long distance ones) still poses a great challenge to statistical machine translation (SMT). The basic phrase reordering model is a simple unlexicalized, context-insensitive distortion penalty model (Koehn et al., 2003). This model assumes little or no structural divergence between language pairs, preferring the original, translated order by penalizing reordering. This simple model works well when properly coupled with a well-trained language 712 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 712–719, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics model, but is otherwise impoverished without any lexical evidence to characterize the reordering. To address this, lexicalized context-sensitive models incorporate contextual eviden</context>
<context position="27909" citStr="Koehn et al., 2003" startWordPosition="4619" endWordPosition="4622">rase translation table from the baseline and perform an identical set of experiments as the perfect lexical choice scenario, except that we only report the result for N=128, due to space constraint. Table 3 reports the resulting BLEU scores. As shown, the FWS model improves BLEU score significantly over the baseline. We observe the same trend as the one in perfect lexical choice scenario where top 128 most frequent words provides the majority of improvement. However, the pb features yields no noticeable improvement unlike in prefect lexical choice scenario; this is similar to the findings in (Koehn et al., 2003). 718 N=0 N=1 N=4 N=16 N=64 N=128 N=256 N=1024 Orientation lm+d 72.55 Acc. (%) +ori 62.28 76.52 76.58 77.38 77.54 78.17 77.76 78.38 +ori+pref 76.66 76.82 77.57 77.74 78.13 77.94 78.54 +ori+pref+pb 76.70 76.85 77.58 77.70 78.20 77.94 78.56 BLEU lm+d 75.13 +ori 66.54 77.54 77.57 78.22 78.48 78.76 78.58 79.20 +ori+pref 77.60 77.70 78.29 78.65 78.77 78.70 79.30 +ori+pref+pb 77.69 77.80 78.34 78.65 78.93 78.79 79.30 Table 4: Results using perfect aligned input. Here, (lm+d) is the baseline; (+ori), (+ori+pref) and (+ori+pref+pb) are different FWS configurations. The results of the model (where N is</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In HLT-NAACL 2003, pp. 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>A Comparative Study on Reordering Constraints in Statistical Machine Translation. In</title>
<date>2003</date>
<booktitle>ACL</booktitle>
<contexts>
<context position="11297" citStr="Zens and Ney, 2003" startWordPosition="1783" endWordPosition="1786">ed. Step 2 reorders phrases according to knowledge embedded in function words. A new indexed symbol is introduced to indicate previously reordered phrases for conciseness. Step 3 finally maps Chinese phrases to their English translation. 4 The FWS Model We first discuss the extension of standard ITG to accommodate gapping and then detail the statistical components of the model later. 4.1 Single Gap ITG (SG-ITG) The FWS model employs a synchronous grammar to describe the admissible orderings. The utility of ITG as a reordering constraint for most language pairs, is well-known both empirically (Zens and Ney, 2003) and analytically (Wu, 1997), however ITG’s straight (monotone) and inverted (reverse) rules exhibit strong cohesiveness, which is inadequate to express orientations that require gaps. We propose SG-ITG that follows Wellington et al. (2006)’s suggestion to model at most one gap. We show the rules for SG-ITG below. Rules 1- 3 are identical to those defined in standard ITG, in which monotone and reverse orderings are represented by square and angle brackets, respectively. #1 #2 #3 714 Rank Word unigram MAL RAL MGL RGL MAR RAR MGR RGR 1 { 0.0580 0.45 0.52 0.01 0.02 0.44 0.52 0.01 0.03 2 0.0507 0.</context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>Richard Zens and Hermann Ney. 2003. A Comparative Study on Reordering Constraints in Statistical Machine Translation. In ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Left-to-Right Target Generation for Hierarchical Phrase-Based Translation.</title>
<date>2006</date>
<booktitle>In ACL/COLING</booktitle>
<pages>777--784</pages>
<contexts>
<context position="4898" citStr="Watanabe et al. (2006)" startWordPosition="715" endWordPosition="718">r words at the phrase boundary (Nagata et al., 2006) as the phrase identity. The benefit of introducing lexical evidence without being fully lexicalized has been demonstrated by a recent state-of-the-art formally syntax-based model1, Hiero (Chiang, 2005). Hiero performs phrase ordering by using linked non-terminal symbols in its synchronous CFG production rules coupled with lexical evidence. However, since it is difficult to specify a well-defined rule, Hiero has to rely on weak heuristics (i.e., length-based thresholds) to extract rules. As a result, Hiero produces grammars of enormous size. Watanabe et al. (2006) further reduces the grammar’s size by enforcing all rules to comply with Greibach Normal Form. Taking the lexicalization an intuitive a step forward, we propose a novel, finer-grained solution which models the content and context information encoded by function words - approximated by high frequency words. Inspired by the success of syntaxbased approaches, we propose a synchronous grammar that accommodates gapping production rules, while focusing on the statistical modeling in relation to function words. We refer to our approach as the Function Word-centered Syntax-based approach (FWS). Our F</context>
</contexts>
<marker>Watanabe, Tsukada, Isozaki, 2006</marker>
<rawString>Taro Watanabe, Hajime Tsukada, and Hideki Isozaki. 2006. Left-to-Right Target Generation for Hierarchical Phrase-Based Translation. In ACL/COLING 2006, pp. 777–784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
</authors>
<title>Yujie Zhang and Hitoshi Isahara</title>
<date>2006</date>
<booktitle>In ACL 2006 Poster Sessions,</booktitle>
<pages>97--104</pages>
<marker>Chen, 2006</marker>
<rawString>Wenliang Chen, Yujie Zhang and Hitoshi Isahara 2006. An Empirical Study of Chinese Chunking In ACL 2006 Poster Sessions, pp. 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion Models for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In ACL/COLING</booktitle>
<pages>529--536</pages>
<contexts>
<context position="3996" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="576" endWordPosition="579"> of Computational Linguistics, pages 712–719, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics model, but is otherwise impoverished without any lexical evidence to characterize the reordering. To address this, lexicalized context-sensitive models incorporate contextual evidence. The local prediction model (Tillmann and Zhang, 2005) models structural divergence as the relative position between the translation of two neighboring phrases. Other further generalizations of orientation include the global prediction model (Nagata et al., 2006) and distortion model (Al-Onaizan and Papineni, 2006). However, these models are often fully lexicalized and sensitive to individual phrases. As a result, they are not robust to unseen phrases. A careful approximation is vital to avoid data sparseness. Proposals to alleviate this problem include utilizing bilingual phrase cluster or words at the phrase boundary (Nagata et al., 2006) as the phrase identity. The benefit of introducing lexical evidence without being fully lexicalized has been demonstrated by a recent state-of-the-art formally syntax-based model1, Hiero (Chiang, 2005). Hiero performs phrase ordering by using linked non-terminal symb</context>
</contexts>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion Models for Statistical Machine Translation. In ACL/COLING 2006, pp. 529–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
</authors>
<title>Measuring Confidence Intervals for the Machine Translation Evaluation Metrics. In TMI</title>
<date>2004</date>
<contexts>
<context position="26087" citStr="Zhang and Vogel, 2004" startWordPosition="4313" endWordPosition="4316">model (the baseline) improves the accuracy to 72.55%. This trend is also apparent for the BLEU score. When we incorporate the FSW model, including just the most frequent word (Y1=n, ), we see improvement. This model promotes non-monotone reordering conservatively around Y1 (where the dominant statistic suggests reverse ordering). Increasing the value of N leads to greater improvement. The most effective improvement is obtained by increaspharaoh (dl=5) 22.44 f 0.94 +ori 23.80 f 0.98 +ori+pref 23.85 f 1.00 +ori+pref+pb 23.86 f 1.08 Table 3: BLEU score with the 95% confidence intervals based on (Zhang and Vogel, 2004). All improvement over the baseline (row 1) are statistically significant under paired bootstrap resampling. ing N to 128. Additional (marginal) improvement is obtained at the expense of modeling an additional 900+ lexical items. We see these results as validating our claim that modeling the top few most frequent words captures most important and prevalent ordering productions. Lastly, we study the effect of the pref and pb features. The inclusion of both sub models has little affect on orientation accuracy, but it improves BLEU consistently (although not significantly). This suggests that bot</context>
</contexts>
<marker>Zhang, Vogel, 2004</marker>
<rawString>Ying Zhang and Stephan Vogel. 2004. Measuring Confidence Intervals for the Machine Translation Evaluation Metrics. In TMI 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>