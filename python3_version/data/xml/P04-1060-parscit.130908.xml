<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000044">
<title confidence="0.995562">
Experiments in Parallel-Text Based Grammar Induction
</title>
<author confidence="0.998757">
Jonas Kuhn
</author>
<affiliation confidence="0.9922705">
Department of Linguistics
The University of Texas at Austin
</affiliation>
<address confidence="0.811211">
Austin, TX 78712
</address>
<email confidence="0.998537">
jonak@mail.utexas.edu
</email>
<sectionHeader confidence="0.99388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999812272727273">
This paper discusses the use of statistical word
alignment over multiple parallel texts for the identi-
fication of string spans that cannot be constituents
in one of the languages. This information is ex-
ploited in monolingual PCFG grammar induction
for that language, within an augmented version of
the inside-outside algorithm. Besides the aligned
corpus, no other resources are required. We discuss
an implemented system and present experimental
results with an evaluation against the Penn Tree-
bank.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999905768115942">
There have been a number of recent studies exploit-
ing parallel corpora in bootstrapping of monolin-
gual analysis tools. In the “information projection”
approach (e.g., (Yarowsky and Ngai, 2001)), statis-
tical word alignment is applied to a parallel corpus
of English and some other language for which no
tagger/morphological analyzer/chunker etc. (hence-
forth simply: analysis tool) exists. A high-quality
analysis tool is applied to the English text, and
the statistical word alignment is used to project a
(noisy) target annotation to the version of the text.
Robust learning techniques are then applied to boot-
strap an analysis tool for , using the annotations
projected with high confidence as the initial train-
ing data. (Confidence of both the English analysis
tool and the statistical word alignment is taken into
account.) The results that have been achieved by
this method are very encouraging.
Will the information projection approach also
work for less shallow analysis tools, in particular
full syntactic parsers? An obvious issue is that
one does not expect the phrase structure representa-
tion of English (as produced by state-of-the-art tree-
bank parsers) to carry over to less configurational
languages. Therefore, (Hwa et al., 2002) extract
a more language-independent dependency structure
from the English parse as the basis for projection
to Chinese. From the resulting (noisy) dependency
treebank, a dependency parser is trained using the
techniques of (Collins, 1999). (Hwa et al., 2002) re-
port that the noise in the projected treebank is still
a major challenge, suggesting that a future research
focus should be on the filtering of (parts of) unre-
liable trees and statistical word alignment models
sensitive to the syntactic projection framework.
Our hypothesis is that the quality of the result-
ing parser/grammar for language can be signifi-
cantly improved if the training method for the parser
is changed to accomodate for training data which
are in part unreliable. The experiments we report
in this paper focus on a specific part of the prob-
lem: we replace standard treebank training with
an Expectation-Maximization (EM) algorithm for
PCFGs, augmented by weighting factors for the re-
liability of training data, following the approach of
(Nigam et al., 2000), who apply it for EM train-
ing of a text classifier. The factors are only sen-
sitive to the constituent/distituent (C/D) status of
each span of the string in (cp. (Klein and Man-
ning, 2002)). The C/D status is derived from an
aligned parallel corpus in a way discussed in sec-
tion 2. We use the Europarl corpus (Koehn, 2002),
and the statistical word alignment was performed
with the GIZA++ toolkit (Al-Onaizan et al., 1999;
Och and Ney, 2003).1
For the current experiments we assume no pre-
existing parser for any of the languages, contrary
to the information projection scenario. While bet-
ter absolute results could be expected using one or
more parsers for the languages involved, we think
that it is important to isolate the usefulness of ex-
ploiting just crosslinguistic word order divergences
in order to obtain partial prior knowledge about the
constituent structure of a language, which is then
exploited in an EM learning approach (section 3).
Not using a parser for some languages also makes
it possible to compare various language pairs at the
same level, and specifically, we can experiment with
grammar induction for English exploiting various
</bodyText>
<footnote confidence="0.9441405">
1The software is available at
http://www.isi.edu/˜och/GIZA++.html
</footnote>
<bodyText confidence="0.7280815">
At that moment the voting will commence .
Le vote aura lieu à ce moment -la .
</bodyText>
<figureCaption confidence="0.996987">
Figure 1: Alignment example
</figureCaption>
<bodyText confidence="0.995225333333333">
other languages. Indeed the focus of our initial ex-
periments has been on English (section 4), which
facilitates evaluation against a treebank (section 5).
</bodyText>
<subsectionHeader confidence="0.517735">
2 Cross-language order divergences
</subsectionHeader>
<bodyText confidence="0.9875065">
The English-French example in figure 1 gives a sim-
ple illustration of the partial information about con-
stituency that a word-aligned parallel corpus may
provide. The en bloc reversal of subsequences of
words provides strong evidence that, for instance, [
moment the voting ] or [ aura lieu à ce ] do not form
constituents.
At first sight it appears as if there is also clear ev-
idence for [ at that moment ] forming a constituent,
since it fully covers a substring that appears in a dif-
ferent position in French. Similarly for [ Le vote
aura lieu ]. However, from the distribution of con-
tiguous substrings alone we cannot distinguish be-
tween two the types of situations sketched in (1) and
(2):
A string that is contiguous under projection, like
(1) may be a true constituent, but it may also
be a non-constituent part of a larger constituent as
in in (2).
Word blocks. Let us define the notion of a word
block (as opposed to a phrase or constituent) in-
duced by a word alignment to capture the relevant
property of contiguousness under translation.2 The
alignments induced by GIZA++ (following the IBM
models) are asymmetrical in that several words from
may be aligned with one word in , but not vice
versa. So we can view a word alignment as a func-
tion that maps each word in an -sentence to
a (possibly empty) subset of words from its trans-
lation in . For example, in figure 1, voting
={vote }, and that = {ce -la . Note that
for . The -images of
a sentence need not exhaust the words of the trans-
lation in ; however it is common to assume a
special empty word NULL in each -sentence, for
which by definition NULL is the set of -words
not contained in any -image of the overt words.
We now define an -induced block (or -block
for short) as a substring of a sentence in ,
such that the union over all -images (
or are -induced
blocks.
Let us define a maximal -block as an -block
, such that adding at the beginning or
at the end is either (i) impossible (because it
would lead to a non-block, or or do not
exist as we are at the beginning or end of the string),
or (ii) it would introduce a new crossing alignment
</bodyText>
<footnote confidence="0.998869">
2The block notion we are defining in this section is indi-
rectly related to the concept of a “phrase” in recent work in
Statistical Machine Translation. (Koehn et al., 2003) show that
exploiting all contiguous word blocks in phrase-based align-
ment is better than focusing on syntactic constituents only. In
our context, we are interested in inducing syntactic constituents
based on alignment information; given the observations from
Statistical MT, it does not come as a surprise that there is no di-
rect link from blocks to constituents. Our work can be seen as
an attempt to zero in on the distinction between the concepts;
we find that it is most useful to keep track of the boundaries
between blocks.
</footnote>
<bodyText confidence="0.92506853125">
(Wu, 1997) also includes a brief discussion of crossing con-
straints that can be derived from phrase structure correspon-
dences.
)
forms a contiguous substring in ,modulo the
words from NULL .
For example, in (1) (or (2)) is not
an -block since the union over its -images is
which do not form a contiguous string
in . The sequences
to the block.3
String in (1) is not a maximal -block, be-
cause is an -block; but is maxi-
mal since is the final word of the sentence and
is a non-block.
We can now make the initial observation precise
that (1) and (2) have the same block structure, but
the constituent structures are different (and this is
not due to an incorrect alignment). is a maxi-
mal block in both cases, but while it is a constituent
in (1), it isn’t in (2).
We may call maximal blocks that contain only
non-maximal blocks as substrings first-order max-
imal -blocks. A maximal block that contains other
maximal blocks as substrings is a higher-order
maximal -block. In (1) and (2), the complete
string is a higher-order maximal block.
Note that a higher-order maximal block may contain
substrings which are non-blocks.
Higher-order maximal blocks may still be non-
constituents as the following simple English-French
example shows:
</bodyText>
<listItem confidence="0.854917">
(3) He gave Mary a book
</listItem>
<bodyText confidence="0.989075083333333">
Il a donné un livre à Mary
The three first-order maximal blocks in English are
[He gave], [Mary], and [a book]. [Mary a book] is
a higher-order maximal block, since its “projection”
to French is contiguous, but it is not a constituent.
(Note that the VP constituent gave Mary a book on
the other hand is not a maximal block here.)
Block boundaries. Let us call the string position
between two maximal blocks an -block bound-
ary.4 In (1)/(2), the position between and is a
block boundary.
We can now formulate the
</bodyText>
<listItem confidence="0.655968">
(4) Distituent hypothesis
</listItem>
<bodyText confidence="0.977024">
If a substring of a sentence in language
crosses a first-order -block boundary (zones),
then it can only be a constituent of if it con-
tains at least one of the two maximal -blocks
separated by that boundary in full.
This hypothesis makes it precise under which
conditions we assume to have reliable negative evi-
dence against a constituent. Even examples of com-
plicated structural divergence from the classical MT
3I.e., an element of (or ) continues the -
string at the other end.
</bodyText>
<footnote confidence="0.868718333333333">
4We will come back to the situation where a block boundary
may not be unique below.
5This will be explained below.
</footnote>
<bodyText confidence="0.999818391304348">
literature tend not to pose counterexamples to the
hypothesis, since it is so conservative. Projecting
phrasal constituents from one language to another
is problematic in cases of divergence, but projecting
information about distituents is generally safe.
Mild divergences are best. As should be clear,
the -block-based approach relies on the occurrence
of reorderings of constituents in translation. If two
languages have the exact same structure (and no
paraphrases whatsoever are used in translation), the
approach does not gain any information from a par-
allel text. However, this situation does not occur
realistically. If on the other hand, massive reorder-
ing occurs without preserving any contiguous sub-
blocks, the approach cannot gain information either.
The ideal situation is in the middleground, with a
number of mid-sized blocks in most sentences. The
table in figure 2 shows the distribution of sentences
with -block boundaries based on the alignment
of English and 7 other languages, for a sample of c.
3,000 sentences from the Europarl corpus. We can
see that the occurrence of boundaries is in a range
that should make it indeed useful.6
</bodyText>
<table confidence="0.980128416666667">
: el es fi fr it sv
de
1 82.3% 76.7% 80.9% 70.2% 83.3% 82.9% 67.4%
2 73.5% 64.2% 74.0% 55.7% 76.0% 74.6% 58.0%
3 57.7% 50.4% 57.5% 39.3% 60.5% 60.7% 38.4%
4 47.9% 40.1% 50.9% 29.7% 53.3% 52.1% 31.3%
5 38.0% 30.6% 42.5% 21.5% 45.9% 42.0% 23.0%
6 28.7% 23.2% 33.4% 15.2% 36.1% 33.4% 15.2%
7 22.6% 17.9% 28.0% 10.2% 30.2% 26.6% 11.0%
8 17.0% 13.6% 22.4% 7.6% 24.4% 21.8% 8.0%
9 12.3% 10.3% 17.4% 5.4% 19.7% 17.3% 5.6%
10 9.5% 7.8% 13.7% 3.4% 16.3% 13.1% 4.1%
</table>
<figureCaption confidence="0.626459333333333">
de: German; el: Greek; es: Spanish; fi: Finnish;
fr: French; it: Italian; sv: Swedish.
Figure 2: Proportion of sentences with -block
</figureCaption>
<bodyText confidence="0.9769016">
boundaries for : English
Zero fertility words. So far we have not ad-
dressed the effect of finding zero fertility words,
i.e., words from with . Statistical
word alignment makes frequent use of this mech-
anism. An actual example from our alignment is
shown in figure 3. The English word has is treated
as a zero fertility word. While we can tell from the
block structure that there is a maximal block bound-
ary somewhere between Baringdorf and the, it is
</bodyText>
<footnote confidence="0.9798885">
6The average sentence length for the English sentence is
26.5 words. (Not too suprisingly, Swedish gives rise to the
fewest divergences against English. Note also that the Ro-
mance languages shown here behave very similarly.)
</footnote>
<note confidence="0.7660825">
Mr. Graefe zu Baringdorf has the floor to explain this request .
La parole est à M. Graefe zu Baringdorf pour motiver la demande .
</note>
<figureCaption confidence="0.998562">
Figure 3: Alignment example with zero-fertility word in English
</figureCaption>
<bodyText confidence="0.998802777777778">
unclear on which side has should be located.7
The definitions of the various types of word
blocks cover zero fertility words in principle, but
they are somewhat awkward in that the same word
may belong to two maximal -blocks, on its left and
on its right. It is not clear where the exact block
boundary is located. So we redefine the notion of -
block boundaries. We call the (possibly empty) sub-
string between the rightmost non-zero-fertility word
of one maximal -block and the leftmost non-zero-
fertility word of its right neighbor block the -block
boundary zone.
The distituent hypothesis is sensitive to crossing a
boundary zone, i.e., if a constituent-candidate ends
somewhere in the middle of a non-empty boundary
zone, this does not count as a crossing. This reflects
the intuition of uncertainty and keeps the exclusion
of clear distituents intact.
</bodyText>
<sectionHeader confidence="0.9353425" genericHeader="method">
3 EM grammar induction with weighting
factors
</sectionHeader>
<bodyText confidence="0.946144513513514">
The distituent identification scheme introduced in
the previous section can be used to hypothesize a
fairly reliable exclusion of constituency for many
spans of strings from a parallel corpus. Besides a
statistical word alignment, no further resources are
required.
In order to make use of this scattered (non-) con-
stituency information, a semi-supervised approach
is needed that can fill in the (potentially large) ar-
eas for which no prior information is available. For
the present experiments we decided to choose a con-
ceptually simple such approach, with which we can
build on substantial existing work in grammar in-
duction: we construe the learning problem as PCFG
induction, using the inside-outside algorithm, with
the addition of weighting factors based on the (non-
)constituency information. This use of weighting
factors in EM learning follows the approach dis-
cussed in (Nigam et al., 2000).
Since we are mainly interested in comparative ex-
periments at this stage, the conceptual simplicity,
and the availability of efficient implemented open-
7Since zero-fertility words are often function words, there
is probably a rightward-tendency that one might be able to ex-
ploit; however in the present study we didn’t want to build such
high-level linguistic assumptions into the system.
source systems of a PCFG induction approach out-
weighs the disadvantage of potentially poorer over-
all performance than one might expect from some
other approaches.
The PCFG topology we use is a binary, entirely
unrestricted X-bar-style grammar based on the Penn
Treebank POS-tagset (expanded as in the TreeTag-
ger by (Schmid, 1994)). All possible combinations
of projections of POS-categories X and Y are in-
cluded following the schemata in (5). This gives
rise to 13,110 rules.
</bodyText>
<equation confidence="0.5313314">
(5) a. XP X
b. XP XP YP
c. XP YP XP
d. XP YP X
e. XP X YP
</equation>
<bodyText confidence="0.999932962962963">
We tagged the English version of our training sec-
tion of the Europarl corpus with the TreeTagger and
used the strings of POS-tags as the training cor-
pus for the inside-outside algorithm; however, it is
straightforward to apply our approach to a language
for which no taggers are available if an unsuper-
vised word clustering technique is applied first.
We based our EM training algorithm on Mark
Johnson’s implementation of the inside-outside al-
gorithm.$ The initial parameters on the PCFG rules
are set to be uniform. In the iterative induction pro-
cess of parameter reestimation, the current rule pa-
rameters are used to compute the expectations of
how often each rule occurred in the parses of the
training corpus, and these expectations are used to
adjust the rule parameters, so that the likelihood of
the training data is increased. When the probablity
of a given rule drops below a certain threshold, the
rule is excluded from the grammar. The iteration
is continued until the increase in likelihood of the
training corpus is very small.
Weight factors. The inside-outside algorithm is a
dynamic programming algorithm that uses a chart
in order to compute the rule expectations for each
sentence. We use the information obtained from the
parallel corpus as discussed in section 2 as prior in-
formation (in a Bayesian framework) to adjust the
</bodyText>
<footnote confidence="0.812604">
8http://cog.brown.edu/˜mj/
</footnote>
<figure confidence="0.4230635">
you can table questions under rule 28 , and you no longer have the floor .
vous pouvez poser les questions au moyen de l’ article 28 du réglement . je ne vous donne pas la parole .
</figure>
<figureCaption confidence="0.999085">
Figure 4: Alignment example with higher-fertility words in English
</figureCaption>
<bodyText confidence="0.999915222222222">
expectations that the inside-outside algorithm deter-
mines based on its current rule parameters. Note
that the this prior information is information about
string spans of (non-)constituents – it does not tell
us anything about the categories of the potential
constituents affected. It is combined with the PCFG
expectations as the chart is constructed. For each
span in the chart, we get a weight factor that is mul-
tiplied with the parameter-based expectations.9
</bodyText>
<sectionHeader confidence="0.999742" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.998857260869565">
We applied GIZA++ (Al-Onaizan et al., 1999; Och
and Ney, 2003) to word-align parts of the Eu-
roparl corpus (Koehn, 2002) for English and all
other 10 languages. For the experiments we re-
port in this paper, we only used the 1999 debates,
with the language pairs of English combined with
Finnish, French, German, Greek, Italian, Spanish,
and Swedish.
For computing the weight factors we used a two-
step process implemented in Perl, which first de-
termines the maximal -block boundaries (by de-
tecting discontinuities in the sequence of the -
projected words). Words with fertility whose -
correspondents were non-adjacent (modulo NULL-
projections) were treated like zero fertility words,
i.e., we viewed them as unreliable indicators of
block status (compare figure 4). (7) shows the in-
ternal representation of the block structure for (6)
(compare figure 3). L and R are used for the begin-
ning and end of blocks, when the adjacent boundary
zone is empty; l and r are used next to non-empty
boundary zones. Words that have correspondents in
9In the simplest model, we use the factor 0 for spans sat-
isfying the distituent condition underlying hypothesis (4), and
factor 1 for all other spans; in other words, parses involving a
distituent are cancelled out. We also experimented with various
levels of weight factors: for instance, distituents were assigned
factor 0.01, likely distituents factor 0.1, neutral spans 1, and
likely constituents factor 2. Likely constituents are defined as
spans for which one end is adjacent to an empty block bound-
ary zone (i.e., there is no zero fertility word in the block bound-
ary zone which could be the actual boundary of constituents in
which the block is involved).
Most variations in the weighting scheme did not have a sig-
nificant effect, but they caused differences in coverage because
rules with a probability below a certain threshold were dropped
in training. Below, we report the results of the 0.01–0.1–1–2
scheme, which had a reasonably high coverage on the test data.
the normal sequence are encoded as *, zero fertil-
ity words as -; A and B are used for the first block
in a sentence instead of L and R, unless it arises
from “relocation”, which increases likelihood for
constituent status (likewise for the last block: Y and
Z). Since we are interested only in first-order blocks
here, the compact string-based representation is suf-
ficient.
</bodyText>
<listItem confidence="0.821337">
(6) la parole est à m. graefe zu baring-
dorf pour motiver la demande
</listItem>
<equation confidence="0.991335666666667">
NULL ({ 3 4 11 }) mr ({ 5 }) graefe
({ 6 }) zu ({ 7 }) baringdorf ({ 8 })
has ({ }) the ({ 1 }) floor ({ 2 })
to ({ 9 }) explain ({ 10 }) this ({ })
request ({ 12 })
(7) [L**r-lRY*-*Z]
</equation>
<bodyText confidence="0.9999735">
The second step for computing the weight fac-
tors creates a chart of all string spans over the given
sentence and marks for each span whether it is a
distituent, possible constituent or likely distituent,
based on the location of boundary symbols. (For
instance zu Baringdorf has the is marked as a dis-
tituent; the floor and has the floor are marked as
likely constituents.) The tests are implemented as
simple regular expressions. The chart of weight fac-
tors is represented as an array which is stored in the
training corpus file along with the sentences. We
combine the weight factors from various languages,
since each of them may contribute distinct (non-
)constituent information. The inside-outside algo-
rithm reads in the weight factor array and uses it in
the computation of expected rule counts.
We used the probability of the statistical word
alignment as a confidence measure to filter out un-
reliable training sentences. Due to the conservative
nature of the information we extract from the align-
ment, the results indicate however that filtering is
not necessary.
</bodyText>
<sectionHeader confidence="0.996602" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9771005">
For evaluation, we ran the PCFG resulting from
training with the Viterbi algorithm10 on parts of the
Wall Street Journal (WSJ) section of the Penn Tree-
bank and compared the tree structure for the most
</bodyText>
<footnote confidence="0.937421">
10We used the LoPar parser (Schmid, 2000) for this.
</footnote>
<table confidence="0.998484142857143">
System Unlab. Prec. Unlab. Recall F -Score Crossing Brack.
Left-branching 30.4 35.8 32.9 3.06
Right-branching 36.2 42.6 39.2 2.48
Standard PCFG induction 42.4 64.9 51.3 2.2
PCFG trained with C/D weight 47.8 72.1 57.5 1.7
factors from Europarl corpus
Upper limit 66.08 100.0 79.6 0.0
</table>
<figureCaption confidence="0.985304">
Figure 5: Scores for test sentences from WSJ section 23, up to length 10.
</figureCaption>
<bodyText confidence="0.999988314285714">
probable parse for the test sentences against the
gold standard treebank annotation. (Note that one
does not necessarily expect that an induced gram-
mar will match a treebank annotation, but it may at
least serve as a basis for comparison.) The eval-
uation criteria we apply are unlabeled bracketing
precision and recall (and crossing brackets). We
follow an evaluation criterion that (Klein and Man-
ning, 2002, footnote 3) discuss for the evaluation of
a not fully supervised grammar induction approach
based on a binary grammar topology: bracket multi-
plicity (i.e., non-branching projections) is collapsed
into a single set of brackets (since what is rele-
vant is the constituent structure that was induced).11
For comparison, we provide baseline results that
a uniform left-branching structure and a uniform
right-branching structure (which encodes some non-
trivial information about English syntax) would give
rise to. As an upper boundary for the performance a
binary grammar can achieve on the WSJ, we present
the scores for a minimal binarized extension of the
gold-standard annotation.
The results we can report at this point are based
on a comparatively small training set.12 So, it may
be too early for conclusive results. (An issue that
arises with the small training set is that smoothing
techniques would be required to avoid overtraining,
but these tend to dominate the test application, so
the effect of the parallel-corpus based information
cannot be seen so clearly.) But we think that the
results are rather encouraging.
As the table in figure 5 shows, the PCFG we in-
duced based on the parallel-text derived weight fac-
tors reaches 57.5 as the F -score of unlabeled preci-
sion and recall on sentences up to length 10.13 We
</bodyText>
<footnote confidence="0.976118">
11Note that we removed null elements from the WSJ, but we
left punctuation in place. We used the EVALB program for ob-
taining the measures, however we preprocessed the bracketings
to reflect the criteria we discuss here.
12This is not due to scalability issues of the system; we ex-
pect to be able to run experiments on rather large training sets.
Since no manual annotation is required, the available resources
are practically indefinite.
13For sentences up to length 30, the F -score drops to 28.7
</footnote>
<bodyText confidence="0.9999255">
show the scores for an experiment without smooth-
ing, trained on c. 3,000 sentences. Since no smooth-
ing was applied, the resulting coverage (with low-
probability rules removed) on the test set is about
80%. It took 74 iterations of the inside-outside al-
gorithm to train the weight-factor-trained grammar;
the final version has 1005 rules.
For comparison we induced another PCFG based
on the same X-bar topology without using the
weight factor mechanism. This grammar ended up
with 1145 rules after 115 iterations. The F -score is
only 51.3 (while the coverage is the same as for the
weight-factor-trained grammar).
Figure 6 shows the complete set of (singular)
“NP rules” emerging from the weight-factor-trained
grammar, which are remarkably well-behaved, in
particular when we compare them to the corre-
sponding rules from the PCFG induced in the stan-
dard way (figure 7). (XP categories are written
as POS-TAG -P, X head categories are written as
POS-TAG -0 – so the most probable NP produc-
tions in figure 6 are NP N PP, NP N, NP
ADJP N, NP NP PP, NP N PropNP.)
Of course we are comparing an unsupervised
technique with a mildly supervised technique; but
the results indicate that the relatively subtle infor-
mation discussed in section 2 seems to be indeed
very useful.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.998167833333333">
This paper presented a novel approach of using par-
allel corpora as the only resource in the creation of
a monolingual analysis tools. We believe that in or-
der to induce high-quality tools based on statistical
word alignment, the training approach for the target
language tool has to be able to exploit islands of re-
liable information in a stream of potentially rather
noisy data. We experimented with an initial idea
to address this task, which is conceptually simple
and can be implemented building on existing tech-
nology: using the notion of word blocks projected
(as compared to 23.5 for the standard PCFG).
</bodyText>
<table confidence="0.999283081081081">
0.300467 NN-P --&gt; NN-0 IN-P
0.25727 NN-P --&gt; NN-0
0.222335 NN-P --&gt; JJ-P NN-0
0.0612312 NN-P --&gt; NN-P IN-P
0.0462079 NN-P --&gt; NN-0 NP-P
0.0216048 NN-P --&gt; NN-0 ,-P
0.0173518 NN-P --&gt; NN-P NN-0
0.0114746 NN-P --&gt; NN-0 NNS-P
0.00975112 NN-P --&gt; NN-0 MD-P
0.00719605 NN-P --&gt; NN-0 VBZ-P
0.00556762 NN-P --&gt; NN-0 NN-P
0.00511326 NN-P --&gt; NN-0 VVD-P
0.00438077 NN-P --&gt; NN-P VBD-P
0.00423814 NN-P --&gt; NN-P ,-P
0.00409675 NN-P --&gt; NN-0 CD-P
0.00286634 NN-P --&gt; NN-0 VHZ-P
0.00258022 NN-P --&gt; VVG-P NN-0
0.0018237 NN-P --&gt; NN-0 TO-P
0.00162601 NN-P --&gt; NN-P VVN-P
0.00157752 NN-P --&gt; NN-P VB-P
0.00125101 NN-P --&gt; NN-0 VVN-P
0.00106749 NN-P --&gt; NN-P VBZ-P
0.00105866 NN-P --&gt; NN-0 VBD-P
0.000975359 NN-P --&gt; VVN-P NN-0
0.000957702 NN-P --&gt; NN-0 SENT-P
0.000931056 NN-P --&gt; NN-0 CC-P
0.000902116 NN-P --&gt; NN-P SENT-P
0.000717542 NN-P --&gt; NN-0 VBP-P
0.000620843 NN-P --&gt; RB-P NN-0
0.00059608 NN-P --&gt; NN-0 WP-P
0.000550255 NN-P --&gt; NN-0 PDT-P
0.000539155 NN-P --&gt; NN-P CC-P
0.000341498 NN-P --&gt; WP$-P NN-0
0.000330967 NN-P --&gt; WRB-P NN-0
0.000186441 NN-P --&gt; ,-P NN-0
0.000135449 NN-P --&gt; CD-P NN-0
7.16819e-05 NN-P --&gt; NN-0 POS-P
</table>
<figureCaption confidence="0.9909235">
Figure 6: Full set of rules based on the NN tag in
the C/D-trained PCFG
</figureCaption>
<bodyText confidence="0.9996385">
by word alignment as an indication for (mainly) im-
possible string spans. Applying this information in
order to impose weighting factors on the EM algo-
rithm for PCFG induction gives us a first, simple
instance of the “island-exploiting” system we think
is needed. More sophisticated models may make
use some of the experience gathered in these exper-
iments.
The conservative way in which cross-linguistic
relations between phrase structure is exploited has
the advantage that we don’t have to make unwar-
ranted assumptions about direct correspondences
among the majority of constituent spans, or even
direct correspondences of phrasal categories. The
technique is particularly well-suited for the ex-
ploitation of parallel corpora involving multiple lan-
</bodyText>
<table confidence="0.998295727272727">
0.429157 NN-P --&gt; DT-P NN-0
0.0816385 NN-P --&gt; IN-P NN-0
0.0630426 NN-P --&gt; NN-0
0.0489261 NN-P --&gt; PP$-P NN-0
0.0487434 NN-P --&gt; JJ-P NN-0
0.0451819 NN-P --&gt; NN-P ,-P
0.0389741 NN-P --&gt; NN-P VBZ-P
0.0330732 NN-P --&gt; NN-P NN-0
0.0215872 NN-P --&gt; NN-P MD-P
0.0201612 NN-P --&gt; NN-P TO-P
0.0199536 NN-P --&gt; CC-P NN-0
0.015509 NN-P --&gt; NN-P VVZ-P
0.0112734 NN-P --&gt; NN-P RB-P
0.00977683 NN-P --&gt; NP-P NN-0
0.00943218 NN-P --&gt; CD-P NN-0
0.00922132 NN-P --&gt; NN-P WDT-P
0.00896826 NN-P --&gt; POS-P NN-0
0.00749452 NN-P --&gt; NN-P VHZ-P
0.00621328 NN-P --&gt; NN-0 ,-P
0.00520734 NN-P --&gt; NN-P VBD-P
0.004674 NN-P --&gt; JJR-P NN-0
0.00407644 NN-P --&gt; NN-P VVD-P
0.00394681 NN-P --&gt; NN-P VVN-P
0.00354741 NN-P --&gt; NN-0 MD-P
0.00335451 NN-P --&gt; NN-0 NN-P
0.0030748 NN-P --&gt; EX-P NN-0
0.0026483 NN-P --&gt; WRB-P NN-0
0.00262025 NN-P --&gt; NN-0 TO-P
[...]
0.000403279 NN-P --&gt; NN-0 VBP-P
0.000378414 NN-P --&gt; NN-0 PDT-P
0.000318026 NN-P --&gt; NN-0 VHZ-P
2.27821e-05 NN-P --&gt; NN-P PP-P
</table>
<figureCaption confidence="0.909876">
Figure 7: Standard induced PCFG: Excerpt of rules
based on the NN tag
</figureCaption>
<bodyText confidence="0.999817933333333">
guages like the Europarl corpus. Note that nothing
in our methodology made any language particular
assumptions; future research has to show whether
there are language pairs that are particularly effec-
tive, but in general the technique should be applica-
ble for whatever parallel corpus is at hand.
A number of studies are related to the work we
presented, most specifically work on parallel-text
based “information projection” for parsing (Hwa et
al., 2002), but also grammar induction work based
on constituent/distituent information (Klein and
Manning, 2002) and (language-internal) alignment-
based learning (van Zaanen, 2000). However to our
knowledge the specific way of bringing these as-
pects together is new.
</bodyText>
<sectionHeader confidence="0.996397" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988647479166667">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-
Josef Och, David Purdy, Noah A. Smith, and
David Yarowsky. 1999. Statistical machine
translation. Final report, JHU Workshop.
Michael Collins. 1999. A statistical parser for
Czech. In Proceedings ofACL.
Rebecca Hwa, Philip Resnik, and Amy Weinberg.
2002. Breaking the resource bottleneck for mul-
tilingual parsing. In Proceedings ofLREC.
Dan Klein and Christopher Manning. 2002. A gen-
erative constituent-context model for improved
grammar induction. In Proceedings ofACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the Human Language Technology
Conference 2003 (HLT-NAACL 2003), Edmon-
ton, Canada.
Philipp Koehn. 2002. Europarl: A multilingual cor-
pus for evaluation of machine translation. Ms.,
University of Southern California.
Kamal Nigam, Andrew Kachites McCallum, Se-
bastian Thrun, and Tom M. Mitchell. 2000.
Text classification from labeled and unlabeled
documents using EM. Machine Learning,
39(2/3):103–134.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1):19–51.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International
Conference on New Methods in Language Pro-
cessing, Manchester, UK.
Helmut Schmid. 2000. Lopar: Design and
implementation. Arbeitspapiere des Sonder-
forschungsbereiches 340, No. 149, IMS Stuttgart.
Menno van Zaanen. 2000. ABL: Alignment-based
learning. In COLING 2000 - Proceedings of the
18th International Conference on Computational
Linguistics, pages 961–967.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377–403.
David Yarowsky and Grace Ngai. 2001. Inducing
multilingual POS taggers and NP bracketers via
robust projection across aligned corpora. In Pro-
ceedings ofNAACL.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.633995">
<title confidence="0.999716">Experiments in Parallel-Text Based Grammar Induction</title>
<author confidence="0.999996">Jonas Kuhn</author>
<affiliation confidence="0.9999295">Department of Linguistics The University of Texas at Austin</affiliation>
<address confidence="0.983203">Austin, TX 78712</address>
<email confidence="0.999766">jonak@mail.utexas.edu</email>
<abstract confidence="0.969644416666667">This paper discusses the use of statistical word alignment over multiple parallel texts for the identification of string spans that cannot be constituents in one of the languages. This information is exploited in monolingual PCFG grammar induction for that language, within an augmented version of the inside-outside algorithm. Besides the aligned corpus, no other resources are required. We discuss an implemented system and present experimental results with an evaluation against the Penn Treebank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>FranzJosef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation.</title>
<date>1999</date>
<tech>Final report, JHU Workshop.</tech>
<contexts>
<context position="3386" citStr="Al-Onaizan et al., 1999" startWordPosition="528" endWordPosition="531">e standard treebank training with an Expectation-Maximization (EM) algorithm for PCFGs, augmented by weighting factors for the reliability of training data, following the approach of (Nigam et al., 2000), who apply it for EM training of a text classifier. The factors are only sensitive to the constituent/distituent (C/D) status of each span of the string in (cp. (Klein and Manning, 2002)). The C/D status is derived from an aligned parallel corpus in a way discussed in section 2. We use the Europarl corpus (Koehn, 2002), and the statistical word alignment was performed with the GIZA++ toolkit (Al-Onaizan et al., 1999; Och and Ney, 2003).1 For the current experiments we assume no preexisting parser for any of the languages, contrary to the information projection scenario. While better absolute results could be expected using one or more parsers for the languages involved, we think that it is important to isolate the usefulness of exploiting just crosslinguistic word order divergences in order to obtain partial prior knowledge about the constituent structure of a language, which is then exploited in an EM learning approach (section 3). Not using a parser for some languages also makes it possible to compare </context>
<context position="17160" citStr="Al-Onaizan et al., 1999" startWordPosition="2875" endWordPosition="2878">nt . je ne vous donne pas la parole . Figure 4: Alignment example with higher-fertility words in English expectations that the inside-outside algorithm determines based on its current rule parameters. Note that the this prior information is information about string spans of (non-)constituents – it does not tell us anything about the categories of the potential constituents affected. It is combined with the PCFG expectations as the chart is constructed. For each span in the chart, we get a weight factor that is multiplied with the parameter-based expectations.9 4 Experiments We applied GIZA++ (Al-Onaizan et al., 1999; Och and Ney, 2003) to word-align parts of the Europarl corpus (Koehn, 2002) for English and all other 10 languages. For the experiments we report in this paper, we only used the 1999 debates, with the language pairs of English combined with Finnish, French, German, Greek, Italian, Spanish, and Swedish. For computing the weight factors we used a twostep process implemented in Perl, which first determines the maximal -block boundaries (by detecting discontinuities in the sequence of the - projected words). Words with fertility whose - correspondents were non-adjacent (modulo NULLprojections) w</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, FranzJosef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation. Final report, JHU Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A statistical parser for Czech.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2170" citStr="Collins, 1999" startWordPosition="325" endWordPosition="326">is method are very encouraging. Will the information projection approach also work for less shallow analysis tools, in particular full syntactic parsers? An obvious issue is that one does not expect the phrase structure representation of English (as produced by state-of-the-art treebank parsers) to carry over to less configurational languages. Therefore, (Hwa et al., 2002) extract a more language-independent dependency structure from the English parse as the basis for projection to Chinese. From the resulting (noisy) dependency treebank, a dependency parser is trained using the techniques of (Collins, 1999). (Hwa et al., 2002) report that the noise in the projected treebank is still a major challenge, suggesting that a future research focus should be on the filtering of (parts of) unreliable trees and statistical word alignment models sensitive to the syntactic projection framework. Our hypothesis is that the quality of the resulting parser/grammar for language can be significantly improved if the training method for the parser is changed to accomodate for training data which are in part unreliable. The experiments we report in this paper focus on a specific part of the problem: we replace stand</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. A statistical parser for Czech. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
</authors>
<title>Breaking the resource bottleneck for multilingual parsing.</title>
<date>2002</date>
<booktitle>In Proceedings ofLREC.</booktitle>
<contexts>
<context position="1931" citStr="Hwa et al., 2002" startWordPosition="289" endWordPosition="292">s tool for , using the annotations projected with high confidence as the initial training data. (Confidence of both the English analysis tool and the statistical word alignment is taken into account.) The results that have been achieved by this method are very encouraging. Will the information projection approach also work for less shallow analysis tools, in particular full syntactic parsers? An obvious issue is that one does not expect the phrase structure representation of English (as produced by state-of-the-art treebank parsers) to carry over to less configurational languages. Therefore, (Hwa et al., 2002) extract a more language-independent dependency structure from the English parse as the basis for projection to Chinese. From the resulting (noisy) dependency treebank, a dependency parser is trained using the techniques of (Collins, 1999). (Hwa et al., 2002) report that the noise in the projected treebank is still a major challenge, suggesting that a future research focus should be on the filtering of (parts of) unreliable trees and statistical word alignment models sensitive to the syntactic projection framework. Our hypothesis is that the quality of the resulting parser/grammar for language</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, 2002</marker>
<rawString>Rebecca Hwa, Philip Resnik, and Amy Weinberg. 2002. Breaking the resource bottleneck for multilingual parsing. In Proceedings ofLREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="3153" citStr="Klein and Manning, 2002" startWordPosition="487" endWordPosition="491">ge can be significantly improved if the training method for the parser is changed to accomodate for training data which are in part unreliable. The experiments we report in this paper focus on a specific part of the problem: we replace standard treebank training with an Expectation-Maximization (EM) algorithm for PCFGs, augmented by weighting factors for the reliability of training data, following the approach of (Nigam et al., 2000), who apply it for EM training of a text classifier. The factors are only sensitive to the constituent/distituent (C/D) status of each span of the string in (cp. (Klein and Manning, 2002)). The C/D status is derived from an aligned parallel corpus in a way discussed in section 2. We use the Europarl corpus (Koehn, 2002), and the statistical word alignment was performed with the GIZA++ toolkit (Al-Onaizan et al., 1999; Och and Ney, 2003).1 For the current experiments we assume no preexisting parser for any of the languages, contrary to the information projection scenario. While better absolute results could be expected using one or more parsers for the languages involved, we think that it is important to isolate the usefulness of exploiting just crosslinguistic word order diver</context>
<context position="21855" citStr="Klein and Manning, 2002" startWordPosition="3678" endWordPosition="3682">FG induction 42.4 64.9 51.3 2.2 PCFG trained with C/D weight 47.8 72.1 57.5 1.7 factors from Europarl corpus Upper limit 66.08 100.0 79.6 0.0 Figure 5: Scores for test sentences from WSJ section 23, up to length 10. probable parse for the test sentences against the gold standard treebank annotation. (Note that one does not necessarily expect that an induced grammar will match a treebank annotation, but it may at least serve as a basis for comparison.) The evaluation criteria we apply are unlabeled bracketing precision and recall (and crossing brackets). We follow an evaluation criterion that (Klein and Manning, 2002, footnote 3) discuss for the evaluation of a not fully supervised grammar induction approach based on a binary grammar topology: bracket multiplicity (i.e., non-branching projections) is collapsed into a single set of brackets (since what is relevant is the constituent structure that was induced).11 For comparison, we provide baseline results that a uniform left-branching structure and a uniform right-branching structure (which encodes some nontrivial information about English syntax) would give rise to. As an upper boundary for the performance a binary grammar can achieve on the WSJ, we pres</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher Manning. 2002. A generative constituent-context model for improved grammar induction. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="6766" citStr="Koehn et al., 2003" startWordPosition="1129" endWordPosition="1132">the overt words. We now define an -induced block (or -block for short) as a substring of a sentence in , such that the union over all -images ( or are -induced blocks. Let us define a maximal -block as an -block , such that adding at the beginning or at the end is either (i) impossible (because it would lead to a non-block, or or do not exist as we are at the beginning or end of the string), or (ii) it would introduce a new crossing alignment 2The block notion we are defining in this section is indirectly related to the concept of a “phrase” in recent work in Statistical Machine Translation. (Koehn et al., 2003) show that exploiting all contiguous word blocks in phrase-based alignment is better than focusing on syntactic constituents only. In our context, we are interested in inducing syntactic constituents based on alignment information; given the observations from Statistical MT, it does not come as a surprise that there is no direct link from blocks to constituents. Our work can be seen as an attempt to zero in on the distinction between the concepts; we find that it is most useful to keep track of the boundaries between blocks. (Wu, 1997) also includes a brief discussion of crossing constraints t</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology Conference 2003 (HLT-NAACL 2003), Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A multilingual corpus for evaluation of machine translation.</title>
<date>2002</date>
<institution>Ms., University of Southern California.</institution>
<contexts>
<context position="3287" citStr="Koehn, 2002" startWordPosition="515" endWordPosition="516">e experiments we report in this paper focus on a specific part of the problem: we replace standard treebank training with an Expectation-Maximization (EM) algorithm for PCFGs, augmented by weighting factors for the reliability of training data, following the approach of (Nigam et al., 2000), who apply it for EM training of a text classifier. The factors are only sensitive to the constituent/distituent (C/D) status of each span of the string in (cp. (Klein and Manning, 2002)). The C/D status is derived from an aligned parallel corpus in a way discussed in section 2. We use the Europarl corpus (Koehn, 2002), and the statistical word alignment was performed with the GIZA++ toolkit (Al-Onaizan et al., 1999; Och and Ney, 2003).1 For the current experiments we assume no preexisting parser for any of the languages, contrary to the information projection scenario. While better absolute results could be expected using one or more parsers for the languages involved, we think that it is important to isolate the usefulness of exploiting just crosslinguistic word order divergences in order to obtain partial prior knowledge about the constituent structure of a language, which is then exploited in an EM lear</context>
<context position="17237" citStr="Koehn, 2002" startWordPosition="2891" endWordPosition="2892">words in English expectations that the inside-outside algorithm determines based on its current rule parameters. Note that the this prior information is information about string spans of (non-)constituents – it does not tell us anything about the categories of the potential constituents affected. It is combined with the PCFG expectations as the chart is constructed. For each span in the chart, we get a weight factor that is multiplied with the parameter-based expectations.9 4 Experiments We applied GIZA++ (Al-Onaizan et al., 1999; Och and Ney, 2003) to word-align parts of the Europarl corpus (Koehn, 2002) for English and all other 10 languages. For the experiments we report in this paper, we only used the 1999 debates, with the language pairs of English combined with Finnish, French, German, Greek, Italian, Spanish, and Swedish. For computing the weight factors we used a twostep process implemented in Perl, which first determines the maximal -block boundaries (by detecting discontinuities in the sequence of the - projected words). Words with fertility whose - correspondents were non-adjacent (modulo NULLprojections) were treated like zero fertility words, i.e., we viewed them as unreliable ind</context>
</contexts>
<marker>Koehn, 2002</marker>
<rawString>Philipp Koehn. 2002. Europarl: A multilingual corpus for evaluation of machine translation. Ms., University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew Kachites McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom M Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="2966" citStr="Nigam et al., 2000" startWordPosition="453" endWordPosition="456"> unreliable trees and statistical word alignment models sensitive to the syntactic projection framework. Our hypothesis is that the quality of the resulting parser/grammar for language can be significantly improved if the training method for the parser is changed to accomodate for training data which are in part unreliable. The experiments we report in this paper focus on a specific part of the problem: we replace standard treebank training with an Expectation-Maximization (EM) algorithm for PCFGs, augmented by weighting factors for the reliability of training data, following the approach of (Nigam et al., 2000), who apply it for EM training of a text classifier. The factors are only sensitive to the constituent/distituent (C/D) status of each span of the string in (cp. (Klein and Manning, 2002)). The C/D status is derived from an aligned parallel corpus in a way discussed in section 2. We use the Europarl corpus (Koehn, 2002), and the statistical word alignment was performed with the GIZA++ toolkit (Al-Onaizan et al., 1999; Och and Ney, 2003).1 For the current experiments we assume no preexisting parser for any of the languages, contrary to the information projection scenario. While better absolute </context>
<context position="14099" citStr="Nigam et al., 2000" startWordPosition="2368" endWordPosition="2371">ake use of this scattered (non-) constituency information, a semi-supervised approach is needed that can fill in the (potentially large) areas for which no prior information is available. For the present experiments we decided to choose a conceptually simple such approach, with which we can build on substantial existing work in grammar induction: we construe the learning problem as PCFG induction, using the inside-outside algorithm, with the addition of weighting factors based on the (non)constituency information. This use of weighting factors in EM learning follows the approach discussed in (Nigam et al., 2000). Since we are mainly interested in comparative experiments at this stage, the conceptual simplicity, and the availability of efficient implemented open7Since zero-fertility words are often function words, there is probably a rightward-tendency that one might be able to exploit; however in the present study we didn’t want to build such high-level linguistic assumptions into the system. source systems of a PCFG induction approach outweighs the disadvantage of potentially poorer overall performance than one might expect from some other approaches. The PCFG topology we use is a binary, entirely u</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom M. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3406" citStr="Och and Ney, 2003" startWordPosition="532" endWordPosition="535">ing with an Expectation-Maximization (EM) algorithm for PCFGs, augmented by weighting factors for the reliability of training data, following the approach of (Nigam et al., 2000), who apply it for EM training of a text classifier. The factors are only sensitive to the constituent/distituent (C/D) status of each span of the string in (cp. (Klein and Manning, 2002)). The C/D status is derived from an aligned parallel corpus in a way discussed in section 2. We use the Europarl corpus (Koehn, 2002), and the statistical word alignment was performed with the GIZA++ toolkit (Al-Onaizan et al., 1999; Och and Ney, 2003).1 For the current experiments we assume no preexisting parser for any of the languages, contrary to the information projection scenario. While better absolute results could be expected using one or more parsers for the languages involved, we think that it is important to isolate the usefulness of exploiting just crosslinguistic word order divergences in order to obtain partial prior knowledge about the constituent structure of a language, which is then exploited in an EM learning approach (section 3). Not using a parser for some languages also makes it possible to compare various language pai</context>
<context position="17180" citStr="Och and Ney, 2003" startWordPosition="2879" endWordPosition="2882"> la parole . Figure 4: Alignment example with higher-fertility words in English expectations that the inside-outside algorithm determines based on its current rule parameters. Note that the this prior information is information about string spans of (non-)constituents – it does not tell us anything about the categories of the potential constituents affected. It is combined with the PCFG expectations as the chart is constructed. For each span in the chart, we get a weight factor that is multiplied with the parameter-based expectations.9 4 Experiments We applied GIZA++ (Al-Onaizan et al., 1999; Och and Ney, 2003) to word-align parts of the Europarl corpus (Koehn, 2002) for English and all other 10 languages. For the experiments we report in this paper, we only used the 1999 debates, with the language pairs of English combined with Finnish, French, German, Greek, Italian, Spanish, and Swedish. For computing the weight factors we used a twostep process implemented in Perl, which first determines the maximal -block boundaries (by detecting discontinuities in the sequence of the - projected words). Words with fertility whose - correspondents were non-adjacent (modulo NULLprojections) were treated like zer</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="14817" citStr="Schmid, 1994" startWordPosition="2481" endWordPosition="2482"> the availability of efficient implemented open7Since zero-fertility words are often function words, there is probably a rightward-tendency that one might be able to exploit; however in the present study we didn’t want to build such high-level linguistic assumptions into the system. source systems of a PCFG induction approach outweighs the disadvantage of potentially poorer overall performance than one might expect from some other approaches. The PCFG topology we use is a binary, entirely unrestricted X-bar-style grammar based on the Penn Treebank POS-tagset (expanded as in the TreeTagger by (Schmid, 1994)). All possible combinations of projections of POS-categories X and Y are included following the schemata in (5). This gives rise to 13,110 rules. (5) a. XP X b. XP XP YP c. XP YP XP d. XP YP X e. XP X YP We tagged the English version of our training section of the Europarl corpus with the TreeTagger and used the strings of POS-tags as the training corpus for the inside-outside algorithm; however, it is straightforward to apply our approach to a language for which no taggers are available if an unsupervised word clustering technique is applied first. We based our EM training algorithm on Mark </context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In International Conference on New Methods in Language Processing, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Lopar: Design and implementation.</title>
<date>2000</date>
<booktitle>Arbeitspapiere des Sonderforschungsbereiches 340, No. 149, IMS</booktitle>
<location>Stuttgart.</location>
<contexts>
<context position="21080" citStr="Schmid, 2000" startWordPosition="3554" endWordPosition="3555">eads in the weight factor array and uses it in the computation of expected rule counts. We used the probability of the statistical word alignment as a confidence measure to filter out unreliable training sentences. Due to the conservative nature of the information we extract from the alignment, the results indicate however that filtering is not necessary. 5 Evaluation For evaluation, we ran the PCFG resulting from training with the Viterbi algorithm10 on parts of the Wall Street Journal (WSJ) section of the Penn Treebank and compared the tree structure for the most 10We used the LoPar parser (Schmid, 2000) for this. System Unlab. Prec. Unlab. Recall F -Score Crossing Brack. Left-branching 30.4 35.8 32.9 3.06 Right-branching 36.2 42.6 39.2 2.48 Standard PCFG induction 42.4 64.9 51.3 2.2 PCFG trained with C/D weight 47.8 72.1 57.5 1.7 factors from Europarl corpus Upper limit 66.08 100.0 79.6 0.0 Figure 5: Scores for test sentences from WSJ section 23, up to length 10. probable parse for the test sentences against the gold standard treebank annotation. (Note that one does not necessarily expect that an induced grammar will match a treebank annotation, but it may at least serve as a basis for compa</context>
</contexts>
<marker>Schmid, 2000</marker>
<rawString>Helmut Schmid. 2000. Lopar: Design and implementation. Arbeitspapiere des Sonderforschungsbereiches 340, No. 149, IMS Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Menno van Zaanen</author>
</authors>
<title>ABL: Alignment-based learning.</title>
<date>2000</date>
<booktitle>In COLING 2000 - Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>961--967</pages>
<marker>van Zaanen, 2000</marker>
<rawString>Menno van Zaanen. 2000. ABL: Alignment-based learning. In COLING 2000 - Proceedings of the 18th International Conference on Computational Linguistics, pages 961–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="7307" citStr="Wu, 1997" startWordPosition="1222" endWordPosition="1223">recent work in Statistical Machine Translation. (Koehn et al., 2003) show that exploiting all contiguous word blocks in phrase-based alignment is better than focusing on syntactic constituents only. In our context, we are interested in inducing syntactic constituents based on alignment information; given the observations from Statistical MT, it does not come as a surprise that there is no direct link from blocks to constituents. Our work can be seen as an attempt to zero in on the distinction between the concepts; we find that it is most useful to keep track of the boundaries between blocks. (Wu, 1997) also includes a brief discussion of crossing constraints that can be derived from phrase structure correspondences. ) forms a contiguous substring in ,modulo the words from NULL . For example, in (1) (or (2)) is not an -block since the union over its -images is which do not form a contiguous string in . The sequences to the block.3 String in (1) is not a maximal -block, because is an -block; but is maximal since is the final word of the sentence and is a non-block. We can now make the initial observation precise that (1) and (2) have the same block structure, but the constituent structures ar</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In</title>
<date>2001</date>
<booktitle>Proceedings ofNAACL.</booktitle>
<contexts>
<context position="880" citStr="Yarowsky and Ngai, 2001" startWordPosition="125" endWordPosition="128">xts for the identification of string spans that cannot be constituents in one of the languages. This information is exploited in monolingual PCFG grammar induction for that language, within an augmented version of the inside-outside algorithm. Besides the aligned corpus, no other resources are required. We discuss an implemented system and present experimental results with an evaluation against the Penn Treebank. 1 Introduction There have been a number of recent studies exploiting parallel corpora in bootstrapping of monolingual analysis tools. In the “information projection” approach (e.g., (Yarowsky and Ngai, 2001)), statistical word alignment is applied to a parallel corpus of English and some other language for which no tagger/morphological analyzer/chunker etc. (henceforth simply: analysis tool) exists. A high-quality analysis tool is applied to the English text, and the statistical word alignment is used to project a (noisy) target annotation to the version of the text. Robust learning techniques are then applied to bootstrap an analysis tool for , using the annotations projected with high confidence as the initial training data. (Confidence of both the English analysis tool and the statistical word</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In Proceedings ofNAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>