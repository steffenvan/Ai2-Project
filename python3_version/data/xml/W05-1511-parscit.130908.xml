<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.771758333333333">
Efficacy of Beam Thresholding, Unification Filtering and Hybrid
Parsing in Probabilistic HPSG Parsing
Takashi Ninomiya
</title>
<author confidence="0.371476">
CREST, JST
</author>
<affiliation confidence="0.808087666666667">
and
Department of Computer Science
The University of Tokyo
</affiliation>
<email confidence="0.997394">
ninomi@is.s.u-tokyo.ac.jp
</email>
<author confidence="0.998345">
Yusuke Miyao
</author>
<affiliation confidence="0.9996085">
Department of Computer Science
The University of Tokyo
</affiliation>
<email confidence="0.993931">
yusuke@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.992972" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945111111111">
We investigated the performance efficacy
of beam search parsing and deep parsing
techniques in probabilistic HPSG parsing
using the Penn treebank. We first tested
the beam thresholding and iterative pars-
ing developed for PCFG parsing with an
HPSG. Next, we tested three techniques
originally developed for deep parsing: quick
check, large constituent inhibition, and hy-
brid parsing with a CFG chunk parser. The
contributions of the large constituent inhi-
bition and global thresholding were not sig-
nificant, while the quick check and chunk
parser greatly contributed to total parsing
performance. The precision, recall and av-
erage parsing time for the Penn treebank
(Section 23) were 87.85%, 86.85%, and 360
ms, respectively.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99982025">
We investigated the performance efficacy of beam
search parsing and deep parsing techniques in
probabilistic head-driven phrase structure grammar
(HPSG) parsing for the Penn treebank. We first
applied beam thresholding techniques developed for
CFG parsing to HPSG parsing, including local
thresholding, global thresholding (Goodman, 1997),
and iterative parsing (Tsuruoka and Tsujii, 2005b).
</bodyText>
<note confidence="0.835879">
Yoshimasa Tsuruoka
CREST, JST
and
</note>
<affiliation confidence="0.9979455">
Department of Computer Science
The University of Tokyo
</affiliation>
<email confidence="0.993311">
tsuruoka@is.s.u-tokyo.ac.jp
</email>
<author confidence="0.923392">
Jun’ichi Tsujii
</author>
<affiliation confidence="0.867640833333333">
Department of Computer Science
The University of Tokyo
and
School of Informatics
University of Manchester
and
</affiliation>
<address confidence="0.403224">
CREST, JST
</address>
<email confidence="0.838485">
tsujii@is.s.u-tokyo.ac.jp
</email>
<bodyText confidence="0.999968666666667">
Next, we applied parsing techniques developed for
deep parsing, including quick check (Malouf et al.,
2000), large constituent inhibition (Kaplan et al.,
2004) and hybrid parsing with a CFG chunk parser
(Daum et al., 2003; Frank et al., 2003; Frank, 2004).
The experiments showed how each technique con-
tributes to the final output of parsing in terms of
precision, recall, and speed for the Penn treebank.
Unification-based grammars have been extensively
studied in terms of linguistic formulation and com-
putation efficiency. Although they provide precise
linguistic structures of sentences, their processing is
considered expensive because of the detailed descrip-
tions. Since efficiency is of particular concern in prac-
tical applications, a number of studies have focused
on improving the parsing efficiency of unification-
based grammars (Oepen et al., 2002). Although sig-
nificant improvements in efficiency have been made,
parsing speed is still not high enough for practical
applications.
The recent introduction of probabilistic models of
wide-coverage unification-based grammars (Malouf
and van Noord, 2004; Kaplan et al., 2004; Miyao
and Tsujii, 2005) has opened up the novel possibil-
ity of increasing parsing speed by guiding the search
path using probabilities. That is, since we often re-
quire only the most probable parse result, we can
compute partial parse results that are likely to con-
tribute to the final parse result. This approach has
been extensively studied in the field of probabilistic
</bodyText>
<page confidence="0.990432">
103
</page>
<note confidence="0.7678265">
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 103–114,
Vancouver, October 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.982379631578947">
CFG (PCFG) parsing, such as Viterbi parsing and
beam thresholding.
While many methods of probabilistic parsing for
unification-based grammars have been developed,
their strategy is to first perform exhaustive pars-
ing without using probabilities and then select the
highest probability parse. The behavior of their al-
gorithms is like that of the Viterbi algorithm for
PCFG parsing, so the correct parse with the high-
est probability is guaranteed. The interesting point
of this approach is that, once the exhaustive pars-
ing is completed, the probabilities of non-local de-
pendencies, which cannot be computed during pars-
ing, are computed after making a packed parse for-
est. Probabilistic models where probabilities are as-
signed to the CFG backbone of the unification-based
grammar have been developed (Kasper et al., 1996;
Briscoe and Carroll, 1993; Kiefer et al., 2002), and
the most probable parse is found by PCFG parsing.
This model is based on PCFG and not probabilis-
tic unification-based grammar parsing. Geman and
Johnson (Geman and Johnson, 2002) proposed a dy-
namic programming algorithm for finding the most
probable parse in a packed parse forest generated by
unification-based grammars without expanding the
forest. However, the efficiency of this algorithm is
inherently limited by the inefficiency of exhaustive
parsing.
In this paper we describe the performance of beam
thresholding, including iterative parsing, in proba-
bilistic HPSG parsing for a large-scale corpora, the
Penn treebank. We show how techniques developed
for efficient deep parsing can improve the efficiency
of probabilistic parsing. These techniques were eval-
uated in experiments on the Penn Treebank (Marcus
et al., 1994) with the wide-coverage HPSG parser de-
veloped by Miyao et al. (Miyao et al., 2005; Miyao
and Tsujii, 2005).
</bodyText>
<sectionHeader confidence="0.813567" genericHeader="introduction">
2 HPSG and probabilistic models
</sectionHeader>
<bodyText confidence="0.999141071428571">
HPSG (Pollard and Sag, 1994) is a syntactic theory
based on lexicalized grammar formalism. In HPSG,
a small number of schemata describe general con-
struction rules, and a large number of lexical en-
tries express word-specific characteristics. The struc-
tures of sentences are explained using combinations
of schemata and lexical entries. Both schemata and
lexical entries are represented by typed feature struc-
tures, and constraints represented by feature struc-
tures are checked with unification.
Figure 1 shows an example of HPSG parsing of
the sentence “Spring has come.” First, each of the
lexical entries for “has&amp;quot; and “come” is unified with a
daughter feature structure of the Head-Complement
</bodyText>
<figureCaption confidence="0.998007">
Figure 1: HPSG parsing
</figureCaption>
<bodyText confidence="0.999731375">
Schema. Unification provides the phrasal sign of
the mother. The sign of the larger constituent is
obtained by repeatedly applying schemata to lexi-
cal/phrasal signs. Finally, the parse result is output
as a phrasal sign that dominates the sentence.
Given set W of words and set J7 of feature struc-
tures, an HPSG is formulated as a tuple, G = (L, R),
where
</bodyText>
<equation confidence="0.806999">
L = {l = (w, F)|w E W, F E J7} is a set of lexical
</equation>
<bodyText confidence="0.959678615384615">
entries, and
R is a set of schemata, i.e., r E R is a partial
function: J7 x J7 → J7.
Given a sentence, an HPSG computes a set of phrasal
signs, i.e., feature structures, as a result of parsing.
Previous studies (Abney, 1997; Johnson et al.,
1999; Riezler et al., 2000; Miyao et al., 2003; Mal-
ouf and van Noord, 2004; Kaplan et al., 2004; Miyao
and Tsujii, 2005) defined a probabilistic model of
unification-based grammars as a log-linear model or
maximum entropy model (Berger et al., 1996). The
probability of parse result T assigned to given sen-
tence w = (wl, ... , wn) is
</bodyText>
<equation confidence="0.9864228">
!
1 X
p(T|w) = exp λifi(T)
Z&amp;quot; i
!λifi(T �) ,
</equation>
<bodyText confidence="0.987443428571429">
Spring
where λi is a model parameter, and fi is a feature
function that represents a characteristic of parse tree
T. Intuitively, the probability is defined as the nor-
malized product of the weights exp(λi) when a char-
acteristic corresponding to fi appears in parse result
T. Model parameters λi are estimated using numer-
</bodyText>
<figure confidence="0.997437212765958">
has
come
HEAD verb
HEAD noun
1 SUBJ &lt; &gt;
COMPS &lt; &gt;
SUBJ &lt;
COMPS &lt; &gt;
has
come
Spring
head-comp
HEAD noun
SUBJ &lt; &gt;
COMPS &lt; &gt;
HEAD verb
SUBJ &lt; &gt;
1
COMPS &lt; &gt;
2
HEAD verb
2 SUBJ &lt; &gt;
1
COMPS &lt; &gt;
HEAD verb
SUBJ &lt; &gt;
COMPS &lt; &gt;
subject-head
HEAD verb
SUBJ &lt; &gt;
1
COMPS &lt; &gt;
head-comp
HEAD noun
HEAD verb
SUBJ &lt; &gt; SUBJ &lt; &gt;
COMPS &lt; &gt; COMPS &lt; 2
HEAD verb
2 SUBJ &lt; &gt;
1
COMPS &lt; &gt;
1&gt;
&gt;
��
1
XZ&amp;quot; = Xexp
T1 i
</figure>
<page confidence="0.997056">
104
</page>
<bodyText confidence="0.999812055555555">
ical optimization methods (Malouf, 2002) so as to
maximize the log-likelihood of the training data.
However, the above model cannot be easily esti-
mated because the estimation requires the computa-
tion of p(T|w) for all parse candidates assigned to
sentence w. Because the number of parse candidates
is exponentially related to the length of the sentence,
the estimation is intractable for long sentences.
To make the model estimation tractable, Ge-
man and Johnson (Geman and Johnson, 2002) and
Miyao and Tsujii (Miyao and Tsujii, 2002) proposed
a dynamic programming algorithm for estimating
p(T|w). They assumed that features are functions
on nodes in a packed parse forest. That is, parse tree
T is represented by a set of nodes, i.e., T = {c}, and
the parse forest is represented by an and/or graph
of the nodes. From this assumption, we can redefine
the probability as
</bodyText>
<equation confidence="0.999723">
p(T|w) = 1Z exp
Z.,,, cET
!Aifi(c) �
</equation>
<bodyText confidence="0.999990095238095">
A packed parse forest has a structure similar to a
chart of CFG parsing, and c corresponds to an edge
in the chart. This assumption corresponds to the
independence assumption in PCFG; that is, only
a nonterminal symbol of a mother is considered in
further processing by ignoring the structure of its
daughters. With this assumption, we can compute
the figures of merit (FOMs) of partial parse results.
This assumption restricts the possibility of feature
functions that represent non-local dependencies ex-
pressed in a parse result. Since unification-based
grammars can express semantic relations, such as
predicate-argument relations, in their structure, the
assumption unjustifiably restricts the flexibility of
probabilistic modeling. However, previous research
(Miyao et al., 2003; Clark and Curran, 2004; Kaplan
et al., 2004) showed that predicate-argument rela-
tions can be represented under the assumption of
feature locality. We thus assumed the locality of fea-
ture functions and exploited it for the efficient search
of probable parse results.
</bodyText>
<sectionHeader confidence="0.9637075" genericHeader="method">
3 Techniques for efficient deep
parsing
</sectionHeader>
<bodyText confidence="0.999023160714286">
Many of the techniques for improving the parsing
efficiency of deep linguistic analysis have been de-
veloped in the framework of lexicalized grammars
such as lexical functional grammar (LFG) (Bresnan,
1982), lexicalized tree adjoining grammar (LTAG)
(Shabes et al., 1988), HPSG (Pollard and Sag, 1994)
or combinatory categorial grammar (CCG) (Steed-
man, 2000). Most of them were developed for ex-
haustive parsing, i.e., producing all parse results that
are given by the grammar (Matsumoto et al., 1983;
Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer
et al., 1999; Malouf et al., 2000; Torisawa et al., 2000;
Oepen et al., 2002; Penn and Munteanu, 2003). The
strategy of exhaustive parsing has been widely used
in grammar development and in parameter training
for probabilistic models.
We tested three of these techniques.
Quick check Quick check filters out non-unifiable
feature structures (Malouf et al., 2000). Sup-
pose we have two non-unifiable feature struc-
tures. They are destructively unified by travers-
ing and modifying them, and then finally they
are found to be not unifiable in the middle of the
unification process. Quick check quickly judges
their unifiability by peeping the values of the
given paths. If one of the path values is not
unifiable, the two feature structures cannot be
unified because of the necessary condition of uni-
fication. In our implementation of quick check,
each edge had two types of arrays. One con-
tained the path values of the edge’s sign; we
call this the sign array. The other contained the
path values of the right daughter of a schema
such that its left daughter is unified with the
edge’s sign; we call this a schema array. When
we apply a schema to two edges, e1 and e2, the
schema array of e1 and the sign array of e2 are
quickly checked. If it fails, then quick check re-
turns a unification failure. If it succeeds, the
signs are unified with the schemata, and the re-
sult of unification is returned.
Large constituent inhibition (Kaplan et al.,
2004) It is unlikely for a large medial edge to
contribute to the final parsing result if it spans
more than 20 words and is not adjacent to the
beginning or ending of the sentence. Large
constituent inhibition prevents the parser from
generating medial edges that span more than
some word length.
HPSG parsing with a CFG chunk parser A
hybrid of deep parsing and shallow parsing
was recently found to improve the efficiency
of deep parsing (Daum et al., 2003; Frank et
al., 2003; Frank, 2004). As a preprocessor, the
shallow parsing must be very fast and achieve
high precision but not high recall so that the
</bodyText>
<equation confidence="0.996167888888889">
!Aifi(c)
X
i
XZ.,,, =
T1
Xexp
cET0
X
i
</equation>
<page confidence="0.855171">
105
</page>
<bodyText confidence="0.732106">
procedure Viterbi((w1, ... , wn), (L&apos;, R), r,, 6, 0)
</bodyText>
<equation confidence="0.886251142857143">
for i = 1 to n
foreach Fu E {F|(wi, F) E L}
α = Ei Aifi(Fu)
ir[i − 1, i] — ir[i − 1, i] U {Fu}
if (α &gt; p[i − 1, i, Fu]) then
p[i − 1, i, Fu] — α
for d = 1 to n
for i = 0 to n − d
j = i + d
for k = i + 1 to j − 1
foreach Fs E ir[i, k], Ft E ir[k, j], r E R
if F = r(Fs, Ft) has succeeded
α = p[i, k, Fs] + p[k, j, Ft] + Ei Aifi(F)
ir[i, j] — ir[i, j] U {F}
</equation>
<construct confidence="0.965989">
if (α &gt; p[i, j, F]) then
p[i, j, F] — α
</construct>
<figureCaption confidence="0.998987">
Figure 2: Pseudo-code of Viterbi algorithms for probabilistic HPSG parsing
</figureCaption>
<bodyText confidence="0.999763210526316">
total parsing performance in terms of precision,
recall and speed is not degraded. Because there
is trade-off between speed and accuracy in
this approach, the total parsing performance
for large-scale corpora like the Penn treebank
should be measured. We introduce a CFG
chunk parser (Tsuruoka and Tsujii, 2005a) as a
preprocessor of HPSG parsing. Chunk parsers
meet the requirements for preprocessors; they
are very fast and have high precision. The
grammar for the chunk parser is automatically
extracted from the CFG treebank translated
from the HPSG treebank, which is generated
during grammar extraction from the Penn
treebank. The principal idea of using the chunk
parser is to use the bracket information, i.e.,
parse trees without non-terminal symbols, and
prevent the HPSG parser from generating edges
that cross brackets.
</bodyText>
<sectionHeader confidence="0.7991005" genericHeader="method">
4 Beam thresholding for HPSG
parsing
</sectionHeader>
<subsectionHeader confidence="0.999656">
4.1 Simple beam thresholding
</subsectionHeader>
<bodyText confidence="0.99997697826087">
Many algorithms for improving the efficiency of
PCFG parsing have been extensively investigated.
They include grammar compilation (Tomita, 1986;
Nederhof, 2000), the Viterbi algorithm, controlling
search strategies without FOM such as left-corner
parsing (Rosenkrantz and Lewis II, 1970) or head-
corner parsing (Kay, 1989; van Noord, 1997), and
with FOM such as the beam search, the best-first
search or A* search (Chitrao and Grishman, 1990;
Caraballo and Charniak, 1998; Collins, 1999; Rat-
naparkhi, 1999; Charniak, 2000; Roark, 2001; Klein
and Manning, 2003). The beam search and best-
first search algorithms significantly reduce the time
required for finding the best parse at the cost of los-
ing the guarantee of finding the correct parse.
The CYK algorithm, which is essentially a bottom-
up parser, is a natural choice for non-probabilistic
HPSG parsers. Many of the constraints are ex-
pressed as lexical entries in HPSG, and bottom-up
parsers can use those constraints to reduce the search
space in the early stages of parsing.
For PCFG, extending the CYK algorithm to out-
put the Viterbi parse is straightforward (Ney, 1991;
Jurafsky and Martin, 2000). The parser can effi-
ciently calculate the Viterbi parse by taking the max-
imum of the probabilities of the same nonterminal
symbol in each cell. With the probabilistic model
defined in Section 2, we can also define the Viterbi
search for unification-based grammars (Geman and
Johnson, 2002). Figure 2 shows the pseudo-code of
Viterbi algorithm. The 7r[i, j] represents the set of
partial parse results that cover words wi+1, ... , wj,
and p[i, j, F] stores the maximum FOM of partial
parse result F at cell (i, j). Feature functions are
defined over lexical entries and results of rule appli-
cations, which correspond to conjunctive nodes in a
feature forest. The FOM of a newly created partial
parse, F, is computed by summing the values of p of
the daughters and an additional FOM of F.
The Viterbi algorithm enables various pruning
techniques to be used for efficient parsing. Beam
thresholding (Goodman, 1997) is a simple and effec-
tive technique for pruning edges during parsing. In
each cell of the chart, the method keeps only a por-
tion of the edges which have higher FOMs compared
to the other edges in the same cell.
</bodyText>
<page confidence="0.991516">
106
</page>
<bodyText confidence="0.978948">
procedure BeamThresholding(hw1, ... , wni, hL&apos;, Ri, r,, S, 0)
</bodyText>
<equation confidence="0.945661266666667">
for i = 1 to n
foreach Fu ∈ {F|hwi, Fi ∈ L}
α = Ei Aifi(Fu)
ir[i − 1, i] ← ir[i − 1, i] ∪ {Fu}
if (α &gt; p[i − 1, i, Fu]) then
p[i − 1, i, Fu] ← α
for d = 1 to n
for i = 0 to n − d
j = i + d
for k = i + 1 to j − 1
foreach F3 ∈ ir[i, k], Ft ∈ ir[k, j], r ∈ R
if F = r(F3, Ft) has succeeded
α = p[i, k, F3] + p[k, j, Ft] + Ei Aifi(F)
ir[i, j] ← ir[i, j] ∪ {F}
if (α &gt; p[i, j, F]) then
p[i, j, F] ← α
LocalThresholding(r,, S)
GlobalThresholding(n, 0)
procedure LocalThresholding(r,, S)
sort ir[i, j] according to p[i, j, F]
ir[i, j] ← {ir[i, j]1,... ,ir[i, j].}
αmax = maxF p[i, j, F]
foreach F ∈ ir[i, j]
if p[i, j, F] &lt; αmax − S
ir[i, j] ← ir[i, j]\{F}
procedure GlobalThresholding(n, 0)
f[0..n] ← {0, −∞ − ∞, ... , −∞}
b[0..n] ← {−∞, −∞, ... , −∞, 0}
#forward
for i = 0 to n − 1
for j = i + 1 ton
foreach F ∈ ir[i, j]
f[j] ← max(f[j], f[i] + p[i, j, F])
#backward
for i = n − 1 to 0
for j = i + 1 ton
foreach F ∈ ir[i, j]
b[i] ← max(b[i], b[j] + p[i, j, F])
#global thresholding
αmax = f[n]
for i = 0 to n − 1
for j = i + 1 ton
foreach F ∈ ir[i, j]
if f[i] + p[i, j, F] + b[j] &lt; αmax − 0 then
ir[i, j] ← ir[i, j]\{F}
</equation>
<figureCaption confidence="0.94157">
Figure 3: Pseudo-code of local beam search and global beam search algorithms for probabilistic HPSG
parsing
</figureCaption>
<page confidence="0.974097">
107
</page>
<bodyText confidence="0.77332">
procedure IterativeBeamThresholding(w, G, rc0, S0, B0, Orc, OS, OB, rclast, Slast, Blast)
</bodyText>
<equation confidence="0.256267">
rc ← rc0; S ← S0; B ← B0
loop while rc ≤ rclast and S ≤ Slast and B ≤ Blast
call BeamThresholding(w, G, rc, S, B)
if ir[1, n] # ∅ then exit
rc ← rc + Orc; S ← S + OS; B ← B + OB
</equation>
<figureCaption confidence="0.999626">
Figure 4: Pseudo-code of iterative beam thresholding
</figureCaption>
<bodyText confidence="0.9948005">
We tested three selection schemes for deciding
which edges to keep in each cell.
Local thresholding by number of edges Each
cell keeps the top rc edges based on their FOMs.
Local thresholding by beam width Each cell
keeps the edges whose FOM is greater than
αmax − S, where αmax is the highest FOM
among the edges in the cell.
Global thresholding by beam width Each cell
keeps the edges whose global FOM is greater
than αmax−B, where αmax is the highest global
FOM in the chart.
Figure 3 shows the pseudo-code of local beam
search, and global beam search algorithms for prob-
abilistic HPSG parsing. The code for local thresh-
olding is inserted at the end of the computation for
each cell. In Figure 3, 7r[i, j]k denotes the k-th ele-
ment in sorted set 7r[i, j]. We first take the first rc
elements that have higher FOMs and then remove
the elements with FOMs lower than αmax − S.
Global thresholding is also used for pruning edges,
and was originally proposed for CFG parsing (Good-
man, 1997). It prunes edges based on their global
FOM and the best global FOM in the chart. The
global FOM of an edge is defined as its FOM plus its
forward and backward FOMs, where the forward and
backward FOMs are rough estimations of the outside
FOM of the edge. The global thresholding is per-
formed immediately after each line of the CYK chart
is completed. The forward FOM is calculated first,
and then the backward FOM is calculated. Finally,
all edges with a global FOM lower than αmax − B
are pruned. Figure 3 gives further details of the al-
gorithm.
</bodyText>
<subsectionHeader confidence="0.9362">
4.2 Iterative beam thresholding
</subsectionHeader>
<bodyText confidence="0.9974634">
We tested the iterative beam thresholding proposed
by Tsuruoka and Tsujii (2005b). We started the
parsing with a narrow beam. If the parser output
results, they were taken as the final parse results. If
the parser did not output any results, we widened the
</bodyText>
<tableCaption confidence="0.999171">
Table 1: Abbreviations used in experimental results
</tableCaption>
<bodyText confidence="0.928116684210526">
num local beam thresholding by number
width local beam thresholding by width
global global beam thresholding by width
iterative iterative parsing with local beam
thresholding by number and width
chp parsing with CFC chunk parser
beam, and reran the parsing. We continued widen-
ing the beam until the parser output results or the
beam width reached some limit.
The pseudo-code is presented in Figure 4. It calls
the beam thresholding procedure shown in Figure 3
and increases parameters rc and S until the parser
outputs results, i.e., 7r[1, n] # 0.
Preserved iterative parsing Our implemented
CFG parser with iterative parsing cleared the
chart and edges at every iteration although the
parser regenerated the same edges using those
generated in the previous iteration. This is
because the computational cost of regenerating
edges is smaller than that of reusing edges to
which the rules have already been applied. For
HPSG parsing, the regenerating cost is even
greater than that for CFG parsing. In our
implementation of HPSG parsing, the chart
and edges were not cleared during the iterative
parsing. Instead, the pruned edges were marked
as thresholded ones. The parser counted the
number of iterations, and when edges were
generated, they were marked with the iteration
number, which we call the generation. If
edges were thresholded out, the generation was
replaced with the current iteration number plus
1. Suppose we have two edges, e1 and e2. The
grammar rules are applied iff both e1 and e2 are
not thresholded out, and the generation of e1
or e2 is equal to the current iteration number.
Figure 5 shows the pseudo-code of preserved
iterative parsing.
</bodyText>
<page confidence="0.997898">
108
</page>
<bodyText confidence="0.980722">
procedure BeamThresholding(hw1, ... , wni, hL&apos;, Ri, K, S, B, iternum)
</bodyText>
<equation confidence="0.912044596491228">
for i = 1 to n
foreach Fu ∈ {F|hwi, Fi ∈ L}
α = Ei Aifi(Fu)
ir[i − 1, i] ← ir[i − 1, i] ∪ {Fu}
if (α &gt; p[i − 1, i, Fu]) then
p[i − 1, i, Fu] ← α
for d = 1 to n
for i = 0 to n − d
j = i + d
for k = i + 1 to j − 1
foreach Fs ∈ 0[i, k], Ft ∈ 0[k, j], r ∈ R
if gen[i, k, Fs] = iternum ∨ gen[k, j, Ft] = iternum
if F = r(Fs, Ft) has succeeded
gen[i, j, F] ← iternum
α = p[i, k, Fs] + p[k, j, Ft] + Ei Aifi(F)
ir[i, j] ← ir[i, j] ∪ {F}
if (α &gt; p[i, j, F]) then
p[i, j, F] ← α
LocalThresholding(K, S, iternum)
GlobalThresholding(n, B, iternum)
procedure LocalThresholding(K, S, iternum)
sort ir[i, j] according to p[i, j, F]
0[i, j] ← {ir[i,j]1,... ,ir[i, j]κ}
αmax = maxF p[i, j, F]
foreach F ∈ 0[i, j]
if p[i, j, F] &lt; αmax − S
0[i, j] ← 0[i, j]\{F}
foreach F ∈ (ir[i, j] − 0[i, j])
gen[i, j, F] ← iternum + 1
procedure GlobalThresholding(n, B, iternum)
f[0..n] ← {0, −∞ − ∞, ... , −∞}
b[0..n] ← {−∞, −∞, ... , −∞, 0}
#forward
for i = 0 to n − 1
for j = i + 1 ton
foreach F ∈ ir[i, j]
f[j] ← max(f[j], f[i] + p[i, j, F])
#backward
for i = n − 1 to 0
for j = i + 1 ton
foreach F ∈ ir[i, j]
b[i] ← max(b[i], b[j] + p[i, j, F])
#global thresholding
αmax = f[n]
for i = 0 to n − 1
for j = i + 1 ton
foreach F ∈ 0[i, j]
if f[i] + p[i, j, F] + b[j] &lt; αmax − B then
0[i, j] ← 0[i, j]\{F}
foreach F ∈ (ir[i, j] − 0[i, j])
gen[i, j, F] ← iternum + 1
procedure IterativeBeamThresholding(w, G, K0, S0, B0, OK, OS, OB, Klast, Slast, Blast)
K ← K0; S ← S0; B ← B0; iternum = 0
loop while K ≤ Klast and S ≤ Slast and B ≤ Blast
call BeamThresholding(w, G, K, S, B, iternum)
if ir[1, n] =6 ∅ then exit
K ← K + OK; S ← S + OS; B ← B + OB; iternum ← iternum + 1
</equation>
<figureCaption confidence="0.99755">
Figure 5: Pseudo-code of preserved iterative parsing for HPSG
</figureCaption>
<page confidence="0.999112">
109
</page>
<tableCaption confidence="0.997008">
Table 2: Experimental results for development set (section 22) and test set (section 23)
</tableCaption>
<table confidence="0.984987333333333">
Precision Recall F-score Avg. Time (ms) No. of failed sentences
development set 88.21% 87.32% 87.76% 360 12
test set 87.85% 86.85% 87.35% 360 15
</table>
<figure confidence="0.998912375">
0 2 4 6 6 10 12 14 16
Sent.n�� kngth (—d.)
Pusi�g time (me)
100000000
10000000
1000000
100000
10000
1000
100
10
1
P—i�g time (me)
100000000
10000000
1000000
100000
10000
1000
100
10
1
0 2 4 6 6 10 12 14 16
Sentence length (ward.)
</figure>
<figureCaption confidence="0.98927175">
Figure 7: Parsing time for the sentences in Section 24 of less than 15 words of Viterbi parsing (none) (Left)
and iterative parsing (iterative) (Right)
Figure 6: Parsing time versus sentence length for the
sentences in Section 23 of less than 40 words
</figureCaption>
<sectionHeader confidence="0.99196" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.988866731707317">
We evaluated the efficiency of the parsing techniques
by using the HPSG for English developed by Miyao
et al. (2005). The lexicon of the grammar was ex-
tracted from Sections 02-21 of the Penn Treebank
(Marcus et al., 1994) (39,832 sentences). The gram-
mar consisted of 2,284 lexical entry templates for
10,536 words&apos;. The probabilistic disambiguation
model of the grammar was trained using the same
portion of the treebank (Miyao and Tsujii, 2005).
&apos;Lexical entry templates for POS are also developed.
They are assigned to unknown words.
The model included 529,856 features. The param-
eters for beam searching were determined manually
by trial and error using Section 22; δ0 = 12, Aδ =
6, δlast = 30, κ0 = 6.0, Aκ = 3.0, κlast = 15.1,
θ0 = 8.0, Aθ = 4.0, and θlast = 20.1. We used
the chunk parser developed by Tsuruoka and Tsu-
jii (2005a). Table 1 shows the abbreviations used in
presenting the results.
We measured the accuracy of the predicate-
argument relations output by the parser. A
predicate-argument relation is defined as a tuple
(σ, wh, a, wa), where σ is the predicate type (e.g., ad-
jective, intransitive verb), wh is the head word of the
predicate, a is the argument label (MODARG, ARG1,
..., ARG4), and wa is the head word of the argu-
ment. Precision/recall is the ratio of tuples correctly
identified by the parser. This evaluation scheme was
the same as used in previous evaluations of lexical-
ized grammars (Hockenmaier, 2003; Clark and Cur-
ran, 2004; Miyao and Tsujii, 2005). The experiments
were conducted on an AMD Opteron server with a
2.4-GHz CPU. Section 22 of the Treebank was used
as the development set, and performance was evalu-
ated using sentences of less than 40 words in Section
23 (2,164 sentences, 20.3 words/sentence). The per-
formance of each parsing technique was analyzed us-
ing the sentences in Section 24 of less than 15 words
(305 sentences) and less than 40 words (1145 sen-
tences).
Table 2 shows the parsing performance using all
</bodyText>
<figure confidence="0.993537382352941">
0 5 10 15 20 25 30 35 40 45
Sentence length (words)
Parsing time (ms)
10000
4000
9000
8000
7000
6000
5000
3000
2000
1000
0
110
F-score
86%
84%
82%
80%
78%
76%
74%
72%
70%
150 350 550 750 950 1150
Average parsing time (ms)
num
width
global
num+width
num+width+global
iterative
iterative+global
</figure>
<figureCaption confidence="0.987752">
Figure 8: F-score versus average parsing time
</figureCaption>
<figure confidence="0.9158969">
F-score
86%
84%
82%
80%
78%
76%
74%
72%
70%
150 350 550 750 950
Average parsing time (ms)
num+width
num+width+chp
num+width+global
num+width+global+chp
iterative
iterative+chp
iterative+global
iterative+global+chp
</figure>
<figureCaption confidence="0.992808">
Figure 9: F-score versus average parsing time with/without chunk parser
</figureCaption>
<page confidence="0.998721">
111
</page>
<tableCaption confidence="0.999707">
Table 3: Viterbi parsing versus beam thresholding versus iterative parsing
</tableCaption>
<table confidence="0.99971775">
Precision Recall F-score Avg. Time (ms) No. of failed sentences
viterbi parsing (none) 88.22% 87.94% 88.08% 103923 2
beam search parsing (num+width) 88.96% 82.38% 85.54% 88 26
iterative parsing (iterative) 87.61% 87.24% 87.42% 99 2
</table>
<tableCaption confidence="0.998145">
Table 4: Contribution to performance of each implementation
</tableCaption>
<table confidence="0.801606">
Precision
</table>
<tableCaption confidence="0.951344428571429">
full 85.49%
full−piter 85.74%
full−qc 85.49%
full−chp 85.77%
full−global 85.23%
full−lci 85.68%
full−piter−qc−chp−global−lci 85.33%
</tableCaption>
<table confidence="0.967716384615385">
Recall F-score Avg. Time (ms) diff(*) No. of failed sentences
84.21% 84.84% 407 0 13
84.70% 85.22% 631 224 10
84.21% 84.84% 562 155 13
84.76% 85.26% 510 103 10
84.32% 84.78% 434 27 9
84.40% 85.03% 424 17 13
84.71% 85.02% 1033 626 6
full ... iterative + global + chp
piter ... preserved iterative parsing
qc ... quick check
lci ... large constituent inhibition
diff(*) ... (Avg. Time of full) - (Avg. Time)
</table>
<bodyText confidence="0.975386116666667">
thresholding techniques and implementations de-
scribed in Section 4 for the sentences in the devel-
opment set (Section 22) and the test set (Section 23)
of less than 40 words. In the table, precision, recall,
average parsing time per sentence, and the number of
sentences that the parser failed to parse are detailed.
Figure 6 shows the distribution of parsing time for
the sentence length.
Table 3 shows the performance of the Viterbi pars-
ing, beam search parsing, and iterative parsing for
the sentences in Section 24 of less than 15 words
2. The parsing without beam searching took more
than 1,000 times longer than with beam searching.
However, the beam searching reduced the recall from
87.9% to 82.4%. The main reason for this reduc-
tion was parsing failure. That is, the parser could
not output any results when the beam was too nar-
row instead of producing incorrect parse results. Al-
though iterative parsing was originally developed for
efficiency, the results revealed that it also increases
the recall. This is because the parser continues try-
ing until some results are output. Figure 7 shows the
logarithmic graph of parsing time for the sentence
length. The left side of the figure shows the parsing
time of the Viterbi parsing and the right side shows
the parsing time of the iterative parsing.
Figure 8 shows the performance of the parsing
techniques for different parameters for the sentences
in Section 24 of less than 40 words. The combina-
tions of thresholding techniques achieved better re-
2The sentence length was limited to 15 words because
of inefficiency of Viterbi parsing
sults than the single techniques. Local thresholding
using the width (width) performed better than that
using the number (num). The combination of us-
ing width and number (num+width) performed bet-
ter than single local and single global thresholding.
The superiority of iterative parsing (iterative) was
again demonstrated in this experiment. Although we
did not observe significant improvement with global
thresholding, the global plus iterative combination
slightly improved performance.
Figure 9 shows the performance with and with-
out the chunk parser. The lines with white symbols
represent parsing without the chunk parser, and the
lines with black symbols represent parsing with the
chunk parser. The chunk parser improved the to-
tal parsing performance significantly. The improve-
ments with global thresholding were less with the
chunk parser.
Finally, Table 4 shows the contribution to perfor-
mance of each implementation for the sentences in
Section 24 of less than 40 words. The ‘full’ means
the parser including all thresholding techniques and
implementations described in Section 4. The ‘full
− x&apos; means the full minus x. The preserved itera-
tive parsing, the quick check, and the chunk parser
greatly contributed to the final parsing speed, while
the global thresholding and large constituent inhibi-
tion did not.
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999628">
We have described the results of experiments with a
number of existing techniques in head-driven phrase
</bodyText>
<page confidence="0.99444">
112
</page>
<bodyText confidence="0.9999650625">
structure grammar (HPSG) parsing. Simple beam
thresholding, similar to that for probabilistic CFG
(PCFG) parsing, significantly increased the parsing
speed over Viterbi algorithm, but reduced the re-
call because of parsing failure. Iterative parsing sig-
nificantly increased the parsing speed without de-
grading precision or recall. We tested three tech-
niques originally developed for deep parsing: quick
check, large constituent inhibition, and HPSG pars-
ing with a CFG chunk parser. The contributions
of the large constituent inhibition and global thresh-
olding were not significant, while the quick check and
chunk parser greatly contributed to total parsing per-
formance. The precision, recall and average parsing
time for the Penn treebank (Section 23) were 87.85%,
86.85%, and 360 ms, respectively.
</bodyText>
<sectionHeader confidence="0.998948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999514256097561">
Steven P. Abney. 1997. Stochastic attribute-value
grammars. Computational Linguistics, 23(4):597–
618.
Adam Berger, Stephen Della Pietra, and Vin-
cent Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1):39–71.
Joan Bresnan. 1982. The Mental Representation of
Grammatical Relations. MIT Press, Cambridge,
MA.
Ted Briscoe and John Carroll. 1993. Generalized
probabilistic LR-parsing of natural language (cor-
pora) with unification-based grammars. Compu-
tational Linguistics, 19(1):25–59.
Sharon A. Caraballo and Eugene Charniak. 1998.
New figures of merit for best-first probabilis-
tic chart parsing. Computational Linguistics,
24(2):275–298.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proc. of NAACL-2000, pages
132–139.
Mahesh V. Chitrao and Ralph Grishman. 1990.
Edge-based best-first chart parsing. In Proc. of
the DARPA Speech and Natural Language Work-
shop, pages 263–266.
Stephen Clark and James R. Curran. 2004. Parsing
the WSJ using CCG and log-linear models. In
Proc. of ACL&apos;04, pages 104–111.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
Univ. of Pennsylvania.
Michael Daum, Kilian A. Foth, and Wolfgang Men-
zel. 2003. Constraint based integration of deep
and shallow parsing techniques. In Proc. of EACL-
2003, pages 99–106.
Anette Frank, Markus Becker, Berthold Crysmann,
Bernd Kiefer, and Ulrich Schaefer. 2003. In-
tegrated shallow and deep parsing: TopP meets
HPSG. In Proc. of ACL&apos;03, pages 104–111.
Anette Frank. 2004. Constraint-based RMRS con-
struction from shallow grammars. In Proc. of
COLING-2004, pages 1269–1272.
Stuart Geman and Mark Johnson. 2002. Dy-
namic programming for parsing and estimation of
stochastic unification-based grammars. In Proc. of
ACL&apos;02, pages 279–286.
Joshua Goodman. 1997. Global thresholding and
multiple pass parsing. In Proc. of EMNLP-1997,
pages 11–25.
Julia Hockenmaier. 2003. Parsing with generative
models of predicate-argument structure. In Proc.
of ACL&apos;03, pages 359–366.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic “unification-based&amp;quot; grammars. In Proc.
of ACL &apos;99, pages 535–541.
Dainiel Jurafsky and James H. Martin. 2000. Speech
and Language Processing. Prentice Hall.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc.
of HLT/NAACL&apos;04.
Walter Kasper, Hans-Ulrich Krieger, Jorg Spilker,
and Hans Weber. 1996. From word hypotheses
to logical form: An efficient interleaved approach.
In Proceedings of Natural Language Processing and
Speech Technology. Results of the 3rd KONVENS
Conference, pages 77–88.
Martin Kay. 1989. Head driven parsing. In Proc. of
IWPT&apos;89, pages 52–62.
Bernd Kiefer, Hans-Ulrich Krieger, John Carroll,
and Robert Malouf. 1999. A bag of useful tech-
niques for efficient and robust parsing. In Proc. of
ACL&apos;99, pages 473–480, June.
Bernd Kiefer, Hans-Ulrich Krieger, and Detlef
Prescher. 2002. A novel disambiguation method
for unification-based grammars using probabilistic
context-free approximations. In Proc. of COLING
2002.
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact viterbi parse selection. In
Proc. of HLT-NAACL&apos;03.
</reference>
<page confidence="0.991885">
113
</page>
<reference confidence="0.999334557894737">
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value
grammars. In Proc. of IJCNLP-04 Workshop `Be-
yond Shallow Analyses”.
Robert Malouf, John Carroll, and Ann Copestake.
2000. Efficient feature structure operations with-
out compilation. Journal of Natural Language En-
gineering, 6(1):29–46.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In
Proc. of CoNLL-2002, pages 49–55.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1994. Building a large
annotated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313–330.
Yuji Matsumoto, Hozumi Tanaka, Hideki Hirakawa,
Hideo Miyoshi, and Hideki Yasukawa. 1983. BUP:
A bottom up parser embedded in Prolog. New
Generation Computing, 1(2):145–158.
John Maxwell and Ron Kaplan. 1993. The interface
between phrasal and functional constraints. Com-
putational Linguistics, 19(4):571–589.
Yusuke Miyao and Jun&apos;ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. of
HLT 2002, pages 292–297.
Yusuke Miyao and Jun&apos;ichi Tsujii. 2005. Proba-
bilistic disambiguation models for wide-coverage
HPSG parsing. In Proc. of ACL&apos;05, pages 83–90.
Yusuke Miyao, Takashi Ninomiya, and Jun&apos;ichi Tsu-
jii. 2003. Probabilistic modeling of argument
structures including non-local dependencies. In
Proc. of RANLP &apos;03, pages 285–291.
Yusuke Miyao, Takashi Ninomiya, and Jun&apos;ichi Tsu-
jii, 2005. Keh-Yih Su, Jun&apos;ichi Tsujii, Jong-Hyeok
Lee and Oi Yee Kwong (Eds.), Natural Language
Processing - IJCNLP 2004 LNAI 3248, chapter
Corpus-oriented Grammar Development for Ac-
quiring a Head-driven Phrase Structure Grammar
from the Penn Treebank, pages 684–693. Springer-
Verlag.
Mark-Jan Nederhof. 2000. Practical experiments
with regular approximation of context-free lan-
guages. Computational Linguistics, 26(1):17–44.
H. Ney. 1991. Dynamic programming parsing for
context-free grammars in continuous speech recog-
nition. IEEE Transactions on Signal Processing,
39(2):336–340.
Stephan Oepen, Dan Flickinger, Jun&apos;ichi Tsujii, and
Hans Uszkoreit, editors. 2002. Collaborative Lan-
guage Engineering: A Case Study in Efficient
Grammar-Based Processing. CSLI Publications.
Gerald Penn and Cosmin Munteanu. 2003. A
tabulation-based parsing method that reduces
copying. In Proc. of ACL&apos;03).
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Adwait Ratnaparkhi. 1999. Learning to parse natu-
ral language with maximum entropy models. Ma-
chine Learning, 34(1-3):151–175.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and
Mark Johnson. 2000. Lexicalized stochastic
modeling of constraint-based grammars using log-
linear measures and EM training. In Proc. of
ACL&apos;00, pages 480–487.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27(2):249–276.
Daniel J. Rosenkrantz and Philip M. Lewis II. 1970.
Deterministic left corner parsing. In IEEE Con-
ference Record of the 11th Annual Symposium on
Switching and Automata Theory, pages 139–152.
Yves Shabes, Anne Abeille, and Aravind K. Joshi.
1988. Parsing strategies with `lexicalized&apos; gram-
mars: Application to tree adjoining grammars. In
Proc. of COLING&apos;88, pages 578–583.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press.
Masaru Tomita. 1986. Efficient Parsing for Natural
Language. Kluwer Academic Publishers.
Kentaro Torisawa, Kenji Nishida, Yusuke Miyao, and
Jun&apos;ichi Tsujii. 2000. An HPSG parser with CFG
filtering. Journal of Natural Language Engineer-
ing, 6(1):63–80.
Yoshimasa Tsuruoka and Jun&apos;ichi Tsujii. 2005a.
Chunk parsing revisited. In Proc. of IWPT-2005.
Yoshimasa Tsuruoka and Jun&apos;ichi Tsujii, 2005b.
Keh-Yih Su, Jun&apos;ichi Tsujii, Jong-Hyeok Lee and
Oi Yee Kwong (Eds.), Natural Language Process-
ing - IJCNLP 2004 LNAI 3248, chapter Itera-
tive CKY Parsing for Probabilistic Context-Free
Grammars, pages 52–60. Springer-Verlag.
Gertjan van Noord. 1997. An efficient implemen-
tation of the head-corner parser. Computational
Linguistics, 23(3):425–456.
</reference>
<page confidence="0.998643">
114
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.284590">
<title confidence="0.996784">Efficacy of Beam Thresholding, Unification Filtering and Hybrid Parsing in Probabilistic HPSG Parsing</title>
<author confidence="0.962696">Takashi</author>
<affiliation confidence="0.992721333333333">CREST, Department of Computer The University of</affiliation>
<email confidence="0.650165">ninomi@is.s.u-tokyo.ac.jp</email>
<author confidence="0.639544">Yusuke</author>
<affiliation confidence="0.9971015">Department of Computer The University of</affiliation>
<email confidence="0.848991">yusuke@is.s.u-tokyo.ac.jp</email>
<abstract confidence="0.990450315789474">We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The contributions of the large constituent inhibition and global thresholding were not significant, while the quick check and chunk parser greatly contributed to total parsing performance. The precision, recall and average parsing time for the Penn treebank (Section 23) were 87.85%, 86.85%, and 360 ms, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<pages>618</pages>
<contexts>
<context position="6599" citStr="Abney, 1997" startWordPosition="1008" endWordPosition="1009">provides the phrasal sign of the mother. The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set J7 of feature structures, an HPSG is formulated as a tuple, G = (L, R), where L = {l = (w, F)|w E W, F E J7} is a set of lexical entries, and R is a set of schemata, i.e., r E R is a partial function: J7 x J7 → J7. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = (wl, ... , wn) is ! 1 X p(T|w) = exp λifi(T) Z&amp;quot; i !λifi(T �) , Spring where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T. Intuitively, the probability is defined as the normalized product of the weights exp(λ</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven P. Abney. 1997. Stochastic attribute-value grammars. Computational Linguistics, 23(4):597– 618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="6866" citStr="Berger et al., 1996" startWordPosition="1051" endWordPosition="1054">J7 of feature structures, an HPSG is formulated as a tuple, G = (L, R), where L = {l = (w, F)|w E W, F E J7} is a set of lexical entries, and R is a set of schemata, i.e., r E R is a partial function: J7 x J7 → J7. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = (wl, ... , wn) is ! 1 X p(T|w) = exp λifi(T) Z&amp;quot; i !λifi(T �) , Spring where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T. Intuitively, the probability is defined as the normalized product of the weights exp(λi) when a characteristic corresponding to fi appears in parse result T. Model parameters λi are estimated using numerhas come HEAD verb HEAD noun 1 SUBJ &lt; &gt; COMPS &lt; &gt; SUBJ &lt; COMPS &lt; &gt; has come Spring head-comp HEAD noun SUBJ &lt; &gt; COMPS &lt; &gt; HEAD verb SUBJ &lt; &gt; 1 COMPS &lt;</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>The Mental Representation of Grammatical Relations.</title>
<date>1982</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9912" citStr="Bresnan, 1982" startWordPosition="1589" endWordPosition="1590">y restricts the flexibility of probabilistic modeling. However, previous research (Miyao et al., 2003; Clark and Curran, 2004; Kaplan et al., 2004) showed that predicate-argument relations can be represented under the assumption of feature locality. We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of t</context>
</contexts>
<marker>Bresnan, 1982</marker>
<rawString>Joan Bresnan. 1982. The Mental Representation of Grammatical Relations. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalized probabilistic LR-parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="4253" citStr="Briscoe and Carroll, 1993" startWordPosition="617" endWordPosition="620">hout using probabilities and then select the highest probability parse. The behavior of their algorithms is like that of the Viterbi algorithm for PCFG parsing, so the correct parse with the highest probability is guaranteed. The interesting point of this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest. Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al., 1996; Briscoe and Carroll, 1993; Kiefer et al., 2002), and the most probable parse is found by PCFG parsing. This model is based on PCFG and not probabilistic unification-based grammar parsing. Geman and Johnson (Geman and Johnson, 2002) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Ted Briscoe and John Carroll. 1993. Generalized probabilistic LR-parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):25–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
<author>Eugene Charniak</author>
</authors>
<title>New figures of merit for best-first probabilistic chart parsing.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="14243" citStr="Caraballo and Charniak, 1998" startWordPosition="2360" endWordPosition="2363">on-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 Beam thresholding for HPSG parsing 4.1 Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to </context>
</contexts>
<marker>Caraballo, Charniak, 1998</marker>
<rawString>Sharon A. Caraballo and Eugene Charniak. 1998. New figures of merit for best-first probabilistic chart parsing. Computational Linguistics, 24(2):275–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proc. of NAACL-2000,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="14293" citStr="Charniak, 2000" startWordPosition="2369" endWordPosition="2370"> edges that cross brackets. 4 Beam thresholding for HPSG parsing 4.1 Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Viterbi parse is straightforward (Ney, </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proc. of NAACL-2000, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh V Chitrao</author>
<author>Ralph Grishman</author>
</authors>
<title>Edge-based best-first chart parsing.</title>
<date>1990</date>
<booktitle>In Proc. of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>263--266</pages>
<contexts>
<context position="14213" citStr="Chitrao and Grishman, 1990" startWordPosition="2356" endWordPosition="2359"> i.e., parse trees without non-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 Beam thresholding for HPSG parsing 4.1 Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, e</context>
</contexts>
<marker>Chitrao, Grishman, 1990</marker>
<rawString>Mahesh V. Chitrao and Ralph Grishman. 1990. Edge-based best-first chart parsing. In Proc. of the DARPA Speech and Natural Language Workshop, pages 263–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proc. of ACL&apos;04,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="9423" citStr="Clark and Curran, 2004" startWordPosition="1512" endWordPosition="1515">hat is, only a nonterminal symbol of a mother is considered in further processing by ignoring the structure of its daughters. With this assumption, we can compute the figures of merit (FOMs) of partial parse results. This assumption restricts the possibility of feature functions that represent non-local dependencies expressed in a parse result. Since unification-based grammars can express semantic relations, such as predicate-argument relations, in their structure, the assumption unjustifiably restricts the flexibility of probabilistic modeling. However, previous research (Miyao et al., 2003; Clark and Curran, 2004; Kaplan et al., 2004) showed that predicate-argument relations can be represented under the assumption of feature locality. We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory </context>
<context position="25167" citStr="Clark and Curran, 2004" startWordPosition="4479" endWordPosition="4483">Table 1 shows the abbreviations used in presenting the results. We measured the accuracy of the predicateargument relations output by the parser. A predicate-argument relation is defined as a tuple (σ, wh, a, wa), where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Precision/recall is the ratio of tuples correctly identified by the parser. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark and Curran, 2004; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Section 22 of the Treebank was used as the development set, and performance was evaluated using sentences of less than 40 words in Section 23 (2,164 sentences, 20.3 words/sentence). The performance of each parsing technique was analyzed using the sentences in Section 24 of less than 15 words (305 sentences) and less than 40 words (1145 sentences). Table 2 shows the parsing performance using all 0 5 10 15 20 25 30 35 40 45 Sentence length (words) Parsing time (ms) 10000 4000 9000 8000 7000 600</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and log-linear models. In Proc. of ACL&apos;04, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. of Pennsylvania.</institution>
<contexts>
<context position="14258" citStr="Collins, 1999" startWordPosition="2364" endWordPosition="2365">nt the HPSG parser from generating edges that cross brackets. 4 Beam thresholding for HPSG parsing 4.1 Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Vite</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, Univ. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Daum</author>
<author>Kilian A Foth</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Constraint based integration of deep and shallow parsing techniques.</title>
<date>2003</date>
<booktitle>In Proc. of EACL2003,</booktitle>
<pages>99--106</pages>
<contexts>
<context position="1951" citStr="Daum et al., 2003" startWordPosition="270" endWordPosition="273">luding local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. Unification-based grammars have been extensively studied in terms of linguistic formulation and computation efficiency. Although they provide precise linguistic structures of sentences, their processing is considered expensive because of the detailed descriptions. Since efficiency is of particular concern in practical applications, a number of studies have focused on improving the parsing efficiency of unificati</context>
<context position="12156" citStr="Daum et al., 2003" startWordPosition="1972" endWordPosition="1975">ion failure. If it succeeds, the signs are unified with the schemata, and the result of unification is returned. Large constituent inhibition (Kaplan et al., 2004) It is unlikely for a large medial edge to contribute to the final parsing result if it spans more than 20 words and is not adjacent to the beginning or ending of the sentence. Large constituent inhibition prevents the parser from generating medial edges that span more than some word length. HPSG parsing with a CFG chunk parser A hybrid of deep parsing and shallow parsing was recently found to improve the efficiency of deep parsing (Daum et al., 2003; Frank et al., 2003; Frank, 2004). As a preprocessor, the shallow parsing must be very fast and achieve high precision but not high recall so that the !Aifi(c) X i XZ.,,, = T1 Xexp cET0 X i 105 procedure Viterbi((w1, ... , wn), (L&apos;, R), r,, 6, 0) for i = 1 to n foreach Fu E {F|(wi, F) E L} α = Ei Aifi(Fu) ir[i − 1, i] — ir[i − 1, i] U {Fu} if (α &gt; p[i − 1, i, Fu]) then p[i − 1, i, Fu] — α for d = 1 to n for i = 0 to n − d j = i + d for k = i + 1 to j − 1 foreach Fs E ir[i, k], Ft E ir[k, j], r E R if F = r(Fs, Ft) has succeeded α = p[i, k, Fs] + p[k, j, Ft] + Ei Aifi(F) ir[i, j] — ir[i, j] U </context>
</contexts>
<marker>Daum, Foth, Menzel, 2003</marker>
<rawString>Michael Daum, Kilian A. Foth, and Wolfgang Menzel. 2003. Constraint based integration of deep and shallow parsing techniques. In Proc. of EACL2003, pages 99–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Frank</author>
<author>Markus Becker</author>
<author>Berthold Crysmann</author>
<author>Bernd Kiefer</author>
<author>Ulrich Schaefer</author>
</authors>
<title>Integrated shallow and deep parsing: TopP meets HPSG.</title>
<date>2003</date>
<booktitle>In Proc. of ACL&apos;03,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1971" citStr="Frank et al., 2003" startWordPosition="274" endWordPosition="277">olding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. Unification-based grammars have been extensively studied in terms of linguistic formulation and computation efficiency. Although they provide precise linguistic structures of sentences, their processing is considered expensive because of the detailed descriptions. Since efficiency is of particular concern in practical applications, a number of studies have focused on improving the parsing efficiency of unificationbased grammars (Oe</context>
<context position="12176" citStr="Frank et al., 2003" startWordPosition="1976" endWordPosition="1979">succeeds, the signs are unified with the schemata, and the result of unification is returned. Large constituent inhibition (Kaplan et al., 2004) It is unlikely for a large medial edge to contribute to the final parsing result if it spans more than 20 words and is not adjacent to the beginning or ending of the sentence. Large constituent inhibition prevents the parser from generating medial edges that span more than some word length. HPSG parsing with a CFG chunk parser A hybrid of deep parsing and shallow parsing was recently found to improve the efficiency of deep parsing (Daum et al., 2003; Frank et al., 2003; Frank, 2004). As a preprocessor, the shallow parsing must be very fast and achieve high precision but not high recall so that the !Aifi(c) X i XZ.,,, = T1 Xexp cET0 X i 105 procedure Viterbi((w1, ... , wn), (L&apos;, R), r,, 6, 0) for i = 1 to n foreach Fu E {F|(wi, F) E L} α = Ei Aifi(Fu) ir[i − 1, i] — ir[i − 1, i] U {Fu} if (α &gt; p[i − 1, i, Fu]) then p[i − 1, i, Fu] — α for d = 1 to n for i = 0 to n − d j = i + d for k = i + 1 to j − 1 foreach Fs E ir[i, k], Ft E ir[k, j], r E R if F = r(Fs, Ft) has succeeded α = p[i, k, Fs] + p[k, j, Ft] + Ei Aifi(F) ir[i, j] — ir[i, j] U {F} if (α &gt; p[i, j, </context>
</contexts>
<marker>Frank, Becker, Crysmann, Kiefer, Schaefer, 2003</marker>
<rawString>Anette Frank, Markus Becker, Berthold Crysmann, Bernd Kiefer, and Ulrich Schaefer. 2003. Integrated shallow and deep parsing: TopP meets HPSG. In Proc. of ACL&apos;03, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Frank</author>
</authors>
<title>Constraint-based RMRS construction from shallow grammars.</title>
<date>2004</date>
<booktitle>In Proc. of COLING-2004,</booktitle>
<pages>1269--1272</pages>
<contexts>
<context position="1985" citStr="Frank, 2004" startWordPosition="278" endWordPosition="279">holding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. Unification-based grammars have been extensively studied in terms of linguistic formulation and computation efficiency. Although they provide precise linguistic structures of sentences, their processing is considered expensive because of the detailed descriptions. Since efficiency is of particular concern in practical applications, a number of studies have focused on improving the parsing efficiency of unificationbased grammars (Oepen et al., 20</context>
<context position="12190" citStr="Frank, 2004" startWordPosition="1980" endWordPosition="1981">are unified with the schemata, and the result of unification is returned. Large constituent inhibition (Kaplan et al., 2004) It is unlikely for a large medial edge to contribute to the final parsing result if it spans more than 20 words and is not adjacent to the beginning or ending of the sentence. Large constituent inhibition prevents the parser from generating medial edges that span more than some word length. HPSG parsing with a CFG chunk parser A hybrid of deep parsing and shallow parsing was recently found to improve the efficiency of deep parsing (Daum et al., 2003; Frank et al., 2003; Frank, 2004). As a preprocessor, the shallow parsing must be very fast and achieve high precision but not high recall so that the !Aifi(c) X i XZ.,,, = T1 Xexp cET0 X i 105 procedure Viterbi((w1, ... , wn), (L&apos;, R), r,, 6, 0) for i = 1 to n foreach Fu E {F|(wi, F) E L} α = Ei Aifi(Fu) ir[i − 1, i] — ir[i − 1, i] U {Fu} if (α &gt; p[i − 1, i, Fu]) then p[i − 1, i, Fu] — α for d = 1 to n for i = 0 to n − d j = i + d for k = i + 1 to j − 1 foreach Fs E ir[i, k], Ft E ir[k, j], r E R if F = r(Fs, Ft) has succeeded α = p[i, k, Fs] + p[k, j, Ft] + Ei Aifi(F) ir[i, j] — ir[i, j] U {F} if (α &gt; p[i, j, F]) then p[i, </context>
</contexts>
<marker>Frank, 2004</marker>
<rawString>Anette Frank. 2004. Constraint-based RMRS construction from shallow grammars. In Proc. of COLING-2004, pages 1269–1272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Mark Johnson</author>
</authors>
<title>Dynamic programming for parsing and estimation of stochastic unification-based grammars.</title>
<date>2002</date>
<booktitle>In Proc. of ACL&apos;02,</booktitle>
<pages>279--286</pages>
<contexts>
<context position="4459" citStr="Geman and Johnson, 2002" startWordPosition="651" endWordPosition="654">ity is guaranteed. The interesting point of this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest. Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al., 1996; Briscoe and Carroll, 1993; Kiefer et al., 2002), and the most probable parse is found by PCFG parsing. This model is based on PCFG and not probabilistic unification-based grammar parsing. Geman and Johnson (Geman and Johnson, 2002) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-scale corpora, the Penn treebank. We show how techniques developed for efficient deep parsing can improve the efficiency of probabilistic parsing. These techniques were evaluated in exp</context>
<context position="8195" citStr="Geman and Johnson, 2002" startWordPosition="1317" endWordPosition="1320">S &lt; &gt; head-comp HEAD noun HEAD verb SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; COMPS &lt; 2 HEAD verb 2 SUBJ &lt; &gt; 1 COMPS &lt; &gt; 1&gt; &gt; �� 1 XZ&amp;quot; = Xexp T1 i 104 ical optimization methods (Malouf, 2002) so as to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T|w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T|w). They assumed that features are functions on nodes in a packed parse forest. That is, parse tree T is represented by a set of nodes, i.e., T = {c}, and the parse forest is represented by an and/or graph of the nodes. From this assumption, we can redefine the probability as p(T|w) = 1Z exp Z.,,, cET !Aifi(c) � A packed parse forest has a structure similar to a chart of CFG parsing, and c corresponds to an edge in the chart. This assumption corresponds to the independence assumption in P</context>
<context position="15215" citStr="Geman and Johnson, 2002" startWordPosition="2519" endWordPosition="2522">e for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Viterbi parse is straightforward (Ney, 1991; Jurafsky and Martin, 2000). The parser can efficiently calculate the Viterbi parse by taking the maximum of the probabilities of the same nonterminal symbol in each cell. With the probabilistic model defined in Section 2, we can also define the Viterbi search for unification-based grammars (Geman and Johnson, 2002). Figure 2 shows the pseudo-code of Viterbi algorithm. The 7r[i, j] represents the set of partial parse results that cover words wi+1, ... , wj, and p[i, j, F] stores the maximum FOM of partial parse result F at cell (i, j). Feature functions are defined over lexical entries and results of rule applications, which correspond to conjunctive nodes in a feature forest. The FOM of a newly created partial parse, F, is computed by summing the values of p of the daughters and an additional FOM of F. The Viterbi algorithm enables various pruning techniques to be used for efficient parsing. Beam thresh</context>
</contexts>
<marker>Geman, Johnson, 2002</marker>
<rawString>Stuart Geman and Mark Johnson. 2002. Dynamic programming for parsing and estimation of stochastic unification-based grammars. In Proc. of ACL&apos;02, pages 279–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Global thresholding and multiple pass parsing.</title>
<date>1997</date>
<booktitle>In Proc. of EMNLP-1997,</booktitle>
<pages>11--25</pages>
<contexts>
<context position="1396" citStr="Goodman, 1997" startWordPosition="195" endWordPosition="196"> global thresholding were not significant, while the quick check and chunk parser greatly contributed to total parsing performance. The precision, recall and average parsing time for the Penn treebank (Section 23) were 87.85%, 86.85%, and 360 ms, respectively. 1 Introduction We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank. We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The exper</context>
<context position="15837" citStr="Goodman, 1997" startWordPosition="2628" endWordPosition="2629"> 2 shows the pseudo-code of Viterbi algorithm. The 7r[i, j] represents the set of partial parse results that cover words wi+1, ... , wj, and p[i, j, F] stores the maximum FOM of partial parse result F at cell (i, j). Feature functions are defined over lexical entries and results of rule applications, which correspond to conjunctive nodes in a feature forest. The FOM of a newly created partial parse, F, is computed by summing the values of p of the daughters and an additional FOM of F. The Viterbi algorithm enables various pruning techniques to be used for efficient parsing. Beam thresholding (Goodman, 1997) is a simple and effective technique for pruning edges during parsing. In each cell of the chart, the method keeps only a portion of the edges which have higher FOMs compared to the other edges in the same cell. 106 procedure BeamThresholding(hw1, ... , wni, hL&apos;, Ri, r,, S, 0) for i = 1 to n foreach Fu ∈ {F|hwi, Fi ∈ L} α = Ei Aifi(Fu) ir[i − 1, i] ← ir[i − 1, i] ∪ {Fu} if (α &gt; p[i − 1, i, Fu]) then p[i − 1, i, Fu] ← α for d = 1 to n for i = 0 to n − d j = i + d for k = i + 1 to j − 1 foreach F3 ∈ ir[i, k], Ft ∈ ir[k, j], r ∈ R if F = r(F3, Ft) has succeeded α = p[i, k, F3] + p[k, j, Ft] + Ei </context>
<context position="18651" citStr="Goodman, 1997" startWordPosition="3235" endWordPosition="3237">ps the edges whose global FOM is greater than αmax−B, where αmax is the highest global FOM in the chart. Figure 3 shows the pseudo-code of local beam search, and global beam search algorithms for probabilistic HPSG parsing. The code for local thresholding is inserted at the end of the computation for each cell. In Figure 3, 7r[i, j]k denotes the k-th element in sorted set 7r[i, j]. We first take the first rc elements that have higher FOMs and then remove the elements with FOMs lower than αmax − S. Global thresholding is also used for pruning edges, and was originally proposed for CFG parsing (Goodman, 1997). It prunes edges based on their global FOM and the best global FOM in the chart. The global FOM of an edge is defined as its FOM plus its forward and backward FOMs, where the forward and backward FOMs are rough estimations of the outside FOM of the edge. The global thresholding is performed immediately after each line of the CYK chart is completed. The forward FOM is calculated first, and then the backward FOM is calculated. Finally, all edges with a global FOM lower than αmax − B are pruned. Figure 3 gives further details of the algorithm. 4.2 Iterative beam thresholding We tested the iterat</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Joshua Goodman. 1997. Global thresholding and multiple pass parsing. In Proc. of EMNLP-1997, pages 11–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Parsing with generative models of predicate-argument structure.</title>
<date>2003</date>
<booktitle>In Proc. of ACL&apos;03,</booktitle>
<pages>359--366</pages>
<contexts>
<context position="25143" citStr="Hockenmaier, 2003" startWordPosition="4477" endWordPosition="4478">nd Tsujii (2005a). Table 1 shows the abbreviations used in presenting the results. We measured the accuracy of the predicateargument relations output by the parser. A predicate-argument relation is defined as a tuple (σ, wh, a, wa), where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Precision/recall is the ratio of tuples correctly identified by the parser. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark and Curran, 2004; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Section 22 of the Treebank was used as the development set, and performance was evaluated using sentences of less than 40 words in Section 23 (2,164 sentences, 20.3 words/sentence). The performance of each parsing technique was analyzed using the sentences in Section 24 of less than 15 words (305 sentences) and less than 40 words (1145 sentences). Table 2 shows the parsing performance using all 0 5 10 15 20 25 30 35 40 45 Sentence length (words) Parsing time (ms) 10000</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Parsing with generative models of predicate-argument structure. In Proc. of ACL&apos;03, pages 359–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based&amp;quot; grammars.</title>
<date>1999</date>
<booktitle>In Proc. of ACL &apos;99,</booktitle>
<pages>535--541</pages>
<contexts>
<context position="6621" citStr="Johnson et al., 1999" startWordPosition="1010" endWordPosition="1013">phrasal sign of the mother. The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set J7 of feature structures, an HPSG is formulated as a tuple, G = (L, R), where L = {l = (w, F)|w E W, F E J7} is a set of lexical entries, and R is a set of schemata, i.e., r E R is a partial function: J7 x J7 → J7. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = (wl, ... , wn) is ! 1 X p(T|w) = exp λifi(T) Z&amp;quot; i !λifi(T �) , Spring where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T. Intuitively, the probability is defined as the normalized product of the weights exp(λi) when a characterist</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based&amp;quot; grammars. In Proc. of ACL &apos;99, pages 535–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dainiel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing.</title>
<date>2000</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="14925" citStr="Jurafsky and Martin, 2000" startWordPosition="2472" endWordPosition="2475">k, 2001; Klein and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Viterbi parse is straightforward (Ney, 1991; Jurafsky and Martin, 2000). The parser can efficiently calculate the Viterbi parse by taking the maximum of the probabilities of the same nonterminal symbol in each cell. With the probabilistic model defined in Section 2, we can also define the Viterbi search for unification-based grammars (Geman and Johnson, 2002). Figure 2 shows the pseudo-code of Viterbi algorithm. The 7r[i, j] represents the set of partial parse results that cover words wi+1, ... , wj, and p[i, j, F] stores the maximum FOM of partial parse result F at cell (i, j). Feature functions are defined over lexical entries and results of rule applications, </context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Dainiel Jurafsky and James H. Martin. 2000. Speech and Language Processing. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>S Riezler</author>
<author>T H King</author>
<author>J T Maxwell</author>
<author>A Vasserman</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL&apos;04.</booktitle>
<contexts>
<context position="1889" citStr="Kaplan et al., 2004" startWordPosition="258" endWordPosition="261">holding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. Unification-based grammars have been extensively studied in terms of linguistic formulation and computation efficiency. Although they provide precise linguistic structures of sentences, their processing is considered expensive because of the detailed descriptions. Since efficiency is of particular concern in practical applications, a number of studies</context>
<context position="6712" citStr="Kaplan et al., 2004" startWordPosition="1028" endWordPosition="1031">lying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set J7 of feature structures, an HPSG is formulated as a tuple, G = (L, R), where L = {l = (w, F)|w E W, F E J7} is a set of lexical entries, and R is a set of schemata, i.e., r E R is a partial function: J7 x J7 → J7. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = (wl, ... , wn) is ! 1 X p(T|w) = exp λifi(T) Z&amp;quot; i !λifi(T �) , Spring where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T. Intuitively, the probability is defined as the normalized product of the weights exp(λi) when a characteristic corresponding to fi appears in parse result T. Model parameters λi are estimated using n</context>
<context position="9445" citStr="Kaplan et al., 2004" startWordPosition="1516" endWordPosition="1519">al symbol of a mother is considered in further processing by ignoring the structure of its daughters. With this assumption, we can compute the figures of merit (FOMs) of partial parse results. This assumption restricts the possibility of feature functions that represent non-local dependencies expressed in a parse result. Since unification-based grammars can express semantic relations, such as predicate-argument relations, in their structure, the assumption unjustifiably restricts the flexibility of probabilistic modeling. However, previous research (Miyao et al., 2003; Clark and Curran, 2004; Kaplan et al., 2004) showed that predicate-argument relations can be represented under the assumption of feature locality. We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CC</context>
<context position="11702" citStr="Kaplan et al., 2004" startWordPosition="1892" endWordPosition="1895">n of quick check, each edge had two types of arrays. One contained the path values of the edge’s sign; we call this the sign array. The other contained the path values of the right daughter of a schema such that its left daughter is unified with the edge’s sign; we call this a schema array. When we apply a schema to two edges, e1 and e2, the schema array of e1 and the sign array of e2 are quickly checked. If it fails, then quick check returns a unification failure. If it succeeds, the signs are unified with the schemata, and the result of unification is returned. Large constituent inhibition (Kaplan et al., 2004) It is unlikely for a large medial edge to contribute to the final parsing result if it spans more than 20 words and is not adjacent to the beginning or ending of the sentence. Large constituent inhibition prevents the parser from generating medial edges that span more than some word length. HPSG parsing with a CFG chunk parser A hybrid of deep parsing and shallow parsing was recently found to improve the efficiency of deep parsing (Daum et al., 2003; Frank et al., 2003; Frank, 2004). As a preprocessor, the shallow parsing must be very fast and achieve high precision but not high recall so tha</context>
</contexts>
<marker>Kaplan, Riezler, King, Maxwell, Vasserman, 2004</marker>
<rawString>R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell III, and A. Vasserman. 2004. Speed and accuracy in shallow and deep stochastic parsing. In Proc. of HLT/NAACL&apos;04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kasper</author>
<author>Hans-Ulrich Krieger</author>
<author>Jorg Spilker</author>
<author>Hans Weber</author>
</authors>
<title>From word hypotheses to logical form: An efficient interleaved approach.</title>
<date>1996</date>
<booktitle>In Proceedings of Natural Language Processing and Speech Technology. Results of the 3rd KONVENS Conference,</booktitle>
<pages>77--88</pages>
<contexts>
<context position="4226" citStr="Kasper et al., 1996" startWordPosition="613" endWordPosition="616">xhaustive parsing without using probabilities and then select the highest probability parse. The behavior of their algorithms is like that of the Viterbi algorithm for PCFG parsing, so the correct parse with the highest probability is guaranteed. The interesting point of this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest. Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al., 1996; Briscoe and Carroll, 1993; Kiefer et al., 2002), and the most probable parse is found by PCFG parsing. This model is based on PCFG and not probabilistic unification-based grammar parsing. Geman and Johnson (Geman and Johnson, 2002) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative par</context>
</contexts>
<marker>Kasper, Krieger, Spilker, Weber, 1996</marker>
<rawString>Walter Kasper, Hans-Ulrich Krieger, Jorg Spilker, and Hans Weber. 1996. From word hypotheses to logical form: An efficient interleaved approach. In Proceedings of Natural Language Processing and Speech Technology. Results of the 3rd KONVENS Conference, pages 77–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Head driven parsing.</title>
<date>1989</date>
<booktitle>In Proc. of IWPT&apos;89,</booktitle>
<pages>52--62</pages>
<contexts>
<context position="14093" citStr="Kay, 1989" startWordPosition="2337" endWordPosition="2338">from the Penn treebank. The principal idea of using the chunk parser is to use the bracket information, i.e., parse trees without non-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 Beam thresholding for HPSG parsing 4.1 Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG</context>
</contexts>
<marker>Kay, 1989</marker>
<rawString>Martin Kay. 1989. Head driven parsing. In Proc. of IWPT&apos;89, pages 52–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Kiefer</author>
<author>Hans-Ulrich Krieger</author>
<author>John Carroll</author>
<author>Robert Malouf</author>
</authors>
<title>A bag of useful techniques for efficient and robust parsing.</title>
<date>1999</date>
<booktitle>In Proc. of ACL&apos;99,</booktitle>
<pages>473--480</pages>
<contexts>
<context position="10269" citStr="Kiefer et al., 1999" startWordPosition="1644" endWordPosition="1647">e parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification process. Quick check quickly judges their un</context>
</contexts>
<marker>Kiefer, Krieger, Carroll, Malouf, 1999</marker>
<rawString>Bernd Kiefer, Hans-Ulrich Krieger, John Carroll, and Robert Malouf. 1999. A bag of useful techniques for efficient and robust parsing. In Proc. of ACL&apos;99, pages 473–480, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Kiefer</author>
<author>Hans-Ulrich Krieger</author>
<author>Detlef Prescher</author>
</authors>
<title>A novel disambiguation method for unification-based grammars using probabilistic context-free approximations.</title>
<date>2002</date>
<booktitle>In Proc. of COLING</booktitle>
<contexts>
<context position="4275" citStr="Kiefer et al., 2002" startWordPosition="621" endWordPosition="624">d then select the highest probability parse. The behavior of their algorithms is like that of the Viterbi algorithm for PCFG parsing, so the correct parse with the highest probability is guaranteed. The interesting point of this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest. Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al., 1996; Briscoe and Carroll, 1993; Kiefer et al., 2002), and the most probable parse is found by PCFG parsing. This model is based on PCFG and not probabilistic unification-based grammar parsing. Geman and Johnson (Geman and Johnson, 2002) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-s</context>
</contexts>
<marker>Kiefer, Krieger, Prescher, 2002</marker>
<rawString>Bernd Kiefer, Hans-Ulrich Krieger, and Detlef Prescher. 2002. A novel disambiguation method for unification-based grammars using probabilistic context-free approximations. In Proc. of COLING 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* parsing: Fast exact viterbi parse selection.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL&apos;03.</booktitle>
<contexts>
<context position="14332" citStr="Klein and Manning, 2003" startWordPosition="2373" endWordPosition="2376"> Beam thresholding for HPSG parsing 4.1 Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Viterbi parse is straightforward (Ney, 1991; Jurafsky and Martin, 2000). The p</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. A* parsing: Fast exact viterbi parse selection. In Proc. of HLT-NAACL&apos;03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars.</title>
<date>2004</date>
<booktitle>In Proc. of IJCNLP-04 Workshop `Beyond Shallow Analyses”.</booktitle>
<marker>Malouf, van Noord, 2004</marker>
<rawString>Robert Malouf and Gertjan van Noord. 2004. Wide coverage parsing with stochastic attribute value grammars. In Proc. of IJCNLP-04 Workshop `Beyond Shallow Analyses”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>John Carroll</author>
<author>Ann Copestake</author>
</authors>
<title>Efficient feature structure operations without compilation.</title>
<date>2000</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="1837" citStr="Malouf et al., 2000" startWordPosition="251" endWordPosition="254">g for the Penn treebank. We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. Unification-based grammars have been extensively studied in terms of linguistic formulation and computation efficiency. Although they provide precise linguistic structures of sentences, their processing is considered expensive because of the detailed descriptions. Since efficiency is of particular co</context>
<context position="10290" citStr="Malouf et al., 2000" startWordPosition="1648" endWordPosition="1651">chniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification process. Quick check quickly judges their unifiability by peeping</context>
</contexts>
<marker>Malouf, Carroll, Copestake, 2000</marker>
<rawString>Robert Malouf, John Carroll, and Ann Copestake. 2000. Efficient feature structure operations without compilation. Journal of Natural Language Engineering, 6(1):29–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proc. of CoNLL-2002,</booktitle>
<pages>49--55</pages>
<contexts>
<context position="7748" citStr="Malouf, 2002" startWordPosition="1247" endWordPosition="1248">bility is defined as the normalized product of the weights exp(λi) when a characteristic corresponding to fi appears in parse result T. Model parameters λi are estimated using numerhas come HEAD verb HEAD noun 1 SUBJ &lt; &gt; COMPS &lt; &gt; SUBJ &lt; COMPS &lt; &gt; has come Spring head-comp HEAD noun SUBJ &lt; &gt; COMPS &lt; &gt; HEAD verb SUBJ &lt; &gt; 1 COMPS &lt; &gt; 2 HEAD verb 2 SUBJ &lt; &gt; 1 COMPS &lt; &gt; HEAD verb SUBJ &lt; &gt; COMPS &lt; &gt; subject-head HEAD verb SUBJ &lt; &gt; 1 COMPS &lt; &gt; head-comp HEAD noun HEAD verb SUBJ &lt; &gt; SUBJ &lt; &gt; COMPS &lt; &gt; COMPS &lt; 2 HEAD verb 2 SUBJ &lt; &gt; 1 COMPS &lt; &gt; 1&gt; &gt; �� 1 XZ&amp;quot; = Xexp T1 i 104 ical optimization methods (Malouf, 2002) so as to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T|w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T|w). They assumed that features are functions o</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proc. of CoNLL-2002, pages 49–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="5110" citStr="Marcus et al., 1994" startWordPosition="747" endWordPosition="750"> algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-scale corpora, the Penn treebank. We show how techniques developed for efficient deep parsing can improve the efficiency of probabilistic parsing. These techniques were evaluated in experiments on the Penn Treebank (Marcus et al., 1994) with the wide-coverage HPSG parser developed by Miyao et al. (Miyao et al., 2005; Miyao and Tsujii, 2005). 2 HPSG and probabilistic models HPSG (Pollard and Sag, 1994) is a syntactic theory based on lexicalized grammar formalism. In HPSG, a small number of schemata describe general construction rules, and a large number of lexical entries express word-specific characteristics. The structures of sentences are explained using combinations of schemata and lexical entries. Both schemata and lexical entries are represented by typed feature structures, and constraints represented by feature structu</context>
<context position="23929" citStr="Marcus et al., 1994" startWordPosition="4265" endWordPosition="4268">000 1000 100 10 1 P—i�g time (me) 100000000 10000000 1000000 100000 10000 1000 100 10 1 0 2 4 6 6 10 12 14 16 Sentence length (ward.) Figure 7: Parsing time for the sentences in Section 24 of less than 15 words of Viterbi parsing (none) (Left) and iterative parsing (iterative) (Right) Figure 6: Parsing time versus sentence length for the sentences in Section 23 of less than 40 words 5 Evaluation We evaluated the efficiency of the parsing techniques by using the HPSG for English developed by Miyao et al. (2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 2,284 lexical entry templates for 10,536 words&apos;. The probabilistic disambiguation model of the grammar was trained using the same portion of the treebank (Miyao and Tsujii, 2005). &apos;Lexical entry templates for POS are also developed. They are assigned to unknown words. The model included 529,856 features. The parameters for beam searching were determined manually by trial and error using Section 22; δ0 = 12, Aδ = 6, δlast = 30, κ0 = 6.0, Aκ = 3.0, κlast = 15.1, θ0 = 8.0, Aθ = 4.0, and θlast = 20.1. We used the chunk parser developed by Tsuruoka and </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Hozumi Tanaka</author>
<author>Hideki Hirakawa</author>
<author>Hideo Miyoshi</author>
<author>Hideki Yasukawa</author>
</authors>
<title>BUP: A bottom up parser embedded in Prolog.</title>
<date>1983</date>
<journal>New Generation Computing,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="10205" citStr="Matsumoto et al., 1983" startWordPosition="1633" endWordPosition="1636">ture functions and exploited it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle</context>
</contexts>
<marker>Matsumoto, Tanaka, Hirakawa, Miyoshi, Yasukawa, 1983</marker>
<rawString>Yuji Matsumoto, Hozumi Tanaka, Hideki Hirakawa, Hideo Miyoshi, and Hideki Yasukawa. 1983. BUP: A bottom up parser embedded in Prolog. New Generation Computing, 1(2):145–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Maxwell</author>
<author>Ron Kaplan</author>
</authors>
<title>The interface between phrasal and functional constraints.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="10231" citStr="Maxwell and Kaplan, 1993" startWordPosition="1637" endWordPosition="1640">ited it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification proces</context>
</contexts>
<marker>Maxwell, Kaplan, 1993</marker>
<rawString>John Maxwell and Ron Kaplan. 1993. The interface between phrasal and functional constraints. Computational Linguistics, 19(4):571–589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proc. of HLT</booktitle>
<pages>292--297</pages>
<contexts>
<context position="8241" citStr="Miyao and Tsujii, 2002" startWordPosition="1325" endWordPosition="1328">BJ &lt; &gt; COMPS &lt; &gt; COMPS &lt; 2 HEAD verb 2 SUBJ &lt; &gt; 1 COMPS &lt; &gt; 1&gt; &gt; �� 1 XZ&amp;quot; = Xexp T1 i 104 ical optimization methods (Malouf, 2002) so as to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T|w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T|w). They assumed that features are functions on nodes in a packed parse forest. That is, parse tree T is represented by a set of nodes, i.e., T = {c}, and the parse forest is represented by an and/or graph of the nodes. From this assumption, we can redefine the probability as p(T|w) = 1Z exp Z.,,, cET !Aifi(c) � A packed parse forest has a structure similar to a chart of CFG parsing, and c corresponds to an edge in the chart. This assumption corresponds to the independence assumption in PCFG; that is, only a nonterminal symbol of a m</context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Yusuke Miyao and Jun&apos;ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proc. of HLT 2002, pages 292–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proc. of ACL&apos;05,</booktitle>
<pages>83--90</pages>
<contexts>
<context position="2886" citStr="Miyao and Tsujii, 2005" startWordPosition="406" endWordPosition="409">h they provide precise linguistic structures of sentences, their processing is considered expensive because of the detailed descriptions. Since efficiency is of particular concern in practical applications, a number of studies have focused on improving the parsing efficiency of unificationbased grammars (Oepen et al., 2002). Although significant improvements in efficiency have been made, parsing speed is still not high enough for practical applications. The recent introduction of probabilistic models of wide-coverage unification-based grammars (Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) has opened up the novel possibility of increasing parsing speed by guiding the search path using probabilities. That is, since we often require only the most probable parse result, we can compute partial parse results that are likely to contribute to the final parse result. This approach has been extensively studied in the field of probabilistic 103 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 103–114, Vancouver, October 2005. c�2005 Association for Computational Linguistics CFG (PCFG) parsing, such as Viterbi parsing and beam thresholding. While many </context>
<context position="5216" citStr="Miyao and Tsujii, 2005" startWordPosition="766" endWordPosition="769">grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-scale corpora, the Penn treebank. We show how techniques developed for efficient deep parsing can improve the efficiency of probabilistic parsing. These techniques were evaluated in experiments on the Penn Treebank (Marcus et al., 1994) with the wide-coverage HPSG parser developed by Miyao et al. (Miyao et al., 2005; Miyao and Tsujii, 2005). 2 HPSG and probabilistic models HPSG (Pollard and Sag, 1994) is a syntactic theory based on lexicalized grammar formalism. In HPSG, a small number of schemata describe general construction rules, and a large number of lexical entries express word-specific characteristics. The structures of sentences are explained using combinations of schemata and lexical entries. Both schemata and lexical entries are represented by typed feature structures, and constraints represented by feature structures are checked with unification. Figure 1 shows an example of HPSG parsing of the sentence “Spring has co</context>
<context position="6737" citStr="Miyao and Tsujii, 2005" startWordPosition="1032" endWordPosition="1035">ical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set J7 of feature structures, an HPSG is formulated as a tuple, G = (L, R), where L = {l = (w, F)|w E W, F E J7} is a set of lexical entries, and R is a set of schemata, i.e., r E R is a partial function: J7 x J7 → J7. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = (wl, ... , wn) is ! 1 X p(T|w) = exp λifi(T) Z&amp;quot; i !λifi(T �) , Spring where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T. Intuitively, the probability is defined as the normalized product of the weights exp(λi) when a characteristic corresponding to fi appears in parse result T. Model parameters λi are estimated using numerhas come HEAD verb HE</context>
<context position="24153" citStr="Miyao and Tsujii, 2005" startWordPosition="4299" endWordPosition="4302"> parsing (none) (Left) and iterative parsing (iterative) (Right) Figure 6: Parsing time versus sentence length for the sentences in Section 23 of less than 40 words 5 Evaluation We evaluated the efficiency of the parsing techniques by using the HPSG for English developed by Miyao et al. (2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 2,284 lexical entry templates for 10,536 words&apos;. The probabilistic disambiguation model of the grammar was trained using the same portion of the treebank (Miyao and Tsujii, 2005). &apos;Lexical entry templates for POS are also developed. They are assigned to unknown words. The model included 529,856 features. The parameters for beam searching were determined manually by trial and error using Section 22; δ0 = 12, Aδ = 6, δlast = 30, κ0 = 6.0, Aκ = 3.0, κlast = 15.1, θ0 = 8.0, Aθ = 4.0, and θlast = 20.1. We used the chunk parser developed by Tsuruoka and Tsujii (2005a). Table 1 shows the abbreviations used in presenting the results. We measured the accuracy of the predicateargument relations output by the parser. A predicate-argument relation is defined as a tuple (σ, wh, a,</context>
</contexts>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Jun&apos;ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proc. of ACL&apos;05, pages 83–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Probabilistic modeling of argument structures including non-local dependencies.</title>
<date>2003</date>
<booktitle>In Proc. of RANLP &apos;03,</booktitle>
<pages>285--291</pages>
<contexts>
<context position="6663" citStr="Miyao et al., 2003" startWordPosition="1018" endWordPosition="1021">larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set J7 of feature structures, an HPSG is formulated as a tuple, G = (L, R), where L = {l = (w, F)|w E W, F E J7} is a set of lexical entries, and R is a set of schemata, i.e., r E R is a partial function: J7 x J7 → J7. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = (wl, ... , wn) is ! 1 X p(T|w) = exp λifi(T) Z&amp;quot; i !λifi(T �) , Spring where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T. Intuitively, the probability is defined as the normalized product of the weights exp(λi) when a characteristic corresponding to fi appears in parse re</context>
<context position="9399" citStr="Miyao et al., 2003" startWordPosition="1508" endWordPosition="1511">ssumption in PCFG; that is, only a nonterminal symbol of a mother is considered in further processing by ignoring the structure of its daughters. With this assumption, we can compute the figures of merit (FOMs) of partial parse results. This assumption restricts the possibility of feature functions that represent non-local dependencies expressed in a parse result. Since unification-based grammars can express semantic relations, such as predicate-argument relations, in their structure, the assumption unjustifiably restricts the flexibility of probabilistic modeling. However, previous research (Miyao et al., 2003; Clark and Curran, 2004; Kaplan et al., 2004) showed that predicate-argument relations can be represented under the assumption of feature locality. We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sa</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2003</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Jun&apos;ichi Tsujii. 2003. Probabilistic modeling of argument structures including non-local dependencies. In Proc. of RANLP &apos;03, pages 285–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Keh-Yih Su, Jun&apos;ichi Tsujii, Jong-Hyeok Lee and Oi Yee Kwong (Eds.), Natural Language Processing - IJCNLP</title>
<date>2005</date>
<pages>684--693</pages>
<publisher>SpringerVerlag.</publisher>
<contexts>
<context position="5191" citStr="Miyao et al., 2005" startWordPosition="762" endWordPosition="765">y unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-scale corpora, the Penn treebank. We show how techniques developed for efficient deep parsing can improve the efficiency of probabilistic parsing. These techniques were evaluated in experiments on the Penn Treebank (Marcus et al., 1994) with the wide-coverage HPSG parser developed by Miyao et al. (Miyao et al., 2005; Miyao and Tsujii, 2005). 2 HPSG and probabilistic models HPSG (Pollard and Sag, 1994) is a syntactic theory based on lexicalized grammar formalism. In HPSG, a small number of schemata describe general construction rules, and a large number of lexical entries express word-specific characteristics. The structures of sentences are explained using combinations of schemata and lexical entries. Both schemata and lexical entries are represented by typed feature structures, and constraints represented by feature structures are checked with unification. Figure 1 shows an example of HPSG parsing of th</context>
<context position="23824" citStr="Miyao et al. (2005)" startWordPosition="4246" endWordPosition="4249"> 360 15 0 2 4 6 6 10 12 14 16 Sent.n�� kngth (—d.) Pusi�g time (me) 100000000 10000000 1000000 100000 10000 1000 100 10 1 P—i�g time (me) 100000000 10000000 1000000 100000 10000 1000 100 10 1 0 2 4 6 6 10 12 14 16 Sentence length (ward.) Figure 7: Parsing time for the sentences in Section 24 of less than 15 words of Viterbi parsing (none) (Left) and iterative parsing (iterative) (Right) Figure 6: Parsing time versus sentence length for the sentences in Section 23 of less than 40 words 5 Evaluation We evaluated the efficiency of the parsing techniques by using the HPSG for English developed by Miyao et al. (2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 2,284 lexical entry templates for 10,536 words&apos;. The probabilistic disambiguation model of the grammar was trained using the same portion of the treebank (Miyao and Tsujii, 2005). &apos;Lexical entry templates for POS are also developed. They are assigned to unknown words. The model included 529,856 features. The parameters for beam searching were determined manually by trial and error using Section 22; δ0 = 12, Aδ = 6, δlast = 30, κ0 = 6.0, Aκ = 3.0</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2005</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Jun&apos;ichi Tsujii, 2005. Keh-Yih Su, Jun&apos;ichi Tsujii, Jong-Hyeok Lee and Oi Yee Kwong (Eds.), Natural Language Processing - IJCNLP 2004 LNAI 3248, chapter Corpus-oriented Grammar Development for Acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank, pages 684–693. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Practical experiments with regular approximation of context-free languages.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="13933" citStr="Nederhof, 2000" startWordPosition="2314" endWordPosition="2315">on. The grammar for the chunk parser is automatically extracted from the CFG treebank translated from the HPSG treebank, which is generated during grammar extraction from the Penn treebank. The principal idea of using the chunk parser is to use the bracket information, i.e., parse trees without non-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 Beam thresholding for HPSG parsing 4.1 Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, </context>
</contexts>
<marker>Nederhof, 2000</marker>
<rawString>Mark-Jan Nederhof. 2000. Practical experiments with regular approximation of context-free languages. Computational Linguistics, 26(1):17–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
</authors>
<title>Dynamic programming parsing for context-free grammars in continuous speech recognition.</title>
<date>1991</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="14897" citStr="Ney, 1991" startWordPosition="2470" endWordPosition="2471"> 2000; Roark, 2001; Klein and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Viterbi parse is straightforward (Ney, 1991; Jurafsky and Martin, 2000). The parser can efficiently calculate the Viterbi parse by taking the maximum of the probabilities of the same nonterminal symbol in each cell. With the probabilistic model defined in Section 2, we can also define the Viterbi search for unification-based grammars (Geman and Johnson, 2002). Figure 2 shows the pseudo-code of Viterbi algorithm. The 7r[i, j] represents the set of partial parse results that cover words wi+1, ... , wj, and p[i, j, F] stores the maximum FOM of partial parse result F at cell (i, j). Feature functions are defined over lexical entries and re</context>
</contexts>
<marker>Ney, 1991</marker>
<rawString>H. Ney. 1991. Dynamic programming parsing for context-free grammars in continuous speech recognition. IEEE Transactions on Signal Processing, 39(2):336–340.</rawString>
</citation>
<citation valid="true">
<title>Collaborative Language Engineering: A Case Study in Efficient Grammar-Based Processing.</title>
<date>2002</date>
<editor>Stephan Oepen, Dan Flickinger, Jun&apos;ichi Tsujii, and Hans Uszkoreit, editors.</editor>
<publisher>CSLI Publications.</publisher>
<marker>2002</marker>
<rawString>Stephan Oepen, Dan Flickinger, Jun&apos;ichi Tsujii, and Hans Uszkoreit, editors. 2002. Collaborative Language Engineering: A Case Study in Efficient Grammar-Based Processing. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Penn</author>
<author>Cosmin Munteanu</author>
</authors>
<title>A tabulation-based parsing method that reduces copying.</title>
<date>2003</date>
<booktitle>In Proc. of ACL&apos;03).</booktitle>
<contexts>
<context position="10359" citStr="Penn and Munteanu, 2003" startWordPosition="1660" endWordPosition="1663">mproving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification process. Quick check quickly judges their unifiability by peeping the values of the given paths. If one of the path values is not unif</context>
</contexts>
<marker>Penn, Munteanu, 2003</marker>
<rawString>Gerald Penn and Cosmin Munteanu. 2003. A tabulation-based parsing method that reduces copying. In Proc. of ACL&apos;03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="5278" citStr="Pollard and Sag, 1994" startWordPosition="776" endWordPosition="779">of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-scale corpora, the Penn treebank. We show how techniques developed for efficient deep parsing can improve the efficiency of probabilistic parsing. These techniques were evaluated in experiments on the Penn Treebank (Marcus et al., 1994) with the wide-coverage HPSG parser developed by Miyao et al. (Miyao et al., 2005; Miyao and Tsujii, 2005). 2 HPSG and probabilistic models HPSG (Pollard and Sag, 1994) is a syntactic theory based on lexicalized grammar formalism. In HPSG, a small number of schemata describe general construction rules, and a large number of lexical entries express word-specific characteristics. The structures of sentences are explained using combinations of schemata and lexical entries. Both schemata and lexical entries are represented by typed feature structures, and constraints represented by feature structures are checked with unification. Figure 1 shows an example of HPSG parsing of the sentence “Spring has come.” First, each of the lexical entries for “has&amp;quot; and “come” i</context>
<context position="10007" citStr="Pollard and Sag, 1994" startWordPosition="1601" endWordPosition="1604">o et al., 2003; Clark and Curran, 2004; Kaplan et al., 2004) showed that predicate-argument relations can be represented under the assumption of feature locality. We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf e</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="14277" citStr="Ratnaparkhi, 1999" startWordPosition="2366" endWordPosition="2368">ser from generating edges that cross brackets. 4 Beam thresholding for HPSG parsing 4.1 Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Viterbi parse is straig</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34(1-3):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Jonas Kuhn</author>
<author>Mark Johnson</author>
</authors>
<title>Lexicalized stochastic modeling of constraint-based grammars using loglinear measures and EM training.</title>
<date>2000</date>
<booktitle>In Proc. of ACL&apos;00,</booktitle>
<pages>480--487</pages>
<contexts>
<context position="6643" citStr="Riezler et al., 2000" startWordPosition="1014" endWordPosition="1017">ther. The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set J7 of feature structures, an HPSG is formulated as a tuple, G = (L, R), where L = {l = (w, F)|w E W, F E J7} is a set of lexical entries, and R is a set of schemata, i.e., r E R is a partial function: J7 x J7 → J7. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = (wl, ... , wn) is ! 1 X p(T|w) = exp λifi(T) Z&amp;quot; i !λifi(T �) , Spring where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T. Intuitively, the probability is defined as the normalized product of the weights exp(λi) when a characteristic corresponding to fi</context>
</contexts>
<marker>Riezler, Prescher, Kuhn, Johnson, 2000</marker>
<rawString>Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark Johnson. 2000. Lexicalized stochastic modeling of constraint-based grammars using loglinear measures and EM training. In Proc. of ACL&apos;00, pages 480–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="14306" citStr="Roark, 2001" startWordPosition="2371" endWordPosition="2372">s brackets. 4 Beam thresholding for HPSG parsing 4.1 Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Viterbi parse is straightforward (Ney, 1991; Jurafsk</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel J Rosenkrantz</author>
<author>Philip M Lewis</author>
</authors>
<title>Deterministic left corner parsing.</title>
<date>1970</date>
<booktitle>In IEEE Conference Record of the 11th Annual Symposium on Switching and Automata Theory,</booktitle>
<pages>139--152</pages>
<marker>Rosenkrantz, Lewis, 1970</marker>
<rawString>Daniel J. Rosenkrantz and Philip M. Lewis II. 1970. Deterministic left corner parsing. In IEEE Conference Record of the 11th Annual Symposium on Switching and Automata Theory, pages 139–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Shabes</author>
<author>Anne Abeille</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing strategies with `lexicalized&apos; grammars: Application to tree adjoining grammars.</title>
<date>1988</date>
<booktitle>In Proc. of COLING&apos;88,</booktitle>
<pages>578--583</pages>
<contexts>
<context position="9977" citStr="Shabes et al., 1988" startWordPosition="1596" endWordPosition="1599">ver, previous research (Miyao et al., 2003; Clark and Curran, 2004; Kaplan et al., 2004) showed that predicate-argument relations can be represented under the assumption of feature locality. We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiabl</context>
</contexts>
<marker>Shabes, Abeille, Joshi, 1988</marker>
<rawString>Yves Shabes, Anne Abeille, and Aravind K. Joshi. 1988. Parsing strategies with `lexicalized&apos; grammars: Application to tree adjoining grammars. In Proc. of COLING&apos;88, pages 578–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="10064" citStr="Steedman, 2000" startWordPosition="1610" endWordPosition="1612">wed that predicate-argument relations can be represented under the assumption of feature locality. We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature s</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language.</title>
<date>1986</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="13916" citStr="Tomita, 1986" startWordPosition="2312" endWordPosition="2313">e high precision. The grammar for the chunk parser is automatically extracted from the CFG treebank translated from the HPSG treebank, which is generated during grammar extraction from the Penn treebank. The principal idea of using the chunk parser is to use the bracket information, i.e., parse trees without non-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 Beam thresholding for HPSG parsing 4.1 Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. Th</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>Masaru Tomita. 1986. Efficient Parsing for Natural Language. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Torisawa</author>
<author>Kenji Nishida</author>
<author>Yusuke Miyao</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>An HPSG parser with CFG filtering.</title>
<date>2000</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="10313" citStr="Torisawa et al., 2000" startWordPosition="1652" endWordPosition="1655">t deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification process. Quick check quickly judges their unifiability by peeping the values of the give</context>
</contexts>
<marker>Torisawa, Nishida, Miyao, Tsujii, 2000</marker>
<rawString>Kentaro Torisawa, Kenji Nishida, Yusuke Miyao, and Jun&apos;ichi Tsujii. 2000. An HPSG parser with CFG filtering. Journal of Natural Language Engineering, 6(1):63–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Chunk parsing revisited.</title>
<date>2005</date>
<booktitle>In Proc. of IWPT-2005.</booktitle>
<contexts>
<context position="1446" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="200" endWordPosition="203">nt, while the quick check and chunk parser greatly contributed to total parsing performance. The precision, recall and average parsing time for the Penn treebank (Section 23) were 87.85%, 86.85%, and 360 ms, respectively. 1 Introduction We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank. We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to th</context>
<context position="13185" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="2201" endWordPosition="2204">o n for i = 0 to n − d j = i + d for k = i + 1 to j − 1 foreach Fs E ir[i, k], Ft E ir[k, j], r E R if F = r(Fs, Ft) has succeeded α = p[i, k, Fs] + p[k, j, Ft] + Ei Aifi(F) ir[i, j] — ir[i, j] U {F} if (α &gt; p[i, j, F]) then p[i, j, F] — α Figure 2: Pseudo-code of Viterbi algorithms for probabilistic HPSG parsing total parsing performance in terms of precision, recall and speed is not degraded. Because there is trade-off between speed and accuracy in this approach, the total parsing performance for large-scale corpora like the Penn treebank should be measured. We introduce a CFG chunk parser (Tsuruoka and Tsujii, 2005a) as a preprocessor of HPSG parsing. Chunk parsers meet the requirements for preprocessors; they are very fast and have high precision. The grammar for the chunk parser is automatically extracted from the CFG treebank translated from the HPSG treebank, which is generated during grammar extraction from the Penn treebank. The principal idea of using the chunk parser is to use the bracket information, i.e., parse trees without non-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 Beam thresholding for HPSG parsing 4.1 Simple beam thresholding Many algorit</context>
<context position="19310" citStr="Tsuruoka and Tsujii (2005" startWordPosition="3351" endWordPosition="3354">lobal FOM and the best global FOM in the chart. The global FOM of an edge is defined as its FOM plus its forward and backward FOMs, where the forward and backward FOMs are rough estimations of the outside FOM of the edge. The global thresholding is performed immediately after each line of the CYK chart is completed. The forward FOM is calculated first, and then the backward FOM is calculated. Finally, all edges with a global FOM lower than αmax − B are pruned. Figure 3 gives further details of the algorithm. 4.2 Iterative beam thresholding We tested the iterative beam thresholding proposed by Tsuruoka and Tsujii (2005b). We started the parsing with a narrow beam. If the parser output results, they were taken as the final parse results. If the parser did not output any results, we widened the Table 1: Abbreviations used in experimental results num local beam thresholding by number width local beam thresholding by width global global beam thresholding by width iterative iterative parsing with local beam thresholding by number and width chp parsing with CFC chunk parser beam, and reran the parsing. We continued widening the beam until the parser output results or the beam width reached some limit. The pseudo-</context>
<context position="24541" citStr="Tsuruoka and Tsujii (2005" startWordPosition="4373" endWordPosition="4377">et al., 1994) (39,832 sentences). The grammar consisted of 2,284 lexical entry templates for 10,536 words&apos;. The probabilistic disambiguation model of the grammar was trained using the same portion of the treebank (Miyao and Tsujii, 2005). &apos;Lexical entry templates for POS are also developed. They are assigned to unknown words. The model included 529,856 features. The parameters for beam searching were determined manually by trial and error using Section 22; δ0 = 12, Aδ = 6, δlast = 30, κ0 = 6.0, Aκ = 3.0, κlast = 15.1, θ0 = 8.0, Aθ = 4.0, and θlast = 20.1. We used the chunk parser developed by Tsuruoka and Tsujii (2005a). Table 1 shows the abbreviations used in presenting the results. We measured the accuracy of the predicateargument relations output by the parser. A predicate-argument relation is defined as a tuple (σ, wh, a, wa), where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Precision/recall is the ratio of tuples correctly identified by the parser. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 20</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun&apos;ichi Tsujii. 2005a. Chunk parsing revisited. In Proc. of IWPT-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Keh-Yih Su, Jun&apos;ichi Tsujii, Jong-Hyeok Lee and Oi Yee Kwong (Eds.), Natural Language Processing - IJCNLP</title>
<date>2005</date>
<pages>52--60</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1446" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="200" endWordPosition="203">nt, while the quick check and chunk parser greatly contributed to total parsing performance. The precision, recall and average parsing time for the Penn treebank (Section 23) were 87.85%, 86.85%, and 360 ms, respectively. 1 Introduction We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank. We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to th</context>
<context position="13185" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="2201" endWordPosition="2204">o n for i = 0 to n − d j = i + d for k = i + 1 to j − 1 foreach Fs E ir[i, k], Ft E ir[k, j], r E R if F = r(Fs, Ft) has succeeded α = p[i, k, Fs] + p[k, j, Ft] + Ei Aifi(F) ir[i, j] — ir[i, j] U {F} if (α &gt; p[i, j, F]) then p[i, j, F] — α Figure 2: Pseudo-code of Viterbi algorithms for probabilistic HPSG parsing total parsing performance in terms of precision, recall and speed is not degraded. Because there is trade-off between speed and accuracy in this approach, the total parsing performance for large-scale corpora like the Penn treebank should be measured. We introduce a CFG chunk parser (Tsuruoka and Tsujii, 2005a) as a preprocessor of HPSG parsing. Chunk parsers meet the requirements for preprocessors; they are very fast and have high precision. The grammar for the chunk parser is automatically extracted from the CFG treebank translated from the HPSG treebank, which is generated during grammar extraction from the Penn treebank. The principal idea of using the chunk parser is to use the bracket information, i.e., parse trees without non-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 Beam thresholding for HPSG parsing 4.1 Simple beam thresholding Many algorit</context>
<context position="19310" citStr="Tsuruoka and Tsujii (2005" startWordPosition="3351" endWordPosition="3354">lobal FOM and the best global FOM in the chart. The global FOM of an edge is defined as its FOM plus its forward and backward FOMs, where the forward and backward FOMs are rough estimations of the outside FOM of the edge. The global thresholding is performed immediately after each line of the CYK chart is completed. The forward FOM is calculated first, and then the backward FOM is calculated. Finally, all edges with a global FOM lower than αmax − B are pruned. Figure 3 gives further details of the algorithm. 4.2 Iterative beam thresholding We tested the iterative beam thresholding proposed by Tsuruoka and Tsujii (2005b). We started the parsing with a narrow beam. If the parser output results, they were taken as the final parse results. If the parser did not output any results, we widened the Table 1: Abbreviations used in experimental results num local beam thresholding by number width local beam thresholding by width global global beam thresholding by width iterative iterative parsing with local beam thresholding by number and width chp parsing with CFC chunk parser beam, and reran the parsing. We continued widening the beam until the parser output results or the beam width reached some limit. The pseudo-</context>
<context position="24541" citStr="Tsuruoka and Tsujii (2005" startWordPosition="4373" endWordPosition="4377">et al., 1994) (39,832 sentences). The grammar consisted of 2,284 lexical entry templates for 10,536 words&apos;. The probabilistic disambiguation model of the grammar was trained using the same portion of the treebank (Miyao and Tsujii, 2005). &apos;Lexical entry templates for POS are also developed. They are assigned to unknown words. The model included 529,856 features. The parameters for beam searching were determined manually by trial and error using Section 22; δ0 = 12, Aδ = 6, δlast = 30, κ0 = 6.0, Aκ = 3.0, κlast = 15.1, θ0 = 8.0, Aθ = 4.0, and θlast = 20.1. We used the chunk parser developed by Tsuruoka and Tsujii (2005a). Table 1 shows the abbreviations used in presenting the results. We measured the accuracy of the predicateargument relations output by the parser. A predicate-argument relation is defined as a tuple (σ, wh, a, wa), where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Precision/recall is the ratio of tuples correctly identified by the parser. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 20</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun&apos;ichi Tsujii, 2005b. Keh-Yih Su, Jun&apos;ichi Tsujii, Jong-Hyeok Lee and Oi Yee Kwong (Eds.), Natural Language Processing - IJCNLP 2004 LNAI 3248, chapter Iterative CKY Parsing for Probabilistic Context-Free Grammars, pages 52–60. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>An efficient implementation of the head-corner parser.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<marker>van Noord, 1997</marker>
<rawString>Gertjan van Noord. 1997. An efficient implementation of the head-corner parser. Computational Linguistics, 23(3):425–456.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>