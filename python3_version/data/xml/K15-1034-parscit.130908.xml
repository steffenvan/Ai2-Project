<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.7839075">
Feature Selection for Short Text Classification using Wavelet Packet
Transform
</title>
<author confidence="0.977591">
Anuj Mahajan
</author>
<affiliation confidence="0.985761">
Indian Institute of Technology, Delhi
</affiliation>
<author confidence="0.447325">
Hauz Khas, New Delhi 110016
</author>
<affiliation confidence="0.719464">
India
</affiliation>
<email confidence="0.989752">
anujmahajan.iitd@gmail.com
</email>
<author confidence="0.655199">
Sharmistha, Shourya Roy
</author>
<affiliation confidence="0.654891">
Xerox Research Centre India
</affiliation>
<address confidence="0.4891505">
Bangalore - 560103
India
</address>
<email confidence="0.986296">
Isharmistha.jat,shourya.royl@xerox.com
</email>
<sectionHeader confidence="0.997053" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941727272727">
Text classification tasks suffer from curse of
dimensionality due to large feature space.
Short text data further exacerbates the prob-
lem due to their sparse and noisy nature. Fea-
ture selection thus becomes an important step
in improving the classification performance.
In this paper, we propose a novel feature se-
lection method using Wavelet Packet Trans-
form. Wavelet Packet Transform (WPT) has
been used widely in various fields due to its
efficiency in encoding transient signals. We
demonstrate how short text classification task
can be benefited by feature selection using
WPT due to their sparse nature. Our technique
chooses the most discriminating features by
computing inter-class distances in the trans-
formed space. We experimented extensively
with several short text datasets. Compared to
well known techniques our approach reduces
the feature space size and improves the overall
classification performance significantly in all
the datasets.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999529611111111">
Text classification task consists of assigning a docu-
ment to one or more classes. This can be done us-
ing machine learning techniques by training a model
with labelled documents. Documents are usually repre-
sented as vectors with a variety of techniques like bag-
of-words(unigram, bigram), TFIDF representation, etc.
Typically, text corpora have very high dimensional
document representation equal to the size of vocabu-
lary. This leads to curse of dimensionality1 in machine
learning models, thereby degrading the performance.
Short text corpora, like SMS, tweets, etc., in partic-
ular suffer from sparse high dimensional feature space,
due to large vocabulary and short document length. To
give an idea as to how these factors affect the size
of the feature space we compare Reuters with Twitter
data corpus. In Reuters-21578 corpus there are approx-
imately 2.5 Million words in total and 14506 unique
vocabulary entries after standard preprocessing steps
</bodyText>
<footnote confidence="0.826835">
1https://en.wikipedia.org/wiki/Curse_
of_dimensionality
</footnote>
<bodyText confidence="0.999933076923077">
(which is the dimensionality of the feature space).
However, Twitter 1 corpus, we used for experiments
has approximately 15,000 words in total and feature
space size of 7423 words. Additionally, the average
length of an English tweet is 10-12 words whereas the
average length of a document in Reuters-21578 news
classification corpus is 200 words. Therefore, the di-
mensionality is extremely high even for small corpora
with short texts. In addition, the average number of
words in a document is significantly less in short text
data leading to higher sparsity of feature space repre-
sentation of documents.
Owing to this high dimensionality problem, one of
the important steps in text classification workflows is
feature selection. Feature selection techniques for tradi-
tional documents have been aplenty and a few seminal
survey articles have been written on this topic (Blitzer,
2008). In contrast, for short text there is much less
work on statistical feature selection but more focus has
gone to feature engineering towards word normaliza-
tion, canonicalization etc. (Han and Baldwin, 2011).
In this paper, we propose a dimensionality reduc-
tion technique for short text using Wavelet packet trans-
form called Improvised Adaptive Discriminant Wavelet
Packet Transform (IADWPT). IAWDPT does dimen-
sionality reduction by selecting discriminative features
(wavelet coefficients) from the Wavelet Packet Trans-
form (WPT) representation. Short text data resembles
transient signals in vector representation and WPT en-
codes transient signals (signals lasting for very short
duration) well (Learned and Willsky, 1995), using very
few coefficients. This leads to considerable decrease in
the dimensionality of the feature space along with in-
crease in classification accuracy. Additionally, we op-
timise the procedure to select the most discriminative
features from WPT representation. To the best of our
knowledge this is the first attempt to apply an algorithm
based on wavelet packet transform to the feature selec-
tion in short text classification.
</bodyText>
<sectionHeader confidence="0.999912" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9978346">
Feature selection has been widely adopted for dimen-
sionality reduction of text datasets in the past. Yim-
ing Yang et al. (Yang and Pedersen, 1997) performed
a comparative study of some of these methods includ-
ing, document frequency, information gain(IG), mutual
</bodyText>
<page confidence="0.987908">
321
</page>
<note confidence="0.612231">
Proceedings of the 19th Conference on Computational Language Learning, pages 321–326,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.99940825">
information(MI), X2-test(CHI) and term strength(TS).
They concluded that IG and CHI are the most effective
in aggressive dimensionality reduction. The mRMR
technique proposed by (Peng et al., 2005) selects the
best feature subset by increasing relevancy of feature
with target class and reducing redundancy between
chosen features.
Wavelet transform provides time-frequency repre-
sentation of a given signal. The time-frequency rep-
resentation is useful for describing signals with time
varying frequency content. Detailed explanation of
wavelet transform theory is beyond the scope of
this paper. For detailed theory, refer to Daubechies
(Daubechies, 2006; Daubechies, 1992; Coifman and
Wickerhauser, 2006) and Robi Polikar (Polikar, ). First
use of wavelet transform for compression was proposed
by Ronald R Coifman et al. (Coifman et al., 1994).
Hammad Qureshi et al. (Qureshi et al., 2008) pro-
posed an adaptive discriminant wavelet packet trans-
form(ADWPT) for feature reduction.
In past wavelet transform has been applied to natural
language processing tasks. A survey on wavelet appli-
cations in data mining (Li et al., 2002), discusses the
basics and properties of wavelets which make it a very
effective technique in Data Mining. CC Aggarwal (Ag-
garwal, 2002) uses wavelets for strings classification.
He notes that wavelet technique creates a hierarchical
decomposition of the data which can capture trends at
varying levels of granularity and thus helps classifica-
tion task with the new representation. Geraldo Xexeo
et al. (Xexeo et al., 2008) used wavelet transform to
represent documents for classification.
</bodyText>
<sectionHeader confidence="0.7573695" genericHeader="method">
3 Wavelet Packet Transform for
Short-text Dimensionality Reduction
</sectionHeader>
<bodyText confidence="0.999976184210526">
Feature selection performs compression of feature
space to preserve maximum discriminative power of
features for classification. We use this analogy to do
compression of document feature space using Wavelet
Packet Transform. Vector format(e.g. dictionary en-
coded vector) representation of a document is equiv-
alent to a digital representation. This vector format
can then be processed using wavelet transform to get
a compressed representation of the document in terms
of wavelet coefficients. Document features are trans-
formed into wavelet coefficients. Wavelet coefficients
are ranked and selected based on their discrimination
power between classes. Classification model is trained
on these highly informative coefficients. Results show
a considerable improvement in model accuracy using
our dimensionality reduction technique.
Typically, vector representation of short text will
have very few non-zero entries due to short length of
the documents. If we plot count of each word in the dic-
tionary on y-axis v/s distinct words on x-axis. Just like
transient signals, the resulting graph will have very few
spikes. Transient signals last for a very little time in the
whole duration of the observation. (Learned and Will-
sky, 1995) show the efficacy of wavelet packet trans-
form in representing transient signal. This motivates
our use of Wavelet Packet Transform to encode short
text.
Wavelet transform is a popular choice for feature
representation in image processing. Our approach is in-
spired by a related work by (Qureshi et al., 2008). They
propose Adaptive Discriminant Wavelet Packet Trans-
form (ADWPT) based representation for meningioma
subtype classfication. ADWPT obtains a wavelet based
representation by optimising the discrimination power
of the various features. Proposed technique IADWPT
differs from ADWPT in the way discriminative fea-
ture are selected. Next section provides details about
the proposed approach IADWPT.
</bodyText>
<sectionHeader confidence="0.987284" genericHeader="method">
4 IADWPT - Improvised Adaptive
Discriminant Wavelet Packet
Transform
</sectionHeader>
<bodyText confidence="0.9996851">
This section presents the proposed short text feature
selection technique IADWPT. IADWPT uses wavelet
packet transform of the data to extract useful discrimi-
native features from the sub-bands at various depths.
Natural language processing tasks usually represent
their documents in dictionary encoded bag-of-words
representation. This numerical vector representation of
a document is equivalent to signal representation. In or-
der to get IADWPT representation of the document fol-
lowing steps should be computed:
</bodyText>
<listItem confidence="0.994123">
1) Compute full wavelet packet transform of the doc-
ument vector representation.
2) Compute the discrimination power of each coeffi-
cient in wavelet packet transform representation.
3) Select the most discriminative coefficients to rep-
resent all the documents in the corpus.
</listItem>
<bodyText confidence="0.9996335">
Once the 1-D wavelet transform is computed at a de-
sired level l, wavelet packet transform (WPT) produces
2l different sets of coefficients (nodes in WPT tree).
These coefficients represent the magnitude of various
frequencies present in the signal at a given time. We
select the most discriminative coefficients to represent
all the documents in the corpus by calculating the dis-
criminative power of each coefficient.
The classification task consists of c classes with d
documents. 1-D Wavelet Packet Transform of the dthk
document yields l levels with f sub bands consisting
of m coefficients in each sub band. xm,f,l represent
the coefficients of Wavelet Packet Transform. Follow-
ing terms are defined for Algorithm 1.
</bodyText>
<listItem confidence="0.823270666666667">
• probability density estimates (Sm,f,l) of a partic-
ular sub-band in a level l a training sample docu-
ment dik of a given Class ci is given by:
</listItem>
<bodyText confidence="0.773993">
Sk (xk m,f,l)2
m,f,l =
Ej (xk j,f,l)2
Here, xm,f,l is the mth coefficient in fth sub-
band of lth level of document dk. Where, j varies
</bodyText>
<page confidence="0.990297">
322
</page>
<bodyText confidence="0.441792">
Algorithm 1 IADWPT Algorithm for best discriminative feature selection
</bodyText>
<listItem confidence="0.99428925">
1: for all classes C do
2: Calculate Wavelet Packet Transform for all the documents dk in class ci
3: for all Documents dk do
4: Calculate probability density estimates Skm,f,l
5: end for
6: for all Levels of WPT l and their sub bands f do
7: for all Wavelet Packet Transform Coefficients m in subband f do
8: Calculate average probability density Aci m,f,l
9: end for
10: end for
11: end for
12: for all Class Pairs ca, cb do
13: Calculate discriminative power Da,b
m,f,l
14: end for
15: Select top m&apos; coefficients for representing documents in corpus
</listItem>
<bodyText confidence="0.998103666666667">
over the length of sub-band. Wavelet supports vary
with the bands, Normalization ensures that the
feature selection done gives uniform weightage to
the features from different bands. This step cal-
culates the normalised value of coefficients in a
sub-band.
</bodyText>
<listItem confidence="0.996406">
• Average probability density (Acim,f,l) estimates are
derived using all the training samples dkin a given
class ci.
</listItem>
<equation confidence="0.7704005">
Aci _ Ek Smk,f,l
m,f, l — d
</equation>
<bodyText confidence="0.9997282">
for, coefficient m in sub-band f for class ci and
d is the total number of documents in the class. k
varies over the number of documents in the class.
It measures the average value of a coefficient in all
the documents belonging to a class.
</bodyText>
<listItem confidence="0.7641845">
• Discriminative power (Da,b
m,f,l) of each coefficient
</listItem>
<bodyText confidence="0.987032">
in lth level’s fth sub band’s mth coefficient, be-
tween classes a and b is defined as follows:
</bodyText>
<equation confidence="0.904094">
DMa Ab
bf,l =  |�Am,f,l − � m,f,l|
</equation>
<bodyText confidence="0.999723529411765">
Discriminative power is the hellinger distance be-
tween the average probability density estimates
of a coefficient for the two classes. It quantifies
the difference in the average value of a coeffi-
cient between a pair of classes. More the differ-
ence, better the discriminative power of the coef-
ficient. Thus discriminative features tend to have
a higher average probability density in one of the
classes whereas redundant features cancel out in
taking the difference in computing the distance.
(Rajpoot, 2003) have shown efficacy of Hellinger
distance applied to ADWPT.
Selecting coefficients with greater discriminative
power helps the classifier perform well. Full algorithm
is mentioned in algorithm 1.
Multi class classification can then be handled in this
framework using one-vs-one classification. We select
the top m&apos; features from the wavelet representation for
representing the data in the classification task. Time
complexity of the algorithm is polynomial. The method
is based on adaptive discriminant wavelet packet trans-
form (ADWPT) (Qureshi et al., 2008). Therefore, we
name it as improvised adaptive discriminant wavelet
packet transform (IADWPT). ADWPT uses best basis
for classification which is a union of the various sub-
bands selected that can span the transformed space, so
noise is still retained in the signal whereas IADWPT
selects coefficients from the sub-band having maximal
discriminative power thus improving the classification
results. As opposed to ADWPT, IADWPT is a one way
transform, original signal cannot be recovered from the
transform domain. Experimental results confirm that
IADWPT performs better than ADWPT in short text
datasets.
</bodyText>
<subsectionHeader confidence="0.906192">
4.1 IADWPT Example
</subsectionHeader>
<bodyText confidence="0.99695175">
Figure 1 Gives intuition of the workings of IAD-
WPT transform. Uppermost graph displays the Aver-
age probability density in positive class samples. Mid-
dle graph in the Figure 1 shows the Average probabil-
ity density in negative class samples. These two energy
values are then subtracted, resulting values are shown
in the bottom most component of Figure 1. Peak posi-
tive and negative values in the bottommost graph rep-
resent the most discriminant features. Absolute value
of the discriminative power can then be used to select
the most discriminative features to represent each doc-
ument in corpus.
</bodyText>
<sectionHeader confidence="0.99874" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.856809714285714">
We used multiple short text datasets to prove efficacy of
proposed algorithm against state of the art algorithms
for feature selection.
1) Twitter 1: This dataset is a part of the SemEval
2013 task B dataset (Nakov et al., ) for two class senti-
ment classification. We gathered 624 examples in pos-
itive and negative class each for our experiments.
</bodyText>
<page confidence="0.99763">
323
</page>
<figure confidence="0.969766875">
Calculation of Discriminative features in Wavelet Packet Transform Representation
0.2
00 500 1000 1500 2000 2500
0.4
0.2
500 1000 1500 2000 2500
500 1000 1500 2000 2500
Wavelet Features
</figure>
<figureCaption confidence="0.9991885">
Figure 1: x-axis represents the wavelet packet transform coefficients, y-axis represents amplitude. From top to
bottom, 1) Average probability density Aam,f,lvalue of coefficients in positive class a), 2) Average probability
</figureCaption>
<tableCaption confidence="0.6607475">
density Abm,f,lvalue of coefficients in negative class (b), 3) Difference between Aam,f,l and Abm,f,l
Table 1: CLASSIFICATION RESULTS - SUPPORT VECTOR MACHINE (SVM) AND LOGISTIC REGRESSION (LR)
</tableCaption>
<table confidence="0.997865571428572">
Dataset Baseline MI-avg XZ PCA ADWPT IADWPT
Classification accuracy - SVM
Twitter 1 47.04 47.57 45.62 59.28 46 63.44
SMS Spam 1 - HAM Accuracy 99.82 99.71 99.77 99.87 99.63 99.94
SMS Spam 1 - SPAM Accuracy 83.10 83.32 82.96 83.81 83.57 83.92
SMS Spam 2 - HAM Accuracy 55.2 56.31 55.62 81.12 54.1 87.7
SMS Spam 2 - SPAM Accuracy 46.6 46.7 46.49 92.39 47.3 99.42
Total dimensions in best classification accuracy result - SVM
Twitter 1 7423 2065 515 540 7423 23
SMS Spam 1 9394 3540 550 750 9394 815
SMS Spam 2 10681 2985 490 855 1068 250
Classification accuracy - Logistic Regression
Twitter 1 75.8 74.97 75.21 76.28 68.2 76.72
SMS Spam 1 - HAM Accuracy 97.91 94.67 95.28 98.71 98.03 99.61
SMS Spam 1 - SPAM Accuracy 95.48 85.34 86.37 91.37 82.2 87.54
SMS Spam 2 - HAM Accuracy 96.02 89.54 92.76 71.21 95.09 98.5
SMS Spam 2 - SPAM Accuracy 91.2 88.37 91.38 89.15 92.2 94.51
Total dimensions in best classification accuracy result - Logistic Regression
Twitter 1 7423 5600 3250 3575 7423 2749
SMS Spam 1 9394 7545 1755 2350 9394 1680
SMS Spam 2 10681 6550 3000 3050 10681 9981
</table>
<figure confidence="0.971344285714286">
0.4
Energy
00
0.5
0
−0.5
0
</figure>
<listItem confidence="0.989054071428571">
2) SMS Spam 1: UCI spam dataset (Almeida, )
consists of 5,574 instances of SMS classified into
SPAM and HAM classes. SPAM class is defined as
messages which are not useful and HAM is the class
of useful messages. We compare our results with the
results they published in their paper (Almeida et al.,
2013). Therefore, we followed the same experiment
procedure as cited in the paper. First 30% samples were
used in train and the rest in test set as reported in the
paper.
3) SMS Spam 2: The dataset was published by Ya-
dav et al. (Yadav et al., 2011). It consists of 2000 SMS,
1000 SPAM and 1000 HAM messages. Experiment set-
tings are same as that of dataset SMS Spam 1.
</listItem>
<bodyText confidence="0.998844142857143">
The goal of our experiments is to examine the ef-
fectiveness of the proposed algorithm in feature selec-
tion for short text datasets. We measure the effective-
ness of the feature selection technique with respect to
the increase in accuracy in the final machine learning
task. Our method does not depend on a specific classi-
fier used in the final classification. Therefore, we used
</bodyText>
<figure confidence="0.313727">
Number of dimensions
</figure>
<figureCaption confidence="0.992182">
Figure 2: Spam Classification Accuracy comparison
across various feature selection algorithm for SMS
Spam 2 dataset
</figureCaption>
<figure confidence="0.996494421052632">
Comparision of different feature selection methods: Spam Accuracy
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
50 100 150 200 250 300 350 400 450 500
Spam Accuracy
MRMR
IADWPT
PCA
Chi
MI
</figure>
<page confidence="0.575151">
324
</page>
<figure confidence="0.99735035">
Comparision of different feature selection methods: Ham Accuracy
0
50 100 150 200 250 300 350 400 450 500
Number of Dimensions
Ham Accuracy
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
MRMR
IADWPT
PCA
Chi
MI
</figure>
<figureCaption confidence="0.996725666666667">
Figure 3: Ham Classification Accuracy comparison
across various feature selection algorithm for SMS
Spam 2 dataset
</figureCaption>
<bodyText confidence="0.96775275">
popular classifiers like Support Vector Machine (RBF
kernel with grid search) (Cortes and Vapnik, 1995) and
Logistic Regression with and without dimensionality
reduction for unigram representation to benchmark the
performance. All the experiments were done with 10
fold cross validation and grid search on C parameter.
We report results with respect to classification accuracy
which is measured as #correctly classified datapoints .
</bodyText>
<subsubsectionHeader confidence="0.445524">
#total datapoints
</subsubsectionHeader>
<bodyText confidence="0.9999185">
We conducted detailed experiments comparing IAD-
WPT using Coiflets of order 2 with other feature selec-
tion techniques such as PCA, Mutual Information, x2,
mRMR (Peng et al., 2005) and ADWPT (Qureshi et
al., 2008). Results are reported in Table 1. The table
reports best accuracy values and respective feature set
size selected by the technique. It can be observed that
IADWPT gives best accuracy in most of the cases with
very few features.
We compared performance of our algorithm with
mRMR. Results for SMS Spam 2 dataset are shown
in Figure 2 and Figure 3. The plots prove efficacy of
our algorithm versus state of the art mRMR algorithm.
mRMR technique could not finish execution for the rest
of the datasets. It can also be observed from results
in Table 1 and Figure 1,2 that performance of feature
selection algorithms follow consistent pattern in short
text. Following is observed order of performance of al-
gorithms in decreasing order, IADWPT, mRMR, PCA,
Chi Square, MI. Further, it is observed that IADWPT
performs well at feature selection without losing dis-
criminative information, even when the dimensional-
ity of feature space is reduced to as far as 1/40th of
original feature space and steadily maintains the ac-
curacy as dimensionality is reduced, which makes it a
suitable technique for aggressive dimensionality reduc-
tion. This also helps in learning ML (machine learn-
ing) models faster due to reduced dimensionality. We
plotted the discrimination power of coefficients in each
dataset. Plot suggested that very few coefficients con-
tained most of the discriminative power. And, therefore
just working with these coefficients can help in getting
good accuracies resulting in aggressive dimensionality
reduction. Results establish the effectiveness of IAD-
WPT for applicability in compressing short text feature
representation and reducing noise to improve classifi-
</bodyText>
<figureCaption confidence="0.996991">
Figure 4: Plot of Discriminative Power (Da,b
</figureCaption>
<bodyText confidence="0.789551333333333">
m,f,l) ar-
ranged in descending order
cation accuracy.
</bodyText>
<subsectionHeader confidence="0.73468">
5.1 IADWPT Effectiveness
</subsectionHeader>
<bodyText confidence="0.9999879">
Short text data is noisy and consists of many features
which are irrelevant to the task of classification of data.
IADWPT effectively gets rid of the noise in approxima-
tions(as Signal strength is greater than the noise), the
feature selection step at the sub-band level as described
in Algorithm 1, it enforces selection of good discrim-
inative features and thus improves classifier accuracy,
reducing feature space dimensionality at the same time.
Features from sub-bands of the signal are chosen based
on their discriminative power, therefore, the original
signal information is lost and the transform is not re-
versible.
IADWPT gives good compression of data and with-
out losing discriminative information, even when the
dimensionality of space is reduced to as far as 1/40th
of original feature space and is thus steadily maintain-
ing the accuracy as dimensionality is reduced, which
makes it a suitable technique for dimensionality re-
duction. This also helps learning machine learning
models faster due to reduced dimensionality. Figure
</bodyText>
<sectionHeader confidence="0.578171" genericHeader="method">
4 shows the plot of Discriminative Power Da,b
</sectionHeader>
<bodyText confidence="0.961462">
m,f,l val-
ues for coefficients arranged in descending order for
SMS Spam 2 dataset. Other datasets displayed similar
graph for Discriminative Power. From the figure it can
be observed that few coefficients hold most of the dis-
criminative power, and thus aggressive dimensionality
reduction is possible with IADWPT algorithm. Results
establish the effectiveness of IADWPT for applicabil-
ity in compressing short text feature representation and
reducing noise.
</bodyText>
<sectionHeader confidence="0.998674" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999285">
In this paper, we have proposed IADWPT algorithm
for effective dimensionality reduction for short text cor-
pus. The algorithm can be used in a number of scenar-
ios where high dimensionality and sparsity pose chal-
lenge. Experiments prove efficacy of IADWPT based
dimensionality reduction for short text data. This tech-
nique can prove useful to a number of social media data
analysis applications. In future, we would like to ex-
plore theoretical bounds on best number of dimensions
to choose from wavelet representation.
</bodyText>
<page confidence="0.998497">
325
</page>
<sectionHeader confidence="0.996252" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996230606741573">
Charu C. Aggarwal. 2002. On effective classification
of strings with wavelets.
Tiago A Almeida. Sms spam collection v.1.
http://www.dt.fee.unicamp.br/
˜tiago/smsspamcollection/,[Online;
accessed 10-September-2014].
T. Almeida, J. M. G. Hidalgo, and T. P. Silva. 2013.
Towards sms spam filtering: Results under a new
dataset. International Journal of Information Secu-
rity Science.
John Blitzer. 2008. A survey of dimensional-
ity reduction techniques for natural language.
http://john.blitzer.com/papers/
wpe2.pdf,[Online; accessed 10-September-2014].
R. R. Coifman and M. V. Wickerhauser. 2006.
Entropy-based algorithms for best basis selection.
IEEE Trans. Inf. Theor., 38(2):713–718, September.
RonaldR. Coifman, Yves Meyer, Steven Quake, and
M.Victor Wickerhauser. 1994. Signal process-
ing and compression with wavelet packets. In J.S.
Byrnes, JenniferL. Byrnes, KathrynA. Hargreaves,
and Karl Berry, editors, Wavelets and TheirApplica-
tions, volume 442 of NATO ASI Series, pages 363–
379. Springer Netherlands.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Mach. Learn., 20(3):273–297,
September.
Ingrid Daubechies. 1992. Ten Lectures on Wavelets.
Society for Industrial and Applied Mathematics,
Philadelphia, PA, USA.
I. Daubechies. 2006. The wavelet transform, time-
frequency localization and signal analysis. IEEE
Trans. Inf. Theor., 36(5):961–1005, September.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
- Volume 1, HLT ’11, pages 368–378, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Rachel E. Learned and Alan S. Willsky. 1995. A
wavelet packet approach to transient signal classifi-
cation. Applied and Computational Harmonic Anal-
ysis, 2(3):265 – 278.
Tao Li, Qi Li, Shenghuo Zhu, and Mitsunori Ogihara.
2002. A survey on wavelet applications in data min-
ing. SIGKDD Explor. Newsl., 4(2):49–68, Decem-
ber.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
Semeval-2013 task 2: Sentiment analysis in twitter.
Hanchuan Peng, Fuhui Long, and Chris Ding. 2005.
Feature selection based on mutual information: cri-
teria of max-dependency, max-relevance, and min-
redundancy. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 27:1226–1238.
Robi Polikar. Wavelet Tutorial. http:
//users.rowan.edu/˜polikar/
wavelets/wttutorial.html,[Online;
accessed 10-January-2015].
Hammad Qureshi, Olcay Sertel, Nasir Rajpoot, Roland
Wilson, and Metin Gurcan. 2008. Adaptive dis-
criminant wavelet packet transform and local binary
patterns for meningioma subtype classification. In
Dimitris N. Metaxas, Leon Axel, Gabor Fichtinger,
and Gbor Szkely, editors, MICCAI (2), volume 5242
of Lecture Notes in Computer Science, pages 196–
204. Springer.
NM Rajpoot. 2003. Local discriminant wavelet packet
basis for texture classification. In SPIE Wavelets X,
pages 774–783. SPIE.
Geraldo Xexeo, Jano de Souza, Patricia F. Castro, and
Wallace A. Pinheiro. 2008. Using wavelets to
classify documents. Web Intelligence and Intel-
ligent Agent Technology, IEEE/WIC/ACM Interna-
tional Conference on, 1:272–278.
Kuldeep Yadav, Ponnurangam Kumaraguru, Atul
Goyal, Ashish Gupta, and Vinayak Naik. 2011. Sm-
sassassin: Crowdsourcing driven mobile-based sys-
tem for sms spam filtering. In Proceedings of the
12th Workshop on Mobile Computing Systems and
Applications, HotMobile ’11, pages 1–6, New York,
NY, USA. ACM.
Yiming Yang and Jan O. Pedersen. 1997. A compar-
ative study on feature selection in text categoriza-
tion. In Proceedings of the Fourteenth International
Conference on Machine Learning, ICML ’97, pages
412–420, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
</reference>
<page confidence="0.999117">
326
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.245908">
<title confidence="0.836202">Feature Selection for Short Text Classification using Wavelet Packet Transform Anuj</title>
<affiliation confidence="0.995983">Indian Institute of Technology,</affiliation>
<address confidence="0.771325">Hauz Khas, New Delhi</address>
<email confidence="0.999265">anujmahajan.iitd@gmail.com</email>
<author confidence="0.880072">Shourya Sharmistha</author>
<affiliation confidence="0.986755">Xerox Research Centre</affiliation>
<address confidence="0.668144">Bangalore -</address>
<abstract confidence="0.999472695652174">Text classification tasks suffer from curse of dimensionality due to large feature space. Short text data further exacerbates the problem due to their sparse and noisy nature. Feature selection thus becomes an important step in improving the classification performance. In this paper, we propose a novel feature selection method using Wavelet Packet Transform. Wavelet Packet Transform (WPT) has been used widely in various fields due to its efficiency in encoding transient signals. We demonstrate how short text classification task can be benefited by feature selection using WPT due to their sparse nature. Our technique chooses the most discriminating features by computing inter-class distances in the transformed space. We experimented extensively with several short text datasets. Compared to well known techniques our approach reduces the feature space size and improves the overall classification performance significantly in all the datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Charu C Aggarwal</author>
</authors>
<title>On effective classification of strings with wavelets.</title>
<date>2002</date>
<contexts>
<context position="6043" citStr="Aggarwal, 2002" startWordPosition="893" endWordPosition="895">s, 2006; Daubechies, 1992; Coifman and Wickerhauser, 2006) and Robi Polikar (Polikar, ). First use of wavelet transform for compression was proposed by Ronald R Coifman et al. (Coifman et al., 1994). Hammad Qureshi et al. (Qureshi et al., 2008) proposed an adaptive discriminant wavelet packet transform(ADWPT) for feature reduction. In past wavelet transform has been applied to natural language processing tasks. A survey on wavelet applications in data mining (Li et al., 2002), discusses the basics and properties of wavelets which make it a very effective technique in Data Mining. CC Aggarwal (Aggarwal, 2002) uses wavelets for strings classification. He notes that wavelet technique creates a hierarchical decomposition of the data which can capture trends at varying levels of granularity and thus helps classification task with the new representation. Geraldo Xexeo et al. (Xexeo et al., 2008) used wavelet transform to represent documents for classification. 3 Wavelet Packet Transform for Short-text Dimensionality Reduction Feature selection performs compression of feature space to preserve maximum discriminative power of features for classification. We use this analogy to do compression of document </context>
</contexts>
<marker>Aggarwal, 2002</marker>
<rawString>Charu C. Aggarwal. 2002. On effective classification of strings with wavelets.</rawString>
</citation>
<citation valid="false">
<title>Tiago A Almeida. Sms spam collection v.1. http://www.dt.fee.unicamp.br/ ˜tiago/smsspamcollection/,[Online; accessed 10-September-2014].</title>
<marker></marker>
<rawString>Tiago A Almeida. Sms spam collection v.1. http://www.dt.fee.unicamp.br/ ˜tiago/smsspamcollection/,[Online; accessed 10-September-2014].</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Almeida</author>
<author>J M G Hidalgo</author>
<author>T P Silva</author>
</authors>
<title>Towards sms spam filtering: Results under a new dataset.</title>
<date>2013</date>
<journal>International Journal of Information Security Science.</journal>
<contexts>
<context position="16376" citStr="Almeida et al., 2013" startWordPosition="2552" endWordPosition="2555">54 92.76 71.21 95.09 98.5 SMS Spam 2 - SPAM Accuracy 91.2 88.37 91.38 89.15 92.2 94.51 Total dimensions in best classification accuracy result - Logistic Regression Twitter 1 7423 5600 3250 3575 7423 2749 SMS Spam 1 9394 7545 1755 2350 9394 1680 SMS Spam 2 10681 6550 3000 3050 10681 9981 0.4 Energy 00 0.5 0 −0.5 0 2) SMS Spam 1: UCI spam dataset (Almeida, ) consists of 5,574 instances of SMS classified into SPAM and HAM classes. SPAM class is defined as messages which are not useful and HAM is the class of useful messages. We compare our results with the results they published in their paper (Almeida et al., 2013). Therefore, we followed the same experiment procedure as cited in the paper. First 30% samples were used in train and the rest in test set as reported in the paper. 3) SMS Spam 2: The dataset was published by Yadav et al. (Yadav et al., 2011). It consists of 2000 SMS, 1000 SPAM and 1000 HAM messages. Experiment settings are same as that of dataset SMS Spam 1. The goal of our experiments is to examine the effectiveness of the proposed algorithm in feature selection for short text datasets. We measure the effectiveness of the feature selection technique with respect to the increase in accuracy </context>
</contexts>
<marker>Almeida, Hidalgo, Silva, 2013</marker>
<rawString>T. Almeida, J. M. G. Hidalgo, and T. P. Silva. 2013. Towards sms spam filtering: Results under a new dataset. International Journal of Information Security Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
</authors>
<title>A survey of dimensionality reduction techniques for natural language. http://john.blitzer.com/papers/ wpe2.pdf,[Online; accessed 10-September-2014].</title>
<date>2008</date>
<contexts>
<context position="3177" citStr="Blitzer, 2008" startWordPosition="469" endWordPosition="470">ngth of a document in Reuters-21578 news classification corpus is 200 words. Therefore, the dimensionality is extremely high even for small corpora with short texts. In addition, the average number of words in a document is significantly less in short text data leading to higher sparsity of feature space representation of documents. Owing to this high dimensionality problem, one of the important steps in text classification workflows is feature selection. Feature selection techniques for traditional documents have been aplenty and a few seminal survey articles have been written on this topic (Blitzer, 2008). In contrast, for short text there is much less work on statistical feature selection but more focus has gone to feature engineering towards word normalization, canonicalization etc. (Han and Baldwin, 2011). In this paper, we propose a dimensionality reduction technique for short text using Wavelet packet transform called Improvised Adaptive Discriminant Wavelet Packet Transform (IADWPT). IAWDPT does dimensionality reduction by selecting discriminative features (wavelet coefficients) from the Wavelet Packet Transform (WPT) representation. Short text data resembles transient signals in vector </context>
</contexts>
<marker>Blitzer, 2008</marker>
<rawString>John Blitzer. 2008. A survey of dimensionality reduction techniques for natural language. http://john.blitzer.com/papers/ wpe2.pdf,[Online; accessed 10-September-2014].</rawString>
</citation>
<citation valid="true">
<authors>
<author>R R Coifman</author>
<author>M V Wickerhauser</author>
</authors>
<title>Entropy-based algorithms for best basis selection.</title>
<date>2006</date>
<journal>IEEE Trans. Inf. Theor.,</journal>
<volume>38</volume>
<issue>2</issue>
<contexts>
<context position="5486" citStr="Coifman and Wickerhauser, 2006" startWordPosition="801" endWordPosition="804">G and CHI are the most effective in aggressive dimensionality reduction. The mRMR technique proposed by (Peng et al., 2005) selects the best feature subset by increasing relevancy of feature with target class and reducing redundancy between chosen features. Wavelet transform provides time-frequency representation of a given signal. The time-frequency representation is useful for describing signals with time varying frequency content. Detailed explanation of wavelet transform theory is beyond the scope of this paper. For detailed theory, refer to Daubechies (Daubechies, 2006; Daubechies, 1992; Coifman and Wickerhauser, 2006) and Robi Polikar (Polikar, ). First use of wavelet transform for compression was proposed by Ronald R Coifman et al. (Coifman et al., 1994). Hammad Qureshi et al. (Qureshi et al., 2008) proposed an adaptive discriminant wavelet packet transform(ADWPT) for feature reduction. In past wavelet transform has been applied to natural language processing tasks. A survey on wavelet applications in data mining (Li et al., 2002), discusses the basics and properties of wavelets which make it a very effective technique in Data Mining. CC Aggarwal (Aggarwal, 2002) uses wavelets for strings classification. </context>
</contexts>
<marker>Coifman, Wickerhauser, 2006</marker>
<rawString>R. R. Coifman and M. V. Wickerhauser. 2006. Entropy-based algorithms for best basis selection. IEEE Trans. Inf. Theor., 38(2):713–718, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Meyer Coifman</author>
<author>Steven Quake</author>
<author>M Victor Wickerhauser</author>
</authors>
<title>Signal processing and compression with wavelet packets.</title>
<date>1994</date>
<booktitle>Wavelets and TheirApplications, volume 442 of NATO ASI Series,</booktitle>
<pages>363--379</pages>
<editor>In J.S. Byrnes, JenniferL. Byrnes, KathrynA. Hargreaves, and Karl Berry, editors,</editor>
<publisher>Springer Netherlands.</publisher>
<contexts>
<context position="5626" citStr="Coifman et al., 1994" startWordPosition="825" endWordPosition="828">ubset by increasing relevancy of feature with target class and reducing redundancy between chosen features. Wavelet transform provides time-frequency representation of a given signal. The time-frequency representation is useful for describing signals with time varying frequency content. Detailed explanation of wavelet transform theory is beyond the scope of this paper. For detailed theory, refer to Daubechies (Daubechies, 2006; Daubechies, 1992; Coifman and Wickerhauser, 2006) and Robi Polikar (Polikar, ). First use of wavelet transform for compression was proposed by Ronald R Coifman et al. (Coifman et al., 1994). Hammad Qureshi et al. (Qureshi et al., 2008) proposed an adaptive discriminant wavelet packet transform(ADWPT) for feature reduction. In past wavelet transform has been applied to natural language processing tasks. A survey on wavelet applications in data mining (Li et al., 2002), discusses the basics and properties of wavelets which make it a very effective technique in Data Mining. CC Aggarwal (Aggarwal, 2002) uses wavelets for strings classification. He notes that wavelet technique creates a hierarchical decomposition of the data which can capture trends at varying levels of granularity a</context>
</contexts>
<marker>Coifman, Quake, Wickerhauser, 1994</marker>
<rawString>RonaldR. Coifman, Yves Meyer, Steven Quake, and M.Victor Wickerhauser. 1994. Signal processing and compression with wavelet packets. In J.S. Byrnes, JenniferL. Byrnes, KathrynA. Hargreaves, and Karl Berry, editors, Wavelets and TheirApplications, volume 442 of NATO ASI Series, pages 363– 379. Springer Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<journal>Mach. Learn.,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="17859" citStr="Cortes and Vapnik, 1995" startWordPosition="2812" endWordPosition="2815">SMS Spam 2 dataset Comparision of different feature selection methods: Spam Accuracy 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 50 100 150 200 250 300 350 400 450 500 Spam Accuracy MRMR IADWPT PCA Chi MI 324 Comparision of different feature selection methods: Ham Accuracy 0 50 100 150 200 250 300 350 400 450 500 Number of Dimensions Ham Accuracy 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 MRMR IADWPT PCA Chi MI Figure 3: Ham Classification Accuracy comparison across various feature selection algorithm for SMS Spam 2 dataset popular classifiers like Support Vector Machine (RBF kernel with grid search) (Cortes and Vapnik, 1995) and Logistic Regression with and without dimensionality reduction for unigram representation to benchmark the performance. All the experiments were done with 10 fold cross validation and grid search on C parameter. We report results with respect to classification accuracy which is measured as #correctly classified datapoints . #total datapoints We conducted detailed experiments comparing IADWPT using Coiflets of order 2 with other feature selection techniques such as PCA, Mutual Information, x2, mRMR (Peng et al., 2005) and ADWPT (Qureshi et al., 2008). Results are reported in Table 1. The ta</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Mach. Learn., 20(3):273–297, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Daubechies</author>
</authors>
<title>Ten Lectures on Wavelets. Society for Industrial and Applied Mathematics,</title>
<date>1992</date>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="5453" citStr="Daubechies, 1992" startWordPosition="799" endWordPosition="800">y concluded that IG and CHI are the most effective in aggressive dimensionality reduction. The mRMR technique proposed by (Peng et al., 2005) selects the best feature subset by increasing relevancy of feature with target class and reducing redundancy between chosen features. Wavelet transform provides time-frequency representation of a given signal. The time-frequency representation is useful for describing signals with time varying frequency content. Detailed explanation of wavelet transform theory is beyond the scope of this paper. For detailed theory, refer to Daubechies (Daubechies, 2006; Daubechies, 1992; Coifman and Wickerhauser, 2006) and Robi Polikar (Polikar, ). First use of wavelet transform for compression was proposed by Ronald R Coifman et al. (Coifman et al., 1994). Hammad Qureshi et al. (Qureshi et al., 2008) proposed an adaptive discriminant wavelet packet transform(ADWPT) for feature reduction. In past wavelet transform has been applied to natural language processing tasks. A survey on wavelet applications in data mining (Li et al., 2002), discusses the basics and properties of wavelets which make it a very effective technique in Data Mining. CC Aggarwal (Aggarwal, 2002) uses wave</context>
</contexts>
<marker>Daubechies, 1992</marker>
<rawString>Ingrid Daubechies. 1992. Ten Lectures on Wavelets. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Daubechies</author>
</authors>
<title>The wavelet transform, timefrequency localization and signal analysis.</title>
<date>2006</date>
<journal>IEEE Trans. Inf. Theor.,</journal>
<volume>36</volume>
<issue>5</issue>
<contexts>
<context position="5435" citStr="Daubechies, 2006" startWordPosition="797" endWordPosition="798"> strength(TS). They concluded that IG and CHI are the most effective in aggressive dimensionality reduction. The mRMR technique proposed by (Peng et al., 2005) selects the best feature subset by increasing relevancy of feature with target class and reducing redundancy between chosen features. Wavelet transform provides time-frequency representation of a given signal. The time-frequency representation is useful for describing signals with time varying frequency content. Detailed explanation of wavelet transform theory is beyond the scope of this paper. For detailed theory, refer to Daubechies (Daubechies, 2006; Daubechies, 1992; Coifman and Wickerhauser, 2006) and Robi Polikar (Polikar, ). First use of wavelet transform for compression was proposed by Ronald R Coifman et al. (Coifman et al., 1994). Hammad Qureshi et al. (Qureshi et al., 2008) proposed an adaptive discriminant wavelet packet transform(ADWPT) for feature reduction. In past wavelet transform has been applied to natural language processing tasks. A survey on wavelet applications in data mining (Li et al., 2002), discusses the basics and properties of wavelets which make it a very effective technique in Data Mining. CC Aggarwal (Aggarwa</context>
</contexts>
<marker>Daubechies, 2006</marker>
<rawString>I. Daubechies. 2006. The wavelet transform, timefrequency localization and signal analysis. IEEE Trans. Inf. Theor., 36(5):961–1005, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>368--378</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3384" citStr="Han and Baldwin, 2011" startWordPosition="499" endWordPosition="502">f words in a document is significantly less in short text data leading to higher sparsity of feature space representation of documents. Owing to this high dimensionality problem, one of the important steps in text classification workflows is feature selection. Feature selection techniques for traditional documents have been aplenty and a few seminal survey articles have been written on this topic (Blitzer, 2008). In contrast, for short text there is much less work on statistical feature selection but more focus has gone to feature engineering towards word normalization, canonicalization etc. (Han and Baldwin, 2011). In this paper, we propose a dimensionality reduction technique for short text using Wavelet packet transform called Improvised Adaptive Discriminant Wavelet Packet Transform (IADWPT). IAWDPT does dimensionality reduction by selecting discriminative features (wavelet coefficients) from the Wavelet Packet Transform (WPT) representation. Short text data resembles transient signals in vector representation and WPT encodes transient signals (signals lasting for very short duration) well (Learned and Willsky, 1995), using very few coefficients. This leads to considerable decrease in the dimensiona</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 368–378, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachel E Learned</author>
<author>Alan S Willsky</author>
</authors>
<title>A wavelet packet approach to transient signal classification.</title>
<date>1995</date>
<journal>Applied and Computational Harmonic Analysis,</journal>
<volume>2</volume>
<issue>3</issue>
<pages>278</pages>
<contexts>
<context position="3900" citStr="Learned and Willsky, 1995" startWordPosition="570" endWordPosition="573">ocus has gone to feature engineering towards word normalization, canonicalization etc. (Han and Baldwin, 2011). In this paper, we propose a dimensionality reduction technique for short text using Wavelet packet transform called Improvised Adaptive Discriminant Wavelet Packet Transform (IADWPT). IAWDPT does dimensionality reduction by selecting discriminative features (wavelet coefficients) from the Wavelet Packet Transform (WPT) representation. Short text data resembles transient signals in vector representation and WPT encodes transient signals (signals lasting for very short duration) well (Learned and Willsky, 1995), using very few coefficients. This leads to considerable decrease in the dimensionality of the feature space along with increase in classification accuracy. Additionally, we optimise the procedure to select the most discriminative features from WPT representation. To the best of our knowledge this is the first attempt to apply an algorithm based on wavelet packet transform to the feature selection in short text classification. 2 Related Work Feature selection has been widely adopted for dimensionality reduction of text datasets in the past. Yiming Yang et al. (Yang and Pedersen, 1997) perform</context>
<context position="7694" citStr="Learned and Willsky, 1995" startWordPosition="1134" endWordPosition="1138"> on their discrimination power between classes. Classification model is trained on these highly informative coefficients. Results show a considerable improvement in model accuracy using our dimensionality reduction technique. Typically, vector representation of short text will have very few non-zero entries due to short length of the documents. If we plot count of each word in the dictionary on y-axis v/s distinct words on x-axis. Just like transient signals, the resulting graph will have very few spikes. Transient signals last for a very little time in the whole duration of the observation. (Learned and Willsky, 1995) show the efficacy of wavelet packet transform in representing transient signal. This motivates our use of Wavelet Packet Transform to encode short text. Wavelet transform is a popular choice for feature representation in image processing. Our approach is inspired by a related work by (Qureshi et al., 2008). They propose Adaptive Discriminant Wavelet Packet Transform (ADWPT) based representation for meningioma subtype classfication. ADWPT obtains a wavelet based representation by optimising the discrimination power of the various features. Proposed technique IADWPT differs from ADWPT in the wa</context>
</contexts>
<marker>Learned, Willsky, 1995</marker>
<rawString>Rachel E. Learned and Alan S. Willsky. 1995. A wavelet packet approach to transient signal classification. Applied and Computational Harmonic Analysis, 2(3):265 – 278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Li</author>
<author>Qi Li</author>
<author>Shenghuo Zhu</author>
<author>Mitsunori Ogihara</author>
</authors>
<title>A survey on wavelet applications in data mining.</title>
<date>2002</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="5908" citStr="Li et al., 2002" startWordPosition="870" endWordPosition="873"> Detailed explanation of wavelet transform theory is beyond the scope of this paper. For detailed theory, refer to Daubechies (Daubechies, 2006; Daubechies, 1992; Coifman and Wickerhauser, 2006) and Robi Polikar (Polikar, ). First use of wavelet transform for compression was proposed by Ronald R Coifman et al. (Coifman et al., 1994). Hammad Qureshi et al. (Qureshi et al., 2008) proposed an adaptive discriminant wavelet packet transform(ADWPT) for feature reduction. In past wavelet transform has been applied to natural language processing tasks. A survey on wavelet applications in data mining (Li et al., 2002), discusses the basics and properties of wavelets which make it a very effective technique in Data Mining. CC Aggarwal (Aggarwal, 2002) uses wavelets for strings classification. He notes that wavelet technique creates a hierarchical decomposition of the data which can capture trends at varying levels of granularity and thus helps classification task with the new representation. Geraldo Xexeo et al. (Xexeo et al., 2008) used wavelet transform to represent documents for classification. 3 Wavelet Packet Transform for Short-text Dimensionality Reduction Feature selection performs compression of fe</context>
</contexts>
<marker>Li, Li, Zhu, Ogihara, 2002</marker>
<rawString>Tao Li, Qi Li, Shenghuo Zhu, and Mitsunori Ogihara. 2002. A survey on wavelet applications in data mining. SIGKDD Explor. Newsl., 4(2):49–68, December.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Preslav Nakov</author>
<author>Zornitsa Kozareva</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<marker>Nakov, Kozareva, Ritter, Rosenthal, Stoyanov, Wilson, </marker>
<rawString>Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara Rosenthal, Veselin Stoyanov, and Theresa Wilson. Semeval-2013 task 2: Sentiment analysis in twitter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanchuan Peng</author>
<author>Fuhui Long</author>
<author>Chris Ding</author>
</authors>
<title>Feature selection based on mutual information: criteria of max-dependency, max-relevance, and minredundancy.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>27--1226</pages>
<contexts>
<context position="4978" citStr="Peng et al., 2005" startWordPosition="731" endWordPosition="734">tion has been widely adopted for dimensionality reduction of text datasets in the past. Yiming Yang et al. (Yang and Pedersen, 1997) performed a comparative study of some of these methods including, document frequency, information gain(IG), mutual 321 Proceedings of the 19th Conference on Computational Language Learning, pages 321–326, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics information(MI), X2-test(CHI) and term strength(TS). They concluded that IG and CHI are the most effective in aggressive dimensionality reduction. The mRMR technique proposed by (Peng et al., 2005) selects the best feature subset by increasing relevancy of feature with target class and reducing redundancy between chosen features. Wavelet transform provides time-frequency representation of a given signal. The time-frequency representation is useful for describing signals with time varying frequency content. Detailed explanation of wavelet transform theory is beyond the scope of this paper. For detailed theory, refer to Daubechies (Daubechies, 2006; Daubechies, 1992; Coifman and Wickerhauser, 2006) and Robi Polikar (Polikar, ). First use of wavelet transform for compression was proposed b</context>
<context position="18385" citStr="Peng et al., 2005" startWordPosition="2890" endWordPosition="2893">ssifiers like Support Vector Machine (RBF kernel with grid search) (Cortes and Vapnik, 1995) and Logistic Regression with and without dimensionality reduction for unigram representation to benchmark the performance. All the experiments were done with 10 fold cross validation and grid search on C parameter. We report results with respect to classification accuracy which is measured as #correctly classified datapoints . #total datapoints We conducted detailed experiments comparing IADWPT using Coiflets of order 2 with other feature selection techniques such as PCA, Mutual Information, x2, mRMR (Peng et al., 2005) and ADWPT (Qureshi et al., 2008). Results are reported in Table 1. The table reports best accuracy values and respective feature set size selected by the technique. It can be observed that IADWPT gives best accuracy in most of the cases with very few features. We compared performance of our algorithm with mRMR. Results for SMS Spam 2 dataset are shown in Figure 2 and Figure 3. The plots prove efficacy of our algorithm versus state of the art mRMR algorithm. mRMR technique could not finish execution for the rest of the datasets. It can also be observed from results in Table 1 and Figure 1,2 th</context>
</contexts>
<marker>Peng, Long, Ding, 2005</marker>
<rawString>Hanchuan Peng, Fuhui Long, and Chris Ding. 2005. Feature selection based on mutual information: criteria of max-dependency, max-relevance, and minredundancy. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27:1226–1238.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robi Polikar</author>
</authors>
<title>Wavelet Tutorial. http: //users.rowan.edu/˜polikar/ wavelets/wttutorial.html,[Online; accessed 10-January-2015].</title>
<marker>Polikar, </marker>
<rawString>Robi Polikar. Wavelet Tutorial. http: //users.rowan.edu/˜polikar/ wavelets/wttutorial.html,[Online; accessed 10-January-2015].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hammad Qureshi</author>
<author>Olcay Sertel</author>
<author>Nasir Rajpoot</author>
<author>Roland Wilson</author>
<author>Metin Gurcan</author>
</authors>
<title>Adaptive discriminant wavelet packet transform and local binary patterns for meningioma subtype classification.</title>
<date>2008</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>2</volume>
<pages>196--204</pages>
<editor>In Dimitris N. Metaxas, Leon Axel, Gabor Fichtinger, and Gbor Szkely, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="5672" citStr="Qureshi et al., 2008" startWordPosition="833" endWordPosition="836">target class and reducing redundancy between chosen features. Wavelet transform provides time-frequency representation of a given signal. The time-frequency representation is useful for describing signals with time varying frequency content. Detailed explanation of wavelet transform theory is beyond the scope of this paper. For detailed theory, refer to Daubechies (Daubechies, 2006; Daubechies, 1992; Coifman and Wickerhauser, 2006) and Robi Polikar (Polikar, ). First use of wavelet transform for compression was proposed by Ronald R Coifman et al. (Coifman et al., 1994). Hammad Qureshi et al. (Qureshi et al., 2008) proposed an adaptive discriminant wavelet packet transform(ADWPT) for feature reduction. In past wavelet transform has been applied to natural language processing tasks. A survey on wavelet applications in data mining (Li et al., 2002), discusses the basics and properties of wavelets which make it a very effective technique in Data Mining. CC Aggarwal (Aggarwal, 2002) uses wavelets for strings classification. He notes that wavelet technique creates a hierarchical decomposition of the data which can capture trends at varying levels of granularity and thus helps classification task with the new</context>
<context position="8002" citStr="Qureshi et al., 2008" startWordPosition="1185" endWordPosition="1188">to short length of the documents. If we plot count of each word in the dictionary on y-axis v/s distinct words on x-axis. Just like transient signals, the resulting graph will have very few spikes. Transient signals last for a very little time in the whole duration of the observation. (Learned and Willsky, 1995) show the efficacy of wavelet packet transform in representing transient signal. This motivates our use of Wavelet Packet Transform to encode short text. Wavelet transform is a popular choice for feature representation in image processing. Our approach is inspired by a related work by (Qureshi et al., 2008). They propose Adaptive Discriminant Wavelet Packet Transform (ADWPT) based representation for meningioma subtype classfication. ADWPT obtains a wavelet based representation by optimising the discrimination power of the various features. Proposed technique IADWPT differs from ADWPT in the way discriminative feature are selected. Next section provides details about the proposed approach IADWPT. 4 IADWPT - Improvised Adaptive Discriminant Wavelet Packet Transform This section presents the proposed short text feature selection technique IADWPT. IADWPT uses wavelet packet transform of the data to </context>
<context position="12770" citStr="Qureshi et al., 2008" startWordPosition="1948" endWordPosition="1951">king the difference in computing the distance. (Rajpoot, 2003) have shown efficacy of Hellinger distance applied to ADWPT. Selecting coefficients with greater discriminative power helps the classifier perform well. Full algorithm is mentioned in algorithm 1. Multi class classification can then be handled in this framework using one-vs-one classification. We select the top m&apos; features from the wavelet representation for representing the data in the classification task. Time complexity of the algorithm is polynomial. The method is based on adaptive discriminant wavelet packet transform (ADWPT) (Qureshi et al., 2008). Therefore, we name it as improvised adaptive discriminant wavelet packet transform (IADWPT). ADWPT uses best basis for classification which is a union of the various subbands selected that can span the transformed space, so noise is still retained in the signal whereas IADWPT selects coefficients from the sub-band having maximal discriminative power thus improving the classification results. As opposed to ADWPT, IADWPT is a one way transform, original signal cannot be recovered from the transform domain. Experimental results confirm that IADWPT performs better than ADWPT in short text datase</context>
<context position="18418" citStr="Qureshi et al., 2008" startWordPosition="2896" endWordPosition="2899">achine (RBF kernel with grid search) (Cortes and Vapnik, 1995) and Logistic Regression with and without dimensionality reduction for unigram representation to benchmark the performance. All the experiments were done with 10 fold cross validation and grid search on C parameter. We report results with respect to classification accuracy which is measured as #correctly classified datapoints . #total datapoints We conducted detailed experiments comparing IADWPT using Coiflets of order 2 with other feature selection techniques such as PCA, Mutual Information, x2, mRMR (Peng et al., 2005) and ADWPT (Qureshi et al., 2008). Results are reported in Table 1. The table reports best accuracy values and respective feature set size selected by the technique. It can be observed that IADWPT gives best accuracy in most of the cases with very few features. We compared performance of our algorithm with mRMR. Results for SMS Spam 2 dataset are shown in Figure 2 and Figure 3. The plots prove efficacy of our algorithm versus state of the art mRMR algorithm. mRMR technique could not finish execution for the rest of the datasets. It can also be observed from results in Table 1 and Figure 1,2 that performance of feature selecti</context>
</contexts>
<marker>Qureshi, Sertel, Rajpoot, Wilson, Gurcan, 2008</marker>
<rawString>Hammad Qureshi, Olcay Sertel, Nasir Rajpoot, Roland Wilson, and Metin Gurcan. 2008. Adaptive discriminant wavelet packet transform and local binary patterns for meningioma subtype classification. In Dimitris N. Metaxas, Leon Axel, Gabor Fichtinger, and Gbor Szkely, editors, MICCAI (2), volume 5242 of Lecture Notes in Computer Science, pages 196– 204. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NM Rajpoot</author>
</authors>
<title>Local discriminant wavelet packet basis for texture classification.</title>
<date>2003</date>
<booktitle>In SPIE Wavelets X,</booktitle>
<pages>774--783</pages>
<publisher>SPIE.</publisher>
<contexts>
<context position="12211" citStr="Rajpoot, 2003" startWordPosition="1869" endWordPosition="1870"> band’s mth coefficient, between classes a and b is defined as follows: DMa Ab bf,l = |�Am,f,l − � m,f,l| Discriminative power is the hellinger distance between the average probability density estimates of a coefficient for the two classes. It quantifies the difference in the average value of a coefficient between a pair of classes. More the difference, better the discriminative power of the coefficient. Thus discriminative features tend to have a higher average probability density in one of the classes whereas redundant features cancel out in taking the difference in computing the distance. (Rajpoot, 2003) have shown efficacy of Hellinger distance applied to ADWPT. Selecting coefficients with greater discriminative power helps the classifier perform well. Full algorithm is mentioned in algorithm 1. Multi class classification can then be handled in this framework using one-vs-one classification. We select the top m&apos; features from the wavelet representation for representing the data in the classification task. Time complexity of the algorithm is polynomial. The method is based on adaptive discriminant wavelet packet transform (ADWPT) (Qureshi et al., 2008). Therefore, we name it as improvised ada</context>
</contexts>
<marker>Rajpoot, 2003</marker>
<rawString>NM Rajpoot. 2003. Local discriminant wavelet packet basis for texture classification. In SPIE Wavelets X, pages 774–783. SPIE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geraldo Xexeo</author>
<author>Jano de Souza</author>
<author>Patricia F Castro</author>
<author>Wallace A Pinheiro</author>
</authors>
<title>Using wavelets to classify documents. Web Intelligence and Intelligent Agent Technology,</title>
<date>2008</date>
<booktitle>IEEE/WIC/ACM International Conference on,</booktitle>
<pages>1--272</pages>
<marker>Xexeo, de Souza, Castro, Pinheiro, 2008</marker>
<rawString>Geraldo Xexeo, Jano de Souza, Patricia F. Castro, and Wallace A. Pinheiro. 2008. Using wavelets to classify documents. Web Intelligence and Intelligent Agent Technology, IEEE/WIC/ACM International Conference on, 1:272–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuldeep Yadav</author>
<author>Ponnurangam Kumaraguru</author>
<author>Atul Goyal</author>
<author>Ashish Gupta</author>
<author>Vinayak Naik</author>
</authors>
<title>Smsassassin: Crowdsourcing driven mobile-based system for sms spam filtering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th Workshop on Mobile Computing Systems and Applications, HotMobile ’11,</booktitle>
<pages>1--6</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="16619" citStr="Yadav et al., 2011" startWordPosition="2599" endWordPosition="2602"> SMS Spam 2 10681 6550 3000 3050 10681 9981 0.4 Energy 00 0.5 0 −0.5 0 2) SMS Spam 1: UCI spam dataset (Almeida, ) consists of 5,574 instances of SMS classified into SPAM and HAM classes. SPAM class is defined as messages which are not useful and HAM is the class of useful messages. We compare our results with the results they published in their paper (Almeida et al., 2013). Therefore, we followed the same experiment procedure as cited in the paper. First 30% samples were used in train and the rest in test set as reported in the paper. 3) SMS Spam 2: The dataset was published by Yadav et al. (Yadav et al., 2011). It consists of 2000 SMS, 1000 SPAM and 1000 HAM messages. Experiment settings are same as that of dataset SMS Spam 1. The goal of our experiments is to examine the effectiveness of the proposed algorithm in feature selection for short text datasets. We measure the effectiveness of the feature selection technique with respect to the increase in accuracy in the final machine learning task. Our method does not depend on a specific classifier used in the final classification. Therefore, we used Number of dimensions Figure 2: Spam Classification Accuracy comparison across various feature selectio</context>
</contexts>
<marker>Yadav, Kumaraguru, Goyal, Gupta, Naik, 2011</marker>
<rawString>Kuldeep Yadav, Ponnurangam Kumaraguru, Atul Goyal, Ashish Gupta, and Vinayak Naik. 2011. Smsassassin: Crowdsourcing driven mobile-based system for sms spam filtering. In Proceedings of the 12th Workshop on Mobile Computing Systems and Applications, HotMobile ’11, pages 1–6, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Machine Learning, ICML ’97,</booktitle>
<pages>412--420</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4492" citStr="Yang and Pedersen, 1997" startWordPosition="665" endWordPosition="668">well (Learned and Willsky, 1995), using very few coefficients. This leads to considerable decrease in the dimensionality of the feature space along with increase in classification accuracy. Additionally, we optimise the procedure to select the most discriminative features from WPT representation. To the best of our knowledge this is the first attempt to apply an algorithm based on wavelet packet transform to the feature selection in short text classification. 2 Related Work Feature selection has been widely adopted for dimensionality reduction of text datasets in the past. Yiming Yang et al. (Yang and Pedersen, 1997) performed a comparative study of some of these methods including, document frequency, information gain(IG), mutual 321 Proceedings of the 19th Conference on Computational Language Learning, pages 321–326, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics information(MI), X2-test(CHI) and term strength(TS). They concluded that IG and CHI are the most effective in aggressive dimensionality reduction. The mRMR technique proposed by (Peng et al., 2005) selects the best feature subset by increasing relevancy of feature with target class and reducing redundancy betw</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proceedings of the Fourteenth International Conference on Machine Learning, ICML ’97, pages 412–420, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>