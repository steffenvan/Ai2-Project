<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9973935">
Part-of-Speech Induction in Dependency Trees for Statistical Machine
Translation
</title>
<author confidence="0.9049405">
Akihiro Tamura†,$, Taro Watanabe†, Eiichiro Sumita†,
Hiroya Takamura$, Manabu Okumura$
</author>
<affiliation confidence="0.949584">
† National Institute of Information and Communications Technology
</affiliation>
<email confidence="0.828719">
{akihiro.tamura, taro.watanabe, eiichiro.sumita}@nict.go.jp
</email>
<note confidence="0.490899">
† Precision and Intelligence Laboratory, Tokyo Institute of Technology
</note>
<email confidence="0.990024">
{takamura, oku}@pi.titech.ac.jp
</email>
<sectionHeader confidence="0.994552" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999904">
This paper proposes a nonparametric
Bayesian method for inducing Part-of-
Speech (POS) tags in dependency trees
to improve the performance of statistical
machine translation (SMT). In particular,
we extend the monolingual infinite tree
model (Finkel et al., 2007) to a bilin-
gual scenario: each hidden state (POS tag)
of a source-side dependency tree emits a
source word together with its aligned tar-
get word, either jointly (joint model), or
independently (independent model). Eval-
uations of Japanese-to-English translation
on the NTCIR-9 data show that our in-
duced Japanese POS tags for dependency
trees improve the performance of a forest-
to-string SMT system. Our independent
model gains over 1 point in BLEU by re-
solving the sparseness problem introduced
in the joint model.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999817444444445">
In recent years, syntax-based SMT has made
promising progress by employing either depen-
dency parsing (Lin, 2004; Ding and Palmer, 2005;
Quirk et al., 2005; Shen et al., 2008; Mi and Liu,
2010) or constituency parsing (Huang et al., 2006;
Liu et al., 2006; Galley et al., 2006; Mi and Huang,
2008; Zhang et al., 2008; Cohn and Blunsom,
2009; Liu et al., 2009; Mi and Liu, 2010; Zhang
et al., 2011) on the source side, the target side,
or both. However, dependency parsing, which
is a popular choice for Japanese, can incorporate
only shallow syntactic information, i.e., POS tags,
compared with the richer syntactic phrasal cate-
gories in constituency parsing. Moreover, exist-
ing POS tagsets might not be optimal for SMT
because they are constructed without considering
the language in the other side. Consider the ex-
amples in Figure 1. The Japanese noun “利用” in
</bodyText>
<figure confidence="0.785937333333333">
jbtst_ [t 4!43 t v bt Rim TjL,- tsL%
noun particle noun particle noun verb auxiliary verb
You can not use the Internet .
U bt Rim At � #b5
noun particlenoun noun particle verb
I pay usage fees .
</figure>
<figureCaption confidence="0.9650195">
Figure 1: Examples of Existing Japanese POS
Tags and Dependency Structures
</figureCaption>
<bodyText confidence="0.999663413793103">
Example 1 corresponds to the English verb “use”,
while that in Example 2 corresponds to the English
noun “usage”. Thus, Japanese nouns act like verbs
in English in one situation, and nouns in English
in another. If we could discriminate POS tags for
two cases, we might improve the performance of a
Japanese-to-English SMT system.
In the face of the above situations, this pa-
per proposes an unsupervised method for inducing
POS tags for SMT, and aims to improve the perfor-
mance of syntax-based SMT by utilizing the in-
duced POS tagset. The proposed method is based
on the infinite tree model proposed by Finkel et
al. (2007), which is a nonparametric Bayesian
method for inducing POS tags from syntactic de-
pendency structures. In this model, hidden states
represent POS tags, the observations they generate
represent the words themselves, and tree structures
represent syntactic dependencies between pairs of
POS tags.
The proposed method builds on this model by
incorporating the aligned words in the other lan-
guage into the observations. We investigate two
types of models: (i) a joint model and (ii) an in-
dependent model. In the joint model, each hid-
den state jointly emits both a source word and its
aligned target word as an observation. The in-
dependent model separately emits words in two
languages from hidden states. By inferring POS
</bodyText>
<figure confidence="0.9948955">
[Example 1]
Japanese POS:
[Example 2]
Japanese POS:
</figure>
<page confidence="0.975027">
841
</page>
<note confidence="0.913874">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 841–851,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99951768">
tags based on bilingual observations, both mod-
els can induce POS tags by incorporating infor-
mation from the other language. Consider, for ex-
ample, inducing a POS tag for the Japanese word “
利用” in Figure 1. Under a monolingual induction
method (e.g., the infinite tree model), the “利用”
in Example 1 and 2 would both be assigned the
same POS tag since they share the same observa-
tion. However, our models would assign separate
tags for the two different instances since the “利
用” in Example 1 and Example 2 could be disam-
biguated by encoding the target-side information,
either “use” or “usage”, in the observations.
Inference is efficiently carried out by beam sam-
pling (Gael et al., 2008), which combines slice
sampling and dynamic programming. Experi-
ments are carried out on the NTCIR-9 Japanese-
to-English task using a binarized forest-to-string
SMT system with dependency trees as its source
side. Our bilingually-induced tagset signifi-
cantly outperforms the original tagset and the
monolingually-induced tagset. Further, our inde-
pendent model achieves a more than 1 point gain
in BLEU, which resolves the sparseness problem
introduced by the bi-word observations.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999949583333334">
A number of unsupervised methods have been
proposed for inducing POS tags. Early methods
have the problem that the number of possible POS
tags must be provided preliminarily. This limita-
tion has been overcome by automatically adjust-
ing the number of possible POS tags using non-
parametric Bayesian methods (Finkel et al., 2007;
Gael et al., 2009; Blunsom and Cohn, 2011; Sirts
and Alum¨ae, 2012). Gael et al. (2009) applied
infinite HMM (iHMM) (Beal et al., 2001; Teh
et al., 2006), a nonparametric version of HMM,
to POS induction. Blunsom and Cohn (2011)
used a hierarchical Pitman-Yor process prior to the
transition and emission distribution for sophisti-
cated smoothing. Sirts and Alum¨ae (2012) built a
model that combines POS induction and morpho-
logical segmentation into a single learning prob-
lem. Finkel et al. (2007) proposed the infinite
tree model, which represents recursive branching
structures over infinite hidden states and induces
POS tags from syntactic dependency structures. In
the following, we overview the infinite tree model,
which is the basis of our proposed model. In par-
ticular, we will describe the independent children
</bodyText>
<figureCaption confidence="0.923681">
Figure 2: A Graphical Representation of the Finite
Tree Model
</figureCaption>
<bodyText confidence="0.955464666666667">
model (Finkel et al., 2007), where children are
dependent only on their parents, used in our pro-
posed model&apos;.
</bodyText>
<subsectionHeader confidence="0.947645">
2.1 Finite Tree Model
</subsectionHeader>
<bodyText confidence="0.994785">
We first review the finite tree model, which can
be graphically represented in Figure 2. Let
Tt denote the tree whose root node is t. A
node t has a hidden state zt (the POS tag)
and an observation xt (the word). The prob-
ability of a tree Tt, pT(Tt), is recursively de-
</bodyText>
<equation confidence="0.750117">
fined: pT(Tt) = p(xt|zt) rl p(zt′|zt)pT (Tt′),
t′∈c(t)
</equation>
<bodyText confidence="0.996081">
where c(t) is the set of the children of t.
Let each hidden state variable have C possible
values indexed by k. For each state k, there is
a parameter Ok which parameterizes the observa-
tion distribution for that state: xt|zt ∼ F(Ozt). Ok
is distributed according to a prior distribution H:
Ok ∼ H.
Transitions between states are governed by
Markov dynamics parameterized by π, where
πij = p(zc(t) = j|zt = i) and πk are the transition
probabilities from the parent’s state k. πk is dis-
tributed according to a Dirichlet distribution with
parameter ρ: πk|ρ ∼ Dirichlet(ρ, ... , ρ). The
hidden state of each child zt′ is distributed accord-
ing to a multinomial distribution πzt specific to the
parent’s state zt: zt′|zt ∼ Multinomial(πzt).
</bodyText>
<subsectionHeader confidence="0.985687">
2.2 Infinite Tree Model
</subsectionHeader>
<bodyText confidence="0.9974132">
In the infinite tree model, the number of possible
hidden states is potentially infinite. The infinite
model is formed by extending the finite tree model
using a hierarchical Dirichlet process (HDP) (Teh
et al., 2006). The reason for using an HDP rather
&apos;Finkel et al. (2007) originally proposed three types of
models: besides the independent children model, the simul-
taneous children model and the markov children model. Al-
though we could apply the other two models, we leave this
for future work.
</bodyText>
<equation confidence="0.998358272727273">
πk  |p ~ Dirichlet(p,..., p
)
φk
H
ρ πk
z1
z2 z3
H φk
k=1,...,C
x1 x2 x3
~
</equation>
<page confidence="0.949823">
842
</page>
<figure confidence="0.910322617647059">
ala Ilk
H Ok
Y N
00
z1
x1 x2 x3
β
π α β α β
 |, ~ DP( ,
k 0 0
φ k
z2 z3
|γ
~
H
~
GEM
( )
γ
)
LY0 Ilk
Y 8 z1
00
k
z4 zS z6
9
A
10
11
!&amp;11*11
1111
11
+pay° +fees° +usage°
z2 z3
</figure>
<figureCaption confidence="0.999643">
Figure 4: An Example of the Joint Model
</figureCaption>
<bodyText confidence="0.820511666666667">
tween Figure 2 an
d Figure 3 is whether the number
of copies of the state is finite or not.
</bodyText>
<figureCaption confidence="0.595554875">
transitions from different 3 Bilingual Infinite Tree Model
states. A similar
measure was adopted in iHMM (Beal et al., 2001).
HDP is a set of DPs coupled through a shared
random base measure which is itself drawn from
a DP: each Gk
with a shared base
and
</figureCaption>
<figure confidence="0.63101716">
H) with a global
base measure H. From the viewpoint of the stick-
breaking
(Sethuraman
parent’s
∼DP(α0,G0)
G0,
G0∼DP(γ,
construction3
, 1994), the
HDP is interpreted as follows: G0 = 00 d the independent model.
k′=1
d Gk = 00 3.1 Joint Model
k′=1
H,
β|γ∼GEM(γ),πk|α0,β∼DP(α0,β),ϕk∼
zt′|zt∼Multinomial(πzt),
xt|zt ∼ F (ϕzt).
is a measure on measures.
has two parameters, a
scaling parameter
and a base measure H:
H).
3Sethuraman (1994) showed a definition of a measure
G—
</figure>
<figureCaption confidence="0.249989">
First, infinite sequences of i.i.d variables
</figureCaption>
<bodyText confidence="0.575605">
and
</bodyText>
<equation confidence="0.968302740740741">
are generated:
—
k �
. Then, G is defined as:
k
�
� i (1 —
),
G=
a is defined by this process, then we
write a
2DP
It
α
DP(α,
DP(α0,G0).
(π′k)∞k=1
(ϕk)∞k=1
π′k|α0
Beta(1, α0),ϕ
0
π
=π
∑
π′l
∑∞k=1πkδϕk.If
GEM(α0).
</equation>
<bodyText confidence="0.980142625">
while observations in the monolingual in-
finite tree model represent only source words. For
each source word, all the aligned target words are
copied and sorted in alphabetical order, and then
concatenated into a single observation. Therefore,
a single target word maybe emitted multiple times
if the target word is aligned with multiple source
words. Likewise, there maybe target words which
may not be emitted by our model, if the target
words are not align
words4,
ed.
Figure 4 shows the process of generating Exam-
ple 2 in Figure 1 through the joint model, where
aligned words are jointly emitted as observations.
In Figure 4, the POS tag of “利用” (z5) generates
</bodyText>
<figure confidence="0.704967">
H O
</figure>
<figureCaption confidence="0.9632605">
Figure 3: A Graphical Representation of the Infi-
nite Tree Model
</figureCaption>
<bodyText confidence="0.931128295454545">
than a simple Dirichlet process (DP)2 (Ferguson,
1973) is that we have to introduce coupling across
measure
an
βk′δϕk′
where
πkk′δϕk′,
β∼GEM(γ),
and
H.
We regard each Gk as two coindexed distribu-
tions:
a distribution over the transition prob-
abilities from the
state k, and
an ob-
servation distribution for the state
πk∼DP(α0,β),
ϕk′∼
πk,
parent’s
ϕk′,
k′. Then, the
infinite tree model is formally defined as follows:
Figure 3 shows the graphical representation of the
infinite tree model. The primary difference
be-
We propose a bilingual variant of the infinite tree
model, the bilingual infinite tree model, which uti-
lizes information from the other language. Specifi-
cally, the proposed model introduces bilingual ob-
servations by embedding the aligned target words
in the source-side dependency trees. This paper
proposes two types of models that differ in their
processes for generating observations: the joint
model an
The joint model is a simple application of the in-
finite tree model under a bilingual scenario. The
model is formally defined in the same way as in
Section 2.2 and is graphically represented simi-
larly to Figure 3. The only difference from the
infinite tree model is the instances of observations
(xt). Observations in the joint model are the com-
bination of source words and their aligned target
</bodyText>
<footnote confidence="0.983116">
4When no target words are aligned, we simply add a
NULL target word.
</footnote>
<page confidence="0.997547">
843
</page>
<figureCaption confidence="0.9873755">
Figure 5: A Graphical Representation of the Inde-
pendent Model
</figureCaption>
<bodyText confidence="0.999816833333333">
the string “利用+usage” as the observation (x5).
Similarly, the POS tag of “利用” in Example 1
would generate the string “利用+use”. Hence, this
model can assign different POS tags to the two dif-
ferent instances of the word “利用”, based on the
different observation distributions in inference.
</bodyText>
<subsectionHeader confidence="0.952605">
3.2 Independent Model
</subsectionHeader>
<bodyText confidence="0.99989675">
The joint model is prone to a data sparseness prob-
lem, since each observation is a combination of a
source word and its aligned target word. Thus, we
propose an independent model, where each hidden
state generates a source word and its aligned target
word separately. For the aligned target side, we in-
troduce an observation variable x′t for each zt and
a parameter ϕ′k for each state k, which parame-
terizes adistinct distribution over the observations
x′t for that state. ϕ′k is distributed according to a
prior distribution H′. Specifically, the indepen-
dent model is formally defined as follows:
</bodyText>
<equation confidence="0.9971886">
)3|γ ∼ GEM(γ),
7rk|α0, )3 ∼ DP(α0, )3),
ϕk ∼ H, ϕ′k ∼ H′,
zt′|zt ∼ Multinomial(7rzt),
xt|zt ∼ F(ϕzt), x′t|zt ∼ F′(ϕ′ zt).
</equation>
<bodyText confidence="0.999080272727273">
When multiple target words are aligned to a single
source word, each aligned word is generated sepa-
rately from observation distribution parameterized
by ϕ′k.
Figure 5 graphs the process of generating Ex-
ample 2 in Figure 1 using the independent model.
x′t and ϕ′k are introduced for aligned target words.
The state of “利用” (z5) generates the Japanese
word “利用” as x5 and the English word “usage”
as x′5. Due to this factorization, the independent
model is less subject to the sparseness problem.
</bodyText>
<subsectionHeader confidence="0.999287">
3.3 Introduction of Other Factors
</subsectionHeader>
<bodyText confidence="0.999990565217391">
We assumed the surface form of aligned target
words as additional observations in previous sec-
tions. Here, we introduce additional factors, i.e.,
the POS of aligned target words, in the observa-
tions. Note that POSs of target words are assigned
by a POS tagger in the target language and are not
inferred in the proposed model.
First, we can simply replace surface forms of
target words with their POSs to overcome the
sparseness problem. Second, we can incorporate
both information from the target language as ob-
servations. In the joint model, two pieces of in-
formation are concatenated into a single observa-
tion. In the independent model, we introduce ob-
servation variables (e.g., x′t and x′′t ) and parame-
ters (e.g., ϕ′k and ϕ′′k) for each information. Specif-
ically, x′t and ϕ′k are introduced for the surface
form of aligned words, and x′′t and ϕ′′k for the POS
of aligned words. Consider, for example, Example
1 in Figure 1. The POS tag of “利用” generates the
string “利用+use+verb” as the observation in the
joint model, while it generates “利用”, “use”, and
“verb” independently in the independent model.
</bodyText>
<subsectionHeader confidence="0.902364">
3.4 POS Refinement
</subsectionHeader>
<bodyText confidence="0.999942928571429">
We have assumed a completely unsupervised way
of inducing POS tags in dependency trees. An-
other realistic scenario is to refine the existing POS
tags (Finkel et al., 2007; Liang et al., 2007) so
that each refined sub-POS tag may reflect the in-
formation from the aligned words while preserv-
ing the handcrafted distinction from original POS
tagset. Major difference is that we introduce sep-
arate transition probabilities 7rsk and observation
distributions (ϕsk, ϕ′sk ) for each existing POS tag s.
Then, each node t is constrained to follow the dis-
tributions indicated by the initially assigned POS
tag st, and we use the pair (st, zt) as a state repre-
sentation.
</bodyText>
<sectionHeader confidence="0.940413" genericHeader="method">
3.5 Inference
</sectionHeader>
<subsectionHeader confidence="0.436909">
k
</subsectionHeader>
<bodyText confidence="0.999887444444444">
In inference, we find the state set that maximizes
the posterior probability of state transitions given
observations (i.e., P(z1:n|x1:n)). However, we
cannot evaluate the probability for all possible
states because the number of states is infinite.
Finkel et al. (2007) presented a sampling algo-
rithm for the infinite tree model, which is based on
the Gibbs sampling in the direct assignment rep-
resentation for iHMM (Teh et al., 2006). In the
</bodyText>
<figure confidence="0.995494518518518">
y ,B z1
β
πk  |α0,β~DP(α0,β
|γ
~
GEM( )
γ
)
z2 z3
φ k
~
H, φk
~
H
&apos;
CY0 Ilk
#y3 pay
z4 z5
z6
H Ok
I
1
*4
fees
bt NONE *Jffl usage J� NONE
H’ O&apos;k
co
</figure>
<page confidence="0.992172">
844
</page>
<bodyText confidence="0.972525777777778">
Gibbs sampling, individual hidden state variables
are resampled conditioned on all other variables.
Unfortunately, its convergence is slow in HMM
settings because sequential data is likely to have
a strong correlation between hidden states (Gael
et al., 2008).
We present an inference procedure based on
beam sampling (Gael et al., 2008) for the joint
model and the independent model. Beam sam-
pling limits the number of possible state transi-
tions for each node to a finite number using slice
sampling (Neal, 2003), and then efficiently sam-
ples whole hidden state transitions using dynamic
programming. Beam sampling does not suffer
from slow convergence as in Gibbs sampling by
sampling the whole state variables at once. In ad-
dition, Gael et al. (2008) showed that beam sam-
pling is more robust to initialization and hyperpa-
rameter choice than Gibbs sampling.
Specifically, we introduce an auxiliary variable
ut for each node in a dependency tree to limit
the number of possible transitions. Our procedure
alternates between sampling each of the follow-
ing variables: the auxiliary variables u, the state
assignments z, the transition probabilities 7r, the
shared DP parameters ,3, and the hyperparameters
α0 and γ. We can parallelize procedures in sam-
pling u and z because the slice sampling for u and
the dynamic programing for z are independent for
each sentence. See Gael el al. (2009) for details.
The only difference between inferences in the
joint model and the independent model is in com-
puting the posterior probability of state transi-
tions given observations (e.g., p(z1:n|x1:n) and
p(z1:n|x1:n, x′1:n)) in sampling z. In the follow-
ing, we describe each sampling stage. See Teh et
al., (2006) for details of sampling 7r, ,3, α0 and γ.
Sampling u:
Each ut is sampled from the uniform distribu-
tion on [0,πzd(t)zt], where d(t) is the parent of
t: ut ∼ Uniform(0,πzd(t)zt). Note that ut is a
positive number, since each transition probability
is larger than zero.
πzd(t)zt
Sampling z:
Possible values k of zt are divided into the two
sets using ut: a finite set with πzd(t)k &gt; ut and
an infinite set with πzd(t)k ≤ ut. The beam
sampling considers only the former set. Owing
to the truncation of the latter set, we can compute
the posterior probability of a state zt given ob-
servations for all t (t = 1, ... , T) using dynamic
programming as follows:
In the joint model, p(zt|xσ(t), uσ(t)) ∝
</bodyText>
<equation confidence="0.946299">
p(xt|zt) · E p(zd(t)|xσ(d(t)), uσ(d(t))),
zd(t):πzd(t)zt&gt;ut
</equation>
<bodyText confidence="0.988819">
and in the independent model,
</bodyText>
<equation confidence="0.977018333333333">
p(zt|xσ(t), x′σ(t), uσ(t)) ∝ p(xt|zt) · p(x′t|zt)
�· p(zd(t)|xσ(d(t)),x′σ(d(t)), uσ(d(t))),
zd(t):πzd(t)zt&gt;ut
</equation>
<bodyText confidence="0.989173259259259">
where xσ(t) (or uσ(t)) denotes the set of xt (or ut)
on the path from the root node to the node t in a
tree.
In our experiments, we assume that F (ϕk)
is Multinomial(ϕk) and H is Dirichlet(ρ, ... , ρ),
which is the same in Finkel et al. (2007). Un-
der this assumption, the posterior probability of an
observation is as follows: p(xt|zt) = n·k + Nρ,
nxtk + ρ
where nxk is the number of observations x with
state k, n·k is the number of hidden states whose
values are k, and N is the total number of observa-
tions x. Similarly, p(x′t|zt) = n·k + N′ρ ′,
�nx′tk + ρ′ where
N′ is the total number of observations x′.
When the posterior probability of a state zt
given observations for all t can be computed,
we first sample the state of each leaf node and
then perform backtrack sampling for every other
zt where the zt is sampled given the sample
for zc(t) as follows: p(zt|zc(t), x1:T, u1:T) ∝
p(zt |xσ(t), uσ(t)) Ht′,c(t) p(zt′|zt, ut′).
Sampling 7r:
We introduce a count variable nij ∈ n,
which is the number of observations with
state j whose parent’s state is i. Then,
we sample 7r using the Dirichlet distri-
</bodyText>
<equation confidence="0.985931333333333">
bution: (πk1, . . . , πkK, Ek/=K+1 πkk′) ∼
Dirichlet(nk1 + α0β1, . . . , nkK +
��
</equation>
<bodyText confidence="0.939691111111111">
α0βK, α0 k′=K+1 βk′), where K is the
number of distinct states in z.
Sampling ,C3:
We introduce a set of auxiliary variables m, where
mij ∈ m is the number of elements of 7rj
corresponding to βi. The conditional distribu-
tion of each variable is p(mij = m|z,,3, α0) ∝
S(nij, m)(α0βj)m, where S(n, m) are unsigned
Stirling numbers of the first kinds.
</bodyText>
<equation confidence="0.887622777777778">
5S(0, 0) = S(1, 1) = 1, S(n, 0) = 0 for n &gt; 0,
S(n, m) = 0 for m &gt; n, and S(n + 1, m) = S(n, m −
1) + nS(n, m) for others.
845
The parameters ,3 are sampled using the Dirich-
let distribution: (β1,... ,βK, E∞k′=K+1 βk′) �
Dirichlet(m·1, ... , m·K, γ), where m·k =
EKk′=1 mk′k.
Sampling α0:
</equation>
<bodyText confidence="0.994390666666667">
α0 is parameterized by a gamma hyperprior
with hyperparameters αa and αb. We introduce
two types of auxiliary variables for each state
</bodyText>
<equation confidence="0.7389604">
(k = 1, ... , K), wk E [0, 1] and vk E 10, 11.
The conditional distribution of each wk is
p(wk|α0) a wα0
k (1−wk)n·k−1 and that of each vk
vk,
</equation>
<bodyText confidence="0.943514">
where n·k = EKk′=1 nk′k.
The conditional distribution of α0 given wk
</bodyText>
<equation confidence="0.963547571428571">
and vk (k = 1,...,K) is p(α0|w, v) a
αa−1+m..−∑K k=1 vk
αe
0
m·· = EK EK
k′=1 k′′=1 mk′k′′.
Sampling γ:
</equation>
<bodyText confidence="0.997122166666667">
γ is parameterized by a gamma hyperprior with
hyperparameters γa and γb. We introduce an
auxiliary variable η, whose conditional distribu-
tion is p(η|γ) a ηγ(1 − η)m··−1. The con-
ditional distribution of γ given η is p(γ|η) a
γγa−1+Ke−γ(γb−lo9η).
</bodyText>
<sectionHeader confidence="0.998587" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999967125">
We tested our proposed models under the
NTCIR-9 Japanese-to-English patent translation
task (Goto et al., 2011), consisting of approxi-
mately 3.2 million bilingual sentences. Both the
development data and the test data consist of 2,000
sentences. We also used the NTCIR-7 develop-
ment data consisting of 2,741 sentences for devel-
opment testing purposes.
</bodyText>
<subsectionHeader confidence="0.978866">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.97805642">
We evaluated our bilingual infinite tree model
for POS induction using an in-house developed
syntax-based forest-to-string SMT system. In
the training process, the following steps are per-
formed sequentially: preprocessing, inducing a
POS tagset for a source language, training a POS
tagger and a dependency parser, and training a
forest-to-string MT model.
Step 1. Preprocessing
We used the first 10,000 Japanese-English sen-
tence pairs in the NTCIR-9 training data for in-
ducing a POS tagset for Japanese6. The Japanese
sentences were segmented using MeCab7, and the
English sentences were tokenized and POS tagged
using TreeTagger (Schmid, 1994), where 43 and
58 types of POS tags are included in the Japanese
sentences and the English sentences, respectively.
The Japanese POS tags come from the second-
level POS tags in the IPA POS tagset (Asahara and
Matsumoto, 2003) and the English POS tags are
derived from the Penn Treebank. Note that the
Japanese POS tags are used for initialization of
hidden states and the English POS tags are used
as observations emitted by hidden states.
Word-by-word alignments for the sentence
pairs are produced by first running GIZA++ (Och
and Ney, 2003) in both directions and then com-
bining the alignments using the “grow-diag-final-
and” heuristic (Koehn et al., 2003). Note that we
ran GIZA++ on all of the NTCIR-9 training data
in order to obtain better alignements.
The Japanese sentences are parsed using
CaboCha (Kudo and Matsumoto, 2002), which
generates dependency structures using a phrasal
unit called a bunsetsu$, rather than a word unit as
in English or Chinese dependency parsing. Since
we focus on the word-level POS induction, each
bunsetsu-based dependency tree is converted into
its corresponding word-based dependency tree us-
ing the following heuristic9: first, the last func-
tion word inside each bunsetsu is identified as
the head word10; then, the remaining words are
treated as dependents of the head word in the same
bunsetsu; finally, a bunsetsu-based dependency
structure is transformed to a word-based depen-
dency structure by preserving the head/modifier
relationships of the determined head words.
Step 2. POS Induction
A POS tag for each word in the Japanese sentences
is inferred by our bilingual infinite tree model, ei-
</bodyText>
<footnote confidence="0.7192125">
6Due to the high computational cost, we did not use all
the NTCIR-9 training data. We leave scaling up to a larger
dataset for future work.
7http://mecab.googlecode.com/svn/
trunk/mecab/doc/index.html
8A bunsetsu is the smallest meaningful sequence con-
</footnote>
<bodyText confidence="0.901852">
sisting of a content word and accompanying function words
(e.g., a noun and a particle).
9We could use other word-based dependency trees such
as trees by the infinite PCFG model (Liang et al., 2007)
and syntactic-head or semantic-head dependency trees in
Nakazawa and Kurohashi (2012), although it is not our major
focus. We leave this for future work.
</bodyText>
<footnote confidence="0.6809135">
10If no function words exist in a bunsetsu, the last content
word is treated as the head word.
</footnote>
<equation confidence="0.942291">
is p(vk|α0) a (n·k )
α0
−α0(αb−∑K k=1 lo9wk), where
</equation>
<page confidence="0.991994">
846
</page>
<bodyText confidence="0.981436795918367">
ther jointly (Joint) or independently (Ind). We
also performed monolingual induction of Finkel et
al. (2007) for comparison (Mono). In each model,
a sequence of sampling u, z, 7r, ,3, α0, and ry is
repeated 10,000 times. In sampling α0 and ry, hy-
perparameters αa, αb, rya, and ryb are set to 2, 1,
1, and 1, respectively, which is the same setting in
Gael et al. (2008). In sampling z, parameters p, p′,
..., are set to 0.01. In the experiments, three types
of factors for the aligned English words are com-
pared: surface forms (‘s’), POS tags (‘P’), and the
combination of both (‘s+P’). Further, two types of
inference frameworks are compared: induction
(IND) and refinement (REF). In both frame-
works, each hidden state zt is first initialized to
the POS tags assigned by MeCab (the IPA POS
tagset), and then each state is updated through
the inference procedure described in Section 3.5.
Note that in REF, the sampling distribution over
zt is constrained to include only states that are a
refinement of the initially assigned POS tag.
Step 3. Training a POS Tagger and a
Dependency Parser
In this step, we train a Japanese dependency parser
from the 10,000 Japanese dependency trees with
the induced POS tags which are derived from Step
2. We employed a transition-based dependency
parser which can jointly learn POS tagging and
dependency parsing (Hatori et al., 2011) under an
incremental framework&amp;quot;. Note that the learned
parser can identify dependencies between words
and attach an induced POS tag for each word.
Step 4. Training a Forest-to-String MT
In this step, we train a forest-to-string MT model
based on the learned dependency parser in Step 3.
We use an in-house developed hypergraph-based
toolkit, cicada, for training and decoding with a
tree-to-string model, which has been successfully
employed in our previous work for system com-
bination (Watanabe and Sumita, 2011) and online
learning (Watanabe, 2012). All the Japanese and
English sentences in the NTCIR-9 training data
are segmented in the same way as in Step 1, and
then each Japanese sentence is parsed by the de-
pendency parser learned in Step 3, which simul-
taneously assigns induced POS tags and word de-
pendencies. Finally, a forest-to-string MT model
is learned with Zhang et al., (2011), which ex-
tracts translation rules by a forest-based variant of
</bodyText>
<footnote confidence="0.381149">
llhttp://triplet.cc/software/corbit/
</footnote>
<table confidence="0.991706555555556">
IND REF
B5 27.54
Mono 27.66 26.83
Joint[s] 28.00 28.00
Joint[P] 26.36 26.72
Joint[s+P] 27.99 27.82
Ind[s] 28.00 27.93
Ind[P] 28.11 28.63
Ind[s+P] 28.13 28.62
</table>
<tableCaption confidence="0.9441275">
Table 1: Performance on Japanese-to-English
Translation Measured by BLEU (%)
</tableCaption>
<bodyText confidence="0.9994882">
the GHKM algorithm (Mi and Huang, 2008) af-
ter each parse tree is restructured into a binarized
packed forest. Parameters are tuned on the devel-
opment data using xBLEU (Rosti et al., 2011) as
an objective and L-BFGS (Liu and Nocedal, 1989)
as an optimization toolkit, since it is stable and less
prone to randomness, unlike MERT (Och, 2003)
or PRO (Hopkins and May, 2011). The develop-
ment test data is used to set up hyperparameters,
i.e., to terminate tuning iterations.
When translating Japanese sentences, a parse
tree for each sentence is constructed in the same
way as described earlier in this step, and then the
parse trees are translated into English sentences
using the learned forest-to-string MT model.
</bodyText>
<subsectionHeader confidence="0.99301">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.986099571428572">
Table 1 shows the performance for the test data
measured by case sensitive BLEU (Papineni et
al., 2002). We also present the performance of
our baseline forest-to-string MT system (B5) us-
ing the original IPA POS tags. In Table 1, num-
bers in bold indicate that the systems outperform
the baselines, B5 and Mono. Under the Moses
phrase-based SMT system (Koehn et al., 2007)
with the default settings, we achieved a 26.80%
BLEU score.
Table 1 shows that the proposed systems outper-
form the baseline Mono. The differences between
the performance of Ind[s+P] and Mono are statis-
tically significant in the bootstrap method (Koehn,
2004), with a 1% significance level both in IND
and REF. The results indicate that integrating the
aligned target-side information in POS induction
makes inferred tagsets more suitable for SMT.
Table 1 also shows that the independent model
is more effective for SMT than the joint model.
This means that sparseness is a severe problem in
</bodyText>
<page confidence="0.993059">
847
</page>
<table confidence="0.99597825">
Model IND REF
Joint[s+P] 164 620
Ind[s+P] 102 517
IPA POS tags 42
</table>
<tableCaption confidence="0.999162">
Table 2: The Number of POS Tags
</tableCaption>
<bodyText confidence="0.99972280952381">
POS induction when jointly encoding bilingual in-
formation into observations. Additionally, all the
systems using the independent model outperform
B5. The improvements are statistically significant
in the bootstrap method (Koehn, 2004), with a 1%
significance level. The results show that the pro-
posed models can generate more favorable POS
tagsets for SMT than an existing POS tagset.
In Table 1, REFs are at least comparable to, or
better than, INDs except for Mono. This shows
that REF achieves better performance by preserv-
ing the clues from the original POS tagset. How-
ever, REF may suffer sever overfitting problem
for Mono since no bilingual information was in-
corporated. Further, when the full-level IPA POS
tags12 were used in B5, the system achieved a
27.49% BLEU score, which is worse than the re-
sult using the second-level IPA POS tags. This
means that manual refinement without bilingual
information may also cause an overfitting problem
in MT.
</bodyText>
<sectionHeader confidence="0.999601" genericHeader="method">
5 Discussion
</sectionHeader>
<subsectionHeader confidence="0.996695">
5.1 Comparison to the IPA POS Tagset
</subsectionHeader>
<bodyText confidence="0.95210335">
Table 2 shows the number of the IPA POS tags
used in the experiments and the POS tags induced
by the proposed models. This table shows that
each induced tagset contains more POS tags than
the IPA POS tagset. In the experimental data,
some of Japanese verbs correspond to genuine En-
glish verbs, some are nominalized, and others cor-
respond to English past participle verbs or present
participle verbs which modify other words. Re-
spective examples are “I use a card.”, “Using the
index is faster.”, and “I explain using an exam-
ple.”, where all the underlined words correspond
to the same Japanese word, “fい”, whose IPA
POS tag is a verb. Ind[s+P] in REF generated
the POS tagset where the three types are assigned
to separate POS groups.
The Japanese particle “Iz” is sometimes at-
tached to nouns to give them adverb roles. For
12377 types of full-level IPA POS tags were included in our
experimental data.
</bodyText>
<table confidence="0.9996058">
Tagging Dependency
IND REF IND REF
Original 90.37 93.62
Mono 90.75 88.04 91.77 91.51
Joint[s] 89.08 86.73 91.55 91.14
Joint[P] 80.54 79.98 91.06 91.29
Joint[s+P] 87.56 84.92 91.31 91.10
Ind[s] 87.62 84.33 92.06 92.58
Ind[P] 90.21 88.50 92.85 93.03
Ind[s+P] 89.57 86.12 92.96 92.78
</table>
<tableCaption confidence="0.999894">
Table 3: Tagging and Dependency Accuracy (%)
</tableCaption>
<bodyText confidence="0.999838909090909">
example, “�� (mutual) Iz” is translated as
the adverb “mutually” in English. Other times,
it is attached to words to make them the objects
of verbs. For example, “V (he) Iz 4xra
(give)” is translated as “give him”. The POS tags
by Ind[s+P] in REF discriminated the two types.
These examples show that the proposed mod-
els can disambiguate POS tags that have different
functions in English, whereas the IPA POS tagset
treats them jointly. Thus, such discrimination im-
proves the performance of a forest-to-string SMT.
</bodyText>
<subsectionHeader confidence="0.8259695">
5.2 Impact of Tagging and Dependency
Accuracy
</subsectionHeader>
<bodyText confidence="0.999989708333333">
The performance of our methods depends not only
on the quality of the induced tag sets but also on
the performance of the dependency parser learned
in Step 3 of Section 4.1. We cannot directly eval-
uate the tagging accuracy of the parser trained
through Step 3 because we do not have any data
with induced POS tags other than the 10,000-
sentence data gained through Step 2. Thus we split
the 10,000 data into the first 9,000 data for train-
ing and the remaining 1,000 for testing, and then
a dependency parser was learned in the same way
as in Step 3.
Table 3 shows the results. Original is the per-
formance of the parser learned from the training
data with the original POS tagset. Note that the de-
pendency accuracies are measured on the automat-
ically parsed dependency trees, not on the syntac-
tically correct gold standard trees. Thus Original
achieved the best dependency accuracy.
In Table 3, the performance for our bilingually-
induced POSs, Joint and Ind, are lower than
Original and Mono. It seems performing pars-
ing and tagging with the bilingually-induced POS
tagset is too difficult when only monolingual in-
</bodyText>
<page confidence="0.99108">
848
</page>
<bodyText confidence="0.999971545454545">
formation is available to the parser. However, our
bilingually-induced POSs, except for Joint[P],
with the lower accuracies are more effective for
SMT than the monolingually-induced POSs and
the original POSs, as indicated in Table 1. The
tagging accuracies for Joint[P] both in IND and
REF are significantly lower than the others, while
the dependency accuracies do not differ signifi-
cantly. The lower tagging accuracies may directly
reflect the lower translation qualities for Joint[P]
in Table 1.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99999580952381">
We proposed a novel method for inducing POS
tags for SMT. The proposed method is a non-
parametric Bayesian method, which infers hidden
states (i.e., POS tags) based on observations repre-
senting not only source words themselves but also
aligned target words. Our experiments showed
that a more favorable POS tagset can be induced
by integrating aligned information, and further-
more, the POS tagset generated by the proposed
method is more effective for SMT than an existing
POS tagset (the IPA POS tagset).
Even though we employed word alignment
from GIZA++ with potential errors, large gains
were achieved using our proposed method. We
would like to investigate the influence of align-
ment errors in the future. In addition, we are plan-
ning to prove the effectiveness of our proposed
method for language pairs other than Japanese-to-
English. We are also planning to introduce our
proposed method to other syntax-based SMT, such
as a string-to-tree SMT and a tree-to-tree SMT.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99979325">
We thank Isao Goto for helpful discussions and
anonymous reviewers for valuable comments. We
also thank Jun Hatori for helping us to apply his
software, Corbit, to our induced POS tagsets.
</bodyText>
<sectionHeader confidence="0.99819" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998439571428572">
Masayuki Asahara and Yuji Matsumoto. 2003.
IPADIC User Manual. Technical report, Japan.
Matthew J. Beal, Zoubin Ghahramani, and Carl E. Ras-
mussen. 2001. The Infinite Hidden Markov Model.
In Advances in Neural Information Processing Sys-
tems, pages 577–584.
Phil Blunsom and Trevor Cohn. 2011. A Hierarchical
Pitman-Yor Process HMM for Unsupervised Part of
Speech Induction. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 865–874.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian
Model of Syntax-Directed Tree to String Grammar
Induction. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 352–361.
Yuan Ding and Martha Palmer. 2005. Machine Trans-
lation Using Probabilistic Synchronous Dependency
Insertion Grammars. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 541–548.
Thomas S. Ferguson. 1973. A Bayesian Analysis
of Some Nonparametric Problems. The Annals of
Statistics, 1(2):209–230.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2007. The Infinite Tree. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 272–279.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam Sampling for
the Infinite Hidden Markov Model. In Proceedings
of the 25th International Conference on Machine
Learning, pages 1088–1095.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsuper-
vised PoS tagging. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2 - Volume 2, pages 678–687.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961–968.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the Patent
Machine Translation Task at the NTCIR-9 Work-
shop. In Proceedings of the 9th NTCIR Workshop,
pages 559–578.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2011. Incremental Joint POS Tag-
ging and Dependency Parsing in Chinese. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 1216–1224.
Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A Syntax-Directed Translator with Extended Do-
main of Locality. In Proceedings of the Workshop on
</reference>
<page confidence="0.989321">
849
</page>
<reference confidence="0.9993445">
Computationally Hard Problemsand Joint Inference
in Speech and Language Processing, pages 1–8.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference: North American Chapter of the Associ-
ation for Computational Linguistics, pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constrantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics on In-
teractive Poster and Demonstration Sessions, pages
177–180.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388–395.
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning, pages 63–69.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The Infinite PCFG using Hierarchi-
cal Dirichlet Processes. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 688–697.
Dekang Lin. 2004. A Path-based Transfer Model for
Machine Translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguis-
tics, pages 625–630.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B, 45(3):503–528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609–616.
Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improv-
ing Tree-to-Tree Translation with Packed Forests.
In Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics and the
4th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation ofNatural
Language Processing, pages 558–566.
Haitao Mi and Liang Huang. 2008. Forest-based
Translation Rule Extraction. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 206–214.
Haitao Mi and Qun Liu. 2010. Constituency to De-
pendency Translation with Forests. In Proceedings
of the 48th Annual Conference of the Association for
Computational Linguistics, pages 1433–1442.
Toshiaki Nakazawa and Sadao Kurohashi. 2012.
Alignment by Bilingual Generation and Monolin-
gual Derivation. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics,
pages 1963–1978.
Radford M. Neal. 2003. Slice Sampling. Annals of
Statistics, 31:705–767.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19–51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proceedings of the 43rd
Annual Conference of the Association for Computa-
tional Linguistics, pages 271–279.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2011. Expected BLEU
Training for Graphs: BBN System Description for
WMT11 System Combination Task. In Proceedings
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 159–165.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in
Language Processing, pages 44–49.
Jayaram Sethuraman. 1994. A Constructive Definition
of Dirichlet Priors. Statistica Sinica, 4(2):639–650.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. In Proceedings of the 46th Annual Confer-
ence of the Association for Computational Linguis-
tics: Human Language Technologies, pages 577–
585.
Kairit Sirts and Tanel Alum¨ae. 2012. A Hierarchi-
cal Dirichlet Process Model for Joint Part-of-Speech
and Morphology Induction. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 407–416.
</reference>
<page confidence="0.97763">
850
</page>
<reference confidence="0.99929092">
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
Processes. Journal of the American Statistical Asso-
ciation, 101(476):1566–1581.
Taro Watanabe and Eiichiro Sumita. 2011. Machine
Translation System Combination by Confusion For-
est. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1249–1257.
Taro Watanabe. 2012. Optimized Online Rank Learn-
ing for Machine Translation. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 253–262.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation
Model. In Proceedings of the 46th Annual Confer-
ence of the Association for Computational Linguis-
tics: Human Language Technologies, pages 559–
567.
Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.
2011. Binarized Forest to String Translation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 19–24.
</reference>
<page confidence="0.998668">
851
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.075869">
<title confidence="0.999335">Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</title>
<author confidence="0.689976">Taro Eiichiro</author>
<email confidence="0.321514">Manabu</email>
<note confidence="0.284240333333333">Institute of Information and Communications taro.watanabe, and Intelligence Laboratory, Tokyo Institute of</note>
<abstract confidence="0.999240761904762">This paper proposes a nonparametric Bayesian method for inducing Part-of- Speech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>IPADIC User Manual.</title>
<date>2003</date>
<tech>Technical report, Japan.</tech>
<contexts>
<context position="21900" citStr="Asahara and Matsumoto, 2003" startWordPosition="3737" endWordPosition="3740">set for a source language, training a POS tagger and a dependency parser, and training a forest-to-string MT model. Step 1. Preprocessing We used the first 10,000 Japanese-English sentence pairs in the NTCIR-9 training data for inducing a POS tagset for Japanese6. The Japanese sentences were segmented using MeCab7, and the English sentences were tokenized and POS tagged using TreeTagger (Schmid, 1994), where 43 and 58 types of POS tags are included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the secondlevel POS tags in the IPA POS tagset (Asahara and Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumot</context>
</contexts>
<marker>Asahara, Matsumoto, 2003</marker>
<rawString>Masayuki Asahara and Yuji Matsumoto. 2003. IPADIC User Manual. Technical report, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew J Beal</author>
<author>Zoubin Ghahramani</author>
<author>Carl E Rasmussen</author>
</authors>
<title>The Infinite Hidden Markov Model.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>577--584</pages>
<contexts>
<context position="5556" citStr="Beal et al., 2001" startWordPosition="883" endWordPosition="886">r independent model achieves a more than 1 point gain in BLEU, which resolves the sparseness problem introduced by the bi-word observations. 2 Related Work A number of unsupervised methods have been proposed for inducing POS tags. Early methods have the problem that the number of possible POS tags must be provided preliminarily. This limitation has been overcome by automatically adjusting the number of possible POS tags using nonparametric Bayesian methods (Finkel et al., 2007; Gael et al., 2009; Blunsom and Cohn, 2011; Sirts and Alum¨ae, 2012). Gael et al. (2009) applied infinite HMM (iHMM) (Beal et al., 2001; Teh et al., 2006), a nonparametric version of HMM, to POS induction. Blunsom and Cohn (2011) used a hierarchical Pitman-Yor process prior to the transition and emission distribution for sophisticated smoothing. Sirts and Alum¨ae (2012) built a model that combines POS induction and morphological segmentation into a single learning problem. Finkel et al. (2007) proposed the infinite tree model, which represents recursive branching structures over infinite hidden states and induces POS tags from syntactic dependency structures. In the following, we overview the infinite tree model, which is the</context>
<context position="8527" citStr="Beal et al., 2001" startWordPosition="1424" endWordPosition="1427">l and the markov children model. Although we could apply the other two models, we leave this for future work. πk |p ~ Dirichlet(p,..., p ) φk H ρ πk z1 z2 z3 H φk k=1,...,C x1 x2 x3 ~ 842 ala Ilk H Ok Y N 00 z1 x1 x2 x3 β π α β α β |, ~ DP( , k 0 0 φ k z2 z3 |γ ~ H ~ GEM ( ) γ ) LY0 Ilk Y 8 z1 00 k z4 zS z6 9 A 10 11 !&amp;11*11 1111 11 +pay° +fees° +usage° z2 z3 Figure 4: An Example of the Joint Model tween Figure 2 an d Figure 3 is whether the number of copies of the state is finite or not. transitions from different 3 Bilingual Infinite Tree Model states. A similar measure was adopted in iHMM (Beal et al., 2001). HDP is a set of DPs coupled through a shared random base measure which is itself drawn from a DP: each Gk with a shared base and H) with a global base measure H. From the viewpoint of the stickbreaking (Sethuraman parent’s ∼DP(α0,G0) G0, G0∼DP(γ, construction3 , 1994), the HDP is interpreted as follows: G0 = 00 d the independent model. k′=1 d Gk = 00 3.1 Joint Model k′=1 H, β|γ∼GEM(γ),πk|α0,β∼DP(α0,β),ϕk∼ zt′|zt∼Multinomial(πzt), xt|zt ∼ F (ϕzt). is a measure on measures. has two parameters, a scaling parameter and a base measure H: H). 3Sethuraman (1994) showed a definition of a measure G— </context>
</contexts>
<marker>Beal, Ghahramani, Rasmussen, 2001</marker>
<rawString>Matthew J. Beal, Zoubin Ghahramani, and Carl E. Rasmussen. 2001. The Infinite Hidden Markov Model. In Advances in Neural Information Processing Systems, pages 577–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>865--874</pages>
<contexts>
<context position="5463" citStr="Blunsom and Cohn, 2011" startWordPosition="867" endWordPosition="870">et significantly outperforms the original tagset and the monolingually-induced tagset. Further, our independent model achieves a more than 1 point gain in BLEU, which resolves the sparseness problem introduced by the bi-word observations. 2 Related Work A number of unsupervised methods have been proposed for inducing POS tags. Early methods have the problem that the number of possible POS tags must be provided preliminarily. This limitation has been overcome by automatically adjusting the number of possible POS tags using nonparametric Bayesian methods (Finkel et al., 2007; Gael et al., 2009; Blunsom and Cohn, 2011; Sirts and Alum¨ae, 2012). Gael et al. (2009) applied infinite HMM (iHMM) (Beal et al., 2001; Teh et al., 2006), a nonparametric version of HMM, to POS induction. Blunsom and Cohn (2011) used a hierarchical Pitman-Yor process prior to the transition and emission distribution for sophisticated smoothing. Sirts and Alum¨ae (2012) built a model that combines POS induction and morphological segmentation into a single learning problem. Finkel et al. (2007) proposed the infinite tree model, which represents recursive branching structures over infinite hidden states and induces POS tags from syntact</context>
</contexts>
<marker>Blunsom, Cohn, 2011</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2011. A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 865–874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian Model of Syntax-Directed Tree to String Grammar Induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>352--361</pages>
<contexts>
<context position="1536" citStr="Cohn and Blunsom, 2009" startWordPosition="221" endWordPosition="224">h translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 1 Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in jbtst_ [t 4!43 t v bt Rim TjL,- tsL% noun particle noun particle noun verb aux</context>
</contexts>
<marker>Cohn, Blunsom, 2009</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2009. A Bayesian Model of Syntax-Directed Tree to String Grammar Induction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 352–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>541--548</pages>
<contexts>
<context position="1331" citStr="Ding and Palmer, 2005" startWordPosition="182" endWordPosition="185"> (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 1 Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they ar</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 541–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Ferguson</author>
</authors>
<title>A Bayesian Analysis of Some Nonparametric Problems.</title>
<date>1973</date>
<journal>The Annals of Statistics,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="10175" citStr="Ferguson, 1973" startWordPosition="1716" endWordPosition="1717">ical order, and then concatenated into a single observation. Therefore, a single target word maybe emitted multiple times if the target word is aligned with multiple source words. Likewise, there maybe target words which may not be emitted by our model, if the target words are not align words4, ed. Figure 4 shows the process of generating Example 2 in Figure 1 through the joint model, where aligned words are jointly emitted as observations. In Figure 4, the POS tag of “利用” (z5) generates H O Figure 3: A Graphical Representation of the Infinite Tree Model than a simple Dirichlet process (DP)2 (Ferguson, 1973) is that we have to introduce coupling across measure an βk′δϕk′ where πkk′δϕk′, β∼GEM(γ), and H. We regard each Gk as two coindexed distributions: a distribution over the transition probabilities from the state k, and an observation distribution for the state πk∼DP(α0,β), ϕk′∼ πk, parent’s ϕk′, k′. Then, the infinite tree model is formally defined as follows: Figure 3 shows the graphical representation of the infinite tree model. The primary difference beWe propose a bilingual variant of the infinite tree model, the bilingual infinite tree model, which utilizes information from the other lang</context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Thomas S. Ferguson. 1973. A Bayesian Analysis of Some Nonparametric Problems. The Annals of Statistics, 1(2):209–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>The Infinite Tree.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>272--279</pages>
<contexts>
<context position="667" citStr="Finkel et al., 2007" startWordPosition="76" endWordPosition="79">r Statistical Machine Translation Akihiro Tamura†,$, Taro Watanabe†, Eiichiro Sumita†, Hiroya Takamura$, Manabu Okumura$ † National Institute of Information and Communications Technology {akihiro.tamura, taro.watanabe, eiichiro.sumita}@nict.go.jp † Precision and Intelligence Laboratory, Tokyo Institute of Technology {takamura, oku}@pi.titech.ac.jp Abstract This paper proposes a nonparametric Bayesian method for inducing Part-ofSpeech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 1 Introduction In recent years, syntax-based SMT has made promising progress by emplo</context>
<context position="2951" citStr="Finkel et al. (2007)" startWordPosition="466" endWordPosition="469">mple 1 corresponds to the English verb “use”, while that in Example 2 corresponds to the English noun “usage”. Thus, Japanese nouns act like verbs in English in one situation, and nouns in English in another. If we could discriminate POS tags for two cases, we might improve the performance of a Japanese-to-English SMT system. In the face of the above situations, this paper proposes an unsupervised method for inducing POS tags for SMT, and aims to improve the performance of syntax-based SMT by utilizing the induced POS tagset. The proposed method is based on the infinite tree model proposed by Finkel et al. (2007), which is a nonparametric Bayesian method for inducing POS tags from syntactic dependency structures. In this model, hidden states represent POS tags, the observations they generate represent the words themselves, and tree structures represent syntactic dependencies between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word </context>
<context position="5420" citStr="Finkel et al., 2007" startWordPosition="859" endWordPosition="862">ource side. Our bilingually-induced tagset significantly outperforms the original tagset and the monolingually-induced tagset. Further, our independent model achieves a more than 1 point gain in BLEU, which resolves the sparseness problem introduced by the bi-word observations. 2 Related Work A number of unsupervised methods have been proposed for inducing POS tags. Early methods have the problem that the number of possible POS tags must be provided preliminarily. This limitation has been overcome by automatically adjusting the number of possible POS tags using nonparametric Bayesian methods (Finkel et al., 2007; Gael et al., 2009; Blunsom and Cohn, 2011; Sirts and Alum¨ae, 2012). Gael et al. (2009) applied infinite HMM (iHMM) (Beal et al., 2001; Teh et al., 2006), a nonparametric version of HMM, to POS induction. Blunsom and Cohn (2011) used a hierarchical Pitman-Yor process prior to the transition and emission distribution for sophisticated smoothing. Sirts and Alum¨ae (2012) built a model that combines POS induction and morphological segmentation into a single learning problem. Finkel et al. (2007) proposed the infinite tree model, which represents recursive branching structures over infinite hidd</context>
<context position="7795" citStr="Finkel et al. (2007)" startWordPosition="1260" endWordPosition="1263"> are the transition probabilities from the parent’s state k. πk is distributed according to a Dirichlet distribution with parameter ρ: πk|ρ ∼ Dirichlet(ρ, ... , ρ). The hidden state of each child zt′ is distributed according to a multinomial distribution πzt specific to the parent’s state zt: zt′|zt ∼ Multinomial(πzt). 2.2 Infinite Tree Model In the infinite tree model, the number of possible hidden states is potentially infinite. The infinite model is formed by extending the finite tree model using a hierarchical Dirichlet process (HDP) (Teh et al., 2006). The reason for using an HDP rather &apos;Finkel et al. (2007) originally proposed three types of models: besides the independent children model, the simultaneous children model and the markov children model. Although we could apply the other two models, we leave this for future work. πk |p ~ Dirichlet(p,..., p ) φk H ρ πk z1 z2 z3 H φk k=1,...,C x1 x2 x3 ~ 842 ala Ilk H Ok Y N 00 z1 x1 x2 x3 β π α β α β |, ~ DP( , k 0 0 φ k z2 z3 |γ ~ H ~ GEM ( ) γ ) LY0 Ilk Y 8 z1 00 k z4 zS z6 9 A 10 11 !&amp;11*11 1111 11 +pay° +fees° +usage° z2 z3 Figure 4: An Example of the Joint Model tween Figure 2 an d Figure 3 is whether the number of copies of the state is finite </context>
<context position="14424" citStr="Finkel et al., 2007" startWordPosition="2425" endWordPosition="2428">e.g., x′t and x′′t ) and parameters (e.g., ϕ′k and ϕ′′k) for each information. Specifically, x′t and ϕ′k are introduced for the surface form of aligned words, and x′′t and ϕ′′k for the POS of aligned words. Consider, for example, Example 1 in Figure 1. The POS tag of “利用” generates the string “利用+use+verb” as the observation in the joint model, while it generates “利用”, “use”, and “verb” independently in the independent model. 3.4 POS Refinement We have assumed a completely unsupervised way of inducing POS tags in dependency trees. Another realistic scenario is to refine the existing POS tags (Finkel et al., 2007; Liang et al., 2007) so that each refined sub-POS tag may reflect the information from the aligned words while preserving the handcrafted distinction from original POS tagset. Major difference is that we introduce separate transition probabilities 7rsk and observation distributions (ϕsk, ϕ′sk ) for each existing POS tag s. Then, each node t is constrained to follow the distributions indicated by the initially assigned POS tag st, and we use the pair (st, zt) as a state representation. 3.5 Inference k In inference, we find the state set that maximizes the posterior probability of state transit</context>
<context position="18364" citStr="Finkel et al. (2007)" startWordPosition="3103" endWordPosition="3106">can compute the posterior probability of a state zt given observations for all t (t = 1, ... , T) using dynamic programming as follows: In the joint model, p(zt|xσ(t), uσ(t)) ∝ p(xt|zt) · E p(zd(t)|xσ(d(t)), uσ(d(t))), zd(t):πzd(t)zt&gt;ut and in the independent model, p(zt|xσ(t), x′σ(t), uσ(t)) ∝ p(xt|zt) · p(x′t|zt) �· p(zd(t)|xσ(d(t)),x′σ(d(t)), uσ(d(t))), zd(t):πzd(t)zt&gt;ut where xσ(t) (or uσ(t)) denotes the set of xt (or ut) on the path from the root node to the node t in a tree. In our experiments, we assume that F (ϕk) is Multinomial(ϕk) and H is Dirichlet(ρ, ... , ρ), which is the same in Finkel et al. (2007). Under this assumption, the posterior probability of an observation is as follows: p(xt|zt) = n·k + Nρ, nxtk + ρ where nxk is the number of observations x with state k, n·k is the number of hidden states whose values are k, and N is the total number of observations x. Similarly, p(x′t|zt) = n·k + N′ρ ′, �nx′tk + ρ′ where N′ is the total number of observations x′. When the posterior probability of a state zt given observations for all t can be computed, we first sample the state of each leaf node and then perform backtrack sampling for every other zt where the zt is sampled given the sample fo</context>
<context position="24176" citStr="Finkel et al. (2007)" startWordPosition="4101" endWordPosition="4104">ence consisting of a content word and accompanying function words (e.g., a noun and a particle). 9We could use other word-based dependency trees such as trees by the infinite PCFG model (Liang et al., 2007) and syntactic-head or semantic-head dependency trees in Nakazawa and Kurohashi (2012), although it is not our major focus. We leave this for future work. 10If no function words exist in a bunsetsu, the last content word is treated as the head word. is p(vk|α0) a (n·k ) α0 −α0(αb−∑K k=1 lo9wk), where 846 ther jointly (Joint) or independently (Ind). We also performed monolingual induction of Finkel et al. (2007) for comparison (Mono). In each model, a sequence of sampling u, z, 7r, ,3, α0, and ry is repeated 10,000 times. In sampling α0 and ry, hyperparameters αa, αb, rya, and ryb are set to 2, 1, 1, and 1, respectively, which is the same setting in Gael et al. (2008). In sampling z, parameters p, p′, ..., are set to 0.01. In the experiments, three types of factors for the aligned English words are compared: surface forms (‘s’), POS tags (‘P’), and the combination of both (‘s+P’). Further, two types of inference frameworks are compared: induction (IND) and refinement (REF). In both frameworks, each h</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2007</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2007. The Infinite Tree. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 272–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Yunus Saatci</author>
<author>Yee Whye Teh</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Beam Sampling for the Infinite Hidden Markov Model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>1088--1095</pages>
<marker>Van Gael, Saatci, Teh, Ghahramani, 2008</marker>
<rawString>Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and Zoubin Ghahramani. 2008. Beam Sampling for the Infinite Hidden Markov Model. In Proceedings of the 25th International Conference on Machine Learning, pages 1088–1095.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Andreas Vlachos</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>The infinite HMM for unsupervised PoS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>678--687</pages>
<marker>Van Gael, Vlachos, Ghahramani, 2009</marker>
<rawString>Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahramani. 2009. The infinite HMM for unsupervised PoS tagging. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, pages 678–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Translation Models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="1472" citStr="Galley et al., 2006" startWordPosition="209" endWordPosition="212">dently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 1 Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in jbtst_ [t 4!43</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Bin Lu</author>
<author>Ka Po Chow</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<date>2011</date>
<booktitle>Overview of the Patent Machine Translation Task at the NTCIR-9 Workshop. In Proceedings of the 9th NTCIR Workshop,</booktitle>
<pages>559--578</pages>
<contexts>
<context position="20767" citStr="Goto et al., 2011" startWordPosition="3561" endWordPosition="3564">ribution of each wk is p(wk|α0) a wα0 k (1−wk)n·k−1 and that of each vk vk, where n·k = EKk′=1 nk′k. The conditional distribution of α0 given wk and vk (k = 1,...,K) is p(α0|w, v) a αa−1+m..−∑K k=1 vk αe 0 m·· = EK EK k′=1 k′′=1 mk′k′′. Sampling γ: γ is parameterized by a gamma hyperprior with hyperparameters γa and γb. We introduce an auxiliary variable η, whose conditional distribution is p(η|γ) a ηγ(1 − η)m··−1. The conditional distribution of γ given η is p(γ|η) a γγa−1+Ke−γ(γb−lo9η). 4 Experiment We tested our proposed models under the NTCIR-9 Japanese-to-English patent translation task (Goto et al., 2011), consisting of approximately 3.2 million bilingual sentences. Both the development data and the test data consist of 2,000 sentences. We also used the NTCIR-7 development data consisting of 2,741 sentences for development testing purposes. 4.1 Experimental Setup We evaluated our bilingual infinite tree model for POS induction using an in-house developed syntax-based forest-to-string SMT system. In the training process, the following steps are performed sequentially: preprocessing, inducing a POS tagset for a source language, training a POS tagger and a dependency parser, and training a forest</context>
</contexts>
<marker>Goto, Lu, Chow, Sumita, Tsou, 2011</marker>
<rawString>Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K. Tsou. 2011. Overview of the Patent Machine Translation Task at the NTCIR-9 Workshop. In Proceedings of the 9th NTCIR Workshop, pages 559–578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Hatori</author>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Incremental Joint POS Tagging and Dependency Parsing in Chinese.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1216--1224</pages>
<contexts>
<context position="25438" citStr="Hatori et al., 2011" startWordPosition="4320" endWordPosition="4323"> POS tags assigned by MeCab (the IPA POS tagset), and then each state is updated through the inference procedure described in Section 3.5. Note that in REF, the sampling distribution over zt is constrained to include only states that are a refinement of the initially assigned POS tag. Step 3. Training a POS Tagger and a Dependency Parser In this step, we train a Japanese dependency parser from the 10,000 Japanese dependency trees with the induced POS tags which are derived from Step 2. We employed a transition-based dependency parser which can jointly learn POS tagging and dependency parsing (Hatori et al., 2011) under an incremental framework&amp;quot;. Note that the learned parser can identify dependencies between words and attach an induced POS tag for each word. Step 4. Training a Forest-to-String MT In this step, we train a forest-to-string MT model based on the learned dependency parser in Step 3. We use an in-house developed hypergraph-based toolkit, cicada, for training and decoding with a tree-to-string model, which has been successfully employed in our previous work for system combination (Watanabe and Sumita, 2011) and online learning (Watanabe, 2012). All the Japanese and English sentences in the N</context>
</contexts>
<marker>Hatori, Matsuzaki, Miyao, Tsujii, 2011</marker>
<rawString>Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2011. Incremental Joint POS Tagging and Dependency Parsing in Chinese. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1216–1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as Ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<contexts>
<context position="27037" citStr="Hopkins and May, 2011" startWordPosition="4576" endWordPosition="4579">software/corbit/ IND REF B5 27.54 Mono 27.66 26.83 Joint[s] 28.00 28.00 Joint[P] 26.36 26.72 Joint[s+P] 27.99 27.82 Ind[s] 28.00 27.93 Ind[P] 28.11 28.63 Ind[s+P] 28.13 28.62 Table 1: Performance on Japanese-to-English Translation Measured by BLEU (%) the GHKM algorithm (Mi and Huang, 2008) after each parse tree is restructured into a binarized packed forest. Parameters are tuned on the development data using xBLEU (Rosti et al., 2011) as an objective and L-BFGS (Liu and Nocedal, 1989) as an optimization toolkit, since it is stable and less prone to randomness, unlike MERT (Och, 2003) or PRO (Hopkins and May, 2011). The development test data is used to set up hyperparameters, i.e., to terminate tuning iterations. When translating Japanese sentences, a parse tree for each sentence is constructed in the same way as described earlier in this step, and then the parse trees are translated into English sentences using the learned forest-to-string MT model. 4.2 Experimental Results Table 1 shows the performance for the test data measured by case sensitive BLEU (Papineni et al., 2002). We also present the performance of our baseline forest-to-string MT system (B5) using the original IPA POS tags. In Table 1, nu</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as Ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>A Syntax-Directed Translator with Extended Domain of Locality.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1433" citStr="Huang et al., 2006" startWordPosition="201" endWordPosition="204">ther jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 1 Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. T</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A Syntax-Directed Translator with Extended Domain of Locality. In Proceedings of the Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference: North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="22333" citStr="Koehn et al., 2003" startWordPosition="3808" endWordPosition="3811">e included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the secondlevel POS tags in the IPA POS tagset (Asahara and Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu$, rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the following heuristic9: first, the last function word inside each bunsetsu is identified as the head word10; then, the r</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the 2003 Human Language Technology Conference: North American Chapter of the Association for Computational Linguistics, pages 48–54.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar, Alexandra Constrantin, and Evan Herbst.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="27776" citStr="Koehn et al., 2007" startWordPosition="4697" endWordPosition="4700">panese sentences, a parse tree for each sentence is constructed in the same way as described earlier in this step, and then the parse trees are translated into English sentences using the learned forest-to-string MT model. 4.2 Experimental Results Table 1 shows the performance for the test data measured by case sensitive BLEU (Papineni et al., 2002). We also present the performance of our baseline forest-to-string MT system (B5) using the original IPA POS tags. In Table 1, numbers in bold indicate that the systems outperform the baselines, B5 and Mono. Under the Moses phrase-based SMT system (Koehn et al., 2007) with the default settings, we achieved a 26.80% BLEU score. Table 1 shows that the proposed systems outperform the baseline Mono. The differences between the performance of Ind[s+P] and Mono are statistically significant in the bootstrap method (Koehn, 2004), with a 1% significance level both in IND and REF. The results indicate that integrating the aligned target-side information in POS induction makes inferred tagsets more suitable for SMT. Table 1 also shows that the independent model is more effective for SMT than the joint model. This means that sparseness is a severe problem in 847 Mode</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constrantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics on Interactive Poster and Demonstration Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="28035" citStr="Koehn, 2004" startWordPosition="4740" endWordPosition="4741">performance for the test data measured by case sensitive BLEU (Papineni et al., 2002). We also present the performance of our baseline forest-to-string MT system (B5) using the original IPA POS tags. In Table 1, numbers in bold indicate that the systems outperform the baselines, B5 and Mono. Under the Moses phrase-based SMT system (Koehn et al., 2007) with the default settings, we achieved a 26.80% BLEU score. Table 1 shows that the proposed systems outperform the baseline Mono. The differences between the performance of Ind[s+P] and Mono are statistically significant in the bootstrap method (Koehn, 2004), with a 1% significance level both in IND and REF. The results indicate that integrating the aligned target-side information in POS induction makes inferred tagsets more suitable for SMT. Table 1 also shows that the independent model is more effective for SMT than the joint model. This means that sparseness is a severe problem in 847 Model IND REF Joint[s+P] 164 620 Ind[s+P] 102 517 IPA POS tags 42 Table 2: The Number of POS Tags POS induction when jointly encoding bilingual information into observations. Additionally, all the systems using the independent model outperform B5. The improvement</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese Dependency Analysis using Cascaded Chunking.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning,</booktitle>
<pages>63--69</pages>
<contexts>
<context position="22508" citStr="Kudo and Matsumoto, 2002" startWordPosition="3837" endWordPosition="3840"> Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu$, rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the following heuristic9: first, the last function word inside each bunsetsu is identified as the head word10; then, the remaining words are treated as dependents of the head word in the same bunsetsu; finally, a bunsetsu-based dependency structure is transformed to a word-based dependency struct</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese Dependency Analysis using Cascaded Chunking. In Proceedings of the 6th Conference on Natural Language Learning, pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The Infinite PCFG using Hierarchical Dirichlet Processes.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>688--697</pages>
<contexts>
<context position="14445" citStr="Liang et al., 2007" startWordPosition="2429" endWordPosition="2432">and parameters (e.g., ϕ′k and ϕ′′k) for each information. Specifically, x′t and ϕ′k are introduced for the surface form of aligned words, and x′′t and ϕ′′k for the POS of aligned words. Consider, for example, Example 1 in Figure 1. The POS tag of “利用” generates the string “利用+use+verb” as the observation in the joint model, while it generates “利用”, “use”, and “verb” independently in the independent model. 3.4 POS Refinement We have assumed a completely unsupervised way of inducing POS tags in dependency trees. Another realistic scenario is to refine the existing POS tags (Finkel et al., 2007; Liang et al., 2007) so that each refined sub-POS tag may reflect the information from the aligned words while preserving the handcrafted distinction from original POS tagset. Major difference is that we introduce separate transition probabilities 7rsk and observation distributions (ϕsk, ϕ′sk ) for each existing POS tag s. Then, each node t is constrained to follow the distributions indicated by the initially assigned POS tag st, and we use the pair (st, zt) as a state representation. 3.5 Inference k In inference, we find the state set that maximizes the posterior probability of state transitions given observatio</context>
<context position="23762" citStr="Liang et al., 2007" startWordPosition="4032" endWordPosition="4035"> relationships of the determined head words. Step 2. POS Induction A POS tag for each word in the Japanese sentences is inferred by our bilingual infinite tree model, ei6Due to the high computational cost, we did not use all the NTCIR-9 training data. We leave scaling up to a larger dataset for future work. 7http://mecab.googlecode.com/svn/ trunk/mecab/doc/index.html 8A bunsetsu is the smallest meaningful sequence consisting of a content word and accompanying function words (e.g., a noun and a particle). 9We could use other word-based dependency trees such as trees by the infinite PCFG model (Liang et al., 2007) and syntactic-head or semantic-head dependency trees in Nakazawa and Kurohashi (2012), although it is not our major focus. We leave this for future work. 10If no function words exist in a bunsetsu, the last content word is treated as the head word. is p(vk|α0) a (n·k ) α0 −α0(αb−∑K k=1 lo9wk), where 846 ther jointly (Joint) or independently (Ind). We also performed monolingual induction of Finkel et al. (2007) for comparison (Mono). In each model, a sequence of sampling u, z, 7r, ,3, α0, and ry is repeated 10,000 times. In sampling α0 and ry, hyperparameters αa, αb, rya, and ryb are set to 2,</context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein. 2007. The Infinite PCFG using Hierarchical Dirichlet Processes. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 688–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A Path-based Transfer Model for Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>625--630</pages>
<contexts>
<context position="1308" citStr="Lin, 2004" startWordPosition="180" endWordPosition="181">idden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 1 Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Dekang Lin. 2004. A Path-based Transfer Model for Machine Translation. In Proceedings of the 20th International Conference on Computational Linguistics, pages 625–630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming B,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="26905" citStr="Liu and Nocedal, 1989" startWordPosition="4553" endWordPosition="4556">ng MT model is learned with Zhang et al., (2011), which extracts translation rules by a forest-based variant of llhttp://triplet.cc/software/corbit/ IND REF B5 27.54 Mono 27.66 26.83 Joint[s] 28.00 28.00 Joint[P] 26.36 26.72 Joint[s+P] 27.99 27.82 Ind[s] 28.00 27.93 Ind[P] 28.11 28.63 Ind[s+P] 28.13 28.62 Table 1: Performance on Japanese-to-English Translation Measured by BLEU (%) the GHKM algorithm (Mi and Huang, 2008) after each parse tree is restructured into a binarized packed forest. Parameters are tuned on the development data using xBLEU (Rosti et al., 2011) as an objective and L-BFGS (Liu and Nocedal, 1989) as an optimization toolkit, since it is stable and less prone to randomness, unlike MERT (Och, 2003) or PRO (Hopkins and May, 2011). The development test data is used to set up hyperparameters, i.e., to terminate tuning iterations. When translating Japanese sentences, a parse tree for each sentence is constructed in the same way as described earlier in this step, and then the parse trees are translated into English sentences using the learned forest-to-string MT model. 4.2 Experimental Results Table 1 shows the performance for the test data measured by case sensitive BLEU (Papineni et al., 20</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming B, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-toString Alignment Template for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1451" citStr="Liu et al., 2006" startWordPosition="205" endWordPosition="208">model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 1 Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-toString Alignment Template for Statistical Machine Translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Improving Tree-to-Tree Translation with Packed Forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation ofNatural Language Processing,</booktitle>
<pages>558--566</pages>
<marker>Liu, L¨u, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving Tree-to-Tree Translation with Packed Forests. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation ofNatural Language Processing, pages 558–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based Translation Rule Extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>206--214</pages>
<contexts>
<context position="1492" citStr="Mi and Huang, 2008" startWordPosition="213" endWordPosition="216">odel). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 1 Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in jbtst_ [t 4!43 t v bt Rim TjL,- ts</context>
<context position="26706" citStr="Mi and Huang, 2008" startWordPosition="4518" endWordPosition="4521">y as in Step 1, and then each Japanese sentence is parsed by the dependency parser learned in Step 3, which simultaneously assigns induced POS tags and word dependencies. Finally, a forest-to-string MT model is learned with Zhang et al., (2011), which extracts translation rules by a forest-based variant of llhttp://triplet.cc/software/corbit/ IND REF B5 27.54 Mono 27.66 26.83 Joint[s] 28.00 28.00 Joint[P] 26.36 26.72 Joint[s+P] 27.99 27.82 Ind[s] 28.00 27.93 Ind[P] 28.11 28.63 Ind[s+P] 28.13 28.62 Table 1: Performance on Japanese-to-English Translation Measured by BLEU (%) the GHKM algorithm (Mi and Huang, 2008) after each parse tree is restructured into a binarized packed forest. Parameters are tuned on the development data using xBLEU (Rosti et al., 2011) as an objective and L-BFGS (Liu and Nocedal, 1989) as an optimization toolkit, since it is stable and less prone to randomness, unlike MERT (Och, 2003) or PRO (Hopkins and May, 2011). The development test data is used to set up hyperparameters, i.e., to terminate tuning iterations. When translating Japanese sentences, a parse tree for each sentence is constructed in the same way as described earlier in this step, and then the parse trees are trans</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based Translation Rule Extraction. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>Constituency to Dependency Translation with Forests.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>1433--1442</pages>
<contexts>
<context position="1389" citStr="Mi and Liu, 2010" startWordPosition="194" endWordPosition="197">d together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 1 Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the othe</context>
</contexts>
<marker>Mi, Liu, 2010</marker>
<rawString>Haitao Mi and Qun Liu. 2010. Constituency to Dependency Translation with Forests. In Proceedings of the 48th Annual Conference of the Association for Computational Linguistics, pages 1433–1442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiaki Nakazawa</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Alignment by Bilingual Generation and Monolingual Derivation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics,</booktitle>
<pages>1963--1978</pages>
<contexts>
<context position="23848" citStr="Nakazawa and Kurohashi (2012)" startWordPosition="4043" endWordPosition="4046"> for each word in the Japanese sentences is inferred by our bilingual infinite tree model, ei6Due to the high computational cost, we did not use all the NTCIR-9 training data. We leave scaling up to a larger dataset for future work. 7http://mecab.googlecode.com/svn/ trunk/mecab/doc/index.html 8A bunsetsu is the smallest meaningful sequence consisting of a content word and accompanying function words (e.g., a noun and a particle). 9We could use other word-based dependency trees such as trees by the infinite PCFG model (Liang et al., 2007) and syntactic-head or semantic-head dependency trees in Nakazawa and Kurohashi (2012), although it is not our major focus. We leave this for future work. 10If no function words exist in a bunsetsu, the last content word is treated as the head word. is p(vk|α0) a (n·k ) α0 −α0(αb−∑K k=1 lo9wk), where 846 ther jointly (Joint) or independently (Ind). We also performed monolingual induction of Finkel et al. (2007) for comparison (Mono). In each model, a sequence of sampling u, z, 7r, ,3, α0, and ry is repeated 10,000 times. In sampling α0 and ry, hyperparameters αa, αb, rya, and ryb are set to 2, 1, 1, and 1, respectively, which is the same setting in Gael et al. (2008). In sampli</context>
</contexts>
<marker>Nakazawa, Kurohashi, 2012</marker>
<rawString>Toshiaki Nakazawa and Sadao Kurohashi. 2012. Alignment by Bilingual Generation and Monolingual Derivation. In Proceedings of the 24th International Conference on Computational Linguistics, pages 1963–1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Slice Sampling.</title>
<date>2003</date>
<journal>Annals of Statistics,</journal>
<pages>31--705</pages>
<contexts>
<context position="16038" citStr="Neal, 2003" startWordPosition="2711" endWordPosition="2712">~ H, φk ~ H &apos; CY0 Ilk #y3 pay z4 z5 z6 H Ok I 1 *4 fees bt NONE *Jffl usage J� NONE H’ O&apos;k co 844 Gibbs sampling, individual hidden state variables are resampled conditioned on all other variables. Unfortunately, its convergence is slow in HMM settings because sequential data is likely to have a strong correlation between hidden states (Gael et al., 2008). We present an inference procedure based on beam sampling (Gael et al., 2008) for the joint model and the independent model. Beam sampling limits the number of possible state transitions for each node to a finite number using slice sampling (Neal, 2003), and then efficiently samples whole hidden state transitions using dynamic programming. Beam sampling does not suffer from slow convergence as in Gibbs sampling by sampling the whole state variables at once. In addition, Gael et al. (2008) showed that beam sampling is more robust to initialization and hyperparameter choice than Gibbs sampling. Specifically, we introduce an auxiliary variable ut for each node in a dependency tree to limit the number of possible transitions. Our procedure alternates between sampling each of the following variables: the auxiliary variables u, the state assignmen</context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford M. Neal. 2003. Slice Sampling. Annals of Statistics, 31:705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics,</title>
<date>2003</date>
<pages>29--19</pages>
<contexts>
<context position="22218" citStr="Och and Ney, 2003" startWordPosition="3790" endWordPosition="3793">lish sentences were tokenized and POS tagged using TreeTagger (Schmid, 1994), where 43 and 58 types of POS tags are included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the secondlevel POS tags in the IPA POS tagset (Asahara and Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu$, rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the fol</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29:19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="27006" citStr="Och, 2003" startWordPosition="4572" endWordPosition="4573">lhttp://triplet.cc/software/corbit/ IND REF B5 27.54 Mono 27.66 26.83 Joint[s] 28.00 28.00 Joint[P] 26.36 26.72 Joint[s+P] 27.99 27.82 Ind[s] 28.00 27.93 Ind[P] 28.11 28.63 Ind[s+P] 28.13 28.62 Table 1: Performance on Japanese-to-English Translation Measured by BLEU (%) the GHKM algorithm (Mi and Huang, 2008) after each parse tree is restructured into a binarized packed forest. Parameters are tuned on the development data using xBLEU (Rosti et al., 2011) as an objective and L-BFGS (Liu and Nocedal, 1989) as an optimization toolkit, since it is stable and less prone to randomness, unlike MERT (Och, 2003) or PRO (Hopkins and May, 2011). The development test data is used to set up hyperparameters, i.e., to terminate tuning iterations. When translating Japanese sentences, a parse tree for each sentence is constructed in the same way as described earlier in this step, and then the parse trees are translated into English sentences using the learned forest-to-string MT model. 4.2 Experimental Results Table 1 shows the performance for the test data measured by case sensitive BLEU (Papineni et al., 2002). We also present the performance of our baseline forest-to-string MT system (B5) using the origin</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="27508" citStr="Papineni et al., 2002" startWordPosition="4651" endWordPosition="4654"> and Nocedal, 1989) as an optimization toolkit, since it is stable and less prone to randomness, unlike MERT (Och, 2003) or PRO (Hopkins and May, 2011). The development test data is used to set up hyperparameters, i.e., to terminate tuning iterations. When translating Japanese sentences, a parse tree for each sentence is constructed in the same way as described earlier in this step, and then the parse trees are translated into English sentences using the learned forest-to-string MT model. 4.2 Experimental Results Table 1 shows the performance for the test data measured by case sensitive BLEU (Papineni et al., 2002). We also present the performance of our baseline forest-to-string MT system (B5) using the original IPA POS tags. In Table 1, numbers in bold indicate that the systems outperform the baselines, B5 and Mono. Under the Moses phrase-based SMT system (Koehn et al., 2007) with the default settings, we achieved a 26.80% BLEU score. Table 1 shows that the proposed systems outperform the baseline Mono. The differences between the performance of Ind[s+P] and Mono are statistically significant in the bootstrap method (Koehn, 2004), with a 1% significance level both in IND and REF. The results indicate </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency Treelet Translation: Syntactically Informed Phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="1351" citStr="Quirk et al., 2005" startWordPosition="186" endWordPosition="189">side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 1 Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed withou</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically Informed Phrasal SMT. In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Expected BLEU Training for Graphs: BBN System Description for WMT11 System Combination Task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>159--165</pages>
<contexts>
<context position="26854" citStr="Rosti et al., 2011" startWordPosition="4544" endWordPosition="4547">and word dependencies. Finally, a forest-to-string MT model is learned with Zhang et al., (2011), which extracts translation rules by a forest-based variant of llhttp://triplet.cc/software/corbit/ IND REF B5 27.54 Mono 27.66 26.83 Joint[s] 28.00 28.00 Joint[P] 26.36 26.72 Joint[s+P] 27.99 27.82 Ind[s] 28.00 27.93 Ind[P] 28.11 28.63 Ind[s+P] 28.13 28.62 Table 1: Performance on Japanese-to-English Translation Measured by BLEU (%) the GHKM algorithm (Mi and Huang, 2008) after each parse tree is restructured into a binarized packed forest. Parameters are tuned on the development data using xBLEU (Rosti et al., 2011) as an objective and L-BFGS (Liu and Nocedal, 1989) as an optimization toolkit, since it is stable and less prone to randomness, unlike MERT (Och, 2003) or PRO (Hopkins and May, 2011). The development test data is used to set up hyperparameters, i.e., to terminate tuning iterations. When translating Japanese sentences, a parse tree for each sentence is constructed in the same way as described earlier in this step, and then the parse trees are translated into English sentences using the learned forest-to-string MT model. 4.2 Experimental Results Table 1 shows the performance for the test data m</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2011</marker>
<rawString>Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2011. Expected BLEU Training for Graphs: BBN System Description for WMT11 System Combination Task. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<contexts>
<context position="21676" citStr="Schmid, 1994" startWordPosition="3700" endWordPosition="3701">ite tree model for POS induction using an in-house developed syntax-based forest-to-string SMT system. In the training process, the following steps are performed sequentially: preprocessing, inducing a POS tagset for a source language, training a POS tagger and a dependency parser, and training a forest-to-string MT model. Step 1. Preprocessing We used the first 10,000 Japanese-English sentence pairs in the NTCIR-9 training data for inducing a POS tagset for Japanese6. The Japanese sentences were segmented using MeCab7, and the English sentences were tokenized and POS tagged using TreeTagger (Schmid, 1994), where 43 and 58 types of POS tags are included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the secondlevel POS tags in the IPA POS tagset (Asahara and Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments usin</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proceedings of the International Conference on New Methods in Language Processing, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayaram Sethuraman</author>
</authors>
<title>A Constructive Definition of Dirichlet Priors.</title>
<date>1994</date>
<journal>Statistica Sinica,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="9090" citStr="Sethuraman (1994)" startWordPosition="1523" endWordPosition="1524">imilar measure was adopted in iHMM (Beal et al., 2001). HDP is a set of DPs coupled through a shared random base measure which is itself drawn from a DP: each Gk with a shared base and H) with a global base measure H. From the viewpoint of the stickbreaking (Sethuraman parent’s ∼DP(α0,G0) G0, G0∼DP(γ, construction3 , 1994), the HDP is interpreted as follows: G0 = 00 d the independent model. k′=1 d Gk = 00 3.1 Joint Model k′=1 H, β|γ∼GEM(γ),πk|α0,β∼DP(α0,β),ϕk∼ zt′|zt∼Multinomial(πzt), xt|zt ∼ F (ϕzt). is a measure on measures. has two parameters, a scaling parameter and a base measure H: H). 3Sethuraman (1994) showed a definition of a measure G— First, infinite sequences of i.i.d variables and are generated: — k � . Then, G is defined as: k � � i (1 — ), G= a is defined by this process, then we write a 2DP It α DP(α, DP(α0,G0). (π′k)∞k=1 (ϕk)∞k=1 π′k|α0 Beta(1, α0),ϕ 0 π =π ∑ π′l ∑∞k=1πkδϕk.If GEM(α0). while observations in the monolingual infinite tree model represent only source words. For each source word, all the aligned target words are copied and sorted in alphabetical order, and then concatenated into a single observation. Therefore, a single target word maybe emitted multiple times if the t</context>
</contexts>
<marker>Sethuraman, 1994</marker>
<rawString>Jayaram Sethuraman. 1994. A Constructive Definition of Dirichlet Priors. Statistica Sinica, 4(2):639–650.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="1370" citStr="Shen et al., 2008" startWordPosition="190" endWordPosition="193"> emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 1 Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the l</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model. In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies, pages 577– 585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kairit Sirts</author>
<author>Tanel Alum¨ae</author>
</authors>
<title>A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>407--416</pages>
<marker>Sirts, Alum¨ae, 2012</marker>
<rawString>Kairit Sirts and Tanel Alum¨ae. 2012. A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 407–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet Processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="5575" citStr="Teh et al., 2006" startWordPosition="887" endWordPosition="890"> achieves a more than 1 point gain in BLEU, which resolves the sparseness problem introduced by the bi-word observations. 2 Related Work A number of unsupervised methods have been proposed for inducing POS tags. Early methods have the problem that the number of possible POS tags must be provided preliminarily. This limitation has been overcome by automatically adjusting the number of possible POS tags using nonparametric Bayesian methods (Finkel et al., 2007; Gael et al., 2009; Blunsom and Cohn, 2011; Sirts and Alum¨ae, 2012). Gael et al. (2009) applied infinite HMM (iHMM) (Beal et al., 2001; Teh et al., 2006), a nonparametric version of HMM, to POS induction. Blunsom and Cohn (2011) used a hierarchical Pitman-Yor process prior to the transition and emission distribution for sophisticated smoothing. Sirts and Alum¨ae (2012) built a model that combines POS induction and morphological segmentation into a single learning problem. Finkel et al. (2007) proposed the infinite tree model, which represents recursive branching structures over infinite hidden states and induces POS tags from syntactic dependency structures. In the following, we overview the infinite tree model, which is the basis of our propo</context>
<context position="7737" citStr="Teh et al., 2006" startWordPosition="1249" endWordPosition="1252">ameterized by π, where πij = p(zc(t) = j|zt = i) and πk are the transition probabilities from the parent’s state k. πk is distributed according to a Dirichlet distribution with parameter ρ: πk|ρ ∼ Dirichlet(ρ, ... , ρ). The hidden state of each child zt′ is distributed according to a multinomial distribution πzt specific to the parent’s state zt: zt′|zt ∼ Multinomial(πzt). 2.2 Infinite Tree Model In the infinite tree model, the number of possible hidden states is potentially infinite. The infinite model is formed by extending the finite tree model using a hierarchical Dirichlet process (HDP) (Teh et al., 2006). The reason for using an HDP rather &apos;Finkel et al. (2007) originally proposed three types of models: besides the independent children model, the simultaneous children model and the markov children model. Although we could apply the other two models, we leave this for future work. πk |p ~ Dirichlet(p,..., p ) φk H ρ πk z1 z2 z3 H φk k=1,...,C x1 x2 x3 ~ 842 ala Ilk H Ok Y N 00 z1 x1 x2 x3 β π α β α β |, ~ DP( , k 0 0 φ k z2 z3 |γ ~ H ~ GEM ( ) γ ) LY0 Ilk Y 8 z1 00 k z4 zS z6 9 A 10 11 !&amp;11*11 1111 11 +pay° +fees° +usage° z2 z3 Figure 4: An Example of the Joint Model tween Figure 2 an d Figure</context>
<context position="15365" citStr="Teh et al., 2006" startWordPosition="2580" endWordPosition="2583">. Then, each node t is constrained to follow the distributions indicated by the initially assigned POS tag st, and we use the pair (st, zt) as a state representation. 3.5 Inference k In inference, we find the state set that maximizes the posterior probability of state transitions given observations (i.e., P(z1:n|x1:n)). However, we cannot evaluate the probability for all possible states because the number of states is infinite. Finkel et al. (2007) presented a sampling algorithm for the infinite tree model, which is based on the Gibbs sampling in the direct assignment representation for iHMM (Teh et al., 2006). In the y ,B z1 β πk |α0,β~DP(α0,β |γ ~ GEM( ) γ ) z2 z3 φ k ~ H, φk ~ H &apos; CY0 Ilk #y3 pay z4 z5 z6 H Ok I 1 *4 fees bt NONE *Jffl usage J� NONE H’ O&apos;k co 844 Gibbs sampling, individual hidden state variables are resampled conditioned on all other variables. Unfortunately, its convergence is slow in HMM settings because sequential data is likely to have a strong correlation between hidden states (Gael et al., 2008). We present an inference procedure based on beam sampling (Gael et al., 2008) for the joint model and the independent model. Beam sampling limits the number of possible state trans</context>
<context position="17226" citStr="Teh et al., (2006)" startWordPosition="2902" endWordPosition="2905">riables u, the state assignments z, the transition probabilities 7r, the shared DP parameters ,3, and the hyperparameters α0 and γ. We can parallelize procedures in sampling u and z because the slice sampling for u and the dynamic programing for z are independent for each sentence. See Gael el al. (2009) for details. The only difference between inferences in the joint model and the independent model is in computing the posterior probability of state transitions given observations (e.g., p(z1:n|x1:n) and p(z1:n|x1:n, x′1:n)) in sampling z. In the following, we describe each sampling stage. See Teh et al., (2006) for details of sampling 7r, ,3, α0 and γ. Sampling u: Each ut is sampled from the uniform distribution on [0,πzd(t)zt], where d(t) is the parent of t: ut ∼ Uniform(0,πzd(t)zt). Note that ut is a positive number, since each transition probability is larger than zero. πzd(t)zt Sampling z: Possible values k of zt are divided into the two sets using ut: a finite set with πzd(t)k &gt; ut and an infinite set with πzd(t)k ≤ ut. The beam sampling considers only the former set. Owing to the truncation of the latter set, we can compute the posterior probability of a state zt given observations for all t (</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet Processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Machine Translation System Combination by Confusion Forest.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1249--1257</pages>
<contexts>
<context position="25952" citStr="Watanabe and Sumita, 2011" startWordPosition="4400" endWordPosition="4403">nsition-based dependency parser which can jointly learn POS tagging and dependency parsing (Hatori et al., 2011) under an incremental framework&amp;quot;. Note that the learned parser can identify dependencies between words and attach an induced POS tag for each word. Step 4. Training a Forest-to-String MT In this step, we train a forest-to-string MT model based on the learned dependency parser in Step 3. We use an in-house developed hypergraph-based toolkit, cicada, for training and decoding with a tree-to-string model, which has been successfully employed in our previous work for system combination (Watanabe and Sumita, 2011) and online learning (Watanabe, 2012). All the Japanese and English sentences in the NTCIR-9 training data are segmented in the same way as in Step 1, and then each Japanese sentence is parsed by the dependency parser learned in Step 3, which simultaneously assigns induced POS tags and word dependencies. Finally, a forest-to-string MT model is learned with Zhang et al., (2011), which extracts translation rules by a forest-based variant of llhttp://triplet.cc/software/corbit/ IND REF B5 27.54 Mono 27.66 26.83 Joint[s] 28.00 28.00 Joint[P] 26.36 26.72 Joint[s+P] 27.99 27.82 Ind[s] 28.00 27.93 In</context>
</contexts>
<marker>Watanabe, Sumita, 2011</marker>
<rawString>Taro Watanabe and Eiichiro Sumita. 2011. Machine Translation System Combination by Confusion Forest. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1249–1257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
</authors>
<title>Optimized Online Rank Learning for Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>253--262</pages>
<contexts>
<context position="25989" citStr="Watanabe, 2012" startWordPosition="4407" endWordPosition="4408">y learn POS tagging and dependency parsing (Hatori et al., 2011) under an incremental framework&amp;quot;. Note that the learned parser can identify dependencies between words and attach an induced POS tag for each word. Step 4. Training a Forest-to-String MT In this step, we train a forest-to-string MT model based on the learned dependency parser in Step 3. We use an in-house developed hypergraph-based toolkit, cicada, for training and decoding with a tree-to-string model, which has been successfully employed in our previous work for system combination (Watanabe and Sumita, 2011) and online learning (Watanabe, 2012). All the Japanese and English sentences in the NTCIR-9 training data are segmented in the same way as in Step 1, and then each Japanese sentence is parsed by the dependency parser learned in Step 3, which simultaneously assigns induced POS tags and word dependencies. Finally, a forest-to-string MT model is learned with Zhang et al., (2011), which extracts translation rules by a forest-based variant of llhttp://triplet.cc/software/corbit/ IND REF B5 27.54 Mono 27.66 26.83 Joint[s] 28.00 28.00 Joint[P] 26.36 26.72 Joint[s+P] 27.99 27.82 Ind[s] 28.00 27.93 Ind[P] 28.11 28.63 Ind[s+P] 28.13 28.62</context>
</contexts>
<marker>Watanabe, 2012</marker>
<rawString>Taro Watanabe. 2012. Optimized Online Rank Learning for Machine Translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 253–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A Tree Sequence Alignment-based Tree-to-Tree Translation Model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>559--567</pages>
<contexts>
<context position="1512" citStr="Zhang et al., 2008" startWordPosition="217" endWordPosition="220">f Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 1 Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in jbtst_ [t 4!43 t v bt Rim TjL,- tsL% noun particle nou</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A Tree Sequence Alignment-based Tree-to-Tree Translation Model. In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies, pages 559– 567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Licheng Fang</author>
<author>Peng Xu</author>
<author>Xiaoyun Wu</author>
</authors>
<title>Binarized Forest to String Translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="1593" citStr="Zhang et al., 2011" startWordPosition="233" endWordPosition="236">nese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 1 Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in jbtst_ [t 4!43 t v bt Rim TjL,- tsL% noun particle noun particle noun verb auxiliary verb You can not use the Internet . U bt Rim At � </context>
<context position="26331" citStr="Zhang et al., (2011)" startWordPosition="4465" endWordPosition="4468">ser in Step 3. We use an in-house developed hypergraph-based toolkit, cicada, for training and decoding with a tree-to-string model, which has been successfully employed in our previous work for system combination (Watanabe and Sumita, 2011) and online learning (Watanabe, 2012). All the Japanese and English sentences in the NTCIR-9 training data are segmented in the same way as in Step 1, and then each Japanese sentence is parsed by the dependency parser learned in Step 3, which simultaneously assigns induced POS tags and word dependencies. Finally, a forest-to-string MT model is learned with Zhang et al., (2011), which extracts translation rules by a forest-based variant of llhttp://triplet.cc/software/corbit/ IND REF B5 27.54 Mono 27.66 26.83 Joint[s] 28.00 28.00 Joint[P] 26.36 26.72 Joint[s+P] 27.99 27.82 Ind[s] 28.00 27.93 Ind[P] 28.11 28.63 Ind[s+P] 28.13 28.62 Table 1: Performance on Japanese-to-English Translation Measured by BLEU (%) the GHKM algorithm (Mi and Huang, 2008) after each parse tree is restructured into a binarized packed forest. Parameters are tuned on the development data using xBLEU (Rosti et al., 2011) as an objective and L-BFGS (Liu and Nocedal, 1989) as an optimization toolki</context>
</contexts>
<marker>Zhang, Fang, Xu, Wu, 2011</marker>
<rawString>Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu. 2011. Binarized Forest to String Translation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 19–24.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>