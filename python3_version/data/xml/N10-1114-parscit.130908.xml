<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.447696">
<title confidence="0.988661">
Music, Language, and Computational Modeling: Lessons from
the Key-Finding Problem
</title>
<author confidence="0.975249">
David Temperley
</author>
<affiliation confidence="0.921971">
Eastman School of Music, University of Rochester
</affiliation>
<address confidence="0.8798265">
26 Gibbs St.
Rochester, NY 14604
</address>
<email confidence="0.997353">
dtemperley@esm.rochester.edu
</email>
<sectionHeader confidence="0.978153" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999560333333333">
Recent research in computational music research, including my own, has been greatly influenced by methods in
computational linguistics. But I believe the influence could also go the other way: Music may offer some interesting
lessons for language research, particularly with regard to the modeling of cognition.
In this talk I will focus on an important problem in music cognition: the problem of key identification. I will argue
that this problem is in some ways analogous to the problem of syntactic parsing in language. I will present a simple
Bayesian model that performs well at the key-finding task. I will then consider some implications of the model for
other issues. The model represents moment-to-moment changes in key over time and captures “reanalysis” effects in
key perception. The model can be used to estimate the tonal ambiguity of a musical passage, and can also be used to
estimate the probability of note patterns (just as a probabilistic grammar can be used to estimate the probability of
word strings). An interesting question here concerns expectation: In forming expectations for the next surface ele-
ment (note or word), do we consider all possible structures (syntactic structures or keys) or just the most probable
one? Finally, the model sheds light on the concept of “information flow.” It has been suggested that language re-
flects a tendency towards uniform density of information, in that less probable elements are spread out or elongated;
I will suggest that the same may be true in music.
Slides for the talk will be available at my website, &lt;www.theory.esm.rochester.edu/temperley&gt;.
</bodyText>
<page confidence="0.983584">
741
</page>
<reference confidence="0.64629">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, page 741,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.501870">
<title confidence="0.997323">Music, Language, and Computational Modeling: Lessons the Key-Finding Problem</title>
<author confidence="0.961961">David</author>
<affiliation confidence="0.993518">Eastman School of Music, University of</affiliation>
<address confidence="0.8887435">26 Gibbs Rochester, NY</address>
<email confidence="0.999665">dtemperley@esm.rochester.edu</email>
<abstract confidence="0.997432733333333">Recent research in computational music research, including my own, has been greatly influenced by methods in computational linguistics. But I believe the influence could also go the other way: Music may offer some interesting lessons for language research, particularly with regard to the modeling of cognition. In this talk I will focus on an important problem in music cognition: the problem of key identification. I will argue that this problem is in some ways analogous to the problem of syntactic parsing in language. I will present a simple Bayesian model that performs well at the key-finding task. I will then consider some implications of the model for other issues. The model represents moment-to-moment changes in key over time and captures “reanalysis” effects in key perception. The model can be used to estimate the tonal ambiguity of a musical passage, and can also be used to estimate the probability of note patterns (just as a probabilistic grammar can be used to estimate the probability of word strings). An interesting question here concerns expectation: In forming expectations for the next surface element (note or word), do we consider all possible structures (syntactic structures or keys) or just the most probable one? Finally, the model sheds light on the concept of “information flow.” It has been suggested that language reflects a tendency towards uniform density of information, in that less probable elements are spread out or elongated; I will suggest that the same may be true in music.</abstract>
<note confidence="0.90791675">Slides for the talk will be available at my website, &lt;www.theory.esm.rochester.edu/temperley&gt;. 741 Language Technologies: The 2010 Annual Conference of the North American Chapter of the page Angeles, California, June 2010. Association for Computational Linguistics</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Human Language</author>
</authors>
<title>Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the ACL,</booktitle>
<pages>741</pages>
<location>Los Angeles, California,</location>
<marker>Language, 2010</marker>
<rawString>Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, page 741, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>