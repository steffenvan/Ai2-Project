<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.9989895">
Improving English Subcategorization Acquisition with Diathesis Al-
ternations as Heuristic Information
</title>
<author confidence="0.981621">
Xiwu Han
</author>
<affiliation confidence="0.96952">
Institute of Computational
</affiliation>
<author confidence="0.265855">
Linguistics
</author>
<affiliation confidence="0.860216">
Heilongjiang University
</affiliation>
<address confidence="0.905405">
Harbin City 150080 China
</address>
<email confidence="0.996087">
hxw@hlju.edu.cn
</email>
<author confidence="0.992816">
Tiejun Zhao
</author>
<affiliation confidence="0.995679">
School of Computer Science and
</affiliation>
<author confidence="0.4712">
Technology
</author>
<affiliation confidence="0.944333">
Harbin Institute of Technology
</affiliation>
<address confidence="0.941279">
Harbin City 150001 China
</address>
<email confidence="0.997904">
tjzhao@mtlab.hit.edu.cn
</email>
<author confidence="0.804022">
Xingshang Fu
</author>
<affiliation confidence="0.803307">
Institute of Computational
</affiliation>
<author confidence="0.268669">
Linguistics
</author>
<affiliation confidence="0.843302">
Heilongjiang University
</affiliation>
<address confidence="0.900517">
Harbin City 150080 China
</address>
<email confidence="0.997376">
fxs@hlju.edu.cn
</email>
<sectionHeader confidence="0.998589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999907692307692">
Automatically acquired lexicons with
subcategorization information have al-
ready proved accurate and useful enough
for some purposes but their accuracy still
shows room for improvement. By means
of diathesis alternation, this paper pro-
poses a new filtering method, which im-
proved the performance of Korhonen’s
acquisition system remarkably, with the
precision increased to 91.18% and recall
unchanged, making the acquired lexicon
much more practical for further manual
proofreading and other NLP uses.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999989285714286">
Subcategorization is the process that further clas-
sifies a syntactic category into its subsets. Chom-
sky (1965) defines the function of strict subcate-
gorization features as appointing a set of con-
straints that dominate the selection of verbs and
other arguments in deep structure. Large sub-
categorized verbal lexicons have proved to be
crucially important for many tasks of natural
language processing, such as probabilistic pars-
ers (Korhonen, 2001, 2002) and verb classifica-
tions (Schulte im Walde, 2002; Korhonen, 2003).
Since Brent (1993) a considerable amount of re-
search focusing on large-scaled automatic acqui-
sition of subcategorization frames (SCF) has met
with some success not only in English but also in
many other languages, including German
(Schulte im Walde, 2002), Spanish (Chrupala,
2003), Czech (Sarkar and Zeman, 2000), Portu-
guese (Gamallo et. al, 2002), and Chinese (Han
et al, 2004). The general objective of this re-
search is to acquire from a given corpus the SCF
types and numbers for predicate verbs. Two typi-
cal steps during the process of automatic acquisi-
tion are hypothesis generation and selection.
Usually based on heuristic rules, the first step
generates SCF hypotheses for involved verbs;
and the second selects reliable ones via statistical
methods, such as BHT (binomial hypothesis test-
ing), LLR (log likelihood ratio) and MLE
(maximum likelihood estimation). This second
step is also called statistical filtering and has
been widely regarded as problematic. English
researchers have proposed some methods adjust-
ing the corpus hypothesis frequencies before or
while filtering. These methods are often called
backoff techniques for SCF acquisition. Some of
them represent a remarkable improvement in the
acquisition performance, for example diathesis
alternation and semantic motivation (Korhonen,
1998, 2001, 2002).
For the convenience of comparison between
performances of different SCF acquisition meth-
ods, we define absolute and relative recall in this
paper. By absolute recall, we mean the figure
computed against the background of input corpus,
while relative recall is against the set of gener-
ated hypotheses.
At present, automatically acquired verb lexi-
cons with SCF information have already proved
accurate and useful enough for some NLP pur-
poses (Korhonen, 2001; Han et al, 2004). As for
English, Korhonen (2002) reported that semanti-
cally motivated SCF acquisition achieved a pre-
cision of 87.1%, an absolute recall of 71.2% and
a relative recall of 85.27%, thus making the ac-
quired lexicon much more accurate and useful.
However, the accuracy still shows room for im-
provement, especially for those SCF hypotheses
with low frequencies. Detailed analysis on the
acquisition system and some resulting data
shows that three main causes should account for
the comparatively unsatisfactory performance: a.
the imperfect hypothesis generator, b. the Zipfian
</bodyText>
<page confidence="0.982232">
331
</page>
<bodyText confidence="0.9352901">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 331–336,
Sydney, July 2006. c�2006 Association for Computational Linguistics
distribution of syntactic patterns, c. the incom-
plete partition over SCF types of a given verb.
The first problem mainly comes from the inade-
quate parsing performance and noises existing in
the corpus, while the other two problems are in-
herent to natural languages and should be solved
in terms of acquisition techniques particularly
during the process of hypothesis selection.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.860752555555556">
The empirical background of this paper is the
public resource for subcategorization acquisition
of English verbs, provided by Anna Korhonen
(2005) in her personal home page. The data in-
clude 30 verbs, as shown in Table 1, and their
unfiltered SCF hypotheses, which were auto-
matically generated via Briscoe and Carroll’s
(1997) SCF acquisition system, and the manually
established standard.
</bodyText>
<tableCaption confidence="0.982134">
Table 1. English Verbs in Use.
</tableCaption>
<bodyText confidence="0.993843111111111">
add agree attach
bring carry carve
chop cling clip
fly cut travel
drag communicate give
lend lock marry
meet mix move
offer provide visit
push sail send
slice supply swing
For each verb, there is a corpus of 1000 sen-
tences extracted from the BNC, and all together
42 SCF types are involved in the corpus. The
framework of Briscoe and Carroll’s system con-
sists of six overall components, which are ap-
plied in sequence to sentences containing a spe-
cific predicate in order to retrieve a set of SCFs
for that verb:
</bodyText>
<listItem confidence="0.992488714285714">
• A tagger, a first-order Hidden Markov
Model POS and punctuation tag disam-
biguator.
• A lemmatizer, an enhanced version of the
General Architecture for Text Engineering
project stemmer.
• A probabilistic LR parser, trained on a
tree-bank derived semi-automatically from
the SUSANNE corpus, returns ranked
analyses using a feature-based unification
grammar.
• A pattern extractor, which extracts
subcategorization patterns, i.e. local
syntactic frames, including the syntactic
</listItem>
<bodyText confidence="0.616354">
frames, including the syntactic categories
and head lemmas.
</bodyText>
<listItem confidence="0.98267925">
• A pattern classifier, which assigns patterns
to SCFs or rejects them as unclassifiable.
• A SCF filter, which evaluates sets of SCFs
gathered for a predicate verb.
</listItem>
<bodyText confidence="0.999411875">
Nowadays, in most related researches, the per-
formances of subcategorization acquisition sys-
tems are often evaluated in terms of precision,
recall and F measure of SCF types (Korhonen,
2001, 2002). Generally, precision is the percent-
age of SCFs that the system proposes correctly,
while recall is the percentage of SCFs in the gold
standard that the system proposes:
</bodyText>
<equation confidence="0.612449666666667">
|True positives|
|True positives|+|False positives|
|True positives|
|True positives|+|False negatives|
2 * Precision * Recall
Precision + Recall
</equation>
<bodyText confidence="0.9971174">
Here, true positives are correct SCF types pro-
posed by the system, false positives are incorrect
SCF types proposed by system, and false nega-
tives are correct SCF types not proposed by the
system.
</bodyText>
<sectionHeader confidence="0.997333" genericHeader="method">
3 The MLE Filtering Method
</sectionHeader>
<bodyText confidence="0.999970956521739">
The present SCF acquisition system for English
verbs employs a MLE filter to test the automati-
cally generated SCF hypotheses. Due to noises
accumulated while tagging, lemmatizing and
parsing the corpus, even though correction is im-
plemented for some typical errors when classify-
ing the extracted patterns, the hypothesis genera-
tor does not perform as efficiently as hoped.
Sampling analysis on the unfiltered hypotheses
in Korhonen’s evaluation corpus indicates that
about 74% incorrectly proposed and rejected
SCF types come from the defects of the MLE
filtering method.
Performance of the MLE filter is closely re-
lated to the actual distributions p(scfi|v) over
predicates and SCF types in the input corpus.
First, from the overall corpus a training set is
drawn randomly; it must be large enough to en-
sure a similar distribution. Then, the frequency
of a subcategorization frame scfi occurring with a
verb v is recorded and used to estimate the prob-
ability p(scfi|v). Thirdly, an empirical threshold θ
is determined, which ensures that a maximum
</bodyText>
<equation confidence="0.997925333333333">
Precision =
Recall =
F-measure =
</equation>
<page confidence="0.983534">
332
</page>
<bodyText confidence="0.999967555555556">
value of the F-measure will result for the training
set. Finally, the threshold is used to filter out
from the total set those SCF hypotheses with fre-
quencies lower than θ.
Therefore, the statistical foundation of this fil-
tering method is the assumption of independence
among the SCFs that a verb enters, which can be
probabilistically expressed in two formulas as
follows:
</bodyText>
<equation confidence="0.9997668">
b&apos;i,b&apos;j,i # j, p(scfi  |scfj,v) = 0 ... (1)
n
Ep scf i v
(  |) =
i=1
</equation>
<bodyText confidence="0.999959181818182">
Here, i and j are natural numbers, scfi and scfj are
two SCF types that verb v enters, and variables in
formulas henceforth will hold the same meanings.
In actual application, the probability p(scfi|v) is
estimated from the observed frequency f(scfi, v),
and the conditional probability p(scfi|scfj, v) is
assumed to be zero. This means any two SCF
types entered by a given verb are taken for
granted to be probabilistically independent from
each other. However, this assumption can some-
times be far from appropriate.
</bodyText>
<sectionHeader confidence="0.978904" genericHeader="method">
4 Diathesis Alternations and Filtering
</sectionHeader>
<bodyText confidence="0.896289741935484">
Much linguistic research focusing on child lan-
guage acquisition has revealed that many chil-
dren are able to produce new grammatical sen-
tences from what they have learned (Peters, 1983;
Ellis, 1985). This implies that the widely-used
independence assumption in the field of NLP
may not be very appropriate, at least for syntactic
patterns. If this assumption should be removed, a
possible heuristic could be the information of
diathesis alternations, which is also another con-
vincing counterargument. Diathesis alternations
are generally regarded as alternative ways in
which verbs express their arguments. Examples
are as follows:
a. He broke the glass.
b. The glass broke.
c. Ta1 chi1 le0 pin2guo3.
( )
d. Ta1 ba3 pin2guo3 chi1 le01.
( )
In the above examples, the English verb break
takes the causative-inchoative alternation as
shown in sentences a and b, while sentences c
and d indicate that the Chinese verb chi1 ( , eat)
1 The numbers in sentences c and d, which are pinyin nota-
tions, show tones of the Chinese syllables, and the two sen-
tences, in English, generally mean He ate an apple.
may enter the ba-object-raising alternation where
the object is shifted forward by the preposition
ba3 ( ) to the location between the subject and
the predicate, as illustrated in Figure 1.
</bodyText>
<figure confidence="0.918982666666667">
ba3
Ta1 chi1 le0 pin2guo3.
ba-object-raising
</figure>
<figureCaption confidence="0.999785">
Figure 1. Ba-object-raising Alternation.
</figureCaption>
<bodyText confidence="0.999988619047619">
Subcategorization of verbs has much to do
with diathesis alternations, and most SCF re-
searchers regard information of diathesis alterna-
tion as an indispensable part of subcategorization
(Korhonen, 2001; McCarthy, 2001). Therefore,
one may conclude that, for subcategorization
acquisition, the independence assumption sup-
porting the MLE filter is not as appropriate as
previously thought.
For a given verb, the assumption will be ap-
propriate and sufficient if and only if there is no
diathesis alternation between all the SCFs it en-
ters, and formula (1) and (2) in Section 3 are ef-
ficient enough to serve as a foundation for the
MLE filtering method. Otherwise, if there are
diathesis alternations between some of the SCFs
that a verb enters, then formula (1) and (2) must
be modified as illustrated in formula (3) and (4).
In either case, for the sake of convenience, it
would be better to combine the formulas as
shown in (5) and (6).
</bodyText>
<equation confidence="0.9998285">
3 i, 3j, i # j, p(scfi  |scfj, v) &gt; 0 ... (3)
n
Ep scf i v
(  |) &gt;
i=1
b&apos;i,b&apos;j,i # j, p(scfi  |scfj,v) &gt;_ 0 ... (5)
n
Ep scf i v
(  |) &gt;_
i=1
</equation>
<bodyText confidence="0.999977357142857">
For English verbs, previous research has
achieved great progress in diathesis alternation
and relative applications, such as the work of
Levin (1993) and McCarthy (2001). Besides,
Korhonen (1998) has proved that diathesis alter-
nation could be used as heuristic information for
backoff estimates to improve the general per-
formance of subcategorization acquisition. How-
ever, determining where and how to seed the
heuristic remains difficult.
Korhonen (1998) employed diathesis alterna-
tions in Briscoe and Carroll’s system to improve
the performance of their BHT filter. Although
the precision rate increased from 61.22% to
</bodyText>
<equation confidence="0.998328">
1 ... (2)
1 ... (4)
1 ... (6)
</equation>
<page confidence="0.995205">
333
</page>
<bodyText confidence="0.997441727272727">
69.42% and the recall rate from 44.70% to
50.81%, the results were still not accurate
enough for possible practical NLP uses.
Korhonen obtained her one-way diathesis al-
ternations from the ANLT dictionary (Boguraev
and Briscoe, 1987), calculated the alternating
probability p(scfj|scfi) according to the number of
common verbs that took the alternation
(scfi--).scfj), and used formula (7) and (8), where
w is an empirical weight, to adjust the previously
estimated p(scfi|v):
</bodyText>
<equation confidence="0.984765333333333">
If p(scfi|scfj, v)&gt;0,
p(scfi|v) = p(scfi|v)–w(p(scfi|v)•
p(scfj |scfi)) ...(7)
If p(scfi|v)&gt;0 &amp; p(scfj|v)=0,
p(scfi|v) = p(scfi|v)+w(p(scfi|v)•
p(scfj |scfi)) ...(8)2
</equation>
<bodyText confidence="0.862632972222222">
Following the adjustment, a BHT filter with a
confidence rate of 95% was used to check the
SCF hypotheses.
This method removes the assumption of inde-
pendence among SCF types but establishes an-
other assumption of independence between
p(scfj|scfi) and certain verbs, which assumes that
all verbs take each diathesis alternation with the
same probability. Nevertheless, linguistic knowl-
edge tells us that verbs often enter different dia-
thesis alternations and can be classified accord-
ingly. Consider the following examples:
e. He broke the glass. / The glass broke.
f. The police dispersed the crowd.
/ The crowd dispersed.
g. Mum cut the bread. / *The bread cut.
Both of the English verbs “break” and “disperse”
can take the causative-inchoative alternation and,
hence, may be classified together, while the verb
“cut” does not take this alternation. Therefore,
the newly established assumption doesn’t fit the
actual situation either, and the probability sums
ip(scfi|v) and i,jp(scfi|scfj, v) neither need or
can be normalized.
Based on the above methodology, we formed a
new filtering method with diathesis alternations
as heuristic information, which is, in fact, de-
rived from the simple MLE filter and based on
formula (5) and (6). The algorithm can be briefly
expressed as shown in Table 2.
2 For the sake of consistency in this paper and for the con-
venience of understanding, formulae formats here are modi-
fied. They may look different from those of Korhonen
(1998), but they are actually the same.
Table 2. The New Filtering Method.
For hypotheses of a given verb v,
</bodyText>
<listItem confidence="0.95423825">
1. if p(scfi|v) &gt; 01,
accept scfi into the output set S;
2. else
if p(scfi|v) &gt; 02,
&amp; p(scfi|scfj, v) &gt; 0,
&amp; scfj S,
accept scfi into set S;
3. Go to step 1 until S doesn’t increase.
</listItem>
<bodyText confidence="0.999972375">
In our method, two filters are employed. For
each verb involved, first a common MLE filter is
used, but it employs a threshold 01 that is much
higher than usual, and those SCF hypotheses that
satisfy the requirement are accepted. Then, all of
the remainder of the hypotheses are checked by
another MLE filter seeded with diathesis alterna-
tions as heuristic information and equipped with
a much lower threshold 02. Any hypothesis scfi
left out by the first filter will be accepted if its
probability exceeds 02 and it is an alternative of
an SCF type scfj that has been accepted by the
first filter, which means that p(scfi|scfj, v)&gt;0 and
scfj S. The filtering process will be performed
repeatedly for those unaccepted hypotheses until
no more hypotheses can be accepted for the verb.
</bodyText>
<sectionHeader confidence="0.993945" genericHeader="evaluation">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.99994632">
We implemented an acquisition experiment on
Korhonen’s evaluation resources with the above-
mentioned filtering method.
The diathesis alternations in use are also those
provided by Korhonen, except that we used them
in a two-way manner (scfi scfj) instead of
one-way (scfi--).scfj), because the two involved
SCF types are usually alternative pragmatic for-
mats of the concerned verb, as shown in exam-
ples in Section 3 and 4.
In the experiment we empirically set 01= 0.2,
which is ten times of Korhonen’s threshold for
her MLE filter; 02= 0.002, which is one tenth of
Korhonen’s. Thus, in a token set of hypotheses
no more than 1000, an SCF type scfi will be ac-
cepted if it occurs two times or more and has a
diathesis alternative type scfj already accepted for
the verb.
The gold standard was the manually analysed
results by Korhonen. Precision, recall and F-
measure were calculated via expressions given in
Section 2.
Table 3 lists the performances of the baseline
method of non-filtering (No_f), MLE filtering
with 0 = 0.02, and our filtering method on the
</bodyText>
<page confidence="0.997596">
334
</page>
<bodyText confidence="0.999361625">
evaluation corpus, and also gives the best results
of Korhonen&apos;s method that is using extra seman-
tic information (Kor) to make a comparison.
Here, Ab_R is the absolute recall ratio, Re_R the
relative recall ratio, Ab_F the absolute F-
measure that is calculated from Precision and
Ab_R, and Re_F the relative F-measure that is
from Precision and Re_R.
</bodyText>
<tableCaption confidence="0.997416">
Table 3. Performance Comparison.
</tableCaption>
<table confidence="0.999403833333333">
Methods No-f MLE ours Kor
P(%) 47.85 67.89 91.18 87.1
Ab_R(%) 34.62 32.52 32.52 71.2
Re_R(%) 100 93.93 93.93 85.27
Ab_F 40.17 43.98 47.94 78.35
Re_F 64.73 78.81 92.53 86.18
</table>
<bodyText confidence="0.999953541666667">
The evaluation shows that our new filtering
method improved the acquisition performance
remarkably: a. Compared with MLE, precision
increased by 23.29%, recall ratio remained un-
changed, absolute F-measure increased by 3.96,
and relative F-measure increased by 13.72; b.
Compared with Korhonen’s best results, preci-
sion, Re_R and Re_F also increased respec-
tively3. Thus, the general performance of our
filtering method makes the acquired lexicon
much more practical for further manual proof-
reading and other NLP uses.
What’s more, the data shown in Table 3 im-
plies that there is little room left for improvement
of the statistical filter, since the absolute recall
ratio is only 2.1% lower than that of the non-
filtering method. Whereas, detailed analysis of
the evaluation corpus shows that the hypothesis
generator accounts for about 95% of those unre-
called and wrongly recalled SCF types, which
indicates, for the present time, more improve-
ment efforts need to be made on the first step of
subcategorization acquisition, i.e. hypothesis
generation.
</bodyText>
<sectionHeader confidence="0.999594" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.991372547619048">
Our new filtering method removed the inappro-
priate assumptions and takes much more advan-
3 Korhonen (2002) reported the non-filtering absolute recall
ratio of her experiment was about 83.5%. She didn’t give
any explanation with her evaluation resources why here
non-filtering Ab_R was so much lower. Therefore, the
Ab_R and Ab_F figures are not comparable here.
tage of what can be observed in the corpus by
drawing on the alternative relationship between
SCF hypotheses with higher and lower frequen-
cies. Unlike the semantically motivated method
(Korhonen, 2001, 2002), which is dependent on
verb classifications that linguistic resources are
able to provide, our filter needs no prior knowl-
edge other than reasonable diathesis alternation
information and may work well for most verbs in
other languages with sufficient predicative to-
kens.
Our experimental results suggest that the pro-
posed technique improves the general perform-
ance of the English subcategorization acquisition
system, and leaves only a little room for further
improvement in statistical filtering methods.
However, approaches that are more complicated
still exist theoretically, for instance, some SCF
types unseen by the hypothesis generator may be
recalled by integrating semantic verb-
classification information into the system.
More essential aspects of our future work,
however, will focus on improving the perform-
ance of the hypothesis generator, and testing and
applying the acquired subcategorization lexicons
in some concrete NLP tasks.
Acknowledgement This research has been
jointly sponsored by the NSFC project No.
60373101 and the post-doctor scholarship of for-
eign linguistics and literature in Heilongjiang
University. And at the same time, our great
thanks go to Dr. Anna Korhonen for her public
evaluation resources, and Dr. Chrys Chrystello
for his helpful advice on the English writing of
this paper.
</bodyText>
<sectionHeader confidence="0.999544" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999701647058824">
Boguraev B. K., E. J. Briscoe. Large lexicons for
natural language processing utilizing the grammar
coding system of the Longman Dictionary of Con-
temporary English. Computational Linguistics,
1987: 219-240
Brent, M., From Grammar to Lexicon: unsupervised
learning of lexical syntax, Computational Linguis-
tics 19(3) 1993: 243-262.
Briscoe, Ted and John Carroll, Automatic extraction
of subcategorization from corpora, Proceedings of
the 5th ACL Conference on Applied Natural Lan-
guage Processing, Washington, DC, 1997: 356-
363.
Chomsky, Noam, Aspects of the Theory of Syntax,
MIT Press, Cambridge, 1965.
Chrupala, Grzegorz, Acquiring Verb Subcategoriza-
tion from Spanish Corpora, PhD program “Cogni-
</reference>
<page confidence="0.987761">
335
</page>
<reference confidence="0.999720647058824">
tive Science and Language”, Universitat de Barce-
lona, 2003: 67-68.
Ellis, R Understanding Second language Acquisi-
tion, Oxford University Press.1985
Gamallo, P., Agustini, A. and Lopes Gabriel P., Using
Co-Composition for Acquiring Syntactic and Se-
mantic Subcategorisation, Proceedings of the
Workshop of the ACL Special Interest Group on
the Lexicon (SIGLEX), Philadelphia, 2002: 34-41.
Han, Xiwu, Tiejun Zhao, Haoliang Qi, and Hao Yu,
Subcategorization Acquisition and Evaluation for
Chinese Verbs, Proceedings of the COLING 2004,
2004: 723-728.
Korhonen, Anna, Automatic Extraction of Subcatego-
rization Frames from Corpora –Improving Filtering
with Diathesis Alternations, 1998. Please refer to
http://www.folli.uva.nl/CD/1998/pdf/keller/korhon
en.pdf
Korhonen, Anna, Subcategorization Acquisition, Dis-
sertation for PhD, Trinity Hall University of Cam-
bridge, 2001.
Korhonen, Anna, Subcategorization Acquisition,
Technical Report Number 530, Trinity Hall Uni-
versity of Cambridge, 2002.
Korhonen, Anna, Yuval Krymolowski, Zvika Marx,
Clustering Polysemic Subcategorization Frame
Distributions Semantically, Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, 2003: 64-71.
Korhonen, Anna. Subcategorization Evaluation Re-
sources. http://www.cl.cam.ac.uk/users/alk23/sub-
cat/subcat.html. 2005
Levin, B., English Verb Classes and Alternations,
Chicago University Press, Chicago, 1993.
McCarthy, D., Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Alternations, Sub-
categorization Frames and Selectional Preferences,
PhD thesis, University of Sussex, 2001.
Peters, A. The Unit of Language Acquisition, Cam-
bridge University Press. 1983.
Sarkar, A. and Zeman, D., Automatic Extraction of
Subcategorization Frames for Czech, Proceedings
of the 19th International Conference on Computa-
tional Linguistics, Saarbrucken, Germany, 2000.
Please refer to http://www.sfu.ca/~anoop/papers/
pdf/coling0final.pdf
Shulte im Walde, Sabine, Inducing German Semantic
Verb Classes from Purely Syntactic Subcategoriza-
tion Information, Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, 2002: 223-230.
</reference>
<page confidence="0.999146">
336
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.800780">
<title confidence="0.999222">English Subcategorization Acquisition with Diathesis ternations as Heuristic Information</title>
<author confidence="0.999025">Xiwu Han</author>
<affiliation confidence="0.993747333333333">Institute of Computational Linguistics Heilongjiang University</affiliation>
<address confidence="0.990885">Harbin City 150080 China</address>
<email confidence="0.966934">hxw@hlju.edu.cn</email>
<author confidence="0.997319">Tiejun Zhao</author>
<affiliation confidence="0.993276">School of Computer Science and Technology Harbin Institute of Technology</affiliation>
<address confidence="0.983117">Harbin City 150001 China</address>
<email confidence="0.980348">tjzhao@mtlab.hit.edu.cn</email>
<author confidence="0.952903">Xingshang Fu</author>
<affiliation confidence="0.993575666666667">Institute of Computational Linguistics Heilongjiang University</affiliation>
<address confidence="0.986578">Harbin City 150080 China</address>
<email confidence="0.994714">fxs@hlju.edu.cn</email>
<abstract confidence="0.998929357142857">Automatically acquired lexicons with subcategorization information have already proved accurate and useful enough for some purposes but their accuracy still shows room for improvement. By means of diathesis alternation, this paper proposes a new filtering method, which improved the performance of Korhonen’s acquisition system remarkably, with the precision increased to 91.18% and recall unchanged, making the acquired lexicon much more practical for further manual proofreading and other NLP uses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B K Boguraev</author>
<author>E J Briscoe</author>
</authors>
<title>Large lexicons for natural language processing utilizing the grammar coding system of the Longman Dictionary of Contemporary English. Computational Linguistics,</title>
<date>1987</date>
<pages>219--240</pages>
<contexts>
<context position="12222" citStr="Boguraev and Briscoe, 1987" startWordPosition="1929" endWordPosition="1932"> information for backoff estimates to improve the general performance of subcategorization acquisition. However, determining where and how to seed the heuristic remains difficult. Korhonen (1998) employed diathesis alternations in Briscoe and Carroll’s system to improve the performance of their BHT filter. Although the precision rate increased from 61.22% to 1 ... (2) 1 ... (4) 1 ... (6) 333 69.42% and the recall rate from 44.70% to 50.81%, the results were still not accurate enough for possible practical NLP uses. Korhonen obtained her one-way diathesis alternations from the ANLT dictionary (Boguraev and Briscoe, 1987), calculated the alternating probability p(scfj|scfi) according to the number of common verbs that took the alternation (scfi--).scfj), and used formula (7) and (8), where w is an empirical weight, to adjust the previously estimated p(scfi|v): If p(scfi|scfj, v)&gt;0, p(scfi|v) = p(scfi|v)–w(p(scfi|v)• p(scfj |scfi)) ...(7) If p(scfi|v)&gt;0 &amp; p(scfj|v)=0, p(scfi|v) = p(scfi|v)+w(p(scfi|v)• p(scfj |scfi)) ...(8)2 Following the adjustment, a BHT filter with a confidence rate of 95% was used to check the SCF hypotheses. This method removes the assumption of independence among SCF types but establishes</context>
</contexts>
<marker>Boguraev, Briscoe, 1987</marker>
<rawString>Boguraev B. K., E. J. Briscoe. Large lexicons for natural language processing utilizing the grammar coding system of the Longman Dictionary of Contemporary English. Computational Linguistics, 1987: 219-240</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>From Grammar to Lexicon: unsupervised learning of lexical syntax,</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>3</issue>
<pages>243--262</pages>
<contexts>
<context position="1530" citStr="Brent (1993)" startWordPosition="210" endWordPosition="211">ical for further manual proofreading and other NLP uses. 1 Introduction Subcategorization is the process that further classifies a syntactic category into its subsets. Chomsky (1965) defines the function of strict subcategorization features as appointing a set of constraints that dominate the selection of verbs and other arguments in deep structure. Large subcategorized verbal lexicons have proved to be crucially important for many tasks of natural language processing, such as probabilistic parsers (Korhonen, 2001, 2002) and verb classifications (Schulte im Walde, 2002; Korhonen, 2003). Since Brent (1993) a considerable amount of research focusing on large-scaled automatic acquisition of subcategorization frames (SCF) has met with some success not only in English but also in many other languages, including German (Schulte im Walde, 2002), Spanish (Chrupala, 2003), Czech (Sarkar and Zeman, 2000), Portuguese (Gamallo et. al, 2002), and Chinese (Han et al, 2004). The general objective of this research is to acquire from a given corpus the SCF types and numbers for predicate verbs. Two typical steps during the process of automatic acquisition are hypothesis generation and selection. Usually based </context>
</contexts>
<marker>Brent, 1993</marker>
<rawString>Brent, M., From Grammar to Lexicon: unsupervised learning of lexical syntax, Computational Linguistics 19(3) 1993: 243-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora,</title>
<date>1997</date>
<booktitle>Proceedings of the 5th ACL Conference on Applied Natural Language Processing,</booktitle>
<pages>356--363</pages>
<location>Washington, DC,</location>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Briscoe, Ted and John Carroll, Automatic extraction of subcategorization from corpora, Proceedings of the 5th ACL Conference on Applied Natural Language Processing, Washington, DC, 1997: 356-363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax,</title>
<date>1965</date>
<publisher>MIT Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="1100" citStr="Chomsky (1965)" startWordPosition="143" endWordPosition="145">y acquired lexicons with subcategorization information have already proved accurate and useful enough for some purposes but their accuracy still shows room for improvement. By means of diathesis alternation, this paper proposes a new filtering method, which improved the performance of Korhonen’s acquisition system remarkably, with the precision increased to 91.18% and recall unchanged, making the acquired lexicon much more practical for further manual proofreading and other NLP uses. 1 Introduction Subcategorization is the process that further classifies a syntactic category into its subsets. Chomsky (1965) defines the function of strict subcategorization features as appointing a set of constraints that dominate the selection of verbs and other arguments in deep structure. Large subcategorized verbal lexicons have proved to be crucially important for many tasks of natural language processing, such as probabilistic parsers (Korhonen, 2001, 2002) and verb classifications (Schulte im Walde, 2002; Korhonen, 2003). Since Brent (1993) a considerable amount of research focusing on large-scaled automatic acquisition of subcategorization frames (SCF) has met with some success not only in English but also</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Chomsky, Noam, Aspects of the Theory of Syntax, MIT Press, Cambridge, 1965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupala</author>
</authors>
<title>Acquiring Verb Subcategorization from Spanish Corpora, PhD program “Cognitive Science and Language”, Universitat de</title>
<date>2003</date>
<pages>67--68</pages>
<location>Barcelona,</location>
<contexts>
<context position="1793" citStr="Chrupala, 2003" startWordPosition="250" endWordPosition="251"> of constraints that dominate the selection of verbs and other arguments in deep structure. Large subcategorized verbal lexicons have proved to be crucially important for many tasks of natural language processing, such as probabilistic parsers (Korhonen, 2001, 2002) and verb classifications (Schulte im Walde, 2002; Korhonen, 2003). Since Brent (1993) a considerable amount of research focusing on large-scaled automatic acquisition of subcategorization frames (SCF) has met with some success not only in English but also in many other languages, including German (Schulte im Walde, 2002), Spanish (Chrupala, 2003), Czech (Sarkar and Zeman, 2000), Portuguese (Gamallo et. al, 2002), and Chinese (Han et al, 2004). The general objective of this research is to acquire from a given corpus the SCF types and numbers for predicate verbs. Two typical steps during the process of automatic acquisition are hypothesis generation and selection. Usually based on heuristic rules, the first step generates SCF hypotheses for involved verbs; and the second selects reliable ones via statistical methods, such as BHT (binomial hypothesis testing), LLR (log likelihood ratio) and MLE (maximum likelihood estimation). This secon</context>
</contexts>
<marker>Chrupala, 2003</marker>
<rawString>Chrupala, Grzegorz, Acquiring Verb Subcategorization from Spanish Corpora, PhD program “Cognitive Science and Language”, Universitat de Barcelona, 2003: 67-68.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Ellis</author>
</authors>
<title>Understanding Second language Acquisition,</title>
<location>Oxford University Press.1985</location>
<marker>Ellis, </marker>
<rawString>Ellis, R Understanding Second language Acquisition, Oxford University Press.1985</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Gamallo</author>
<author>A Agustini</author>
<author>Lopes Gabriel P</author>
</authors>
<title>Using Co-Composition for Acquiring Syntactic and Semantic Subcategorisation,</title>
<date>2002</date>
<booktitle>Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon (SIGLEX),</booktitle>
<pages>34--41</pages>
<location>Philadelphia,</location>
<marker>Gamallo, Agustini, P, 2002</marker>
<rawString>Gamallo, P., Agustini, A. and Lopes Gabriel P., Using Co-Composition for Acquiring Syntactic and Semantic Subcategorisation, Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia, 2002: 34-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiwu Han</author>
<author>Tiejun Zhao</author>
<author>Haoliang Qi</author>
<author>Hao Yu</author>
</authors>
<title>Subcategorization Acquisition and Evaluation for Chinese Verbs,</title>
<date>2004</date>
<booktitle>Proceedings of the COLING</booktitle>
<pages>723--728</pages>
<contexts>
<context position="1891" citStr="Han et al, 2004" startWordPosition="265" endWordPosition="268"> subcategorized verbal lexicons have proved to be crucially important for many tasks of natural language processing, such as probabilistic parsers (Korhonen, 2001, 2002) and verb classifications (Schulte im Walde, 2002; Korhonen, 2003). Since Brent (1993) a considerable amount of research focusing on large-scaled automatic acquisition of subcategorization frames (SCF) has met with some success not only in English but also in many other languages, including German (Schulte im Walde, 2002), Spanish (Chrupala, 2003), Czech (Sarkar and Zeman, 2000), Portuguese (Gamallo et. al, 2002), and Chinese (Han et al, 2004). The general objective of this research is to acquire from a given corpus the SCF types and numbers for predicate verbs. Two typical steps during the process of automatic acquisition are hypothesis generation and selection. Usually based on heuristic rules, the first step generates SCF hypotheses for involved verbs; and the second selects reliable ones via statistical methods, such as BHT (binomial hypothesis testing), LLR (log likelihood ratio) and MLE (maximum likelihood estimation). This second step is also called statistical filtering and has been widely regarded as problematic. English r</context>
<context position="3312" citStr="Han et al, 2004" startWordPosition="480" endWordPosition="483">remarkable improvement in the acquisition performance, for example diathesis alternation and semantic motivation (Korhonen, 1998, 2001, 2002). For the convenience of comparison between performances of different SCF acquisition methods, we define absolute and relative recall in this paper. By absolute recall, we mean the figure computed against the background of input corpus, while relative recall is against the set of generated hypotheses. At present, automatically acquired verb lexicons with SCF information have already proved accurate and useful enough for some NLP purposes (Korhonen, 2001; Han et al, 2004). As for English, Korhonen (2002) reported that semantically motivated SCF acquisition achieved a precision of 87.1%, an absolute recall of 71.2% and a relative recall of 85.27%, thus making the acquired lexicon much more accurate and useful. However, the accuracy still shows room for improvement, especially for those SCF hypotheses with low frequencies. Detailed analysis on the acquisition system and some resulting data shows that three main causes should account for the comparatively unsatisfactory performance: a. the imperfect hypothesis generator, b. the Zipfian 331 Proceedings of the COLI</context>
</contexts>
<marker>Han, Zhao, Qi, Yu, 2004</marker>
<rawString>Han, Xiwu, Tiejun Zhao, Haoliang Qi, and Hao Yu, Subcategorization Acquisition and Evaluation for Chinese Verbs, Proceedings of the COLING 2004, 2004: 723-728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Automatic Extraction of Subcategorization Frames from Corpora –Improving Filtering with Diathesis Alternations,</title>
<date>1998</date>
<note>Please refer to http://www.folli.uva.nl/CD/1998/pdf/keller/korhon en.pdf</note>
<contexts>
<context position="2824" citStr="Korhonen, 1998" startWordPosition="405" endWordPosition="406">and the second selects reliable ones via statistical methods, such as BHT (binomial hypothesis testing), LLR (log likelihood ratio) and MLE (maximum likelihood estimation). This second step is also called statistical filtering and has been widely regarded as problematic. English researchers have proposed some methods adjusting the corpus hypothesis frequencies before or while filtering. These methods are often called backoff techniques for SCF acquisition. Some of them represent a remarkable improvement in the acquisition performance, for example diathesis alternation and semantic motivation (Korhonen, 1998, 2001, 2002). For the convenience of comparison between performances of different SCF acquisition methods, we define absolute and relative recall in this paper. By absolute recall, we mean the figure computed against the background of input corpus, while relative recall is against the set of generated hypotheses. At present, automatically acquired verb lexicons with SCF information have already proved accurate and useful enough for some NLP purposes (Korhonen, 2001; Han et al, 2004). As for English, Korhonen (2002) reported that semantically motivated SCF acquisition achieved a precision of 8</context>
<context position="11530" citStr="Korhonen (1998)" startWordPosition="1822" endWordPosition="1823">rwise, if there are diathesis alternations between some of the SCFs that a verb enters, then formula (1) and (2) must be modified as illustrated in formula (3) and (4). In either case, for the sake of convenience, it would be better to combine the formulas as shown in (5) and (6). 3 i, 3j, i # j, p(scfi |scfj, v) &gt; 0 ... (3) n Ep scf i v ( |) &gt; i=1 b&apos;i,b&apos;j,i # j, p(scfi |scfj,v) &gt;_ 0 ... (5) n Ep scf i v ( |) &gt;_ i=1 For English verbs, previous research has achieved great progress in diathesis alternation and relative applications, such as the work of Levin (1993) and McCarthy (2001). Besides, Korhonen (1998) has proved that diathesis alternation could be used as heuristic information for backoff estimates to improve the general performance of subcategorization acquisition. However, determining where and how to seed the heuristic remains difficult. Korhonen (1998) employed diathesis alternations in Briscoe and Carroll’s system to improve the performance of their BHT filter. Although the precision rate increased from 61.22% to 1 ... (2) 1 ... (4) 1 ... (6) 333 69.42% and the recall rate from 44.70% to 50.81%, the results were still not accurate enough for possible practical NLP uses. Korhonen obtai</context>
<context position="14103" citStr="Korhonen (1998)" startWordPosition="2222" endWordPosition="2223">ore, the newly established assumption doesn’t fit the actual situation either, and the probability sums ip(scfi|v) and i,jp(scfi|scfj, v) neither need or can be normalized. Based on the above methodology, we formed a new filtering method with diathesis alternations as heuristic information, which is, in fact, derived from the simple MLE filter and based on formula (5) and (6). The algorithm can be briefly expressed as shown in Table 2. 2 For the sake of consistency in this paper and for the convenience of understanding, formulae formats here are modified. They may look different from those of Korhonen (1998), but they are actually the same. Table 2. The New Filtering Method. For hypotheses of a given verb v, 1. if p(scfi|v) &gt; 01, accept scfi into the output set S; 2. else if p(scfi|v) &gt; 02, &amp; p(scfi|scfj, v) &gt; 0, &amp; scfj S, accept scfi into set S; 3. Go to step 1 until S doesn’t increase. In our method, two filters are employed. For each verb involved, first a common MLE filter is used, but it employs a threshold 01 that is much higher than usual, and those SCF hypotheses that satisfy the requirement are accepted. Then, all of the remainder of the hypotheses are checked by another MLE filter seede</context>
</contexts>
<marker>Korhonen, 1998</marker>
<rawString>Korhonen, Anna, Automatic Extraction of Subcategorization Frames from Corpora –Improving Filtering with Diathesis Alternations, 1998. Please refer to http://www.folli.uva.nl/CD/1998/pdf/keller/korhon en.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Subcategorization Acquisition, Dissertation for PhD,</title>
<date>2001</date>
<institution>Trinity Hall University of Cambridge,</institution>
<contexts>
<context position="1437" citStr="Korhonen, 2001" startWordPosition="196" endWordPosition="197">precision increased to 91.18% and recall unchanged, making the acquired lexicon much more practical for further manual proofreading and other NLP uses. 1 Introduction Subcategorization is the process that further classifies a syntactic category into its subsets. Chomsky (1965) defines the function of strict subcategorization features as appointing a set of constraints that dominate the selection of verbs and other arguments in deep structure. Large subcategorized verbal lexicons have proved to be crucially important for many tasks of natural language processing, such as probabilistic parsers (Korhonen, 2001, 2002) and verb classifications (Schulte im Walde, 2002; Korhonen, 2003). Since Brent (1993) a considerable amount of research focusing on large-scaled automatic acquisition of subcategorization frames (SCF) has met with some success not only in English but also in many other languages, including German (Schulte im Walde, 2002), Spanish (Chrupala, 2003), Czech (Sarkar and Zeman, 2000), Portuguese (Gamallo et. al, 2002), and Chinese (Han et al, 2004). The general objective of this research is to acquire from a given corpus the SCF types and numbers for predicate verbs. Two typical steps during</context>
<context position="3294" citStr="Korhonen, 2001" startWordPosition="478" endWordPosition="479">hem represent a remarkable improvement in the acquisition performance, for example diathesis alternation and semantic motivation (Korhonen, 1998, 2001, 2002). For the convenience of comparison between performances of different SCF acquisition methods, we define absolute and relative recall in this paper. By absolute recall, we mean the figure computed against the background of input corpus, while relative recall is against the set of generated hypotheses. At present, automatically acquired verb lexicons with SCF information have already proved accurate and useful enough for some NLP purposes (Korhonen, 2001; Han et al, 2004). As for English, Korhonen (2002) reported that semantically motivated SCF acquisition achieved a precision of 87.1%, an absolute recall of 71.2% and a relative recall of 85.27%, thus making the acquired lexicon much more accurate and useful. However, the accuracy still shows room for improvement, especially for those SCF hypotheses with low frequencies. Detailed analysis on the acquisition system and some resulting data shows that three main causes should account for the comparatively unsatisfactory performance: a. the imperfect hypothesis generator, b. the Zipfian 331 Proce</context>
<context position="6257" citStr="Korhonen, 2001" startWordPosition="943" endWordPosition="944">SANNE corpus, returns ranked analyses using a feature-based unification grammar. • A pattern extractor, which extracts subcategorization patterns, i.e. local syntactic frames, including the syntactic frames, including the syntactic categories and head lemmas. • A pattern classifier, which assigns patterns to SCFs or rejects them as unclassifiable. • A SCF filter, which evaluates sets of SCFs gathered for a predicate verb. Nowadays, in most related researches, the performances of subcategorization acquisition systems are often evaluated in terms of precision, recall and F measure of SCF types (Korhonen, 2001, 2002). Generally, precision is the percentage of SCFs that the system proposes correctly, while recall is the percentage of SCFs in the gold standard that the system proposes: |True positives| |True positives|+|False positives| |True positives| |True positives|+|False negatives| 2 * Precision * Recall Precision + Recall Here, true positives are correct SCF types proposed by the system, false positives are incorrect SCF types proposed by system, and false negatives are correct SCF types not proposed by the system. 3 The MLE Filtering Method The present SCF acquisition system for English verbs</context>
<context position="10462" citStr="Korhonen, 2001" startWordPosition="1627" endWordPosition="1628">d d, which are pinyin notations, show tones of the Chinese syllables, and the two sentences, in English, generally mean He ate an apple. may enter the ba-object-raising alternation where the object is shifted forward by the preposition ba3 ( ) to the location between the subject and the predicate, as illustrated in Figure 1. ba3 Ta1 chi1 le0 pin2guo3. ba-object-raising Figure 1. Ba-object-raising Alternation. Subcategorization of verbs has much to do with diathesis alternations, and most SCF researchers regard information of diathesis alternation as an indispensable part of subcategorization (Korhonen, 2001; McCarthy, 2001). Therefore, one may conclude that, for subcategorization acquisition, the independence assumption supporting the MLE filter is not as appropriate as previously thought. For a given verb, the assumption will be appropriate and sufficient if and only if there is no diathesis alternation between all the SCFs it enters, and formula (1) and (2) in Section 3 are efficient enough to serve as a foundation for the MLE filtering method. Otherwise, if there are diathesis alternations between some of the SCFs that a verb enters, then formula (1) and (2) must be modified as illustrated in</context>
<context position="18439" citStr="Korhonen, 2001" startWordPosition="2942" endWordPosition="2943">cquisition, i.e. hypothesis generation. 6 Conclusion Our new filtering method removed the inappropriate assumptions and takes much more advan3 Korhonen (2002) reported the non-filtering absolute recall ratio of her experiment was about 83.5%. She didn’t give any explanation with her evaluation resources why here non-filtering Ab_R was so much lower. Therefore, the Ab_R and Ab_F figures are not comparable here. tage of what can be observed in the corpus by drawing on the alternative relationship between SCF hypotheses with higher and lower frequencies. Unlike the semantically motivated method (Korhonen, 2001, 2002), which is dependent on verb classifications that linguistic resources are able to provide, our filter needs no prior knowledge other than reasonable diathesis alternation information and may work well for most verbs in other languages with sufficient predicative tokens. Our experimental results suggest that the proposed technique improves the general performance of the English subcategorization acquisition system, and leaves only a little room for further improvement in statistical filtering methods. However, approaches that are more complicated still exist theoretically, for instance,</context>
</contexts>
<marker>Korhonen, 2001</marker>
<rawString>Korhonen, Anna, Subcategorization Acquisition, Dissertation for PhD, Trinity Hall University of Cambridge, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Subcategorization Acquisition,</title>
<date>2002</date>
<tech>Technical Report Number 530,</tech>
<institution>Trinity Hall University of Cambridge,</institution>
<contexts>
<context position="3345" citStr="Korhonen (2002)" startWordPosition="487" endWordPosition="488">isition performance, for example diathesis alternation and semantic motivation (Korhonen, 1998, 2001, 2002). For the convenience of comparison between performances of different SCF acquisition methods, we define absolute and relative recall in this paper. By absolute recall, we mean the figure computed against the background of input corpus, while relative recall is against the set of generated hypotheses. At present, automatically acquired verb lexicons with SCF information have already proved accurate and useful enough for some NLP purposes (Korhonen, 2001; Han et al, 2004). As for English, Korhonen (2002) reported that semantically motivated SCF acquisition achieved a precision of 87.1%, an absolute recall of 71.2% and a relative recall of 85.27%, thus making the acquired lexicon much more accurate and useful. However, the accuracy still shows room for improvement, especially for those SCF hypotheses with low frequencies. Detailed analysis on the acquisition system and some resulting data shows that three main causes should account for the comparatively unsatisfactory performance: a. the imperfect hypothesis generator, b. the Zipfian 331 Proceedings of the COLING/ACL 2006 Main Conference Poste</context>
<context position="17983" citStr="Korhonen (2002)" startWordPosition="2872" endWordPosition="2873"> 3 implies that there is little room left for improvement of the statistical filter, since the absolute recall ratio is only 2.1% lower than that of the nonfiltering method. Whereas, detailed analysis of the evaluation corpus shows that the hypothesis generator accounts for about 95% of those unrecalled and wrongly recalled SCF types, which indicates, for the present time, more improvement efforts need to be made on the first step of subcategorization acquisition, i.e. hypothesis generation. 6 Conclusion Our new filtering method removed the inappropriate assumptions and takes much more advan3 Korhonen (2002) reported the non-filtering absolute recall ratio of her experiment was about 83.5%. She didn’t give any explanation with her evaluation resources why here non-filtering Ab_R was so much lower. Therefore, the Ab_R and Ab_F figures are not comparable here. tage of what can be observed in the corpus by drawing on the alternative relationship between SCF hypotheses with higher and lower frequencies. Unlike the semantically motivated method (Korhonen, 2001, 2002), which is dependent on verb classifications that linguistic resources are able to provide, our filter needs no prior knowledge other tha</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Korhonen, Anna, Subcategorization Acquisition, Technical Report Number 530, Trinity Hall University of Cambridge, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Yuval Krymolowski, Zvika Marx, Clustering Polysemic Subcategorization Frame Distributions Semantically,</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>64--71</pages>
<contexts>
<context position="1510" citStr="Korhonen, 2003" startWordPosition="207" endWordPosition="208">lexicon much more practical for further manual proofreading and other NLP uses. 1 Introduction Subcategorization is the process that further classifies a syntactic category into its subsets. Chomsky (1965) defines the function of strict subcategorization features as appointing a set of constraints that dominate the selection of verbs and other arguments in deep structure. Large subcategorized verbal lexicons have proved to be crucially important for many tasks of natural language processing, such as probabilistic parsers (Korhonen, 2001, 2002) and verb classifications (Schulte im Walde, 2002; Korhonen, 2003). Since Brent (1993) a considerable amount of research focusing on large-scaled automatic acquisition of subcategorization frames (SCF) has met with some success not only in English but also in many other languages, including German (Schulte im Walde, 2002), Spanish (Chrupala, 2003), Czech (Sarkar and Zeman, 2000), Portuguese (Gamallo et. al, 2002), and Chinese (Han et al, 2004). The general objective of this research is to acquire from a given corpus the SCF types and numbers for predicate verbs. Two typical steps during the process of automatic acquisition are hypothesis generation and selec</context>
</contexts>
<marker>Korhonen, 2003</marker>
<rawString>Korhonen, Anna, Yuval Krymolowski, Zvika Marx, Clustering Polysemic Subcategorization Frame Distributions Semantically, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, 2003: 64-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Subcategorization Evaluation Resources.</title>
<date>2005</date>
<contexts>
<context position="4579" citStr="Korhonen (2005)" startWordPosition="673" endWordPosition="674">31–336, Sydney, July 2006. c�2006 Association for Computational Linguistics distribution of syntactic patterns, c. the incomplete partition over SCF types of a given verb. The first problem mainly comes from the inadequate parsing performance and noises existing in the corpus, while the other two problems are inherent to natural languages and should be solved in terms of acquisition techniques particularly during the process of hypothesis selection. 2 Related Work The empirical background of this paper is the public resource for subcategorization acquisition of English verbs, provided by Anna Korhonen (2005) in her personal home page. The data include 30 verbs, as shown in Table 1, and their unfiltered SCF hypotheses, which were automatically generated via Briscoe and Carroll’s (1997) SCF acquisition system, and the manually established standard. Table 1. English Verbs in Use. add agree attach bring carry carve chop cling clip fly cut travel drag communicate give lend lock marry meet mix move offer provide visit push sail send slice supply swing For each verb, there is a corpus of 1000 sentences extracted from the BNC, and all together 42 SCF types are involved in the corpus. The framework of Bri</context>
</contexts>
<marker>Korhonen, 2005</marker>
<rawString>Korhonen, Anna. Subcategorization Evaluation Resources. http://www.cl.cam.ac.uk/users/alk23/subcat/subcat.html. 2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations,</title>
<date>1993</date>
<publisher>Chicago University Press,</publisher>
<location>Chicago,</location>
<contexts>
<context position="11484" citStr="Levin (1993)" startWordPosition="1816" endWordPosition="1817">undation for the MLE filtering method. Otherwise, if there are diathesis alternations between some of the SCFs that a verb enters, then formula (1) and (2) must be modified as illustrated in formula (3) and (4). In either case, for the sake of convenience, it would be better to combine the formulas as shown in (5) and (6). 3 i, 3j, i # j, p(scfi |scfj, v) &gt; 0 ... (3) n Ep scf i v ( |) &gt; i=1 b&apos;i,b&apos;j,i # j, p(scfi |scfj,v) &gt;_ 0 ... (5) n Ep scf i v ( |) &gt;_ i=1 For English verbs, previous research has achieved great progress in diathesis alternation and relative applications, such as the work of Levin (1993) and McCarthy (2001). Besides, Korhonen (1998) has proved that diathesis alternation could be used as heuristic information for backoff estimates to improve the general performance of subcategorization acquisition. However, determining where and how to seed the heuristic remains difficult. Korhonen (1998) employed diathesis alternations in Briscoe and Carroll’s system to improve the performance of their BHT filter. Although the precision rate increased from 61.22% to 1 ... (2) 1 ... (4) 1 ... (6) 333 69.42% and the recall rate from 44.70% to 50.81%, the results were still not accurate enough f</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, B., English Verb Classes and Alternations, Chicago University Press, Chicago, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
</authors>
<title>Lexical Acquisition at the SyntaxSemantics Interface: Diathesis Alternations, Subcategorization Frames and Selectional Preferences, PhD thesis,</title>
<date>2001</date>
<institution>University of Sussex,</institution>
<contexts>
<context position="10479" citStr="McCarthy, 2001" startWordPosition="1629" endWordPosition="1630">inyin notations, show tones of the Chinese syllables, and the two sentences, in English, generally mean He ate an apple. may enter the ba-object-raising alternation where the object is shifted forward by the preposition ba3 ( ) to the location between the subject and the predicate, as illustrated in Figure 1. ba3 Ta1 chi1 le0 pin2guo3. ba-object-raising Figure 1. Ba-object-raising Alternation. Subcategorization of verbs has much to do with diathesis alternations, and most SCF researchers regard information of diathesis alternation as an indispensable part of subcategorization (Korhonen, 2001; McCarthy, 2001). Therefore, one may conclude that, for subcategorization acquisition, the independence assumption supporting the MLE filter is not as appropriate as previously thought. For a given verb, the assumption will be appropriate and sufficient if and only if there is no diathesis alternation between all the SCFs it enters, and formula (1) and (2) in Section 3 are efficient enough to serve as a foundation for the MLE filtering method. Otherwise, if there are diathesis alternations between some of the SCFs that a verb enters, then formula (1) and (2) must be modified as illustrated in formula (3) and </context>
</contexts>
<marker>McCarthy, 2001</marker>
<rawString>McCarthy, D., Lexical Acquisition at the SyntaxSemantics Interface: Diathesis Alternations, Subcategorization Frames and Selectional Preferences, PhD thesis, University of Sussex, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Peters</author>
</authors>
<title>The Unit of Language Acquisition,</title>
<date>1983</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9072" citStr="Peters, 1983" startWordPosition="1403" endWordPosition="1404"> hold the same meanings. In actual application, the probability p(scfi|v) is estimated from the observed frequency f(scfi, v), and the conditional probability p(scfi|scfj, v) is assumed to be zero. This means any two SCF types entered by a given verb are taken for granted to be probabilistically independent from each other. However, this assumption can sometimes be far from appropriate. 4 Diathesis Alternations and Filtering Much linguistic research focusing on child language acquisition has revealed that many children are able to produce new grammatical sentences from what they have learned (Peters, 1983; Ellis, 1985). This implies that the widely-used independence assumption in the field of NLP may not be very appropriate, at least for syntactic patterns. If this assumption should be removed, a possible heuristic could be the information of diathesis alternations, which is also another convincing counterargument. Diathesis alternations are generally regarded as alternative ways in which verbs express their arguments. Examples are as follows: a. He broke the glass. b. The glass broke. c. Ta1 chi1 le0 pin2guo3. ( ) d. Ta1 ba3 pin2guo3 chi1 le01. ( ) In the above examples, the English verb brea</context>
</contexts>
<marker>Peters, 1983</marker>
<rawString>Peters, A. The Unit of Language Acquisition, Cambridge University Press. 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sarkar</author>
<author>D Zeman</author>
</authors>
<title>Automatic Extraction of Subcategorization Frames for Czech,</title>
<date>2000</date>
<booktitle>Proceedings of the 19th International Conference on Computational Linguistics, Saarbrucken,</booktitle>
<note>Please refer to http://www.sfu.ca/~anoop/papers/ pdf/coling0final.pdf</note>
<contexts>
<context position="1825" citStr="Sarkar and Zeman, 2000" startWordPosition="253" endWordPosition="256">inate the selection of verbs and other arguments in deep structure. Large subcategorized verbal lexicons have proved to be crucially important for many tasks of natural language processing, such as probabilistic parsers (Korhonen, 2001, 2002) and verb classifications (Schulte im Walde, 2002; Korhonen, 2003). Since Brent (1993) a considerable amount of research focusing on large-scaled automatic acquisition of subcategorization frames (SCF) has met with some success not only in English but also in many other languages, including German (Schulte im Walde, 2002), Spanish (Chrupala, 2003), Czech (Sarkar and Zeman, 2000), Portuguese (Gamallo et. al, 2002), and Chinese (Han et al, 2004). The general objective of this research is to acquire from a given corpus the SCF types and numbers for predicate verbs. Two typical steps during the process of automatic acquisition are hypothesis generation and selection. Usually based on heuristic rules, the first step generates SCF hypotheses for involved verbs; and the second selects reliable ones via statistical methods, such as BHT (binomial hypothesis testing), LLR (log likelihood ratio) and MLE (maximum likelihood estimation). This second step is also called statistica</context>
</contexts>
<marker>Sarkar, Zeman, 2000</marker>
<rawString>Sarkar, A. and Zeman, D., Automatic Extraction of Subcategorization Frames for Czech, Proceedings of the 19th International Conference on Computational Linguistics, Saarbrucken, Germany, 2000. Please refer to http://www.sfu.ca/~anoop/papers/ pdf/coling0final.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shulte im Walde</author>
<author>Sabine</author>
</authors>
<title>Inducing German Semantic Verb Classes from Purely Syntactic Subcategorization Information,</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>223--230</pages>
<marker>Walde, Sabine, 2002</marker>
<rawString>Shulte im Walde, Sabine, Inducing German Semantic Verb Classes from Purely Syntactic Subcategorization Information, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 2002: 223-230.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>