<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.154837">
<title confidence="0.998452">
A Stochastic Finite-State Morphological Parser for Turkish
</title>
<author confidence="0.998676">
Has¸im Sak &amp; Tunga G¨ung¨or Murat Sarac¸lar
</author>
<affiliation confidence="0.838817333333333">
Dept. of Computer Engineering Dept. of Electrical &amp; Electronics Engineering
Bo˘gazic¸i University Bo˘gazic¸i University
TR-34342, Bebek, ˙Istanbul, Turkey TR-34342, Bebek, ˙Istanbul, Turkey
</affiliation>
<email confidence="0.964715">
hasim.sak@boun.edu.tr murat.saraclar@boun.edu.tr
gungort@boun.edu.tr
</email>
<sectionHeader confidence="0.99368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999906933333333">
This paper presents the first stochastic
finite-state morphological parser for Turk-
ish. The non-probabilistic parser is a
standard finite-state transducer implemen-
tation of two-level morphology formal-
ism. A disambiguated text corpus of
200 million words is used to stochas-
tize the morphotactics transducer, then it
is composed with the morphophonemics
transducer to get a stochastic morpho-
logical parser. We present two applica-
tions to evaluate the effectiveness of the
stochastic parser; spelling correction and
morphology-based language modeling for
speech recognition.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986904761905">
Turkish is an agglutinative language with a highly
productive inflectional and derivational morphol-
ogy. The computational aspects of Turkish mor-
phology have been well studied and several mor-
phological parsers have been built (Oflazer, 1994),
(G¨ung¨or, 1995).
In language processing applications, we may
need to estimate a probability distribution over all
word forms. For example, we need probability es-
timates for unigrams to rank misspelling sugges-
tions for spelling correction. None of the previ-
ous studies for Turkish have addressed this prob-
lem. For morphologically complex languages, es-
timating a probability distribution over a static vo-
cabulary is not very desirable due to high out-of-
vocabulary rates. It would be very convenient for a
morphological parser as a word generator/analyzer
to also output a probability estimate for a word
generated/analyzed. In this work, we build such a
stochastic morphological parser for Turkish1 and
give two example applications for evaluation.
</bodyText>
<footnote confidence="0.938322">
1The stochastic morphological parser is available for re-
search purposes at http://www.cmpe.boun.edu.tr/˜hasim
</footnote>
<sectionHeader confidence="0.93836" genericHeader="method">
2 Language Resources
</sectionHeader>
<bodyText confidence="0.999956192307692">
We built a morphological parser using the two-
level morphology formalism of Koskenniemi
(1984). The two-level phonological rules and the
morphotactics were adapted from the PC-KIMMO
implementation of Oflazer (1994). The rules were
compiled using the twolc rule compiler (Karttunen
and Beesley, 1992). A new root lexicon of 55,278
words based on the Turkish Language Institution
dictionary2 was compiled. For finite-state opera-
tions and for running the parser, we used the Open-
FST weighted finite-state transducer library (Al-
lauzen et al., 2007). The parser can analyze about
8700 words per second on a 2.33 GHz Intel Xeon
processor.
We need a text corpus for estimating the param-
eters of a statistical model of morphology. For this
purpose, we compiled a text corpus of 200 million-
words by collecting texts from online newspa-
pers. The morphological parser can analyze about
96.7% of the tokens.
The morphological parser may output more
than one possible analysis for a word due to am-
biguity. For example, the parser returns four
analyses for the word kedileri as shown below.
The morphological representation is similar to
the one used by Oflazer and Inkelas (2006).
</bodyText>
<equation confidence="0.99345325">
kedi[Noun]+lAr[A3pl]+SH[P3sg]+[Nom] (his/her cats)
kedi[Noun]+lAr[A3pl]+[Pnon]+YH[Acc] (the cats)
kedi[Noun]+lAr[A3pl]+SH[P3pl]+[Nom] (their cats)
kedi[Noun]+[A3sg]+lArH[P3pl]+[Nom] (their cat)
</equation>
<bodyText confidence="0.999301166666667">
We need to resolve this ambiguity to train a prob-
abilistic morphology model. For this purpose, we
used our averaged perceptron-based morphologi-
cal disambiguator (Sak et al., 2008). The disam-
biguation system achieves about 97.05% disam-
biguation accuracy on the test set.
</bodyText>
<footnote confidence="0.985649">
2http://www.tdk.gov.tr
</footnote>
<page confidence="0.945728">
273
</page>
<note confidence="0.9801845">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 273–276,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.99958">
Figure 1: Finite-state transducer for the word kedileri.
</figureCaption>
<figure confidence="0.998008259259259">
i:+SH[P3pl]/2.89
e:+[Nom]
14
k:e/2.34 e:e/1.76 d:e/5.68 i:kedi[Noun]
0 1 2 3 4
l:+lAr[A3pl]/1.19
6
e:e
8
r:e
10
i:+SH[P3sg]/0.62
e:+[Pnon]
e:+[Nom]
13
i:+YH[Acc]/1.66
12
15/3.83
e:+[A3sg]
5
l:+lArH[P3pl]/5.73
7
e:e
9
r:e
11
i:+[Nom]/1.06
</figure>
<sectionHeader confidence="0.869605" genericHeader="method">
3 Stochastic Morphological Parser
</sectionHeader>
<bodyText confidence="0.999837395061729">
The finite-state transducer of the morphological
parser is obtained as the composition of the mor-
phophonemics transducer mp and the morphotac-
tics transducer mt; mp o mt. The morphotac-
tics transducer encodes the morphosyntax of the
language. If we can estimate a statistical mor-
phosyntactic model, we can convert the morpho-
logical parser to a probabilistic one by composing
the probabilistic morphotactics transducer with the
morphophonemics transducer. Eisner (2002) gives
a general EM algorithm for parameter estimation
in probabilistic finite-state transducers. The algo-
rithm uses a bookkeeping trick (expectation semir-
ing) to compute the expected number of traversals
of each arc in the E step. The M step reestimates
the probabilities of the arcs from each state to be
proportional to the expected number of traversals
of each arc - the arc probabilities are normalized
at each state to make the finite-state transducer
Markovian. However, we do not need this general
method of training. Since we can disambiguate
the possible morphosyntactic tag sequences of a
word, there is a single path in the morphotactics
transducer that matches the chosen morphosyntac-
tic tag sequence. Then the maximum-likelihood
estimates of the weights of the arcs in the morpho-
tactics transducer are found by setting the weights
proportional to the number of traversals of each
arc. We can use a specialized semiring to cleanly
and efficiently count the number of traversals of
each arc.
Weights in finite-state transducers are elements
of a semiring, which defines two binary operations
® and ®, where ® is used to combine the weights
of arcs on a path into a path weight and ® is used
to combine the weights of alternative paths (Bers-
tel and Reutenauer, 1988). We define a counting
semiring to keep track of the number of traver-
sals of each arc. The weights in the mt trans-
ducer are converted to the counting semiring. In
this semiring, the weigths are vectors of integers
having dimension as the total number of arcs in
the mt transducer. We number the arcs in the mt
transducer and set the weight of the nth arc as the
nth basis vector. The binary plus ® and the times
® operations of the counting semiring are defined
as the sum of the weight vectors. Thus, the nth
value of the vector in the counting semiring just
counts the appearances of the nth arc of mt in a
path.
To estimate the weights of the stochastic model
of the mt transducer, we use the text corpus col-
lected from the web. First we parse the words
in the corpus to get all the possible analyses of
the words. Then we disambiguate the morpho-
logical analyses of the words to select one of the
morphosyntactic tag sequences xi for each word.
We build a finite-state transducer E x xi that maps
E symbol to xi in the counting semiring. The
weights of this transducer are zero vectors having
the same dimension as the mt transducer. Then the
finite-state transducer (Exxi)o(mtxE) having all
E : E arcs can be minimized to get a one-state FST
which has the weight vector that keeps the number
of traversals of each arc in mt. The weight vec-
tor is accumulated for all the xi morphosyntactic
tag sequences in the corpus. The final accumu-
lated weight vector is used to assign probabilities
to each arc in the mt transducer proportional to
the traversal count of the arc, hence resulting in
the stochastic morphotactics transducer ˜mt. We
use add-one smoothing to prevent the arcs having
zero probability. The ˜mt transducer is composed
with the morphophonemics transducer mp to get a
stochastic morphological parser.
The stochastic parser now returns probabilities
with the possible analyses of a word. Figure 1
shows the weighted paths for the four possible
analyses of the word kedileri as represented in the
stochastic parser. The weights are negative log
probabilities.
</bodyText>
<sectionHeader confidence="0.992888" genericHeader="method">
4 Spelling Correction
</sectionHeader>
<bodyText confidence="0.9997905">
The productive morphology of Turkish allows
one to generate very long words such as
</bodyText>
<page confidence="0.994787">
274
</page>
<bodyText confidence="0.99813758974359">
¨ol¨ums¨uzles¸tirdi˘gimizden. Therefore, the detection
and the correction of spelling errors by present-
ing the user with a ranked list of spelling sugges-
tions are highly desired. There have been some
previous studies for spelling checking (Solak and
Oflazer, 1993) and spelling correction (Oflazer,
1996). However there has been no study to ad-
dress the problem of ranking spelling suggestions.
One can use a stochastic morphological parser to
do spelling checking and correction, and present
spelling suggestions ranked with the parser output
probabilities. We assume that a word is misspelled
if the parser fails to return an analysis of the word.
Our method for spelling correction is to enumerate
all the valid and invalid candidates that resemble
the incorrect input word and filter the invalid ones
with the morphological parser.
To enumerate the alternative spellings for a mis-
spelled word, we generate all the words in one-
character edit distance with the input word, where
we consider one symbol insertion, deletion or sub-
stitution, or transposition of adjacent symbols.
The Turkish alphabet includes six special letters
(c¸, ˘g, ı, ¨o, s¸, ¨u) that do not exist in English.
These characters may not be supported in some
keyboards and message transfer protocols; thus
people frequently use their nearest ASCII equiv-
alents (c, g, i, o, s, u, respectively) instead of the
correct forms, e.g., spelling nasılsın as nasilsin.
Therefore, in addition to enumerating words in
one edit distance, we also enumerate all the words
from which the misspelled word can be obtained
by replacing these special Turkish characters with
their ASCII counterparts. For instance, for the
word nasilsin, the alternative spellings nasılsin,
nasilsın, and nasılsın will also be generated.
Note that although the context is important for
spelling correction, we use only unigrams. One
can build a morpheme based language model to
incorporate the context information. We also lim-
ited the edit distance to 1, but it is straightfor-
ward to allow longer edit distances. We can build
a finite-state transducer to enumerate and repre-
sent efficiently all the valid and invalid word forms
that can be obtained by these edit operations on
a word. For example, the deletion of a charac-
ter can be represented by the regular expression
E*(E : c)E* which can be compiled as a finite-
state transducer, where E is the alphabet. The
union of the transducers encoding one-edit dis-
tance operations and the restoration of the special
Turkish characters is precompiled and optimized
with determinization and minimization algorithms
for efficiency. A misspelled input word transducer
can be composed with the resulting transducer and
in turn with the morphological parser to filter out
the invalid word forms. The words with their es-
timated probabilities can be read from the output
transducer and constitute the list of spelling sug-
gestions for the word. The probabilities are used
to rank the list to show to the user. We also handle
the spelling errors where omission of a space char-
acter causes joining of two correct words by split-
ting the word into all combinations of two strings
and checking if the string pieces are valid word
forms. An example list of suggestions with the as-
signed negative log probabilities and their English
glosses for the misspelled word nasilsin is given
below.
nasılsın (14.2) (How are you), nakilsin (15.3) (You are
a transfer), nesilsin (21.0) (You are a generation), nasipsin
(21.2) (You are a share), basilsin (23.9) (You are a bacillus)
On a manually chosen test set containing 225 cor-
rect words which have relatively more complex
morphology and 43 commonly misspelled words,
the Precision and the Recall scores for the detec-
tion of spelling errors are 0.81 and 0.93, respec-
tively.
</bodyText>
<sectionHeader confidence="0.875122" genericHeader="method">
5 Morphology-based Language
Modeling
</sectionHeader>
<bodyText confidence="0.999921619047619">
The closure of the transducer for the stochastic
parser can be considered as a morphology-based
unigram language model. Different than standard
unigram word language models, this morphology-
based model can assign probabilities to words not
seen in the training corpus. It can also achieve
lower out-of-vocabulary (OOV) rates than models
that use a static vocabulary by employing a rela-
tively smaller number of root words in the lexicon.
We compared the performances of the
morphology-based unigram language model
and the unigram word language model on a broad-
cast news transcription task. The acoustic model
uses Hidden Markov Models (HMMs) trained on
183.8 hours of broadcast news speech data. The
test set contains 3.1 hours of speech data (2,410
utterances). A text corpus of 1.2 million words
from the transcriptions of the news recordings was
used to train the stochastic parser as explained in
Section 3 and unigram word language models.
We experimented with four different language
</bodyText>
<page confidence="0.992336">
275
</page>
<figure confidence="0.792374">
Real−time factor (cpu time/audio time)
</figure>
<figureCaption confidence="0.992662">
Figure 2: Word error rate versus real-time factor
obtained by changing the pruning beam width.
</figureCaption>
<bodyText confidence="0.999809733333333">
models. Figure 2 shows the word error rate ver-
sus run-time factor for these models. In this fig-
ure the Word-50K and Word-100K are unigram
word models with the specified vocabulary size
and have the OOV rates 7% and 4.7% on the test
set, respectively. The morphology-based model is
based on the stochastic parser and has the OOV
rate 2.8% . The ‘word+morphology’ model is the
union of the morphology-based model and the un-
igram word model.
Even though the morphology-based model has
a better OOV rate than the word models, the word
error rate (WER) is higher. One of the reasons is
that the transducer for the morphological parser is
ambiguous and cannot be optimized for recogni-
tion in contrast to the word models. Another rea-
son is that the probability estimates of this model
are not as good as the word models since proba-
bility mass is distributed among ambiguous parses
of a word and over the paths in the transducer.
The ‘word+morphology’ model seems to allevi-
ate most of the shortcomings of the morphology
model. It performs better than 50K word model
and is very close to the 100K word model. The
main advantage of morphology-based models is
that we have at hand the morphological analyses
of the words during recognition. We plan to train
a language model over the morphological features
and use this model to rescore the hypothesis gener-
ated by the morphology-based models on-the-fly.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999975">
We described the first stochastic morphological
parser for Turkish and gave two applications. The
first application is a very efficient spelling correc-
tion system where probability estimates are used
for ranking misspelling suggestions. We also gave
the preliminary results for incorporating the mor-
phology as a knowledge source in speech recogni-
tion and the results look promising.
</bodyText>
<sectionHeader confidence="0.995492" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996190125">
This work was supported by the Bo˘gazic¸i Uni-
versity Research Fund under the grant numbers
06A102 and 08M103, the Scientific and Techno-
logical Research Council of Turkey (T ¨UB˙ITAK)
under the grant number 107E261, the Turk-
ish State Planning Organization (DPT) under
the TAM Project, number 2007K120610 and
T ¨UB˙ITAK B˙IDEB 2211.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999276054054054">
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In CIAA 2007, volume 4783 of LNCS, pages
11–23. Springer. http://www.openfst.org.
Jean Berstel and Christophe Reutenauer. 1988. Ratio-
nal Series and their Languages. Springer-Verlag.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL, pages 1–8.
Tunga G¨ung¨or. 1995. Computer Processing of
Turkish: Morphological and Lexical Investigation.
Ph.D. thesis, Bo˘gazic¸i University.
Lauri Karttunen and Kenneth R. Beesley. 1992. Two-
level rule compiler. Technical report, Xerox Palo
Alto Research Center, Palo Alto, CA.
Kimmo Koskenniemi. 1984. A general computational
model for word-form recognition and production. In
ACL, pages 178–181.
Kemal Oflazer and Sharon Inkelas. 2006. The archi-
tecture and the implementation of a finite state pro-
nunciation lexicon for Turkish. Computer Speech
and Language, 20(1):80–106.
Kemal Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Comput-
ing, 9(2):137–148.
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22(1):73–89.
Has¸im Sak, Tunga G¨ung¨or, and Murat Sarac¸lar. 2008.
Turkish language resources: Morphological parser,
morphological disambiguator and web corpus. In
GoTAL 2008, volume 5221 of LNCS, pages 417–
427. Springer.
Ays¸in Solak and Kemal Oflazer. 1993. Design and im-
plementation of a spelling checker for turkish. Lit-
erary and Linguistic Computing, 8(3):113–130.
</reference>
<figure confidence="0.999045714285714">
0.5 1.0 1.5 2.0 2.5
WER (%)
43 44 45 46 47 48
Morphology−based
Word−50K
Word+Morphology
Word−100K
</figure>
<page confidence="0.976108">
276
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.655152">
<title confidence="0.999953">A Stochastic Finite-State Morphological Parser for Turkish</title>
<author confidence="0.998217">Sak</author>
<author confidence="0.998217">Tunga G¨ung¨or Murat</author>
<affiliation confidence="0.999984">Dept. of Computer Engineering Dept. of Electrical &amp; Electronics Engineering University University</affiliation>
<address confidence="0.999811">Bebek, Turkey TR-34342, Bebek, Turkey</address>
<email confidence="0.747452">hasim.sak@boun.edu.trmurat.saraclar@boun.edu.trgungort@boun.edu.tr</email>
<abstract confidence="0.99885825">This paper presents the first stochastic finite-state morphological parser for Turkish. The non-probabilistic parser is a standard finite-state transducer implementation of two-level morphology formalism. A disambiguated text corpus of 200 million words is used to stochastize the morphotactics transducer, then it is composed with the morphophonemics transducer to get a stochastic morphological parser. We present two applications to evaluate the effectiveness of the stochastic parser; spelling correction and morphology-based language modeling for speech recognition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In CIAA 2007,</booktitle>
<volume>4783</volume>
<pages>11--23</pages>
<publisher>Springer. http://www.openfst.org.</publisher>
<contexts>
<context position="2622" citStr="Allauzen et al., 2007" startWordPosition="363" endWordPosition="367">r research purposes at http://www.cmpe.boun.edu.tr/˜hasim 2 Language Resources We built a morphological parser using the twolevel morphology formalism of Koskenniemi (1984). The two-level phonological rules and the morphotactics were adapted from the PC-KIMMO implementation of Oflazer (1994). The rules were compiled using the twolc rule compiler (Karttunen and Beesley, 1992). A new root lexicon of 55,278 words based on the Turkish Language Institution dictionary2 was compiled. For finite-state operations and for running the parser, we used the OpenFST weighted finite-state transducer library (Allauzen et al., 2007). The parser can analyze about 8700 words per second on a 2.33 GHz Intel Xeon processor. We need a text corpus for estimating the parameters of a statistical model of morphology. For this purpose, we compiled a text corpus of 200 millionwords by collecting texts from online newspapers. The morphological parser can analyze about 96.7% of the tokens. The morphological parser may output more than one possible analysis for a word due to ambiguity. For example, the parser returns four analyses for the word kedileri as shown below. The morphological representation is similar to the one used by Oflaz</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In CIAA 2007, volume 4783 of LNCS, pages 11–23. Springer. http://www.openfst.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Berstel</author>
<author>Christophe Reutenauer</author>
</authors>
<title>Rational Series and their Languages.</title>
<date>1988</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="5938" citStr="Berstel and Reutenauer, 1988" startWordPosition="871" endWordPosition="875">s transducer that matches the chosen morphosyntactic tag sequence. Then the maximum-likelihood estimates of the weights of the arcs in the morphotactics transducer are found by setting the weights proportional to the number of traversals of each arc. We can use a specialized semiring to cleanly and efficiently count the number of traversals of each arc. Weights in finite-state transducers are elements of a semiring, which defines two binary operations ® and ®, where ® is used to combine the weights of arcs on a path into a path weight and ® is used to combine the weights of alternative paths (Berstel and Reutenauer, 1988). We define a counting semiring to keep track of the number of traversals of each arc. The weights in the mt transducer are converted to the counting semiring. In this semiring, the weigths are vectors of integers having dimension as the total number of arcs in the mt transducer. We number the arcs in the mt transducer and set the weight of the nth arc as the nth basis vector. The binary plus ® and the times ® operations of the counting semiring are defined as the sum of the weight vectors. Thus, the nth value of the vector in the counting semiring just counts the appearances of the nth arc of</context>
</contexts>
<marker>Berstel, Reutenauer, 1988</marker>
<rawString>Jean Berstel and Christophe Reutenauer. 1988. Rational Series and their Languages. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Parameter estimation for probabilistic finite-state transducers.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="4661" citStr="Eisner (2002)" startWordPosition="662" endWordPosition="663">.62 e:+[Pnon] e:+[Nom] 13 i:+YH[Acc]/1.66 12 15/3.83 e:+[A3sg] 5 l:+lArH[P3pl]/5.73 7 e:e 9 r:e 11 i:+[Nom]/1.06 3 Stochastic Morphological Parser The finite-state transducer of the morphological parser is obtained as the composition of the morphophonemics transducer mp and the morphotactics transducer mt; mp o mt. The morphotactics transducer encodes the morphosyntax of the language. If we can estimate a statistical morphosyntactic model, we can convert the morphological parser to a probabilistic one by composing the probabilistic morphotactics transducer with the morphophonemics transducer. Eisner (2002) gives a general EM algorithm for parameter estimation in probabilistic finite-state transducers. The algorithm uses a bookkeeping trick (expectation semiring) to compute the expected number of traversals of each arc in the E step. The M step reestimates the probabilities of the arcs from each state to be proportional to the expected number of traversals of each arc - the arc probabilities are normalized at each state to make the finite-state transducer Markovian. However, we do not need this general method of training. Since we can disambiguate the possible morphosyntactic tag sequences of a </context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002. Parameter estimation for probabilistic finite-state transducers. In ACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tunga G¨ung¨or</author>
</authors>
<title>Computer Processing of Turkish: Morphological and Lexical Investigation.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Bo˘gazic¸i University.</institution>
<marker>G¨ung¨or, 1995</marker>
<rawString>Tunga G¨ung¨or. 1995. Computer Processing of Turkish: Morphological and Lexical Investigation. Ph.D. thesis, Bo˘gazic¸i University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Kenneth R Beesley</author>
</authors>
<title>Twolevel rule compiler.</title>
<date>1992</date>
<tech>Technical report,</tech>
<institution>Xerox Palo Alto Research Center,</institution>
<location>Palo Alto, CA.</location>
<contexts>
<context position="2377" citStr="Karttunen and Beesley, 1992" startWordPosition="325" endWordPosition="328">yzer to also output a probability estimate for a word generated/analyzed. In this work, we build such a stochastic morphological parser for Turkish1 and give two example applications for evaluation. 1The stochastic morphological parser is available for research purposes at http://www.cmpe.boun.edu.tr/˜hasim 2 Language Resources We built a morphological parser using the twolevel morphology formalism of Koskenniemi (1984). The two-level phonological rules and the morphotactics were adapted from the PC-KIMMO implementation of Oflazer (1994). The rules were compiled using the twolc rule compiler (Karttunen and Beesley, 1992). A new root lexicon of 55,278 words based on the Turkish Language Institution dictionary2 was compiled. For finite-state operations and for running the parser, we used the OpenFST weighted finite-state transducer library (Allauzen et al., 2007). The parser can analyze about 8700 words per second on a 2.33 GHz Intel Xeon processor. We need a text corpus for estimating the parameters of a statistical model of morphology. For this purpose, we compiled a text corpus of 200 millionwords by collecting texts from online newspapers. The morphological parser can analyze about 96.7% of the tokens. The </context>
</contexts>
<marker>Karttunen, Beesley, 1992</marker>
<rawString>Lauri Karttunen and Kenneth R. Beesley. 1992. Twolevel rule compiler. Technical report, Xerox Palo Alto Research Center, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>A general computational model for word-form recognition and production.</title>
<date>1984</date>
<booktitle>In ACL,</booktitle>
<pages>178--181</pages>
<contexts>
<context position="2172" citStr="Koskenniemi (1984)" startWordPosition="298" endWordPosition="299">ating a probability distribution over a static vocabulary is not very desirable due to high out-ofvocabulary rates. It would be very convenient for a morphological parser as a word generator/analyzer to also output a probability estimate for a word generated/analyzed. In this work, we build such a stochastic morphological parser for Turkish1 and give two example applications for evaluation. 1The stochastic morphological parser is available for research purposes at http://www.cmpe.boun.edu.tr/˜hasim 2 Language Resources We built a morphological parser using the twolevel morphology formalism of Koskenniemi (1984). The two-level phonological rules and the morphotactics were adapted from the PC-KIMMO implementation of Oflazer (1994). The rules were compiled using the twolc rule compiler (Karttunen and Beesley, 1992). A new root lexicon of 55,278 words based on the Turkish Language Institution dictionary2 was compiled. For finite-state operations and for running the parser, we used the OpenFST weighted finite-state transducer library (Allauzen et al., 2007). The parser can analyze about 8700 words per second on a 2.33 GHz Intel Xeon processor. We need a text corpus for estimating the parameters of a stat</context>
</contexts>
<marker>Koskenniemi, 1984</marker>
<rawString>Kimmo Koskenniemi. 1984. A general computational model for word-form recognition and production. In ACL, pages 178–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
<author>Sharon Inkelas</author>
</authors>
<title>The architecture and the implementation of a finite state pronunciation lexicon for Turkish. Computer Speech and Language,</title>
<date>2006</date>
<contexts>
<context position="3243" citStr="Oflazer and Inkelas (2006)" startWordPosition="470" endWordPosition="473">2007). The parser can analyze about 8700 words per second on a 2.33 GHz Intel Xeon processor. We need a text corpus for estimating the parameters of a statistical model of morphology. For this purpose, we compiled a text corpus of 200 millionwords by collecting texts from online newspapers. The morphological parser can analyze about 96.7% of the tokens. The morphological parser may output more than one possible analysis for a word due to ambiguity. For example, the parser returns four analyses for the word kedileri as shown below. The morphological representation is similar to the one used by Oflazer and Inkelas (2006). kedi[Noun]+lAr[A3pl]+SH[P3sg]+[Nom] (his/her cats) kedi[Noun]+lAr[A3pl]+[Pnon]+YH[Acc] (the cats) kedi[Noun]+lAr[A3pl]+SH[P3pl]+[Nom] (their cats) kedi[Noun]+[A3sg]+lArH[P3pl]+[Nom] (their cat) We need to resolve this ambiguity to train a probabilistic morphology model. For this purpose, we used our averaged perceptron-based morphological disambiguator (Sak et al., 2008). The disambiguation system achieves about 97.05% disambiguation accuracy on the test set. 2http://www.tdk.gov.tr 273 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 273–276, Suntec, Singapore, 4 August 2009</context>
</contexts>
<marker>Oflazer, Inkelas, 2006</marker>
<rawString>Kemal Oflazer and Sharon Inkelas. 2006. The architecture and the implementation of a finite state pronunciation lexicon for Turkish. Computer Speech and Language, 20(1):80–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Two-level description of Turkish morphology.</title>
<date>1994</date>
<journal>Literary and Linguistic Computing,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="1198" citStr="Oflazer, 1994" startWordPosition="155" endWordPosition="156">iguated text corpus of 200 million words is used to stochastize the morphotactics transducer, then it is composed with the morphophonemics transducer to get a stochastic morphological parser. We present two applications to evaluate the effectiveness of the stochastic parser; spelling correction and morphology-based language modeling for speech recognition. 1 Introduction Turkish is an agglutinative language with a highly productive inflectional and derivational morphology. The computational aspects of Turkish morphology have been well studied and several morphological parsers have been built (Oflazer, 1994), (G¨ung¨or, 1995). In language processing applications, we may need to estimate a probability distribution over all word forms. For example, we need probability estimates for unigrams to rank misspelling suggestions for spelling correction. None of the previous studies for Turkish have addressed this problem. For morphologically complex languages, estimating a probability distribution over a static vocabulary is not very desirable due to high out-ofvocabulary rates. It would be very convenient for a morphological parser as a word generator/analyzer to also output a probability estimate for a </context>
</contexts>
<marker>Oflazer, 1994</marker>
<rawString>Kemal Oflazer. 1994. Two-level description of Turkish morphology. Literary and Linguistic Computing, 9(2):137–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="8426" citStr="Oflazer, 1996" startWordPosition="1301" endWordPosition="1302">possible analyses of a word. Figure 1 shows the weighted paths for the four possible analyses of the word kedileri as represented in the stochastic parser. The weights are negative log probabilities. 4 Spelling Correction The productive morphology of Turkish allows one to generate very long words such as 274 ¨ol¨ums¨uzles¸tirdi˘gimizden. Therefore, the detection and the correction of spelling errors by presenting the user with a ranked list of spelling suggestions are highly desired. There have been some previous studies for spelling checking (Solak and Oflazer, 1993) and spelling correction (Oflazer, 1996). However there has been no study to address the problem of ranking spelling suggestions. One can use a stochastic morphological parser to do spelling checking and correction, and present spelling suggestions ranked with the parser output probabilities. We assume that a word is misspelled if the parser fails to return an analysis of the word. Our method for spelling correction is to enumerate all the valid and invalid candidates that resemble the incorrect input word and filter the invalid ones with the morphological parser. To enumerate the alternative spellings for a misspelled word, we gene</context>
</contexts>
<marker>Oflazer, 1996</marker>
<rawString>Kemal Oflazer. 1996. Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction. Computational Linguistics, 22(1):73–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Has¸im Sak</author>
<author>Tunga G¨ung¨or</author>
<author>Murat Sarac¸lar</author>
</authors>
<title>Turkish language resources: Morphological parser, morphological disambiguator and web corpus.</title>
<date>2008</date>
<booktitle>In GoTAL</booktitle>
<volume>5221</volume>
<pages>417--427</pages>
<publisher>Springer.</publisher>
<marker>Sak, G¨ung¨or, Sarac¸lar, 2008</marker>
<rawString>Has¸im Sak, Tunga G¨ung¨or, and Murat Sarac¸lar. 2008. Turkish language resources: Morphological parser, morphological disambiguator and web corpus. In GoTAL 2008, volume 5221 of LNCS, pages 417– 427. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ays¸in Solak</author>
<author>Kemal Oflazer</author>
</authors>
<title>Design and implementation of a spelling checker for turkish.</title>
<date>1993</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>8--3</pages>
<contexts>
<context position="8386" citStr="Solak and Oflazer, 1993" startWordPosition="1294" endWordPosition="1297">chastic parser now returns probabilities with the possible analyses of a word. Figure 1 shows the weighted paths for the four possible analyses of the word kedileri as represented in the stochastic parser. The weights are negative log probabilities. 4 Spelling Correction The productive morphology of Turkish allows one to generate very long words such as 274 ¨ol¨ums¨uzles¸tirdi˘gimizden. Therefore, the detection and the correction of spelling errors by presenting the user with a ranked list of spelling suggestions are highly desired. There have been some previous studies for spelling checking (Solak and Oflazer, 1993) and spelling correction (Oflazer, 1996). However there has been no study to address the problem of ranking spelling suggestions. One can use a stochastic morphological parser to do spelling checking and correction, and present spelling suggestions ranked with the parser output probabilities. We assume that a word is misspelled if the parser fails to return an analysis of the word. Our method for spelling correction is to enumerate all the valid and invalid candidates that resemble the incorrect input word and filter the invalid ones with the morphological parser. To enumerate the alternative </context>
</contexts>
<marker>Solak, Oflazer, 1993</marker>
<rawString>Ays¸in Solak and Kemal Oflazer. 1993. Design and implementation of a spelling checker for turkish. Literary and Linguistic Computing, 8(3):113–130.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>