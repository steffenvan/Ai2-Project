<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998728">
Inducing Probabilistic CCG Grammars from Logical Form
with Higher-Order Unification
</title>
<author confidence="0.994987">
Tom Kwiatkowski* Luke Zettlemoyer† Sharon Goldwater* Mark Steedman*
</author>
<email confidence="0.781277">
t.m.kwiatkowksi@sms.ed.ac.uk lsz@cs.washington.edu sgwater@inf.ed.ac.uk steedman@inf.ed.ac.uk
</email>
<affiliation confidence="0.991037">
*School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.760945">
Edinburgh, EH8 9AB, UK
</address>
<sectionHeader confidence="0.970763" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999573761904762">
This paper addresses the problem of learn-
ing to map sentences to logical form, given
training data consisting of natural language
sentences paired with logical representations
of their meaning. Previous approaches have
been designed for particular natural languages
or specific meaning representations; here we
present a more general method. The approach
induces a probabilistic CCG grammar that
represents the meaning of individual words
and defines how these meanings can be com-
bined to analyze complete sentences. We
use higher-order unification to define a hy-
pothesis space containing all grammars con-
sistent with the training data, and develop
an online learning algorithm that efficiently
searches this space while simultaneously es-
timating the parameters of a log-linear parsing
model. Experiments demonstrate high accu-
racy on benchmark data sets in four languages
with two different meaning representations.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99816">
A key aim in natural language processing is to learn
a mapping from natural language sentences to for-
mal representations of their meaning. Recent work
has addressed this problem by learning semantic
parsers given sentences paired with logical meaning
representations (Thompson &amp; Mooney, 2002; Kate
et al., 2005; Kate &amp; Mooney, 2006; Wong &amp;
Mooney, 2006, 2007; Zettlemoyer &amp; Collins, 2005,
2007; Lu et al., 2008). For example, the training
data might consist of English sentences paired with
lambda-calculus meaning representations:
</bodyText>
<affiliation confidence="0.702478">
†Computer Science &amp; Engineering
University of Washington
Seattle, WA 98195
</affiliation>
<note confidence="0.3178875">
Sentence: which states border texas
Meaning: Ax.state(x) n next to(x, tex)
</note>
<bodyText confidence="0.994178266666667">
Given pairs like this, the goal is to learn to map new,
unseen, sentences to their corresponding meaning.
Previous approaches to this problem have been
tailored to specific natural languages, specific mean-
ing representations, or both. Here, we develop an
approach that can learn to map any natural language
to a wide variety of logical representations of lin-
guistic meaning. In addition to data like the above,
this approach can also learn from examples such as:
Sentence: hangi eyaletin texas ye siniri vardir
Meaning: answer(state(borders(tex)))
where the sentence is in Turkish and the meaning
representation is a variable-free logical expression
of the type that has been used in recent work (Kate
et al., 2005; Kate &amp; Mooney, 2006; Wong &amp;
Mooney, 2006; Lu et al., 2008).
The reason for generalizing to multiple languages
is obvious. The need to learn over multiple repre-
sentations arises from the fact that there is no stan-
dard representation for logical form for natural lan-
guage. Instead, existing representations are ad hoc,
tailored to the application of interest. For example,
the variable-free representation above was designed
for building natural language interfaces to databases.
Our approach works by inducing a combinatory
categorial grammar (CCG) (Steedman, 1996, 2000).
A CCG grammar consists of a language-specific
lexicon, whose entries pair individual words and
phrases with both syntactic and semantic informa-
tion, and a universal set of combinatory rules that
</bodyText>
<page confidence="0.727166">
1223
</page>
<note confidence="0.814312">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1223–1233,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999819139534884">
project that lexicon onto the sentences and meanings
of the language via syntactic derivations. The learn-
ing process starts by postulating, for each sentence
in the training data, a single multi-word lexical item
pairing that sentence with its complete logical form.
These entries are iteratively refined with a restricted
higher-order unification procedure (Huet, 1975) that
defines all possible ways to subdivide them, consis-
tent with the requirement that each training sentence
can still be parsed to yield its labeled meaning.
For the data sets we consider, the space of pos-
sible grammars is too large to explicitly enumerate.
The induced grammar is also typically highly am-
biguous, producing a large number of possible anal-
yses for each sentence. Our approach discriminates
between analyses using a log-linear CCG parsing
model, similar to those used in previous work (Clark
&amp; Curran, 2003, 2007), but differing in that the syn-
tactic parses are treated as a hidden variable during
training, following the approach of Zettlemoyer &amp;
Collins (2005, 2007). We present an algorithm that
incrementally learns the parameters of this model
while simultaneously exploring the space of possi-
ble grammars. The model is used to guide the pro-
cess of grammar refinement during training as well
as providing a metric for selecting the best analysis
for each new sentence.
We evaluate the approach on benchmark datasets
from a natural language interface to a database of
US Geography (Zelle &amp; Mooney, 1996). We show
that accurate models can be learned for multiple
languages with both the variable-free and lambda-
calculus meaning representations introduced above.
We also compare performance to previous methods
(Kate &amp; Mooney, 2006; Wong &amp; Mooney, 2006,
2007; Zettlemoyer &amp; Collins, 2005, 2007; Lu et al.,
2008), which are designed with either language- or
representation- specific constraints that limit gener-
alization, as discussed in more detail in Section 6.
Despite being the only approach that is general
enough to run on all of the data sets, our algorithm
achieves similar performance to the others, even out-
performing them in several cases.
</bodyText>
<sectionHeader confidence="0.743124" genericHeader="introduction">
2 Overview of the Approach
</sectionHeader>
<bodyText confidence="0.998175333333333">
The goal of our algorithm is to find a function
f : x —* z that maps sentences x to logical ex-
pressions z. We learn this function by inducing a
probabilistic CCG (PCCG) grammar from a train-
ing set {(xZ, zz)|i = 1... n} containing example
(sentence, logical-form) pairs such as (“New York
borders Vermont”, next to(ny, vt)). The induced
grammar consists of two components which the al-
gorithm must learn:
</bodyText>
<listItem confidence="0.997353">
• A CCG lexicon, A, containing lexical items
that define the space of possible parses y for
an input sentence x. Each parse contains both
syntactic and semantic information, and defines
the output logical form z.
• A parameter vector, 0, that defines a distribu-
</listItem>
<bodyText confidence="0.9372013">
tion over the possible parses y, conditioned on
the sentence x.
We will present the approach in two parts. The
lexical induction process (Section 4) uses a re-
stricted form of higher order unification along with
the CCG combinatory rules to propose new entries
for A. The complete learning algorithm (Section 5)
integrates this lexical induction with a parameter es-
timation scheme that learns 0. Before presenting the
details, we first review necessary background.
</bodyText>
<sectionHeader confidence="0.996031" genericHeader="method">
3 Background
</sectionHeader>
<bodyText confidence="0.999841333333333">
This section provides an introduction to the ways in
which we will use lambda calculus and higher-order
unification to construct meaning representations. It
also reviews the CCG grammar formalism and prob-
abilistic extensions to it, including existing parsing
and parameter estimation techniques.
</bodyText>
<subsectionHeader confidence="0.9955095">
3.1 Lambda Calculus and Higher-Order
Unification
</subsectionHeader>
<bodyText confidence="0.999853909090909">
We assume that sentence meanings are represented
as logical expressions, which we will construct from
the meaning of individual words by using the op-
erations defined in the lambda calculus. We use a
version of the typed lambda calculus (cf. Carpenter
(1997)), in which the basic types include e, for en-
tities; t, for truth values; and i for numbers. There
are also function types of the form (e, t) that are as-
signed to lambda expressions, such as Ax.state(x),
which take entities and return truth values. We
represent the meaning of words and phrases using
</bodyText>
<page confidence="0.989666">
1224
</page>
<bodyText confidence="0.999955535714286">
lambda-calculus expressions that can contain con-
stants, quantifiers, logical connectors, and lambda
abstractions.
The advantage of using the lambda calculus
lies in its generality. The meanings of individ-
ual words and phrases can be arbitrary lambda ex-
pressions, while the final meaning for a sentence
can take different forms. It can be a full lambda-
calculus expression, a variable-free expression such
as answer(state(borders(tex))), or any other log-
ical expression that can be built from the primitive
meanings via function application and composition.
The higher-order unification problem (Huet,
1975) involves finding a substitution for the free
variables in a pair of lambda-calculus expressions
that, when applied, makes the expressions equal
each other. This problem is notoriously complex;
in the unrestricted form (Huet, 1973), it is undecid-
able. In this paper, we will guide the grammar in-
duction process using a restricted version of higher-
order unification that is tractable. For a given ex-
pression h, we will need to find expressions for f
and g such that either h = f(g) or h = Ax.f(g(x)).
This limited form of the unification problem will al-
low us to define the ways to split h into subparts
that can be recombined with CCG parsing opera-
tions, which we will define in the next section, to
reconstruct h.
</bodyText>
<subsectionHeader confidence="0.997832">
3.2 Combinatory Categorial Grammar
</subsectionHeader>
<bodyText confidence="0.9993324">
CCG (Steedman, 2000) is a linguistic formalism
that tightly couples syntax and semantics, and
can be used to model a wide range of language
phenomena. For present purposes a CCG grammar
includes a lexicon A with entries like the following:
</bodyText>
<equation confidence="0.830129666666667">
New York �- NP : ny
borders �- S\NP/NP : AxAy.next to(y, x)
Vermont �- NP : vt
</equation>
<bodyText confidence="0.999600375">
where each lexical item w �- X : h has words w, a
syntactic category X, and a logical form h expressed
as a lambda-calculus expression. For the first exam-
ple, these are “New York,” NP, and ny. CCG syn-
tactic categories may be atomic (such as S, NP) or
complex (such as S\NP/NP).
CCG combines categories using a set of com-
binatory rules. For example, the forward (&gt;) and
</bodyText>
<equation confidence="0.993739333333333">
backward (&lt;) application rules are:
X/Y : f Y : g X : f(g) (&gt;)
Y : g X\Y : f X : f(g) (&lt;)
</equation>
<bodyText confidence="0.9920815">
These rules apply to build syntactic and semantic
derivations under the control of the word order infor-
mation encoded in the slash directions of the lexical
entries. For example, given the lexicon above, the
sentence New York borders Vermont can be parsed
to produce:
</bodyText>
<figure confidence="0.430902571428571">
New York borders Vermont
NP (S\NP)/NP NP
ny AxAy.next to(y, x) vt
(S\NP)
Ay.next to(y, vt)
S
next to(ny, vt)
</figure>
<bodyText confidence="0.906452">
where each step in the parse is labeled with the com-
binatory rule (− &gt; or − &lt;) that was used.
CCG also includes combinatory rules of forward
(&gt; B) and backward (&lt; B) composition:
</bodyText>
<equation confidence="0.8869645">
X/Y : f Y/Z : g X/Z : Ax.f(g(x)) (&gt; B)
Y \Z : g X\Y : f X\Z : Ax.f(g(x)) (&lt; B)
</equation>
<bodyText confidence="0.999929076923077">
These rules provide for a relaxed notion of con-
stituency which will be useful during learning as we
reason about possible refinements of the grammar.
We also allow vertical slashes in CCG categories,
which act as wild cards. For example, with this
extension the forward application combinator (&gt;)
could be used to combine the category S/(S�NP)
with any of S\NP, S/NP, or SNP. Figure 1
shows two parses where the composition combina-
tors and vertical slashes are used. These parses
closely resemble the types of analyses that will be
possible under the grammars we learn in the experi-
ments described in Section 8.
</bodyText>
<subsectionHeader confidence="0.998997">
3.3 Probabilistic CCGs
</subsectionHeader>
<bodyText confidence="0.999927833333333">
Given a CCG lexicon A, there will, in general, be
many possible parses for each sentence. We select
the most likely alternative using a log-linear model,
which consists of a feature vector 0 and a parame-
ter vector 0. The joint probability of a logical form
z constructed with a parse y, given a sentence x is
</bodyText>
<page confidence="0.901206">
1225
</page>
<figure confidence="0.941825565217391">
hangi eyaletin texas ye siniri vardir
ax.answer(x) ax.state(x) tex ax.border(x)
S/NP NP/NP NP NP\NP
&lt;
NP
border(tex)
what states border texas
S/(S|NP ) S|NP/(S|NP ) S\NP/NP NP
afax.f(x) afax.state(x)nf(x) ayax.next to(x, y) tex
&gt;s
&gt;
S|NP
ax.state(x) n next to(x, tex)
&gt;
S|NP/NP
ayax.state(x) n next to(x, y)
S
ax.state(x) n next to(x, tex)
&gt;
NP
state(border(tex))
S &gt;
answer(state(border(tex)))
</figure>
<figureCaption confidence="0.999902">
Figure 1: Two examples of CCG parses with different logical form representations.
</figureCaption>
<equation confidence="0.89582925">
defined as:
eθ*x,y,z)
P(y, z|x; 0, A) =(1)
F-(y, z,) eθ&apos;φ(x,y,,z,)
</equation>
<bodyText confidence="0.999143444444444">
Section 7 defines the features used in the experi-
ments, which include, for example, lexical features
that indicate when specific lexical items in A are
used in the parse y. For parsing and parameter es-
timation, we use standard algorithms (Clark &amp; Cur-
ran, 2007), as described below.
The parsing, or inference, problem is to find the
most likely logical form z given a sentence x, as-
suming the parameters 0 and lexicon A are known:
</bodyText>
<equation confidence="0.866927">
p(z|x; 0, A) (2)
</equation>
<bodyText confidence="0.999805">
where the probability of the logical form is found by
summing over all parses that produce it:
</bodyText>
<equation confidence="0.6824255">
p(z|x; 0, A) = � p(y, z|x; 0, A) (3)
y
</equation>
<bodyText confidence="0.999994142857143">
In this approach the distribution over parse trees y
is modeled as a hidden variable. The sum over
parses in Eq. 3 can be calculated efficiently using
the inside-outside algorithm with a CKY-style pars-
ing algorithm.
To estimate the parameters themselves, we
use stochastic gradient updates (LeCun et al.,
1998). Given a set of n sentence-meaning pairs
{(xi, zi) : i = 1...n}, we update the parameters 0 it-
eratively, for each example i, by following the local
gradient of the conditional log-likelihood objective
Oi = log P(zi|xi; 0, A). The local gradient of the
individual parameter 0j associated with feature Oj
and training instance (xi, zi) is given by:
</bodyText>
<equation confidence="0.978605">
aOi = Ep(y|xi,zifθ,A)[Oj(xi, y, zi)1 (4)
−Ep(y,z|xifθ,A)[Oj(xi, y, z)1
</equation>
<bodyText confidence="0.9999785">
As with Eq. 3, all of the expectations in Eq. 4 are
calculated through the use of the inside-outside al-
gorithm on a pruned parse chart. In the experiments,
each chart cell was pruned to the top 200 entries.
</bodyText>
<sectionHeader confidence="0.948058" genericHeader="method">
4 Splitting Lexical Items
</sectionHeader>
<bodyText confidence="0.999960777777778">
Before presenting a complete learning algorithm, we
first describe how to use higher-order unification to
define a procedure for splitting CCG lexical entries.
This splitting process is used to expand the lexicon
during learning. We seed the lexical induction with
a multi-word lexical item xi`S:zi for each training
example (xi, zi), consisting of the entire sentence xi
and its associated meaning representation zi. For ex-
ample, one initial lexical item might be:
</bodyText>
<subsectionHeader confidence="0.692307">
New York borders Vermont `S:next to(ny, vt) (5)
</subsectionHeader>
<bodyText confidence="0.952938736842106">
Although these initial, sentential lexical items
can parse the training data, they will not generalize
well to unseen data. To learn effectively, we will
need to split overly specific entries of this type into
pairs of new, smaller, entries that generalize better.
For example, one possible split of the lexical entry
given in (5) would be the pair:
New York borders ` S/NP : Ax.next to(ny, x),
Vermont ` NP : vt
where we broke the original logical expression into
two new ones Ax.next to(ny, x) and vt, and paired
them with syntactic categories that allow the new
lexical entries to be recombined to produce the orig-
inal analysis. The next three subsections define the
set of possible splits for any given lexical item. The
process is driven by solving a higher-order unifica-
tion problem that defines all of the ways of splitting
the logical expression into two parts, as described in
Section 4.1. Section 4.2 describes how to construct
</bodyText>
<equation confidence="0.863284666666667">
f(x) = arg max
z
a0j
</equation>
<page confidence="0.85835">
1226
</page>
<bodyText confidence="0.999890666666667">
syntactic categories that are consistent with the two
new fragments of logical form and which will allow
the new lexical items to recombine. Finally, Sec-
tion 4.3 defines the full set of lexical entry pairs that
can be created by splitting a lexical entry.
As we will see, this splitting process is overly pro-
lific for any single language and will yield many
lexical items that do not generalize well. For
example, there is nothing in our original lexical
entry above that provides evidence that the split
should pair “Vermont” with the constant vt and not
Ax.next to(ny, x). Section 5 describes how we
estimate the parameters of a probabilistic parsing
model and how this parsing model can be used to
guide the selection of items to add to the lexicon.
</bodyText>
<subsectionHeader confidence="0.995811">
4.1 Restricted Higher-Order Unification
</subsectionHeader>
<bodyText confidence="0.999478238095238">
The set of possible splits for a logical expression
h is defined as the solution to a pair of higher-
order unification problems. We find pairs of logi-
cal expressions (f, g) such that either f(g) = h or
Ax.f(g(x)) = h. Solving these problems creates
new expressions f and g that can be recombined ac-
cording to the CCG combinators, as defined in Sec-
tion 3.2, to produce h.
In the unrestricted case, there can be infinitely
many solution pairs (f, g) for a given expression h.
For example, when h = tex and f = Ax.tex, the
expression g can be anything. Although it would be
simple enough to forbid vacuous variables in f and
g, the number of solutions would still be exponen-
tial in the size of h. For example, when h contains a
conjunction, such as h = Ax.city(x) n major(x) n
in(x, tex), any subset of the expressions in the con-
junction can be assigned to f (or g).
To limit the number of possible splits, we enforce
the following restrictions on the possible higher-
order solutions that will be used during learning:
</bodyText>
<listItem confidence="0.9324414375">
• No Vacuous Variables: Neither g or f can be a
function of the form Ax.e where the expression
e does not contain the variable x. This rules out
functions such as Ax.tex.
• Limited Coordination Extraction: The ex-
pression g cannot contain more than N of the
conjuncts that appear in any coordination in
h. For example, with N = 1 the expression
g = Ax.city(x) n major(x) could not be used
as a solution given the h conjuction above. We
use N = 4 in our experimental evaluation.
• Limited Application: The function f can-
not contain new variables applied to any non-
variable subexpressions from h. For example,
if h = Ax.in(x, tex), the pair f = Aq.q(tex)
and g = AyAx.in(x, y) is forbidden.
</listItem>
<bodyText confidence="0.999934333333333">
Together, these three restrictions guarantee that
the number of splits is, in the worst case, an N-
degree polynomial of the number of constants in h.
The constraints were designed to increase the effi-
ciency of the splitting algorithm without impacting
performance on the development data.
</bodyText>
<subsectionHeader confidence="0.989145">
4.2 Splitting Categories
</subsectionHeader>
<bodyText confidence="0.99939675">
We define the set of possible splits for a category
X:h with syntax X and logical form h by enumer-
ating the solution pairs (f, g) to the higher-order
unification problems defined above and creating
syntactic categories for the resulting expressions.
For example, given X :h = S\NP :Ax.in(x, tex),
f = AyAx.in(x, y), and g = tex, we would
produce the following two pairs of new categories:
</bodyText>
<construct confidence="0.5162745">
( S\NP/NP:AyAx.in(x, y) , NP:tex )
( NP:tex , S\NP\NP:AyAx.in(x, y) )
</construct>
<bodyText confidence="0.9996235">
which were constructed by first choosing the syntac-
tic category for g, in this case NP, and then enumer-
ating the possible directions for the new slash in the
category containing f. We consider each of these
two steps in more detail below.
The new syntactic category for g is determined
based on its type, T(g). For example, T(tex) = e
and T(Ax.state(x)) = (e, t). Then, the function
QT) takes an input type T and returns the syntactic
category of T as follows:
</bodyText>
<equation confidence="0.99801">
{ NP if T = e
S ifT=t
C(T2)|C(T1) when T = (T1, T2)
</equation>
<bodyText confidence="0.999910142857143">
The basic types e and t are assigned syntactic
categories NP and S, and all functional types
are assigned categories recursively. For exam-
ple Q(e, t)) = S|NP and Q(e, (e, t))) =
S|NP|NP. This definition of CCG categories is
unconventional in that it never assigns atomic cate-
gories to functional types. For example, there is no
</bodyText>
<equation confidence="0.957423">
QT) =
</equation>
<page confidence="0.90531">
1227
</page>
<bodyText confidence="0.9986778">
distinct syntactic category N for nouns (which have
semantic type he, ti). Instead, the more complex cat-
egory S|NP is used.
Now, we are ready to define the set of all category
splits. For a category A = X:h we can define
</bodyText>
<equation confidence="0.992962">
Sc(A) = {FA(A) ∪ BA(A) ∪ FC(A) ∪ BC(A)}
</equation>
<bodyText confidence="0.998376333333333">
which is a union of sets, each of which includes
splits for a single CCG operator. For example,
FA(X:h) is the set of category pairs
</bodyText>
<equation confidence="0.995996">
FA(X:h) = {(X/Y:f,Y:g)  |h=f(g) ∧ Y =C(T(g))}
</equation>
<bodyText confidence="0.999180714285714">
where each pair can be combined with the forward
application combinator, described in Section 3.2, to
reconstruct X:h.
The remaining three sets are defined similarly,
and are associated with the backward application
and forward and backward composition operators,
respectively:
</bodyText>
<equation confidence="0.99985">
BA(X:h) = {(Y :g, X\Y :f)  |h=f(g) ∧ Y =C(T (g))}
FC(X/Y:h) = {(X/W:f,W/Y:g) |
h=Ax.f(g(x)) ∧ W=C(T(g(x)))}
BC(X\Y:h) = {(W\Y :g, X\W:f) |
h=Ax.f(g(x)) ∧ W=C(T(g(x)))}
</equation>
<bodyText confidence="0.999985666666667">
where the composition sets FC and BC only accept
input categories with the appropriate outermost slash
direction, for example FC(X/Y:h).
</bodyText>
<subsectionHeader confidence="0.999669">
4.3 Splitting Lexical Items
</subsectionHeader>
<bodyText confidence="0.9997125">
We can now define the lexical splits that will be used
during learning. For lexical entry w0:n ` A, with
word sequence w0:n = hw0, ... , wni and CCG cat-
egory A, define the set SL of splits to be:
</bodyText>
<equation confidence="0.9988775">
SL(w0:n`A) = {(w0:i`B, wi+1:n`C) |
0 ≤ i &lt; n ∧ (B, C) ∈ SC(A)}
</equation>
<bodyText confidence="0.999914">
where we enumerate all ways of splitting the words
sequence w0:n and aligning the subsequences with
categories in SC(A), as defined in the last section.
</bodyText>
<sectionHeader confidence="0.989495" genericHeader="method">
5 Learning Algorithm
</sectionHeader>
<bodyText confidence="0.999776557692308">
The previous section described how a splitting pro-
cedure can be used to break apart overly specific
lexical items into smaller ones that may generalize
better to unseen data. The space of possible lexi-
cal items supported by this splitting procedure is too
large to explicitly enumerate. Instead, we learn the
parameters of a PCCG, which is used both to guide
the splitting process, and also to select the best parse,
given a learned lexicon.
Figure 2 presents the unification-based learning
algorithm, UBL. This algorithm steps through the
data incrementally and performs two steps for each
training example. First, new lexical items are in-
duced for the training instance by splitting and merg-
ing nodes in the best correct parse, given the current
parameters. Next, the parameters of the PCCG are
updated by making a stochastic gradient update on
the marginal likelihood, given the updated lexicon.
Inputs and Initialization The algorithm takes as
input the training set of n (sentence, logical form)
pairs {(xi, zi) : i = 1...n} along with an NP list,
ANP, of proper noun lexical items such as Texas `
NP:tex. The lexicon, A, is initialized with a single
lexical item xi `S :zi for each of the training pairs
along with the contents of the NP list. It is possible
to run the algorithm without the initial NP list; we
include it to allow direct comparisons with previous
approaches, which also included NP lists. Features
and initial feature weights are described in Section 7.
Step 1: Updating the Lexicon In the lexical up-
date step the algorithm first computes the best cor-
rect parse tree y* for the current training exam-
ple and then uses y* as input to the procedure
NEW-LEX, which determines which (if any) new
lexical items to add to A. NEW-LEX begins by enu-
merating all pairs (C, wi:j), for i &lt; j, where C is a
category occurring at a node in y* and wi:j are the
(two or more) words it spans. For example, in the
left parse in Figure 1, there would be four pairs: one
with the category C = NP\NP:Ax.border(x) and
the phrase wi:j =“ye siniri vardir”, and one for each
non-leaf node in the tree.
For each pair (C, wi:j), NEW-LEX considers in-
troducing a new lexical item wi:j `C, which allows
for the possibility of a parse where the subtree rooted
at C is replaced with this new entry. (If C is a leaf
node, this item will already exist.) NEW-LEX also
considers adding each pair of new lexical items that
is obtained by splitting wi:j`C as described in Sec-
tion 4, thereby considering many different ways of
reanalyzing the node. This process creates a set of
possible new lexicons, where each lexicon expands
</bodyText>
<page confidence="0.982181">
1228
</page>
<bodyText confidence="0.999688828571429">
A in a different way by adding the items from either
a single split or a single merge of a node in y*.
For each potential new lexicon A&apos;, NEW-LEX
computes the probability p(y*|xi, zi; B&apos;, A&apos;) of the
original parse y* under A&apos; and parameters B&apos; that are
the same as B but have weights for the new lexical
items, as described in Section 7. It also finds the
best new parse y&apos; = arg maxy p(y|xi, zi; B&apos;, A&apos;).1
Finally, NEW-LEX selects the A&apos; with the largest
difference in log probability between y&apos; and y*, and
returns the new entries in A&apos;. If y* is the best parse
for every A&apos;, NEW-LEX returns the empty set; the
lexicon will not change.
Step 2: Parameter Updates For each training ex-
ample we update the parameters B using the stochas-
tic gradient updates given by Eq. 4.
Discussion The alternation between refining the
lexicon and updating the parameters drives the learn-
ing process. The initial model assigns a conditional
likelihood of one to each training example (there
is a single lexical item for each sentence xi, and
it contains the labeled logical form zi). Although
the splitting step often decreases the probability of
the data, the new entries it produces are less spe-
cific and should generalize better. Since we initially
assign positive weights to the parameters for new
lexical items, the overall approach prefers splitting;
trees with many lexical items will initially be much
more likely. However, if the learned lexical items
are used in too many incorrect parses, the stochastic
gradient updates will down weight them to the point
where the lexical induction step can merge or re-split
nodes in the trees that contain them. This allows the
approach to correct the lexicon and, hopefully, im-
prove future performance.
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.924253285714286">
Previous work has focused on a variety of different
meaning representations. Several approaches have
been designed for the variable-free logical repre-
sentations shown in examples throughout this pa-
per. For example, Kate &amp; Mooney (2006) present a
method (KRISP) that extends an existing SVM learn-
ing algorithm to recover logical representations. The
1This computation can be performed efficiently by incre-
mentally updating the parse chart used to find y*.
Inputs: Training set {(xi, zi) : i = 1... n} where each
example is a sentence xi paired with a logical form
zi. Set of NP lexical items ANP. Number of iter-
ations T. Learning rate parameter α0 and cooling
rate parameter c.
Definitions: The function NEW-LEX(y) takes a parse
y and returns a set of new lexical items found by
splitting and merging categories in y, as described
in Section 5. The distributions p(y|x, z; B, A) and
p(y, z|x; B, A) are defined by the log-linear model,
as described in Section 3.3.
Initialization:
</bodyText>
<listItem confidence="0.9500695">
• Set A = {xi I- S : zi} for all i = 1 ... n.
• SetA=AUANP
• Initialize B using coocurrence statistics, as de-
scribed in Section 7.
</listItem>
<figure confidence="0.6025695">
Algorithm:
Fort= 1 ... T,i = 1...n :
</figure>
<figureCaption confidence="0.586874">
Step 1: (Update Lexicon)
</figureCaption>
<listItem confidence="0.9885855">
• Let y* = arg maxy p(y|xi, zi; B, A)
• Set A = A U NEW-LEX(y*) and expand the
parameter vector B to contain entries for the
new lexical items, as described in Section 7.
</listItem>
<figure confidence="0.591664285714286">
Step 2: (Update Parameters)
• Let y= ��
1+�xk where k = i + t x n.
• Let O = Ep(y|xt,zt;B,A)[O(xi, y, zi)]
−Ep(y,z|x.;B,A)[O(xi, y, z)]
• Set B = B + yO
Output: Lexicon A and parameters B.
</figure>
<figureCaption confidence="0.99989">
Figure 2: The UBL learning algorithm.
</figureCaption>
<bodyText confidence="0.99770575">
WASP system (Wong &amp; Mooney, 2006) uses statis-
tical machine translation techniques to learn syn-
chronous context free grammars containing both
words and logic. Lu et al. (2008) (Lu08) developed
a generative model that builds a single hybrid tree
of words, syntax and meaning representation. These
algorithms are all language independent but repre-
sentation specific.
Other algorithms have been designed to re-
cover lambda-calculus representations. For exam-
ple, Wong &amp; Mooney (2007) developed a variant
of WASP (A-WASP) specifically designed for this
alternate representation. Zettlemoyer &amp; Collins
(2005, 2007) developed CCG grammar induction
techniques where lexical items are proposed accord-
ing to a set of hand-engineered lexical templates.
</bodyText>
<page confidence="0.989492">
1229
</page>
<bodyText confidence="0.99997045">
Our approach eliminates this need for manual effort.
Another line of work has focused on recover-
ing meaning representations that are not based on
logic. Examples include an early statistical method
for learning to fill slot-value representations (Miller
et al., 1996) and a more recent approach for recover-
ing semantic parse trees (Ge &amp; Mooney, 2006). Ex-
ploring the extent to which these representations are
compatible with the logic-based learning approach
we developed is an important area for future work.
Finally, there is work on using categorial gram-
mars to solve other, related learning problems.
For example, Buszkowski &amp; Penn (1990) describe
a unification-based approach for grammar discov-
ery from bracketed natural language sentences and
Villavicencio (2002) developed an approach for
modeling child language acquisition. Additionally,
Bos et al. (2004) consider the challenging problem
of constructing broad-coverage semantic representa-
tions with CCG, but do not learn the lexicon.
</bodyText>
<sectionHeader confidence="0.991966" genericHeader="method">
7 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99936093220339">
Features We use two types of features in our
model. First, we include a set of lexical features:
For each lexical item L E A, we include a feature
OL that fires when L is used. Second, we include se-
mantic features that are functions of the output logi-
cal expression z. Each time a predicate p in z takes
an argument a with type T (a) in position i it trig-
gers two binary indicator features: O(p,a,i) for the
predicate-argument relation; and O(p,T(a),i) for the
predicate argument-type relation.
Initialization The weights for the semantic fea-
tures are initialized to zero. The weights for the lex-
ical features are initialized according to coocurrance
statistics estimated with the Giza++ (Och &amp; Ney,
2003) implementation of IBM Model 1. We com-
pute translation scores for (word, constant) pairs that
cooccur in examples in the training data. The initial
weight for each OL is set to ten times the average
score over the (word, constant) pairs in L, except for
the weights of seed lexical entries in ANP which are
set to 10 (equivalent to the highest possible coocur-
rence score). We used the learning rate α0 = 1.0
and cooling rate c = 10−5 in all training scenar-
ios, and ran the algorithm for T = 20 iterations.
These values were selected with cross validation on
the Geo880 development set, described below.
Data and Evaluation We evaluate our system
on the GeoQuery datasets, which contain natural-
language queries of a geographical database paired
with logical representations of each query’s mean-
ing. The full Geo880 dataset contains 880 (English-
sentence, logical-form) pairs, which we split into a
development set of 600 pairs and a test set of 280
pairs, following Zettlemoyer &amp; Collins (2005). The
Geo250 dataset is a subset of Geo880 containing
250 sentences that have been translated into Turk-
ish, Spanish and Japanese as well as the original En-
glish. Due to the small size of this dataset we use
10-fold cross validation for evaluation. We use the
same folds as Wong &amp; Mooney (2006, 2007) and Lu
et al. (2008), allowing a direct comparison.
The GeoQuery data is annotated with both
lambda-calculus and variable-free meaning rep-
resentations, which we have seen examples of
throughout the paper. We report results for both rep-
resentations, using the standard measures of Recall
(percentage of test sentences assigned correct log-
ical forms), Precision (percentage of logical forms
returned that are correct) and F1 (the harmonic mean
of Precision and Recall).
Two-Pass Parsing To investigate the trade-off be-
tween precision and recall, we report results with a
two-pass parsing strategy. When the parser fails to
return an analysis for a test sentence due to novel
words or usage, we reparse the sentence and allow
the parser to skip words, with a fixed cost. Skip-
ping words can potentially increase recall, if the ig-
nored word is an unknown function word that does
not contribute semantic content.
</bodyText>
<sectionHeader confidence="0.997369" genericHeader="evaluation">
8 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9992926">
Tables 1, 2, and 3 present the results for all of the ex-
periments. In aggregate, they demonstrate that our
algorithm, UBL, learns accurate models across lan-
guages and for both meaning representations. This
is a new result; no previous system is as general.
We also see the expected tradeoff between preci-
sion and recall that comes from the two-pass parsing
approach, which is labeled UBL-s. With the abil-
ity to skip words, UBL-s achieves the highest recall
of all reported systems for all evaluation conditions.
</bodyText>
<page confidence="0.950646">
1230
</page>
<table confidence="0.998924416666667">
System Rec. English F1 Rec. Spanish F1
Pre. Pre.
WASP 70.0 95.4 80.8 72.4 91.2 81.0
Lu08 72.8 91.5 81.1 79.2 95.2 86.5
UBL 78.1 88.2 82.7 76.8 86.8 81.4
UBL-s 80.4 80.8 80.6 79.7 80.6 80.1
System Rec. Japanese F1 Rec. Turkish F1
Pre. Pre.
WASP 74.4 92.0 82.9 62.4 97.0 75.9
Lu08 76.0 87.6 81.4 66.8 93.8 78.0
UBL 78.5 85.5 81.8 70.4 89.4 78.6
UBL-s 80.5 80.6 80.6 74.2 75.6 74.9
</table>
<tableCaption confidence="0.992729">
Table 1: Performance across languages on Geo250 with
variable-free meaning representations.
</tableCaption>
<table confidence="0.9999462">
System English F1 Rec. Spanish F1
Rec. Pre. Pre.
A-WASP 75.6 91.8 82.9 80.0 92.5 85.8
UBL 78.0 93.2 84.7 75.9 93.4 83.6
UBL-s 81.8 83.5 82.6 81.4 83.4 82.4
System Japanese F1 Rec. Turkish F1
Rec. Pre. Pre.
A-WASP 81.2 90.1 85.8 68.8 90.4 78.1
UBL 78.9 90.9 84.4 67.4 93.4 78.1
UBL-s 83.0 83.2 83.1 71.8 77.8 74.6
</table>
<tableCaption confidence="0.9525285">
Table 2: Performance across languages on Geo250 with
lambda-calculus meaning representations.
</tableCaption>
<bodyText confidence="0.999905782608696">
However, UBL achieves much higher precision and
better overall F1 scores, which are generally compa-
rable to the best performing systems.
The comparison to the CCG induction techniques
of ZC05 and ZC07 (Table 3) is particularly striking.
These approaches used language-specific templates
to propose new lexical items and also required as in-
put a set of hand-engineered lexical entries to model
phenomena such as quantification and determiners.
However, the use of higher-order unification allows
UBL to achieve comparable performance while au-
tomatically inducing these types of entries.
For a more qualitative evaluation, Table 4 shows a
selection of lexical items learned with high weights
for the lambda-calculus meaning representations.
Nouns such as “state” or “estado” are consistently
learned across languages with the category S|NP,
which stands in for the more conventional N. The
algorithm also learns language-specific construc-
tions such as the Japanese case markers “no” and
“wa”, which are treated as modifiers that do not add
semantic content. Language-specific word order is
also encoded, using the slash directions of the CCG
</bodyText>
<table confidence="0.999569583333333">
System Variable Free Lambda Calculus
Rec. Pre. F1 Rec. Pre. F1
Cross Validation Results
KRISP 71.7 93.3 81.1 – – –
WASP 74.8 87.2 80.5 – – –
Lu08 81.5 89.3 85.2 – – –
A-WASP – – – 86.6 92.0 89.2
Independent Test Set
ZC05 – – – 79.3 96.3 87.0
ZC07 – – – 86.1 91.6 88.8
UBL 81.4 89.4 85.2 85.0 94.1 89.3
UBL-s 84.3 85.2 84.7 87.9 88.5 88.2
</table>
<tableCaption confidence="0.998031">
Table 3: Performance on the Geo880 data set, with varied
meaning representations.
</tableCaption>
<bodyText confidence="0.999938866666667">
categories. For example, “what” and “que” take
their arguments to the right in the wh-initial English
and Spanish. However, the Turkish wh-word “nel-
erdir” and the Japanese question marker “nan desu
ka” are sentence final, and therefore take their argu-
ments to the left. Learning regularities of this type
allows UBL to generalize well to unseen data.
There is less variation and complexity in the
learned lexical items for the variable-free represen-
tation. The fact that the meaning representation is
deeply nested influences the form of the induced
grammar. For example, recall that the sentence
“what states border texas” would be paired with the
meaning answer(state(borders(tex))). For this
representation, lexical items such as:
</bodyText>
<equation confidence="0.607744">
what ` S/NP : Ax.answer(x)
states ` NP/NP : Ax.state(x)
border ` NP/NP: Ax.borders(x)
texas ` NP : tex
</equation>
<bodyText confidence="0.999315785714286">
can be used to construct the desired output. In
practice, UBL often learns entries with only a sin-
gle slash, like those above, varying only in the di-
rection, as required for the language. Even the
more complex items, such as those for quantifiers,
are consistently simpler than those induced from
the lambda-calculus meaning representations. For
example, one of the most complex entries learned
in the experiments for English is the smallest `
NP\NP/(NP|NP):AfAx.smallest one(f(x)).
There are also differences in the aggregate statis-
tics of the learned lexicons. For example, the aver-
age length of a learned lexical item for the (lambda-
calculus, variable-free) meaning representations is:
</bodyText>
<page confidence="0.966279">
1231
</page>
<bodyText confidence="0.990462684210527">
(1.21,1.08) for Turkish, (1.34,1.19) for English,
(1.43,1.25) for Spanish and (1.63,1.42) for Japanese.
For both meaning representations the model learns
significantly more multiword lexical items for the
somewhat analytic Japanese than the agglutinative
Turkish. There are also variations in the average
number of learned lexical items in the best parses
during the final pass of training: 192 for Japanese,
206 for Spanish, 188 for English and 295 for Turk-
ish. As compared to the other languages, the mor-
pologically rich Turkish requires significantly more
lexical variation to explain the data.
Finally, there are a number of cases where the
UBL algorithm could be improved in future work.
In cases where there are multiple allowable word or-
ders, the UBL algorithm must learn individual en-
tries for each possibility. For example, the following
two categories are often learned with high weight for
the Japanese word “chiisai”:
</bodyText>
<construct confidence="0.6721685">
NP/(S|NP)\(NP|NP):AfAg.argmin(x, g(x), f(x))
NP|(S|NP)/(NP|NP):AfAg.argmin(x, g(x), f(x))
</construct>
<bodyText confidence="0.9999285">
and are treated as distinct entries in the lexicon. Sim-
ilarly, the approach presented here does not model
morphology, and must repeatedly learn the correct
categories for the Turkish words “nehri,” “nehir,”
“nehirler,” and “nehirlerin”, all of which correspond
to the logical form Ax.river(x).
</bodyText>
<sectionHeader confidence="0.99243" genericHeader="conclusions">
9 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999892722222222">
This paper has presented a method for inducing
probabilistic CCGs from sentences paired with log-
ical forms. The approach uses higher-order unifi-
cation to define the space of possible grammars in
a language- and representation-independent manner,
paired with an algorithm that learns a probabilistic
parsing model. We evaluated the approach on four
languages with two meaning representations each,
achieving high accuracy across all scenarios.
For future work, we are interested in exploring
the generality of the approach while extending it to
new understanding problems. One potential limi-
tation is in the constraints we introduced to ensure
the tractability of the higher-order unification proce-
dure. These restrictions will not allow the approach
to induce lexical items that would be used with,
among other things, many of the type-raised combi-
nators commonly employed in CCG grammars. We
</bodyText>
<table confidence="0.999410909090909">
English
population of �- NP/NP : Ax.population(x)
smallest �- NP/(S|NP) : Af.arg min(y, f(y), size(y))
what �- S|NP/(S|NP) : AfAx.f(x)
border �- S|NP/NP : AxAy.next to(y, x)
state �- S|NP : Ax.state(x)
most �- NP/(S|NP)\(S|NP)\(S|NP|NP) :
AfAgAhAx.argmax(y, g(y), count(z, f(z, y) n h(z)))
Japanese
no �- NP|NP/(NP|NP) : AfAx.f(x)
shuu �- S|NP : Ax.state(x)
nan desu ka �- S\NP\(NP|NP) : AfAx.f(x)
wa �- NP|NP\(NP|NP) : AfAx.f(x)
ikutsu �- NP|(S|NP)\(S|NP|(S|NP)) :
AfAg.count(x, f(g(x)))
chiiki �- NP\NP:Ax.area(x)
Turkish
nedir �- S\NP\(NP|NP) : AfAx.f(x)
sehir �- S|NP : Ax.city(x)
nufus yogunlugu �- NP|NP : Ax.density(x)
siniri�- S|NP/NP : AxAy.next to(y, x)
kac tane �- S\NP/(S|NP|NP)\(S|NP) :
AfAgAx.count(y, f(y) n g(y, x))
ya siniri S|NP\NP : AxAy.next to(y, x)
Spanish
en S|NP/NP: AxAy.loc(y, x)
que es la �- S/NP/(NP|NP): AfAx.f(x)
pequena �- NP\(S|NP)\(NP|NP) :
AgAf.arg min(y, f(y), g(y))
estado �- S|NP : Ax.state(x)
mas �- S\(S|NP)/(S|NP)\(NP|NP|(S|NP)) :
AfAgAh.argmax(x, h(x), f(g, x))
mayores S|NP\(S|NP) :AfAx.f(x) n major(x)
</table>
<tableCaption confidence="0.981351">
Table 4: Example learned lexical items for each language
on the Geo250 lambda-calculus data sets.
</tableCaption>
<bodyText confidence="0.999112714285714">
are also interested in developing similar grammar
induction techniques for context-dependent under-
standing problems, such as the one considered by
Zettlemoyer &amp; Collins (2009). Such an approach
would complement ideas for using high-order unifi-
cation to model a wider range of language phenom-
ena, such as VP ellipsis (Dalrymple et al., 1991).
</bodyText>
<sectionHeader confidence="0.996548" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999692125">
We thank the reviewers for useful feedback. This
work was supported by the EU under IST Cog-
nitive Systems grant IP FP6-2004-IST-4-27657
“Paco-Plus” and ERC Advanced Fellowship 249520
“GRAMPLUS” to Steedman. Kwiatkowski was
supported by an EPRSC studentship. Zettlemoyer
was supported by a US NSF International Research
Fellowship.
</bodyText>
<page confidence="0.991925">
1232
</page>
<sectionHeader confidence="0.995871" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999808909090909">
Bos, J., Clark, S., Steedman, M., Curran, J. R., &amp; Hock-
enmaier, J. (2004). Wide-coverage semantic represen-
tations from a CCG parser. In Proceedings of the In-
ternational Conference on Computational Linguistics.
Buszkowski, W. &amp; Penn, G. (1990). Categorial grammars
determined from linguistic data by unification. Studia
Logica, 49, 431–454.
Carpenter, B. (1997). Type-Logical Semantics. The MIT
Press.
Clark, S. &amp; Curran, J. R. (2003). Log-linear models
for wide-coverage CCG parsing. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Clark, S. &amp; Curran, J. R. (2007). Wide-coverage effi-
cient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4), 493–552.
Dalrymple, M., Shieber, S., &amp; Pereira, F. (1991). Ellipsis
and higher-order unification. Linguistics and Philoso-
phy,14, 399–452.
Ge, R. &amp; Mooney, R. J. (2006). Discriminative rerank-
ing for semantic parsing. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions.
Huet, G. (1975). A unification algorithm for typed A-
calculus. Theoretical Computer Science, 1, 27–57.
Huet, G. P. (1973). The undecidability of unification in
third order logic. Information and Control, 22(3), 257–
267.
Kate, R. J. &amp; Mooney, R. J. (2006). Using string-kernels
for learning semantic parsers. In Proceedings of the
44th Annual Meeting of the Association for Computa-
tional Linguistics.
Kate, R. J., Wong, Y. W., &amp; Mooney, R. J. (2005). Learn-
ing to transform natural to formal languages. In Pro-
ceedings of the National Conference on Artificial In-
telligence.
LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998).
Gradient-based learning applied to document recogni-
tion. Proceedings of the IEEE, 86(11), 2278–2324.
Lu, W., Ng, H. T., Lee, W. S., &amp; Zettlemoyer, L. S.
(2008). A generative model for parsing natural lan-
guage to meaning representations. In Proceedings of
The Conference on Empirical Methods in Natural Lan-
guage Processing.
Miller, S., Stallard, D., Bobrow, R. J., &amp; Schwartz, R. L.
(1996). A fully statistical approach to natural language
interfaces. In Proc. of the Association for Computa-
tional Linguistics.
Och, F. J. &amp; Ney, H. (2003). A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1), 19–51.
Steedman, M. (1996). Surface Structure and Interpreta-
tion. The MIT Press.
Steedman, M. (2000). The Syntactic Process. The MIT
Press.
Thompson, C. A. &amp; Mooney, R. J. (2002). Acquiring
word-meaning mappings for natural language inter-
faces. Journal of Artificial Intelligence Research, 18.
Villavicencio, A. (2002). The acquisition of a unification-
based generalised categorial grammar. Ph.D. thesis,
University of Cambridge.
Wong, Y. W. &amp; Mooney, R. (2006). Learning for seman-
tic parsing with statistical machine translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL.
Wong, Y. W. &amp; Mooney, R. (2007). Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the Association for Com-
putational Linguistics.
Zelle, J. M. &amp; Mooney, R. J. (1996). Learning to parse
database queries using inductive logic programming.
In Proceedings of the National Conference on Artifi-
cial Intelligence.
Zettlemoyer, L. S. &amp; Collins, M. (2005). Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the Conference on Uncertainty in Artificial
Intelligence.
Zettlemoyer, L. S. &amp; Collins, M. (2007). Online learning
of relaxed CCG grammars for parsing to logical form.
In Proc. of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning.
Zettlemoyer, L. S. &amp; Collins, M. (2009). Learning
context-dependent mappings from sentences to logical
form. In Proceedings of The Joint Conference of the
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing.
</reference>
<page confidence="0.97368">
1233
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902681">
<title confidence="0.996001">Inducing Probabilistic CCG Grammars from Logical with Higher-Order Unification</title>
<author confidence="0.914386">t m kwiatkowksisms ed ac uk lszcs washington edu sgwaterinf ed ac uk steedmaninf ed ac uk</author>
<affiliation confidence="0.9979205">of Informatics University of Edinburgh</affiliation>
<address confidence="0.998377">Edinburgh, EH8 9AB, UK</address>
<abstract confidence="0.999723136363636">This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>S Clark</author>
<author>M Steedman</author>
<author>J R Curran</author>
<author>J Hockenmaier</author>
</authors>
<title>Wide-coverage semantic representations from a CCG parser.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="28270" citStr="Bos et al. (2004)" startWordPosition="4751" endWordPosition="4754">(Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge &amp; Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski &amp; Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon. 7 Experimental Setup Features We use two types of features in our model. First, we include a set of lexical features: For each lexical item L E A, we include a feature OL that fires when L is used. Second, we include semantic features that are functions of the output logical expression z. Each time a predicate p in z takes an argument a with type T (a) in position i it triggers two binary indicator features: O(p,a,i) for the predicate-argument relation; and O(p,T(a)</context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>Bos, J., Clark, S., Steedman, M., Curran, J. R., &amp; Hockenmaier, J. (2004). Wide-coverage semantic representations from a CCG parser. In Proceedings of the International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Buszkowski</author>
<author>G Penn</author>
</authors>
<title>Categorial grammars determined from linguistic data by unification.</title>
<date>1990</date>
<journal>Studia Logica,</journal>
<volume>49</volume>
<pages>431--454</pages>
<contexts>
<context position="28048" citStr="Buszkowski &amp; Penn (1990)" startWordPosition="4722" endWordPosition="4725">inates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge &amp; Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski &amp; Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon. 7 Experimental Setup Features We use two types of features in our model. First, we include a set of lexical features: For each lexical item L E A, we include a feature OL that fires when L is used. Second, we include semantic features that are func</context>
</contexts>
<marker>Buszkowski, Penn, 1990</marker>
<rawString>Buszkowski, W. &amp; Penn, G. (1990). Categorial grammars determined from linguistic data by unification. Studia Logica, 49, 431–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
</authors>
<title>Type-Logical Semantics.</title>
<date>1997</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="7534" citStr="Carpenter (1997)" startWordPosition="1153" endWordPosition="1154"> 3 Background This section provides an introduction to the ways in which we will use lambda calculus and higher-order unification to construct meaning representations. It also reviews the CCG grammar formalism and probabilistic extensions to it, including existing parsing and parameter estimation techniques. 3.1 Lambda Calculus and Higher-Order Unification We assume that sentence meanings are represented as logical expressions, which we will construct from the meaning of individual words by using the operations defined in the lambda calculus. We use a version of the typed lambda calculus (cf. Carpenter (1997)), in which the basic types include e, for entities; t, for truth values; and i for numbers. There are also function types of the form (e, t) that are assigned to lambda expressions, such as Ax.state(x), which take entities and return truth values. We represent the meaning of words and phrases using 1224 lambda-calculus expressions that can contain constants, quantifiers, logical connectors, and lambda abstractions. The advantage of using the lambda calculus lies in its generality. The meanings of individual words and phrases can be arbitrary lambda expressions, while the final meaning for a s</context>
</contexts>
<marker>Carpenter, 1997</marker>
<rawString>Carpenter, B. (1997). Type-Logical Semantics. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>Log-linear models for wide-coverage CCG parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4518" citStr="Clark &amp; Curran, 2003" startWordPosition="664" endWordPosition="667">ntries are iteratively refined with a restricted higher-order unification procedure (Huet, 1975) that defines all possible ways to subdivide them, consistent with the requirement that each training sentence can still be parsed to yield its labeled meaning. For the data sets we consider, the space of possible grammars is too large to explicitly enumerate. The induced grammar is also typically highly ambiguous, producing a large number of possible analyses for each sentence. Our approach discriminates between analyses using a log-linear CCG parsing model, similar to those used in previous work (Clark &amp; Curran, 2003, 2007), but differing in that the syntactic parses are treated as a hidden variable during training, following the approach of Zettlemoyer &amp; Collins (2005, 2007). We present an algorithm that incrementally learns the parameters of this model while simultaneously exploring the space of possible grammars. The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence. We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography (Zelle &amp; Mooney, 1996</context>
</contexts>
<marker>Clark, Curran, 2003</marker>
<rawString>Clark, S. &amp; Curran, J. R. (2003). Log-linear models for wide-coverage CCG parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<pages>493--552</pages>
<contexts>
<context position="12361" citStr="Clark &amp; Curran, 2007" startWordPosition="1990" endWordPosition="1994">x) afax.state(x)nf(x) ayax.next to(x, y) tex &gt;s &gt; S|NP ax.state(x) n next to(x, tex) &gt; S|NP/NP ayax.state(x) n next to(x, y) S ax.state(x) n next to(x, tex) &gt; NP state(border(tex)) S &gt; answer(state(border(tex))) Figure 1: Two examples of CCG parses with different logical form representations. defined as: eθ*x,y,z) P(y, z|x; 0, A) =(1) F-(y, z,) eθ&apos;φ(x,y,,z,) Section 7 defines the features used in the experiments, which include, for example, lexical features that indicate when specific lexical items in A are used in the parse y. For parsing and parameter estimation, we use standard algorithms (Clark &amp; Curran, 2007), as described below. The parsing, or inference, problem is to find the most likely logical form z given a sentence x, assuming the parameters 0 and lexicon A are known: p(z|x; 0, A) (2) where the probability of the logical form is found by summing over all parses that produce it: p(z|x; 0, A) = � p(y, z|x; 0, A) (3) y In this approach the distribution over parse trees y is modeled as a hidden variable. The sum over parses in Eq. 3 can be calculated efficiently using the inside-outside algorithm with a CKY-style parsing algorithm. To estimate the parameters themselves, we use stochastic gradie</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Clark, S. &amp; Curran, J. R. (2007). Wide-coverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4), 493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dalrymple</author>
<author>S Shieber</author>
<author>F Pereira</author>
</authors>
<date>1991</date>
<booktitle>Ellipsis and higher-order unification. Linguistics and Philosophy,14,</booktitle>
<pages>399--452</pages>
<marker>Dalrymple, Shieber, Pereira, 1991</marker>
<rawString>Dalrymple, M., Shieber, S., &amp; Pereira, F. (1991). Ellipsis and higher-order unification. Linguistics and Philosophy,14, 399–452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ge</author>
<author>R J Mooney</author>
</authors>
<title>Discriminative reranking for semantic parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions.</booktitle>
<contexts>
<context position="27757" citStr="Ge &amp; Mooney, 2006" startWordPosition="4677" endWordPosition="4680"> developed a variant of WASP (A-WASP) specifically designed for this alternate representation. Zettlemoyer &amp; Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. 1229 Our approach eliminates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge &amp; Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski &amp; Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) consider the challenging problem of constructing broad-coverage semantic representatio</context>
</contexts>
<marker>Ge, Mooney, 2006</marker>
<rawString>Ge, R. &amp; Mooney, R. J. (2006). Discriminative reranking for semantic parsing. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Huet</author>
</authors>
<title>A unification algorithm for typed Acalculus.</title>
<date>1975</date>
<journal>Theoretical Computer Science,</journal>
<volume>1</volume>
<pages>27--57</pages>
<contexts>
<context position="3994" citStr="Huet, 1975" startWordPosition="580" endWordPosition="581">iversal set of combinatory rules that 1223 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1223–1233, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics project that lexicon onto the sentences and meanings of the language via syntactic derivations. The learning process starts by postulating, for each sentence in the training data, a single multi-word lexical item pairing that sentence with its complete logical form. These entries are iteratively refined with a restricted higher-order unification procedure (Huet, 1975) that defines all possible ways to subdivide them, consistent with the requirement that each training sentence can still be parsed to yield its labeled meaning. For the data sets we consider, the space of possible grammars is too large to explicitly enumerate. The induced grammar is also typically highly ambiguous, producing a large number of possible analyses for each sentence. Our approach discriminates between analyses using a log-linear CCG parsing model, similar to those used in previous work (Clark &amp; Curran, 2003, 2007), but differing in that the syntactic parses are treated as a hidden </context>
<context position="8445" citStr="Huet, 1975" startWordPosition="1296" endWordPosition="1297"> 1224 lambda-calculus expressions that can contain constants, quantifiers, logical connectors, and lambda abstractions. The advantage of using the lambda calculus lies in its generality. The meanings of individual words and phrases can be arbitrary lambda expressions, while the final meaning for a sentence can take different forms. It can be a full lambdacalculus expression, a variable-free expression such as answer(state(borders(tex))), or any other logical expression that can be built from the primitive meanings via function application and composition. The higher-order unification problem (Huet, 1975) involves finding a substitution for the free variables in a pair of lambda-calculus expressions that, when applied, makes the expressions equal each other. This problem is notoriously complex; in the unrestricted form (Huet, 1973), it is undecidable. In this paper, we will guide the grammar induction process using a restricted version of higherorder unification that is tractable. For a given expression h, we will need to find expressions for f and g such that either h = f(g) or h = Ax.f(g(x)). This limited form of the unification problem will allow us to define the ways to split h into subpar</context>
</contexts>
<marker>Huet, 1975</marker>
<rawString>Huet, G. (1975). A unification algorithm for typed Acalculus. Theoretical Computer Science, 1, 27–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G P Huet</author>
</authors>
<title>The undecidability of unification in third order logic.</title>
<date>1973</date>
<journal>Information and Control,</journal>
<volume>22</volume>
<issue>3</issue>
<pages>257--267</pages>
<contexts>
<context position="8676" citStr="Huet, 1973" startWordPosition="1330" endWordPosition="1331"> can be arbitrary lambda expressions, while the final meaning for a sentence can take different forms. It can be a full lambdacalculus expression, a variable-free expression such as answer(state(borders(tex))), or any other logical expression that can be built from the primitive meanings via function application and composition. The higher-order unification problem (Huet, 1975) involves finding a substitution for the free variables in a pair of lambda-calculus expressions that, when applied, makes the expressions equal each other. This problem is notoriously complex; in the unrestricted form (Huet, 1973), it is undecidable. In this paper, we will guide the grammar induction process using a restricted version of higherorder unification that is tractable. For a given expression h, we will need to find expressions for f and g such that either h = f(g) or h = Ax.f(g(x)). This limited form of the unification problem will allow us to define the ways to split h into subparts that can be recombined with CCG parsing operations, which we will define in the next section, to reconstruct h. 3.2 Combinatory Categorial Grammar CCG (Steedman, 2000) is a linguistic formalism that tightly couples syntax and se</context>
</contexts>
<marker>Huet, 1973</marker>
<rawString>Huet, G. P. (1973). The undecidability of unification in third order logic. Information and Control, 22(3), 257– 267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>R J Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1587" citStr="Kate &amp; Mooney, 2006" startWordPosition="217" endWordPosition="220">velop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. 1 Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson &amp; Mooney, 2002; Kate et al., 2005; Kate &amp; Mooney, 2006; Wong &amp; Mooney, 2006, 2007; Zettlemoyer &amp; Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: †Computer Science &amp; Engineering University of Washington Seattle, WA 98195 Sentence: which states border texas Meaning: Ax.state(x) n next to(x, tex) Given pairs like this, the goal is to learn to map new, unseen, sentences to their corresponding meaning. Previous approaches to this problem have been tailored to specific natural languages, specific meaning representations, or both. Here, we devel</context>
<context position="5345" citStr="Kate &amp; Mooney, 2006" startWordPosition="795" endWordPosition="798">earns the parameters of this model while simultaneously exploring the space of possible grammars. The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence. We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography (Zelle &amp; Mooney, 1996). We show that accurate models can be learned for multiple languages with both the variable-free and lambdacalculus meaning representations introduced above. We also compare performance to previous methods (Kate &amp; Mooney, 2006; Wong &amp; Mooney, 2006, 2007; Zettlemoyer &amp; Collins, 2005, 2007; Lu et al., 2008), which are designed with either language- or representation- specific constraints that limit generalization, as discussed in more detail in Section 6. Despite being the only approach that is general enough to run on all of the data sets, our algorithm achieves similar performance to the others, even outperforming them in several cases. 2 Overview of the Approach The goal of our algorithm is to find a function f : x —* z that maps sentences x to logical expressions z. We learn this function by inducing a probabilis</context>
<context position="25326" citStr="Kate &amp; Mooney (2006)" startWordPosition="4259" endWordPosition="4262">tems will initially be much more likely. However, if the learned lexical items are used in too many incorrect parses, the stochastic gradient updates will down weight them to the point where the lexical induction step can merge or re-split nodes in the trees that contain them. This allows the approach to correct the lexicon and, hopefully, improve future performance. 6 Related Work Previous work has focused on a variety of different meaning representations. Several approaches have been designed for the variable-free logical representations shown in examples throughout this paper. For example, Kate &amp; Mooney (2006) present a method (KRISP) that extends an existing SVM learning algorithm to recover logical representations. The 1This computation can be performed efficiently by incrementally updating the parse chart used to find y*. Inputs: Training set {(xi, zi) : i = 1... n} where each example is a sentence xi paired with a logical form zi. Set of NP lexical items ANP. Number of iterations T. Learning rate parameter α0 and cooling rate parameter c. Definitions: The function NEW-LEX(y) takes a parse y and returns a set of new lexical items found by splitting and merging categories in y, as described in Se</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Kate, R. J. &amp; Mooney, R. J. (2006). Using string-kernels for learning semantic parsers. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages.</title>
<date>2005</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1566" citStr="Kate et al., 2005" startWordPosition="213" endWordPosition="216">aining data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. 1 Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson &amp; Mooney, 2002; Kate et al., 2005; Kate &amp; Mooney, 2006; Wong &amp; Mooney, 2006, 2007; Zettlemoyer &amp; Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: †Computer Science &amp; Engineering University of Washington Seattle, WA 98195 Sentence: which states border texas Meaning: Ax.state(x) n next to(x, tex) Given pairs like this, the goal is to learn to map new, unseen, sentences to their corresponding meaning. Previous approaches to this problem have been tailored to specific natural languages, specific meaning representations, or</context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>Kate, R. J., Wong, Y. W., &amp; Mooney, R. J. (2005). Learning to transform natural to formal languages. In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y LeCun</author>
<author>L Bottou</author>
<author>Y Bengio</author>
<author>P Haffner</author>
</authors>
<title>Gradient-based learning applied to document recognition.</title>
<date>1998</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>86</volume>
<issue>11</issue>
<pages>2278--2324</pages>
<contexts>
<context position="12992" citStr="LeCun et al., 1998" startWordPosition="2104" endWordPosition="2107">ed below. The parsing, or inference, problem is to find the most likely logical form z given a sentence x, assuming the parameters 0 and lexicon A are known: p(z|x; 0, A) (2) where the probability of the logical form is found by summing over all parses that produce it: p(z|x; 0, A) = � p(y, z|x; 0, A) (3) y In this approach the distribution over parse trees y is modeled as a hidden variable. The sum over parses in Eq. 3 can be calculated efficiently using the inside-outside algorithm with a CKY-style parsing algorithm. To estimate the parameters themselves, we use stochastic gradient updates (LeCun et al., 1998). Given a set of n sentence-meaning pairs {(xi, zi) : i = 1...n}, we update the parameters 0 iteratively, for each example i, by following the local gradient of the conditional log-likelihood objective Oi = log P(zi|xi; 0, A). The local gradient of the individual parameter 0j associated with feature Oj and training instance (xi, zi) is given by: aOi = Ep(y|xi,zifθ,A)[Oj(xi, y, zi)1 (4) −Ep(y,z|xifθ,A)[Oj(xi, y, z)1 As with Eq. 3, all of the expectations in Eq. 4 are calculated through the use of the inside-outside algorithm on a pruned parse chart. In the experiments, each chart cell was prune</context>
</contexts>
<marker>LeCun, Bottou, Bengio, Haffner, 1998</marker>
<rawString>LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lu</author>
<author>H T Ng</author>
<author>W S Lee</author>
<author>L S Zettlemoyer</author>
</authors>
<title>A generative model for parsing natural language to meaning representations.</title>
<date>2008</date>
<booktitle>In Proceedings of The Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1667" citStr="Lu et al., 2008" startWordPosition="231" endWordPosition="234">taneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. 1 Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson &amp; Mooney, 2002; Kate et al., 2005; Kate &amp; Mooney, 2006; Wong &amp; Mooney, 2006, 2007; Zettlemoyer &amp; Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: †Computer Science &amp; Engineering University of Washington Seattle, WA 98195 Sentence: which states border texas Meaning: Ax.state(x) n next to(x, tex) Given pairs like this, the goal is to learn to map new, unseen, sentences to their corresponding meaning. Previous approaches to this problem have been tailored to specific natural languages, specific meaning representations, or both. Here, we develop an approach that can learn to map any natural language to a wide variety of l</context>
<context position="5425" citStr="Lu et al., 2008" startWordPosition="809" endWordPosition="812">ible grammars. The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence. We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography (Zelle &amp; Mooney, 1996). We show that accurate models can be learned for multiple languages with both the variable-free and lambdacalculus meaning representations introduced above. We also compare performance to previous methods (Kate &amp; Mooney, 2006; Wong &amp; Mooney, 2006, 2007; Zettlemoyer &amp; Collins, 2005, 2007; Lu et al., 2008), which are designed with either language- or representation- specific constraints that limit generalization, as discussed in more detail in Section 6. Despite being the only approach that is general enough to run on all of the data sets, our algorithm achieves similar performance to the others, even outperforming them in several cases. 2 Overview of the Approach The goal of our algorithm is to find a function f : x —* z that maps sentences x to logical expressions z. We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xZ, zz)|i = 1... n} containing exam</context>
<context position="26836" citStr="Lu et al. (2008)" startWordPosition="4540" endWordPosition="4543"> ... T,i = 1...n : Step 1: (Update Lexicon) • Let y* = arg maxy p(y|xi, zi; B, A) • Set A = A U NEW-LEX(y*) and expand the parameter vector B to contain entries for the new lexical items, as described in Section 7. Step 2: (Update Parameters) • Let y= �� 1+�xk where k = i + t x n. • Let O = Ep(y|xt,zt;B,A)[O(xi, y, zi)] −Ep(y,z|x.;B,A)[O(xi, y, z)] • Set B = B + yO Output: Lexicon A and parameters B. Figure 2: The UBL learning algorithm. WASP system (Wong &amp; Mooney, 2006) uses statistical machine translation techniques to learn synchronous context free grammars containing both words and logic. Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation. These algorithms are all language independent but representation specific. Other algorithms have been designed to recover lambda-calculus representations. For example, Wong &amp; Mooney (2007) developed a variant of WASP (A-WASP) specifically designed for this alternate representation. Zettlemoyer &amp; Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. 1229 Our approach eliminates this </context>
<context position="30438" citStr="Lu et al. (2008)" startWordPosition="5122" endWordPosition="5125">guage queries of a geographical database paired with logical representations of each query’s meaning. The full Geo880 dataset contains 880 (Englishsentence, logical-form) pairs, which we split into a development set of 600 pairs and a test set of 280 pairs, following Zettlemoyer &amp; Collins (2005). The Geo250 dataset is a subset of Geo880 containing 250 sentences that have been translated into Turkish, Spanish and Japanese as well as the original English. Due to the small size of this dataset we use 10-fold cross validation for evaluation. We use the same folds as Wong &amp; Mooney (2006, 2007) and Lu et al. (2008), allowing a direct comparison. The GeoQuery data is annotated with both lambda-calculus and variable-free meaning representations, which we have seen examples of throughout the paper. We report results for both representations, using the standard measures of Recall (percentage of test sentences assigned correct logical forms), Precision (percentage of logical forms returned that are correct) and F1 (the harmonic mean of Precision and Recall). Two-Pass Parsing To investigate the trade-off between precision and recall, we report results with a two-pass parsing strategy. When the parser fails to</context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Lu, W., Ng, H. T., Lee, W. S., &amp; Zettlemoyer, L. S. (2008). A generative model for parsing natural language to meaning representations. In Proceedings of The Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>D Stallard</author>
<author>R J Bobrow</author>
<author>R L Schwartz</author>
</authors>
<title>A fully statistical approach to natural language interfaces.</title>
<date>1996</date>
<booktitle>In Proc. of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27674" citStr="Miller et al., 1996" startWordPosition="4662" endWordPosition="4665">esigned to recover lambda-calculus representations. For example, Wong &amp; Mooney (2007) developed a variant of WASP (A-WASP) specifically designed for this alternate representation. Zettlemoyer &amp; Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. 1229 Our approach eliminates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge &amp; Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski &amp; Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) con</context>
</contexts>
<marker>Miller, Stallard, Bobrow, Schwartz, 1996</marker>
<rawString>Miller, S., Stallard, D., Bobrow, R. J., &amp; Schwartz, R. L. (1996). A fully statistical approach to natural language interfaces. In Proc. of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="29126" citStr="Och &amp; Ney, 2003" startWordPosition="4894" endWordPosition="4897">atures: For each lexical item L E A, we include a feature OL that fires when L is used. Second, we include semantic features that are functions of the output logical expression z. Each time a predicate p in z takes an argument a with type T (a) in position i it triggers two binary indicator features: O(p,a,i) for the predicate-argument relation; and O(p,T(a),i) for the predicate argument-type relation. Initialization The weights for the semantic features are initialized to zero. The weights for the lexical features are initialized according to coocurrance statistics estimated with the Giza++ (Och &amp; Ney, 2003) implementation of IBM Model 1. We compute translation scores for (word, constant) pairs that cooccur in examples in the training data. The initial weight for each OL is set to ten times the average score over the (word, constant) pairs in L, except for the weights of seed lexical entries in ANP which are set to 10 (equivalent to the highest possible coocurrence score). We used the learning rate α0 = 1.0 and cooling rate c = 10−5 in all training scenarios, and ran the algorithm for T = 20 iterations. These values were selected with cross validation on the Geo880 development set, described belo</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, F. J. &amp; Ney, H. (2003). A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1), 19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Surface Structure and Interpretation.</title>
<date>1996</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="3217" citStr="Steedman, 1996" startWordPosition="469" endWordPosition="470">t has been used in recent work (Kate et al., 2005; Kate &amp; Mooney, 2006; Wong &amp; Mooney, 2006; Lu et al., 2008). The reason for generalizing to multiple languages is obvious. The need to learn over multiple representations arises from the fact that there is no standard representation for logical form for natural language. Instead, existing representations are ad hoc, tailored to the application of interest. For example, the variable-free representation above was designed for building natural language interfaces to databases. Our approach works by inducing a combinatory categorial grammar (CCG) (Steedman, 1996, 2000). A CCG grammar consists of a language-specific lexicon, whose entries pair individual words and phrases with both syntactic and semantic information, and a universal set of combinatory rules that 1223 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1223–1233, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics project that lexicon onto the sentences and meanings of the language via syntactic derivations. The learning process starts by postulating, for each sentence in the training data, a single mult</context>
</contexts>
<marker>Steedman, 1996</marker>
<rawString>Steedman, M. (1996). Surface Structure and Interpretation. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="9215" citStr="Steedman, 2000" startWordPosition="1429" endWordPosition="1430">. This problem is notoriously complex; in the unrestricted form (Huet, 1973), it is undecidable. In this paper, we will guide the grammar induction process using a restricted version of higherorder unification that is tractable. For a given expression h, we will need to find expressions for f and g such that either h = f(g) or h = Ax.f(g(x)). This limited form of the unification problem will allow us to define the ways to split h into subparts that can be recombined with CCG parsing operations, which we will define in the next section, to reconstruct h. 3.2 Combinatory Categorial Grammar CCG (Steedman, 2000) is a linguistic formalism that tightly couples syntax and semantics, and can be used to model a wide range of language phenomena. For present purposes a CCG grammar includes a lexicon A with entries like the following: New York �- NP : ny borders �- S\NP/NP : AxAy.next to(y, x) Vermont �- NP : vt where each lexical item w �- X : h has words w, a syntactic category X, and a logical form h expressed as a lambda-calculus expression. For the first example, these are “New York,” NP, and ny. CCG syntactic categories may be atomic (such as S, NP) or complex (such as S\NP/NP). CCG combines categories</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Steedman, M. (2000). The Syntactic Process. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C A Thompson</author>
<author>R J Mooney</author>
</authors>
<title>Acquiring word-meaning mappings for natural language interfaces.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>18</volume>
<contexts>
<context position="1547" citStr="Thompson &amp; Mooney, 2002" startWordPosition="209" endWordPosition="212">rs consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. 1 Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson &amp; Mooney, 2002; Kate et al., 2005; Kate &amp; Mooney, 2006; Wong &amp; Mooney, 2006, 2007; Zettlemoyer &amp; Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: †Computer Science &amp; Engineering University of Washington Seattle, WA 98195 Sentence: which states border texas Meaning: Ax.state(x) n next to(x, tex) Given pairs like this, the goal is to learn to map new, unseen, sentences to their corresponding meaning. Previous approaches to this problem have been tailored to specific natural languages, specific meaning </context>
</contexts>
<marker>Thompson, Mooney, 2002</marker>
<rawString>Thompson, C. A. &amp; Mooney, R. J. (2002). Acquiring word-meaning mappings for natural language interfaces. Journal of Artificial Intelligence Research, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Villavicencio</author>
</authors>
<title>The acquisition of a unificationbased generalised categorial grammar.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="28175" citStr="Villavicencio (2002)" startWordPosition="4740" endWordPosition="4741">gic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge &amp; Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work. Finally, there is work on using categorial grammars to solve other, related learning problems. For example, Buszkowski &amp; Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition. Additionally, Bos et al. (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon. 7 Experimental Setup Features We use two types of features in our model. First, we include a set of lexical features: For each lexical item L E A, we include a feature OL that fires when L is used. Second, we include semantic features that are functions of the output logical expression z. Each time a predicate p in z takes an argument a with type T (a) in position i it tri</context>
</contexts>
<marker>Villavicencio, 2002</marker>
<rawString>Villavicencio, A. (2002). The acquisition of a unificationbased generalised categorial grammar. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL.</booktitle>
<contexts>
<context position="1608" citStr="Wong &amp; Mooney, 2006" startWordPosition="221" endWordPosition="224">ing algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. 1 Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson &amp; Mooney, 2002; Kate et al., 2005; Kate &amp; Mooney, 2006; Wong &amp; Mooney, 2006, 2007; Zettlemoyer &amp; Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: †Computer Science &amp; Engineering University of Washington Seattle, WA 98195 Sentence: which states border texas Meaning: Ax.state(x) n next to(x, tex) Given pairs like this, the goal is to learn to map new, unseen, sentences to their corresponding meaning. Previous approaches to this problem have been tailored to specific natural languages, specific meaning representations, or both. Here, we develop an approach that c</context>
<context position="5366" citStr="Wong &amp; Mooney, 2006" startWordPosition="799" endWordPosition="802">of this model while simultaneously exploring the space of possible grammars. The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence. We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography (Zelle &amp; Mooney, 1996). We show that accurate models can be learned for multiple languages with both the variable-free and lambdacalculus meaning representations introduced above. We also compare performance to previous methods (Kate &amp; Mooney, 2006; Wong &amp; Mooney, 2006, 2007; Zettlemoyer &amp; Collins, 2005, 2007; Lu et al., 2008), which are designed with either language- or representation- specific constraints that limit generalization, as discussed in more detail in Section 6. Despite being the only approach that is general enough to run on all of the data sets, our algorithm achieves similar performance to the others, even outperforming them in several cases. 2 Overview of the Approach The goal of our algorithm is to find a function f : x —* z that maps sentences x to logical expressions z. We learn this function by inducing a probabilistic CCG (PCCG) gramma</context>
<context position="26695" citStr="Wong &amp; Mooney, 2006" startWordPosition="4518" endWordPosition="4521">t A = {xi I- S : zi} for all i = 1 ... n. • SetA=AUANP • Initialize B using coocurrence statistics, as described in Section 7. Algorithm: Fort= 1 ... T,i = 1...n : Step 1: (Update Lexicon) • Let y* = arg maxy p(y|xi, zi; B, A) • Set A = A U NEW-LEX(y*) and expand the parameter vector B to contain entries for the new lexical items, as described in Section 7. Step 2: (Update Parameters) • Let y= �� 1+�xk where k = i + t x n. • Let O = Ep(y|xt,zt;B,A)[O(xi, y, zi)] −Ep(y,z|x.;B,A)[O(xi, y, z)] • Set B = B + yO Output: Lexicon A and parameters B. Figure 2: The UBL learning algorithm. WASP system (Wong &amp; Mooney, 2006) uses statistical machine translation techniques to learn synchronous context free grammars containing both words and logic. Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation. These algorithms are all language independent but representation specific. Other algorithms have been designed to recover lambda-calculus representations. For example, Wong &amp; Mooney (2007) developed a variant of WASP (A-WASP) specifically designed for this alternate representation. Zettlemoyer &amp; Collins (2005, 2007) developed CCG grammar indu</context>
<context position="30410" citStr="Wong &amp; Mooney (2006" startWordPosition="5116" endWordPosition="5119">asets, which contain naturallanguage queries of a geographical database paired with logical representations of each query’s meaning. The full Geo880 dataset contains 880 (Englishsentence, logical-form) pairs, which we split into a development set of 600 pairs and a test set of 280 pairs, following Zettlemoyer &amp; Collins (2005). The Geo250 dataset is a subset of Geo880 containing 250 sentences that have been translated into Turkish, Spanish and Japanese as well as the original English. Due to the small size of this dataset we use 10-fold cross validation for evaluation. We use the same folds as Wong &amp; Mooney (2006, 2007) and Lu et al. (2008), allowing a direct comparison. The GeoQuery data is annotated with both lambda-calculus and variable-free meaning representations, which we have seen examples of throughout the paper. We report results for both representations, using the standard measures of Recall (percentage of test sentences assigned correct logical forms), Precision (percentage of logical forms returned that are correct) and F1 (the harmonic mean of Precision and Recall). Two-Pass Parsing To investigate the trade-off between precision and recall, we report results with a two-pass parsing strate</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Wong, Y. W. &amp; Mooney, R. (2006). Learning for semantic parsing with statistical machine translation. In Proceedings of the Human Language Technology Conference of the NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27139" citStr="Wong &amp; Mooney (2007)" startWordPosition="4584" endWordPosition="4587">,zt;B,A)[O(xi, y, zi)] −Ep(y,z|x.;B,A)[O(xi, y, z)] • Set B = B + yO Output: Lexicon A and parameters B. Figure 2: The UBL learning algorithm. WASP system (Wong &amp; Mooney, 2006) uses statistical machine translation techniques to learn synchronous context free grammars containing both words and logic. Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation. These algorithms are all language independent but representation specific. Other algorithms have been designed to recover lambda-calculus representations. For example, Wong &amp; Mooney (2007) developed a variant of WASP (A-WASP) specifically designed for this alternate representation. Zettlemoyer &amp; Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. 1229 Our approach eliminates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Wong, Y. W. &amp; Mooney, R. (2007). Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="5119" citStr="Zelle &amp; Mooney, 1996" startWordPosition="762" endWordPosition="765">Clark &amp; Curran, 2003, 2007), but differing in that the syntactic parses are treated as a hidden variable during training, following the approach of Zettlemoyer &amp; Collins (2005, 2007). We present an algorithm that incrementally learns the parameters of this model while simultaneously exploring the space of possible grammars. The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence. We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography (Zelle &amp; Mooney, 1996). We show that accurate models can be learned for multiple languages with both the variable-free and lambdacalculus meaning representations introduced above. We also compare performance to previous methods (Kate &amp; Mooney, 2006; Wong &amp; Mooney, 2006, 2007; Zettlemoyer &amp; Collins, 2005, 2007; Lu et al., 2008), which are designed with either language- or representation- specific constraints that limit generalization, as discussed in more detail in Section 6. Despite being the only approach that is general enough to run on all of the data sets, our algorithm achieves similar performance to the other</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>Zelle, J. M. &amp; Mooney, R. J. (1996). Learning to parse database queries using inductive logic programming. In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="1643" citStr="Zettlemoyer &amp; Collins, 2005" startWordPosition="226" endWordPosition="229">tly searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. 1 Introduction A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning. Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson &amp; Mooney, 2002; Kate et al., 2005; Kate &amp; Mooney, 2006; Wong &amp; Mooney, 2006, 2007; Zettlemoyer &amp; Collins, 2005, 2007; Lu et al., 2008). For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: †Computer Science &amp; Engineering University of Washington Seattle, WA 98195 Sentence: which states border texas Meaning: Ax.state(x) n next to(x, tex) Given pairs like this, the goal is to learn to map new, unseen, sentences to their corresponding meaning. Previous approaches to this problem have been tailored to specific natural languages, specific meaning representations, or both. Here, we develop an approach that can learn to map any natural languag</context>
<context position="4673" citStr="Zettlemoyer &amp; Collins (2005" startWordPosition="689" endWordPosition="692">onsistent with the requirement that each training sentence can still be parsed to yield its labeled meaning. For the data sets we consider, the space of possible grammars is too large to explicitly enumerate. The induced grammar is also typically highly ambiguous, producing a large number of possible analyses for each sentence. Our approach discriminates between analyses using a log-linear CCG parsing model, similar to those used in previous work (Clark &amp; Curran, 2003, 2007), but differing in that the syntactic parses are treated as a hidden variable during training, following the approach of Zettlemoyer &amp; Collins (2005, 2007). We present an algorithm that incrementally learns the parameters of this model while simultaneously exploring the space of possible grammars. The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence. We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography (Zelle &amp; Mooney, 1996). We show that accurate models can be learned for multiple languages with both the variable-free and lambdacalculus meaning representations introduced abo</context>
<context position="27261" citStr="Zettlemoyer &amp; Collins (2005" startWordPosition="4600" endWordPosition="4603">The UBL learning algorithm. WASP system (Wong &amp; Mooney, 2006) uses statistical machine translation techniques to learn synchronous context free grammars containing both words and logic. Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation. These algorithms are all language independent but representation specific. Other algorithms have been designed to recover lambda-calculus representations. For example, Wong &amp; Mooney (2007) developed a variant of WASP (A-WASP) specifically designed for this alternate representation. Zettlemoyer &amp; Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates. 1229 Our approach eliminates this need for manual effort. Another line of work has focused on recovering meaning representations that are not based on logic. Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge &amp; Mooney, 2006). Exploring the extent to which these representations are compatible with the logic-based learning appro</context>
<context position="30118" citStr="Zettlemoyer &amp; Collins (2005)" startWordPosition="5063" endWordPosition="5066"> score). We used the learning rate α0 = 1.0 and cooling rate c = 10−5 in all training scenarios, and ran the algorithm for T = 20 iterations. These values were selected with cross validation on the Geo880 development set, described below. Data and Evaluation We evaluate our system on the GeoQuery datasets, which contain naturallanguage queries of a geographical database paired with logical representations of each query’s meaning. The full Geo880 dataset contains 880 (Englishsentence, logical-form) pairs, which we split into a development set of 600 pairs and a test set of 280 pairs, following Zettlemoyer &amp; Collins (2005). The Geo250 dataset is a subset of Geo880 containing 250 sentences that have been translated into Turkish, Spanish and Japanese as well as the original English. Due to the small size of this dataset we use 10-fold cross validation for evaluation. We use the same folds as Wong &amp; Mooney (2006, 2007) and Lu et al. (2008), allowing a direct comparison. The GeoQuery data is annotated with both lambda-calculus and variable-free meaning representations, which we have seen examples of throughout the paper. We report results for both representations, using the standard measures of Recall (percentage o</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Zettlemoyer, L. S. &amp; Collins, M. (2005). Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proc. of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Zettlemoyer, L. S. &amp; Collins, M. (2007). Online learning of relaxed CCG grammars for parsing to logical form. In Proc. of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In Proceedings of The Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing.</booktitle>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Zettlemoyer, L. S. &amp; Collins, M. (2009). Learning context-dependent mappings from sentences to logical form. In Proceedings of The Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>