<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9962335">
Graph-based Semi-Supervised Model for Joint Chinese Word
Segmentation and Part-of-Speech Tagging
</title>
<note confidence="0.6254805">
Xiaodong Zeng† Derek F. Wong† Lidia S. Chao† Isabel Trancoso‡†Department of Computer and Information Science, University of Macau
‡INESC-ID / Instituto Superior T´ecnico, Lisboa, Portugal
</note>
<email confidence="0.6592785">
nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo,
isabel.trancoso@inesc-id.pt
</email>
<sectionHeader confidence="0.985943" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999832142857143">
This paper introduces a graph-based semi-
supervised joint model of Chinese word
segmentation and part-of-speech tagging.
The proposed approach is based on a
graph-based label propagation technique.
One constructs a nearest-neighbor simi-
larity graph over all trigrams of labeled
and unlabeled data for propagating syn-
tactic information, i.e., label distribution-
s. The derived label distributions are re-
garded as virtual evidences to regular-
ize the learning of linear conditional ran-
dom fields (CRFs) on unlabeled data. An
inductive character-based joint model is
obtained eventually. Empirical results on
Chinese tree bank (CTB-7) and Microsoft
Research corpora (MSR) reveal that the
proposed model can yield better result-
s than the supervised baselines and other
competitive semi-supervised CRFs in this
task.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937431034483">
Word segmentation and part-of-speech (POS) tag-
ging are two critical and necessary initial proce-
dures with respect to the majority of high-level
Chinese language processing tasks such as syn-
tax parsing, information extraction and machine
translation. The traditional way of segmentation
and tagging is performed in a pipeline approach,
first segmenting a sentence into words, and then
assigning each word a POS tag. The pipeline ap-
proach is very simple to implement, but frequently
causes error propagation, given that wrong seg-
mentations in the earlier stage harm the subse-
quent POS tagging (Ng and Low, 2004). The join-
t approaches of word segmentation and POS tag-
ging (joint S&amp;T) are proposed to resolve these t-
wo tasks simultaneously. They effectively allevi-
ate the error propagation, because segmentation
and tagging have strong interaction, given that
most segmentation ambiguities cannot be resolved
without considering the surrounding grammatical
constructions encoded in a POS sequence (Qian
and Liu, 2012).
In the past years, several proposed supervised
joint models (Ng and Low, 2004; Zhang and
Clark, 2008; Jiang et al., 2009; Zhang and Clark,
2010) achieved reasonably accurate results, but the
outstanding problem among these models is that
they rely heavily on a large amount of labeled data,
i.e., segmented texts with POS tags. However, the
production of such labeled data is extremely time-
consuming and expensive (Jiao et al., 2006; Jiang
et al., 2009). Therefore, semi-supervised join-
t S&amp;T appears to be a natural solution for easily in-
corporating accessible unlabeled data to improve
the joint S&amp;T model. This study focuses on using
a graph-based label propagation method to build
a semi-supervised joint S&amp;T model. Graph-based
label propagation methods have recently shown
they can outperform the state-of-the-art in sever-
al natural language processing (NLP) tasks, e.g.,
POS tagging (Subramanya et al., 2010), knowl-
edge acquisition (Talukdar et al., 2008), shallow
semantic parsing for unknown predicate (Das and
Smith, 2011). As far as we know, however, these
methods have not yet been applied to resolve
the problem of joint Chinese word segmentation
(CWS) and POS tagging.
Motivated by the works in (Subramanya et al.,
2010; Das and Smith, 2011), for structured prob-
lems, graph-based label propagation can be em-
ployed to infer valuable syntactic information (n-
gram-level label distributions) from labeled data
to unlabeled data. This study extends this intui-
tion to construct a similarity graph for propagating
trigram-level label distributions. The derived label
distributions are regarded as prior knowledge to
regularize the learning of a sequential model, con-
ditional random fields (CRFs) in this case, on both
</bodyText>
<page confidence="0.950193">
770
</page>
<note confidence="0.914032">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 770–779,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99938552631579">
labeled and unlabeled data to achieve the semi-
supervised learning. The approach performs the
incorporation of the derived labeled distributions
by manipulating a “virtual evidence” function as
described in (Li, 2009). Experiments on the da-
ta from the Chinese tree bank (CTB-7) and Mi-
crosoft Research (MSR) show that the proposed
model results in significant improvement over oth-
er comparative candidates in terms of F-score and
out-of-vocabulary (OOV) recall.
This paper is structured as follows: Section
2 points out the main differences with the re-
lated work of this study. Section 3 reviews the
background, including supervised character-based
joint S&amp;T model based on CRFs and graph-based
label propagation. Section 4 presents the details of
the proposed approach. Section 5 reports the ex-
periment results. The conclusion is drawn in Sec-
tion 6.
</bodyText>
<sectionHeader confidence="0.999832" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9997944375">
Prior supervised joint S&amp;T models present ap-
proximate 0.2% - 1.3% improvement in F-score
over supervised pipeline ones. The state-of-the-
art joint models include reranking approaches (Shi
and Wang, 2007), hybrid approaches (Nakagawa
and Uchimoto, 2007; Jiang et al., 2008; Sun,
2011), and single-model approaches (Ng and Low,
2004; Zhang and Clark, 2008; Kruengkrai et al.,
2009; Zhang and Clark, 2010). The proposed ap-
proach in this paper belongs to the single-model
type.
There are few explorations of semi-supervised
approaches for CWS or POS tagging in previ-
ous works. Xu et al. (2008) described a Bayesian
semi-supervised CWS model by considering the
segmentation as the hidden variable in machine
translation. Unlike this model, the proposed ap-
proach is targeted at a general model, instead of
one oriented to machine translation task. Sun and
Xu (2011) enhanced a CWS model by interpolat-
ing statistical features of unlabeled data into the
CRFs model. Wang et al. (2011) proposed a semi-
supervised pipeline S&amp;T model by incorporating
n-gram and lexicon features derived from unla-
beled data. Different from their concern, our em-
phasis is to learn the semi-supervised model by
injecting the label information from a similarity
graph constructed from labeled and unlabeled da-
ta.
The induction method of the proposed approach
also differs from other semi-supervised CRFs al-
gorithms. Jiao et al. (2006), extended by Mann
and McCallum (2007), reported a semi-supervised
CRFs model which aims to guide the learning
by minimizing the conditional entropy of unla-
beled data. The proposed approach regularizes the
CRFs by the graph information. Subramanya et
al. (2010) proposed a graph-based self-train style
semi-supervised CRFs algorithm. In the proposed
approach, an analogous way of graph construction
intuition is applied. But overall, our approach dif-
fers in three important aspects: first, novel feature
templates are defined for measuring the similari-
ty between vertices. Second, the critical property,
i.e., sparsity, is considered among label propaga-
tion. And third, the derived label information from
the graph is smoothed into the model by optimiz-
ing a modified objective function.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="method">
3 Background
</sectionHeader>
<subsectionHeader confidence="0.999611">
3.1 Supervised Character-based Model
</subsectionHeader>
<bodyText confidence="0.998418818181818">
The character-based joint S&amp;T approach is oper-
ated as a sequence labeling fashion that each Chi-
nese character, i.e., hanzi, in the sequence is as-
signed with a tag. To perform segmentation and
tagging simultaneously in a uniform framework,
according to Ng and Low (2004), the tag is com-
posed of a word boundary part, and a POS part,
e.g., “B NN” refers to the first character in a word
with POS tag “NN”. In this paper, 4 word bound-
ary tags are employed: B (beginning of a word),
M (middle part of a word), E (end of a word) and
S (single character). As for the POS tag, we shal-
l use the 33 tags in the Chinese tree bank. Thus,
the potential composite tags of joint S&amp;T consist
of 132 (4x33) classes.
The first-order CRFs model (Lafferty et al.,
2001) has been the most common one in this
task. Given a set of labeled examples Dl =
{(xi, yi)}li=1, where xi = x1ix2 i ...xNi is the se-
quence of characters in the ith sentence, and yi =
y1i y2i ...yNi is the corresponding label sequence.
The goal is to learn a CRFs model in the form,
</bodyText>
<equation confidence="0.991602">
p(yi|xi; A) =
1 exp{
Z(xi; A)
(1)
</equation>
<bodyText confidence="0.94343">
where Z(xi; A) is the partition function that nor-
malizes the exponential form to be a probability
distribution, and fk(yj−1
i , yji , xi, j). In this study,
</bodyText>
<equation confidence="0.947372333333333">
Akfk(yj−1
i , yji , xi, j)}
N
j=1
K
k=1
</equation>
<page confidence="0.979149">
771
</page>
<bodyText confidence="0.999793571428571">
the baseline feature templates of joint S&amp;T are
the ones used in (Ng and Low, 2004; Jiang et al.,
2008), as shown in Table 1. A = {λ1λ2...λK} ∈
RK are the weight parameters to be learned. In su-
pervised training, the aim is to estimate the A that
maximizes the conditional likelihood of the train-
ing data while regularizing model parameters:
</bodyText>
<equation confidence="0.9995455">
L(A) = �l log p(yi|xi; A) − R(A) (2)
i=1
</equation>
<bodyText confidence="0.922293333333334">
R(A) can be any standard regularizer on parame-
ters, e.g., R(A) =k A k /2δ2, to limit overfitting
on rare features and avoid degeneracy in the case
of correlated features. This objective function can
be optimized by the stochastic gradient method or
other numerical optimization methods.
</bodyText>
<figure confidence="0.4970098">
Type Font Size
Unigram Cn(n = −2,−1,0,1,2)
Bigram CnCn+1(n = −2, −1, 0,1)
Date, Digit and T (C−2)T (C−1)T (C0)
Alphabetic Letter T(C1)T(C2)
</figure>
<tableCaption confidence="0.998061">
Table 1: The feature templates of joint S&amp;T.
</tableCaption>
<subsectionHeader confidence="0.999707">
3.2 Graph-based Label Propagation
</subsectionHeader>
<bodyText confidence="0.9998961875">
Graph-based label propagation, a critical subclass
of semi-supervised learning (SSL), has been wide-
ly used and shown to outperform other SSL meth-
ods (Chapelle et al., 2006). Most of these algo-
rithms are transductive in nature, so they cannot
be used to predict an unseen test example in the fu-
ture (Belkin et al., 2006). Typically, graph-based
label propagation algorithms are run in two main
steps: graph construction and label propagation.
The graph construction provides a natural way to
represent data in a variety of target domains. One
constructs a graph whose vertices consist of la-
beled and unlabeled examples. Pairs of vertices
are connected by weighted edges which encode
the degree to which they are expected to have the
same label (Zhu et al., 2003). Popular graph con-
struction methods include k-nearest neighbors (k-
NN) (Bentley, 1980; Beygelzimer et al., 2006),
b-matching (Jebara et al., 2009) and local recon-
struction (Daitch et al., 2009). Label propaga-
tion operates on the constructed graph. The pri-
mary objective is to propagate labels from a few
labeled vertices to the entire graph by optimiz-
ing a loss function based on the constraints or
properties derived from the graph, e.g., smooth-
ness (Zhu et al., 2003; Subramanya et al., 2010;
Talukdar et al., 2008), or sparsity (Das and Smith,
2012). State-of-the-art label propagation algo-
rithms include LP-ZGL (Zhu et al., 2003), Ad-
sorption (Baluja et al., 2008), MAD (Talukdar
and Crammer, 2009) and Sparse Inducing Penal-
ties (Das and Smith, 2012).
</bodyText>
<sectionHeader confidence="0.974375" genericHeader="method">
4 Method
</sectionHeader>
<bodyText confidence="0.934256">
The emphasis of this work is on building a joint
S&amp;T model based on two different kinds of data
sources, labeled and unlabeled data. In essence,
this learning problem can be treated as incorporat-
ing certain gainful information, e.g., prior knowl-
edge or label constraints, of unlabeled data into
the supervised model. The proposed approach em-
ploys a transductive graph-based label propagation
method to acquire such gainful information, i.e.,
label distributions from a similarity graph con-
structed over labeled and unlabeled data. Then,
the derived label distributions are injected as vir-
tual evidences for guiding the learning of CRFs.
Algorithm 1 semi-supervised joint S&amp;T induction
Input:
</bodyText>
<equation confidence="0.879676666666667">
Dl = {(xi, yi)}li=1 labeled sentences
Du = {(xi)}l+u
i=l+1 unlabeled sentences
</equation>
<bodyText confidence="0.7328665">
Output:
A: a set of feature weights
</bodyText>
<listItem confidence="0.977128333333333">
1: Begin
2: {G} = construct graph (Dl, Du)
3: {q0} = init labelDist ({G})
4: {q} = propagate label ({G}, {q0})
5: {A} = train crf (Dl ∪ Du, {q})
6: End
</listItem>
<bodyText confidence="0.952860666666667">
The model induction includes the following
steps (see Algorithm 1): firstly, given labeled
and unlabeled data, i.e., Dl = {(xi, yi)}li=1
with l labeled sentences and Du = {(xi)}l+u
i=l+1
with u unlabeled sentences, a specific similarity
graph G representing Dl and Du is constructed
(construct graph). The vertices (Section 4.1) in
the constructed graph consist of all trigrams that
occur in labeled and unlabeled sentences, and edge
weights between vertices are computed using the
cosine distance between pointwise mutual infor-
mation (PMI) statistics. Afterwards, the estimated
label distributions q0 of vertices in the graph G are
randomly initialized (init labelDist). Subsequently,
</bodyText>
<page confidence="0.984873">
772
</page>
<bodyText confidence="0.999963625">
the label propagation procedure (propagate label)
is conducted for projecting label distributions q
from labeled vertices to the entire graph, using
the algorithm of Sparse-Inducing Penalties (Das
and Smith, 2012) (Section 4.2). The final step
(train crf) of the induction is incorporating the in-
ferred trigram-level label distributions q into CRFs
model (Section 4.3).
</bodyText>
<subsectionHeader confidence="0.994724">
4.1 Graph Construction
</subsectionHeader>
<bodyText confidence="0.999044433333333">
In most graph-based label propagation tasks, the
final effect depends heavily on the quality of
the graph. Graph construction thus plays a cen-
tral role in graph-based label propagation (Zhu et
al., 2003). For character-based joint S&amp;T, unlike
the unstructured learning problem whose vertices
are formed directly by labeled and unlabeled in-
stances, the graph construction is non-trivial. Das
and Petrov (2011) mentioned that taking individu-
al characters as the vertices would result in various
ambiguities, whereas the similarity measurement
is still challenging if vertices corresponding to en-
tire sentences.
This study follows the intuitions of graph con-
struction from Subramanya et al. (2010) in which
vertices are represented by character trigrams oc-
curring in labeled and unlabeled sentences. For-
mally, given a set of labeled sentences Dl, and un-
labeled ones Du, where D °_ {Dl, Du}, the goal is
to form an undirected weighted graph G = (V, E),
where V is defined as the set of vertices which
covers all trigrams extracted from Dl and Du.
Here, V = Vl U Vu, where Vl refers to trigrams
that occurs at least once in labeled sentences and
Vu refers to trigrams that occur only in unlabeled
sentences. The edges E E Vl x Vu, connect all
the vertices. This study makes use of a symmet-
ric k-NN graph (k = 5) and the edge weights are
measured by a symmetric similarity function (E-
quation (3)):
</bodyText>
<equation confidence="0.9519055">
wi,j = � sim(xi, xj) if j E K(i) or i E K(j)
0 otherwise
</equation>
<bodyText confidence="0.978378388888889">
(3)
where K(i) is the set of the k nearest neighbors of
xi(|K(i) = k,bi|) and sim(xi,xj) is a similari-
ty measure between two vertices. The similarity
is computed based on the co-occurrence statistic-
s over the features in Table 2. Most features we
adopted are selected from those of (Subramanya
et al., 2010). Note that a novel feature in the last
row encodes the classes of surrounding character-
s, where four types are defined: number, punctu-
ation, alphabetic letter and other. It is especially
helpful for the graph to make connections with tri-
grams that may not have been seen in labeled data
but have similar label information. The pointwise
mutual information values between the trigram-
s and each feature instantiation that they have in
common are summed to sparse vectors, and their
cosine distances are computed as the similarities.
</bodyText>
<figure confidence="0.870970416666666">
Description Feature
Trigram + Context x1x2x3x4x5
Trigram x2x3x4
Left Context x1x2
Right Context x4x5
Center Word x3
Trigram - Center Word x2x4
Left Word + Right Context x2x4x5
Right Word + Left Context x1x2x3
Type of Trigram: number,
punctuation, alphabetic letter t(x2)t(x3)t(x4)
and other
</figure>
<tableCaption confidence="0.8492185">
Table 2: Features employed to measure the sim-
ilarity between two vertices, in a given tex-
</tableCaption>
<bodyText confidence="0.980324277777778">
t “x1x2x3x4x5”, where the trigram is “x2x3x4”.
The nature of the similarity graph enforces that
the connected trigrams with high weight appearing
in different texts should have similar syntax con-
figurations. Thus, the constructed graph is expect-
ed to provide additional information that cannot
be expressed directly in a sequence model (Subra-
manya et al., 2010). One primary benefit of this
property is on enriching vocabulary coverage. In
other words, the new features of various trigram-
s only occurring in unlabeled data can be discov-
ered. As the excerpt in Figure 1 shows, the trigram
“天津港” (Tianjin port) has no any label informa-
tion, as it only occurs in unlabeled data, but for-
tunately its neighborhoods with similar syntax in-
formation, e.g., “上海港” (Shanghai port), “广州
港” (Guangzhou port), can assist to infer the cor-
rect tag “M NN”.
</bodyText>
<subsectionHeader confidence="0.967228">
4.2 Label Propagation
</subsectionHeader>
<bodyText confidence="0.999859333333333">
In order to induce trigram-level label distributions
from the graph constructed by the previous step,
a label propagation algorithm, Sparsity-Inducing
Penalties, proposed by Das and Smith (2012), is
employed. This algorithm is used because it cap-
tures the property of sparsity that only a few labels
</bodyText>
<page confidence="0.99731">
773
</page>
<figureCaption confidence="0.989893">
Figure 1: An excerpt from the similarity graph
over trigrams on labeled and unlabeled data.
</figureCaption>
<bodyText confidence="0.9999108">
are typically associated with a given instance. In
fact, the sparsity is also a common phenomenon
among character-based CWS and POS tagging.
The following convex objective is optimized on
the similarity graph in this case:
</bodyText>
<equation confidence="0.8422416">
11 qj − rj 112
+µ �l+u wik 11 qi − qk 112 +λ �l+u 11 qi 112
i=1,kEN(i) i=1
s.t. qi &gt; 0,bi E V
(4)
</equation>
<bodyText confidence="0.999955285714286">
where rj denotes empirical label distributions of
labeled vertices, and qi denotes unnormalized es-
timate measures in every vertex. The wik refers to
the similarity between the ith trigram and the kth
trigram, and N(i) is a set of neighbors of the ith
trigram. µ and λ are two hyperparameters whose
values are discussed in Section 5. The squared-
loss criterion1 is used to formulate the objective
function. The first term in Equation (4) is the seed
match loss which penalizes the estimated label dis-
tributions qj, if they go too far away from the em-
pirical labeled distributions rj. The second term
is the edge smoothness loss that requires qi should
be smooth with respect to the graph, such that two
vertices connected by an edge with high weight
should be assigned similar labels. The final term
is a regularizer to incorporate the prior knowledge,
e.g., uniform distributions used in (Talukdar et al.,
2008; Das and Smith, 2011). This study applies
the squared norm of q to encourage sparsity per
vertex. Note that the estimated label distribution
</bodyText>
<footnote confidence="0.9970826">
1It can be seen as a multi-class extension of quadratic cost
criterion (Bengio et al., 2006) or as a variant of the objective
in (Zhu et al., 2003). An entropic distance measure could also
be used, e.g., KL-divergence (Subramanya et al., 2010; Das
and Smith, 2012).
</footnote>
<bodyText confidence="0.999660125">
qi in Equation (4) is relaxed to be unnormalized,
which simplifies the optimization. Thus, the objec-
tive function can be optimized by L-BFGS-B (Zhu
et al., 1997), a generic quasi-Newton gradient-
based optimizer. The partial derivatives of Equa-
tion (4) are computed for each parameter of q and
then passed on to the optimizer that updates them
such that Equation (4) is maximized.
</bodyText>
<subsectionHeader confidence="0.997291">
4.3 Semi-Supervised CRFs Training
</subsectionHeader>
<bodyText confidence="0.99989165">
The trigram-level label distributions inferred in the
propagation step can be viewed as a kind of valu-
able “prior knowledge” to regularize the learning
on unlabeled data. The final step of the induc-
tion is thus to incorporate such prior knowledge
into CRFs. Li (2009) generalizes the use of vir-
tual evidence to undirected graphical models and,
in particular, to CRFs for incorporating external
knowledge. By extending the similar intuition, as
illustrated in Figure 2, we modify the structure of
a regular linear-chain CRFs on unlabeled data for
smoothing the derived label distributions, where
virtual evidences, i.e., q in our case, are donated
by {v1, v2, ... , vT}, in parallel with the state vari-
ables {y1, y2, ... , yT}. The modified CRFs model
allows us to flexibly define the interaction between
estimated state values and virtual evidences by po-
tential functions. Therefore, given labeled and un-
labeled data, the learning objective is defined as
follows:
</bodyText>
<equation confidence="0.909099">
L(A) + �l+u Ep(yiJxi,vi;ng)[log p(yi, viIxi; A)]
i=l+1
(5)
</equation>
<bodyText confidence="0.9995355">
where the conditional probability in the second
term is denoted as
</bodyText>
<equation confidence="0.998397666666667">
p(yi, viIxi; A) =
1 exp{
ZI (xi; A)
N
+α s(yti, vti)}
t=1
</equation>
<bodyText confidence="0.950012333333333">
(6)
The first term in Equation (5) is the same as E-
quation (2), which is the traditional CRFs learn-
ing objective function on the labeled data. The
second term is the expected conditional likelihood
of unlabeled data. It is directed to maximize the
conditional likelihood of hidden states with the
derived label distributions on unlabeled data, i.e.,
p(y, vJx), where y and v are jointly modeled but
</bodyText>
<equation confidence="0.9868817">
�l
j=1
argmin
q
λkfk(yj−1
i , yji , xi, j)
N
j=1
K
k=1
</equation>
<page confidence="0.985851">
774
</page>
<bodyText confidence="0.998291125">
the probability is still conditional on x. Here,
Z&apos;(x; Λ) is the partition function of normalization
that is achieved by summing the numerator over
both y and v. A virtual evidence feature function
of s(yti, vti) with pre-defined weight α is defined
to regularize the conditional distributions of states
over the derived label distributions. The learning
is impacted by the derived label distributions as E-
</bodyText>
<equation confidence="0.6911105">
quation 7 firstly, the tri ram xt−1 xtxt+1 at
q () � Y, g i Z Z
</equation>
<bodyText confidence="0.999912545454545">
current position does have no corresponding de-
rived label distributions (vti = null), the value of
zero is assigned to all state hypotheses so that the
posteriors would not affected by the derived infor-
mation. Secondly, if it does have a derived label
distribution, since the virtual evidence in this case
is a distribution instead of a specific label, the la-
bel probability in the distribution under the current
state hypothesis is assigned. This means that the
values of state variables are constrained to agree
with the derived distributions.
</bodyText>
<equation confidence="0.9332145">
�
t t qxt−1XtXt+1(yt) if vti � null
s(yi,vi) = i z z
0 else
</equation>
<bodyText confidence="0.992034555555556">
The second term in Equation (5) can be op-
timized by using the expectation maximization
(EM) algorithm in the same fashion as in the
generative approach, following (Li, 2009). One
can iteratively optimize the Q function Q(Λ) =
Ey p(yi|xi; Λg) log p(yi, vi|xi; Λ), in which Λg is
the model estimated from the previous iteration.
Here the gradient of the Q function can be mea-
sured by:
</bodyText>
<equation confidence="0.93004">
(p(yt−1
i , yti|xi, vi; Λ) − p(yt−1
i , yti|xi; Λ))
</equation>
<bodyText confidence="0.992441416666667">
The forward-backward algorithm is used to mea-
sure p(yt−1
i , yt i|xi, vi; Λ) and p(yt−1
i , yti|xi; Λ).
Thus, the objective function Equation (5) is op-
timized as follows: for the instances i = 1, 2, ...,l,
the parameters Λ are learned as the supervised
manner; for the instances i = l+ 1, l+ 2,..., u + l,
in the E-step, the expected value of Q function is
computed, based on the current model Λg. In the
M-step, the posteriors are fixed and updated Λ that
maximizes Equation (5).
</bodyText>
<figureCaption confidence="0.975276">
Figure 2: Modified linear-chain CRFs integrating
virtual evidences on unlabeled data.
</figureCaption>
<sectionHeader confidence="0.991364" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.995479">
5.1 Setting
</subsectionHeader>
<bodyText confidence="0.999985727272727">
The experimental data are mainly taken from the
Chinese tree bank (CTB-7) and Microsoft Re-
search (MSR)2. CTB-7 consists of over one mil-
lion words of annotated and parsed text from Chi-
nese newswire, magazine news, various broadcast
news and broadcast conversation programs, web
newsgroups and weblogs. It is a segmented, POS
tagged3 and fully bracketed corpus. The train, de-
velopment and test sets4 from CTB-7 and their
corresponding statistics are reported in Table 3.
To satisfy the characteristic of the semi-supervised
learning problem, the train set, i.e., the labeled da-
ta, is formed by a relatively small amount of an-
notated texts sampled from CTB-7. For the un-
labeled data in this experiment, a greater amount
of texts is extracted from CTB-7 and MSR, which
contains 53,108 sentences with 2,418,690 charac-
ters.
The performance measurement indicators for
word segmentation and POS tagging (joint S&amp;T)
are balance F-score, F = 2PR/(P+R), the harmon-
ic mean of precision (P) and recall (R), and out-
of-vocabulary recall (OOV-R). For segmentation,
a token is regarded to be correct if its boundaries
match the ones of a word in the gold standard.
For the POS tagging, it is correct only if both the
boundaries and the POS tags are perfect matches.
The experimental platform is implemented
based on two toolkits: Mallet (McCallum and
Kachites, 2002) and Junto (Talukdar and Pereira,
2010). Mallet is a java-based package for s-
tatistical natural language processing, which in-
cludes the CRFs implementation. Junto is a graph-
</bodyText>
<footnote confidence="0.973612">
2It can be download at: www.sighan.org/bakeoff2005.
3There is a total of 33 POS tags in CTB-7.
4The extracted sentences in train, development and test set
were assigned with the composite tags as described in Section
3.1.
</footnote>
<equation confidence="0.613186333333333">
∂Q(Λ) �= E fk (yit−
∂Λk t t−1 t 1,
yi ,yi yti, xi, t).
</equation>
<page confidence="0.9795">
775
</page>
<bodyText confidence="0.8217565">
based label propagation toolkit that provides sev-
eral state-of-the-art algorithms.
</bodyText>
<table confidence="0.9990265">
Data #Sent #Word #Char #OOV
Train 17,968 374,697 596,360
Develop 1,659 46,637 79,283 0.074
Test 2,037 65,219 104,502 0.089
</table>
<tableCaption confidence="0.999842">
Table 3: Training, development and testing data.
</tableCaption>
<subsectionHeader confidence="0.992281">
5.2 Baseline and Proposed Models
</subsectionHeader>
<bodyText confidence="0.999662">
In the experiment, the baseline supervised pipeline
and joint S&amp;T models are built only on the train
data. The proposed model will also be compared
with the semi-supervised pipeline S&amp;T model de-
scribed in (Wang et al., 2011). In addition, two
state-of-the-art semi-supervised CRFs algorithms,
Jiao’s CRFs (Jiao et al., 2006) and Subramanya’s
CRFs (Subramanya et al., 2010), are also used to
build joint S&amp;T models. The corresponding set-
tings of the above candidates are listed below:
</bodyText>
<listItem confidence="0.962667217391305">
• Baseline I: a supervised CRFs pipeline S&amp;T
model. The feature templates are from Zhao
et al. (2006) and Wu et al. (2008).
• Wang’s model: a semi-supervised CRFs
pipeline S&amp;T model. The same feature tem-
plates in (Wang et al., 2011) are used, i.e.,
66 +n-
f+n- gram+cluster+lexicon”.
• Baseline II: a supervised CRFs joint S&amp;T
model. The feature templates introduced in
Section 3.1 are used.
• Jiao’s model: a semi-supervised CRFs joint
S&amp;T model trained using the entropy regular-
ization (ER) criteria (Jiao et al., 2006). The
optimization method proposed by Mann and
McCallum (2007) is applied.
• Subramanya’s model: a self-train style
semi-supervised CRFs joint S&amp;T model
based on the same parameters used in (Sub-
ramanya et al., 2010).
• Our model: several parameters in our model
are needed to tune based on the development
set, e.g., p, A and α.
</listItem>
<bodyText confidence="0.969638">
In all the CRFs models above, the Gaussian reg-
ularizer and stochastic gradient descent method
are employed.
</bodyText>
<subsectionHeader confidence="0.999453">
5.3 Main Results
</subsectionHeader>
<bodyText confidence="0.9999515">
This experiment yielded a similarity graph that
consists of 462,962 trigrams from labeled and un-
labeled data. The majority (317,677 trigrams) oc-
curred only in unlabeled data. Based on the de-
velopment data, the hyperparameters of our mod-
el were tuned among the following settings: for
the graph propagation, p E 10.2, 0.5, 0.8} and
A E 10.1, 0.3, 0.5, 0.8}; for the CRFs training,
α E 10.1, 0.3, 0.5, 0.7, 0.9}. The best performed
joint settings are p = 0.5, A = 0.3 and α = 0.7.
With the chosen set of hyperparameters, the test
data was used to measure the final performance.
</bodyText>
<table confidence="0.99963525">
Model Segmentation POS Tagging
Fl OOV-R Fl OOV-R
Baseline I 94.27 60.12 91.08 51.72
Wang’s 95.17 63.10 91.64 53.29
Baseline II 95.14 61.52 91.61 52.29
Jiao’s 95.58 63.05 92.11 53.27
Subramanya’s 96.30 67.12 92.46 57.15
Our model 96.85 68.09 92.89 58.36
</table>
<tableCaption confidence="0.9379885">
Table 4: The performance of segmentation and
POS tagging on testing data.
</tableCaption>
<bodyText confidence="0.989708964285714">
Table 4 summarizes the performance of seg-
mentation and POS tagging on the test data, in
comparison with the other five models. First-
ly, as expected, for the two supervised baselines,
the joint model outperforms the pipeline one, e-
specially on segmentation. It obtains 0.92% and
2.32% increase in terms of F-score and OOV-R
respectively. This outcome verifies the commonly
accepted fact that the joint model can substantially
improve the pipeline one, since POS tags provide
additional information to word segmentation (Ng
and Low, 2004). Secondly, it is also noticed that
all four semi-supervised models are able to benefit
from unlabeled data and greatly improve the re-
sults with respect to the baselines. On the whole,
for segmentation, they achieve average improve-
ments of 1.02% and 6.8% in F-score and OOV-R;
whereas for POS tagging, the average increments
of F-sore and OOV-R are 0.87% and 6.45%. An
interesting phenomenon is found among the com-
parisons with baselines that the supervised joint
model (Baseline II) is even competitive with semi-
supervised pipeline one (Wang et al., 2011). This
illustrates the effects of error propagation in the
pipeline approach. Thirdly, in what concerns the
semi-supervised approaches, the three joint S&amp;T
models, i.e., Jiao’s, Subramanya’s and our mod-
el, are superior to the pipeline model, i.e., Wang’s
</bodyText>
<page confidence="0.995886">
776
</page>
<bodyText confidence="0.992822555555556">
model. Moreover, the two graph-based approach-
es, i.e., Subramanya’s and our model, outperform
the others. Most importantly, the boldface num-
bers in the last row illustrate that our model does
achieve the best performance. Overall, for word
segmentation, it obtains average improvements of
1.43% and 8.09% in F-score and OOV-R over oth-
ers; for POS tagging, it achieves average improve-
ments of 1.09% and 7.73%.
</bodyText>
<figureCaption confidence="0.972657">
Figure 3: The learning curves of semi-supervised
models on unlabeled data, where left graphs are
segmentation and the right ones are tagging.
</figureCaption>
<subsectionHeader confidence="0.999138">
5.4 Learning Curve
</subsectionHeader>
<bodyText confidence="0.999913230769231">
An additional experiment was conducted to inves-
tigate the impact of unlabeled data for the four
semi-supervised models. Figure 3 illustrates the
curves of F-score and OOV-R for segmentation
and tagging respectively, as the unlabeled data
size is progressively increased in steps of 6,000
sentences. It can be clearly observed that al-
l curves of our model are able to mount up steadi-
ly and achieve better gains over others consistent-
ly. The most competitive performance of the oth-
er three candidates is achieved by Subramanya’s
model. This strongly reveals that the knowledge
derived from the similarity graph does effectively
strengthen the model. But in Subramanya’s mod-
el, when the unlabeled size ascends to approxi-
mately 30,000 sentences the curves become nearly
asymptotic. The semi-supervised pipeline model,
Wang’s model, presents a much slower growth on
all curves over the others and also begins to over-
fit with large unlabeled data sizes (&gt;25,000 sen-
tences). The figure also shows an erratic fluctu-
ation of Jiao’s model. Since this approach aims
at minimizing conditional entropy over unlabeled
data and encourages finding putative labelings for
unlabeled data, it results in a data-sensitive mod-
el (Li et al., 2009).
</bodyText>
<subsectionHeader confidence="0.998722">
5.5 Analysis &amp; Discussion
</subsectionHeader>
<bodyText confidence="0.99926292">
A statistical analysis of the segmentation and tag-
ging results of the supervised joint model (Base-
line II) and our model is carried out to comprehend
the influence of the graph-based semi-supervised
behavior. For word segmentation, the most signif-
icant improvement of our model is mainly concen-
trated on two kinds of words which are known for
their difficulties in terms of CWS: a) named enti-
ties (NE), e.g., “天津港” (Tianjin port) and “保税
区” (free tax zone); and b) Chinese numbers (CN),
e.g., “八点i17-” (eight hundred and fifty million)
and “百分2 �十—9” (seventy two percent). Very
often, these words do not exist in the labeled data,
so the supervised model is hard to learn their fea-
tures. Part of these words, however, may occur in
the unlabeled data. The proposed semi-supervised
approach is able to discover their label information
with the help of a similarity graph. Specifically, it
learns the label distributions from similar words
(neighborhoods), e.g., “±海港” (Shanghai port),
“保护区” (protection zone), “�i点�17-” (nine
hundred and seventy million). The statistics in Ta-
ble 5 demonstrate significant error reductions of
50.44% and 48.74% on test data, corresponding to
NE and CN respectively.
</bodyText>
<table confidence="0.979190333333333">
Type #word #baErr #gbErr ErrDec%
NE 471 226 112 50.44
CN 181 119 61 48.74
</table>
<tableCaption confidence="0.979193">
Table 5: The statistics of segmentation error for
</tableCaption>
<bodyText confidence="0.994921">
named entities (NE) and Chinese numbers (CN)
in test data. #baErr and #gbErr denote the count
of segmentations by Baseline II and our model;
ErrDec% denotes the error reduction.
On the other hand, to better understand the tag-
ging results, we summarize the increase and de-
crease of the top five common tagging error pat-
terns of our model over Baseline II for the cor-
rectly segmented words, as shown in Table 6. The
error pattern is defined by “A→B” that refers the
true tag of “A” is annotated by a tag of “B”. The
obvious improvement brought by our model oc-
curs with the tags “NN”, “CD”, “NR”, “JJ” and
“NR”, where errors are reduced 60.74% on aver-
</bodyText>
<figure confidence="0.963884">
F-score
OOV-R
F-score
OOV-R
</figure>
<page confidence="0.948486">
777
</page>
<table confidence="0.999410166666667">
Pattern #baErr Pattern #baErr T
NN-+VV 58 38 NN-+NR 13 6
CD-+NN 41 27 IJ-+ON 9 5
NR-+VV 29 17 VV-+NN 4 3
JJ-+NN 18 11 NR-+NN 1 3
NR-+VA 19 10 JJ-+AD 1 2
</table>
<tableCaption confidence="0.860156">
Table 6: The statistics of POS tagging error pat-
terns in test data. #baErr denote the count of tag-
</tableCaption>
<bodyText confidence="0.996638545454545">
ging error by Baseline II, while ↓ and ↑ denotes
the number of error reduced or increased by our
model.
age. More impressively, there is a large portion of
fixed error pattern instances stemming from OOV
words. Meanwhile, it is also observed that the dis-
ambiguation of error patterns in the right portion
of the table slightly suffers from our approach. In
reality, it is impossible and unrealistic to request
a model to be “no harms but only benefits” under
whatever circumstances.
</bodyText>
<sectionHeader confidence="0.999015" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999655">
This study introduces a novel semi-supervised ap-
proach for joint Chinese word segmentation and
POS tagging. The approach performs the semi-
supervised learning in the way that the trigram-
level distributions inferred from a similarity graph
are used to regularize the learning of CRFs model
on labeled and unlabeled data. The empirical re-
sults indicate that the similarity graph information
and the incorporation manner of virtual evidences
present a positive effect to the model induction.
</bodyText>
<sectionHeader confidence="0.99756" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999959142857143">
The authors are grateful to the Science and Tech-
nology Development Fund of Macau and the Re-
search Committee of the University of Macau
for the funding support for our research, un-
der the reference No. 017/2009/A and RG060/09-
10S/CS/FST. The authors also wish to thank the
anonymous reviewers for many helpful comments.
</bodyText>
<sectionHeader confidence="0.99848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999826887096774">
Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi
Jing, Jay Yagnik, Shankar Kumar, Deepak Ravich,
and Mohamed Aly. 2008. Video suggestion and
discovery for youtube: taking random walks through
the view graph. In Proceedings of WWW, pages 895-
904, Beijing, China.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization. Journal of machine
learning research, 7:2399–2434.
Yoshua Bengio, Olivier Delalleau, and Nicolas Le
Roux. 2006. Label propogation and quadratic crite-
rion. MIT Press.
Jon Louis Bentley. 1980. Multidimensional divide-and
-conquer. Communications of the ACM, 23(4):214 -
229.
Alina Beygelzimer, Sham Kakade, and John Langford.
2006. Cover trees for nearest neighbor. In Proceed-
ings of ICML, pages 97-104, New York, USA
Olivier Chapelle, Bernhard Sch¨o lkopf, and Alexander
Zien. 2006. Semi-supervised learning. MIT Press.
Samuel I. Daitch, Jonathan A. Kelner, and Daniel A.
Spielman. 2009. Fitting a graph to vector data. In
Proceedings of ICML, 201-208, NY, USA.
Dipanjan Das and Noah A. Smith. 2011. Semi-
supervised framesemantic parsing for unknown
predicates. In Proceedings of ACL, pages 1435-
1444, Portland, Oregon, USA.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
Part-of-Speech Tagging with Bilingual Graph-based
Projections. In Proceedings of ACL, pages 1435-
1444, Portland, Oregon, USA.
Dipanjan Das and Noah A. Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In Proceedings ofNAACL, pages 677-687, Montr´eal,
Canada.
Tony Jebara, Jun Wang, and Shih-Fu Chang. 2009.
Graph construction and b-matching for semi-
supervised learning. In Proceedings of ICML, 441-
448, New York, USA.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu.
2008. A Cascaded Linear Model for Joint Chinese
Word Segmentation and Part-of-Speech Tagging. In
Proceedings of ACL, pages 897-904, Columbus, O-
hio.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging – A Case
Study. In Proceedings of he ACL and the 4th IJC-
NLP of the AFNLP, pages 522–530, Suntec, Singa-
pore.
Feng Jiao, Shaojun Wang, and Chi-Hoon Lee. 2006.
Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In In
Proceedings of ACL, pages 209–216, Sydney, Aus-
tralia.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint Chinese word segmentation and
POS tagging. In Proceedings of ACL and IJCNLP
of the AFNLP, pages 513- 521, Suntec, Singapore
August.
</reference>
<page confidence="0.975291">
778
</page>
<reference confidence="0.999852490196078">
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Field: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML, pages 282-
289, Williams College, USA.
Xiao Li. 2009. On the use of virtual evidence in con-
ditional random fields. In Proceedings of EMNLP,
pages 1289-1297, Singapore.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields In Pro-
ceedings of ACM SIGIR, pages 572-579, Boston,
USA.
Gideon S. Mann and Andrew McCallum. 2007. Ef-
ficient computation of entropy gradient for semi-
supervised conditional random fields. In Proceed-
ings of NAACL, pages 109-112, New York, USA.
McCallum and Andrew Kachites. 2002. MALLET: A
Machine Learning for Language Toolkit. Software
at http://mallet.cs.umass.edu.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and POS tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sion, pages 217–220, Prague, Czech Republic.
Hwee Tou Ng and Jin Kiat Low 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP, Barcelona, Spain.
Xian Qian and Yang Liu. 2012. Joint Chinese Word
Segmentation, POS Tagging and Parsing. In Pro-
ceedings of EMNLP-CoNLL, pages 501-511, Jeju Is-
land, Korea.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
CRF based joint decoding method for cascade seg-
mentation and labelling tasks. In Proceedings of IJ-
CAI, Hyderabad, India.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models.
In Proceedings of EMNLP, pages 167-176, Mas-
sachusetts, USA.
Weiwei Sun. 2011. A Stacked Sub-Word Model
for Joint Chinese Word Segmentation and Part-of-
Speech Tagging. In Proceedings of ACL, pages
1385–1394, Portland, Oregon.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of EMNLP, pages 970-979, Scotland, UK.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pas-
ca, Deepak Ravichandran, Rahul Bhagat, and Fer-
nando Pereira. 2008. Weakly Supervised Acquisi-
tion of Labeled Class Instances using Graph Ran-
dom Walks. In Proceedings of EMNLP, pages 582-
590, Hawaii, USA.
Partha Pratim Talukdar and Koby Crammer. 2009.
New Regularized Algorithms for Transductive
Learning. In Proceedings of ECML-PKDD, pages
442 - 457, Bled, Slovenia.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learn-
ing methods for class-instance acquisition. In Pro-
ceedings of ACL, pages 1473-1481, Uppsala, Swe-
den.
Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisa-
wa. 2011. Improving Chinese word segmentation
and POS tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proceedings of IJC-
NLP, pages 309–317, Chiang Mai, Thailand.
Yu-Chieh Wu Jie-Chi Yang, and Yue-Shi Lee. 2008.
Description of the NCU Chinese Word Segmenta-
tion and Part-of-Speech Tagging for SIGHAN Bake-
off. In Proceedings of the SIGHAN Workshop on
Chinese Language Processing, pages 161-166, Hy-
derabad, India.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Herman-
n Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLING, pages 1017-1024,
Manchester, UK.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of EMNLP, pages 888-896,
Columbus, Ohio.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of
EMNLP, pages 843-852, Massachusetts, USA.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In Proceedings of PACLIC, pages 87-94,
Wuhan, China.
Xiaojin Zhu, Zoubin Ghahramani, and John Laffer-
ty. 2003. Semi-supervised learning using Gaussian
fields and harmonic functions. In Proceedings of
ICML, pages 912–919, Washington DC, USA.
Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge
Nocedal. 1997. L-BFGS-B: Fortran subroutines for
large scale bound constrained optimization. ACM
Transactions on Mathematical Software, 23:550-
560.
</reference>
<page confidence="0.998636">
779
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.564620">
<title confidence="0.996919">Graph-based Semi-Supervised Model for Joint Chinese Segmentation and Part-of-Speech Tagging</title>
<author confidence="0.668055">F S of Computer</author>
<author confidence="0.668055">Information Science</author>
<author confidence="0.668055">University of</author>
<affiliation confidence="0.755525">Instituto Superior T´ecnico, Lisboa,</affiliation>
<email confidence="0.989636">isabel.trancoso@inesc-id.pt</email>
<abstract confidence="0.997922363636364">This paper introduces a graph-based semisupervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information, i.e., label distributions. The derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random fields (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised CRFs in this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shumeet Baluja</author>
<author>Rohan Seth</author>
<author>D Sivakumar</author>
<author>Yushi Jing</author>
<author>Jay Yagnik</author>
<author>Shankar Kumar</author>
<author>Deepak Ravich</author>
<author>Mohamed Aly</author>
</authors>
<title>Video suggestion and discovery for youtube: taking random walks through the view graph.</title>
<date>2008</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>895--904</pages>
<location>Beijing, China.</location>
<contexts>
<context position="10813" citStr="Baluja et al., 2008" startWordPosition="1719" endWordPosition="1722">nearest neighbors (kNN) (Bentley, 1980; Beygelzimer et al., 2006), b-matching (Jebara et al., 2009) and local reconstruction (Daitch et al., 2009). Label propagation operates on the constructed graph. The primary objective is to propagate labels from a few labeled vertices to the entire graph by optimizing a loss function based on the constraints or properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdar and Crammer, 2009) and Sparse Inducing Penalties (Das and Smith, 2012). 4 Method The emphasis of this work is on building a joint S&amp;T model based on two different kinds of data sources, labeled and unlabeled data. In essence, this learning problem can be treated as incorporating certain gainful information, e.g., prior knowledge or label constraints, of unlabeled data into the supervised model. The proposed approach employs a transductive graph-based label propagation method to acquire such gainful information, i.e., label distributions from a similarity graph constructed over </context>
</contexts>
<marker>Baluja, Seth, Sivakumar, Jing, Yagnik, Kumar, Ravich, Aly, 2008</marker>
<rawString>Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi Jing, Jay Yagnik, Shankar Kumar, Deepak Ravich, and Mohamed Aly. 2008. Video suggestion and discovery for youtube: taking random walks through the view graph. In Proceedings of WWW, pages 895-904, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Belkin</author>
<author>Partha Niyogi</author>
<author>Vikas Sindhwani</author>
</authors>
<title>Manifold regularization.</title>
<date>2006</date>
<journal>Journal of machine learning research,</journal>
<pages>7--2399</pages>
<contexts>
<context position="9704" citStr="Belkin et al., 2006" startWordPosition="1540" endWordPosition="1543">imized by the stochastic gradient method or other numerical optimization methods. Type Font Size Unigram Cn(n = −2,−1,0,1,2) Bigram CnCn+1(n = −2, −1, 0,1) Date, Digit and T (C−2)T (C−1)T (C0) Alphabetic Letter T(C1)T(C2) Table 1: The feature templates of joint S&amp;T. 3.2 Graph-based Label Propagation Graph-based label propagation, a critical subclass of semi-supervised learning (SSL), has been widely used and shown to outperform other SSL methods (Chapelle et al., 2006). Most of these algorithms are transductive in nature, so they cannot be used to predict an unseen test example in the future (Belkin et al., 2006). Typically, graph-based label propagation algorithms are run in two main steps: graph construction and label propagation. The graph construction provides a natural way to represent data in a variety of target domains. One constructs a graph whose vertices consist of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges which encode the degree to which they are expected to have the same label (Zhu et al., 2003). Popular graph construction methods include k-nearest neighbors (kNN) (Bentley, 1980; Beygelzimer et al., 2006), b-matching (Jebara et al., 2009) and local r</context>
</contexts>
<marker>Belkin, Niyogi, Sindhwani, 2006</marker>
<rawString>Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. 2006. Manifold regularization. Journal of machine learning research, 7:2399–2434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Olivier Delalleau</author>
<author>Nicolas Le Roux</author>
</authors>
<title>Label propogation and quadratic criterion.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<marker>Bengio, Delalleau, Le Roux, 2006</marker>
<rawString>Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. 2006. Label propogation and quadratic criterion. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Louis Bentley</author>
</authors>
<title>Multidimensional divide-and -conquer.</title>
<date>1980</date>
<journal>Communications of the ACM,</journal>
<volume>23</volume>
<issue>4</issue>
<pages>229</pages>
<contexts>
<context position="10231" citStr="Bentley, 1980" startWordPosition="1625" endWordPosition="1626">y cannot be used to predict an unseen test example in the future (Belkin et al., 2006). Typically, graph-based label propagation algorithms are run in two main steps: graph construction and label propagation. The graph construction provides a natural way to represent data in a variety of target domains. One constructs a graph whose vertices consist of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges which encode the degree to which they are expected to have the same label (Zhu et al., 2003). Popular graph construction methods include k-nearest neighbors (kNN) (Bentley, 1980; Beygelzimer et al., 2006), b-matching (Jebara et al., 2009) and local reconstruction (Daitch et al., 2009). Label propagation operates on the constructed graph. The primary objective is to propagate labels from a few labeled vertices to the entire graph by optimizing a loss function based on the constraints or properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdar an</context>
</contexts>
<marker>Bentley, 1980</marker>
<rawString>Jon Louis Bentley. 1980. Multidimensional divide-and -conquer. Communications of the ACM, 23(4):214 -229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alina Beygelzimer</author>
<author>Sham Kakade</author>
<author>John Langford</author>
</authors>
<title>Cover trees for nearest neighbor.</title>
<date>2006</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>97--104</pages>
<location>New York, USA</location>
<contexts>
<context position="10258" citStr="Beygelzimer et al., 2006" startWordPosition="1627" endWordPosition="1630">d to predict an unseen test example in the future (Belkin et al., 2006). Typically, graph-based label propagation algorithms are run in two main steps: graph construction and label propagation. The graph construction provides a natural way to represent data in a variety of target domains. One constructs a graph whose vertices consist of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges which encode the degree to which they are expected to have the same label (Zhu et al., 2003). Popular graph construction methods include k-nearest neighbors (kNN) (Bentley, 1980; Beygelzimer et al., 2006), b-matching (Jebara et al., 2009) and local reconstruction (Daitch et al., 2009). Label propagation operates on the constructed graph. The primary objective is to propagate labels from a few labeled vertices to the entire graph by optimizing a loss function based on the constraints or properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdar and Crammer, 2009) and Sparse</context>
</contexts>
<marker>Beygelzimer, Kakade, Langford, 2006</marker>
<rawString>Alina Beygelzimer, Sham Kakade, and John Langford. 2006. Cover trees for nearest neighbor. In Proceedings of ICML, pages 97-104, New York, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Chapelle</author>
<author>Bernhard Sch¨o lkopf</author>
<author>Alexander Zien</author>
</authors>
<title>Semi-supervised learning.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9557" citStr="Chapelle et al., 2006" startWordPosition="1512" endWordPosition="1515">, R(A) =k A k /2δ2, to limit overfitting on rare features and avoid degeneracy in the case of correlated features. This objective function can be optimized by the stochastic gradient method or other numerical optimization methods. Type Font Size Unigram Cn(n = −2,−1,0,1,2) Bigram CnCn+1(n = −2, −1, 0,1) Date, Digit and T (C−2)T (C−1)T (C0) Alphabetic Letter T(C1)T(C2) Table 1: The feature templates of joint S&amp;T. 3.2 Graph-based Label Propagation Graph-based label propagation, a critical subclass of semi-supervised learning (SSL), has been widely used and shown to outperform other SSL methods (Chapelle et al., 2006). Most of these algorithms are transductive in nature, so they cannot be used to predict an unseen test example in the future (Belkin et al., 2006). Typically, graph-based label propagation algorithms are run in two main steps: graph construction and label propagation. The graph construction provides a natural way to represent data in a variety of target domains. One constructs a graph whose vertices consist of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges which encode the degree to which they are expected to have the same label (Zhu et al., 2003). Popular g</context>
</contexts>
<marker>Chapelle, lkopf, Zien, 2006</marker>
<rawString>Olivier Chapelle, Bernhard Sch¨o lkopf, and Alexander Zien. 2006. Semi-supervised learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel I Daitch</author>
<author>Jonathan A Kelner</author>
<author>Daniel A Spielman</author>
</authors>
<title>Fitting a graph to vector data.</title>
<date>2009</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>201--208</pages>
<location>NY, USA.</location>
<contexts>
<context position="10339" citStr="Daitch et al., 2009" startWordPosition="1640" endWordPosition="1643">aph-based label propagation algorithms are run in two main steps: graph construction and label propagation. The graph construction provides a natural way to represent data in a variety of target domains. One constructs a graph whose vertices consist of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges which encode the degree to which they are expected to have the same label (Zhu et al., 2003). Popular graph construction methods include k-nearest neighbors (kNN) (Bentley, 1980; Beygelzimer et al., 2006), b-matching (Jebara et al., 2009) and local reconstruction (Daitch et al., 2009). Label propagation operates on the constructed graph. The primary objective is to propagate labels from a few labeled vertices to the entire graph by optimizing a loss function based on the constraints or properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdar and Crammer, 2009) and Sparse Inducing Penalties (Das and Smith, 2012). 4 Method The emphasis of this work is </context>
</contexts>
<marker>Daitch, Kelner, Spielman, 2009</marker>
<rawString>Samuel I. Daitch, Jonathan A. Kelner, and Daniel A. Spielman. 2009. Fitting a graph to vector data. In Proceedings of ICML, 201-208, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Semisupervised framesemantic parsing for unknown predicates.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1435--1444</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="3244" citStr="Das and Smith, 2011" startWordPosition="472" endWordPosition="475">ive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&amp;T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&amp;T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&amp;T model. Graph-based label propagation methods have recently shown they can outperform the state-of-the-art in several natural language processing (NLP) tasks, e.g., POS tagging (Subramanya et al., 2010), knowledge acquisition (Talukdar et al., 2008), shallow semantic parsing for unknown predicate (Das and Smith, 2011). As far as we know, however, these methods have not yet been applied to resolve the problem of joint Chinese word segmentation (CWS) and POS tagging. Motivated by the works in (Subramanya et al., 2010; Das and Smith, 2011), for structured problems, graph-based label propagation can be employed to infer valuable syntactic information (ngram-level label distributions) from labeled data to unlabeled data. This study extends this intuition to construct a similarity graph for propagating trigram-level label distributions. The derived label distributions are regarded as prior knowledge to regulariz</context>
<context position="18131" citStr="Das and Smith, 2011" startWordPosition="2931" endWordPosition="2934">scussed in Section 5. The squaredloss criterion1 is used to formulate the objective function. The first term in Equation (4) is the seed match loss which penalizes the estimated label distributions qj, if they go too far away from the empirical labeled distributions rj. The second term is the edge smoothness loss that requires qi should be smooth with respect to the graph, such that two vertices connected by an edge with high weight should be assigned similar labels. The final term is a regularizer to incorporate the prior knowledge, e.g., uniform distributions used in (Talukdar et al., 2008; Das and Smith, 2011). This study applies the squared norm of q to encourage sparsity per vertex. Note that the estimated label distribution 1It can be seen as a multi-class extension of quadratic cost criterion (Bengio et al., 2006) or as a variant of the objective in (Zhu et al., 2003). An entropic distance measure could also be used, e.g., KL-divergence (Subramanya et al., 2010; Das and Smith, 2012). qi in Equation (4) is relaxed to be unnormalized, which simplifies the optimization. Thus, the objective function can be optimized by L-BFGS-B (Zhu et al., 1997), a generic quasi-Newton gradientbased optimizer. The</context>
</contexts>
<marker>Das, Smith, 2011</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2011. Semisupervised framesemantic parsing for unknown predicates. In Proceedings of ACL, pages 1435-1444, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised Part-of-Speech Tagging with Bilingual Graph-based Projections.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1435--1444</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="13360" citStr="Das and Petrov (2011)" startWordPosition="2115" endWordPosition="2118">g Penalties (Das and Smith, 2012) (Section 4.2). The final step (train crf) of the induction is incorporating the inferred trigram-level label distributions q into CRFs model (Section 4.3). 4.1 Graph Construction In most graph-based label propagation tasks, the final effect depends heavily on the quality of the graph. Graph construction thus plays a central role in graph-based label propagation (Zhu et al., 2003). For character-based joint S&amp;T, unlike the unstructured learning problem whose vertices are formed directly by labeled and unlabeled instances, the graph construction is non-trivial. Das and Petrov (2011) mentioned that taking individual characters as the vertices would result in various ambiguities, whereas the similarity measurement is still challenging if vertices corresponding to entire sentences. This study follows the intuitions of graph construction from Subramanya et al. (2010) in which vertices are represented by character trigrams occurring in labeled and unlabeled sentences. Formally, given a set of labeled sentences Dl, and unlabeled ones Du, where D °_ {Dl, Du}, the goal is to form an undirected weighted graph G = (V, E), where V is defined as the set of vertices which covers all </context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised Part-of-Speech Tagging with Bilingual Graph-based Projections. In Proceedings of ACL, pages 1435-1444, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Graph-based lexicon expansion with sparsity-inducing penalties.</title>
<date>2012</date>
<booktitle>In Proceedings ofNAACL,</booktitle>
<pages>677--687</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="10698" citStr="Das and Smith, 2012" startWordPosition="1702" endWordPosition="1705">to which they are expected to have the same label (Zhu et al., 2003). Popular graph construction methods include k-nearest neighbors (kNN) (Bentley, 1980; Beygelzimer et al., 2006), b-matching (Jebara et al., 2009) and local reconstruction (Daitch et al., 2009). Label propagation operates on the constructed graph. The primary objective is to propagate labels from a few labeled vertices to the entire graph by optimizing a loss function based on the constraints or properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdar and Crammer, 2009) and Sparse Inducing Penalties (Das and Smith, 2012). 4 Method The emphasis of this work is on building a joint S&amp;T model based on two different kinds of data sources, labeled and unlabeled data. In essence, this learning problem can be treated as incorporating certain gainful information, e.g., prior knowledge or label constraints, of unlabeled data into the supervised model. The proposed approach employs a transductive graph-based label propagat</context>
<context position="12772" citStr="Das and Smith, 2012" startWordPosition="2026" endWordPosition="2029">(construct graph). The vertices (Section 4.1) in the constructed graph consist of all trigrams that occur in labeled and unlabeled sentences, and edge weights between vertices are computed using the cosine distance between pointwise mutual information (PMI) statistics. Afterwards, the estimated label distributions q0 of vertices in the graph G are randomly initialized (init labelDist). Subsequently, 772 the label propagation procedure (propagate label) is conducted for projecting label distributions q from labeled vertices to the entire graph, using the algorithm of Sparse-Inducing Penalties (Das and Smith, 2012) (Section 4.2). The final step (train crf) of the induction is incorporating the inferred trigram-level label distributions q into CRFs model (Section 4.3). 4.1 Graph Construction In most graph-based label propagation tasks, the final effect depends heavily on the quality of the graph. Graph construction thus plays a central role in graph-based label propagation (Zhu et al., 2003). For character-based joint S&amp;T, unlike the unstructured learning problem whose vertices are formed directly by labeled and unlabeled instances, the graph construction is non-trivial. Das and Petrov (2011) mentioned t</context>
<context position="16677" citStr="Das and Smith (2012)" startWordPosition="2675" endWordPosition="2678">In other words, the new features of various trigrams only occurring in unlabeled data can be discovered. As the excerpt in Figure 1 shows, the trigram “天津港” (Tianjin port) has no any label information, as it only occurs in unlabeled data, but fortunately its neighborhoods with similar syntax information, e.g., “上海港” (Shanghai port), “广州 港” (Guangzhou port), can assist to infer the correct tag “M NN”. 4.2 Label Propagation In order to induce trigram-level label distributions from the graph constructed by the previous step, a label propagation algorithm, Sparsity-Inducing Penalties, proposed by Das and Smith (2012), is employed. This algorithm is used because it captures the property of sparsity that only a few labels 773 Figure 1: An excerpt from the similarity graph over trigrams on labeled and unlabeled data. are typically associated with a given instance. In fact, the sparsity is also a common phenomenon among character-based CWS and POS tagging. The following convex objective is optimized on the similarity graph in this case: 11 qj − rj 112 +µ �l+u wik 11 qi − qk 112 +λ �l+u 11 qi 112 i=1,kEN(i) i=1 s.t. qi &gt; 0,bi E V (4) where rj denotes empirical label distributions of labeled vertices, and qi de</context>
<context position="18515" citStr="Das and Smith, 2012" startWordPosition="2996" endWordPosition="2999">t two vertices connected by an edge with high weight should be assigned similar labels. The final term is a regularizer to incorporate the prior knowledge, e.g., uniform distributions used in (Talukdar et al., 2008; Das and Smith, 2011). This study applies the squared norm of q to encourage sparsity per vertex. Note that the estimated label distribution 1It can be seen as a multi-class extension of quadratic cost criterion (Bengio et al., 2006) or as a variant of the objective in (Zhu et al., 2003). An entropic distance measure could also be used, e.g., KL-divergence (Subramanya et al., 2010; Das and Smith, 2012). qi in Equation (4) is relaxed to be unnormalized, which simplifies the optimization. Thus, the objective function can be optimized by L-BFGS-B (Zhu et al., 1997), a generic quasi-Newton gradientbased optimizer. The partial derivatives of Equation (4) are computed for each parameter of q and then passed on to the optimizer that updates them such that Equation (4) is maximized. 4.3 Semi-Supervised CRFs Training The trigram-level label distributions inferred in the propagation step can be viewed as a kind of valuable “prior knowledge” to regularize the learning on unlabeled data. The final step</context>
</contexts>
<marker>Das, Smith, 2012</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2012. Graph-based lexicon expansion with sparsity-inducing penalties. In Proceedings ofNAACL, pages 677-687, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Jebara</author>
<author>Jun Wang</author>
<author>Shih-Fu Chang</author>
</authors>
<title>Graph construction and b-matching for semisupervised learning.</title>
<date>2009</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>441--448</pages>
<location>New York, USA.</location>
<contexts>
<context position="10292" citStr="Jebara et al., 2009" startWordPosition="1632" endWordPosition="1635">the future (Belkin et al., 2006). Typically, graph-based label propagation algorithms are run in two main steps: graph construction and label propagation. The graph construction provides a natural way to represent data in a variety of target domains. One constructs a graph whose vertices consist of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges which encode the degree to which they are expected to have the same label (Zhu et al., 2003). Popular graph construction methods include k-nearest neighbors (kNN) (Bentley, 1980; Beygelzimer et al., 2006), b-matching (Jebara et al., 2009) and local reconstruction (Daitch et al., 2009). Label propagation operates on the constructed graph. The primary objective is to propagate labels from a few labeled vertices to the entire graph by optimizing a loss function based on the constraints or properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdar and Crammer, 2009) and Sparse Inducing Penalties (Das and Smith</context>
</contexts>
<marker>Jebara, Wang, Chang, 2009</marker>
<rawString>Tony Jebara, Jun Wang, and Shih-Fu Chang. 2009. Graph construction and b-matching for semisupervised learning. In Proceedings of ICML, 441-448, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan Liu</author>
</authors>
<title>A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>897--904</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="5262" citStr="Jiang et al., 2008" startWordPosition="780" endWordPosition="783">t the main differences with the related work of this study. Section 3 reviews the background, including supervised character-based joint S&amp;T model based on CRFs and graph-based label propagation. Section 4 presents the details of the proposed approach. Section 5 reports the experiment results. The conclusion is drawn in Section 6. 2 Related Work Prior supervised joint S&amp;T models present approximate 0.2% - 1.3% improvement in F-score over supervised pipeline ones. The state-of-theart joint models include reranking approaches (Shi and Wang, 2007), hybrid approaches (Nakagawa and Uchimoto, 2007; Jiang et al., 2008; Sun, 2011), and single-model approaches (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). The proposed approach in this paper belongs to the single-model type. There are few explorations of semi-supervised approaches for CWS or POS tagging in previous works. Xu et al. (2008) described a Bayesian semi-supervised CWS model by considering the segmentation as the hidden variable in machine translation. Unlike this model, the proposed approach is targeted at a general model, instead of one oriented to machine translation task. Sun and Xu (2011) enhanced a </context>
<context position="8600" citStr="Jiang et al., 2008" startWordPosition="1352" endWordPosition="1355">has been the most common one in this task. Given a set of labeled examples Dl = {(xi, yi)}li=1, where xi = x1ix2 i ...xNi is the sequence of characters in the ith sentence, and yi = y1i y2i ...yNi is the corresponding label sequence. The goal is to learn a CRFs model in the form, p(yi|xi; A) = 1 exp{ Z(xi; A) (1) where Z(xi; A) is the partition function that normalizes the exponential form to be a probability distribution, and fk(yj−1 i , yji , xi, j). In this study, Akfk(yj−1 i , yji , xi, j)} N j=1 K k=1 771 the baseline feature templates of joint S&amp;T are the ones used in (Ng and Low, 2004; Jiang et al., 2008), as shown in Table 1. A = {λ1λ2...λK} ∈ RK are the weight parameters to be learned. In supervised training, the aim is to estimate the A that maximizes the conditional likelihood of the training data while regularizing model parameters: L(A) = �l log p(yi|xi; A) − R(A) (2) i=1 R(A) can be any standard regularizer on parameters, e.g., R(A) =k A k /2δ2, to limit overfitting on rare features and avoid degeneracy in the case of correlated features. This objective function can be optimized by the stochastic gradient method or other numerical optimization methods. Type Font Size Unigram Cn(n = −2,−</context>
</contexts>
<marker>Jiang, Huang, Liu, Liu, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu. 2008. A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging. In Proceedings of ACL, pages 897-904, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging – A Case Study.</title>
<date>2009</date>
<booktitle>In Proceedings of he ACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>522--530</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="2334" citStr="Jiang et al., 2009" startWordPosition="333" endWordPosition="336"> wrong segmentations in the earlier stage harm the subsequent POS tagging (Ng and Low, 2004). The joint approaches of word segmentation and POS tagging (joint S&amp;T) are proposed to resolve these two tasks simultaneously. They effectively alleviate the error propagation, because segmentation and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&amp;T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&amp;T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&amp;T model. G</context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging – A Case Study. In Proceedings of he ACL and the 4th IJCNLP of the AFNLP, pages 522–530, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Jiao</author>
<author>Shaojun Wang</author>
<author>Chi-Hoon Lee</author>
</authors>
<title>Semi-supervised conditional random fields for improved sequence segmentation and labeling. In</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>209--216</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2646" citStr="Jiao et al., 2006" startWordPosition="383" endWordPosition="386">g interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&amp;T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&amp;T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&amp;T model. Graph-based label propagation methods have recently shown they can outperform the state-of-the-art in several natural language processing (NLP) tasks, e.g., POS tagging (Subramanya et al., 2010), knowledge acquisition (Talukdar et al., 2008), shallow semantic parsing for unknown predicate (Das and Smith, 2011). </context>
<context position="6393" citStr="Jiao et al. (2006)" startWordPosition="961" endWordPosition="964">del, instead of one oriented to machine translation task. Sun and Xu (2011) enhanced a CWS model by interpolating statistical features of unlabeled data into the CRFs model. Wang et al. (2011) proposed a semisupervised pipeline S&amp;T model by incorporating n-gram and lexicon features derived from unlabeled data. Different from their concern, our emphasis is to learn the semi-supervised model by injecting the label information from a similarity graph constructed from labeled and unlabeled data. The induction method of the proposed approach also differs from other semi-supervised CRFs algorithms. Jiao et al. (2006), extended by Mann and McCallum (2007), reported a semi-supervised CRFs model which aims to guide the learning by minimizing the conditional entropy of unlabeled data. The proposed approach regularizes the CRFs by the graph information. Subramanya et al. (2010) proposed a graph-based self-train style semi-supervised CRFs algorithm. In the proposed approach, an analogous way of graph construction intuition is applied. But overall, our approach differs in three important aspects: first, novel feature templates are defined for measuring the similarity between vertices. Second, the critical proper</context>
<context position="25054" citStr="Jiao et al., 2006" startWordPosition="4103" endWordPosition="4106">. 775 based label propagation toolkit that provides several state-of-the-art algorithms. Data #Sent #Word #Char #OOV Train 17,968 374,697 596,360 Develop 1,659 46,637 79,283 0.074 Test 2,037 65,219 104,502 0.089 Table 3: Training, development and testing data. 5.2 Baseline and Proposed Models In the experiment, the baseline supervised pipeline and joint S&amp;T models are built only on the train data. The proposed model will also be compared with the semi-supervised pipeline S&amp;T model described in (Wang et al., 2011). In addition, two state-of-the-art semi-supervised CRFs algorithms, Jiao’s CRFs (Jiao et al., 2006) and Subramanya’s CRFs (Subramanya et al., 2010), are also used to build joint S&amp;T models. The corresponding settings of the above candidates are listed below: • Baseline I: a supervised CRFs pipeline S&amp;T model. The feature templates are from Zhao et al. (2006) and Wu et al. (2008). • Wang’s model: a semi-supervised CRFs pipeline S&amp;T model. The same feature templates in (Wang et al., 2011) are used, i.e., 66 +nf+n- gram+cluster+lexicon”. • Baseline II: a supervised CRFs joint S&amp;T model. The feature templates introduced in Section 3.1 are used. • Jiao’s model: a semi-supervised CRFs joint S&amp;T m</context>
</contexts>
<marker>Jiao, Wang, Lee, 2006</marker>
<rawString>Feng Jiao, Shaojun Wang, and Chi-Hoon Lee. 2006. Semi-supervised conditional random fields for improved sequence segmentation and labeling. In In Proceedings of ACL, pages 209–216, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL and IJCNLP of the AFNLP,</booktitle>
<pages>513--521</pages>
<location>Suntec, Singapore</location>
<contexts>
<context position="5369" citStr="Kruengkrai et al., 2009" startWordPosition="797" endWordPosition="800">g supervised character-based joint S&amp;T model based on CRFs and graph-based label propagation. Section 4 presents the details of the proposed approach. Section 5 reports the experiment results. The conclusion is drawn in Section 6. 2 Related Work Prior supervised joint S&amp;T models present approximate 0.2% - 1.3% improvement in F-score over supervised pipeline ones. The state-of-theart joint models include reranking approaches (Shi and Wang, 2007), hybrid approaches (Nakagawa and Uchimoto, 2007; Jiang et al., 2008; Sun, 2011), and single-model approaches (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). The proposed approach in this paper belongs to the single-model type. There are few explorations of semi-supervised approaches for CWS or POS tagging in previous works. Xu et al. (2008) described a Bayesian semi-supervised CWS model by considering the segmentation as the hidden variable in machine translation. Unlike this model, the proposed approach is targeted at a general model, instead of one oriented to machine translation task. Sun and Xu (2011) enhanced a CWS model by interpolating statistical features of unlabeled data into the CRFs model. Wang et al. (2011) p</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In Proceedings of ACL and IJCNLP of the AFNLP, pages 513- 521, Suntec, Singapore August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Field: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>282--289</pages>
<location>Williams College, USA.</location>
<contexts>
<context position="7980" citStr="Lafferty et al., 2001" startWordPosition="1226" endWordPosition="1229">he sequence is assigned with a tag. To perform segmentation and tagging simultaneously in a uniform framework, according to Ng and Low (2004), the tag is composed of a word boundary part, and a POS part, e.g., “B NN” refers to the first character in a word with POS tag “NN”. In this paper, 4 word boundary tags are employed: B (beginning of a word), M (middle part of a word), E (end of a word) and S (single character). As for the POS tag, we shall use the 33 tags in the Chinese tree bank. Thus, the potential composite tags of joint S&amp;T consist of 132 (4x33) classes. The first-order CRFs model (Lafferty et al., 2001) has been the most common one in this task. Given a set of labeled examples Dl = {(xi, yi)}li=1, where xi = x1ix2 i ...xNi is the sequence of characters in the ith sentence, and yi = y1i y2i ...yNi is the corresponding label sequence. The goal is to learn a CRFs model in the form, p(yi|xi; A) = 1 exp{ Z(xi; A) (1) where Z(xi; A) is the partition function that normalizes the exponential form to be a probability distribution, and fk(yj−1 i , yji , xi, j). In this study, Akfk(yj−1 i , yji , xi, j)} N j=1 K k=1 771 the baseline feature templates of joint S&amp;T are the ones used in (Ng and Low, 2004;</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Field: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of ICML, pages 282-289, Williams College, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
</authors>
<title>On the use of virtual evidence in conditional random fields.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1289--1297</pages>
<contexts>
<context position="4344" citStr="Li, 2009" startWordPosition="638" endWordPosition="639">igram-level label distributions. The derived label distributions are regarded as prior knowledge to regularize the learning of a sequential model, conditional random fields (CRFs) in this case, on both 770 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 770–779, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics labeled and unlabeled data to achieve the semisupervised learning. The approach performs the incorporation of the derived labeled distributions by manipulating a “virtual evidence” function as described in (Li, 2009). Experiments on the data from the Chinese tree bank (CTB-7) and Microsoft Research (MSR) show that the proposed model results in significant improvement over other comparative candidates in terms of F-score and out-of-vocabulary (OOV) recall. This paper is structured as follows: Section 2 points out the main differences with the related work of this study. Section 3 reviews the background, including supervised character-based joint S&amp;T model based on CRFs and graph-based label propagation. Section 4 presents the details of the proposed approach. Section 5 reports the experiment results. The c</context>
<context position="19197" citStr="Li (2009)" startWordPosition="3110" endWordPosition="3111">optimization. Thus, the objective function can be optimized by L-BFGS-B (Zhu et al., 1997), a generic quasi-Newton gradientbased optimizer. The partial derivatives of Equation (4) are computed for each parameter of q and then passed on to the optimizer that updates them such that Equation (4) is maximized. 4.3 Semi-Supervised CRFs Training The trigram-level label distributions inferred in the propagation step can be viewed as a kind of valuable “prior knowledge” to regularize the learning on unlabeled data. The final step of the induction is thus to incorporate such prior knowledge into CRFs. Li (2009) generalizes the use of virtual evidence to undirected graphical models and, in particular, to CRFs for incorporating external knowledge. By extending the similar intuition, as illustrated in Figure 2, we modify the structure of a regular linear-chain CRFs on unlabeled data for smoothing the derived label distributions, where virtual evidences, i.e., q in our case, are donated by {v1, v2, ... , vT}, in parallel with the state variables {y1, y2, ... , yT}. The modified CRFs model allows us to flexibly define the interaction between estimated state values and virtual evidences by potential funct</context>
<context position="21785" citStr="Li, 2009" startWordPosition="3555" endWordPosition="3556">ot affected by the derived information. Secondly, if it does have a derived label distribution, since the virtual evidence in this case is a distribution instead of a specific label, the label probability in the distribution under the current state hypothesis is assigned. This means that the values of state variables are constrained to agree with the derived distributions. � t t qxt−1XtXt+1(yt) if vti � null s(yi,vi) = i z z 0 else The second term in Equation (5) can be optimized by using the expectation maximization (EM) algorithm in the same fashion as in the generative approach, following (Li, 2009). One can iteratively optimize the Q function Q(Λ) = Ey p(yi|xi; Λg) log p(yi, vi|xi; Λ), in which Λg is the model estimated from the previous iteration. Here the gradient of the Q function can be measured by: (p(yt−1 i , yti|xi, vi; Λ) − p(yt−1 i , yti|xi; Λ)) The forward-backward algorithm is used to measure p(yt−1 i , yt i|xi, vi; Λ) and p(yt−1 i , yti|xi; Λ). Thus, the objective function Equation (5) is optimized as follows: for the instances i = 1, 2, ...,l, the parameters Λ are learned as the supervised manner; for the instances i = l+ 1, l+ 2,..., u + l, in the E-step, the expected valu</context>
</contexts>
<marker>Li, 2009</marker>
<rawString>Xiao Li. 2009. On the use of virtual evidence in conditional random fields. In Proceedings of EMNLP, pages 1289-1297, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
<author>Ye-Yi Wang</author>
<author>Alex Acero</author>
</authors>
<title>Extracting structured information from user queries with semi-supervised conditional random fields</title>
<date>2009</date>
<booktitle>In Proceedings of ACM SIGIR,</booktitle>
<pages>572--579</pages>
<location>Boston, USA.</location>
<contexts>
<context position="30236" citStr="Li et al., 2009" startWordPosition="4950" endWordPosition="4953">does effectively strengthen the model. But in Subramanya’s model, when the unlabeled size ascends to approximately 30,000 sentences the curves become nearly asymptotic. The semi-supervised pipeline model, Wang’s model, presents a much slower growth on all curves over the others and also begins to overfit with large unlabeled data sizes (&gt;25,000 sentences). The figure also shows an erratic fluctuation of Jiao’s model. Since this approach aims at minimizing conditional entropy over unlabeled data and encourages finding putative labelings for unlabeled data, it results in a data-sensitive model (Li et al., 2009). 5.5 Analysis &amp; Discussion A statistical analysis of the segmentation and tagging results of the supervised joint model (Baseline II) and our model is carried out to comprehend the influence of the graph-based semi-supervised behavior. For word segmentation, the most significant improvement of our model is mainly concentrated on two kinds of words which are known for their difficulties in terms of CWS: a) named entities (NE), e.g., “天津港” (Tianjin port) and “保税 区” (free tax zone); and b) Chinese numbers (CN), e.g., “八点i17-” (eight hundred and fifty million) and “百分2 �十—9” (seventy two percent)</context>
</contexts>
<marker>Li, Wang, Acero, 2009</marker>
<rawString>Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extracting structured information from user queries with semi-supervised conditional random fields In Proceedings of ACM SIGIR, pages 572-579, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient computation of entropy gradient for semisupervised conditional random fields.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>109--112</pages>
<location>New York, USA.</location>
<contexts>
<context position="6431" citStr="Mann and McCallum (2007)" startWordPosition="967" endWordPosition="970">machine translation task. Sun and Xu (2011) enhanced a CWS model by interpolating statistical features of unlabeled data into the CRFs model. Wang et al. (2011) proposed a semisupervised pipeline S&amp;T model by incorporating n-gram and lexicon features derived from unlabeled data. Different from their concern, our emphasis is to learn the semi-supervised model by injecting the label information from a similarity graph constructed from labeled and unlabeled data. The induction method of the proposed approach also differs from other semi-supervised CRFs algorithms. Jiao et al. (2006), extended by Mann and McCallum (2007), reported a semi-supervised CRFs model which aims to guide the learning by minimizing the conditional entropy of unlabeled data. The proposed approach regularizes the CRFs by the graph information. Subramanya et al. (2010) proposed a graph-based self-train style semi-supervised CRFs algorithm. In the proposed approach, an analogous way of graph construction intuition is applied. But overall, our approach differs in three important aspects: first, novel feature templates are defined for measuring the similarity between vertices. Second, the critical property, i.e., sparsity, is considered amon</context>
<context position="25795" citStr="Mann and McCallum (2007)" startWordPosition="4226" endWordPosition="4229"> of the above candidates are listed below: • Baseline I: a supervised CRFs pipeline S&amp;T model. The feature templates are from Zhao et al. (2006) and Wu et al. (2008). • Wang’s model: a semi-supervised CRFs pipeline S&amp;T model. The same feature templates in (Wang et al., 2011) are used, i.e., 66 +nf+n- gram+cluster+lexicon”. • Baseline II: a supervised CRFs joint S&amp;T model. The feature templates introduced in Section 3.1 are used. • Jiao’s model: a semi-supervised CRFs joint S&amp;T model trained using the entropy regularization (ER) criteria (Jiao et al., 2006). The optimization method proposed by Mann and McCallum (2007) is applied. • Subramanya’s model: a self-train style semi-supervised CRFs joint S&amp;T model based on the same parameters used in (Subramanya et al., 2010). • Our model: several parameters in our model are needed to tune based on the development set, e.g., p, A and α. In all the CRFs models above, the Gaussian regularizer and stochastic gradient descent method are employed. 5.3 Main Results This experiment yielded a similarity graph that consists of 462,962 trigrams from labeled and unlabeled data. The majority (317,677 trigrams) occurred only in unlabeled data. Based on the development data, th</context>
</contexts>
<marker>Mann, McCallum, 2007</marker>
<rawString>Gideon S. Mann and Andrew McCallum. 2007. Efficient computation of entropy gradient for semisupervised conditional random fields. In Proceedings of NAACL, pages 109-112, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>McCallum</author>
<author>Andrew Kachites</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit. Software at http://mallet.cs.umass.edu.</title>
<date>2002</date>
<contexts>
<context position="23988" citStr="McCallum and Kachites, 2002" startWordPosition="3932" endWordPosition="3935">exts is extracted from CTB-7 and MSR, which contains 53,108 sentences with 2,418,690 characters. The performance measurement indicators for word segmentation and POS tagging (joint S&amp;T) are balance F-score, F = 2PR/(P+R), the harmonic mean of precision (P) and recall (R), and outof-vocabulary recall (OOV-R). For segmentation, a token is regarded to be correct if its boundaries match the ones of a word in the gold standard. For the POS tagging, it is correct only if both the boundaries and the POS tags are perfect matches. The experimental platform is implemented based on two toolkits: Mallet (McCallum and Kachites, 2002) and Junto (Talukdar and Pereira, 2010). Mallet is a java-based package for statistical natural language processing, which includes the CRFs implementation. Junto is a graph2It can be download at: www.sighan.org/bakeoff2005. 3There is a total of 33 POS tags in CTB-7. 4The extracted sentences in train, development and test set were assigned with the composite tags as described in Section 3.1. ∂Q(Λ) �= E fk (yit− ∂Λk t t−1 t 1, yi ,yi yti, xi, t). 775 based label propagation toolkit that provides several state-of-the-art algorithms. Data #Sent #Word #Char #OOV Train 17,968 374,697 596,360 Develo</context>
</contexts>
<marker>McCallum, Kachites, 2002</marker>
<rawString>McCallum and Andrew Kachites. 2002. MALLET: A Machine Learning for Language Toolkit. Software at http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kiyotaka Uchimoto</author>
</authors>
<title>A hybrid approach to word segmentation and POS tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL Demo and Poster Session,</booktitle>
<pages>217--220</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5242" citStr="Nakagawa and Uchimoto, 2007" startWordPosition="776" endWordPosition="779"> follows: Section 2 points out the main differences with the related work of this study. Section 3 reviews the background, including supervised character-based joint S&amp;T model based on CRFs and graph-based label propagation. Section 4 presents the details of the proposed approach. Section 5 reports the experiment results. The conclusion is drawn in Section 6. 2 Related Work Prior supervised joint S&amp;T models present approximate 0.2% - 1.3% improvement in F-score over supervised pipeline ones. The state-of-theart joint models include reranking approaches (Shi and Wang, 2007), hybrid approaches (Nakagawa and Uchimoto, 2007; Jiang et al., 2008; Sun, 2011), and single-model approaches (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). The proposed approach in this paper belongs to the single-model type. There are few explorations of semi-supervised approaches for CWS or POS tagging in previous works. Xu et al. (2008) described a Bayesian semi-supervised CWS model by considering the segmentation as the hidden variable in machine translation. Unlike this model, the proposed approach is targeted at a general model, instead of one oriented to machine translation task. Sun and X</context>
</contexts>
<marker>Nakagawa, Uchimoto, 2007</marker>
<rawString>Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A hybrid approach to word segmentation and POS tagging. In Proceedings of ACL Demo and Poster Session, pages 217–220, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based?</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1808" citStr="Ng and Low, 2004" startWordPosition="253" endWordPosition="256">duction Word segmentation and part-of-speech (POS) tagging are two critical and necessary initial procedures with respect to the majority of high-level Chinese language processing tasks such as syntax parsing, information extraction and machine translation. The traditional way of segmentation and tagging is performed in a pipeline approach, first segmenting a sentence into words, and then assigning each word a POS tag. The pipeline approach is very simple to implement, but frequently causes error propagation, given that wrong segmentations in the earlier stage harm the subsequent POS tagging (Ng and Low, 2004). The joint approaches of word segmentation and POS tagging (joint S&amp;T) are proposed to resolve these two tasks simultaneously. They effectively alleviate the error propagation, because segmentation and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the out</context>
<context position="5321" citStr="Ng and Low, 2004" startWordPosition="789" endWordPosition="792">ection 3 reviews the background, including supervised character-based joint S&amp;T model based on CRFs and graph-based label propagation. Section 4 presents the details of the proposed approach. Section 5 reports the experiment results. The conclusion is drawn in Section 6. 2 Related Work Prior supervised joint S&amp;T models present approximate 0.2% - 1.3% improvement in F-score over supervised pipeline ones. The state-of-theart joint models include reranking approaches (Shi and Wang, 2007), hybrid approaches (Nakagawa and Uchimoto, 2007; Jiang et al., 2008; Sun, 2011), and single-model approaches (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). The proposed approach in this paper belongs to the single-model type. There are few explorations of semi-supervised approaches for CWS or POS tagging in previous works. Xu et al. (2008) described a Bayesian semi-supervised CWS model by considering the segmentation as the hidden variable in machine translation. Unlike this model, the proposed approach is targeted at a general model, instead of one oriented to machine translation task. Sun and Xu (2011) enhanced a CWS model by interpolating statistical features of unlabele</context>
<context position="7499" citStr="Ng and Low (2004)" startWordPosition="1129" endWordPosition="1132"> novel feature templates are defined for measuring the similarity between vertices. Second, the critical property, i.e., sparsity, is considered among label propagation. And third, the derived label information from the graph is smoothed into the model by optimizing a modified objective function. 3 Background 3.1 Supervised Character-based Model The character-based joint S&amp;T approach is operated as a sequence labeling fashion that each Chinese character, i.e., hanzi, in the sequence is assigned with a tag. To perform segmentation and tagging simultaneously in a uniform framework, according to Ng and Low (2004), the tag is composed of a word boundary part, and a POS part, e.g., “B NN” refers to the first character in a word with POS tag “NN”. In this paper, 4 word boundary tags are employed: B (beginning of a word), M (middle part of a word), E (end of a word) and S (single character). As for the POS tag, we shall use the 33 tags in the Chinese tree bank. Thus, the potential composite tags of joint S&amp;T consist of 132 (4x33) classes. The first-order CRFs model (Lafferty et al., 2001) has been the most common one in this task. Given a set of labeled examples Dl = {(xi, yi)}li=1, where xi = x1ix2 i ...</context>
<context position="27626" citStr="Ng and Low, 2004" startWordPosition="4535" endWordPosition="4538"> 58.36 Table 4: The performance of segmentation and POS tagging on testing data. Table 4 summarizes the performance of segmentation and POS tagging on the test data, in comparison with the other five models. Firstly, as expected, for the two supervised baselines, the joint model outperforms the pipeline one, especially on segmentation. It obtains 0.92% and 2.32% increase in terms of F-score and OOV-R respectively. This outcome verifies the commonly accepted fact that the joint model can substantially improve the pipeline one, since POS tags provide additional information to word segmentation (Ng and Low, 2004). Secondly, it is also noticed that all four semi-supervised models are able to benefit from unlabeled data and greatly improve the results with respect to the baselines. On the whole, for segmentation, they achieve average improvements of 1.02% and 6.8% in F-score and OOV-R; whereas for POS tagging, the average increments of F-sore and OOV-R are 0.87% and 6.45%. An interesting phenomenon is found among the comparisons with baselines that the supervised joint model (Baseline II) is even competitive with semisupervised pipeline one (Wang et al., 2011). This illustrates the effects of error prop</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low 2004. Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based? In Proceedings of EMNLP, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Joint Chinese Word Segmentation, POS Tagging and Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>501--511</pages>
<location>Jeju Island,</location>
<contexts>
<context position="2212" citStr="Qian and Liu, 2012" startWordPosition="312" endWordPosition="315">each word a POS tag. The pipeline approach is very simple to implement, but frequently causes error propagation, given that wrong segmentations in the earlier stage harm the subsequent POS tagging (Ng and Low, 2004). The joint approaches of word segmentation and POS tagging (joint S&amp;T) are proposed to resolve these two tasks simultaneously. They effectively alleviate the error propagation, because segmentation and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&amp;T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S</context>
</contexts>
<marker>Qian, Liu, 2012</marker>
<rawString>Xian Qian and Yang Liu. 2012. Joint Chinese Word Segmentation, POS Tagging and Parsing. In Proceedings of EMNLP-CoNLL, pages 501-511, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanxin Shi</author>
<author>Mengqiu Wang</author>
</authors>
<title>A dual-layer CRF based joint decoding method for cascade segmentation and labelling tasks.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="5194" citStr="Shi and Wang, 2007" startWordPosition="770" endWordPosition="773">OOV) recall. This paper is structured as follows: Section 2 points out the main differences with the related work of this study. Section 3 reviews the background, including supervised character-based joint S&amp;T model based on CRFs and graph-based label propagation. Section 4 presents the details of the proposed approach. Section 5 reports the experiment results. The conclusion is drawn in Section 6. 2 Related Work Prior supervised joint S&amp;T models present approximate 0.2% - 1.3% improvement in F-score over supervised pipeline ones. The state-of-theart joint models include reranking approaches (Shi and Wang, 2007), hybrid approaches (Nakagawa and Uchimoto, 2007; Jiang et al., 2008; Sun, 2011), and single-model approaches (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). The proposed approach in this paper belongs to the single-model type. There are few explorations of semi-supervised approaches for CWS or POS tagging in previous works. Xu et al. (2008) described a Bayesian semi-supervised CWS model by considering the segmentation as the hidden variable in machine translation. Unlike this model, the proposed approach is targeted at a general model, instead of one</context>
</contexts>
<marker>Shi, Wang, 2007</marker>
<rawString>Yanxin Shi and Mengqiu Wang. 2007. A dual-layer CRF based joint decoding method for cascade segmentation and labelling tasks. In Proceedings of IJCAI, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amarnag Subramanya</author>
<author>Slav Petrov</author>
<author>Fernando Pereira</author>
</authors>
<title>Efficient graph-based semisupervised learning of structured tagging models.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>167--176</pages>
<location>Massachusetts, USA.</location>
<contexts>
<context position="3127" citStr="Subramanya et al., 2010" startWordPosition="455" endWordPosition="458">, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&amp;T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&amp;T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&amp;T model. Graph-based label propagation methods have recently shown they can outperform the state-of-the-art in several natural language processing (NLP) tasks, e.g., POS tagging (Subramanya et al., 2010), knowledge acquisition (Talukdar et al., 2008), shallow semantic parsing for unknown predicate (Das and Smith, 2011). As far as we know, however, these methods have not yet been applied to resolve the problem of joint Chinese word segmentation (CWS) and POS tagging. Motivated by the works in (Subramanya et al., 2010; Das and Smith, 2011), for structured problems, graph-based label propagation can be employed to infer valuable syntactic information (ngram-level label distributions) from labeled data to unlabeled data. This study extends this intuition to construct a similarity graph for propag</context>
<context position="6654" citStr="Subramanya et al. (2010)" startWordPosition="1001" endWordPosition="1004">n-gram and lexicon features derived from unlabeled data. Different from their concern, our emphasis is to learn the semi-supervised model by injecting the label information from a similarity graph constructed from labeled and unlabeled data. The induction method of the proposed approach also differs from other semi-supervised CRFs algorithms. Jiao et al. (2006), extended by Mann and McCallum (2007), reported a semi-supervised CRFs model which aims to guide the learning by minimizing the conditional entropy of unlabeled data. The proposed approach regularizes the CRFs by the graph information. Subramanya et al. (2010) proposed a graph-based self-train style semi-supervised CRFs algorithm. In the proposed approach, an analogous way of graph construction intuition is applied. But overall, our approach differs in three important aspects: first, novel feature templates are defined for measuring the similarity between vertices. Second, the critical property, i.e., sparsity, is considered among label propagation. And third, the derived label information from the graph is smoothed into the model by optimizing a modified objective function. 3 Background 3.1 Supervised Character-based Model The character-based join</context>
<context position="10639" citStr="Subramanya et al., 2010" startWordPosition="1692" endWordPosition="1695">tices are connected by weighted edges which encode the degree to which they are expected to have the same label (Zhu et al., 2003). Popular graph construction methods include k-nearest neighbors (kNN) (Bentley, 1980; Beygelzimer et al., 2006), b-matching (Jebara et al., 2009) and local reconstruction (Daitch et al., 2009). Label propagation operates on the constructed graph. The primary objective is to propagate labels from a few labeled vertices to the entire graph by optimizing a loss function based on the constraints or properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdar and Crammer, 2009) and Sparse Inducing Penalties (Das and Smith, 2012). 4 Method The emphasis of this work is on building a joint S&amp;T model based on two different kinds of data sources, labeled and unlabeled data. In essence, this learning problem can be treated as incorporating certain gainful information, e.g., prior knowledge or label constraints, of unlabeled data into the supervised model. The proposed</context>
<context position="13646" citStr="Subramanya et al. (2010)" startWordPosition="2157" endWordPosition="2160">heavily on the quality of the graph. Graph construction thus plays a central role in graph-based label propagation (Zhu et al., 2003). For character-based joint S&amp;T, unlike the unstructured learning problem whose vertices are formed directly by labeled and unlabeled instances, the graph construction is non-trivial. Das and Petrov (2011) mentioned that taking individual characters as the vertices would result in various ambiguities, whereas the similarity measurement is still challenging if vertices corresponding to entire sentences. This study follows the intuitions of graph construction from Subramanya et al. (2010) in which vertices are represented by character trigrams occurring in labeled and unlabeled sentences. Formally, given a set of labeled sentences Dl, and unlabeled ones Du, where D °_ {Dl, Du}, the goal is to form an undirected weighted graph G = (V, E), where V is defined as the set of vertices which covers all trigrams extracted from Dl and Du. Here, V = Vl U Vu, where Vl refers to trigrams that occurs at least once in labeled sentences and Vu refers to trigrams that occur only in unlabeled sentences. The edges E E Vl x Vu, connect all the vertices. This study makes use of a symmetric k-NN g</context>
<context position="15981" citStr="Subramanya et al., 2010" startWordPosition="2561" endWordPosition="2565">rigram - Center Word x2x4 Left Word + Right Context x2x4x5 Right Word + Left Context x1x2x3 Type of Trigram: number, punctuation, alphabetic letter t(x2)t(x3)t(x4) and other Table 2: Features employed to measure the similarity between two vertices, in a given text “x1x2x3x4x5”, where the trigram is “x2x3x4”. The nature of the similarity graph enforces that the connected trigrams with high weight appearing in different texts should have similar syntax configurations. Thus, the constructed graph is expected to provide additional information that cannot be expressed directly in a sequence model (Subramanya et al., 2010). One primary benefit of this property is on enriching vocabulary coverage. In other words, the new features of various trigrams only occurring in unlabeled data can be discovered. As the excerpt in Figure 1 shows, the trigram “天津港” (Tianjin port) has no any label information, as it only occurs in unlabeled data, but fortunately its neighborhoods with similar syntax information, e.g., “上海港” (Shanghai port), “广州 港” (Guangzhou port), can assist to infer the correct tag “M NN”. 4.2 Label Propagation In order to induce trigram-level label distributions from the graph constructed by the previous st</context>
<context position="18493" citStr="Subramanya et al., 2010" startWordPosition="2992" endWordPosition="2995">ct to the graph, such that two vertices connected by an edge with high weight should be assigned similar labels. The final term is a regularizer to incorporate the prior knowledge, e.g., uniform distributions used in (Talukdar et al., 2008; Das and Smith, 2011). This study applies the squared norm of q to encourage sparsity per vertex. Note that the estimated label distribution 1It can be seen as a multi-class extension of quadratic cost criterion (Bengio et al., 2006) or as a variant of the objective in (Zhu et al., 2003). An entropic distance measure could also be used, e.g., KL-divergence (Subramanya et al., 2010; Das and Smith, 2012). qi in Equation (4) is relaxed to be unnormalized, which simplifies the optimization. Thus, the objective function can be optimized by L-BFGS-B (Zhu et al., 1997), a generic quasi-Newton gradientbased optimizer. The partial derivatives of Equation (4) are computed for each parameter of q and then passed on to the optimizer that updates them such that Equation (4) is maximized. 4.3 Semi-Supervised CRFs Training The trigram-level label distributions inferred in the propagation step can be viewed as a kind of valuable “prior knowledge” to regularize the learning on unlabele</context>
<context position="25102" citStr="Subramanya et al., 2010" startWordPosition="4110" endWordPosition="4113"> provides several state-of-the-art algorithms. Data #Sent #Word #Char #OOV Train 17,968 374,697 596,360 Develop 1,659 46,637 79,283 0.074 Test 2,037 65,219 104,502 0.089 Table 3: Training, development and testing data. 5.2 Baseline and Proposed Models In the experiment, the baseline supervised pipeline and joint S&amp;T models are built only on the train data. The proposed model will also be compared with the semi-supervised pipeline S&amp;T model described in (Wang et al., 2011). In addition, two state-of-the-art semi-supervised CRFs algorithms, Jiao’s CRFs (Jiao et al., 2006) and Subramanya’s CRFs (Subramanya et al., 2010), are also used to build joint S&amp;T models. The corresponding settings of the above candidates are listed below: • Baseline I: a supervised CRFs pipeline S&amp;T model. The feature templates are from Zhao et al. (2006) and Wu et al. (2008). • Wang’s model: a semi-supervised CRFs pipeline S&amp;T model. The same feature templates in (Wang et al., 2011) are used, i.e., 66 +nf+n- gram+cluster+lexicon”. • Baseline II: a supervised CRFs joint S&amp;T model. The feature templates introduced in Section 3.1 are used. • Jiao’s model: a semi-supervised CRFs joint S&amp;T model trained using the entropy regularization (E</context>
</contexts>
<marker>Subramanya, Petrov, Pereira, 2010</marker>
<rawString>Amarnag Subramanya, Slav Petrov, and Fernando Pereira. 2010. Efficient graph-based semisupervised learning of structured tagging models. In Proceedings of EMNLP, pages 167-176, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-ofSpeech Tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1385--1394</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="5274" citStr="Sun, 2011" startWordPosition="784" endWordPosition="785">es with the related work of this study. Section 3 reviews the background, including supervised character-based joint S&amp;T model based on CRFs and graph-based label propagation. Section 4 presents the details of the proposed approach. Section 5 reports the experiment results. The conclusion is drawn in Section 6. 2 Related Work Prior supervised joint S&amp;T models present approximate 0.2% - 1.3% improvement in F-score over supervised pipeline ones. The state-of-theart joint models include reranking approaches (Shi and Wang, 2007), hybrid approaches (Nakagawa and Uchimoto, 2007; Jiang et al., 2008; Sun, 2011), and single-model approaches (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). The proposed approach in this paper belongs to the single-model type. There are few explorations of semi-supervised approaches for CWS or POS tagging in previous works. Xu et al. (2008) described a Bayesian semi-supervised CWS model by considering the segmentation as the hidden variable in machine translation. Unlike this model, the proposed approach is targeted at a general model, instead of one oriented to machine translation task. Sun and Xu (2011) enhanced a CWS model by</context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Weiwei Sun. 2011. A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-ofSpeech Tagging. In Proceedings of ACL, pages 1385–1394, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing Chinese word segmentation using unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>970--979</pages>
<location>Scotland, UK.</location>
<contexts>
<context position="5850" citStr="Sun and Xu (2011)" startWordPosition="875" endWordPosition="878">oto, 2007; Jiang et al., 2008; Sun, 2011), and single-model approaches (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). The proposed approach in this paper belongs to the single-model type. There are few explorations of semi-supervised approaches for CWS or POS tagging in previous works. Xu et al. (2008) described a Bayesian semi-supervised CWS model by considering the segmentation as the hidden variable in machine translation. Unlike this model, the proposed approach is targeted at a general model, instead of one oriented to machine translation task. Sun and Xu (2011) enhanced a CWS model by interpolating statistical features of unlabeled data into the CRFs model. Wang et al. (2011) proposed a semisupervised pipeline S&amp;T model by incorporating n-gram and lexicon features derived from unlabeled data. Different from their concern, our emphasis is to learn the semi-supervised model by injecting the label information from a similarity graph constructed from labeled and unlabeled data. The induction method of the proposed approach also differs from other semi-supervised CRFs algorithms. Jiao et al. (2006), extended by Mann and McCallum (2007), reported a semi-s</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word segmentation using unlabeled data. In Proceedings of EMNLP, pages 970-979, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Joseph Reisinger</author>
<author>Marius Pasca</author>
<author>Deepak Ravichandran</author>
<author>Rahul Bhagat</author>
<author>Fernando Pereira</author>
</authors>
<title>Weakly Supervised Acquisition of Labeled Class Instances using Graph Random Walks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>582--590</pages>
<location>Hawaii, USA.</location>
<contexts>
<context position="3174" citStr="Talukdar et al., 2008" startWordPosition="462" endWordPosition="465">he production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&amp;T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&amp;T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&amp;T model. Graph-based label propagation methods have recently shown they can outperform the state-of-the-art in several natural language processing (NLP) tasks, e.g., POS tagging (Subramanya et al., 2010), knowledge acquisition (Talukdar et al., 2008), shallow semantic parsing for unknown predicate (Das and Smith, 2011). As far as we know, however, these methods have not yet been applied to resolve the problem of joint Chinese word segmentation (CWS) and POS tagging. Motivated by the works in (Subramanya et al., 2010; Das and Smith, 2011), for structured problems, graph-based label propagation can be employed to infer valuable syntactic information (ngram-level label distributions) from labeled data to unlabeled data. This study extends this intuition to construct a similarity graph for propagating trigram-level label distributions. The de</context>
<context position="10663" citStr="Talukdar et al., 2008" startWordPosition="1696" endWordPosition="1699">ighted edges which encode the degree to which they are expected to have the same label (Zhu et al., 2003). Popular graph construction methods include k-nearest neighbors (kNN) (Bentley, 1980; Beygelzimer et al., 2006), b-matching (Jebara et al., 2009) and local reconstruction (Daitch et al., 2009). Label propagation operates on the constructed graph. The primary objective is to propagate labels from a few labeled vertices to the entire graph by optimizing a loss function based on the constraints or properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdar and Crammer, 2009) and Sparse Inducing Penalties (Das and Smith, 2012). 4 Method The emphasis of this work is on building a joint S&amp;T model based on two different kinds of data sources, labeled and unlabeled data. In essence, this learning problem can be treated as incorporating certain gainful information, e.g., prior knowledge or label constraints, of unlabeled data into the supervised model. The proposed approach employs a tran</context>
<context position="18109" citStr="Talukdar et al., 2008" startWordPosition="2927" endWordPosition="2930">ers whose values are discussed in Section 5. The squaredloss criterion1 is used to formulate the objective function. The first term in Equation (4) is the seed match loss which penalizes the estimated label distributions qj, if they go too far away from the empirical labeled distributions rj. The second term is the edge smoothness loss that requires qi should be smooth with respect to the graph, such that two vertices connected by an edge with high weight should be assigned similar labels. The final term is a regularizer to incorporate the prior knowledge, e.g., uniform distributions used in (Talukdar et al., 2008; Das and Smith, 2011). This study applies the squared norm of q to encourage sparsity per vertex. Note that the estimated label distribution 1It can be seen as a multi-class extension of quadratic cost criterion (Bengio et al., 2006) or as a variant of the objective in (Zhu et al., 2003). An entropic distance measure could also be used, e.g., KL-divergence (Subramanya et al., 2010; Das and Smith, 2012). qi in Equation (4) is relaxed to be unnormalized, which simplifies the optimization. Thus, the objective function can be optimized by L-BFGS-B (Zhu et al., 1997), a generic quasi-Newton gradie</context>
</contexts>
<marker>Talukdar, Reisinger, Pasca, Ravichandran, Bhagat, Pereira, 2008</marker>
<rawString>Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca, Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira. 2008. Weakly Supervised Acquisition of Labeled Class Instances using Graph Random Walks. In Proceedings of EMNLP, pages 582-590, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Koby Crammer</author>
</authors>
<title>New Regularized Algorithms for Transductive Learning.</title>
<date>2009</date>
<booktitle>In Proceedings of ECML-PKDD,</booktitle>
<pages>442--457</pages>
<location>Bled, Slovenia.</location>
<contexts>
<context position="10847" citStr="Talukdar and Crammer, 2009" startWordPosition="1724" endWordPosition="1727">ntley, 1980; Beygelzimer et al., 2006), b-matching (Jebara et al., 2009) and local reconstruction (Daitch et al., 2009). Label propagation operates on the constructed graph. The primary objective is to propagate labels from a few labeled vertices to the entire graph by optimizing a loss function based on the constraints or properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdar and Crammer, 2009) and Sparse Inducing Penalties (Das and Smith, 2012). 4 Method The emphasis of this work is on building a joint S&amp;T model based on two different kinds of data sources, labeled and unlabeled data. In essence, this learning problem can be treated as incorporating certain gainful information, e.g., prior knowledge or label constraints, of unlabeled data into the supervised model. The proposed approach employs a transductive graph-based label propagation method to acquire such gainful information, i.e., label distributions from a similarity graph constructed over labeled and unlabeled data. Then, </context>
</contexts>
<marker>Talukdar, Crammer, 2009</marker>
<rawString>Partha Pratim Talukdar and Koby Crammer. 2009. New Regularized Algorithms for Transductive Learning. In Proceedings of ECML-PKDD, pages 442 - 457, Bled, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Fernando Pereira</author>
</authors>
<title>Experiments in graph-based semi-supervised learning methods for class-instance acquisition.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1473--1481</pages>
<location>Uppsala,</location>
<contexts>
<context position="24027" citStr="Talukdar and Pereira, 2010" startWordPosition="3938" endWordPosition="3941">ich contains 53,108 sentences with 2,418,690 characters. The performance measurement indicators for word segmentation and POS tagging (joint S&amp;T) are balance F-score, F = 2PR/(P+R), the harmonic mean of precision (P) and recall (R), and outof-vocabulary recall (OOV-R). For segmentation, a token is regarded to be correct if its boundaries match the ones of a word in the gold standard. For the POS tagging, it is correct only if both the boundaries and the POS tags are perfect matches. The experimental platform is implemented based on two toolkits: Mallet (McCallum and Kachites, 2002) and Junto (Talukdar and Pereira, 2010). Mallet is a java-based package for statistical natural language processing, which includes the CRFs implementation. Junto is a graph2It can be download at: www.sighan.org/bakeoff2005. 3There is a total of 33 POS tags in CTB-7. 4The extracted sentences in train, development and test set were assigned with the composite tags as described in Section 3.1. ∂Q(Λ) �= E fk (yit− ∂Λk t t−1 t 1, yi ,yi yti, xi, t). 775 based label propagation toolkit that provides several state-of-the-art algorithms. Data #Sent #Word #Char #OOV Train 17,968 374,697 596,360 Develop 1,659 46,637 79,283 0.074 Test 2,037 </context>
</contexts>
<marker>Talukdar, Pereira, 2010</marker>
<rawString>Partha Pratim Talukdar and Fernando Pereira. 2010. Experiments in graph-based semi-supervised learning methods for class-instance acquisition. In Proceedings of ACL, pages 1473-1481, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiou Wang</author>
<author>Jun’ichi Kazama</author>
<author>Yoshimasa Tsuruoka</author>
<author>Wenliang Chen</author>
<author>Yujie Zhang</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving Chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>309--317</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="5967" citStr="Wang et al. (2011)" startWordPosition="895" endWordPosition="898">engkrai et al., 2009; Zhang and Clark, 2010). The proposed approach in this paper belongs to the single-model type. There are few explorations of semi-supervised approaches for CWS or POS tagging in previous works. Xu et al. (2008) described a Bayesian semi-supervised CWS model by considering the segmentation as the hidden variable in machine translation. Unlike this model, the proposed approach is targeted at a general model, instead of one oriented to machine translation task. Sun and Xu (2011) enhanced a CWS model by interpolating statistical features of unlabeled data into the CRFs model. Wang et al. (2011) proposed a semisupervised pipeline S&amp;T model by incorporating n-gram and lexicon features derived from unlabeled data. Different from their concern, our emphasis is to learn the semi-supervised model by injecting the label information from a similarity graph constructed from labeled and unlabeled data. The induction method of the proposed approach also differs from other semi-supervised CRFs algorithms. Jiao et al. (2006), extended by Mann and McCallum (2007), reported a semi-supervised CRFs model which aims to guide the learning by minimizing the conditional entropy of unlabeled data. The pr</context>
<context position="24954" citStr="Wang et al., 2011" startWordPosition="4090" endWordPosition="4093">he composite tags as described in Section 3.1. ∂Q(Λ) �= E fk (yit− ∂Λk t t−1 t 1, yi ,yi yti, xi, t). 775 based label propagation toolkit that provides several state-of-the-art algorithms. Data #Sent #Word #Char #OOV Train 17,968 374,697 596,360 Develop 1,659 46,637 79,283 0.074 Test 2,037 65,219 104,502 0.089 Table 3: Training, development and testing data. 5.2 Baseline and Proposed Models In the experiment, the baseline supervised pipeline and joint S&amp;T models are built only on the train data. The proposed model will also be compared with the semi-supervised pipeline S&amp;T model described in (Wang et al., 2011). In addition, two state-of-the-art semi-supervised CRFs algorithms, Jiao’s CRFs (Jiao et al., 2006) and Subramanya’s CRFs (Subramanya et al., 2010), are also used to build joint S&amp;T models. The corresponding settings of the above candidates are listed below: • Baseline I: a supervised CRFs pipeline S&amp;T model. The feature templates are from Zhao et al. (2006) and Wu et al. (2008). • Wang’s model: a semi-supervised CRFs pipeline S&amp;T model. The same feature templates in (Wang et al., 2011) are used, i.e., 66 +nf+n- gram+cluster+lexicon”. • Baseline II: a supervised CRFs joint S&amp;T model. The feat</context>
<context position="28182" citStr="Wang et al., 2011" startWordPosition="4626" endWordPosition="4629">additional information to word segmentation (Ng and Low, 2004). Secondly, it is also noticed that all four semi-supervised models are able to benefit from unlabeled data and greatly improve the results with respect to the baselines. On the whole, for segmentation, they achieve average improvements of 1.02% and 6.8% in F-score and OOV-R; whereas for POS tagging, the average increments of F-sore and OOV-R are 0.87% and 6.45%. An interesting phenomenon is found among the comparisons with baselines that the supervised joint model (Baseline II) is even competitive with semisupervised pipeline one (Wang et al., 2011). This illustrates the effects of error propagation in the pipeline approach. Thirdly, in what concerns the semi-supervised approaches, the three joint S&amp;T models, i.e., Jiao’s, Subramanya’s and our model, are superior to the pipeline model, i.e., Wang’s 776 model. Moreover, the two graph-based approaches, i.e., Subramanya’s and our model, outperform the others. Most importantly, the boldface numbers in the last row illustrate that our model does achieve the best performance. Overall, for word segmentation, it obtains average improvements of 1.43% and 8.09% in F-score and OOV-R over others; fo</context>
</contexts>
<marker>Wang, Kazama, Tsuruoka, Chen, Zhang, Torisawa, 2011</marker>
<rawString>Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa. 2011. Improving Chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data. In Proceedings of IJCNLP, pages 309–317, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Chieh Wu Jie-Chi Yang</author>
<author>Yue-Shi Lee</author>
</authors>
<title>Description of the NCU Chinese Word Segmentation and Part-of-Speech Tagging for SIGHAN Bakeoff.</title>
<date>2008</date>
<booktitle>In Proceedings of the SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--166</pages>
<location>Hyderabad, India.</location>
<marker>Yang, Lee, 2008</marker>
<rawString>Yu-Chieh Wu Jie-Chi Yang, and Yue-Shi Lee. 2008. Description of the NCU Chinese Word Segmentation and Part-of-Speech Tagging for SIGHAN Bakeoff. In Proceedings of the SIGHAN Workshop on Chinese Language Processing, pages 161-166, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Hermann Ney</author>
</authors>
<title>Bayesian semi-supervised chinese word segmentation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1017--1024</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="5580" citStr="Xu et al. (2008)" startWordPosition="833" endWordPosition="836">n Section 6. 2 Related Work Prior supervised joint S&amp;T models present approximate 0.2% - 1.3% improvement in F-score over supervised pipeline ones. The state-of-theart joint models include reranking approaches (Shi and Wang, 2007), hybrid approaches (Nakagawa and Uchimoto, 2007; Jiang et al., 2008; Sun, 2011), and single-model approaches (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). The proposed approach in this paper belongs to the single-model type. There are few explorations of semi-supervised approaches for CWS or POS tagging in previous works. Xu et al. (2008) described a Bayesian semi-supervised CWS model by considering the segmentation as the hidden variable in machine translation. Unlike this model, the proposed approach is targeted at a general model, instead of one oriented to machine translation task. Sun and Xu (2011) enhanced a CWS model by interpolating statistical features of unlabeled data into the CRFs model. Wang et al. (2011) proposed a semisupervised pipeline S&amp;T model by incorporating n-gram and lexicon features derived from unlabeled data. Different from their concern, our emphasis is to learn the semi-supervised model by injecting</context>
</contexts>
<marker>Xu, Gao, Toutanova, Ney, 2008</marker>
<rawString>Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann Ney. 2008. Bayesian semi-supervised chinese word segmentation for statistical machine translation. In Proceedings of COLING, pages 1017-1024, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>888--896</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="2314" citStr="Zhang and Clark, 2008" startWordPosition="329" endWordPosition="332">propagation, given that wrong segmentations in the earlier stage harm the subsequent POS tagging (Ng and Low, 2004). The joint approaches of word segmentation and POS tagging (joint S&amp;T) are proposed to resolve these two tasks simultaneously. They effectively alleviate the error propagation, because segmentation and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&amp;T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&amp;T model. This study focuses on using a graph-based label propagation method to build a semi-supervise</context>
<context position="5344" citStr="Zhang and Clark, 2008" startWordPosition="793" endWordPosition="796">he background, including supervised character-based joint S&amp;T model based on CRFs and graph-based label propagation. Section 4 presents the details of the proposed approach. Section 5 reports the experiment results. The conclusion is drawn in Section 6. 2 Related Work Prior supervised joint S&amp;T models present approximate 0.2% - 1.3% improvement in F-score over supervised pipeline ones. The state-of-theart joint models include reranking approaches (Shi and Wang, 2007), hybrid approaches (Nakagawa and Uchimoto, 2007; Jiang et al., 2008; Sun, 2011), and single-model approaches (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). The proposed approach in this paper belongs to the single-model type. There are few explorations of semi-supervised approaches for CWS or POS tagging in previous works. Xu et al. (2008) described a Bayesian semi-supervised CWS model by considering the segmentation as the hidden variable in machine translation. Unlike this model, the proposed approach is targeted at a general model, instead of one oriented to machine translation task. Sun and Xu (2011) enhanced a CWS model by interpolating statistical features of unlabeled data into the CRFs mo</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Proceedings of EMNLP, pages 888-896, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and POS-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>843--852</pages>
<location>Massachusetts, USA.</location>
<contexts>
<context position="2358" citStr="Zhang and Clark, 2010" startWordPosition="337" endWordPosition="340"> in the earlier stage harm the subsequent POS tagging (Ng and Low, 2004). The joint approaches of word segmentation and POS tagging (joint S&amp;T) are proposed to resolve these two tasks simultaneously. They effectively alleviate the error propagation, because segmentation and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&amp;T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&amp;T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&amp;T model. Graph-based label propaga</context>
<context position="5393" citStr="Zhang and Clark, 2010" startWordPosition="801" endWordPosition="804">sed joint S&amp;T model based on CRFs and graph-based label propagation. Section 4 presents the details of the proposed approach. Section 5 reports the experiment results. The conclusion is drawn in Section 6. 2 Related Work Prior supervised joint S&amp;T models present approximate 0.2% - 1.3% improvement in F-score over supervised pipeline ones. The state-of-theart joint models include reranking approaches (Shi and Wang, 2007), hybrid approaches (Nakagawa and Uchimoto, 2007; Jiang et al., 2008; Sun, 2011), and single-model approaches (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). The proposed approach in this paper belongs to the single-model type. There are few explorations of semi-supervised approaches for CWS or POS tagging in previous works. Xu et al. (2008) described a Bayesian semi-supervised CWS model by considering the segmentation as the hidden variable in machine translation. Unlike this model, the proposed approach is targeted at a general model, instead of one oriented to machine translation task. Sun and Xu (2011) enhanced a CWS model by interpolating statistical features of unlabeled data into the CRFs model. Wang et al. (2011) proposed a semisupervised</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. In Proceedings of EMNLP, pages 843-852, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>Bao-Liang Lu</author>
</authors>
<title>Effective tag set selection in Chinese word segmentation via conditional random field modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of PACLIC,</booktitle>
<pages>87--94</pages>
<location>Wuhan, China.</location>
<contexts>
<context position="25315" citStr="Zhao et al. (2006)" startWordPosition="4147" endWordPosition="4150">5.2 Baseline and Proposed Models In the experiment, the baseline supervised pipeline and joint S&amp;T models are built only on the train data. The proposed model will also be compared with the semi-supervised pipeline S&amp;T model described in (Wang et al., 2011). In addition, two state-of-the-art semi-supervised CRFs algorithms, Jiao’s CRFs (Jiao et al., 2006) and Subramanya’s CRFs (Subramanya et al., 2010), are also used to build joint S&amp;T models. The corresponding settings of the above candidates are listed below: • Baseline I: a supervised CRFs pipeline S&amp;T model. The feature templates are from Zhao et al. (2006) and Wu et al. (2008). • Wang’s model: a semi-supervised CRFs pipeline S&amp;T model. The same feature templates in (Wang et al., 2011) are used, i.e., 66 +nf+n- gram+cluster+lexicon”. • Baseline II: a supervised CRFs joint S&amp;T model. The feature templates introduced in Section 3.1 are used. • Jiao’s model: a semi-supervised CRFs joint S&amp;T model trained using the entropy regularization (ER) criteria (Jiao et al., 2006). The optimization method proposed by Mann and McCallum (2007) is applied. • Subramanya’s model: a self-train style semi-supervised CRFs joint S&amp;T model based on the same parameters </context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu. 2006. Effective tag set selection in Chinese word segmentation via conditional random field modeling. In Proceedings of PACLIC, pages 87-94, Wuhan, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>912--919</pages>
<location>Washington DC, USA.</location>
<contexts>
<context position="10146" citStr="Zhu et al., 2003" startWordPosition="1611" endWordPosition="1614">hods (Chapelle et al., 2006). Most of these algorithms are transductive in nature, so they cannot be used to predict an unseen test example in the future (Belkin et al., 2006). Typically, graph-based label propagation algorithms are run in two main steps: graph construction and label propagation. The graph construction provides a natural way to represent data in a variety of target domains. One constructs a graph whose vertices consist of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges which encode the degree to which they are expected to have the same label (Zhu et al., 2003). Popular graph construction methods include k-nearest neighbors (kNN) (Bentley, 1980; Beygelzimer et al., 2006), b-matching (Jebara et al., 2009) and local reconstruction (Daitch et al., 2009). Label propagation operates on the constructed graph. The primary objective is to propagate labels from a few labeled vertices to the entire graph by optimizing a loss function based on the constraints or properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms </context>
<context position="13155" citStr="Zhu et al., 2003" startWordPosition="2086" endWordPosition="2089">). Subsequently, 772 the label propagation procedure (propagate label) is conducted for projecting label distributions q from labeled vertices to the entire graph, using the algorithm of Sparse-Inducing Penalties (Das and Smith, 2012) (Section 4.2). The final step (train crf) of the induction is incorporating the inferred trigram-level label distributions q into CRFs model (Section 4.3). 4.1 Graph Construction In most graph-based label propagation tasks, the final effect depends heavily on the quality of the graph. Graph construction thus plays a central role in graph-based label propagation (Zhu et al., 2003). For character-based joint S&amp;T, unlike the unstructured learning problem whose vertices are formed directly by labeled and unlabeled instances, the graph construction is non-trivial. Das and Petrov (2011) mentioned that taking individual characters as the vertices would result in various ambiguities, whereas the similarity measurement is still challenging if vertices corresponding to entire sentences. This study follows the intuitions of graph construction from Subramanya et al. (2010) in which vertices are represented by character trigrams occurring in labeled and unlabeled sentences. Formal</context>
<context position="18398" citStr="Zhu et al., 2003" startWordPosition="2978" endWordPosition="2981"> The second term is the edge smoothness loss that requires qi should be smooth with respect to the graph, such that two vertices connected by an edge with high weight should be assigned similar labels. The final term is a regularizer to incorporate the prior knowledge, e.g., uniform distributions used in (Talukdar et al., 2008; Das and Smith, 2011). This study applies the squared norm of q to encourage sparsity per vertex. Note that the estimated label distribution 1It can be seen as a multi-class extension of quadratic cost criterion (Bengio et al., 2006) or as a variant of the objective in (Zhu et al., 2003). An entropic distance measure could also be used, e.g., KL-divergence (Subramanya et al., 2010; Das and Smith, 2012). qi in Equation (4) is relaxed to be unnormalized, which simplifies the optimization. Thus, the objective function can be optimized by L-BFGS-B (Zhu et al., 1997), a generic quasi-Newton gradientbased optimizer. The partial derivatives of Equation (4) are computed for each parameter of q and then passed on to the optimizer that updates them such that Equation (4) is maximized. 4.3 Semi-Supervised CRFs Training The trigram-level label distributions inferred in the propagation st</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 2003. Semi-supervised learning using Gaussian fields and harmonic functions. In Proceedings of ICML, pages 912–919, Washington DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciyou Zhu</author>
<author>Richard H Byrd</author>
<author>Peihuang Lu</author>
<author>Jorge Nocedal</author>
</authors>
<title>L-BFGS-B: Fortran subroutines for large scale bound constrained optimization.</title>
<date>1997</date>
<journal>ACM Transactions on Mathematical Software,</journal>
<pages>23--550</pages>
<contexts>
<context position="18678" citStr="Zhu et al., 1997" startWordPosition="3023" endWordPosition="3026">orm distributions used in (Talukdar et al., 2008; Das and Smith, 2011). This study applies the squared norm of q to encourage sparsity per vertex. Note that the estimated label distribution 1It can be seen as a multi-class extension of quadratic cost criterion (Bengio et al., 2006) or as a variant of the objective in (Zhu et al., 2003). An entropic distance measure could also be used, e.g., KL-divergence (Subramanya et al., 2010; Das and Smith, 2012). qi in Equation (4) is relaxed to be unnormalized, which simplifies the optimization. Thus, the objective function can be optimized by L-BFGS-B (Zhu et al., 1997), a generic quasi-Newton gradientbased optimizer. The partial derivatives of Equation (4) are computed for each parameter of q and then passed on to the optimizer that updates them such that Equation (4) is maximized. 4.3 Semi-Supervised CRFs Training The trigram-level label distributions inferred in the propagation step can be viewed as a kind of valuable “prior knowledge” to regularize the learning on unlabeled data. The final step of the induction is thus to incorporate such prior knowledge into CRFs. Li (2009) generalizes the use of virtual evidence to undirected graphical models and, in p</context>
</contexts>
<marker>Zhu, Byrd, Lu, Nocedal, 1997</marker>
<rawString>Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge Nocedal. 1997. L-BFGS-B: Fortran subroutines for large scale bound constrained optimization. ACM Transactions on Mathematical Software, 23:550-560.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>