<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000398">
<title confidence="0.9958855">
Bootstrapping Named Entity Recognition
with Automatically Generated Gazetteer Lists
</title>
<author confidence="0.986062">
Zornitsa Kozareva
</author>
<affiliation confidence="0.9715795">
Dept. de Lenguajes y Sistemas Inform´aticos
University of Alicante
</affiliation>
<address confidence="0.699675">
Alicante, Spain
</address>
<email confidence="0.99729">
zkozareva@dlsi.ua.es
</email>
<sectionHeader confidence="0.983028" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9970425">
Current Named Entity Recognition sys-
tems suffer from the lack of hand-tagged
data as well as degradation when mov-
ing to other domain. This paper explores
two aspects: the automatic generation of
gazetteer lists from unlabeled data; and the
building of a Named Entity Recognition
system with labeled and unlabeled data.
</bodyText>
<sectionHeader confidence="0.995502" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999687625">
Automatic information extraction and information
retrieval concerning particular person, location,
organization, title of movie or book, juxtaposes to
the Named Entity Recognition (NER) task. NER
consists in detecting the most silent and informa-
tive elements in a text such as names of people,
company names, location, monetary currencies,
dates. Early NER systems (Fisher et al., 1997),
(Black et al., 1998) etc., participating in Message
Understanding Conferences (MUC), used linguis-
tic tools and gazetteer lists. However these are dif-
ficult to develop and domain sensitive.
To surmount these obstacles, application of
machine learning approaches to NER became a
research subject. Various state-of-the-art ma-
chine learning algorithms such as Maximum En-
tropy (Borthwick, 1999), AdaBoost(Carreras et
al., 2002), Hidden Markov Models (Bikel et al., ),
Memory-based Based learning (Tjong Kim Sang,
2002b), have been used1. (Klein et al., 2003),
(Mayfield et al., 2003), (Wu et al., 2003),
(Kozareva et al., 2005c) among others, combined
several classifiers to obtain better named entity
coverage rate.
</bodyText>
<footnote confidence="0.997098666666667">
1For other machine learning methods, consult
http://www.cnts.ua.ac.be/conll2002/ner/
http://www.cnts.ua.ac.be/conll2003/ner/
</footnote>
<bodyText confidence="0.998515774193548">
Nevertheless all these machine learning algo-
rithms rely on previously hand-labeled training
data. Obtaining such data is labor-intensive, time
consuming and even might not be present for lan-
guages with limited funding. Resource limitation,
directed NER research (Collins and Singer, 1999),
(Carreras et al., 2003), (Kozareva et al., 2005a)
toward the usage of semi-supervised techniques.
These techniques are needed, as we live in a multi-
lingual society and access to information from var-
ious language sources is reality. The development
of NER systems for languages other than English
commenced.
This paper presents the development of a Span-
ish Named Recognition system based on machine
learning approach. For it no morphologic or syn-
tactic information was used. However, we pro-
pose and incorporate a very simple method for
automatic gazetteer2 construction. Such method
can be easily adapted to other languages and it is
low-costly obtained as it relies on n-gram extrac-
tion from unlabeled data. We compare the perfor-
mance of our NER system when labeled and unla-
beled training data is present.
The paper is organized in the following way:
brief explanation about NER process is repre-
sented in Section 2. In Section 3 follows feature
extraction. The experimental evaluation for the
Named Entity detection and classification tasks
with and without labeled data are in Sections 4 and
5. We conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.805024" genericHeader="method">
2 The NER how to
</sectionHeader>
<bodyText confidence="0.9033">
A Named Entity Recognition task can be de-
scribed as composition of two subtasks, entity de-
</bodyText>
<footnote confidence="0.970204">
2specialized lists of names for location and person names,
e.g. Madrid is in the location gazetteer, Mary is in the person
gazetteer
</footnote>
<page confidence="0.997588">
15
</page>
<bodyText confidence="0.999758822222222">
tection and entity classification. Entity delimita-
tion consist in determining the boundaries of the
entity (e.g. the place from where it starts and the
place it finishes). This is important for tracing
entities composed of two or more words such as
”Presidente de los Estados Unidos ”3, ”Universi-
dad Politecnica de Catalu˜na”4. For this purpose,
the BIO scheme was incorporated. In this scheme,
tag B denotes the start of an entity, tag I continues
the entity and tag O marks words that do not form
part of an entity. This scheme was initially intro-
duced in CoNLL’s (Tjong Kim Sang, 2002a) and
(Tjong Kim Sang and De Meulder, 2003) NER
competitions, and we decided to adapt it for our
experimental work.
Once all entities in the text are detected, they
are passed for classification in a predefined set of
categories such as location, person, organization
or miscellaneous5 names. This task is known as
entity classification. The final NER performance
is measured considering the entity detection and
classification tasks together.
Our NER approach is based on machine learn-
ing. The two algorithms we used for the experi-
ments were instance-based and decision trees, im-
plemented by (Daelemans et al., 2003). They were
used with their default parameter settings. We
selected the instance-based model, because it is
known to be useful when the amount of training
data is not sufficient.
Important part in the NE process takes the lo-
cation and person gazetteer lists which were au-
tomatically extracted from unlabeled data. More
detailed explanation about their generation can be
found in Section 3.
To explore the effect of labeled and unlabeled
training data to our NER, two types of experiments
were conducted. For the supervised approach, the
labels in the training data were previously known.
For the semi-supervised approach, the labels in the
training data were hidden. We used bootstrapping
(Abney, 2002) which refers to a problem setting
in which one is given a small set of labeled data
and a large set of unlabeled data, and the task is to
induce a classifier.
</bodyText>
<listItem confidence="0.938702333333333">
• Goals:
- utilize a minimal amount of supervised ex-
amples;
</listItem>
<footnote confidence="0.980685">
3”President of the United States”
4”Technical University of Catalu˜na”
5book titles, sport events, etc.
</footnote>
<listItem confidence="0.906325166666667">
- obtain learning from many unlabeled ex-
amples;
• General scheme:
- initial supervision seed examples for train-
ing an initial model;
- corpus classification with seed model;
</listItem>
<bodyText confidence="0.952602111111111">
- add most confident classifications to train-
ing data and iterate.
In our bootstrapping, a newly labeled example
was added into the training data L, if the two clas-
sifiers C1 and C2 agreed on the class of that ex-
ample. The number n of iterations for our ex-
periments is set up to 25 and when this bound is
reached the bootstrapping stops. The scheme we
follow is described below.
</bodyText>
<listItem confidence="0.990895">
1. for iteration = 0 ... n do
2. pool 1000 examples from unlabeled data;
3. annotate all 1000 examples with classifier C1
and C2;
4. for each of the 1000 examples compare
classes of C1 and C2;
5. add example into L only if classes of C1 and
C2 agree;
6. train model with L;
7. calculate result
8. end for
</listItem>
<bodyText confidence="0.999635375">
Bootstrapping was previously used by (Carreras
et al., 2003), who were interested in recognizing
Catalan names using Spanish resources. (Becker
et al., 2005) employed bootstrapping in an ac-
tive learning method for tagging entities in an as-
tronomic domain. (Yarowsky, 1995) and (Mi-
halcea and Moldovan, 2001) utilized bootstrap-
ping for word sense disambiguation. (Collins and
Singer, 1999) classified NEs through co-training,
(Kozareva et al., 2005a) used self-training and co-
training to detect and classify named entities in
news domain, (Shen et al., 2004) conducted ex-
periments with multi-criteria-based active learning
for biomedical NER.
The experimental data we work with is taken
from the CoNLL-2002 competition. The Spanish
</bodyText>
<page confidence="0.983214">
16
</page>
<bodyText confidence="0.99964075">
corpus6 comes from news domain and was previ-
ously manually annotated. The train data set con-
tains 264715 words of which 18798 are entities
and the test set has 51533 words of which 3558
are entities.
We decided to work with available NE anno-
tated corpora in order to conduct an exhaustive and
comparative NER study when labeled and unla-
beld data is present. For our bootstrapping experi-
ment, we simply ignored the presence of the labels
in the training data. Of course this approach can be
applied to other domain or language, the only need
is labeled test data to conduct correct evaluation.
The evaluation is computed per NE class by the
help of conlleval7 script. The evaluation measures
are:
</bodyText>
<equation confidence="0.800500833333333">
Precision = number of correct answers found by the system (1)
number of answers given by the system
Recall = number of correct answers found by the system (2)
number of correct answers in the test corpus
2 × Precision × Recall
Fβ=1 = Precision + Recall (3)
</equation>
<sectionHeader confidence="0.867059" genericHeader="method">
3 Feature extraction
</sectionHeader>
<bodyText confidence="0.960821130434782">
Recently diverse machine learning techniques are
utilized to resolve various NLP tasks. For all of
them crucial role plays the feature extraction and
selection module, which leads to optimal classifier
performance. This section describes the features
used for our Named Entity Recognition task.
Feature vectors Oz={f1,...,fn} are constructed.
The total number of features is denoted by n, and
Oz corresponds to the number of examples in the
data. In our experiment features represent contex-
tual, lexical and gazetteer information. Here we
number each feature and its corresponding argu-
ment.
f1: all letters of w08 are in capitals;
f2-fs: w−3, w−2, w−1, w0, w+1, w+2, w+3 ini-
tiate in capitals;
fg: position of w0 in the current sentence;
f10: frequency of w0;
f11-f17: word forms of w0 and the words in
[−3,+3] window;
f1s: first word making up the entity;
f1g: second word making up the entity, if
present;
</bodyText>
<footnote confidence="0.999415">
6http://www.cnts.ua.ac.be/conll2002/ner/data/
7http://www.cnts.ua.ac.be/conll2002/ner/bin/
8w0 indicates the word to be classified.
</footnote>
<listItem confidence="0.66887">
f20: w−1 is trigger word for location, person or
organization;
f21: w+1 is trigger word for location, person or
organization;
</listItem>
<bodyText confidence="0.985702255319149">
f22: w0 belongs to location gazetteer list;
f23: w0 belongs to first person name gazetteer
list;
f24: w0 belongs to family name gazetteer list;
f25: 0 if the majority of the words in an entity
are locations, 1 if the majority of the words in an
entity are persons and 2 otherwise.
Features f22, f23, f24 were automatically ex-
tracted by a simple pattern validation method we
propose below.
The corpus from where the gazetteer lists were
extracted, forms part of Efe94 and Efe95 Spanish
corpora provided for the CLEF9 competitions. We
conducted a simple preprocessing, where all sgml
documents were merged in a single file and only
the content situated among the text tags was ex-
tracted and considered for further processing. As
a result, we obtained 1 Gigabyte of unlabeled data,
containing 173468453 words. The text was tok-
enized and the frequency of all unigrams in the
corpus was gathered.
The algorithm we propose and use to obtain
location and person gazetteer lists is very simple.
It consists in finding and validating common pat-
terns, which can be constructed and utilized also
for languages other than Spanish.
The location pattern (prepz, wj), looks for
preposition i which indicates location in the Span-
ish language and all corresponding right capital-
ized context words wj for preposition i. The de-
pendency relation between prepz and wj, con-
veys the semantic information on the selection re-
strictions imposed by the two related words. In
a walk through example the pattern (en, *), ex-
tracts all right capitalized context words wj as
{Argentina, Barcelona, Madrid, Valencia} placed
next to preposition ”en”. These words are taken
as location candidates. The selection restriction
implies searching for words appearing after the
preposition ”en” (e.g. en Madrid) and not before
the preposition (e.g. Madrid en).
The termination of the pattern extraction (en,*),
initiates the extraction phase for the next preposi-
tions in prepz = {en, En, desde, Desde, hacia, Ha-
cia}. This processes is repeated until the complete
set of words in the preposition set are validated.
Table 1 represents the number of entities extracted
</bodyText>
<footnote confidence="0.916562">
9http://www.clef-campaign.org/
</footnote>
<page confidence="0.989865">
17
</page>
<table confidence="0.954168">
by each one of the preposition patterns.
pi en En desde Desde hacia Hacia
wj 15567 2381 1773 320 1336 134
</table>
<tableCaption confidence="0.999787">
Table 1: Extracted entities
</tableCaption>
<bodyText confidence="0.999433325">
The extracted capitalized words are passed
through a filtering process. Bigrams ”prepi
Capitalized wordj” with frequency lower than
20 were automatically discarded, because we
saw that this threshold removes words that do
not tend to appear very often with the lo-
cation prepositions. In this way misspelled
words as Bacelona instead of Barcelona were
filtered. From another side, every capitalized
word composed of two or three characters, for
instance ”La, Las” was initiated in a trigram
(prepi,Capitalized wordj, Capitalized wordj+1) val-
idation pattern. If these words were seen in com-
bination with other capitalized words and their tri-
gram frequency was higher then 20 they were in-
cluded in the location gazetteer file. With this tri-
gram validation pattern, locations as ”Los Ange-
les”, ”Las Palmas”, ”La Coru˜na” ,”Nueva York”10
were extracted.
In total 16819 entities with no repetition were
automatically obtained. The words represent
countries around the world, European capitals and
mostly Spanish cities. Some noisy elements found
in the file were person names, which were accom-
panied by the preposition ”en”. As person names
were capitalized and had frequency higher than the
threshold we placed, it was impossible for these
names to be automatically detected as erroneous
and filtered. However we left these names, since
the gazetteer attributes we maintain are mutually
nonexclusive. This means the name ”Jordan” can
be seen in location gazetteer indicating the coun-
try Jordan and in the same time can be seen in the
person name list indicating the person Jordan. In
a real NE application such case is reality, but for
the determination of the right category name en-
tity disambiguation is needed as in (Pedersen et
al., 2005).
Person gazetteer is constructed with graph ex-
ploration algorithm. The graph consists of:
</bodyText>
<listItem confidence="0.842417">
1. two kinds of nodes:
• First Names
• Family Names
10New York
2. undirected connections between First Names
and Family Names.
</listItem>
<bodyText confidence="0.995261314285714">
The graph connects Family Names with First
Names, and vice versa. In practice, such a graph is
not necessarily connected, as there can be unusual
first names and surnames which have no relation
with other names in the corpus. Though, the cor-
pus is supposed to contain mostly common names
in one and the same language, names from other
languages might be present too. In this case, if
the foreign name is not connected with a Spanish
name, it will never be included in the name list.
Therefore, starting from some common Span-
ish name will very probably place us in the largest
connected component11. If there exist other differ-
ent connected components in the graph, these will
be outliers, corresponding to names pertaining to
some other language, or combinations of both very
unusual first name and family name. The larger
the corpus is, the smaller the presence of such ad-
ditional connected components will be.
The algorithm performs an uninformed breadth-
first search. As the graph is not a tree, the stop
condition occurs when no more nodes are found.
Nodes and connections are found following the
pattern (First name, Family name). The node
from which we start the search can be a common
Spanish first or family name. In our example we
started from the Spanish common first name Jos´e.
The notation (i, j) E C refers to finding in the
corpus C the regular expression12
[A-Z][a-z]* [A-Z][a-z]*
This regular expression indicates a possible rela-
tion between first name and family name. The
scheme of the algorithm is the following:
Let C be the corpus, F be the set of first names,
and S be the set of family names.
</bodyText>
<figure confidence="0.334146833333333">
1. F = {”Jos´e”}
2. biEFdo f
Snew = Snew U {j } , bj  |(i, j) E C
3. S = S U Snew
4. bj E S do
Fnew = Fnew U {i} ,bi  |(i, j) E C
</figure>
<footnote confidence="0.4537714">
11A connected component refers to a maximal connected
subgraph, in graph theory. A connected graph, is a graph
containing only one connected component.
12For Spanish some other characters have to be added to
the regular expression, such as n˜ and accents.
</footnote>
<page confidence="0.998568">
18
</page>
<figureCaption confidence="0.99794">
Figure 1: An example of connected components.
</figureCaption>
<listItem confidence="0.96061475">
5. F=FUFnew
6. if (Fnew =�0) ∨ (Snew =�0)
then goto 2.
else finish.
</listItem>
<bodyText confidence="0.999683111111111">
Suppose we have a corpus containing the fol-
lowing person names: {”Jos´e Garcfa”, ”Jos´e
Mart´ınez”, ”Manolo Garcfa”, ”Marfa Mart´ınez”,
”Marfa Fern´andez”, ”John Lennon”} C C.
Initially we have F = {”Jos´e”} and S = 0. Af-
ter the 3rd step we would have S = {”Garcfa”,
”Martfnez”}, and after the 5th step: F = {”Jos´e”,
”Manolo”, ”Marfa”}. During the next iteration
”Fern´andez” would also be added to S, as ”Marfa”
is already present in F. Neither ”John”, nor
”Lennon” are connected to the rest of the names,
so these will never be added to the sets. This can
be seen in Figure 1 as well.
In our implementation, we filtered relations ap-
pearing less than 10 times. Thus rare combina-
tions like ”Jose Madrid, Mercedes Benz” are fil-
tered. Noise was introduced from names related to
both person and organization names. For example
the Spanish girl name Mercedes, lead to the node
Benz, and as ”Mercedes Benz” refers also to the
car producing company, noisy elements started to
be added through the node ”Benz”. In total 13713
fist names and 103008 surnames have been auto-
matically extracted.
We believe and prove that constructing auto-
matic location and person name gazetteer lists
with the pattern search and validation model we
propose is a very easy and practical task. With
our approach thousands of names can be obtained,
especially given the ample presence of unlabeled
data and the World Wide Web.
The purpose of our gazetteer construction was
not to make complete gazetteer lists, but rather
generate in a quick and automatic way lists of
names that can help during our feature construc-
tion module.
</bodyText>
<sectionHeader confidence="0.995943" genericHeader="method">
4 Experiments for delimitation process
</sectionHeader>
<bodyText confidence="0.996534363636364">
In this section we describe the conducted exper-
iments for named entity detection. Previously
(Kozareva et al., 2005b) demonstrated that in su-
pervised learning only superficial features as con-
text and ortografics are sufficient to identify the
boundaries of a Named Entity. In our experiment
the superficial features fi ÷ fio were used by the
supervised and semi-supervised classifiers. Table
2 shows the obtained results for Begin and Inside
tags, which actually detect the entities and the total
BIO tag performance.
</bodyText>
<table confidence="0.983678">
experiment B I BIO
Supervised 94.40 85.74 91.88
Bootstrapped 87.47 68.95 81.62
</table>
<tableCaption confidence="0.996824">
Table 2: F-score of detected entities.
</tableCaption>
<bodyText confidence="0.99992964">
On the first row are the results of the super-
vised method and on the second row are the high-
est results of the bootstrapping achieved in its
seventeenth iteration. For the supervised learn-
ing 91.88% of the entity boundaries were cor-
rectly identified and for the bootstrapping 81.62%
were correctly detected. The lower performance
of bootstrapping is due to the noise introduced dur-
ing the learning. Some examples were learned
with the wrong class and others didn’t introduce
new information in the training data.
Figure 2 presents the learning curve of the boot-
strapping processes for 25 iterations. On each it-
eration 1000 examples were tagged, but only the
examples having classes that coincide by the two
classifiers were later included in the training data.
We should note that for each iteration the same
amount of B, I and O classes was included. Thus
the balance among the three different classes in the
training data is maintained.
According to z� statistics (Dietterich, 1998),
the highest score reached by bootstrapping can-
not outperform the supervised method, however if
both methods were evaluated on small amount of
data the results were similar.
</bodyText>
<figure confidence="0.96966105">
Connected
Component
Connected
Component
Manolo
Jose
Maria
John Lennon
Garcia
Martinez
Fernandez
19
85
80
75
70
65
600 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
iterations
f−score
</figure>
<figureCaption confidence="0.998216">
Figure 2: Bootstrapping performance
</figureCaption>
<sectionHeader confidence="0.987057" genericHeader="method">
5 Experiments for classification process
</sectionHeader>
<bodyText confidence="0.9977752">
In a Named Entity classification process, to the
previously detected Named Entities a predefined
category of interest such as name of person, orga-
nization, location or miscellaneous names should
be assigned. To obtain a better idea of the perfor-
mance of the classification methods, several exper-
iments were conducted. The influence of the au-
tomatically extracted gazetteers was studied, and a
comparison of the supervised and semi-supervised
methods was done.
</bodyText>
<table confidence="0.944383">
experiment PER LOC ORG MISC
NoGazetteerSup. 80.98 71.66 73.72 49.94
GazetteerSup. 84.32 75.06 77.83 53.98
Bootstrapped 62.59 51.19 50.18 33.04
</table>
<tableCaption confidence="0.995876">
Table 3: F-score of classified entities.
</tableCaption>
<bodyText confidence="0.999328">
Table 3 shows the obtained results for each one
of the experimental settings. The first row indi-
cates the performance of the supervised classifier
when no gazetteer information is present. The
classifier used f1, f2, f3, f4, f5, f6, f7, f8, f18,
f1g, f20, f21 attributes. The performance of the
second row concerns the same classifier, but in-
cluding the gazetteer information by adding f22,
f23, f24 and f25 attributes. The third row relates to
the bootstrapping process. The attributes used for
the supervised and semi-supervised learning were
the same.
Results show that among all classes, miscella-
neous is the one with the lowest performance. This
is related to the heterogeneous information of the
category. The other three categories performed
above 70%. As expected gazetteer information
contributed for better distinction of person and lo-
cation names. Organization names benefitted from
the contextual information, the organization trig-
ger words and the attribute validating if an entity
is not a person or location then is treated as an
organization. Bootstrapping performance was not
high, due to the previously 81% correctly detected
named entity boundaries and from another side to
the training examples which were incorrectly clas-
sified and included into the training data.
In our experiment, unlabeled data was used to
construct in an easy and effective way person and
location gazetteer lists. By their help supervised
and semi-supervised classifiers improved perfor-
mance. Although one semi-supervised method
cannot reach the performance of a supervised clas-
sifier, we can say that results are promising. We
call them promising in the aspect of constructing
NE recognizer for languages with no resources or
even adapting the present Spanish Named Entity
system to other domain.
</bodyText>
<sectionHeader confidence="0.987845" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.996997037037037">
In this paper we proposed and implemented a
pattern validation search in an unlabeled corpus
though which gazetteer lists were automatically
generated. The gazetteers were used as features
by a Named Entity Recognition system. The per-
formance of this NER system, when labeled and
unlabeled training data was available, was mea-
sured. A comparative study for the information
contributed by the gazetteers in the entity classifi-
cation process was shown.
In the future we intend to develop automatic
gazetteers for organization and product names. It
is also of interest to divide location gazetteers in
subcategories as countries, cities, rivers, moun-
tains as they are useful for Geographic Informa-
tion Retrieval systems. To explore the behavior
of named entity bootstrapping, other domains as
bioinformatics will be explored.
Acknowledgements Many thanks to the three
anonymous reviewers for their useful comments
and suggestions.
This research has been partially funded by the
Spanish Government under project CICyT number
TIC2003-0664-C02-02 and PROFIT number FIT-
340100-2004-14 and by the Valencia Government
under project numbers GV04B-276 and GV04B-
268.
</bodyText>
<page confidence="0.991082">
20
</page>
<sectionHeader confidence="0.976505" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999827215686274">
Steven P. Abney. 2002. Bootstrapping. In Proceedings
of Association of Computational Linguists, pages
360–367.
Markus Becker, Ben Hachey, Beatrice Alex, and Claire
Grover. 2005. Optimising selective sampling for
bootstrapping named entity recognition. In Pro-
ceedings of the Workshop on Learning with Multiple
View, ICML, pages 5–10. Bonn, Germany.
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. Nymble: a high-performance
learning name-finder. In Proceedings of Conference
on Applied Natural Language Processing.
William J Black, Fabio Rinaldi, and David Mowatt.
1998. Facile: Description of the ne system used for
muc-7. In Proceedings of MUC-7.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University., September.
Xavier Carreras, Llu´ıs M`arques, and Llu´ıs Padr´o.
2002. Named entity extraction using adaboost.
In Proceedings of CoNLL-2002, pages 167–170.
Taipei, Taiwan.
Xavier Carreras, Llu´ıs M`arquez, and Llu´ıs Padr´o.
2003. Named entity recognition for catalan us-
ing only spanish resources and unlabelled data. In
EACL, pages 43–50.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In Pro-
ceedings of the Joint SIGDAT Conference on Empir-
ical Methods in Natural Language Processing and
Very Large Corpora.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2003. Timbl: Tilburg
memory-based learner. Technical Report ILK 03-
10, Tilburg University, November.
Thomas G. Dietterich. 1998. Approximate statistical
test for comparing supervised classification learning
algorithms. Neural Computation, 10(7):1895–1923.
David Fisher, Stephen Soderland, Joseph McCarthy,
Fangfang Feng, and Wendy Lehnert. 1997. De-
scription of the umass system as used for muc-6. In
Proceedings of MUC-6.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Walter Daelemans
and Miles Osborne, editors, Proceedings of CoNLL-
2003, pages 180–183. Edmonton, Canada.
Zornitsa Kozareva, Boyan Bonev, and Andres Mon-
toyo. 2005a. Self-training and co-training for span-
ish named entity recognition. In 4th Mexican Inter-
national Conference on Artificial Intelligence, pages
770–780.
Zornitsa Kozareva, Oscar Ferr´andez, Andres Montoyo,
and Rafael Mu˜noz. 2005b. Using language re-
source independent detection for spanish named en-
tity recognition. In Proceedings of the Conference
on Recent Advances in Natural Language Process-
ing (RANLP 2005), pages 279–283.
Zornitsa Kozareva, Oscar Ferr´andez, Andr´es Montoyo,
Rafael Mu˜noz, and Armando Su´arez. 2005c. Com-
bining data-driven systems for improving named en-
tity recognition. In NLDB, pages 80–90.
James Mayfield, Paul McNamee, and Christine Pi-
atko. 2003. Named entity recognition using hun-
dreds of thousands of features. In Walter Daelemans
and Miles Osborne, editors, Proceedings of CoNLL-
2003, pages 184–187. Edmonton, Canada.
Rada Mihalcea and Dan I. Moldovan. 2001. A highly
accurate bootstrapping algorithm for word sense dis-
ambiguation. International Journal on Artificial In-
telligence Tools, 10(1-2):5–21.
Ted Pedersen, Amruta Purandare, and Anagha Kulka-
rni. 2005. Name discrimination by clustering sim-
ilar contexts. In Computational Linguistics and In-
telligent Text Processing, 6th International Confer-
ence, CICLing 2005, Mexico City, pages 226–237.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew-Lim Tan. 2004. Multi-criteria-based active
learning for named entity recognition. In Proceed-
ings of Association of Computational Linguists.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142–147. Edmon-
ton, Canada.
Erik F. Tjong Kim Sang. 2002a. Introduction to
the conll-2002 shared task: Language-independent
named entity recognition. In Proceedings of
CoNLL-2002, pages 155–158. Taipei, Taiwan.
Erik F. Tjong Kim Sang. 2002b. Memory-based
named entity recognition. In Proceedings of
CoNLL-2002, pages 203–206. Taipei, Taiwan.
Dekai Wu, Grace Ngai, and Marine Carpuat. 2003.
A stacked, voted, stacked model for named entity
recognition. In Walter Daelemans and Miles Os-
borne, editors, Proceedings of CoNLL-2003, pages
200–203. Edmonton, Canada.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Meet-
ing of the Association for Computational Linguis-
tics, pages 189–196.
</reference>
<page confidence="0.999438">
21
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.935478">
<title confidence="0.9991925">Bootstrapping Named Entity Recognition with Automatically Generated Gazetteer Lists</title>
<author confidence="0.998812">Zornitsa Kozareva</author>
<affiliation confidence="0.9994925">Dept. de Lenguajes y Sistemas Inform´aticos University of Alicante</affiliation>
<address confidence="0.966843">Alicante, Spain</address>
<email confidence="0.987078">zkozareva@dlsi.ua.es</email>
<abstract confidence="0.997934888888889">Current Named Entity Recognition systems suffer from the lack of hand-tagged data as well as degradation when moving to other domain. This paper explores two aspects: the automatic generation of gazetteer lists from unlabeled data; and the building of a Named Entity Recognition system with labeled and unlabeled data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<date>2002</date>
<booktitle>Bootstrapping. In Proceedings of Association of Computational Linguists,</booktitle>
<pages>360--367</pages>
<contexts>
<context position="5336" citStr="Abney, 2002" startWordPosition="823" endWordPosition="824">el, because it is known to be useful when the amount of training data is not sufficient. Important part in the NE process takes the location and person gazetteer lists which were automatically extracted from unlabeled data. More detailed explanation about their generation can be found in Section 3. To explore the effect of labeled and unlabeled training data to our NER, two types of experiments were conducted. For the supervised approach, the labels in the training data were previously known. For the semi-supervised approach, the labels in the training data were hidden. We used bootstrapping (Abney, 2002) which refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier. • Goals: - utilize a minimal amount of supervised examples; 3”President of the United States” 4”Technical University of Catalu˜na” 5book titles, sport events, etc. - obtain learning from many unlabeled examples; • General scheme: - initial supervision seed examples for training an initial model; - corpus classification with seed model; - add most confident classifications to training data and iterate. In our bootstrapping, a newly label</context>
</contexts>
<marker>Abney, 2002</marker>
<rawString>Steven P. Abney. 2002. Bootstrapping. In Proceedings of Association of Computational Linguists, pages 360–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Becker</author>
<author>Ben Hachey</author>
<author>Beatrice Alex</author>
<author>Claire Grover</author>
</authors>
<title>Optimising selective sampling for bootstrapping named entity recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Learning with Multiple View, ICML,</booktitle>
<pages>5--10</pages>
<location>Bonn, Germany.</location>
<contexts>
<context position="6675" citStr="Becker et al., 2005" startWordPosition="1062" endWordPosition="1065">e number n of iterations for our experiments is set up to 25 and when this bound is reached the bootstrapping stops. The scheme we follow is described below. 1. for iteration = 0 ... n do 2. pool 1000 examples from unlabeled data; 3. annotate all 1000 examples with classifier C1 and C2; 4. for each of the 1000 examples compare classes of C1 and C2; 5. add example into L only if classes of C1 and C2 agree; 6. train model with L; 7. calculate result 8. end for Bootstrapping was previously used by (Carreras et al., 2003), who were interested in recognizing Catalan names using Spanish resources. (Becker et al., 2005) employed bootstrapping in an active learning method for tagging entities in an astronomic domain. (Yarowsky, 1995) and (Mihalcea and Moldovan, 2001) utilized bootstrapping for word sense disambiguation. (Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al., 2005a) used self-training and cotraining to detect and classify named entities in news domain, (Shen et al., 2004) conducted experiments with multi-criteria-based active learning for biomedical NER. The experimental data we work with is taken from the CoNLL-2002 competition. The Spanish 16 corpus6 comes from news </context>
</contexts>
<marker>Becker, Hachey, Alex, Grover, 2005</marker>
<rawString>Markus Becker, Ben Hachey, Beatrice Alex, and Claire Grover. 2005. Optimising selective sampling for bootstrapping named entity recognition. In Proceedings of the Workshop on Learning with Multiple View, ICML, pages 5–10. Bonn, Germany.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel M Bikel</author>
<author>Scott Miller</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>Nymble: a high-performance learning name-finder.</title>
<booktitle>In Proceedings of Conference on Applied Natural Language Processing.</booktitle>
<marker>Bikel, Miller, Schwartz, Weischedel, </marker>
<rawString>Daniel M. Bikel, Scott Miller, Richard Schwartz, and Ralph Weischedel. Nymble: a high-performance learning name-finder. In Proceedings of Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William J Black</author>
<author>Fabio Rinaldi</author>
<author>David Mowatt</author>
</authors>
<title>Facile: Description of the ne system used for muc-7.</title>
<date>1998</date>
<booktitle>In Proceedings of MUC-7.</booktitle>
<contexts>
<context position="957" citStr="Black et al., 1998" startWordPosition="133" endWordPosition="136">er domain. This paper explores two aspects: the automatic generation of gazetteer lists from unlabeled data; and the building of a Named Entity Recognition system with labeled and unlabeled data. 1 Introduction Automatic information extraction and information retrieval concerning particular person, location, organization, title of movie or book, juxtaposes to the Named Entity Recognition (NER) task. NER consists in detecting the most silent and informative elements in a text such as names of people, company names, location, monetary currencies, dates. Early NER systems (Fisher et al., 1997), (Black et al., 1998) etc., participating in Message Understanding Conferences (MUC), used linguistic tools and gazetteer lists. However these are difficult to develop and domain sensitive. To surmount these obstacles, application of machine learning approaches to NER became a research subject. Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost(Carreras et al., 2002), Hidden Markov Models (Bikel et al., ), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used1. (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 200</context>
</contexts>
<marker>Black, Rinaldi, Mowatt, 1998</marker>
<rawString>William J Black, Fabio Rinaldi, and David Mowatt. 1998. Facile: Description of the ne system used for muc-7. In Proceedings of MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>New York University.,</institution>
<contexts>
<context position="1326" citStr="Borthwick, 1999" startWordPosition="187" endWordPosition="188">Entity Recognition (NER) task. NER consists in detecting the most silent and informative elements in a text such as names of people, company names, location, monetary currencies, dates. Early NER systems (Fisher et al., 1997), (Black et al., 1998) etc., participating in Message Understanding Conferences (MUC), used linguistic tools and gazetteer lists. However these are difficult to develop and domain sensitive. To surmount these obstacles, application of machine learning approaches to NER became a research subject. Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost(Carreras et al., 2002), Hidden Markov Models (Bikel et al., ), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used1. (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 2005c) among others, combined several classifiers to obtain better named entity coverage rate. 1For other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/ Nevertheless all these machine learning algorithms rely on previously hand-labeled training data. Obtaining such data is labor-intensive, time consuming</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Borthwick. 1999. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. thesis, New York University., September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arques</author>
<author>Llu´ıs Padr´o</author>
</authors>
<title>Named entity extraction using adaboost.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>167--170</pages>
<location>Taipei, Taiwan.</location>
<marker>Carreras, M`arques, Padr´o, 2002</marker>
<rawString>Xavier Carreras, Llu´ıs M`arques, and Llu´ıs Padr´o. 2002. Named entity extraction using adaboost. In Proceedings of CoNLL-2002, pages 167–170. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
<author>Llu´ıs Padr´o</author>
</authors>
<title>Named entity recognition for catalan using only spanish resources and unlabelled data. In</title>
<date>2003</date>
<booktitle>EACL,</booktitle>
<pages>43--50</pages>
<marker>Carreras, M`arquez, Padr´o, 2003</marker>
<rawString>Xavier Carreras, Llu´ıs M`arquez, and Llu´ıs Padr´o. 2003. Named entity recognition for catalan using only spanish resources and unlabelled data. In EACL, pages 43–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="2062" citStr="Collins and Singer, 1999" startWordPosition="283" endWordPosition="286">m Sang, 2002b), have been used1. (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 2005c) among others, combined several classifiers to obtain better named entity coverage rate. 1For other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/ Nevertheless all these machine learning algorithms rely on previously hand-labeled training data. Obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding. Resource limitation, directed NER research (Collins and Singer, 1999), (Carreras et al., 2003), (Kozareva et al., 2005a) toward the usage of semi-supervised techniques. These techniques are needed, as we live in a multilingual society and access to information from various language sources is reality. The development of NER systems for languages other than English commenced. This paper presents the development of a Spanish Named Recognition system based on machine learning approach. For it no morphologic or syntactic information was used. However, we propose and incorporate a very simple method for automatic gazetteer2 construction. Such method can be easily ad</context>
<context position="6905" citStr="Collins and Singer, 1999" startWordPosition="1097" endWordPosition="1100">data; 3. annotate all 1000 examples with classifier C1 and C2; 4. for each of the 1000 examples compare classes of C1 and C2; 5. add example into L only if classes of C1 and C2 agree; 6. train model with L; 7. calculate result 8. end for Bootstrapping was previously used by (Carreras et al., 2003), who were interested in recognizing Catalan names using Spanish resources. (Becker et al., 2005) employed bootstrapping in an active learning method for tagging entities in an astronomic domain. (Yarowsky, 1995) and (Mihalcea and Moldovan, 2001) utilized bootstrapping for word sense disambiguation. (Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al., 2005a) used self-training and cotraining to detect and classify named entities in news domain, (Shen et al., 2004) conducted experiments with multi-criteria-based active learning for biomedical NER. The experimental data we work with is taken from the CoNLL-2002 competition. The Spanish 16 corpus6 comes from news domain and was previously manually annotated. The train data set contains 264715 words of which 18798 are entities and the test set has 51533 words of which 3558 are entities. We decided to work with available NE annotated corpora</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory-based learner.</title>
<date>2003</date>
<tech>Technical Report ILK 03-10,</tech>
<institution>Tilburg University,</institution>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2003</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2003. Timbl: Tilburg memory-based learner. Technical Report ILK 03-10, Tilburg University, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Approximate statistical test for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<volume>10</volume>
<issue>7</issue>
<contexts>
<context position="19015" citStr="Dietterich, 1998" startWordPosition="3113" endWordPosition="3114">introduced during the learning. Some examples were learned with the wrong class and others didn’t introduce new information in the training data. Figure 2 presents the learning curve of the bootstrapping processes for 25 iterations. On each iteration 1000 examples were tagged, but only the examples having classes that coincide by the two classifiers were later included in the training data. We should note that for each iteration the same amount of B, I and O classes was included. Thus the balance among the three different classes in the training data is maintained. According to z� statistics (Dietterich, 1998), the highest score reached by bootstrapping cannot outperform the supervised method, however if both methods were evaluated on small amount of data the results were similar. Connected Component Connected Component Manolo Jose Maria John Lennon Garcia Martinez Fernandez 19 85 80 75 70 65 600 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 iterations f−score Figure 2: Bootstrapping performance 5 Experiments for classification process In a Named Entity classification process, to the previously detected Named Entities a predefined category of interest such as name of person, org</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>Thomas G. Dietterich. 1998. Approximate statistical test for comparing supervised classification learning algorithms. Neural Computation, 10(7):1895–1923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Fisher</author>
<author>Stephen Soderland</author>
<author>Joseph McCarthy</author>
<author>Fangfang Feng</author>
<author>Wendy Lehnert</author>
</authors>
<title>Description of the umass system as used for muc-6.</title>
<date>1997</date>
<booktitle>In Proceedings of MUC-6.</booktitle>
<contexts>
<context position="935" citStr="Fisher et al., 1997" startWordPosition="129" endWordPosition="132">tion when moving to other domain. This paper explores two aspects: the automatic generation of gazetteer lists from unlabeled data; and the building of a Named Entity Recognition system with labeled and unlabeled data. 1 Introduction Automatic information extraction and information retrieval concerning particular person, location, organization, title of movie or book, juxtaposes to the Named Entity Recognition (NER) task. NER consists in detecting the most silent and informative elements in a text such as names of people, company names, location, monetary currencies, dates. Early NER systems (Fisher et al., 1997), (Black et al., 1998) etc., participating in Message Understanding Conferences (MUC), used linguistic tools and gazetteer lists. However these are difficult to develop and domain sensitive. To surmount these obstacles, application of machine learning approaches to NER became a research subject. Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost(Carreras et al., 2002), Hidden Markov Models (Bikel et al., ), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used1. (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003),</context>
</contexts>
<marker>Fisher, Soderland, McCarthy, Feng, Lehnert, 1997</marker>
<rawString>David Fisher, Stephen Soderland, Joseph McCarthy, Fangfang Feng, and Wendy Lehnert. 1997. Description of the umass system as used for muc-6. In Proceedings of MUC-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Joseph Smarr</author>
<author>Huy Nguyen</author>
<author>Christopher D Manning</author>
</authors>
<title>Named entity recognition with character-level models.</title>
<date>2003</date>
<booktitle>In Walter Daelemans</booktitle>
<pages>180--183</pages>
<editor>and Miles Osborne, editors,</editor>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1490" citStr="Klein et al., 2003" startWordPosition="210" endWordPosition="213">etary currencies, dates. Early NER systems (Fisher et al., 1997), (Black et al., 1998) etc., participating in Message Understanding Conferences (MUC), used linguistic tools and gazetteer lists. However these are difficult to develop and domain sensitive. To surmount these obstacles, application of machine learning approaches to NER became a research subject. Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost(Carreras et al., 2002), Hidden Markov Models (Bikel et al., ), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used1. (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 2005c) among others, combined several classifiers to obtain better named entity coverage rate. 1For other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/ Nevertheless all these machine learning algorithms rely on previously hand-labeled training data. Obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding. Resource limitation, directed NER research (Collins and Singer, 1999), (Carreras et al., 2003), (</context>
</contexts>
<marker>Klein, Smarr, Nguyen, Manning, 2003</marker>
<rawString>Dan Klein, Joseph Smarr, Huy Nguyen, and Christopher D. Manning. 2003. Named entity recognition with character-level models. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL2003, pages 180–183. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Boyan Bonev</author>
<author>Andres Montoyo</author>
</authors>
<title>Self-training and co-training for spanish named entity recognition.</title>
<date>2005</date>
<booktitle>In 4th Mexican International Conference on Artificial Intelligence,</booktitle>
<pages>770--780</pages>
<contexts>
<context position="1558" citStr="Kozareva et al., 2005" startWordPosition="222" endWordPosition="225">(Black et al., 1998) etc., participating in Message Understanding Conferences (MUC), used linguistic tools and gazetteer lists. However these are difficult to develop and domain sensitive. To surmount these obstacles, application of machine learning approaches to NER became a research subject. Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost(Carreras et al., 2002), Hidden Markov Models (Bikel et al., ), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used1. (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 2005c) among others, combined several classifiers to obtain better named entity coverage rate. 1For other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/ Nevertheless all these machine learning algorithms rely on previously hand-labeled training data. Obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding. Resource limitation, directed NER research (Collins and Singer, 1999), (Carreras et al., 2003), (Kozareva et al., 2005a) toward the usage of semi-supervised techniqu</context>
<context position="6964" citStr="Kozareva et al., 2005" startWordPosition="1105" endWordPosition="1108">4. for each of the 1000 examples compare classes of C1 and C2; 5. add example into L only if classes of C1 and C2 agree; 6. train model with L; 7. calculate result 8. end for Bootstrapping was previously used by (Carreras et al., 2003), who were interested in recognizing Catalan names using Spanish resources. (Becker et al., 2005) employed bootstrapping in an active learning method for tagging entities in an astronomic domain. (Yarowsky, 1995) and (Mihalcea and Moldovan, 2001) utilized bootstrapping for word sense disambiguation. (Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al., 2005a) used self-training and cotraining to detect and classify named entities in news domain, (Shen et al., 2004) conducted experiments with multi-criteria-based active learning for biomedical NER. The experimental data we work with is taken from the CoNLL-2002 competition. The Spanish 16 corpus6 comes from news domain and was previously manually annotated. The train data set contains 264715 words of which 18798 are entities and the test set has 51533 words of which 3558 are entities. We decided to work with available NE annotated corpora in order to conduct an exhaustive and comparative NER stud</context>
<context position="17510" citStr="Kozareva et al., 2005" startWordPosition="2866" endWordPosition="2869"> location and person name gazetteer lists with the pattern search and validation model we propose is a very easy and practical task. With our approach thousands of names can be obtained, especially given the ample presence of unlabeled data and the World Wide Web. The purpose of our gazetteer construction was not to make complete gazetteer lists, but rather generate in a quick and automatic way lists of names that can help during our feature construction module. 4 Experiments for delimitation process In this section we describe the conducted experiments for named entity detection. Previously (Kozareva et al., 2005b) demonstrated that in supervised learning only superficial features as context and ortografics are sufficient to identify the boundaries of a Named Entity. In our experiment the superficial features fi ÷ fio were used by the supervised and semi-supervised classifiers. Table 2 shows the obtained results for Begin and Inside tags, which actually detect the entities and the total BIO tag performance. experiment B I BIO Supervised 94.40 85.74 91.88 Bootstrapped 87.47 68.95 81.62 Table 2: F-score of detected entities. On the first row are the results of the supervised method and on the second row</context>
</contexts>
<marker>Kozareva, Bonev, Montoyo, 2005</marker>
<rawString>Zornitsa Kozareva, Boyan Bonev, and Andres Montoyo. 2005a. Self-training and co-training for spanish named entity recognition. In 4th Mexican International Conference on Artificial Intelligence, pages 770–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Oscar Ferr´andez</author>
<author>Andres Montoyo</author>
<author>Rafael Mu˜noz</author>
</authors>
<title>Using language resource independent detection for spanish named entity recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Recent Advances in Natural Language Processing (RANLP</booktitle>
<pages>279--283</pages>
<marker>Kozareva, Ferr´andez, Montoyo, Mu˜noz, 2005</marker>
<rawString>Zornitsa Kozareva, Oscar Ferr´andez, Andres Montoyo, and Rafael Mu˜noz. 2005b. Using language resource independent detection for spanish named entity recognition. In Proceedings of the Conference on Recent Advances in Natural Language Processing (RANLP 2005), pages 279–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Oscar Ferr´andez</author>
<author>Andr´es Montoyo</author>
<author>Rafael Mu˜noz</author>
<author>Armando Su´arez</author>
</authors>
<title>Combining data-driven systems for improving named entity recognition.</title>
<date>2005</date>
<booktitle>In NLDB,</booktitle>
<pages>80--90</pages>
<marker>Kozareva, Ferr´andez, Montoyo, Mu˜noz, Su´arez, 2005</marker>
<rawString>Zornitsa Kozareva, Oscar Ferr´andez, Andr´es Montoyo, Rafael Mu˜noz, and Armando Su´arez. 2005c. Combining data-driven systems for improving named entity recognition. In NLDB, pages 80–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Mayfield</author>
<author>Paul McNamee</author>
<author>Christine Piatko</author>
</authors>
<title>Named entity recognition using hundreds of thousands of features.</title>
<date>2003</date>
<booktitle>In Walter Daelemans</booktitle>
<pages>184--187</pages>
<editor>and Miles Osborne, editors,</editor>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1515" citStr="Mayfield et al., 2003" startWordPosition="214" endWordPosition="217">s. Early NER systems (Fisher et al., 1997), (Black et al., 1998) etc., participating in Message Understanding Conferences (MUC), used linguistic tools and gazetteer lists. However these are difficult to develop and domain sensitive. To surmount these obstacles, application of machine learning approaches to NER became a research subject. Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost(Carreras et al., 2002), Hidden Markov Models (Bikel et al., ), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used1. (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 2005c) among others, combined several classifiers to obtain better named entity coverage rate. 1For other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/ Nevertheless all these machine learning algorithms rely on previously hand-labeled training data. Obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding. Resource limitation, directed NER research (Collins and Singer, 1999), (Carreras et al., 2003), (Kozareva et al., 2005a) t</context>
</contexts>
<marker>Mayfield, McNamee, Piatko, 2003</marker>
<rawString>James Mayfield, Paul McNamee, and Christine Piatko. 2003. Named entity recognition using hundreds of thousands of features. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL2003, pages 184–187. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan I Moldovan</author>
</authors>
<title>A highly accurate bootstrapping algorithm for word sense disambiguation.</title>
<date>2001</date>
<journal>International Journal on Artificial Intelligence Tools,</journal>
<pages>10--1</pages>
<contexts>
<context position="6824" citStr="Mihalcea and Moldovan, 2001" startWordPosition="1085" endWordPosition="1089">described below. 1. for iteration = 0 ... n do 2. pool 1000 examples from unlabeled data; 3. annotate all 1000 examples with classifier C1 and C2; 4. for each of the 1000 examples compare classes of C1 and C2; 5. add example into L only if classes of C1 and C2 agree; 6. train model with L; 7. calculate result 8. end for Bootstrapping was previously used by (Carreras et al., 2003), who were interested in recognizing Catalan names using Spanish resources. (Becker et al., 2005) employed bootstrapping in an active learning method for tagging entities in an astronomic domain. (Yarowsky, 1995) and (Mihalcea and Moldovan, 2001) utilized bootstrapping for word sense disambiguation. (Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al., 2005a) used self-training and cotraining to detect and classify named entities in news domain, (Shen et al., 2004) conducted experiments with multi-criteria-based active learning for biomedical NER. The experimental data we work with is taken from the CoNLL-2002 competition. The Spanish 16 corpus6 comes from news domain and was previously manually annotated. The train data set contains 264715 words of which 18798 are entities and the test set has 51533 words o</context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>Rada Mihalcea and Dan I. Moldovan. 2001. A highly accurate bootstrapping algorithm for word sense disambiguation. International Journal on Artificial Intelligence Tools, 10(1-2):5–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Amruta Purandare</author>
<author>Anagha Kulkarni</author>
</authors>
<title>Name discrimination by clustering similar contexts.</title>
<date>2005</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing, 6th International Conference, CICLing 2005, Mexico City,</booktitle>
<pages>226--237</pages>
<contexts>
<context position="13420" citStr="Pedersen et al., 2005" startWordPosition="2150" endWordPosition="2153">n”. As person names were capitalized and had frequency higher than the threshold we placed, it was impossible for these names to be automatically detected as erroneous and filtered. However we left these names, since the gazetteer attributes we maintain are mutually nonexclusive. This means the name ”Jordan” can be seen in location gazetteer indicating the country Jordan and in the same time can be seen in the person name list indicating the person Jordan. In a real NE application such case is reality, but for the determination of the right category name entity disambiguation is needed as in (Pedersen et al., 2005). Person gazetteer is constructed with graph exploration algorithm. The graph consists of: 1. two kinds of nodes: • First Names • Family Names 10New York 2. undirected connections between First Names and Family Names. The graph connects Family Names with First Names, and vice versa. In practice, such a graph is not necessarily connected, as there can be unusual first names and surnames which have no relation with other names in the corpus. Though, the corpus is supposed to contain mostly common names in one and the same language, names from other languages might be present too. In this case, i</context>
</contexts>
<marker>Pedersen, Purandare, Kulkarni, 2005</marker>
<rawString>Ted Pedersen, Amruta Purandare, and Anagha Kulkarni. 2005. Name discrimination by clustering similar contexts. In Computational Linguistics and Intelligent Text Processing, 6th International Conference, CICLing 2005, Mexico City, pages 226–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
<author>Guodong Zhou</author>
<author>Chew-Lim Tan</author>
</authors>
<title>Multi-criteria-based active learning for named entity recognition.</title>
<date>2004</date>
<booktitle>In Proceedings of Association of Computational Linguists.</booktitle>
<contexts>
<context position="7074" citStr="Shen et al., 2004" startWordPosition="1123" endWordPosition="1126"> agree; 6. train model with L; 7. calculate result 8. end for Bootstrapping was previously used by (Carreras et al., 2003), who were interested in recognizing Catalan names using Spanish resources. (Becker et al., 2005) employed bootstrapping in an active learning method for tagging entities in an astronomic domain. (Yarowsky, 1995) and (Mihalcea and Moldovan, 2001) utilized bootstrapping for word sense disambiguation. (Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al., 2005a) used self-training and cotraining to detect and classify named entities in news domain, (Shen et al., 2004) conducted experiments with multi-criteria-based active learning for biomedical NER. The experimental data we work with is taken from the CoNLL-2002 competition. The Spanish 16 corpus6 comes from news domain and was previously manually annotated. The train data set contains 264715 words of which 18798 are entities and the test set has 51533 words of which 3558 are entities. We decided to work with available NE annotated corpora in order to conduct an exhaustive and comparative NER study when labeled and unlabeld data is present. For our bootstrapping experiment, we simply ignored the presence </context>
</contexts>
<marker>Shen, Zhang, Su, Zhou, Tan, 2004</marker>
<rawString>Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-Lim Tan. 2004. Multi-criteria-based active learning for named entity recognition. In Proceedings of Association of Computational Linguists.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Walter Daelemans</booktitle>
<pages>142--147</pages>
<editor>and Miles Osborne, editors,</editor>
<location>Edmonton, Canada.</location>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL-2003, pages 142–147. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Introduction to the conll-2002 shared task: Language-independent named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>155--158</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1449" citStr="Sang, 2002" startWordPosition="205" endWordPosition="206">le, company names, location, monetary currencies, dates. Early NER systems (Fisher et al., 1997), (Black et al., 1998) etc., participating in Message Understanding Conferences (MUC), used linguistic tools and gazetteer lists. However these are difficult to develop and domain sensitive. To surmount these obstacles, application of machine learning approaches to NER became a research subject. Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost(Carreras et al., 2002), Hidden Markov Models (Bikel et al., ), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used1. (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 2005c) among others, combined several classifiers to obtain better named entity coverage rate. 1For other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/ Nevertheless all these machine learning algorithms rely on previously hand-labeled training data. Obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding. Resource limitation, directed NER research (Collins and </context>
<context position="4016" citStr="Sang, 2002" startWordPosition="613" endWordPosition="614">he person gazetteer 15 tection and entity classification. Entity delimitation consist in determining the boundaries of the entity (e.g. the place from where it starts and the place it finishes). This is important for tracing entities composed of two or more words such as ”Presidente de los Estados Unidos ”3, ”Universidad Politecnica de Catalu˜na”4. For this purpose, the BIO scheme was incorporated. In this scheme, tag B denotes the start of an entity, tag I continues the entity and tag O marks words that do not form part of an entity. This scheme was initially introduced in CoNLL’s (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. Once all entities in the text are detected, they are passed for classification in a predefined set of categories such as location, person, organization or miscellaneous5 names. This task is known as entity classification. The final NER performance is measured considering the entity detection and classification tasks together. Our NER approach is based on machine learning. The two algorithms we used for the experiments were instance-based and decision trees, implemented by (Daele</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik F. Tjong Kim Sang. 2002a. Introduction to the conll-2002 shared task: Language-independent named entity recognition. In Proceedings of CoNLL-2002, pages 155–158. Taipei, Taiwan.</rawString>
</citation>
<citation valid="false">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang. 2002b. Memory-based named entity recognition.</title>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>203--206</pages>
<location>Taipei, Taiwan.</location>
<marker>Erik, </marker>
<rawString>Erik F. Tjong Kim Sang. 2002b. Memory-based named entity recognition. In Proceedings of CoNLL-2002, pages 203–206. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Grace Ngai</author>
<author>Marine Carpuat</author>
</authors>
<title>A stacked, voted, stacked model for named entity recognition.</title>
<date>2003</date>
<booktitle>In Walter Daelemans</booktitle>
<pages>200--203</pages>
<editor>and Miles Osborne, editors,</editor>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1534" citStr="Wu et al., 2003" startWordPosition="218" endWordPosition="221">her et al., 1997), (Black et al., 1998) etc., participating in Message Understanding Conferences (MUC), used linguistic tools and gazetteer lists. However these are difficult to develop and domain sensitive. To surmount these obstacles, application of machine learning approaches to NER became a research subject. Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost(Carreras et al., 2002), Hidden Markov Models (Bikel et al., ), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used1. (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 2005c) among others, combined several classifiers to obtain better named entity coverage rate. 1For other machine learning methods, consult http://www.cnts.ua.ac.be/conll2002/ner/ http://www.cnts.ua.ac.be/conll2003/ner/ Nevertheless all these machine learning algorithms rely on previously hand-labeled training data. Obtaining such data is labor-intensive, time consuming and even might not be present for languages with limited funding. Resource limitation, directed NER research (Collins and Singer, 1999), (Carreras et al., 2003), (Kozareva et al., 2005a) toward the usage of </context>
</contexts>
<marker>Wu, Ngai, Carpuat, 2003</marker>
<rawString>Dekai Wu, Grace Ngai, and Marine Carpuat. 2003. A stacked, voted, stacked model for named entity recognition. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL-2003, pages 200–203. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="6790" citStr="Yarowsky, 1995" startWordPosition="1082" endWordPosition="1083"> scheme we follow is described below. 1. for iteration = 0 ... n do 2. pool 1000 examples from unlabeled data; 3. annotate all 1000 examples with classifier C1 and C2; 4. for each of the 1000 examples compare classes of C1 and C2; 5. add example into L only if classes of C1 and C2 agree; 6. train model with L; 7. calculate result 8. end for Bootstrapping was previously used by (Carreras et al., 2003), who were interested in recognizing Catalan names using Spanish resources. (Becker et al., 2005) employed bootstrapping in an active learning method for tagging entities in an astronomic domain. (Yarowsky, 1995) and (Mihalcea and Moldovan, 2001) utilized bootstrapping for word sense disambiguation. (Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al., 2005a) used self-training and cotraining to detect and classify named entities in news domain, (Shen et al., 2004) conducted experiments with multi-criteria-based active learning for biomedical NER. The experimental data we work with is taken from the CoNLL-2002 competition. The Spanish 16 corpus6 comes from news domain and was previously manually annotated. The train data set contains 264715 words of which 18798 are entities </context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Meeting of the Association for Computational Linguistics, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>