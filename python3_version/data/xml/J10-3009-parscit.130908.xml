<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996652">
Linguistically Annotated Reordering:
Evaluation and Analysis
</title>
<author confidence="0.99757">
Deyi Xiong*
</author>
<affiliation confidence="0.985589">
Institute for Infocomm Research
</affiliation>
<author confidence="0.981625">
Min Zhang**
</author>
<affiliation confidence="0.978297">
Institute for Infocomm Research
</affiliation>
<author confidence="0.941049">
Aiti Awt
</author>
<affiliation confidence="0.974919">
Institute for Infocomm Research
</affiliation>
<author confidence="0.964336">
Haizhou Li$
</author>
<affiliation confidence="0.964877">
Institute for Infocomm Research
</affiliation>
<bodyText confidence="0.938427857142857">
Linguistic knowledge plays an important role in phrase movement in statistical machine trans-
lation. To efficiently incorporate linguistic knowledge into phrase reordering, we propose a new
approach: Linguistically Annotated Reordering (LAR). In LAR, we build hard hierarchical skele-
tons and inject soft linguistic knowledge from source parse trees to nodes of hard skeletons during
translation. The experimental results on large-scale training data show that LAR is comparable
to boundary word-based reordering (BWR) (Xiong, Liu, and Lin 2006), which is a very compet-
itive lexicalized reordering approach. When combined with BWR, LAR provides complementary
information for phrase reordering, which collectively improves the BLEU score significantly.
To further understand the contribution of linguistic knowledge in LAR to phrase reordering,
we introduce a syntax-based analysis method to automatically detect constituent movement in
both reference and system translations, and summarize syntactic reordering patterns that are
captured by reordering models. With the proposed analysis method, we conduct a comparative
analysis that not only provides the insight into how linguistic knowledge affects phrase move-
ment but also reveals new challenges in phrase reordering.
</bodyText>
<sectionHeader confidence="0.996874" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99992325">
The phrase-based approach is a widely accepted formalism in statistical machine trans-
lation (SMT). It segments the source sentence into a sequence of phrases (not necessarily
syntactic phrases), then translates and reorders these phrases in the target. The reason
for the popularity of phrasal SMT is its capability of non-compositional translations and
</bodyText>
<footnote confidence="0.9995135">
* 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: dyxiong@i2r.a-star.edu.sg.
** 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: mzhang@i2r.a-star.edu.sg.
t 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: aaiti@i2r.a-star.edu.sg.
t 1 Fusionopolis Way #21-01 Connexis Singapore 138632. E-mail: hli@i2r.a-star.edu.sg.
</footnote>
<note confidence="0.8797015">
Submission received: 24 October 2008; revised submission received: 12 March 2010; accepted for publication:
21 April 2010.
© 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.999624928571428">
local word reorderings within phrases. Unfortunately, reordering at the phrase level is
still problematic for phrasal SMT. The default distortion-based reordering model simply
penalizes phrase movement according to the jump distance, without considering any
linguistic contexts (morphological, lexical, or syntactic) around phrases.
In order to utilize lexical information for phrase reordering, Tillman (2004) and
Koehn et al. (2005) propose lexicalized reordering models which directly condition
phrase movement on phrases themselves. One problem with such lexicalized reordering
models is that they are restricted only to reorderings of phrases seen in training data.
To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words
of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering
evidence. Although these lexicalized reordering models significantly outperform the
distortion-based reordering model as reported, only using lexical information (e.g.,
boundary words) is not adequate to move phrases to appropriate positions.
Consider the following Chinese example with its English translation:
</bodyText>
<equation confidence="0.884581333333333">
[VP [PP (while) $0�(develop) �4)c(related) 90(legislation) HJ] [VP [VV 5�
!,(consider)] [NP [DNP [NP i22X(this) -�--&apos;R%�&amp;(referendum)] [DEG 0`, (of)]] [NP p
*,(results)]]]]1
</equation>
<bodyText confidence="0.991131625">
consider the results of this referendum while developing related legislation
In this example, boundary words ;i� and H-t are able to decide that the translation of the
PP phrase ;i�...H-t should be postponed until some phrase that succeeds it is translated.
But they cannot provide further information about exactly which succeeding phrase
should be translated first. If high-level linguistic knowledge, such as the syntactic
context VP→PP VP, is given, the position of the PP phrase can be easily determined
since the pre-verbal modifier PP in Chinese is frequently translated into a post-verbal
counterpart in English.
In this article, we focus on linguistically motivated phrase reordering, which in-
tegrates high-level linguistic knowledge in phrase reordering. We adopt a two-step
strategy. In the first step, we establish a hierarchical skeleton in phrasal SMT by in-
corporating Bracketing Transduction Grammar (BTG) (Wu 1997) into phrasal SMT. In
the second step, we inject soft linguistic information into nodes of the skeleton.
There are two significant advantages to using BTG in phrasal SMT. First, BTG is able
to generate hierarchical structures.2 This not only enhances phrasal SMT’s capability
for hierarchical and long-distance reordering but also establishes a platform for phrasal
SMT to incorporate knowledge from linguistic structure. Second, phrase reordering is
restricted by the ITG constraint (Wu 1997). Although it only allows two orders (straight
or inverted) of nodes in any binary branching structure, it is broadly verified that the
ITG constraint has good coverage of word reorderings on various language pairs (Wu,
Carpuat, and Shen 2006). This makes phrase reordering in phrasal SMT a more tractable
task.
After enhancing phrasal SMT with a hard hierarchical skeleton, we further inject
soft linguistic information into the nodes of the skeleton. We annotate each BTG node
</bodyText>
<footnote confidence="0.945184333333333">
1 In this article, we use Penn Chinese Treebank phrase labels (Xue et al. 2000).
2 Chiang (2005) also generates hierarchical structures in phrasal SMT. One difference is that Chiang’s
hierarchical grammar is lexicon-sensitive because the model requires at least one pair of aligned words in
each rule except for the “glue rule.” The other difference is that his grammar allows multiple
nonterminals. These two differences make Chiang’s grammar more expressive than the BTG but at the
cost of learning a larger model.
</footnote>
<page confidence="0.988691">
536
</page>
<note confidence="0.989655">
Xiong et al. Linguistically Annotated Reordering
</note>
<bodyText confidence="0.999618676470588">
with syntactic and lexical elements by projecting the source parse tree onto the BTG
binary tree. The challenge, of course, is that BTG hierarchical structures are not always
aligned with the linguistic structures in the source language parse tree. To address this
issue, we propose an annotation algorithm. The algorithm is able to label any BTG
nodes during decoding with very little overhead, regardless of whether the BTG nodes
are aligned with syntactic constituent nodes in the source parse tree. The annotated
linguistic elements are then used to guide phrase reordering under the ITG constraint.
We call this two-step phrase reordering strategy linguistically annotated reorder-
ing (LAR) (Xiong et al. 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reorder-
ing strategy based on BTG. However, they use boundary words as reordering features
at the second step. To distinguish this from our work, we call their approach boundary
word–based reordering (BWR). LAR and BWR can be considered as two reordering
variants for BTG-based phrasal SMT, which have similar training procedures. Further-
more, they can be combined.
We evaluate LAR vs. BWR using the automatic metric BLEU (Papineni et al. 2002).
The BLEU scores show that LAR is comparable to BWR and significantly improves
phrase reordering when combined with BWR.
We want to further study what happens when we combine BWR with LAR. In
particular, we want to investigate to what extent the integrated linguistic knowledge
(from LAR) changes phrase movement in an actual SMT system, and in what direction
the change takes place. The investigations will enable us to have a better understanding
of the relationship between phrase movement and linguistic context, and therefore to
explore linguistic knowledge more effectively in phrasal SMT.
Because syntactic constituents are often moved together across languages during
translation (Fox 2002), we particularly study how linguistic knowledge affects syntactic
constituent movement. To that end, we introduce a syntax-based analysis method. We
parse source sentences, and align the parse trees with reference translations as well as
system translations. We then summarize syntactic reordering patterns using context-
free grammar (CFG) rules from the obtained tree-to-string alignments. The extracted
reordering patterns clearly show the trace of syntactic constituent movement in both
reference translations and system translations.
With the proposed analysis method, we analyze the combination of BWR and LAR
vs. BWR alone. There are essentially three issues that are addressed in this syntax-based
comparative analysis.
</bodyText>
<listItem confidence="0.734008181818182">
1. The first issue concerns syntactic constituent movement in human/
machine translations. Fox (2002) investigates syntactic constituent
movement in human translations. We study syntactic constituent
movement in both human translations and machine translations that are
generated by an actual SMT system and compare them.
2. The second issue concerns the change of phrase movement after rich
linguistic knowledge is integrated into phrase reordering. To gain a better
insight into this issue, we study phrase movement patterns for 13 specific
syntactic constituents.
3. The last issue concerns which constituents remain difficult to reorder even
though rich linguistic knowledge is employed.
</listItem>
<bodyText confidence="0.997473">
The rest of the article is structured as follows. Section 2 introduces background
information about BTG-based phrasal SMT and phrase reordering under the ITG
</bodyText>
<page confidence="0.989091">
537
</page>
<note confidence="0.793303">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.993102714285714">
constraint. Section 3 describes algorithms which extract training instances for reorder-
ing models of BWR and LAR. Section 4 introduces the BWR model as our baseline
reordering model. Section 5 describes LAR and the combination of LAR with BWR.
Section 6 elaborates the syntax-based analysis method. Section 7 reports our evaluation
results on large-scale data. Section 8 demonstrates our analysis results and addresses the
various issues discussed above. Section 9 discusses related work. And finally, Section 10
summarizes our main conclusions.
</bodyText>
<sectionHeader confidence="0.997065" genericHeader="keywords">
2. Background
</sectionHeader>
<subsectionHeader confidence="0.978808">
2.1 BTG-Based Phrasal SMT
</subsectionHeader>
<bodyText confidence="0.999587333333333">
We establish a unified framework for BTG-based phrasal SMT in this section. There
are two kinds of rules in BTG, lexical rules (denoted as rl) and merging rules (denoted
as rm):3
</bodyText>
<equation confidence="0.9987195">
rl : Ap → x/y
rm : Ap → [Al, Ar]|(Al, Ar) (1)
</equation>
<bodyText confidence="0.999832">
A lexical rule translates a source phrase x into a target phrase y and generates a leaf
node A in the BTG tree. Merging rules combine left and right neighboring phrases Al
and Ar into a larger phrase Ap in an order o ∈ {straight, inverted}. In this article, we use
“[]” to denote a straight order and “()” an inverted order.
We define a BTG derivation D as a sequence of independent applications of lexical
and merging rules (D = (rl1..nl, rm1..nm)). Given a source sentence, the decoding task of
BTG-based SMT is to find a best derivation, which yields the best translation.4
We assign a probability to each rule using a log-linear model with different features
and corresponding weights λ, then multiply them to obtain P(D). To keep in line with
the common understanding of standard phrasal SMT (Koehn, Och, and Marcu 2003),
here we re-organize these features into a translation model (PT), a reordering model
(PR), and a target language model (PL) as follows:
</bodyText>
<equation confidence="0.970291">
P(D) = PT(rl1..nl) · PR(rm1..nm)λR · PL(e)λL
· exp(|e|)λw (2)
</equation>
<bodyText confidence="0.8336935">
where exp(|e|) is the word penalty.
The translation model is defined as
</bodyText>
<equation confidence="0.9999585">
PT(rl1..nl) =
P(rl) = p(x|y)λ1 · p(y|x)λ2 · plex(x|y)λ3 · plex(y|x)λ4 · exp(1)λ5 (3)
</equation>
<bodyText confidence="0.998760666666667">
where p(·) represents the phrase translation probabilities in both directions, plex(·) de-
notes the lexical translation probabilities in both directions, and exp(1) is the phrase
penalty.
</bodyText>
<footnote confidence="0.969588333333333">
3 The subscripts l, r, p of A do not mean that we categorize A into three different nonterminals. We use them
to represent the left node, right node, and parent node.
4 In this article, we use c to denote a source sentence and e a target sentence.
</footnote>
<equation confidence="0.54637975">
��nj
11
i=1
P(rli)
</equation>
<page confidence="0.997604">
538
</page>
<note confidence="0.990098">
Xiong et al. Linguistically Annotated Reordering
</note>
<bodyText confidence="0.883348">
Similarly, the reordering model is defined on the merging rules as follows:
</bodyText>
<equation confidence="0.567579666666667">
PR(rm 1..nm ) = fi�n. P(rmi ) (4)
11
i=1
</equation>
<bodyText confidence="0.9975225">
One of the most important and challenging tasks in building a BTG-based phrasal SMT
system is to define P(rm).
</bodyText>
<subsectionHeader confidence="0.998728">
2.2 Reordering Under the ITG Constraint
</subsectionHeader>
<bodyText confidence="0.999938333333333">
Under the ITG constraint, three nodes {Al, Ar, Ap} are involved when we consider the
order o between the two children {Al, Ar} in any binary subtrees. Therefore it is natural
to define the ITG reordering P(rm) as a function as follows:
</bodyText>
<equation confidence="0.998555">
P(rm) = f(Al, Ar, Ap,o) (5)
</equation>
<bodyText confidence="0.9987378">
where o ∈ {straight, inverted}.
Based on this function, various reordering models are built according to different
assumptions. For example, the flat reordering model in the original BTG (Wu 1996)
assigns prior probabilities for the straight and inverted order assuming the order is
highly related to the properties of language pairs. It is formulated as
</bodyText>
<equation confidence="0.995177333333333">
� ps, o = straight
P(rm) = (6)
1 − ps, o = inverted
</equation>
<bodyText confidence="0.999570333333333">
Supposing French and English are the source and target language, respectively, the
value of ps can be set as high as 0.8 to prefer monotone orientations because the two
languages have similar word orders in most cases.
The main problem of the flat reordering model is also the problem of the standard
distortion model (Koehn, Och, and Marcu 2003): Neither model considers linguistic
contexts. To be context-dependent, the ITG reordering might directly model the condi-
tional probability P(o|Al,Ar). This probability could be calculated using the maximum
likelihood estimate (MLE) by taking counts from training data, in the manner of the
lexicalized reordering model (Tillman 2004; Koehn et al. 2005):
</bodyText>
<equation confidence="0.995829">
Count(o, Al, Ar)
P(o|Al,Ar) = (7)
Count(Al,Ar)
</equation>
<bodyText confidence="0.999413222222222">
Unfortunately this lexicalized reordering method usually leads to a serious data sparse-
ness problem under the ITG constraint because Al and Ar become larger and larger due
to the merging rules, and are finally unseen in the training data.
To avoid the data sparseness problem yet be contextually informative, attributes
of Al and Ar, instead of nodes themselves, are used as reordering evidence in a new
perspective of the ITG reordering (Xiong, Liu, and Lin 2006). The new perspective treats
the ITG reordering as a binary-classification problem where the possible order straight
or inverted between two children nodes is the target class which the reordering model
predicts given Al, Ar, and Ap.
</bodyText>
<page confidence="0.994796">
539
</page>
<note confidence="0.393507">
Computational Linguistics Volume 36, Number 3
</note>
<sectionHeader confidence="0.84046" genericHeader="introduction">
3. Reordering Example Extraction
</sectionHeader>
<bodyText confidence="0.99967225">
Because we consider the ITG reordering as a classification problem, we need to obtain
training instances to build a classifier. Here we refer to a training instance as a reorder-
ing example, which is formally defined as a triple of (o, bl, br) where bl and br are two
neighboring blocks and o E {straight, inverted} is the order between them.
</bodyText>
<equation confidence="0.99026075">
The block is a pair of aligned source phrase and target phrase
b — (ci2
i1, ej2
j1 ) (8)
</equation>
<bodyText confidence="0.936299">
where b must be consistent with the word alignment M
</bodyText>
<equation confidence="0.885334">
∀(i,j) E M,i1 &lt; i &lt; i2 ↔ j1 &lt; j &lt; j2 (9)
</equation>
<bodyText confidence="0.998978571428571">
By this, we require that no words inside the source phrase ci2i1 are aligned to words
outside the target phrase ej2
j1 and that no words outside the source phrase are aligned to
words inside the target phrase. This definition is similar to that of the bilingual phrase
except that there is no length limitation over blocks. Figure 1 shows a word alignment
matrix between a Chinese sentence and English sentence. In the matrix, each block can
be represented as a rectangle, for example, blocks (c44, e44), (c54, e54), (c74, e94) in red rectangles,
and (c32, e33), (ci, ei) in blue rectangles.
In this section, we discuss two algorithms for extracting reordering examples from
word-aligned bilingual data. The first algorithm AExtractor (described in Section 3.1)
extracts reordering examples directly from word alignments by extending the bilin-
gual phrase extraction algorithm. The second algorithm TExtractor (described in Sec-
tion 3.2) extracts reordering examples from BTG-style trees which are built from word
alignments.
</bodyText>
<figureCaption confidence="0.726682">
Figure 1
</figureCaption>
<bodyText confidence="0.65221">
A word alignment matrix between a Chinese sentence and English sentence. Bold dots represent
junctions which connect two neighboring blocks. Red and blue rectangles are blocks which are
connected by junction J2.
</bodyText>
<page confidence="0.994916">
540
</page>
<note confidence="0.955991">
Xiong et al. Linguistically Annotated Reordering
</note>
<subsectionHeader confidence="0.995948">
3.1 AExtractor: Extracting Reordering Examples from Word Alignments
</subsectionHeader>
<bodyText confidence="0.995484">
Before we describe this algorithm, we introduce the concept of junction in the word
alignment matrix. We define a junction as a vertex shared by two neighboring blocks.
There are two types of junctions: a straight junction, which connects two neighboring
blocks in a straight order (e.g., black dots J1 – J4 in Figure 1) and an inverted junction,
which connects two neighboring blocks in an inverted order (e.g., the red dot J5 in
Figure 1).
The algorithm for AExtractor is shown in Figure 2. This completes three sub-tasks
as follows.
</bodyText>
<listItem confidence="0.9978754">
1. Find blocks (lines 4 and 5). This is similar to the standard phrase extraction
algorithm (Och 2002) except that we find blocks with arbitrary length.
2. Detect junctions and store blocks in the arrays of detected junctions (lines 7
and 8). Junctions that are included the current block can be easily detected
by looking at the previous and next blocks. A junction can connect
</listItem>
<bodyText confidence="0.678043">
multiple blocks on its left and right sides. For example, the second
junction J2 in Figure 1 connects two blocks on the left side and three blocks
on the right side. To store these blocks, we maintain two arrays (left and
right) for each junction.
</bodyText>
<listItem confidence="0.8919616">
3. Select block pairs from each detected junction as reordering examples
(lines 12–16). This is the most challenging task for AExtractor. Because a
junction may have n blocks on its left side and m blocks on its right side,
we will obtain nm reordering examples if we enumerate all block pairs.
This will quickly increase the number of reordering examples, especially
</listItem>
<figureCaption confidence="0.6020945">
Figure 2
AExtractor.
</figureCaption>
<page confidence="0.974109">
541
</page>
<note confidence="0.289886">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.9909035">
those with the straight order. To keep the number of reordering examples
tractable, we define various selection rules r to heuristically select special
block pairs as reordering examples.
We define four selection rules as follows.
</bodyText>
<listItem confidence="0.999836928571429">
1. strINV: We select the smallest blocks (in terms of the target length) for
straight junctions, and the largest blocks for inverted junctions. Take the
straight junction J2 in Figure 1 as an example, the extracted reordering
example is (straight, H Zk I five, --�f7 |flights).
2. STRinv: We select the largest blocks (in terms of the target length) for
straight junctions, and the smallest blocks for inverted junctions. Still
taking the straight junction J2 as an example, this time the extracted
reordering example is (straight, l&amp;quot;-A H Zk I The last five, --�f7 9 QM( Y,--RI
flights all fail due to accidents).
3. RANDOM: For any junction, we randomly select one block pair from its
arrays.
4. COMBO: For each junction, we first select two block pairs using selection
rule strINV and STRinv. If there are unselected blocks, we randomly select
one block pair from the remaining blocks.
</listItem>
<subsectionHeader confidence="0.99054">
3.2 TExtractor: Extracting Reordering Examples from BTG-Style Trees
</subsectionHeader>
<bodyText confidence="0.999895846153846">
A potential problem for AExtractor is caused by the use of heuristic selection rules:
keeping some block pairs as reordering examples while abandoning other block pairs.
The kept block pairs are not necessarily the best training instances for tuning an ITG
order predictor. To avoid this problem we can extract reordering examples from the
BTG trees of sentence pairs. Reordering examples extracted in this way are naturally
suitable for BTG order prediction.
There are various ways to build BTG trees over sentence pairs. One can use BTG to
produce bilingual parses of sentence pairs, similar to the approaches proposed by Wu
(1997) and Zhang and Gildea (2005) but using the more sophisticated reordering models
BWR or LAR. After parsing, reordering examples can be extracted from bilingual parse
trees and a better reordering model is therefore induced from the extracted reordering
examples. Using the better reordering model, the bilingual sentences are parsed again.
This procedure is run iteratively until no performance gain is obtained in terms of
translation or parsing accuracy. Formally, we can use expectation-maximization (EM)
training in this procedure. In the expectation step, we first estimate the likelihood of all
BTG trees of sentence pairs with the current BTG model. Then we extract reordering
examples and collect counts for them, weighted with the probability of the BTG tree
where they occur. In the maximization step, we can train a more accurate reordering
model with updated reordering examples. Unfortunately, this method is at high com-
putational cost.
Instead, here we adopt a less expensive alternative method to produce BTG trees
over sentence pairs. Supposing we have word alignments produced by GIZA++, we
then use the shift-reduce algorithm (SRA) introduced by Zhang, Gildea, and Chiang
(2008) to decompose word alignments into hierarchical trees. The SRA can guarantee
that each node is a bilingual phrase in the decomposition tree. If the fan-out of a node
is larger than two, we binarize it from left to right: for two neighboring child nodes, if
</bodyText>
<page confidence="0.994867">
542
</page>
<note confidence="0.916368">
Xiong et al. Linguistically Annotated Reordering
</note>
<bodyText confidence="0.999482857142857">
they are also neighboring on both the source and target sides, we combine them and
create a new node to dominate them. In this way, we can transform the decomposition
tree into a BTG-style tree. Note that not all multi-branching nodes can be binarized. We
extract reordering examples only from binary nodes.
Figure 3 shows the BTG-style tree which is built from the word alignment in Figure 1
according to the method mentioned here. From this tree, we can easily extract four re-
ordering examples in a straight order and one reordering example in an inverted order.
</bodyText>
<sectionHeader confidence="0.847315" genericHeader="method">
4. Boundary Word-Based Reordering
</sectionHeader>
<bodyText confidence="0.997201666666667">
Following the binary-classification perspective of the ITG reordering, Xiong, Liu, and
Lin (2006) propose a reordering model which exploits the maximum entropy (MaxEnt)
classifier for BTG order prediction
</bodyText>
<equation confidence="0.9888415">
PR (rm) = Pe (o�A1.Ar,A) = exp(Ei eihi(o,Al,Ar,Ap)) (10)
b P Ed exp(Ei θihi(o&apos;, Al, Ar, Ap))
</equation>
<bodyText confidence="0.99784325">
where the functions hi ∈ {0, 1} are reordering features and the θi are the weights of these
features.
Xiong, Liu, and Lin (2006) define reordering features using the boundary words of
the source/target sides of both children {Al,Ar}. Supposing that we have a reordering
example (inverted, T 7A 15Q  |on July 15, *T ME —&apos;-1 Q/j�  |held its presidential and
parliament elections), leftmost/rightmost source words {T, 15Q, T, l*} and target
words {on, 15, held, elections} will be extracted as boundary words. Each boundary word
will form a reordering feature as follows
</bodyText>
<equation confidence="0.9653675">
� 1, fn = bval, o = inverted
hi(o, Al, Ar, Ap) = 0, otherwise
</equation>
<bodyText confidence="0.997079">
where fn denotes the feature name, and bval is the corresponding boundary word.
There are two reasons why boundary words are used as important clues for
reordering:
</bodyText>
<listItem confidence="0.6475215">
1. Phrases frequently cohere across languages (Fox 2002). In cohesive phrase
movement, boundary words directly interact with the external contexts of
</listItem>
<figureCaption confidence="0.523723">
Figure 3
</figureCaption>
<bodyText confidence="0.998787">
The BTG-style tree built from the word alignment in Figure 1. We use ([i, j], [p, q]) to denote a tree
node, where i, j and p, q are the beginning and ending indices in the source and target language,
respectively.
</bodyText>
<page confidence="0.994257">
543
</page>
<note confidence="0.293003">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.994583666666667">
phrases. This suggests that boundary words might contain information for
phrase reordering.
2. The quantitative analysis in Xiong, Liu, and Lin (2006, page 525) further
shows that boundary words indeed contain information for order
prediction.
To train a BWR model, we follow three steps. First, we extract reordering examples
from word-aligned bilingual data as described in the last section, then generate reorder-
ing features using boundary words from the reordering examples, and finally estimate
feature weights.
</bodyText>
<sectionHeader confidence="0.961595" genericHeader="method">
5. Linguistically Annotated Reordering
</sectionHeader>
<bodyText confidence="0.9999588">
In order to employ more linguistic knowledge in the ITG reordering, we annotate each
BTG node involved in reordering using linguistic elements from the source-side parse
trees. The linguistic elements include: (1) the head word hw, (2) the part-of-speech (POS)
tag ht of the head word, and (3) its syntactic category sc. In this section, we describe the
annotation algorithm and the LAR model, as well as the combination of LAR and BWR.
</bodyText>
<subsectionHeader confidence="0.997661">
5.1 Annotation Algorithm
</subsectionHeader>
<bodyText confidence="0.999948333333334">
There are two steps to annotating a BTG node using source-side parse tree information:
(1) determining the sequence on the source side which is exactly covered by the node,
then (2) annotating the sequence according to the source-side parse tree. If the sequence
is exactly covered by a single subtree in the source-side parse tree, it is called a syntactic
sequence, otherwise it is a non-syntactic sequence. One of the challenges in this an-
notation is that phrases (BTG nodes) do not always cover syntactic sequences; in other
words, they are not always aligned to constituent nodes in the source-side tree. To solve
this problem, we generate a pseudo head word and composite category which consists
of the syntactic categories of three relevant constituents for the non-syntactic sequences.
In this way, our annotation is capable of labelling both syntactic and non-syntactic
phrases and therefore providing linguistic information for any phrase reordering.
The annotation algorithm is shown in Figure 4. For a syntactic sequence, the an-
notation is trivial. Annotation elements directly come from the subtree that covers the
sequence exactly. For a non-syntactic sequence, the process is more complicated. Firstly,
we need to locate the smallest subtree c* covering the sequence (line 6). Secondly, we
try to identify the head word/tag of the sequence (lines 7–12) by using its head word
directly if it is within the sequence. Otherwise, the word within the sequence which is
nearest to hw will be assigned as the head word of the sequence. Finally, we determine
the composite category of the sequence (lines 13–15), which is formulated as L-C-R.
L/R refers to the syntactic category of the left/right boundary node of s, which is the
highest leftmost/rightmost sub-node of c* not overlapping the sequence. If there is no
such boundary node (the sequence s is exactly aligned to the left/right boundary of c*),
L/R will be set to NULL. C is the syntactic category of c*. L, R, and C together describe
the external syntactic context of s. The composite category we define for non-syntactic
phrases is similar to the CCG-style category in Zollmann, Venugopal, and Vogel (2008).
Figure 5 shows a syntactic parse tree for a Chinese sentence, with the head word
annotated for each internal node. Some sample annotations are given in Table 1.
</bodyText>
<page confidence="0.997153">
544
</page>
<note confidence="0.97007">
Xiong et al. Linguistically Annotated Reordering
</note>
<figureCaption confidence="0.966889">
Figure 4
</figureCaption>
<subsectionHeader confidence="0.463997">
The Annotation Algorithm.
</subsectionHeader>
<bodyText confidence="0.800559">
Figure 5
A syntactic parse tree with the head word annotated for each internal node. The superscripts on
leaf nodes denote their surface positions from left to right.
</bodyText>
<subsectionHeader confidence="0.990843">
5.2 Reordering Model
</subsectionHeader>
<bodyText confidence="0.9984975">
The linguistically annotated reordering model PRa is a MaxEnt-based classification
model, which can be formulated as
</bodyText>
<equation confidence="0.979702142857143">
PRa (rm) = pθ(o|Aapp ,Aal
l ,Aar
r ) = exp(Ei θihi(o,Aapp ,Aal
l ,Arr)) (11)
Ed exp(Ei θihi(oI,Aapp ,Aal
l ,Aar
r ))
</equation>
<bodyText confidence="0.998785">
where the feature functions hi E {0, 1} are defined using annotated linguistic elements
of each BTG node. Here we use the superscripts al, ar, and ap to stress that the BTG nodes
are linguistically annotated.
</bodyText>
<page confidence="0.993836">
545
</page>
<table confidence="0.425612">
Computational Linguistics Volume 36, Number 3
</table>
<tableCaption confidence="0.993517">
Table 1
</tableCaption>
<bodyText confidence="0.916506">
Annotation samples according to the tree shown in Figure 5.
</bodyText>
<equation confidence="0.8330795">
sequence hw ht sc
(1, 2) NN NULL-NP-NN
(2, 3) NN NP
(2, 4) YR% VV NP-IP-NP
(3, 4) YR% VV NP-IP-NP
hw/ht = the head word/tag, respectively; sc = syntactic category.
Each merging rule involves three nodes (Aapp ,Aal
l , Aar
</equation>
<bodyText confidence="0.9266894">
r ) and each node has three
linguistic elements (hw, ht, sc). Therefore, the model has nine features in total. Taking
the left node Aal
l as an example, the model could use its head word w as a feature as
follows:
</bodyText>
<equation confidence="0.9743375">
� 1, Aal
hi(o,Aap l .hw = w,o = straight
p ,Aal
l ,Aar
r ) =
0, otherwise
</equation>
<bodyText confidence="0.9937618">
Training an LAR model also takes three steps. Firstly, we extract annotated reorder-
ing examples from source-side parsed, word-aligned bilingual data using the reordering
example extraction algorithm and the annotation algorithm. We then generate features
using the linguistic elements of these examples. Finally we tune feature weights to build
the MaxEnt model.
</bodyText>
<subsectionHeader confidence="0.99772">
5.3 Combining LAR and BWR
</subsectionHeader>
<bodyText confidence="0.953358">
LAR and BWR can be combined at two different levels:
</bodyText>
<listItem confidence="0.999118833333334">
1. Feature level. Because both LAR and BWR are trained under the
maximum entropy principle, we can combine linguistically annotated
features from LAR and boundary word features from BWR together and
train a single MaxEnt model. We call this method All-in-One combination.
2. Model level. We can also train two reordering models separately and
integrate them into BTG-based SMT
</listItem>
<equation confidence="0.9848045">
P(D) = PT(rl1..nl ) ·PRb(rm1..nm)λRb ·
PRa(rm1..nm)λRa ·PL(e)λL · exp(|e|)λw (12)
</equation>
<bodyText confidence="0.997693">
where PRb is the BWR reordering model and PRa is the LAR reordering
model. We call this combination BWR+LAR.
We will empirically compare these two combination methods in Section 7.4.
</bodyText>
<sectionHeader confidence="0.97417" genericHeader="method">
6. A New Syntax-Based Reordering Analysis Method
</sectionHeader>
<bodyText confidence="0.985101">
In order to understand the influence of linguistic knowledge on phrase reordering, we
propose a syntax-based method to analyze phrase reordering. In this analysis method,
</bodyText>
<page confidence="0.987778">
546
</page>
<note confidence="0.874679">
Xiong et al. Linguistically Annotated Reordering
</note>
<bodyText confidence="0.978357">
we leverage the alignments between source-side parse trees and reference/system
translations to summarize syntactic reordering patterns and calculate syntax-based
measures of precision and recall for each syntactic constituent.
</bodyText>
<subsectionHeader confidence="0.991425">
6.1 Overview
</subsectionHeader>
<bodyText confidence="0.999945666666667">
The alignment between a source parse tree and a target string is a collection of rela-
tionships between parse tree nodes and their corresponding target spans.5 A syntactic
reordering pattern (SRP) is defined as
</bodyText>
<equation confidence="0.548659">
(α -4 β1...βn a [i1]...[in])
</equation>
<bodyText confidence="0.9926468125">
The first part of an SRP is a CFG structure on the source side and the second part
[i1]...[in] indicates the order of target spans βT1 ...βTn of nonterminals β1...βn on the target
side.6
Let’s take the VP structure VP -4 PP1VP2 as an example to explain how the pre-
cision and recall can be obtained. On the target side, the order of PPT1 and VPT2
might be [1][2] or [2][1]. Therefore we have two syntactic reordering patterns for this
structure:
(VP -4 PP1VP2 a [1][2]) and (VP -4 PP1VP2 a [2][1])
Suppose that the two reordering patterns occur a times in the alignments between
source parse trees and reference translations, b times in the alignments between source
parse trees and system translations, and c times in both alignments. Then the reordering
precision/recall for this structure is c/b and c/a, respectively. We can further calculate
the F1-score as 2 x c/(a + b). These syntax-based metrics intuitively show how well the
reordering model can reorder this structure. By summarizing all reordering patterns of
all constituents, we can obtain an overall precision, recall, and F1-score for the tested
reordering model.
This new syntax-based analysis for reordering is motivated in part by recent work
which transforms the order of nodes in the source-side parse tree before translation
(Xia and McCord 2004; Collins, Koehn, and Kucerova 2005; Li et al. 2007; Wang,
Collins, and Koehn 2007). Here we focus on the order transformation of syntactic con-
stituents performed by reordering models during translation. In addition to aligning
parse trees with reference translations, we also align parse trees with system transla-
tions so that we can learn the movement of syntactic constituents carried out by the
reordering models and investigate the performance of the reordering models by com-
paring both alignments.
For notational convenience, we denote syntactic reordering patterns that are ex-
tracted from the alignments between source parse trees and reference translations as
REF-SRP and those from the alignments between source parse trees and system trans-
lations as SYS-SRP. We refer to those present in both alignments under some conditions
5 We adopt the definition of span from Fox (2002): Given a node n that covers a word sequence sp...si...sq
and a word alignment matrix M, the target words aligned to n are {ti : ti E M(si)}. We define the target
span of node n as nT = (min({ti}), max({ti})). Note that nT may contain words that are not in {ti}.
</bodyText>
<page confidence="0.73372">
6 Please note that the order of structures may not be defined in some cases (see Section 6.3).
547
</page>
<figure confidence="0.296167">
Computational Linguistics Volume 36, Number 3
</figure>
<listItem confidence="0.902355733333334">
that will be described in Section 6.4 as MATCH-SRP. To conduct a thorough analysis on
the reorderings, we carry out the following steps on the test corpus (source sentences +
reference translations):
1. Parse source sentences.
2. Generate word alignments between source sentences and reference
translations as well as word alignments between source sentences and
system translations.
3. According to the word alignments of Step 2, for each multi-branching
node o → β1...βn in the source parse tree generated in Step 1, find the
target spans βT1 ...βTn and their order [i1]...[in] in the reference and system
translations, respectively.
4. Generate REF-SRPs, SYS-SRPs, and MATCH-SRPs according to the target
orders generated in Step 3 for each multi-branching node.
5. Summarize all SRPs and calculate the precision and recall as described
above.
</listItem>
<bodyText confidence="0.963949">
We further elaborate Steps 2–4 in the Sections 6.2–6.4.
</bodyText>
<subsectionHeader confidence="0.999143">
6.2 Generating Word Alignments
</subsectionHeader>
<bodyText confidence="0.99999525">
To obtain word alignments between source sentences and multiple reference transla-
tions, we pair the source sentences with each of the reference translations and include
the created sentence pairs in our bilingual training corpus. Then we run GIZA++ on the
new corpus in both directions, and apply the “grow-diag-final” refinement rule (Koehn
et al. 2005) to produce the final word alignments.
To obtain word alignments between source sentences and system translations, we
store the word alignments within each phrase pair in our phrase table. When we output
the system translation for a source sentence, we trace back the original source phrase
for each target phrase in the system translation. This will generate a phrase alignment
between the source sentence and system translation. Given the phrase alignment and
word alignments within the phrase stored in the phrase table, we can easily obtain word
alignments between the whole source sentence and system translation.
</bodyText>
<subsectionHeader confidence="0.999484">
6.3 Generating Target Spans and Orders
</subsectionHeader>
<bodyText confidence="0.9999943">
Given the source parse tree and the word alignment between a source sentence and
a reference/system translation, for each multi-branching node o → β1...βn, we firstly
determine the target span βTi for each child node βi following Fox (2002). If one child
node is aligned to NULL, we define a special target span for it. The order for this special
target span will remain the same as the child node occurring in β1...βn.
Two target spans may overlap with each other because of inherent divergences
between two languages or noise in the word alignment. When this happens on two
neighboring nodes βi and βi+1, we combine these two nodes together and redefine a
target span βTi&amp;i+1 for the combined node. This process will be repeated until no more
neighboring nodes can be combined. For example, the target span of nodes a and b in
</bodyText>
<page confidence="0.996081">
548
</page>
<note confidence="0.971295">
Xiong et al. Linguistically Annotated Reordering
</note>
<figureCaption confidence="0.967585">
Figure 6
</figureCaption>
<bodyText confidence="0.796572428571429">
An example source parse tree with the word alignment between the source sentence and the
target translation. Dotted lines show the word alignment.
Figure 6 overlap ((1, 3) vs. (2, 2)). Therefore these two nodes are to be combined into a
new node, whose target span is (1, 3).
After performing all necessary node combinations, if there are no more overlaps, we
call the multi-branching node reorderable, otherwise non-reorderable. To get a clearer
picture of the reorderable nodes, we divided them into two categories:
</bodyText>
<listItem confidence="0.994657666666667">
• fully reorderable if all target spans of child nodes don’t overlap;
• partially reorderable if some child nodes are combined due to
overlapping.
</listItem>
<bodyText confidence="0.9998">
In Figure 6, both nodes a and c are fully reorderable nodes.7 Node d is a partially
reorderable node. Node g is a non-reorderable node because (1) the target spans of its
child nodes d and f overlap, and (2) child nodes d and f cannot be combined because
they are not neighbors.
Because we have multiple reference translations for each source sentence, we can
define multiple orders for {βTi }n1. If one node is non-reorderable in all reference trans-
lations, we call it REF-non-reorderable, otherwise REF-reorderable. To specify the
reorderable attribute of a node in the system translation, we prefix “SYS-” to {non-
reorderable, reorderable, fully-reorderable, partially-reorderable}.
</bodyText>
<subsectionHeader confidence="0.997423">
6.4 Generating SRPs
</subsectionHeader>
<bodyText confidence="0.999876">
After we obtain the orders of the child nodes for each multi-branching node, we gener-
ate REF-SRPs and SYS-SRPs from the fully/partially reorderable nodes. We obtain the
</bodyText>
<footnote confidence="0.589086">
7 Their target translations are interrupted by the other node’s translation. We will discuss this situation in
Section 8.5.
</footnote>
<page confidence="0.984673">
549
</page>
<note confidence="0.276935">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.958354">
MATCH-SRP for each multi-branching node by comparing the obtained SYS-SRP with
the REF-SRPs for this node under the following conditions:
</bodyText>
<listItem confidence="0.988851">
1. Because we have multiple reference translations, we may have different
REF-SRPs. We compare the SYS-SRP with the REF-SRP where the
reference translation for this node (the sequence within the target span of
the node defined by the REF-SRP) has the shortest Levenshtein distance
(Navarro 2001) to that of the system translation.
2. If there are combined nodes in SYS/REF-SRPs, they are treated as a unit
when comparing, without considering the order within each combined
node. If the order of the SYS-SRP and the selected REF-SRP matches, we
have one MATCH-SRP for the node.
</listItem>
<bodyText confidence="0.65031">
Let’s give an example to explain these conditions. Suppose that we are processing
the structure VP → PP1ADVP2VP3. We obtain four REF-SRPs from four different ref-
erence translations and one SYS-SRP from the system output. Here we only show the
orders:
</bodyText>
<equation confidence="0.95208875">
Ref.a : [3][1][2]
Ref.b : [3][2][1]
Ref.c&amp;d : [2][3][1]
SYS: [3][1&amp;2]
</equation>
<bodyText confidence="0.965606857142857">
References c and d have the same order. Therefore we have three different REF-SRPs
for this structure. In the SYS-SRP, PP1 and ADVP2 are combined and moved to the right
side of VP3. Supposing that the system translation for this structure has the shortest edit
distance to that of Reference b, we use the order of Reference b to compare the system
order. In the Reference b order, both PP1 and ADVP2 are also moved to the right side of
VP3. Therefore the two orders of Reference b and SYS match. We have one matched SRP
for this structure.
</bodyText>
<sectionHeader confidence="0.942052" genericHeader="method">
7. Evaluation
</sectionHeader>
<bodyText confidence="0.999853142857143">
Our system is a BTG-based phrasal SMT system, developed following Section 2. We
integrate the boundary word–based reordering model and the linguistically annotated
reordering model into our system according to our reordering configuration. We car-
ried out various experiments to evaluate the reordering example extraction algorithms
of Section 3, the linguistically annotated reordering model vs. boundary word–based
reordering model, and the effects of linguistically annotated features on the Chinese-to-
English translation task of the NIST MT-05 using large scale training data.
</bodyText>
<subsectionHeader confidence="0.950584">
7.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999472333333333">
We ran GIZA++ (Och and Ney 2000) on the parallel corpora (consisting of 101.93M
Chinese words and 112.78M English words) listed in Table 2 in both directions and
then applied the “grow-diag-final” refinement rule (Koehn, Och, and Marcu 2003) to
</bodyText>
<page confidence="0.982687">
550
</page>
<table confidence="0.920130692307692">
Xiong et al. Linguistically Annotated Reordering
Table 2
Corpora used.
Corpus LDC catalog Chinese words English words
United Nations LDC2004E12 68.63M 76.99M
Hong Kong News LDC2004T08 15.07M 15.89M
Sinorama Magazine LDC2005T10 10.26M 9.64M
FBIS LDC2003E14 7.09M 9.28M
Xinhua LDC2002E18 0.40M 0.43M
Chinese News Translation LDC2005T06 0.28M 0.31M
Chinese Treebank LDC2003E07 0.10M 0.13M
Multiple Translation Chinese LDC2004T07 0.10M 0.11M
Total —— 101.93M 112.78M
</table>
<bodyText confidence="0.998705541666666">
obtain many-to-many word alignments. From the word-aligned corpora, we extracted
bilingual phrases.
We used all corpora listed in Table 2 except for the United Nations corpus to train
our reordering models, which consist of 33.3M Chinese words and 35.79M English
words. We ran the reordering example extractor AExtractor and TExtractor of Section 3
on the chosen word-aligned corpora. We then extracted boundary word features from
the reordering examples. To extract linguistically annotated features, we parsed the
Chinese side of the chosen parallel text using a Chinese parser (Xiong, Liu, and Lin
2005) which was trained on the Penn Chinese Treebank with an F1-score of 79.4%. We
ran the off-the-shelf MaxEnt toolkit8 to tune the reordering feature weights with the
iteration number set to 100 and Gaussian prior to 1 to avoid overfitting.
We built our 4-gram language model using the SRILM toolkit (Stolcke 2002), which
was trained on the Xinhua section of the English Gigaword corpus (181.1M words).
We selected 580 short sentences (not exceeding 50 characters per sentence) from the
NIST MT-02 evaluation test data as our development set (18 words/31 characters per
sentence). The NIST MT-05 test set includes 1,082 sentences with an average of 27.4
words/47.6 characters per sentence. The reference corpus for the NIST MT-05 test set
contains four translations per source sentence. Both the development and test sets were
also parsed using the parser mentioned above.
Our evaluation metric is the case-insensitive BLEU-4 (Papineni et al. 2002) using the
shortest reference sentence length for the brevity penalty. The model feature weights are
tuned on the development set to maximize BLEU using MERT (Och 2003). Statistical
significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn
2004).
</bodyText>
<subsectionHeader confidence="0.999247">
7.2 Bias in AExtractor
</subsectionHeader>
<bodyText confidence="0.9996475">
As described in Section 3, AExtractor selectively extracts reordering examples. This
selective extraction raises three questions:
</bodyText>
<footnote confidence="0.9554755">
1. Is it necessary to extract all reordering examples?
8 Available at:http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
</footnote>
<page confidence="0.977084">
551
</page>
<table confidence="0.139177">
Computational Linguistics Volume 36, Number 3
</table>
<listItem confidence="0.982657857142857">
2. If it is not necessary, do the heuristic selection rules impose any bias on
the reordering model? For example, if we use the strINV selection rule,
meaning that we always extract the largest block pairs for inverted
reordering examples, does the reordering model prefer swappings on
larger blocks to those on smaller blocks?
3. Does the bias have a strong impact on the performance in terms of BLEU
score?
</listItem>
<bodyText confidence="0.999969388888889">
The answer to the first question is no. Firstly, it is practically undesirable to extract
all reordering examples because even a very small training set will produce millions of
reordering examples if we enumerate all block pair combinations. Secondly, extracting
all reordering examples introduces a great amount of noise into training and therefore
undermines the final reordering model. In Table 3, we show the number of reorder-
ing examples extracted using different extraction algorithms and selection rules. The
AExtractor with the COMBO selection rule extracts the largest number of reordering
examples. However, it does not obtain the highest BLEU score compared with other
selection rules which extract a smaller number of reordering examples. This empirically
suggests that there is no need to extract all reordering examples.
To answer the second question, we trace the best BTG trees produced on the test set
by our system. The BWR reordering model is trained on reordering examples which are
extracted using different selection rules. Then we calculate the average number of words
on the target side which are covered by binary nodes in a straight order. We refer to this
number as straight average length. Similarly, inverted average length is calculated on
all binary nodes in an inverted order. The third and fourth columns of Table 3 show the
two average variables. Comparing these average numbers, we clearly observe that two
selection rules indeed impose noticeable bias on the reordering model.
</bodyText>
<listItem confidence="0.977578">
• The strINV selection rule, which always extracts the largest block pairs for
inverted reordering examples, has the largest inverted average length.
This indicates that the strINV rule biases the reordering model towards
larger swappings.
• On the contrary, the STRinv selection rule, which extracts the largest block
pairs for straight reordering examples and smallest pairs for inverted
reordering examples, has the largest straight average length and a
</listItem>
<tableCaption confidence="0.691215666666667">
Table 3
Comparison of reordering example extraction algorithms and selection rules. We only use BWR
as the reordering model for this comparison.
</tableCaption>
<table confidence="0.996344428571429">
Ext. Alg. Reordering Straight Inverted BLEU
(Sel. rule) Examples Avg. Len. Avg. Len.
AExtractor (strINV) 10.06M 15.8 14.5 32.37
AExtractor (STRinv) 10.06M 17.3 12.8 32.47
AExtractor (RANDOM) 10.06M 14.7 11.8 32.24
AExtractor (COMBO) 23.27M 13.8 13.5 32.10
TExtractor 14.30M 15.0 14.1 29.95
</table>
<page confidence="0.980178">
552
</page>
<note confidence="0.946261">
Xiong et al. Linguistically Annotated Reordering
</note>
<bodyText confidence="0.999869361111111">
relatively much smaller inverted average length. This suggests that the
STRinv rule makes the reordering model prefer smaller swappings.
Note that the selection rules RANDOM and COMBO do not impose bias on the length
of extracted reordering examples compared with strINV and STRinv. The latter two se-
lection rules have special preferences on the length of reordering examples and transfer
these preferences to the reordering models as shown in Table 3.
Because the preference for reordering larger/smaller blocks is imposed by the
reordering example extraction algorithm with special selection rules, one might wonder
whether we can allow the decoder to decide its own reordering preference. We add two
new features to our translation model: reordering count penalty (rc) and reordering
length penalty (rl). We accumulate rc whenever two neighboring BTG nodes are re-
ordered. And at the same time we add the number of words which are covered by these
two neighboring nodes to rl. Their weights are tuned using MERT to maximize BLEU
score on the development set with other model feature weights. These two features
are similar to the widely used word/phrase penalty features. Tuning the weights of
the word/phrase penalty features, we can allow the decoder to favor shorter or longer
phrases. Similarly, with the two new features rc and rl, we can allow the decoder to
favor shorter or longer reorderings.
We conducted experiments using reordering examples which are extracted with the
RANDOM and COMBO selection rules because these two rules do not impose bias on
the length of reordering examples. Observing the optimized weights of rc and rl on
the development set, we find that the decoder rewards larger rc but smaller rl. This
means that the decoder prefers shorter reorderings to longer reorderings. However, the
BLEU scores on the test set are 31.89 and 32.0 for RANDOM and COMBO, respectively,
which are worse than the BLEU scores of RANDOM and COMBO without using rc
and rl in Table 3, and also worse than the performance of strINV and STRinv which
impose preferences on reordering examples. This seems to suggest that the preference
for shorter/longer reorderings imposed by the reordering example extraction algorithm
is better than that decided by the decoder itself.
Finally, for the last question, we observe from Table 3 that BLEU scores are not that
much different although we have quite the opposite bias imposed by different selection
rules. The changes in BLEU score, which happen when we shift from one selection rule
to the other, are limited to a maximum of 1.2%. Among the four selection rules, the
STRinv rule achieves the highest BLEU score. The reason might be that the bias towards
smaller swappings imposed by this rule helps the decoder to reduce incorrect long-
distance swappings (Xiong et al. 2008b).
</bodyText>
<subsectionHeader confidence="0.99089">
7.3 AExtractor vs. TExtractor
</subsectionHeader>
<bodyText confidence="0.9998066">
We further compared the two algorithms for reordering example extraction. In Table 3,
we find that TExtractor significantly underperforms in comparison to AExtractor. This
is because the transformation from decomposition trees to BTG trees is not complete.
Many crossing links due to errors and noise in word alignments generated by GIZA++
make it impossible to build BTG nodes over the corresponding words. It would be better
to use alignments induced by the ITG and EM procedure described in Section 3.2 but
this has a very high cost.
Given the comparison in Table 3, we use AExtractor with the STRinv selection rule
to extract reordering examples for both BWR and LAR in all experiments described
below.
</bodyText>
<page confidence="0.991421">
553
</page>
<note confidence="0.456326">
Computational Linguistics Volume 36, Number 3
</note>
<subsectionHeader confidence="0.751157">
7.4 LAR vs. BWR
</subsectionHeader>
<bodyText confidence="0.995689333333333">
Table 4 shows the results of the different integration of BWR and LAR into our systems.
Only using LAR achieves a BLEU score of 32.17, which is comparable to that of BWR.
This suggests that LAR is promising given that:
</bodyText>
<listItem confidence="0.872145">
• LAR uses many fewer features than BWR does. According to our statistics,
LAR contains only 166.1k linguistically annotated features whereas BWR
has 451.4k boundary word features.
• Syntactic divergences between the source and target languages as well as
parse errors prevent the effective use of syntactic knowledge for phrase
reordering (see the in-depth analysis in Section 8.2.2).
</listItem>
<bodyText confidence="0.998594133333333">
Although BWR marginally outperforms LAR (32.47 vs. 32.17), simple boundary
word features are not adequate to move phrases to appropriate positions because
they cannot recognize syntactic contexts which are very relevant to phrase reordering.
Therefore the best way to reorder a phrase is to combine BWR and LAR so that we
can use syntactic information on the one hand and not worry too much about syntactic
divergences on the other hand.
As described in Section 5.3, we can combine BWR and LAR at two levels: the feature
level and the model level. When we combine them at the model level, we achieve an
absolute improvement of 0.83 and 1.13 BLEU points over BWR and LAR, respectively,
which are both statistically significant (p &lt; 0.01). This shows that LAR and BWR are
complementary to each other and in particular that using linguistic knowledge can
significantly improve a very competitive lexicalized reordering model (BWR).
The other combination method All-in-One (at the feature level) also obtains signif-
icant improvements over BWR and LAR but marginally underperforms compared to
BWR+LAR. In our later experiments we use the combination method BWR+LAR.
</bodyText>
<subsectionHeader confidence="0.997994">
7.5 Varying Training Data Size
</subsectionHeader>
<bodyText confidence="0.9961435">
To investigate how LAR improves BWR when we vary our training data size, we carried
out experiments on three different training data sets: FBIS (7.09M Chinese words, 9.28M
English words); Large1, which includes all corpora listed in Table 2 except for the United
Nations corpus (33.3M Chinese words, 35.79M English words); and Large2, which
</bodyText>
<tableCaption confidence="0.852584">
Table 4
BLEU scores for LAR, BWR, and their combinations.
</tableCaption>
<table confidence="0.656785833333333">
Reordering Configuration BLEU
BWR 32.47
LAR 32.17
All-in-One 33.03**++
BWR+LAR 33.30**++
** = Significantly better than BWR (p &lt; 0.01); ++ = significantly better than LAR (p &lt; 0.01).
</table>
<page confidence="0.995931">
554
</page>
<note confidence="0.986884">
Xiong et al. Linguistically Annotated Reordering
</note>
<tableCaption confidence="0.998938">
Table 5
</tableCaption>
<table confidence="0.942527166666667">
BLEU scores on different training data sets. Large1 refers to the corpora listed in Table 2 except
for the United Nations corpus. Large2 includes all corpora listed in Table 2.
Training Data BWR BWR+LAR Improvement
FBIS 24.97 26.52 1.55
Large1 29.96 30.78 0.82
Large2 32.47 33.30 0.83
</table>
<bodyText confidence="0.970332888888889">
consists of Large1 and the United Nations corpus (101.93M Chinese words, 112.78
English words). The language model remains the same for these three data sets because
it is trained on a much larger data set (181.1M words).
Table 5 shows the results. We observe that BWR+LAR is able to achieve a larger
improvement of 1.55 BLEU points over BWR on smaller training data. When we enlarge
the training data set from FBIS to Large1, both BWR and BWR+LAR improve quite a
bit. The difference between them is narrower, 0.82 BLEU points, but still significant.
When we continue to use more training data (Large2), the improvement obtained by
integrating LAR becomes stable at the 0.8 level.
</bodyText>
<subsectionHeader confidence="0.997268">
7.6 Effects of Linguistically Annotated Features
</subsectionHeader>
<bodyText confidence="0.999213444444444">
We conducted further experiments to evaluate the effects of individual linguistically an-
notated features. Using the reordering configuration of BWR+LAR, we augment LAR’s
feature pool incrementally: firstly using only syntactic categories9(sc) as features (170
features in total), then constructing composite categories (cc) for non-syntactic phrases
(sc + cc) (8.6K features), and finally introducing head words and their POS tags into
the feature pool (sc + cc + hw + ht) (166.1K features). This series of experiments demon-
strates the impact and degree of contribution made by each feature for reordering.
The experimental results are presented in Table 6, from which we have the following
observations:
</bodyText>
<listItem confidence="0.929290454545454">
1. Syntactic category alone improves the performance statistically
significantly. The baseline feature set sc with only 170 features improves
the BLEU score from 32.47 to 32.87.
2. Other linguistic information, provided by the categories of boundary
nodes (cc) and head word/tag pairs (hw + ht), also improves phrase
reordering. Producing composite categories for non-syntactic BTG nodes
and integrating head word/tag pairs into LAR as reordering features are
both effective, indicating that context information complements syntactic
category for capturing reordering patterns.
9 For a non-syntactic node, we only use the single category C, without constructing the composite category
L-C-R.
</listItem>
<page confidence="0.99335">
555
</page>
<table confidence="0.408363">
Computational Linguistics Volume 36, Number 3
</table>
<tableCaption confidence="0.983963">
Table 6
</tableCaption>
<bodyText confidence="0.4557815">
The effect of the linguistically annotated reordering model. (sc) is the baseline feature set,
(sc + cc) and (sc + cc + hw + ht) are extended feature sets for LAR.
</bodyText>
<table confidence="0.843217142857143">
Reordering Configuration BLEU
BWR 32.47
BWR + LAR (sc) 32.87*
BWR + LAR (sc + cc) 33.06**
BWR + LAR (sc + cc + hw + ht) 33.30**++
* = almost significantly better than BWR (p &lt; 0.075); ** = significantly better than BWR (p &lt; 0.01); ++ =
significantly better than BWR + LAR (sc) (p &lt; 0.01).
</table>
<sectionHeader confidence="0.706958" genericHeader="method">
8. Analysis
</sectionHeader>
<bodyText confidence="0.999723333333333">
We first obtain system translations of the test corpus. We generate word alignments
between source sentences and system/reference translations as described in Section 6.2.
Then we follow the analysis steps of Section 6.1 to investigate syntactic constituent
movement in the reference translations and system translations which are generated
using two different reordering configurations: BWR+LAR vs. BWR. In LAR, we use the
best reordering feature set (sc + cc + hw + ht).
</bodyText>
<subsectionHeader confidence="0.98006">
8.1 Syntactic Constituent Movement: Overview
</subsectionHeader>
<bodyText confidence="0.998670333333333">
If a syntactic constituent is fully reorderable or partially reorderable, it is considered to
be movable as a unit. To denote the proportion of syntactic constituents to be moved as
a unit, we introduce two variables REF-R-rate and SYS-R-rate, which are defined as
</bodyText>
<equation confidence="0.962709666666667">
count(SYS-reorderable nodes)
SYS-R-rate = (13)
count(multi-branching nodes)
count(REF-reorderable nodes)
REF-R-rate = (14)
count(multi-branching nodes)
</equation>
<bodyText confidence="0.8007795">
Table 7 shows the statistics of REF/SYS-reorderable nodes on the test corpus. From
this table, we have the following observations:
</bodyText>
<listItem confidence="0.996771">
1. A large number of nodes are REF-reorderable, accounting for 79.82% of all
the multi-branching nodes. This number shows that, in reference
</listItem>
<bodyText confidence="0.644803285714286">
translations, a majority of syntactic constituent movement across
Chinese–English can be performed by directly permuting constituents in a
sub-tree.
2. The R-rates of BWR and BWR+LAR are 77.46% and 81.79%, respectively.
The R-rate of BWR+LAR is obviously higher than that of BWR, which
suggests that BWR+LAR tends towards moving more syntactic
constituents together than BWR does. We will discuss this further later.
</bodyText>
<page confidence="0.998334">
556
</page>
<note confidence="0.989431">
Xiong et al. Linguistically Annotated Reordering
</note>
<tableCaption confidence="0.998261">
Table 7
</tableCaption>
<table confidence="0.960791125">
Statistics of multi-branching and REF/SYS-reorderable nodes per sentence.
BWR BWR+LAR
multi-branching node 18.68
REF-reorderable node 14.91
REF-R-rate 79.82%
SYS-fully-reorderable node 13.16 14.01
SYS-partially-reorderable node 1.31 1.26
SYS-R-rate 77.46% 81.79%
</table>
<subsectionHeader confidence="0.841912">
8.2 Syntactic Constituent Movement among Multiple Reference Translations
</subsectionHeader>
<bodyText confidence="0.993548588235294">
8.2.1 Differences in Movement Orientation. Because each source sentence is translated by
four different human experts, we would like to analyze the differences among reference
translations, especially on the orders of constituents being translated. Table 8 shows
the overall distribution over the number of different orders for each multi-branching
constituent among the reference translations.
In most cases (75.4%), four reference translations have completely the same order
for syntactic constituents. This makes it easier for our analysis to compare the system
order with the reference order. However, there are 22% cases where two different orders
are provided, which shows the flexibility of translation. According to our study, noun
phrases taking DNP or CP modifiers, as well as DNPs and CPs themselves, are more
likely to be translated in two different orders. Table 9 shows the percentages in which
two different orders for these constituents are observed in the reference corpus.
DNP and CP are always used as pre-modifiers of noun phrases in Chinese. They
often include the particle word RAJ (of ) at the ending position. The difference is that
DNP constructs a phrasal modifier whereas CP constructs a relative-clause modifier.
There is no fixed reordering pattern for DNP and CP and therefore for NP which takes
DNP/CP as a pre-modifier. In the DNP → NP DEG structure, the DEG (RAJ) can be
</bodyText>
<tableCaption confidence="0.981231">
Table 8
</tableCaption>
<table confidence="0.93179325">
Distribution of number of different orders by which syntactic constituents are translated in
references.
Number of different orders 1 2 3 4
Percentage 75.40 22 2.33 0.33
</table>
<tableCaption confidence="0.997224">
Table 9
</tableCaption>
<table confidence="0.930121166666667">
Two-order translation distribution of 4 NP-related constituents.
Constituent 2-order translation percentage
NP → DNP NP 16.93
NP → CP NP 9.43
CP → IP DEC 24.79
DNP → NP DEG 34.58
</table>
<page confidence="0.872432">
557
</page>
<note confidence="0.428311">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.979362">
translated into ’s or of, which are both appropriate in most cases, depending on the
translator’s preference. If the former is chosen, the order of DNP and therefore the
order for NP → DNP NP will both be straight: [1][2]. Otherwise, the two orders will be
inverted: [2][1]. Similarly, there are also different translation patterns for CP → IP DEC
and NP → CP NP. CP can be translated into “that + clause” or adjective-like phrases
in English. Figure 7 shows an example where the CP constituent is translated into an
adjective-like phrase. Although the “that + clause” must be placed behind the noun
phrase which it modifies, the order for adjective-like phrases is flexible (see Figure 7).
For those constituents with different reference orders, we compare the order of
the system translation to that of the reference translation which has the shortest edit
distance to the system translation as described herein so that we can take into account
the potential influence of different translations on the order of syntactic constituents.
8.2.2 REF-Non-Reorderable Constituents. We also study REF-R-rates for the 13 most fre-
quent constituents listed in Table 10. We find that two constituents, VP1 → PP VP2 and
NP1 → CP NP2, have the lowest REF-R-rates, 58.20% and 61.77%, respectively. This
means that about 40% of them are REF-non-reorderable. In order to understand the
reasons why they are non-reorderable in reference translations, we further investigate
REF-non-reorderable cases for the constituent type VP1 → PP VP2 and roughly classify
the reasons into three categories as follows.
</bodyText>
<listItem confidence="0.994144666666667">
1. Outside interruption. The reordering of PP and VP2 is interrupted by
other constituents outside VP1. For example, the Chinese sentence [NP A
A/somebody ] [VP1 [PP -J�...H-Vwhen...] [VP2 [Alsay NP[...] ] ] ] is translated
into when..., somebody said .... Here the translation of the first NP which is
outside VP1 is inserted between the translations of PP and VP2 and
therefore interrupts their reordering. Outside interruption accounts for
21.65% of REF-non-reorderable cases.
2. Inside interruption. The reordering of PP and VP2 is interrupted by the
combination of PP’s subnodes with VP2’s subnodes. Inside interruption
accounts for 48.45% of REF-non-reorderable cases, suggesting that it is the
major factor which decreases the reorderability of VP → PP VP. Because
both PP and VP have their own complex sub-structures, the inside
</listItem>
<figureCaption confidence="0.997822">
Figure 7
</figureCaption>
<bodyText confidence="0.773359333333333">
An example of the translation of NP → CP NP. This constituent can be translated in two
different orders: 1) the recently adopted statistical method (straight order); 2) the statistical
method recently adopted (inverted order).
</bodyText>
<page confidence="0.99766">
558
</page>
<note confidence="0.989362">
Xiong et al. Linguistically Annotated Reordering
</note>
<tableCaption confidence="0.998418">
Table 10
</tableCaption>
<table confidence="0.970186882352941">
F1-scores ( BWR+LAR vs. BWR) for the 13 most frequent constituents in the test corpus.
Constituents indicated in bold have relatively lower F1 score for reordering.
Type Constituent Percent. (%) SYS-R-rate (%) F1-score (%)
BWR BWR+LAR BWR BWR+LAR
VP VP → VV NP 8.12 79.22 84.10 76.97 80.53
NP VP → ADVP VP 4.30 63.45 65.86 70.83 73.67
Misc. VP → PP VP 1.87 60.32 70.37 39.29 40.33
VP → VV IP 1.82 79.35 86.14 77.16 82.26
NP → NN NN 6.88 84.68 85.18 76.17 79.10
NP → NP NP 5.12 82.13 84.93 69.25 72.17
NP → DNP NP 2.14 69.75 74.83 56.68 56.61
NP → CP NP 2.12 59.67 73.43 48.75 54.48
IP → NP VP 6.78 71.99 79.80 63.22 65.79
PP → P NP 3.63 80.63 85.95 82.75 84.93
CP → IP DEC 3.51 83.94 87.89 69.91 72.24
QP → CD CLP 2.74 66 65 67.52 68.47
DNP → NP DEG 2.43 85.98 89.84 67.5 68.75
</table>
<bodyText confidence="0.519383">
interruption is very complicated and includes a variety of cases, some of
</bodyText>
<listItem confidence="0.924569">
which are quite unexpected. Here we show two frequent examples of
inside interruption:
a. The preposition in the PP and the verb word/phrase of VP2 are
aligned to only one target word or one continuous phrase. For
example, 01...1��/pressure, �1...�Mb/be confident of, Q... __a/
suffer from, and so on. This is caused by the lexical divergence
problem.
b. The PP is first combined with the verb word of VP2 in an inverted
order, then combined with the remainder of VP2 in a straight order.
For example, [PP [P R] [omission1]] [VP [VV TWYIJ] [omission2]]
might be translated into learned from omission1 that omission2.
3. Parse error. This accounts for 29.90% of REF-non-reorderable cases.
Although these reasons are summarized from our analysis on the constituent type
VP → PP VP, they can be used to explain other REF-non-reorderable constituents, such
as NP → CP NP.
</listItem>
<subsectionHeader confidence="0.939959">
8.3 Syntactic Constituent Movement in System Translations
</subsectionHeader>
<footnote confidence="0.9170908">
8.3.1 Overall Reordering Precision and Recall of Syntactic Constituents. By summarizing all
syntactic reordering patterns (REF-SRP, SYS-SRP, and Match-SRP) for all constituents,
we can calculate the overall reordering precision and recall of syntactic constituents.
Table 11 shows the results for both BWR+LAR and BWR, where BWR+LAR clearly
outperforms BWR.
</footnote>
<page confidence="0.990569">
559
</page>
<note confidence="0.45212">
Computational Linguistics Volume 36, Number 3
</note>
<tableCaption confidence="0.993953">
Table 11
</tableCaption>
<table confidence="0.919672">
Syntactic reordering precision and recall of BWR+LAR vs. BWR on the test corpus.
Precision Recall F1
BWR 70.89 68.79 69.83
BWR+LAR 71.32 73.08 72.19
</table>
<subsubsectionHeader confidence="0.985508">
8.3.2 The Effect of Linguistic Knowledge on Phrase Movement. To understand the change
</subsubsectionHeader>
<bodyText confidence="0.9999464">
in phrase movement caused by linguistic knowledge, we further investigate how well
BWR and BWR+LAR reorder certain constituents, especially those with high distribu-
tion probability. Table 10 lists the 13 most frequent constituents, which jointly account
for 51.46% of all multi-branching constituents. Except for NP → DNP NP, the reorder-
ing F1 score of all these constituents in BWR+LAR is better than that in BWR.
Our hypothesis for the phrase movement change in BWR+LAR is that the integrated
linguistic knowledge makes phrase movement in BWR+LAR pay more respect to syn-
tactic constituent boundaries. The overall R-rates of BWR+LAR vs. BWR described in
Section 8.1 indicate that BWR+LAR tends towards moving more syntactic constituents
together than BWR does. We want to know whether this is also true for a specific
constituent type. The fourth and fifth columns in Table 10 present the R-rate for each
individual constituent type that we have analyzed. It is obvious that the R-rate of
BWR+LAR is much higher than that of BWR for almost all constituents. This indicates
that higher R-rate is one of the reasons for the higher performance of BWR+LAR.
To gain a more concrete understanding of this change, we show two examples for
the reordering of VP → PP VP in Figure 8. In both examples, BWR fails to move the PP
constituent to the right of the VP constituent, whereas BWR+LAR does it successfully.
By tracing the binary BTG trees generated by the decoder, we find that BWR generated
a very different BTG tree from the source parse tree whereas the BTG tree in BWR+LAR
almost matches the source parse tree. In the first example, BWR combines the VP phrase
</bodyText>
<figureCaption confidence="0.657509">
Figure 8
</figureCaption>
<bodyText confidence="0.833635">
Two examples for the translation of VP → PP VP. Square brackets indicate combinations in a
straight order and angular brackets represent combinations in an inverted order.
</bodyText>
<page confidence="0.993742">
560
</page>
<note confidence="0.975531">
Xiong et al. Linguistically Annotated Reordering
</note>
<bodyText confidence="0.985279041666666">
4S L, with Fj7 and then combines iEiG. The preposition word 01 is combined with
the NP phrase NHK, which makes the translation of NHK interrupt the reordering of
VP → PP VP in this example. The BWR tree in the second example is even worse. The
non-syntactic phrase Wf F �1 in the VP phrase is first combined with AV --,kR,
which is a sub-phrase of PP preceding VP in an inverted order. The remaining part of
the VP phrase is then merged. This merging process continues regardless of the source
parse tree. The comparison of BTG trees of BWR+LAR and BWR in the two examples
suggests that reordering models should respect syntactic structures in order to capture
reorderings under these structures.
Our observation on phrase movement change resonates with the recent efforts in
phrasal SMT that allow the decoder to prefer translations which show more respect
for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto,
Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other
words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early
syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has
receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and
non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008)
and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses
which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose
this as a hard constraint on the ITG constraint to allow reorderings which respect the
source parse tree. They all report significant improvements on different language pairs,
which indicates that syntactic cohesion is very useful for phrasal SMT. Our analysis
demonstrates that linguistically annotated reordering provides an alternative way to
incorporate syntactic cohesion into phrasal SMT.
</bodyText>
<subsectionHeader confidence="0.991234">
8.4 Challenges in Phrase Reordering and Suggestions
</subsectionHeader>
<bodyText confidence="0.9977644">
We highlight three constituent types in Table 10 (indicated in bold) which are much
more difficult to reorder, as indicated by their relatively lower F1 scores. The lower F1
scores indicate that BWR+LAR is not fully sufficient for reordering these constituents
although it performs much better than BWR. We find two main reasons for the lower F1
scores and provide suggestions accordingly as follows.
</bodyText>
<listItem confidence="0.8142496">
1. Constrained decoding. We observe that in reorderable constituents which
involve long-distance reorderings, their boundaries are easily violated by
phrases outside them. To prohibit boundary violations, we propose
constrained decoding. In constrained decoding, we define special zones
in source sentences. Reorderings and translations within the zones cannot
</listItem>
<bodyText confidence="0.875898666666667">
be interrupted by fragments outside the zones. We can also define other
constrained operations on the zones. For example, we can prohibit
swappings in any zones which contain punctuation (Xiong et al. 2008b).
The beginning and ending positions of a zone are automatically learned.
To be more flexible, they are not necessarily constituent boundaries.
Constrained decoding is different from both soft constraints (Cherry 2008;
Marton and Resnik 2008) and hard constraints (Yamamoto, Okuma, and
Sumita 2008). It can be considered as in between both of these because it is
harder than the former but softer than the latter.
</bodyText>
<listItem confidence="0.534661">
2. Integrating special reordering rules. Some constituents are indeed
non-reorderable as we discussed in Section 8.2.2. Inside or outside
</listItem>
<page confidence="0.99111">
561
</page>
<note confidence="0.590391">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.999936">
interruptions have to be allowed to obtain fluent translations for these
constituents. However, the allowance of interruptions is sometimes
beyond the representability of BTG rules. For example, to solve the lexical
divergence problem, bilingual rules with aligned lexicons have to be
introduced. To capture reorderings of these constituents, we propose to
integrate special reordering rules with richer contextual information into
BTG to extend BTG’s ability to deal with interruptions. Completely
replacing BTG with richer formalisms, such as hierarchical phrase
(Chiang 2005) and tree-to-string (Liu, Liu, and Lin 2006) or string-to-tree
(Marcu et al. 2006), introduces a huge extra cost. Instead, integrating a
small number of reordering rules into BTG to model reorderings of
non-reorderable constituents would be more desirable.
</bodyText>
<subsectionHeader confidence="0.874311">
8.5 Discussion
</subsectionHeader>
<bodyText confidence="0.9999447">
In the definition of syntactic reordering patterns, we only consider the relative order
of individual constituents on the target side. We do not consider whether or not they
remain contiguous on the target side. It is possible that other words are inserted be-
tween spans of two contiguous constituents. We use the term gap to refer to when
this happens. The absence of a gap in the definition of syntactic reordering patterns
may produce more matched SRPs and therefore lead to higher precision and recall.
Table 12 shows the revised overall precision and recall of syntactic reordering patterns
when we also compare gaps. The revised results show that BWR+LAR still significantly
outperforms BWR. This also applies to the 13 constituents identified in Table 10. The
analysis results obtained before are still valid when we consider gaps.
</bodyText>
<sectionHeader confidence="0.999495" genericHeader="related work">
9. Related Work
</sectionHeader>
<subsectionHeader confidence="0.994941">
9.1 Linguistically Motivated Phrase Reordering
</subsectionHeader>
<bodyText confidence="0.999992222222222">
There are various approaches which are devoted to incorporating linguistic knowledge
into phrase reordering. Generally, these approaches can be roughly divided into three
categories: (1) reordering the source language in a preprocessing step before decoding
begins; (2) estimating phrase movement with reordering models; and (3) capturing
reorderings by synchronous grammars. The preprocessing approach applies manual or
automatically extracted reordering knowledge from linguistic structures to transform
the source language sentence into a word order that is closer to the target sentence.
The second reordering approach moves phrases under certain reordering constraints
and estimates the probabilities of movement with linguistic information. In the third
</bodyText>
<tableCaption confidence="0.994639">
Table 12
</tableCaption>
<table confidence="0.9427986">
Revised overall precision and recall of BWR+LAR vs. BWR on the test corpus when we consider
the gap in syntactic reordering patterns.
Precision Recall F1
BWR (gap) 46.28 44.91 45.58
BWR+LAR (gap) 48.80 50 49.39
</table>
<page confidence="0.994806">
562
</page>
<note confidence="0.970041">
Xiong et al. Linguistically Annotated Reordering
</note>
<bodyText confidence="0.990400309523809">
approach, reordering knowledge is included in synchronous rules. The last two cate-
gories reorder the source sentence during decoding, which distinguishes them from the
first approach. Note that some researchers integrate multiple reordering approaches in
one decoder (Lin 2004; Quirk, Menezes, and Cherry 2005; Ge, Ittycheriah, and Papineni
2008).
9.1.1 The Preprocessing Approach. In early work, Brown et al. (1992) describe an approach
to reordering French phrases in a preprocessing step. Xia and McCord (2004) present a
preprocessing approach which automatically learns reordering patterns based on CFG
productions. Since then, the preprocessing approach seems to have been more popular.
Collins, Koehn, and Kucerova (2005) propose reordering German clauses with six types
of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees
using fine-grained human-written rules, mostly concentrating on VP and NP structures.
Li et al. (2007) improve the preprocessing approach by generating n-best reordered
source sentences with reordering knowledge automatically learned from the alignments
between source parse trees and target translations. The approach proposed in Li et al.
also enhances the connection between the preprocessing and decoding by adding a
source reordering probability feature. Other approaches introduced in Nießn and Ney
(2001), Popovi´c and Ney (2006), and Zhang, Zens, and Ney (2007) use morphological,
POS, and chunk knowledge in the preprocessing approach, respectively.
9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM con-
straint (Zens and Ney 2003), the early work uses a distortion-based reordering model
to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG
constraint, the corresponding model is the flat model which assigns a prior probability
to the straight or inverted order (Wu 1996). These two models don’t respect the content
of phrases which are moved. To address this issue, lexicalized reordering models which
are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn
et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and
Lin (2006) introduce a more flexible reordering model under the ITG constraint using
discriminative features which are automatically learned from a training corpus. Zhang
et al. (2007) propose a model for syntactic phrase reordering which uses syntactic
knowledge from source parse trees. Our reordering approach is most similar to those in
Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic
knowledge and allowing non-syntactic phrase reordering.
9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use
synchronous grammars to capture reorderings between two languages. Chiang (2005)
introduces formal synchronous grammars for phrase-based translation. In his work,
hierarchical reordering knowledge is included in synchronous rules which are automat-
ically learned from word-aligned corpus. In linguistically syntax-based models, string-
to-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and
Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are
explored. Linguistical reordering knowledge is naturally included in these syntax-based
translation rules.
</bodyText>
<subsectionHeader confidence="0.997258">
9.2 Automatic Analysis of Reordering
</subsectionHeader>
<bodyText confidence="0.9839965">
Although there is a variety of work on phrase reordering, automatic analysis of phrase
reordering is not widely explored in the SMT literature. Chiang et al. (2005) propose
</bodyText>
<page confidence="0.9946">
563
</page>
<note confidence="0.591717">
Computational Linguistics Volume 36, Number 3
</note>
<bodyText confidence="0.999913052631579">
an automatic method to compare different system outputs in a fine-grained manner
with regard to reordering. In their method, common word n-grams occurring in both
reference translations and system translations are extracted and generalized to part-of-
speech tag sequences. A recall is calculated for each certain tag sequence to indicate the
ability of reordering models to capture this tag sequence in system translations. Popovic
et al. (2006) use the relative difference between WER (word error rate) and PER (position
independent word error rate) to indicate reordering errors. The larger the difference, the
more reordering errors there are.
Callison-Burch et al. (2007) propose a constituent-based evaluation that is very simi-
lar to our method in Steps (1)–(3). They also parse the source sentence and automatically
align the parse tree with the reference/system translations. The difference is that they
highlight constituents from the parse tree to enable human evaluation of the translations
of these constituents, rather than automatically analyzing constituent movement. They
use this method for human evaluation in the shared translation task of the 2007 and
2008 ACL Workshop on Statistical Machine Translation.
Fox (2002) systematically studies syntactic cohesion between French and English
using human translations and alignments. Compared with her work, our analysis here
includes, but is not limited to, an investigation of syntactic cohesion in an actual MT
system.
</bodyText>
<sectionHeader confidence="0.927074" genericHeader="conclusions">
10. Conclusion
</sectionHeader>
<bodyText confidence="0.9999724">
We have presented a novel linguistically motivated phrase reordering approach:
Linguistically Annotated Reordering. The LAR approach incorporates soft linguistic
knowledge from the source parse tree into hard hierarchical skeletons generated by
BTG in phrasal SMT. To automatically learn reordering features, we have introduced
algorithms for reordering example extraction and linguistic annotation. We have also
proposed a new syntax-based analysis method to detect syntactic constituent movement
in human/machine translations.
We have conducted experiments on large-scale training data to evaluate LAR and
BWR as well as the reordering example extraction algorithms. Our evaluation results
show that:
</bodyText>
<listItem confidence="0.937754375">
1. Extracting reordering examples directly from word alignments is much
better than from BTG-style trees which are built from word alignments.
2. Selection rules which bias the reordering model towards smaller
swappings improve translation quality.
3. BWR+LAR significantly outperforms BWR, which suggests that the
integration of linguistic knowledge improves reordering; and tuning two
separate reordering models is better than the All-in-One combination
method.
</listItem>
<bodyText confidence="0.8839655">
We have further analyzed the outputs of BWR+LAR vs. BWR using the proposed
syntax-based analysis method. Our analysis results show that:
1. BWR+LAR achieves a significantly higher reordering precision and recall
than BWR does with regard to syntactic constituent movement.
</bodyText>
<page confidence="0.990833">
564
</page>
<note confidence="0.780978">
Xiong et al. Linguistically Annotated Reordering
</note>
<bodyText confidence="0.932913357142857">
2. For most reorderable constituents, integrating source-side linguistic
knowledge into the reordering model can significantly improve
reorderings by guiding reordering models to prefer hypotheses that pay
more respect to constituent boundaries.
3. For non-reorderable constituents or constituents involving long-distance
reorderings, integrating source-side linguistic knowledge into the
reordering model is not sufficient to avoid illegal boundary violations or to
capture reordering patterns.
To avoid illegal boundary violations in long-span constituents, we suggest con-
strained decoding, which protects special zones in the source sentence from being
interrupted by phrases outside the zones. Beginning and ending positions of the zones
are automatically learned using lexical and syntactic knowledge. To capture complex re-
orderings which cross constituent boundaries, phrasal SMT should integrate reordering
rules with richer contextual information.
</bodyText>
<sectionHeader confidence="0.998368" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999548333333333">
We would like to thank the three anonymous
reviewers for their helpful comments and
suggestions.
</bodyText>
<sectionHeader confidence="0.998286" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996361219178082">
Al-Onaizan, Yaser and Kishore Papineni.
2006. Distortion models for statistical
machine translation. In Proceedings
of the 21st International Conference on
Computational Linguistics and 44th
Annual Meeting of the Association for
Computational Linguistics, pages 529–536,
Sydney.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, John D. Lafferty,
and Robert L. Mercer. 1992. Analysis,
statistical transfer, and synthesis in
machine translation. In Proceedings
of the Fourth International Conference on
Theoretical and Methodological Issues in
Machine Translation, pages 83–100,
Montreal.
Callison-Burch, Chris, Cameron Fordyce,
Philipp Koehn, Christof Monz, and Josh
Schroeder. 2007. (Meta-) evaluation of
machine translation. In Proceedings of the
Second Workshop on Statistical Machine
Translation, pages 136–158, Prague.
Cherry, Colin. 2008. Cohesive phrase-based
decoding for statistical machine
translation. In Proceedings of ACL-08: HLT,
pages 72–80, Columbus, OH.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
translation. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics, pages 263–270,
Ann Arbor, MI.
Chiang, David, Adam Lopez, Nitin
Madnani, Christof Monz, Philip Resnik,
and Michael Subotin. 2005. The hiero
machine translation system: Extensions,
evaluation, and analysis. In Proceedings of
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 779–786,
Vancouver.
Collins, Michael, Philipp Koehn, and Ivona
Kucerova. 2005. Clause restructuring for
statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 531–540, Ann Arbor, MI.
Eisner, Jason. 2003. Learning non-isomorphic
tree mappings for machine translation. In
The Companion Volume to the Proceedings of
41st Annual Meeting of the Association for
Computational Linguistics, pages 205–208,
Sapporo.
Fox, Heidi. 2002. Phrasal cohesion and
statistical machine translation. In
Proceedings of the 2002 Conference on
Empirical Methods in Natural Language
Processing, pages 304–311,
Philadelphia, PA.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What’s in a
translation rule? In Proceedings of the
Human Language Technology Conference
of the North American Chapter of the
Association for Computational Linguistics:
HLT-NAACL 2004, pages 273–280,
Boston, MA.
Ge, Niyu, Abe Ittycheriah, and Kishore
Papineni. 2008. Multiple reorderings in
phrase-based machine translation. In
Proceedings of the ACL-08: HLT Second
Workshop on Syntax and Structure in
</reference>
<page confidence="0.992012">
565
</page>
<note confidence="0.520535">
Computational Linguistics Volume 36, Number 3
</note>
<reference confidence="0.997583822033899">
Statistical Translation (SSST-2), pages 61–68,
Columbus, OH.
Huang, Liang, Kevi Knight, and Aravind
Joshi. 2006. Statistical syntax-directed
translation with extended domain of
locality. In Proceedings of the 7th Conference
of the Association for Machine Translation
of the Americas, pages 66–73,
Cambridge, MA.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation. In
Proceedings of EMNLP 2004, pages 388–395,
Barcelona.
Koehn, Philipp, Amittai Axelrod, Alexandra
Birch Mayne, Chris Callison-Burch,
Miles Osborne, and David Talbot. 2005.
Edinburgh system description for the
2005 IWSLT speech translation
evaluation. In Proceedings of the
International Workshop on Spoken
Language Translation 2005, pages 78–85,
Pittsburgh, PA.
Koehn, Philipp, Franz Joseph Och, and
Daniel Marcu. 2003. Statistical
phrase-based translation. In Proceedings of
the 2003 Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 58–54, Edmonton.
Kumar, Shankar and William Byrne. 2005.
Local phrase reordering models for
statistical machine translation. In
Proceedings of Human Language Technology
Conference and Conference on Empirical
Methods in Natural Language Processing,
pages 161–168, Vancouver.
Li, Chi-Ho, Minghui Li, Dongdong Zhang,
Mu Li, Ming Zhou, and Yi Guan. 2007. A
probabilistic approach to syntax-based
reordering for statistical machine
translation. In Proceedings of the 45th
Annual Meeting of the Association of
Computational Linguistics, pages 720–727,
Prague.
Lin, Dekang. 2004. A path-based transfer
model for machine translation. In
Proceedings of the 20th International
Conference on Computational Linguistics
(Coling 2004), pages 625–630, Geneva.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006.
Tree-to-string alignment template for
statistical machine translation. In
Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 609–616,
Sydney.
Marcu, Daniel, Wei Wang, Abdessamad
Echihabi, and Kevin Knight. 2006. SPMT:
Statistical machine translation with
syntactified target language phrases. In
Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 44–52, Sydney.
Marton, Yuval and Philip Resnik. 2008. Soft
syntactic constraints for hierarchical
phrased-based translation. In Proceedings
of ACL-08: HLT, pages 1003–1011,
Columbus, OH.
Navarro, Gonzalo. 2001. A guided tour to
approximate string matching. ACM
Computing Surveys, 33(1):31–88.
Nießn, Sonja and Hermann Ney. 2001.
Morpho-syntactic analysis for
reordering in statistical machine
translation. In Proceedings of MT Summit
VIII, pages 247–252, Santiago de
Compostela.
Och, Franz Josef. 2002. Statistical Machine
Translation: From Single-Word Models to
Alignment Templates. Ph.D. thesis, RWTH
Aachen University, Germany.
Och, Franz Josef. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics,
pages 160–167, Sapporo.
Och, Franz Josef and Hermann Ney. 2000.
Improved statistical alignment models. In
Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics,
pages 440–447, Hong Kong.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU: A
method for automatic evaluation of
machine translation. In Proceedings of 40th
Annual Meeting of the Association for
Computational Linguistics, pages 311–318,
Philadelphia, PA.
Popovic, Maja, Adri`a de Gispert, Deepa
Gupta, Patrik Lambert, Hermann Ney,
Jos´e B. Mari˜no, Marcello Federico, and
Rafael Banchs. 2006. Morpho-syntactic
information for automatic error analysis
of statistical machine translation output.
In Proceedings on the Workshop on
Statistical Machine Translation, pages 1–6,
New York, NY.
Popovi´c, Maja and Hermann Ney. 2006.
Pos-based word reorderings for statistical
machine translation. In Proceedings of the
Fifth International Conference on Language
Resources and Evaluation (LREC 2006),
pages 1278–1283, Genoa.
Quirk, Chris, Arul Menezes, and Colin
Cherry. 2005. Dependency treelet
translations: Syntactically informed
phrasal smt. In Proceedings of the 43rd
</reference>
<page confidence="0.997475">
566
</page>
<note confidence="0.972485">
Xiong et al. Linguistically Annotated Reordering
</note>
<reference confidence="0.999524093220339">
Annual Meeting of the ACL, pages 271–279,
Ann Arbor, MI.
Stolcke, Andreas. 2002. SRILM—an
extensible language modeling toolkit.
In Proceedings of the 7th International
Conference on Spoken Language
Processing (ICSLP 2002), pages 901–904,
Denver, CO.
Tillman, Christoph. 2004. A unigram
orientation model for statistical machine
translation. In Proceedings of the Human
Language Technology Conference of the North
American Chapter of the Association for
Computational Linguistics (HLT-NAACL
2004): Short Papers, pages 101–104,
Boston, MA.
Wang, Chao, Michael Collins, and Philipp
Koehn. 2007. Chinese syntactic reordering
for statistical machine translation. In
Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL),
pages 737–745, Prague.
Wu, Dekai. 1996. A polynomial-time
algorithm for statistical machine
translation. In Proceedings of the 34th
Annual Meeting of the Association for
Computational Linguistics, pages 152–158,
Santa Cruz, CA.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377–403.
Wu, Dekai, Marine Carpuat, and Yihai Shen.
2006. Inversion transduction grammar
coverage of Arabic-English word
alignment for tree-structured statistical
machine translation. In Proceeding
of the IEEE/ACL 2006 Workshop on
Spoken Language Technology (SLT 2006),
pages 234–237, Aruba.
Xia, Fei and Michael McCord. 2004.
Improving a statistical MT system
with automatically learned rewrite
patterns. In Proceedings of the 20th
International Conference on Computational
Linguistics (Coling 2004), pages 508–514,
Geneva.
Xiong, Deyi, Qun Liu, and Shouxun Lin.
2005. Parsing the penn chinese treebank
with semantic knowledge. In Proceedings of
The 2nd International Joint Conference on
Natural Language Processing (IJCNLP-05),
pages 70–81, Jeju Island.
Xiong, Deyi, Qun Liu, and Shouxun Lin.
2006. Maximum entropy based phrase
reordering model for statistical machine
translation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 521–528, Sydney.
Xiong, Deyi, Min Zhang, Aiti Aw, and
Haizhou Li. 2008a. A linguistically
annotated reordering model for
BTG-based statistical machine
translation. In Proceedings of ACL-08:
HLT, Short Papers, pages 149–152,
Columbus, OH.
Xiong, Deyi, Min Zhang, Aiti Aw, Haitao Mi,
Qun Liu, and Shouxun Lin. 2008b.
Refinements in BTG-based statistical
machine translation. In Proceedings
of the Third International Joint Conference
on Natural Language Processing,
pages 505–512, Hyderabad.
Xue, Nianwen, Fei Xia, Shizhe Huang, and
Anthony Kroch. 2000. The bracketing
guidelines for the Penn Chinese treebank
(3.0). Technical report IRCS 00-07,
University of Pennsylvania Institute for
Research in Cognitive Science,
Philadelphia.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics,
pages 523–530, Toulouse.
Yamamoto, Hirofumi, Hideo Okuma, and
Eiichiro Sumita. 2008. Imposing
constraints from the source tree on ITG
constraints for SMT. In Proceedings of the
ACL-08: HLT Second Workshop on Syntax
and Structure in Statistical Translation
(SSST-2), pages 1–9, Columbus, OH.
Zens, Richard, and Hermann Ney. 2003.
A comparative study on reordering
constraints in statistical machine
translation. In Proceedings of the 41st
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 144–151, Sapporo, Japan.
Zhang, Dongdong, Mu Li, Chi-Ho Li, and
Ming Zhou. 2007. Phrase reordering
model integrating syntactic knowledge
for SMT. In Proceedings of the 2007 Joint
Conference on Empirical Methods in
Natural Language Processing and
Computational Natural Language
Learning (EMNLP-CoNLL), pages 533–540,
Prague.
Zhang, Hao and Daniel Gildea. 2005.
Stochastic lexicalized inversion
transduction grammar for alignment. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 475–482, Ann Arbor, MI.
</reference>
<page confidence="0.931556">
567
</page>
<reference confidence="0.979097689655172">
Computational Linguistics Volume 36, Number 3
Zhang, Hao, Daniel Gildea, and David
Chiang. 2008. Extracting synchronous
grammar rules from word-level
alignments in linear time. In Proceedings of
the 22nd International Conference on
Computational Linguistics (Coling 2008),
pages 1081–1088, Manchester.
Zhang, Min, Hongfei Jiang, Aiti Aw, Haizhou
Li, Chew Lim Tan, and Sheng Li. 2008. A
tree sequence alignment-based tree-to-tree
translation model. In Proceedings of ACL-08:
HLT, pages 559–567, Columbus, OH.
Zhang, Yuqi, Richard Zens, and Hermann
Ney. 2007. Chunk-level reordering of
source language sentences with
automatically learned rules for statistical
machine translation. In Proceedings
of SSST, NAACL-HLT 2007/AMTA
Workshop on Syntax and Structure in
Statistical Translation, pages 1–8,
Rochester, NY.
Zollmann, Andreas, Ashish Venugopal, and
Stephan Vogel. 2008. The CMU
syntax-augmented machine translation
system: SAMT on hadoop with N-best
alignments. In Proceedings of International
Workshop on Spoken Language Translation
(IWSLT), pages 18–25, Honolulu, HI.
</reference>
<page confidence="0.996725">
568
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.917012">
<title confidence="0.991813">Linguistically Annotated Reordering: Evaluation and Analysis</title>
<affiliation confidence="0.99631175">Institute for Infocomm Research Institute for Infocomm Research Institute for Infocomm Research Institute for Infocomm Research</affiliation>
<abstract confidence="0.995877714285714">Linguistic knowledge plays an important role in phrase movement in statistical machine translation. To efficiently incorporate linguistic knowledge into phrase reordering, we propose a new approach: Linguistically Annotated Reordering (LAR). In LAR, we build hard hierarchical skeletons and inject soft linguistic knowledge from source parse trees to nodes of hard skeletons during translation. The experimental results on large-scale training data show that LAR is comparable to boundary word-based reordering (BWR) (Xiong, Liu, and Lin 2006), which is a very competitive lexicalized reordering approach. When combined with BWR, LAR provides complementary information for phrase reordering, which collectively improves the BLEU score significantly. To further understand the contribution of linguistic knowledge in LAR to phrase reordering, we introduce a syntax-based analysis method to automatically detect constituent movement in both reference and system translations, and summarize syntactic reordering patterns that are captured by reordering models. With the proposed analysis method, we conduct a comparative analysis that not only provides the insight into how linguistic knowledge affects phrase movement but also reveals new challenges in phrase reordering.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>529--536</pages>
<location>Sydney.</location>
<contexts>
<context position="73731" citStr="Al-Onaizan and Papineni 2006" startWordPosition="11604" endWordPosition="11607">th Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the flat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more flexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous </context>
</contexts>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Al-Onaizan, Yaser and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 529–536, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
</authors>
<title>Analysis, statistical transfer, and synthesis in machine translation.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>83--100</pages>
<location>Montreal.</location>
<contexts>
<context position="71959" citStr="Brown et al. (1992)" startWordPosition="11346" endWordPosition="11349">e test corpus when we consider the gap in syntactic reordering patterns. Precision Recall F1 BWR (gap) 46.28 44.91 45.58 BWR+LAR (gap) 48.80 50 49.39 562 Xiong et al. Linguistically Annotated Reordering approach, reordering knowledge is included in synchronous rules. The last two categories reorder the source sentence during decoding, which distinguishes them from the first approach. Note that some researchers integrate multiple reordering approaches in one decoder (Lin 2004; Quirk, Menezes, and Cherry 2005; Ge, Ittycheriah, and Papineni 2008). 9.1.1 The Preprocessing Approach. In early work, Brown et al. (1992) describe an approach to reordering French phrases in a preprocessing step. Xia and McCord (2004) present a preprocessing approach which automatically learns reordering patterns based on CFG productions. Since then, the preprocessing approach seems to have been more popular. Collins, Koehn, and Kucerova (2005) propose reordering German clauses with six types of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees using fine-grained human-written rules, mostly concentrating on VP and NP structures. Li et al. (2007) improve the preprocessing approach by generating</context>
</contexts>
<marker>Brown, Pietra, Pietra, Lafferty, Mercer, 1992</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, John D. Lafferty, and Robert L. Mercer. 1992. Analysis, statistical transfer, and synthesis in machine translation. In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation, pages 83–100, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(Meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>136--158</pages>
<location>Prague.</location>
<contexts>
<context position="75874" citStr="Callison-Burch et al. (2007)" startWordPosition="11916" endWordPosition="11919">m outputs in a fine-grained manner with regard to reordering. In their method, common word n-grams occurring in both reference translations and system translations are extracted and generalized to part-ofspeech tag sequences. A recall is calculated for each certain tag sequence to indicate the ability of reordering models to capture this tag sequence in system translations. Popovic et al. (2006) use the relative difference between WER (word error rate) and PER (position independent word error rate) to indicate reordering errors. The larger the difference, the more reordering errors there are. Callison-Burch et al. (2007) propose a constituent-based evaluation that is very similar to our method in Steps (1)–(3). They also parse the source sentence and automatically align the parse tree with the reference/system translations. The difference is that they highlight constituents from the parse tree to enable human evaluation of the translations of these constituents, rather than automatically analyzing constituent movement. They use this method for human evaluation in the shared translation task of the 2007 and 2008 ACL Workshop on Statistical Machine Translation. Fox (2002) systematically studies syntactic cohesi</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Callison-Burch, Chris, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (Meta-) evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 136–158, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Cohesive phrase-based decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>72--80</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="66125" citStr="Cherry 2008" startWordPosition="10487" endWordPosition="10488">st combined with AV --,kR, which is a sub-phrase of PP preceding VP in an inverted order. The remaining part of the VP phrase is then merged. This merging process continues regardless of the source parse tree. The comparison of BTG trees of BWR+LAR and BWR in the two examples suggests that reordering models should respect syntactic structures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita</context>
<context position="68373" citStr="Cherry 2008" startWordPosition="10817" endWordPosition="10818"> prohibit boundary violations, we propose constrained decoding. In constrained decoding, we define special zones in source sentences. Reorderings and translations within the zones cannot be interrupted by fragments outside the zones. We can also define other constrained operations on the zones. For example, we can prohibit swappings in any zones which contain punctuation (Xiong et al. 2008b). The beginning and ending positions of a zone are automatically learned. To be more flexible, they are not necessarily constituent boundaries. Constrained decoding is different from both soft constraints (Cherry 2008; Marton and Resnik 2008) and hard constraints (Yamamoto, Okuma, and Sumita 2008). It can be considered as in between both of these because it is harder than the former but softer than the latter. 2. Integrating special reordering rules. Some constituents are indeed non-reorderable as we discussed in Section 8.2.2. Inside or outside 561 Computational Linguistics Volume 36, Number 3 interruptions have to be allowed to obtain fluent translations for these constituents. However, the allowance of interruptions is sometimes beyond the representability of BTG rules. For example, to solve the lexical</context>
</contexts>
<marker>Cherry, 2008</marker>
<rawString>Cherry, Colin. 2008. Cohesive phrase-based decoding for statistical machine translation. In Proceedings of ACL-08: HLT, pages 72–80, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="5778" citStr="Chiang (2005)" startWordPosition="812" endWordPosition="813">ricted by the ITG constraint (Wu 1997). Although it only allows two orders (straight or inverted) of nodes in any binary branching structure, it is broadly verified that the ITG constraint has good coverage of word reorderings on various language pairs (Wu, Carpuat, and Shen 2006). This makes phrase reordering in phrasal SMT a more tractable task. After enhancing phrasal SMT with a hard hierarchical skeleton, we further inject soft linguistic information into the nodes of the skeleton. We annotate each BTG node 1 In this article, we use Penn Chinese Treebank phrase labels (Xue et al. 2000). 2 Chiang (2005) also generates hierarchical structures in phrasal SMT. One difference is that Chiang’s hierarchical grammar is lexicon-sensitive because the model requires at least one pair of aligned words in each rule except for the “glue rule.” The other difference is that his grammar allows multiple nonterminals. These two differences make Chiang’s grammar more expressive than the BTG but at the cost of learning a larger model. 536 Xiong et al. Linguistically Annotated Reordering with syntactic and lexical elements by projecting the source parse tree onto the BTG binary tree. The challenge, of course, is</context>
<context position="66484" citStr="Chiang 2005" startWordPosition="10543" endWordPosition="10544">apture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose this as a hard constraint on the ITG constraint to allow reorderings which respect the source parse tree. They all report significant improvements on different language pairs, which indicates that syntactic cohesion is very useful for phrasal SMT. Our analysis demonstrates that linguistically annotated reordering provides an alternative way t</context>
<context position="69336" citStr="Chiang 2005" startWordPosition="10958" endWordPosition="10959">l Linguistics Volume 36, Number 3 interruptions have to be allowed to obtain fluent translations for these constituents. However, the allowance of interruptions is sometimes beyond the representability of BTG rules. For example, to solve the lexical divergence problem, bilingual rules with aligned lexicons have to be introduced. To capture reorderings of these constituents, we propose to integrate special reordering rules with richer contextual information into BTG to extend BTG’s ability to deal with interruptions. Completely replacing BTG with richer formalisms, such as hierarchical phrase (Chiang 2005) and tree-to-string (Liu, Liu, and Lin 2006) or string-to-tree (Marcu et al. 2006), introduces a huge extra cost. Instead, integrating a small number of reordering rules into BTG to model reorderings of non-reorderable constituents would be more desirable. 8.5 Discussion In the definition of syntactic reordering patterns, we only consider the relative order of individual constituents on the target side. We do not consider whether or not they remain contiguous on the target side. It is possible that other words are inserted between spans of two contiguous constituents. We use the term gap to re</context>
<context position="74399" citStr="Chiang (2005)" startWordPosition="11704" endWordPosition="11705">reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax-based translation rules. 9.2 Automatic Analysis of Reordering Although there is a va</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>Chiang, David. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 263–270, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Adam Lopez</author>
<author>Nitin Madnani</author>
<author>Christof Monz</author>
<author>Philip Resnik</author>
<author>Michael Subotin</author>
</authors>
<title>The hiero machine translation system: Extensions, evaluation, and analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>779--786</pages>
<location>Vancouver.</location>
<contexts>
<context position="75141" citStr="Chiang et al. (2005)" startWordPosition="11807" endWordPosition="11810">cluded in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax-based translation rules. 9.2 Automatic Analysis of Reordering Although there is a variety of work on phrase reordering, automatic analysis of phrase reordering is not widely explored in the SMT literature. Chiang et al. (2005) propose 563 Computational Linguistics Volume 36, Number 3 an automatic method to compare different system outputs in a fine-grained manner with regard to reordering. In their method, common word n-grams occurring in both reference translations and system translations are extracted and generalized to part-ofspeech tag sequences. A recall is calculated for each certain tag sequence to indicate the ability of reordering models to capture this tag sequence in system translations. Popovic et al. (2006) use the relative difference between WER (word error rate) and PER (position independent word err</context>
</contexts>
<marker>Chiang, Lopez, Madnani, Monz, Resnik, Subotin, 2005</marker>
<rawString>Chiang, David, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, and Michael Subotin. 2005. The hiero machine translation system: Extensions, evaluation, and analysis. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 779–786, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>531--540</pages>
<location>Ann Arbor, MI.</location>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Collins, Michael, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 531–540, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>205--208</pages>
<location>Sapporo.</location>
<contexts>
<context position="74314" citStr="Eisner (2003)" startWordPosition="11693" endWordPosition="11694"> Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more flexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Eisner, Jason. 2003. Learning non-isomorphic tree mappings for machine translation. In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics, pages 205–208, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>304--311</pages>
<contexts>
<context position="8160" citStr="Fox 2002" startWordPosition="1181" endWordPosition="1182">rdering when combined with BWR. We want to further study what happens when we combine BWR with LAR. In particular, we want to investigate to what extent the integrated linguistic knowledge (from LAR) changes phrase movement in an actual SMT system, and in what direction the change takes place. The investigations will enable us to have a better understanding of the relationship between phrase movement and linguistic context, and therefore to explore linguistic knowledge more effectively in phrasal SMT. Because syntactic constituents are often moved together across languages during translation (Fox 2002), we particularly study how linguistic knowledge affects syntactic constituent movement. To that end, we introduce a syntax-based analysis method. We parse source sentences, and align the parse trees with reference translations as well as system translations. We then summarize syntactic reordering patterns using contextfree grammar (CFG) rules from the obtained tree-to-string alignments. The extracted reordering patterns clearly show the trace of syntactic constituent movement in both reference translations and system translations. With the proposed analysis method, we analyze the combination </context>
<context position="23319" citStr="Fox 2002" startWordPosition="3640" endWordPosition="3641">ing that we have a reordering example (inverted, T 7A 15Q |on July 15, *T ME —&apos;-1 Q/j� |held its presidential and parliament elections), leftmost/rightmost source words {T, 15Q, T, l*} and target words {on, 15, held, elections} will be extracted as boundary words. Each boundary word will form a reordering feature as follows � 1, fn = bval, o = inverted hi(o, Al, Ar, Ap) = 0, otherwise where fn denotes the feature name, and bval is the corresponding boundary word. There are two reasons why boundary words are used as important clues for reordering: 1. Phrases frequently cohere across languages (Fox 2002). In cohesive phrase movement, boundary words directly interact with the external contexts of Figure 3 The BTG-style tree built from the word alignment in Figure 1. We use ([i, j], [p, q]) to denote a tree node, where i, j and p, q are the beginning and ending indices in the source and target language, respectively. 543 Computational Linguistics Volume 36, Number 3 phrases. This suggests that boundary words might contain information for phrase reordering. 2. The quantitative analysis in Xiong, Liu, and Lin (2006, page 525) further shows that boundary words indeed contain information for order </context>
<context position="32430" citStr="Fox (2002)" startWordPosition="5115" endWordPosition="5116">o align parse trees with system translations so that we can learn the movement of syntactic constituents carried out by the reordering models and investigate the performance of the reordering models by comparing both alignments. For notational convenience, we denote syntactic reordering patterns that are extracted from the alignments between source parse trees and reference translations as REF-SRP and those from the alignments between source parse trees and system translations as SYS-SRP. We refer to those present in both alignments under some conditions 5 We adopt the definition of span from Fox (2002): Given a node n that covers a word sequence sp...si...sq and a word alignment matrix M, the target words aligned to n are {ti : ti E M(si)}. We define the target span of node n as nT = (min({ti}), max({ti})). Note that nT may contain words that are not in {ti}. 6 Please note that the order of structures may not be defined in some cases (see Section 6.3). 547 Computational Linguistics Volume 36, Number 3 that will be described in Section 6.4 as MATCH-SRP. To conduct a thorough analysis on the reorderings, we carry out the following steps on the test corpus (source sentences + reference transla</context>
<context position="35023" citStr="Fox (2002)" startWordPosition="5532" endWordPosition="5533">se for each target phrase in the system translation. This will generate a phrase alignment between the source sentence and system translation. Given the phrase alignment and word alignments within the phrase stored in the phrase table, we can easily obtain word alignments between the whole source sentence and system translation. 6.3 Generating Target Spans and Orders Given the source parse tree and the word alignment between a source sentence and a reference/system translation, for each multi-branching node o → β1...βn, we firstly determine the target span βTi for each child node βi following Fox (2002). If one child node is aligned to NULL, we define a special target span for it. The order for this special target span will remain the same as the child node occurring in β1...βn. Two target spans may overlap with each other because of inherent divergences between two languages or noise in the word alignment. When this happens on two neighboring nodes βi and βi+1, we combine these two nodes together and redefine a target span βTi&amp;i+1 for the combined node. This process will be repeated until no more neighboring nodes can be combined. For example, the target span of nodes a and b in 548 Xiong e</context>
<context position="66278" citStr="Fox 2002" startWordPosition="10509" endWordPosition="10510">rocess continues regardless of the source parse tree. The comparison of BTG trees of BWR+LAR and BWR in the two examples suggests that reordering models should respect syntactic structures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose this as a hard constraint on the ITG constraint to allow reorderings which respect the source parse tree. They all report significant impr</context>
<context position="76434" citStr="Fox (2002)" startWordPosition="12001" endWordPosition="12002">ing errors there are. Callison-Burch et al. (2007) propose a constituent-based evaluation that is very similar to our method in Steps (1)–(3). They also parse the source sentence and automatically align the parse tree with the reference/system translations. The difference is that they highlight constituents from the parse tree to enable human evaluation of the translations of these constituents, rather than automatically analyzing constituent movement. They use this method for human evaluation in the shared translation task of the 2007 and 2008 ACL Workshop on Statistical Machine Translation. Fox (2002) systematically studies syntactic cohesion between French and English using human translations and alignments. Compared with her work, our analysis here includes, but is not limited to, an investigation of syntactic cohesion in an actual MT system. 10. Conclusion We have presented a novel linguistically motivated phrase reordering approach: Linguistically Annotated Reordering. The LAR approach incorporates soft linguistic knowledge from the source parse tree into hard hierarchical skeletons generated by BTG in phrasal SMT. To automatically learn reordering features, we have introduced algorith</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Fox, Heidi. 2002. Phrasal cohesion and statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 304–311,</rawString>
</citation>
<citation valid="false">
<location>Philadelphia, PA.</location>
<marker></marker>
<rawString>Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL</booktitle>
<pages>273--280</pages>
<location>Boston, MA.</location>
<contexts>
<context position="66470" citStr="Galley et al. 2004" startWordPosition="10539" endWordPosition="10542">ctures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose this as a hard constraint on the ITG constraint to allow reorderings which respect the source parse tree. They all report significant improvements on different language pairs, which indicates that syntactic cohesion is very useful for phrasal SMT. Our analysis demonstrates that linguistically annotated reordering provides an alt</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Galley, Michel, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 273–280, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
<author>Abe Ittycheriah</author>
<author>Kishore Papineni</author>
</authors>
<title>Multiple reorderings in phrase-based machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2),</booktitle>
<pages>61--68</pages>
<location>Columbus, OH.</location>
<marker>Ge, Ittycheriah, Papineni, 2008</marker>
<rawString>Ge, Niyu, Abe Ittycheriah, and Kishore Papineni. 2008. Multiple reorderings in phrase-based machine translation. In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 61–68, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevi Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation of the Americas,</booktitle>
<pages>66--73</pages>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Huang, Liang, Kevi Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of the 7th Conference of the Association for Machine Translation of the Americas, pages 66–73,</rawString>
</citation>
<citation valid="false">
<location>Cambridge, MA.</location>
<marker></marker>
<rawString>Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<location>Barcelona.</location>
<contexts>
<context position="42118" citStr="Koehn 2004" startWordPosition="6655" endWordPosition="6656">s 1,082 sentences with an average of 27.4 words/47.6 characters per sentence. The reference corpus for the NIST MT-05 test set contains four translations per source sentence. Both the development and test sets were also parsed using the parser mentioned above. Our evaluation metric is the case-insensitive BLEU-4 (Papineni et al. 2002) using the shortest reference sentence length for the brevity penalty. The model feature weights are tuned on the development set to maximize BLEU using MERT (Och 2003). Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn 2004). 7.2 Bias in AExtractor As described in Section 3, AExtractor selectively extracts reordering examples. This selective extraction raises three questions: 1. Is it necessary to extract all reordering examples? 8 Available at:http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html. 551 Computational Linguistics Volume 36, Number 3 2. If it is not necessary, do the heuristic selection rules impose any bias on the reordering model? For example, if we use the strINV selection rule, meaning that we always extract the largest block pairs for inverted reordering examples, does the reordering model</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Koehn, Philipp. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004, pages 388–395, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation</booktitle>
<pages>78--85</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2880" citStr="Koehn et al. (2005)" startWordPosition="384" endWordPosition="387">008; revised submission received: 12 March 2010; accepted for publication: 21 April 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 local word reorderings within phrases. Unfortunately, reordering at the phrase level is still problematic for phrasal SMT. The default distortion-based reordering model simply penalizes phrase movement according to the jump distance, without considering any linguistic contexts (morphological, lexical, or syntactic) around phrases. In order to utilize lexical information for phrase reordering, Tillman (2004) and Koehn et al. (2005) propose lexicalized reordering models which directly condition phrase movement on phrases themselves. One problem with such lexicalized reordering models is that they are restricted only to reorderings of phrases seen in training data. To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering evidence. Although these lexicalized reordering models significantly outperform the distortion-based reordering model as reported, only using lexical information (e.g., boundary words) i</context>
<context position="13925" citStr="Koehn et al. 2005" startWordPosition="2103" endWordPosition="2106">e value of ps can be set as high as 0.8 to prefer monotone orientations because the two languages have similar word orders in most cases. The main problem of the flat reordering model is also the problem of the standard distortion model (Koehn, Och, and Marcu 2003): Neither model considers linguistic contexts. To be context-dependent, the ITG reordering might directly model the conditional probability P(o|Al,Ar). This probability could be calculated using the maximum likelihood estimate (MLE) by taking counts from training data, in the manner of the lexicalized reordering model (Tillman 2004; Koehn et al. 2005): Count(o, Al, Ar) P(o|Al,Ar) = (7) Count(Al,Ar) Unfortunately this lexicalized reordering method usually leads to a serious data sparseness problem under the ITG constraint because Al and Ar become larger and larger due to the merging rules, and are finally unseen in the training data. To avoid the data sparseness problem yet be contextually informative, attributes of Al and Ar, instead of nodes themselves, are used as reordering evidence in a new perspective of the ITG reordering (Xiong, Liu, and Lin 2006). The new perspective treats the ITG reordering as a binary-classification problem wher</context>
<context position="34125" citStr="Koehn et al. 2005" startWordPosition="5389" endWordPosition="5392">nd MATCH-SRPs according to the target orders generated in Step 3 for each multi-branching node. 5. Summarize all SRPs and calculate the precision and recall as described above. We further elaborate Steps 2–4 in the Sections 6.2–6.4. 6.2 Generating Word Alignments To obtain word alignments between source sentences and multiple reference translations, we pair the source sentences with each of the reference translations and include the created sentence pairs in our bilingual training corpus. Then we run GIZA++ on the new corpus in both directions, and apply the “grow-diag-final” refinement rule (Koehn et al. 2005) to produce the final word alignments. To obtain word alignments between source sentences and system translations, we store the word alignments within each phrase pair in our phrase table. When we output the system translation for a source sentence, we trace back the original source phrase for each target phrase in the system translation. This will generate a phrase alignment between the source sentence and system translation. Given the phrase alignment and word alignments within the phrase stored in the phrase table, we can easily obtain word alignments between the whole source sentence and s</context>
<context position="73678" citStr="Koehn et al. 2005" startWordPosition="11596" endWordPosition="11599">vely. 9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the flat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more flexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous G</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Koehn, Philipp, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proceedings of the International Workshop on Spoken Language Translation 2005, pages 78–85, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>58--54</pages>
<location>Edmonton.</location>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, Philipp, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 58–54, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>161--168</pages>
<location>Vancouver.</location>
<contexts>
<context position="73700" citStr="Kumar and Byrne 2005" startWordPosition="11600" endWordPosition="11603">ing Phrase Movement with Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the flat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more flexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Kumar, Shankar and William Byrne. 2005. Local phrase reordering models for statistical machine translation. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 161–168, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
<author>Minghui Li</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Yi Guan</author>
</authors>
<title>A probabilistic approach to syntax-based reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>720--727</pages>
<location>Prague.</location>
<contexts>
<context position="31596" citStr="Li et al. 2007" startWordPosition="4984" endWordPosition="4987">g precision/recall for this structure is c/b and c/a, respectively. We can further calculate the F1-score as 2 x c/(a + b). These syntax-based metrics intuitively show how well the reordering model can reorder this structure. By summarizing all reordering patterns of all constituents, we can obtain an overall precision, recall, and F1-score for the tested reordering model. This new syntax-based analysis for reordering is motivated in part by recent work which transforms the order of nodes in the source-side parse tree before translation (Xia and McCord 2004; Collins, Koehn, and Kucerova 2005; Li et al. 2007; Wang, Collins, and Koehn 2007). Here we focus on the order transformation of syntactic constituents performed by reordering models during translation. In addition to aligning parse trees with reference translations, we also align parse trees with system translations so that we can learn the movement of syntactic constituents carried out by the reordering models and investigate the performance of the reordering models by comparing both alignments. For notational convenience, we denote syntactic reordering patterns that are extracted from the alignments between source parse trees and reference</context>
<context position="72510" citStr="Li et al. (2007)" startWordPosition="11425" endWordPosition="11428">.1 The Preprocessing Approach. In early work, Brown et al. (1992) describe an approach to reordering French phrases in a preprocessing step. Xia and McCord (2004) present a preprocessing approach which automatically learns reordering patterns based on CFG productions. Since then, the preprocessing approach seems to have been more popular. Collins, Koehn, and Kucerova (2005) propose reordering German clauses with six types of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees using fine-grained human-written rules, mostly concentrating on VP and NP structures. Li et al. (2007) improve the preprocessing approach by generating n-best reordered source sentences with reordering knowledge automatically learned from the alignments between source parse trees and target translations. The approach proposed in Li et al. also enhances the connection between the preprocessing and decoding by adding a source reordering probability feature. Other approaches introduced in Nießn and Ney (2001), Popovi´c and Ney (2006), and Zhang, Zens, and Ney (2007) use morphological, POS, and chunk knowledge in the preprocessing approach, respectively. 9.1.2 Estimating Phrase Movement with Embed</context>
</contexts>
<marker>Li, Li, Zhang, Li, Zhou, Guan, 2007</marker>
<rawString>Li, Chi-Ho, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou, and Yi Guan. 2007. A probabilistic approach to syntax-based reordering for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 720–727, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A path-based transfer model for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (Coling</booktitle>
<pages>625--630</pages>
<location>Geneva.</location>
<contexts>
<context position="71819" citStr="Lin 2004" startWordPosition="11327" endWordPosition="11328">ties of movement with linguistic information. In the third Table 12 Revised overall precision and recall of BWR+LAR vs. BWR on the test corpus when we consider the gap in syntactic reordering patterns. Precision Recall F1 BWR (gap) 46.28 44.91 45.58 BWR+LAR (gap) 48.80 50 49.39 562 Xiong et al. Linguistically Annotated Reordering approach, reordering knowledge is included in synchronous rules. The last two categories reorder the source sentence during decoding, which distinguishes them from the first approach. Note that some researchers integrate multiple reordering approaches in one decoder (Lin 2004; Quirk, Menezes, and Cherry 2005; Ge, Ittycheriah, and Papineni 2008). 9.1.1 The Preprocessing Approach. In early work, Brown et al. (1992) describe an approach to reordering French phrases in a preprocessing step. Xia and McCord (2004) present a preprocessing approach which automatically learns reordering patterns based on CFG productions. Since then, the preprocessing approach seems to have been more popular. Collins, Koehn, and Kucerova (2005) propose reordering German clauses with six types of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees using fine-</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Lin, Dekang. 2004. A path-based transfer model for machine translation. In Proceedings of the 20th International Conference on Computational Linguistics (Coling 2004), pages 625–630, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-to-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>609--616</pages>
<location>Sydney.</location>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 609–616, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>44--52</pages>
<location>Sydney.</location>
<contexts>
<context position="69418" citStr="Marcu et al. 2006" startWordPosition="10969" endWordPosition="10972"> fluent translations for these constituents. However, the allowance of interruptions is sometimes beyond the representability of BTG rules. For example, to solve the lexical divergence problem, bilingual rules with aligned lexicons have to be introduced. To capture reorderings of these constituents, we propose to integrate special reordering rules with richer contextual information into BTG to extend BTG’s ability to deal with interruptions. Completely replacing BTG with richer formalisms, such as hierarchical phrase (Chiang 2005) and tree-to-string (Liu, Liu, and Lin 2006) or string-to-tree (Marcu et al. 2006), introduces a huge extra cost. Instead, integrating a small number of reordering rules into BTG to model reorderings of non-reorderable constituents would be more desirable. 8.5 Discussion In the definition of syntactic reordering patterns, we only consider the relative order of individual constituents on the target side. We do not consider whether or not they remain contiguous on the target side. It is possible that other words are inserted between spans of two contiguous constituents. We use the term gap to refer to when this happens. The absence of a gap in the definition of syntactic reor</context>
<context position="74679" citStr="Marcu et al. 2006" startWordPosition="11738" endWordPosition="11741"> is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax-based translation rules. 9.2 Automatic Analysis of Reordering Although there is a variety of work on phrase reordering, automatic analysis of phrase reordering is not widely explored in the SMT literature. Chiang et al. (2005) propose 563 Computational Linguistics Volume 36, Number 3 an automatic method to compare different system outputs in a fine-grained manne</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Marcu, Daniel, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 44–52, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1003--1011</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="66149" citStr="Marton and Resnik 2008" startWordPosition="10489" endWordPosition="10492">ith AV --,kR, which is a sub-phrase of PP preceding VP in an inverted order. The remaining part of the VP phrase is then merged. This merging process continues regardless of the source parse tree. The comparison of BTG trees of BWR+LAR and BWR in the two examples suggests that reordering models should respect syntactic structures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose this as a</context>
<context position="68398" citStr="Marton and Resnik 2008" startWordPosition="10819" endWordPosition="10822">ndary violations, we propose constrained decoding. In constrained decoding, we define special zones in source sentences. Reorderings and translations within the zones cannot be interrupted by fragments outside the zones. We can also define other constrained operations on the zones. For example, we can prohibit swappings in any zones which contain punctuation (Xiong et al. 2008b). The beginning and ending positions of a zone are automatically learned. To be more flexible, they are not necessarily constituent boundaries. Constrained decoding is different from both soft constraints (Cherry 2008; Marton and Resnik 2008) and hard constraints (Yamamoto, Okuma, and Sumita 2008). It can be considered as in between both of these because it is harder than the former but softer than the latter. 2. Integrating special reordering rules. Some constituents are indeed non-reorderable as we discussed in Section 8.2.2. Inside or outside 561 Computational Linguistics Volume 36, Number 3 interruptions have to be allowed to obtain fluent translations for these constituents. However, the allowance of interruptions is sometimes beyond the representability of BTG rules. For example, to solve the lexical divergence problem, bili</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Marton, Yuval and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings of ACL-08: HLT, pages 1003–1011, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Navarro</author>
</authors>
<title>A guided tour to approximate string matching.</title>
<date>2001</date>
<journal>ACM Computing Surveys,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="37820" citStr="Navarro 2001" startWordPosition="5984" endWordPosition="5985">n the 7 Their target translations are interrupted by the other node’s translation. We will discuss this situation in Section 8.5. 549 Computational Linguistics Volume 36, Number 3 MATCH-SRP for each multi-branching node by comparing the obtained SYS-SRP with the REF-SRPs for this node under the following conditions: 1. Because we have multiple reference translations, we may have different REF-SRPs. We compare the SYS-SRP with the REF-SRP where the reference translation for this node (the sequence within the target span of the node defined by the REF-SRP) has the shortest Levenshtein distance (Navarro 2001) to that of the system translation. 2. If there are combined nodes in SYS/REF-SRPs, they are treated as a unit when comparing, without considering the order within each combined node. If the order of the SYS-SRP and the selected REF-SRP matches, we have one MATCH-SRP for the node. Let’s give an example to explain these conditions. Suppose that we are processing the structure VP → PP1ADVP2VP3. We obtain four REF-SRPs from four different reference translations and one SYS-SRP from the system output. Here we only show the orders: Ref.a : [3][1][2] Ref.b : [3][2][1] Ref.c&amp;d : [2][3][1] SYS: [3][1&amp;</context>
</contexts>
<marker>Navarro, 2001</marker>
<rawString>Navarro, Gonzalo. 2001. A guided tour to approximate string matching. ACM Computing Surveys, 33(1):31–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießn</author>
<author>Hermann Ney</author>
</authors>
<title>Morpho-syntactic analysis for reordering in statistical machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of MT Summit VIII,</booktitle>
<pages>247--252</pages>
<location>Santiago de Compostela.</location>
<contexts>
<context position="72919" citStr="Nießn and Ney (2001)" startWordPosition="11481" endWordPosition="11484">auses with six types of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees using fine-grained human-written rules, mostly concentrating on VP and NP structures. Li et al. (2007) improve the preprocessing approach by generating n-best reordered source sentences with reordering knowledge automatically learned from the alignments between source parse trees and target translations. The approach proposed in Li et al. also enhances the connection between the preprocessing and decoding by adding a source reordering probability feature. Other approaches introduced in Nießn and Ney (2001), Popovi´c and Ney (2006), and Zhang, Zens, and Ney (2007) use morphological, POS, and chunk knowledge in the preprocessing approach, respectively. 9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the flat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved</context>
</contexts>
<marker>Nießn, Ney, 2001</marker>
<rawString>Nießn, Sonja and Hermann Ney. 2001. Morpho-syntactic analysis for reordering in statistical machine translation. In Proceedings of MT Summit VIII, pages 247–252, Santiago de Compostela.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Statistical Machine Translation: From Single-Word Models to Alignment Templates.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>RWTH Aachen University,</institution>
<contexts>
<context position="17284" citStr="Och 2002" startWordPosition="2659" endWordPosition="2660">ithm, we introduce the concept of junction in the word alignment matrix. We define a junction as a vertex shared by two neighboring blocks. There are two types of junctions: a straight junction, which connects two neighboring blocks in a straight order (e.g., black dots J1 – J4 in Figure 1) and an inverted junction, which connects two neighboring blocks in an inverted order (e.g., the red dot J5 in Figure 1). The algorithm for AExtractor is shown in Figure 2. This completes three sub-tasks as follows. 1. Find blocks (lines 4 and 5). This is similar to the standard phrase extraction algorithm (Och 2002) except that we find blocks with arbitrary length. 2. Detect junctions and store blocks in the arrays of detected junctions (lines 7 and 8). Junctions that are included the current block can be easily detected by looking at the previous and next blocks. A junction can connect multiple blocks on its left and right sides. For example, the second junction J2 in Figure 1 connects two blocks on the left side and three blocks on the right side. To store these blocks, we maintain two arrays (left and right) for each junction. 3. Select block pairs from each detected junction as reordering examples (l</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>Och, Franz Josef. 2002. Statistical Machine Translation: From Single-Word Models to Alignment Templates. Ph.D. thesis, RWTH Aachen University, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo.</location>
<contexts>
<context position="42011" citStr="Och 2003" startWordPosition="6641" endWordPosition="6642">n test data as our development set (18 words/31 characters per sentence). The NIST MT-05 test set includes 1,082 sentences with an average of 27.4 words/47.6 characters per sentence. The reference corpus for the NIST MT-05 test set contains four translations per source sentence. Both the development and test sets were also parsed using the parser mentioned above. Our evaluation metric is the case-insensitive BLEU-4 (Papineni et al. 2002) using the shortest reference sentence length for the brevity penalty. The model feature weights are tuned on the development set to maximize BLEU using MERT (Och 2003). Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn 2004). 7.2 Bias in AExtractor As described in Section 3, AExtractor selectively extracts reordering examples. This selective extraction raises three questions: 1. Is it necessary to extract all reordering examples? 8 Available at:http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html. 551 Computational Linguistics Volume 36, Number 3 2. If it is not necessary, do the heuristic selection rules impose any bias on the reordering model? For example, if we use the strINV selection rule, meaning</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, Franz Josef. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hong Kong.</location>
<contexts>
<context position="39612" citStr="Och and Ney 2000" startWordPosition="6274" endWordPosition="6277">G-based phrasal SMT system, developed following Section 2. We integrate the boundary word–based reordering model and the linguistically annotated reordering model into our system according to our reordering configuration. We carried out various experiments to evaluate the reordering example extraction algorithms of Section 3, the linguistically annotated reordering model vs. boundary word–based reordering model, and the effects of linguistically annotated features on the Chinese-toEnglish translation task of the NIST MT-05 using large scale training data. 7.1 Experimental Setup We ran GIZA++ (Och and Ney 2000) on the parallel corpora (consisting of 101.93M Chinese words and 112.78M English words) listed in Table 2 in both directions and then applied the “grow-diag-final” refinement rule (Koehn, Och, and Marcu 2003) to 550 Xiong et al. Linguistically Annotated Reordering Table 2 Corpora used. Corpus LDC catalog Chinese words English words United Nations LDC2004E12 68.63M 76.99M Hong Kong News LDC2004T08 15.07M 15.89M Sinorama Magazine LDC2005T10 10.26M 9.64M FBIS LDC2003E14 7.09M 9.28M Xinhua LDC2002E18 0.40M 0.43M Chinese News Translation LDC2005T06 0.28M 0.31M Chinese Treebank LDC2003E07 0.10M 0.1</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="7461" citStr="Papineni et al. 2002" startWordPosition="1073" endWordPosition="1076">ITG constraint. We call this two-step phrase reordering strategy linguistically annotated reordering (LAR) (Xiong et al. 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reordering strategy based on BTG. However, they use boundary words as reordering features at the second step. To distinguish this from our work, we call their approach boundary word–based reordering (BWR). LAR and BWR can be considered as two reordering variants for BTG-based phrasal SMT, which have similar training procedures. Furthermore, they can be combined. We evaluate LAR vs. BWR using the automatic metric BLEU (Papineni et al. 2002). The BLEU scores show that LAR is comparable to BWR and significantly improves phrase reordering when combined with BWR. We want to further study what happens when we combine BWR with LAR. In particular, we want to investigate to what extent the integrated linguistic knowledge (from LAR) changes phrase movement in an actual SMT system, and in what direction the change takes place. The investigations will enable us to have a better understanding of the relationship between phrase movement and linguistic context, and therefore to explore linguistic knowledge more effectively in phrasal SMT. Bec</context>
<context position="41843" citStr="Papineni et al. 2002" startWordPosition="6612" endWordPosition="6615">rained on the Xinhua section of the English Gigaword corpus (181.1M words). We selected 580 short sentences (not exceeding 50 characters per sentence) from the NIST MT-02 evaluation test data as our development set (18 words/31 characters per sentence). The NIST MT-05 test set includes 1,082 sentences with an average of 27.4 words/47.6 characters per sentence. The reference corpus for the NIST MT-05 test set contains four translations per source sentence. Both the development and test sets were also parsed using the parser mentioned above. Our evaluation metric is the case-insensitive BLEU-4 (Papineni et al. 2002) using the shortest reference sentence length for the brevity penalty. The model feature weights are tuned on the development set to maximize BLEU using MERT (Och 2003). Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling (Koehn 2004). 7.2 Bias in AExtractor As described in Section 3, AExtractor selectively extracts reordering examples. This selective extraction raises three questions: 1. Is it necessary to extract all reordering examples? 8 Available at:http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html. 551 Computational Linguistics Volume 36,</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovic</author>
<author>Adri`a de Gispert</author>
<author>Deepa Gupta</author>
<author>Patrik Lambert</author>
<author>Hermann Ney</author>
<author>Jos´e B Mari˜no</author>
<author>Marcello Federico</author>
<author>Rafael Banchs</author>
</authors>
<title>Morpho-syntactic information for automatic error analysis of statistical machine translation output.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>1--6</pages>
<location>New York, NY.</location>
<marker>Popovic, de Gispert, Gupta, Lambert, Ney, Mari˜no, Federico, Banchs, 2006</marker>
<rawString>Popovic, Maja, Adri`a de Gispert, Deepa Gupta, Patrik Lambert, Hermann Ney, Jos´e B. Mari˜no, Marcello Federico, and Rafael Banchs. 2006. Morpho-syntactic information for automatic error analysis of statistical machine translation output. In Proceedings on the Workshop on Statistical Machine Translation, pages 1–6, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>Pos-based word reorderings for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>1278--1283</pages>
<location>Genoa.</location>
<marker>Popovi´c, Ney, 2006</marker>
<rawString>Popovi´c, Maja and Hermann Ney. 2006. Pos-based word reorderings for statistical machine translation. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC 2006), pages 1278–1283, Genoa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translations: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>271--279</pages>
<location>Ann Arbor, MI.</location>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Quirk, Chris, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translations: Syntactically informed phrasal smt. In Proceedings of the 43rd Annual Meeting of the ACL, pages 271–279, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="41209" citStr="Stolcke 2002" startWordPosition="6516" endWordPosition="6517">actor AExtractor and TExtractor of Section 3 on the chosen word-aligned corpora. We then extracted boundary word features from the reordering examples. To extract linguistically annotated features, we parsed the Chinese side of the chosen parallel text using a Chinese parser (Xiong, Liu, and Lin 2005) which was trained on the Penn Chinese Treebank with an F1-score of 79.4%. We ran the off-the-shelf MaxEnt toolkit8 to tune the reordering feature weights with the iteration number set to 100 and Gaussian prior to 1 to avoid overfitting. We built our 4-gram language model using the SRILM toolkit (Stolcke 2002), which was trained on the Xinhua section of the English Gigaword corpus (181.1M words). We selected 580 short sentences (not exceeding 50 characters per sentence) from the NIST MT-02 evaluation test data as our development set (18 words/31 characters per sentence). The NIST MT-05 test set includes 1,082 sentences with an average of 27.4 words/47.6 characters per sentence. The reference corpus for the NIST MT-05 test set contains four translations per source sentence. Both the development and test sets were also parsed using the parser mentioned above. Our evaluation metric is the case-insensi</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, Andreas. 2002. SRILM—an extensible language modeling toolkit. In Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP 2002), pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004): Short Papers,</booktitle>
<pages>101--104</pages>
<location>Boston, MA.</location>
<contexts>
<context position="2856" citStr="Tillman (2004)" startWordPosition="381" endWordPosition="382">eived: 24 October 2008; revised submission received: 12 March 2010; accepted for publication: 21 April 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 local word reorderings within phrases. Unfortunately, reordering at the phrase level is still problematic for phrasal SMT. The default distortion-based reordering model simply penalizes phrase movement according to the jump distance, without considering any linguistic contexts (morphological, lexical, or syntactic) around phrases. In order to utilize lexical information for phrase reordering, Tillman (2004) and Koehn et al. (2005) propose lexicalized reordering models which directly condition phrase movement on phrases themselves. One problem with such lexicalized reordering models is that they are restricted only to reorderings of phrases seen in training data. To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering evidence. Although these lexicalized reordering models significantly outperform the distortion-based reordering model as reported, only using lexical information </context>
<context position="13905" citStr="Tillman 2004" startWordPosition="2101" endWordPosition="2102">spectively, the value of ps can be set as high as 0.8 to prefer monotone orientations because the two languages have similar word orders in most cases. The main problem of the flat reordering model is also the problem of the standard distortion model (Koehn, Och, and Marcu 2003): Neither model considers linguistic contexts. To be context-dependent, the ITG reordering might directly model the conditional probability P(o|Al,Ar). This probability could be calculated using the maximum likelihood estimate (MLE) by taking counts from training data, in the manner of the lexicalized reordering model (Tillman 2004; Koehn et al. 2005): Count(o, Al, Ar) P(o|Al,Ar) = (7) Count(Al,Ar) Unfortunately this lexicalized reordering method usually leads to a serious data sparseness problem under the ITG constraint because Al and Ar become larger and larger due to the merging rules, and are finally unseen in the training data. To avoid the data sparseness problem yet be contextually informative, attributes of Al and Ar, instead of nodes themselves, are used as reordering evidence in a new perspective of the ITG reordering (Xiong, Liu, and Lin 2006). The new perspective treats the ITG reordering as a binary-classif</context>
<context position="73659" citStr="Tillman 2004" startWordPosition="11594" endWordPosition="11595">oach, respectively. 9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the flat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more flexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderin</context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Tillman, Christoph. 2004. A unigram orientation model for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004): Short Papers, pages 101–104, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>737--745</pages>
<location>Prague.</location>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Wang, Chao, Michael Collins, and Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 737–745, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>152--158</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="13015" citStr="Wu 1996" startWordPosition="1957" endWordPosition="1958"> of the most important and challenging tasks in building a BTG-based phrasal SMT system is to define P(rm). 2.2 Reordering Under the ITG Constraint Under the ITG constraint, three nodes {Al, Ar, Ap} are involved when we consider the order o between the two children {Al, Ar} in any binary subtrees. Therefore it is natural to define the ITG reordering P(rm) as a function as follows: P(rm) = f(Al, Ar, Ap,o) (5) where o ∈ {straight, inverted}. Based on this function, various reordering models are built according to different assumptions. For example, the flat reordering model in the original BTG (Wu 1996) assigns prior probabilities for the straight and inverted order assuming the order is highly related to the properties of language pairs. It is formulated as � ps, o = straight P(rm) = (6) 1 − ps, o = inverted Supposing French and English are the source and target language, respectively, the value of ps can be set as high as 0.8 to prefer monotone orientations because the two languages have similar word orders in most cases. The main problem of the flat reordering model is also the problem of the standard distortion model (Koehn, Och, and Marcu 2003): Neither model considers linguistic contex</context>
<context position="73448" citStr="Wu 1996" startWordPosition="11563" endWordPosition="11564">dering probability feature. Other approaches introduced in Nießn and Ney (2001), Popovi´c and Ney (2006), and Zhang, Zens, and Ney (2007) use morphological, POS, and chunk knowledge in the preprocessing approach, respectively. 9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the flat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more flexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reorde</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Wu, Dekai. 1996. A polynomial-time algorithm for statistical machine translation. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 152–158, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="4710" citStr="Wu 1997" startWordPosition="645" endWordPosition="646">succeeding phrase should be translated first. If high-level linguistic knowledge, such as the syntactic context VP→PP VP, is given, the position of the PP phrase can be easily determined since the pre-verbal modifier PP in Chinese is frequently translated into a post-verbal counterpart in English. In this article, we focus on linguistically motivated phrase reordering, which integrates high-level linguistic knowledge in phrase reordering. We adopt a two-step strategy. In the first step, we establish a hierarchical skeleton in phrasal SMT by incorporating Bracketing Transduction Grammar (BTG) (Wu 1997) into phrasal SMT. In the second step, we inject soft linguistic information into nodes of the skeleton. There are two significant advantages to using BTG in phrasal SMT. First, BTG is able to generate hierarchical structures.2 This not only enhances phrasal SMT’s capability for hierarchical and long-distance reordering but also establishes a platform for phrasal SMT to incorporate knowledge from linguistic structure. Second, phrase reordering is restricted by the ITG constraint (Wu 1997). Although it only allows two orders (straight or inverted) of nodes in any binary branching structure, it </context>
<context position="20068" citStr="Wu (1997)" startWordPosition="3116" endWordPosition="3117">AExtractor is caused by the use of heuristic selection rules: keeping some block pairs as reordering examples while abandoning other block pairs. The kept block pairs are not necessarily the best training instances for tuning an ITG order predictor. To avoid this problem we can extract reordering examples from the BTG trees of sentence pairs. Reordering examples extracted in this way are naturally suitable for BTG order prediction. There are various ways to build BTG trees over sentence pairs. One can use BTG to produce bilingual parses of sentence pairs, similar to the approaches proposed by Wu (1997) and Zhang and Gildea (2005) but using the more sophisticated reordering models BWR or LAR. After parsing, reordering examples can be extracted from bilingual parse trees and a better reordering model is therefore induced from the extracted reordering examples. Using the better reordering model, the bilingual sentences are parsed again. This procedure is run iteratively until no performance gain is obtained in terms of translation or parsing accuracy. Formally, we can use expectation-maximization (EM) training in this procedure. In the expectation step, we first estimate the likelihood of all </context>
<context position="66361" citStr="Wu 1997" startWordPosition="10523" endWordPosition="10524"> BWR+LAR and BWR in the two examples suggests that reordering models should respect syntactic structures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose this as a hard constraint on the ITG constraint to allow reorderings which respect the source parse tree. They all report significant improvements on different language pairs, which indicates that syntactic cohesion is ve</context>
<context position="74296" citStr="Wu (1997)" startWordPosition="11690" endWordPosition="11691">nd Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more flexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally includ</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Wu, Dekai. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Marine Carpuat</author>
<author>Yihai Shen</author>
</authors>
<title>Inversion transduction grammar coverage of Arabic-English word alignment for tree-structured statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceeding of the IEEE/ACL 2006 Workshop on Spoken Language Technology (SLT</booktitle>
<pages>234--237</pages>
<location>Aruba.</location>
<marker>Wu, Carpuat, Shen, 2006</marker>
<rawString>Wu, Dekai, Marine Carpuat, and Yihai Shen. 2006. Inversion transduction grammar coverage of Arabic-English word alignment for tree-structured statistical machine translation. In Proceeding of the IEEE/ACL 2006 Workshop on Spoken Language Technology (SLT 2006), pages 234–237, Aruba.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (Coling</booktitle>
<pages>508--514</pages>
<location>Geneva.</location>
<contexts>
<context position="31545" citStr="Xia and McCord 2004" startWordPosition="4975" endWordPosition="4978">ions, and c times in both alignments. Then the reordering precision/recall for this structure is c/b and c/a, respectively. We can further calculate the F1-score as 2 x c/(a + b). These syntax-based metrics intuitively show how well the reordering model can reorder this structure. By summarizing all reordering patterns of all constituents, we can obtain an overall precision, recall, and F1-score for the tested reordering model. This new syntax-based analysis for reordering is motivated in part by recent work which transforms the order of nodes in the source-side parse tree before translation (Xia and McCord 2004; Collins, Koehn, and Kucerova 2005; Li et al. 2007; Wang, Collins, and Koehn 2007). Here we focus on the order transformation of syntactic constituents performed by reordering models during translation. In addition to aligning parse trees with reference translations, we also align parse trees with system translations so that we can learn the movement of syntactic constituents carried out by the reordering models and investigate the performance of the reordering models by comparing both alignments. For notational convenience, we denote syntactic reordering patterns that are extracted from the </context>
<context position="72056" citStr="Xia and McCord (2004)" startWordPosition="11361" endWordPosition="11364">R (gap) 46.28 44.91 45.58 BWR+LAR (gap) 48.80 50 49.39 562 Xiong et al. Linguistically Annotated Reordering approach, reordering knowledge is included in synchronous rules. The last two categories reorder the source sentence during decoding, which distinguishes them from the first approach. Note that some researchers integrate multiple reordering approaches in one decoder (Lin 2004; Quirk, Menezes, and Cherry 2005; Ge, Ittycheriah, and Papineni 2008). 9.1.1 The Preprocessing Approach. In early work, Brown et al. (1992) describe an approach to reordering French phrases in a preprocessing step. Xia and McCord (2004) present a preprocessing approach which automatically learns reordering patterns based on CFG productions. Since then, the preprocessing approach seems to have been more popular. Collins, Koehn, and Kucerova (2005) propose reordering German clauses with six types of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees using fine-grained human-written rules, mostly concentrating on VP and NP structures. Li et al. (2007) improve the preprocessing approach by generating n-best reordered source sentences with reordering knowledge automatically learned from the align</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Xia, Fei and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In Proceedings of the 20th International Conference on Computational Linguistics (Coling 2004), pages 508–514, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Parsing the penn chinese treebank with semantic knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings of The 2nd International Joint Conference on Natural Language Processing (IJCNLP-05),</booktitle>
<pages>70--81</pages>
<location>Jeju Island.</location>
<marker>Xiong, Liu, Lin, 2005</marker>
<rawString>Xiong, Deyi, Qun Liu, and Shouxun Lin. 2005. Parsing the penn chinese treebank with semantic knowledge. In Proceedings of The 2nd International Joint Conference on Natural Language Processing (IJCNLP-05), pages 70–81, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st</booktitle>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Xiong, Deyi, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of the 21st</rawString>
</citation>
<citation valid="false">
<booktitle>International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>521--528</pages>
<location>Sydney.</location>
<marker></marker>
<rawString>International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 521–528, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>A linguistically annotated reordering model for BTG-based statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>149--152</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="6965" citStr="Xiong et al. 2008" startWordPosition="993" endWordPosition="996">. The challenge, of course, is that BTG hierarchical structures are not always aligned with the linguistic structures in the source language parse tree. To address this issue, we propose an annotation algorithm. The algorithm is able to label any BTG nodes during decoding with very little overhead, regardless of whether the BTG nodes are aligned with syntactic constituent nodes in the source parse tree. The annotated linguistic elements are then used to guide phrase reordering under the ITG constraint. We call this two-step phrase reordering strategy linguistically annotated reordering (LAR) (Xiong et al. 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reordering strategy based on BTG. However, they use boundary words as reordering features at the second step. To distinguish this from our work, we call their approach boundary word–based reordering (BWR). LAR and BWR can be considered as two reordering variants for BTG-based phrasal SMT, which have similar training procedures. Furthermore, they can be combined. We evaluate LAR vs. BWR using the automatic metric BLEU (Papineni et al. 2002). The BLEU scores show that LAR is comparable to BWR and significantly improves phrase reordering when c</context>
<context position="48147" citStr="Xiong et al. 2008" startWordPosition="7601" endWordPosition="7604">traction algorithm is better than that decided by the decoder itself. Finally, for the last question, we observe from Table 3 that BLEU scores are not that much different although we have quite the opposite bias imposed by different selection rules. The changes in BLEU score, which happen when we shift from one selection rule to the other, are limited to a maximum of 1.2%. Among the four selection rules, the STRinv rule achieves the highest BLEU score. The reason might be that the bias towards smaller swappings imposed by this rule helps the decoder to reduce incorrect longdistance swappings (Xiong et al. 2008b). 7.3 AExtractor vs. TExtractor We further compared the two algorithms for reordering example extraction. In Table 3, we find that TExtractor significantly underperforms in comparison to AExtractor. This is because the transformation from decomposition trees to BTG trees is not complete. Many crossing links due to errors and noise in word alignments generated by GIZA++ make it impossible to build BTG nodes over the corresponding words. It would be better to use alignments induced by the ITG and EM procedure described in Section 3.2 but this has a very high cost. Given the comparison in Table</context>
<context position="68154" citStr="Xiong et al. 2008" startWordPosition="10784" endWordPosition="10787">es and provide suggestions accordingly as follows. 1. Constrained decoding. We observe that in reorderable constituents which involve long-distance reorderings, their boundaries are easily violated by phrases outside them. To prohibit boundary violations, we propose constrained decoding. In constrained decoding, we define special zones in source sentences. Reorderings and translations within the zones cannot be interrupted by fragments outside the zones. We can also define other constrained operations on the zones. For example, we can prohibit swappings in any zones which contain punctuation (Xiong et al. 2008b). The beginning and ending positions of a zone are automatically learned. To be more flexible, they are not necessarily constituent boundaries. Constrained decoding is different from both soft constraints (Cherry 2008; Marton and Resnik 2008) and hard constraints (Yamamoto, Okuma, and Sumita 2008). It can be considered as in between both of these because it is harder than the former but softer than the latter. 2. Integrating special reordering rules. Some constituents are indeed non-reorderable as we discussed in Section 8.2.2. Inside or outside 561 Computational Linguistics Volume 36, Numbe</context>
</contexts>
<marker>Xiong, Zhang, Aw, Li, 2008</marker>
<rawString>Xiong, Deyi, Min Zhang, Aiti Aw, and Haizhou Li. 2008a. A linguistically annotated reordering model for BTG-based statistical machine translation. In Proceedings of ACL-08: HLT, Short Papers, pages 149–152, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Refinements in BTG-based statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing,</booktitle>
<pages>505--512</pages>
<location>Hyderabad.</location>
<contexts>
<context position="6965" citStr="Xiong et al. 2008" startWordPosition="993" endWordPosition="996">. The challenge, of course, is that BTG hierarchical structures are not always aligned with the linguistic structures in the source language parse tree. To address this issue, we propose an annotation algorithm. The algorithm is able to label any BTG nodes during decoding with very little overhead, regardless of whether the BTG nodes are aligned with syntactic constituent nodes in the source parse tree. The annotated linguistic elements are then used to guide phrase reordering under the ITG constraint. We call this two-step phrase reordering strategy linguistically annotated reordering (LAR) (Xiong et al. 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reordering strategy based on BTG. However, they use boundary words as reordering features at the second step. To distinguish this from our work, we call their approach boundary word–based reordering (BWR). LAR and BWR can be considered as two reordering variants for BTG-based phrasal SMT, which have similar training procedures. Furthermore, they can be combined. We evaluate LAR vs. BWR using the automatic metric BLEU (Papineni et al. 2002). The BLEU scores show that LAR is comparable to BWR and significantly improves phrase reordering when c</context>
<context position="48147" citStr="Xiong et al. 2008" startWordPosition="7601" endWordPosition="7604">traction algorithm is better than that decided by the decoder itself. Finally, for the last question, we observe from Table 3 that BLEU scores are not that much different although we have quite the opposite bias imposed by different selection rules. The changes in BLEU score, which happen when we shift from one selection rule to the other, are limited to a maximum of 1.2%. Among the four selection rules, the STRinv rule achieves the highest BLEU score. The reason might be that the bias towards smaller swappings imposed by this rule helps the decoder to reduce incorrect longdistance swappings (Xiong et al. 2008b). 7.3 AExtractor vs. TExtractor We further compared the two algorithms for reordering example extraction. In Table 3, we find that TExtractor significantly underperforms in comparison to AExtractor. This is because the transformation from decomposition trees to BTG trees is not complete. Many crossing links due to errors and noise in word alignments generated by GIZA++ make it impossible to build BTG nodes over the corresponding words. It would be better to use alignments induced by the ITG and EM procedure described in Section 3.2 but this has a very high cost. Given the comparison in Table</context>
<context position="68154" citStr="Xiong et al. 2008" startWordPosition="10784" endWordPosition="10787">es and provide suggestions accordingly as follows. 1. Constrained decoding. We observe that in reorderable constituents which involve long-distance reorderings, their boundaries are easily violated by phrases outside them. To prohibit boundary violations, we propose constrained decoding. In constrained decoding, we define special zones in source sentences. Reorderings and translations within the zones cannot be interrupted by fragments outside the zones. We can also define other constrained operations on the zones. For example, we can prohibit swappings in any zones which contain punctuation (Xiong et al. 2008b). The beginning and ending positions of a zone are automatically learned. To be more flexible, they are not necessarily constituent boundaries. Constrained decoding is different from both soft constraints (Cherry 2008; Marton and Resnik 2008) and hard constraints (Yamamoto, Okuma, and Sumita 2008). It can be considered as in between both of these because it is harder than the former but softer than the latter. 2. Integrating special reordering rules. Some constituents are indeed non-reorderable as we discussed in Section 8.2.2. Inside or outside 561 Computational Linguistics Volume 36, Numbe</context>
</contexts>
<marker>Xiong, Zhang, Aw, Mi, Liu, Lin, 2008</marker>
<rawString>Xiong, Deyi, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu, and Shouxun Lin. 2008b. Refinements in BTG-based statistical machine translation. In Proceedings of the Third International Joint Conference on Natural Language Processing, pages 505–512, Hyderabad.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Shizhe Huang</author>
<author>Anthony Kroch</author>
</authors>
<title>The bracketing guidelines for the Penn Chinese treebank (3.0).</title>
<date>2000</date>
<tech>Technical report IRCS 00-07,</tech>
<institution>University of Pennsylvania Institute for Research in Cognitive Science,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="5761" citStr="Xue et al. 2000" startWordPosition="807" endWordPosition="810">e reordering is restricted by the ITG constraint (Wu 1997). Although it only allows two orders (straight or inverted) of nodes in any binary branching structure, it is broadly verified that the ITG constraint has good coverage of word reorderings on various language pairs (Wu, Carpuat, and Shen 2006). This makes phrase reordering in phrasal SMT a more tractable task. After enhancing phrasal SMT with a hard hierarchical skeleton, we further inject soft linguistic information into the nodes of the skeleton. We annotate each BTG node 1 In this article, we use Penn Chinese Treebank phrase labels (Xue et al. 2000). 2 Chiang (2005) also generates hierarchical structures in phrasal SMT. One difference is that Chiang’s hierarchical grammar is lexicon-sensitive because the model requires at least one pair of aligned words in each rule except for the “glue rule.” The other difference is that his grammar allows multiple nonterminals. These two differences make Chiang’s grammar more expressive than the BTG but at the cost of learning a larger model. 536 Xiong et al. Linguistically Annotated Reordering with syntactic and lexical elements by projecting the source parse tree onto the BTG binary tree. The challen</context>
</contexts>
<marker>Xue, Xia, Huang, Kroch, 2000</marker>
<rawString>Xue, Nianwen, Fei Xia, Shizhe Huang, and Anthony Kroch. 2000. The bracketing guidelines for the Penn Chinese treebank (3.0). Technical report IRCS 00-07, University of Pennsylvania Institute for Research in Cognitive Science, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<location>Toulouse.</location>
<contexts>
<context position="66386" citStr="Yamada and Knight 2001" startWordPosition="10525" endWordPosition="10528">and BWR in the two examples suggests that reordering models should respect syntactic structures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose this as a hard constraint on the ITG constraint to allow reorderings which respect the source parse tree. They all report significant improvements on different language pairs, which indicates that syntactic cohesion is very useful for phrasal SMT</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Yamada, Kenji and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 523–530, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirofumi Yamamoto</author>
<author>Hideo Okuma</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Imposing constraints from the source tree on ITG constraints for SMT.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2),</booktitle>
<pages>1--9</pages>
<location>Columbus, OH.</location>
<marker>Yamamoto, Okuma, Sumita, 2008</marker>
<rawString>Yamamoto, Hirofumi, Hideo Okuma, and Eiichiro Sumita. 2008. Imposing constraints from the source tree on ITG constraints for SMT. In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 1–9, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>A comparative study on reordering constraints in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>144--151</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="73177" citStr="Zens and Ney 2003" startWordPosition="11519" endWordPosition="11522"> n-best reordered source sentences with reordering knowledge automatically learned from the alignments between source parse trees and target translations. The approach proposed in Li et al. also enhances the connection between the preprocessing and decoding by adding a source reordering probability feature. Other approaches introduced in Nießn and Ney (2001), Popovi´c and Ney (2006), and Zhang, Zens, and Ney (2007) use morphological, POS, and chunk knowledge in the preprocessing approach, respectively. 9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the flat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more </context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>Zens, Richard, and Hermann Ney. 2003. A comparative study on reordering constraints in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 144–151, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Chi-Ho Li</author>
<author>Ming Zhou</author>
</authors>
<title>Phrase reordering model integrating syntactic knowledge for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>533--540</pages>
<location>Prague.</location>
<contexts>
<context position="73933" citStr="Zhang et al. (2007)" startWordPosition="11634" endWordPosition="11637">TG constraint, the corresponding model is the flat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more flexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in sy</context>
</contexts>
<marker>Zhang, Li, Li, Zhou, 2007</marker>
<rawString>Zhang, Dongdong, Mu Li, Chi-Ho Li, and Ming Zhou. 2007. Phrase reordering model integrating syntactic knowledge for SMT. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 533–540, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic lexicalized inversion transduction grammar for alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>475--482</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="20096" citStr="Zhang and Gildea (2005)" startWordPosition="3119" endWordPosition="3122">caused by the use of heuristic selection rules: keeping some block pairs as reordering examples while abandoning other block pairs. The kept block pairs are not necessarily the best training instances for tuning an ITG order predictor. To avoid this problem we can extract reordering examples from the BTG trees of sentence pairs. Reordering examples extracted in this way are naturally suitable for BTG order prediction. There are various ways to build BTG trees over sentence pairs. One can use BTG to produce bilingual parses of sentence pairs, similar to the approaches proposed by Wu (1997) and Zhang and Gildea (2005) but using the more sophisticated reordering models BWR or LAR. After parsing, reordering examples can be extracted from bilingual parse trees and a better reordering model is therefore induced from the extracted reordering examples. Using the better reordering model, the bilingual sentences are parsed again. This procedure is run iteratively until no performance gain is obtained in terms of translation or parsing accuracy. Formally, we can use expectation-maximization (EM) training in this procedure. In the expectation step, we first estimate the likelihood of all BTG trees of sentence pairs </context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>Zhang, Hao and Daniel Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 475–482, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
<author>David Chiang</author>
</authors>
<title>Extracting synchronous grammar rules from word-level alignments in linear time.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1081--1088</pages>
<location>Manchester.</location>
<contexts>
<context position="74789" citStr="Zhang et al. 2008" startWordPosition="11755" endWordPosition="11758">tactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax-based translation rules. 9.2 Automatic Analysis of Reordering Although there is a variety of work on phrase reordering, automatic analysis of phrase reordering is not widely explored in the SMT literature. Chiang et al. (2005) propose 563 Computational Linguistics Volume 36, Number 3 an automatic method to compare different system outputs in a fine-grained manner with regard to reordering. In their method, common word n-grams occurring in both reference translations and</context>
</contexts>
<marker>Zhang, Gildea, Chiang, 2008</marker>
<rawString>Zhang, Hao, Daniel Gildea, and David Chiang. 2008. Extracting synchronous grammar rules from word-level alignments in linear time. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1081–1088, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>559--567</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="74789" citStr="Zhang et al. 2008" startWordPosition="11755" endWordPosition="11758">tactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax-based translation rules. 9.2 Automatic Analysis of Reordering Although there is a variety of work on phrase reordering, automatic analysis of phrase reordering is not widely explored in the SMT literature. Chiang et al. (2005) propose 563 Computational Linguistics Volume 36, Number 3 an automatic method to compare different system outputs in a fine-grained manner with regard to reordering. In their method, common word n-grams occurring in both reference translations and</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Zhang, Min, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proceedings of ACL-08: HLT, pages 559–567, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqi Zhang</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Chunk-level reordering of source language sentences with automatically learned rules for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of SSST, NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>1--8</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="73933" citStr="Zhang et al. (2007)" startWordPosition="11634" endWordPosition="11637">TG constraint, the corresponding model is the flat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more flexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in sy</context>
</contexts>
<marker>Zhang, Zens, Ney, 2007</marker>
<rawString>Zhang, Yuqi, Richard Zens, and Hermann Ney. 2007. Chunk-level reordering of source language sentences with automatically learned rules for statistical machine translation. In Proceedings of SSST, NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Statistical Translation, pages 1–8, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
</authors>
<title>The CMU syntax-augmented machine translation system: SAMT on hadoop with N-best alignments.</title>
<date>2008</date>
<booktitle>In Proceedings of International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>18--25</pages>
<location>Honolulu, HI.</location>
<marker>Zollmann, Venugopal, Vogel, 2008</marker>
<rawString>Zollmann, Andreas, Ashish Venugopal, and Stephan Vogel. 2008. The CMU syntax-augmented machine translation system: SAMT on hadoop with N-best alignments. In Proceedings of International Workshop on Spoken Language Translation (IWSLT), pages 18–25, Honolulu, HI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>