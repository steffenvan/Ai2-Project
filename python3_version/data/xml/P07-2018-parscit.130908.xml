<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.141052">
<title confidence="0.9868125">
Rethinking Chinese Word Segmentation: Tokenization, Character
Classification, or Wordbreak Identification
</title>
<author confidence="0.999056">
Chu-Ren Huang Petr Simon Shu-Kai Hsieh Laurent Pr´evot
</author>
<affiliation confidence="0.8257445">
Institute of Linguistics Institute of Linguistics DoFLAL CLLE-ERSS, CNRS
Academia Sinica,Taiwan Academia Sinica,Taiwan NIU, Taiwan Universit´e de Toulouse, France
</affiliation>
<email confidence="0.961095">
churen@gate.sinica.edu.tw sim@klubko.net shukai@gmail.com prevot@univ-tlse2.fr
</email>
<sectionHeader confidence="0.994733" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999635476190476">
This paper addresses two remaining chal-
lenges in Chinese word segmentation. The
challenge in HLT is to find a robust seg-
mentation method that requires no prior lex-
ical knowledge and no extensive training to
adapt to new types of data. The challenge
in modelling human cognition and acqui-
sition it to segment words efficiently with-
out using knowledge of wordhood. We pro-
pose a radical method of word segmenta-
tion to meet both challenges. The most
critical concept that we introduce is that
Chinese word segmentation is the classifi-
cation of a string of character-boundaries
(CB’s) into either word-boundaries (WB’s)
and non-word-boundaries. In Chinese, CB’s
are delimited and distributed in between two
characters. Hence we can use the distri-
butional properties of CB among the back-
ground character strings to predict which
CB’s are WB’s.
</bodyText>
<sectionHeader confidence="0.9801165" genericHeader="keywords">
1 Introduction: modeling and theoretical
challenges
</sectionHeader>
<bodyText confidence="0.999963108108108">
The fact that word segmentation remains a main re-
search topic in the field of Chinese language pro-
cessing indicates that there maybe unresolved theo-
retical and processing issues. In terms of processing,
the fact is that none of exiting algorithms is robust
enough to reliably segment unfamiliar types of texts
before fine-tuning with massive training data. It is
true that performance of participating teams have
steadily improved since the first SigHAN Chinese
segmentation bakeoff (Sproat and Emerson, 2004).
Bakeoff 3 in 2006 produced best f-scores at 95%
and higher. However, these can only be achieved af-
ter training with the pre-segmented training dataset.
This is still very far away from real-world applica-
tion where any varieties of Chinese texts must be
successfully segmented without prior training for
HLT applications.
In terms of modeling, all exiting algorithms suffer
from the same dilemma. Word segmentation is sup-
posed to identify word boundaries in a running text,
and words defined by these boundaries are then com-
pared with the mental/electronic lexicon for POS
tagging and meaning assignments. All existing seg-
mentation algorithms, however, presuppose and/or
utilize a large lexical databases (e.g. (Chen and Liu,
1992) and many subsequent works), or uses the po-
sition of characters in a word as the basis for seg-
mentation (Xue, 2003).
In terms of processing model, this is a contradic-
tion since segmentation should be the pre-requisite
of dictionary lookup and should not presuppose lex-
ical information. In terms of cognitive modeling,
such as for acquisition, the model must be able to ac-
count for how words can be successfully segmented
and learned by a child/speaker without formal train-
ing or a priori knowledge of that word. All current
models assume comprehensive lexical knowledge.
</bodyText>
<sectionHeader confidence="0.994957" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.995734666666667">
Tokenization model. The classical model, de-
scribed in (Chen and Liu, 1992) and still adopted in
many recent works, considers text segmentation as a
</bodyText>
<page confidence="0.989882">
69
</page>
<bodyText confidence="0.96751115">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 69–72,
Prague, June 2007. c�2007 Association for Computational Linguistics
tokenization. Segmentation is typically divided into
two stages: dictionary lookup and out of vocabulary
(OOV) word identification. This approach requires
comparing and matching tens of thousands of dic-
tionary entries in addition to guessing thousands of
OOV words. That is, this is a 104x104 scale map-
ping problem with unavoidable data sparseness.
More precisely the task consist in finding
all sequences of characters Ci, ... , C,,, such that
[Ci,... C,,,] either matches an entry in the lexicon
or is guessed to be so by an unknown word resolu-
tion algorithm. One typical kind of the complexity
this model faces is the overlapping ambiguity where
e.g. a string [Ci − 1, Ci, Ci + 1] contains multiple
substrings, such as [Ci − 1, Ci, ] and [Ci, Ci + 1],
which are entries in the dictionary. The degree of
such ambiguities is estimated to fall between 5% to
20% (Chiang et al., 1996; Meng and Ip, 1999).
</bodyText>
<subsectionHeader confidence="0.915916">
2.1 Character classification model
</subsectionHeader>
<bodyText confidence="0.997429090909091">
A popular recent innovation addresses the scale
and sparseness problem by modeling segmentation
as character classification (Xue, 2003; Gao et al.,
2004). This approach observes that by classifying
characters as word-initial, word-final, penultimate,
etc., word segmentation can be reduced to a simple
classification problem which involves about 6,000
characters and around 10 positional classes. Hence
the complexity is reduced and the data sparseness
problem resolved. It is not surprising then that the
character classification approach consistently yields
better results than the tokenization approach. This
approach, however, still leaves two fundamental
questions unanswered. In terms of modeling, us-
ing character classification to predict segmentation
not only increases the complexity but also necessar-
ily creates a lower ceiling of performance In terms
of language use, actual distribution of characters is
affected by various factors involving linguistic vari-
ation, such as topic, genre, region, etc. Hence the
robustness of the character classification approach
is restricted.
The character classification model typically clas-
sifies all characters present in a string into at least
three classes: word Initial, Middle or Final po-
sitions, with possible additional classification for
word-middle characters. Word boundaries are in-
ferred based on the character classes of ‘Initial’ or
‘Final’.
This method typically yields better result than the
tokenization model. For instance, Huang and Zhao
(2006) claims to have a f-score of around 97% for
various SIGHAN bakeoff tasks.
</bodyText>
<sectionHeader confidence="0.974194" genericHeader="method">
3 A radical model
</sectionHeader>
<bodyText confidence="0.9999768125">
We propose a radical model that returns to the
core issue of word segmentation in Chinese. Cru-
cially, we no longer pre-suppose any lexical knowl-
edge. Any unsegmented text is viewed as a string
of character-breaks (CB’s) which are evenly dis-
tributed and delimited by characters. The characters
are not considered as components of words, instead,
they are contextual background providing informa-
tion about the likelihood of whether each CB is also
a wordbreak (WB). In other words, we model Chi-
nese word segmentation as wordbreak (WB) iden-
tification which takes all CB’s as candidates and
returns a subset which also serves as wordbreaks.
More crucially, this model can be trained efficiently
with a small corpus marked with wordbreaks and
does not require any lexical database.
</bodyText>
<subsectionHeader confidence="0.991044">
3.1 General idea
</subsectionHeader>
<bodyText confidence="0.92941">
Any Chinese text is envisioned as se-
quence of characters and character-boundaries
CB0C1CB1C2 ... CBi−1CiCBi ... CBn−1CnCBn The
segmentation task is reduced to finding all CBs
which are also wordbreaks WB.
</bodyText>
<subsectionHeader confidence="0.999936">
3.2 Modeling character-based information
</subsectionHeader>
<bodyText confidence="0.999931">
Since CBs are all the same and do not carry any
information, we have to rely on their distribution
among different characters to obtain useful infor-
mation for modeling. In a segmented corpus, each
WB can be differentiated from a non-WB CB by the
character string before and after it. We can assume
a reduced model where either one character imme-
diately before and after a CB is considered or two
characters (bigram). These options correspond to
consider (i) only word-initial and word-final posi-
tions (hereafter the 2-CB-model or 2CBM) or (ii) to
add second and penultimate positions (hereafter the
4-CB-model or 4CBM). All these positions are well-
attested as morphologically significant.
</bodyText>
<page confidence="0.987879">
70
</page>
<subsectionHeader confidence="0.993953">
3.3 The nature of segmentation
</subsectionHeader>
<bodyText confidence="0.999894714285714">
It is important to note that in this approaches,
although characters are recognized, unlike (Xue,
2003) and Huang et al. (2006), charactes simply
are in the background. That is, they are the neces-
sary delimiter, which allows us to look at the string
of CB’s and obtaining distributional information of
them.
</bodyText>
<sectionHeader confidence="0.97779" genericHeader="evaluation">
4 Implementation and experiments
</sectionHeader>
<bodyText confidence="0.999956">
In this section we slightly change our notation to
allow for more precise explanation. As noted be-
fore, Chinese text can be formalized as a sequence
of characters and intervals as illustrated in we call
this representation an interval form.
</bodyText>
<equation confidence="0.848106">
c1I1c2I2 ... cn−1In−1cn.
</equation>
<bodyText confidence="0.9999565">
In such a representation, each interval Ik is either
classified as a plain character boundary (CB) or as
a word boundary (WB).
We represent the neighborhood of the character
ci as (ci−2, Ii−2, ci−1, Ii−1, ci, Ii, ci+1, Ii+1), which
we can be simplified as (I−2, I−1, ci, I+1, I+2) by
removing all the neighboring characters and retain-
ing only the intervals.
</bodyText>
<subsectionHeader confidence="0.992917">
4.1 Data collection models
</subsectionHeader>
<bodyText confidence="0.999725392857143">
This section makes use of the notation introduced
above for presenting several models accounting for
character-interval class co-occurrence.
Word based model. In this model, statistical data
about word boundary frequencies for each character
is retrieved word-wise. For example, in the case of
a monosyllabic word only two word boundaries are
considered: one before and one after the character
that constitutes the monosyllabic word in question.
The method consists in mapping all the Chinese
characters available in the training corpus to a vector
of word boundary frequencies. These frequencies
are normalized by the total frequency of the char-
acter in a corpus and thus represent probability of a
word boundary occurring at a specified position with
regard to the character.
Let us consider for example, a tri-syllabic word
W = c1c2c3, that can be rewritten as the following
interval form as WI = IB−1c1IN1 c2IN2 c3IB3 .
In this interval form, each interval Ik is marked
as word boundary B or N for intervals within words.
When we consider a particular character c1 in W,
there is a word boundary at index −1 and 3. We store
this information in a mapping c1 = {−1 : 1, 3 : 11.
For each occurrence of this character in the corpus,
we modify the character vector accordingly, each
WB corresponding to an increment of the relevant
position in the vector. Every character in every word
of the corpus in processed in a similar way.
Obviously, each character yields only information
about positions of word boundaries of a word this
particular character belongs to. This means that the
index I−1 and I3 are not necessarily incremented
everytime (e.g. for monosyllabic and bi-syllabic
words)
Sliding window model. This model does not op-
erate on words, but within a window of a give size
(span) sliding through the corpus. We have exper-
imented this method with a window of size 4. Let
us consider a string, s = &amp;quot;c1c2c3c4&amp;quot; which is not
necessarily a word and is rewritten into an interval
form as sI = &amp;quot;c1I1c2I2c3I3c4I4&amp;quot;. We store the
co-occurrence character/word boundaries informa-
tion in a fixed size (span) vector.
For example, we collect the information for
character c3 and thus arrive at a vector c3 =
(I1, I2, I3, I4), where 1 is incremented at the respec-
tive position ifIk = WB, zero otherwise.
This model provides slightly different informa-
tion that the previous one. For example, if
a sequence of four characters is segmented as
c1IN1 c2IB2 c3IB3 c4IB4 (a sequence of one bi-syllabic
and two monosyllabic words), for c3 we would also
get probability of I4, i.e. an interval with index +2
. In other words, this model enables to learn WB
probability across words.
</bodyText>
<subsectionHeader confidence="0.998374">
4.2 Training corpus
</subsectionHeader>
<bodyText confidence="0.9998554">
In the next step, we convert our training corpus into
a corpus of interval vectors of specified dimension.
Let’s assume we are using dimension span = 4.
Each value in such a vector represents the proba-
bility of this interval to be a word boundary. This
probability is assigned by character for each position
with regard to the interval. For example, we have
segmented corpus C = c1I1c2I2 ... cn−1In−1cn,
where each Ik is labeled as B for word boundary
or N for non-boundary.
</bodyText>
<page confidence="0.996871">
71
</page>
<bodyText confidence="0.999954714285714">
In the second step, we move our 4-sized window
through the corpus and for each interval we query
a character at the corresponding position from the
interval to retrieve the word boundary occurrence
probability. This procedure provides us with a vec-
tor of 4 probability values for each interval. Since
we are creating this training corpus from an already
segmented text, a class (B or N) is assigned to each
interval.
The testing corpus (unsegmented) is encoded in a
similar way, but does not contain the class labels B
and N.
Finally, we automatically assign probability of 0.5
for unseen events.
</bodyText>
<subsectionHeader confidence="0.983813">
4.3 Predicting word boundary with a classifier
</subsectionHeader>
<bodyText confidence="0.999957842105263">
The Sinica corpus contains 6820 types of characters
(including Chinese characters, numbers, punctua-
tion, Latin alphabet, etc.). When the Sinica corpus is
converted into our interval vector corpus, it provides
14.4 million labeled interval vectors. In this first
study we have implement a baseline model, without
any pre-processing of punctuation, numbers, names.
A decision tree classifier (Ruggieri, 2004) has
been adopted to overcome the non-linearity issue.
The classifier was trained on the whole Sinica cor-
pus, i.e. on 14.4 million interval vectors. Due to
space limit, actual bakeoff experiment result will be
reported in our poster presentation.
Our best results is based on the sliding window
model, which provides better results. It has to be
emphasized that the test corpora were not processed
in any way, i.e. our method is sufficiently robust to
account for a large number of ambiguities like nu-
merals, foreign words.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999967428571429">
In this paper, we presented a radical and robust
model of Chinese segmentation which is supported
by initial experiment results. The model does not
pre-suppose any lexical information and it treats
character strings as context which provides infor-
mation on the possible classification of character-
breaks as word-breaks. We are confident that once
a standard model of pre-segmentation, using tex-
tual encoding information to identify WB’s which
involves non-Chinese characters, will enable us to
achieve even better results. In addition, we are look-
ing at other alternative formalisms and tools to im-
plement this model to achieve the optimal results.
Other possible extensions including experiments to
simulate acquisition of wordhood knowledge to pro-
vide support of cognitive modeling, similar to the
simulation work on categorization in Chinese by
(Redington et al., 1995). Last, but not the least,
we will explore the possibility of implementing a
sharable tool for robust segmentation for all Chinese
texts without training.
</bodyText>
<sectionHeader confidence="0.999167" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999836657142857">
Academia Sinica Balanced Corpus of Modern Chinese.
http://www.sinica.edu.tw/SinicaCorpus/
Chen K.J and Liu S.H. 1992. Word Identification for
Mandarin Chinese sentences. Proceedings of the 14th
conference on Computational Linguistics, p.101-107,
France.
Chiang,T.-H., J.-S. Chang, M.-Y. Lin and K.-Y. Su. 1996.
Statistical Word Segmentation. In C.-R. Huang, K.-J.
Chen and B.K. T’sou (eds.): Journal of Chinese Lin-
guistics, Monograph Series, Number 9, Readings in
Chinese Natural Language Processing, pp. 147-173.
Gao, J. and A. Wu and Mu Li and C.-N.Huang and H. Li
and X. Xia and H. Qin. 2004. Adaptive Chinese Word
Segmentation. In Proceedings of ACL-2004.
Meng, H. and C. W. Ip. 1999. An Analytical Study of
Transformational Tagging for Chinese Text. In. Pro-
ceedings of ROCLING XII. 101-122. Taipei
Ruggieri S. 2004. YaDT: Yet another Decision Tree
builder. Proceedings of the 16th International Con-
ference on Tools with Artificial Intelligence (ICTAI
2004): 260-265. IEEE Press, November 2004.
Richard Sproat and Thomas Emerson. 2003. The
First International Chinese Word Segmentation Bake-
off. Proceedings of the Second SIGHAN Workshop on
Chinese Language Processing, Sapporo, Japan, July
2003.
Xue, N. 2003. Chinese Word Segmentation as Charac-
ter Tagging. Computational Linguistics and Chinese
Language Processing. 8(1): 29-48
Redington, M. and N. Chater and C. Huang and L. Chang
and K. Chen. 1995. The Universality of Simple Dis-
tributional Methods: Identifying Syntactic Categories
in Mandarin Chinese. Presented at the Proceedings of
the International Conference on Cognitive Science and
Natural Language Processing. Dublin City University.
</reference>
<page confidence="0.998725">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.259494">
<title confidence="0.998599">Rethinking Chinese Word Segmentation: Tokenization, Character Classification, or Wordbreak Identification</title>
<author confidence="0.99444">Chu-Ren Huang Petr Simon Shu-Kai Hsieh Laurent Pr´evot</author>
<affiliation confidence="0.971118">Institute of Linguistics Institute of Linguistics DoFLAL CLLE-ERSS, CNRS</affiliation>
<author confidence="0.313795">Academia Sinica</author>
<author confidence="0.313795">Taiwan Academia Sinica</author>
<author confidence="0.313795">Taiwan NIU</author>
<author confidence="0.313795">Taiwan Universit´e de_Toulouse</author>
<author confidence="0.313795">France</author>
<email confidence="0.589887">churen@gate.sinica.edu.twsim@klubko.netshukai@gmail.comprevot@univ-tlse2.fr</email>
<abstract confidence="0.999741318181818">This paper addresses two remaining challenges in Chinese word segmentation. The challenge in HLT is to find a robust segmentation method that requires no prior lexical knowledge and no extensive training to adapt to new types of data. The challenge in modelling human cognition and acquisition it to segment words efficiently without using knowledge of wordhood. We propose a radical method of word segmentation to meet both challenges. The most critical concept that we introduce is that Chinese word segmentation is the classification of a string of character-boundaries (CB’s) into either word-boundaries (WB’s) and non-word-boundaries. In Chinese, CB’s are delimited and distributed in between two characters. Hence we can use the distributional properties of CB among the background character strings to predict which CB’s are WB’s.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Academia Sinica Balanced Corpus of Modern Chinese.</title>
<note>http://www.sinica.edu.tw/SinicaCorpus/</note>
<marker></marker>
<rawString>Academia Sinica Balanced Corpus of Modern Chinese. http://www.sinica.edu.tw/SinicaCorpus/</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J Chen</author>
<author>S H Liu</author>
</authors>
<title>Word Identification for Mandarin Chinese sentences.</title>
<date>1992</date>
<booktitle>Proceedings of the 14th conference on Computational Linguistics,</booktitle>
<pages>101--107</pages>
<contexts>
<context position="2543" citStr="Chen and Liu, 1992" startWordPosition="373" endWordPosition="376">with the pre-segmented training dataset. This is still very far away from real-world application where any varieties of Chinese texts must be successfully segmented without prior training for HLT applications. In terms of modeling, all exiting algorithms suffer from the same dilemma. Word segmentation is supposed to identify word boundaries in a running text, and words defined by these boundaries are then compared with the mental/electronic lexicon for POS tagging and meaning assignments. All existing segmentation algorithms, however, presuppose and/or utilize a large lexical databases (e.g. (Chen and Liu, 1992) and many subsequent works), or uses the position of characters in a word as the basis for segmentation (Xue, 2003). In terms of processing model, this is a contradiction since segmentation should be the pre-requisite of dictionary lookup and should not presuppose lexical information. In terms of cognitive modeling, such as for acquisition, the model must be able to account for how words can be successfully segmented and learned by a child/speaker without formal training or a priori knowledge of that word. All current models assume comprehensive lexical knowledge. 2 Previous work Tokenization </context>
</contexts>
<marker>Chen, Liu, 1992</marker>
<rawString>Chen K.J and Liu S.H. 1992. Word Identification for Mandarin Chinese sentences. Proceedings of the 14th conference on Computational Linguistics, p.101-107, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T-H Chiang</author>
<author>J-S Chang</author>
<author>M-Y Lin</author>
<author>K-Y Su</author>
</authors>
<title>Statistical Word Segmentation.</title>
<date>1996</date>
<booktitle>Journal of Chinese Linguistics, Monograph Series, Number 9, Readings in Chinese Natural Language Processing,</booktitle>
<pages>147--173</pages>
<editor>In C.-R. Huang, K.-J. Chen and B.K. T’sou (eds.):</editor>
<contexts>
<context position="4298" citStr="Chiang et al., 1996" startWordPosition="668" endWordPosition="671">s. That is, this is a 104x104 scale mapping problem with unavoidable data sparseness. More precisely the task consist in finding all sequences of characters Ci, ... , C,,, such that [Ci,... C,,,] either matches an entry in the lexicon or is guessed to be so by an unknown word resolution algorithm. One typical kind of the complexity this model faces is the overlapping ambiguity where e.g. a string [Ci − 1, Ci, Ci + 1] contains multiple substrings, such as [Ci − 1, Ci, ] and [Ci, Ci + 1], which are entries in the dictionary. The degree of such ambiguities is estimated to fall between 5% to 20% (Chiang et al., 1996; Meng and Ip, 1999). 2.1 Character classification model A popular recent innovation addresses the scale and sparseness problem by modeling segmentation as character classification (Xue, 2003; Gao et al., 2004). This approach observes that by classifying characters as word-initial, word-final, penultimate, etc., word segmentation can be reduced to a simple classification problem which involves about 6,000 characters and around 10 positional classes. Hence the complexity is reduced and the data sparseness problem resolved. It is not surprising then that the character classification approach con</context>
</contexts>
<marker>Chiang, Chang, Lin, Su, 1996</marker>
<rawString>Chiang,T.-H., J.-S. Chang, M.-Y. Lin and K.-Y. Su. 1996. Statistical Word Segmentation. In C.-R. Huang, K.-J. Chen and B.K. T’sou (eds.): Journal of Chinese Linguistics, Monograph Series, Number 9, Readings in Chinese Natural Language Processing, pp. 147-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>A Wu</author>
<author>Mu Li</author>
<author>C-N Huang</author>
<author>H Li</author>
<author>X Xia</author>
<author>H Qin</author>
</authors>
<title>Adaptive Chinese Word Segmentation.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-2004.</booktitle>
<contexts>
<context position="4508" citStr="Gao et al., 2004" startWordPosition="698" endWordPosition="701">n entry in the lexicon or is guessed to be so by an unknown word resolution algorithm. One typical kind of the complexity this model faces is the overlapping ambiguity where e.g. a string [Ci − 1, Ci, Ci + 1] contains multiple substrings, such as [Ci − 1, Ci, ] and [Ci, Ci + 1], which are entries in the dictionary. The degree of such ambiguities is estimated to fall between 5% to 20% (Chiang et al., 1996; Meng and Ip, 1999). 2.1 Character classification model A popular recent innovation addresses the scale and sparseness problem by modeling segmentation as character classification (Xue, 2003; Gao et al., 2004). This approach observes that by classifying characters as word-initial, word-final, penultimate, etc., word segmentation can be reduced to a simple classification problem which involves about 6,000 characters and around 10 positional classes. Hence the complexity is reduced and the data sparseness problem resolved. It is not surprising then that the character classification approach consistently yields better results than the tokenization approach. This approach, however, still leaves two fundamental questions unanswered. In terms of modeling, using character classification to predict segment</context>
</contexts>
<marker>Gao, Wu, Li, Huang, Li, Xia, Qin, 2004</marker>
<rawString>Gao, J. and A. Wu and Mu Li and C.-N.Huang and H. Li and X. Xia and H. Qin. 2004. Adaptive Chinese Word Segmentation. In Proceedings of ACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Meng</author>
<author>C W Ip</author>
</authors>
<title>An Analytical Study of Transformational Tagging for Chinese Text. In.</title>
<date>1999</date>
<booktitle>Proceedings of ROCLING XII.</booktitle>
<pages>101--122</pages>
<location>Taipei</location>
<contexts>
<context position="4318" citStr="Meng and Ip, 1999" startWordPosition="672" endWordPosition="675"> 104x104 scale mapping problem with unavoidable data sparseness. More precisely the task consist in finding all sequences of characters Ci, ... , C,,, such that [Ci,... C,,,] either matches an entry in the lexicon or is guessed to be so by an unknown word resolution algorithm. One typical kind of the complexity this model faces is the overlapping ambiguity where e.g. a string [Ci − 1, Ci, Ci + 1] contains multiple substrings, such as [Ci − 1, Ci, ] and [Ci, Ci + 1], which are entries in the dictionary. The degree of such ambiguities is estimated to fall between 5% to 20% (Chiang et al., 1996; Meng and Ip, 1999). 2.1 Character classification model A popular recent innovation addresses the scale and sparseness problem by modeling segmentation as character classification (Xue, 2003; Gao et al., 2004). This approach observes that by classifying characters as word-initial, word-final, penultimate, etc., word segmentation can be reduced to a simple classification problem which involves about 6,000 characters and around 10 positional classes. Hence the complexity is reduced and the data sparseness problem resolved. It is not surprising then that the character classification approach consistently yields bet</context>
</contexts>
<marker>Meng, Ip, 1999</marker>
<rawString>Meng, H. and C. W. Ip. 1999. An Analytical Study of Transformational Tagging for Chinese Text. In. Proceedings of ROCLING XII. 101-122. Taipei</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ruggieri</author>
</authors>
<title>YaDT: Yet another Decision Tree builder.</title>
<date>2004</date>
<booktitle>Proceedings of the 16th International Conference on Tools with Artificial Intelligence (ICTAI</booktitle>
<pages>260--265</pages>
<publisher>IEEE Press,</publisher>
<contexts>
<context position="12926" citStr="Ruggieri, 2004" startWordPosition="2061" endWordPosition="2062"> (unsegmented) is encoded in a similar way, but does not contain the class labels B and N. Finally, we automatically assign probability of 0.5 for unseen events. 4.3 Predicting word boundary with a classifier The Sinica corpus contains 6820 types of characters (including Chinese characters, numbers, punctuation, Latin alphabet, etc.). When the Sinica corpus is converted into our interval vector corpus, it provides 14.4 million labeled interval vectors. In this first study we have implement a baseline model, without any pre-processing of punctuation, numbers, names. A decision tree classifier (Ruggieri, 2004) has been adopted to overcome the non-linearity issue. The classifier was trained on the whole Sinica corpus, i.e. on 14.4 million interval vectors. Due to space limit, actual bakeoff experiment result will be reported in our poster presentation. Our best results is based on the sliding window model, which provides better results. It has to be emphasized that the test corpora were not processed in any way, i.e. our method is sufficiently robust to account for a large number of ambiguities like numerals, foreign words. 5 Conclusion In this paper, we presented a radical and robust model of Chine</context>
</contexts>
<marker>Ruggieri, 2004</marker>
<rawString>Ruggieri S. 2004. YaDT: Yet another Decision Tree builder. Proceedings of the 16th International Conference on Tools with Artificial Intelligence (ICTAI 2004): 260-265. IEEE Press, November 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Thomas Emerson</author>
</authors>
<title>The First International Chinese Word Segmentation Bakeoff.</title>
<date>2003</date>
<booktitle>Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<location>Sapporo, Japan,</location>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Thomas Emerson. 2003. The First International Chinese Word Segmentation Bakeoff. Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan, July 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
</authors>
<title>Chinese Word Segmentation as Character Tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing.</booktitle>
<volume>8</volume>
<issue>1</issue>
<pages>29--48</pages>
<contexts>
<context position="2658" citStr="Xue, 2003" startWordPosition="397" endWordPosition="398">ese texts must be successfully segmented without prior training for HLT applications. In terms of modeling, all exiting algorithms suffer from the same dilemma. Word segmentation is supposed to identify word boundaries in a running text, and words defined by these boundaries are then compared with the mental/electronic lexicon for POS tagging and meaning assignments. All existing segmentation algorithms, however, presuppose and/or utilize a large lexical databases (e.g. (Chen and Liu, 1992) and many subsequent works), or uses the position of characters in a word as the basis for segmentation (Xue, 2003). In terms of processing model, this is a contradiction since segmentation should be the pre-requisite of dictionary lookup and should not presuppose lexical information. In terms of cognitive modeling, such as for acquisition, the model must be able to account for how words can be successfully segmented and learned by a child/speaker without formal training or a priori knowledge of that word. All current models assume comprehensive lexical knowledge. 2 Previous work Tokenization model. The classical model, described in (Chen and Liu, 1992) and still adopted in many recent works, considers tex</context>
<context position="4489" citStr="Xue, 2003" startWordPosition="696" endWordPosition="697">r matches an entry in the lexicon or is guessed to be so by an unknown word resolution algorithm. One typical kind of the complexity this model faces is the overlapping ambiguity where e.g. a string [Ci − 1, Ci, Ci + 1] contains multiple substrings, such as [Ci − 1, Ci, ] and [Ci, Ci + 1], which are entries in the dictionary. The degree of such ambiguities is estimated to fall between 5% to 20% (Chiang et al., 1996; Meng and Ip, 1999). 2.1 Character classification model A popular recent innovation addresses the scale and sparseness problem by modeling segmentation as character classification (Xue, 2003; Gao et al., 2004). This approach observes that by classifying characters as word-initial, word-final, penultimate, etc., word segmentation can be reduced to a simple classification problem which involves about 6,000 characters and around 10 positional classes. Hence the complexity is reduced and the data sparseness problem resolved. It is not surprising then that the character classification approach consistently yields better results than the tokenization approach. This approach, however, still leaves two fundamental questions unanswered. In terms of modeling, using character classification</context>
<context position="7822" citStr="Xue, 2003" startWordPosition="1207" endWordPosition="1208">iated from a non-WB CB by the character string before and after it. We can assume a reduced model where either one character immediately before and after a CB is considered or two characters (bigram). These options correspond to consider (i) only word-initial and word-final positions (hereafter the 2-CB-model or 2CBM) or (ii) to add second and penultimate positions (hereafter the 4-CB-model or 4CBM). All these positions are wellattested as morphologically significant. 70 3.3 The nature of segmentation It is important to note that in this approaches, although characters are recognized, unlike (Xue, 2003) and Huang et al. (2006), charactes simply are in the background. That is, they are the necessary delimiter, which allows us to look at the string of CB’s and obtaining distributional information of them. 4 Implementation and experiments In this section we slightly change our notation to allow for more precise explanation. As noted before, Chinese text can be formalized as a sequence of characters and intervals as illustrated in we call this representation an interval form. c1I1c2I2 ... cn−1In−1cn. In such a representation, each interval Ik is either classified as a plain character boundary (C</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Xue, N. 2003. Chinese Word Segmentation as Character Tagging. Computational Linguistics and Chinese Language Processing. 8(1): 29-48</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Redington</author>
<author>N Chater</author>
<author>C Huang</author>
<author>L Chang</author>
<author>K Chen</author>
</authors>
<title>The Universality of Simple Distributional Methods: Identifying Syntactic Categories</title>
<date>1995</date>
<booktitle>in Mandarin Chinese. Presented at the Proceedings of the International Conference on Cognitive Science and Natural Language Processing.</booktitle>
<institution>Dublin City University.</institution>
<marker>Redington, Chater, Huang, Chang, Chen, 1995</marker>
<rawString>Redington, M. and N. Chater and C. Huang and L. Chang and K. Chen. 1995. The Universality of Simple Distributional Methods: Identifying Syntactic Categories in Mandarin Chinese. Presented at the Proceedings of the International Conference on Cognitive Science and Natural Language Processing. Dublin City University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>