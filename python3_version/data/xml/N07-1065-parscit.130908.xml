<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004176">
<title confidence="0.997685">
Automatic Answer Typing for How-Questions
</title>
<author confidence="0.997643">
Christopher Pinchak and Shane Bergsma
</author>
<affiliation confidence="0.9981845">
Department of Computing Science
University of Alberta
</affiliation>
<address confidence="0.943041">
Edmonton, Alberta, T6G 2E8, Canada
</address>
<email confidence="0.995429">
pinchak,bergsma @cs.ualberta.ca
</email>
<sectionHeader confidence="0.995583" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998221">
We introduce an answer typing strategy
specific to quantifiable how questions. Us-
ing the web as a data source, we auto-
matically collect answer units appropri-
ate to a given how-question type. Exper-
imental results show answer typing with
these units outperforms traditional fixed-
category answer typing and other strate-
gies based on the occurrences of numeri-
cal entities in text.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988417113207547">
Question answering (QA) systems are emerging as
a viable means of obtaining specific information in
the face of large availability. Answer typing is an
important part of QA because it allows the system
to greatly reduce the number of potential answers,
using general knowledge of the answer form for a
specific question. For example, for what, where, and
who questions like “What is the capital of Canada?”,
answer typing can filter the phrases which might be
proposed as candidate answers, perhaps only identi-
fying those textual entities known to be cities.
We focus on answer typing for how-questions, a
subset of questions which have received little spe-
cific attention in the QA community. Rather than
seeking an open-ended noun or verb phrase, how-
questions often seek a numerical measurement ex-
pressed in terms of a certain kind of unit, as in the
following example:
Example 1: “How heavy is a grizzly bear?”
An answer typing system might expect answers to
include units like kilograms, pounds, or tons. Enti-
ties with inappropriate units, such as feet, meters, or
honey pots, would be excluded as candidate answers.
We specifically handle the subset of how-
questions that we call how-adjective questions; that
is, questions of the form “How adjective...?” such
as Example 1. In particular, we do not address “how
many” questions, which usually specify the units di-
rectly following many, nor “how much” questions,
which generally seek a monetary value.
Hand-crafting a comprehensive list of units ap-
propriate to many different adjectives is time-
consuming and likely to miss important units. For
example, an annotator might miss gigabytes for a
measure of “how large.” Instead of compiling a list
manually, we propose a means of automatically gen-
erating lists of appropriate units for a number of real-
world questions.
How-adjective questions represent a significant
portion of queries sent to search engines; of the
35 million queries in the AOL search query data
set (Pass et al., 2006), over 11,000 are of the form
“how adjective...” – close to one in every three thou-
sand queries. Of those 11,000 queries, 152 different
adjectives are used, ranging from the expected “how
old” and “how far” to the obscure “how orwellian.”
This high proportion of queries is especially strik-
ing given that search engines provide little sup-
port for answering how-adjective questions. Indeed,
most IR systems work by keyword matching. En-
tering Example 1 into a search engine returns doc-
uments discussing the grizzly’s “heavy fur,” “heavy,
shaggy coat” and “heavy stout body.” When faced
</bodyText>
<page confidence="0.973514">
516
</page>
<note confidence="0.7984675">
Proceedings of NAACL HLT 2007, pages 516–523,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999961714285714">
with such results, a smart search engine user knows
to inject answer units into their query to refine their
search, perhaps querying “grizzly pounds.” They
may also convert their adjective (heavy) to a related
concept (weight), for the query “grizzly weight.”
Similarly, our approach discovers unit types by
first converting the adjective to a related concept, us-
ing information in a structured ontology. For exam-
ple, “big” can be used to obtain “size,” and “tall” can
derive “height.” We then use an online search engine
to automatically find units appropriate to the con-
cept, given the assumption that the concept is explic-
itly measured in terms of specific units, e.g., height
can be measured in feet, weight can be measured in
pounds, and size can be measured in gigabytes.
By automatically extracting units, we do not re-
quire a set of prior questions with associated an-
swers. Instead, we use actual questions as a source
of realistic adjectives only. This is important be-
cause while large sets of existing questions can be
obtained (Li and Roth, 2002), there are many fewer
questions with available answers.
Our experiments demonstrate that how-question-
specific unit lists consistently achieve higher answer
identification performance than fixed-type, general-
purpose answer typing (which propose all numeri-
cal entities as answer candidates). Furthermore, our
precomputed, automatically-generated unit lists are
shown to consistently achieve better performance
than baseline systems which derive unit lists at run-
time from documents relevant to the answer query,
even when such documents are gathered using per-
fect knowledge of the answer distribution.
The outline of the paper is as follows. In Section 2
we outline related work. In Section 3 we provide the
framework of our answer-typing model. Section 4
describes the implementation details of the model.
Section 5 describes our experimental methodology,
while Section 6 shows the benefits of using auto-
matic how-question answer-typing. We conclude
with possible directions of future research opened
by this novel problem formulation.
</bodyText>
<sectionHeader confidence="0.994867" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.99910168627451">
Answer typing is an important component of any
QA system, but varies greatly in the approach
taken (Prager et al., 2003; Harabagiu et al., 2005).
Basically, answer typing provides a means of filter-
ing answer candidates as either appropriate or in-
appropriate to the question. For example, Li and
Roth (2002) assign one of fifty possible types to a
question based on features present in the question.
Answer candidates can then be selected from text
by finding entities whose type matches that of the
input question. Similarly, Ittycheriah et al. (2000)
assign one of the MUC named-entity types to each
input question. In these fixed-category approaches,
how-questions are assigned a fixed type in the same
manner as other questions. For how-questions, this
corresponds to a numerical type. However, retriev-
ing all numerical entities will provide lower answer
identification precision than a system that only pro-
vides those specified with the expected answer units.
Pinchak and Lin (2006) propose a dynamic an-
swer typing system which computes a unique score
for the appropriateness of any word to a particu-
lar question. Unfortunately, their question context-
mapping is limited to what, where, and who ques-
tions, and thus is not defined for how-questions.
Wu et al. (2005) handle how-questions differently
than other questions. They use special hand-crafted
rules to assign a particular answer target during the
answer typing phase. In this way, they take advan-
tage of the structure inherent in how-questions rather
than just treating them as general queries. However,
manually hand-crafting types is costly, and would
have to be repeated if the system was moved to a
new language or a new query domain. Our auto-
matic approach does not suffer from this drawback.
Light et al. (2001) showed that for a small fixed
set of answer types, multiple words tagged with
the same type will exist even with perfect passage
retrieval, sentence retrieval, and type assignment.
For example, Example 1 may be answered with a
sentence such as “bears range in weight from the
smaller black bear at 400 pounds to the gigantic griz-
zly at over 1200 pounds” in which two answers have
appropriate units but only one of which is correct.
We provide results in Section 6 confirming the lim-
its of answer typing at narrowing answer focus, us-
ing varying levels of perfect information.
Our approach makes use of the web as a large
corpus of useful information. Exploiting the vast
amount of data on the web is part of a growing trend
in Natural Language Processing (Keller and Lapata,
</bodyText>
<page confidence="0.993258">
517
</page>
<bodyText confidence="0.999891">
2003). Indeed, many QA systems have been devel-
oped using the web (to varying degrees) to assist in
finding a correct answer (Brill et al., 2001; Cucerzan
and Agichtein, 2005; Radev et al., 2001), as the web
is the largest available corpus even if its information
can be difficult to harness. Rather than relying on
the web to find the answer to a question, we rely on it
as a source of information on appropriate units only.
Should the domain of the question answering system
change from general factoid questions, units may be
extracted from a smaller, domain-specific corpus.
</bodyText>
<sectionHeader confidence="0.988808" genericHeader="method">
3 Model Framework
</sectionHeader>
<bodyText confidence="0.999428315789474">
The objective of our model is to create a list of rel-
evant units for an adjective that may be found in a
how-question. We wish to create these lists a pri-
ori and off-line so that they are applicable to future
questions. Although the model described here can
be applied on-line at the time of question answering,
the resources and time required make off-line gener-
ation of unit lists the preferred approach.
We wish to automatically learn a mapping
in which is a set of adjectives derived
from how-questions and is a set of lists of units
associated with these adjectives. For example, an
element of this mapping might be:
high feet, meter, foot, inches, ...
which assigns height measurements to “how high”
questions. Inducing this mapping means establish-
ing a connection, or co-occurrence, between each
adjective and its units . In the following sub-
sections, we show how to establish this connection.
</bodyText>
<subsectionHeader confidence="0.999884">
3.1 Using WordNet for Adjective Expansion
</subsectionHeader>
<bodyText confidence="0.999970157894737">
In common documents, such as news articles or
web pages, the co-occurrence of an adjective and
its units may be unlikely. For example, the co-
occurrence between “heavy” and “pounds” may
not be as prevalent as the co-occurrence between
“weight” and “pounds.” We therefore propose us-
ing WordNet (Fellbaum, 1998) to expand the how-
adjective to a set of related concepts the adjective
may be used to describe. We denote a related con-
cept of as. In the above example, “heavy” can be
used to describe a “weight.” Two useful WordNet re-
lations are the attribute relation, in which the adjec-
tive is an attribute of the concept, and in cases where
no attribute exists, the derivationally-related words.
“Heavy” is an attribute of “weight” whereas the
derivationally-related form is “heaviness,” a plausi-
ble but less useful concept. Next we describe how
the particular co-occurrence of the related concept
and unit is obtained.
</bodyText>
<subsectionHeader confidence="0.999919">
3.2 Using Google to Obtain Counts
</subsectionHeader>
<bodyText confidence="0.988011242424242">
We selected the Google search engine as a source
of co-occurrence data due to the large number of in-
dexed documents from which co-occurrence counts
can be derived. To further enhance the quality of
co-occurrence data, we search on the specific phrase
“ is measured in” in which is one of the related
concepts of . This allows for the simultaneous dis-
covery of unknown units and the retrieval of their
co-occurrence counts.
Sentences in which the pattern occurs are parsed
using Minipar (Lin, 1998b) so that we can obtain
the word related to “measured” via the preposi-
tional in relation. This allows us to handle senten-
tial constructions that may intervene between “mea-
sured” and a meaningful unit. For each unit that
is related to “measured” via in, we increment the
co-occurrence count , thereby collecting fre-
quency counts for each with.
The pattern’s precision prevents incidental co-
occurrence between a related concept and some unit
that may occur simply because of the general topic
of the document. For example, “size is measured
in” matches “Size is measured in gigabytes, and per-
formance is measured in milliseconds”. In this ex-
ample, the co-occurrence count gigabytessize
would be incremented by one, whereas there is no
co-occurrence between “size” and “milliseconds.”
Due to the large amount of data available to Google,
we can afford to restrict ourselves to a single pattern
and still expect to find meaningful units.
To gather the co-occurrence counts between an
adjective and a unit , we first expand to the
set of related concepts and then compute:
</bodyText>
<equation confidence="0.517646">
(1)
</equation>
<bodyText confidence="0.997663">
These frequencies can then be used by the scoring
functions described in the following section.
</bodyText>
<page confidence="0.98304">
518
</page>
<subsectionHeader confidence="0.99866">
3.3 Filtering the Unit List
</subsectionHeader>
<bodyText confidence="0.99964880952381">
For a given adjective and a particular unit
with co-occurrence , we define two impor-
tant statistics:
The first equation measures the likelihood of a
unit being an answer unit for a how-question with
the given adjective . The second equation mea-
sures, for a given unit , how likely a how question
with adjective asked the question answered by .
The second measure is particularly useful in cases
where a unit co-occurs with a number of differ-
ent adjectives. These units are inherently less useful
for answer typing. For example, if the word “terms”
occurs on the unit list of adjectives such as “high,”
“long,” and “heavy,” it may indicate that “terms”
is not an appropriate measure for any of these con-
cepts, but rather just a word likely to co-occur with
nouns that can be measured.
We propose using the measures and
to score and rank our how-adjective
unit lists. alone showed inferior perfor-
mance on the development set and so will not be
further considered. approximates
the standard - measure (Salton and Buckley,
1988). is the term frequency in the unit
list and is the inverse document frequency
of the unit over all unit lists. Using these mea-
sures, we can create a unit list for an adjective as
in which is the score of unit with ad-
or ) and is
some threshold imposed to deal with the amount of
noise present in the co-occurrence data. This thresh-
old allows us to vary the required strength of the as-
sociation between the unit and the question in or-
der to consider the unit as appropriate to the how-
adjective. In Section 6, we demonstrate this flexi-
bility by showing how answer identification preci-
sion and recall can be traded off as desired by the
given application. The value can also
be passed to downstream modules of the question
answering process (such as the answer extractor),
which may then exploit the association value di-
rectly.
</bodyText>
<sectionHeader confidence="0.998754" genericHeader="method">
4 Implementation Details
</sectionHeader>
<subsectionHeader confidence="0.999264">
4.1 Automatic How-Adjective Discovery
</subsectionHeader>
<bodyText confidence="0.999930407407407">
An initial step in implementing answer typing for
how-adjective questions is to decide which adjec-
tives would benefit from types. WordNet gives a
set of all adjectives, but providing answer type units
for all these adjectives is unnecessary and poten-
tially misleading. Many adjectives would clearly
never occur in a how-adjective query (i.e., “how ve-
hicular...?”), and even some that do, like the “how
orwellian” query mentioned above, are difficult to
quantify. For these, a simple search with keyword
matching as in typical information retrieval would
be preferable.
We have a two-stage process for identifying unit-
typable how-adjectives. First, we examine the AOL
query data (Pass et al., 2006) and extract as candi-
dates all 152 adjectives that occur with the pattern
“how adjective is/are/was/were.” Second, we fil-
ter adjectives that do not have a related concept in
WordNet (Section 3.1). We built unit lists for the
104 adjectives that remained.
Given that both the query database and WordNet
may lack information, it is important to consider the
coverage of actual how-adjective questions that unit
lists collected this way may have. Reassuringly, ex-
periments have shown 100% coverage of the 96 ad-
jectives in our development and test question set,
taken from the TREC QA corpus (see Section 5).
</bodyText>
<subsectionHeader confidence="0.977974">
4.2 Similar Word Expansion
</subsectionHeader>
<bodyText confidence="0.999965636363636">
Unfortunately, we found that search results obtained
using the pattern described in Section 3.2 do not pro-
duce a wide variety of units. Web pages often do
not use a slang term when mentioning the unit of
measurement; a search for “size is measured in gigs”
on Google returns zero pages. Also, searching with
Google’s API and obtaining relevant documents can
be time consuming, and we must limit the number
of pages considered. Thus, there is strong motiva-
tion to expand the list of units obtained from Google
by automatically considering similar units.
</bodyText>
<figure confidence="0.29517">
jective (either
</figure>
<page confidence="0.971887">
519
</page>
<bodyText confidence="0.993737161290323">
We gather similar units from an automatically-
constructed thesaurus of distributionally similar
words (Lin, 1998a). The similar word expansion can
add a term like gigs as a unit for size by virtue of its
association with gigabytes, which is on the original
list.
Unit similarity can be thought of as a mapping
in which is a set of units and
is sets of related units. If is an element of for
a particular adjective , the mapping gives
us a way to add new words to the unit list for .
For example, the similar word list for “gigabytes”
might be GB, megabytes, kilobytes, KB, byte, GHz,
gigs... , which can all be added to the unit list for
the adjective “large.”
After expanding each element of the unit list
for adjective , we have a new set of units :
where .
For each there is an associated score
that measures how similar is to
. We define the score of units that do not co-occur
on similar word lists to be zero and the similarity of
two identical units to be one. We can then use these
scores to assign estimated co-occurrence counts for
any unit in the expanded unit list :
If a unit also occurs in the set of expanded
similar units for another another unit , that
is, , then the original co-occurrence fre-
quency of and ,, will be boosted
by the similarity-weighted frequency of on the ex-
panded unit list of ,
</bodyText>
<subsectionHeader confidence="0.99997">
4.3 Selection of Answer Candidates
</subsectionHeader>
<bodyText confidence="0.999854846153846">
For a given how-adjective question and a document
of interest, we use a two-stage process to identify
the entities in the document that are suitable answers
for the question. First, the named entity recognizer
of Minipar is used to identify all numerical entities
in text, labeled as NUM. Minipar labels times, dates,
monetary amounts, and address numbers with types
other than NUM and so we can correctly exclude
these from consideration. We then inspect the con-
text of all NUM entities to see if a unit exists on the
pre-computed unit list for the given how-adjective.
Textual entities that pass both stages of our identifi-
cation process are considered as candidate answers.
</bodyText>
<sectionHeader confidence="0.99873" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999959666666667">
This section presents experiments comparing our
how-adjective answer typing approach to alterna-
tive schemes on an answer identification task. We
compare our two unit ranking functions and
(Section 3.3) and test the merits of
using the similar unit expansion (Section 4.2).
</bodyText>
<subsectionHeader confidence="0.988787">
5.1 Evaluation Questions
</subsectionHeader>
<bodyText confidence="0.999961454545455">
The clearest way to test a QA system is to evalu-
ate it on a large set of questions. Although our an-
swer typing system is not capable of fully answer-
ing questions, we will make use of the how-adjective
questions from TREC 2002-2005 (Vorhees, 2002) as
a set of test data. We take eight of the questions as a
development set (used for preliminary investigations
of scoring functions – no parameters can be set on
the development set specifically) and 86 of the ques-
tions as a final, unseen test set. Seventeen different
adjectives occur in the test questions.
</bodyText>
<subsectionHeader confidence="0.995837">
5.2 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.988413652173913">
We evaluate our system with an approach we call
Answer-Identification Precision Recall (AIPR). For
a particular scoring threshold (Section 3.3), each ad-
jective has a corresponding unit list, which is used to
extract answer candidates from documents (Section
4.3). To ensure the performance of the IR-engine is
not an issue in evaluation, we only use documents
judged to contain the correct answer by TREC.
Answer-identification precision corresponds to
the number of correct answers among the candi-
date answers extracted by our system. Answer-
identification recall is the number of correct answers
extracted among the total number of correct answers
in the answer documents.
A plot of AIPR allows the designer of a particular
QA system to decide on the optimum PR-tradeoff
for the answer typing task. If other stages of QA
rely on a large number of candidates, a high recall
value may be desired so no potential answers are
missed. If answer typing is used as a means of boost-
ing already-likely answers, high precision may in-
stead be favoured.
.
</bodyText>
<page confidence="0.984149">
520
</page>
<subsectionHeader confidence="0.995995">
5.3 Comparison Systems
</subsectionHeader>
<bodyText confidence="0.999452413043478">
This section describes the various systems we com-
pare with our approach. Recall that perfect AIPR
performance is not possible with typing alone (Sec-
tion 2, (Light et al., 2001)), and thus we pro-
vide some of our comparison systems with varying
amounts of perfect answer information in order to
establish the highest performance possible in differ-
ent scenarios on the given dataset.
Query-specific Oracle: The best possible system
creates a unit list for each specific how-question in-
dividually. This list is created using only those units
in the answer pattern of the TREC-provided judge-
ment for this specific question.
Adjective-specific Oracle: This system is de-
signed, like ours, to provide a unit list for each how-
adjective, rather than for a specific question. The
unit list for a particular adjective contains all the
units from all the test set answers of how-adjective
questions containing that adjective. It is optimal in
the sense it will identify every correct answer for
each how-adjective, but only contains those units
necessary for this identification.
Fixed-Category: This system gives the perfor-
mance of a general-purpose, fixed-category answer
typing approach applied to how-questions. In a
fixed-category approach, all how-questions are clas-
sified as seeking numerical answers, and thus all nu-
merical answers are returned as answer candidates.
IR-Document Inferred: Here we infer question
units from documents believed to be relevant to the
question. An IR system (TREC’s PRISE) is given
a how-adjective question, and returns a set of doc-
uments for that query. Every numerical digit in the
documents can be considered a possible answer to
the question, and the units associated with those val-
ues can be collected as the unit list, ranked (and
thresholded) by frequency. We remove units that oc-
cur in a list of 527 stopwords, and filter numerical
modifiers like “hundred, thousand, million, etc.”
Answer-Document Inferred: This approach is
identical to the IR-Document Inferred approach,
except the documents are only those documents
judged by TREC to contain the answer. In this way
the Answer-Document Inferred approach provides
somewhat of an upper bound on Document Inferred
unit typing, by assuming perfect document retrieval.
</bodyText>
<figure confidence="0.9580755">
0 0.2 0.4 0.6 0.8 1
Recall
</figure>
<figureCaption confidence="0.9962615">
Figure 1: Microaveraged AIPR with different scor-
ing functions, unit lists.
</figureCaption>
<bodyText confidence="0.9998479">
Inferring the answer units from the set of rele-
vant documents is similar in spirit to (Daum´e III and
Marcu, 2006). In one of their experiments in query-
focused summarization, they show competitive sum-
marization performance without even providing the
query, as the query model is inferred solely from
the commonality in relevant documents. In our case,
good performance will also be possible if the actual
answers have the highest commonality among the
numerical values in the relevant documents.
</bodyText>
<sectionHeader confidence="0.999297" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.928206357142857">
The microaveraged Answer-Identification Precision
Recall over all question-answer pairs is plotted in
Figures 1 and 2. Macroaveraged results are similar.
For our own automatic answer typing approaches,
our first observation is the benefit of ranking with
as opposed to (Figure 1).
Over most of the recall range, both the unexpanded
(automatic) unit lists and the expanded unit lists
improve in precision by a few percent when using
both probabilistic scoring statistics. Secondly, note
that both systems using the expanded unit lists can
achieve almost 20% higher maximum recall than the
unexpanded unit list systems. This provides strong
justification for the small overhead of looking up
</bodyText>
<figure confidence="0.9853055">
Automatic Units, P(u|a)
Automatic Units, P(u|a)P(a|u)
Expanded Auto, P(u|a)
Expanded Auto, P(u|a)P(a|u)
Interpolated Precision 0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
</figure>
<page confidence="0.994429">
521
</page>
<bodyText confidence="0.999899458333333">
similar words for items on our unit list.
We next examine the AIPR performance of our
comparison systems versus our best-performing au-
tomatic unit typing approach (Figure 2). The query-
specific oracle is able to achieve the highest perfor-
mance because of perfect knowledge of the units ap-
propriate to a given question. However, its preci-
sion is only 42.2%. That is, the answer identifica-
tion accuracy is limited because the correct answer
shares its units with other numerical entities in the
answer documents. Slightly worse, the adjective-
specific oracle is limited to 34.2% precision. Un-
like the query-specific oracle, if the question is “how
long did WWII last?”, the entities with the irrele-
vant units “meters” and “kilometers” must also be
proposed as candidate answers because they occur
in answers to other “how long” questions. This ora-
cle thus provides a more appropriate upper bound on
automatic unit-typing performance as our automatic
approaches also build unit lists for adjectives rather
than questions. Note again that unit lists for adjec-
tives can be generated off-line whereas unit lists for
specific questions need the query before processing.
In terms of recall, both upper-bound systems top
out at around 78% (with our expanded systems
reaching close to this at about 72%). At first, this
number seems fairly disappointing: if how-adjective
questions only have answer units in 78% of the
cases, perhaps our typing approach is not entirely
appropriate. On inspecting the actual misses, how-
ever, we find that 10 of the 16 missed questions cor-
respond to “how old” questions. These are often
answered without units (e.g. “at age 52.”). Higher
recall would be possible if the system defaults to ex-
tracting all numerical entities for “how old” ques-
tions. On the remaining questions, high recall can
indeed be obtained.
Also of note is the clear disadvantage of using the
standard fixed-category approach to how-question
answer typing (Figure 2). Its precision runs at just
under 5%, about a quarter of the lowest precision of
any of our unit-list approaches at any recall value.
However, fixed-category typing does achieve high
recall, roughly 96%, missing only numerical entities
unrecognized by Minipar. This high recall is possi-
ble because fixed-category typing does not miss an-
swers for “how old” questions.
Both inferred approaches also perform worse than
</bodyText>
<figure confidence="0.999207888888889">
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
0 0.2 0.4 0.6 0.8 1
Recall
Query-Specific Oracle
Adjective-Specific Oracle
Fixed-Category
IR-Document Inferred
Answer-Document Inferred
Expanded Auto, P(u|a)P(a|u)
</figure>
<figureCaption confidence="0.9345675">
Figure 2: Microaveraged AIPR of our approach ver-
sus comparison systems.
</figureCaption>
<bodyText confidence="0.9997846">
our system (Figure 2). Thus inferring units from
relevant documents does not seem promising, as
even the unrealistic approach of inferring only from
known answer documents cannot achieve as high in
answer-identification precision. Also, realistically
using IR-retrieved documents has universally lower
AIPR. As expected, answer-document inferred re-
call plateaus at the same spot as the oracle systems,
as it also requires a unit after each numerical en-
tity (hurting it, again, on the “how old” questions).
Despite their lower performance, note that these in-
ferred approaches are completely orthogonal to our
offline automatic answer-typing, so a future pos-
sibility remains to combine both kinds of systems
within a unified framework.
</bodyText>
<sectionHeader confidence="0.997847" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99729375">
Although it is difficult to evaluate the impact of our
approach until it is integrated into a full QA-system,
we have clearly demonstrated the advantages of au-
tomatic answer typing for how-questions. We have
</bodyText>
<figure confidence="0.545255">
Interpolated Precision
</figure>
<page confidence="0.979826">
522
</page>
<bodyText confidence="0.99999528125">
shown the improvements possible by ranking with
different co-occurrence statistics, and the benefit of
expanding unit lists with similar words. Experi-
mental results show our approaches achieve superior
AIPR performance over all realistic baselines.
In addition to proposing a competitive system, we
believe we have established a framework and eval-
uation methodology that may be of use to other re-
searchers. For example, although manual typing re-
mains an option, our approach can at least provide
a good set of candidate units to consider. Further-
more, a similar-word database can expand the list
obtained by manual typing. Finally, users may also
wish to rank the manual types in some way, and thus
configure the system for a particular level of answer-
identification precision/recall.
Our success with these unit lists has encouraged
two main directions of future work. First, we plan
to move to a discriminative approach to combin-
ing scores and weighting unit features using a small
labeled set. Secondly, we will look at incorporat-
ing units into the information retrieval process. Our
motivating example in Section 1 retrieved irrelevant
documents when given to a search engine, and this
seems to be a general trend in how-question IR. Less
than 60% of the TREC how-questions have a unit
of the correct type anywhere in the top ten docu-
ments returned by the PRISE IR engine, and less
than half correspondingly had a correct answer in
the top ten at all. Making the information retrieval
process aware of the desired answer types will be an
important future direction of QA research.
</bodyText>
<sectionHeader confidence="0.99837" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99279575">
We gratefully acknowledge support from the Natu-
ral Sciences and Engineering Research Council of
Canada, the Alberta Ingenuity Fund, and the Alberta
Informatics Circle of Research Excellence.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999922395833333">
E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2001. Data-
Intensive Question Answering. In TREC.
S. Cucerzan and E. Agichtein. 2005. Factoid Question An-
swering over Unstructured and Structured Web Content. In
TREC.
H. Daum´e III and D. Marcu. 2006. Bayesian query-focused
summarization. In COLING-ACL, pages 305–312.
C. Fellbaum. 1998. WordNet: An Electronic Lexical Database.
MIT Press.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl,
and P. Wang. 2005. Employing Two Question Answering
Systems in TREC-2005. In TREC.
A. Ittycheriah, M. Franz, W-J. Zhu, A. Ratnaparkhi, and
R. Mammone. 2000. IBM’s Statistical Question Answer-
ing System. In TREC.
Frank Keller and Mirella Lapata. 2003. Using the web to obtain
frequencies for unseen bigrams. Computational Linguistics,
29(3):459–484.
X. Li and D. Roth. 2002. Learning Question Classifiers. In
COLING, pages 556–562.
M. Light, G. Mann, E. Riloff, and E. Breck. 2001. Analyses for
Elucidating Current Question Answering Technology. Nat-
ural Language Engineering, 7(4):325–342.
D. Lin. 1998a. Automatic retrieval and clustering of similar
words. In COLING-ACL, pages 768–773.
D. Lin. 1998b. Dependency-based evaluation of MINIPAR.
In Workshop on the Evaluation of Parsing Systems, First In-
ternational Conference on Language Resources and Evalu-
ation.
G. Pass, A. Chowdhury, and C. Torgeson. 2006. A picture of
search. In The First International Conference on Scalable
Information Systems.
C. Pinchak and D. Lin. 2006. A Probabilistic Answer Type
Model. In EACL.
J. Prager, J. Chu-Carroll, K. Czuba, C. Welty, A. Ittycheriah,
and R. Mahindru. 2003. IBM’s PIQUANT in TREC2003.
In TREC.
D. Radev, H. Qi, Z. Zheng, S. Blair-Goldensohn, Z. Zhang,
W. Fan, and J. Prager. 2001. Mining the Web for Answers
to Natural Language Questions. In CIKM.
G. Salton and C. Buckley. 1988. Term weighting approaches in
automatic text retrieval. Information Processing and Man-
agement, 24(5):513–523.
E. Vorhees. 2002. Overview of the TREC 2002 question an-
swering track. In TREC.
M. Wu, M. Duan, S. Shaikh, S. Small, and T. Strzalkowski.
2005. ILQUA – An IE-Driven Question Answering System.
In TREC.
</reference>
<page confidence="0.998931">
523
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.829165">
<title confidence="0.999928">Automatic Answer Typing for How-Questions</title>
<author confidence="0.972192">Pinchak</author>
<affiliation confidence="0.9997165">Department of Computing University of</affiliation>
<address confidence="0.936273">Edmonton, Alberta, T6G 2E8,</address>
<email confidence="0.938424">pinchak,bergsma@cs.ualberta.ca</email>
<abstract confidence="0.995912">We introduce an answer typing strategy to quantifiable Using the web as a data source, we automatically collect answer units appropriate to a given how-question type. Experimental results show answer typing with these units outperforms traditional fixedcategory answer typing and other strategies based on the occurrences of numerical entities in text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Lin</author>
<author>M Banko</author>
<author>S Dumais</author>
<author>A Ng</author>
</authors>
<title>DataIntensive Question Answering. In TREC.</title>
<date>2001</date>
<contexts>
<context position="8108" citStr="Brill et al., 2001" startWordPosition="1300" endWordPosition="1303">ounds to the gigantic grizzly at over 1200 pounds” in which two answers have appropriate units but only one of which is correct. We provide results in Section 6 confirming the limits of answer typing at narrowing answer focus, using varying levels of perfect information. Our approach makes use of the web as a large corpus of useful information. Exploiting the vast amount of data on the web is part of a growing trend in Natural Language Processing (Keller and Lapata, 517 2003). Indeed, many QA systems have been developed using the web (to varying degrees) to assist in finding a correct answer (Brill et al., 2001; Cucerzan and Agichtein, 2005; Radev et al., 2001), as the web is the largest available corpus even if its information can be difficult to harness. Rather than relying on the web to find the answer to a question, we rely on it as a source of information on appropriate units only. Should the domain of the question answering system change from general factoid questions, units may be extracted from a smaller, domain-specific corpus. 3 Model Framework The objective of our model is to create a list of relevant units for an adjective that may be found in a how-question. We wish to create these list</context>
</contexts>
<marker>Brill, Lin, Banko, Dumais, Ng, 2001</marker>
<rawString>E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2001. DataIntensive Question Answering. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>E Agichtein</author>
</authors>
<title>Factoid Question Answering over Unstructured and Structured Web Content. In TREC.</title>
<date>2005</date>
<contexts>
<context position="8138" citStr="Cucerzan and Agichtein, 2005" startWordPosition="1304" endWordPosition="1307">c grizzly at over 1200 pounds” in which two answers have appropriate units but only one of which is correct. We provide results in Section 6 confirming the limits of answer typing at narrowing answer focus, using varying levels of perfect information. Our approach makes use of the web as a large corpus of useful information. Exploiting the vast amount of data on the web is part of a growing trend in Natural Language Processing (Keller and Lapata, 517 2003). Indeed, many QA systems have been developed using the web (to varying degrees) to assist in finding a correct answer (Brill et al., 2001; Cucerzan and Agichtein, 2005; Radev et al., 2001), as the web is the largest available corpus even if its information can be difficult to harness. Rather than relying on the web to find the answer to a question, we rely on it as a source of information on appropriate units only. Should the domain of the question answering system change from general factoid questions, units may be extracted from a smaller, domain-specific corpus. 3 Model Framework The objective of our model is to create a list of relevant units for an adjective that may be found in a how-question. We wish to create these lists a priori and off-line so tha</context>
</contexts>
<marker>Cucerzan, Agichtein, 2005</marker>
<rawString>S. Cucerzan and E. Agichtein. 2005. Factoid Question Answering over Unstructured and Structured Web Content. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>D Marcu</author>
</authors>
<title>Bayesian query-focused summarization.</title>
<date>2006</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>305--312</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>H. Daum´e III and D. Marcu. 2006. Bayesian query-focused summarization. In COLING-ACL, pages 305–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9812" citStr="Fellbaum, 1998" startWordPosition="1588" endWordPosition="1589">er, foot, inches, ... which assigns height measurements to “how high” questions. Inducing this mapping means establishing a connection, or co-occurrence, between each adjective and its units . In the following subsections, we show how to establish this connection. 3.1 Using WordNet for Adjective Expansion In common documents, such as news articles or web pages, the co-occurrence of an adjective and its units may be unlikely. For example, the cooccurrence between “heavy” and “pounds” may not be as prevalent as the co-occurrence between “weight” and “pounds.” We therefore propose using WordNet (Fellbaum, 1998) to expand the howadjective to a set of related concepts the adjective may be used to describe. We denote a related concept of as. In the above example, “heavy” can be used to describe a “weight.” Two useful WordNet relations are the attribute relation, in which the adjective is an attribute of the concept, and in cases where no attribute exists, the derivationally-related words. “Heavy” is an attribute of “weight” whereas the derivationally-related form is “heaviness,” a plausible but less useful concept. Next we describe how the particular co-occurrence of the related concept and unit is obt</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>C Clark</author>
<author>M Bowden</author>
<author>A Hickl</author>
<author>P Wang</author>
</authors>
<title>Employing Two Question Answering Systems in TREC-2005.</title>
<date>2005</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="5552" citStr="Harabagiu et al., 2005" startWordPosition="874" endWordPosition="877">of the answer distribution. The outline of the paper is as follows. In Section 2 we outline related work. In Section 3 we provide the framework of our answer-typing model. Section 4 describes the implementation details of the model. Section 5 describes our experimental methodology, while Section 6 shows the benefits of using automatic how-question answer-typing. We conclude with possible directions of future research opened by this novel problem formulation. 2 Previous Work Answer typing is an important component of any QA system, but varies greatly in the approach taken (Prager et al., 2003; Harabagiu et al., 2005). Basically, answer typing provides a means of filtering answer candidates as either appropriate or inappropriate to the question. For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question. Answer candidates can then be selected from text by finding entities whose type matches that of the input question. Similarly, Ittycheriah et al. (2000) assign one of the MUC named-entity types to each input question. In these fixed-category approaches, how-questions are assigned a fixed type in the same manner as other questions. For how-ques</context>
</contexts>
<marker>Harabagiu, Moldovan, Clark, Bowden, Hickl, Wang, 2005</marker>
<rawString>S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang. 2005. Employing Two Question Answering Systems in TREC-2005. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>M Franz</author>
<author>W-J Zhu</author>
<author>A Ratnaparkhi</author>
<author>R Mammone</author>
</authors>
<title>IBM’s Statistical Question Answering System. In TREC.</title>
<date>2000</date>
<contexts>
<context position="5959" citStr="Ittycheriah et al. (2000)" startWordPosition="940" endWordPosition="943">future research opened by this novel problem formulation. 2 Previous Work Answer typing is an important component of any QA system, but varies greatly in the approach taken (Prager et al., 2003; Harabagiu et al., 2005). Basically, answer typing provides a means of filtering answer candidates as either appropriate or inappropriate to the question. For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question. Answer candidates can then be selected from text by finding entities whose type matches that of the input question. Similarly, Ittycheriah et al. (2000) assign one of the MUC named-entity types to each input question. In these fixed-category approaches, how-questions are assigned a fixed type in the same manner as other questions. For how-questions, this corresponds to a numerical type. However, retrieving all numerical entities will provide lower answer identification precision than a system that only provides those specified with the expected answer units. Pinchak and Lin (2006) propose a dynamic answer typing system which computes a unique score for the appropriateness of any word to a particular question. Unfortunately, their question con</context>
</contexts>
<marker>Ittycheriah, Franz, Zhu, Ratnaparkhi, Mammone, 2000</marker>
<rawString>A. Ittycheriah, M. Franz, W-J. Zhu, A. Ratnaparkhi, and R. Mammone. 2000. IBM’s Statistical Question Answering System. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Mirella Lapata</author>
</authors>
<title>Using the web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<marker>Keller, Lapata, 2003</marker>
<rawString>Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3):459–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning Question Classifiers. In</title>
<date>2002</date>
<booktitle>COLING,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="4356" citStr="Li and Roth, 2002" startWordPosition="698" endWordPosition="701"> obtain “size,” and “tall” can derive “height.” We then use an online search engine to automatically find units appropriate to the concept, given the assumption that the concept is explicitly measured in terms of specific units, e.g., height can be measured in feet, weight can be measured in pounds, and size can be measured in gigabytes. By automatically extracting units, we do not require a set of prior questions with associated answers. Instead, we use actual questions as a source of realistic adjectives only. This is important because while large sets of existing questions can be obtained (Li and Roth, 2002), there are many fewer questions with available answers. Our experiments demonstrate that how-questionspecific unit lists consistently achieve higher answer identification performance than fixed-type, generalpurpose answer typing (which propose all numerical entities as answer candidates). Furthermore, our precomputed, automatically-generated unit lists are shown to consistently achieve better performance than baseline systems which derive unit lists at runtime from documents relevant to the answer query, even when such documents are gathered using perfect knowledge of the answer distribution.</context>
<context position="5714" citStr="Li and Roth (2002)" startWordPosition="900" endWordPosition="903">del. Section 4 describes the implementation details of the model. Section 5 describes our experimental methodology, while Section 6 shows the benefits of using automatic how-question answer-typing. We conclude with possible directions of future research opened by this novel problem formulation. 2 Previous Work Answer typing is an important component of any QA system, but varies greatly in the approach taken (Prager et al., 2003; Harabagiu et al., 2005). Basically, answer typing provides a means of filtering answer candidates as either appropriate or inappropriate to the question. For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question. Answer candidates can then be selected from text by finding entities whose type matches that of the input question. Similarly, Ittycheriah et al. (2000) assign one of the MUC named-entity types to each input question. In these fixed-category approaches, how-questions are assigned a fixed type in the same manner as other questions. For how-questions, this corresponds to a numerical type. However, retrieving all numerical entities will provide lower answer identification precision than a system that only</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>X. Li and D. Roth. 2002. Learning Question Classifiers. In COLING, pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Light</author>
<author>G Mann</author>
<author>E Riloff</author>
<author>E Breck</author>
</authors>
<title>Analyses for Elucidating Current Question Answering Technology.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="7186" citStr="Light et al. (2001)" startWordPosition="1137" endWordPosition="1140">ing is limited to what, where, and who questions, and thus is not defined for how-questions. Wu et al. (2005) handle how-questions differently than other questions. They use special hand-crafted rules to assign a particular answer target during the answer typing phase. In this way, they take advantage of the structure inherent in how-questions rather than just treating them as general queries. However, manually hand-crafting types is costly, and would have to be repeated if the system was moved to a new language or a new query domain. Our automatic approach does not suffer from this drawback. Light et al. (2001) showed that for a small fixed set of answer types, multiple words tagged with the same type will exist even with perfect passage retrieval, sentence retrieval, and type assignment. For example, Example 1 may be answered with a sentence such as “bears range in weight from the smaller black bear at 400 pounds to the gigantic grizzly at over 1200 pounds” in which two answers have appropriate units but only one of which is correct. We provide results in Section 6 confirming the limits of answer typing at narrowing answer focus, using varying levels of perfect information. Our approach makes use o</context>
<context position="20134" citStr="Light et al., 2001" startWordPosition="3342" endWordPosition="3345"> of correct answers in the answer documents. A plot of AIPR allows the designer of a particular QA system to decide on the optimum PR-tradeoff for the answer typing task. If other stages of QA rely on a large number of candidates, a high recall value may be desired so no potential answers are missed. If answer typing is used as a means of boosting already-likely answers, high precision may instead be favoured. . 520 5.3 Comparison Systems This section describes the various systems we compare with our approach. Recall that perfect AIPR performance is not possible with typing alone (Section 2, (Light et al., 2001)), and thus we provide some of our comparison systems with varying amounts of perfect answer information in order to establish the highest performance possible in different scenarios on the given dataset. Query-specific Oracle: The best possible system creates a unit list for each specific how-question individually. This list is created using only those units in the answer pattern of the TREC-provided judgement for this specific question. Adjective-specific Oracle: This system is designed, like ours, to provide a unit list for each howadjective, rather than for a specific question. The unit li</context>
</contexts>
<marker>Light, Mann, Riloff, Breck, 2001</marker>
<rawString>M. Light, G. Mann, E. Riloff, and E. Breck. 2001. Analyses for Elucidating Current Question Answering Technology. Natural Language Engineering, 7(4):325–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>768--773</pages>
<contexts>
<context position="10949" citStr="Lin, 1998" startWordPosition="1781" endWordPosition="1782"> how the particular co-occurrence of the related concept and unit is obtained. 3.2 Using Google to Obtain Counts We selected the Google search engine as a source of co-occurrence data due to the large number of indexed documents from which co-occurrence counts can be derived. To further enhance the quality of co-occurrence data, we search on the specific phrase “ is measured in” in which is one of the related concepts of . This allows for the simultaneous discovery of unknown units and the retrieval of their co-occurrence counts. Sentences in which the pattern occurs are parsed using Minipar (Lin, 1998b) so that we can obtain the word related to “measured” via the prepositional in relation. This allows us to handle sentential constructions that may intervene between “measured” and a meaningful unit. For each unit that is related to “measured” via in, we increment the co-occurrence count , thereby collecting frequency counts for each with. The pattern’s precision prevents incidental cooccurrence between a related concept and some unit that may occur simply because of the general topic of the document. For example, “size is measured in” matches “Size is measured in gigabytes, and performance </context>
<context position="16075" citStr="Lin, 1998" startWordPosition="2638" endWordPosition="2639">ribed in Section 3.2 do not produce a wide variety of units. Web pages often do not use a slang term when mentioning the unit of measurement; a search for “size is measured in gigs” on Google returns zero pages. Also, searching with Google’s API and obtaining relevant documents can be time consuming, and we must limit the number of pages considered. Thus, there is strong motivation to expand the list of units obtained from Google by automatically considering similar units. jective (either 519 We gather similar units from an automaticallyconstructed thesaurus of distributionally similar words (Lin, 1998a). The similar word expansion can add a term like gigs as a unit for size by virtue of its association with gigabytes, which is on the original list. Unit similarity can be thought of as a mapping in which is a set of units and is sets of related units. If is an element of for a particular adjective , the mapping gives us a way to add new words to the unit list for . For example, the similar word list for “gigabytes” might be GB, megabytes, kilobytes, KB, byte, GHz, gigs... , which can all be added to the unit list for the adjective “large.” After expanding each element of the unit list for a</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998a. Automatic retrieval and clustering of similar words. In COLING-ACL, pages 768–773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems, First International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="10949" citStr="Lin, 1998" startWordPosition="1781" endWordPosition="1782"> how the particular co-occurrence of the related concept and unit is obtained. 3.2 Using Google to Obtain Counts We selected the Google search engine as a source of co-occurrence data due to the large number of indexed documents from which co-occurrence counts can be derived. To further enhance the quality of co-occurrence data, we search on the specific phrase “ is measured in” in which is one of the related concepts of . This allows for the simultaneous discovery of unknown units and the retrieval of their co-occurrence counts. Sentences in which the pattern occurs are parsed using Minipar (Lin, 1998b) so that we can obtain the word related to “measured” via the prepositional in relation. This allows us to handle sentential constructions that may intervene between “measured” and a meaningful unit. For each unit that is related to “measured” via in, we increment the co-occurrence count , thereby collecting frequency counts for each with. The pattern’s precision prevents incidental cooccurrence between a related concept and some unit that may occur simply because of the general topic of the document. For example, “size is measured in” matches “Size is measured in gigabytes, and performance </context>
<context position="16075" citStr="Lin, 1998" startWordPosition="2638" endWordPosition="2639">ribed in Section 3.2 do not produce a wide variety of units. Web pages often do not use a slang term when mentioning the unit of measurement; a search for “size is measured in gigs” on Google returns zero pages. Also, searching with Google’s API and obtaining relevant documents can be time consuming, and we must limit the number of pages considered. Thus, there is strong motivation to expand the list of units obtained from Google by automatically considering similar units. jective (either 519 We gather similar units from an automaticallyconstructed thesaurus of distributionally similar words (Lin, 1998a). The similar word expansion can add a term like gigs as a unit for size by virtue of its association with gigabytes, which is on the original list. Unit similarity can be thought of as a mapping in which is a set of units and is sets of related units. If is an element of for a particular adjective , the mapping gives us a way to add new words to the unit list for . For example, the similar word list for “gigabytes” might be GB, megabytes, kilobytes, KB, byte, GHz, gigs... , which can all be added to the unit list for the adjective “large.” After expanding each element of the unit list for a</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998b. Dependency-based evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems, First International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Pass</author>
<author>A Chowdhury</author>
<author>C Torgeson</author>
</authors>
<title>A picture of search.</title>
<date>2006</date>
<booktitle>In The First International Conference on Scalable Information Systems.</booktitle>
<contexts>
<context position="2590" citStr="Pass et al., 2006" startWordPosition="409" endWordPosition="412">tly following many, nor “how much” questions, which generally seek a monetary value. Hand-crafting a comprehensive list of units appropriate to many different adjectives is timeconsuming and likely to miss important units. For example, an annotator might miss gigabytes for a measure of “how large.” Instead of compiling a list manually, we propose a means of automatically generating lists of appropriate units for a number of realworld questions. How-adjective questions represent a significant portion of queries sent to search engines; of the 35 million queries in the AOL search query data set (Pass et al., 2006), over 11,000 are of the form “how adjective...” – close to one in every three thousand queries. Of those 11,000 queries, 152 different adjectives are used, ranging from the expected “how old” and “how far” to the obscure “how orwellian.” This high proportion of queries is especially striking given that search engines provide little support for answering how-adjective questions. Indeed, most IR systems work by keyword matching. Entering Example 1 into a search engine returns documents discussing the grizzly’s “heavy fur,” “heavy, shaggy coat” and “heavy stout body.” When faced 516 Proceedings </context>
<context position="14759" citStr="Pass et al., 2006" startWordPosition="2421" endWordPosition="2424"> decide which adjectives would benefit from types. WordNet gives a set of all adjectives, but providing answer type units for all these adjectives is unnecessary and potentially misleading. Many adjectives would clearly never occur in a how-adjective query (i.e., “how vehicular...?”), and even some that do, like the “how orwellian” query mentioned above, are difficult to quantify. For these, a simple search with keyword matching as in typical information retrieval would be preferable. We have a two-stage process for identifying unittypable how-adjectives. First, we examine the AOL query data (Pass et al., 2006) and extract as candidates all 152 adjectives that occur with the pattern “how adjective is/are/was/were.” Second, we filter adjectives that do not have a related concept in WordNet (Section 3.1). We built unit lists for the 104 adjectives that remained. Given that both the query database and WordNet may lack information, it is important to consider the coverage of actual how-adjective questions that unit lists collected this way may have. Reassuringly, experiments have shown 100% coverage of the 96 adjectives in our development and test question set, taken from the TREC QA corpus (see Section</context>
</contexts>
<marker>Pass, Chowdhury, Torgeson, 2006</marker>
<rawString>G. Pass, A. Chowdhury, and C. Torgeson. 2006. A picture of search. In The First International Conference on Scalable Information Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pinchak</author>
<author>D Lin</author>
</authors>
<title>A Probabilistic Answer Type Model.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="6394" citStr="Pinchak and Lin (2006)" startWordPosition="1006" endWordPosition="1009"> features present in the question. Answer candidates can then be selected from text by finding entities whose type matches that of the input question. Similarly, Ittycheriah et al. (2000) assign one of the MUC named-entity types to each input question. In these fixed-category approaches, how-questions are assigned a fixed type in the same manner as other questions. For how-questions, this corresponds to a numerical type. However, retrieving all numerical entities will provide lower answer identification precision than a system that only provides those specified with the expected answer units. Pinchak and Lin (2006) propose a dynamic answer typing system which computes a unique score for the appropriateness of any word to a particular question. Unfortunately, their question contextmapping is limited to what, where, and who questions, and thus is not defined for how-questions. Wu et al. (2005) handle how-questions differently than other questions. They use special hand-crafted rules to assign a particular answer target during the answer typing phase. In this way, they take advantage of the structure inherent in how-questions rather than just treating them as general queries. However, manually hand-craftin</context>
</contexts>
<marker>Pinchak, Lin, 2006</marker>
<rawString>C. Pinchak and D. Lin. 2006. A Probabilistic Answer Type Model. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
<author>C Welty</author>
<author>A Ittycheriah</author>
<author>R Mahindru</author>
</authors>
<date>2003</date>
<booktitle>IBM’s PIQUANT in TREC2003. In TREC.</booktitle>
<contexts>
<context position="5527" citStr="Prager et al., 2003" startWordPosition="870" endWordPosition="873">ng perfect knowledge of the answer distribution. The outline of the paper is as follows. In Section 2 we outline related work. In Section 3 we provide the framework of our answer-typing model. Section 4 describes the implementation details of the model. Section 5 describes our experimental methodology, while Section 6 shows the benefits of using automatic how-question answer-typing. We conclude with possible directions of future research opened by this novel problem formulation. 2 Previous Work Answer typing is an important component of any QA system, but varies greatly in the approach taken (Prager et al., 2003; Harabagiu et al., 2005). Basically, answer typing provides a means of filtering answer candidates as either appropriate or inappropriate to the question. For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question. Answer candidates can then be selected from text by finding entities whose type matches that of the input question. Similarly, Ittycheriah et al. (2000) assign one of the MUC named-entity types to each input question. In these fixed-category approaches, how-questions are assigned a fixed type in the same manner as othe</context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, Welty, Ittycheriah, Mahindru, 2003</marker>
<rawString>J. Prager, J. Chu-Carroll, K. Czuba, C. Welty, A. Ittycheriah, and R. Mahindru. 2003. IBM’s PIQUANT in TREC2003. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>H Qi</author>
<author>Z Zheng</author>
<author>S Blair-Goldensohn</author>
<author>Z Zhang</author>
<author>W Fan</author>
<author>J Prager</author>
</authors>
<title>Mining the Web for Answers to Natural Language Questions.</title>
<date>2001</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="8159" citStr="Radev et al., 2001" startWordPosition="1308" endWordPosition="1311"> in which two answers have appropriate units but only one of which is correct. We provide results in Section 6 confirming the limits of answer typing at narrowing answer focus, using varying levels of perfect information. Our approach makes use of the web as a large corpus of useful information. Exploiting the vast amount of data on the web is part of a growing trend in Natural Language Processing (Keller and Lapata, 517 2003). Indeed, many QA systems have been developed using the web (to varying degrees) to assist in finding a correct answer (Brill et al., 2001; Cucerzan and Agichtein, 2005; Radev et al., 2001), as the web is the largest available corpus even if its information can be difficult to harness. Rather than relying on the web to find the answer to a question, we rely on it as a source of information on appropriate units only. Should the domain of the question answering system change from general factoid questions, units may be extracted from a smaller, domain-specific corpus. 3 Model Framework The objective of our model is to create a list of relevant units for an adjective that may be found in a how-question. We wish to create these lists a priori and off-line so that they are applicable</context>
</contexts>
<marker>Radev, Qi, Zheng, Blair-Goldensohn, Zhang, Fan, Prager, 2001</marker>
<rawString>D. Radev, H. Qi, Z. Zheng, S. Blair-Goldensohn, Z. Zhang, W. Fan, and J. Prager. 2001. Mining the Web for Answers to Natural Language Questions. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>24--5</pages>
<contexts>
<context position="13174" citStr="Salton and Buckley, 1988" startWordPosition="2154" endWordPosition="2157">e a unit co-occurs with a number of different adjectives. These units are inherently less useful for answer typing. For example, if the word “terms” occurs on the unit list of adjectives such as “high,” “long,” and “heavy,” it may indicate that “terms” is not an appropriate measure for any of these concepts, but rather just a word likely to co-occur with nouns that can be measured. We propose using the measures and to score and rank our how-adjective unit lists. alone showed inferior performance on the development set and so will not be further considered. approximates the standard - measure (Salton and Buckley, 1988). is the term frequency in the unit list and is the inverse document frequency of the unit over all unit lists. Using these measures, we can create a unit list for an adjective as in which is the score of unit with ador ) and is some threshold imposed to deal with the amount of noise present in the co-occurrence data. This threshold allows us to vary the required strength of the association between the unit and the question in order to consider the unit as appropriate to the howadjective. In Section 6, we demonstrate this flexibility by showing how answer identification precision and recall ca</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>G. Salton and C. Buckley. 1988. Term weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Vorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2002</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="18548" citStr="Vorhees, 2002" startWordPosition="3081" endWordPosition="3082">tification process are considered as candidate answers. 5 Experiments This section presents experiments comparing our how-adjective answer typing approach to alternative schemes on an answer identification task. We compare our two unit ranking functions and (Section 3.3) and test the merits of using the similar unit expansion (Section 4.2). 5.1 Evaluation Questions The clearest way to test a QA system is to evaluate it on a large set of questions. Although our answer typing system is not capable of fully answering questions, we will make use of the how-adjective questions from TREC 2002-2005 (Vorhees, 2002) as a set of test data. We take eight of the questions as a development set (used for preliminary investigations of scoring functions – no parameters can be set on the development set specifically) and 86 of the questions as a final, unseen test set. Seventeen different adjectives occur in the test questions. 5.2 Evaluation Methodology We evaluate our system with an approach we call Answer-Identification Precision Recall (AIPR). For a particular scoring threshold (Section 3.3), each adjective has a corresponding unit list, which is used to extract answer candidates from documents (Section 4.3)</context>
</contexts>
<marker>Vorhees, 2002</marker>
<rawString>E. Vorhees. 2002. Overview of the TREC 2002 question answering track. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wu</author>
<author>M Duan</author>
<author>S Shaikh</author>
<author>S Small</author>
<author>T Strzalkowski</author>
</authors>
<title>ILQUA – An IE-Driven Question Answering System. In TREC.</title>
<date>2005</date>
<contexts>
<context position="6676" citStr="Wu et al. (2005)" startWordPosition="1054" endWordPosition="1057"> how-questions are assigned a fixed type in the same manner as other questions. For how-questions, this corresponds to a numerical type. However, retrieving all numerical entities will provide lower answer identification precision than a system that only provides those specified with the expected answer units. Pinchak and Lin (2006) propose a dynamic answer typing system which computes a unique score for the appropriateness of any word to a particular question. Unfortunately, their question contextmapping is limited to what, where, and who questions, and thus is not defined for how-questions. Wu et al. (2005) handle how-questions differently than other questions. They use special hand-crafted rules to assign a particular answer target during the answer typing phase. In this way, they take advantage of the structure inherent in how-questions rather than just treating them as general queries. However, manually hand-crafting types is costly, and would have to be repeated if the system was moved to a new language or a new query domain. Our automatic approach does not suffer from this drawback. Light et al. (2001) showed that for a small fixed set of answer types, multiple words tagged with the same ty</context>
</contexts>
<marker>Wu, Duan, Shaikh, Small, Strzalkowski, 2005</marker>
<rawString>M. Wu, M. Duan, S. Shaikh, S. Small, and T. Strzalkowski. 2005. ILQUA – An IE-Driven Question Answering System. In TREC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>