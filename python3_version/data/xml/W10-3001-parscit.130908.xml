<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000477">
<title confidence="0.9814635">
The CoNLL-2010 Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text
</title>
<author confidence="0.999238">
Rich´ard Farkas1,2, Veronika Vincze1, Gy¨orgy M´ora1, J´anos Csirik1,2, Gy¨orgy Szarvas3
</author>
<affiliation confidence="0.925742333333333">
1 University of Szeged, Department of Informatics
2 Hungarian Academy of Sciences, Research Group on Artificial Intelligence
3 Technische Universit¨at Darmstadt, Ubiquitous Knowledge Processing Lab
</affiliation>
<email confidence="0.979087">
{rfarkas,vinczev,gymora,csirik}@inf.u-szeged.hu,
szarvas@tk.informatik.tu-darmstadt.de
</email>
<sectionHeader confidence="0.992587" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9990815625">
The CoNLL-2010 Shared Task was dedi-
cated to the detection of uncertainty cues
and their linguistic scope in natural lan-
guage texts. The motivation behind this
task was that distinguishing factual and
uncertain information in texts is of essen-
tial importance in information extraction.
This paper provides a general overview
of the shared task, including the annota-
tion protocols of the training and evalua-
tion datasets, the exact task definitions, the
evaluation metrics employed and the over-
all results. The paper concludes with an
analysis of the prominent approaches and
an overview of the systems submitted to
the shared task.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960333333333">
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
provides a competitive shared task for the Com-
putational Linguistics community. After a five-
year period of multi-language semantic role label-
ing and syntactic dependency parsing tasks, a new
task was introduced in 2010, namely the detection
of uncertainty and its linguistic scope in natural
language sentences.
In natural language processing (NLP) – and in
particular, in information extraction (IE) – many
applications seek to extract factual information
from text. In order to distinguish facts from unre-
liable or uncertain information, linguistic devices
such as hedges (indicating that authors do not
or cannot back up their opinions/statements with
facts) have to be identified. Applications should
handle detected speculative parts in a different
manner. A typical example is protein-protein in-
teraction extraction from biological texts, where
the aim is to mine text evidence for biological enti-
ties that are in a particular relation with each other.
Here, while an uncertain relation might be of some
interest for an end-user as well, such information
must not be confused with factual textual evidence
(reliable information).
Uncertainty detection has two levels. Auto-
matic hedge detectors might attempt to identify
sentences which contain uncertain information
and handle whole sentences in a different man-
ner or they might attempt to recognize in-sentence
spans which are speculative. In-sentence uncer-
tainty detection is a more complicated task com-
pared to the sentence-level one, but it has bene-
fits for NLP applications as there may be spans
containing useful factual information in a sentence
that otherwise contains uncertain parts. For ex-
ample, in the following sentence the subordinated
clause starting with although contains factual in-
formation while uncertain information is included
in the main clause and the embedded question.
Although IL-1 has been reported to con-
tribute to Th17 differentiation in mouse
and man, it remains to be determined
{whether therapeutic targeting of IL-1
will substantially affect IL-17 in RA}.
Both tasks were addressed in the CoNLL-2010
Shared Task, in order to provide uniform manu-
ally annotated benchmark datasets for both and to
compare their difficulties and state-of-the-art so-
lutions for them. The uncertainty detection prob-
lem consists of two stages. First, keywords/cues
indicating uncertainty should be recognized then
either a sentence-level decision is made or the lin-
guistic scope of the cue words has to be identified.
The latter task falls within the scope of semantic
analysis of sentences exploiting syntactic patterns,
as hedge spans can usually be determined on the
basis of syntactic patterns dependent on the key-
word.
</bodyText>
<page confidence="0.822694">
1
</page>
<note confidence="0.9703895">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997446" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999484355555556">
The term hedging was originally introduced by
Lakoff (1972). However, hedge detection has re-
ceived considerable interest just recently in the
NLP community. Light et al. (2004) used a hand-
crafted list of hedge cues to identify specula-
tive sentences in MEDLINE abstracts and several
biomedical NLP applications incorporate rules for
identifying the certainty of extracted information
(Friedman et al., 1994; Chapman et al., 2007; Ara-
maki et al., 2009; Conway et al., 2009).
The most recent approaches to uncertainty de-
tection exploit machine learning models that uti-
lize manually labeled corpora. Medlock and
Briscoe (2007) used single words as input features
in order to classify sentences from biological ar-
ticles (FlyBase) as speculative or non-speculative
based on semi-automatically collected training ex-
amples. Szarvas (2008) extended the methodology
of Medlock and Briscoe (2007) to use n-gram fea-
tures and a semi-supervised selection of the key-
word features. Kilicoglu and Bergler (2008) pro-
posed a linguistically motivated approach based
on syntactic information to semi-automatically re-
fine a list of hedge cues. Ganter and Strube (2009)
proposed an approach for the automatic detec-
tion of sentences containing uncertainty based on
Wikipedia weasel tags and syntactic patterns.
The BioScope corpus (Vincze et al., 2008) is
manually annotated with negation and specula-
tion cues and their linguistic scope. It consists
of clinical free-texts, biological texts from full pa-
pers and scientific abstracts. Using BioScope for
training and evaluation, Morante and Daelemans
(2009) developed a scope detector following a su-
pervised sequence labeling approach while ¨Ozg¨ur
and Radev (2009) developed a rule-based system
that exploits syntactic patterns.
Several related works have also been published
within the framework of The BioNLP’09 Shared
Task on Event Extraction (Kim et al., 2009), where
a separate subtask was dedicated to predicting
whether the recognized biological events are un-
der negation or speculation, based on the GENIA
event corpus annotations (Kilicoglu and Bergler,
2009; Van Landeghem et al., 2009).
</bodyText>
<sectionHeader confidence="0.995307" genericHeader="method">
3 Uncertainty Annotation Guidelines
</sectionHeader>
<bodyText confidence="0.9996753125">
The shared task addressed the detection of uncer-
tainty in two domains. As uncertainty detection
is extremely important for biomedical information
extraction and most existing approaches have tar-
geted such applications, participants were asked
to develop systems for hedge detection in bio-
logical scientific articles. Uncertainty detection
is also important, e.g. in encyclopedias, where
the goal is to collect reliable world knowledge
about real-world concepts and topics. For exam-
ple, Wikipedia explicitly declares that statements
reflecting author opinions or those not backed up
by facts (e.g. references) should be avoided (see
3.2 for details). Thus, the community-edited en-
cyclopedia, Wikipedia became one of the subjects
of the shared task as well.
</bodyText>
<subsectionHeader confidence="0.999455">
3.1 Hedges in Biological Scientific Articles
</subsectionHeader>
<bodyText confidence="0.997874">
In the biomedical domain, sentences were manu-
ally annotated for both hedge cues and their lin-
guistic scope. Hedging is typically expressed by
using specific linguistic devices (which we refer to
as cues in this article) that modify the meaning or
reflect the author’s attitude towards the content of
the text. Typical hedge cues fall into the following
categories:
</bodyText>
<listItem confidence="0.999404375">
• auxiliaries: may, might, can, would, should,
could, etc.
• verbs of hedging or verbs with speculative
content: suggest, question, presume, suspect,
indicate, suppose, seem, appear, favor, etc.
• adjectives or adverbs: probable, likely, possi-
ble, unsure, etc.
• conjunctions: or, and/or, either ... or, etc.
</listItem>
<bodyText confidence="0.9974155625">
However, there are some cases where a hedge is
expressed via a phrase rather than a single word.
Complex keywords are phrases that express un-
certainty together, but not on their own (either the
semantic interpretation or the hedging strength of
its subcomponents are significantly different from
those of the whole phrase). An instance of a com-
plex keyword can be seen in the following sen-
tence:
Mild bladder wall thickening {raises
the question of cystitis}.
The expression raises the question of may be sub-
stituted by suggests and neither the verb raises nor
the noun question convey speculative meaning on
their own. However, the whole phrase is specula-
tive therefore it is marked as a hedge cue.
</bodyText>
<page confidence="0.988277">
2
</page>
<bodyText confidence="0.999995333333333">
During the annotation process, a min-max strat-
egy for the marking of keywords (min) and their
scope (max) was followed. On the one hand, when
marking the keywords, the minimal unit that ex-
presses hedging and determines the actual strength
of hedging was marked as a keyword. On the other
hand, when marking the scopes of speculative key-
words, the scope was extended to the largest syn-
tactic unit possible. That is, all constituents that
fell within the uncertain interpretation were in-
cluded in the scope. Our motivation here was that
in this way, if we simply disregard the marked text
span, the rest of the sentence can usually be used
for extracting factual information (if there is any).
For instance, in the example above, we can be sure
that the symptom mild bladder wall thickening is
exhibited by the patient but a diagnosis of cystitis
would be questionable.
The scope of a speculative element can be de-
termined on the basis of syntax. The scopes of
the BioScope corpus are regarded as consecutive
text spans and their annotation was based on con-
stituency grammar. The scope of verbs, auxil-
iaries, adjectives and adverbs usually starts right
with the keyword. In the case of verbal elements,
i.e. verbs and auxiliaries, it ends at the end of the
clause or sentence, thus all complements and ad-
juncts are included. The scope of attributive ad-
jectives generally extends to the following noun
phrase, whereas the scope of predicative adjec-
tives includes the whole sentence. Sentential ad-
verbs have a scope over the entire sentence, while
the scope of other adverbs usually ends at the end
of the clause or sentence. Conjunctions generally
have a scope over the syntactic unit whose mem-
bers they coordinate. Some linguistic phenomena
(e.g. passive voice or raising) can change scope
boundaries in the sentence, thus they were given
special attention during the annotation phase.
</bodyText>
<subsectionHeader confidence="0.999714">
3.2 Wikipedia Weasels
</subsectionHeader>
<bodyText confidence="0.999942875">
The chief editors of Wikipedia have drawn the at-
tention of the public to uncertainty issues they call
weasel1. A word is considered to be a weasel
word if it creates an impression that something im-
portant has been said, but what is really commu-
nicated is vague, misleading, evasive or ambigu-
ous. Weasel words do not give a neutral account
of facts, rather, they offer an opinion without any
</bodyText>
<footnote confidence="0.987076">
1http://en.wikipedia.org/wiki/Weasel_
word
</footnote>
<bodyText confidence="0.911559857142857">
backup or source. The following sentence does
not specify the source of information, it is just the
vague term some people that refers to the holder of
this opinion:
Some people claim that this results in a
better taste than that of other diet colas
(most of which are sweetened with as-
partame alone).
Statements with weasel words usually evoke ques-
tions such as Who says that?, Whose opinion is
this? and How many people think so?.
Typical instances of weasels can be grouped in
the following way (we offer some examples as
well):
</bodyText>
<listItem confidence="0.994257">
• Adjectives and adverbs
</listItem>
<bodyText confidence="0.836280923076923">
– elements referring to uncertainty: prob-
able, likely, possible, unsure, often, pos-
sibly, allegedly, apparently, perhaps,
etc.
– elements denoting generalization:
widely, traditionally, generally, broadly-
accepted, widespread, etc.
– qualifiers and superlatives: global, su-
perior, excellent, immensely, legendary,
best, (one of the) largest, most promi-
nent, etc.
– elements expressing obviousness:
clearly, obviously, arguably, etc.
</bodyText>
<listItem confidence="0.877701941176471">
• Auxiliaries
– may, might, would, should, etc.
• Verbs
– verbs with speculative content and their
passive forms: suggest, question, pre-
sume, suspect, indicate, suppose, seem,
appear, favor, etc.
– passive forms with dummy subjects: It
is claimed that ... It has been men-
tioned ... It is known ...
– there is / there are constructions: There
is evidence/concern/indication that.. .
• Numerically vague expressions / quantifiers
– certain, numerous, many, most, some,
much, everyone, few, various, one group
of, etc. Experts say ... Some people
think ... More than 60% percent ...
</listItem>
<page confidence="0.935695">
3
</page>
<listItem confidence="0.97852975">
• Nouns
– speculation, proposal, consideration,
etc. Rumour has it that ... Common
sense insists that ...
</listItem>
<bodyText confidence="0.999988884615384">
However, the use of the above words or grammat-
ical devices does not necessarily entail their being
a weasel cue since their use may be justifiable in
their contexts.
As the main application goal of weasel detec-
tion is to highlight articles which should be im-
proved (by reformulating or adding factual is-
sues), we decided to annotate only weasel cues
in Wikipedia articles, but we did not mark their
scopes.
During the manual annotation process, the fol-
lowing cue marking principles were employed.
Complex verb phrases were annotated as weasel
cues since in some cases, both the passive con-
struction and the verb itself are responsible for the
weasel. In passive forms with dummy subjects and
there is / there are constructions, the weasel cue
included the grammatical subject (i.e. it and there)
as well. As for numerically vague expressions, the
noun phrase containing a quantifier was marked
as a weasel cue. If there was no quantifier (in the
case of a bare plural), the noun was annotated as
a weasel cue. Comparatives and superlatives were
annotated together with their article. Anaphoric
pronouns referring to a weasel word were also an-
notated as weasel cues.
</bodyText>
<sectionHeader confidence="0.996268" genericHeader="method">
4 Task Definitions
</sectionHeader>
<bodyText confidence="0.999976833333333">
Two uncertainty detection tasks (sentence clas-
sification and in-sentence hedge scope detec-
tion) in two domains (biological publications and
Wikipedia articles) with three types of submis-
sions (closed, cross and open) were given to the
participants of the CoNLL-2010 Shared Task.
</bodyText>
<subsectionHeader confidence="0.999892">
4.1 Detection of Uncertain Sentences
</subsectionHeader>
<bodyText confidence="0.999274714285714">
The aim of Task1 was to develop automatic proce-
dures for identifying sentences in texts which con-
tain unreliable or uncertain information. In par-
ticular, this task is a binary classification problem,
i.e. factual and uncertain sentences have to be dis-
tinguished.
As training and evaluation data
</bodyText>
<listItem confidence="0.9543812">
• Task1B: biological abstracts and full articles
(evaluation data contained only full articles)
from the BioScope corpus and
• Task1W: paragraphs from Wikipedia possi-
bly containing weasel information
</listItem>
<bodyText confidence="0.999605555555556">
were provided. The annotation of weasel/hedge
cues was carried out on the phrase level, and sen-
tences containing at least one cue were considered
as uncertain, while sentences with no cues were
considered as factual. The participating systems
had to submit a binary classification (certain vs.
uncertain) of the test sentences while marking cues
in the submissions was voluntary (but participants
were encouraged to do this).
</bodyText>
<subsectionHeader confidence="0.988516">
4.2 In-sentence Hedge Scope Resolution
</subsectionHeader>
<bodyText confidence="0.999985888888889">
For Task2, in-sentence scope resolvers had to be
developed. The training and evaluation data con-
sisted of biological scientific texts, in which in-
stances of speculative spans – that is, keywords
and their linguistic scope – were annotated manu-
ally. Submissions to Task2 were expected to auto-
matically annotate the cue phrases and the left and
right boundaries of their scopes (exactly one scope
must be assigned to a cue phrase).
</bodyText>
<subsectionHeader confidence="0.99864">
4.3 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999992384615385">
The evaluation for Task1 was carried out at the
sentence level, i.e. the cue annotations in the sen-
tence were not taken into account. The Fβ=1 mea-
sure (the harmonic mean of precision and recall)
of the uncertain class was employed as the chief
evaluation metric.
The Task2 systems were expected to mark cue-
and corresponding scope begin/end tags linked to-
gether by using some unique IDs. A scope-level
Fβ=1 measure was used as the chief evaluation
metric where true positives were scopes which ex-
actly matched the gold standard cue phrases and
gold standard scope boundaries assigned to the cue
word. That is, correct scope boundaries with in-
correct cue annotation and correct cue words with
bad scope boundaries were both treated as errors.
This scope-level metric is very strict. For in-
stance, the requirement of the precise match of the
cue phrase is questionable as – from an application
point of view – the goal is to find uncertain text
spans and the evidence for this is not so impor-
tant. However, the annotation of cues in datasets
is essential for training scope detectors since lo-
cating the cues usually precedes the identification
of their scope. Hence we decided to incorporate
cue matches into the evaluation metric.
</bodyText>
<page confidence="0.98979">
4
</page>
<bodyText confidence="0.999993125">
Another questionable issue is the strict bound-
ary matching requirement. For example, includ-
ing or excluding punctuations, citations or some
bracketed expressions, like (see Figure 1) from
a scope is not crucial for an otherwise accurate
scope detector. On the other hand, the list of
such ignorable phenomena is arguable, especially
across domains. Thus, we considered the strict
boundary matching to be a straightforward and un-
ambiguous evaluation criterion. Minor issues like
those mentioned above could be handled by sim-
ple post-processing rules. In conclusion we think
that the uncertainty detection community may find
more flexible evaluation criteria in the future but
the strict scope-level metric is definitely a good
starting point for evaluation.
</bodyText>
<subsectionHeader confidence="0.994081">
4.4 Closed and Open Challenges
</subsectionHeader>
<bodyText confidence="0.999953">
Participants were invited to submit results in dif-
ferent configurations, where systems were allowed
to exploit different kinds of annotated resources.
The three possible submission categories were:
</bodyText>
<listItem confidence="0.888597458333333">
• Closed, where only the labeled and unla-
beled data provided for the shared task were
allowed, separately for each domain (i.e.
biomedical train data for biomedical test set
and Wikipedia train data for Wikipedia test
set). No further manually crafted resources
of uncertainty information (i.e. lists, anno-
tated data, etc.) could be used in any domain.
On the other hand, tools exploiting the man-
ual annotation of linguistic phenomena not
related to uncertainty (such as POS taggers
and parsers trained on labeled corpora) were
allowed.
• Cross-domain was the same as the closed one
but all data provided for the shared task were
allowed for both domains (i.e. Wikipedia
train data for the biomedical test set, the
biomedical train data for Wikipedia test set
or a union of Wikipedia and biomedical train
data for both test sets).
• Open, where any data and/or any additional
manually created information and resource
(which may be related to uncertainty) were
allowed for both domains.
</listItem>
<bodyText confidence="0.9998592">
The motivation behind the cross-domain and the
open challenges was that in this way, we could
assess whether adding extra (i.e. not domain-
specific) information to the systems can contribute
to the overall performance.
</bodyText>
<sectionHeader confidence="0.995501" genericHeader="method">
5 Datasets
</sectionHeader>
<bodyText confidence="0.999949214285714">
Training and evaluation corpora were annotated
manually for hedge/weasel cues and their scope
by two independent linguist annotators. Any dif-
ferences between the two annotations were later
resolved by the chief annotator, who was also re-
sponsible for creating the annotation guidelines
and training the two annotators. The datasets
are freely available2 for further benchmark experi-
ments at http://www.inf.u-szeged.hu/
rgai/conll2010st.
Since uncertainty cues play an important role
in detecting sentences containing uncertainty, they
are tagged in the Task1 datasets as well to enhance
training and evaluation of systems.
</bodyText>
<subsectionHeader confidence="0.990247">
5.1 Biological Publications
</subsectionHeader>
<bodyText confidence="0.999937035714286">
The biological training dataset consisted of the bi-
ological part of the BioScope corpus (Vincze et al.,
2008), hence it included abstracts from the GE-
NIA corpus, 5 full articles from the functional ge-
nomics literature (related to the fruit fly) and 4 ar-
ticles from the open access BMC Bioinformatics
website. The automatic segmentation of the doc-
uments was corrected manually and the sentences
(14541 in number) were annotated manually for
hedge cues and their scopes.
The evaluation dataset was based on 15 biomed-
ical articles downloaded from the publicly avail-
able PubMedCentral database, including 5 ran-
dom articles taken from the BMC Bioinformat-
ics journal in October 2009, 5 random articles to
which the drosophila MeSH term was assigned
and 5 random articles having the MeSH terms
human, blood cells and transcription factor (the
same terms which were used to create the Genia
corpus). These latter ten articles were also pub-
lished in 2009. The aim of this article selection
procedure was to have a theme that was close to
the training corpus. The evaluation set contained
5003 sentences, out of which 790 were uncertain.
These texts were manually annotated for hedge
cues and their scope. To annotate the training and
the evaluation datasets, the same annotation prin-
ciples were applied.
</bodyText>
<footnote confidence="0.9953045">
2under the Creative Commons Attribute Share Alike li-
cense
</footnote>
<page confidence="0.991088">
5
</page>
<bodyText confidence="0.9996278">
For both Task1 and Task2, the same dataset was
provided, the difference being that for Task1, only
hedge cues and sentence-level uncertainty were
given, however, for Task2, hedge cues and their
scope were marked in the text.
</bodyText>
<subsectionHeader confidence="0.996472">
5.2 Wikipedia Datasets
</subsectionHeader>
<bodyText confidence="0.999930655172414">
2186 paragraphs collected from Wikipedia
archives were also offered as Task1 training
data (11111 sentences containing 2484 uncertain
ones). The evaluation dataset contained 2346
Wikipedia paragraphs with 9634 sentences, out of
which 2234 were uncertain.
For the selection of the Wikipedia paragraphs
used to construct the training and evaluation
datasets, we exploited the weasel tags added by
the editors of the encyclopedia (marking unsup-
ported opinions or expressions of a non-neutral
point of view). Each paragraph containing weasel
tags (5874 different ones) was extracted from the
history dump of English Wikipedia. First, 438 ran-
domly selected paragraphs were manually anno-
tated from this pool then the most frequent cue
phrases were collected. Later on, two other sets
of Wikipedia paragraphs were gathered on the ba-
sis of whether they contained such cue phrases or
not. The aim of this sampling procedure was to
provide large enough training and evaluation sam-
ples containing weasel words and also occurrences
of typical weasel words in non-weasel contexts.
Each sentence was annotated manually for
weasel cues. Sentences were treated as uncer-
tain if they contained at least one weasel cue, i.e.
the scope of weasel words was the entire sentence
(which is supposed to be rewritten by Wikipedia
editors).
</bodyText>
<subsectionHeader confidence="0.997981">
5.3 Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.999939444444444">
Unannotated but pre-processed full biological arti-
cles (150 articles from the publicly available Pub-
MedCentral database) and 1 million paragraphs
from Wikipedia were offered to the participants as
well. These datasets did not contain any manual
annotation for uncertainty, but their usage permit-
ted data sampling from a large pool of in-domain
texts without time-wasting pre-processing tasks
(cleaning and sentence splitting).
</bodyText>
<subsectionHeader confidence="0.984905">
5.4 Data Format
</subsectionHeader>
<bodyText confidence="0.999979333333334">
Both training and evaluation data were released
in a custom XML format. For each task, a sep-
arate XML file was made available containing the
whole document set for the given task. Evaluation
datasets were available in the same format as train-
ing data without any sentence-level certainty, cue
or scope annotations.
The XML format enabled us to provide more
detailed information about the documents such as
segment boundaries and types (e.g. section titles,
figure captions) and it is the straightforward for-
mat to represent nested scopes. Nested scopes
have overlapping text spans which may contain
cues for multiple scopes (there were 1058 occur-
rences in the training and evaluation datasets to-
gether). The XML format utilizes id-references
to determine the scope of a given cue. Nested
constructions are rather complicated to represent
in the standard IOB format, moreover, we did not
want to enforce a uniform tokenization.
To support the processing of the data files,
reader and writer software modules were devel-
oped and offered to the participants for the uCom-
pare (Kano et al., 2009) framework. uCompare
provides a universal interface (UIMA) and several
text mining and natural language processing tools
(tokenizers, POS taggers, syntactic parsers, etc.)
for general and biological domains. In this way
participants could configure and execute a flexible
chain of analyzing tools even with a graphical UI.
</bodyText>
<sectionHeader confidence="0.988697" genericHeader="evaluation">
6 Submissions and Results
</sectionHeader>
<bodyText confidence="0.999683285714286">
Participants uploaded their results through the
shared task website, and the official evaluation was
performed centrally. After the evaluation period,
the results were published for the participants on
the Web. A total of 23 teams participated in the
shared task. 22, 16 and 13 teams submitted output
for Task1B, Task1W and Task2, respectively.
</bodyText>
<subsectionHeader confidence="0.653045">
6.1 Results
</subsectionHeader>
<bodyText confidence="0.993046416666667">
Tables 1, 2 and 3 contain the results of the submit-
ted systems for Task1 and Task2. The last name
of the first author of the system description pa-
per (published in these proceedings) is used here
as a system name3. The last column contains the
type of submission. The system of Kilicoglu and
Bergler (2010) is the only open submission. They
adapted their system introduced in Kilicoglu and
Bergler (2008) to the datasets of the shared task.
Regarding cross submissions, Zhao et al. (2010)
and Ji et al. (2010) managed to achieve a no-
ticeable improvement by exploiting cross-domain
</bodyText>
<footnote confidence="0.570162">
3 ¨Ozg¨ur did not publish a description of her system.
</footnote>
<page confidence="0.995385">
6
</page>
<table confidence="0.999941333333333">
Name P / R / F type
Georgescul 72.0 / 51.7 / 60.2 C
Ji 62.7 / 55.3 / 58.7 X
Chen 68.0 / 49.7 / 57.4 C
Morante 80.6 / 44.5 / 57.3 C
Zhang 76.6 / 44.4 / 56.2 C
Zheng 76.3 / 43.6 / 55.5 C
T¨ackstr¨om 78.3 / 42.8 / 55.4 C
Mamani S´anchez 68.3 / 46.2 / 55.1 C
Tang 82.3 / 41.4 / 55.0 C
Kilicoglu 67.9 / 46.0 / 54.9 O
Tjong Kim Sang 74.0 / 43.0 / 54.4 C
Clausen 75.1 / 42.0 / 53.9 C
¨Ozg¨ur 59.4 / 47.9 / 53.1 C
Zhou 85.3 / 36.5 / 51.1 C
Li 88.4 / 31.9 / 46.9 C
Prabhakaran 88.0 / 28.4 / 43.0 C
Ji 94.2 / 6.6 / 12.3 C
</table>
<tableCaption confidence="0.977932">
Table 1: Task1 Wikipedia results (type E
{Closed(C), Cross(X), Open(O)J).
</tableCaption>
<bodyText confidence="0.92955125">
data. Zhao et al. (2010) extended the biological
cue word dictionary of their system – using it as
a feature for classification – by the frequent cues
of the Wikipedia dataset, while Ji et al. (2010)
used the union of the two datasets for training
(they have reported an improvement from 47.0 to
58.7 on the Wikipedia evaluation set after a post-
challenge bugfix).
</bodyText>
<table confidence="0.9997536">
Name P / R / F type
Morante 59.6 / 55.2 / 57.3 C
Rei 56.7 / 54.6 / 55.6 C
Velldal 56.7 / 54.0 / 55.3 C
Kilicoglu 62.5 / 49.5 / 55.2 O
Li 57.4 / 47.9 / 52.2 C
Zhou 45.6 / 43.9 / 44.7 O
Zhou 45.3 / 43.6 / 44.4 C
Zhang 46.0 / 42.9 / 44.4 C
Fernandes 46.0 / 38.0 / 41.6 C
Vlachos 41.2 / 35.9 / 38.4 C
Zhao 34.8 / 41.0 / 37.7 C
Tang 34.5 / 31.8 / 33.1 C
Ji 21.9 / 17.2 / 19.3 C
T¨ackstr¨om 2.3 / 2.0 / 2.1 C
</table>
<tableCaption confidence="0.9441865">
Table 2: Task2 results (type E {Closed(C),
Open(O)J).
</tableCaption>
<table confidence="0.987711076923077">
Each Task2 and Task1W system achieved a
Name P / R / F type
Tang 85.0 / 87.7 / 86.4 C
Zhou 86.5 / 85.1 / 85.8 C
Li 90.4 / 81.0 / 85.4 C
Velldal 85.5 / 84.9 / 85.2 C
Vlachos 85.5 / 84.9 / 85.2 C
T¨ackstr¨om 87.1 / 83.4 / 85.2 C
Shimizu 88.1 / 82.3 / 85.1 C
Zhao 83.4 / 84.8 / 84.1 X
¨Ozg¨ur 77.8 / 91.3 / 84.0 C
Rei 83.8 / 84.2 / 84.0 C
Zhang 82.6 / 84.7 / 83.6 C
Kilicoglu 92.1 / 74.9 / 82.6 O
Morante 80.5 / 83.3 / 81.9 X
Morante 81.1 / 82.3 / 81.7 C
Zheng 73.3 / 90.8 / 81.1 C
Tjong Kim Sang 74.3 / 87.1 / 80.2 C
Clausen 79.3 / 80.6 / 80.0 C
Szidarovszky 70.3 / 91.0 / 79.3 C
Georgescul 69.1 / 91.0 / 78.5 C
Zhao 71.0 / 86.6 / 78.0 C
Ji 79.4 / 76.3 / 77.9 C
Chen 74.9 / 79.1 / 76.9 C
Fernandes 70.1 / 71.1 / 70.6 C
Prabhakaran 67.5 / 19.5 / 30.3 X
</table>
<tableCaption confidence="0.968993">
Table 3: Task1 biological results (type E
{Closed(C), Cross(X), Open(O)J).
</tableCaption>
<bodyText confidence="0.9995205">
higher precision than recall. There may be two
reasons for this. The systems may have applied
only reliable patterns, or patterns occurring in the
evaluation set may be imperfectly covered by the
training datasets. The most intense participation
was on Task1B. Here, participants applied vari-
ous precision/recall trade-off strategies. For in-
stance, Tang et al. (2010) achieved a balanced pre-
cision/recall configuration, while Li et al. (2010)
achieved third place thanks to their superior preci-
sion.
Tables 4 and 5 show the cue-level performances,
i.e. the F-measure of cue phrase matching where
true positives were strict matches. Note that it was
optional to submit cue annotations for Task1 (if
participants submitted systems for both Task2 and
Task1B with cue tagging, only the better score of
the two was considered).
It is interesting to see that Morante et al. (2010)
who obtained the best results on Task2 achieved
a medium-ranked F-measure on the cue-level (e.g.
their result on the cue-level is lower by 4% com-
</bodyText>
<page confidence="0.998777">
7
</page>
<bodyText confidence="0.9962954">
pared to Zhou et al. (2010), while on the scope-
level the difference is 13% in the reverse direc-
tion), which indicates that the real strength of the
system of Morante et al. (2010) is the accurate de-
tection of scope boundaries.
</bodyText>
<table confidence="0.9994982">
Name P / R / F
Tang 63.0 / 25.7 / 36.5
Li 76.1 / 21.6 / 33.7
¨Ozg¨ur 28.9 / 14.7 / 19.5
Morante 24.6 / 7.3 / 11.3
</table>
<tableCaption confidence="0.969724">
Table 4: Wikipedia cue-level results.
</tableCaption>
<table confidence="0.999955">
Name P / R / F type
Tang 81.7 / 81.0 / 81.3 C
Zhou 83.1 / 78.8 / 80.9 C
Li 87.4 / 73.4 / 79.8 C
Rei 81.4 / 77.4 / 79.3 C
Velldal 81.2 / 76.3 / 78.7 C
Zhang 82.1 / 75.3 / 78.5 C
Ji 78.7 / 76.2 / 77.4 C
Morante 78.8 / 74.7 / 76.7 C
Kilicoglu 86.5 / 67.7 / 76.0 O
Vlachos 82.0 / 70.6 / 75.9 C
Zhao 76.7 / 73.9 / 75.3 X
Fernandes 79.2 / 64.7 / 71.2 C
Zhao 63.7 / 74.1 / 68.5 C
T¨ackstr¨om 66.9 / 58.6 / 62.5 C
¨Ozg¨ur 49.1 / 57.8 / 53.1 C
</table>
<tableCaption confidence="0.9796105">
Table 5: Biological cue-level results (type ∈
{Closed(C), Cross(X), Open(O)}).
</tableCaption>
<subsectionHeader confidence="0.996889">
6.2 Approaches
</subsectionHeader>
<bodyText confidence="0.999992328358209">
The approaches to Task1 fall into two major cat-
egories. There were six systems which handled
the task as a classical sentence classification prob-
lem and employed essentially a bag-of-words fea-
ture representation (they are marked as BoW in
Table 6). The remaining teams focused on the
cue phrases and sought to classify every token if
it was a part of a cue phrase, then a sentence was
predicted as uncertain if it contained at least one
recognized cue phrase. Five systems followed a
pure token classification approach (TC) for cue de-
tection while others used sequential labeling tech-
niques (usually Conditional Random Fields) to
identify cue phrases in sentences (SL).
The feature set employed in Task1 systems typ-
ically consisted of the wordform, its lemma or
stem, POS and chunk codes and about the half of
the participants constructed features from the de-
pendency and/or constituent parse tree of the sen-
tences as well (see Table 6 for details).
It is interesting to see that the top ranked sys-
tems of Task1B followed a sequence labeling ap-
proach, while the best systems on Task1W applied
a bag-of-words sentence classification. This may
be due to the fact that biological sentences have
relatively simple patterns. Thus the context of the
cue words (token classification-based approaches
used features derived from a window of the token
in question, thus, they exploited the relationship
among the tokens and their contexts) can be uti-
lized while Wikipedia weasels have a diverse na-
ture. Another observation is that the top systems
in both Task1B and Task1W are the ones which
did not derive features from syntactic parsing.
Each Task2 system was built upon a Task1 sys-
tem, i.e. they attempted to recognize the scopes
for the predicted cue phrases (however, Zhang et
al. (2010) have argued that the objective functions
of Task1 and Task2 cue detection problems are
different because of sentences containing multiple
hedge spans).
Most systems regarded multiple cues in a sen-
tence to be independent from each other and
formed different classification instances from
them. There were three systems which incor-
porated information about other hedge cues (e.g.
their distance) of the sentence into the feature
space and Zhang et al. (2010) constructed a cas-
cade system which utilized directly the predicted
scopes (it processes cue phrases from left to right)
during predicting other scopes in the same sen-
tence.
The identification of the scope for a certain cue
was typically carried out by classifying each to-
ken in the sentence. Task2 systems differ in the
number of class labels used as target and in the
machine learning approaches applied. Most sys-
tems – following Morante and Daelemans (2009)
– used three class labels (F)IRST, (L)AST and
NONE. Two participants used four classes by
adding (I)NSIDE, while three systems followed
a binary classification approach (SCOPE versus
NONSCOPE). The systems typically included a
post-processing procedure to force scopes to be
continuous and to include the cue phrase in ques-
tion. The machine learning methods applied can
be again categorized into sequence labeling (SL)
</bodyText>
<page confidence="0.997557">
8
</page>
<figureCaption confidence="0.642618166666667">
Table 6: System architectures overview for Task1. Approaches: sequence labeling (SL), token classification (TC), bag-of-words model (BoW); Machine learners:
Entropy Guided Transformation Learning (ETL), Averaged Perceptron (AP), k-nearest neighbour (KNN); Feature selection: gathering phrases from the training
corpus using statistical thresholds (statistical); Features: orthographical information about the token (ortho), lemma or stem of the token (stem), Part-of-Speech
codes (POS), syntactic chunk information (chunk), dependency parsing (dep), position inside the document or section information (docpos)
and token classification (TC) approaches (see Table 7). The feature sets used here are the same as for Task1, extended by several features describing the
relationship between the cue phrase and the token in question mostly by describing the dependency path between them.
</figureCaption>
<table confidence="0.98092591089109">
NEs, unlabeled data
Constituent Parsing
hedge cue distance
sentencelength
sentencelength
external dict
LevinClass
WordNet
other
NEs
docpart
+
+
+
+
+
features employed
dep
+
+
+
+
+
+
+
+
+
+
chunk
+
+
+
+
+
+
+
+
+
+
+
POS
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
lemma/stem
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
ortho
+
+
+
dict
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
</table>
<figure confidence="0.987670944444444">
greedy fwd
greedy fwd
greedy fwd
greedy fwd
exhaustive
statistical
statistical
statistical
statistical
statistical
statistical
selection
manual
manual
manual
feature
GA
CRF+feature combination
Bayes Point Machines
ModAvgPerceptron
SVM+paramtuning
Bayesian LogReg
CRF,SVMHMM
SVMTreeKernel
SVM+postproc
CRF+postproc
CRF,MaxEnt
Naive Bayes
machine
MaxEnt
MaxEnt
MaxEnt
manual
learner
KNN
SVM
CRF
CRF
CRF
CRF
CRF
ETL
approach
BoW
BoW
BoW
BoW
BoW
TC
TC
TC
TC
TC
TC
SL
SL
SL
SL
SL
SL
SL
SL
SL
SL
SL
SL
Mamani S´anchez
Tjong Kim Sang
Morante (wiki)
Morante (bio)
Szidarovszky
Prabhakaran
Georgescul
T¨ackstr¨om
Fernandes
Kilicoglu
Shimizu
Vlachos
Clausen
NAME
Velldal
Zhang
Zheng
Chen
Zhou
Zhao
Tang
Rei
Li
Ji
</figure>
<page confidence="0.956766">
9
</page>
<table confidence="0.997976857142857">
NAME approach scope ML postproc tree dep multihedge
Fernandes TC FL ETL
Ji TC I AP +
Kilicoglu HC manual + + +
Li SL FL CRF, SVMHMM + + +
Morante TC FL KNN + +
Rei SL FIL manual+CRF + +
T¨ackstr¨om TC FI SVM +
Tang SL FL CRF + + +
Velldal HC manual +
Vlachos TC I Bayesian MaxEnt + +
Zhang SL FIL CRF + +
Zhao SL FL CRF +
Zhou SL FL CRF + +
</table>
<tableCaption confidence="0.975931">
Table 7: System architectures overview for Task2. Approaches: sequence labeling (SL), token clas-
</tableCaption>
<bodyText confidence="0.9706971">
sification (TC), hand-crafted rules (HC); Machine learners: Entropy Guided Transformation Learning
(ETL), Averaged Perceptron (AP), k-nearest neighbour (KNN); The way of identifying scopes: predict-
ing first/last tokens (FL), first/inside/last tokens (FIL), just inside tokens (I); Multiple Hedges: the system
applied a mechanism for handling multiple hedges inside a sentence
and token classification (TC) approaches (see Ta-
ble 7). The feature sets used here are the same
as for Task1, extended by several features describ-
ing the relationship between the cue phrase and the
token in question mostly by describing the depen-
dency path between them.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999881958333333">
The CoNLL-2010 Shared Task introduced the
novel task of uncertainty detection. The challenge
consisted of a sentence identification task on un-
certainty (Task1) and an in-sentence hedge scope
detection task (Task2). In the latter task the goal
of automatic systems was to recognize speculative
text spans inside sentences.
The relatively high number of participants in-
dicates that the problem is rather interesting for
the Natural Language Processing community. We
think that this is due to the practical importance
of the task for (principally biomedical) applica-
tions and because it addresses several open re-
search questions. Although several approaches
were introduced by the participants of the shared
task and we believe that the ideas described in
this proceedings can serve as an excellent starting
point for the development of an uncertainty de-
tector, there is a lot of room for improving such
systems. The manually annotated datasets and
software tools developed for the shared task may
act as benchmarks for these future experiments
(they are freely available at http://www.inf.
u-szeged.hu/rgai/conll2010st).
</bodyText>
<sectionHeader confidence="0.994727" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997314">
The authors would like to thank Joakim Nivre
and Llu´ıs M´arquez for their useful suggestions,
comments and help during the organisation of the
shared task.
This work was supported in part by the
National Office for Research and Technol-
ogy (NKTH, http://www.nkth.gov.hu/)
of the Hungarian government within the frame-
work of the projects TEXTREND, BELAMI and
MASZEKER.
</bodyText>
<sectionHeader confidence="0.998588" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999266133333333">
Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,
Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko
Ohe. 2009. TEXT2TABLE: Medical Text Summa-
rization System Based on Named Entity Recogni-
tion and Modality Identification. In Proceedings of
the BioNLP 2009 Workshop, pages 185–192, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Wendy W. Chapman, David Chu, and John N. Dowl-
ing. 2007. ConText: An Algorithm for Identifying
Contextual Features from Clinical Text. In Proceed-
ings of the ACL Workshop on BioNLP 2007, pages
81–88.
Mike Conway, Son Doan, and Nigel Collier. 2009. Us-
ing Hedges to Enhance a Disease Outbreak Report
</reference>
<page confidence="0.94928">
10
</page>
<reference confidence="0.999707234234235">
Text Mining System. In Proceedings of the BioNLP
2009 Workshop, pages 142–143, Boulder, Colorado,
June. Association for Computational Linguistics.
Carol Friedman, Philip O. Alderson, John H. M.
Austin, James J. Cimino, and Stephen B. Johnson.
1994. A General Natural-language Text Processor
for Clinical Radiology. Journal of the American
Medical Informatics Association, 1(2):161–174.
Viola Ganter and Michael Strube. 2009. Finding
Hedges by Chasing Weasels: Hedge Detection Us-
ing Wikipedia Tags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 173–176, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Feng Ji, Xipeng Qiu, and Xuanjing Huang. 2010. De-
tecting Hedge Cues and their Scopes with Average
Perceptron. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing (CoNLL-2010): Shared Task, pages 139–146,
Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Yoshinobu Kano, William A. Baumgartner, Luke
McCrohon, Sophia Ananiadou, Kevin B. Cohen,
Lawrence Hunter, and Jun’ichi Tsujii. 2009. U-
Compare: Share and Compare Text Mining Tools
with UIMA. Bioinformatics, 25(15):1997–1998,
August.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing Speculative Language in Biomedical Research
Articles: A Linguistically Motivated Perspective.
In Proceedings of the Workshop on Current Trends
in Biomedical Natural Language Processing, pages
46–53, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2009. Syn-
tactic Dependency Based Heuristics for Biological
Event Extraction. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 119–127, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2010. A High-
Precision Approach to Detecting Hedges and Their
Scopes. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 103–110, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview
of BioNLP’09 Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop Com-
panion Volume for Shared Task, pages 1–9, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
George Lakoff. 1972. Linguistics and natural logic.
In The Semantics of Natural Language, pages 545–
665, Dordrecht. Reidel.
Xinxin Li, Jianping Shen, Xiang Gao, and Xuan
Wang. 2010. Exploiting Rich Features for Detect-
ing Hedges and Their Scope. In Proceedings of the
Fourteenth Conference on Computational Natural
Language Learning (CoNLL-2010): Shared Task,
pages 36–41, Uppsala, Sweden, July. Association
for Computational Linguistics.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The Language of Bioscience: Facts, Spec-
ulations, and Statements in Between. In Proceed-
ings of the HLT-NAACL 2004 Workshop: Biolink
2004, Linking Biological Literature, Ontologies and
Databases, pages 17–24.
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific
Literature. In Proceedings of the ACL, pages 992–
999, Prague, Czech Republic, June.
Roser Morante and Walter Daelemans. 2009. Learning
the Scope of Hedge Cues in Biomedical Texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28–36, Boulder, Colorado, June. Association for
Computational Linguistics.
Roser Morante, Vincent Van Asch, and Walter Daele-
mans. 2010. Memory-based Resolution of In-
sentence Scopes of Hedge Cues. In Proceedings of
the Fourteenth Conference on Computational Nat-
ural Language Learning (CoNLL-2010): Shared
Task, pages 48–55, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Arzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. De-
tecting Speculations and their Scopes in Scientific
Text. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1398–1407, Singapore, August. Associ-
ation for Computational Linguistics.
Gy¨orgy Szarvas. 2008. Hedge Classification in
Biomedical Texts with a Weakly Supervised Selec-
tion of Keywords. In Proceedings of ACL-08: HLT,
pages 281–289, Columbus, Ohio, June. Association
for Computational Linguistics.
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A Cascade Method for De-
tecting Hedges and their Scope in Natural Language
Text. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 25–29, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Sofie Van Landeghem, Yvan Saeys, Bernard De Baets,
and Yves Van de Peer. 2009. Analyzing Text in
Search of Bio-molecular Events: A High-precision
Machine Learning Framework. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
</reference>
<page confidence="0.990141">
11
</page>
<reference confidence="0.999468466666667">
Shared Task, pages 128–136, Boulder, Colorado,
June. Association for Computational Linguistics.
Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas,
Gy¨orgy M´ora, and J´anos Csirik. 2008. The Bio-
Scope Corpus: Biomedical Texts Annotated for Un-
certainty, Negation and their Scopes. BMC Bioin-
formatics, 9(Suppl 11):S9.
Shaodian Zhang, Hai Zhao, Guodong Zhou, and Bao-
liang Lu. 2010. Hedge Detection and Scope Find-
ing by Sequence Labeling with Procedural Feature
Selection. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 70–77, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Qi Zhao, Chengjie Sun, Bingquan Liu, and Yong
Cheng. 2010. Learning to Detect Hedges and their
Scope Using CRF. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning (CoNLL-2010): Shared Task, pages 64–
69, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Huiwei Zhou, Xiaoyan Li, Degen Huang, Zezhong Li,
and Yuansheng Yang. 2010. Exploiting Multi-
Features to Detect Hedges and Their Scope in
Biomedical Texts. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning (CoNLL-2010): Shared Task, pages 56–
63, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.998455">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.422034">
<title confidence="0.982571">The CoNLL-2010 Shared Task: Learning to Detect Hedges and Scope in Natural Language Text</title>
<author confidence="0.997337">Veronika Gy¨orgy J´anos Gy¨orgy</author>
<note confidence="0.655307333333333">1University of Szeged, Department of Informatics 2Hungarian Academy of Sciences, Research Group on Artificial Intelligence 3Technische Universit¨at Darmstadt, Ubiquitous Knowledge Processing</note>
<email confidence="0.891187">szarvas@tk.informatik.tu-darmstadt.de</email>
<abstract confidence="0.997691941176471">The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts. The motivation behind this task was that distinguishing factual and uncertain information in texts is of essential importance in information extraction. This paper provides a general overview of the shared task, including the annotation protocols of the training and evaluation datasets, the exact task definitions, the evaluation metrics employed and the overall results. The paper concludes with an analysis of the prominent approaches and an overview of the systems submitted to the shared task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eiji Aramaki</author>
<author>Yasuhide Miura</author>
</authors>
<title>Masatsugu Tonoike, Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko Ohe.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop,</booktitle>
<pages>185--192</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<marker>Aramaki, Miura, 2009</marker>
<rawString>Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike, Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko Ohe. 2009. TEXT2TABLE: Medical Text Summarization System Based on Named Entity Recognition and Modality Identification. In Proceedings of the BioNLP 2009 Workshop, pages 185–192, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy W Chapman</author>
<author>David Chu</author>
<author>John N Dowling</author>
</authors>
<title>ConText: An Algorithm for Identifying Contextual Features from Clinical Text.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on BioNLP</booktitle>
<pages>81--88</pages>
<contexts>
<context position="4541" citStr="Chapman et al., 2007" startWordPosition="669" endWordPosition="672">the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Related Work The term hedging was originally introduced by Lakoff (1972). However, hedge detection has received considerable interest just recently in the NLP community. Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Kilicoglu and Bergler (2008) proposed a linguistically motiva</context>
</contexts>
<marker>Chapman, Chu, Dowling, 2007</marker>
<rawString>Wendy W. Chapman, David Chu, and John N. Dowling. 2007. ConText: An Algorithm for Identifying Contextual Features from Clinical Text. In Proceedings of the ACL Workshop on BioNLP 2007, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Conway</author>
<author>Son Doan</author>
<author>Nigel Collier</author>
</authors>
<title>Using Hedges to Enhance a Disease Outbreak Report Text Mining System.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop,</booktitle>
<pages>142--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="4585" citStr="Conway et al., 2009" startWordPosition="678" endWordPosition="681">atural Language Learning: Shared Task, pages 1–12, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Related Work The term hedging was originally introduced by Lakoff (1972). However, hedge detection has received considerable interest just recently in the NLP community. Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information </context>
</contexts>
<marker>Conway, Doan, Collier, 2009</marker>
<rawString>Mike Conway, Son Doan, and Nigel Collier. 2009. Using Hedges to Enhance a Disease Outbreak Report Text Mining System. In Proceedings of the BioNLP 2009 Workshop, pages 142–143, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Friedman</author>
<author>Philip O Alderson</author>
<author>John H M Austin</author>
<author>James J Cimino</author>
<author>Stephen B Johnson</author>
</authors>
<title>A General Natural-language Text Processor for Clinical Radiology.</title>
<date>1994</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="4519" citStr="Friedman et al., 1994" startWordPosition="665" endWordPosition="668">word. 1 Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Related Work The term hedging was originally introduced by Lakoff (1972). However, hedge detection has received considerable interest just recently in the NLP community. Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Kilicoglu and Bergler (2008) proposed a</context>
</contexts>
<marker>Friedman, Alderson, Austin, Cimino, Johnson, 1994</marker>
<rawString>Carol Friedman, Philip O. Alderson, John H. M. Austin, James J. Cimino, and Stephen B. Johnson. 1994. A General Natural-language Text Processor for Clinical Radiology. Journal of the American Medical Informatics Association, 1(2):161–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viola Ganter</author>
<author>Michael Strube</author>
</authors>
<title>Finding Hedges by Chasing Weasels: Hedge Detection Using Wikipedia Tags and Shallow Linguistic Features.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>173--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="5260" citStr="Ganter and Strube (2009)" startWordPosition="778" endWordPosition="781">on exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information to semi-automatically refine a list of hedge cues. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing uncertainty based on Wikipedia weasel tags and syntactic patterns. The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope. It consists of clinical free-texts, biological texts from full papers and scientific abstracts. Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a scope detector following a supervised sequence labeling approach while ¨Ozg¨ur and Radev (2009) developed a rule-based system that exploits syntactic p</context>
</contexts>
<marker>Ganter, Strube, 2009</marker>
<rawString>Viola Ganter and Michael Strube. 2009. Finding Hedges by Chasing Weasels: Hedge Detection Using Wikipedia Tags and Shallow Linguistic Features. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 173–176, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Ji</author>
<author>Xipeng Qiu</author>
<author>Xuanjing Huang</author>
</authors>
<title>Detecting Hedge Cues and their Scopes with Average Perceptron.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>139--146</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="25150" citStr="Ji et al. (2010)" startWordPosition="3943" endWordPosition="3946"> shared task. 22, 16 and 13 teams submitted output for Task1B, Task1W and Task2, respectively. 6.1 Results Tables 1, 2 and 3 contain the results of the submitted systems for Task1 and Task2. The last name of the first author of the system description paper (published in these proceedings) is used here as a system name3. The last column contains the type of submission. The system of Kilicoglu and Bergler (2010) is the only open submission. They adapted their system introduced in Kilicoglu and Bergler (2008) to the datasets of the shared task. Regarding cross submissions, Zhao et al. (2010) and Ji et al. (2010) managed to achieve a noticeable improvement by exploiting cross-domain 3 ¨Ozg¨ur did not publish a description of her system. 6 Name P / R / F type Georgescul 72.0 / 51.7 / 60.2 C Ji 62.7 / 55.3 / 58.7 X Chen 68.0 / 49.7 / 57.4 C Morante 80.6 / 44.5 / 57.3 C Zhang 76.6 / 44.4 / 56.2 C Zheng 76.3 / 43.6 / 55.5 C T¨ackstr¨om 78.3 / 42.8 / 55.4 C Mamani S´anchez 68.3 / 46.2 / 55.1 C Tang 82.3 / 41.4 / 55.0 C Kilicoglu 67.9 / 46.0 / 54.9 O Tjong Kim Sang 74.0 / 43.0 / 54.4 C Clausen 75.1 / 42.0 / 53.9 C ¨Ozg¨ur 59.4 / 47.9 / 53.1 C Zhou 85.3 / 36.5 / 51.1 C Li 88.4 / 31.9 / 46.9 C Prabhakaran 88.</context>
</contexts>
<marker>Ji, Qiu, Huang, 2010</marker>
<rawString>Feng Ji, Xipeng Qiu, and Xuanjing Huang. 2010. Detecting Hedge Cues and their Scopes with Average Perceptron. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 139–146, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshinobu Kano</author>
<author>William A Baumgartner</author>
<author>Luke McCrohon</author>
<author>Sophia Ananiadou</author>
<author>Kevin B Cohen</author>
<author>Lawrence Hunter</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>UCompare: Share and Compare Text Mining Tools with UIMA.</title>
<date>2009</date>
<journal>Bioinformatics,</journal>
<volume>25</volume>
<issue>15</issue>
<contexts>
<context position="23935" citStr="Kano et al., 2009" startWordPosition="3748" endWordPosition="3751">) and it is the straightforward format to represent nested scopes. Nested scopes have overlapping text spans which may contain cues for multiple scopes (there were 1058 occurrences in the training and evaluation datasets together). The XML format utilizes id-references to determine the scope of a given cue. Nested constructions are rather complicated to represent in the standard IOB format, moreover, we did not want to enforce a uniform tokenization. To support the processing of the data files, reader and writer software modules were developed and offered to the participants for the uCompare (Kano et al., 2009) framework. uCompare provides a universal interface (UIMA) and several text mining and natural language processing tools (tokenizers, POS taggers, syntactic parsers, etc.) for general and biological domains. In this way participants could configure and execute a flexible chain of analyzing tools even with a graphical UI. 6 Submissions and Results Participants uploaded their results through the shared task website, and the official evaluation was performed centrally. After the evaluation period, the results were published for the participants on the Web. A total of 23 teams participated in the </context>
</contexts>
<marker>Kano, Baumgartner, McCrohon, Ananiadou, Cohen, Hunter, Tsujii, 2009</marker>
<rawString>Yoshinobu Kano, William A. Baumgartner, Luke McCrohon, Sophia Ananiadou, Kevin B. Cohen, Lawrence Hunter, and Jun’ichi Tsujii. 2009. UCompare: Share and Compare Text Mining Tools with UIMA. Bioinformatics, 25(15):1997–1998, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>Recognizing Speculative Language in Biomedical Research Articles: A Linguistically Motivated Perspective.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,</booktitle>
<pages>46--53</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="5108" citStr="Kilicoglu and Bergler (2008)" startWordPosition="755" endWordPosition="758">cted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information to semi-automatically refine a list of hedge cues. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing uncertainty based on Wikipedia weasel tags and syntactic patterns. The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope. It consists of clinical free-texts, biological texts from full papers and scientific abstracts. Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a</context>
<context position="25045" citStr="Kilicoglu and Bergler (2008)" startWordPosition="3924" endWordPosition="3927">valuation period, the results were published for the participants on the Web. A total of 23 teams participated in the shared task. 22, 16 and 13 teams submitted output for Task1B, Task1W and Task2, respectively. 6.1 Results Tables 1, 2 and 3 contain the results of the submitted systems for Task1 and Task2. The last name of the first author of the system description paper (published in these proceedings) is used here as a system name3. The last column contains the type of submission. The system of Kilicoglu and Bergler (2010) is the only open submission. They adapted their system introduced in Kilicoglu and Bergler (2008) to the datasets of the shared task. Regarding cross submissions, Zhao et al. (2010) and Ji et al. (2010) managed to achieve a noticeable improvement by exploiting cross-domain 3 ¨Ozg¨ur did not publish a description of her system. 6 Name P / R / F type Georgescul 72.0 / 51.7 / 60.2 C Ji 62.7 / 55.3 / 58.7 X Chen 68.0 / 49.7 / 57.4 C Morante 80.6 / 44.5 / 57.3 C Zhang 76.6 / 44.4 / 56.2 C Zheng 76.3 / 43.6 / 55.5 C T¨ackstr¨om 78.3 / 42.8 / 55.4 C Mamani S´anchez 68.3 / 46.2 / 55.1 C Tang 82.3 / 41.4 / 55.0 C Kilicoglu 67.9 / 46.0 / 54.9 O Tjong Kim Sang 74.0 / 43.0 / 54.4 C Clausen 75.1 / 42.</context>
</contexts>
<marker>Kilicoglu, Bergler, 2008</marker>
<rawString>Halil Kilicoglu and Sabine Bergler. 2008. Recognizing Speculative Language in Biomedical Research Articles: A Linguistically Motivated Perspective. In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing, pages 46–53, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>Syntactic Dependency Based Heuristics for Biological Event Extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task,</booktitle>
<pages>119--127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="6207" citStr="Kilicoglu and Bergler, 2009" startWordPosition="918" endWordPosition="921"> from full papers and scientific abstracts. Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a scope detector following a supervised sequence labeling approach while ¨Ozg¨ur and Radev (2009) developed a rule-based system that exploits syntactic patterns. Several related works have also been published within the framework of The BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009), where a separate subtask was dedicated to predicting whether the recognized biological events are under negation or speculation, based on the GENIA event corpus annotations (Kilicoglu and Bergler, 2009; Van Landeghem et al., 2009). 3 Uncertainty Annotation Guidelines The shared task addressed the detection of uncertainty in two domains. As uncertainty detection is extremely important for biomedical information extraction and most existing approaches have targeted such applications, participants were asked to develop systems for hedge detection in biological scientific articles. Uncertainty detection is also important, e.g. in encyclopedias, where the goal is to collect reliable world knowledge about real-world concepts and topics. For example, Wikipedia explicitly declares that statements r</context>
</contexts>
<marker>Kilicoglu, Bergler, 2009</marker>
<rawString>Halil Kilicoglu and Sabine Bergler. 2009. Syntactic Dependency Based Heuristics for Biological Event Extraction. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 119–127, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>A HighPrecision Approach to Detecting Hedges and Their Scopes.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>103--110</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="24947" citStr="Kilicoglu and Bergler (2010)" startWordPosition="3909" endWordPosition="3912"> through the shared task website, and the official evaluation was performed centrally. After the evaluation period, the results were published for the participants on the Web. A total of 23 teams participated in the shared task. 22, 16 and 13 teams submitted output for Task1B, Task1W and Task2, respectively. 6.1 Results Tables 1, 2 and 3 contain the results of the submitted systems for Task1 and Task2. The last name of the first author of the system description paper (published in these proceedings) is used here as a system name3. The last column contains the type of submission. The system of Kilicoglu and Bergler (2010) is the only open submission. They adapted their system introduced in Kilicoglu and Bergler (2008) to the datasets of the shared task. Regarding cross submissions, Zhao et al. (2010) and Ji et al. (2010) managed to achieve a noticeable improvement by exploiting cross-domain 3 ¨Ozg¨ur did not publish a description of her system. 6 Name P / R / F type Georgescul 72.0 / 51.7 / 60.2 C Ji 62.7 / 55.3 / 58.7 X Chen 68.0 / 49.7 / 57.4 C Morante 80.6 / 44.5 / 57.3 C Zhang 76.6 / 44.4 / 56.2 C Zheng 76.3 / 43.6 / 55.5 C T¨ackstr¨om 78.3 / 42.8 / 55.4 C Mamani S´anchez 68.3 / 46.2 / 55.1 C Tang 82.3 / 4</context>
</contexts>
<marker>Kilicoglu, Bergler, 2010</marker>
<rawString>Halil Kilicoglu and Sabine Bergler. 2010. A HighPrecision Approach to Detecting Hedges and Their Scopes. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 103–110, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Overview of BioNLP’09 Shared Task on Event Extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="6004" citStr="Kim et al., 2009" startWordPosition="888" endWordPosition="891">tic patterns. The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope. It consists of clinical free-texts, biological texts from full papers and scientific abstracts. Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a scope detector following a supervised sequence labeling approach while ¨Ozg¨ur and Radev (2009) developed a rule-based system that exploits syntactic patterns. Several related works have also been published within the framework of The BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009), where a separate subtask was dedicated to predicting whether the recognized biological events are under negation or speculation, based on the GENIA event corpus annotations (Kilicoglu and Bergler, 2009; Van Landeghem et al., 2009). 3 Uncertainty Annotation Guidelines The shared task addressed the detection of uncertainty in two domains. As uncertainty detection is extremely important for biomedical information extraction and most existing approaches have targeted such applications, participants were asked to develop systems for hedge detection in biological scientific articles. Uncertainty d</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun’ichi Tsujii. 2009. Overview of BioNLP’09 Shared Task on Event Extraction. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 1–9, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
</authors>
<title>Linguistics and natural logic.</title>
<date>1972</date>
<booktitle>In The Semantics of Natural Language,</booktitle>
<pages>545--665</pages>
<location>Dordrecht. Reidel.</location>
<contexts>
<context position="4173" citStr="Lakoff (1972)" startWordPosition="615" endWordPosition="616">rtainty should be recognized then either a sentence-level decision is made or the linguistic scope of the cue words has to be identified. The latter task falls within the scope of semantic analysis of sentences exploiting syntactic patterns, as hedge spans can usually be determined on the basis of syntactic patterns dependent on the keyword. 1 Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Related Work The term hedging was originally introduced by Lakoff (1972). However, hedge detection has received considerable interest just recently in the NLP community. Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features </context>
</contexts>
<marker>Lakoff, 1972</marker>
<rawString>George Lakoff. 1972. Linguistics and natural logic. In The Semantics of Natural Language, pages 545– 665, Dordrecht. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinxin Li</author>
<author>Jianping Shen</author>
<author>Xiang Gao</author>
<author>Xuan Wang</author>
</authors>
<title>Exploiting Rich Features for Detecting Hedges and Their Scope.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>36--41</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="27953" citStr="Li et al. (2010)" startWordPosition="4541" endWordPosition="4544">/ 77.9 C Chen 74.9 / 79.1 / 76.9 C Fernandes 70.1 / 71.1 / 70.6 C Prabhakaran 67.5 / 19.5 / 30.3 X Table 3: Task1 biological results (type E {Closed(C), Cross(X), Open(O)J). higher precision than recall. There may be two reasons for this. The systems may have applied only reliable patterns, or patterns occurring in the evaluation set may be imperfectly covered by the training datasets. The most intense participation was on Task1B. Here, participants applied various precision/recall trade-off strategies. For instance, Tang et al. (2010) achieved a balanced precision/recall configuration, while Li et al. (2010) achieved third place thanks to their superior precision. Tables 4 and 5 show the cue-level performances, i.e. the F-measure of cue phrase matching where true positives were strict matches. Note that it was optional to submit cue annotations for Task1 (if participants submitted systems for both Task2 and Task1B with cue tagging, only the better score of the two was considered). It is interesting to see that Morante et al. (2010) who obtained the best results on Task2 achieved a medium-ranked F-measure on the cue-level (e.g. their result on the cue-level is lower by 4% com7 pared to Zhou et al.</context>
</contexts>
<marker>Li, Shen, Gao, Wang, 2010</marker>
<rawString>Xinxin Li, Jianping Shen, Xiang Gao, and Xuan Wang. 2010. Exploiting Rich Features for Detecting Hedges and Their Scope. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 36–41, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
<author>Xin Ying Qiu</author>
<author>Padmini Srinivasan</author>
</authors>
<title>The Language of Bioscience: Facts, Speculations, and Statements in Between.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL 2004 Workshop: Biolink</booktitle>
<pages>17--24</pages>
<contexts>
<context position="4290" citStr="Light et al. (2004)" startWordPosition="631" endWordPosition="634">ords has to be identified. The latter task falls within the scope of semantic analysis of sentences exploiting syntactic patterns, as hedge spans can usually be determined on the basis of syntactic patterns dependent on the keyword. 1 Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Related Work The term hedging was originally introduced by Lakoff (1972). However, hedge detection has received considerable interest just recently in the NLP community. Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-aut</context>
</contexts>
<marker>Light, Qiu, Srinivasan, 2004</marker>
<rawString>Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004. The Language of Bioscience: Facts, Speculations, and Statements in Between. In Proceedings of the HLT-NAACL 2004 Workshop: Biolink 2004, Linking Biological Literature, Ontologies and Databases, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
<author>Ted Briscoe</author>
</authors>
<title>Weakly Supervised Learning for Hedge Classification in Scientific Literature.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>992--999</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4736" citStr="Medlock and Briscoe (2007)" startWordPosition="700" endWordPosition="703">ork The term hedging was originally introduced by Lakoff (1972). However, hedge detection has received considerable interest just recently in the NLP community. Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information to semi-automatically refine a list of hedge cues. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing un</context>
</contexts>
<marker>Medlock, Briscoe, 2007</marker>
<rawString>Ben Medlock and Ted Briscoe. 2007. Weakly Supervised Learning for Hedge Classification in Scientific Literature. In Proceedings of the ACL, pages 992– 999, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>Learning the Scope of Hedge Cues in Biomedical Texts.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="5696" citStr="Morante and Daelemans (2009)" startWordPosition="842" endWordPosition="845">d features. Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information to semi-automatically refine a list of hedge cues. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing uncertainty based on Wikipedia weasel tags and syntactic patterns. The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope. It consists of clinical free-texts, biological texts from full papers and scientific abstracts. Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a scope detector following a supervised sequence labeling approach while ¨Ozg¨ur and Radev (2009) developed a rule-based system that exploits syntactic patterns. Several related works have also been published within the framework of The BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009), where a separate subtask was dedicated to predicting whether the recognized biological events are under negation or speculation, based on the GENIA event corpus annotations (Kilicoglu and Bergler, 2009; Van Landeghem et al., 2009). 3 Uncertainty Annotation Guidelines The shared task addres</context>
<context position="32140" citStr="Morante and Daelemans (2009)" startWordPosition="5286" endWordPosition="5289">hem. There were three systems which incorporated information about other hedge cues (e.g. their distance) of the sentence into the feature space and Zhang et al. (2010) constructed a cascade system which utilized directly the predicted scopes (it processes cue phrases from left to right) during predicting other scopes in the same sentence. The identification of the scope for a certain cue was typically carried out by classifying each token in the sentence. Task2 systems differ in the number of class labels used as target and in the machine learning approaches applied. Most systems – following Morante and Daelemans (2009) – used three class labels (F)IRST, (L)AST and NONE. Two participants used four classes by adding (I)NSIDE, while three systems followed a binary classification approach (SCOPE versus NONSCOPE). The systems typically included a post-processing procedure to force scopes to be continuous and to include the cue phrase in question. The machine learning methods applied can be again categorized into sequence labeling (SL) 8 Table 6: System architectures overview for Task1. Approaches: sequence labeling (SL), token classification (TC), bag-of-words model (BoW); Machine learners: Entropy Guided Transf</context>
</contexts>
<marker>Morante, Daelemans, 2009</marker>
<rawString>Roser Morante and Walter Daelemans. 2009. Learning the Scope of Hedge Cues in Biomedical Texts. In Proceedings of the BioNLP 2009 Workshop, pages 28–36, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Vincent Van Asch</author>
<author>Walter Daelemans</author>
</authors>
<title>Memory-based Resolution of Insentence Scopes of Hedge Cues.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>48--55</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Morante, Van Asch, Daelemans, 2010</marker>
<rawString>Roser Morante, Vincent Van Asch, and Walter Daelemans. 2010. Memory-based Resolution of Insentence Scopes of Hedge Cues. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 48–55, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arzucan ¨Ozg¨ur</author>
<author>Dragomir R Radev</author>
</authors>
<title>Detecting Speculations and their Scopes in Scientific Text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1398--1407</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>¨Ozg¨ur, Radev, 2009</marker>
<rawString>Arzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. Detecting Speculations and their Scopes in Scientific Text. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1398–1407, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>Hedge Classification in Biomedical Texts with a Weakly Supervised Selection of Keywords.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>281--289</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4944" citStr="Szarvas (2008)" startWordPosition="731" endWordPosition="732">o identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information to semi-automatically refine a list of hedge cues. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing uncertainty based on Wikipedia weasel tags and syntactic patterns. The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope. It consists of c</context>
</contexts>
<marker>Szarvas, 2008</marker>
<rawString>Gy¨orgy Szarvas. 2008. Hedge Classification in Biomedical Texts with a Weakly Supervised Selection of Keywords. In Proceedings of ACL-08: HLT, pages 281–289, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Buzhou Tang</author>
<author>Xiaolong Wang</author>
<author>Xuan Wang</author>
<author>Bo Yuan</author>
<author>Shixi Fan</author>
</authors>
<title>A Cascade Method for Detecting Hedges and their Scope in Natural Language Text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>25--29</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="27878" citStr="Tang et al. (2010)" startWordPosition="4530" endWordPosition="4533">3 C Georgescul 69.1 / 91.0 / 78.5 C Zhao 71.0 / 86.6 / 78.0 C Ji 79.4 / 76.3 / 77.9 C Chen 74.9 / 79.1 / 76.9 C Fernandes 70.1 / 71.1 / 70.6 C Prabhakaran 67.5 / 19.5 / 30.3 X Table 3: Task1 biological results (type E {Closed(C), Cross(X), Open(O)J). higher precision than recall. There may be two reasons for this. The systems may have applied only reliable patterns, or patterns occurring in the evaluation set may be imperfectly covered by the training datasets. The most intense participation was on Task1B. Here, participants applied various precision/recall trade-off strategies. For instance, Tang et al. (2010) achieved a balanced precision/recall configuration, while Li et al. (2010) achieved third place thanks to their superior precision. Tables 4 and 5 show the cue-level performances, i.e. the F-measure of cue phrase matching where true positives were strict matches. Note that it was optional to submit cue annotations for Task1 (if participants submitted systems for both Task2 and Task1B with cue tagging, only the better score of the two was considered). It is interesting to see that Morante et al. (2010) who obtained the best results on Task2 achieved a medium-ranked F-measure on the cue-level (</context>
</contexts>
<marker>Tang, Wang, Wang, Yuan, Fan, 2010</marker>
<rawString>Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan, and Shixi Fan. 2010. A Cascade Method for Detecting Hedges and their Scope in Natural Language Text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 25–29, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sofie Van Landeghem</author>
<author>Yvan Saeys</author>
<author>Bernard De Baets</author>
<author>Yves Van de Peer</author>
</authors>
<title>Analyzing Text in Search of Bio-molecular Events: A High-precision Machine Learning Framework.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task,</booktitle>
<pages>128--136</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<marker>Van Landeghem, Saeys, De Baets, Van de Peer, 2009</marker>
<rawString>Sofie Van Landeghem, Yvan Saeys, Bernard De Baets, and Yves Van de Peer. 2009. Analyzing Text in Search of Bio-molecular Events: A High-precision Machine Learning Framework. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 128–136, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>Gy¨orgy Szarvas</author>
<author>Rich´ard Farkas</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
</authors>
<title>The BioScope Corpus: Biomedical Texts Annotated for Uncertainty, Negation and their Scopes.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<pages>11--9</pages>
<marker>Vincze, Szarvas, Farkas, M´ora, Csirik, 2008</marker>
<rawString>Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas, Gy¨orgy M´ora, and J´anos Csirik. 2008. The BioScope Corpus: Biomedical Texts Annotated for Uncertainty, Negation and their Scopes. BMC Bioinformatics, 9(Suppl 11):S9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shaodian Zhang</author>
<author>Hai Zhao</author>
<author>Guodong Zhou</author>
<author>Baoliang Lu</author>
</authors>
<title>Hedge Detection and Scope Finding by Sequence Labeling with Procedural Feature Selection.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>70--77</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="31223" citStr="Zhang et al. (2010)" startWordPosition="5138" endWordPosition="5141">at biological sentences have relatively simple patterns. Thus the context of the cue words (token classification-based approaches used features derived from a window of the token in question, thus, they exploited the relationship among the tokens and their contexts) can be utilized while Wikipedia weasels have a diverse nature. Another observation is that the top systems in both Task1B and Task1W are the ones which did not derive features from syntactic parsing. Each Task2 system was built upon a Task1 system, i.e. they attempted to recognize the scopes for the predicted cue phrases (however, Zhang et al. (2010) have argued that the objective functions of Task1 and Task2 cue detection problems are different because of sentences containing multiple hedge spans). Most systems regarded multiple cues in a sentence to be independent from each other and formed different classification instances from them. There were three systems which incorporated information about other hedge cues (e.g. their distance) of the sentence into the feature space and Zhang et al. (2010) constructed a cascade system which utilized directly the predicted scopes (it processes cue phrases from left to right) during predicting othe</context>
</contexts>
<marker>Zhang, Zhao, Zhou, Lu, 2010</marker>
<rawString>Shaodian Zhang, Hai Zhao, Guodong Zhou, and Baoliang Lu. 2010. Hedge Detection and Scope Finding by Sequence Labeling with Procedural Feature Selection. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 70–77, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhao</author>
<author>Chengjie Sun</author>
<author>Bingquan Liu</author>
<author>Yong Cheng</author>
</authors>
<title>Learning to Detect Hedges and their Scope Using CRF.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>64--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="25129" citStr="Zhao et al. (2010)" startWordPosition="3938" endWordPosition="3941">ams participated in the shared task. 22, 16 and 13 teams submitted output for Task1B, Task1W and Task2, respectively. 6.1 Results Tables 1, 2 and 3 contain the results of the submitted systems for Task1 and Task2. The last name of the first author of the system description paper (published in these proceedings) is used here as a system name3. The last column contains the type of submission. The system of Kilicoglu and Bergler (2010) is the only open submission. They adapted their system introduced in Kilicoglu and Bergler (2008) to the datasets of the shared task. Regarding cross submissions, Zhao et al. (2010) and Ji et al. (2010) managed to achieve a noticeable improvement by exploiting cross-domain 3 ¨Ozg¨ur did not publish a description of her system. 6 Name P / R / F type Georgescul 72.0 / 51.7 / 60.2 C Ji 62.7 / 55.3 / 58.7 X Chen 68.0 / 49.7 / 57.4 C Morante 80.6 / 44.5 / 57.3 C Zhang 76.6 / 44.4 / 56.2 C Zheng 76.3 / 43.6 / 55.5 C T¨ackstr¨om 78.3 / 42.8 / 55.4 C Mamani S´anchez 68.3 / 46.2 / 55.1 C Tang 82.3 / 41.4 / 55.0 C Kilicoglu 67.9 / 46.0 / 54.9 O Tjong Kim Sang 74.0 / 43.0 / 54.4 C Clausen 75.1 / 42.0 / 53.9 C ¨Ozg¨ur 59.4 / 47.9 / 53.1 C Zhou 85.3 / 36.5 / 51.1 C Li 88.4 / 31.9 / 4</context>
</contexts>
<marker>Zhao, Sun, Liu, Cheng, 2010</marker>
<rawString>Qi Zhao, Chengjie Sun, Bingquan Liu, and Yong Cheng. 2010. Learning to Detect Hedges and their Scope Using CRF. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 64– 69, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huiwei Zhou</author>
<author>Xiaoyan Li</author>
<author>Degen Huang</author>
<author>Zezhong Li</author>
<author>Yuansheng Yang</author>
</authors>
<title>Exploiting MultiFeatures to Detect Hedges and Their Scope in Biomedical Texts.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>56--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="28560" citStr="Zhou et al. (2010)" startWordPosition="4645" endWordPosition="4648"> al. (2010) achieved third place thanks to their superior precision. Tables 4 and 5 show the cue-level performances, i.e. the F-measure of cue phrase matching where true positives were strict matches. Note that it was optional to submit cue annotations for Task1 (if participants submitted systems for both Task2 and Task1B with cue tagging, only the better score of the two was considered). It is interesting to see that Morante et al. (2010) who obtained the best results on Task2 achieved a medium-ranked F-measure on the cue-level (e.g. their result on the cue-level is lower by 4% com7 pared to Zhou et al. (2010), while on the scopelevel the difference is 13% in the reverse direction), which indicates that the real strength of the system of Morante et al. (2010) is the accurate detection of scope boundaries. Name P / R / F Tang 63.0 / 25.7 / 36.5 Li 76.1 / 21.6 / 33.7 ¨Ozg¨ur 28.9 / 14.7 / 19.5 Morante 24.6 / 7.3 / 11.3 Table 4: Wikipedia cue-level results. Name P / R / F type Tang 81.7 / 81.0 / 81.3 C Zhou 83.1 / 78.8 / 80.9 C Li 87.4 / 73.4 / 79.8 C Rei 81.4 / 77.4 / 79.3 C Velldal 81.2 / 76.3 / 78.7 C Zhang 82.1 / 75.3 / 78.5 C Ji 78.7 / 76.2 / 77.4 C Morante 78.8 / 74.7 / 76.7 C Kilicoglu 86.5 / 6</context>
</contexts>
<marker>Zhou, Li, Huang, Li, Yang, 2010</marker>
<rawString>Huiwei Zhou, Xiaoyan Li, Degen Huang, Zezhong Li, and Yuansheng Yang. 2010. Exploiting MultiFeatures to Detect Hedges and Their Scope in Biomedical Texts. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 56– 63, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>