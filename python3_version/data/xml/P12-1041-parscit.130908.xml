<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001581">
<title confidence="0.882964">
Coreference Semantics from Web Features
</title>
<author confidence="0.977765">
Mohit Bansal and Dan Klein
</author>
<affiliation confidence="0.9956995">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.99756">
{mbansal,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.994771" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999620153846154">
To address semantic ambiguities in corefer-
ence resolution, we use Web n-gram features
that capture a range of world knowledge in a
diffuse but robust way. Specifically, we ex-
ploit short-distance cues to hypernymy, se-
mantic compatibility, and semantic context, as
well as general lexical co-occurrence. When
added to a state-of-the-art coreference base-
line, our Web features give significant gains on
multiple datasets (ACE 2004 and ACE 2005)
and metrics (MUC and B3), resulting in the
best results reported to date for the end-to-end
task of coreference resolution.
</bodyText>
<sectionHeader confidence="0.998422" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999708894736842">
Many of the most difficult ambiguities in corefer-
ence resolution are semantic in nature. For instance,
consider the following example:
When Obama met Jobs, the president dis-
cussed the economy, technology, and educa-
tion. His election campaign is expected to [...]
For resolving coreference in this example, a sys-
tem would benefit from the world knowledge that
Obama is the president. Also, to resolve the pro-
noun his to the correct antecedent Obama, we can
use the knowledge that Obama has an election cam-
paign while Jobs does not. Such ambiguities are
difficult to resolve on purely syntactic or configu-
rational grounds.
There have been multiple previous systems that
incorporate some form of world knowledge in coref-
erence resolution tasks. Most work (Poesio et
al., 2004; Markert and Nissim, 2005; Yang et
al., 2005; Bergsma and Lin, 2006) addresses spe-
cial cases and subtasks such as bridging anaphora,
other anaphora, definite NP reference, and pronoun
resolution, computing semantic compatibility via
Web-hits and counts from large corpora. There
is also work on end-to-end coreference resolution
that uses large noun-similarity lists (Daum´e III and
Marcu, 2005) or structured knowledge bases such as
Wikipedia (Yang and Su, 2007; Haghighi and Klein,
2009; Kobdani et al., 2011) and YAGO (Rahman
and Ng, 2011). However, such structured knowledge
bases are of limited scope, and, while Haghighi and
Klein (2010) self-acquires knowledge about corefer-
ence, it does so only via reference constructions and
on a limited scale.
In this paper, we look to the Web for broader if
shallower sources of semantics. In order to harness
the information on the Web without presupposing
a deep understanding of all Web text, we instead
turn to a diverse collection of Web n-gram counts
(Brants and Franz, 2006) which, in aggregate, con-
tain diffuse and indirect, but often robust, cues to
reference. For example, we can collect the co-
occurrence statistics of an anaphor with various can-
didate antecedents to judge relative surface affinities
(i.e., (Obama, president) versus (Jobs, president)).
We can also count co-occurrence statistics of com-
peting antecedents when placed in the context of an
anaphoric pronoun (i.e., Obama’s election campaign
versus Jobs’ election campaign).
All of our features begin with a pair of head-
words from candidate mention pairs and compute
statistics derived from various potentially informa-
tive queries’ counts. We explore five major cat-
egories of semantically informative Web features,
based on (1) general lexical affinities (via generic
co-occurrence statistics), (2) lexical relations (via
Hearst-style hypernymy patterns), (3) similarity of
entity-based context (e.g., common values of y for
</bodyText>
<page confidence="0.985578">
389
</page>
<note confidence="0.9857595">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 389–398,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999838111111111">
which h is a y is attested), (4) matches of distribu-
tional soft cluster ids, and (5) attested substitutions
of candidate antecedents in the context of a pronom-
inal anaphor.
We first describe a strong baseline consisting of
the mention-pair model of the Reconcile system
(Stoyanov et al., 2009; Stoyanov et al., 2010) us-
ing a decision tree (DT) as its pairwise classifier. To
this baseline system, we add our suite of features
in turn, each class of features providing substantial
gains. Altogether, our final system produces the best
numbers reported to date on end-to-end coreference
resolution (with automatically detected system men-
tions) on multiple data sets (ACE 2004 and ACE
2005) and metrics (MUC and B3), achieving signif-
icant improvements over the Reconcile DT baseline
and over the state-of-the-art results of Haghighi and
Klein (2010).
</bodyText>
<sectionHeader confidence="0.989463" genericHeader="method">
2 Baseline System
</sectionHeader>
<bodyText confidence="0.999890875">
Before describing our semantic Web features, we
first describe our baseline. The core inference and
features come from the Reconcile package (Stoy-
anov et al., 2009; Stoyanov et al., 2010), with modi-
fications described below. Our baseline differs most
substantially from Stoyanov et al. (2009) in using a
decision tree classifier rather than an averaged linear
perceptron.
</bodyText>
<subsectionHeader confidence="0.973111">
2.1 Reconcile
</subsectionHeader>
<bodyText confidence="0.999978851851852">
Reconcile is one of the best implementations of the
mention-pair model (Soon et al., 2001) of coref-
erence resolution. The mention-pair model relies
on a pairwise function to determine whether or not
two mentions are coreferent. Pairwise predictions
are then consolidated by transitive closure (or some
other clustering method) to form the final set of
coreference clusters (chains). While our Web fea-
tures could be adapted to entity-mention systems,
their current form was most directly applicable to
the mention-pair approach, making Reconcile a par-
ticularly well-suited platform for this investigation.
The Reconcile system provides baseline features,
learning mechanisms, and resolution procedures that
already achieve near state-of-the-art results on mul-
tiple popular datasets using multiple standard met-
rics. It includes over 80 core features that exploit
various automatically generated annotations such as
named entity tags, syntactic parses, and WordNet
classes, inspired by Soon et al. (2001), Ng and
Cardie (2002), and Bengtson and Roth (2008). The
Reconcile system also facilitates standardized em-
pirical evaluation to past work.1
In this paper, we develop a suite of simple seman-
tic Web features based on pairs of mention head-
words which stack with the default Reconcile fea-
tures to surpass past state-of-the-art results.
</bodyText>
<subsectionHeader confidence="0.999183">
2.2 Decision Tree Classifier
</subsectionHeader>
<bodyText confidence="0.999964222222222">
Among the various learning algorithms that Recon-
cile supports, we chose the decision tree classifier,
available in Weka (Hall et al., 2009) as J48, an open
source Java implementation of the C4.5 algorithm of
Quinlan (1993).
The C4.5 algorithm builds decision trees by incre-
mentally maximizing information gain. The train-
ing data is a set of already classified samples, where
each sample is a vector of attributes or features. At
each node of the tree, C4.5 splits the data on an
attribute that most effectively splits its set of sam-
ples into more ordered subsets, and then recurses on
these smaller subsets. The decision tree can then be
used to classify a new sample by following a path
from the root downward based on the attribute val-
ues of the sample.
We find the decision tree classifier to work better
than the default averaged perceptron (used by Stoy-
anov et al. (2009)), on multiple datasets using multi-
ple metrics (see Section 4.3). Many advantages have
been claimed for decision tree classifiers, including
interpretability and robustness. However, we sus-
pect that the aspect most relevant to our case is that
decision trees can capture non-linear interactions be-
tween features. For example, recency is very im-
portant for pronoun reference but much less so for
nominal reference.
</bodyText>
<sectionHeader confidence="0.972235" genericHeader="method">
3 Semantics via Web Features
</sectionHeader>
<bodyText confidence="0.9988285">
Our Web features for coreference resolution are sim-
ple and capture a range of diffuse world knowledge.
Given a mention pair, we use the head finder in Rec-
oncile to find the lexical heads of both mentions (for
</bodyText>
<footnote confidence="0.995909">
1We use the default configuration settings of Reconcile
(Stoyanov et al., 2010) unless mentioned otherwise.
</footnote>
<page confidence="0.998089">
390
</page>
<bodyText confidence="0.999974916666667">
example, the head of the Palestinian territories is the
word territories). Next, we take each headword pair
(h1, h2) and compute various Web-count functions
on it that can signal whether or not this mention pair
is coreferent.
As the source of Web information, we use the
Google n-grams corpus (Brants and Franz, 2006)
which contains English n-grams (n = 1 to 5) and
their Web frequency counts, derived from nearly 1
trillion word tokens and 95 billion sentences. Be-
cause we have many queries that must be run against
this corpus, we apply the trie-based hashing algo-
rithm of Bansal and Klein (2011) to efficiently an-
swer all of them in one pass over it. The features
that require word clusters (Section 3.4) use the out-
put of Lin et al. (2010).2
We describe our five types of features in turn. The
first four types are most intuitive for mention pairs
where both members are non-pronominal, but, aside
from the general co-occurrence group, helped for all
mention pair types. The fifth feature group applies
only to pairs in which the anaphor is a pronoun but
the antecedent is a non-pronoun. Related work for
each feature category is discussed inline.
</bodyText>
<subsectionHeader confidence="0.993617">
3.1 General co-occurrence
</subsectionHeader>
<bodyText confidence="0.9999629">
These features capture co-occurrence statistics of
the two headwords, i.e., how often h1 and h2 are
seen adjacent or nearly adjacent on the Web. This
count can be a useful coreference signal because,
in general, mentions referring to the same entity
will co-occur more frequently (in large corpora) than
those that do not. Using the n-grams corpus (for n
= 1 to 5), we collect co-occurrence Web-counts by
allowing a varying number of wildcards between h1
and h2 in the query. The co-occurrence value is:
</bodyText>
<equation confidence="0.986549">
Cc11 / )
C2
</equation>
<bodyText confidence="0.9921748">
2These clusters are derived form the V2 Google n-grams
corpus. The V2 corpus itself is not publicly available, but
the clusters are available at http://www.clsp.jhu.edu/
˜sbergsma/PhrasalClusters
where
</bodyText>
<equation confidence="0.999831">
c12 = count(“h1 ? h2”)
+ count(“h1 ? ? h2”)
+ count(“h1 ? ? ? h2”),
c1 = count(“h1”), and
c2 = count(“h2”).
</equation>
<bodyText confidence="0.999951428571429">
We normalize the overall co-occurrence count of the
headword pair c12 by the unigram counts of the indi-
vidual headwords c1 and c2, so that high-frequency
headwords do not unfairly get a high feature value
(this is similar to computing scaled mutual infor-
mation MI (Church and Hanks, 1989)).3 This nor-
malized value is quantized by taking its log10 and
binning. The actual feature that fires is an indica-
tor of which quantized bin the query produced. As
a real example from our development set, the co-
occurrence count c12 for the headword pair (leader,
president) is 11383, while it is only 95 for the head-
word pair (voter, president); after normalization and
log10, the values are -10.9 and -12.0, respectively.
These kinds of general Web co-occurrence statis-
tics have been used previously for other supervised
NLP tasks such as spelling correction and syntac-
tic parsing (Bergsma et al., 2010; Bansal and Klein,
2011). In coreference, similar word-association
scores were used by Kobdani et al. (2011), but from
Wikipedia and for self-training.
</bodyText>
<subsectionHeader confidence="0.999339">
3.2 Hearst co-occurrence
</subsectionHeader>
<bodyText confidence="0.999452888888889">
These features capture templated co-occurrence of
the two headwords h1 and h2 in the Web-corpus.
Here, we only collect statistics of the headwords co-
occurring with a generalized Hearst pattern (Hearst,
1992) in between. Hearst patterns capture various
lexical semantic relations between items. For ex-
ample, seeing X is a Y or X and other Y indicates
hypernymy and also tends to cue coreference. The
specific patterns we use are:
</bodyText>
<listItem confidence="0.997996666666667">
• h1 {is  |are  |was  |were} {a  |an  |the}? h2
• h1 {and  |or} {other  |the other  |another} h2
• h1 other than {a  |an  |the}? h2
</listItem>
<footnote confidence="0.868742">
3We also tried adding count(“h1 h2&amp;quot;) to c12 but this
decreases performance, perhaps because truly adjacent occur-
rences are often not grammatical.
Cbin log10
</footnote>
<page confidence="0.990852">
391
</page>
<listItem confidence="0.99945225">
• h1 such as {a  |an  |the}? h2
• h1 , including {a  |an  |the}? h2
• h1 , especially {a  |an  |the}? h2
• h1 of {the |all}? h2
</listItem>
<bodyText confidence="0.999901">
For this feature, we again use a quantized nor-
malized count as in Section 3.1, but c12 here is re-
stricted to n-grams where one of the above patterns
occurs in between the headwords. We did not al-
low wildcards in between the headwords and the
Hearst-patterns because this introduced a significant
amount of noise. Also, we do not constrain the or-
der of h1 and h2 because these patterns can hold
for either direction of coreference.4 As a real ex-
ample from our development set, the c12 count for
the headword pair (leader, president) is 752, while
for (voter, president), it is 0.
Hypernymic semantic compatibility for corefer-
ence is intuitive and has been explored in varying
forms by previous work. Poesio et al. (2004) and
Markert and Nissim (2005) employ a subset of our
Hearst patterns and Web-hits for the subtasks of
bridging anaphora, other-anaphora, and definite NP
resolution. Others (Haghighi and Klein, 2009; Rah-
man and Ng, 2011; Daum´e III and Marcu, 2005)
use similar relations to extract compatibility statis-
tics from Wikipedia, YAGO, and noun-similarity
lists. Yang and Su (2007) use Wikipedia to auto-
matically extract semantic patterns, which are then
used as features in a learning setup. Instead of ex-
tracting patterns from the training data, we use all
the above patterns, which helps us generalize to new
datasets for end-to-end coreference resolution (see
Section 4.3).
</bodyText>
<subsectionHeader confidence="0.989661">
3.3 Entity-based context
</subsectionHeader>
<bodyText confidence="0.939761428571429">
For each headword h, we first collect context seeds
y using the pattern
h {is  |are  |was  |were} {a  |an  |the}? y
taking seeds y in order of decreasing Web count.
The corresponding ordered seed list Y = {y} gives
us useful information about the headword’s entity
type. For example, for h = president, the top
</bodyText>
<footnote confidence="0.9633535">
4Two minor variants not listed above are h1 including h2
and h1 especially h2.
</footnote>
<bodyText confidence="0.999317407407407">
30 seeds (and their parts of speech) include impor-
tant cues such as president is elected (verb), pres-
ident is authorized (verb), president is responsible
(adjective), president is the chief (adjective), presi-
dent is above (preposition), and president is the head
(noun).
Matches in the seed lists of two headwords can
be a strong signal that they are coreferent. For ex-
ample, in the top 30 seed lists for the headword
pair (leader, president), we get matches including
elected, responsible, and expected. To capture this
effect, we create a feature that indicates whether
there is a match in the top k seeds of the two head-
words (where k is a hyperparameter to tune).
We create another feature that indicates whether
the dominant parts of speech in the seed lists
matches for the headword pair. We first collect the
POS tags (using length 2 character prefixes to indi-
cate coarse parts of speech) of the seeds matched in
the top k� seed lists of the two headwords, where
k� is another hyperparameter to tune. If the domi-
nant tags match and are in a small list of important
tags ({JJ, NN, RB, VB}), we fire an indicator feature
specifying the matched tag, otherwise we fire a no-
match indicator. To obtain POS tags for the seeds,
we use a unigram-based POS tagger trained on the
WSJ treebank training set.
</bodyText>
<subsectionHeader confidence="0.877303">
3.4 Cluster information
</subsectionHeader>
<bodyText confidence="0.999956368421053">
The distributional hypothesis of Harris (1954) says
that words that occur in similar contexts tend to have
a similar linguistic behavior. Here, we design fea-
tures with the idea that this hypothesis extends to
reference: mentions occurring in similar contexts
in large document sets such as the Web tend to be
compatible for coreference. Instead of collecting the
contexts of each mention and creating sparse fea-
tures from them, we use Web-scale distributional
clustering to summarize compatibility.
Specifically, we begin with the phrase-based clus-
ters from Lin et al. (2010), which were created us-
ing the Google n-grams V2 corpus. These clusters
come from distributional K-Means clustering (with
K = 1000) on phrases, using the n-gram context as
features. The cluster data contains almost 10 mil-
lion phrases and their soft cluster memberships. Up
to twenty cluster ids with the highest centroid sim-
ilarities are included for each phrase in this dataset
</bodyText>
<page confidence="0.988873">
392
</page>
<figureCaption confidence="0.412553">
(Lin et al., 2010).
</figureCaption>
<bodyText confidence="0.999884318181818">
Our cluster-based features assume that if the
headwords of the two mentions have matches in
their cluster id lists, then they are more compatible
for coreference. We check the match of not just the
top 1 cluster ids, but also farther down in the 20 sized
lists because, as discussed in Lin and Wu (2009),
the soft cluster assignments often reveal different
senses of a word. However, we also assume that
higher-ranked matches tend to imply closer mean-
ings. To this end, we fire a feature indicating the
value bin(i+j), where i and j are the earliest match
positions in the cluster id lists of h1 and h2. Binning
here means that match positions in a close range
generally trigger the same feature.
Recent previous work has used clustering infor-
mation to improve the performance of supervised
NLP tasks such as NER and dependency parsing
(Koo et al., 2008; Lin and Wu, 2009). However, in
coreference, the only related work to our knowledge
is from Daum´e III and Marcu (2005), who use word
class features derived from a Web-scale corpus via a
process described in Ravichandran et al. (2005).
</bodyText>
<subsectionHeader confidence="0.959003">
3.5 Pronoun context
</subsectionHeader>
<bodyText confidence="0.9999485">
Our last feature category specifically addresses pro-
noun reference, for cases when the anaphoric men-
tion NP2 (and hence its headword h2) is a pronoun,
while the candidate antecedent mention NP1 (and
hence its headword h1) is not. For such a head-
word pair (h1, h2), the idea is to substitute the non-
pronoun h1 into h2’s position and see whether the
result is attested on the Web.
If the anaphoric pronominal mention is h2 and its
sentential context is l’ l h2 r r’, then the substituted
phrase will be l’ l h1 r r’.5 High Web counts of sub-
stituted phrases tend to indicate semantic compati-
bility. Perhaps unsurprisingly for English, only the
right context was useful in this capacity. We chose
the following three context types, based on perfor-
mance on a development set:
</bodyText>
<footnote confidence="0.8743896">
5Possessive pronouns are replaced with an additional apos-
trophe, i.e., hl ’s. We also use features (see R1Gap) that allow
wildcards (*) in between the headword and the context when
collecting Web-counts, in order to allow for determiners and
other filler words.
</footnote>
<listItem confidence="0.999257666666667">
• h1 r (R1)
• h1 r r’ (R2)
• h1 * r (R1Gap)
</listItem>
<bodyText confidence="0.9685535">
As an example of the R1Gap feature, if the
anaphor h2 + context is his victory and one candidate
antecedent h1 is Bush, then we compute the normal-
ized value
</bodyText>
<equation confidence="0.9866716">
count(“Bush &apos;s * victory”)
count(“ * Is * victory&amp;quot;)count(“Bush&amp;quot;)
In general, we compute
count(“h1 Is * r&amp;quot;)
count(“ * Is * r&amp;quot;)count(“h1&amp;quot;)
</equation>
<bodyText confidence="0.9278531">
The final feature value is again a normalized count
converted to log10 and then binned.6 We have three
separate features for the R1, R2, and R1Gap context
types. We tune a separate bin-size hyperparameter
for each of these three features.
These pronoun resolution features are similar to
selectional preference work by Yang et al. (2005)
and Bergsma and Lin (2006), who compute seman-
tic compatibility for pronouns in specific syntactic
relationships such as possessive-noun, subject-verb,
etc. In our case, we directly use the general context
of any pronominal anaphor to find its most compat-
ible antecedent.
Note that all our above features are designed to be
non-sparse by firing indicators of the quantized Web
statistics and not the lexical- or class-based identities
of the mention pair. This keeps the total number of
features small, which is important for the relatively
small datasets used for coreference resolution. We
go from around 100 features in the Reconcile base-
line to around 165 features after adding all our Web
features.
6Normalization helps us with two kinds of balancing. First,
we divide by the count of the antecedent so that when choos-
ing the best antecedent for a fixed anaphor, we are not biased
towards more frequently occurring antecedents. Second, we di-
vide by the count of the context so that across anaphora, an
anaphor with rarer context does not get smaller values (for all its
candidate antecedents) than another anaphor with a more com-
mon context.
</bodyText>
<page confidence="0.997692">
393
</page>
<table confidence="0.9962135">
Dataset docs dev test ment chn
ACE04 128 63/27 90/38 3037 1332
ACE05 81 40/17 57/24 1991 775
ACE05-ALL 599 337/145 482/117 9217 3050
</table>
<tableCaption confidence="0.9521738">
Table 1: Dataset characteristics – docs: the total number of doc-
uments; dev: the train/test split during development; test: the
train/test split during testing; ment: the number of gold men-
tions in the test split; chn: the number of coreference chains in
the test split.
</tableCaption>
<sectionHeader confidence="0.997505" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.954464">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.9999719">
We show results on three popular and comparatively
larger coreference resolution data sets – the ACE04,
ACE05, and ACE05-ALL datasets from the ACE
Program (NIST, 2004). In ACE04 and ACE05, we
have only the newswire portion (of the original ACE
2004 and 2005 training sets) and use the standard
train/test splits reported in Stoyanov et al. (2009)
and Haghighi and Klein (2010). In ACE05-ALL,
we have the full ACE 2005 training set and use the
standard train/test splits reported in Rahman and Ng
(2009) and Haghighi and Klein (2010). Note that
most previous work does not report (or need) a stan-
dard development set; hence, for tuning our fea-
tures and its hyper-parameters, we randomly split
the original training data into a training and devel-
opment set with a 70/30 ratio (and then use the full
original training set during testing). Details of the
corpora are shown in Table 1.7
Details of the Web-scale corpora used for extract-
ing features are discussed in Section 3.
</bodyText>
<subsectionHeader confidence="0.974587">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.891027">
We evaluated our work on both MUC (Vilain et al.,
1995) and B3 (Bagga and Baldwin, 1998). Both
scorers are available in the Reconcile infrastruc-
ture.8 MUC measures how many predicted clusters
need to be merged to cover the true gold clusters.
B3 computes precision and recall for each mention
by computing the intersection of its predicted and
gold cluster and dividing by the size of the predicted
7Note that the development set is used only for ACE04, be-
cause for ACE05, and ACE05-ALL, we directly test using the
features tuned on ACE04.
8Note that B3 has two versions which handle twinless (spu-
rious) mentions in different ways (see Stoyanov et al. (2009) for
details). We use the B3All version, unless mentioned otherwise.
</bodyText>
<table confidence="0.999695">
Feature P MUC F1 P B3 F1
R R
AvgPerc 69.0 63.1 65.9 82.2 69.9 75.5
DecTree 80.9 61.0 69.5 89.5 69.0 77.9
+ Co-occ 79.8 62.1 69.8 88.7 69.8 78.1
+ Hearst 80.0 62.3 70.0 89.1 70.1 78.5
+ Entity 79.4 63.2 70.4 88.1 70.9 78.6
+ Cluster 79.5 63.6 70.7 87.9 71.2 78.6
+ Pronoun 79.9 64.3 71.3 88.0 71.6 79.0
</table>
<tableCaption confidence="0.989634">
Table 2: Incremental results for the Web features on the ACE04
</tableCaption>
<figureCaption confidence="0.4932504">
development set. AvgPerc is the averaged perceptron baseline,
DecTree is the decision tree baseline, and the +Feature rows
show the effect of adding a particular feature incrementally (not
in isolation) to the DecTree baseline. The feature categories
correspond to those described in Section 3.
</figureCaption>
<bodyText confidence="0.999798142857143">
and gold cluster, respectively. It is well known
(Recasens and Hovy, 2010; Ng, 2010; Kobdani et
al., 2011) that MUC is biased towards large clus-
ters (chains) whereas B3 is biased towards singleton
clusters. Therefore, for a more balanced evaluation,
we show improvements on both metrics simultane-
ously.
</bodyText>
<subsectionHeader confidence="0.938974">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.945525217391304">
We start with the Reconcile baseline but employ the
decision tree (DT) classifier, because it has signifi-
cantly better performance than the default averaged
perceptron classifier used in Stoyanov et al. (2009).9
Table 2 compares the baseline perceptron results to
the DT results and then shows the incremental addi-
tion of the Web features to the DT baseline (on the
ACE04 development set).
The DT classifier, in general, is precision-biased.
The Web features somewhat balance this by increas-
ing the recall and decreasing precision to a lesser ex-
tent, improving overall F1. Each feature type incre-
mentally increases both MUC and B3 F1-measures,
showing that they are not taking advantage of any
bias of either metric. The incremental improve-
ments also show that each Web feature type brings
in some additional benefit over the information al-
ready present in the Reconcile baseline, which in-
cludes alias, animacy, named entity, and WordNet
9Moreover, a DT classifier takes roughly the same amount of
time and memory as a perceptron on our ACE04 development
experiments. It is, however, slower and more memory-intensive
(-3x) on the bigger ACE05-ALL dataset.
</bodyText>
<page confidence="0.994857">
394
</page>
<table confidence="0.998131846153846">
MUC B3
System P R F1 P R F1
ACE04-TEST-RESULTS
Stoyanov et al. (2009) - - 62.0 - - 76.5
Haghighi and Klein (2009) 67.5 61.6 64.4 77.4 69.4 73.2
Haghighi and Klein (2010) 67.4 66.6 67.0 81.2 73.3 77.0
This Work: Perceptron Baseline 65.5 61.9 63.7 84.1 70.9 77.0
This Work: DT Baseline 76.0 60.7 67.5 89.6 70.3 78.8
This Work: DT + Web Features 74.8 64.2 69.1 87.5 73.7 80.0
This Work: A of DT+Web over DT (p &lt; 0.05) 1.7 (p &lt; 0.005) 1.3
ACE05-TEST-RESULTS
Stoyanov et al. (2009) - - 67.4 - - 73.7
Haghighi and Klein (2009) 73.1 58.8 65.2 82.1 63.9 71.8
Haghighi and Klein (2010) 74.6 62.7 68.1 83.2 68.4 75.1
This Work: Perceptron Baseline 72.2 61.6 66.5 85.0 65.5 73.9
This Work: DT Baseline 79.6 59.7 68.2 89.4 64.2 74.7
This Work: DT + Web Features 75.0 64.7 69.5 81.1 70.8 75.6
This Work: A of DT+Web over DT (p &lt; 0.12) 1.3 (p &lt; 0.1) 0.9
ACE05-ALL-TEST-RESULTS
Rahman and Ng (2009) 75.4 64.1 69.3 54.4 70.5 61.4
Haghighi and Klein (2009) 72.9 60.2 67.0 53.2 73.1 61.6
Haghighi and Klein (2010) 77.0 66.9 71.6 55.4 74.8 63.8
This Work: Perceptron Baseline 68.9 60.4 64.4 80.6 60.5 69.1
This Work: DT Baseline 78.0 60.4 68.1 85.1 60.4 70.6
This Work: DT + Web Features 77.6 64.0 70.2 80.7 65.9 72.5
This Work: A of DT+Web over DT (p &lt; 0.001) 2.1 (p &lt; 0.001) 1.9
</table>
<tableCaption confidence="0.868252">
Table 3: Primary test results on the ACE04, ACE05, and ACE05-ALL datasets. All systems reported here use automatically
extracted system mentions. B3 here is the B3All version of Stoyanov et al. (2009). We also report statistical significance of the
</tableCaption>
<bodyText confidence="0.9454813125">
improvements from the Web features on the DT baseline, using the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1993). The
perceptron baseline in this work (Reconcile settings: 15 iterations, threshold = 0.45, SIG for ACE04 and AP for ACE05, ACE05-
ALL) has different results from Stoyanov et al. (2009) because their current publicly available code is different from that used in
their paper (p.c.). Also, the B3 variant used by Rahman and Ng (2009) is slightly different from other systems (they remove all and
only the singleton twinless system mentions, so it is neither B3All nor B3None). For completeness, our (untuned) B3None results
(DT + Web) on the ACE05-ALL dataset are P=69.91R=65.91F1=67.8.
class / sense information.10
Table 3 shows our primary test results on the
ACE04, ACE05, and ACE05-ALL datasets, for the
MUC and B3 metrics. All systems reported use au-
tomatically detected mentions. We report our re-
sults (the 3 rows marked ‘This Work’) on the percep-
tron baseline, the DT baseline, and the Web features
added to the DT baseline. We also report statistical
significance of the improvements from the Web fea-
10We also initially experimented with smaller datasets
(MUC6 and MUC7) and an averaged perceptron baseline, and
we did see similar improvements, arguing that these features are
useful independently of the learning algorithm and dataset.
tures on the DT baseline.11 For significance testing,
we use the bootstrap test (Noreen, 1989; Efron and
Tibshirani, 1993).
Our main comparison is against Haghighi and
Klein (2010), a mostly-unsupervised generative ap-
proach that models latent entity types, which gen-
erate specific entities that in turn render individual
mentions. They learn on large datasets including
11All improvements are significant, except on the small
ACE05 dataset with the MUC metric (where it is weak, at
p &lt; 0.12). However, on the larger version of this dataset,
ACE05-ALL, we get improvements which are both larger and
more significant (at p &lt; 0.001).
</bodyText>
<page confidence="0.997911">
395
</page>
<bodyText confidence="0.999452833333333">
Wikipedia, and their results are state-of-the-art in
coreference resolution. We outperform their system
on most datasets and metrics (except on ACE05-
ALL for the MUC metric). The other systems we
compare to and outperform are the perceptron-based
Reconcile system of Stoyanov et al. (2009), the
strong deterministic system of Haghighi and Klein
(2009), and the cluster-ranking model of Rahman
and Ng (2009).
We develop our features and tune their hyper-
parameter values on the ACE04 development set and
then use these on the ACE04 test set.12 On the
ACE05 and ACE05-ALL datasets, we directly trans-
fer our Web features and their hyper-parameter val-
ues from the ACE04 dev-set, without any retuning.
The test improvements we get on all the datasets (see
Table 3) suggest that our features are generally use-
ful across datasets and metrics.13
</bodyText>
<sectionHeader confidence="0.997193" genericHeader="evaluation">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999945916666667">
In this section, we briefly discuss errors (in the DT
baseline) corrected by our Web features, and ana-
lyze the decision tree classifier built during training
(based on the ACE04 development experiments).
To study error correction, we begin with the men-
tion pairs that are coreferent according to the gold-
standard annotation (after matching the system men-
tions to the gold ones). We consider the pairs that are
wrongly predicted to be non-coreferent by the base-
line DT system but correctly predicted to be corefer-
ent when we add our Web features. Some examples
of such pairs include:
</bodyText>
<construct confidence="0.9928995">
Iran ; the country
the EPA ; the agency
athletic director; Mulcahy
Democrat Al Gore ; the vice president
</construct>
<bodyText confidence="0.996563846153846">
12Note that for the ACE04 dataset only, we use the ‘SmartIn-
stanceGenerator’ (SIG) filter of Reconcile that uses only a fil-
tered set of mention-pairs (based on distance and other proper-
ties of the pair) instead of the ‘AllPairs’ (AP) setting that uses
all pairs of mentions, and makes training and tuning very slow.
13For the ACE05 and ACE05-ALL datasets, we revert to the
‘AllPairs’ (AP) setting of Reconcile because this gives us base-
lines competitive with Haghighi and Klein (2010). Since we did
not need to retune on these datasets, training and tuning speed
were not a bottleneck. Moreover, the improvements from our
Web features are similar even when tried over the SIG baseline;
hence, the filter choice doesn’t affect the performance gain from
the Web features.
</bodyText>
<construct confidence="0.695319">
Barry Bonds ; the best baseball player
Vojislav Kostunica ; the pro-democracy leader
its closest rival ; the German magazine Das Motorrad
One of those difficult-to-dislodge judges; John Marshall
</construct>
<bodyText confidence="0.99998925">
These pairs are cases where our features
on Hearst-style co-occurrence and entity-based
context-match are informative and help discriminate
in favor of the correct antecedents. One advan-
tage of using Web-based features is that the Web
has a surprising amount of information on even rare
entities such as proper names. Our features also
correct coreference for various cases of pronominal
anaphora, but these corrections are harder to convey
out of context.
Next, we analyze the decision tree built after
training the classifier (with all our Web features in-
cluded). Around 30% of the decision nodes (both
non-terminals and leaves) correspond to Web fea-
tures, and the average error in classification at the
Web-feature leaves is only around 2.5%, suggest-
ing that our features are strongly discriminative for
pairwise coreference decisions. Some of the most
discriminative nodes correspond to the general co-
occurrence feature for most (binned) log-count val-
ues, the Hearst-style co-occurrence feature for its
zero-count value, the cluster-match feature for its
zero-match value, and the R2 pronoun context fea-
ture for certain (binned) log-count values.
</bodyText>
<sectionHeader confidence="0.999199" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999959555555555">
We have presented a collection of simple Web-count
features for coreference resolution that capture a
range of world knowledge via statistics of general
lexical co-occurrence, hypernymy, semantic com-
patibility, and semantic context. When added to a
strong decision tree baseline, these features give sig-
nificant improvements and achieve the best results
reported to date, across multiple datasets and met-
rics.
</bodyText>
<sectionHeader confidence="0.998026" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999568166666667">
We would like to thank Nathan Gilbert, Adam Pauls,
and the anonymous reviewers for their helpful sug-
gestions. This research is supported by Qualcomm
via an Innovation Fellowship to the first author
and by BBN under DARPA contract HR0011-12-C-
0014.
</bodyText>
<page confidence="0.998227">
396
</page>
<sectionHeader confidence="0.983323" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999685274509804">
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of MUC-7
and LREC Workshop.
Mohit Bansal and Dan Klein. 2011. Web-scale features
for full-scale parsing. In Proceedings ofACL.
Eric Bengtson and Dan Roth. 2008. Understanding the
value of features for coreference resolution. In Pro-
ceedings of EMNLP.
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping path-based pronoun resolution. In Proceedings
of COLING-ACL.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In Proceedings ofACL.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram corpus version 1.1. LDC2006T13.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicogra-
phy. In Proceedings ofACL.
Hal Daum´e III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
EMNLP.
B. Efron and R. Tibshirani. 1993. An introduction to the
bootstrap. Chapman &amp; Hall CRC.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of NAACL-HLT.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Zellig Harris. 1954. Distributional structure. Word,
10(23):146162.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING.
Hamidreza Kobdani, Hinrich Schutze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrap-
ping coreference resolution using word associations.
In Proceedings ofACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings ofACL.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings ofACL.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of LREC.
Katja Markert and Malvina Nissim. 2005. Comparing
knowledge sources for nominal anaphora resolution.
Computational Linguistics, 31(3):367–402.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings ofACL.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of
ACL.
NIST. 2004. The ACE evaluation plan. In NIST.
E.W. Noreen. 1989. Computer intensive methods for
hypothesis testing: An introduction. Wiley, New York.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings ofACL.
J. R. Quinlan. 1993. C4.5: Programs for machine learn-
ing. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP.
Altaf Rahman and Vincent Ng. 2011. Coreference reso-
lution with world knowledge. In Proceedings ofACL.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and NLP: Using local-
ity sensitive hash functions for high speed noun clus-
tering. In Proceedings ofACL.
Marta Recasens and Eduard Hovy. 2010. Corefer-
ence resolution across corpora: Languages, coding
schemes, and preprocessing information. In Proceed-
ings ofACL.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521–544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-art.
In Proceedings ofACL/IJCNLP.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Rec-
oncile: A coreference resolution research platform. In
Technical report, Cornell University.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6.
Xiaofeng Yang and Jian Su. 2007. Coreference resolu-
tion using semantic relatedness information from auto-
matically discovered patterns. In Proceedings ofACL.
</reference>
<page confidence="0.984114">
397
</page>
<reference confidence="0.76293925">
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
ACL.
</reference>
<page confidence="0.998105">
398
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956336">
<title confidence="0.998872">Coreference Semantics from Web Features</title>
<author confidence="0.980625">Bansal</author>
<affiliation confidence="0.99989">Computer Science University of California,</affiliation>
<abstract confidence="0.998240142857143">To address semantic ambiguities in coreferresolution, we use Web features that capture a range of world knowledge in a diffuse but robust way. Specifically, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence. When added to a state-of-the-art coreference baseline, our Web features give significant gains on multiple datasets (ACE 2004 and ACE 2005) metrics (MUC and resulting in the best results reported to date for the end-to-end task of coreference resolution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of MUC-7 and LREC Workshop.</booktitle>
<contexts>
<context position="21414" citStr="Bagga and Baldwin, 1998" startWordPosition="3530" endWordPosition="3533">est splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). Note that most previous work does not report (or need) a standard development set; hence, for tuning our features and its hyper-parameters, we randomly split the original training data into a training and development set with a 70/30 ratio (and then use the full original training set during testing). Details of the corpora are shown in Table 1.7 Details of the Web-scale corpora used for extracting features are discussed in Section 3. 4.2 Evaluation Metrics We evaluated our work on both MUC (Vilain et al., 1995) and B3 (Bagga and Baldwin, 1998). Both scorers are available in the Reconcile infrastructure.8 MUC measures how many predicted clusters need to be merged to cover the true gold clusters. B3 computes precision and recall for each mention by computing the intersection of its predicted and gold cluster and dividing by the size of the predicted 7Note that the development set is used only for ACE04, because for ACE05, and ACE05-ALL, we directly test using the features tuned on ACE04. 8Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al. (2009) for details). We use the B3Al</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of MUC-7 and LREC Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Web-scale features for full-scale parsing.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8521" citStr="Bansal and Klein (2011)" startWordPosition="1333" endWordPosition="1336">entioned otherwise. 390 example, the head of the Palestinian territories is the word territories). Next, we take each headword pair (h1, h2) and compute various Web-count functions on it that can signal whether or not this mention pair is coreferent. As the source of Web information, we use the Google n-grams corpus (Brants and Franz, 2006) which contains English n-grams (n = 1 to 5) and their Web frequency counts, derived from nearly 1 trillion word tokens and 95 billion sentences. Because we have many queries that must be run against this corpus, we apply the trie-based hashing algorithm of Bansal and Klein (2011) to efficiently answer all of them in one pass over it. The features that require word clusters (Section 3.4) use the output of Lin et al. (2010).2 We describe our five types of features in turn. The first four types are most intuitive for mention pairs where both members are non-pronominal, but, aside from the general co-occurrence group, helped for all mention pair types. The fifth feature group applies only to pairs in which the anaphor is a pronoun but the antecedent is a non-pronoun. Related work for each feature category is discussed inline. 3.1 General co-occurrence These features captu</context>
<context position="10843" citStr="Bansal and Klein, 2011" startWordPosition="1725" endWordPosition="1728">his normalized value is quantized by taking its log10 and binning. The actual feature that fires is an indicator of which quantized bin the query produced. As a real example from our development set, the cooccurrence count c12 for the headword pair (leader, president) is 11383, while it is only 95 for the headword pair (voter, president); after normalization and log10, the values are -10.9 and -12.0, respectively. These kinds of general Web co-occurrence statistics have been used previously for other supervised NLP tasks such as spelling correction and syntactic parsing (Bergsma et al., 2010; Bansal and Klein, 2011). In coreference, similar word-association scores were used by Kobdani et al. (2011), but from Wikipedia and for self-training. 3.2 Hearst co-occurrence These features capture templated co-occurrence of the two headwords h1 and h2 in the Web-corpus. Here, we only collect statistics of the headwords cooccurring with a generalized Hearst pattern (Hearst, 1992) in between. Hearst patterns capture various lexical semantic relations between items. For example, seeing X is a Y or X and other Y indicates hypernymy and also tends to cue coreference. The specific patterns we use are: • h1 {is |are |was</context>
</contexts>
<marker>Bansal, Klein, 2011</marker>
<rawString>Mohit Bansal and Dan Klein. 2011. Web-scale features for full-scale parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5972" citStr="Bengtson and Roth (2008)" startWordPosition="907" endWordPosition="910">ity-mention systems, their current form was most directly applicable to the mention-pair approach, making Reconcile a particularly well-suited platform for this investigation. The Reconcile system provides baseline features, learning mechanisms, and resolution procedures that already achieve near state-of-the-art results on multiple popular datasets using multiple standard metrics. It includes over 80 core features that exploit various automatically generated annotations such as named entity tags, syntactic parses, and WordNet classes, inspired by Soon et al. (2001), Ng and Cardie (2002), and Bengtson and Roth (2008). The Reconcile system also facilitates standardized empirical evaluation to past work.1 In this paper, we develop a suite of simple semantic Web features based on pairs of mention headwords which stack with the default Reconcile features to surpass past state-of-the-art results. 2.2 Decision Tree Classifier Among the various learning algorithms that Reconcile supports, we chose the decision tree classifier, available in Weka (Hall et al., 2009) as J48, an open source Java implementation of the C4.5 algorithm of Quinlan (1993). The C4.5 algorithm builds decision trees by incrementally maximizi</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Eric Bengtson and Dan Roth. 2008. Understanding the value of features for coreference resolution. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
</authors>
<title>Bootstrapping path-based pronoun resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="1591" citStr="Bergsma and Lin, 2006" startWordPosition="243" endWordPosition="246"> His election campaign is expected to [...] For resolving coreference in this example, a system would benefit from the world knowledge that Obama is the president. Also, to resolve the pronoun his to the correct antecedent Obama, we can use the knowledge that Obama has an election campaign while Jobs does not. Such ambiguities are difficult to resolve on purely syntactic or configurational grounds. There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, other anaphora, definite NP reference, and pronoun resolution, computing semantic compatibility via Web-hits and counts from large corpora. There is also work on end-to-end coreference resolution that uses large noun-similarity lists (Daum´e III and Marcu, 2005) or structured knowledge bases such as Wikipedia (Yang and Su, 2007; Haghighi and Klein, 2009; Kobdani et al., 2011) and YAGO (Rahman and Ng, 2011). However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge </context>
<context position="18780" citStr="Bergsma and Lin (2006)" startWordPosition="3088" endWordPosition="3091">if the anaphor h2 + context is his victory and one candidate antecedent h1 is Bush, then we compute the normalized value count(“Bush &apos;s * victory”) count(“ * Is * victory&amp;quot;)count(“Bush&amp;quot;) In general, we compute count(“h1 Is * r&amp;quot;) count(“ * Is * r&amp;quot;)count(“h1&amp;quot;) The final feature value is again a normalized count converted to log10 and then binned.6 We have three separate features for the R1, R2, and R1Gap context types. We tune a separate bin-size hyperparameter for each of these three features. These pronoun resolution features are similar to selectional preference work by Yang et al. (2005) and Bergsma and Lin (2006), who compute semantic compatibility for pronouns in specific syntactic relationships such as possessive-noun, subject-verb, etc. In our case, we directly use the general context of any pronominal anaphor to find its most compatible antecedent. Note that all our above features are designed to be non-sparse by firing indicators of the quantized Web statistics and not the lexical- or class-based identities of the mention pair. This keeps the total number of features small, which is important for the relatively small datasets used for coreference resolution. We go from around 100 features in the </context>
</contexts>
<marker>Bergsma, Lin, 2006</marker>
<rawString>Shane Bergsma and Dekang Lin. 2006. Bootstrapping path-based pronoun resolution. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Emily Pitler</author>
<author>Dekang Lin</author>
</authors>
<title>Creating robust supervised classifiers via web-scale ngram data.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="10818" citStr="Bergsma et al., 2010" startWordPosition="1721" endWordPosition="1724"> and Hanks, 1989)).3 This normalized value is quantized by taking its log10 and binning. The actual feature that fires is an indicator of which quantized bin the query produced. As a real example from our development set, the cooccurrence count c12 for the headword pair (leader, president) is 11383, while it is only 95 for the headword pair (voter, president); after normalization and log10, the values are -10.9 and -12.0, respectively. These kinds of general Web co-occurrence statistics have been used previously for other supervised NLP tasks such as spelling correction and syntactic parsing (Bergsma et al., 2010; Bansal and Klein, 2011). In coreference, similar word-association scores were used by Kobdani et al. (2011), but from Wikipedia and for self-training. 3.2 Hearst co-occurrence These features capture templated co-occurrence of the two headwords h1 and h2 in the Web-corpus. Here, we only collect statistics of the headwords cooccurring with a generalized Hearst pattern (Hearst, 1992) in between. Hearst patterns capture various lexical semantic relations between items. For example, seeing X is a Y or X and other Y indicates hypernymy and also tends to cue coreference. The specific patterns we us</context>
</contexts>
<marker>Bergsma, Pitler, Lin, 2010</marker>
<rawString>Shane Bergsma, Emily Pitler, and Dekang Lin. 2010. Creating robust supervised classifiers via web-scale ngram data. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>The Google Web 1T 5-gram corpus version 1.1.</title>
<date>2006</date>
<pages>2006--13</pages>
<contexts>
<context position="2550" citStr="Brants and Franz, 2006" startWordPosition="394" endWordPosition="397">tured knowledge bases such as Wikipedia (Yang and Su, 2007; Haghighi and Klein, 2009; Kobdani et al., 2011) and YAGO (Rahman and Ng, 2011). However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffuse and indirect, but often robust, cues to reference. For example, we can collect the cooccurrence statistics of an anaphor with various candidate antecedents to judge relative surface affinities (i.e., (Obama, president) versus (Jobs, president)). We can also count co-occurrence statistics of competing antecedents when placed in the context of an anaphoric pronoun (i.e., Obama’s election campaign versus Jobs’ election campaign). All of our features begin with a pair of headwords from candidate mention pairs and compute statistics derived from various potenti</context>
<context position="8240" citStr="Brants and Franz, 2006" startWordPosition="1283" endWordPosition="1286"> for coreference resolution are simple and capture a range of diffuse world knowledge. Given a mention pair, we use the head finder in Reconcile to find the lexical heads of both mentions (for 1We use the default configuration settings of Reconcile (Stoyanov et al., 2010) unless mentioned otherwise. 390 example, the head of the Palestinian territories is the word territories). Next, we take each headword pair (h1, h2) and compute various Web-count functions on it that can signal whether or not this mention pair is coreferent. As the source of Web information, we use the Google n-grams corpus (Brants and Franz, 2006) which contains English n-grams (n = 1 to 5) and their Web frequency counts, derived from nearly 1 trillion word tokens and 95 billion sentences. Because we have many queries that must be run against this corpus, we apply the trie-based hashing algorithm of Bansal and Klein (2011) to efficiently answer all of them in one pass over it. The features that require word clusters (Section 3.4) use the output of Lin et al. (2010).2 We describe our five types of features in turn. The first four types are most intuitive for mention pairs where both members are non-pronominal, but, aside from the genera</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. The Google Web 1T 5-gram corpus version 1.1. LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="10215" citStr="Church and Hanks, 1989" startWordPosition="1620" endWordPosition="1623">ce value is: Cc11 / ) C2 2These clusters are derived form the V2 Google n-grams corpus. The V2 corpus itself is not publicly available, but the clusters are available at http://www.clsp.jhu.edu/ ˜sbergsma/PhrasalClusters where c12 = count(“h1 ? h2”) + count(“h1 ? ? h2”) + count(“h1 ? ? ? h2”), c1 = count(“h1”), and c2 = count(“h2”). We normalize the overall co-occurrence count of the headword pair c12 by the unigram counts of the individual headwords c1 and c2, so that high-frequency headwords do not unfairly get a high feature value (this is similar to computing scaled mutual information MI (Church and Hanks, 1989)).3 This normalized value is quantized by taking its log10 and binning. The actual feature that fires is an indicator of which quantized bin the query produced. As a real example from our development set, the cooccurrence count c12 for the headword pair (leader, president) is 11383, while it is only 95 for the headword pair (voter, president); after normalization and log10, the values are -10.9 and -12.0, respectively. These kinds of general Web co-occurrence statistics have been used previously for other supervised NLP tasks such as spelling correction and syntactic parsing (Bergsma et al., 2</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicography. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A large-scale exploration of effective global features for a joint entity detection and tracking model.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005. A large-scale exploration of effective global features for a joint entity detection and tracking model. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R Tibshirani</author>
</authors>
<title>An introduction to the bootstrap.</title>
<date>1993</date>
<publisher>Chapman &amp; Hall CRC.</publisher>
<contexts>
<context position="25821" citStr="Efron and Tibshirani, 1993" startWordPosition="4291" endWordPosition="4294">71.6 55.4 74.8 63.8 This Work: Perceptron Baseline 68.9 60.4 64.4 80.6 60.5 69.1 This Work: DT Baseline 78.0 60.4 68.1 85.1 60.4 70.6 This Work: DT + Web Features 77.6 64.0 70.2 80.7 65.9 72.5 This Work: A of DT+Web over DT (p &lt; 0.001) 2.1 (p &lt; 0.001) 1.9 Table 3: Primary test results on the ACE04, ACE05, and ACE05-ALL datasets. All systems reported here use automatically extracted system mentions. B3 here is the B3All version of Stoyanov et al. (2009). We also report statistical significance of the improvements from the Web features on the DT baseline, using the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1993). The perceptron baseline in this work (Reconcile settings: 15 iterations, threshold = 0.45, SIG for ACE04 and AP for ACE05, ACE05- ALL) has different results from Stoyanov et al. (2009) because their current publicly available code is different from that used in their paper (p.c.). Also, the B3 variant used by Rahman and Ng (2009) is slightly different from other systems (they remove all and only the singleton twinless system mentions, so it is neither B3All nor B3None). For completeness, our (untuned) B3None results (DT + Web) on the ACE05-ALL dataset are P=69.91R=65.91F1=67.8. class / sense</context>
<context position="27188" citStr="Efron and Tibshirani, 1993" startWordPosition="4509" endWordPosition="4512">reported use automatically detected mentions. We report our results (the 3 rows marked ‘This Work’) on the perceptron baseline, the DT baseline, and the Web features added to the DT baseline. We also report statistical significance of the improvements from the Web fea10We also initially experimented with smaller datasets (MUC6 and MUC7) and an averaged perceptron baseline, and we did see similar improvements, arguing that these features are useful independently of the learning algorithm and dataset. tures on the DT baseline.11 For significance testing, we use the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1993). Our main comparison is against Haghighi and Klein (2010), a mostly-unsupervised generative approach that models latent entity types, which generate specific entities that in turn render individual mentions. They learn on large datasets including 11All improvements are significant, except on the small ACE05 dataset with the MUC metric (where it is weak, at p &lt; 0.12). However, on the larger version of this dataset, ACE05-ALL, we get improvements which are both larger and more significant (at p &lt; 0.001). 395 Wikipedia, and their results are state-of-the-art in coreference resolution. We outperf</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>B. Efron and R. Tibshirani. 1993. An introduction to the bootstrap. Chapman &amp; Hall CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2011" citStr="Haghighi and Klein, 2009" startWordPosition="304" endWordPosition="307">multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, other anaphora, definite NP reference, and pronoun resolution, computing semantic compatibility via Web-hits and counts from large corpora. There is also work on end-to-end coreference resolution that uses large noun-similarity lists (Daum´e III and Marcu, 2005) or structured knowledge bases such as Wikipedia (Yang and Su, 2007; Haghighi and Klein, 2009; Kobdani et al., 2011) and YAGO (Rahman and Ng, 2011). However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffuse and indirect, but often</context>
<context position="12747" citStr="Haghighi and Klein, 2009" startWordPosition="2054" endWordPosition="2057">nt amount of noise. Also, we do not constrain the order of h1 and h2 because these patterns can hold for either direction of coreference.4 As a real example from our development set, the c12 count for the headword pair (leader, president) is 752, while for (voter, president), it is 0. Hypernymic semantic compatibility for coreference is intuitive and has been explored in varying forms by previous work. Poesio et al. (2004) and Markert and Nissim (2005) employ a subset of our Hearst patterns and Web-hits for the subtasks of bridging anaphora, other-anaphora, and definite NP resolution. Others (Haghighi and Klein, 2009; Rahman and Ng, 2011; Daum´e III and Marcu, 2005) use similar relations to extract compatibility statistics from Wikipedia, YAGO, and noun-similarity lists. Yang and Su (2007) use Wikipedia to automatically extract semantic patterns, which are then used as features in a learning setup. Instead of extracting patterns from the training data, we use all the above patterns, which helps us generalize to new datasets for end-to-end coreference resolution (see Section 4.3). 3.3 Entity-based context For each headword h, we first collect context seeds y using the pattern h {is |are |was |were} {a |an </context>
<context position="24301" citStr="Haghighi and Klein (2009)" startWordPosition="4014" endWordPosition="4017">ing that they are not taking advantage of any bias of either metric. The incremental improvements also show that each Web feature type brings in some additional benefit over the information already present in the Reconcile baseline, which includes alias, animacy, named entity, and WordNet 9Moreover, a DT classifier takes roughly the same amount of time and memory as a perceptron on our ACE04 development experiments. It is, however, slower and more memory-intensive (-3x) on the bigger ACE05-ALL dataset. 394 MUC B3 System P R F1 P R F1 ACE04-TEST-RESULTS Stoyanov et al. (2009) - - 62.0 - - 76.5 Haghighi and Klein (2009) 67.5 61.6 64.4 77.4 69.4 73.2 Haghighi and Klein (2010) 67.4 66.6 67.0 81.2 73.3 77.0 This Work: Perceptron Baseline 65.5 61.9 63.7 84.1 70.9 77.0 This Work: DT Baseline 76.0 60.7 67.5 89.6 70.3 78.8 This Work: DT + Web Features 74.8 64.2 69.1 87.5 73.7 80.0 This Work: A of DT+Web over DT (p &lt; 0.05) 1.7 (p &lt; 0.005) 1.3 ACE05-TEST-RESULTS Stoyanov et al. (2009) - - 67.4 - - 73.7 Haghighi and Klein (2009) 73.1 58.8 65.2 82.1 63.9 71.8 Haghighi and Klein (2010) 74.6 62.7 68.1 83.2 68.4 75.1 This Work: Perceptron Baseline 72.2 61.6 66.5 85.0 65.5 73.9 This Work: DT Baseline 79.6 59.7 68.2 89.4 64</context>
<context position="28053" citStr="Haghighi and Klein (2009)" startWordPosition="4644" endWordPosition="4647">cluding 11All improvements are significant, except on the small ACE05 dataset with the MUC metric (where it is weak, at p &lt; 0.12). However, on the larger version of this dataset, ACE05-ALL, we get improvements which are both larger and more significant (at p &lt; 0.001). 395 Wikipedia, and their results are state-of-the-art in coreference resolution. We outperform their system on most datasets and metrics (except on ACE05- ALL for the MUC metric). The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al. (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). We develop our features and tune their hyperparameter values on the ACE04 development set and then use these on the ACE04 test set.12 On the ACE05 and ACE05-ALL datasets, we directly transfer our Web features and their hyper-parameter values from the ACE04 dev-set, without any retuning. The test improvements we get on all the datasets (see Table 3) suggest that our features are generally useful across datasets and metrics.13 5 Analysis In this section, we briefly discuss errors (in the DT baseline) corrected by our Web features, and anal</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="2166" citStr="Haghighi and Klein (2010)" startWordPosition="329" endWordPosition="332">m, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, other anaphora, definite NP reference, and pronoun resolution, computing semantic compatibility via Web-hits and counts from large corpora. There is also work on end-to-end coreference resolution that uses large noun-similarity lists (Daum´e III and Marcu, 2005) or structured knowledge bases such as Wikipedia (Yang and Su, 2007; Haghighi and Klein, 2009; Kobdani et al., 2011) and YAGO (Rahman and Ng, 2011). However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffuse and indirect, but often robust, cues to reference. For example, we can collect the cooccurrence statistics of an anaphor with various candidate antecedents to judge relative surf</context>
<context position="4513" citStr="Haghighi and Klein (2010)" startWordPosition="691" endWordPosition="694">f the mention-pair model of the Reconcile system (Stoyanov et al., 2009; Stoyanov et al., 2010) using a decision tree (DT) as its pairwise classifier. To this baseline system, we add our suite of features in turn, each class of features providing substantial gains. Altogether, our final system produces the best numbers reported to date on end-to-end coreference resolution (with automatically detected system mentions) on multiple data sets (ACE 2004 and ACE 2005) and metrics (MUC and B3), achieving significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). 2 Baseline System Before describing our semantic Web features, we first describe our baseline. The core inference and features come from the Reconcile package (Stoyanov et al., 2009; Stoyanov et al., 2010), with modifications described below. Our baseline differs most substantially from Stoyanov et al. (2009) in using a decision tree classifier rather than an averaged linear perceptron. 2.1 Reconcile Reconcile is one of the best implementations of the mention-pair model (Soon et al., 2001) of coreference resolution. The mention-pair model relies on a pairwise function to determine whether or</context>
<context position="20707" citStr="Haghighi and Klein (2010)" startWordPosition="3407" endWordPosition="3410">total number of documents; dev: the train/test split during development; test: the train/test split during testing; ment: the number of gold mentions in the test split; chn: the number of coreference chains in the test split. 4 Experiments 4.1 Data We show results on three popular and comparatively larger coreference resolution data sets – the ACE04, ACE05, and ACE05-ALL datasets from the ACE Program (NIST, 2004). In ACE04 and ACE05, we have only the newswire portion (of the original ACE 2004 and 2005 training sets) and use the standard train/test splits reported in Stoyanov et al. (2009) and Haghighi and Klein (2010). In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). Note that most previous work does not report (or need) a standard development set; hence, for tuning our features and its hyper-parameters, we randomly split the original training data into a training and development set with a 70/30 ratio (and then use the full original training set during testing). Details of the corpora are shown in Table 1.7 Details of the Web-scale corpora used for extracting features are discussed in Section 3. 4.2 </context>
<context position="24357" citStr="Haghighi and Klein (2010)" startWordPosition="4024" endWordPosition="4027">ther metric. The incremental improvements also show that each Web feature type brings in some additional benefit over the information already present in the Reconcile baseline, which includes alias, animacy, named entity, and WordNet 9Moreover, a DT classifier takes roughly the same amount of time and memory as a perceptron on our ACE04 development experiments. It is, however, slower and more memory-intensive (-3x) on the bigger ACE05-ALL dataset. 394 MUC B3 System P R F1 P R F1 ACE04-TEST-RESULTS Stoyanov et al. (2009) - - 62.0 - - 76.5 Haghighi and Klein (2009) 67.5 61.6 64.4 77.4 69.4 73.2 Haghighi and Klein (2010) 67.4 66.6 67.0 81.2 73.3 77.0 This Work: Perceptron Baseline 65.5 61.9 63.7 84.1 70.9 77.0 This Work: DT Baseline 76.0 60.7 67.5 89.6 70.3 78.8 This Work: DT + Web Features 74.8 64.2 69.1 87.5 73.7 80.0 This Work: A of DT+Web over DT (p &lt; 0.05) 1.7 (p &lt; 0.005) 1.3 ACE05-TEST-RESULTS Stoyanov et al. (2009) - - 67.4 - - 73.7 Haghighi and Klein (2009) 73.1 58.8 65.2 82.1 63.9 71.8 Haghighi and Klein (2010) 74.6 62.7 68.1 83.2 68.4 75.1 This Work: Perceptron Baseline 72.2 61.6 66.5 85.0 65.5 73.9 This Work: DT Baseline 79.6 59.7 68.2 89.4 64.2 74.7 This Work: DT + Web Features 75.0 64.7 69.5 81.1</context>
<context position="27246" citStr="Haghighi and Klein (2010)" startWordPosition="4518" endWordPosition="4521">results (the 3 rows marked ‘This Work’) on the perceptron baseline, the DT baseline, and the Web features added to the DT baseline. We also report statistical significance of the improvements from the Web fea10We also initially experimented with smaller datasets (MUC6 and MUC7) and an averaged perceptron baseline, and we did see similar improvements, arguing that these features are useful independently of the learning algorithm and dataset. tures on the DT baseline.11 For significance testing, we use the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1993). Our main comparison is against Haghighi and Klein (2010), a mostly-unsupervised generative approach that models latent entity types, which generate specific entities that in turn render individual mentions. They learn on large datasets including 11All improvements are significant, except on the small ACE05 dataset with the MUC metric (where it is weak, at p &lt; 0.12). However, on the larger version of this dataset, ACE05-ALL, we get improvements which are both larger and more significant (at p &lt; 0.001). 395 Wikipedia, and their results are state-of-the-art in coreference resolution. We outperform their system on most datasets and metrics (except on A</context>
<context position="29721" citStr="Haghighi and Klein (2010)" startWordPosition="4926" endWordPosition="4929">Some examples of such pairs include: Iran ; the country the EPA ; the agency athletic director; Mulcahy Democrat Al Gore ; the vice president 12Note that for the ACE04 dataset only, we use the ‘SmartInstanceGenerator’ (SIG) filter of Reconcile that uses only a filtered set of mention-pairs (based on distance and other properties of the pair) instead of the ‘AllPairs’ (AP) setting that uses all pairs of mentions, and makes training and tuning very slow. 13For the ACE05 and ACE05-ALL datasets, we revert to the ‘AllPairs’ (AP) setting of Reconcile because this gives us baselines competitive with Haghighi and Klein (2010). Since we did not need to retune on these datasets, training and tuning speed were not a bottleneck. Moreover, the improvements from our Web features are similar even when tried over the SIG baseline; hence, the filter choice doesn’t affect the performance gain from the Web features. Barry Bonds ; the best baseball player Vojislav Kostunica ; the pro-democracy leader its closest rival ; the German magazine Das Motorrad One of those difficult-to-dislodge judges; John Marshall These pairs are cases where our features on Hearst-style co-occurrence and entity-based context-match are informative a</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="6421" citStr="Hall et al., 2009" startWordPosition="978" endWordPosition="981">enerated annotations such as named entity tags, syntactic parses, and WordNet classes, inspired by Soon et al. (2001), Ng and Cardie (2002), and Bengtson and Roth (2008). The Reconcile system also facilitates standardized empirical evaluation to past work.1 In this paper, we develop a suite of simple semantic Web features based on pairs of mention headwords which stack with the default Reconcile features to surpass past state-of-the-art results. 2.2 Decision Tree Classifier Among the various learning algorithms that Reconcile supports, we chose the decision tree classifier, available in Weka (Hall et al., 2009) as J48, an open source Java implementation of the C4.5 algorithm of Quinlan (1993). The C4.5 algorithm builds decision trees by incrementally maximizing information gain. The training data is a set of already classified samples, where each sample is a vector of attributes or features. At each node of the tree, C4.5 splits the data on an attribute that most effectively splits its set of samples into more ordered subsets, and then recurses on these smaller subsets. The decision tree can then be used to classify a new sample by following a path from the root downward based on the attribute value</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="15003" citStr="Harris (1954)" startWordPosition="2442" endWordPosition="2443">sts matches for the headword pair. We first collect the POS tags (using length 2 character prefixes to indicate coarse parts of speech) of the seeds matched in the top k� seed lists of the two headwords, where k� is another hyperparameter to tune. If the dominant tags match and are in a small list of important tags ({JJ, NN, RB, VB}), we fire an indicator feature specifying the matched tag, otherwise we fire a nomatch indicator. To obtain POS tags for the seeds, we use a unigram-based POS tagger trained on the WSJ treebank training set. 3.4 Cluster information The distributional hypothesis of Harris (1954) says that words that occur in similar contexts tend to have a similar linguistic behavior. Here, we design features with the idea that this hypothesis extends to reference: mentions occurring in similar contexts in large document sets such as the Web tend to be compatible for coreference. Instead of collecting the contexts of each mention and creating sparse features from them, we use Web-scale distributional clustering to summarize compatibility. Specifically, we begin with the phrase-based clusters from Lin et al. (2010), which were created using the Google n-grams V2 corpus. These clusters</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="11203" citStr="Hearst, 1992" startWordPosition="1780" endWordPosition="1781">, the values are -10.9 and -12.0, respectively. These kinds of general Web co-occurrence statistics have been used previously for other supervised NLP tasks such as spelling correction and syntactic parsing (Bergsma et al., 2010; Bansal and Klein, 2011). In coreference, similar word-association scores were used by Kobdani et al. (2011), but from Wikipedia and for self-training. 3.2 Hearst co-occurrence These features capture templated co-occurrence of the two headwords h1 and h2 in the Web-corpus. Here, we only collect statistics of the headwords cooccurring with a generalized Hearst pattern (Hearst, 1992) in between. Hearst patterns capture various lexical semantic relations between items. For example, seeing X is a Y or X and other Y indicates hypernymy and also tends to cue coreference. The specific patterns we use are: • h1 {is |are |was |were} {a |an |the}? h2 • h1 {and |or} {other |the other |another} h2 • h1 other than {a |an |the}? h2 3We also tried adding count(“h1 h2&amp;quot;) to c12 but this decreases performance, perhaps because truly adjacent occurrences are often not grammatical. Cbin log10 391 • h1 such as {a |an |the}? h2 • h1 , including {a |an |the}? h2 • h1 , especially {a |an |the}?</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamidreza Kobdani</author>
<author>Hinrich Schutze</author>
<author>Michael Schiehlen</author>
<author>Hans Kamp</author>
</authors>
<title>Bootstrapping coreference resolution using word associations.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2034" citStr="Kobdani et al., 2011" startWordPosition="308" endWordPosition="311">that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, other anaphora, definite NP reference, and pronoun resolution, computing semantic compatibility via Web-hits and counts from large corpora. There is also work on end-to-end coreference resolution that uses large noun-similarity lists (Daum´e III and Marcu, 2005) or structured knowledge bases such as Wikipedia (Yang and Su, 2007; Haghighi and Klein, 2009; Kobdani et al., 2011) and YAGO (Rahman and Ng, 2011). However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffuse and indirect, but often robust, cues to refere</context>
<context position="10927" citStr="Kobdani et al. (2011)" startWordPosition="1737" endWordPosition="1740">that fires is an indicator of which quantized bin the query produced. As a real example from our development set, the cooccurrence count c12 for the headword pair (leader, president) is 11383, while it is only 95 for the headword pair (voter, president); after normalization and log10, the values are -10.9 and -12.0, respectively. These kinds of general Web co-occurrence statistics have been used previously for other supervised NLP tasks such as spelling correction and syntactic parsing (Bergsma et al., 2010; Bansal and Klein, 2011). In coreference, similar word-association scores were used by Kobdani et al. (2011), but from Wikipedia and for self-training. 3.2 Hearst co-occurrence These features capture templated co-occurrence of the two headwords h1 and h2 in the Web-corpus. Here, we only collect statistics of the headwords cooccurring with a generalized Hearst pattern (Hearst, 1992) in between. Hearst patterns capture various lexical semantic relations between items. For example, seeing X is a Y or X and other Y indicates hypernymy and also tends to cue coreference. The specific patterns we use are: • h1 {is |are |was |were} {a |an |the}? h2 • h1 {and |or} {other |the other |another} h2 • h1 other th</context>
<context position="22819" citStr="Kobdani et al., 2011" startWordPosition="3772" endWordPosition="3775"> Hearst 80.0 62.3 70.0 89.1 70.1 78.5 + Entity 79.4 63.2 70.4 88.1 70.9 78.6 + Cluster 79.5 63.6 70.7 87.9 71.2 78.6 + Pronoun 79.9 64.3 71.3 88.0 71.6 79.0 Table 2: Incremental results for the Web features on the ACE04 development set. AvgPerc is the averaged perceptron baseline, DecTree is the decision tree baseline, and the +Feature rows show the effect of adding a particular feature incrementally (not in isolation) to the DecTree baseline. The feature categories correspond to those described in Section 3. and gold cluster, respectively. It is well known (Recasens and Hovy, 2010; Ng, 2010; Kobdani et al., 2011) that MUC is biased towards large clusters (chains) whereas B3 is biased towards singleton clusters. Therefore, for a more balanced evaluation, we show improvements on both metrics simultaneously. 4.3 Results We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al. (2009).9 Table 2 compares the baseline perceptron results to the DT results and then shows the incremental addition of the Web features to the DT baseline (on the ACE04 development set). T</context>
</contexts>
<marker>Kobdani, Schutze, Schiehlen, Kamp, 2011</marker>
<rawString>Hamidreza Kobdani, Hinrich Schutze, Michael Schiehlen, and Hans Kamp. 2011. Bootstrapping coreference resolution using word associations. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="16788" citStr="Koo et al., 2008" startWordPosition="2740" endWordPosition="2743">n in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. However, we also assume that higher-ranked matches tend to imply closer meanings. To this end, we fire a feature indicating the value bin(i+j), where i and j are the earliest match positions in the cluster id lists of h1 and h2. Binning here means that match positions in a close range generally trigger the same feature. Recent previous work has used clustering information to improve the performance of supervised NLP tasks such as NER and dependency parsing (Koo et al., 2008; Lin and Wu, 2009). However, in coreference, the only related work to our knowledge is from Daum´e III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al. (2005). 3.5 Pronoun context Our last feature category specifically addresses pronoun reference, for cases when the anaphoric mention NP2 (and hence its headword h2) is a pronoun, while the candidate antecedent mention NP1 (and hence its headword h1) is not. For such a headword pair (h1, h2), the idea is to substitute the nonpronoun h1 into h2’s position and see whether</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Xiaoyun Wu</author>
</authors>
<title>Phrase clustering for discriminative learning.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="16238" citStr="Lin and Wu (2009)" startWordPosition="2646" endWordPosition="2649">utional K-Means clustering (with K = 1000) on phrases, using the n-gram context as features. The cluster data contains almost 10 million phrases and their soft cluster memberships. Up to twenty cluster ids with the highest centroid similarities are included for each phrase in this dataset 392 (Lin et al., 2010). Our cluster-based features assume that if the headwords of the two mentions have matches in their cluster id lists, then they are more compatible for coreference. We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. However, we also assume that higher-ranked matches tend to imply closer meanings. To this end, we fire a feature indicating the value bin(i+j), where i and j are the earliest match positions in the cluster id lists of h1 and h2. Binning here means that match positions in a close range generally trigger the same feature. Recent previous work has used clustering information to improve the performance of supervised NLP tasks such as NER and dependency parsing (Koo et al., 2008; Lin and Wu, 2009). However, in coreference, the </context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering for discriminative learning. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Kenneth Church</author>
<author>Heng Ji</author>
<author>Satoshi Sekine</author>
<author>David Yarowsky</author>
<author>Shane Bergsma</author>
</authors>
<title>New tools for web-scale n-grams.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<institution>Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant</institution>
<contexts>
<context position="8666" citStr="Lin et al. (2010)" startWordPosition="1362" endWordPosition="1365">e various Web-count functions on it that can signal whether or not this mention pair is coreferent. As the source of Web information, we use the Google n-grams corpus (Brants and Franz, 2006) which contains English n-grams (n = 1 to 5) and their Web frequency counts, derived from nearly 1 trillion word tokens and 95 billion sentences. Because we have many queries that must be run against this corpus, we apply the trie-based hashing algorithm of Bansal and Klein (2011) to efficiently answer all of them in one pass over it. The features that require word clusters (Section 3.4) use the output of Lin et al. (2010).2 We describe our five types of features in turn. The first four types are most intuitive for mention pairs where both members are non-pronominal, but, aside from the general co-occurrence group, helped for all mention pair types. The fifth feature group applies only to pairs in which the anaphor is a pronoun but the antecedent is a non-pronoun. Related work for each feature category is discussed inline. 3.1 General co-occurrence These features capture co-occurrence statistics of the two headwords, i.e., how often h1 and h2 are seen adjacent or nearly adjacent on the Web. This count can be a </context>
<context position="15532" citStr="Lin et al. (2010)" startWordPosition="2524" endWordPosition="2527">ank training set. 3.4 Cluster information The distributional hypothesis of Harris (1954) says that words that occur in similar contexts tend to have a similar linguistic behavior. Here, we design features with the idea that this hypothesis extends to reference: mentions occurring in similar contexts in large document sets such as the Web tend to be compatible for coreference. Instead of collecting the contexts of each mention and creating sparse features from them, we use Web-scale distributional clustering to summarize compatibility. Specifically, we begin with the phrase-based clusters from Lin et al. (2010), which were created using the Google n-grams V2 corpus. These clusters come from distributional K-Means clustering (with K = 1000) on phrases, using the n-gram context as features. The cluster data contains almost 10 million phrases and their soft cluster memberships. Up to twenty cluster ids with the highest centroid similarities are included for each phrase in this dataset 392 (Lin et al., 2010). Our cluster-based features assume that if the headwords of the two mentions have matches in their cluster id lists, then they are more compatible for coreference. We check the match of not just the</context>
</contexts>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, 2010</marker>
<rawString>Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale. 2010. New tools for web-scale n-grams. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Comparing knowledge sources for nominal anaphora resolution.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="1548" citStr="Markert and Nissim, 2005" startWordPosition="235" endWordPosition="238">ussed the economy, technology, and education. His election campaign is expected to [...] For resolving coreference in this example, a system would benefit from the world knowledge that Obama is the president. Also, to resolve the pronoun his to the correct antecedent Obama, we can use the knowledge that Obama has an election campaign while Jobs does not. Such ambiguities are difficult to resolve on purely syntactic or configurational grounds. There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, other anaphora, definite NP reference, and pronoun resolution, computing semantic compatibility via Web-hits and counts from large corpora. There is also work on end-to-end coreference resolution that uses large noun-similarity lists (Daum´e III and Marcu, 2005) or structured knowledge bases such as Wikipedia (Yang and Su, 2007; Haghighi and Klein, 2009; Kobdani et al., 2011) and YAGO (Rahman and Ng, 2011). However, such structured knowledge bases are of limited scope, and, while Haghigh</context>
<context position="12579" citStr="Markert and Nissim (2005)" startWordPosition="2029" endWordPosition="2032">ne of the above patterns occurs in between the headwords. We did not allow wildcards in between the headwords and the Hearst-patterns because this introduced a significant amount of noise. Also, we do not constrain the order of h1 and h2 because these patterns can hold for either direction of coreference.4 As a real example from our development set, the c12 count for the headword pair (leader, president) is 752, while for (voter, president), it is 0. Hypernymic semantic compatibility for coreference is intuitive and has been explored in varying forms by previous work. Poesio et al. (2004) and Markert and Nissim (2005) employ a subset of our Hearst patterns and Web-hits for the subtasks of bridging anaphora, other-anaphora, and definite NP resolution. Others (Haghighi and Klein, 2009; Rahman and Ng, 2011; Daum´e III and Marcu, 2005) use similar relations to extract compatibility statistics from Wikipedia, YAGO, and noun-similarity lists. Yang and Su (2007) use Wikipedia to automatically extract semantic patterns, which are then used as features in a learning setup. Instead of extracting patterns from the training data, we use all the above patterns, which helps us generalize to new datasets for end-to-end c</context>
</contexts>
<marker>Markert, Nissim, 2005</marker>
<rawString>Katja Markert and Malvina Nissim. 2005. Comparing knowledge sources for nominal anaphora resolution. Computational Linguistics, 31(3):367–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="5942" citStr="Ng and Cardie (2002)" startWordPosition="902" endWordPosition="905">es could be adapted to entity-mention systems, their current form was most directly applicable to the mention-pair approach, making Reconcile a particularly well-suited platform for this investigation. The Reconcile system provides baseline features, learning mechanisms, and resolution procedures that already achieve near state-of-the-art results on multiple popular datasets using multiple standard metrics. It includes over 80 core features that exploit various automatically generated annotations such as named entity tags, syntactic parses, and WordNet classes, inspired by Soon et al. (2001), Ng and Cardie (2002), and Bengtson and Roth (2008). The Reconcile system also facilitates standardized empirical evaluation to past work.1 In this paper, we develop a suite of simple semantic Web features based on pairs of mention headwords which stack with the default Reconcile features to surpass past state-of-the-art results. 2.2 Decision Tree Classifier Among the various learning algorithms that Reconcile supports, we chose the decision tree classifier, available in Weka (Hall et al., 2009) as J48, an open source Java implementation of the C4.5 algorithm of Quinlan (1993). The C4.5 algorithm builds decision t</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Supervised noun phrase coreference research: The first fifteen years.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="22796" citStr="Ng, 2010" startWordPosition="3770" endWordPosition="3771">9.8 78.1 + Hearst 80.0 62.3 70.0 89.1 70.1 78.5 + Entity 79.4 63.2 70.4 88.1 70.9 78.6 + Cluster 79.5 63.6 70.7 87.9 71.2 78.6 + Pronoun 79.9 64.3 71.3 88.0 71.6 79.0 Table 2: Incremental results for the Web features on the ACE04 development set. AvgPerc is the averaged perceptron baseline, DecTree is the decision tree baseline, and the +Feature rows show the effect of adding a particular feature incrementally (not in isolation) to the DecTree baseline. The feature categories correspond to those described in Section 3. and gold cluster, respectively. It is well known (Recasens and Hovy, 2010; Ng, 2010; Kobdani et al., 2011) that MUC is biased towards large clusters (chains) whereas B3 is biased towards singleton clusters. Therefore, for a more balanced evaluation, we show improvements on both metrics simultaneously. 4.3 Results We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al. (2009).9 Table 2 compares the baseline perceptron results to the DT results and then shows the incremental addition of the Web features to the DT baseline (on the AC</context>
</contexts>
<marker>Ng, 2010</marker>
<rawString>Vincent Ng. 2010. Supervised noun phrase coreference research: The first fifteen years. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The ACE evaluation plan. In</title>
<date>2004</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="20498" citStr="NIST, 2004" startWordPosition="3373" endWordPosition="3374">mon context. 393 Dataset docs dev test ment chn ACE04 128 63/27 90/38 3037 1332 ACE05 81 40/17 57/24 1991 775 ACE05-ALL 599 337/145 482/117 9217 3050 Table 1: Dataset characteristics – docs: the total number of documents; dev: the train/test split during development; test: the train/test split during testing; ment: the number of gold mentions in the test split; chn: the number of coreference chains in the test split. 4 Experiments 4.1 Data We show results on three popular and comparatively larger coreference resolution data sets – the ACE04, ACE05, and ACE05-ALL datasets from the ACE Program (NIST, 2004). In ACE04 and ACE05, we have only the newswire portion (of the original ACE 2004 and 2005 training sets) and use the standard train/test splits reported in Stoyanov et al. (2009) and Haghighi and Klein (2010). In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). Note that most previous work does not report (or need) a standard development set; hence, for tuning our features and its hyper-parameters, we randomly split the original training data into a training and development set with a 70/30</context>
</contexts>
<marker>NIST, 2004</marker>
<rawString>NIST. 2004. The ACE evaluation plan. In NIST. E.W. Noreen. 1989. Computer intensive methods for hypothesis testing: An introduction. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Rahul Mehta</author>
<author>Axel Maroudas</author>
<author>Janet Hitzeman</author>
</authors>
<title>Learning to resolve bridging references.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1522" citStr="Poesio et al., 2004" startWordPosition="231" endWordPosition="234">s, the president discussed the economy, technology, and education. His election campaign is expected to [...] For resolving coreference in this example, a system would benefit from the world knowledge that Obama is the president. Also, to resolve the pronoun his to the correct antecedent Obama, we can use the knowledge that Obama has an election campaign while Jobs does not. Such ambiguities are difficult to resolve on purely syntactic or configurational grounds. There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, other anaphora, definite NP reference, and pronoun resolution, computing semantic compatibility via Web-hits and counts from large corpora. There is also work on end-to-end coreference resolution that uses large noun-similarity lists (Daum´e III and Marcu, 2005) or structured knowledge bases such as Wikipedia (Yang and Su, 2007; Haghighi and Klein, 2009; Kobdani et al., 2011) and YAGO (Rahman and Ng, 2011). However, such structured knowledge bases are of limited</context>
<context position="12549" citStr="Poesio et al. (2004)" startWordPosition="2024" endWordPosition="2027">ricted to n-grams where one of the above patterns occurs in between the headwords. We did not allow wildcards in between the headwords and the Hearst-patterns because this introduced a significant amount of noise. Also, we do not constrain the order of h1 and h2 because these patterns can hold for either direction of coreference.4 As a real example from our development set, the c12 count for the headword pair (leader, president) is 752, while for (voter, president), it is 0. Hypernymic semantic compatibility for coreference is intuitive and has been explored in varying forms by previous work. Poesio et al. (2004) and Markert and Nissim (2005) employ a subset of our Hearst patterns and Web-hits for the subtasks of bridging anaphora, other-anaphora, and definite NP resolution. Others (Haghighi and Klein, 2009; Rahman and Ng, 2011; Daum´e III and Marcu, 2005) use similar relations to extract compatibility statistics from Wikipedia, YAGO, and noun-similarity lists. Yang and Su (2007) use Wikipedia to automatically extract semantic patterns, which are then used as features in a learning setup. Instead of extracting patterns from the training data, we use all the above patterns, which helps us generalize to</context>
</contexts>
<marker>Poesio, Mehta, Maroudas, Hitzeman, 2004</marker>
<rawString>Massimo Poesio, Rahul Mehta, Axel Maroudas, and Janet Hitzeman. 2004. Learning to resolve bridging references. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for machine learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="6504" citStr="Quinlan (1993)" startWordPosition="994" endWordPosition="995">inspired by Soon et al. (2001), Ng and Cardie (2002), and Bengtson and Roth (2008). The Reconcile system also facilitates standardized empirical evaluation to past work.1 In this paper, we develop a suite of simple semantic Web features based on pairs of mention headwords which stack with the default Reconcile features to surpass past state-of-the-art results. 2.2 Decision Tree Classifier Among the various learning algorithms that Reconcile supports, we chose the decision tree classifier, available in Weka (Hall et al., 2009) as J48, an open source Java implementation of the C4.5 algorithm of Quinlan (1993). The C4.5 algorithm builds decision trees by incrementally maximizing information gain. The training data is a set of already classified samples, where each sample is a vector of attributes or features. At each node of the tree, C4.5 splits the data on an attribute that most effectively splits its set of samples into more ordered subsets, and then recurses on these smaller subsets. The decision tree can then be used to classify a new sample by following a path from the root downward based on the attribute values of the sample. We find the decision tree classifier to work better than the defau</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. R. Quinlan. 1993. C4.5: Programs for machine learning. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Supervised models for coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="20833" citStr="Rahman and Ng (2009)" startWordPosition="3429" endWordPosition="3432">r of gold mentions in the test split; chn: the number of coreference chains in the test split. 4 Experiments 4.1 Data We show results on three popular and comparatively larger coreference resolution data sets – the ACE04, ACE05, and ACE05-ALL datasets from the ACE Program (NIST, 2004). In ACE04 and ACE05, we have only the newswire portion (of the original ACE 2004 and 2005 training sets) and use the standard train/test splits reported in Stoyanov et al. (2009) and Haghighi and Klein (2010). In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). Note that most previous work does not report (or need) a standard development set; hence, for tuning our features and its hyper-parameters, we randomly split the original training data into a training and development set with a 70/30 ratio (and then use the full original training set during testing). Details of the corpora are shown in Table 1.7 Details of the Web-scale corpora used for extracting features are discussed in Section 3. 4.2 Evaluation Metrics We evaluated our work on both MUC (Vilain et al., 1995) and B3 (Bagga and Baldwin, 1998). Both scorers are </context>
<context position="25071" citStr="Rahman and Ng (2009)" startWordPosition="4160" endWordPosition="4163">is Work: DT Baseline 76.0 60.7 67.5 89.6 70.3 78.8 This Work: DT + Web Features 74.8 64.2 69.1 87.5 73.7 80.0 This Work: A of DT+Web over DT (p &lt; 0.05) 1.7 (p &lt; 0.005) 1.3 ACE05-TEST-RESULTS Stoyanov et al. (2009) - - 67.4 - - 73.7 Haghighi and Klein (2009) 73.1 58.8 65.2 82.1 63.9 71.8 Haghighi and Klein (2010) 74.6 62.7 68.1 83.2 68.4 75.1 This Work: Perceptron Baseline 72.2 61.6 66.5 85.0 65.5 73.9 This Work: DT Baseline 79.6 59.7 68.2 89.4 64.2 74.7 This Work: DT + Web Features 75.0 64.7 69.5 81.1 70.8 75.6 This Work: A of DT+Web over DT (p &lt; 0.12) 1.3 (p &lt; 0.1) 0.9 ACE05-ALL-TEST-RESULTS Rahman and Ng (2009) 75.4 64.1 69.3 54.4 70.5 61.4 Haghighi and Klein (2009) 72.9 60.2 67.0 53.2 73.1 61.6 Haghighi and Klein (2010) 77.0 66.9 71.6 55.4 74.8 63.8 This Work: Perceptron Baseline 68.9 60.4 64.4 80.6 60.5 69.1 This Work: DT Baseline 78.0 60.4 68.1 85.1 60.4 70.6 This Work: DT + Web Features 77.6 64.0 70.2 80.7 65.9 72.5 This Work: A of DT+Web over DT (p &lt; 0.001) 2.1 (p &lt; 0.001) 1.9 Table 3: Primary test results on the ACE04, ACE05, and ACE05-ALL datasets. All systems reported here use automatically extracted system mentions. B3 here is the B3All version of Stoyanov et al. (2009). We also report stat</context>
<context position="28108" citStr="Rahman and Ng (2009)" startWordPosition="4653" endWordPosition="4656">all ACE05 dataset with the MUC metric (where it is weak, at p &lt; 0.12). However, on the larger version of this dataset, ACE05-ALL, we get improvements which are both larger and more significant (at p &lt; 0.001). 395 Wikipedia, and their results are state-of-the-art in coreference resolution. We outperform their system on most datasets and metrics (except on ACE05- ALL for the MUC metric). The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al. (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). We develop our features and tune their hyperparameter values on the ACE04 development set and then use these on the ACE04 test set.12 On the ACE05 and ACE05-ALL datasets, we directly transfer our Web features and their hyper-parameter values from the ACE04 dev-set, without any retuning. The test improvements we get on all the datasets (see Table 3) suggest that our features are generally useful across datasets and metrics.13 5 Analysis In this section, we briefly discuss errors (in the DT baseline) corrected by our Web features, and analyze the decision tree classifier built during training </context>
</contexts>
<marker>Rahman, Ng, 2009</marker>
<rawString>Altaf Rahman and Vincent Ng. 2009. Supervised models for coreference resolution. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Coreference resolution with world knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2065" citStr="Rahman and Ng, 2011" startWordPosition="314" endWordPosition="317">rld knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, other anaphora, definite NP reference, and pronoun resolution, computing semantic compatibility via Web-hits and counts from large corpora. There is also work on end-to-end coreference resolution that uses large noun-similarity lists (Daum´e III and Marcu, 2005) or structured knowledge bases such as Wikipedia (Yang and Su, 2007; Haghighi and Klein, 2009; Kobdani et al., 2011) and YAGO (Rahman and Ng, 2011). However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffuse and indirect, but often robust, cues to reference. For example, we can collec</context>
<context position="12768" citStr="Rahman and Ng, 2011" startWordPosition="2058" endWordPosition="2062">we do not constrain the order of h1 and h2 because these patterns can hold for either direction of coreference.4 As a real example from our development set, the c12 count for the headword pair (leader, president) is 752, while for (voter, president), it is 0. Hypernymic semantic compatibility for coreference is intuitive and has been explored in varying forms by previous work. Poesio et al. (2004) and Markert and Nissim (2005) employ a subset of our Hearst patterns and Web-hits for the subtasks of bridging anaphora, other-anaphora, and definite NP resolution. Others (Haghighi and Klein, 2009; Rahman and Ng, 2011; Daum´e III and Marcu, 2005) use similar relations to extract compatibility statistics from Wikipedia, YAGO, and noun-similarity lists. Yang and Su (2007) use Wikipedia to automatically extract semantic patterns, which are then used as features in a learning setup. Instead of extracting patterns from the training data, we use all the above patterns, which helps us generalize to new datasets for end-to-end coreference resolution (see Section 4.3). 3.3 Entity-based context For each headword h, we first collect context seeds y using the pattern h {is |are |was |were} {a |an |the}? y taking seeds</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011. Coreference resolution with world knowledge. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized algorithms and NLP: Using locality sensitive hash functions for high speed noun clustering.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="17023" citStr="Ravichandran et al. (2005)" startWordPosition="2780" endWordPosition="2783">is end, we fire a feature indicating the value bin(i+j), where i and j are the earliest match positions in the cluster id lists of h1 and h2. Binning here means that match positions in a close range generally trigger the same feature. Recent previous work has used clustering information to improve the performance of supervised NLP tasks such as NER and dependency parsing (Koo et al., 2008; Lin and Wu, 2009). However, in coreference, the only related work to our knowledge is from Daum´e III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al. (2005). 3.5 Pronoun context Our last feature category specifically addresses pronoun reference, for cases when the anaphoric mention NP2 (and hence its headword h2) is a pronoun, while the candidate antecedent mention NP1 (and hence its headword h1) is not. For such a headword pair (h1, h2), the idea is to substitute the nonpronoun h1 into h2’s position and see whether the result is attested on the Web. If the anaphoric pronominal mention is h2 and its sentential context is l’ l h2 r r’, then the substituted phrase will be l’ l h1 r r’.5 High Web counts of substituted phrases tend to indicate semant</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized algorithms and NLP: Using locality sensitive hash functions for high speed noun clustering. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>Coreference resolution across corpora: Languages, coding schemes, and preprocessing information.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="22786" citStr="Recasens and Hovy, 2010" startWordPosition="3766" endWordPosition="3769">occ 79.8 62.1 69.8 88.7 69.8 78.1 + Hearst 80.0 62.3 70.0 89.1 70.1 78.5 + Entity 79.4 63.2 70.4 88.1 70.9 78.6 + Cluster 79.5 63.6 70.7 87.9 71.2 78.6 + Pronoun 79.9 64.3 71.3 88.0 71.6 79.0 Table 2: Incremental results for the Web features on the ACE04 development set. AvgPerc is the averaged perceptron baseline, DecTree is the decision tree baseline, and the +Feature rows show the effect of adding a particular feature incrementally (not in isolation) to the DecTree baseline. The feature categories correspond to those described in Section 3. and gold cluster, respectively. It is well known (Recasens and Hovy, 2010; Ng, 2010; Kobdani et al., 2011) that MUC is biased towards large clusters (chains) whereas B3 is biased towards singleton clusters. Therefore, for a more balanced evaluation, we show improvements on both metrics simultaneously. 4.3 Results We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al. (2009).9 Table 2 compares the baseline perceptron results to the DT results and then shows the incremental addition of the Web features to the DT baseline </context>
</contexts>
<marker>Recasens, Hovy, 2010</marker>
<rawString>Marta Recasens and Eduard Hovy. 2010. Coreference resolution across corpora: Languages, coding schemes, and preprocessing information. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="5009" citStr="Soon et al., 2001" startWordPosition="768" endWordPosition="771">significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). 2 Baseline System Before describing our semantic Web features, we first describe our baseline. The core inference and features come from the Reconcile package (Stoyanov et al., 2009; Stoyanov et al., 2010), with modifications described below. Our baseline differs most substantially from Stoyanov et al. (2009) in using a decision tree classifier rather than an averaged linear perceptron. 2.1 Reconcile Reconcile is one of the best implementations of the mention-pair model (Soon et al., 2001) of coreference resolution. The mention-pair model relies on a pairwise function to determine whether or not two mentions are coreferent. Pairwise predictions are then consolidated by transitive closure (or some other clustering method) to form the final set of coreference clusters (chains). While our Web features could be adapted to entity-mention systems, their current form was most directly applicable to the mention-pair approach, making Reconcile a particularly well-suited platform for this investigation. The Reconcile system provides baseline features, learning mechanisms, and resolution </context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Nathan Gilbert</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
</authors>
<title>Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL/IJCNLP.</booktitle>
<contexts>
<context position="3959" citStr="Stoyanov et al., 2009" startWordPosition="604" endWordPosition="607"> (2) lexical relations (via Hearst-style hypernymy patterns), (3) similarity of entity-based context (e.g., common values of y for 389 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 389–398, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics which h is a y is attested), (4) matches of distributional soft cluster ids, and (5) attested substitutions of candidate antecedents in the context of a pronominal anaphor. We first describe a strong baseline consisting of the mention-pair model of the Reconcile system (Stoyanov et al., 2009; Stoyanov et al., 2010) using a decision tree (DT) as its pairwise classifier. To this baseline system, we add our suite of features in turn, each class of features providing substantial gains. Altogether, our final system produces the best numbers reported to date on end-to-end coreference resolution (with automatically detected system mentions) on multiple data sets (ACE 2004 and ACE 2005) and metrics (MUC and B3), achieving significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). 2 Baseline System Before describing our sema</context>
<context position="7158" citStr="Stoyanov et al. (2009)" startWordPosition="1107" endWordPosition="1111">sion trees by incrementally maximizing information gain. The training data is a set of already classified samples, where each sample is a vector of attributes or features. At each node of the tree, C4.5 splits the data on an attribute that most effectively splits its set of samples into more ordered subsets, and then recurses on these smaller subsets. The decision tree can then be used to classify a new sample by following a path from the root downward based on the attribute values of the sample. We find the decision tree classifier to work better than the default averaged perceptron (used by Stoyanov et al. (2009)), on multiple datasets using multiple metrics (see Section 4.3). Many advantages have been claimed for decision tree classifiers, including interpretability and robustness. However, we suspect that the aspect most relevant to our case is that decision trees can capture non-linear interactions between features. For example, recency is very important for pronoun reference but much less so for nominal reference. 3 Semantics via Web Features Our Web features for coreference resolution are simple and capture a range of diffuse world knowledge. Given a mention pair, we use the head finder in Reconc</context>
<context position="20677" citStr="Stoyanov et al. (2009)" startWordPosition="3402" endWordPosition="3405">haracteristics – docs: the total number of documents; dev: the train/test split during development; test: the train/test split during testing; ment: the number of gold mentions in the test split; chn: the number of coreference chains in the test split. 4 Experiments 4.1 Data We show results on three popular and comparatively larger coreference resolution data sets – the ACE04, ACE05, and ACE05-ALL datasets from the ACE Program (NIST, 2004). In ACE04 and ACE05, we have only the newswire portion (of the original ACE 2004 and 2005 training sets) and use the standard train/test splits reported in Stoyanov et al. (2009) and Haghighi and Klein (2010). In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). Note that most previous work does not report (or need) a standard development set; hence, for tuning our features and its hyper-parameters, we randomly split the original training data into a training and development set with a 70/30 ratio (and then use the full original training set during testing). Details of the corpora are shown in Table 1.7 Details of the Web-scale corpora used for extracting features ar</context>
<context position="21984" citStr="Stoyanov et al. (2009)" startWordPosition="3626" endWordPosition="3629">ilain et al., 1995) and B3 (Bagga and Baldwin, 1998). Both scorers are available in the Reconcile infrastructure.8 MUC measures how many predicted clusters need to be merged to cover the true gold clusters. B3 computes precision and recall for each mention by computing the intersection of its predicted and gold cluster and dividing by the size of the predicted 7Note that the development set is used only for ACE04, because for ACE05, and ACE05-ALL, we directly test using the features tuned on ACE04. 8Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al. (2009) for details). We use the B3All version, unless mentioned otherwise. Feature P MUC F1 P B3 F1 R R AvgPerc 69.0 63.1 65.9 82.2 69.9 75.5 DecTree 80.9 61.0 69.5 89.5 69.0 77.9 + Co-occ 79.8 62.1 69.8 88.7 69.8 78.1 + Hearst 80.0 62.3 70.0 89.1 70.1 78.5 + Entity 79.4 63.2 70.4 88.1 70.9 78.6 + Cluster 79.5 63.6 70.7 87.9 71.2 78.6 + Pronoun 79.9 64.3 71.3 88.0 71.6 79.0 Table 2: Incremental results for the Web features on the ACE04 development set. AvgPerc is the averaged perceptron baseline, DecTree is the decision tree baseline, and the +Feature rows show the effect of adding a particular feat</context>
<context position="23237" citStr="Stoyanov et al. (2009)" startWordPosition="3836" endWordPosition="3839">tion) to the DecTree baseline. The feature categories correspond to those described in Section 3. and gold cluster, respectively. It is well known (Recasens and Hovy, 2010; Ng, 2010; Kobdani et al., 2011) that MUC is biased towards large clusters (chains) whereas B3 is biased towards singleton clusters. Therefore, for a more balanced evaluation, we show improvements on both metrics simultaneously. 4.3 Results We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al. (2009).9 Table 2 compares the baseline perceptron results to the DT results and then shows the incremental addition of the Web features to the DT baseline (on the ACE04 development set). The DT classifier, in general, is precision-biased. The Web features somewhat balance this by increasing the recall and decreasing precision to a lesser extent, improving overall F1. Each feature type incrementally increases both MUC and B3 F1-measures, showing that they are not taking advantage of any bias of either metric. The incremental improvements also show that each Web feature type brings in some additional </context>
<context position="24664" citStr="Stoyanov et al. (2009)" startWordPosition="4082" endWordPosition="4085">as a perceptron on our ACE04 development experiments. It is, however, slower and more memory-intensive (-3x) on the bigger ACE05-ALL dataset. 394 MUC B3 System P R F1 P R F1 ACE04-TEST-RESULTS Stoyanov et al. (2009) - - 62.0 - - 76.5 Haghighi and Klein (2009) 67.5 61.6 64.4 77.4 69.4 73.2 Haghighi and Klein (2010) 67.4 66.6 67.0 81.2 73.3 77.0 This Work: Perceptron Baseline 65.5 61.9 63.7 84.1 70.9 77.0 This Work: DT Baseline 76.0 60.7 67.5 89.6 70.3 78.8 This Work: DT + Web Features 74.8 64.2 69.1 87.5 73.7 80.0 This Work: A of DT+Web over DT (p &lt; 0.05) 1.7 (p &lt; 0.005) 1.3 ACE05-TEST-RESULTS Stoyanov et al. (2009) - - 67.4 - - 73.7 Haghighi and Klein (2009) 73.1 58.8 65.2 82.1 63.9 71.8 Haghighi and Klein (2010) 74.6 62.7 68.1 83.2 68.4 75.1 This Work: Perceptron Baseline 72.2 61.6 66.5 85.0 65.5 73.9 This Work: DT Baseline 79.6 59.7 68.2 89.4 64.2 74.7 This Work: DT + Web Features 75.0 64.7 69.5 81.1 70.8 75.6 This Work: A of DT+Web over DT (p &lt; 0.12) 1.3 (p &lt; 0.1) 0.9 ACE05-ALL-TEST-RESULTS Rahman and Ng (2009) 75.4 64.1 69.3 54.4 70.5 61.4 Haghighi and Klein (2009) 72.9 60.2 67.0 53.2 73.1 61.6 Haghighi and Klein (2010) 77.0 66.9 71.6 55.4 74.8 63.8 This Work: Perceptron Baseline 68.9 60.4 64.4 80.6</context>
<context position="26007" citStr="Stoyanov et al. (2009)" startWordPosition="4321" endWordPosition="4324">5 This Work: A of DT+Web over DT (p &lt; 0.001) 2.1 (p &lt; 0.001) 1.9 Table 3: Primary test results on the ACE04, ACE05, and ACE05-ALL datasets. All systems reported here use automatically extracted system mentions. B3 here is the B3All version of Stoyanov et al. (2009). We also report statistical significance of the improvements from the Web features on the DT baseline, using the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1993). The perceptron baseline in this work (Reconcile settings: 15 iterations, threshold = 0.45, SIG for ACE04 and AP for ACE05, ACE05- ALL) has different results from Stoyanov et al. (2009) because their current publicly available code is different from that used in their paper (p.c.). Also, the B3 variant used by Rahman and Ng (2009) is slightly different from other systems (they remove all and only the singleton twinless system mentions, so it is neither B3All nor B3None). For completeness, our (untuned) B3None results (DT + Web) on the ACE05-ALL dataset are P=69.91R=65.91F1=67.8. class / sense information.10 Table 3 shows our primary test results on the ACE04, ACE05, and ACE05-ALL datasets, for the MUC and B3 metrics. All systems reported use automatically detected mentions. </context>
<context position="27991" citStr="Stoyanov et al. (2009)" startWordPosition="4635" endWordPosition="4638">render individual mentions. They learn on large datasets including 11All improvements are significant, except on the small ACE05 dataset with the MUC metric (where it is weak, at p &lt; 0.12). However, on the larger version of this dataset, ACE05-ALL, we get improvements which are both larger and more significant (at p &lt; 0.001). 395 Wikipedia, and their results are state-of-the-art in coreference resolution. We outperform their system on most datasets and metrics (except on ACE05- ALL for the MUC metric). The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al. (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). We develop our features and tune their hyperparameter values on the ACE04 development set and then use these on the ACE04 test set.12 On the ACE05 and ACE05-ALL datasets, we directly transfer our Web features and their hyper-parameter values from the ACE04 dev-set, without any retuning. The test improvements we get on all the datasets (see Table 3) suggest that our features are generally useful across datasets and metrics.13 5 Analysis In this section, we briefly discuss error</context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art. In Proceedings ofACL/IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
<author>Nathan Gilbert</author>
<author>Ellen Riloff</author>
<author>David Buttler</author>
<author>David Hysom</author>
</authors>
<title>Reconcile: A coreference resolution research platform. In</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>Cornell University.</institution>
<contexts>
<context position="3983" citStr="Stoyanov et al., 2010" startWordPosition="608" endWordPosition="611">(via Hearst-style hypernymy patterns), (3) similarity of entity-based context (e.g., common values of y for 389 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 389–398, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics which h is a y is attested), (4) matches of distributional soft cluster ids, and (5) attested substitutions of candidate antecedents in the context of a pronominal anaphor. We first describe a strong baseline consisting of the mention-pair model of the Reconcile system (Stoyanov et al., 2009; Stoyanov et al., 2010) using a decision tree (DT) as its pairwise classifier. To this baseline system, we add our suite of features in turn, each class of features providing substantial gains. Altogether, our final system produces the best numbers reported to date on end-to-end coreference resolution (with automatically detected system mentions) on multiple data sets (ACE 2004 and ACE 2005) and metrics (MUC and B3), achieving significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). 2 Baseline System Before describing our semantic Web features, we fi</context>
<context position="7889" citStr="Stoyanov et al., 2010" startWordPosition="1226" endWordPosition="1229">ion tree classifiers, including interpretability and robustness. However, we suspect that the aspect most relevant to our case is that decision trees can capture non-linear interactions between features. For example, recency is very important for pronoun reference but much less so for nominal reference. 3 Semantics via Web Features Our Web features for coreference resolution are simple and capture a range of diffuse world knowledge. Given a mention pair, we use the head finder in Reconcile to find the lexical heads of both mentions (for 1We use the default configuration settings of Reconcile (Stoyanov et al., 2010) unless mentioned otherwise. 390 example, the head of the Palestinian territories is the word territories). Next, we take each headword pair (h1, h2) and compute various Web-count functions on it that can signal whether or not this mention pair is coreferent. As the source of Web information, we use the Google n-grams corpus (Brants and Franz, 2006) which contains English n-grams (n = 1 to 5) and their Web frequency counts, derived from nearly 1 trillion word tokens and 95 billion sentences. Because we have many queries that must be run against this corpus, we apply the trie-based hashing algo</context>
</contexts>
<marker>Stoyanov, Cardie, Gilbert, Riloff, Buttler, Hysom, 2010</marker>
<rawString>Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010. Reconcile: A coreference resolution research platform. In Technical report, Cornell University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of MUC-6.</booktitle>
<contexts>
<context position="21381" citStr="Vilain et al., 1995" startWordPosition="3524" endWordPosition="3527"> and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). Note that most previous work does not report (or need) a standard development set; hence, for tuning our features and its hyper-parameters, we randomly split the original training data into a training and development set with a 70/30 ratio (and then use the full original training set during testing). Details of the corpora are shown in Table 1.7 Details of the Web-scale corpora used for extracting features are discussed in Section 3. 4.2 Evaluation Metrics We evaluated our work on both MUC (Vilain et al., 1995) and B3 (Bagga and Baldwin, 1998). Both scorers are available in the Reconcile infrastructure.8 MUC measures how many predicted clusters need to be merged to cover the true gold clusters. B3 computes precision and recall for each mention by computing the intersection of its predicted and gold cluster and dividing by the size of the predicted 7Note that the development set is used only for ACE04, because for ACE05, and ACE05-ALL, we directly test using the features tuned on ACE04. 8Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al. (20</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of MUC-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
</authors>
<title>Coreference resolution using semantic relatedness information from automatically discovered patterns.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1985" citStr="Yang and Su, 2007" startWordPosition="300" endWordPosition="303">s. There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, other anaphora, definite NP reference, and pronoun resolution, computing semantic compatibility via Web-hits and counts from large corpora. There is also work on end-to-end coreference resolution that uses large noun-similarity lists (Daum´e III and Marcu, 2005) or structured knowledge bases such as Wikipedia (Yang and Su, 2007; Haghighi and Klein, 2009; Kobdani et al., 2011) and YAGO (Rahman and Ng, 2011). However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. In this paper, we look to the Web for broader if shallower sources of semantics. In order to harness the information on the Web without presupposing a deep understanding of all Web text, we instead turn to a diverse collection of Web n-gram counts (Brants and Franz, 2006) which, in aggregate, contain diffu</context>
<context position="12923" citStr="Yang and Su (2007)" startWordPosition="2082" endWordPosition="2085"> the c12 count for the headword pair (leader, president) is 752, while for (voter, president), it is 0. Hypernymic semantic compatibility for coreference is intuitive and has been explored in varying forms by previous work. Poesio et al. (2004) and Markert and Nissim (2005) employ a subset of our Hearst patterns and Web-hits for the subtasks of bridging anaphora, other-anaphora, and definite NP resolution. Others (Haghighi and Klein, 2009; Rahman and Ng, 2011; Daum´e III and Marcu, 2005) use similar relations to extract compatibility statistics from Wikipedia, YAGO, and noun-similarity lists. Yang and Su (2007) use Wikipedia to automatically extract semantic patterns, which are then used as features in a learning setup. Instead of extracting patterns from the training data, we use all the above patterns, which helps us generalize to new datasets for end-to-end coreference resolution (see Section 4.3). 3.3 Entity-based context For each headword h, we first collect context seeds y using the pattern h {is |are |was |were} {a |an |the}? y taking seeds y in order of decreasing Web count. The corresponding ordered seed list Y = {y} gives us useful information about the headword’s entity type. For example,</context>
</contexts>
<marker>Yang, Su, 2007</marker>
<rawString>Xiaofeng Yang and Jian Su. 2007. Coreference resolution using semantic relatedness information from automatically discovered patterns. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Improving pronoun resolution using statistics-based semantic compatibility information.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1567" citStr="Yang et al., 2005" startWordPosition="239" endWordPosition="242">ogy, and education. His election campaign is expected to [...] For resolving coreference in this example, a system would benefit from the world knowledge that Obama is the president. Also, to resolve the pronoun his to the correct antecedent Obama, we can use the knowledge that Obama has an election campaign while Jobs does not. Such ambiguities are difficult to resolve on purely syntactic or configurational grounds. There have been multiple previous systems that incorporate some form of world knowledge in coreference resolution tasks. Most work (Poesio et al., 2004; Markert and Nissim, 2005; Yang et al., 2005; Bergsma and Lin, 2006) addresses special cases and subtasks such as bridging anaphora, other anaphora, definite NP reference, and pronoun resolution, computing semantic compatibility via Web-hits and counts from large corpora. There is also work on end-to-end coreference resolution that uses large noun-similarity lists (Daum´e III and Marcu, 2005) or structured knowledge bases such as Wikipedia (Yang and Su, 2007; Haghighi and Klein, 2009; Kobdani et al., 2011) and YAGO (Rahman and Ng, 2011). However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) </context>
<context position="18753" citStr="Yang et al. (2005)" startWordPosition="3083" endWordPosition="3086"> of the R1Gap feature, if the anaphor h2 + context is his victory and one candidate antecedent h1 is Bush, then we compute the normalized value count(“Bush &apos;s * victory”) count(“ * Is * victory&amp;quot;)count(“Bush&amp;quot;) In general, we compute count(“h1 Is * r&amp;quot;) count(“ * Is * r&amp;quot;)count(“h1&amp;quot;) The final feature value is again a normalized count converted to log10 and then binned.6 We have three separate features for the R1, R2, and R1Gap context types. We tune a separate bin-size hyperparameter for each of these three features. These pronoun resolution features are similar to selectional preference work by Yang et al. (2005) and Bergsma and Lin (2006), who compute semantic compatibility for pronouns in specific syntactic relationships such as possessive-noun, subject-verb, etc. In our case, we directly use the general context of any pronominal anaphor to find its most compatible antecedent. Note that all our above features are designed to be non-sparse by firing indicators of the quantized Web statistics and not the lexical- or class-based identities of the mention pair. This keeps the total number of features small, which is important for the relatively small datasets used for coreference resolution. We go from </context>
</contexts>
<marker>Yang, Su, Tan, 2005</marker>
<rawString>Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Improving pronoun resolution using statistics-based semantic compatibility information. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>